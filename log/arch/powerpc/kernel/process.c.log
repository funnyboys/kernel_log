commit 75613939084f59c0848b146e54ba463dc494c433
Merge: 93bbca271a71 c0e1c8c22beb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 21 10:02:53 2020 -0700

    Merge tag 'powerpc-5.8-3' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
    
     - One fix for the interrupt rework we did last release which broke
       KVM-PR
    
     - Three commits fixing some fallout from the READ_ONCE() changes
       interacting badly with our 8xx 16K pages support, which uses a pte_t
       that is a structure of 4 actual PTEs
    
     - A cleanup of the 8xx pte_update() to use the newly added pmd_off()
    
     - A fix for a crash when handling an oops if CONFIG_DEBUG_VIRTUAL is
       enabled
    
     - A minor fix for the SPU syscall generation
    
    Thanks to Aneesh Kumar K.V, Christian Zigotzky, Christophe Leroy, Mike
    Rapoport, Nicholas Piggin.
    
    * tag 'powerpc-5.8-3' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      powerpc/8xx: Provide ptep_get() with 16k pages
      mm: Allow arches to provide ptep_get()
      mm/gup: Use huge_ptep_get() in gup_hugepte()
      powerpc/syscalls: Use the number when building SPU syscall table
      powerpc/8xx: use pmd_off() to access a PMD entry in pte_update()
      powerpc/64s: Fix KVM interrupt using wrong save area
      powerpc: Fix kernel crash in show_instructions() w/DEBUG_VIRTUAL

commit 25f12ae45fc1931a1dce3cc59f9989a9d87834b0
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 17 09:37:55 2020 +0200

    maccess: rename probe_kernel_address to get_kernel_nofault
    
    Better describe what this helper does, and match the naming of
    copy_from_kernel_nofault.
    
    Also switch the argument order around, so that it acts and looks
    like get_user().
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index d4d0d1048500..30955a0c32d0 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1271,7 +1271,7 @@ static void show_instructions(struct pt_regs *regs)
 #endif
 
 		if (!__kernel_text_address(pc) ||
-		    probe_kernel_address((const void *)pc, instr)) {
+		    get_kernel_nofault(instr, (const void *)pc)) {
 			pr_cont("XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)

commit c0ee37e85e0e47402b8bbe35b6cec8e06937ca58
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 17 09:37:54 2020 +0200

    maccess: rename probe_user_{read,write} to copy_{from,to}_user_nofault
    
    Better describe what these functions do.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7bb7faf84490..d4d0d1048500 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1305,7 +1305,8 @@ void show_user_instructions(struct pt_regs *regs)
 		for (i = 0; i < 8 && n; i++, n--, pc += sizeof(int)) {
 			int instr;
 
-			if (probe_user_read(&instr, (void __user *)pc, sizeof(instr))) {
+			if (copy_from_user_nofault(&instr, (void __user *)pc,
+					sizeof(instr))) {
 				seq_buf_printf(&s, "XXXXXXXX ");
 				continue;
 			}

commit a6e2c226c3d51fd93636320e47cabc8a8f0824c5
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Sun May 24 15:08:19 2020 +0530

    powerpc: Fix kernel crash in show_instructions() w/DEBUG_VIRTUAL
    
    With CONFIG_DEBUG_VIRTUAL=y, we can hit a BUG() if we take a hard
    lockup watchdog interrupt when in OPAL mode.
    
    This happens in show_instructions() if the kernel takes the watchdog
    NMI IPI, or any other interrupt, with MSR_IR == 0. show_instructions()
    updates the variable pc in the loop and the second iteration will
    result in BUG().
    
    We hit the BUG_ON due the below check in  __va()
    
      #define __va(x)
      ({
            VIRTUAL_BUG_ON((unsigned long)(x) >= PAGE_OFFSET);
            (void *)(unsigned long)((phys_addr_t)(x) | PAGE_OFFSET);
      })
    
    Fix it by moving the check out of the loop. Also update nip so that
    the nip == pc check still matches.
    
    Fixes: 4dd7554a6456 ("powerpc/64: Add VIRTUAL_BUG_ON checks for __va and __pa addresses")
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    [mpe: Use IS_ENABLED(), massage change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200524093822.423487-1-aneesh.kumar@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7bb7faf84490..a2f1f0e70a4b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1252,29 +1252,31 @@ struct task_struct *__switch_to(struct task_struct *prev,
 static void show_instructions(struct pt_regs *regs)
 {
 	int i;
+	unsigned long nip = regs->nip;
 	unsigned long pc = regs->nip - (NR_INSN_TO_PRINT * 3 / 4 * sizeof(int));
 
 	printk("Instruction dump:");
 
+	/*
+	 * If we were executing with the MMU off for instructions, adjust pc
+	 * rather than printing XXXXXXXX.
+	 */
+	if (!IS_ENABLED(CONFIG_BOOKE) && !(regs->msr & MSR_IR)) {
+		pc = (unsigned long)phys_to_virt(pc);
+		nip = (unsigned long)phys_to_virt(regs->nip);
+	}
+
 	for (i = 0; i < NR_INSN_TO_PRINT; i++) {
 		int instr;
 
 		if (!(i % 8))
 			pr_cont("\n");
 
-#if !defined(CONFIG_BOOKE)
-		/* If executing with the IMMU off, adjust pc rather
-		 * than print XXXXXXXX.
-		 */
-		if (!(regs->msr & MSR_IR))
-			pc = (unsigned long)phys_to_virt(pc);
-#endif
-
 		if (!__kernel_text_address(pc) ||
 		    probe_kernel_address((const void *)pc, instr)) {
 			pr_cont("XXXXXXXX ");
 		} else {
-			if (regs->nip == pc)
+			if (nip == pc)
 				pr_cont("<%08x> ", instr);
 			else
 				pr_cont("%08x ", instr);

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 99d619f81cb5..7bb7faf84490 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -41,7 +41,6 @@
 #include <linux/pkeys.h>
 #include <linux/seq_buf.h>
 
-#include <asm/pgtable.h>
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/mmu.h>

commit 9cb8f069deeed708bf19486d5893e297dc467ae0
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:32:29 2020 -0700

    kernel: rename show_stack_loglvl() => show_stack()
    
    Now the last users of show_stack() got converted to use an explicit log
    level, show_stack_loglvl() can drop it's redundant suffix and become once
    again well known show_stack().
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-51-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a456b4454b3f..99d619f81cb5 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1456,7 +1456,7 @@ void show_regs(struct pt_regs * regs)
 	printk("NIP ["REG"] %pS\n", regs->nip, (void *)regs->nip);
 	printk("LR ["REG"] %pS\n", regs->link, (void *)regs->link);
 #endif
-	show_stack(current, (unsigned long *) regs->gpr[1]);
+	show_stack(current, (unsigned long *) regs->gpr[1], KERN_DEFAULT);
 	if (!user_mode(regs))
 		show_instructions(regs);
 }
@@ -2063,8 +2063,8 @@ unsigned long get_wchan(struct task_struct *p)
 
 static int kstack_depth_to_print = CONFIG_PRINT_STACK_DEPTH;
 
-void show_stack_loglvl(struct task_struct *tsk, unsigned long *stack,
-		       const char *loglvl)
+void show_stack(struct task_struct *tsk, unsigned long *stack,
+		const char *loglvl)
 {
 	unsigned long sp, ip, lr, newsp;
 	int count = 0;
@@ -2133,11 +2133,6 @@ void show_stack_loglvl(struct task_struct *tsk, unsigned long *stack,
 	put_task_stack(tsk);
 }
 
-void show_stack(struct task_struct *tsk, unsigned long *stack)
-{
-	show_stack_loglvl(tsk, stack, KERN_DEFAULT);
-}
-
 #ifdef CONFIG_PPC64
 /* Called with hard IRQs off */
 void notrace __ppc64_runlatch_on(void)

commit b9677a8cf60995acdd1b36ff59e6b437154bff9e
Author: Dmitry Safonov <dima@arista.com>
Date:   Mon Jun 8 21:31:14 2020 -0700

    powerpc: add show_stack_loglvl()
    
    Currently, the log-level of show_stack() depends on a platform
    realization.  It creates situations where the headers are printed with
    lower log level or higher than the stacktrace (depending on a platform or
    user).
    
    Furthermore, it forces the logic decision from user to an architecture
    side.  In result, some users as sysrq/kdb/etc are doing tricks with
    temporary rising console_loglevel while printing their messages.  And in
    result it not only may print unwanted messages from other CPUs, but also
    omit printing at all in the unlucky case where the printk() was deferred.
    
    Introducing log-level parameter and KERN_UNSUPPRESSED [1] seems an easier
    approach than introducing more printk buffers.  Also, it will consolidate
    printings with headers.
    
    Introduce show_stack_loglvl(), that eventually will substitute
    show_stack().
    
    [1]: https://lore.kernel.org/lkml/20190528002412.1625-1-dima@arista.com/T/#u
    
    Signed-off-by: Dmitry Safonov <dima@arista.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Link: http://lkml.kernel.org/r/20200418201944.482088-27-dima@arista.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 048d64c4e115..a456b4454b3f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -2063,7 +2063,8 @@ unsigned long get_wchan(struct task_struct *p)
 
 static int kstack_depth_to_print = CONFIG_PRINT_STACK_DEPTH;
 
-void show_stack(struct task_struct *tsk, unsigned long *stack)
+void show_stack_loglvl(struct task_struct *tsk, unsigned long *stack,
+		       const char *loglvl)
 {
 	unsigned long sp, ip, lr, newsp;
 	int count = 0;
@@ -2088,7 +2089,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	}
 
 	lr = 0;
-	printk("Call Trace:\n");
+	printk("%sCall Trace:\n", loglvl);
 	do {
 		if (!validate_sp(sp, tsk, STACK_FRAME_OVERHEAD))
 			break;
@@ -2097,7 +2098,8 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		newsp = stack[0];
 		ip = stack[STACK_FRAME_LR_SAVE];
 		if (!firstframe || ip != lr) {
-			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
+			printk("%s["REG"] ["REG"] %pS",
+				loglvl, sp, ip, (void *)ip);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 			ret_addr = ftrace_graph_ret_addr(current,
 						&ftrace_idx, ip, stack);
@@ -2119,8 +2121,9 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 			struct pt_regs *regs = (struct pt_regs *)
 				(sp + STACK_FRAME_OVERHEAD);
 			lr = regs->link;
-			printk("--- interrupt: %lx at %pS\n    LR = %pS\n",
-			       regs->trap, (void *)regs->nip, (void *)lr);
+			printk("%s--- interrupt: %lx at %pS\n    LR = %pS\n",
+			       loglvl, regs->trap,
+			       (void *)regs->nip, (void *)lr);
 			firstframe = 1;
 		}
 
@@ -2130,6 +2133,11 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	put_task_stack(tsk);
 }
 
+void show_stack(struct task_struct *tsk, unsigned long *stack)
+{
+	show_stack_loglvl(tsk, stack, KERN_DEFAULT);
+}
+
 #ifdef CONFIG_PPC64
 /* Called with hard IRQs off */
 void notrace __ppc64_runlatch_on(void)

commit 74c6881019b7d56c327fffc268d97adb5eb1b4f9
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:38 2020 +0530

    powerpc/watchpoint: Prepare handler to handle more than one watchpoint
    
    Currently we assume that we have only one watchpoint supported by hw.
    Get rid of that assumption and use dynamic loop instead. This should
    make supporting more watchpoints very easy.
    
    With more than one watchpoint, exception handler needs to know which
    DAWR caused the exception, and hw currently does not provide it. So
    we need sw logic for the same. To figure out which DAWR caused the
    exception, check all different combinations of user specified range,
    DAWR address range, actual access range and DAWRX constrains. For ex,
    if user specified range and actual access range overlaps but DAWRX is
    configured for readonly watchpoint and the instruction is store, this
    DAWR must not have caused exception.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    [mpe: Unsplit multi-line printk() strings, fix some sparse warnings]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200514111741.97993-14-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9b11575dcb8a..048d64c4e115 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -629,9 +629,6 @@ void do_break (struct pt_regs *regs, unsigned long address,
 	if (debugger_break_match(regs))
 		return;
 
-	/* Clear the breakpoint */
-	hw_breakpoint_disable();
-
 	/* Deliver the signal to userspace */
 	force_sig_fault(SIGTRAP, TRAP_HWBKPT, (void __user *)address);
 }

commit e68ef121c1f4c38edf87a3354661ceb99d522729
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:37 2020 +0530

    powerpc/watchpoint: Use builtin ALIGN*() macros
    
    Currently we calculate hw aligned start and end addresses manually.
    Replace them with builtin ALIGN_DOWN() and ALIGN() macros.
    
    So far end_addr was inclusive but this patch makes it exclusive (by
    avoiding -1) for better readability.
    
    Suggested-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    Link: https://lore.kernel.org/r/20200514111741.97993-13-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 77ec1299e2fd..9b11575dcb8a 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -800,12 +800,12 @@ static inline int set_breakpoint_8xx(struct arch_hw_breakpoint *brk)
 	unsigned long lctrl1 = LCTRL1_CTE_GT | LCTRL1_CTF_LT | LCTRL1_CRWE_RW |
 			       LCTRL1_CRWF_RW;
 	unsigned long lctrl2 = LCTRL2_LW0EN | LCTRL2_LW0LADC | LCTRL2_SLW0EN;
-	unsigned long start_addr = brk->address & ~HW_BREAKPOINT_ALIGN;
-	unsigned long end_addr = (brk->address + brk->len - 1) | HW_BREAKPOINT_ALIGN;
+	unsigned long start_addr = ALIGN_DOWN(brk->address, HW_BREAKPOINT_SIZE);
+	unsigned long end_addr = ALIGN(brk->address + brk->len, HW_BREAKPOINT_SIZE);
 
 	if (start_addr == 0)
 		lctrl2 |= LCTRL2_LW0LA_F;
-	else if (end_addr == ~0U)
+	else if (end_addr == 0)
 		lctrl2 |= LCTRL2_LW0LA_E;
 	else
 		lctrl2 |= LCTRL2_LW0LA_EandF;
@@ -821,7 +821,7 @@ static inline int set_breakpoint_8xx(struct arch_hw_breakpoint *brk)
 		lctrl1 |= LCTRL1_CRWE_WO | LCTRL1_CRWF_WO;
 
 	mtspr(SPRN_CMPE, start_addr - 1);
-	mtspr(SPRN_CMPF, end_addr + 1);
+	mtspr(SPRN_CMPF, end_addr);
 	mtspr(SPRN_LCTRL1, lctrl1);
 	mtspr(SPRN_LCTRL2, lctrl2);
 

commit 6b424efa119d5ea06b15ff240dddc3b4b9f9cdfb
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:35 2020 +0530

    powerpc/watchpoint: Use loop for thread_struct->ptrace_bps
    
    ptrace_bps is already an array of size HBP_NUM_MAX. But we use
    hardcoded index 0 while fetching/updating it. Convert such code
    to loop over array.
    
    ptrace interface to use multiple watchpoint remains same. eg:
    two PPC_PTRACE_SETHWDEBUG calls will create two watchpoint if
    underneath hw supports it.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    Link: https://lore.kernel.org/r/20200514111741.97993-11-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index d94d8925711c..77ec1299e2fd 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1604,6 +1604,9 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long usp,
 	void (*f)(void);
 	unsigned long sp = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 	struct thread_info *ti = task_thread_info(p);
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	int i;
+#endif
 
 	klp_init_thread_info(p);
 
@@ -1663,7 +1666,8 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long usp,
 	p->thread.ksp_limit = (unsigned long)end_of_stack(p);
 #endif
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
-	p->thread.ptrace_bps[0] = NULL;
+	for (i = 0; i < nr_wp_slots(); i++)
+		p->thread.ptrace_bps[i] = NULL;
 #endif
 
 	p->thread.fp_save_area = NULL;

commit 303e6a9ddcdc168e92253c78cdb4bbe1e10d78b3
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:34 2020 +0530

    powerpc/watchpoint: Convert thread_struct->hw_brk to an array
    
    So far powerpc hw supported only one watchpoint. But Power10 is
    introducing 2nd DAWR. Convert thread_struct->hw_brk into an array.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    Link: https://lore.kernel.org/r/20200514111741.97993-10-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f303aea61794..d94d8925711c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -711,21 +711,49 @@ void switch_booke_debug_regs(struct debug_reg *new_debug)
 EXPORT_SYMBOL_GPL(switch_booke_debug_regs);
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
-static void set_breakpoint(struct arch_hw_breakpoint *brk)
+static void set_breakpoint(int i, struct arch_hw_breakpoint *brk)
 {
 	preempt_disable();
-	__set_breakpoint(0, brk);
+	__set_breakpoint(i, brk);
 	preempt_enable();
 }
 
 static void set_debug_reg_defaults(struct thread_struct *thread)
 {
-	thread->hw_brk.address = 0;
-	thread->hw_brk.type = 0;
-	thread->hw_brk.len = 0;
-	thread->hw_brk.hw_len = 0;
-	if (ppc_breakpoint_available())
-		set_breakpoint(&thread->hw_brk);
+	int i;
+	struct arch_hw_breakpoint null_brk = {0};
+
+	for (i = 0; i < nr_wp_slots(); i++) {
+		thread->hw_brk[i] = null_brk;
+		if (ppc_breakpoint_available())
+			set_breakpoint(i, &thread->hw_brk[i]);
+	}
+}
+
+static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
+				struct arch_hw_breakpoint *b)
+{
+	if (a->address != b->address)
+		return false;
+	if (a->type != b->type)
+		return false;
+	if (a->len != b->len)
+		return false;
+	/* no need to check hw_len. it's calculated from address and len */
+	return true;
+}
+
+static void switch_hw_breakpoint(struct task_struct *new)
+{
+	int i;
+
+	for (i = 0; i < nr_wp_slots(); i++) {
+		if (likely(hw_brk_match(this_cpu_ptr(&current_brk[i]),
+					&new->thread.hw_brk[i])))
+			continue;
+
+		__set_breakpoint(i, &new->thread.hw_brk[i]);
+	}
 }
 #endif /* !CONFIG_HAVE_HW_BREAKPOINT */
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
@@ -829,19 +857,6 @@ bool ppc_breakpoint_available(void)
 }
 EXPORT_SYMBOL_GPL(ppc_breakpoint_available);
 
-static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
-			      struct arch_hw_breakpoint *b)
-{
-	if (a->address != b->address)
-		return false;
-	if (a->type != b->type)
-		return false;
-	if (a->len != b->len)
-		return false;
-	/* no need to check hw_len. it's calculated from address and len */
-	return true;
-}
-
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 
 static inline bool tm_enabled(struct task_struct *tsk)
@@ -1174,8 +1189,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
  * schedule DABR
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
-	if (unlikely(!hw_brk_match(this_cpu_ptr(&current_brk[0]), &new->thread.hw_brk)))
-		__set_breakpoint(0, &new->thread.hw_brk);
+	switch_hw_breakpoint(new);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 

commit 4a8a9379f2af4c9928529b3959bc2d8f7023c6bc
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:31 2020 +0530

    powerpc/watchpoint: Provide DAWR number to __set_breakpoint
    
    Introduce new parameter 'nr' to __set_breakpoint() which indicates
    which DAWR should be programed. Also convert current_brk variable
    to an array.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    Link: https://lore.kernel.org/r/20200514111741.97993-7-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dc161b0adc82..f303aea61794 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -637,7 +637,7 @@ void do_break (struct pt_regs *regs, unsigned long address,
 }
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 
-static DEFINE_PER_CPU(struct arch_hw_breakpoint, current_brk);
+static DEFINE_PER_CPU(struct arch_hw_breakpoint, current_brk[HBP_NUM_MAX]);
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 /*
@@ -714,7 +714,7 @@ EXPORT_SYMBOL_GPL(switch_booke_debug_regs);
 static void set_breakpoint(struct arch_hw_breakpoint *brk)
 {
 	preempt_disable();
-	__set_breakpoint(brk);
+	__set_breakpoint(0, brk);
 	preempt_enable();
 }
 
@@ -800,13 +800,13 @@ static inline int set_breakpoint_8xx(struct arch_hw_breakpoint *brk)
 	return 0;
 }
 
-void __set_breakpoint(struct arch_hw_breakpoint *brk)
+void __set_breakpoint(int nr, struct arch_hw_breakpoint *brk)
 {
-	memcpy(this_cpu_ptr(&current_brk), brk, sizeof(*brk));
+	memcpy(this_cpu_ptr(&current_brk[nr]), brk, sizeof(*brk));
 
 	if (dawr_enabled())
 		// Power8 or later
-		set_dawr(0, brk);
+		set_dawr(nr, brk);
 	else if (IS_ENABLED(CONFIG_PPC_8xx))
 		set_breakpoint_8xx(brk);
 	else if (!cpu_has_feature(CPU_FTR_ARCH_207S))
@@ -1174,8 +1174,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
  * schedule DABR
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
-	if (unlikely(!hw_brk_match(this_cpu_ptr(&current_brk), &new->thread.hw_brk)))
-		__set_breakpoint(&new->thread.hw_brk);
+	if (unlikely(!hw_brk_match(this_cpu_ptr(&current_brk[0]), &new->thread.hw_brk)))
+		__set_breakpoint(0, &new->thread.hw_brk);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 

commit a18b834625d345bfa89c4e2754dd6cbb0133c4d7
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu May 14 16:47:30 2020 +0530

    powerpc/watchpoint: Provide DAWR number to set_dawr
    
    Introduce new parameter 'nr' to set_dawr() which indicates which DAWR
    should be programed.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Michael Neuling <mikey@neuling.org>
    Link: https://lore.kernel.org/r/20200514111741.97993-6-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index db766252238f..dc161b0adc82 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -806,7 +806,7 @@ void __set_breakpoint(struct arch_hw_breakpoint *brk)
 
 	if (dawr_enabled())
 		// Power8 or later
-		set_dawr(brk);
+		set_dawr(0, brk);
 	else if (IS_ENABLED(CONFIG_PPC_8xx))
 		set_breakpoint_8xx(brk);
 	else if (!cpu_has_feature(CPU_FTR_ARCH_207S))

commit 912237ea166428edcbf3c137adf12cb987c477f2
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 7 22:13:31 2020 +1000

    powerpc: trap_is_syscall() helper to hide syscall trap number
    
    A new system call interrupt will be added with a new trap number.
    Hide the explicit 0xc00 test behind an accessor to reduce churn
    in callers.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Make it a static inline]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200507121332.2233629-3-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8af3583546b7..db766252238f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1413,7 +1413,7 @@ void show_regs(struct pt_regs * regs)
 	print_msr_bits(regs->msr);
 	pr_cont("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
-	if ((TRAP(regs) != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
+	if (!trap_is_syscall(regs) && cpu_has_feature(CPU_FTR_CFAR))
 		pr_cont("CFAR: "REG" ", regs->orig_gpr3);
 	if (trap == 0x200 || trap == 0x300 || trap == 0x600)
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)

commit feb9df3462e688d073848d85c8bb132fe8fd9ae5
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 7 22:13:29 2020 +1000

    powerpc/64s: Always has full regs, so remove remnant checks
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200507121332.2233629-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8479c762aef2..8af3583546b7 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1720,7 +1720,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	 * FULL_REGS(regs) return true.  This is necessary to allow
 	 * ptrace to examine the thread immediately after exec.
 	 */
-	regs->trap &= ~1UL;
+	SET_FULL_REGS(regs);
 
 #ifdef CONFIG_PPC32
 	regs->mq = 0;

commit c420644c0a8f8839ca7269acbb8a3fc7fe1ec97d
Author: Haren Myneni <haren@linux.ibm.com>
Date:   Wed Apr 15 23:08:11 2020 -0700

    powerpc: Use mm_context vas_windows counter to issue CP_ABORT
    
    set_thread_uses_vas() sets used_vas flag for a process that opened VAS
    window and issue CP_ABORT during context switch for only that process.
    In multi-thread application, windows can be shared. For example Thread
    A can open a window and Thread B can run COPY/PASTE instructions to
    send NX request which may cause corruption or snooping or a covert
    channel Also once this flag is set, continue to run CP_ABORT even the
    VAS window is closed.
    
    So define vas-windows counter in process mm_context, increment this
    counter for each window open and decrement it for window close. If
    vas-windows is set, issue CP_ABORT during context switch. It means
    clear the foreign real address mapping only if the process / thread
    uses COPY/PASTE. Then disable it for that process if windows are not
    open.
    
    Moved set_thread_uses_vas() code to vas_tx_win_open() as this
    functionality is needed only for userspace open windows. We are adding
    VAS userspace support along with this fix. So no need to include this
    fix in stable releases.
    
    Fixes: 9d2a4d71332c ("powerpc: Define set_thread_uses_vas()")
    Signed-off-by: Haren Myneni <haren@linux.ibm.com>
    Reported-by: Nicholas Piggin <npiggin@gmail.com>
    Suggested-by: Milton Miller <miltonm@us.ibm.com>
    Suggested-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/1587017291.2275.1077.camel@hbabu-laptop

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9c21288f8645..8479c762aef2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1228,7 +1228,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		 * mappings, we must issue a cp_abort to clear any state and
 		 * prevent snooping, corruption or a covert channel.
 		 */
-		if (current->thread.used_vas)
+		if (current->mm &&
+			atomic_read(&current->mm->context.vas_windows))
 			asm volatile(PPC_CP_ABORT);
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
@@ -1467,27 +1468,6 @@ void arch_setup_new_exec(void)
 }
 #endif
 
-int set_thread_uses_vas(void)
-{
-#ifdef CONFIG_PPC_BOOK3S_64
-	if (!cpu_has_feature(CPU_FTR_ARCH_300))
-		return -EINVAL;
-
-	current->thread.used_vas = 1;
-
-	/*
-	 * Even a process that has no foreign real address mapping can use
-	 * an unpaired COPY instruction (to no real effect). Issue CP_ABORT
-	 * to clear any pending COPY and prevent a covert channel.
-	 *
-	 * __switch_to() will issue CP_ABORT on future context switches.
-	 */
-	asm volatile(PPC_CP_ABORT);
-
-#endif /* CONFIG_PPC_BOOK3S_64 */
-	return 0;
-}
-
 #ifdef CONFIG_PPC64
 /**
  * Assign a TIDR (thread ID) for task @t and set it in the thread

commit 6cc0c16d82f889f0083f3608237189afb55b67be
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 26 03:35:37 2020 +1000

    powerpc/64s: Implement interrupt exit logic in C
    
    Implement the bulk of interrupt return logic in C. The asm return code
    must handle a few cases: restoring full GPRs, and emulating stack
    store.
    
    The stack store emulation is significantly simplfied, rather than
    creating a new return frame and switching to that before performing
    the store, it uses the PACA to keep a scratch register around to
    perform the store.
    
    The asm return code is moved into 64e for now. The new logic has made
    allowance for 64e, but I don't have a full environment that works well
    to test it, and even booting in emulated qemu is not great for stress
    testing. 64e shouldn't be too far off working with this, given a bit
    more testing and auditing of the logic.
    
    This is slightly faster on a POWER9 (page fault speed increases about
    1.1%), probably due to reduced mtmsrd.
    
    mpe: Includes fixes from Nick for _TIF_EMULATE_STACK_STORE
    handling (including the fast_interrupt_return path), to remove
    trace_hardirqs_on(), and fixes the interrupt-return part of the
    MSR_VSX restore bug caught by tm-unavailable selftest.
    
    mpe: Incorporate fix from Nick:
    
    The return-to-kernel path has to replay any soft-pending interrupts if
    it is returning to a context that had interrupts soft-enabled. It has
    to do this carefully and avoid plain enabling interrupts if this is an
    irq context, which can cause multiple nesting of interrupts on the
    stack, and other unexpected issues.
    
    The code which avoided this case got the soft-mask state wrong, and
    marked interrupts as enabled before going around again to retry. This
    seems to be mostly harmless except when PREEMPT=y, this calls
    preempt_schedule_irq with irqs apparently enabled and runs into a BUG
    in kernel/sched/core.c
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200225173541.1549955-29-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 009833f928bf..9c21288f8645 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -236,23 +236,9 @@ void enable_kernel_fp(void)
 	}
 }
 EXPORT_SYMBOL(enable_kernel_fp);
-
-static int restore_fp(struct task_struct *tsk)
-{
-	if (tsk->thread.load_fp) {
-		load_fp_state(&current->thread.fp_state);
-		current->thread.load_fp++;
-		return 1;
-	}
-	return 0;
-}
-#else
-static int restore_fp(struct task_struct *tsk) { return 0; }
 #endif /* CONFIG_PPC_FPU */
 
 #ifdef CONFIG_ALTIVEC
-#define loadvec(thr) ((thr).load_vec)
-
 static void __giveup_altivec(struct task_struct *tsk)
 {
 	unsigned long msr;
@@ -318,21 +304,6 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 	}
 }
 EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
-
-static int restore_altivec(struct task_struct *tsk)
-{
-	if (cpu_has_feature(CPU_FTR_ALTIVEC) && (tsk->thread.load_vec)) {
-		load_vr_state(&tsk->thread.vr_state);
-		tsk->thread.used_vr = 1;
-		tsk->thread.load_vec++;
-
-		return 1;
-	}
-	return 0;
-}
-#else
-#define loadvec(thr) 0
-static inline int restore_altivec(struct task_struct *tsk) { return 0; }
 #endif /* CONFIG_ALTIVEC */
 
 #ifdef CONFIG_VSX
@@ -400,18 +371,6 @@ void flush_vsx_to_thread(struct task_struct *tsk)
 	}
 }
 EXPORT_SYMBOL_GPL(flush_vsx_to_thread);
-
-static int restore_vsx(struct task_struct *tsk)
-{
-	if (cpu_has_feature(CPU_FTR_VSX)) {
-		tsk->thread.used_vsr = 1;
-		return 1;
-	}
-
-	return 0;
-}
-#else
-static inline int restore_vsx(struct task_struct *tsk) { return 0; }
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_SPE
@@ -511,6 +470,53 @@ void giveup_all(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(giveup_all);
 
+#ifdef CONFIG_PPC_BOOK3S_64
+#ifdef CONFIG_PPC_FPU
+static int restore_fp(struct task_struct *tsk)
+{
+	if (tsk->thread.load_fp) {
+		load_fp_state(&current->thread.fp_state);
+		current->thread.load_fp++;
+		return 1;
+	}
+	return 0;
+}
+#else
+static int restore_fp(struct task_struct *tsk) { return 0; }
+#endif /* CONFIG_PPC_FPU */
+
+#ifdef CONFIG_ALTIVEC
+#define loadvec(thr) ((thr).load_vec)
+static int restore_altivec(struct task_struct *tsk)
+{
+	if (cpu_has_feature(CPU_FTR_ALTIVEC) && (tsk->thread.load_vec)) {
+		load_vr_state(&tsk->thread.vr_state);
+		tsk->thread.used_vr = 1;
+		tsk->thread.load_vec++;
+
+		return 1;
+	}
+	return 0;
+}
+#else
+#define loadvec(thr) 0
+static inline int restore_altivec(struct task_struct *tsk) { return 0; }
+#endif /* CONFIG_ALTIVEC */
+
+#ifdef CONFIG_VSX
+static int restore_vsx(struct task_struct *tsk)
+{
+	if (cpu_has_feature(CPU_FTR_VSX)) {
+		tsk->thread.used_vsr = 1;
+		return 1;
+	}
+
+	return 0;
+}
+#else
+static inline int restore_vsx(struct task_struct *tsk) { return 0; }
+#endif /* CONFIG_VSX */
+
 /*
  * The exception exit path calls restore_math() with interrupts hard disabled
  * but the soft irq state not "reconciled". ftrace code that calls
@@ -551,6 +557,7 @@ void notrace restore_math(struct pt_regs *regs)
 
 	regs->msr = msr;
 }
+#endif
 
 static void save_all(struct task_struct *tsk)
 {

commit a2e366832f3f4d5e1b47b7c7f7c41977bd5100f4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Mar 25 20:41:44 2020 +1000

    powerpc/64: mark emergency stacks valid to unwind
    
    Before:
    
      WARNING: CPU: 0 PID: 494 at arch/powerpc/kernel/irq.c:343
      CPU: 0 PID: 494 Comm: a Tainted: G        W
      NIP:  c00000000001ed2c LR: c000000000d13190 CTR: c00000000003f910
      REGS: c0000001fffd3870 TRAP: 0700   Tainted: G        W
      MSR:  8000000000021003 <SF,ME,RI,LE>  CR: 28000488  XER: 00000000
      CFAR: c00000000001ec90 IRQMASK: 0
      GPR00: c000000000aeb12c c0000001fffd3b00 c0000000012ba300 0000000000000000
      GPR04: 0000000000000000 0000000000000000 000000010bd207c8 6b00696e74657272
      GPR08: 0000000000000000 0000000000000000 0000000000000000 efbeadde00000000
      GPR12: 0000000000000000 c0000000014a0000 0000000000000000 0000000000000000
      GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR20: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR24: 0000000000000000 0000000000000000 0000000000000000 000000010bd207bc
      GPR28: 0000000000000000 c00000000148a898 0000000000000000 c0000001ffff3f50
      NIP [c00000000001ed2c] arch_local_irq_restore.part.0+0xac/0x100
      LR [c000000000d13190] _raw_spin_unlock_irqrestore+0x50/0xc0
      Call Trace:
      Instruction dump:
      60000000 7d2000a6 71298000 41820068 39200002 7d210164 4bffff9c 60000000
      60000000 7d2000a6 71298000 4c820020 <0fe00000> 4e800020 60000000 60000000
    
    After:
    
      WARNING: CPU: 0 PID: 499 at arch/powerpc/kernel/irq.c:343
      CPU: 0 PID: 499 Comm: a Not tainted
      NIP:  c00000000001ed2c LR: c000000000d13210 CTR: c00000000003f980
      REGS: c0000001fffd3870 TRAP: 0700   Not tainted
      MSR:  8000000000021003 <SF,ME,RI,LE>  CR: 28000488  XER: 00000000
      CFAR: c00000000001ec90 IRQMASK: 0
      GPR00: c000000000aeb1ac c0000001fffd3b00 c0000000012ba300 0000000000000000
      GPR04: 0000000000000000 0000000000000000 00000001347607c8 6b00696e74657272
      GPR08: 0000000000000000 0000000000000000 0000000000000000 efbeadde00000000
      GPR12: 0000000000000000 c0000000014a0000 0000000000000000 0000000000000000
      GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR20: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR24: 0000000000000000 0000000000000000 0000000000000000 00000001347607bc
      GPR28: 0000000000000000 c00000000148a898 0000000000000000 c0000001ffff3f50
      NIP [c00000000001ed2c] arch_local_irq_restore.part.0+0xac/0x100
      LR [c000000000d13210] _raw_spin_unlock_irqrestore+0x50/0xc0
      Call Trace:
      [c0000001fffd3b20] [c000000000aeb1ac] of_find_property+0x6c/0x90
      [c0000001fffd3b70] [c000000000aeb1f0] of_get_property+0x20/0x40
      [c0000001fffd3b90] [c000000000042cdc] rtas_token+0x3c/0x70
      [c0000001fffd3bb0] [c0000000000dc318] fwnmi_release_errinfo+0x28/0x70
      [c0000001fffd3c10] [c0000000000dcd8c] pseries_machine_check_realmode+0x1dc/0x540
      [c0000001fffd3cd0] [c00000000003fe04] machine_check_early+0x54/0x70
      [c0000001fffd3d00] [c000000000008384] machine_check_early_common+0x134/0x1f0
      --- interrupt: 200 at 0x1347607c8
          LR = 0x7fffafbd8328
      Instruction dump:
      60000000 7d2000a6 71298000 41820068 39200002 7d210164 4bffff9c 60000000
      60000000 7d2000a6 71298000 4c820020 <0fe00000> 4e800020 60000000 60000000
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200325104144.158362-1-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 110db94cdf3c..009833f928bf 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1974,6 +1974,32 @@ static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,
 	return 0;
 }
 
+static inline int valid_emergency_stack(unsigned long sp, struct task_struct *p,
+					unsigned long nbytes)
+{
+#ifdef CONFIG_PPC64
+	unsigned long stack_page;
+	unsigned long cpu = task_cpu(p);
+
+	stack_page = (unsigned long)paca_ptrs[cpu]->emergency_sp - THREAD_SIZE;
+	if (sp >= stack_page && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+
+# ifdef CONFIG_PPC_BOOK3S_64
+	stack_page = (unsigned long)paca_ptrs[cpu]->nmi_emergency_sp - THREAD_SIZE;
+	if (sp >= stack_page && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+
+	stack_page = (unsigned long)paca_ptrs[cpu]->mc_emergency_sp - THREAD_SIZE;
+	if (sp >= stack_page && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+# endif
+#endif
+
+	return 0;
+}
+
+
 int validate_sp(unsigned long sp, struct task_struct *p,
 		       unsigned long nbytes)
 {
@@ -1985,7 +2011,10 @@ int validate_sp(unsigned long sp, struct task_struct *p,
 	if (sp >= stack_page && sp <= stack_page + THREAD_SIZE - nbytes)
 		return 1;
 
-	return valid_irq_stack(sp, p, nbytes);
+	if (valid_irq_stack(sp, p, nbytes))
+		return 1;
+
+	return valid_emergency_stack(sp, p, nbytes);
 }
 
 EXPORT_SYMBOL(validate_sp);

commit 3d13e839e801e081bdece0127c2affa33d0f77cf
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Feb 20 22:51:37 2020 +1100

    powerpc: Rename current_stack_pointer() to current_stack_frame()
    
    current_stack_pointer(), which was called __get_SP(), used to just
    return the value in r1.
    
    But that caused problems in some cases, so it was turned into a
    function in commit bfe9a2cfe91a ("powerpc: Reimplement __get_SP() as a
    function not a define").
    
    Because it's a function in a separate compilation unit to all its
    callers, it has the effect of causing a stack frame to be created, and
    then returns the address of that frame. This is good in some cases
    like those described in the above commit, but in other cases it's
    overkill, we just need to know what stack page we're on.
    
    On some other arches current_stack_pointer is just a register global
    giving the stack pointer, and we'd like to do that too. So rename our
    current_stack_pointer() to current_stack_frame() to make that
    possible.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Link: https://lore.kernel.org/r/20200220115141.2707-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e730b8e522b0..110db94cdf3c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -2051,7 +2051,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	sp = (unsigned long) stack;
 	if (sp == 0) {
 		if (tsk == current)
-			sp = current_stack_pointer();
+			sp = current_stack_frame();
 		else
 			sp = tsk->thread.ksp;
 	}

commit ba32f4b02105e57627912b42e141d65d90074c64
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Jan 29 19:50:07 2020 +0000

    powerpc/process: Remove unneccessary #ifdef CONFIG_PPC64 in copy_thread_tls()
    
    is_32bit_task() exists on both PPC64 and PPC32, no need of an ifdefery.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/6ecbda05b4119c40222dc8ec284604e1597c9bff.1580327381.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index fad50db9dcf2..e730b8e522b0 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1634,11 +1634,9 @@ int copy_thread_tls(unsigned long clone_flags, unsigned long usp,
 		p->thread.regs = childregs;
 		childregs->gpr[3] = 0;  /* Result from fork() */
 		if (clone_flags & CLONE_SETTLS) {
-#ifdef CONFIG_PPC64
 			if (!is_32bit_task())
 				childregs->gpr[13] = tls;
 			else
-#endif
 				childregs->gpr[2] = tls;
 		}
 

commit def0bfdbd6039e96a9eb2baaa4470b079daab0d4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 23 17:30:47 2020 +0000

    powerpc: use probe_user_read() and probe_user_write()
    
    Instead of opencoding, use probe_user_read() to failessly read
    a user location and probe_user_write() for writing to user.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/e041f5eedb23f09ab553be8a91c3de2087147320.1579800517.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7fcf72e58826..fad50db9dcf2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1277,16 +1277,6 @@ void show_user_instructions(struct pt_regs *regs)
 
 	pc = regs->nip - (NR_INSN_TO_PRINT * 3 / 4 * sizeof(int));
 
-	/*
-	 * Make sure the NIP points at userspace, not kernel text/data or
-	 * elsewhere.
-	 */
-	if (!__access_ok(pc, NR_INSN_TO_PRINT * sizeof(int), USER_DS)) {
-		pr_info("%s[%d]: Bad NIP, not dumping instructions.\n",
-			current->comm, current->pid);
-		return;
-	}
-
 	seq_buf_init(&s, buf, sizeof(buf));
 
 	while (n) {
@@ -1297,7 +1287,7 @@ void show_user_instructions(struct pt_regs *regs)
 		for (i = 0; i < 8 && n; i++, n--, pc += sizeof(int)) {
 			int instr;
 
-			if (probe_kernel_address((const void *)pc, instr)) {
+			if (probe_user_read(&instr, (void __user *)pc, sizeof(instr))) {
 				seq_buf_printf(&s, "XXXXXXXX ");
 				continue;
 			}

commit 39413ae009674c6ba745850515b551bbb9d6374b
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Nov 26 17:43:29 2019 +0000

    powerpc/hw_breakpoints: Rewrite 8xx breakpoints to allow any address range size.
    
    Unlike standard powerpc, Powerpc 8xx doesn't have SPRN_DABR, but
    it has a breakpoint support based on a set of comparators which
    allow more flexibility.
    
    Commit 4ad8622dc548 ("powerpc/8xx: Implement hw_breakpoint")
    implemented breakpoints by emulating the DABR behaviour. It did
    this by setting one comparator the match 4 bytes at breakpoint address
    and the other comparator to match 4 bytes at breakpoint address + 4.
    
    Rewrite 8xx hw_breakpoint to make breakpoints match all addresses
    defined by the breakpoint address and length by making full use of
    comparators.
    
    Now, comparator E is set to match any address greater than breakpoint
    address minus one. Comparator F is set to match any address lower than
    breakpoint address plus breakpoint length. Addresses are aligned
    to 32 bits.
    
    When the breakpoint range starts at address 0, the breakpoint is set
    to match comparator F only. When the breakpoint range end at address
    0xffffffff, the breakpoint is set to match comparator E only.
    Otherwise the breakpoint is set to match comparator E and F.
    
    At the same time, use registers bit names instead of hardcoded values.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/05105deeaf63bc02151aea2cdeaf525534e0e9d4.1574790198.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4df94b6e2f32..7fcf72e58826 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -740,28 +740,6 @@ static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
 		mtspr(SPRN_DABRX, dabrx);
 	return 0;
 }
-#elif defined(CONFIG_PPC_8xx)
-static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
-{
-	unsigned long addr = dabr & ~HW_BRK_TYPE_DABR;
-	unsigned long lctrl1 = 0x90000000; /* compare type: equal on E & F */
-	unsigned long lctrl2 = 0x8e000002; /* watchpoint 1 on cmp E | F */
-
-	if ((dabr & HW_BRK_TYPE_RDWR) == HW_BRK_TYPE_READ)
-		lctrl1 |= 0xa0000;
-	else if ((dabr & HW_BRK_TYPE_RDWR) == HW_BRK_TYPE_WRITE)
-		lctrl1 |= 0xf0000;
-	else if ((dabr & HW_BRK_TYPE_RDWR) == 0)
-		lctrl2 = 0;
-
-	mtspr(SPRN_LCTRL2, 0);
-	mtspr(SPRN_CMPE, addr);
-	mtspr(SPRN_CMPF, addr + 4);
-	mtspr(SPRN_LCTRL1, lctrl1);
-	mtspr(SPRN_LCTRL2, lctrl2);
-
-	return 0;
-}
 #else
 static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
 {
@@ -782,6 +760,39 @@ static inline int set_dabr(struct arch_hw_breakpoint *brk)
 	return __set_dabr(dabr, dabrx);
 }
 
+static inline int set_breakpoint_8xx(struct arch_hw_breakpoint *brk)
+{
+	unsigned long lctrl1 = LCTRL1_CTE_GT | LCTRL1_CTF_LT | LCTRL1_CRWE_RW |
+			       LCTRL1_CRWF_RW;
+	unsigned long lctrl2 = LCTRL2_LW0EN | LCTRL2_LW0LADC | LCTRL2_SLW0EN;
+	unsigned long start_addr = brk->address & ~HW_BREAKPOINT_ALIGN;
+	unsigned long end_addr = (brk->address + brk->len - 1) | HW_BREAKPOINT_ALIGN;
+
+	if (start_addr == 0)
+		lctrl2 |= LCTRL2_LW0LA_F;
+	else if (end_addr == ~0U)
+		lctrl2 |= LCTRL2_LW0LA_E;
+	else
+		lctrl2 |= LCTRL2_LW0LA_EandF;
+
+	mtspr(SPRN_LCTRL2, 0);
+
+	if ((brk->type & HW_BRK_TYPE_RDWR) == 0)
+		return 0;
+
+	if ((brk->type & HW_BRK_TYPE_RDWR) == HW_BRK_TYPE_READ)
+		lctrl1 |= LCTRL1_CRWE_RO | LCTRL1_CRWF_RO;
+	if ((brk->type & HW_BRK_TYPE_RDWR) == HW_BRK_TYPE_WRITE)
+		lctrl1 |= LCTRL1_CRWE_WO | LCTRL1_CRWF_WO;
+
+	mtspr(SPRN_CMPE, start_addr - 1);
+	mtspr(SPRN_CMPF, end_addr + 1);
+	mtspr(SPRN_LCTRL1, lctrl1);
+	mtspr(SPRN_LCTRL2, lctrl2);
+
+	return 0;
+}
+
 void __set_breakpoint(struct arch_hw_breakpoint *brk)
 {
 	memcpy(this_cpu_ptr(&current_brk), brk, sizeof(*brk));
@@ -789,6 +800,8 @@ void __set_breakpoint(struct arch_hw_breakpoint *brk)
 	if (dawr_enabled())
 		// Power8 or later
 		set_dawr(brk);
+	else if (IS_ENABLED(CONFIG_PPC_8xx))
+		set_breakpoint_8xx(brk);
 	else if (!cpu_has_feature(CPU_FTR_ARCH_207S))
 		// Power7 or earlier
 		set_dabr(brk);

commit b57aeab811db07295f646808b1b17c312d17f57d
Author: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
Date:   Thu Oct 17 15:01:59 2019 +0530

    powerpc/watchpoint: Fix length calculation for unaligned target
    
    Watchpoint match range is always doubleword(8 bytes) aligned on
    powerpc. If the given range is crossing doubleword boundary, we need
    to increase the length such that next doubleword also get
    covered. Ex,
    
              address   len = 6 bytes
                    |=========.
       |------------v--|------v--------|
       | | | | | | | | | | | | | | | | |
       |---------------|---------------|
        <---8 bytes--->
    
    In such case, current code configures hw as:
      start_addr = address & ~HW_BREAKPOINT_ALIGN
      len = 8 bytes
    
    And thus read/write in last 4 bytes of the given range is ignored.
    Fix this by including next doubleword in the length.
    
    Signed-off-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191017093204.7511-3-ravi.bangoria@linux.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 639ceae7da9d..4df94b6e2f32 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -715,6 +715,8 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 {
 	thread->hw_brk.address = 0;
 	thread->hw_brk.type = 0;
+	thread->hw_brk.len = 0;
+	thread->hw_brk.hw_len = 0;
 	if (ppc_breakpoint_available())
 		set_breakpoint(&thread->hw_brk);
 }
@@ -816,6 +818,7 @@ static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
 		return false;
 	if (a->len != b->len)
 		return false;
+	/* no need to check hw_len. it's calculated from address and len */
 	return true;
 }
 

commit 45824fc0da6e46cc5d563105e1eaaf3098a686f9
Merge: 8c2b418c3f95 d9101bfa6adc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Sep 20 11:48:06 2019 -0700

    Merge tag 'powerpc-5.4-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "This is a bit late, partly due to me travelling, and partly due to a
      power outage knocking out some of my test systems *while* I was
      travelling.
    
       - Initial support for running on a system with an Ultravisor, which
         is software that runs below the hypervisor and protects guests
         against some attacks by the hypervisor.
    
       - Support for building the kernel to run as a "Secure Virtual
         Machine", ie. as a guest capable of running on a system with an
         Ultravisor.
    
       - Some changes to our DMA code on bare metal, to allow devices with
         medium sized DMA masks (> 32 && < 59 bits) to use more than 2GB of
         DMA space.
    
       - Support for firmware assisted crash dumps on bare metal (powernv).
    
       - Two series fixing bugs in and refactoring our PCI EEH code.
    
       - A large series refactoring our exception entry code to use gas
         macros, both to make it more readable and also enable some future
         optimisations.
    
      As well as many cleanups and other minor features & fixups.
    
      Thanks to: Adam Zerella, Alexey Kardashevskiy, Alistair Popple, Andrew
      Donnellan, Aneesh Kumar K.V, Anju T Sudhakar, Anshuman Khandual,
      Balbir Singh, Benjamin Herrenschmidt, Cdric Le Goater, Christophe
      JAILLET, Christophe Leroy, Christopher M. Riedl, Christoph Hellwig,
      Claudio Carvalho, Daniel Axtens, David Gibson, David Hildenbrand,
      Desnes A. Nunes do Rosario, Ganesh Goudar, Gautham R. Shenoy, Greg
      Kurz, Guerney Hunt, Gustavo Romero, Halil Pasic, Hari Bathini, Joakim
      Tjernlund, Jonathan Neuschafer, Jordan Niethe, Leonardo Bras, Lianbo
      Jiang, Madhavan Srinivasan, Mahesh Salgaonkar, Mahesh Salgaonkar,
      Masahiro Yamada, Maxiwell S. Garcia, Michael Anderson, Nathan
      Chancellor, Nathan Lynch, Naveen N. Rao, Nicholas Piggin, Oliver
      O'Halloran, Qian Cai, Ram Pai, Ravi Bangoria, Reza Arbab, Ryan Grimm,
      Sam Bobroff, Santosh Sivaraj, Segher Boessenkool, Sukadev Bhattiprolu,
      Thiago Bauermann, Thiago Jung Bauermann, Thomas Gleixner, Tom
      Lendacky, Vasant Hegde"
    
    * tag 'powerpc-5.4-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (264 commits)
      powerpc/mm/mce: Keep irqs disabled during lockless page table walk
      powerpc: Use ftrace_graph_ret_addr() when unwinding
      powerpc/ftrace: Enable HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
      ftrace: Look up the address of return_to_handler() using helpers
      powerpc: dump kernel log before carrying out fadump or kdump
      docs: powerpc: Add missing documentation reference
      powerpc/xmon: Fix output of XIVE IPI
      powerpc/xmon: Improve output of XIVE interrupts
      powerpc/mm/radix: remove useless kernel messages
      powerpc/fadump: support holes in kernel boot memory area
      powerpc/fadump: remove RMA_START and RMA_END macros
      powerpc/fadump: update documentation about option to release opalcore
      powerpc/fadump: consider f/w load area
      powerpc/opalcore: provide an option to invalidate /sys/firmware/opal/core file
      powerpc/opalcore: export /sys/firmware/opal/core for analysing opal crashes
      powerpc/fadump: update documentation about CONFIG_PRESERVE_FA_DUMP
      powerpc/fadump: add support to preserve crash data on FADUMP disabled kernel
      powerpc/fadump: improve how crashed kernel's memory is reserved
      powerpc/fadump: consider reserved ranges while releasing memory
      powerpc/fadump: make crash memory ranges array allocation generic
      ...

commit 7c1bb6bbf75d8ca5ec878627d3170effcaf54f27
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Sep 5 23:50:30 2019 +0530

    powerpc: Use ftrace_graph_ret_addr() when unwinding
    
    With support for HAVE_FUNCTION_GRAPH_RET_ADDR_PTR,
    ftrace_graph_ret_addr() provides more robust unwinding when function
    graph is in use. Update show_stack() to use the same.
    
    With dump_stack() added to sysrq_sysctl_handler(), before this patch:
      root@(none):/sys/kernel/debug/tracing# cat /proc/sys/kernel/sysrq
      CPU: 0 PID: 218 Comm: cat Not tainted 5.3.0-rc7-00868-g8453ad4a078c-dirty #20
      Call Trace:
      [c0000000d1e13c30] [c00000000006ab98] return_to_handler+0x0/0x40 (dump_stack+0xe8/0x164) (unreliable)
      [c0000000d1e13c80] [c000000000145680] sysrq_sysctl_handler+0x48/0xb8
      [c0000000d1e13cd0] [c00000000006ab98] return_to_handler+0x0/0x40 (proc_sys_call_handler+0x274/0x2a0)
      [c0000000d1e13d60] [c00000000006ab98] return_to_handler+0x0/0x40 (return_to_handler+0x0/0x40)
      [c0000000d1e13d80] [c00000000006ab98] return_to_handler+0x0/0x40 (__vfs_read+0x3c/0x70)
      [c0000000d1e13dd0] [c00000000006ab98] return_to_handler+0x0/0x40 (vfs_read+0xb8/0x1b0)
      [c0000000d1e13e20] [c00000000006ab98] return_to_handler+0x0/0x40 (ksys_read+0x7c/0x140)
    
    After this patch:
      Call Trace:
      [c0000000d1e33c30] [c00000000006ab58] return_to_handler+0x0/0x40 (dump_stack+0xe8/0x164) (unreliable)
      [c0000000d1e33c80] [c000000000145680] sysrq_sysctl_handler+0x48/0xb8
      [c0000000d1e33cd0] [c00000000006ab58] return_to_handler+0x0/0x40 (proc_sys_call_handler+0x274/0x2a0)
      [c0000000d1e33d60] [c00000000006ab58] return_to_handler+0x0/0x40 (__vfs_read+0x3c/0x70)
      [c0000000d1e33d80] [c00000000006ab58] return_to_handler+0x0/0x40 (vfs_read+0xb8/0x1b0)
      [c0000000d1e33dd0] [c00000000006ab58] return_to_handler+0x0/0x40 (ksys_read+0x7c/0x140)
      [c0000000d1e33e20] [c00000000006ab58] return_to_handler+0x0/0x40 (system_call+0x5c/0x68)
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/dc89c9a887121342d9c7819482c3dabdece2a323.1567707399.git.naveen.n.rao@linux.vnet.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 24621e7e5033..f289bdd2b562 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -2047,10 +2047,8 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	int count = 0;
 	int firstframe = 1;
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	struct ftrace_ret_stack *ret_stack;
-	extern void return_to_handler(void);
-	unsigned long rth = (unsigned long)return_to_handler;
-	int curr_frame = 0;
+	unsigned long ret_addr;
+	int ftrace_idx = 0;
 #endif
 
 	if (tsk == NULL)
@@ -2079,15 +2077,10 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		if (!firstframe || ip != lr) {
 			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-			if ((ip == rth) && curr_frame >= 0) {
-				ret_stack = ftrace_graph_get_ret_stack(current,
-								  curr_frame++);
-				if (ret_stack)
-					pr_cont(" (%pS)",
-						(void *)ret_stack->ret);
-				else
-					curr_frame = -1;
-			}
+			ret_addr = ftrace_graph_ret_addr(current,
+						&ftrace_idx, ip, stack);
+			if (ret_addr != ip)
+				pr_cont(" (%pS)", (void *)ret_addr);
 #endif
 			if (firstframe)
 				pr_cont(" (unreliable)");

commit a8318c13e79badb92bc6640704a64cc022a6eb97
Author: Gustavo Romero <gromero@linux.ibm.com>
Date:   Wed Sep 4 00:55:28 2019 -0400

    powerpc/tm: Fix restoring FP/VMX facility incorrectly on interrupts
    
    When in userspace and MSR FP=0 the hardware FP state is unrelated to
    the current process. This is extended for transactions where if tbegin
    is run with FP=0, the hardware checkpoint FP state will also be
    unrelated to the current process. Due to this, we need to ensure this
    hardware checkpoint is updated with the correct state before we enable
    FP for this process.
    
    Unfortunately we get this wrong when returning to a process from a
    hardware interrupt. A process that starts a transaction with FP=0 can
    take an interrupt. When the kernel returns back to that process, we
    change to FP=1 but with hardware checkpoint FP state not updated. If
    this transaction is then rolled back, the FP registers now contain the
    wrong state.
    
    The process looks like this:
       Userspace:                      Kernel
    
                   Start userspace
                    with MSR FP=0 TM=1
                      < -----
       ...
       tbegin
       bne
                   Hardware interrupt
                       ---- >
                                        <do_IRQ...>
                                        ....
                                        ret_from_except
                                          restore_math()
                                            /* sees FP=0 */
                                            restore_fp()
                                              tm_active_with_fp()
                                                /* sees FP=1 (Incorrect) */
                                              load_fp_state()
                                            FP = 0 -> 1
                      < -----
                   Return to userspace
                     with MSR TM=1 FP=1
                     with junk in the FP TM checkpoint
       TM rollback
       reads FP junk
    
    When returning from the hardware exception, tm_active_with_fp() is
    incorrectly making restore_fp() call load_fp_state() which is setting
    FP=1.
    
    The fix is to remove tm_active_with_fp().
    
    tm_active_with_fp() is attempting to handle the case where FP state
    has been changed inside a transaction. In this case the checkpointed
    and transactional FP state is different and hence we must restore the
    FP state (ie. we can't do lazy FP restore inside a transaction that's
    used FP). It's safe to remove tm_active_with_fp() as this case is
    handled by restore_tm_state(). restore_tm_state() detects if FP has
    been using inside a transaction and will set load_fp and call
    restore_math() to ensure the FP state (checkpoint and transaction) is
    restored.
    
    This is a data integrity problem for the current process as the FP
    registers are corrupted. It's also a security problem as the FP
    registers from one process may be leaked to another.
    
    Similarly for VMX.
    
    A simple testcase to replicate this will be posted to
    tools/testing/selftests/powerpc/tm/tm-poison.c
    
    This fixes CVE-2019-15031.
    
    Fixes: a7771176b439 ("powerpc: Don't enable FP/Altivec if not checkpointed")
    Cc: stable@vger.kernel.org # 4.15+
    Signed-off-by: Gustavo Romero <gromero@linux.ibm.com>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190904045529.23002-2-gromero@linux.vnet.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 437b57068cf8..7a84c9f1778e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -101,21 +101,8 @@ static void check_if_tm_restore_required(struct task_struct *tsk)
 	}
 }
 
-static bool tm_active_with_fp(struct task_struct *tsk)
-{
-	return MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
-		(tsk->thread.ckpt_regs.msr & MSR_FP);
-}
-
-static bool tm_active_with_altivec(struct task_struct *tsk)
-{
-	return MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
-		(tsk->thread.ckpt_regs.msr & MSR_VEC);
-}
 #else
 static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
-static inline bool tm_active_with_fp(struct task_struct *tsk) { return false; }
-static inline bool tm_active_with_altivec(struct task_struct *tsk) { return false; }
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
 bool strict_msr_control;
@@ -252,7 +239,7 @@ EXPORT_SYMBOL(enable_kernel_fp);
 
 static int restore_fp(struct task_struct *tsk)
 {
-	if (tsk->thread.load_fp || tm_active_with_fp(tsk)) {
+	if (tsk->thread.load_fp) {
 		load_fp_state(&current->thread.fp_state);
 		current->thread.load_fp++;
 		return 1;
@@ -334,8 +321,7 @@ EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
 
 static int restore_altivec(struct task_struct *tsk)
 {
-	if (cpu_has_feature(CPU_FTR_ALTIVEC) &&
-		(tsk->thread.load_vec || tm_active_with_altivec(tsk))) {
+	if (cpu_has_feature(CPU_FTR_ALTIVEC) && (tsk->thread.load_vec)) {
 		load_vr_state(&tsk->thread.vr_state);
 		tsk->thread.used_vr = 1;
 		tsk->thread.load_vec++;

commit 8205d5d98ef7f155de211f5e2eb6ca03d95a5a60
Author: Gustavo Romero <gromero@linux.ibm.com>
Date:   Wed Sep 4 00:55:27 2019 -0400

    powerpc/tm: Fix FP/VMX unavailable exceptions inside a transaction
    
    When we take an FP unavailable exception in a transaction we have to
    account for the hardware FP TM checkpointed registers being
    incorrect. In this case for this process we know the current and
    checkpointed FP registers must be the same (since FP wasn't used
    inside the transaction) hence in the thread_struct we copy the current
    FP registers to the checkpointed ones.
    
    This copy is done in tm_reclaim_thread(). We use thread->ckpt_regs.msr
    to determine if FP was on when in userspace. thread->ckpt_regs.msr
    represents the state of the MSR when exiting userspace. This is setup
    by check_if_tm_restore_required().
    
    Unfortunatley there is an optimisation in giveup_all() which returns
    early if tsk->thread.regs->msr (via local variable `usermsr`) has
    FP=VEC=VSX=SPE=0. This optimisation means that
    check_if_tm_restore_required() is not called and hence
    thread->ckpt_regs.msr is not updated and will contain an old value.
    
    This can happen if due to load_fp=255 we start a userspace process
    with MSR FP=1 and then we are context switched out. In this case
    thread->ckpt_regs.msr will contain FP=1. If that same process is then
    context switched in and load_fp overflows, MSR will have FP=0. If that
    process now enters a transaction and does an FP instruction, the FP
    unavailable will not update thread->ckpt_regs.msr (the bug) and MSR
    FP=1 will be retained in thread->ckpt_regs.msr.  tm_reclaim_thread()
    will then not perform the required memcpy and the checkpointed FP regs
    in the thread struct will contain the wrong values.
    
    The code path for this happening is:
    
           Userspace:                      Kernel
                       Start userspace
                        with MSR FP/VEC/VSX/SPE=0 TM=1
                          < -----
           ...
           tbegin
           bne
           fp instruction
                       FP unavailable
                           ---- >
                                            fp_unavailable_tm()
                                              tm_reclaim_current()
                                                tm_reclaim_thread()
                                                  giveup_all()
                                                    return early since FP/VMX/VSX=0
                                                    /* ckpt MSR not updated (Incorrect) */
                                                  tm_reclaim()
                                                    /* thread_struct ckpt FP regs contain junk (OK) */
                                                  /* Sees ckpt MSR FP=1 (Incorrect) */
                                                  no memcpy() performed
                                                    /* thread_struct ckpt FP regs not fixed (Incorrect) */
                                              tm_recheckpoint()
                                                 /* Put junk in hardware checkpoint FP regs */
                                             ....
                          < -----
                       Return to userspace
                         with MSR TM=1 FP=1
                         with junk in the FP TM checkpoint
           TM rollback
           reads FP junk
    
    This is a data integrity problem for the current process as the FP
    registers are corrupted. It's also a security problem as the FP
    registers from one process may be leaked to another.
    
    This patch moves up check_if_tm_restore_required() in giveup_all() to
    ensure thread->ckpt_regs.msr is updated correctly.
    
    A simple testcase to replicate this will be posted to
    tools/testing/selftests/powerpc/tm/tm-poison.c
    
    Similarly for VMX.
    
    This fixes CVE-2019-15030.
    
    Fixes: f48e91e87e67 ("powerpc/tm: Fix FP and VMX register corruption")
    Cc: stable@vger.kernel.org # 4.12+
    Signed-off-by: Gustavo Romero <gromero@linux.vnet.ibm.com>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190904045529.23002-1-gromero@linux.vnet.ibm.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8fc4de0d22b4..437b57068cf8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -497,13 +497,14 @@ void giveup_all(struct task_struct *tsk)
 	if (!tsk->thread.regs)
 		return;
 
+	check_if_tm_restore_required(tsk);
+
 	usermsr = tsk->thread.regs->msr;
 
 	if ((usermsr & msr_all_available) == 0)
 		return;
 
 	msr_check_and_set(msr_all_available);
-	check_if_tm_restore_required(tsk);
 
 	WARN_ON((usermsr & MSR_VSX) && !((usermsr & MSR_FP) && (usermsr & MSR_VEC)));
 

commit facd04a904ff6cdc6ee85d6e85d500f478a1bec4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Aug 27 13:30:06 2019 +1000

    powerpc: convert to copy_thread_tls
    
    Commit 3033f14ab78c3 ("clone: support passing tls argument via C rather
    than pt_regs magic") introduced the HAVE_COPY_THREAD_TLS option. Use it
    to avoid a subtle assumption about the argument ordering of clone type
    syscalls.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190827033010.28090-2-npiggin@gmail.com

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8fc4de0d22b4..24621e7e5033 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1600,8 +1600,9 @@ static void setup_ksp_vsid(struct task_struct *p, unsigned long sp)
 /*
  * Copy architecture-specific thread state
  */
-int copy_thread(unsigned long clone_flags, unsigned long usp,
-		unsigned long kthread_arg, struct task_struct *p)
+int copy_thread_tls(unsigned long clone_flags, unsigned long usp,
+		unsigned long kthread_arg, struct task_struct *p,
+		unsigned long tls)
 {
 	struct pt_regs *childregs, *kregs;
 	extern void ret_from_fork(void);
@@ -1642,10 +1643,10 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		if (clone_flags & CLONE_SETTLS) {
 #ifdef CONFIG_PPC64
 			if (!is_32bit_task())
-				childregs->gpr[13] = childregs->gpr[6];
+				childregs->gpr[13] = tls;
 			else
 #endif
-				childregs->gpr[2] = childregs->gpr[6];
+				childregs->gpr[2] = tls;
 		}
 
 		f = ret_from_fork;

commit 192f0f8e9db7efe4ac98d47f5fa4334e43c1204d
Merge: ec9249752465 f5a9e488d623
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 13 16:08:36 2019 -0700

    Merge tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Removal of the NPU DMA code, used by the out-of-tree Nvidia driver,
         as well as some other functions only used by drivers that haven't
         (yet?) made it upstream.
    
       - A fix for a bug in our handling of hardware watchpoints (eg. perf
         record -e mem: ...) which could lead to register corruption and
         kernel crashes.
    
       - Enable HAVE_ARCH_HUGE_VMAP, which allows us to use large pages for
         vmalloc when using the Radix MMU.
    
       - A large but incremental rewrite of our exception handling code to
         use gas macros rather than multiple levels of nested CPP macros.
    
      And the usual small fixes, cleanups and improvements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Andreas Schwab,
      Aneesh Kumar K.V, Anju T Sudhakar, Anton Blanchard, Arnd Bergmann,
      Athira Rajeev, Cdric Le Goater, Christian Lamparter, Christophe
      Leroy, Christophe Lombard, Christoph Hellwig, Daniel Axtens, Denis
      Efremov, Enrico Weigelt, Frederic Barrat, Gautham R. Shenoy, Geert
      Uytterhoeven, Geliang Tang, Gen Zhang, Greg Kroah-Hartman, Greg Kurz,
      Gustavo Romero, Krzysztof Kozlowski, Madhavan Srinivasan, Masahiro
      Yamada, Mathieu Malaterre, Michael Neuling, Nathan Lynch, Naveen N.
      Rao, Nicholas Piggin, Nishad Kamdar, Oliver O'Halloran, Qian Cai, Ravi
      Bangoria, Sachin Sant, Sam Bobroff, Satheesh Rajendran, Segher
      Boessenkool, Shaokun Zhang, Shawn Anastasio, Stewart Smith, Suraj
      Jitindar Singh, Thiago Jung Bauermann, YueHaibing"
    
    * tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (163 commits)
      powerpc/powernv/idle: Fix restore of SPRN_LDBAR for POWER9 stop state.
      powerpc/eeh: Handle hugepages in ioremap space
      ocxl: Update for AFU descriptor template version 1.1
      powerpc/boot: pass CONFIG options in a simpler and more robust way
      powerpc/boot: add {get, put}_unaligned_be32 to xz_config.h
      powerpc/irq: Don't WARN continuously in arch_local_irq_restore()
      powerpc/module64: Use symbolic instructions names.
      powerpc/module32: Use symbolic instructions names.
      powerpc: Move PPC_HA() PPC_HI() and PPC_LO() to ppc-opcode.h
      powerpc/module64: Fix comment in R_PPC64_ENTRY handling
      powerpc/boot: Add lzo support for uImage
      powerpc/boot: Add lzma support for uImage
      powerpc/boot: don't force gzipped uImage
      powerpc/8xx: Add microcode patch to move SMC parameter RAM.
      powerpc/8xx: Use IO accessors in microcode programming.
      powerpc/8xx: replace #ifdefs by IS_ENABLED() in microcode.c
      powerpc/8xx: refactor programming of microcode CPM params.
      powerpc/8xx: refactor printing of microcode patch name.
      powerpc/8xx: Refactor microcode write
      powerpc/8xx: refactor writing of CPM microcode arrays
      ...

commit 5ad18b2e60b75c7297a998dea702451d33a052ed
Merge: 92c1d6522135 318759b4737c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 21:48:15 2019 -0700

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull force_sig() argument change from Eric Biederman:
     "A source of error over the years has been that force_sig has taken a
      task parameter when it is only safe to use force_sig with the current
      task.
    
      The force_sig function is built for delivering synchronous signals
      such as SIGSEGV where the userspace application caused a synchronous
      fault (such as a page fault) and the kernel responded with a signal.
    
      Because the name force_sig does not make this clear, and because the
      force_sig takes a task parameter the function force_sig has been
      abused for sending other kinds of signals over the years. Slowly those
      have been fixed when the oopses have been tracked down.
    
      This set of changes fixes the remaining abusers of force_sig and
      carefully rips out the task parameter from force_sig and friends
      making this kind of error almost impossible in the future"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (27 commits)
      signal/x86: Move tsk inside of CONFIG_MEMORY_FAILURE in do_sigbus
      signal: Remove the signal number and task parameters from force_sig_info
      signal: Factor force_sig_info_to_task out of force_sig_info
      signal: Generate the siginfo in force_sig
      signal: Move the computation of force into send_signal and correct it.
      signal: Properly set TRACE_SIGNAL_LOSE_INFO in __send_signal
      signal: Remove the task parameter from force_sig_fault
      signal: Use force_sig_fault_to_task for the two calls that don't deliver to current
      signal: Explicitly call force_sig_fault on current
      signal/unicore32: Remove tsk parameter from __do_user_fault
      signal/arm: Remove tsk parameter from __do_user_fault
      signal/arm: Remove tsk parameter from ptrace_break
      signal/nds32: Remove tsk parameter from send_sigtrap
      signal/riscv: Remove tsk parameter from do_trap
      signal/sh: Remove tsk parameter from force_sig_info_fault
      signal/um: Remove task parameter from send_sigtrap
      signal/x86: Remove task parameter from send_sigtrap
      signal: Remove task parameter from force_sig_mceerr
      signal: Remove task parameter from force_sig
      signal: Remove task parameter from force_sigsegv
      ...

commit a278e7ea608bea5fe6df9b6ae91fa134655c5d2c
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Jun 4 13:00:37 2019 +1000

    powerpc: Fix compile issue with force DAWR
    
    If you compile with KVM but without CONFIG_HAVE_HW_BREAKPOINT you fail
    at linking with:
      arch/powerpc/kvm/book3s_hv_rmhandlers.o:(.text+0x708): undefined reference to `dawr_force_enable'
    
    This was caused by commit c1fe190c0672 ("powerpc: Add force enable of
    DAWR on P9 option").
    
    This moves a bunch of code around to fix this. It moves a lot of the
    DAWR code in a new file and creates a new CONFIG_PPC_DAWR to enable
    compiling it.
    
    Fixes: c1fe190c0672 ("powerpc: Add force enable of DAWR on P9 option")
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    [mpe: Minor formatting in set_dawr()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 87da40129927..03a2da35ce61 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -797,34 +797,6 @@ static inline int set_dabr(struct arch_hw_breakpoint *brk)
 	return __set_dabr(dabr, dabrx);
 }
 
-int set_dawr(struct arch_hw_breakpoint *brk)
-{
-	unsigned long dawr, dawrx, mrd;
-
-	dawr = brk->address;
-
-	dawrx  = (brk->type & (HW_BRK_TYPE_READ | HW_BRK_TYPE_WRITE)) \
-		                   << (63 - 58); //* read/write bits */
-	dawrx |= ((brk->type & (HW_BRK_TYPE_TRANSLATE)) >> 2) \
-		                   << (63 - 59); //* translate */
-	dawrx |= (brk->type & (HW_BRK_TYPE_PRIV_ALL)) \
-		                   >> 3; //* PRIM bits */
-	/* dawr length is stored in field MDR bits 48:53.  Matches range in
-	   doublewords (64 bits) baised by -1 eg. 0b000000=1DW and
-	   0b111111=64DW.
-	   brk->len is in bytes.
-	   This aligns up to double word size, shifts and does the bias.
-	*/
-	mrd = ((brk->len + 7) >> 3) - 1;
-	dawrx |= (mrd & 0x3f) << (63 - 53);
-
-	if (ppc_md.set_dawr)
-		return ppc_md.set_dawr(dawr, dawrx);
-	mtspr(SPRN_DAWR, dawr);
-	mtspr(SPRN_DAWRX, dawrx);
-	return 0;
-}
-
 void __set_breakpoint(struct arch_hw_breakpoint *brk)
 {
 	memcpy(this_cpu_ptr(&current_brk), brk, sizeof(*brk));

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 87da40129927..f0fbbf6a6a1f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  *  Derived from "arch/i386/kernel/process.c"
  *    Copyright (C) 1995  Linus Torvalds
@@ -7,11 +8,6 @@
  *
  *  PowerPC version
  *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
- *
- *  This program is free software; you can redistribute it and/or
- *  modify it under the terms of the GNU General Public License
- *  as published by the Free Software Foundation; either version
- *  2 of the License, or (at your option) any later version.
  */
 
 #include <linux/errno.h>

commit 2e1661d2673667d886cd40ad9f414cb6db48d8da
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu May 23 11:04:24 2019 -0500

    signal: Remove the task parameter from force_sig_fault
    
    As synchronous exceptions really only make sense against the current
    task (otherwise how are you synchronous) remove the task parameter
    from from force_sig_fault to make it explicit that is what is going
    on.
    
    The two known exceptions that deliver a synchronous exception to a
    stopped ptraced task have already been changed to
    force_sig_fault_to_task.
    
    The callers have been changed with the following emacs regular expression
    (with obvious variations on the architectures that take more arguments)
    to avoid typos:
    
    force_sig_fault[(]\([^,]+\)[,]\([^,]+\)[,]\([^,]+\)[,]\W+current[)]
    ->
    force_sig_fault(\1,\2,\3)
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 87da40129927..1b5b1477afa2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -643,7 +643,7 @@ void do_break (struct pt_regs *regs, unsigned long address,
 	hw_breakpoint_disable();
 
 	/* Deliver the signal to userspace */
-	force_sig_fault(SIGTRAP, TRAP_HWBKPT, (void __user *)address, current);
+	force_sig_fault(SIGTRAP, TRAP_HWBKPT, (void __user *)address);
 }
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 

commit e2b36d591720d81741f37e047a6f0047e8c89369
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 2 15:21:07 2019 +1000

    powerpc/64: Don't trace code that runs with the soft irq mask unreconciled
    
    "Reconciling" in terms of interrupt handling, is to bring the soft irq
    mask state in to synch with the hardware, after an interrupt causes
    MSR[EE] to be cleared (while the soft mask may be enabled, and hard
    irqs not marked disabled).
    
    General kernel code should not be called while unreconciled, because
    local_irq_disable, etc. manipulations can cause surprising irq traces,
    and it's fragile because the soft irq code does not really expect to
    be called in this situation.
    
    When exiting from an interrupt, MSR[EE] is cleared to prevent races,
    but soft irq state is enabled for the returned-to context, so this is
    now an unreconciled state. restore_math is called in this state, and
    that can be ftraced, and the ftrace subsystem disables local irqs.
    
    Mark restore_math and its callees as notrace. Restore a sanity check
    in the soft irq code that had to be disabled for this case, by commit
    4da1f79227ad4 ("powerpc/64: Disable irq restore warning for now").
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0c2017357073..87da40129927 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -134,7 +134,8 @@ static int __init enable_strict_msr_control(char *str)
 }
 early_param("ppc_strict_facility_enable", enable_strict_msr_control);
 
-unsigned long msr_check_and_set(unsigned long bits)
+/* notrace because it's called by restore_math */
+unsigned long notrace msr_check_and_set(unsigned long bits)
 {
 	unsigned long oldmsr = mfmsr();
 	unsigned long newmsr;
@@ -153,7 +154,8 @@ unsigned long msr_check_and_set(unsigned long bits)
 }
 EXPORT_SYMBOL_GPL(msr_check_and_set);
 
-void __msr_check_and_clear(unsigned long bits)
+/* notrace because it's called by restore_math */
+void notrace __msr_check_and_clear(unsigned long bits)
 {
 	unsigned long oldmsr = mfmsr();
 	unsigned long newmsr;
@@ -526,7 +528,17 @@ void giveup_all(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(giveup_all);
 
-void restore_math(struct pt_regs *regs)
+/*
+ * The exception exit path calls restore_math() with interrupts hard disabled
+ * but the soft irq state not "reconciled". ftrace code that calls
+ * local_irq_save/restore causes warnings.
+ *
+ * Rather than complicate the exit path, just don't trace restore_math. This
+ * could be done by having ftrace entry code check for this un-reconciled
+ * condition where MSR[EE]=0 and PACA_IRQ_HARD_DIS is not set, and
+ * temporarily fix it up for the duration of the ftrace call.
+ */
+void notrace restore_math(struct pt_regs *regs)
 {
 	unsigned long msr;
 

commit a5ae043de7678f189303559782f6057078459a41
Author: Mathieu Malaterre <malat@debian.org>
Date:   Wed Mar 13 21:00:30 2019 +0100

    powerpc/64s: Remove 'dummy_copy_buffer'
    
    In commit 2bf1071a8d50 ("powerpc/64s: Remove POWER9 DD1 support") the
    function __switch_to remove usage for 'dummy_copy_buffer'. Since it is
    not used anywhere else, remove it completely.
    
    This remove the following warning:
      arch/powerpc/kernel/process.c:1156:17: error: 'dummy_copy_buffer' defined but not used
    
    Suggested-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 64e1494d3a1d..0c2017357073 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1152,11 +1152,6 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 	thread_pkey_regs_restore(new_thread, old_thread);
 }
 
-#ifdef CONFIG_PPC_BOOK3S_64
-#define CP_SIZE 128
-static const u8 dummy_copy_buffer[CP_SIZE] __attribute__((aligned(CP_SIZE)));
-#endif
-
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {

commit bdc7c970bcdce1c018957e0158230bc025682ba2
Merge: 7ae3f6e130e8 e9cef0189c5b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 30 22:52:03 2019 +1000

    Merge branch 'topic/ppc-kvm' into next
    
    Merge our topic branch shared with KVM. In particular this includes the
    rewrite of the idle code into C.

commit c1fe190c06723322f2dfac31d3b982c581e434ef
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon Apr 1 17:03:12 2019 +1100

    powerpc: Add force enable of DAWR on P9 option
    
    This adds a flag so that the DAWR can be enabled on P9 via:
      echo Y > /sys/kernel/debug/powerpc/dawr_enable_dangerous
    
    The DAWR was previously force disabled on POWER9 in:
      9654153158 powerpc: Disable DAWR in the base POWER9 CPU features
    Also see Documentation/powerpc/DAWR-POWER9.txt
    
    This is a dangerous setting, USE AT YOUR OWN RISK.
    
    Some users may not care about a bad user crashing their box
    (ie. single user/desktop systems) and really want the DAWR.  This
    allows them to force enable DAWR.
    
    This flag can also be used to disable DAWR access. Once this is
    cleared, all DAWR access should be cleared immediately and your
    machine once again safe from crashing.
    
    Userspace may get confused by toggling this. If DAWR is force
    enabled/disabled between getting the number of breakpoints (via
    PTRACE_GETHWDBGINFO) and setting the breakpoint, userspace will get an
    inconsistent view of what's available. Similarly for guests.
    
    For the DAWR to be enabled in a KVM guest, the DAWR needs to be force
    enabled in the host AND the guest. For this reason, this won't work on
    POWERVM as it doesn't allow the HCALL to work. Writes of 'Y' to the
    dawr_enable_dangerous file will fail if the hypervisor doesn't support
    writing the DAWR.
    
    To double check the DAWR is working, run this kernel selftest:
      tools/testing/selftests/powerpc/ptrace/ptrace-hwbreak.c
    Any errors/failures/skips mean something is wrong.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dd9e0d5386ee..225705aac814 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -67,6 +67,7 @@
 #include <asm/cpu_has_feature.h>
 #include <asm/asm-prototypes.h>
 #include <asm/stacktrace.h>
+#include <asm/hw_breakpoint.h>
 
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
@@ -784,7 +785,7 @@ static inline int set_dabr(struct arch_hw_breakpoint *brk)
 	return __set_dabr(dabr, dabrx);
 }
 
-static inline int set_dawr(struct arch_hw_breakpoint *brk)
+int set_dawr(struct arch_hw_breakpoint *brk)
 {
 	unsigned long dawr, dawrx, mrd;
 
@@ -816,7 +817,7 @@ void __set_breakpoint(struct arch_hw_breakpoint *brk)
 {
 	memcpy(this_cpu_ptr(&current_brk), brk, sizeof(*brk));
 
-	if (cpu_has_feature(CPU_FTR_DAWR))
+	if (dawr_enabled())
 		// Power8 or later
 		set_dawr(brk);
 	else if (!cpu_has_feature(CPU_FTR_ARCH_207S))
@@ -830,8 +831,8 @@ void __set_breakpoint(struct arch_hw_breakpoint *brk)
 /* Check if we have DAWR or DABR hardware */
 bool ppc_breakpoint_available(void)
 {
-	if (cpu_has_feature(CPU_FTR_DAWR))
-		return true; /* POWER8 DAWR */
+	if (dawr_enabled())
+		return true; /* POWER8 DAWR or POWER9 forced DAWR */
 	if (cpu_has_feature(CPU_FTR_ARCH_207S))
 		return false; /* POWER9 with DAWR disabled */
 	/* DABR: Everything but POWER8 and POWER9 */

commit f89bd8ba834e392ff614a7be9ee68c5679675122
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue Apr 9 09:33:28 2019 +0530

    powerpc/mm/radix: Don't do SLB preload when using the radix MMU
    
    Add radix_enabled() check to avoid SLB preload with radix translation.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dd9e0d5386ee..f7b2e3b3db28 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1729,7 +1729,8 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	unsigned long load_addr = regs->gpr[2];	/* saved by ELF_PLAT_INIT */
 
 #ifdef CONFIG_PPC_BOOK3S_64
-	preload_new_slb_context(start, sp);
+	if (!radix_enabled())
+		preload_new_slb_context(start, sp);
 #endif
 #endif
 

commit a7916a1de526162d73e894b6d3ebd895d4302078
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:09:00 2019 +0000

    powerpc: regain entire stack space
    
    thread_info is not anymore in the stack, so the entire stack
    can now be used.
    
    There is also no risk anymore of corrupting task_cpu(p) with a
    stack overflow so the patch removes the test.
    
    When doing this, an explicit test for NULL stack pointer is
    needed in validate_sp() as it is not anymore implicitely covered
    by the sizeof(thread_info) gap.
    
    In the meantime, with the previous patch all pointers to the stacks
    are not anymore pointers to thread_info so this patch changes them
    to void*
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index fd07711035bd..dd9e0d5386ee 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1691,8 +1691,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	sp -= STACK_FRAME_OVERHEAD;
 	p->thread.ksp = sp;
 #ifdef CONFIG_PPC32
-	p->thread.ksp_limit = (unsigned long)task_stack_page(p) +
-				_ALIGN_UP(sizeof(struct thread_info), 16);
+	p->thread.ksp_limit = (unsigned long)end_of_stack(p);
 #endif
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	p->thread.ptrace_bps[0] = NULL;
@@ -1995,21 +1994,14 @@ static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,
 	unsigned long stack_page;
 	unsigned long cpu = task_cpu(p);
 
-	/*
-	 * Avoid crashing if the stack has overflowed and corrupted
-	 * task_cpu(p), which is in the thread_info struct.
-	 */
-	if (cpu < NR_CPUS && cpu_possible(cpu)) {
-		stack_page = (unsigned long) hardirq_ctx[cpu];
-		if (sp >= stack_page + sizeof(struct thread_struct)
-		    && sp <= stack_page + THREAD_SIZE - nbytes)
-			return 1;
-
-		stack_page = (unsigned long) softirq_ctx[cpu];
-		if (sp >= stack_page + sizeof(struct thread_struct)
-		    && sp <= stack_page + THREAD_SIZE - nbytes)
-			return 1;
-	}
+	stack_page = (unsigned long)hardirq_ctx[cpu];
+	if (sp >= stack_page && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+
+	stack_page = (unsigned long)softirq_ctx[cpu];
+	if (sp >= stack_page && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+
 	return 0;
 }
 
@@ -2018,8 +2010,10 @@ int validate_sp(unsigned long sp, struct task_struct *p,
 {
 	unsigned long stack_page = (unsigned long)task_stack_page(p);
 
-	if (sp >= stack_page + sizeof(struct thread_struct)
-	    && sp <= stack_page + THREAD_SIZE - nbytes)
+	if (sp < THREAD_SIZE)
+		return 0;
+
+	if (sp >= stack_page && sp <= stack_page + THREAD_SIZE - nbytes)
 		return 1;
 
 	return valid_irq_stack(sp, p, nbytes);

commit ed1cd6deb013a11959d17a94e35ce159197632da
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:58 2019 +0000

    powerpc: Activate CONFIG_THREAD_INFO_IN_TASK
    
    This patch activates CONFIG_THREAD_INFO_IN_TASK which
    moves the thread_info into task_struct.
    
    Moving thread_info into task_struct has the following advantages:
      - It protects thread_info from corruption in the case of stack
        overflows.
      - Its address is harder to determine if stack addresses are leaked,
        making a number of attacks more difficult.
    
    This has the following consequences:
      - thread_info is now located at the beginning of task_struct.
      - The 'cpu' field is now in task_struct, and only exists when
        CONFIG_SMP is active.
      - thread_info doesn't have anymore the 'task' field.
    
    This patch:
      - Removes all recopy of thread_info struct when the stack changes.
      - Changes the CURRENT_THREAD_INFO() macro to point to current.
      - Selects CONFIG_THREAD_INFO_IN_TASK.
      - Modifies raw_smp_processor_id() to get ->cpu from current without
        including linux/sched.h to avoid circular inclusion and without
        including asm/asm-offsets.h to avoid symbol names duplication
        between ASM constants and C constants.
      - Modifies klp_init_thread_info() to take a task_struct pointer
        argument.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add task_stack.h to livepatch.h to fix build fails]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dc2aaaf75c87..fd07711035bd 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1634,7 +1634,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	unsigned long sp = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 	struct thread_info *ti = task_thread_info(p);
 
-	klp_init_thread_info(ti);
+	klp_init_thread_info(p);
 
 	/* Copy registers */
 	sp -= sizeof(struct pt_regs);

commit 05b98791ec60f6a1862c58b3424f6aaeb00dfb72
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 17 23:25:12 2019 +1100

    powerpc: Replace current_thread_info()->task with current
    
    We have a few places that use current_thread_info()->task to access
    current. This won't work with THREAD_INFO_IN_TASK so fix them now.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Split out of larger patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 791bd8ea475d..dc2aaaf75c87 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1231,8 +1231,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		batch->active = 1;
 	}
 
-	if (current_thread_info()->task->thread.regs) {
-		restore_math(current_thread_info()->task->thread.regs);
+	if (current->thread.regs) {
+		restore_math(current->thread.regs);
 
 		/*
 		 * The copy-paste buffer can only store into foreign real
@@ -1242,7 +1242,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		 * mappings, we must issue a cp_abort to clear any state and
 		 * prevent snooping, corruption or a covert channel.
 		 */
-		if (current_thread_info()->task->thread.used_vas)
+		if (current->thread.used_vas)
 			asm volatile(PPC_CP_ABORT);
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */

commit 018cce33c5e62dda265df8ae0ddf7f3a3357ad1f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:52 2019 +0000

    powerpc: prep stack walkers for THREAD_INFO_IN_TASK
    
    [text copied from commit 9bbd4c56b0b6
    ("arm64: prep stack walkers for THREAD_INFO_IN_TASK")]
    
    When CONFIG_THREAD_INFO_IN_TASK is selected, task stacks may be freed
    before a task is destroyed. To account for this, the stacks are
    refcounted, and when manipulating the stack of another task, it is
    necessary to get/put the stack to ensure it isn't freed and/or re-used
    while we do so.
    
    This patch reworks the powerpc stack walking code to account for this.
    When CONFIG_THREAD_INFO_IN_TASK is not selected these perform no
    refcounting, and this should only be a structural change that does not
    affect behaviour.
    
    Acked-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Move try_get_task_stack() below tsk == NULL check in show_stack()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 71bad4b6f80d..791bd8ea475d 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -2027,7 +2027,7 @@ int validate_sp(unsigned long sp, struct task_struct *p,
 
 EXPORT_SYMBOL(validate_sp);
 
-unsigned long get_wchan(struct task_struct *p)
+static unsigned long __get_wchan(struct task_struct *p)
 {
 	unsigned long ip, sp;
 	int count = 0;
@@ -2053,6 +2053,20 @@ unsigned long get_wchan(struct task_struct *p)
 	return 0;
 }
 
+unsigned long get_wchan(struct task_struct *p)
+{
+	unsigned long ret;
+
+	if (!try_get_task_stack(p))
+		return 0;
+
+	ret = __get_wchan(p);
+
+	put_task_stack(p);
+
+	return ret;
+}
+
 static int kstack_depth_to_print = CONFIG_PRINT_STACK_DEPTH;
 
 void show_stack(struct task_struct *tsk, unsigned long *stack)
@@ -2067,9 +2081,13 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	int curr_frame = 0;
 #endif
 
-	sp = (unsigned long) stack;
 	if (tsk == NULL)
 		tsk = current;
+
+	if (!try_get_task_stack(tsk))
+		return;
+
+	sp = (unsigned long) stack;
 	if (sp == 0) {
 		if (tsk == current)
 			sp = current_stack_pointer();
@@ -2081,7 +2099,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	printk("Call Trace:\n");
 	do {
 		if (!validate_sp(sp, tsk, STACK_FRAME_OVERHEAD))
-			return;
+			break;
 
 		stack = (unsigned long *) sp;
 		newsp = stack[0];
@@ -2121,6 +2139,8 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 
 		sp = newsp;
 	} while (count++ < kstack_depth_to_print);
+
+	put_task_stack(tsk);
 }
 
 #ifdef CONFIG_PPC64

commit fe1ef6bcdb4fca33434256a802a3ed6aacf0bd2f
Author: Mark Cave-Ayland <mark.cave-ayland@ilande.co.uk>
Date:   Fri Feb 8 14:33:19 2019 +0000

    powerpc: Fix 32-bit KVM-PR lockup and host crash with MacOS guest
    
    Commit 8792468da5e1 "powerpc: Add the ability to save FPU without
    giving it up" unexpectedly removed the MSR_FE0 and MSR_FE1 bits from
    the bitmask used to update the MSR of the previous thread in
    __giveup_fpu() causing a KVM-PR MacOS guest to lockup and panic the
    host kernel.
    
    Leaving FE0/1 enabled means unrelated processes might receive FPEs
    when they're not expecting them and crash. In particular if this
    happens to init the host will then panic.
    
    eg (transcribed):
      qemu-system-ppc[837]: unhandled signal 8 at 12cc9ce4 nip 12cc9ce4 lr 12cc9ca4 code 0
      systemd[1]: unhandled signal 8 at 202f02e0 nip 202f02e0 lr 001003d4 code 0
      Kernel panic - not syncing: Attempted to kill init! exitcode=0x0000000b
    
    Reinstate these bits to the MSR bitmask to enable MacOS guests to run
    under 32-bit KVM-PR once again without issue.
    
    Fixes: 8792468da5e1 ("powerpc: Add the ability to save FPU without giving it up")
    Cc: stable@vger.kernel.org # v4.6+
    Signed-off-by: Mark Cave-Ayland <mark.cave-ayland@ilande.co.uk>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ce393df243aa..71bad4b6f80d 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -176,7 +176,7 @@ static void __giveup_fpu(struct task_struct *tsk)
 
 	save_fpu(tsk);
 	msr = tsk->thread.regs->msr;
-	msr &= ~MSR_FP;
+	msr &= ~(MSR_FP|MSR_FE0|MSR_FE1);
 #ifdef CONFIG_VSX
 	if (cpu_has_feature(CPU_FTR_VSX))
 		msr &= ~MSR_VSX;

commit 0fad8bfef7b08a68507178f8e278d013b60ff966
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 7 12:35:47 2018 -0500

    powerpc/frace: Use ftrace_graph_get_ret_stack() instead of curr_ret_stack
    
    The structure of the ret_stack array on the task struct is going to
    change, and accessing it directly via the curr_ret_stack index will no
    longer give the ret_stack entry that holds the return address. To access
    that, architectures must now use ftrace_graph_get_ret_stack() to get the
    associated ret_stack that matches the saved return address.
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 96f34730010f..ce393df243aa 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -2061,9 +2061,10 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	int count = 0;
 	int firstframe = 1;
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-	int curr_frame = current->curr_ret_stack;
+	struct ftrace_ret_stack *ret_stack;
 	extern void return_to_handler(void);
 	unsigned long rth = (unsigned long)return_to_handler;
+	int curr_frame = 0;
 #endif
 
 	sp = (unsigned long) stack;
@@ -2089,9 +2090,13 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 			if ((ip == rth) && curr_frame >= 0) {
-				pr_cont(" (%pS)",
-				       (void *)current->ret_stack[curr_frame].ret);
-				curr_frame--;
+				ret_stack = ftrace_graph_get_ret_stack(current,
+								  curr_frame++);
+				if (ret_stack)
+					pr_cont(" (%pS)",
+						(void *)ret_stack->ret);
+				else
+					curr_frame = -1;
 			}
 #endif
 			if (firstframe)

commit b69f9e17a57a50bc34d88975afce4425086e525d
Merge: 63c6e188f639 1936f094e164
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 2 09:19:35 2018 -0700

    Merge tag 'powerpc-4.20-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
     "Some things that I missed due to travel, or that came in late.
    
      Two fixes also going to stable:
    
       - A revert of a buggy change to the 8xx TLB miss handlers.
    
       - Our flushing of SPE (Signal Processing Engine) registers on fork
         was broken.
    
      Other changes:
    
       - A change to the KVM decrementer emulation to use proper APIs.
    
       - Some cleanups to the way we do code patching in the 8xx code.
    
       - Expose the maximum possible memory for the system in
         /proc/powerpc/lparcfg.
    
       - Merge some updates from Scott: "a couple device tree updates, and a
         fix for a missing prototype warning"
    
      A few other minor fixes and a handful of fixes for our selftests.
    
      Thanks to: Aravinda Prasad, Breno Leitao, Camelia Groza, Christophe
      Leroy, Felipe Rechia, Joel Stanley, Naveen N. Rao, Paul Mackerras,
      Scott Wood, Tyrel Datwyler"
    
    * tag 'powerpc-4.20-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (21 commits)
      selftests/powerpc: Fix compilation issue due to asm label
      selftests/powerpc/cache_shape: Fix out-of-tree build
      selftests/powerpc/switch_endian: Fix out-of-tree build
      selftests/powerpc/pmu: Link ebb tests with -no-pie
      selftests/powerpc/signal: Fix out-of-tree build
      selftests/powerpc/ptrace: Fix out-of-tree build
      powerpc/xmon: Relax frame size for clang
      selftests: powerpc: Fix warning for security subdir
      selftests/powerpc: Relax L1d miss targets for rfi_flush test
      powerpc/process: Fix flush_all_to_thread for SPE
      powerpc/pseries: add missing cpumask.h include file
      selftests/powerpc: Fix ptrace tm failure
      KVM: PPC: Use exported tb_to_ns() function in decrementer emulation
      powerpc/pseries: Export maximum memory value
      powerpc/8xx: Use patch_site for perf counters setup
      powerpc/8xx: Use patch_site for memory setup patching
      powerpc/code-patching: Add a helper to get the address of a patch_site
      Revert "powerpc/8xx: Use L1 entry APG to handle _PAGE_ACCESSED for CONFIG_SWAP"
      powerpc/8xx: add missing header in 8xx_mmu.c
      powerpc/8xx: Add DT node for using the SEC engine of the MPC885
      ...

commit 685f7e4f161425b137056abe35ba8ef7b669d83d
Merge: c7a2c49ea6c9 58cfbac25b1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 14:36:21 2018 -0700

    Merge tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - A large series to rewrite our SLB miss handling, replacing a lot of
         fairly complicated asm with much fewer lines of C.
    
       - Following on from that, we now maintain a cache of SLB entries for
         each process and preload them on context switch. Leading to a 27%
         speedup for our context switch benchmark on Power9.
    
       - Improvements to our handling of SLB multi-hit errors. We now print
         more debug information when they occur, and try to continue running
         by flushing the SLB and reloading, rather than treating them as
         fatal.
    
       - Enable THP migration on 64-bit Book3S machines (eg. Power7/8/9).
    
       - Add support for physical memory up to 2PB in the linear mapping on
         64-bit Book3S. We only support up to 512TB as regular system
         memory, otherwise the percpu allocator runs out of vmalloc space.
    
       - Add stack protector support for 32 and 64-bit, with a per-task
         canary.
    
       - Add support for PTRACE_SYSEMU and PTRACE_SYSEMU_SINGLESTEP.
    
       - Support recognising "big cores" on Power9, where two SMT4 cores are
         presented to us as a single SMT8 core.
    
       - A large series to cleanup some of our ioremap handling and PTE
         flags.
    
       - Add a driver for the PAPR SCM (storage class memory) interface,
         allowing guests to operate on SCM devices (acked by Dan).
    
       - Changes to our ftrace code to handle very large kernels, where we
         need to use a trampoline to get to ftrace_caller().
    
      And many other smaller enhancements and cleanups.
    
      Thanks to: Alan Modra, Alistair Popple, Aneesh Kumar K.V, Anton
      Blanchard, Aravinda Prasad, Bartlomiej Zolnierkiewicz, Benjamin
      Herrenschmidt, Breno Leitao, Cdric Le Goater, Christophe Leroy,
      Christophe Lombard, Dan Carpenter, Daniel Axtens, Finn Thain, Gautham
      R. Shenoy, Gustavo Romero, Haren Myneni, Hari Bathini, Jia Hongtao,
      Joel Stanley, John Allen, Laurent Dufour, Madhavan Srinivasan, Mahesh
      Salgaonkar, Mark Hairgrove, Masahiro Yamada, Michael Bringmann,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Nathan
      Fontenot, Naveen N. Rao, Nicholas Piggin, Nick Desaulniers, Oliver
      O'Halloran, Paul Mackerras, Petr Vorel, Rashmica Gupta, Reza Arbab,
      Rob Herring, Sam Bobroff, Samuel Mendoza-Jonas, Scott Wood, Stan
      Johnson, Stephen Rothwell, Stewart Smith, Suraj Jitindar Singh, Tyrel
      Datwyler, Vaibhav Jain, Vasant Hegde, YueHaibing, zhong jiang"
    
    * tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (221 commits)
      Revert "selftests/powerpc: Fix out-of-tree build errors"
      powerpc/msi: Fix compile error on mpc83xx
      powerpc: Fix stack protector crashes on CPU hotplug
      powerpc/traps: restore recoverability of machine_check interrupts
      powerpc/64/module: REL32 relocation range check
      powerpc/64s/radix: Fix radix__flush_tlb_collapsed_pmd double flushing pmd
      selftests/powerpc: Add a test of wild bctr
      powerpc/mm: Fix page table dump to work on Radix
      powerpc/mm/radix: Display if mappings are exec or not
      powerpc/mm/radix: Simplify split mapping logic
      powerpc/mm/radix: Remove the retry in the split mapping logic
      powerpc/mm/radix: Fix small page at boundary when splitting
      powerpc/mm/radix: Fix overuse of small pages in splitting logic
      powerpc/mm/radix: Fix off-by-one in split mapping logic
      powerpc/ftrace: Handle large kernel configs
      powerpc/mm: Fix WARN_ON with THP NUMA migration
      selftests/powerpc: Fix out-of-tree build errors
      powerpc/time: no steal_time when CONFIG_PPC_SPLPAR is not selected
      powerpc/time: Only set CONFIG_ARCH_HAS_SCALED_CPUTIME on PPC64
      powerpc/time: isolate scaled cputime accounting in dedicated functions.
      ...

commit e901378578c62202594cba0f6c076f3df365ec91
Author: Felipe Rechia <felipe.rechia@datacom.com.br>
Date:   Wed Oct 24 10:57:22 2018 -0300

    powerpc/process: Fix flush_all_to_thread for SPE
    
    Fix a bug introduced by the creation of flush_all_to_thread() for
    processors that have SPE (Signal Processing Engine) and use it to
    compute floating-point operations.
    
    >From userspace perspective, the problem was seen in attempts of
    computing floating-point operations which should generate exceptions.
    For example:
    
      fork();
      float x = 0.0 / 0.0;
      isnan(x);           // forked process returns False (should be True)
    
    The operation above also should always cause the SPEFSCR FINV bit to
    be set. However, the SPE floating-point exceptions were turned off
    after a fork().
    
    Kernel versions prior to the bug used flush_spe_to_thread(), which
    first saves SPEFSCR register values in tsk->thread and then calls
    giveup_spe(tsk).
    
    After commit 579e633e764e, the save_all() function was called first
    to giveup_spe(), and then the SPEFSCR register values were saved in
    tsk->thread. This would save the SPEFSCR register values after
    disabling SPE for that thread, causing the bug described above.
    
    Fixes 579e633e764e ("powerpc: create flush_all_to_thread()")
    Signed-off-by: Felipe Rechia <felipe.rechia@datacom.com.br>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7ad304a3cc7d..bcb36229d4fd 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -590,12 +590,11 @@ void flush_all_to_thread(struct task_struct *tsk)
 	if (tsk->thread.regs) {
 		preempt_disable();
 		BUG_ON(tsk != current);
-		save_all(tsk);
-
 #ifdef CONFIG_SPE
 		if (tsk->thread.regs->msr & MSR_SPE)
 			tsk->thread.spefscr = mfspr(SPRN_SPEFSCR);
 #endif
+		save_all(tsk);
 
 		preempt_enable();
 	}

commit ba9f6f8954afa5224e3ed60332f7b92242b7ed0f
Merge: a978a5b8d83f a36700589b85
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 24 11:22:39 2018 +0100

    Merge branch 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull siginfo updates from Eric Biederman:
     "I have been slowly sorting out siginfo and this is the culmination of
      that work.
    
      The primary result is in several ways the signal infrastructure has
      been made less error prone. The code has been updated so that manually
      specifying SEND_SIG_FORCED is never necessary. The conversion to the
      new siginfo sending functions is now complete, which makes it
      difficult to send a signal without filling in the proper siginfo
      fields.
    
      At the tail end of the patchset comes the optimization of decreasing
      the size of struct siginfo in the kernel from 128 bytes to about 48
      bytes on 64bit. The fundamental observation that enables this is by
      definition none of the known ways to use struct siginfo uses the extra
      bytes.
    
      This comes at the cost of a small user space observable difference.
      For the rare case of siginfo being injected into the kernel only what
      can be copied into kernel_siginfo is delivered to the destination, the
      rest of the bytes are set to 0. For cases where the signal and the
      si_code are known this is safe, because we know those bytes are not
      used. For cases where the signal and si_code combination is unknown
      the bits that won't fit into struct kernel_siginfo are tested to
      verify they are zero, and the send fails if they are not.
    
      I made an extensive search through userspace code and I could not find
      anything that would break because of the above change. If it turns out
      I did break something it will take just the revert of a single change
      to restore kernel_siginfo to the same size as userspace siginfo.
    
      Testing did reveal dependencies on preferring the signo passed to
      sigqueueinfo over si->signo, so bit the bullet and added the
      complexity necessary to handle that case.
    
      Testing also revealed bad things can happen if a negative signal
      number is passed into the system calls. Something no sane application
      will do but something a malicious program or a fuzzer might do. So I
      have fixed the code that performs the bounds checks to ensure negative
      signal numbers are handled"
    
    * 'siginfo-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (80 commits)
      signal: Guard against negative signal numbers in copy_siginfo_from_user32
      signal: Guard against negative signal numbers in copy_siginfo_from_user
      signal: In sigqueueinfo prefer sig not si_signo
      signal: Use a smaller struct siginfo in the kernel
      signal: Distinguish between kernel_siginfo and siginfo
      signal: Introduce copy_siginfo_from_user and use it's return value
      signal: Remove the need for __ARCH_SI_PREABLE_SIZE and SI_PAD_SIZE
      signal: Fail sigqueueinfo if si_signo != sig
      signal/sparc: Move EMT_TAGOVF into the generic siginfo.h
      signal/unicore32: Use force_sig_fault where appropriate
      signal/unicore32: Generate siginfo in ucs32_notify_die
      signal/unicore32: Use send_sig_fault where appropriate
      signal/arc: Use force_sig_fault where appropriate
      signal/arc: Push siginfo generation into unhandled_exception
      signal/ia64: Use force_sig_fault where appropriate
      signal/ia64: Use the force_sig(SIGSEGV,...) in ia64_rt_sigreturn
      signal/ia64: Use the generic force_sigsegv in setup_frame
      signal/arm/kvm: Use send_sig_mceerr
      signal/arm: Use send_sig_fault where appropriate
      signal/arm: Use force_sig_fault where appropriate
      ...

commit 5434ae74629af58ad0fc27143a9ea435f7734410
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:56 2018 +1000

    powerpc/64s/hash: Add a SLB preload cache
    
    When switching processes, currently all user SLBEs are cleared, and a
    few (exec_base, pc, and stack) are preloaded. In trivial testing with
    small apps, this tends to miss the heap and low 256MB segments, and it
    will also miss commonly accessed segments on large memory workloads.
    
    Add a simple round-robin preload cache that just inserts the last SLB
    miss into the head of the cache and preloads those at context switch
    time. Every 256 context switches, the oldest entry is removed from the
    cache to shrink the cache and require fewer slbmte if they are unused.
    
    Much more could go into this, including into the SLB entry reclaim
    side to track some LRU information etc, which would require a study of
    large memory workloads. But this is a simple thing we can do now that
    is an obvious win for common workloads.
    
    With the full series, process switching speed on the context_switch
    benchmark on POWER9/hash (with kernel speculation security masures
    disabled) increases from 140K/s to 178K/s (27%).
    
    POWER8 does not change much (within 1%), it's unclear why it does not
    see a big gain like POWER9.
    
    Booting to busybox init with 256MB segments has SLB misses go down
    from 945 to 69, and with 1T segments 900 to 21. These could almost all
    be eliminated by preloading a bit more carefully with ELF binary
    loading.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 96cd9cd1a119..7ad304a3cc7d 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1727,6 +1727,8 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	return 0;
 }
 
+void preload_new_slb_context(unsigned long start, unsigned long sp);
+
 /*
  * Set up a thread for executing a new program
  */
@@ -1734,6 +1736,10 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 {
 #ifdef CONFIG_PPC64
 	unsigned long load_addr = regs->gpr[2];	/* saved by ELF_PLAT_INIT */
+
+#ifdef CONFIG_PPC_BOOK3S_64
+	preload_new_slb_context(start, sp);
+#endif
 #endif
 
 	/*
@@ -1824,6 +1830,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 #ifdef CONFIG_VSX
 	current->thread.used_vsr = 0;
 #endif
+	current->thread.load_slb = 0;
 	current->thread.load_fp = 0;
 	memset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));
 	current->thread.fp_save_area = NULL;

commit 425d33146260a4a2e8a1ba64003d6c8ff3bdfcc4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:55 2018 +1000

    powerpc/64s/hash: Provide arch_setup_exec() hooks for hash slice setup
    
    This will be used by the SLB code in the next patch, but for now this
    sets the slb_addr_limit to the correct size for 32-bit tasks.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f9d1cca28cce..96cd9cd1a119 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1490,6 +1490,15 @@ void flush_thread(void)
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 }
 
+#ifdef CONFIG_PPC_BOOK3S_64
+void arch_setup_new_exec(void)
+{
+	if (radix_enabled())
+		return;
+	hash__setup_new_exec();
+}
+#endif
+
 int set_thread_uses_vas(void)
 {
 #ifdef CONFIG_PPC_BOOK3S_64

commit 4c2de74cc8696154b283f241d74ec0bb24438e22
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Oct 13 00:15:16 2018 +1100

    powerpc/64: Interrupts save PPR on stack rather than thread_struct
    
    PPR is the odd register out when it comes to interrupt handling, it is
    saved in current->thread.ppr while all others are saved on the stack.
    
    The difficulty with this is that accessing thread.ppr can cause a SLB
    fault, but the SLB fault handler implementation in C change had
    assumed the normal exception entry handlers would not cause an SLB
    fault.
    
    Fix this by allocating room in the interrupt stack to save PPR.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0ed8d0968515..f9d1cca28cce 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1710,7 +1710,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		p->thread.dscr = mfspr(SPRN_DSCR);
 	}
 	if (cpu_has_feature(CPU_FTR_HAS_PPR))
-		p->thread.ppr = INIT_PPR;
+		childregs->ppr = DEFAULT_PPR;
 
 	p->thread.tidr = 0;
 #endif

commit df13102f82f1c8d0a1f43505275bf18246d7f9a0
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Oct 6 16:51:16 2018 +0000

    powerpc/process: Constify the number of insns printed by show instructions functions.
    
    instructions_to_print var is assigned value 16 and there is no
    way to change it.
    
    This patch replaces it by a constant.
    
    Reviewed-by: Murilo Opsfelder Araujo <muriloo@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 050f1136f587..0ed8d0968515 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1258,17 +1258,16 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	return last;
 }
 
-static int instructions_to_print = 16;
+#define NR_INSN_TO_PRINT	16
 
 static void show_instructions(struct pt_regs *regs)
 {
 	int i;
-	unsigned long pc = regs->nip - (instructions_to_print * 3 / 4 *
-			sizeof(int));
+	unsigned long pc = regs->nip - (NR_INSN_TO_PRINT * 3 / 4 * sizeof(int));
 
 	printk("Instruction dump:");
 
-	for (i = 0; i < instructions_to_print; i++) {
+	for (i = 0; i < NR_INSN_TO_PRINT; i++) {
 		int instr;
 
 		if (!(i % 8))
@@ -1301,17 +1300,17 @@ static void show_instructions(struct pt_regs *regs)
 void show_user_instructions(struct pt_regs *regs)
 {
 	unsigned long pc;
-	int n = instructions_to_print;
+	int n = NR_INSN_TO_PRINT;
 	struct seq_buf s;
 	char buf[96]; /* enough for 8 times 9 + 2 chars */
 
-	pc = regs->nip - (instructions_to_print * 3 / 4 * sizeof(int));
+	pc = regs->nip - (NR_INSN_TO_PRINT * 3 / 4 * sizeof(int));
 
 	/*
 	 * Make sure the NIP points at userspace, not kernel text/data or
 	 * elsewhere.
 	 */
-	if (!__access_ok(pc, instructions_to_print * sizeof(int), USER_DS)) {
+	if (!__access_ok(pc, NR_INSN_TO_PRINT * sizeof(int), USER_DS)) {
 		pr_info("%s[%d]: Bad NIP, not dumping instructions.\n",
 			current->comm, current->pid);
 		return;

commit fb2d9505c0dbd4f5e00db70f7ca0ca7a3d75ca63
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Oct 6 16:51:14 2018 +0000

    powerpc/process: Fix interleaved output in show_user_instructions()
    
    When two processes crash at the same time, we sometimes encounter
    interleaving in the middle of a line:
    
      init[1]: segfault (11) at 0 nip 0 lr 0 code 1
      init[1]: code: XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX
      init[74]: segfault (11) at 10a74 nip 1000c198 lr 100078c8 code 1 in sh[10000000+14000]
      XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX
      init[1]: code: XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX XXXXXXXX
      init[74]: code: 90010024 bf61000c 91490a7c 3fa01002 3be00000 7d3e4b78 3bbd0c20 3b600000
      init[74]: code: 3b9d0040 7c7fe02e 2f830000 419e0028 <89230000> 2f890000 41be001c 4b7f6e79
    
    This patch fixes it by preparing complete lines in a buffer and
    printing it at once.
    
    Fixes: 88b0fe1757359 ("powerpc: Add show_user_instructions()")
    Reviewed-by: Murilo Opsfelder Araujo <muriloo@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Use seq_buf_printf() not seq_buf_puts() which doesn't NULL terminate]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3396c419abf2..050f1136f587 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -43,6 +43,7 @@
 #include <linux/uaccess.h>
 #include <linux/elf-randomize.h>
 #include <linux/pkeys.h>
+#include <linux/seq_buf.h>
 
 #include <asm/pgtable.h>
 #include <asm/io.h>
@@ -1300,7 +1301,9 @@ static void show_instructions(struct pt_regs *regs)
 void show_user_instructions(struct pt_regs *regs)
 {
 	unsigned long pc;
-	int i;
+	int n = instructions_to_print;
+	struct seq_buf s;
+	char buf[96]; /* enough for 8 times 9 + 2 chars */
 
 	pc = regs->nip - (instructions_to_print * 3 / 4 * sizeof(int));
 
@@ -1314,29 +1317,27 @@ void show_user_instructions(struct pt_regs *regs)
 		return;
 	}
 
-	pr_info("%s[%d]: code: ", current->comm, current->pid);
+	seq_buf_init(&s, buf, sizeof(buf));
 
-	for (i = 0; i < instructions_to_print; i++) {
-		int instr;
+	while (n) {
+		int i;
 
-		if (!(i % 8) && (i > 0)) {
-			pr_cont("\n");
-			pr_info("%s[%d]: code: ", current->comm, current->pid);
-		}
+		seq_buf_clear(&s);
 
-		if (probe_kernel_address((const void *)pc, instr)) {
-			pr_cont("XXXXXXXX ");
-		} else {
-			if (regs->nip == pc)
-				pr_cont("<%08x> ", instr);
-			else
-				pr_cont("%08x ", instr);
+		for (i = 0; i < 8 && n; i++, n--, pc += sizeof(int)) {
+			int instr;
+
+			if (probe_kernel_address((const void *)pc, instr)) {
+				seq_buf_printf(&s, "XXXXXXXX ");
+				continue;
+			}
+			seq_buf_printf(&s, regs->nip == pc ? "<%08x> " : "%08x ", instr);
 		}
 
-		pc += sizeof(int);
+		if (!seq_buf_has_overflowed(&s))
+			pr_info("%s[%d]: code: %s\n", current->comm,
+				current->pid, s.buffer);
 	}
-
-	pr_cont("\n");
 }
 
 struct regbit {

commit c9386bfd37d37f29588de9ea9add455510049c33
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Oct 9 16:46:25 2018 +1100

    powerpc/process: Add missing include of stacktrace.h
    
    As spotted by sparse:
    
      arch/powerpc/kernel/process.c:1302:6: warning: symbol 'show_user_instructions' was not declared. Should it be static?
    
    Fixes: 88b0fe1757359 ("powerpc: Add show_user_instructions()")
    Reviewed-by: Murilo Opsfelder Araujo <muriloo@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Split out of larger patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c7af42052041..3396c419abf2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -65,6 +65,7 @@
 #include <asm/livepatch.h>
 #include <asm/cpu_has_feature.h>
 #include <asm/asm-prototypes.h>
+#include <asm/stacktrace.h>
 
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>

commit 3b35bd48b8a06e02a25af84baba782876b8a6572
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Oct 6 16:51:12 2018 +0000

    powerpc/process: Fix sparse address space warnings
    
    This patch fixes the following warnings, which are leftovers
    from when __get_user() was replaced by probe_kernel_address().
    
    arch/powerpc/kernel/process.c:1287:22: warning: incorrect type in argument 2 (different address spaces)
    arch/powerpc/kernel/process.c:1287:22:    expected void const *src
    arch/powerpc/kernel/process.c:1287:22:    got unsigned int [noderef] <asn:1>*<noident>
    arch/powerpc/kernel/process.c:1319:21: warning: incorrect type in argument 2 (different address spaces)
    arch/powerpc/kernel/process.c:1319:21:    expected void const *src
    arch/powerpc/kernel/process.c:1319:21:    got unsigned int [noderef] <asn:1>*<noident>
    
    Fixes: 7b051f665c32d ("powerpc: Use probe_kernel_address in show_instructions")
    Reviewed-by: Murilo Opsfelder Araujo <muriloo@linux.ibm.com>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Split out of larger patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index d9d4eb2ea6c9..c7af42052041 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1281,7 +1281,7 @@ static void show_instructions(struct pt_regs *regs)
 #endif
 
 		if (!__kernel_text_address(pc) ||
-		     probe_kernel_address((unsigned int __user *)pc, instr)) {
+		    probe_kernel_address((const void *)pc, instr)) {
 			pr_cont("XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)
@@ -1323,7 +1323,7 @@ void show_user_instructions(struct pt_regs *regs)
 			pr_info("%s[%d]: code: ", current->comm, current->pid);
 		}
 
-		if (probe_kernel_address((unsigned int __user *)pc, instr)) {
+		if (probe_kernel_address((const void *)pc, instr)) {
 			pr_cont("XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)

commit 9b7e4d601baac83a7104652042107ce94f245524
Merge: 47fd2060660e ac1788cc7da4
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Oct 9 16:51:05 2018 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch. It has a few important fixes that are needed for
    futher testing and also some commits that will conflict with content in
    next.

commit a932ed3b718147c6537da290b7a91e990fdedb43
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Oct 5 16:43:55 2018 +1000

    powerpc: Don't print kernel instructions in show_user_instructions()
    
    Recently we implemented show_user_instructions() which dumps the code
    around the NIP when a user space process dies with an unhandled
    signal. This was modelled on the x86 code, and we even went so far as
    to implement the exact same bug, namely that if the user process
    crashed with its NIP pointing into the kernel we will dump kernel text
    to dmesg. eg:
    
      bad-bctr[2996]: segfault (11) at c000000000010000 nip c000000000010000 lr 12d0b0894 code 1
      bad-bctr[2996]: code: fbe10068 7cbe2b78 7c7f1b78 fb610048 38a10028 38810020 fb810050 7f8802a6
      bad-bctr[2996]: code: 3860001c f8010080 48242371 60000000 <7c7b1b79> 4082002c e8010080 eb610048
    
    This was discovered on x86 by Jann Horn and fixed in commit
    342db04ae712 ("x86/dumpstack: Don't dump kernel memory based on usermode RIP").
    
    Fix it by checking the adjusted NIP value (pc) and number of
    instructions against USER_DS, and bail if we fail the check, eg:
    
      bad-bctr[2969]: segfault (11) at c000000000010000 nip c000000000010000 lr 107930894 code 1
      bad-bctr[2969]: Bad NIP, not dumping instructions.
    
    Fixes: 88b0fe175735 ("powerpc: Add show_user_instructions()")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 913c5725cdb2..bb6ac471a784 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1306,6 +1306,16 @@ void show_user_instructions(struct pt_regs *regs)
 
 	pc = regs->nip - (instructions_to_print * 3 / 4 * sizeof(int));
 
+	/*
+	 * Make sure the NIP points at userspace, not kernel text/data or
+	 * elsewhere.
+	 */
+	if (!__access_ok(pc, instructions_to_print * sizeof(int), USER_DS)) {
+		pr_info("%s[%d]: Bad NIP, not dumping instructions.\n",
+			current->comm, current->pid);
+		return;
+	}
+
 	pr_info("%s[%d]: code: ", current->comm, current->pid);
 
 	for (i = 0; i < instructions_to_print; i++) {

commit 5c784c8414fba11b62e12439f11e109fb5751f38
Author: Breno Leitao <leitao@debian.org>
Date:   Thu Aug 16 14:21:07 2018 -0300

    powerpc/tm: Remove msr_tm_active()
    
    Currently msr_tm_active() is a wrapper around MSR_TM_ACTIVE() if
    CONFIG_PPC_TRANSACTIONAL_MEM is set, or it is just a function that
    returns false if CONFIG_PPC_TRANSACTIONAL_MEM is not set.
    
    This function is not necessary, since MSR_TM_ACTIVE() just do the same and
    could be used, removing the dualism and simplifying the code.
    
    This patchset remove every instance of msr_tm_active() and replaced it
    by MSR_TM_ACTIVE().
    
    Signed-off-by: Breno Leitao <leitao@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 913c5725cdb2..ec264a6f0eb3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -102,24 +102,18 @@ static void check_if_tm_restore_required(struct task_struct *tsk)
 	}
 }
 
-static inline bool msr_tm_active(unsigned long msr)
-{
-	return MSR_TM_ACTIVE(msr);
-}
-
 static bool tm_active_with_fp(struct task_struct *tsk)
 {
-	return msr_tm_active(tsk->thread.regs->msr) &&
+	return MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
 		(tsk->thread.ckpt_regs.msr & MSR_FP);
 }
 
 static bool tm_active_with_altivec(struct task_struct *tsk)
 {
-	return msr_tm_active(tsk->thread.regs->msr) &&
+	return MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
 		(tsk->thread.ckpt_regs.msr & MSR_VEC);
 }
 #else
-static inline bool msr_tm_active(unsigned long msr) { return false; }
 static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
 static inline bool tm_active_with_fp(struct task_struct *tsk) { return false; }
 static inline bool tm_active_with_altivec(struct task_struct *tsk) { return false; }
@@ -247,7 +241,8 @@ void enable_kernel_fp(void)
 		 * giveup as this would save  to the 'live' structure not the
 		 * checkpointed structure.
 		 */
-		if(!msr_tm_active(cpumsr) && msr_tm_active(current->thread.regs->msr))
+		if (!MSR_TM_ACTIVE(cpumsr) &&
+		     MSR_TM_ACTIVE(current->thread.regs->msr))
 			return;
 		__giveup_fpu(current);
 	}
@@ -311,7 +306,8 @@ void enable_kernel_altivec(void)
 		 * giveup as this would save  to the 'live' structure not the
 		 * checkpointed structure.
 		 */
-		if(!msr_tm_active(cpumsr) && msr_tm_active(current->thread.regs->msr))
+		if (!MSR_TM_ACTIVE(cpumsr) &&
+		     MSR_TM_ACTIVE(current->thread.regs->msr))
 			return;
 		__giveup_altivec(current);
 	}
@@ -397,7 +393,8 @@ void enable_kernel_vsx(void)
 		 * giveup as this would save  to the 'live' structure not the
 		 * checkpointed structure.
 		 */
-		if(!msr_tm_active(cpumsr) && msr_tm_active(current->thread.regs->msr))
+		if (!MSR_TM_ACTIVE(cpumsr) &&
+		     MSR_TM_ACTIVE(current->thread.regs->msr))
 			return;
 		__giveup_vsx(current);
 	}
@@ -530,7 +527,7 @@ void restore_math(struct pt_regs *regs)
 {
 	unsigned long msr;
 
-	if (!msr_tm_active(regs->msr) &&
+	if (!MSR_TM_ACTIVE(regs->msr) &&
 		!current->thread.load_fp && !loadvec(current->thread))
 		return;
 

commit 54be0b9c7c9888ebe63b89a31a17ee3df6a68d61
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Oct 2 23:56:39 2018 +1000

    Revert "convert SLB miss handlers to C" and subsequent commits
    
    This reverts commits:
      5e46e29e6a97 ("powerpc/64s/hash: convert SLB miss handlers to C")
      8fed04d0f6ae ("powerpc/64s/hash: remove user SLB data from the paca")
      655deecf67b2 ("powerpc/64s/hash: SLB allocation status bitmaps")
      2e1626744e8d ("powerpc/64s/hash: provide arch_setup_exec hooks for hash slice setup")
      89ca4e126a3f ("powerpc/64s/hash: Add a SLB preload cache")
    
    This series had a few bugs, and the fixes are not all trivial. So
    revert most of it for now.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 03c2e1f134bc..913c5725cdb2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1482,15 +1482,6 @@ void flush_thread(void)
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 }
 
-#ifdef CONFIG_PPC_BOOK3S_64
-void arch_setup_new_exec(void)
-{
-	if (radix_enabled())
-		return;
-	hash__setup_new_exec();
-}
-#endif
-
 int set_thread_uses_vas(void)
 {
 #ifdef CONFIG_PPC_BOOK3S_64
@@ -1719,8 +1710,6 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	return 0;
 }
 
-void preload_new_slb_context(unsigned long start, unsigned long sp);
-
 /*
  * Set up a thread for executing a new program
  */
@@ -1728,10 +1717,6 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 {
 #ifdef CONFIG_PPC64
 	unsigned long load_addr = regs->gpr[2];	/* saved by ELF_PLAT_INIT */
-
-#ifdef CONFIG_PPC_BOOK3S_64
-	preload_new_slb_context(start, sp);
-#endif
 #endif
 
 	/*
@@ -1822,7 +1807,6 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 #ifdef CONFIG_VSX
 	current->thread.used_vsr = 0;
 #endif
-	current->thread.load_slb = 0;
 	current->thread.load_fp = 0;
 	memset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));
 	current->thread.fp_save_area = NULL;

commit f383d8b4aec3c238c8d5f56854ddc7a7c3d1cc20
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 18 10:00:32 2018 +0200

    signal/powerpc: Use force_sig_fault where appropriate
    
    Reviewed-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 913c5725cdb2..553a396e7fc1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -620,8 +620,6 @@ void do_send_trap(struct pt_regs *regs, unsigned long address,
 void do_break (struct pt_regs *regs, unsigned long address,
 		    unsigned long error_code)
 {
-	siginfo_t info;
-
 	current->thread.trap_nr = TRAP_HWBKPT;
 	if (notify_die(DIE_DABR_MATCH, "dabr_match", regs, error_code,
 			11, SIGSEGV) == NOTIFY_STOP)
@@ -634,12 +632,7 @@ void do_break (struct pt_regs *regs, unsigned long address,
 	hw_breakpoint_disable();
 
 	/* Deliver the signal to userspace */
-	clear_siginfo(&info);
-	info.si_signo = SIGTRAP;
-	info.si_errno = 0;
-	info.si_code = TRAP_HWBKPT;
-	info.si_addr = (void __user *)address;
-	force_sig_info(SIGTRAP, &info, current);
+	force_sig_fault(SIGTRAP, TRAP_HWBKPT, (void __user *)address, current);
 }
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 

commit 89ca4e126a3f519ccbd42670b38d78700802c10b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:56 2018 +1000

    powerpc/64s/hash: Add a SLB preload cache
    
    When switching processes, currently all user SLBEs are cleared, and a
    few (exec_base, pc, and stack) are preloaded. In trivial testing with
    small apps, this tends to miss the heap and low 256MB segments, and it
    will also miss commonly accessed segments on large memory workloads.
    
    Add a simple round-robin preload cache that just inserts the last SLB
    miss into the head of the cache and preloads those at context switch
    time. Every 256 context switches, the oldest entry is removed from the
    cache to shrink the cache and require fewer slbmte if they are unused.
    
    Much more could go into this, including into the SLB entry reclaim
    side to track some LRU information etc, which would require a study of
    large memory workloads. But this is a simple thing we can do now that
    is an obvious win for common workloads.
    
    With the full series, process switching speed on the context_switch
    benchmark on POWER9/hash (with kernel speculation security masures
    disabled) increases from 140K/s to 178K/s (27%).
    
    POWER8 does not change much (within 1%), it's unclear why it does not
    see a big gain like POWER9.
    
    Booting to busybox init with 256MB segments has SLB misses go down
    from 945 to 69, and with 1T segments 900 to 21. These could almost all
    be eliminated by preloading a bit more carefully with ELF binary
    loading.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e4feb45ae4c6..03c2e1f134bc 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1719,6 +1719,8 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	return 0;
 }
 
+void preload_new_slb_context(unsigned long start, unsigned long sp);
+
 /*
  * Set up a thread for executing a new program
  */
@@ -1726,6 +1728,10 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 {
 #ifdef CONFIG_PPC64
 	unsigned long load_addr = regs->gpr[2];	/* saved by ELF_PLAT_INIT */
+
+#ifdef CONFIG_PPC_BOOK3S_64
+	preload_new_slb_context(start, sp);
+#endif
 #endif
 
 	/*
@@ -1816,6 +1822,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 #ifdef CONFIG_VSX
 	current->thread.used_vsr = 0;
 #endif
+	current->thread.load_slb = 0;
 	current->thread.load_fp = 0;
 	memset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));
 	current->thread.fp_save_area = NULL;

commit 2e1626744e8da01eb5a2a0aaa3f365e41f1feb49
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:55 2018 +1000

    powerpc/64s/hash: provide arch_setup_exec hooks for hash slice setup
    
    This will be used by the SLB code in the next patch, but for now this
    sets the slb_addr_limit to the correct size for 32-bit tasks.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 913c5725cdb2..e4feb45ae4c6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1482,6 +1482,15 @@ void flush_thread(void)
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 }
 
+#ifdef CONFIG_PPC_BOOK3S_64
+void arch_setup_new_exec(void)
+{
+	if (radix_enabled())
+		return;
+	hash__setup_new_exec();
+}
+#endif
+
 int set_thread_uses_vas(void)
 {
 #ifdef CONFIG_PPC_BOOK3S_64

commit 88b0fe17573592a8e3196bf143f865da460178e7
Author: Murilo Opsfelder Araujo <muriloo@linux.ibm.com>
Date:   Wed Aug 1 18:33:19 2018 -0300

    powerpc: Add show_user_instructions()
    
    show_user_instructions() is a slightly modified version of
    show_instructions() that allows userspace instruction dump.
    
    This will be useful within show_signal_msg() to dump userspace
    instructions of the faulty location.
    
    Here is a sample of what show_user_instructions() outputs:
    
      pandafault[10850]: code: 4bfffeec 4bfffee8 3c401002 38427f00 fbe1fff8 f821ffc1 7c3f0b78 3d22fffe
      pandafault[10850]: code: 392988d0 f93f0020 e93f0020 39400048 <99490000> 39200000 7d234b78 383f0040
    
    The current->comm and current->pid printed can serve as a glue that
    links the instructions dump to its originator, allowing messages to be
    interleaved in the logs.
    
    Signed-off-by: Murilo Opsfelder Araujo <muriloo@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2172f9908633..913c5725cdb2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1299,6 +1299,38 @@ static void show_instructions(struct pt_regs *regs)
 	pr_cont("\n");
 }
 
+void show_user_instructions(struct pt_regs *regs)
+{
+	unsigned long pc;
+	int i;
+
+	pc = regs->nip - (instructions_to_print * 3 / 4 * sizeof(int));
+
+	pr_info("%s[%d]: code: ", current->comm, current->pid);
+
+	for (i = 0; i < instructions_to_print; i++) {
+		int instr;
+
+		if (!(i % 8) && (i > 0)) {
+			pr_cont("\n");
+			pr_info("%s[%d]: code: ", current->comm, current->pid);
+		}
+
+		if (probe_kernel_address((unsigned int __user *)pc, instr)) {
+			pr_cont("XXXXXXXX ");
+		} else {
+			if (regs->nip == pc)
+				pr_cont("<%08x> ", instr);
+			else
+				pr_cont("%08x ", instr);
+		}
+
+		pc += sizeof(int);
+	}
+
+	pr_cont("\n");
+}
+
 struct regbit {
 	unsigned long bit;
 	const char *name;

commit b5ac51d747122f8858bdcb3fc7a5c702ef06f6c5
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:25:05 2018 +0000

    powerpc: declare set_breakpoint() static
    
    set_breakpoint() is only used in process.c so make it static
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e9533b4d2f08..2172f9908633 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -717,6 +717,13 @@ void switch_booke_debug_regs(struct debug_reg *new_debug)
 EXPORT_SYMBOL_GPL(switch_booke_debug_regs);
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
+static void set_breakpoint(struct arch_hw_breakpoint *brk)
+{
+	preempt_disable();
+	__set_breakpoint(brk);
+	preempt_enable();
+}
+
 static void set_debug_reg_defaults(struct thread_struct *thread)
 {
 	thread->hw_brk.address = 0;
@@ -829,13 +836,6 @@ void __set_breakpoint(struct arch_hw_breakpoint *brk)
 		WARN_ON_ONCE(1);
 }
 
-void set_breakpoint(struct arch_hw_breakpoint *brk)
-{
-	preempt_disable();
-	__set_breakpoint(brk);
-	preempt_enable();
-}
-
 /* Check if we have DAWR or DABR hardware */
 bool ppc_breakpoint_available(void)
 {

commit edd00b830731be468fd3caf7f9154d13228f4a93
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Thu Feb 1 12:07:46 2018 +1100

    powerpc/tm: Remove struct thread_info param from tm_reclaim_thread()
    
    Since commit dc3106690b20 ("powerpc: tm: Always use fp_state and
    vr_state to store live registers") tm_reclaim_thread() doesn't use the
    parameter anymore, both callers have to bother getting it as they have
    no need for a struct thread_info either.
    
    Just remove it and adjust the callers.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index eca92aac2e17..e9533b4d2f08 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -867,8 +867,7 @@ static inline bool tm_enabled(struct task_struct *tsk)
 	return tsk && tsk->thread.regs && (tsk->thread.regs->msr & MSR_TM);
 }
 
-static void tm_reclaim_thread(struct thread_struct *thr,
-			      struct thread_info *ti, uint8_t cause)
+static void tm_reclaim_thread(struct thread_struct *thr, uint8_t cause)
 {
 	/*
 	 * Use the current MSR TM suspended bit to track if we have
@@ -915,7 +914,7 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 void tm_reclaim_current(uint8_t cause)
 {
 	tm_enable();
-	tm_reclaim_thread(&current->thread, current_thread_info(), cause);
+	tm_reclaim_thread(&current->thread, cause);
 }
 
 static inline void tm_reclaim_task(struct task_struct *tsk)
@@ -946,7 +945,7 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 		 thr->regs->ccr, thr->regs->msr,
 		 thr->regs->trap);
 
-	tm_reclaim_thread(thr, task_thread_info(tsk), TM_CAUSE_RESCHED);
+	tm_reclaim_thread(thr, TM_CAUSE_RESCHED);
 
 	TM_DEBUG("--- tm_reclaim on pid %d complete\n",
 		 tsk->pid);

commit c76662e825f507b98938dc3bb141c4505bd4968c
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Tue Jul 17 06:51:05 2018 -0700

    powerpc/pkeys: Save the pkey registers before fork
    
    When a thread forks the contents of AMR, IAMR, UAMOR registers in the
    newly forked thread are not inherited.
    
    Save the registers before forking, for content of those
    registers to be automatically copied into the new thread.
    
    Fixes: cf43d3b26452 ("powerpc: Enable pkey subsystem")
    Cc: stable@vger.kernel.org # v4.16+
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 27f0caee55ea..eca92aac2e17 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -583,6 +583,7 @@ static void save_all(struct task_struct *tsk)
 		__giveup_spe(tsk);
 
 	msr_check_and_clear(msr_all_available);
+	thread_pkey_regs_save(&tsk->thread);
 }
 
 void flush_all_to_thread(struct task_struct *tsk)

commit 2bf1071a8d50928a4ae366bb3108833166c2b70c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Jul 5 18:47:00 2018 +1000

    powerpc/64s: Remove POWER9 DD1 support
    
    POWER9 DD1 was never a product. It is no longer supported by upstream
    firmware, and it is not effectively supported in Linux due to lack of
    testing.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Michael Ellerman <mpe@ellerman.id.au>
    [mpe: Remove arch_make_huge_pte() entirely]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9ef4aea9fffe..27f0caee55ea 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1250,17 +1250,9 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		 * mappings. If the new process has the foreign real address
 		 * mappings, we must issue a cp_abort to clear any state and
 		 * prevent snooping, corruption or a covert channel.
-		 *
-		 * DD1 allows paste into normal system memory so we do an
-		 * unpaired copy, rather than cp_abort, to clear the buffer,
-		 * since cp_abort is quite expensive.
 		 */
-		if (current_thread_info()->task->thread.used_vas) {
+		if (current_thread_info()->task->thread.used_vas)
 			asm volatile(PPC_CP_ABORT);
-		} else if (cpu_has_feature(CPU_FTR_POWER9_DD1)) {
-			asm volatile(PPC_COPY(%0, %1)
-					: : "r"(dummy_copy_buffer), "r"(0));
-		}
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
 

commit c90fca951e90ba470a3dc6087667edffcf8db21b
Merge: c0ab85267e25 ff5bc793e47b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 7 10:23:33 2018 -0700

    Merge tag 'powerpc-4.18-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Support for split PMD page table lock on 64-bit Book3S (Power8/9).
    
       - Add support for HAVE_RELIABLE_STACKTRACE, so we properly support
         live patching again.
    
       - Add support for patching barrier_nospec in copy_from_user() and
         syscall entry.
    
       - A couple of fixes for our data breakpoints on Book3S.
    
       - A series from Nick optimising TLB/mm handling with the Radix MMU.
    
       - Numerous small cleanups to squash sparse/gcc warnings from Mathieu
         Malaterre.
    
       - Several series optimising various parts of the 32-bit code from
         Christophe Leroy.
    
       - Removal of support for two old machines, "SBC834xE" and "C2K"
         ("GEFanuc,C2K"), which is why the diffstat has so many deletions.
    
      And many other small improvements & fixes.
    
      There's a few out-of-area changes. Some minor ftrace changes OK'ed by
      Steve, and a fix to our powernv cpuidle driver. Then there's a series
      touching mm, x86 and fs/proc/task_mmu.c, which cleans up some details
      around pkey support. It was ack'ed/reviewed by Ingo & Dave and has
      been in next for several weeks.
    
      Thanks to: Akshay Adiga, Alastair D'Silva, Alexey Kardashevskiy, Al
      Viro, Andrew Donnellan, Aneesh Kumar K.V, Anju T Sudhakar, Arnd
      Bergmann, Balbir Singh, Cdric Le Goater, Christophe Leroy, Christophe
      Lombard, Colin Ian King, Dave Hansen, Fabio Estevam, Finn Thain,
      Frederic Barrat, Gautham R. Shenoy, Haren Myneni, Hari Bathini, Ingo
      Molnar, Jonathan Neuschfer, Josh Poimboeuf, Kamalesh Babulal,
      Madhavan Srinivasan, Mahesh Salgaonkar, Mark Greer, Mathieu Malaterre,
      Matthew Wilcox, Michael Neuling, Michal Suchanek, Naveen N. Rao,
      Nicholas Piggin, Nicolai Stange, Olof Johansson, Paul Gortmaker, Paul
      Mackerras, Peter Rosin, Pridhiviraj Paidipeddi, Ram Pai, Rashmica
      Gupta, Ravi Bangoria, Russell Currey, Sam Bobroff, Samuel
      Mendoza-Jonas, Segher Boessenkool, Shilpasri G Bhat, Simon Guo,
      Souptick Joarder, Stewart Smith, Thiago Jung Bauermann, Torsten Duwe,
      Vaibhav Jain, Wei Yongjun, Wolfram Sang, Yisheng Xie, YueHaibing"
    
    * tag 'powerpc-4.18-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (251 commits)
      powerpc/64s/radix: Fix missing ptesync in flush_cache_vmap
      cpuidle: powernv: Fix promotion from snooze if next state disabled
      powerpc: fix build failure by disabling attribute-alias warning in pci_32
      ocxl: Fix missing unlock on error in afu_ioctl_enable_p9_wait()
      powerpc-opal: fix spelling mistake "Uniterrupted" -> "Uninterrupted"
      powerpc: fix spelling mistake: "Usupported" -> "Unsupported"
      powerpc/pkeys: Detach execute_only key on !PROT_EXEC
      powerpc/powernv: copy/paste - Mask SO bit in CR
      powerpc: Remove core support for Marvell mv64x60 hostbridges
      powerpc/boot: Remove core support for Marvell mv64x60 hostbridges
      powerpc/boot: Remove support for Marvell mv64x60 i2c controller
      powerpc/boot: Remove support for Marvell MPSC serial controller
      powerpc/embedded6xx: Remove C2K board support
      powerpc/lib: optimise PPC32 memcmp
      powerpc/lib: optimise 32 bits __clear_user()
      powerpc/time: inline arch_vtime_task_switch()
      powerpc/Makefile: set -mcpu=860 flag for the 8xx
      powerpc: Implement csum_ipv6_magic in assembly
      powerpc/32: Optimise __csum_partial()
      powerpc/lib: Adjust .balign inside string functions for PPC32
      ...

commit 71cc64a85d8d99936f6851709a07f18c87a0adab
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Fri May 11 16:12:59 2018 +1000

    powerpc: use task_pid_nr() for TID allocation
    
    The current implementation of TID allocation, using a global IDR, may
    result in an errant process starving the system of available TIDs.
    Instead, use task_pid_nr(), as mentioned by the original author. The
    scenario described which prevented it's use is not applicable, as
    set_thread_tidr can only be called after the task struct has been
    populated.
    
    In the unlikely event that 2 threads share the TID and are waiting,
    all potential outcomes have been determined safe.
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e8b1d3c30669..ebcd3956f2be 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1479,103 +1479,41 @@ int set_thread_uses_vas(void)
 }
 
 #ifdef CONFIG_PPC64
-static DEFINE_SPINLOCK(vas_thread_id_lock);
-static DEFINE_IDA(vas_thread_ida);
-
-/*
- * We need to assign a unique thread id to each thread in a process.
+/**
+ * Assign a TIDR (thread ID) for task @t and set it in the thread
+ * structure. For now, we only support setting TIDR for 'current' task.
  *
- * This thread id, referred to as TIDR, and separate from the Linux's tgid,
- * is intended to be used to direct an ASB_Notify from the hardware to the
- * thread, when a suitable event occurs in the system.
+ * Since the TID value is a truncated form of it PID, it is possible
+ * (but unlikely) for 2 threads to have the same TID. In the unlikely event
+ * that 2 threads share the same TID and are waiting, one of the following
+ * cases will happen:
  *
- * One such event is a "paste" instruction in the context of Fast Thread
- * Wakeup (aka Core-to-core wake up in the Virtual Accelerator Switchboard
- * (VAS) in POWER9.
+ * 1. The correct thread is running, the wrong thread is not
+ * In this situation, the correct thread is woken and proceeds to pass it's
+ * condition check.
  *
- * To get a unique TIDR per process we could simply reuse task_pid_nr() but
- * the problem is that task_pid_nr() is not yet available copy_thread() is
- * called. Fixing that would require changing more intrusive arch-neutral
- * code in code path in copy_process()?.
+ * 2. Neither threads are running
+ * In this situation, neither thread will be woken. When scheduled, the waiting
+ * threads will execute either a wait, which will return immediately, followed
+ * by a condition check, which will pass for the correct thread and fail
+ * for the wrong thread, or they will execute the condition check immediately.
  *
- * Further, to assign unique TIDRs within each process, we need an atomic
- * field (or an IDR) in task_struct, which again intrudes into the arch-
- * neutral code. So try to assign globally unique TIDRs for now.
+ * 3. The wrong thread is running, the correct thread is not
+ * The wrong thread will be woken, but will fail it's condition check and
+ * re-execute wait. The correct thread, when scheduled, will execute either
+ * it's condition check (which will pass), or wait, which returns immediately
+ * when called the first time after the thread is scheduled, followed by it's
+ * condition check (which will pass).
  *
- * NOTE: TIDR 0 indicates that the thread does not need a TIDR value.
- *	 For now, only threads that expect to be notified by the VAS
- *	 hardware need a TIDR value and we assign values > 0 for those.
- */
-#define MAX_THREAD_CONTEXT	((1 << 16) - 1)
-static int assign_thread_tidr(void)
-{
-	int index;
-	int err;
-	unsigned long flags;
-
-again:
-	if (!ida_pre_get(&vas_thread_ida, GFP_KERNEL))
-		return -ENOMEM;
-
-	spin_lock_irqsave(&vas_thread_id_lock, flags);
-	err = ida_get_new_above(&vas_thread_ida, 1, &index);
-	spin_unlock_irqrestore(&vas_thread_id_lock, flags);
-
-	if (err == -EAGAIN)
-		goto again;
-	else if (err)
-		return err;
-
-	if (index > MAX_THREAD_CONTEXT) {
-		spin_lock_irqsave(&vas_thread_id_lock, flags);
-		ida_remove(&vas_thread_ida, index);
-		spin_unlock_irqrestore(&vas_thread_id_lock, flags);
-		return -ENOMEM;
-	}
-
-	return index;
-}
-
-static void free_thread_tidr(int id)
-{
-	unsigned long flags;
-
-	spin_lock_irqsave(&vas_thread_id_lock, flags);
-	ida_remove(&vas_thread_ida, id);
-	spin_unlock_irqrestore(&vas_thread_id_lock, flags);
-}
-
-/*
- * Clear any TIDR value assigned to this thread.
- */
-void clear_thread_tidr(struct task_struct *t)
-{
-	if (!t->thread.tidr)
-		return;
-
-	if (!cpu_has_feature(CPU_FTR_P9_TIDR)) {
-		WARN_ON_ONCE(1);
-		return;
-	}
-
-	mtspr(SPRN_TIDR, 0);
-	free_thread_tidr(t->thread.tidr);
-	t->thread.tidr = 0;
-}
-
-void arch_release_task_struct(struct task_struct *t)
-{
-	clear_thread_tidr(t);
-}
-
-/*
- * Assign a unique TIDR (thread id) for task @t and set it in the thread
- * structure. For now, we only support setting TIDR for 'current' task.
+ * 4. Both threads are running
+ * Both threads will be woken. The wrong thread will fail it's condition check
+ * and execute another wait, while the correct thread will pass it's condition
+ * check.
+ *
+ * @t: the task to set the thread ID for
  */
 int set_thread_tidr(struct task_struct *t)
 {
-	int rc;
-
 	if (!cpu_has_feature(CPU_FTR_P9_TIDR))
 		return -EINVAL;
 
@@ -1585,11 +1523,7 @@ int set_thread_tidr(struct task_struct *t)
 	if (t->thread.tidr)
 		return 0;
 
-	rc = assign_thread_tidr();
-	if (rc < 0)
-		return rc;
-
-	t->thread.tidr = rc;
+	t->thread.tidr = (u16)task_pid_nr(t);
 	mtspr(SPRN_TIDR, t->thread.tidr);
 
 	return 0;

commit 3449f191ca9be1a6ac9757b8ab55f239092362e5
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Fri May 11 16:12:58 2018 +1000

    powerpc: Use TIDR CPU feature to control TIDR allocation
    
    Switch the use of TIDR on it's CPU feature, rather than assuming it
    is available based on architecture.
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8f35b30956f4..e8b1d3c30669 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1151,7 +1151,7 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 			mtspr(SPRN_TAR, new_thread->tar);
 	}
 
-	if (cpu_has_feature(CPU_FTR_ARCH_300) &&
+	if (cpu_has_feature(CPU_FTR_P9_TIDR) &&
 	    old_thread->tidr != new_thread->tidr)
 		mtspr(SPRN_TIDR, new_thread->tidr);
 #endif
@@ -1553,7 +1553,7 @@ void clear_thread_tidr(struct task_struct *t)
 	if (!t->thread.tidr)
 		return;
 
-	if (!cpu_has_feature(CPU_FTR_ARCH_300)) {
+	if (!cpu_has_feature(CPU_FTR_P9_TIDR)) {
 		WARN_ON_ONCE(1);
 		return;
 	}
@@ -1576,7 +1576,7 @@ int set_thread_tidr(struct task_struct *t)
 {
 	int rc;
 
-	if (!cpu_has_feature(CPU_FTR_ARCH_300))
+	if (!cpu_has_feature(CPU_FTR_P9_TIDR))
 		return -EINVAL;
 
 	if (t != current)

commit 3130a7bb6eb595f2d963976a4d3e57db77bcf06f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu May 10 11:04:24 2018 +1000

    powerpc/64: change softe to irqmask in show_regs and xmon
    
    When the soft enabled flag was changed to a soft disable mask, xmon
    and register dump code was not updated to reflect that, which is
    confusing ('SOFTE: 1' previously meant interrupts were soft enabled,
    currently it means the opposite, the general interrupt type has been
    disabled).
    
    Fix this by using the name irqmask, and printing it in hex.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2a7fa5000cce..8f35b30956f4 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1420,7 +1420,7 @@ void show_regs(struct pt_regs * regs)
 		pr_cont("DAR: "REG" DSISR: %08lx ", regs->dar, regs->dsisr);
 #endif
 #ifdef CONFIG_PPC64
-	pr_cont("SOFTE: %ld ", regs->softe);
+	pr_cont("IRQMASK: %lx ", regs->softe);
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	if (MSR_TM_ACTIVE(regs->msr))

commit 3d3a6021ddcbe9c31520e4e7b65e5ce5dc58274d
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:30 2018 +1000

    powerpc/pseries: lparcfg calculate PURR on demand
    
    For SPLPAR, lparcfg provides a sum of PURR registers for all CPUs.
    Currently this is done by reading PURR in context switch and timer
    interrupt, and storing that into a per-CPU variable. These are summed
    to provide the value.
    
    This does not work with all timer schemes (e.g., NO_HZ_FULL), and it
    is sub-optimal for performance because it reads the PURR register on
    every context switch, although that's been difficult to distinguish
    from noise in the contxt_switch microbenchmark.
    
    This patch implements the sum by calling a function on each CPU, to
    read and add PURR values of each CPU.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f4e5291584c5..2a7fa5000cce 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -846,10 +846,6 @@ bool ppc_breakpoint_available(void)
 }
 EXPORT_SYMBOL_GPL(ppc_breakpoint_available);
 
-#ifdef CONFIG_PPC64
-DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);
-#endif
-
 static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
 			      struct arch_hw_breakpoint *b)
 {
@@ -1182,16 +1178,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	WARN_ON(!irqs_disabled());
 
-#ifdef CONFIG_PPC64
-	/*
-	 * Collect processor utilization data per process
-	 */
-	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
-		struct cpu_usage *cu = this_cpu_ptr(&cpu_usage_array);
-		cu->current_tb = mfspr(SPRN_PURR);
-	}
-#endif /* CONFIG_PPC64 */
-
 #ifdef CONFIG_PPC_BOOK3S_64
 	batch = this_cpu_ptr(&ppc64_tlb_batch);
 	if (batch->active) {

commit 36d632ea831fd2fa3cb62599a465825f59076f64
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat May 5 03:19:29 2018 +1000

    powerpc/64: remove start_tb and accum_tb from thread_struct
    
    These fields are only written to.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 25db000fa5b3..f4e5291584c5 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1188,11 +1188,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 */
 	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
 		struct cpu_usage *cu = this_cpu_ptr(&cpu_usage_array);
-		long unsigned start_tb, current_tb;
-		start_tb = old_thread->start_tb;
-		cu->current_tb = current_tb = mfspr(SPRN_PURR);
-		old_thread->accum_tb += (current_tb - start_tb);
-		new_thread->start_tb = current_tb;
+		cu->current_tb = mfspr(SPRN_PURR);
 	}
 #endif /* CONFIG_PPC64 */
 

commit d1c7211281c5e1799f00b2228157530e0f7a671c
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Wed May 23 15:01:44 2018 +0800

    powerpc: Export msr_check_and_set() to modules
    
    PR KVM will need to reuse msr_check_and_set().
    This patch exports this API for reuse.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1237f13fed51..25db000fa5b3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -154,6 +154,7 @@ unsigned long msr_check_and_set(unsigned long bits)
 
 	return newmsr;
 }
+EXPORT_SYMBOL_GPL(msr_check_and_set);
 
 void __msr_check_and_clear(unsigned long bits)
 {

commit 3eb0f5193b497083391aa05d35210d5645211eef
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Apr 17 15:26:37 2018 -0500

    signal: Ensure every siginfo we send has all bits initialized
    
    Call clear_siginfo to ensure every stack allocated siginfo is properly
    initialized before being passed to the signal sending functions.
    
    Note: It is not safe to depend on C initializers to initialize struct
    siginfo on the stack because C is allowed to skip holes when
    initializing a structure.
    
    The initialization of struct siginfo in tracehook_report_syscall_exit
    was moved from the helper user_single_step_siginfo into
    tracehook_report_syscall_exit itself, to make it clear that the local
    variable siginfo gets fully initialized.
    
    In a few cases the scope of struct siginfo has been reduced to make it
    clear that siginfo siginfo is not used on other paths in the function
    in which it is declared.
    
    Instances of using memset to initialize siginfo have been replaced
    with calls clear_siginfo for clarity.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1237f13fed51..26ea9793d290 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -632,6 +632,7 @@ void do_break (struct pt_regs *regs, unsigned long address,
 	hw_breakpoint_disable();
 
 	/* Deliver the signal to userspace */
+	clear_siginfo(&info);
 	info.si_signo = SIGTRAP;
 	info.si_errno = 0;
 	info.si_code = TRAP_HWBKPT;

commit 252988cbf037f3d446eea222afb46cc134d32c71
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sun Apr 1 15:50:36 2018 +1000

    powerpc: Don't write to DABR on >= Power8 if DAWR is disabled
    
    flush_thread() calls __set_breakpoint() via set_debug_reg_defaults()
    without checking ppc_breakpoint_available(). On Power8 or later CPUs
    which have the DAWR feature disabled that will cause a write to the
    DABR which is incorrect as those CPUs don't have a DABR.
    
    Fix it two ways, by checking ppc_breakpoint_available() in
    set_debug_reg_defaults(), and also by reworking __set_breakpoint() to
    only write to DABR on Power7 or earlier.
    
    Fixes: 9654153158d3 ("powerpc: Disable DAWR in the base POWER9 CPU features")
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Rework the logic in __set_breakpoint()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 24a591b4dbe9..1237f13fed51 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -718,7 +718,8 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 {
 	thread->hw_brk.address = 0;
 	thread->hw_brk.type = 0;
-	set_breakpoint(&thread->hw_brk);
+	if (ppc_breakpoint_available())
+		set_breakpoint(&thread->hw_brk);
 }
 #endif /* !CONFIG_HAVE_HW_BREAKPOINT */
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
@@ -815,9 +816,14 @@ void __set_breakpoint(struct arch_hw_breakpoint *brk)
 	memcpy(this_cpu_ptr(&current_brk), brk, sizeof(*brk));
 
 	if (cpu_has_feature(CPU_FTR_DAWR))
+		// Power8 or later
 		set_dawr(brk);
-	else
+	else if (!cpu_has_feature(CPU_FTR_ARCH_207S))
+		// Power7 or earlier
 		set_dabr(brk);
+	else
+		// Shouldn't happen due to higher level checks
+		WARN_ON_ONCE(1);
 }
 
 void set_breakpoint(struct arch_hw_breakpoint *brk)

commit c0b346729b5dd3c7d0232f043f5b15947ffc7978
Merge: 34a286a4ac57 9654153158d3
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Mar 27 23:55:49 2018 +1100

    Merge branch 'topic/ppc-kvm' into next
    
    Merge the DAWR series, which touches arch code and KVM code and may need
    to be merged into the kvm-ppc tree.

commit 404b27d66ed657ebccb08a9c8f8f65523e9b666b
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Mar 27 15:37:17 2018 +1100

    powerpc: Add ppc_breakpoint_available()
    
    Add ppc_breakpoint_available() to determine if a breakpoint is
    available currently via the DAWR or DABR.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1738c4127b32..4466e3db46d4 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -827,6 +827,18 @@ void set_breakpoint(struct arch_hw_breakpoint *brk)
 	preempt_enable();
 }
 
+/* Check if we have DAWR or DABR hardware */
+bool ppc_breakpoint_available(void)
+{
+	if (cpu_has_feature(CPU_FTR_DAWR))
+		return true; /* POWER8 DAWR */
+	if (cpu_has_feature(CPU_FTR_ARCH_207S))
+		return false; /* POWER9 with DAWR disabled */
+	/* DABR: Everything but POWER8 and POWER9 */
+	return true;
+}
+EXPORT_SYMBOL_GPL(ppc_breakpoint_available);
+
 #ifdef CONFIG_PPC64
 DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);
 #endif

commit 1cdf039bf82a39d816ca4b8161b01c0acfca3e62
Author: Mathieu Malaterre <malat@debian.org>
Date:   Sun Feb 25 18:22:23 2018 +0100

    powerpc/kernel: Make function __giveup_fpu() static
    
    __giveup_fpu() is never called outside process.c, so it can be static.
    That also means we don't need an empty definition in switch_to.h
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    [mpe: Also drop the empty version, rewrite change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1738c4127b32..ec4f363ebb89 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -173,7 +173,7 @@ void __msr_check_and_clear(unsigned long bits)
 EXPORT_SYMBOL(__msr_check_and_clear);
 
 #ifdef CONFIG_PPC_FPU
-void __giveup_fpu(struct task_struct *tsk)
+static void __giveup_fpu(struct task_struct *tsk)
 {
 	unsigned long msr;
 
@@ -556,7 +556,7 @@ void restore_math(struct pt_regs *regs)
 	regs->msr = msr;
 }
 
-void save_all(struct task_struct *tsk)
+static void save_all(struct task_struct *tsk)
 {
 	unsigned long usermsr;
 

commit 03f51d4efa2287cc628bb20b0c032036d2a9e66a
Merge: 367b0df173b0 015eb1b89e95
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Feb 2 10:01:04 2018 -0800

    Merge tag 'powerpc-4.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
    
       - Enable support for memory protection keys aka "pkeys" on Power7/8/9
         when using the hash table MMU.
    
       - Extend our interrupt soft masking to support masking PMU interrupts
         as well as "normal" interrupts, and then use that to implement
         local_t for a ~4x speedup vs the current atomics-based
         implementation.
    
       - A new driver "ocxl" for "Open Coherent Accelerator Processor
         Interface (OpenCAPI)" devices.
    
       - Support for new device tree properties on PowerVM to describe
         hotpluggable memory and devices.
    
       - Add support for CLOCK_{REALTIME/MONOTONIC}_COARSE to the 64-bit
         VDSO.
    
       - Freescale updates from Scott: fixes for CPM GPIO and an FSL PCI
         erratum workaround, plus a minor cleanup patch.
    
      As well as quite a lot of other changes all over the place, and small
      fixes and cleanups as always.
    
      Thanks to: Alan Modra, Alastair D'Silva, Alexey Kardashevskiy,
      Alistair Popple, Andreas Schwab, Andrew Donnellan, Aneesh Kumar K.V,
      Anju T Sudhakar, Anshuman Khandual, Anton Blanchard, Arnd Bergmann,
      Balbir Singh, Benjamin Herrenschmidt, Bhaktipriya Shridhar, Bryant G.
      Ly, Cdric Le Goater, Christophe Leroy, Christophe Lombard, Cyril Bur,
      David Gibson, Desnes A. Nunes do Rosario, Dmitry Torokhov, Frederic
      Barrat, Geert Uytterhoeven, Guilherme G. Piccoli, Gustavo A. R. Silva,
      Gustavo Romero, Ivan Mikhaylov, Joakim Tjernlund, Joe Perches, Josh
      Poimboeuf, Juan J. Alvarez, Julia Cartwright, Kamalesh Babulal,
      Madhavan Srinivasan, Mahesh Salgaonkar, Mathieu Malaterre, Michael
      Bringmann, Michael Hanselmann, Michael Neuling, Nathan Fontenot,
      Naveen N. Rao, Nicholas Piggin, Paul Mackerras, Philippe Bergheaud,
      Ram Pai, Russell Currey, Santosh Sivaraj, Scott Wood, Seth Forshee,
      Simon Guo, Stewart Smith, Sukadev Bhattiprolu, Thiago Jung Bauermann,
      Vaibhav Jain, Vasyl Gomonovych"
    
    * tag 'powerpc-4.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (199 commits)
      powerpc/mm/radix: Fix build error when RADIX_MMU=n
      macintosh/ams-input: Use true and false for boolean values
      macintosh: change some data types from int to bool
      powerpc/watchdog: Print the NIP in soft_nmi_interrupt()
      powerpc/watchdog: regs can't be null in soft_nmi_interrupt()
      powerpc/watchdog: Tweak watchdog printks
      powerpc/cell: Remove axonram driver
      rtc-opal: Fix handling of firmware error codes, prevent busy loops
      powerpc/mpc52xx_gpt: make use of raw_spinlock variants
      macintosh/adb: Properly mark continued kernel messages
      powerpc/pseries: Fix cpu hotplug crash with memoryless nodes
      powerpc/numa: Ensure nodes initialized for hotplug
      powerpc/numa: Use ibm,max-associativity-domains to discover possible nodes
      powerpc/kernel: Block interrupts when updating TIDR
      powerpc/powernv/idoa: Remove unnecessary pcidev from pci_dn
      powerpc/mm/nohash: do not flush the entire mm when range is a single page
      powerpc/pseries: Add Initialization of VF Bars
      powerpc/pseries/pci: Associate PEs to VFs in configure SR-IOV
      powerpc/eeh: Add EEH notify resume sysfs
      powerpc/eeh: Add EEH operations to notify resume
      ...

commit 384dfd627f1ee67d028e6f14c6e9bf5a1e2a7a24
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Nov 28 13:39:43 2017 -0600

    powerpc/kernel: Block interrupts when updating TIDR
    
    clear_thread_tidr() is called in interrupt context as a part of delayed
    put of the task structure (i.e as a part of timer interrupt). To prevent
    a deadlock, block interrupts when holding vas_thread_id_lock to set/
    clear TIDR for a task.
    
    Fixes: ec233ede4c86 ("powerpc: Add support for setting SPRN_TIDR")
    Cc: stable@vger.kernel.org # v4.15+
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2fd01c9045ba..69af583dce98 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1515,14 +1515,15 @@ static int assign_thread_tidr(void)
 {
 	int index;
 	int err;
+	unsigned long flags;
 
 again:
 	if (!ida_pre_get(&vas_thread_ida, GFP_KERNEL))
 		return -ENOMEM;
 
-	spin_lock(&vas_thread_id_lock);
+	spin_lock_irqsave(&vas_thread_id_lock, flags);
 	err = ida_get_new_above(&vas_thread_ida, 1, &index);
-	spin_unlock(&vas_thread_id_lock);
+	spin_unlock_irqrestore(&vas_thread_id_lock, flags);
 
 	if (err == -EAGAIN)
 		goto again;
@@ -1530,9 +1531,9 @@ static int assign_thread_tidr(void)
 		return err;
 
 	if (index > MAX_THREAD_CONTEXT) {
-		spin_lock(&vas_thread_id_lock);
+		spin_lock_irqsave(&vas_thread_id_lock, flags);
 		ida_remove(&vas_thread_ida, index);
-		spin_unlock(&vas_thread_id_lock);
+		spin_unlock_irqrestore(&vas_thread_id_lock, flags);
 		return -ENOMEM;
 	}
 
@@ -1541,9 +1542,11 @@ static int assign_thread_tidr(void)
 
 static void free_thread_tidr(int id)
 {
-	spin_lock(&vas_thread_id_lock);
+	unsigned long flags;
+
+	spin_lock_irqsave(&vas_thread_id_lock, flags);
 	ida_remove(&vas_thread_ida, id);
-	spin_unlock(&vas_thread_id_lock);
+	spin_unlock_irqrestore(&vas_thread_id_lock, flags);
 }
 
 /*

commit f71dd7dc2dc989dc712b246a74d243e4b2c5f8a7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jan 22 14:37:25 2018 -0600

    signal/ptrace: Add force_sig_ptrace_errno_trap and use it where needed
    
    There are so many places that build struct siginfo by hand that at
    least one of them is bound to get it wrong.  A handful of cases in the
    kernel arguably did just that when using the errno field of siginfo to
    pass no errno values to userspace.  The usage is limited to a single
    si_code so at least does not mess up anything else.
    
    Encapsulate this questionable pattern in a helper function so
    that the userspace ABI is preserved.
    
    Update all of the places that use this pattern to use the new helper
    function.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index bfb48cf56bc3..4208cbe2fb7f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -603,19 +603,14 @@ EXPORT_SYMBOL(flush_all_to_thread);
 void do_send_trap(struct pt_regs *regs, unsigned long address,
 		  unsigned long error_code, int breakpt)
 {
-	siginfo_t info;
-
 	current->thread.trap_nr = TRAP_HWBKPT;
 	if (notify_die(DIE_DABR_MATCH, "dabr_match", regs, error_code,
 			11, SIGSEGV) == NOTIFY_STOP)
 		return;
 
 	/* Deliver the signal to userspace */
-	info.si_signo = SIGTRAP;
-	info.si_errno = breakpt;	/* breakpoint or watchpoint id */
-	info.si_code = TRAP_HWBKPT;
-	info.si_addr = (void __user *)address;
-	force_sig_info(SIGTRAP, &info, current);
+	force_sig_ptrace_errno_trap(breakpt, /* breakpoint or watchpoint id */
+				    (void __user *)address);
 }
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
 void do_break (struct pt_regs *regs, unsigned long address,

commit 47355040d2760566901057287b35d5f10e217e12
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Jan 16 16:12:38 2018 -0600

    signal/powerpc: Remove unnecessary signal_code parameter of do_send_trap
    
    signal_code is always TRAP_HWBKPT
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 72be0c32e902..bfb48cf56bc3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -601,11 +601,11 @@ EXPORT_SYMBOL(flush_all_to_thread);
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 void do_send_trap(struct pt_regs *regs, unsigned long address,
-		  unsigned long error_code, int signal_code, int breakpt)
+		  unsigned long error_code, int breakpt)
 {
 	siginfo_t info;
 
-	current->thread.trap_nr = signal_code;
+	current->thread.trap_nr = TRAP_HWBKPT;
 	if (notify_die(DIE_DABR_MATCH, "dabr_match", regs, error_code,
 			11, SIGSEGV) == NOTIFY_STOP)
 		return;
@@ -613,7 +613,7 @@ void do_send_trap(struct pt_regs *regs, unsigned long address,
 	/* Deliver the signal to userspace */
 	info.si_signo = SIGTRAP;
 	info.si_errno = breakpt;	/* breakpoint or watchpoint id */
-	info.si_code = signal_code;
+	info.si_code = TRAP_HWBKPT;
 	info.si_addr = (void __user *)address;
 	force_sig_info(SIGTRAP, &info, current);
 }

commit ebf0b6a8b1e445d2be66087732aafcda12ab9f59
Merge: 5400fc229e60 1b689a95ce74
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jan 21 23:21:14 2018 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch from the 4.15 cycle.
    
    Unusually the fixes branch saw some significant features merged,
    notably the RFI flush patches, so we want the code in next to be
    tested against that, to avoid any surprises when the two are merged.
    
    There's also some other work on the panic handling that was reverted
    in fixes and we now want to do properly in next, which would conflict.
    
    And we also fix a few other minor merge conflicts.

commit 06bb53b33804613627c7ca1eda246459a7be2803
Author: Ram Pai <linuxram@us.ibm.com>
Date:   Thu Jan 18 17:50:31 2018 -0800

    powerpc: store and restore the pkey state across context switches
    
    Store and restore the AMR, IAMR and UAMOR register state of the task
    before scheduling out and after scheduling in, respectively.
    
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9eb78ec0ce5b..755fd7e23ed4 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -42,6 +42,7 @@
 #include <linux/hw_breakpoint.h>
 #include <linux/uaccess.h>
 #include <linux/elf-randomize.h>
+#include <linux/pkeys.h>
 
 #include <asm/pgtable.h>
 #include <asm/io.h>
@@ -1103,6 +1104,8 @@ static inline void save_sprs(struct thread_struct *t)
 		t->tar = mfspr(SPRN_TAR);
 	}
 #endif
+
+	thread_pkey_regs_save(t);
 }
 
 static inline void restore_sprs(struct thread_struct *old_thread,
@@ -1142,6 +1145,8 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 	    old_thread->tidr != new_thread->tidr)
 		mtspr(SPRN_TIDR, new_thread->tidr);
 #endif
+
+	thread_pkey_regs_restore(new_thread, old_thread);
 }
 
 #ifdef CONFIG_PPC_BOOK3S_64
@@ -1867,6 +1872,8 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	current->thread.tm_tfiar = 0;
 	current->thread.load_tm = 0;
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
+
+	thread_pkey_regs_init(&current->thread);
 }
 EXPORT_SYMBOL(start_thread);
 

commit b1db551324f72fa14ad82ca31237a7ed418104df
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Thu Jan 11 09:55:25 2018 +0100

    cxl: Add support for ASB_Notify on POWER9
    
    The POWER9 core supports a new feature: ASB_Notify which requires the
    support of the Special Purpose Register: TIDR.
    
    The ASB_Notify command, generated by the AFU, will attempt to
    wake-up the host thread identified by the particular LPID:PID:TID.
    
    This patch assign a unique TIDR (thread id) for the current thread which
    will be used in the process element entry.
    
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Acked-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1563d1190da3..9eb78ec0ce5b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1590,6 +1590,7 @@ int set_thread_tidr(struct task_struct *t)
 
 	return 0;
 }
+EXPORT_SYMBOL_GPL(set_thread_tidr);
 
 #endif /* CONFIG_PPC64 */
 

commit c2e480ba822718190e58849b79a76db13c3dac18
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:42 2017 +0530

    powerpc/64: Add #defines for paca->soft_enabled flags
    
    Two #defines IRQS_ENABLED and IRQS_DISABLED are added to be used when
    updating paca->soft_enabled. Replace the hardcoded values used when
    updating paca->soft_enabled with IRQ_(EN|DIS)ABLED #define. No logic
    change.
    
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index bcd4441304a5..1563d1190da3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -57,6 +57,7 @@
 #include <asm/debug.h>
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
+#include <asm/hw_irq.h>
 #endif
 #include <asm/code-patching.h>
 #include <asm/exec.h>
@@ -1674,7 +1675,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 			childregs->gpr[14] = ppc_function_entry((void *)usp);
 #ifdef CONFIG_PPC64
 		clear_tsk_thread_flag(p, TIF_32BIT);
-		childregs->softe = 1;
+		childregs->softe = IRQS_ENABLED;
 #endif
 		childregs->gpr[15] = kthread_arg;
 		p->thread.regs = NULL;	/* no user register state */

commit 2271db20e4b362405bacc0e4095df4177d38129e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jan 12 13:28:49 2018 +1100

    powerpc: Use the TRAP macro whenever comparing a trap number
    
    Trap numbers can have extra bits at the bottom that need to
    be filtered out. There are a few cases where we don't do that.
    
    It's possible that we got lucky but better safe than sorry.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 5acb5a176dbe..bcd4441304a5 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1409,7 +1409,7 @@ void show_regs(struct pt_regs * regs)
 	print_msr_bits(regs->msr);
 	pr_cont("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
-	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
+	if ((TRAP(regs) != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
 		pr_cont("CFAR: "REG" ", regs->orig_gpr3);
 	if (trap == 0x200 || trap == 0x300 || trap == 0x600)
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)

commit 182dc9c7f217146d69d9c0b75c150c0314b9b170
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Dec 18 16:33:36 2017 +1100

    powerpc/kernel: Print actual address of regs when oopsing
    
    When we oops or otherwise call show_regs() we print the address of the
    regs structure. Being able to see the address is fairly useful,
    firstly to verify that the regs pointer is not completely bogus, and
    secondly it allows you to dump the regs and surrounding memory with a
    debugger if you have one.
    
    In the normal case the regs will be located somewhere on the stack, so
    printing their location discloses no further information than printing
    the stack pointer does already.
    
    So switch to %px and print the actual address, not the hashed value.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 5acb5a176dbe..72be0c32e902 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1403,7 +1403,7 @@ void show_regs(struct pt_regs * regs)
 
 	printk("NIP:  "REG" LR: "REG" CTR: "REG"\n",
 	       regs->nip, regs->link, regs->ctr);
-	printk("REGS: %p TRAP: %04lx   %s  (%s)\n",
+	printk("REGS: %px TRAP: %04lx   %s  (%s)\n",
 	       regs, regs->trap, print_tainted(), init_utsname()->release);
 	printk("MSR:  "REG" ", regs->msr);
 	print_msr_bits(regs->msr);

commit 7e4d4233260be0611c7fbdb2730c12731c4b8dc0
Author: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
Date:   Fri Nov 24 14:03:38 2017 +0530

    powerpc: Do not assign thread.tidr if already assigned
    
    If set_thread_tidr() is called twice for same task_struct then it will
    allocate a new tidr value to it leaving the previous value still
    dangling in the vas_thread_ida table.
    
    To fix this the patch changes set_thread_tidr() to check if a tidr
    value is already assigned to the task_struct and if yes then returns
    zero.
    
    Fixes: ec233ede4c86("powerpc: Add support for setting SPRN_TIDR")
    Signed-off-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    [mpe: Modify to return 0 in the success case, not the TID value]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index d205b52e3850..5acb5a176dbe 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1577,6 +1577,9 @@ int set_thread_tidr(struct task_struct *t)
 	if (t != current)
 		return -EINVAL;
 
+	if (t->thread.tidr)
+		return 0;
+
 	rc = assign_thread_tidr();
 	if (rc < 0)
 		return rc;

commit aca7573fde95152378361cba734996b384f3b1d3
Author: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
Date:   Tue Nov 28 08:23:04 2017 +0530

    powerpc: Avoid signed to unsigned conversion in set_thread_tidr()
    
    There is an unsafe signed to unsigned conversion in set_thread_tidr()
    that may cause an error value to be assigned to SPRN_TIDR register and
    used as thread-id.
    
    The issue happens as assign_thread_tidr() returns an int and
    thread.tidr is an unsigned-long. So a negative error code returned
    from assign_thread_tidr() will fail the error check and gets assigned
    as tidr as a large positive value.
    
    To fix this the patch assigns the return value of assign_thread_tidr()
    to a temporary int and assigns it to thread.tidr iff its '> 0'.
    
    The patch shouldn't impact the calling convention of set_thread_tidr()
    i.e all -ve return-values are error codes and a return value of '0'
    indicates success.
    
    Fixes: ec233ede4c86("powerpc: Add support for setting SPRN_TIDR")
    Signed-off-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Reviewed-by: Christophe Lombard clombard@linux.vnet.ibm.com
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index bfdd783e3916..d205b52e3850 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1569,16 +1569,19 @@ void arch_release_task_struct(struct task_struct *t)
  */
 int set_thread_tidr(struct task_struct *t)
 {
+	int rc;
+
 	if (!cpu_has_feature(CPU_FTR_ARCH_300))
 		return -EINVAL;
 
 	if (t != current)
 		return -EINVAL;
 
-	t->thread.tidr = assign_thread_tidr();
-	if (t->thread.tidr < 0)
-		return t->thread.tidr;
+	rc = assign_thread_tidr();
+	if (rc < 0)
+		return rc;
 
+	t->thread.tidr = rc;
 	mtspr(SPRN_TIDR, t->thread.tidr);
 
 	return 0;

commit 9d2a4d71332cfdf4ea90754ad9b2f05a5ee5f6c7
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Nov 7 18:23:54 2017 -0800

    powerpc: Define set_thread_uses_vas()
    
    A CP_ABORT instruction is required in processes that have mapped a VAS
    "paste address" with the intention of using COPY/PASTE instructions.
    But since CP_ABORT is expensive, we want to restrict it to only
    processes that use/intend to use COPY/PASTE.
    
    Define an interface, set_thread_uses_vas(), that VAS can use to
    indicate that the current process opened a send window. During context
    switch, issue CP_ABORT only for processes that have the flag set.
    
    Thanks for input from Nick Piggin, Michael Ellerman.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    [mpe: Fix to not use new_thread after _switch() returns]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 887ae5047288..bfdd783e3916 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1244,17 +1244,17 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		 * The copy-paste buffer can only store into foreign real
 		 * addresses, so unprivileged processes can not see the
 		 * data or use it in any way unless they have foreign real
-		 * mappings. We don't have a VAS driver that allocates those
-		 * yet, so no cpabort is required.
+		 * mappings. If the new process has the foreign real address
+		 * mappings, we must issue a cp_abort to clear any state and
+		 * prevent snooping, corruption or a covert channel.
+		 *
+		 * DD1 allows paste into normal system memory so we do an
+		 * unpaired copy, rather than cp_abort, to clear the buffer,
+		 * since cp_abort is quite expensive.
 		 */
-		if (cpu_has_feature(CPU_FTR_POWER9_DD1)) {
-			/*
-			 * DD1 allows paste into normal system memory, so we
-			 * do an unpaired copy here to clear the buffer and
-			 * prevent a covert channel being set up.
-			 *
-			 * cpabort is not used because it is quite expensive.
-			 */
+		if (current_thread_info()->task->thread.used_vas) {
+			asm volatile(PPC_CP_ABORT);
+		} else if (cpu_has_feature(CPU_FTR_POWER9_DD1)) {
 			asm volatile(PPC_COPY(%0, %1)
 					: : "r"(dummy_copy_buffer), "r"(0));
 		}
@@ -1455,6 +1455,27 @@ void flush_thread(void)
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 }
 
+int set_thread_uses_vas(void)
+{
+#ifdef CONFIG_PPC_BOOK3S_64
+	if (!cpu_has_feature(CPU_FTR_ARCH_300))
+		return -EINVAL;
+
+	current->thread.used_vas = 1;
+
+	/*
+	 * Even a process that has no foreign real address mapping can use
+	 * an unpaired COPY instruction (to no real effect). Issue CP_ABORT
+	 * to clear any pending COPY and prevent a covert channel.
+	 *
+	 * __switch_to() will issue CP_ABORT on future context switches.
+	 */
+	asm volatile(PPC_CP_ABORT);
+
+#endif /* CONFIG_PPC_BOOK3S_64 */
+	return 0;
+}
+
 #ifdef CONFIG_PPC64
 static DEFINE_SPINLOCK(vas_thread_id_lock);
 static DEFINE_IDA(vas_thread_ida);

commit ec233ede4c8654894610ea54f4dae7adc954ac62
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Tue Nov 7 18:23:53 2017 -0800

    powerpc: Add support for setting SPRN_TIDR
    
    We need the SPRN_TIDR to be set for use with fast thread-wakeup (core-
    to-core wakeup) and also with CAPI.
    
    Each thread in a process needs to have a unique id within the process.
    But for now, we assign globally unique thread ids to all threads in
    the system.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    [mpe: Simplify tidr clearing on fork() and ctx switch code]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4083b737decb..887ae5047288 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1136,6 +1136,10 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 		if (old_thread->tar != new_thread->tar)
 			mtspr(SPRN_TAR, new_thread->tar);
 	}
+
+	if (cpu_has_feature(CPU_FTR_ARCH_300) &&
+	    old_thread->tidr != new_thread->tidr)
+		mtspr(SPRN_TIDR, new_thread->tidr);
 #endif
 }
 
@@ -1451,6 +1455,116 @@ void flush_thread(void)
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 }
 
+#ifdef CONFIG_PPC64
+static DEFINE_SPINLOCK(vas_thread_id_lock);
+static DEFINE_IDA(vas_thread_ida);
+
+/*
+ * We need to assign a unique thread id to each thread in a process.
+ *
+ * This thread id, referred to as TIDR, and separate from the Linux's tgid,
+ * is intended to be used to direct an ASB_Notify from the hardware to the
+ * thread, when a suitable event occurs in the system.
+ *
+ * One such event is a "paste" instruction in the context of Fast Thread
+ * Wakeup (aka Core-to-core wake up in the Virtual Accelerator Switchboard
+ * (VAS) in POWER9.
+ *
+ * To get a unique TIDR per process we could simply reuse task_pid_nr() but
+ * the problem is that task_pid_nr() is not yet available copy_thread() is
+ * called. Fixing that would require changing more intrusive arch-neutral
+ * code in code path in copy_process()?.
+ *
+ * Further, to assign unique TIDRs within each process, we need an atomic
+ * field (or an IDR) in task_struct, which again intrudes into the arch-
+ * neutral code. So try to assign globally unique TIDRs for now.
+ *
+ * NOTE: TIDR 0 indicates that the thread does not need a TIDR value.
+ *	 For now, only threads that expect to be notified by the VAS
+ *	 hardware need a TIDR value and we assign values > 0 for those.
+ */
+#define MAX_THREAD_CONTEXT	((1 << 16) - 1)
+static int assign_thread_tidr(void)
+{
+	int index;
+	int err;
+
+again:
+	if (!ida_pre_get(&vas_thread_ida, GFP_KERNEL))
+		return -ENOMEM;
+
+	spin_lock(&vas_thread_id_lock);
+	err = ida_get_new_above(&vas_thread_ida, 1, &index);
+	spin_unlock(&vas_thread_id_lock);
+
+	if (err == -EAGAIN)
+		goto again;
+	else if (err)
+		return err;
+
+	if (index > MAX_THREAD_CONTEXT) {
+		spin_lock(&vas_thread_id_lock);
+		ida_remove(&vas_thread_ida, index);
+		spin_unlock(&vas_thread_id_lock);
+		return -ENOMEM;
+	}
+
+	return index;
+}
+
+static void free_thread_tidr(int id)
+{
+	spin_lock(&vas_thread_id_lock);
+	ida_remove(&vas_thread_ida, id);
+	spin_unlock(&vas_thread_id_lock);
+}
+
+/*
+ * Clear any TIDR value assigned to this thread.
+ */
+void clear_thread_tidr(struct task_struct *t)
+{
+	if (!t->thread.tidr)
+		return;
+
+	if (!cpu_has_feature(CPU_FTR_ARCH_300)) {
+		WARN_ON_ONCE(1);
+		return;
+	}
+
+	mtspr(SPRN_TIDR, 0);
+	free_thread_tidr(t->thread.tidr);
+	t->thread.tidr = 0;
+}
+
+void arch_release_task_struct(struct task_struct *t)
+{
+	clear_thread_tidr(t);
+}
+
+/*
+ * Assign a unique TIDR (thread id) for task @t and set it in the thread
+ * structure. For now, we only support setting TIDR for 'current' task.
+ */
+int set_thread_tidr(struct task_struct *t)
+{
+	if (!cpu_has_feature(CPU_FTR_ARCH_300))
+		return -EINVAL;
+
+	if (t != current)
+		return -EINVAL;
+
+	t->thread.tidr = assign_thread_tidr();
+	if (t->thread.tidr < 0)
+		return t->thread.tidr;
+
+	mtspr(SPRN_TIDR, t->thread.tidr);
+
+	return 0;
+}
+
+#endif /* CONFIG_PPC64 */
+
 void
 release_thread(struct task_struct *t)
 {
@@ -1597,6 +1711,8 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	}
 	if (cpu_has_feature(CPU_FTR_HAS_PPR))
 		p->thread.ppr = INIT_PPR;
+
+	p->thread.tidr = 0;
 #endif
 	kregs->nip = ppc_function_entry(f);
 	return 0;

commit eb5c3f1c86470fc1a57ab28cce15c12e4d6cdf8b
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Thu Nov 2 14:09:05 2017 +1100

    powerpc: Always save/restore checkpointed regs during treclaim/trecheckpoint
    
    Lazy save and restore of FP/Altivec means that a userspace process can
    be sent to userspace with FP or Altivec disabled and loaded only as
    required (by way of an FP/Altivec unavailable exception). Transactional
    Memory complicates this situation as a transaction could be started
    without FP/Altivec being loaded up. This causes the hardware to
    checkpoint incorrect registers. Handling FP/Altivec unavailable
    exceptions while a thread is transactional requires a reclaim and
    recheckpoint to ensure the CPU has correct state for both sets of
    registers.
    
    tm_reclaim() has optimisations to not always save the FP/Altivec
    registers to the checkpointed save area. This was originally done
    because the caller might have information that the checkpointed
    registers aren't valid due to lazy save and restore. We've also been a
    little vague as to how tm_reclaim() leaves the FP/Altivec state since it
    doesn't necessarily always save it to the thread struct. This has lead
    to an (incorrect) assumption that it leaves the checkpointed state on
    the CPU.
    
    tm_recheckpoint() has similar optimisations in reverse. It may not
    always reload the checkpointed FP/Altivec registers from the thread
    struct before the trecheckpoint. It is therefore quite unclear where it
    expects to get the state from. This didn't help with the assumption
    made about tm_reclaim().
    
    These optimisations sit in what is by definition a slow path. If a
    process has to go through a reclaim/recheckpoint then its transaction
    will be doomed on returning to userspace. This mean that the process
    will be unable to complete its transaction and be forced to its failure
    handler. This is already an out if line case for userspace. Furthermore,
    the cost of copying 64 times 128 bits from registers isn't very long[0]
    (at all) on modern processors. As such it appears these optimisations
    have only served to increase code complexity and are unlikely to have
    had a measurable performance impact.
    
    Our transactional memory handling has been riddled with bugs. A cause
    of this has been difficulty in following the code flow, code complexity
    has not been our friend here. It makes sense to remove these
    optimisations in favour of a (hopefully) more stable implementation.
    
    This patch does mean that some times the assembly will needlessly save
    'junk' registers which will subsequently get overwritten with the
    correct value by the C code which calls the assembly function. This
    small inefficiency is far outweighed by the reduction in complexity for
    general TM code, context switching paths, and transactional facility
    unavailable exception handler.
    
    0: I tried to measure it once for other work and found that it was
    hiding in the noise of everything else I was working with. I find it
    exceedingly likely this will be the case here.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ca8c33b41989..4083b737decb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -876,6 +876,8 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 
 	giveup_all(container_of(thr, struct task_struct, thread));
 
+	tm_reclaim(thr, cause);
+
 	/*
 	 * If we are in a transaction and FP is off then we can't have
 	 * used FP inside that transaction. Hence the checkpointed
@@ -894,8 +896,6 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 	if ((thr->ckpt_regs.msr & MSR_VEC) == 0)
 		memcpy(&thr->ckvr_state, &thr->vr_state,
 		       sizeof(struct thread_vr_state));
-
-	tm_reclaim(thr, thr->ckpt_regs.msr, cause);
 }
 
 void tm_reclaim_current(uint8_t cause)
@@ -946,11 +946,9 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 	tm_save_sprs(thr);
 }
 
-extern void __tm_recheckpoint(struct thread_struct *thread,
-			      unsigned long orig_msr);
+extern void __tm_recheckpoint(struct thread_struct *thread);
 
-void tm_recheckpoint(struct thread_struct *thread,
-		     unsigned long orig_msr)
+void tm_recheckpoint(struct thread_struct *thread)
 {
 	unsigned long flags;
 
@@ -969,15 +967,13 @@ void tm_recheckpoint(struct thread_struct *thread,
 	 */
 	tm_restore_sprs(thread);
 
-	__tm_recheckpoint(thread, orig_msr);
+	__tm_recheckpoint(thread);
 
 	local_irq_restore(flags);
 }
 
 static inline void tm_recheckpoint_new_task(struct task_struct *new)
 {
-	unsigned long msr;
-
 	if (!cpu_has_feature(CPU_FTR_TM))
 		return;
 
@@ -996,13 +992,11 @@ static inline void tm_recheckpoint_new_task(struct task_struct *new)
 		tm_restore_sprs(&new->thread);
 		return;
 	}
-	msr = new->thread.ckpt_regs.msr;
 	/* Recheckpoint to restore original checkpointed register state. */
-	TM_DEBUG("*** tm_recheckpoint of pid %d "
-		 "(new->msr 0x%lx, new->origmsr 0x%lx)\n",
-		 new->pid, new->thread.regs->msr, msr);
+	TM_DEBUG("*** tm_recheckpoint of pid %d (new->msr 0x%lx)\n",
+		 new->pid, new->thread.regs->msr);
 
-	tm_recheckpoint(&new->thread, msr);
+	tm_recheckpoint(&new->thread);
 
 	/*
 	 * The checkpointed state has been restored but the live state has

commit 91381b9cb1c3afc162f830ebf698721402c7d577
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Thu Nov 2 14:09:04 2017 +1100

    powerpc: Force reload for recheckpoint during tm {fp, vec, vsx} unavailable exception
    
    Lazy save and restore of FP/Altivec means that a userspace process can
    be sent to userspace with FP or Altivec disabled and loaded only as
    required (by way of an FP/Altivec unavailable exception). Transactional
    Memory complicates this situation as a transaction could be started
    without FP/Altivec being loaded up. This causes the hardware to
    checkpoint incorrect registers. Handling FP/Altivec unavailable
    exceptions while a thread is transactional requires a reclaim and
    recheckpoint to ensure the CPU has correct state for both sets of
    registers.
    
    tm_reclaim() has optimisations to not always save the FP/Altivec
    registers to the checkpointed save area. This was originally done
    because the caller might have information that the checkpointed
    registers aren't valid due to lazy save and restore. We've also been a
    little vague as to how tm_reclaim() leaves the FP/Altivec state since it
    doesn't necessarily always save it to the thread struct. This has lead
    to an (incorrect) assumption that it leaves the checkpointed state on
    the CPU.
    
    tm_recheckpoint() has similar optimisations in reverse. It may not
    always reload the checkpointed FP/Altivec registers from the thread
    struct before the trecheckpoint. It is therefore quite unclear where it
    expects to get the state from. This didn't help with the assumption
    made about tm_reclaim().
    
    This patch is a minimal fix for ease of backporting. A more correct fix
    which removes the msr parameter to tm_reclaim() and tm_recheckpoint()
    altogether has been upstreamed to apply on top of this patch.
    
    Fixes: dc3106690b20 ("powerpc: tm: Always use fp_state and vr_state to
    store live registers")
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e2980a22c487..ca8c33b41989 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -874,6 +874,8 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 	if (!MSR_TM_SUSPENDED(mfmsr()))
 		return;
 
+	giveup_all(container_of(thr, struct task_struct, thread));
+
 	/*
 	 * If we are in a transaction and FP is off then we can't have
 	 * used FP inside that transaction. Hence the checkpointed
@@ -893,8 +895,6 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 		memcpy(&thr->ckvr_state, &thr->vr_state,
 		       sizeof(struct thread_vr_state));
 
-	giveup_all(container_of(thr, struct task_struct, thread));
-
 	tm_reclaim(thr, thr->ckpt_regs.msr, cause);
 }
 

commit a7771176b4392fbc3a17399c51a8c11f2f681afe
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Thu Nov 2 14:09:03 2017 +1100

    powerpc: Don't enable FP/Altivec if not checkpointed
    
    Lazy save and restore of FP/Altivec means that a userspace process can
    be sent to userspace with FP or Altivec disabled and loaded only as
    required (by way of an FP/Altivec unavailable exception). Transactional
    Memory complicates this situation as a transaction could be started
    without FP/Altivec being loaded up. This causes the hardware to
    checkpoint incorrect registers. Handling FP/Altivec unavailable
    exceptions while a thread is transactional requires a reclaim and
    recheckpoint to ensure the CPU has correct state for both sets of
    registers.
    
    Lazy save and restore of FP/Altivec cannot be done if a process is
    transactional. If a facility was enabled it must remain enabled whenever
    a thread is transactional.
    
    Commit dc16b553c949 ("powerpc: Always restore FPU/VEC/VSX if hardware
    transactional memory in use") ensures that the facilities are always
    enabled if a thread is transactional. A bug in the introduced code may
    cause it to inadvertently enable a facility that was (and should remain)
    disabled. The problem with this extraneous enablement is that the
    registers for the erroneously enabled facility have not been correctly
    recheckpointed - the recheckpointing code assumed the facility would
    remain disabled.
    
    Further compounding the issue, the transactional {fp,altivec,vsx}
    unavailable code has been incorrectly using the MSR to enable
    facilities. The presence of the {FP,VEC,VSX} bit in the regs->msr simply
    means if the registers are live on the CPU, not if the kernel should
    load them before returning to userspace. This has worked due to the bug
    mentioned above.
    
    This causes transactional threads which return to their failure handler
    to observe incorrect checkpointed registers. Perhaps an example will
    help illustrate the problem:
    
    A userspace process is running and uses both FP and Altivec registers.
    This process then continues to run for some time without touching
    either sets of registers. The kernel subsequently disables the
    facilities as part of lazy save and restore. The userspace process then
    performs a tbegin and the CPU checkpoints 'junk' FP and Altivec
    registers. The process then performs a floating point instruction
    triggering a fp unavailable exception in the kernel.
    
    The kernel then loads the FP registers - and only the FP registers.
    Since the thread is transactional it must perform a reclaim and
    recheckpoint to ensure both the checkpointed registers and the
    transactional registers are correct. It then (correctly) enables
    MSR[FP] for the process. Later (on exception exist) the kernel also
    (inadvertently) enables MSR[VEC]. The process is then returned to
    userspace.
    
    Since the act of loading the FP registers doomed the transaction we know
    CPU will fail the transaction, restore its checkpointed registers, and
    return the process to its failure handler. The problem is that we're
    now running with Altivec enabled and the 'junk' checkpointed registers
    are restored. The kernel had only recheckpointed FP.
    
    This patch solves this by only activating FP/Altivec if userspace was
    using them when it entered the kernel and not simply if the process is
    transactional.
    
    Fixes: dc16b553c949 ("powerpc: Always restore FPU/VEC/VSX if hardware
    transactional memory in use")
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2b602d60a9c7..e2980a22c487 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -104,9 +104,23 @@ static inline bool msr_tm_active(unsigned long msr)
 {
 	return MSR_TM_ACTIVE(msr);
 }
+
+static bool tm_active_with_fp(struct task_struct *tsk)
+{
+	return msr_tm_active(tsk->thread.regs->msr) &&
+		(tsk->thread.ckpt_regs.msr & MSR_FP);
+}
+
+static bool tm_active_with_altivec(struct task_struct *tsk)
+{
+	return msr_tm_active(tsk->thread.regs->msr) &&
+		(tsk->thread.ckpt_regs.msr & MSR_VEC);
+}
 #else
 static inline bool msr_tm_active(unsigned long msr) { return false; }
 static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
+static inline bool tm_active_with_fp(struct task_struct *tsk) { return false; }
+static inline bool tm_active_with_altivec(struct task_struct *tsk) { return false; }
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
 bool strict_msr_control;
@@ -239,7 +253,7 @@ EXPORT_SYMBOL(enable_kernel_fp);
 
 static int restore_fp(struct task_struct *tsk)
 {
-	if (tsk->thread.load_fp || msr_tm_active(tsk->thread.regs->msr)) {
+	if (tsk->thread.load_fp || tm_active_with_fp(tsk)) {
 		load_fp_state(&current->thread.fp_state);
 		current->thread.load_fp++;
 		return 1;
@@ -321,7 +335,7 @@ EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
 static int restore_altivec(struct task_struct *tsk)
 {
 	if (cpu_has_feature(CPU_FTR_ALTIVEC) &&
-		(tsk->thread.load_vec || msr_tm_active(tsk->thread.regs->msr))) {
+		(tsk->thread.load_vec || tm_active_with_altivec(tsk))) {
 		load_vr_state(&tsk->thread.vr_state);
 		tsk->thread.used_vr = 1;
 		tsk->thread.load_vec++;

commit 4e003747043d57aa75c9762fa148ef38afe68dd8
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Oct 19 15:08:43 2017 +1100

    powerpc/64s: Replace CONFIG_PPC_STD_MMU_64 with CONFIG_PPC_BOOK3S_64
    
    CONFIG_PPC_STD_MMU_64 indicates support for the "standard" powerpc MMU
    on 64-bit CPUs. The "standard" MMU refers to the hash page table MMU
    found in "server" processors, from IBM mainly.
    
    Currently CONFIG_PPC_STD_MMU_64 is == CONFIG_PPC_BOOK3S_64. While it's
    annoying to have two symbols that always have the same value, it's not
    quite annoying enough to bother removing one.
    
    However with the arrival of Power9, we now have the situation where
    CONFIG_PPC_STD_MMU_64 is enabled, but the kernel is running using the
    Radix MMU - *not* the "standard" MMU. So it is now actively confusing
    to use it, because it implies that code is disabled or inactive when
    the Radix MMU is in use, however that is not necessarily true.
    
    So s/CONFIG_PPC_STD_MMU_64/CONFIG_PPC_BOOK3S_64/, and do some minor
    formatting updates of some of the affected lines.
    
    This will be a pain for backports, but c'est la vie.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c051dc2b42ad..2b602d60a9c7 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1164,7 +1164,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	}
 #endif /* CONFIG_PPC64 */
 
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	batch = this_cpu_ptr(&ppc64_tlb_batch);
 	if (batch->active) {
 		current_thread_info()->local_flags |= _TLF_LAZY_MMU;
@@ -1172,7 +1172,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 			__flush_tlb_pending(batch);
 		batch->active = 0;
 	}
-#endif /* CONFIG_PPC_STD_MMU_64 */
+#endif /* CONFIG_PPC_BOOK3S_64 */
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	switch_booke_debug_regs(&new->thread.debug);
@@ -1218,7 +1218,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	last = _switch(old_thread, new_thread);
 
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	if (current_thread_info()->local_flags & _TLF_LAZY_MMU) {
 		current_thread_info()->local_flags &= ~_TLF_LAZY_MMU;
 		batch = this_cpu_ptr(&ppc64_tlb_batch);
@@ -1247,7 +1247,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 					: : "r"(dummy_copy_buffer), "r"(0));
 		}
 	}
-#endif /* CONFIG_PPC_STD_MMU_64 */
+#endif /* CONFIG_PPC_BOOK3S_64 */
 
 	return last;
 }
@@ -1476,7 +1476,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 
 static void setup_ksp_vsid(struct task_struct *p, unsigned long sp)
 {
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	unsigned long sp_vsid;
 	unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
 
@@ -2056,7 +2056,7 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	unsigned long base = mm->brk;
 	unsigned long ret;
 
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	/*
 	 * If we are using 1TB segments and we are allowed to randomise
 	 * the heap, we can put it above 1TB so it is backed by a 1TB

commit 92fb8690bd04cb421d987d246deac60eef85d272
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Oct 12 21:17:19 2017 +1100

    powerpc/tm: P9 disable transactionally suspended sigcontexts
    
    Unfortunately userspace can construct a sigcontext which enables
    suspend. Thus userspace can force Linux into a path where trechkpt is
    executed.
    
    This patch blocks this from happening on POWER9 by sanity checking
    sigcontexts passed in.
    
    ptrace doesn't have this problem as only MSR SE and BE can be changed
    via ptrace.
    
    This patch also adds a number of WARN_ON()s in case we ever enter
    suspend when we shouldn't. This should not happen, but if it does the
    symptoms are soft lockup warnings which are not obviously TM related,
    so the WARN_ON()s should make it obvious what's happening.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b02807ea54dc..c051dc2b42ad 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -910,6 +910,8 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 	if (!MSR_TM_ACTIVE(thr->regs->msr))
 		goto out_and_saveregs;
 
+	WARN_ON(tm_suspend_disabled);
+
 	TM_DEBUG("--- tm_reclaim on pid %d (NIP=%lx, "
 		 "ccr=%lx, msr=%lx, trap=%lx)\n",
 		 tsk->pid, thr->regs->nip,

commit 54820530c5faa9fd78e1c08cb6449100b1a19157
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Oct 12 21:17:18 2017 +1100

    powerpc/powernv: Enable TM without suspend if possible
    
    Some Power9 revisions can run in a mode where TM operates without
    suspended state. If we find ourself on a CPU that might be in this
    mode, we query OPAL to check, and if so we reenable TM in CPU
    features, and enable a new user feature to signal to userspace that we
    are in this mode.
    
    We do not enable the "normal" user feature, PPC_FEATURE2_HTM, but we
    do enable PPC_FEATURE2_HTM_NOSC because that indicates to userspace
    that the kernel will abort transactions on syscall entry, which is
    true regardless of the suspend mode.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 166145b18728..b02807ea54dc 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -77,6 +77,13 @@
 extern unsigned long _get_SP(void);
 
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+/*
+ * Are we running in "Suspend disabled" mode? If so we have to block any
+ * sigreturn that would get us into suspended state, and we also warn in some
+ * other paths that we should never reach with suspend disabled.
+ */
+bool tm_suspend_disabled __ro_after_init = false;
+
 static void check_if_tm_restore_required(struct task_struct *tsk)
 {
 	/*

commit 4ca360f3dbf2036d964cdf3a6c4a45a81fdf8e18
Author: Kautuk Consul <kautuk.consul.1980@gmail.com>
Date:   Tue Apr 19 15:48:21 2016 +0530

    powerpc: get_wchan(): solve possible race scenario due to parallel wakeup
    
    Add a check for p->state == TASK_RUNNING so that any wake-ups on
    task_struct p in the interim lead to 0 being returned by get_wchan().
    
    Signed-off-by: Kautuk Consul <kautuk.consul.1980@gmail.com>
    [mpe: Confirmed other architectures do similar]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a0c74bbf3454..166145b18728 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1898,7 +1898,8 @@ unsigned long get_wchan(struct task_struct *p)
 
 	do {
 		sp = *(unsigned long *)sp;
-		if (!validate_sp(sp, p, STACK_FRAME_OVERHEAD))
+		if (!validate_sp(sp, p, STACK_FRAME_OVERHEAD) ||
+		    p->state == TASK_RUNNING)
 			return 0;
 		if (count > 0) {
 			ip = ((unsigned long *)sp)[STACK_FRAME_LR_SAVE];

commit a6036100edd1d8e024beb4b97c1f15c114660c6c
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Aug 23 23:56:24 2017 +1000

    powerpc/oops: Line up NIP & MSR with other rows
    
    This is purely cosmetic, but does look nicer IMHO:
    
    Before:
    
      task: c000000001453400 task.stack: c000000001c6c000
      NIP: c000000000a0fbfc LR: c000000000a0fbf4 CTR: c000000000ba6220
      REGS: c0000001fffef820 TRAP: 0300   Not tainted  (4.13.0-rc6-gcc-6.3.1-00234-g423af27f7d81)
      MSR: 8000000000009033 <SF,EE,ME,IR,DR,RI,LE>  CR: 88088242  XER: 00000000
      CFAR: c0000000000b3488 DAR: 0000000000000000 DSISR: 42000000 SOFTE: 0
    
    After:
      task: c000000001453400 task.stack: c000000001c6c000
      NIP:  c000000000a0fbfc LR: c000000000a0fbf4 CTR: c000000000ba6220
      REGS: c0000001fffef820 TRAP: 0300   Not tainted  (4.13.0-rc6-gcc-6.3.1-00234-g423af27f7d81-dirty)
      MSR:  8000000000009033 <SF,EE,ME,IR,DR,RI,LE>  CR: 88088242  XER: 00000000
      CFAR: c0000000000b34a4 DAR: 0000000000000000 DSISR: 42000000 SOFTE: 0
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1e24d6f1be90..a0c74bbf3454 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1380,11 +1380,11 @@ void show_regs(struct pt_regs * regs)
 
 	show_regs_print_info(KERN_DEFAULT);
 
-	printk("NIP: "REG" LR: "REG" CTR: "REG"\n",
+	printk("NIP:  "REG" LR: "REG" CTR: "REG"\n",
 	       regs->nip, regs->link, regs->ctr);
 	printk("REGS: %p TRAP: %04lx   %s  (%s)\n",
 	       regs, regs->trap, print_tainted(), init_utsname()->release);
-	printk("MSR: "REG" ", regs->msr);
+	printk("MSR:  "REG" ", regs->msr);
 	print_msr_bits(regs->msr);
 	pr_cont("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);

commit f6fc73fb965f6c1fd7ad75aabfdee6b1af0f7093
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Aug 23 23:56:23 2017 +1000

    powerpc/oops: Print CR/XER on same line as MSR
    
    Somehow we missed this when the pr_cont() changes went in. Fix CR/XER
    to go on the same line as MSR, as they have historically, eg:
    
      MSR: 8000000000009032 <SF,EE,ME,IR,DR,RI>  CR: 4804408a  XER: 20000000
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0a00d59df537..1e24d6f1be90 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1386,7 +1386,7 @@ void show_regs(struct pt_regs * regs)
 	       regs, regs->trap, print_tainted(), init_utsname()->release);
 	printk("MSR: "REG" ", regs->msr);
 	print_msr_bits(regs->msr);
-	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
+	pr_cont("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
 	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
 		pr_cont("CFAR: "REG" ", regs->orig_gpr3);

commit d1d0d5ffb3006eaf9b5f41c89fe801e032cbbfe4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Aug 12 02:39:07 2017 +1000

    powerpc/64: Optimise set/clear of CTRL[RUN] (runlatch)
    
    On modern CPUs the CTRL register is read-only except bit 63 which is
    the run latch control. This means it can be updated with a mtspr
    rather than mfspr/mtspr.
    
    To accomodate older CPUs (Cell at least), where there are other bits
    in the register, we still do a read/modify/write on pre 2.06 CPUs.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Update change log to mention 2.06 workaround]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 34bd94b090e2..0a00d59df537 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1979,11 +1979,25 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 void notrace __ppc64_runlatch_on(void)
 {
 	struct thread_info *ti = current_thread_info();
-	unsigned long ctrl;
 
-	ctrl = mfspr(SPRN_CTRLF);
-	ctrl |= CTRL_RUNLATCH;
-	mtspr(SPRN_CTRLT, ctrl);
+	if (cpu_has_feature(CPU_FTR_ARCH_206)) {
+		/*
+		 * Least significant bit (RUN) is the only writable bit of
+		 * the CTRL register, so we can avoid mfspr. 2.06 is not the
+		 * earliest ISA where this is the case, but it's convenient.
+		 */
+		mtspr(SPRN_CTRLT, CTRL_RUNLATCH);
+	} else {
+		unsigned long ctrl;
+
+		/*
+		 * Some architectures (e.g., Cell) have writable fields other
+		 * than RUN, so do the read-modify-write.
+		 */
+		ctrl = mfspr(SPRN_CTRLF);
+		ctrl |= CTRL_RUNLATCH;
+		mtspr(SPRN_CTRLT, ctrl);
+	}
 
 	ti->local_flags |= _TLF_RUNLATCH;
 }
@@ -1992,13 +2006,18 @@ void notrace __ppc64_runlatch_on(void)
 void notrace __ppc64_runlatch_off(void)
 {
 	struct thread_info *ti = current_thread_info();
-	unsigned long ctrl;
 
 	ti->local_flags &= ~_TLF_RUNLATCH;
 
-	ctrl = mfspr(SPRN_CTRLF);
-	ctrl &= ~CTRL_RUNLATCH;
-	mtspr(SPRN_CTRLT, ctrl);
+	if (cpu_has_feature(CPU_FTR_ARCH_206)) {
+		mtspr(SPRN_CTRLT, 0);
+	} else {
+		unsigned long ctrl;
+
+		ctrl = mfspr(SPRN_CTRLF);
+		ctrl &= ~CTRL_RUNLATCH;
+		mtspr(SPRN_CTRLT, ctrl);
+	}
 }
 #endif /* CONFIG_PPC64 */
 

commit 15c659ff9d5b367c886166a9854a89b72c524a68
Merge: 516fa8d0e19d 1a92a80ad386
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Aug 23 22:20:10 2017 +1000

    Merge branch 'fixes' into next
    
    There's a non-trivial dependency between some commits we want to put in
    next and the KVM prefetch work around that went into fixes. So merge
    fixes into next.

commit 96c79b6bd74039e8a799e4aa2d331cbd478ab5a1
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 16 16:01:18 2017 +1000

    powerpc: Remove more redundant VSX save/tests
    
    __giveup_vsx/save_vsx are completely equivalent to testing MSR_FP
    and MSR_VEC and calling the corresponding giveup/save function so
    just remove the spurious VSX cases. Also add WARN_ONs checking that
    we never have VSX enabled without the two other.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ff522bf75d53..cc5bae4bba7b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -355,14 +355,6 @@ static void giveup_vsx(struct task_struct *tsk)
 	msr_check_and_clear(MSR_FP|MSR_VEC|MSR_VSX);
 }
 
-static void save_vsx(struct task_struct *tsk)
-{
-	if (tsk->thread.regs->msr & MSR_FP)
-		save_fpu(tsk);
-	if (tsk->thread.regs->msr & MSR_VEC)
-		save_altivec(tsk);
-}
-
 void enable_kernel_vsx(void)
 {
 	unsigned long cpumsr;
@@ -411,7 +403,6 @@ static int restore_vsx(struct task_struct *tsk)
 }
 #else
 static inline int restore_vsx(struct task_struct *tsk) { return 0; }
-static inline void save_vsx(struct task_struct *tsk) { }
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_SPE
@@ -491,6 +482,8 @@ void giveup_all(struct task_struct *tsk)
 	msr_check_and_set(msr_all_available);
 	check_if_tm_restore_required(tsk);
 
+	WARN_ON((usermsr & MSR_VSX) && !((usermsr & MSR_FP) && (usermsr & MSR_VEC)));
+
 #ifdef CONFIG_PPC_FPU
 	if (usermsr & MSR_FP)
 		__giveup_fpu(tsk);
@@ -499,10 +492,6 @@ void giveup_all(struct task_struct *tsk)
 	if (usermsr & MSR_VEC)
 		__giveup_altivec(tsk);
 #endif
-#ifdef CONFIG_VSX
-	if (usermsr & MSR_VSX)
-		__giveup_vsx(tsk);
-#endif
 #ifdef CONFIG_SPE
 	if (usermsr & MSR_SPE)
 		__giveup_spe(tsk);
@@ -561,19 +550,13 @@ void save_all(struct task_struct *tsk)
 
 	msr_check_and_set(msr_all_available);
 
-	/*
-	 * Saving the way the register space is in hardware, save_vsx boils
-	 * down to a save_fpu() and save_altivec()
-	 */
-	if (usermsr & MSR_VSX) {
-		save_vsx(tsk);
-	} else {
-		if (usermsr & MSR_FP)
-			save_fpu(tsk);
+	WARN_ON((usermsr & MSR_VSX) && !((usermsr & MSR_FP) && (usermsr & MSR_VEC)));
 
-		if (usermsr & MSR_VEC)
-			save_altivec(tsk);
-	}
+	if (usermsr & MSR_FP)
+		save_fpu(tsk);
+
+	if (usermsr & MSR_VEC)
+		save_altivec(tsk);
 
 	if (usermsr & MSR_SPE)
 		__giveup_spe(tsk);

commit dc801081f2eae57a389bc9230ff4fb0d91487990
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 16 16:01:17 2017 +1000

    powerpc: Remove redundant clear of MSR_VSX in __giveup_vsx()
    
    __giveup_fpu() already does it and we cannot have MSR_VSX set
    without having MSR_FP also set.
    
    This also adds a warning to check we indeed do
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 32b58648052f..ff522bf75d53 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -331,11 +331,19 @@ static inline int restore_altivec(struct task_struct *tsk) { return 0; }
 #ifdef CONFIG_VSX
 static void __giveup_vsx(struct task_struct *tsk)
 {
-	if (tsk->thread.regs->msr & MSR_FP)
+	unsigned long msr = tsk->thread.regs->msr;
+
+	/*
+	 * We should never be ssetting MSR_VSX without also setting
+	 * MSR_FP and MSR_VEC
+	 */
+	WARN_ON((msr & MSR_VSX) && !((msr & MSR_FP) && (msr & MSR_VEC)));
+
+	/* __giveup_fpu will clear MSR_VSX */
+	if (msr & MSR_FP)
 		__giveup_fpu(tsk);
-	if (tsk->thread.regs->msr & MSR_VEC)
+	if (msr & MSR_VEC)
 		__giveup_altivec(tsk);
-	tsk->thread.regs->msr &= ~MSR_VSX;
 }
 
 static void giveup_vsx(struct task_struct *tsk)

commit 746874d31cd1173f2dd234c1a5f692939afe0b71
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 16 16:01:16 2017 +1000

    powerpc: Remove redundant FP/Altivec giveup code
    
    __giveup_vsx() already calls those two functions.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index cd476e338768..32b58648052f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -374,10 +374,6 @@ void enable_kernel_vsx(void)
 		 */
 		if(!msr_tm_active(cpumsr) && msr_tm_active(current->thread.regs->msr))
 			return;
-		if (current->thread.regs->msr & MSR_FP)
-			__giveup_fpu(current);
-		if (current->thread.regs->msr & MSR_VEC)
-			__giveup_altivec(current);
 		__giveup_vsx(current);
 	}
 }

commit 6a303833b5e3acbb4c97cc11cc688650d070da19
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 16 16:01:15 2017 +1000

    powerpc: Fix missing newline before {
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9f3e2c932dcc..cd476e338768 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -230,7 +230,8 @@ void enable_kernel_fp(void)
 }
 EXPORT_SYMBOL(enable_kernel_fp);
 
-static int restore_fp(struct task_struct *tsk) {
+static int restore_fp(struct task_struct *tsk)
+{
 	if (tsk->thread.load_fp || msr_tm_active(tsk->thread.regs->msr)) {
 		load_fp_state(&current->thread.fp_state);
 		current->thread.load_fp++;

commit 5a69aec945d27e78abac9fd032533d3aaebf7c1e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 16 16:01:14 2017 +1000

    powerpc: Fix VSX enabling/flushing to also test MSR_FP and MSR_VEC
    
    VSX uses a combination of the old vector registers, the old FP
    registers and new "second halves" of the FP registers.
    
    Thus when we need to see the VSX state in the thread struct
    (flush_vsx_to_thread()) or when we'll use the VSX in the kernel
    (enable_kernel_vsx()) we need to ensure they are all flushed into
    the thread struct if either of them is individually enabled.
    
    Unfortunately we only tested if the whole VSX was enabled, not if they
    were individually enabled.
    
    Fixes: 72cd7b44bc99 ("powerpc: Uncomment and make enable_kernel_vsx() routine available")
    Cc: stable@vger.kernel.org # v4.3+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ec480966f9bf..1f0fd361e09b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -362,7 +362,8 @@ void enable_kernel_vsx(void)
 
 	cpumsr = msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX)) {
+	if (current->thread.regs &&
+	    (current->thread.regs->msr & (MSR_VSX|MSR_VEC|MSR_FP))) {
 		check_if_tm_restore_required(current);
 		/*
 		 * If a thread has already been reclaimed then the
@@ -386,7 +387,7 @@ void flush_vsx_to_thread(struct task_struct *tsk)
 {
 	if (tsk->thread.regs) {
 		preempt_disable();
-		if (tsk->thread.regs->msr & MSR_VSX) {
+		if (tsk->thread.regs->msr & (MSR_VSX|MSR_VEC|MSR_FP)) {
 			BUG_ON(tsk != current);
 			giveup_vsx(tsk);
 		}

commit 44a12806d010944a5727f1dc99123121e3e2c8c6
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Aug 7 21:25:01 2017 +1000

    Revert "powerpc/64: Avoid restore_math call if possible in syscall exit"
    
    This reverts commit bc4f65e4cf9d6cc43e0e9ba0b8648cf9201cd55f.
    
    As reported by Andreas, this commit is causing unrecoverable SLB misses in the
    system call exit path:
    
      Unrecoverable exception 4100 at c00000000000a1ec
      Oops: Unrecoverable exception, sig: 6 [#1]
      SMP NR_CPUS=2 PowerMac
      ...
      CPU: 0 PID: 18626 Comm: rm Not tainted 4.13.0-rc3 #1
      task: c00000018335e080 task.stack: c000000139e50000
      NIP: c00000000000a1ec LR: c00000000000a118 CTR: 0000000000000000
      REGS: c000000139e53bb0 TRAP: 4100   Not tainted  (4.13.0-rc3)
      MSR: 9000000000001030 <SF,HV,ME,IR,DR> CR: 24000044  XER: 20000000 SOFTE: 1
      GPR00: 0000000000000000 c000000139e53e30 c000000000abb500 fffffffffffffffe
      GPR04: c0000001eb866298 0000000000000000 0000000000000000 c00000018335e080
      GPR08: 900000000000d032 0000000000000000 0000000000000002 fffffffffffff001
      GPR12: c000000139e50000 c00000000ffff000 00003fffa8c0dca0 00003fffa8c0dc88
      GPR16: 0000000010000000 0000000000000001 00003fffa8c0eaa0 0000000000000000
      GPR20: 00003fffa8c27528 00003fffa8c27b00 0000000000000000 0000000000000000
      GPR24: 00003fffa8c0d918 00003ffff1b3efa0 00003fffa8c26d68 0000000000000000
      GPR28: 00003fffa8c249e8 00003fffa8c263d0 00003fffa8c27550 00003ffff1b3ef10
      NIP [c00000000000a1ec] system_call_exit+0xc0/0x21c
      LR [c00000000000a118] system_call+0x58/0x6c
      Call Trace:
      [c000000139e53e30] [c00000000000a118] system_call+0x58/0x6c (unreliable)
      Instruction dump:
      64a51000 7c6300d0 f8a101a0 4bffff9c 3c000000 60000006 780007c6 64000000
      60000000 7c004039 4082001c e8ed0170 <88070b78> 88c70b79 7c003214 2c200000
    
    This is caused by us trying to load THREAD_LOAD_FP with MSR_RI=0, and taking an
    SLB miss on the thread struct.
    
    Reported-by: Andreas Schwab <schwab@linux-m68k.org>
    Diagnosed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9f3e2c932dcc..ec480966f9bf 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -511,10 +511,6 @@ void restore_math(struct pt_regs *regs)
 {
 	unsigned long msr;
 
-	/*
-	 * Syscall exit makes a similar initial check before branching
-	 * to restore_math. Keep them in synch.
-	 */
 	if (!msr_tm_active(regs->msr) &&
 		!current->thread.load_fp && !loadvec(current->thread))
 		return;

commit 218ea31039e84901b449c3769035456688f6e17d
Merge: 5405c92bc2cd d6bd8194e286
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Jul 3 23:05:43 2017 +1000

    Merge branch 'fixes' into next
    
    Merge our fixes branch, a few of them are tripping people up while
    working on top of next, and we also have a dependency between the CXL
    fixes and new CXL code we want to merge into next.

commit 07d2a628bc0008f90754ac7982289f6cb0f46cf8
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jun 9 01:36:09 2017 +1000

    powerpc/64s: Avoid cpabort in context switch when possible
    
    The ISA v3.0B copy-paste facility only requires cpabort when switching
    to a process that has foreign real addresses mapped (direct access to
    accelerators), to clear a potential copy buffer filled by a previous
    thread. There is no accelerator driver implemented yet, so cpabort can
    be removed. It can be be re-added when a driver is implemented.
    
    POWER9 DD1 requires the copy buffer to always be cleared on context
    switch, but if accelerators are not in use, then an unpaired copy from
    a dummy region is sufficient to clear data out of the copy buffer.
    
    This increases context switch performance by about 5% on POWER9.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 45faa9a32a01..6273b5d5baec 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1137,6 +1137,11 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 #endif
 }
 
+#ifdef CONFIG_PPC_BOOK3S_64
+#define CP_SIZE 128
+static const u8 dummy_copy_buffer[CP_SIZE] __attribute__((aligned(CP_SIZE)));
+#endif
+
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {
@@ -1226,8 +1231,28 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		batch->active = 1;
 	}
 
-	if (current_thread_info()->task->thread.regs)
+	if (current_thread_info()->task->thread.regs) {
 		restore_math(current_thread_info()->task->thread.regs);
+
+		/*
+		 * The copy-paste buffer can only store into foreign real
+		 * addresses, so unprivileged processes can not see the
+		 * data or use it in any way unless they have foreign real
+		 * mappings. We don't have a VAS driver that allocates those
+		 * yet, so no cpabort is required.
+		 */
+		if (cpu_has_feature(CPU_FTR_POWER9_DD1)) {
+			/*
+			 * DD1 allows paste into normal system memory, so we
+			 * do an unpaired copy here to clear the buffer and
+			 * prevent a covert channel being set up.
+			 *
+			 * cpabort is not used because it is quite expensive.
+			 */
+			asm volatile(PPC_COPY(%0, %1)
+					: : "r"(dummy_copy_buffer), "r"(0));
+		}
+	}
 #endif /* CONFIG_PPC_STD_MMU_64 */
 
 	return last;

commit e4c0fc5f72bca11432297168338aef46c12793a4
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jun 9 01:36:06 2017 +1000

    powerpc/64s: Leave interrupts hard enabled in context switch for radix
    
    Commit 4387e9ff25 ("[POWERPC] Fix PMU + soft interrupt disable bug")
    hard disabled interrupts over the low level context switch, because
    the SLB management can't cope with a PMU interrupt accesing the stack
    in that window.
    
    Radix based kernel mapping does not use the SLB so it does not require
    interrupts hard disabled here.
    
    This is worth 1-2% in context switch performance on POWER9.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 5cbb8b1faf7e..45faa9a32a01 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1199,12 +1199,14 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	__switch_to_tm(prev, new);
 
-	/*
-	 * We can't take a PMU exception inside _switch() since there is a
-	 * window where the kernel stack SLB and the kernel stack are out
-	 * of sync. Hard disable here.
-	 */
-	hard_irq_disable();
+	if (!radix_enabled()) {
+		/*
+		 * We can't take a PMU exception inside _switch() since there
+		 * is a window where the kernel stack SLB and the kernel stack
+		 * are out of sync. Hard disable here.
+		 */
+		hard_irq_disable();
+	}
 
 	/*
 	 * Call restore_sprs() before calling _switch(). If we move it after

commit bc4f65e4cf9d6cc43e0e9ba0b8648cf9201cd55f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jun 9 01:35:05 2017 +1000

    powerpc/64: Avoid restore_math call if possible in syscall exit
    
    The syscall exit code that branches to restore_math is quite heavy on
    Book3S, consisting of 2 mtmsr instructions. Threads that don't use both
    FP and vector can get caught here if the kernel ever uses FP or vector.
    Lazy-FP/vec context switching also trips this case.
    
    So check for lazy FP and vector before switching RI for restore_math.
    Move most of this case out of line.
    
    For threads that do want to restore math registers, the MSR switches are
    still suboptimal. Future direction may be to use a soft-RI bit to avoid
    MSR switches in kernel (similar to soft-EE), but for now at least the
    no-restore
    
    POWER9 context switch rate increases by about 5% due to sched_yield(2)
    return performance. I haven't constructed a test to measure the syscall
    cost.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index baae104b16c7..5cbb8b1faf7e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -511,6 +511,10 @@ void restore_math(struct pt_regs *regs)
 {
 	unsigned long msr;
 
+	/*
+	 * Syscall exit makes a similar initial check before branching
+	 * to restore_math. Keep them in synch.
+	 */
 	if (!msr_tm_active(regs->msr) &&
 		!current->thread.load_fp && !loadvec(current->thread))
 		return;

commit 7f22ced4377628074e2ac25f41a88f98eb3b03f1
Author: Breno Leitao <leitao@debian.org>
Date:   Mon Jun 5 11:40:59 2017 -0300

    powerpc/kernel: Initialize load_tm on task creation
    
    Currently tsk->thread.load_tm is not initialized in the task creation
    and can contain garbage on a new task.
    
    This is an undesired behaviour, since it affects the timing to enable
    and disable the transactional memory laziness (disabling and enabling
    the MSR TM bit, which affects TM reclaim and recheckpoint in the
    scheduling process).
    
    Fixes: 5d176f751ee3 ("powerpc: tm: Enable transactional memory (TM) lazily for userspace")
    Cc: stable@vger.kernel.org # v4.9+
    Signed-off-by: Breno Leitao <leitao@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a9435397eab8..2ad725ef4368 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1687,6 +1687,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	current->thread.tm_tfhar = 0;
 	current->thread.tm_texasr = 0;
 	current->thread.tm_tfiar = 0;
+	current->thread.load_tm = 0;
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 }
 EXPORT_SYMBOL(start_thread);

commit 1195892c091a15cc862f4e202482a36adc924e12
Author: Breno Leitao <leitao@debian.org>
Date:   Fri Jun 2 18:43:30 2017 -0300

    powerpc/kernel: Fix FP and vector register restoration
    
    Currently tsk->thread->load_vec and load_fp are not initialized during
    task creation, which can lead to garbage values in these variables (non-zero
    values).
    
    These variables will be checked later in restore_math() to validate if the
    FP and vector registers are being utilized. Since these values might be
    non-zero, the restore_math() will continue to save the FP and vectors even if
    they were never utilized by the userspace application. load_fp and load_vec
    counters will then overflow (they wrap at 255) and the FP and Altivec will be
    finally disabled, but before that condition is reached (counter overflow)
    several context switches will have restored FP and vector registers without
    need, causing a performance degradation.
    
    Fixes: 70fe3d980f5f ("powerpc: Restore FPU/VEC/VSX if previously used")
    Cc: stable@vger.kernel.org # v4.6+
    Signed-off-by: Breno Leitao <leitao@debian.org>
    Signed-off-by: Gustavo Romero <gusbromero@gmail.com>
    Acked-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index baae104b16c7..a9435397eab8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1666,6 +1666,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 #ifdef CONFIG_VSX
 	current->thread.used_vsr = 0;
 #endif
+	current->thread.load_fp = 0;
 	memset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));
 	current->thread.fp_save_area = NULL;
 #ifdef CONFIG_ALTIVEC
@@ -1674,6 +1675,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	current->thread.vr_save_area = NULL;
 	current->thread.vrsave = 0;
 	current->thread.used_vr = 0;
+	current->thread.load_vec = 0;
 #endif /* CONFIG_ALTIVEC */
 #ifdef CONFIG_SPE
 	memset(current->thread.evr, 0, sizeof(current->thread.evr));

commit f48e91e87e67b56bef63393d1a02c6e22c1d7078
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon May 8 17:16:26 2017 +1000

    powerpc/tm: Fix FP and VMX register corruption
    
    In commit dc3106690b20 ("powerpc: tm: Always use fp_state and vr_state
    to store live registers"), a section of code was removed that copied
    the current state to checkpointed state. That code should not have been
    removed.
    
    When an FP (Floating Point) unavailable is taken inside a transaction,
    we need to abort the transaction. This is because at the time of the
    tbegin, the FP state is bogus so the state stored in the checkpointed
    registers is incorrect. To fix this, we treclaim (to get the
    checkpointed GPRs) and then copy the thread_struct FP live state into
    the checkpointed state. We then trecheckpoint so that the FP state is
    correctly restored into the CPU.
    
    The copying of the FP registers from live to checkpointed is what was
    missing.
    
    This simplifies the logic slightly from the original patch.
    tm_reclaim_thread() will now always write the checkpointed FP
    state. Either the checkpointed FP state will be written as part of
    the actual treclaim (in tm.S), or it'll be a copy of the live
    state. Which one we use is based on MSR[FP] from userspace.
    
    Similarly for VMX.
    
    Fixes: dc3106690b20 ("powerpc: tm: Always use fp_state and vr_state to store live registers")
    Cc: stable@vger.kernel.org # 4.9+
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Reviewed-by: cyrilbur@gmail.com
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index d645da302bf2..baae104b16c7 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -864,6 +864,25 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 	if (!MSR_TM_SUSPENDED(mfmsr()))
 		return;
 
+	/*
+	 * If we are in a transaction and FP is off then we can't have
+	 * used FP inside that transaction. Hence the checkpointed
+	 * state is the same as the live state. We need to copy the
+	 * live state to the checkpointed state so that when the
+	 * transaction is restored, the checkpointed state is correct
+	 * and the aborted transaction sees the correct state. We use
+	 * ckpt_regs.msr here as that's what tm_reclaim will use to
+	 * determine if it's going to write the checkpointed state or
+	 * not. So either this will write the checkpointed registers,
+	 * or reclaim will. Similarly for VMX.
+	 */
+	if ((thr->ckpt_regs.msr & MSR_FP) == 0)
+		memcpy(&thr->ckfp_state, &thr->fp_state,
+		       sizeof(struct thread_fp_state));
+	if ((thr->ckpt_regs.msr & MSR_VEC) == 0)
+		memcpy(&thr->ckvr_state, &thr->vr_state,
+		       sizeof(struct thread_vr_state));
+
 	giveup_all(container_of(thr, struct task_struct, thread));
 
 	tm_reclaim(thr, thr->ckpt_regs.msr, cause);

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b99b12656f6f..d645da302bf2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -18,6 +18,7 @@
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/smp.h>

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 76f58b87dcb2..b99b12656f6f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -17,6 +17,7 @@
 #include <linux/errno.h>
 #include <linux/sched.h>
 #include <linux/sched/debug.h>
+#include <linux/sched/task.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/smp.h>

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4379a079b3c2..76f58b87dcb2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -16,6 +16,7 @@
 
 #include <linux/errno.h>
 #include <linux/sched.h>
+#include <linux/sched/debug.h>
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/smp.h>

commit b286cedd473006b33d5ae076afac509e6b2c3bf4
Merge: 522214d9be9c 9f3768e02335
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 1 10:10:16 2017 -0800

    Merge tag 'powerpc-4.11-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull more powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - an update of the disassembly code used by xmon to the latest
         versions in binutils. We've received permission from all the
         authors of the relevant binutils changes to relicense their changes
         to the relevant files from GPLv3 to GPLv2, for inclusion in Linux.
         Thanks to Peter Bergner for doing the leg work to get permission
         from everyone.
    
       - addition of the "architected" Power9 CPU table entry, allowing us
         to boot in Power9 architected mode under a hypervisor.
    
       - updates to the Power9 PMU code.
    
       - implementation of clear_bit_unlock_is_negative_byte() to optimise
         unlock_page().
    
       - Freescale updates from Scott: "Highlights include 8xx breakpoints
         and perf, t1042rdb display support, and board updates."
    
      Thanks to:
        Al Viro, Andrew Donnellan, Aneesh Kumar K.V, Balbir Singh, Douglas
        Miller, Frdric Weisbecker, Gavin Shan, Madhavan Srinivasan,
        Michael Roth, Nathan Fontenot, Naveen N. Rao, Nicholas Piggin, Peter
        Bergner, Paul E. McKenney, Rashmica Gupta, Russell Currey, Sahil
        Mehta, Stewart Smith"
    
    * tag 'powerpc-4.11-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (48 commits)
      powerpc: Remove leftover cputime_to_nsecs call causing build error
      powerpc/mm/hash: Always clear UPRT and Host Radix bits when setting up CPU
      powerpc/optprobes: Fix TOC handling in optprobes trampoline
      powerpc/pseries: Advertise Hot Plug Event support to firmware
      cxl: fix nested locking hang during EEH hotplug
      powerpc/xmon: Dump memory in CPU endian format
      powerpc/pseries: Revert 'Auto-online hotplugged memory'
      powerpc/powernv: Make PCI non-optional
      powerpc/64: Implement clear_bit_unlock_is_negative_byte()
      powerpc/powernv: Remove unused variable in pnv_pci_sriov_disable()
      powerpc/kernel: Remove error message in pcibios_setup_phb_resources()
      powerpc/mm: Fix typo in set_pte_at()
      pci/hotplug/pnv-php: Disable MSI and PCI device properly
      pci/hotplug/pnv-php: Disable surprise hotplug capability on conflicts
      pci/hotplug/pnv-php: Remove WARN_ON() in pnv_php_put_slot()
      powerpc: Add POWER9 architected mode to cputable
      powerpc/perf: use is_kernel_addr macro in perf_get_misc_flags()
      powerpc/perf: Avoid FAB_*_MATCH checks for power9
      powerpc/perf: Add restrictions to PMC5 in power9 DD1
      powerpc/perf: Use Instruction Counter value
      ...

commit 4ad8622dc54895c0072ddc919a83ea2a2f05605f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Nov 29 09:52:15 2016 +0100

    powerpc/8xx: Implement hw_breakpoint
    
    This patch implements HW breakpoint on the 8xx. The 8xx has
    capability to manage HW breakpoints, which is slightly different
    than BOOK3S:
    1/ The breakpoint match doesn't trigger a DSI exception but a
    dedicated data breakpoint exception.
    2/ The breakpoint happens after the instruction has completed,
    no need to single step or emulate the instruction,
    3/ Matched address is not set in DAR but in BAR,
    4/ DABR register doesn't exist, instead we have registers
    LCTRL1, LCTRL2 and CMPx registers,
    5/ The match on one comparator is not on a double word but
    on a single word.
    
    The patch does:
    1/ Prepare the dedicated registers in call to __set_dabr(). In order
    to emulate the double word handling of BOOK3S, comparator E is set to
    DABR address value and comparator F to address + 4. Then breakpoint 1
    is set to match comparator E or F,
    2/ Skip the singlestepping stage when compiled for CONFIG_PPC_8xx,
    3/ Implement the exception. In that exception, the matched address
    is taken from SPRN_BAR and manage as if it was from SPRN_DAR.
    4/ I/D TLB error exception routines perform a tlbie on bad TLBs. That
    tlbie triggers the breakpoint exception when performed on the
    breakpoint address. For this reason, the routine returns if the match
    is from one of those two tlbie.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 04885cec24df..2dcb65fee638 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -736,6 +736,28 @@ static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
 		mtspr(SPRN_DABRX, dabrx);
 	return 0;
 }
+#elif defined(CONFIG_PPC_8xx)
+static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
+{
+	unsigned long addr = dabr & ~HW_BRK_TYPE_DABR;
+	unsigned long lctrl1 = 0x90000000; /* compare type: equal on E & F */
+	unsigned long lctrl2 = 0x8e000002; /* watchpoint 1 on cmp E | F */
+
+	if ((dabr & HW_BRK_TYPE_RDWR) == HW_BRK_TYPE_READ)
+		lctrl1 |= 0xa0000;
+	else if ((dabr & HW_BRK_TYPE_RDWR) == HW_BRK_TYPE_WRITE)
+		lctrl1 |= 0xf0000;
+	else if ((dabr & HW_BRK_TYPE_RDWR) == 0)
+		lctrl2 = 0;
+
+	mtspr(SPRN_LCTRL2, 0);
+	mtspr(SPRN_CMPE, addr);
+	mtspr(SPRN_CMPF, addr + 4);
+	mtspr(SPRN_LCTRL1, lctrl1);
+	mtspr(SPRN_LCTRL2, lctrl2);
+
+	return 0;
+}
 #else
 static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
 {

commit f2574030b0e33263b8a1c28fa3c4fa9292283799
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jan 24 21:37:20 2017 +1100

    powerpc: Revert the initial stack protector support
    
    Unfortunately the stack protector support we merged recently only works
    on some toolchains. If the toolchain is built without glibc support
    everything works fine, but if glibc is built then it leads to a panic
    at boot.
    
    The solution is not rc5 material, so revert the support for now. This
    reverts commits:
    
    6533b7c16ee5 ("powerpc: Initial stack protector (-fstack-protector) support")
    902e06eb86cd ("powerpc/32: Change the stack protector canary value per task")
    
    Fixes: 6533b7c16ee5 ("powerpc: Initial stack protector (-fstack-protector) support")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 04885cec24df..5dd056df0baa 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -64,12 +64,6 @@
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 
-#ifdef CONFIG_CC_STACKPROTECTOR
-#include <linux/stackprotector.h>
-unsigned long __stack_chk_guard __read_mostly;
-EXPORT_SYMBOL(__stack_chk_guard);
-#endif
-
 /* Transactional Memory debug */
 #ifdef TM_DEBUG_SW
 #define TM_DEBUG(x...) printk(KERN_INFO x)

commit de399813b521ea7e38bbfb5e5b620b5e202e5783
Merge: 57ca04ab4401 c6f6634721c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 09:26:42 2016 -0800

    Merge tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for the kexec_file_load() syscall, which is a prereq for
         secure and trusted boot.
    
       - Prevent kernel execution of userspace on P9 Radix (similar to
         SMEP/PXN).
    
       - Sort the exception tables at build time, to save time at boot, and
         store them as relative offsets to save space in the kernel image &
         memory.
    
       - Allow building the kernel with thin archives, which should allow us
         to build an allyesconfig once some other fixes land.
    
       - Build fixes to allow us to correctly rebuild when changing the
         kernel endian from big to little or vice versa.
    
       - Plumbing so that we can avoid doing a full mm TLB flush on P9
         Radix.
    
       - Initial stack protector support (-fstack-protector).
    
       - Support for dumping the radix (aka. Linux) and hash page tables via
         debugfs.
    
       - Fix an oops in cxl coredump generation when cxl_get_fd() is used.
    
       - Freescale updates from Scott: "Highlights include 8xx hugepage
         support, qbman fixes/cleanup, device tree updates, and some misc
         cleanup."
    
       - Many and varied fixes and minor enhancements as always.
    
      Thanks to:
        Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anshuman
        Khandual, Anton Blanchard, Balbir Singh, Bartlomiej Zolnierkiewicz,
        Christophe Jaillet, Christophe Leroy, Denis Kirjanov, Elimar
        Riesebieter, Frederic Barrat, Gautham R. Shenoy, Geliang Tang, Geoff
        Levand, Jack Miller, Johan Hovold, Lars-Peter Clausen, Libin,
        Madhavan Srinivasan, Michael Neuling, Nathan Fontenot, Naveen N.
        Rao, Nicholas Piggin, Pan Xinhui, Peter Senna Tschudin, Rashmica
        Gupta, Rui Teng, Russell Currey, Scott Wood, Simon Guo, Suraj
        Jitindar Singh, Thiago Jung Bauermann, Tobias Klauser, Vaibhav Jain"
    
    [ And thanks to Michael, who took time off from a new baby to get this
      pull request done.   - Linus ]
    
    * tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (174 commits)
      powerpc/fsl/dts: add FMan node for t1042d4rdb
      powerpc/fsl/dts: add sg_2500_aqr105_phy4 alias on t1024rdb
      powerpc/fsl/dts: add QMan and BMan nodes on t1024
      powerpc/fsl/dts: add QMan and BMan nodes on t1023
      soc/fsl/qman: test: use DEFINE_SPINLOCK()
      powerpc/fsl-lbc: use DEFINE_SPINLOCK()
      powerpc/8xx: Implement support of hugepages
      powerpc: get hugetlbpage handling more generic
      powerpc: port 64 bits pgtable_cache to 32 bits
      powerpc/boot: Request no dynamic linker for boot wrapper
      soc/fsl/bman: Use resource_size instead of computation
      soc/fsl/qe: use builtin_platform_driver
      powerpc/fsl_pmc: use builtin_platform_driver
      powerpc/83xx/suspend: use builtin_platform_driver
      powerpc/ftrace: Fix the comments for ftrace_modify_code
      powerpc/perf: macros for power9 format encoding
      powerpc/perf: power9 raw event format encoding
      powerpc/perf: update attribute_group data structure
      powerpc/perf: factor out the event format field
      powerpc/mm/iommu, vfio/spapr: Put pages on VFIO container shutdown
      ...

commit 6533b7c16ee5712041b4e324100550e02a9a5dda
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Nov 22 11:49:30 2016 +0100

    powerpc: Initial stack protector (-fstack-protector) support
    
    Partialy copied from commit c743f38013aef ("ARM: initial stack protector
    (-fstack-protector) support")
    
    This is the very basic stuff without the changing canary upon
    task switch yet.  Just the Kconfig option and a constant canary
    value initialized at boot time.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 5b62e8c36210..9da9a42595ce 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -64,6 +64,12 @@
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 
+#ifdef CONFIG_CC_STACKPROTECTOR
+#include <linux/stackprotector.h>
+unsigned long __stack_chk_guard __read_mostly;
+EXPORT_SYMBOL(__stack_chk_guard);
+#endif
+
 /* Transactional Memory debug */
 #ifdef TM_DEBUG_SW
 #define TM_DEBUG(x...) printk(KERN_INFO x)

commit 29a969b764817c1dce819c2bc8c00a147529a5ef
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon Oct 31 13:19:39 2016 +1100

    powerpc: Revert Load Monitor Register Support
    
    Load monitored is no longer supported on POWER9 so let's remove the
    code.
    
    This reverts commit bd3ea317fddf ("powerpc: Load Monitor Register
    Support").
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ce6dc61b15b2..5b62e8c36210 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1051,14 +1051,6 @@ static inline void save_sprs(struct thread_struct *t)
 		 */
 		t->tar = mfspr(SPRN_TAR);
 	}
-
-	if (cpu_has_feature(CPU_FTR_ARCH_300)) {
-		/* Conditionally save Load Monitor registers, if enabled */
-		if (t->fscr & FSCR_LM) {
-			t->lmrr = mfspr(SPRN_LMRR);
-			t->lmser = mfspr(SPRN_LMSER);
-		}
-	}
 #endif
 }
 
@@ -1094,16 +1086,6 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 		if (old_thread->tar != new_thread->tar)
 			mtspr(SPRN_TAR, new_thread->tar);
 	}
-
-	if (cpu_has_feature(CPU_FTR_ARCH_300)) {
-		/* Conditionally restore Load Monitor registers, if enabled */
-		if (new_thread->fscr & FSCR_LM) {
-			if (old_thread->lmrr != new_thread->lmrr)
-				mtspr(SPRN_LMRR, new_thread->lmrr);
-			if (old_thread->lmser != new_thread->lmser)
-				mtspr(SPRN_LMSER, new_thread->lmser);
-		}
-	}
 #endif
 }
 

commit 2ffd04dee0dacff36c03a02434965a96da032bcd
Author: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
Date:   Fri Nov 4 17:20:40 2016 +1100

    powerpc/oops: Fix missing pr_cont()s in instruction dump
    
    Since the KERN_CONT changes, the current code in show_instructions()
    prints out a whole bunch of unnecessary newlines. Change occurrences of
    printk("\n") to pr_cont("\n"). While we're here, change all the other
    cases of printk(KERN_CONT ...) to pr_cont() as well.
    
    Signed-off-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6fe8fa481f8a..49a680d5ae37 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1215,7 +1215,7 @@ static void show_instructions(struct pt_regs *regs)
 		int instr;
 
 		if (!(i % 8))
-			printk("\n");
+			pr_cont("\n");
 
 #if !defined(CONFIG_BOOKE)
 		/* If executing with the IMMU off, adjust pc rather
@@ -1227,18 +1227,18 @@ static void show_instructions(struct pt_regs *regs)
 
 		if (!__kernel_text_address(pc) ||
 		     probe_kernel_address((unsigned int __user *)pc, instr)) {
-			printk(KERN_CONT "XXXXXXXX ");
+			pr_cont("XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)
-				printk(KERN_CONT "<%08x> ", instr);
+				pr_cont("<%08x> ", instr);
 			else
-				printk(KERN_CONT "%08x ", instr);
+				pr_cont("%08x ", instr);
 		}
 
 		pc += sizeof(int);
 	}
 
-	printk("\n");
+	pr_cont("\n");
 }
 
 struct regbit {

commit 7dae865f5878fc0c2edfb3b9165712ef33ce03df
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Nov 3 20:45:26 2016 +1100

    powerpc/oops: Fix missing pr_cont()s in show_regs()
    
    Fix up our oops output by converting continuation lines to use
    pr_cont(). Some of these are dubious, eg. printing a continuation line
    which starts with a newline, but seem to work OK for now. This whole
    function needs a rewrite in the next release.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 38f85d7a1e06..6fe8fa481f8a 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1347,29 +1347,29 @@ void show_regs(struct pt_regs * regs)
 	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
 	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
-		printk("CFAR: "REG" ", regs->orig_gpr3);
+		pr_cont("CFAR: "REG" ", regs->orig_gpr3);
 	if (trap == 0x200 || trap == 0x300 || trap == 0x600)
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
-		printk("DEAR: "REG" ESR: "REG" ", regs->dar, regs->dsisr);
+		pr_cont("DEAR: "REG" ESR: "REG" ", regs->dar, regs->dsisr);
 #else
-		printk("DAR: "REG" DSISR: %08lx ", regs->dar, regs->dsisr);
+		pr_cont("DAR: "REG" DSISR: %08lx ", regs->dar, regs->dsisr);
 #endif
 #ifdef CONFIG_PPC64
-	printk("SOFTE: %ld ", regs->softe);
+	pr_cont("SOFTE: %ld ", regs->softe);
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	if (MSR_TM_ACTIVE(regs->msr))
-		printk("\nPACATMSCRATCH: %016llx ", get_paca()->tm_scratch);
+		pr_cont("\nPACATMSCRATCH: %016llx ", get_paca()->tm_scratch);
 #endif
 
 	for (i = 0;  i < 32;  i++) {
 		if ((i % REGS_PER_LINE) == 0)
-			printk("\nGPR%02d: ", i);
-		printk(REG " ", regs->gpr[i]);
+			pr_cont("\nGPR%02d: ", i);
+		pr_cont(REG " ", regs->gpr[i]);
 		if (i == LAST_VOLATILE && !FULL_REGS(regs))
 			break;
 	}
-	printk("\n");
+	pr_cont("\n");
 #ifdef CONFIG_KALLSYMS
 	/*
 	 * Lookup NIP late so we have the best change of getting the

commit db5ba5ae6e8d5374429212de8e20933a8a0ce52e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Nov 2 22:20:47 2016 +1100

    powerpc/oops: Fix missing pr_cont()s in print_msr_bits() et. al.
    
    Since the KERN_CONT changes these are being horribly split across lines,
    for example:
    
        MSR: 8000000000009033 <
        SF,EE
        ,ME,IR
        ,DR,RI
        ,LE>
    
    So fix it by using pr_cont() where appropriate.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 621d9b23df72..38f85d7a1e06 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1282,7 +1282,7 @@ static void print_bits(unsigned long val, struct regbit *bits, const char *sep)
 
 	for (; bits->bit; ++bits)
 		if (val & bits->bit) {
-			printk("%s%s", s, bits->name);
+			pr_cont("%s%s", s, bits->name);
 			s = sep;
 		}
 }
@@ -1305,9 +1305,9 @@ static void print_tm_bits(unsigned long val)
  *   T: Transactional	(bit 34)
  */
 	if (val & (MSR_TM | MSR_TS_S | MSR_TS_T)) {
-		printk(",TM[");
+		pr_cont(",TM[");
 		print_bits(val, msr_tm_bits, "");
-		printk("]");
+		pr_cont("]");
 	}
 }
 #else
@@ -1316,10 +1316,10 @@ static void print_tm_bits(unsigned long val) {}
 
 static void print_msr_bits(unsigned long val)
 {
-	printk("<");
+	pr_cont("<");
 	print_bits(val, msr_bits, ",");
 	print_tm_bits(val);
-	printk(">");
+	pr_cont(">");
 }
 
 #ifdef CONFIG_PPC64

commit 9a1f490f358e44a1cf463ba8124ca39fcc042992
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Nov 2 22:20:46 2016 +1100

    powerpc/oops: Fix missing pr_cont()s in show_stack()
    
    Previously we got away with printing the stack trace in multiple pieces
    and it usually looked right.  But since commit 4bcc595ccd80 ("printk:
    reinstate KERN_CONT for printing continuation lines"), KERN_CONT is now
    required when printing continuation lines. Use pr_cont() as appropriate.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ce6dc61b15b2..621d9b23df72 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1900,14 +1900,14 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 			if ((ip == rth) && curr_frame >= 0) {
-				printk(" (%pS)",
+				pr_cont(" (%pS)",
 				       (void *)current->ret_stack[curr_frame].ret);
 				curr_frame--;
 			}
 #endif
 			if (firstframe)
-				printk(" (unreliable)");
-			printk("\n");
+				pr_cont(" (unreliable)");
+			pr_cont("\n");
 		}
 		firstframe = 0;
 

commit 39715bf972ed4fee18fe5409609a971fb16b1771
Author: Valentin Rothberg <valentinrothberg@gmail.com>
Date:   Wed Oct 5 07:57:26 2016 +0200

    powerpc/process: Fix CONFIG_ALIVEC typo in restore_tm_state()
    
    It should be ALTIVEC, not ALIVEC.
    
    Cyril explains: If a thread performs a transaction with altivec and then
    gets preempted for whatever reason, this bug may cause the kernel to not
    re-enable altivec when that thread runs again. This will result in an
    altivec unavailable fault, when that fault happens inside a user
    transaction the kernel has no choice but to enable altivec and doom the
    transaction.
    
    The result is that transactions using altivec may get aborted more often
    than they should.
    
    The difficulty in catching this with a selftest is my deliberate use of
    the word may above. Optimisations to avoid FPU/altivec/VSX faults mean
    that the kernel will always leave them on for 255 switches. This code
    prevents the kernel turning it off if it got to the 256th switch (and
    userspace was transactional).
    
    Fixes: dc16b553c949 ("powerpc: Always restore FPU/VEC/VSX if hardware transactional memory in use")
    Reviewed-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Valentin Rothberg <valentinrothberg@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9e7c10fe205f..ce6dc61b15b2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1012,7 +1012,7 @@ void restore_tm_state(struct pt_regs *regs)
 	/* Ensure that restore_math() will restore */
 	if (msr_diff & MSR_FP)
 		current->thread.load_fp = 1;
-#ifdef CONFIG_ALIVEC
+#ifdef CONFIG_ALTIVEC
 	if (cpu_has_feature(CPU_FTR_ALTIVEC) && msr_diff & MSR_VEC)
 		current->thread.load_vec = 1;
 #endif

commit 5d176f751ee3c6eededd984ad409bff201f436a7
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Wed Sep 14 18:02:16 2016 +1000

    powerpc: tm: Enable transactional memory (TM) lazily for userspace
    
    Currently the MSR TM bit is always set if the hardware is TM capable.
    This adds extra overhead as it means the TM SPRS (TFHAR, TEXASR and
    TFAIR) must be swapped for each process regardless of if they use TM.
    
    For processes that don't use TM the TM MSR bit can be turned off
    allowing the kernel to avoid the expensive swap of the TM registers.
    
    A TM unavailable exception will occur if a thread does use TM and the
    kernel will enable MSR_TM and leave it so for some time afterwards.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e22033005d15..9e7c10fe205f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -812,6 +812,12 @@ static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
 }
 
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+
+static inline bool tm_enabled(struct task_struct *tsk)
+{
+	return tsk && tsk->thread.regs && (tsk->thread.regs->msr & MSR_TM);
+}
+
 static void tm_reclaim_thread(struct thread_struct *thr,
 			      struct thread_info *ti, uint8_t cause)
 {
@@ -892,6 +898,9 @@ void tm_recheckpoint(struct thread_struct *thread,
 {
 	unsigned long flags;
 
+	if (!(thread->regs->msr & MSR_TM))
+		return;
+
 	/* We really can't be interrupted here as the TEXASR registers can't
 	 * change and later in the trecheckpoint code, we have a userspace R1.
 	 * So let's hard disable over this region.
@@ -924,7 +933,7 @@ static inline void tm_recheckpoint_new_task(struct task_struct *new)
 	 * unavailable later, we are unable to determine which set of FP regs
 	 * need to be restored.
 	 */
-	if (!new->thread.regs)
+	if (!tm_enabled(new))
 		return;
 
 	if (!MSR_TM_ACTIVE(new->thread.regs->msr)){
@@ -955,8 +964,16 @@ static inline void __switch_to_tm(struct task_struct *prev,
 		struct task_struct *new)
 {
 	if (cpu_has_feature(CPU_FTR_TM)) {
-		tm_enable();
-		tm_reclaim_task(prev);
+		if (tm_enabled(prev) || tm_enabled(new))
+			tm_enable();
+
+		if (tm_enabled(prev)) {
+			prev->thread.load_tm++;
+			tm_reclaim_task(prev);
+			if (!MSR_TM_ACTIVE(prev->thread.regs->msr) && prev->thread.load_tm == 0)
+				prev->thread.regs->msr &= ~MSR_TM;
+		}
+
 		tm_recheckpoint_new_task(new);
 	}
 }
@@ -1393,6 +1410,9 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	 * transitions the CPU out of TM mode.  Hence we need to call
 	 * tm_recheckpoint_new_task() (on the same task) to restore the
 	 * checkpointed state back and the TM mode.
+	 *
+	 * Can't pass dst because it isn't ready. Doesn't matter, passing
+	 * dst is only important for __switch_to()
 	 */
 	__switch_to_tm(src, src);
 
@@ -1636,8 +1656,6 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	current->thread.used_spe = 0;
 #endif /* CONFIG_SPE */
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-	if (cpu_has_feature(CPU_FTR_TM))
-		regs->msr |= MSR_TM;
 	current->thread.tm_tfhar = 0;
 	current->thread.tm_texasr = 0;
 	current->thread.tm_tfiar = 0;

commit 000ec280e3dd5c77a5227db27bfda1511e26db9a
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:25 2016 +1000

    powerpc: tm: Rename transct_(*) to ck(\1)_state
    
    Make the structures being used for checkpointed state named
    consistently with the pt_regs/ckpt_regs.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6e9a0543da12..e22033005d15 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -851,8 +851,8 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 	 *
 	 * In switching we need to maintain a 2nd register state as
 	 * oldtask->thread.ckpt_regs.  We tm_reclaim(oldproc); this saves the
-	 * checkpointed (tbegin) state in ckpt_regs and saves the transactional
-	 * (current) FPRs into oldtask->thread.transact_fpr[].
+	 * checkpointed (tbegin) state in ckpt_regs, ckfp_state and
+	 * ckvr_state
 	 *
 	 * We also context switch (save) TFHAR/TEXASR/TFIAR in here.
 	 */

commit dc3106690b20305c3df06b42456fe386dd632ac9
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:24 2016 +1000

    powerpc: tm: Always use fp_state and vr_state to store live registers
    
    There is currently an inconsistency as to how the entire CPU register
    state is saved and restored when a thread uses transactional memory
    (TM).
    
    Using transactional memory results in the CPU having duplicated
    (almost) all of its register state. This duplication results in a set
    of registers which can be considered 'live', those being currently
    modified by the instructions being executed and another set that is
    frozen at a point in time.
    
    On context switch, both sets of state have to be saved and (later)
    restored. These two states are often called a variety of different
    things. Common terms for the state which only exists after the CPU has
    entered a transaction (performed a TBEGIN instruction) in hardware are
    'transactional' or 'speculative'.
    
    Between a TBEGIN and a TEND or TABORT (or an event that causes the
    hardware to abort), regardless of the use of TSUSPEND the
    transactional state can be referred to as the live state.
    
    The second state is often to referred to as the 'checkpointed' state
    and is a duplication of the live state when the TBEGIN instruction is
    executed. This state is kept in the hardware and will be rolled back
    to on transaction failure.
    
    Currently all the registers stored in pt_regs are ALWAYS the live
    registers, that is, when a thread has transactional registers their
    values are stored in pt_regs and the checkpointed state is in
    ckpt_regs. A strange opposite is true for fp_state/vr_state. When a
    thread is non transactional fp_state/vr_state holds the live
    registers. When a thread has initiated a transaction fp_state/vr_state
    holds the checkpointed state and transact_fp/transact_vr become the
    structure which holds the live state (at this point it is a
    transactional state).
    
    This method creates confusion as to where the live state is, in some
    circumstances it requires extra work to determine where to put the
    live state and prevents the use of common functions designed (probably
    before TM) to save the live state.
    
    With this patch pt_regs, fp_state and vr_state all represent the
    same thing and the other structures [pending rename] are for
    checkpointed state.
    
    Acked-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 45b6ea069f92..6e9a0543da12 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -815,26 +815,6 @@ static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
 static void tm_reclaim_thread(struct thread_struct *thr,
 			      struct thread_info *ti, uint8_t cause)
 {
-	unsigned long msr_diff = 0;
-
-	/*
-	 * If FP/VSX registers have been already saved to the
-	 * thread_struct, move them to the transact_fp array.
-	 * We clear the TIF_RESTORE_TM bit since after the reclaim
-	 * the thread will no longer be transactional.
-	 */
-	if (test_ti_thread_flag(ti, TIF_RESTORE_TM)) {
-		msr_diff = thr->ckpt_regs.msr & ~thr->regs->msr;
-		if (msr_diff & MSR_FP)
-			memcpy(&thr->transact_fp, &thr->fp_state,
-			       sizeof(struct thread_fp_state));
-		if (msr_diff & MSR_VEC)
-			memcpy(&thr->transact_vr, &thr->vr_state,
-			       sizeof(struct thread_vr_state));
-		clear_ti_thread_flag(ti, TIF_RESTORE_TM);
-		msr_diff &= MSR_FP | MSR_VEC | MSR_VSX | MSR_FE0 | MSR_FE1;
-	}
-
 	/*
 	 * Use the current MSR TM suspended bit to track if we have
 	 * checkpointed state outstanding.
@@ -853,15 +833,9 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 	if (!MSR_TM_SUSPENDED(mfmsr()))
 		return;
 
-	tm_reclaim(thr, thr->regs->msr, cause);
+	giveup_all(container_of(thr, struct task_struct, thread));
 
-	/* Having done the reclaim, we now have the checkpointed
-	 * FP/VSX values in the registers.  These might be valid
-	 * even if we have previously called enable_kernel_fp() or
-	 * flush_fp_to_thread(), so update thr->regs->msr to
-	 * indicate their current validity.
-	 */
-	thr->regs->msr |= msr_diff;
+	tm_reclaim(thr, thr->ckpt_regs.msr, cause);
 }
 
 void tm_reclaim_current(uint8_t cause)
@@ -890,14 +864,6 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 	if (!MSR_TM_ACTIVE(thr->regs->msr))
 		goto out_and_saveregs;
 
-	/* Stash the original thread MSR, as giveup_fpu et al will
-	 * modify it.  We hold onto it to see whether the task used
-	 * FP & vector regs.  If the TIF_RESTORE_TM flag is set,
-	 * ckpt_regs.msr is already set.
-	 */
-	if (!test_ti_thread_flag(task_thread_info(tsk), TIF_RESTORE_TM))
-		thr->ckpt_regs.msr = thr->regs->msr;
-
 	TM_DEBUG("--- tm_reclaim on pid %d (NIP=%lx, "
 		 "ccr=%lx, msr=%lx, trap=%lx)\n",
 		 tsk->pid, thr->regs->nip,
@@ -955,7 +921,7 @@ static inline void tm_recheckpoint_new_task(struct task_struct *new)
 	 * If the task was using FP, we non-lazily reload both the original and
 	 * the speculative FP register states.  This is because the kernel
 	 * doesn't see if/when a TM rollback occurs, so if we take an FP
-	 * unavoidable later, we are unable to determine which set of FP regs
+	 * unavailable later, we are unable to determine which set of FP regs
 	 * need to be restored.
 	 */
 	if (!new->thread.regs)
@@ -971,35 +937,27 @@ static inline void tm_recheckpoint_new_task(struct task_struct *new)
 		 "(new->msr 0x%lx, new->origmsr 0x%lx)\n",
 		 new->pid, new->thread.regs->msr, msr);
 
-	/* This loads the checkpointed FP/VEC state, if used */
 	tm_recheckpoint(&new->thread, msr);
 
-	/* This loads the speculative FP/VEC state, if used */
-	if (msr & MSR_FP) {
-		do_load_up_transact_fpu(&new->thread);
-		new->thread.regs->msr |=
-			(MSR_FP | new->thread.fpexc_mode);
-	}
-#ifdef CONFIG_ALTIVEC
-	if (msr & MSR_VEC) {
-		do_load_up_transact_altivec(&new->thread);
-		new->thread.regs->msr |= MSR_VEC;
-	}
-#endif
-	/* We may as well turn on VSX too since all the state is restored now */
-	if (msr & MSR_VSX)
-		new->thread.regs->msr |= MSR_VSX;
+	/*
+	 * The checkpointed state has been restored but the live state has
+	 * not, ensure all the math functionality is turned off to trigger
+	 * restore_math() to reload.
+	 */
+	new->thread.regs->msr &= ~(MSR_FP | MSR_VEC | MSR_VSX);
 
 	TM_DEBUG("*** tm_recheckpoint of pid %d complete "
 		 "(kernel msr 0x%lx)\n",
 		 new->pid, mfmsr());
 }
 
-static inline void __switch_to_tm(struct task_struct *prev)
+static inline void __switch_to_tm(struct task_struct *prev,
+		struct task_struct *new)
 {
 	if (cpu_has_feature(CPU_FTR_TM)) {
 		tm_enable();
 		tm_reclaim_task(prev);
+		tm_recheckpoint_new_task(new);
 	}
 }
 
@@ -1021,6 +979,12 @@ void restore_tm_state(struct pt_regs *regs)
 {
 	unsigned long msr_diff;
 
+	/*
+	 * This is the only moment we should clear TIF_RESTORE_TM as
+	 * it is here that ckpt_regs.msr and pt_regs.msr become the same
+	 * again, anything else could lead to an incorrect ckpt_msr being
+	 * saved and therefore incorrect signal contexts.
+	 */
 	clear_thread_flag(TIF_RESTORE_TM);
 	if (!MSR_TM_ACTIVE(regs->msr))
 		return;
@@ -1042,7 +1006,7 @@ void restore_tm_state(struct pt_regs *regs)
 
 #else
 #define tm_recheckpoint_new_task(new)
-#define __switch_to_tm(prev)
+#define __switch_to_tm(prev, new)
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
 static inline void save_sprs(struct thread_struct *t)
@@ -1183,11 +1147,11 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 */
 	save_sprs(&prev->thread);
 
-	__switch_to_tm(prev);
-
 	/* Save FPU, Altivec, VSX and SPE state */
 	giveup_all(prev);
 
+	__switch_to_tm(prev, new);
+
 	/*
 	 * We can't take a PMU exception inside _switch() since there is a
 	 * window where the kernel stack SLB and the kernel stack are out
@@ -1195,8 +1159,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 */
 	hard_irq_disable();
 
-	tm_recheckpoint_new_task(new);
-
 	/*
 	 * Call restore_sprs() before calling _switch(). If we move it after
 	 * _switch() then we miss out on calling it for new tasks. The reason
@@ -1432,8 +1394,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	 * tm_recheckpoint_new_task() (on the same task) to restore the
 	 * checkpointed state back and the TM mode.
 	 */
-	__switch_to_tm(src);
-	tm_recheckpoint_new_task(src);
+	__switch_to_tm(src, src);
 
 	*dst = *src;
 

commit e909fb83d39292679118761426d7784715ad79ad
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:11 2016 +1000

    powerpc: Never giveup a reclaimed thread when enabling kernel {fp, altivec, vsx}
    
    After a thread is reclaimed from its active or suspended transactional
    state the checkpointed state exists on CPU, this state (along with the
    live/transactional state) has been saved in its entirety by the
    reclaiming process.
    
    There exists a sequence of events that would cause the kernel to call
    one of enable_kernel_fp(), enable_kernel_altivec() or
    enable_kernel_vsx() after a thread has been reclaimed. These functions
    save away any user state on the CPU so that the kernel can use the
    registers. Not only is this saving away unnecessary at this point, it
    is actually incorrect. It causes a save of the checkpointed state to
    the live structures within the thread struct thus destroying the true
    live state for that thread.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 34ee5f2e3271..45b6ea069f92 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -205,12 +205,23 @@ EXPORT_SYMBOL_GPL(flush_fp_to_thread);
 
 void enable_kernel_fp(void)
 {
+	unsigned long cpumsr;
+
 	WARN_ON(preemptible());
 
-	msr_check_and_set(MSR_FP);
+	cpumsr = msr_check_and_set(MSR_FP);
 
 	if (current->thread.regs && (current->thread.regs->msr & MSR_FP)) {
 		check_if_tm_restore_required(current);
+		/*
+		 * If a thread has already been reclaimed then the
+		 * checkpointed registers are on the CPU but have definitely
+		 * been saved by the reclaim code. Don't need to and *cannot*
+		 * giveup as this would save  to the 'live' structure not the
+		 * checkpointed structure.
+		 */
+		if(!msr_tm_active(cpumsr) && msr_tm_active(current->thread.regs->msr))
+			return;
 		__giveup_fpu(current);
 	}
 }
@@ -257,12 +268,23 @@ EXPORT_SYMBOL(giveup_altivec);
 
 void enable_kernel_altivec(void)
 {
+	unsigned long cpumsr;
+
 	WARN_ON(preemptible());
 
-	msr_check_and_set(MSR_VEC);
+	cpumsr = msr_check_and_set(MSR_VEC);
 
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC)) {
 		check_if_tm_restore_required(current);
+		/*
+		 * If a thread has already been reclaimed then the
+		 * checkpointed registers are on the CPU but have definitely
+		 * been saved by the reclaim code. Don't need to and *cannot*
+		 * giveup as this would save  to the 'live' structure not the
+		 * checkpointed structure.
+		 */
+		if(!msr_tm_active(cpumsr) && msr_tm_active(current->thread.regs->msr))
+			return;
 		__giveup_altivec(current);
 	}
 }
@@ -331,12 +353,23 @@ static void save_vsx(struct task_struct *tsk)
 
 void enable_kernel_vsx(void)
 {
+	unsigned long cpumsr;
+
 	WARN_ON(preemptible());
 
-	msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
+	cpumsr = msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
 
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX)) {
 		check_if_tm_restore_required(current);
+		/*
+		 * If a thread has already been reclaimed then the
+		 * checkpointed registers are on the CPU but have definitely
+		 * been saved by the reclaim code. Don't need to and *cannot*
+		 * giveup as this would save  to the 'live' structure not the
+		 * checkpointed structure.
+		 */
+		if(!msr_tm_active(cpumsr) && msr_tm_active(current->thread.regs->msr))
+			return;
 		if (current->thread.regs->msr & MSR_FP)
 			__giveup_fpu(current);
 		if (current->thread.regs->msr & MSR_VEC)

commit 3cee070a13b141b8eb5727c3bfa9920092f87264
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:10 2016 +1000

    powerpc: Return the new MSR from msr_check_and_set()
    
    msr_check_and_set() always performs a mfmsr() to determine if it needs
    to perform an mtmsr(), as mfmsr() can be a costly operation
    msr_check_and_set() could return the MSR now on the CPU to avoid
    callers of msr_check_and_set having to make their own mfmsr() call.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 50295670c617..34ee5f2e3271 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -111,7 +111,7 @@ static int __init enable_strict_msr_control(char *str)
 }
 early_param("ppc_strict_facility_enable", enable_strict_msr_control);
 
-void msr_check_and_set(unsigned long bits)
+unsigned long msr_check_and_set(unsigned long bits)
 {
 	unsigned long oldmsr = mfmsr();
 	unsigned long newmsr;
@@ -125,6 +125,8 @@ void msr_check_and_set(unsigned long bits)
 
 	if (oldmsr != newmsr)
 		mtmsr_isync(newmsr);
+
+	return newmsr;
 }
 
 void __msr_check_and_clear(unsigned long bits)

commit b0f16b46988fde02a1e32078f66a3059d7e53bfc
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:09 2016 +1000

    powerpc: Add check_if_tm_restore_required() to giveup_all()
    
    giveup_all() causes FPU/VMX/VSX facilities to be disabled in a threads
    MSR. If the thread performing the giveup was transactional, the kernel
    must record which facilities were in use before the giveup as the
    thread must have these facilities re-enabled on return to userspace.
    
    >From process.c:
     /*
      * This is called if we are on the way out to userspace and the
      * TIF_RESTORE_TM flag is set.  It checks if we need to reload
      * FP and/or vector state and does so if necessary.
      * If userspace is inside a transaction (whether active or
      * suspended) and FP/VMX/VSX instructions have ever been enabled
      * inside that transaction, then we have to keep them enabled
      * and keep the FP/VMX/VSX state loaded while ever the transaction
      * continues.  The reason is that if we didn't, and subsequently
      * got a FP/VMX/VSX unavailable interrupt inside a transaction,
      * we don't know whether it's the same transaction, and thus we
      * don't know which of the checkpointed state and the transactional
      * state to use.
      */
    
    Calling check_if_tm_restore_required() will set TIF_RESTORE_TM and
    save the MSR if needed.
    
    Fixes: c208505 ("powerpc: create giveup_all()")
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3846fab5d1ce..50295670c617 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -446,6 +446,7 @@ void giveup_all(struct task_struct *tsk)
 		return;
 
 	msr_check_and_set(msr_all_available);
+	check_if_tm_restore_required(tsk);
 
 #ifdef CONFIG_PPC_FPU
 	if (usermsr & MSR_FP)

commit dc16b553c949e81f37555777dc7bab66d78285a7
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:08 2016 +1000

    powerpc: Always restore FPU/VEC/VSX if hardware transactional memory in use
    
    Comment from arch/powerpc/kernel/process.c:967:
     If userspace is inside a transaction (whether active or
     suspended) and FP/VMX/VSX instructions have ever been enabled
     inside that transaction, then we have to keep them enabled
     and keep the FP/VMX/VSX state loaded while ever the transaction
     continues.  The reason is that if we didn't, and subsequently
     got a FP/VMX/VSX unavailable interrupt inside a transaction,
     we don't know whether it's the same transaction, and thus we
     don't know which of the checkpointed state and the ransactional
     state to use.
    
    restore_math() restore_fp() and restore_altivec() currently may not
    restore the registers. It doesn't appear that this is more serious
    than a performance penalty. If the math registers aren't restored the
    userspace thread will still be run with the facility disabled.
    Userspace will not be able to read invalid values. On the first access
    it will take an facility unavailable exception and the kernel will
    detected an active transaction, at which point it will abort the
    transaction. There is the possibility for a pathological case
    preventing any progress by transactions, however, transactions
    are never guaranteed to make progress.
    
    Fixes: 70fe3d9 ("powerpc: Restore FPU/VEC/VSX if previously used")
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ce8a26a0c947..3846fab5d1ce 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -89,7 +89,13 @@ static void check_if_tm_restore_required(struct task_struct *tsk)
 		set_thread_flag(TIF_RESTORE_TM);
 	}
 }
+
+static inline bool msr_tm_active(unsigned long msr)
+{
+	return MSR_TM_ACTIVE(msr);
+}
 #else
+static inline bool msr_tm_active(unsigned long msr) { return false; }
 static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
@@ -209,7 +215,7 @@ void enable_kernel_fp(void)
 EXPORT_SYMBOL(enable_kernel_fp);
 
 static int restore_fp(struct task_struct *tsk) {
-	if (tsk->thread.load_fp) {
+	if (tsk->thread.load_fp || msr_tm_active(tsk->thread.regs->msr)) {
 		load_fp_state(&current->thread.fp_state);
 		current->thread.load_fp++;
 		return 1;
@@ -279,7 +285,8 @@ EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
 
 static int restore_altivec(struct task_struct *tsk)
 {
-	if (cpu_has_feature(CPU_FTR_ALTIVEC) && tsk->thread.load_vec) {
+	if (cpu_has_feature(CPU_FTR_ALTIVEC) &&
+		(tsk->thread.load_vec || msr_tm_active(tsk->thread.regs->msr))) {
 		load_vr_state(&tsk->thread.vr_state);
 		tsk->thread.used_vr = 1;
 		tsk->thread.load_vec++;
@@ -465,7 +472,8 @@ void restore_math(struct pt_regs *regs)
 {
 	unsigned long msr;
 
-	if (!current->thread.load_fp && !loadvec(current->thread))
+	if (!msr_tm_active(regs->msr) &&
+		!current->thread.load_fp && !loadvec(current->thread))
 		return;
 
 	msr = regs->msr;
@@ -984,6 +992,13 @@ void restore_tm_state(struct pt_regs *regs)
 	msr_diff = current->thread.ckpt_regs.msr & ~regs->msr;
 	msr_diff &= MSR_FP | MSR_VEC | MSR_VSX;
 
+	/* Ensure that restore_math() will restore */
+	if (msr_diff & MSR_FP)
+		current->thread.load_fp = 1;
+#ifdef CONFIG_ALIVEC
+	if (cpu_has_feature(CPU_FTR_ALTIVEC) && msr_diff & MSR_VEC)
+		current->thread.load_vec = 1;
+#endif
 	restore_math(regs);
 
 	regs->msr |= msr_diff;

commit 0545d5436aefddff7ca417adc1a431c108403a35
Author: Daniel Axtens <dja@axtens.net>
Date:   Tue Sep 6 15:32:43 2016 +1000

    powerpc/sparse: Add more assembler prototypes
    
    Another set of things that are only called from assembler and so need
    prototypes to keep sparse happy.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9ee2623e0f67..ce8a26a0c947 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -59,6 +59,7 @@
 #include <asm/exec.h>
 #include <asm/livepatch.h>
 #include <asm/cpu_has_feature.h>
+#include <asm/asm-prototypes.h>
 
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>

commit c7a318ba868c61fc9be710a4970172d8c2eeb8b9
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Wed Aug 10 15:44:46 2016 +1000

    powerpc/ptrace: Fix coredump since ptrace TM changes
    
    Commit 8d460f6156cd ("powerpc/process: Add the function
    flush_tmregs_to_thread") added flush_tmregs_to_thread() and included
    the assumption that it would only be called for a task which is not
    current.
    
    Although this is correct for ptrace, when generating a core dump, some
    of the routines which call flush_tmregs_to_thread() are called. This
    leads to a WARNing such as:
    
      Not expecting ptrace on self: TM regs may be incorrect
      ------------[ cut here ]------------
      WARNING: CPU: 123 PID: 7727 at arch/powerpc/kernel/process.c:1088 flush_tmregs_to_thread+0x78/0x80
      CPU: 123 PID: 7727 Comm: libvirtd Not tainted 4.8.0-rc1-gcc6x-g61e8a0d #1
      task: c000000fe631b600 task.stack: c000000fe63b0000
      NIP: c00000000001a1a8 LR: c00000000001a1a4 CTR: c000000000717780
      REGS: c000000fe63b3420 TRAP: 0700   Not tainted  (4.8.0-rc1-gcc6x-g61e8a0d)
      MSR: 900000010282b033 <SF,HV,VEC,VSX,EE,FP,ME,IR,DR,RI,LE,TM[E]>  CR: 28004222  XER: 20000000
      ...
      NIP [c00000000001a1a8] flush_tmregs_to_thread+0x78/0x80
      LR [c00000000001a1a4] flush_tmregs_to_thread+0x74/0x80
      Call Trace:
       flush_tmregs_to_thread+0x74/0x80 (unreliable)
       vsr_get+0x64/0x1a0
       elf_core_dump+0x604/0x1430
       do_coredump+0x5fc/0x1200
       get_signal+0x398/0x740
       do_signal+0x54/0x2b0
       do_notify_resume+0x98/0xb0
       ret_from_except_lite+0x70/0x74
    
    So fix flush_tmregs_to_thread() to detect the case where it is called on
    current, and a transaction is active, and in that case flush the TM regs
    to the thread_struct.
    
    This patch also moves flush_tmregs_to_thread() into ptrace.c as it is
    only called from that file.
    
    Fixes: 8d460f6156cd ("powerpc/process: Add the function flush_tmregs_to_thread")
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    [mpe: Flesh out change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 58ccf86415b4..9ee2623e0f67 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1074,26 +1074,6 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 #endif
 }
 
-#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-void flush_tmregs_to_thread(struct task_struct *tsk)
-{
-	/*
-	 * Process self tracing is not yet supported through
-	 * ptrace interface. Ptrace generic code should have
-	 * prevented this from happening in the first place.
-	 * Warn once here with the message, if some how it
-	 * is attempted.
-	 */
-	WARN_ONCE(tsk == current,
-		"Not expecting ptrace on self: TM regs may be incorrect\n");
-
-	/*
-	 * If task is not current, it should have been flushed
-	 * already to it's thread_struct during __switch_to().
-	 */
-}
-#endif
-
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {

commit 8d460f6156cd55d981d109f01b82cbea8cf80e57
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu Jul 28 10:57:31 2016 +0800

    powerpc/process: Add the function flush_tmregs_to_thread
    
    This patch creates a function flush_tmregs_to_thread which
    will then be used by subsequent patches in this series. The
    function checks for self tracing ptrace interface attempts
    while in the TM context and logs appropriate warning message.
    
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9ee2623e0f67..58ccf86415b4 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1074,6 +1074,26 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 #endif
 }
 
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+void flush_tmregs_to_thread(struct task_struct *tsk)
+{
+	/*
+	 * Process self tracing is not yet supported through
+	 * ptrace interface. Ptrace generic code should have
+	 * prevented this from happening in the first place.
+	 * Warn once here with the message, if some how it
+	 * is attempted.
+	 */
+	WARN_ONCE(tsk == current,
+		"Not expecting ptrace on self: TM regs may be incorrect\n");
+
+	/*
+	 * If task is not current, it should have been flushed
+	 * already to it's thread_struct during __switch_to().
+	 */
+}
+#endif
+
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {

commit b92a226e528423b8d249dd09bb450d53361fbfcb
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sat Jul 23 14:42:40 2016 +0530

    powerpc: Move cpu_has_feature() to a separate file
    
    We plan to use jump label for cpu_has_feature(). In order to implement
    this we need to include the linux/jump_label.h in asm/cputable.h.
    
    Unfortunately if we do that it leads to an include loop. The root of the
    problem seems to be that reg.h needs cputable.h (for CPU_FTRs), and then
    cputable.h via jump_label.h eventually pulls in hw_irq.h which needs
    reg.h (for MSR_EE).
    
    So move cpu_has_feature() to a separate file on its own.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Rename to cpu_has_feature.h and flesh out change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a8cca88e972f..9ee2623e0f67 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -58,6 +58,7 @@
 #include <asm/code-patching.h>
 #include <asm/exec.h>
 #include <asm/livepatch.h>
+#include <asm/cpu_has_feature.h>
 
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>

commit b5f1bf48f2644ee7151674819f1c15c86dca87f1
Merge: 95ec77c06e8e bfa37087aa04
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Jul 15 14:57:47 2016 +1000

    Merge tag 'powerpc-4.7-5' into next
    
    Pull in the fixes we sent during 4.7, we have code we want to merge into
    next that depends on some of them.

commit 8e96a87c5431c256feb65bcfc5aec92d9f7839b6
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Jun 17 14:58:34 2016 +1000

    powerpc/tm: Always reclaim in start_thread() for exec() class syscalls
    
    Userspace can quite legitimately perform an exec() syscall with a
    suspended transaction. exec() does not return to the old process, rather
    it load a new one and starts that, the expectation therefore is that the
    new process starts not in a transaction. Currently exec() is not treated
    any differently to any other syscall which creates problems.
    
    Firstly it could allow a new process to start with a suspended
    transaction for a binary that no longer exists. This means that the
    checkpointed state won't be valid and if the suspended transaction were
    ever to be resumed and subsequently aborted (a possibility which is
    exceedingly likely as exec()ing will likely doom the transaction) the
    new process will jump to invalid state.
    
    Secondly the incorrect attempt to keep the transactional state while
    still zeroing state for the new process creates at least two TM Bad
    Things. The first triggers on the rfid to return to userspace as
    start_thread() has given the new process a 'clean' MSR but the suspend
    will still be set in the hardware MSR. The second TM Bad Thing triggers
    in __switch_to() as the processor is still transactionally suspended but
    __switch_to() wants to zero the TM sprs for the new process.
    
    This is an example of the outcome of calling exec() with a suspended
    transaction. Note the first 700 is likely the first TM bad thing
    decsribed earlier only the kernel can't report it as we've loaded
    userspace registers. c000000000009980 is the rfid in
    fast_exception_return()
    
      Bad kernel stack pointer 3fffcfa1a370 at c000000000009980
      Oops: Bad kernel stack pointer, sig: 6 [#1]
      CPU: 0 PID: 2006 Comm: tm-execed Not tainted
      NIP: c000000000009980 LR: 0000000000000000 CTR: 0000000000000000
      REGS: c00000003ffefd40 TRAP: 0700   Not tainted
      MSR: 8000000300201031 <SF,ME,IR,DR,LE,TM[SE]>  CR: 00000000  XER: 00000000
      CFAR: c0000000000098b4 SOFTE: 0
      PACATMSCRATCH: b00000010000d033
      GPR00: 0000000000000000 00003fffcfa1a370 0000000000000000 0000000000000000
      GPR04: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR08: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR12: 00003fff966611c0 0000000000000000 0000000000000000 0000000000000000
      NIP [c000000000009980] fast_exception_return+0xb0/0xb8
      LR [0000000000000000]           (null)
      Call Trace:
      Instruction dump:
      f84d0278 e9a100d8 7c7b03a6 e84101a0 7c4ff120 e8410170 7c5a03a6 e8010070
      e8410080 e8610088 e8810090 e8210078 <4c000024> 48000000 e8610178 88ed023b
    
      Kernel BUG at c000000000043e80 [verbose debug info unavailable]
      Unexpected TM Bad Thing exception at c000000000043e80 (msr 0x201033)
      Oops: Unrecoverable exception, sig: 6 [#2]
      CPU: 0 PID: 2006 Comm: tm-execed Tainted: G      D
      task: c0000000fbea6d80 ti: c00000003ffec000 task.ti: c0000000fb7ec000
      NIP: c000000000043e80 LR: c000000000015a24 CTR: 0000000000000000
      REGS: c00000003ffef7e0 TRAP: 0700   Tainted: G      D
      MSR: 8000000300201033 <SF,ME,IR,DR,RI,LE,TM[SE]>  CR: 28002828  XER: 00000000
      CFAR: c000000000015a20 SOFTE: 0
      PACATMSCRATCH: b00000010000d033
      GPR00: 0000000000000000 c00000003ffefa60 c000000000db5500 c0000000fbead000
      GPR04: 8000000300001033 2222222222222222 2222222222222222 00000000ff160000
      GPR08: 0000000000000000 800000010000d033 c0000000fb7e3ea0 c00000000fe00004
      GPR12: 0000000000002200 c00000000fe00000 0000000000000000 0000000000000000
      GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR20: 0000000000000000 0000000000000000 c0000000fbea7410 00000000ff160000
      GPR24: c0000000ffe1f600 c0000000fbea8700 c0000000fbea8700 c0000000fbead000
      GPR28: c000000000e20198 c0000000fbea6d80 c0000000fbeab680 c0000000fbea6d80
      NIP [c000000000043e80] tm_restore_sprs+0xc/0x1c
      LR [c000000000015a24] __switch_to+0x1f4/0x420
      Call Trace:
      Instruction dump:
      7c800164 4e800020 7c0022a6 f80304a8 7c0222a6 f80304b0 7c0122a6 f80304b8
      4e800020 e80304a8 7c0023a6 e80304b0 <7c0223a6> e80304b8 7c0123a6 4e800020
    
    This fixes CVE-2016-5828.
    
    Fixes: bc2a9408fa65 ("powerpc: Hook in new transactional memory code")
    Cc: stable@vger.kernel.org # v3.9+
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e2f12cbcade9..0b93893424f5 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1505,6 +1505,16 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 		current->thread.regs = regs - 1;
 	}
 
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	/*
+	 * Clear any transactional state, we're exec()ing. The cause is
+	 * not important as there will never be a recheckpoint so it's not
+	 * user visible.
+	 */
+	if (MSR_TM_SUSPENDED(mfmsr()))
+		tm_reclaim_current(0);
+#endif
+
 	memset(regs->gpr, 0, sizeof(regs->gpr));
 	regs->ctr = 0;
 	regs->link = 0;

commit bd3ea317fddfd0f2044f94bed294b90c4bc8e69e
Author: Jack Miller <jack@codezen.org>
Date:   Thu Jun 9 12:31:09 2016 +1000

    powerpc: Load Monitor Register Support
    
    This enables new registers, LMRR and LMSER, that can trigger an EBB in
    userspace code when a monitored load (via the new ldmx instruction)
    loads memory from a monitored space. This facility is controlled by a
    new FSCR bit, LM.
    
    This patch disables the FSCR LM control bit on task init and enables
    that bit when a load monitor facility unavailable exception is taken
    for using it. On context switch, this bit is then used to determine
    whether the two relevant registers are saved and restored. This is
    done lazily for performance reasons.
    
    Signed-off-by: Jack Miller <jack@codezen.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6d0a831bc7d8..ddceeb96e8fb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1017,6 +1017,14 @@ static inline void save_sprs(struct thread_struct *t)
 		 */
 		t->tar = mfspr(SPRN_TAR);
 	}
+
+	if (cpu_has_feature(CPU_FTR_ARCH_300)) {
+		/* Conditionally save Load Monitor registers, if enabled */
+		if (t->fscr & FSCR_LM) {
+			t->lmrr = mfspr(SPRN_LMRR);
+			t->lmser = mfspr(SPRN_LMSER);
+		}
+	}
 #endif
 }
 
@@ -1052,6 +1060,16 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 		if (old_thread->tar != new_thread->tar)
 			mtspr(SPRN_TAR, new_thread->tar);
 	}
+
+	if (cpu_has_feature(CPU_FTR_ARCH_300)) {
+		/* Conditionally restore Load Monitor registers, if enabled */
+		if (new_thread->fscr & FSCR_LM) {
+			if (old_thread->lmrr != new_thread->lmrr)
+				mtspr(SPRN_LMRR, new_thread->lmrr);
+			if (old_thread->lmser != new_thread->lmser)
+				mtspr(SPRN_LMSER, new_thread->lmser);
+		}
+	}
 #endif
 }
 

commit b57bd2de8c6c9aa03f1b899edd6f5582cc8b5b08
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Jun 9 12:31:08 2016 +1000

    powerpc: Improve FSCR init and context switching
    
    This fixes a few issues with FSCR init and switching.
    
    In commit 152d523e6307 ("powerpc: Create context switch helpers
    save_sprs() and restore_sprs()") we moved the setting of the FSCR
    register from inside an CPU_FTR_ARCH_207S section to inside just a
    CPU_FTR_ARCH_DSCR section. Hence we are setting FSCR on POWER6/7 where
    the FSCR doesn't exist. This is harmless but we shouldn't do it.
    
    Also, we can simplify the FSCR context switch. We don't need to go
    through the calculation involving dscr_inherit. We can just restore
    what we saved last time.
    
    We also set an initial value in INIT_THREAD, so that pid 1 which is
    cloned from that gets a sane value.
    
    Based on patch by Jack Miller.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c5c3ae2ef3c1..6d0a831bc7d8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1031,18 +1031,11 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 #ifdef CONFIG_PPC_BOOK3S_64
 	if (cpu_has_feature(CPU_FTR_DSCR)) {
 		u64 dscr = get_paca()->dscr_default;
-		u64 fscr = old_thread->fscr & ~FSCR_DSCR;
-
-		if (new_thread->dscr_inherit) {
+		if (new_thread->dscr_inherit)
 			dscr = new_thread->dscr;
-			fscr |= FSCR_DSCR;
-		}
 
 		if (old_thread->dscr != dscr)
 			mtspr(SPRN_DSCR, dscr);
-
-		if (old_thread->fscr != fscr)
-			mtspr(SPRN_FSCR, fscr);
 	}
 
 	if (cpu_has_feature(CPU_FTR_ARCH_207S)) {
@@ -1053,6 +1046,9 @@ static inline void restore_sprs(struct thread_struct *old_thread,
 		if (old_thread->ebbrr != new_thread->ebbrr)
 			mtspr(SPRN_EBBRR, new_thread->ebbrr);
 
+		if (old_thread->fscr != new_thread->fscr)
+			mtspr(SPRN_FSCR, new_thread->fscr);
+
 		if (old_thread->tar != new_thread->tar)
 			mtspr(SPRN_TAR, new_thread->tar);
 	}

commit 027dfac694fc27ef0273afb810d9b1f9da57d6e1
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jun 1 16:34:37 2016 +1000

    powerpc: Various typo fixes
    
    Signed-off-by: Andrea Gelmini <andrea.gelmini@gelma.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a2dd3b1276ff..c5c3ae2ef3c1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -802,7 +802,7 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 	 * this state.
 	 * We do this using the current MSR, rather tracking it in
 	 * some specific thread_struct bit, as it has the additional
-	 * benifit of checking for a potential TM bad thing exception.
+	 * benefit of checking for a potential TM bad thing exception.
 	 */
 	if (!MSR_TM_SUSPENDED(mfmsr()))
 		return;

commit 8eb9803723a14fd12675641b953e4ccbd86187a8
Author: Anton Blanchard <anton@samba.org>
Date:   Sun May 29 22:03:50 2016 +1000

    powerpc: Avoid load hit store in __giveup_fpu() and __giveup_altivec()
    
    In both __giveup_fpu() and __giveup_altivec() we make two modifications
    to tsk->thread.regs->msr. gcc decides to do a read/modify/write of
    each change, so we end up with a load hit store:
    
            ld      r9,264(r10)
            rldicl  r9,r9,50,1
            rotldi  r9,r9,14
            std     r9,264(r10)
    ...
            ld      r9,264(r10)
            rldicl  r9,r9,40,1
            rotldi  r9,r9,24
            std     r9,264(r10)
    
    Fix this by using a temporary.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e2f12cbcade9..a2dd3b1276ff 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -139,12 +139,16 @@ EXPORT_SYMBOL(__msr_check_and_clear);
 #ifdef CONFIG_PPC_FPU
 void __giveup_fpu(struct task_struct *tsk)
 {
+	unsigned long msr;
+
 	save_fpu(tsk);
-	tsk->thread.regs->msr &= ~MSR_FP;
+	msr = tsk->thread.regs->msr;
+	msr &= ~MSR_FP;
 #ifdef CONFIG_VSX
 	if (cpu_has_feature(CPU_FTR_VSX))
-		tsk->thread.regs->msr &= ~MSR_VSX;
+		msr &= ~MSR_VSX;
 #endif
+	tsk->thread.regs->msr = msr;
 }
 
 void giveup_fpu(struct task_struct *tsk)
@@ -219,12 +223,16 @@ static int restore_fp(struct task_struct *tsk) { return 0; }
 
 static void __giveup_altivec(struct task_struct *tsk)
 {
+	unsigned long msr;
+
 	save_altivec(tsk);
-	tsk->thread.regs->msr &= ~MSR_VEC;
+	msr = tsk->thread.regs->msr;
+	msr &= ~MSR_VEC;
 #ifdef CONFIG_VSX
 	if (cpu_has_feature(CPU_FTR_VSX))
-		tsk->thread.regs->msr &= ~MSR_VSX;
+		msr &= ~MSR_VSX;
 #endif
+	tsk->thread.regs->msr = msr;
 }
 
 void giveup_altivec(struct task_struct *tsk)

commit 5f56a5dfdb9bcb3bca03df59980d4d2f012cbb53
Author: Jiri Slaby <jslaby@suse.cz>
Date:   Fri May 20 17:00:16 2016 -0700

    exit_thread: remove empty bodies
    
    Define HAVE_EXIT_THREAD for archs which want to do something in
    exit_thread. For others, let's define exit_thread as an empty inline.
    
    This is a cleanup before we change the prototype of exit_thread to
    accept a task parameter.
    
    [akpm@linux-foundation.org: fix mips]
    Signed-off-by: Jiri Slaby <jslaby@suse.cz>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Liqin <liqin.linux@gmail.com>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hans-Christian Egtvedt <egtvedt@samfundet.no>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Steven Miao <realmz6@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ea8a28fd6f31..e2f12cbcade9 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1329,10 +1329,6 @@ void show_regs(struct pt_regs * regs)
 		show_instructions(regs);
 }
 
-void exit_thread(void)
-{
-}
-
 void flush_thread(void)
 {
 #ifdef CONFIG_HAVE_HW_BREAKPOINT

commit caca285e5ab4a7a19fede51688106ceed6fc45dd
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:26:07 2016 +1000

    powerpc/mm/radix: Use STD_MMU_64 to properly isolate hash related code
    
    We also use MMU_FTR_RADIX to branch out from code path specific to
    hash.
    
    No functionality change.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ad79816f13fe..ea8a28fd6f31 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1079,7 +1079,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	}
 #endif /* CONFIG_PPC64 */
 
-#ifdef CONFIG_PPC_BOOK3S_64
+#ifdef CONFIG_PPC_STD_MMU_64
 	batch = this_cpu_ptr(&ppc64_tlb_batch);
 	if (batch->active) {
 		current_thread_info()->local_flags |= _TLF_LAZY_MMU;
@@ -1087,7 +1087,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 			__flush_tlb_pending(batch);
 		batch->active = 0;
 	}
-#endif /* CONFIG_PPC_BOOK3S_64 */
+#endif /* CONFIG_PPC_STD_MMU_64 */
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	switch_booke_debug_regs(&new->thread.debug);
@@ -1133,7 +1133,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	last = _switch(old_thread, new_thread);
 
-#ifdef CONFIG_PPC_BOOK3S_64
+#ifdef CONFIG_PPC_STD_MMU_64
 	if (current_thread_info()->local_flags & _TLF_LAZY_MMU) {
 		current_thread_info()->local_flags &= ~_TLF_LAZY_MMU;
 		batch = this_cpu_ptr(&ppc64_tlb_batch);
@@ -1142,8 +1142,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	if (current_thread_info()->task->thread.regs)
 		restore_math(current_thread_info()->task->thread.regs);
-
-#endif /* CONFIG_PPC_BOOK3S_64 */
+#endif /* CONFIG_PPC_STD_MMU_64 */
 
 	return last;
 }
@@ -1378,6 +1377,9 @@ static void setup_ksp_vsid(struct task_struct *p, unsigned long sp)
 	unsigned long sp_vsid;
 	unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
 
+	if (radix_enabled())
+		return;
+
 	if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
 		sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_1T)
 			<< SLB_VSID_SHIFT_1T;
@@ -1926,7 +1928,8 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	 * the heap, we can put it above 1TB so it is backed by a 1TB
 	 * segment. Otherwise the heap will be in the bottom 1TB
 	 * which always uses 256MB segments and this may result in a
-	 * performance penalty.
+	 * performance penalty. We don't need to worry about radix. For
+	 * radix, mmu_highuser_ssize remains unchanged from 256MB.
 	 */
 	if (!is_32bit_task() && (mmu_highuser_ssize == MMU_SEGSIZE_1T))
 		base = max_t(unsigned long, mm->brk, 1UL << SID_SHIFT_1T);

commit 8404410b296095c78ed63f163ac5d417ff0647dd
Merge: 1050e689a63b 85baa095497f
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Apr 18 20:45:32 2016 +1000

    Merge branch 'topic/livepatch' into next
    
    Merge the support for live patching on ppc64le using mprofile-kernel.
    This branch has also been merged into the livepatching tree for v4.7.

commit 5d31a96e6c0187f2c5d7004e005fd094a1277e9e
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Mar 24 22:04:04 2016 +1100

    powerpc/livepatch: Add livepatch stack to struct thread_info
    
    In order to support live patching we need to maintain an alternate
    stack of TOC & LR values. We use the base of the stack for this, and
    store the "live patch stack pointer" in struct thread_info.
    
    Unlike the other fields of thread_info, we can not statically initialise
    that value, so it must be done at run time.
    
    This patch just adds the code to support that, it is not enabled until
    the next patch which actually adds live patch support.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Balbir Singh <bsingharora@gmail.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dccc87e8fee5..a38ce49648cb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -55,6 +55,8 @@
 #include <asm/firmware.h>
 #endif
 #include <asm/code-patching.h>
+#include <asm/livepatch.h>
+
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 
@@ -1267,13 +1269,15 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	extern void ret_from_kernel_thread(void);
 	void (*f)(void);
 	unsigned long sp = (unsigned long)task_stack_page(p) + THREAD_SIZE;
+	struct thread_info *ti = task_thread_info(p);
+
+	klp_init_thread_info(ti);
 
 	/* Copy registers */
 	sp -= sizeof(struct pt_regs);
 	childregs = (struct pt_regs *) sp;
 	if (unlikely(p->flags & PF_KTHREAD)) {
 		/* kernel thread */
-		struct thread_info *ti = (void *)task_stack_page(p);
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
 		/* function */

commit 7f92bc5694557dee4cefa90df27feec16c7b62da
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed Jan 6 11:45:51 2016 +1100

    powerpc: sparse: Include headers for __weak symbols
    
    Sometimes when sparse warns about undefined symbols, it isn't
    because they should have 'static' added, it's because they're
    overriding __weak symbols defined elsewhere, and the header has
    been missed.
    
    Fix a few of them by adding appropriate headers.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b8500b4ac7fe..4695088e7dd2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -38,6 +38,7 @@
 #include <linux/random.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/uaccess.h>
+#include <linux/elf-randomize.h>
 
 #include <asm/pgtable.h>
 #include <asm/io.h>
@@ -55,6 +56,7 @@
 #include <asm/firmware.h>
 #endif
 #include <asm/code-patching.h>
+#include <asm/exec.h>
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 

commit 01d7c2a2de47890934faba91a71d183795e4348d
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Tue Mar 8 09:08:47 2016 +1100

    powerpc/process: Fix altivec SPR not being saved
    
    In save_sprs() in process.c contains the following test:
    
            if (cpu_has_feature(cpu_has_feature(CPU_FTR_ALTIVEC)))
                    t->vrsave = mfspr(SPRN_VRSAVE);
    
    CPU feature with the mask 0x1 is CPU_FTR_COHERENT_ICACHE so the test
    is equivilent to:
    
            if (cpu_has_feature(CPU_FTR_ALTIVEC) &&
                    cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
    
    On CPUs without support for both (i.e G5) this results in vrsave not
    being saved between context switches. The vector register save/restore
    code doesn't use VRSAVE to determine which registers to save/restore,
    but the value of VRSAVE is used to determine if altivec is being used
    in several code paths.
    
    Fixes: 152d523e6307 ("powerpc: Create context switch helpers save_sprs() and restore_sprs()")
    Cc: stable@vger.kernel.org
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 612df305886b..b8500b4ac7fe 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -983,7 +983,7 @@ void restore_tm_state(struct pt_regs *regs)
 static inline void save_sprs(struct thread_struct *t)
 {
 #ifdef CONFIG_ALTIVEC
-	if (cpu_has_feature(cpu_has_feature(CPU_FTR_ALTIVEC)))
+	if (cpu_has_feature(CPU_FTR_ALTIVEC))
 		t->vrsave = mfspr(SPRN_VRSAVE);
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64

commit d5e2d00898bdfed9586472679760fc81a2ca2d02
Merge: 31e182363b39 6e669f085d59
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 15:38:41 2016 -0700

    Merge tag 'powerpc-4.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "This was delayed a day or two by some build-breakage on old toolchains
      which we've now fixed.
    
      There's two PCI commits both acked by Bjorn.
    
      There's one commit to mm/hugepage.c which is (co)authored by Kirill.
    
      Highlights:
       - Restructure Linux PTE on Book3S/64 to Radix format from Paul
         Mackerras
       - Book3s 64 MMU cleanup in preparation for Radix MMU from Aneesh
         Kumar K.V
       - Add POWER9 cputable entry from Michael Neuling
       - FPU/Altivec/VSX save/restore optimisations from Cyril Bur
       - Add support for new ftrace ABI on ppc64le from Torsten Duwe
    
      Various cleanups & minor fixes from:
       - Adam Buchbinder, Andrew Donnellan, Balbir Singh, Christophe Leroy,
         Cyril Bur, Luis Henriques, Madhavan Srinivasan, Pan Xinhui, Russell
         Currey, Sukadev Bhattiprolu, Suraj Jitindar Singh.
    
      General:
       - atomics: Allow architectures to define their own __atomic_op_*
         helpers from Boqun Feng
       - Implement atomic{, 64}_*_return_* variants and acquire/release/
         relaxed variants for (cmp)xchg from Boqun Feng
       - Add powernv_defconfig from Jeremy Kerr
       - Fix BUG_ON() reporting in real mode from Balbir Singh
       - Add xmon command to dump OPAL msglog from Andrew Donnellan
       - Add xmon command to dump process/task similar to ps(1) from Douglas
         Miller
       - Clean up memory hotplug failure paths from David Gibson
    
      pci/eeh:
       - Redesign SR-IOV on PowerNV to give absolute isolation between VFs
         from Wei Yang.
       - EEH Support for SRIOV VFs from Wei Yang and Gavin Shan.
       - PCI/IOV: Rename and export virtfn_{add, remove} from Wei Yang
       - PCI: Add pcibios_bus_add_device() weak function from Wei Yang
       - MAINTAINERS: Update EEH details and maintainership from Russell
         Currey
    
      cxl:
       - Support added to the CXL driver for running on both bare-metal and
         hypervisor systems, from Christophe Lombard and Frederic Barrat.
       - Ignore probes for virtual afu pci devices from Vaibhav Jain
    
      perf:
       - Export Power8 generic and cache events to sysfs from Sukadev
         Bhattiprolu
       - hv-24x7: Fix usage with chip events, display change in counter
         values, display domain indices in sysfs, eliminate domain suffix in
         event names, from Sukadev Bhattiprolu
    
      Freescale:
       - Updates from Scott: "Highlights include 8xx optimizations, 32-bit
         checksum optimizations, 86xx consolidation, e5500/e6500 cpu
         hotplug, more fman and other dt bits, and minor fixes/cleanup"
    
    * tag 'powerpc-4.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (179 commits)
      powerpc: Fix unrecoverable SLB miss during restore_math()
      powerpc/8xx: Fix do_mtspr_cpu6() build on older compilers
      powerpc/rcpm: Fix build break when SMP=n
      powerpc/book3e-64: Use hardcoded mttmr opcode
      powerpc/fsl/dts: Add "jedec,spi-nor" flash compatible
      powerpc/T104xRDB: add tdm riser card node to device tree
      powerpc32: PAGE_EXEC required for inittext
      powerpc/mpc85xx: Add pcsphy nodes to FManV3 device tree
      powerpc/mpc85xx: Add MDIO bus muxing support to the board device tree(s)
      powerpc/86xx: Introduce and use common dtsi
      powerpc/86xx: Update device tree
      powerpc/86xx: Move dts files to fsl directory
      powerpc/86xx: Switch to kconfig fragments approach
      powerpc/86xx: Update defconfigs
      powerpc/86xx: Consolidate common platform code
      powerpc32: Remove one insn in mulhdu
      powerpc32: small optimisation in flush_icache_range()
      powerpc: Simplify test in __dma_sync()
      powerpc32: move xxxxx_dcache_range() functions inline
      powerpc32: Remove clear_pages() and define clear_page() inline
      ...

commit bf6a4d5b75d1ea87897fe68d0e45d35a2996c678
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:51 2016 +1100

    powerpc: Add the ability to save VSX without giving it up
    
    This patch adds the ability to be able to save the VSX registers to the
    thread struct without giving up (disabling the facility) next time the
    process returns to userspace.
    
    This patch builds on a previous optimisation for the FPU and VEC registers
    in the thread copy path to avoid a possibly pointless reload of VSX state.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 14c09d25de98..d7a9df51b974 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -280,19 +280,31 @@ static inline int restore_altivec(struct task_struct *tsk) { return 0; }
 #endif /* CONFIG_ALTIVEC */
 
 #ifdef CONFIG_VSX
-void giveup_vsx(struct task_struct *tsk)
+static void __giveup_vsx(struct task_struct *tsk)
 {
-	check_if_tm_restore_required(tsk);
-
-	msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
 	if (tsk->thread.regs->msr & MSR_FP)
 		__giveup_fpu(tsk);
 	if (tsk->thread.regs->msr & MSR_VEC)
 		__giveup_altivec(tsk);
+	tsk->thread.regs->msr &= ~MSR_VSX;
+}
+
+static void giveup_vsx(struct task_struct *tsk)
+{
+	check_if_tm_restore_required(tsk);
+
+	msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
 	__giveup_vsx(tsk);
 	msr_check_and_clear(MSR_FP|MSR_VEC|MSR_VSX);
 }
-EXPORT_SYMBOL(giveup_vsx);
+
+static void save_vsx(struct task_struct *tsk)
+{
+	if (tsk->thread.regs->msr & MSR_FP)
+		save_fpu(tsk);
+	if (tsk->thread.regs->msr & MSR_VEC)
+		save_altivec(tsk);
+}
 
 void enable_kernel_vsx(void)
 {
@@ -335,6 +347,7 @@ static int restore_vsx(struct task_struct *tsk)
 }
 #else
 static inline int restore_vsx(struct task_struct *tsk) { return 0; }
+static inline void save_vsx(struct task_struct *tsk) { }
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_SPE
@@ -478,14 +491,19 @@ void save_all(struct task_struct *tsk)
 
 	msr_check_and_set(msr_all_available);
 
-	if (usermsr & MSR_FP)
-		save_fpu(tsk);
-
-	if (usermsr & MSR_VEC)
-		save_altivec(tsk);
+	/*
+	 * Saving the way the register space is in hardware, save_vsx boils
+	 * down to a save_fpu() and save_altivec()
+	 */
+	if (usermsr & MSR_VSX) {
+		save_vsx(tsk);
+	} else {
+		if (usermsr & MSR_FP)
+			save_fpu(tsk);
 
-	if (usermsr & MSR_VSX)
-		__giveup_vsx(tsk);
+		if (usermsr & MSR_VEC)
+			save_altivec(tsk);
+	}
 
 	if (usermsr & MSR_SPE)
 		__giveup_spe(tsk);

commit 6f515d842e8e1b205e54f44b9013bf14870b97a7
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:50 2016 +1100

    powerpc: Add the ability to save Altivec without giving it up
    
    This patch adds the ability to be able to save the VEC registers to the
    thread struct without giving up (disabling the facility) next time the
    process returns to userspace.
    
    This patch builds on a previous optimisation for the FPU registers in the
    thread copy path to avoid a possibly pointless reload of VEC state.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a7e5061187e8..14c09d25de98 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -213,6 +213,16 @@ static int restore_fp(struct task_struct *tsk) { return 0; }
 #ifdef CONFIG_ALTIVEC
 #define loadvec(thr) ((thr).load_vec)
 
+static void __giveup_altivec(struct task_struct *tsk)
+{
+	save_altivec(tsk);
+	tsk->thread.regs->msr &= ~MSR_VEC;
+#ifdef CONFIG_VSX
+	if (cpu_has_feature(CPU_FTR_VSX))
+		tsk->thread.regs->msr &= ~MSR_VSX;
+#endif
+}
+
 void giveup_altivec(struct task_struct *tsk)
 {
 	check_if_tm_restore_required(tsk);
@@ -472,7 +482,7 @@ void save_all(struct task_struct *tsk)
 		save_fpu(tsk);
 
 	if (usermsr & MSR_VEC)
-		__giveup_altivec(tsk);
+		save_altivec(tsk);
 
 	if (usermsr & MSR_VSX)
 		__giveup_vsx(tsk);

commit 8792468da5e12e77e76e1edf081acf0392abb331
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:49 2016 +1100

    powerpc: Add the ability to save FPU without giving it up
    
    This patch adds the ability to be able to save the FPU registers to the
    thread struct without giving up (disabling the facility) next time the
    process returns to userspace.
    
    This patch optimises the thread copy path (as a result of a fork() or
    clone()) so that the parent thread can return to userspace with hot
    registers avoiding a possibly pointless reload of FPU register state.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 29da07fb3b4a..a7e5061187e8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -133,6 +133,16 @@ void __msr_check_and_clear(unsigned long bits)
 EXPORT_SYMBOL(__msr_check_and_clear);
 
 #ifdef CONFIG_PPC_FPU
+void __giveup_fpu(struct task_struct *tsk)
+{
+	save_fpu(tsk);
+	tsk->thread.regs->msr &= ~MSR_FP;
+#ifdef CONFIG_VSX
+	if (cpu_has_feature(CPU_FTR_VSX))
+		tsk->thread.regs->msr &= ~MSR_VSX;
+#endif
+}
+
 void giveup_fpu(struct task_struct *tsk)
 {
 	check_if_tm_restore_required(tsk);
@@ -459,7 +469,7 @@ void save_all(struct task_struct *tsk)
 	msr_check_and_set(msr_all_available);
 
 	if (usermsr & MSR_FP)
-		__giveup_fpu(tsk);
+		save_fpu(tsk);
 
 	if (usermsr & MSR_VEC)
 		__giveup_altivec(tsk);

commit de2a20aa7237b45d3c14a2505804a8daa95a8f53
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:48 2016 +1100

    powerpc: Prepare for splitting giveup_{fpu, altivec, vsx} in two
    
    This prepares for the decoupling of saving {fpu,altivec,vsx} registers and
    marking {fpu,altivec,vsx} as being unused by a thread.
    
    Currently giveup_{fpu,altivec,vsx}() does both however optimisations to
    task switching can be made if these two operations are decoupled.
    save_all() will permit the saving of registers to thread structs and leave
    threads MSR with bits enabled.
    
    This patch introduces no functional change.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 55c1eb0465af..29da07fb3b4a 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -444,12 +444,41 @@ void restore_math(struct pt_regs *regs)
 	regs->msr = msr;
 }
 
+void save_all(struct task_struct *tsk)
+{
+	unsigned long usermsr;
+
+	if (!tsk->thread.regs)
+		return;
+
+	usermsr = tsk->thread.regs->msr;
+
+	if ((usermsr & msr_all_available) == 0)
+		return;
+
+	msr_check_and_set(msr_all_available);
+
+	if (usermsr & MSR_FP)
+		__giveup_fpu(tsk);
+
+	if (usermsr & MSR_VEC)
+		__giveup_altivec(tsk);
+
+	if (usermsr & MSR_VSX)
+		__giveup_vsx(tsk);
+
+	if (usermsr & MSR_SPE)
+		__giveup_spe(tsk);
+
+	msr_check_and_clear(msr_all_available);
+}
+
 void flush_all_to_thread(struct task_struct *tsk)
 {
 	if (tsk->thread.regs) {
 		preempt_disable();
 		BUG_ON(tsk != current);
-		giveup_all(tsk);
+		save_all(tsk);
 
 #ifdef CONFIG_SPE
 		if (tsk->thread.regs->msr & MSR_SPE)

commit 70fe3d980f5f14d8125869125ba9a0ea95e09c6b
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:47 2016 +1100

    powerpc: Restore FPU/VEC/VSX if previously used
    
    Currently the FPU, VEC and VSX facilities are lazily loaded. This is not
    a problem unless a process is using these facilities.
    
    Modern versions of GCC are very good at automatically vectorising code,
    new and modernised workloads make use of floating point and vector
    facilities, even the kernel makes use of vectorised memcpy.
    
    All this combined greatly increases the cost of a syscall since the
    kernel uses the facilities sometimes even in syscall fast-path making it
    increasingly common for a thread to take an *_unavailable exception soon
    after a syscall, not to mention potentially taking all three.
    
    The obvious overcompensation to this problem is to simply always load
    all the facilities on every exit to userspace. Loading up all FPU, VEC
    and VSX registers every time can be expensive and if a workload does
    avoid using them, it should not be forced to incur this penalty.
    
    An 8bit counter is used to detect if the registers have been used in the
    past and the registers are always loaded until the value wraps to back
    to zero.
    
    Several versions of the assembly in entry_64.S were tested:
    
      1. Always calling C.
      2. Performing a common case check and then calling C.
      3. A complex check in asm.
    
    After some benchmarking it was determined that avoiding C in the common
    case is a performance benefit (option 2). The full check in asm (option
    3) greatly complicated that codepath for a negligible performance gain
    and the trade-off was deemed not worth it.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    [mpe: Move load_vec in the struct to fill an existing hole, reword change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    
    fixup

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e0c3d2dc7ca3..55c1eb0465af 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -187,9 +187,22 @@ void enable_kernel_fp(void)
 	}
 }
 EXPORT_SYMBOL(enable_kernel_fp);
+
+static int restore_fp(struct task_struct *tsk) {
+	if (tsk->thread.load_fp) {
+		load_fp_state(&current->thread.fp_state);
+		current->thread.load_fp++;
+		return 1;
+	}
+	return 0;
+}
+#else
+static int restore_fp(struct task_struct *tsk) { return 0; }
 #endif /* CONFIG_PPC_FPU */
 
 #ifdef CONFIG_ALTIVEC
+#define loadvec(thr) ((thr).load_vec)
+
 void giveup_altivec(struct task_struct *tsk)
 {
 	check_if_tm_restore_required(tsk);
@@ -229,6 +242,21 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 	}
 }
 EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
+
+static int restore_altivec(struct task_struct *tsk)
+{
+	if (cpu_has_feature(CPU_FTR_ALTIVEC) && tsk->thread.load_vec) {
+		load_vr_state(&tsk->thread.vr_state);
+		tsk->thread.used_vr = 1;
+		tsk->thread.load_vec++;
+
+		return 1;
+	}
+	return 0;
+}
+#else
+#define loadvec(thr) 0
+static inline int restore_altivec(struct task_struct *tsk) { return 0; }
 #endif /* CONFIG_ALTIVEC */
 
 #ifdef CONFIG_VSX
@@ -275,6 +303,18 @@ void flush_vsx_to_thread(struct task_struct *tsk)
 	}
 }
 EXPORT_SYMBOL_GPL(flush_vsx_to_thread);
+
+static int restore_vsx(struct task_struct *tsk)
+{
+	if (cpu_has_feature(CPU_FTR_VSX)) {
+		tsk->thread.used_vsr = 1;
+		return 1;
+	}
+
+	return 0;
+}
+#else
+static inline int restore_vsx(struct task_struct *tsk) { return 0; }
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_SPE
@@ -374,6 +414,36 @@ void giveup_all(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(giveup_all);
 
+void restore_math(struct pt_regs *regs)
+{
+	unsigned long msr;
+
+	if (!current->thread.load_fp && !loadvec(current->thread))
+		return;
+
+	msr = regs->msr;
+	msr_check_and_set(msr_all_available);
+
+	/*
+	 * Only reload if the bit is not set in the user MSR, the bit BEING set
+	 * indicates that the registers are hot
+	 */
+	if ((!(msr & MSR_FP)) && restore_fp(current))
+		msr |= MSR_FP | current->thread.fpexc_mode;
+
+	if ((!(msr & MSR_VEC)) && restore_altivec(current))
+		msr |= MSR_VEC;
+
+	if ((msr & (MSR_FP | MSR_VEC)) == (MSR_FP | MSR_VEC) &&
+			restore_vsx(current)) {
+		msr |= MSR_VSX;
+	}
+
+	msr_check_and_clear(msr_all_available);
+
+	regs->msr = msr;
+}
+
 void flush_all_to_thread(struct task_struct *tsk)
 {
 	if (tsk->thread.regs) {
@@ -832,17 +902,9 @@ void restore_tm_state(struct pt_regs *regs)
 
 	msr_diff = current->thread.ckpt_regs.msr & ~regs->msr;
 	msr_diff &= MSR_FP | MSR_VEC | MSR_VSX;
-	if (msr_diff & MSR_FP) {
-		msr_check_and_set(MSR_FP);
-		load_fp_state(&current->thread.fp_state);
-		msr_check_and_clear(MSR_FP);
-		regs->msr |= current->thread.fpexc_mode;
-	}
-	if (msr_diff & MSR_VEC) {
-		msr_check_and_set(MSR_VEC);
-		load_vr_state(&current->thread.vr_state);
-		msr_check_and_clear(MSR_VEC);
-	}
+
+	restore_math(regs);
+
 	regs->msr |= msr_diff;
 }
 
@@ -1006,6 +1068,10 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		batch = this_cpu_ptr(&ppc64_tlb_batch);
 		batch->active = 1;
 	}
+
+	if (current_thread_info()->task->thread.regs)
+		restore_math(current_thread_info()->task->thread.regs);
+
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 	return last;

commit d272f6670a0bcc91fa50c234088d21cd6ac30af4
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:46 2016 +1100

    powerpc: Explicitly disable math features when copying thread
    
    Currently when threads get scheduled off they always giveup the FPU,
    Altivec (VMX) and Vector (VSX) units if they were using them. When they are
    scheduled back on a fault is then taken to enable each facility and load
    registers. As a result explicitly disabling FPU/VMX/VSX has not been
    necessary.
    
    Future changes and optimisations remove this mandatory giveup and fault
    which could cause calls such as clone() and fork() to copy threads and run
    them later with FPU/VMX/VSX enabled but no registers loaded.
    
    This patch starts the process of having MSR_{FP,VEC,VSX} mean that a
    threads registers are hot while not having MSR_{FP,VEC,VSX} means that the
    registers must be loaded. This allows for a smarter return to userspace.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dccc87e8fee5..e0c3d2dc7ca3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1307,6 +1307,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 
 		f = ret_from_fork;
 	}
+	childregs->msr &= ~(MSR_FP|MSR_VEC|MSR_VSX);
 	sp -= STACK_FRAME_OVERHEAD;
 
 	/*

commit 5ef11c35ce86b94bfb878b684de4cdaf96f54b2f
Author: Daniel Cashman <dcashman@android.com>
Date:   Fri Feb 26 15:19:37 2016 -0800

    mm: ASLR: use get_random_long()
    
    Replace calls to get_random_int() followed by a cast to (unsigned long)
    with calls to get_random_long().  Also address shifting bug which, in
    case of x86 removed entropy mask for mmap_rnd_bits values > 31 bits.
    
    Signed-off-by: Daniel Cashman <dcashman@android.com>
    Acked-by: Kees Cook <keescook@chromium.org>
    Cc: "Theodore Ts'o" <tytso@mit.edu>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Nick Kralevich <nnk@google.com>
    Cc: Jeff Vander Stoep <jeffv@google.com>
    Cc: Mark Salyzyn <salyzyn@android.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dccc87e8fee5..3c5736e52a14 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1768,9 +1768,9 @@ static inline unsigned long brk_rnd(void)
 
 	/* 8MB for 32bit, 1GB for 64bit */
 	if (is_32bit_task())
-		rnd = (long)(get_random_int() % (1<<(23-PAGE_SHIFT)));
+		rnd = (get_random_long() % (1UL<<(23-PAGE_SHIFT)));
 	else
-		rnd = (long)(get_random_int() % (1<<(30-PAGE_SHIFT)));
+		rnd = (get_random_long() % (1UL<<(30-PAGE_SHIFT)));
 
 	return rnd << PAGE_SHIFT;
 }

commit 1901d8bb45c3b82335c48e6232871f72ad10ed95
Merge: 801c0b2c4db3 7f821fc9c77a
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Dec 14 20:40:32 2015 +1100

    Merge tag 'powerpc-4.4-3' into next
    
    Merge the two TM fixes we merged in 4.4. We are about to merge selftests
    for these, and without the fixes the selftests will oops.
    
    powerpc fixes for 4.4 #2
    
     - tm: Block signal return from setting invalid MSR state from Michael Neuling
     - tm: Check for already reclaimed tasks from Michael Neuling

commit 801c0b2c4db3a33d56b3e19240df7b897e5bbfbc
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Nov 20 15:15:32 2015 +1100

    powerpc: Print MSR TM bits in oops messages
    
    Print MSR TM bits in oops messages.  This appends them to the end
    like this:
    
        MSR: 8000000502823031 <SF,VEC,VSX,FP,ME,IR,DR,LE,TM[TE]>
    
    You get the TM[] only if at least one TM MSR bit is set.  Inside the
    TM[], E means Enabled (bit 32), S means Suspended (bit 33), and T
    means Transactional (bit 34)
    
    If no bits are set, you get no TM[] output.
    
    Include rework of printbits() to handle this case.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6f76f25c3ee8..ab9373bfabda 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1033,10 +1033,12 @@ static void show_instructions(struct pt_regs *regs)
 	printk("\n");
 }
 
-static struct regbit {
+struct regbit {
 	unsigned long bit;
 	const char *name;
-} msr_bits[] = {
+};
+
+static struct regbit msr_bits[] = {
 #if defined(CONFIG_PPC64) && !defined(CONFIG_BOOKE)
 	{MSR_SF,	"SF"},
 	{MSR_HV,	"HV"},
@@ -1066,16 +1068,49 @@ static struct regbit {
 	{0,		NULL}
 };
 
-static void printbits(unsigned long val, struct regbit *bits)
+static void print_bits(unsigned long val, struct regbit *bits, const char *sep)
 {
-	const char *sep = "";
+	const char *s = "";
 
-	printk("<");
 	for (; bits->bit; ++bits)
 		if (val & bits->bit) {
-			printk("%s%s", sep, bits->name);
-			sep = ",";
+			printk("%s%s", s, bits->name);
+			s = sep;
 		}
+}
+
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+static struct regbit msr_tm_bits[] = {
+	{MSR_TS_T,	"T"},
+	{MSR_TS_S,	"S"},
+	{MSR_TM,	"E"},
+	{0,		NULL}
+};
+
+static void print_tm_bits(unsigned long val)
+{
+/*
+ * This only prints something if at least one of the TM bit is set.
+ * Inside the TM[], the output means:
+ *   E: Enabled		(bit 32)
+ *   S: Suspended	(bit 33)
+ *   T: Transactional	(bit 34)
+ */
+	if (val & (MSR_TM | MSR_TS_S | MSR_TS_T)) {
+		printk(",TM[");
+		print_bits(val, msr_tm_bits, "");
+		printk("]");
+	}
+}
+#else
+static void print_tm_bits(unsigned long val) {}
+#endif
+
+static void print_msr_bits(unsigned long val)
+{
+	printk("<");
+	print_bits(val, msr_bits, ",");
+	print_tm_bits(val);
 	printk(">");
 }
 
@@ -1100,7 +1135,7 @@ void show_regs(struct pt_regs * regs)
 	printk("REGS: %p TRAP: %04lx   %s  (%s)\n",
 	       regs, regs->trap, print_tainted(), init_utsname()->release);
 	printk("MSR: "REG" ", regs->msr);
-	printbits(regs->msr, msr_bits);
+	print_msr_bits(regs->msr);
 	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
 	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))

commit db1231dcdb4dc6cdcbdef0babe641a9162c0dc98
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Dec 9 20:11:47 2015 +1100

    powerpc: Fix DSCR inheritance over fork()
    
    Two DSCR tests have a hack in them:
    
            /*
             * XXX: Force a context switch out so that DSCR
             * current value is copied into the thread struct
             * which is required for the child to inherit the
             * changed value.
             */
            sleep(1);
    
    We should not be working around this in the testcase, it is a kernel bug.
    Fix it by copying the current DSCR to the child, instead of what we
    had in the thread struct at last context switch.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9da7b5f0c3a5..6f76f25c3ee8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1287,7 +1287,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 #ifdef CONFIG_PPC64 
 	if (cpu_has_feature(CPU_FTR_DSCR)) {
 		p->thread.dscr_inherit = current->thread.dscr_inherit;
-		p->thread.dscr = current->thread.dscr;
+		p->thread.dscr = mfspr(SPRN_DSCR);
 	}
 	if (cpu_has_feature(CPU_FTR_HAS_PPR))
 		p->thread.ppr = INIT_PPR;

commit 20dbe67062062c2a790832f0d30e73dba45df7c4
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Dec 10 20:44:39 2015 +1100

    powerpc: Call restore_sprs() before _switch()
    
    commit 152d523e6307 ("powerpc: Create context switch helpers save_sprs()
    and restore_sprs()") moved the restore of SPRs after the call to _switch().
    
    There is an issue with this approach - new tasks do not return through
    _switch(), they are set up by copy_thread() to directly return through
    ret_from_fork() or ret_from_kernel_thread(). This means restore_sprs() is
    not getting called for new tasks.
    
    Fix this by moving restore_sprs() before _switch().
    
    Fixes: 152d523e6307 ("powerpc: Create context switch helpers save_sprs() and restore_sprs()")
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1eeda3b80b65..9da7b5f0c3a5 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -971,14 +971,17 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	tm_recheckpoint_new_task(new);
 
-	last = _switch(old_thread, new_thread);
-
-	/* Need to recalculate these after calling _switch() */
-	old_thread = &last->thread;
-	new_thread = &current->thread;
-
+	/*
+	 * Call restore_sprs() before calling _switch(). If we move it after
+	 * _switch() then we miss out on calling it for new tasks. The reason
+	 * for this is we manually create a stack frame for new tasks that
+	 * directly returns through ret_from_fork() or
+	 * ret_from_kernel_thread(). See copy_thread() for details.
+	 */
 	restore_sprs(old_thread, new_thread);
 
+	last = _switch(old_thread, new_thread);
+
 #ifdef CONFIG_PPC_BOOK3S_64
 	if (current_thread_info()->local_flags & _TLF_LAZY_MMU) {
 		current_thread_info()->local_flags &= ~_TLF_LAZY_MMU;

commit d64d02ce4ebaa79bf1c026e81a956f133938af65
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Dec 10 20:04:05 2015 +1100

    powerpc: Call check_if_tm_restore_required() in enable_kernel_*()
    
    Commit a0e72cf12b1a ("powerpc: Create msr_check_and_{set,clear}()")
    removed a call to check_if_tm_restore_required() in the
    enable_kernel_*() functions. Add them back in.
    
    Fixes: a0e72cf12b1a ("powerpc: Create msr_check_and_{set,clear}()")
    Reported-by: Rashmica Gupta <rashmicy@gmail.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 58194c3f421e..1eeda3b80b65 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -181,8 +181,10 @@ void enable_kernel_fp(void)
 
 	msr_check_and_set(MSR_FP);
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_FP))
+	if (current->thread.regs && (current->thread.regs->msr & MSR_FP)) {
+		check_if_tm_restore_required(current);
 		__giveup_fpu(current);
+	}
 }
 EXPORT_SYMBOL(enable_kernel_fp);
 #endif /* CONFIG_PPC_FPU */
@@ -204,8 +206,10 @@ void enable_kernel_altivec(void)
 
 	msr_check_and_set(MSR_VEC);
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC)) {
+		check_if_tm_restore_required(current);
 		__giveup_altivec(current);
+	}
 }
 EXPORT_SYMBOL(enable_kernel_altivec);
 
@@ -249,6 +253,7 @@ void enable_kernel_vsx(void)
 	msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
 
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX)) {
+		check_if_tm_restore_required(current);
 		if (current->thread.regs->msr & MSR_FP)
 			__giveup_fpu(current);
 		if (current->thread.regs->msr & MSR_VEC)
@@ -289,8 +294,10 @@ void enable_kernel_spe(void)
 
 	msr_check_and_set(MSR_SPE);
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE))
+	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE)) {
+		check_if_tm_restore_required(current);
 		__giveup_spe(current);
+	}
 }
 EXPORT_SYMBOL(enable_kernel_spe);
 

commit d1e1cf2e38def301fde42c1a33f896f974941d7b
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:11 2015 +1100

    powerpc: clean up asm/switch_to.h
    
    Remove a bunch of unnecessary fallback functions and group
    things in a more logical way.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 49424dc1168d..58194c3f421e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -174,7 +174,6 @@ void flush_fp_to_thread(struct task_struct *tsk)
 	}
 }
 EXPORT_SYMBOL_GPL(flush_fp_to_thread);
-#endif /* CONFIG_PPC_FPU */
 
 void enable_kernel_fp(void)
 {
@@ -186,6 +185,7 @@ void enable_kernel_fp(void)
 		__giveup_fpu(current);
 }
 EXPORT_SYMBOL(enable_kernel_fp);
+#endif /* CONFIG_PPC_FPU */
 
 #ifdef CONFIG_ALTIVEC
 void giveup_altivec(struct task_struct *tsk)

commit f3d885ccba8539f62e8be3ba29ecf91687120252
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:10 2015 +1100

    powerpc: Rearrange __switch_to()
    
    Most of __switch_to() is housekeeping, TLB batching, timekeeping etc.
    Move these away from the more complex and critical context switching
    code.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7f437e7b273e..49424dc1168d 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -907,30 +907,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	WARN_ON(!irqs_disabled());
 
-	/*
-	 * We need to save SPRs before treclaim/trecheckpoint as these will
-	 * change a number of them.
-	 */
-	save_sprs(&prev->thread);
-
-	__switch_to_tm(prev);
-
-	/* Save FPU, Altivec, VSX and SPE state */
-	giveup_all(prev);
-
-#ifdef CONFIG_PPC_ADV_DEBUG_REGS
-	switch_booke_debug_regs(&new->thread.debug);
-#else
-/*
- * For PPC_BOOK3S_64, we use the hw-breakpoint interfaces that would
- * schedule DABR
- */
-#ifndef CONFIG_HAVE_HW_BREAKPOINT
-	if (unlikely(!hw_brk_match(this_cpu_ptr(&current_brk), &new->thread.hw_brk)))
-		__set_breakpoint(&new->thread.hw_brk);
-#endif /* CONFIG_HAVE_HW_BREAKPOINT */
-#endif
-
 #ifdef CONFIG_PPC64
 	/*
 	 * Collect processor utilization data per process
@@ -955,6 +931,30 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
+	switch_booke_debug_regs(&new->thread.debug);
+#else
+/*
+ * For PPC_BOOK3S_64, we use the hw-breakpoint interfaces that would
+ * schedule DABR
+ */
+#ifndef CONFIG_HAVE_HW_BREAKPOINT
+	if (unlikely(!hw_brk_match(this_cpu_ptr(&current_brk), &new->thread.hw_brk)))
+		__set_breakpoint(&new->thread.hw_brk);
+#endif /* CONFIG_HAVE_HW_BREAKPOINT */
+#endif
+
+	/*
+	 * We need to save SPRs before treclaim/trecheckpoint as these will
+	 * change a number of them.
+	 */
+	save_sprs(&prev->thread);
+
+	__switch_to_tm(prev);
+
+	/* Save FPU, Altivec, VSX and SPE state */
+	giveup_all(prev);
+
 	/*
 	 * We can't take a PMU exception inside _switch() since there is a
 	 * window where the kernel stack SLB and the kernel stack are out
@@ -970,6 +970,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	old_thread = &last->thread;
 	new_thread = &current->thread;
 
+	restore_sprs(old_thread, new_thread);
+
 #ifdef CONFIG_PPC_BOOK3S_64
 	if (current_thread_info()->local_flags & _TLF_LAZY_MMU) {
 		current_thread_info()->local_flags &= ~_TLF_LAZY_MMU;
@@ -978,8 +980,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
-	restore_sprs(old_thread, new_thread);
-
 	return last;
 }
 

commit 579e633e764e6e5f7784b74e7df3e81fe11f40de
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:09 2015 +1100

    powerpc: create flush_all_to_thread()
    
    Create a single function that flushes everything (FP, VMX, VSX, SPE).
    Doing this all at once means we only do one MSR write.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4c087b9ed2d6..7f437e7b273e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -367,6 +367,23 @@ void giveup_all(struct task_struct *tsk)
 }
 EXPORT_SYMBOL(giveup_all);
 
+void flush_all_to_thread(struct task_struct *tsk)
+{
+	if (tsk->thread.regs) {
+		preempt_disable();
+		BUG_ON(tsk != current);
+		giveup_all(tsk);
+
+#ifdef CONFIG_SPE
+		if (tsk->thread.regs->msr & MSR_SPE)
+			tsk->thread.spefscr = mfspr(SPRN_SPEFSCR);
+#endif
+
+		preempt_enable();
+	}
+}
+EXPORT_SYMBOL(flush_all_to_thread);
+
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 void do_send_trap(struct pt_regs *regs, unsigned long address,
 		  unsigned long error_code, int signal_code, int breakpt)
@@ -1137,10 +1154,7 @@ release_thread(struct task_struct *t)
  */
 int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
-	flush_fp_to_thread(src);
-	flush_altivec_to_thread(src);
-	flush_vsx_to_thread(src);
-	flush_spe_to_thread(src);
+	flush_all_to_thread(src);
 	/*
 	 * Flush TM state out so we can copy it.  __switch_to_tm() does this
 	 * flush but it removes the checkpointed state from the current CPU and

commit c208505900b232ecdc81dee54cb3a032e75d88d6
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:08 2015 +1100

    powerpc: create giveup_all()
    
    Create a single function that gives everything up (FP, VMX, VSX, SPE).
    Doing this all at once means we only do one MSR write.
    
    A context switch microbenchmark using yield():
    
    http://ozlabs.org/~anton/junkcode/context_switch2.c
    
    ./context_switch2 --test=yield --fp --altivec --vector 0 0
    
    shows an improvement of 3% on POWER8.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    [mpe: giveup_all() needs to be EXPORT_SYMBOL'ed]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9f8444b84dde..4c087b9ed2d6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -308,6 +308,65 @@ void flush_spe_to_thread(struct task_struct *tsk)
 }
 #endif /* CONFIG_SPE */
 
+static unsigned long msr_all_available;
+
+static int __init init_msr_all_available(void)
+{
+#ifdef CONFIG_PPC_FPU
+	msr_all_available |= MSR_FP;
+#endif
+#ifdef CONFIG_ALTIVEC
+	if (cpu_has_feature(CPU_FTR_ALTIVEC))
+		msr_all_available |= MSR_VEC;
+#endif
+#ifdef CONFIG_VSX
+	if (cpu_has_feature(CPU_FTR_VSX))
+		msr_all_available |= MSR_VSX;
+#endif
+#ifdef CONFIG_SPE
+	if (cpu_has_feature(CPU_FTR_SPE))
+		msr_all_available |= MSR_SPE;
+#endif
+
+	return 0;
+}
+early_initcall(init_msr_all_available);
+
+void giveup_all(struct task_struct *tsk)
+{
+	unsigned long usermsr;
+
+	if (!tsk->thread.regs)
+		return;
+
+	usermsr = tsk->thread.regs->msr;
+
+	if ((usermsr & msr_all_available) == 0)
+		return;
+
+	msr_check_and_set(msr_all_available);
+
+#ifdef CONFIG_PPC_FPU
+	if (usermsr & MSR_FP)
+		__giveup_fpu(tsk);
+#endif
+#ifdef CONFIG_ALTIVEC
+	if (usermsr & MSR_VEC)
+		__giveup_altivec(tsk);
+#endif
+#ifdef CONFIG_VSX
+	if (usermsr & MSR_VSX)
+		__giveup_vsx(tsk);
+#endif
+#ifdef CONFIG_SPE
+	if (usermsr & MSR_SPE)
+		__giveup_spe(tsk);
+#endif
+
+	msr_check_and_clear(msr_all_available);
+}
+EXPORT_SYMBOL(giveup_all);
+
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 void do_send_trap(struct pt_regs *regs, unsigned long address,
 		  unsigned long error_code, int signal_code, int breakpt)
@@ -839,21 +898,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	__switch_to_tm(prev);
 
-	if (prev->thread.regs && (prev->thread.regs->msr & MSR_FP))
-		giveup_fpu(prev);
-#ifdef CONFIG_ALTIVEC
-	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))
-		giveup_altivec(prev);
-#endif /* CONFIG_ALTIVEC */
-#ifdef CONFIG_VSX
-	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VSX))
-		/* VMX and FPU registers are already save here */
-		__giveup_vsx(prev);
-#endif /* CONFIG_VSX */
-#ifdef CONFIG_SPE
-	if ((prev->thread.regs && (prev->thread.regs->msr & MSR_SPE)))
-		giveup_spe(prev);
-#endif /* CONFIG_SPE */
+	/* Save FPU, Altivec, VSX and SPE state */
+	giveup_all(prev);
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	switch_booke_debug_regs(&new->thread.debug);

commit 1f2e25b2d552cade43eacb2edc4e7f01c1cfecb3
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:07 2015 +1100

    powerpc: Remove fp_enable() and vec_enable(), use msr_check_and_{set, clear}()
    
    More consolidation of our MSR available bit handling.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1eafceefeac9..9f8444b84dde 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -732,13 +732,15 @@ void restore_tm_state(struct pt_regs *regs)
 	msr_diff = current->thread.ckpt_regs.msr & ~regs->msr;
 	msr_diff &= MSR_FP | MSR_VEC | MSR_VSX;
 	if (msr_diff & MSR_FP) {
-		fp_enable();
+		msr_check_and_set(MSR_FP);
 		load_fp_state(&current->thread.fp_state);
+		msr_check_and_clear(MSR_FP);
 		regs->msr |= current->thread.fpexc_mode;
 	}
 	if (msr_diff & MSR_VEC) {
-		vec_enable();
+		msr_check_and_set(MSR_VEC);
 		load_vr_state(&current->thread.vr_state);
+		msr_check_and_clear(MSR_VEC);
 	}
 	regs->msr |= msr_diff;
 }

commit 3eb5d5888dc68c9b187998ca4249b8b9fa481eeb
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:06 2015 +1100

    powerpc: Add ppc_strict_facility_enable boot option
    
    Add a boot option that strictly manages the MSR unavailable bits.
    This catches kernel uses of FP/Altivec/SPE that would otherwise
    corrupt user state.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 5cdd35c0b026..1eafceefeac9 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -87,7 +87,19 @@ static void check_if_tm_restore_required(struct task_struct *tsk)
 static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
-static void msr_check_and_set(unsigned long bits)
+bool strict_msr_control;
+EXPORT_SYMBOL(strict_msr_control);
+
+static int __init enable_strict_msr_control(char *str)
+{
+	strict_msr_control = true;
+	pr_info("Enabling strict facility control\n");
+
+	return 0;
+}
+early_param("ppc_strict_facility_enable", enable_strict_msr_control);
+
+void msr_check_and_set(unsigned long bits)
 {
 	unsigned long oldmsr = mfmsr();
 	unsigned long newmsr;
@@ -103,7 +115,7 @@ static void msr_check_and_set(unsigned long bits)
 		mtmsr_isync(newmsr);
 }
 
-static void msr_check_and_clear(unsigned long bits)
+void __msr_check_and_clear(unsigned long bits)
 {
 	unsigned long oldmsr = mfmsr();
 	unsigned long newmsr;
@@ -118,6 +130,7 @@ static void msr_check_and_clear(unsigned long bits)
 	if (oldmsr != newmsr)
 		mtmsr_isync(newmsr);
 }
+EXPORT_SYMBOL(__msr_check_and_clear);
 
 #ifdef CONFIG_PPC_FPU
 void giveup_fpu(struct task_struct *tsk)

commit a0e72cf12b1a1f159b6822ed2e1e41893d996fc7
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:04 2015 +1100

    powerpc: Create msr_check_and_{set,clear}()
    
    Create helper functions to set and clear MSR bits after first
    checking if they are already set. Grouping them will make it
    easy to avoid the MSR writes in a subsequent optimisation.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0cb627662ded..5cdd35c0b026 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -87,23 +87,46 @@ static void check_if_tm_restore_required(struct task_struct *tsk)
 static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
-#ifdef CONFIG_PPC_FPU
-void giveup_fpu(struct task_struct *tsk)
+static void msr_check_and_set(unsigned long bits)
 {
-	u64 oldmsr = mfmsr();
-	u64 newmsr;
+	unsigned long oldmsr = mfmsr();
+	unsigned long newmsr;
 
-	check_if_tm_restore_required(tsk);
+	newmsr = oldmsr | bits;
 
-	newmsr = oldmsr | MSR_FP;
 #ifdef CONFIG_VSX
-	if (cpu_has_feature(CPU_FTR_VSX))
+	if (cpu_has_feature(CPU_FTR_VSX) && (bits & MSR_FP))
 		newmsr |= MSR_VSX;
 #endif
+
 	if (oldmsr != newmsr)
 		mtmsr_isync(newmsr);
+}
+
+static void msr_check_and_clear(unsigned long bits)
+{
+	unsigned long oldmsr = mfmsr();
+	unsigned long newmsr;
+
+	newmsr = oldmsr & ~bits;
+
+#ifdef CONFIG_VSX
+	if (cpu_has_feature(CPU_FTR_VSX) && (bits & MSR_FP))
+		newmsr &= ~MSR_VSX;
+#endif
 
+	if (oldmsr != newmsr)
+		mtmsr_isync(newmsr);
+}
+
+#ifdef CONFIG_PPC_FPU
+void giveup_fpu(struct task_struct *tsk)
+{
+	check_if_tm_restore_required(tsk);
+
+	msr_check_and_set(MSR_FP);
 	__giveup_fpu(tsk);
+	msr_check_and_clear(MSR_FP);
 }
 EXPORT_SYMBOL(giveup_fpu);
 
@@ -144,30 +167,21 @@ void enable_kernel_fp(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_FP)) {
-		giveup_fpu(current);
-	} else {
-		u64 oldmsr = mfmsr();
+	msr_check_and_set(MSR_FP);
 
-		if (!(oldmsr & MSR_FP))
-			mtmsr_isync(oldmsr | MSR_FP);
-	}
+	if (current->thread.regs && (current->thread.regs->msr & MSR_FP))
+		__giveup_fpu(current);
 }
 EXPORT_SYMBOL(enable_kernel_fp);
 
 #ifdef CONFIG_ALTIVEC
 void giveup_altivec(struct task_struct *tsk)
 {
-	u64 oldmsr = mfmsr();
-	u64 newmsr;
-
 	check_if_tm_restore_required(tsk);
 
-	newmsr = oldmsr | MSR_VEC;
-	if (oldmsr != newmsr)
-		mtmsr_isync(newmsr);
-
+	msr_check_and_set(MSR_VEC);
 	__giveup_altivec(tsk);
+	msr_check_and_clear(MSR_VEC);
 }
 EXPORT_SYMBOL(giveup_altivec);
 
@@ -175,14 +189,10 @@ void enable_kernel_altivec(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC)) {
-		giveup_altivec(current);
-	} else {
-		u64 oldmsr = mfmsr();
+	msr_check_and_set(MSR_VEC);
 
-		if (!(oldmsr & MSR_VEC))
-			mtmsr_isync(oldmsr | MSR_VEC);
-	}
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
+		__giveup_altivec(current);
 }
 EXPORT_SYMBOL(enable_kernel_altivec);
 
@@ -207,20 +217,15 @@ EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
 #ifdef CONFIG_VSX
 void giveup_vsx(struct task_struct *tsk)
 {
-	u64 oldmsr = mfmsr();
-	u64 newmsr;
-
 	check_if_tm_restore_required(tsk);
 
-	newmsr = oldmsr | (MSR_FP|MSR_VEC|MSR_VSX);
-	if (oldmsr != newmsr)
-		mtmsr_isync(newmsr);
-
+	msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
 	if (tsk->thread.regs->msr & MSR_FP)
 		__giveup_fpu(tsk);
 	if (tsk->thread.regs->msr & MSR_VEC)
 		__giveup_altivec(tsk);
 	__giveup_vsx(tsk);
+	msr_check_and_clear(MSR_FP|MSR_VEC|MSR_VSX);
 }
 EXPORT_SYMBOL(giveup_vsx);
 
@@ -228,13 +233,14 @@ void enable_kernel_vsx(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX)) {
-		giveup_vsx(current);
-	} else {
-		u64 oldmsr = mfmsr();
+	msr_check_and_set(MSR_FP|MSR_VEC|MSR_VSX);
 
-		if (!(oldmsr & MSR_VSX))
-			mtmsr_isync(oldmsr | MSR_VSX);
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX)) {
+		if (current->thread.regs->msr & MSR_FP)
+			__giveup_fpu(current);
+		if (current->thread.regs->msr & MSR_VEC)
+			__giveup_altivec(current);
+		__giveup_vsx(current);
 	}
 }
 EXPORT_SYMBOL(enable_kernel_vsx);
@@ -256,16 +262,11 @@ EXPORT_SYMBOL_GPL(flush_vsx_to_thread);
 #ifdef CONFIG_SPE
 void giveup_spe(struct task_struct *tsk)
 {
-	u64 oldmsr = mfmsr();
-	u64 newmsr;
-
 	check_if_tm_restore_required(tsk);
 
-	newmsr = oldmsr | MSR_SPE;
-	if (oldmsr != newmsr)
-		mtmsr_isync(newmsr);
-
+	msr_check_and_set(MSR_SPE);
 	__giveup_spe(tsk);
+	msr_check_and_clear(MSR_SPE);
 }
 EXPORT_SYMBOL(giveup_spe);
 
@@ -273,14 +274,10 @@ void enable_kernel_spe(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE)) {
-		giveup_spe(current);
-	} else {
-		u64 oldmsr = mfmsr();
+	msr_check_and_set(MSR_SPE);
 
-		if (!(oldmsr & MSR_SPE))
-			mtmsr_isync(oldmsr | MSR_SPE);
-	}
+	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE))
+		__giveup_spe(current);
 }
 EXPORT_SYMBOL(enable_kernel_spe);
 

commit a7d623d4d053ccb0cdfad210bced2ec25ddf69a2
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:02 2015 +1100

    powerpc: Move part of giveup_vsx into c
    
    Move the MSR modification into c. Removing it from the assembly
    function will allow us to avoid costly MSR writes by batching them
    up.
    
    Check the FP and VMX bits before calling the relevant giveup_*()
    function. This makes giveup_vsx() and flush_vsx_to_thread() perform
    more like their sister functions, and allows us to use
    flush_vsx_to_thread() in the signal code.
    
    Move the check_if_tm_restore_required() check in.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6bcf82bed610..0cb627662ded 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -205,6 +205,25 @@ EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
 #endif /* CONFIG_ALTIVEC */
 
 #ifdef CONFIG_VSX
+void giveup_vsx(struct task_struct *tsk)
+{
+	u64 oldmsr = mfmsr();
+	u64 newmsr;
+
+	check_if_tm_restore_required(tsk);
+
+	newmsr = oldmsr | (MSR_FP|MSR_VEC|MSR_VSX);
+	if (oldmsr != newmsr)
+		mtmsr_isync(newmsr);
+
+	if (tsk->thread.regs->msr & MSR_FP)
+		__giveup_fpu(tsk);
+	if (tsk->thread.regs->msr & MSR_VEC)
+		__giveup_altivec(tsk);
+	__giveup_vsx(tsk);
+}
+EXPORT_SYMBOL(giveup_vsx);
+
 void enable_kernel_vsx(void)
 {
 	WARN_ON(preemptible());
@@ -220,15 +239,6 @@ void enable_kernel_vsx(void)
 }
 EXPORT_SYMBOL(enable_kernel_vsx);
 
-void giveup_vsx(struct task_struct *tsk)
-{
-	check_if_tm_restore_required(tsk);
-	giveup_fpu(tsk);
-	giveup_altivec(tsk);
-	__giveup_vsx(tsk);
-}
-EXPORT_SYMBOL(giveup_vsx);
-
 void flush_vsx_to_thread(struct task_struct *tsk)
 {
 	if (tsk->thread.regs) {

commit 98da581e0846f6d932a4bc46a55458140e20478a
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:44:01 2015 +1100

    powerpc: Move part of giveup_fpu,altivec,spe into c
    
    Move the MSR modification into new c functions. Removing it from
    the low level functions will allow us to avoid costly MSR writes
    by batching them up.
    
    Move the check_if_tm_restore_required() check into these new functions.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 5bf8ec2597d4..6bcf82bed610 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -88,6 +88,25 @@ static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
 #ifdef CONFIG_PPC_FPU
+void giveup_fpu(struct task_struct *tsk)
+{
+	u64 oldmsr = mfmsr();
+	u64 newmsr;
+
+	check_if_tm_restore_required(tsk);
+
+	newmsr = oldmsr | MSR_FP;
+#ifdef CONFIG_VSX
+	if (cpu_has_feature(CPU_FTR_VSX))
+		newmsr |= MSR_VSX;
+#endif
+	if (oldmsr != newmsr)
+		mtmsr_isync(newmsr);
+
+	__giveup_fpu(tsk);
+}
+EXPORT_SYMBOL(giveup_fpu);
+
 /*
  * Make sure the floating-point register state in the
  * the thread_struct is up to date for task tsk.
@@ -113,7 +132,6 @@ void flush_fp_to_thread(struct task_struct *tsk)
 			 * to still have its FP state in the CPU registers.
 			 */
 			BUG_ON(tsk != current);
-			check_if_tm_restore_required(tsk);
 			giveup_fpu(tsk);
 		}
 		preempt_enable();
@@ -127,7 +145,6 @@ void enable_kernel_fp(void)
 	WARN_ON(preemptible());
 
 	if (current->thread.regs && (current->thread.regs->msr & MSR_FP)) {
-		check_if_tm_restore_required(current);
 		giveup_fpu(current);
 	} else {
 		u64 oldmsr = mfmsr();
@@ -139,12 +156,26 @@ void enable_kernel_fp(void)
 EXPORT_SYMBOL(enable_kernel_fp);
 
 #ifdef CONFIG_ALTIVEC
+void giveup_altivec(struct task_struct *tsk)
+{
+	u64 oldmsr = mfmsr();
+	u64 newmsr;
+
+	check_if_tm_restore_required(tsk);
+
+	newmsr = oldmsr | MSR_VEC;
+	if (oldmsr != newmsr)
+		mtmsr_isync(newmsr);
+
+	__giveup_altivec(tsk);
+}
+EXPORT_SYMBOL(giveup_altivec);
+
 void enable_kernel_altivec(void)
 {
 	WARN_ON(preemptible());
 
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC)) {
-		check_if_tm_restore_required(current);
 		giveup_altivec(current);
 	} else {
 		u64 oldmsr = mfmsr();
@@ -165,7 +196,6 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 		preempt_disable();
 		if (tsk->thread.regs->msr & MSR_VEC) {
 			BUG_ON(tsk != current);
-			check_if_tm_restore_required(tsk);
 			giveup_altivec(tsk);
 		}
 		preempt_enable();
@@ -214,6 +244,20 @@ EXPORT_SYMBOL_GPL(flush_vsx_to_thread);
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_SPE
+void giveup_spe(struct task_struct *tsk)
+{
+	u64 oldmsr = mfmsr();
+	u64 newmsr;
+
+	check_if_tm_restore_required(tsk);
+
+	newmsr = oldmsr | MSR_SPE;
+	if (oldmsr != newmsr)
+		mtmsr_isync(newmsr);
+
+	__giveup_spe(tsk);
+}
+EXPORT_SYMBOL(giveup_spe);
 
 void enable_kernel_spe(void)
 {

commit 611b0e5c19963374175b39f42117b03ee7573228
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:59 2015 +1100

    powerpc: Create mtmsrd_isync()
    
    mtmsrd_isync() will do an mtmsrd followed by an isync on older
    processors. On newer processors we avoid the isync via a feature fixup.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ef64219548d5..5bf8ec2597d4 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -130,7 +130,10 @@ void enable_kernel_fp(void)
 		check_if_tm_restore_required(current);
 		giveup_fpu(current);
 	} else {
-		giveup_fpu(NULL);	/* just enables FP for kernel */
+		u64 oldmsr = mfmsr();
+
+		if (!(oldmsr & MSR_FP))
+			mtmsr_isync(oldmsr | MSR_FP);
 	}
 }
 EXPORT_SYMBOL(enable_kernel_fp);
@@ -144,7 +147,10 @@ void enable_kernel_altivec(void)
 		check_if_tm_restore_required(current);
 		giveup_altivec(current);
 	} else {
-		giveup_altivec_notask();
+		u64 oldmsr = mfmsr();
+
+		if (!(oldmsr & MSR_VEC))
+			mtmsr_isync(oldmsr | MSR_VEC);
 	}
 }
 EXPORT_SYMBOL(enable_kernel_altivec);
@@ -173,10 +179,14 @@ void enable_kernel_vsx(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX))
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX)) {
 		giveup_vsx(current);
-	else
-		giveup_vsx(NULL);	/* just enable vsx for kernel - force */
+	} else {
+		u64 oldmsr = mfmsr();
+
+		if (!(oldmsr & MSR_VSX))
+			mtmsr_isync(oldmsr | MSR_VSX);
+	}
 }
 EXPORT_SYMBOL(enable_kernel_vsx);
 
@@ -209,10 +219,14 @@ void enable_kernel_spe(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE))
+	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE)) {
 		giveup_spe(current);
-	else
-		giveup_spe(NULL);	/* just enable SPE for kernel - force */
+	} else {
+		u64 oldmsr = mfmsr();
+
+		if (!(oldmsr & MSR_SPE))
+			mtmsr_isync(oldmsr | MSR_SPE);
+	}
 }
 EXPORT_SYMBOL(enable_kernel_spe);
 

commit b86fd2bd03021ce906bfa0c1456ec38329e31b30
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:58 2015 +1100

    powerpc: Simplify TM restore checks
    
    Instead of having multiple giveup_*_maybe_transactional() functions,
    separate out the TM check into a new function called
    check_if_tm_restore_required().
    
    This will make it easier to optimise the giveup_*() functions in a
    subsequent patch.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e098f4315643..ef64219548d5 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -68,7 +68,7 @@
 extern unsigned long _get_SP(void);
 
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-void giveup_fpu_maybe_transactional(struct task_struct *tsk)
+static void check_if_tm_restore_required(struct task_struct *tsk)
 {
 	/*
 	 * If we are saving the current thread's registers, and the
@@ -82,31 +82,9 @@ void giveup_fpu_maybe_transactional(struct task_struct *tsk)
 		tsk->thread.ckpt_regs.msr = tsk->thread.regs->msr;
 		set_thread_flag(TIF_RESTORE_TM);
 	}
-
-	giveup_fpu(tsk);
-}
-
-void giveup_altivec_maybe_transactional(struct task_struct *tsk)
-{
-	/*
-	 * If we are saving the current thread's registers, and the
-	 * thread is in a transactional state, set the TIF_RESTORE_TM
-	 * bit so that we know to restore the registers before
-	 * returning to userspace.
-	 */
-	if (tsk == current && tsk->thread.regs &&
-	    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
-	    !test_thread_flag(TIF_RESTORE_TM)) {
-		tsk->thread.ckpt_regs.msr = tsk->thread.regs->msr;
-		set_thread_flag(TIF_RESTORE_TM);
-	}
-
-	giveup_altivec(tsk);
 }
-
 #else
-#define giveup_fpu_maybe_transactional(tsk)	giveup_fpu(tsk)
-#define giveup_altivec_maybe_transactional(tsk)	giveup_altivec(tsk)
+static inline void check_if_tm_restore_required(struct task_struct *tsk) { }
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
 #ifdef CONFIG_PPC_FPU
@@ -135,7 +113,8 @@ void flush_fp_to_thread(struct task_struct *tsk)
 			 * to still have its FP state in the CPU registers.
 			 */
 			BUG_ON(tsk != current);
-			giveup_fpu_maybe_transactional(tsk);
+			check_if_tm_restore_required(tsk);
+			giveup_fpu(tsk);
 		}
 		preempt_enable();
 	}
@@ -147,10 +126,12 @@ void enable_kernel_fp(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_FP))
-		giveup_fpu_maybe_transactional(current);
-	else
+	if (current->thread.regs && (current->thread.regs->msr & MSR_FP)) {
+		check_if_tm_restore_required(current);
+		giveup_fpu(current);
+	} else {
 		giveup_fpu(NULL);	/* just enables FP for kernel */
+	}
 }
 EXPORT_SYMBOL(enable_kernel_fp);
 
@@ -159,10 +140,12 @@ void enable_kernel_altivec(void)
 {
 	WARN_ON(preemptible());
 
-	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
-		giveup_altivec_maybe_transactional(current);
-	else
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC)) {
+		check_if_tm_restore_required(current);
+		giveup_altivec(current);
+	} else {
 		giveup_altivec_notask();
+	}
 }
 EXPORT_SYMBOL(enable_kernel_altivec);
 
@@ -176,7 +159,8 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 		preempt_disable();
 		if (tsk->thread.regs->msr & MSR_VEC) {
 			BUG_ON(tsk != current);
-			giveup_altivec_maybe_transactional(tsk);
+			check_if_tm_restore_required(tsk);
+			giveup_altivec(tsk);
 		}
 		preempt_enable();
 	}
@@ -198,8 +182,9 @@ EXPORT_SYMBOL(enable_kernel_vsx);
 
 void giveup_vsx(struct task_struct *tsk)
 {
-	giveup_fpu_maybe_transactional(tsk);
-	giveup_altivec_maybe_transactional(tsk);
+	check_if_tm_restore_required(tsk);
+	giveup_fpu(tsk);
+	giveup_altivec(tsk);
 	__giveup_vsx(tsk);
 }
 EXPORT_SYMBOL(giveup_vsx);

commit af1bbc3dd3d501d27da72e1764afe5f5b0d3882d
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:57 2015 +1100

    powerpc: Remove UP only lazy floating point and vector optimisations
    
    The UP only lazy floating point and vector optimisations were written
    back when SMP was not common, and neither glibc nor gcc used vector
    instructions. Now SMP is very common, glibc aggressively uses vector
    instructions and gcc autovectorises.
    
    We want to add new optimisations that apply to both UP and SMP, but
    in preparation for that remove these UP only optimisations.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3aabed4a60a9..e098f4315643 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -67,13 +67,6 @@
 
 extern unsigned long _get_SP(void);
 
-#ifndef CONFIG_SMP
-struct task_struct *last_task_used_math = NULL;
-struct task_struct *last_task_used_altivec = NULL;
-struct task_struct *last_task_used_vsx = NULL;
-struct task_struct *last_task_used_spe = NULL;
-#endif
-
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 void giveup_fpu_maybe_transactional(struct task_struct *tsk)
 {
@@ -134,16 +127,14 @@ void flush_fp_to_thread(struct task_struct *tsk)
 		 */
 		preempt_disable();
 		if (tsk->thread.regs->msr & MSR_FP) {
-#ifdef CONFIG_SMP
 			/*
 			 * This should only ever be called for current or
 			 * for a stopped child process.  Since we save away
-			 * the FP register state on context switch on SMP,
+			 * the FP register state on context switch,
 			 * there is something wrong if a stopped child appears
 			 * to still have its FP state in the CPU registers.
 			 */
 			BUG_ON(tsk != current);
-#endif
 			giveup_fpu_maybe_transactional(tsk);
 		}
 		preempt_enable();
@@ -156,14 +147,10 @@ void enable_kernel_fp(void)
 {
 	WARN_ON(preemptible());
 
-#ifdef CONFIG_SMP
 	if (current->thread.regs && (current->thread.regs->msr & MSR_FP))
 		giveup_fpu_maybe_transactional(current);
 	else
 		giveup_fpu(NULL);	/* just enables FP for kernel */
-#else
-	giveup_fpu_maybe_transactional(last_task_used_math);
-#endif /* CONFIG_SMP */
 }
 EXPORT_SYMBOL(enable_kernel_fp);
 
@@ -172,14 +159,10 @@ void enable_kernel_altivec(void)
 {
 	WARN_ON(preemptible());
 
-#ifdef CONFIG_SMP
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
 		giveup_altivec_maybe_transactional(current);
 	else
 		giveup_altivec_notask();
-#else
-	giveup_altivec_maybe_transactional(last_task_used_altivec);
-#endif /* CONFIG_SMP */
 }
 EXPORT_SYMBOL(enable_kernel_altivec);
 
@@ -192,9 +175,7 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 	if (tsk->thread.regs) {
 		preempt_disable();
 		if (tsk->thread.regs->msr & MSR_VEC) {
-#ifdef CONFIG_SMP
 			BUG_ON(tsk != current);
-#endif
 			giveup_altivec_maybe_transactional(tsk);
 		}
 		preempt_enable();
@@ -208,14 +189,10 @@ void enable_kernel_vsx(void)
 {
 	WARN_ON(preemptible());
 
-#ifdef CONFIG_SMP
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX))
 		giveup_vsx(current);
 	else
 		giveup_vsx(NULL);	/* just enable vsx for kernel - force */
-#else
-	giveup_vsx(last_task_used_vsx);
-#endif /* CONFIG_SMP */
 }
 EXPORT_SYMBOL(enable_kernel_vsx);
 
@@ -232,9 +209,7 @@ void flush_vsx_to_thread(struct task_struct *tsk)
 	if (tsk->thread.regs) {
 		preempt_disable();
 		if (tsk->thread.regs->msr & MSR_VSX) {
-#ifdef CONFIG_SMP
 			BUG_ON(tsk != current);
-#endif
 			giveup_vsx(tsk);
 		}
 		preempt_enable();
@@ -249,14 +224,10 @@ void enable_kernel_spe(void)
 {
 	WARN_ON(preemptible());
 
-#ifdef CONFIG_SMP
 	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE))
 		giveup_spe(current);
 	else
 		giveup_spe(NULL);	/* just enable SPE for kernel - force */
-#else
-	giveup_spe(last_task_used_spe);
-#endif /* __SMP __ */
 }
 EXPORT_SYMBOL(enable_kernel_spe);
 
@@ -265,9 +236,7 @@ void flush_spe_to_thread(struct task_struct *tsk)
 	if (tsk->thread.regs) {
 		preempt_disable();
 		if (tsk->thread.regs->msr & MSR_SPE) {
-#ifdef CONFIG_SMP
 			BUG_ON(tsk != current);
-#endif
 			tsk->thread.spefscr = mfspr(SPRN_SPEFSCR);
 			giveup_spe(tsk);
 		}
@@ -276,32 +245,6 @@ void flush_spe_to_thread(struct task_struct *tsk)
 }
 #endif /* CONFIG_SPE */
 
-#ifndef CONFIG_SMP
-/*
- * If we are doing lazy switching of CPU state (FP, altivec or SPE),
- * and the current task has some state, discard it.
- */
-void discard_lazy_cpu_state(void)
-{
-	preempt_disable();
-	if (last_task_used_math == current)
-		last_task_used_math = NULL;
-#ifdef CONFIG_ALTIVEC
-	if (last_task_used_altivec == current)
-		last_task_used_altivec = NULL;
-#endif /* CONFIG_ALTIVEC */
-#ifdef CONFIG_VSX
-	if (last_task_used_vsx == current)
-		last_task_used_vsx = NULL;
-#endif /* CONFIG_VSX */
-#ifdef CONFIG_SPE
-	if (last_task_used_spe == current)
-		last_task_used_spe = NULL;
-#endif
-	preempt_enable();
-}
-#endif /* CONFIG_SMP */
-
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 void do_send_trap(struct pt_regs *regs, unsigned long address,
 		  unsigned long error_code, int signal_code, int breakpt)
@@ -831,30 +774,9 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	__switch_to_tm(prev);
 
-#ifdef CONFIG_SMP
-	/* avoid complexity of lazy save/restore of fpu
-	 * by just saving it every time we switch out if
-	 * this task used the fpu during the last quantum.
-	 *
-	 * If it tries to use the fpu again, it'll trap and
-	 * reload its fp regs.  So we don't have to do a restore
-	 * every switch, just a save.
-	 *  -- Cort
-	 */
 	if (prev->thread.regs && (prev->thread.regs->msr & MSR_FP))
 		giveup_fpu(prev);
 #ifdef CONFIG_ALTIVEC
-	/*
-	 * If the previous thread used altivec in the last quantum
-	 * (thus changing altivec regs) then save them.
-	 * We used to check the VRSAVE register but not all apps
-	 * set it, so we don't rely on it now (and in fact we need
-	 * to save & restore VSCR even if VRSAVE == 0).  -- paulus
-	 *
-	 * On SMP we always save/restore altivec regs just to avoid the
-	 * complexity of changing processors.
-	 *  -- Cort
-	 */
 	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))
 		giveup_altivec(prev);
 #endif /* CONFIG_ALTIVEC */
@@ -864,39 +786,10 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		__giveup_vsx(prev);
 #endif /* CONFIG_VSX */
 #ifdef CONFIG_SPE
-	/*
-	 * If the previous thread used spe in the last quantum
-	 * (thus changing spe regs) then save them.
-	 *
-	 * On SMP we always save/restore spe regs just to avoid the
-	 * complexity of changing processors.
-	 */
 	if ((prev->thread.regs && (prev->thread.regs->msr & MSR_SPE)))
 		giveup_spe(prev);
 #endif /* CONFIG_SPE */
 
-#else  /* CONFIG_SMP */
-#ifdef CONFIG_ALTIVEC
-	/* Avoid the trap.  On smp this this never happens since
-	 * we don't set last_task_used_altivec -- Cort
-	 */
-	if (new->thread.regs && last_task_used_altivec == new)
-		new->thread.regs->msr |= MSR_VEC;
-#endif /* CONFIG_ALTIVEC */
-#ifdef CONFIG_VSX
-	if (new->thread.regs && last_task_used_vsx == new)
-		new->thread.regs->msr |= MSR_VSX;
-#endif /* CONFIG_VSX */
-#ifdef CONFIG_SPE
-	/* Avoid the trap.  On smp this this never happens since
-	 * we don't set last_task_used_spe
-	 */
-	if (new->thread.regs && last_task_used_spe == new)
-		new->thread.regs->msr |= MSR_SPE;
-#endif /* CONFIG_SPE */
-
-#endif /* CONFIG_SMP */
-
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	switch_booke_debug_regs(&new->thread.debug);
 #else
@@ -1111,13 +1004,10 @@ void show_regs(struct pt_regs * regs)
 
 void exit_thread(void)
 {
-	discard_lazy_cpu_state();
 }
 
 void flush_thread(void)
 {
-	discard_lazy_cpu_state();
-
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	flush_ptrace_hw_breakpoint(current);
 #else /* CONFIG_HAVE_HW_BREAKPOINT */
@@ -1355,7 +1245,6 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 		regs->msr = MSR_USER32;
 	}
 #endif
-	discard_lazy_cpu_state();
 #ifdef CONFIG_VSX
 	current->thread.used_vsr = 0;
 #endif

commit 152d523e6307c7152f9986a542f873b5c5863937
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Oct 29 11:43:55 2015 +1100

    powerpc: Create context switch helpers save_sprs() and restore_sprs()
    
    Move all our context switch SPR save and restore code into two
    helpers. We do a few optimisations:
    
    - Group all mfsprs and all mtsprs. In many cases an mtspr sets a
    scoreboarding bit that an mfspr waits on, so the current practise of
    mfspr A; mtspr A; mfpsr B; mtspr B is the worst scheduling we can
    do.
    
    - SPR writes are slow, so check that the value is changing before
    writing it.
    
    A context switch microbenchmark using yield():
    
    http://ozlabs.org/~anton/junkcode/context_switch2.c
    
    ./context_switch2 --test=yield 0 0
    
    shows an improvement of almost 10% on POWER8.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 75b6676c1a0b..3aabed4a60a9 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -742,6 +742,73 @@ void restore_tm_state(struct pt_regs *regs)
 #define __switch_to_tm(prev)
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
+static inline void save_sprs(struct thread_struct *t)
+{
+#ifdef CONFIG_ALTIVEC
+	if (cpu_has_feature(cpu_has_feature(CPU_FTR_ALTIVEC)))
+		t->vrsave = mfspr(SPRN_VRSAVE);
+#endif
+#ifdef CONFIG_PPC_BOOK3S_64
+	if (cpu_has_feature(CPU_FTR_DSCR))
+		t->dscr = mfspr(SPRN_DSCR);
+
+	if (cpu_has_feature(CPU_FTR_ARCH_207S)) {
+		t->bescr = mfspr(SPRN_BESCR);
+		t->ebbhr = mfspr(SPRN_EBBHR);
+		t->ebbrr = mfspr(SPRN_EBBRR);
+
+		t->fscr = mfspr(SPRN_FSCR);
+
+		/*
+		 * Note that the TAR is not available for use in the kernel.
+		 * (To provide this, the TAR should be backed up/restored on
+		 * exception entry/exit instead, and be in pt_regs.  FIXME,
+		 * this should be in pt_regs anyway (for debug).)
+		 */
+		t->tar = mfspr(SPRN_TAR);
+	}
+#endif
+}
+
+static inline void restore_sprs(struct thread_struct *old_thread,
+				struct thread_struct *new_thread)
+{
+#ifdef CONFIG_ALTIVEC
+	if (cpu_has_feature(CPU_FTR_ALTIVEC) &&
+	    old_thread->vrsave != new_thread->vrsave)
+		mtspr(SPRN_VRSAVE, new_thread->vrsave);
+#endif
+#ifdef CONFIG_PPC_BOOK3S_64
+	if (cpu_has_feature(CPU_FTR_DSCR)) {
+		u64 dscr = get_paca()->dscr_default;
+		u64 fscr = old_thread->fscr & ~FSCR_DSCR;
+
+		if (new_thread->dscr_inherit) {
+			dscr = new_thread->dscr;
+			fscr |= FSCR_DSCR;
+		}
+
+		if (old_thread->dscr != dscr)
+			mtspr(SPRN_DSCR, dscr);
+
+		if (old_thread->fscr != fscr)
+			mtspr(SPRN_FSCR, fscr);
+	}
+
+	if (cpu_has_feature(CPU_FTR_ARCH_207S)) {
+		if (old_thread->bescr != new_thread->bescr)
+			mtspr(SPRN_BESCR, new_thread->bescr);
+		if (old_thread->ebbhr != new_thread->ebbhr)
+			mtspr(SPRN_EBBHR, new_thread->ebbhr);
+		if (old_thread->ebbrr != new_thread->ebbrr)
+			mtspr(SPRN_EBBRR, new_thread->ebbrr);
+
+		if (old_thread->tar != new_thread->tar)
+			mtspr(SPRN_TAR, new_thread->tar);
+	}
+#endif
+}
+
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {
@@ -751,17 +818,16 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	struct ppc64_tlb_batch *batch;
 #endif
 
+	new_thread = &new->thread;
+	old_thread = &current->thread;
+
 	WARN_ON(!irqs_disabled());
 
-	/* Back up the TAR and DSCR across context switches.
-	 * Note that the TAR is not available for use in the kernel.  (To
-	 * provide this, the TAR should be backed up/restored on exception
-	 * entry/exit instead, and be in pt_regs.  FIXME, this should be in
-	 * pt_regs anyway (for debug).)
-	 * Save the TAR and DSCR here before we do treclaim/trecheckpoint as
-	 * these will change them.
+	/*
+	 * We need to save SPRs before treclaim/trecheckpoint as these will
+	 * change a number of them.
 	 */
-	save_early_sprs(&prev->thread);
+	save_sprs(&prev->thread);
 
 	__switch_to_tm(prev);
 
@@ -844,10 +910,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 
-
-	new_thread = &new->thread;
-	old_thread = &current->thread;
-
 #ifdef CONFIG_PPC64
 	/*
 	 * Collect processor utilization data per process
@@ -883,6 +945,10 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	last = _switch(old_thread, new_thread);
 
+	/* Need to recalculate these after calling _switch() */
+	old_thread = &last->thread;
+	new_thread = &current->thread;
+
 #ifdef CONFIG_PPC_BOOK3S_64
 	if (current_thread_info()->local_flags & _TLF_LAZY_MMU) {
 		current_thread_info()->local_flags &= ~_TLF_LAZY_MMU;
@@ -891,6 +957,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
+	restore_sprs(old_thread, new_thread);
+
 	return last;
 }
 

commit 7f821fc9c77a9b01fe7b1d6e72717b33d8d64142
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Nov 19 15:44:45 2015 +1100

    powerpc/tm: Check for already reclaimed tasks
    
    Currently we can hit a scenario where we'll tm_reclaim() twice.  This
    results in a TM bad thing exception because the second reclaim occurs
    when not in suspend mode.
    
    The scenario in which this can happen is the following.  We attempt to
    deliver a signal to userspace.  To do this we need obtain the stack
    pointer to write the signal context.  To get this stack pointer we
    must tm_reclaim() in case we need to use the checkpointed stack
    pointer (see get_tm_stackpointer()).  Normally we'd then return
    directly to userspace to deliver the signal without going through
    __switch_to().
    
    Unfortunatley, if at this point we get an error (such as a bad
    userspace stack pointer), we need to exit the process.  The exit will
    result in a __switch_to().  __switch_to() will attempt to save the
    process state which results in another tm_reclaim().  This
    tm_reclaim() now causes a TM Bad Thing exception as this state has
    already been saved and the processor is no longer in TM suspend mode.
    Whee!
    
    This patch checks the state of the MSR to ensure we are TM suspended
    before we attempt the tm_reclaim().  If we've already saved the state
    away, we should no longer be in TM suspend mode.  This has the
    additional advantage of checking for a potential TM Bad Thing
    exception.
    
    Found using syscall fuzzer.
    
    Fixes: fb09692e71f1 ("powerpc: Add reclaim and recheckpoint functions for context switching transactional memory processes")
    Cc: stable@vger.kernel.org # v3.9+
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 75b6676c1a0b..646bf4d222c1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -551,6 +551,24 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 		msr_diff &= MSR_FP | MSR_VEC | MSR_VSX | MSR_FE0 | MSR_FE1;
 	}
 
+	/*
+	 * Use the current MSR TM suspended bit to track if we have
+	 * checkpointed state outstanding.
+	 * On signal delivery, we'd normally reclaim the checkpointed
+	 * state to obtain stack pointer (see:get_tm_stackpointer()).
+	 * This will then directly return to userspace without going
+	 * through __switch_to(). However, if the stack frame is bad,
+	 * we need to exit this thread which calls __switch_to() which
+	 * will again attempt to reclaim the already saved tm state.
+	 * Hence we need to check that we've not already reclaimed
+	 * this state.
+	 * We do this using the current MSR, rather tracking it in
+	 * some specific thread_struct bit, as it has the additional
+	 * benifit of checking for a potential TM bad thing exception.
+	 */
+	if (!MSR_TM_SUSPENDED(mfmsr()))
+		return;
+
 	tm_reclaim(thr, thr->regs->msr, cause);
 
 	/* Having done the reclaim, we now have the checkpointed

commit ff474e8ca8547d09cb82ebab56d4c96f9eea01ce
Merge: 4c92b5bb1422 390fd5929f52
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 3 16:41:38 2015 -0700

    Merge tag 'powerpc-4.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - support "hybrid" iommu/direct DMA ops for coherent_mask < dma_mask
       from Benjamin Herrenschmidt
    
     - EEH fixes for SRIOV from Gavin
    
     - introduce rtas_get_sensor_fast() for IRQ handlers from Thomas Huth
    
     - use hardware RNG for arch_get_random_seed_* not arch_get_random_*
       from Paul Mackerras
    
     - seccomp filter support from Michael Ellerman
    
     - opal_cec_reboot2() handling for HMIs & machine checks from Mahesh
       Salgaonkar
    
     - add powerpc timebase as a trace clock source from Naveen N.  Rao
    
     - misc cleanups in the xmon, signal & SLB code from Anshuman Khandual
    
     - add an inline function to update POWER8 HID0 from Gautham R.  Shenoy
    
     - fix pte_pagesize_index() crash on 4K w/64K hash from Michael Ellerman
    
     - drop support for 64K local store on 4K kernels from Michael Ellerman
    
     - move dma_get_required_mask() from pnv_phb to pci_controller_ops from
       Andrew Donnellan
    
     - initialize distance lookup table from drconf path from Nikunj A
       Dadhania
    
     - enable RTC class support from Vaibhav Jain
    
     - disable automatically blocked PCI config from Gavin Shan
    
     - add LEDs driver for PowerNV platform from Vasant Hegde
    
     - fix endianness issues in the HVSI driver from Laurent Dufour
    
     - kexec endian fixes from Samuel Mendoza-Jonas
    
     - fix corrupted pdn list from Gavin Shan
    
     - fix fenced PHB caused by eeh_slot_error_detail() from Gavin Shan
    
     - Freescale updates from Scott: Highlights include 32-bit memcpy/memset
       optimizations, checksum optimizations, 85xx config fragments and
       updates, device tree updates, e6500 fixes for non-SMP, and misc
       cleanup and minor fixes.
    
     - a ton of cxl updates & fixes:
        - add explicit precision specifiers from Rasmus Villemoes
        - use more common format specifier from Rasmus Villemoes
        - destroy cxl_adapter_idr on module_exit from Johannes Thumshirn
        - destroy afu->contexts_idr on release of an afu from Johannes
          Thumshirn
        - compile with -Werror from Daniel Axtens
        - EEH support from Daniel Axtens
        - plug irq_bitmap getting leaked in cxl_context from Vaibhav Jain
        - add alternate MMIO error handling from Ian Munsie
        - allow release of contexts which have been OPENED but not STARTED
          from Andrew Donnellan
        - remove use of macro DEFINE_PCI_DEVICE_TABLE from Vaishali Thakkar
        - release irqs if memory allocation fails from Vaibhav Jain
        - remove racy attempt to force EEH invocation in reset from Daniel
          Axtens
        - fix + cleanup error paths in cxl_dev_context_init from Ian Munsie
        - fix force unmapping mmaps of contexts allocated through the kernel
          api from Ian Munsie
        - set up and enable PSL Timebase from Philippe Bergheaud
    
    * tag 'powerpc-4.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (140 commits)
      cxl: Set up and enable PSL Timebase
      cxl: Fix force unmapping mmaps of contexts allocated through the kernel api
      cxl: Fix + cleanup error paths in cxl_dev_context_init
      powerpc/eeh: Fix fenced PHB caused by eeh_slot_error_detail()
      powerpc/pseries: Cleanup on pci_dn_reconfig_notifier()
      powerpc/pseries: Fix corrupted pdn list
      powerpc/powernv: Enable LEDS support
      powerpc/iommu: Set default DMA offset in dma_dev_setup
      cxl: Remove racy attempt to force EEH invocation in reset
      cxl: Release irqs if memory allocation fails
      cxl: Remove use of macro DEFINE_PCI_DEVICE_TABLE
      powerpc/powernv: Fix mis-merge of OPAL support for LEDS driver
      powerpc/powernv: Reset HILE before kexec_sequence()
      powerpc/kexec: Reset secondary cpu endianness before kexec
      powerpc/hvsi: Fix endianness issues in the HVSI driver
      leds/powernv: Add driver for PowerNV platform
      powerpc/powernv: Create LED platform device
      powerpc/powernv: Add OPAL interfaces for accessing and modifying system LED states
      powerpc/powernv: Fix the log message when disabling VF
      cxl: Allow release of contexts which have been OPENED but not STARTED
      ...

commit 829023df86d4ec39b110860cd5f106b7ac58f772
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Mon Jul 6 16:24:10 2015 +0530

    powerpc/tm: Drop tm_orig_msr from thread_struct
    
    Currently tm_orig_msr is getting used during process context switch only.
    Then there is ckpt_regs which saves the checkpointed userspace context
    The MSR slot contained in ckpt_regs structure can be used during process
    context switch instead of tm_orig_msr, thus allowing us to drop it from
    thread_struct structure. This patch does that change.
    
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8005e18d1b40..99adcbad3690 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -86,7 +86,7 @@ void giveup_fpu_maybe_transactional(struct task_struct *tsk)
 	if (tsk == current && tsk->thread.regs &&
 	    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
 	    !test_thread_flag(TIF_RESTORE_TM)) {
-		tsk->thread.tm_orig_msr = tsk->thread.regs->msr;
+		tsk->thread.ckpt_regs.msr = tsk->thread.regs->msr;
 		set_thread_flag(TIF_RESTORE_TM);
 	}
 
@@ -104,7 +104,7 @@ void giveup_altivec_maybe_transactional(struct task_struct *tsk)
 	if (tsk == current && tsk->thread.regs &&
 	    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
 	    !test_thread_flag(TIF_RESTORE_TM)) {
-		tsk->thread.tm_orig_msr = tsk->thread.regs->msr;
+		tsk->thread.ckpt_regs.msr = tsk->thread.regs->msr;
 		set_thread_flag(TIF_RESTORE_TM);
 	}
 
@@ -543,7 +543,7 @@ static void tm_reclaim_thread(struct thread_struct *thr,
 	 * the thread will no longer be transactional.
 	 */
 	if (test_ti_thread_flag(ti, TIF_RESTORE_TM)) {
-		msr_diff = thr->tm_orig_msr & ~thr->regs->msr;
+		msr_diff = thr->ckpt_regs.msr & ~thr->regs->msr;
 		if (msr_diff & MSR_FP)
 			memcpy(&thr->transact_fp, &thr->fp_state,
 			       sizeof(struct thread_fp_state));
@@ -594,10 +594,10 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 	/* Stash the original thread MSR, as giveup_fpu et al will
 	 * modify it.  We hold onto it to see whether the task used
 	 * FP & vector regs.  If the TIF_RESTORE_TM flag is set,
-	 * tm_orig_msr is already set.
+	 * ckpt_regs.msr is already set.
 	 */
 	if (!test_ti_thread_flag(task_thread_info(tsk), TIF_RESTORE_TM))
-		thr->tm_orig_msr = thr->regs->msr;
+		thr->ckpt_regs.msr = thr->regs->msr;
 
 	TM_DEBUG("--- tm_reclaim on pid %d (NIP=%lx, "
 		 "ccr=%lx, msr=%lx, trap=%lx)\n",
@@ -666,7 +666,7 @@ static inline void tm_recheckpoint_new_task(struct task_struct *new)
 		tm_restore_sprs(&new->thread);
 		return;
 	}
-	msr = new->thread.tm_orig_msr;
+	msr = new->thread.ckpt_regs.msr;
 	/* Recheckpoint to restore original checkpointed register state. */
 	TM_DEBUG("*** tm_recheckpoint of pid %d "
 		 "(new->msr 0x%lx, new->origmsr 0x%lx)\n",
@@ -726,7 +726,7 @@ void restore_tm_state(struct pt_regs *regs)
 	if (!MSR_TM_ACTIVE(regs->msr))
 		return;
 
-	msr_diff = current->thread.tm_orig_msr & ~regs->msr;
+	msr_diff = current->thread.ckpt_regs.msr & ~regs->msr;
 	msr_diff &= MSR_FP | MSR_VEC | MSR_VSX;
 	if (msr_diff & MSR_FP) {
 		fp_enable();

commit 72cd7b44bc99376b3f3c93cedcd052663fcdf705
Author: Leonidas Da Silva Barbosa <leosilva@linux.vnet.ibm.com>
Date:   Mon Jul 13 13:51:01 2015 -0300

    powerpc: Uncomment and make enable_kernel_vsx() routine available
    
    enable_kernel_vsx() function was commented since anything was using
    it. However, vmx-crypto driver uses VSX instructions which are
    only available if VSX is enable. Otherwise it rises an exception oops.
    
    This patch uncomment enable_kernel_vsx() routine and makes it available.
    
    Signed-off-by: Leonidas S. Barbosa <leosilva@linux.vnet.ibm.com>
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8005e18d1b40..64e6e9d9e656 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -204,8 +204,6 @@ EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
 #endif /* CONFIG_ALTIVEC */
 
 #ifdef CONFIG_VSX
-#if 0
-/* not currently used, but some crazy RAID module might want to later */
 void enable_kernel_vsx(void)
 {
 	WARN_ON(preemptible());
@@ -220,7 +218,6 @@ void enable_kernel_vsx(void)
 #endif /* CONFIG_SMP */
 }
 EXPORT_SYMBOL(enable_kernel_vsx);
-#endif
 
 void giveup_vsx(struct task_struct *tsk)
 {

commit 280e109992831ee16a0f74ae887c08fb26c021f1
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu May 21 12:13:02 2015 +0530

    powerpc/kernel: Remove the unused extern dscr_default
    
    The process context switch code no longer uses dscr_default variable
    from the sysfs.c file. The variable became unused when we started
    storing the CPU specific DSCR value in the PACA structure instead.
    This patch just removes this extern declaration. It was originally
    added by the following commit.
    
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index febb50dd5328..8005e18d1b40 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1112,7 +1112,6 @@ static void setup_ksp_vsid(struct task_struct *p, unsigned long sp)
 /*
  * Copy a thread..
  */
-extern unsigned long dscr_default; /* defined in arch/powerpc/kernel/sysfs.c */
 
 /*
  * Copy architecture-specific thread state

commit 6eca8933d3ff17bff39d5f10a2a22366d8622fa6
Author: Alex Dowad <alexinbeijing@gmail.com>
Date:   Fri Mar 13 20:14:46 2015 +0200

    powerpc/kernel: Rename copy_thread() 'arg' argument to 'kthread_arg'
    
    The 'arg' argument to copy_thread() is only ever used when forking a new
    kernel thread. Hence, rename it to 'kthread_arg' for clarity.
    
    Signed-off-by: Alex Dowad <alexinbeijing@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b4cc7bef6b16..febb50dd5328 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1114,8 +1114,11 @@ static void setup_ksp_vsid(struct task_struct *p, unsigned long sp)
  */
 extern unsigned long dscr_default; /* defined in arch/powerpc/kernel/sysfs.c */
 
+/*
+ * Copy architecture-specific thread state
+ */
 int copy_thread(unsigned long clone_flags, unsigned long usp,
-		unsigned long arg, struct task_struct *p)
+		unsigned long kthread_arg, struct task_struct *p)
 {
 	struct pt_regs *childregs, *kregs;
 	extern void ret_from_fork(void);
@@ -1127,6 +1130,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	sp -= sizeof(struct pt_regs);
 	childregs = (struct pt_regs *) sp;
 	if (unlikely(p->flags & PF_KTHREAD)) {
+		/* kernel thread */
 		struct thread_info *ti = (void *)task_stack_page(p);
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
@@ -1137,11 +1141,12 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		clear_tsk_thread_flag(p, TIF_32BIT);
 		childregs->softe = 1;
 #endif
-		childregs->gpr[15] = arg;
+		childregs->gpr[15] = kthread_arg;
 		p->thread.regs = NULL;	/* no user register state */
 		ti->flags |= _TIF_RESTOREALL;
 		f = ret_from_kernel_thread;
 	} else {
+		/* user thread */
 		struct pt_regs *regs = current_pt_regs();
 		CHECK_FULL_REGS(regs);
 		*childregs = *regs;

commit 59994fb01a102a448ba758c9b824a29b4a99cc1b
Author: Vineeth Vijayan <vvijayan@mvista.com>
Date:   Fri Nov 14 14:42:05 2014 +0530

    powerpc: Use generic PIE randomization
    
    Back in 2009 we merged 501cb16d3cfd "Randomise PIEs", which added support for
    randomizing PIE (Position Independent Executable) binaries.
    
    That commit added randomize_et_dyn(), which correctly randomized the addresses,
    but failed to honor PF_RANDOMIZE. That means it was not possible to disable PIE
    randomization via the personality flag, or /proc/sys/kernel/randomize_va_space.
    
    Since then there has been generic support for PIE randomization added to
    binfmt_elf.c, selectable via ARCH_BINFMT_ELF_RANDOMIZE_PIE.
    
    Enabling that allows us to drop randomize_et_dyn(), which means we start
    honoring PF_RANDOMIZE correctly.
    
    It also causes a fairly major change to how we layout PIE binaries.
    
    Currently we will place the binary at 512MB-520MB for 32 bit binaries, or
    512MB-1.5GB for 64 bit binaries, eg:
    
        $ cat /proc/$$/maps
        4e550000-4e580000 r-xp 00000000 08:02 129813       /bin/dash
        4e580000-4e590000 rw-p 00020000 08:02 129813       /bin/dash
        10014110000-10014140000 rw-p 00000000 00:00 0      [heap]
        3fffaa3f0000-3fffaa5a0000 r-xp 00000000 08:02 921  /lib/powerpc64le-linux-gnu/libc-2.19.so
        3fffaa5a0000-3fffaa5b0000 rw-p 001a0000 08:02 921  /lib/powerpc64le-linux-gnu/libc-2.19.so
        3fffaa5c0000-3fffaa5d0000 rw-p 00000000 00:00 0
        3fffaa5d0000-3fffaa5f0000 r-xp 00000000 00:00 0    [vdso]
        3fffaa5f0000-3fffaa620000 r-xp 00000000 08:02 1246 /lib/powerpc64le-linux-gnu/ld-2.19.so
        3fffaa620000-3fffaa630000 rw-p 00020000 08:02 1246 /lib/powerpc64le-linux-gnu/ld-2.19.so
        3ffffc340000-3ffffc370000 rw-p 00000000 00:00 0    [stack]
    
    With this commit applied we don't do any special randomisation for the binary,
    and instead rely on mmap randomisation. This means the binary ends up at high
    addresses, eg:
    
        $ cat /proc/$$/maps
        3fff99820000-3fff999d0000 r-xp 00000000 08:02 921    /lib/powerpc64le-linux-gnu/libc-2.19.so
        3fff999d0000-3fff999e0000 rw-p 001a0000 08:02 921    /lib/powerpc64le-linux-gnu/libc-2.19.so
        3fff999f0000-3fff99a00000 rw-p 00000000 00:00 0
        3fff99a00000-3fff99a20000 r-xp 00000000 00:00 0      [vdso]
        3fff99a20000-3fff99a50000 r-xp 00000000 08:02 1246   /lib/powerpc64le-linux-gnu/ld-2.19.so
        3fff99a50000-3fff99a60000 rw-p 00020000 08:02 1246   /lib/powerpc64le-linux-gnu/ld-2.19.so
        3fff99a60000-3fff99a90000 r-xp 00000000 08:02 129813 /bin/dash
        3fff99a90000-3fff99aa0000 rw-p 00020000 08:02 129813 /bin/dash
        3fffc3de0000-3fffc3e10000 rw-p 00000000 00:00 0      [stack]
        3fffc55e0000-3fffc5610000 rw-p 00000000 00:00 0      [heap]
    
    Although this should be OK, it's possible it might break badly written
    binaries that make assumptions about the address space layout.
    
    Signed-off-by: Vineeth Vijayan <vvijayan@mvista.com>
    [mpe: Rewrite changelog]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f6b82152e7aa..b4cc7bef6b16 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1654,12 +1654,3 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	return ret;
 }
 
-unsigned long randomize_et_dyn(unsigned long base)
-{
-	unsigned long ret = PAGE_ALIGN(base + brk_rnd());
-
-	if (ret < base)
-		return base;
-
-	return ret;
-}

commit 7d56c65a6ff9065c459fc63c509950d8ea66e00c
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Sep 17 17:07:03 2014 +1000

    powerpc/ftrace: Remove mod_return_to_handler
    
    mod_return_to_handler is the same as return_to_handler, except
    it handles the change of the TOC (r2). Add this into
    return_to_handler and remove mod_return_to_handler.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8c2691e445bd..f6b82152e7aa 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1527,13 +1527,6 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	int curr_frame = current->curr_ret_stack;
 	extern void return_to_handler(void);
 	unsigned long rth = (unsigned long)return_to_handler;
-	unsigned long mrth = -1;
-#ifdef CONFIG_PPC64
-	extern void mod_return_to_handler(void);
-	rth = *(unsigned long *)rth;
-	mrth = (unsigned long)mod_return_to_handler;
-	mrth = *(unsigned long *)mrth;
-#endif
 #endif
 
 	sp = (unsigned long) stack;
@@ -1558,7 +1551,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		if (!firstframe || ip != lr) {
 			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-			if ((ip == rth || ip == mrth) && curr_frame >= 0) {
+			if ((ip == rth) && curr_frame >= 0) {
 				printk(" (%pS)",
 				       (void *)current->ret_stack[curr_frame].ret);
 				curr_frame--;

commit 7b051f665c32db3339ac9b6d9806ef508d09647f
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 20:27:15 2014 +1100

    powerpc: Use probe_kernel_address in show_instructions
    
    We really don't want to take a pagefault in show_instructions,
    so use probe_kernel_address instead of __get_user.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 91e132b495bf..8c2691e445bd 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -37,9 +37,9 @@
 #include <linux/personality.h>
 #include <linux/random.h>
 #include <linux/hw_breakpoint.h>
+#include <linux/uaccess.h>
 
 #include <asm/pgtable.h>
-#include <asm/uaccess.h>
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/mmu.h>
@@ -921,12 +921,8 @@ static void show_instructions(struct pt_regs *regs)
 			pc = (unsigned long)phys_to_virt(pc);
 #endif
 
-		/* We use __get_user here *only* to avoid an OOPS on a
-		 * bad address because the pc *should* only be a
-		 * kernel address.
-		 */
 		if (!__kernel_text_address(pc) ||
-		     __get_user(instr, (unsigned int __user *)pc)) {
+		     probe_kernel_address((unsigned int __user *)pc, instr)) {
 			printk(KERN_CONT "XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)

commit 69111bac42f5ceacdd22e30947837ceb2c4493ed
Author: Christoph Lameter <cl@linux.com>
Date:   Tue Oct 21 15:23:25 2014 -0500

    powerpc: Replace __get_cpu_var uses
    
    This still has not been merged and now powerpc is the only arch that does
    not have this change. Sorry about missing linuxppc-dev before.
    
    V2->V2
      - Fix up to work against 3.18-rc1
    
    __get_cpu_var() is used for multiple purposes in the kernel source. One of
    them is address calculation via the form &__get_cpu_var(x).  This calculates
    the address for the instance of the percpu variable of the current processor
    based on an offset.
    
    Other use cases are for storing and retrieving data from the current
    processors percpu area.  __get_cpu_var() can be used as an lvalue when
    writing data or on the right side of an assignment.
    
    __get_cpu_var() is defined as :
    
    __get_cpu_var() always only does an address determination. However, store
    and retrieve operations could use a segment prefix (or global register on
    other platforms) to avoid the address calculation.
    
    this_cpu_write() and this_cpu_read() can directly take an offset into a
    percpu area and use optimized assembly code to read and write per cpu
    variables.
    
    This patch converts __get_cpu_var into either an explicit address
    calculation using this_cpu_ptr() or into a use of this_cpu operations that
    use the offset.  Thereby address calculations are avoided and less registers
    are used when code is generated.
    
    At the end of the patch set all uses of __get_cpu_var have been removed so
    the macro is removed too.
    
    The patch set includes passes over all arches as well. Once these operations
    are used throughout then specialized macros can be defined in non -x86
    arches as well in order to optimize per cpu access by f.e.  using a global
    register that may be set to the per cpu base.
    
    Transformations done to __get_cpu_var()
    
    1. Determine the address of the percpu instance of the current processor.
    
            DEFINE_PER_CPU(int, y);
            int *x = &__get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(&y);
    
    2. Same as #1 but this time an array structure is involved.
    
            DEFINE_PER_CPU(int, y[20]);
            int *x = __get_cpu_var(y);
    
        Converts to
    
            int *x = this_cpu_ptr(y);
    
    3. Retrieve the content of the current processors instance of a per cpu
    variable.
    
            DEFINE_PER_CPU(int, y);
            int x = __get_cpu_var(y)
    
       Converts to
    
            int x = __this_cpu_read(y);
    
    4. Retrieve the content of a percpu struct
    
            DEFINE_PER_CPU(struct mystruct, y);
            struct mystruct x = __get_cpu_var(y);
    
       Converts to
    
            memcpy(&x, this_cpu_ptr(&y), sizeof(x));
    
    5. Assignment to a per cpu variable
    
            DEFINE_PER_CPU(int, y)
            __get_cpu_var(y) = x;
    
       Converts to
    
            __this_cpu_write(y, x);
    
    6. Increment/Decrement etc of a per cpu variable
    
            DEFINE_PER_CPU(int, y);
            __get_cpu_var(y)++
    
       Converts to
    
            __this_cpu_inc(y)
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    CC: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    [mpe: Fix build errors caused by set/or_softirq_pending(), and rework
          assignment in __set_breakpoint() to use memcpy().]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 923cd2daba89..91e132b495bf 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -499,7 +499,7 @@ static inline int set_dawr(struct arch_hw_breakpoint *brk)
 
 void __set_breakpoint(struct arch_hw_breakpoint *brk)
 {
-	__get_cpu_var(current_brk) = *brk;
+	memcpy(this_cpu_ptr(&current_brk), brk, sizeof(*brk));
 
 	if (cpu_has_feature(CPU_FTR_DAWR))
 		set_dawr(brk);
@@ -842,7 +842,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
  * schedule DABR
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
-	if (unlikely(!hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))
+	if (unlikely(!hw_brk_match(this_cpu_ptr(&current_brk), &new->thread.hw_brk)))
 		__set_breakpoint(&new->thread.hw_brk);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
@@ -856,7 +856,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 * Collect processor utilization data per process
 	 */
 	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
-		struct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);
+		struct cpu_usage *cu = this_cpu_ptr(&cpu_usage_array);
 		long unsigned start_tb, current_tb;
 		start_tb = old_thread->start_tb;
 		cu->current_tb = current_tb = mfspr(SPRN_PURR);
@@ -866,7 +866,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #endif /* CONFIG_PPC64 */
 
 #ifdef CONFIG_PPC_BOOK3S_64
-	batch = &__get_cpu_var(ppc64_tlb_batch);
+	batch = this_cpu_ptr(&ppc64_tlb_batch);
 	if (batch->active) {
 		current_thread_info()->local_flags |= _TLF_LAZY_MMU;
 		if (batch->index)
@@ -889,7 +889,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #ifdef CONFIG_PPC_BOOK3S_64
 	if (current_thread_info()->local_flags & _TLF_LAZY_MMU) {
 		current_thread_info()->local_flags &= ~_TLF_LAZY_MMU;
-		batch = &__get_cpu_var(ppc64_tlb_batch);
+		batch = this_cpu_ptr(&ppc64_tlb_batch);
 		batch->active = 1;
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */

commit acf620ecf56cfc4edaffaf158250e128539cdd26
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 19:41:39 2014 +1100

    powerpc: Rename __get_SP() to current_stack_pointer()
    
    Michael points out that __get_SP() is a pretty horrible
    function name. Let's give it a better name.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3cc643988101..923cd2daba89 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1545,7 +1545,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		tsk = current;
 	if (sp == 0) {
 		if (tsk == current)
-			sp = __get_SP();
+			sp = current_stack_pointer();
 		else
 			sp = tsk->thread.ksp;
 	}

commit bfe9a2cfe91a1c920f152ce5fd0a9ad74b3daf12
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Oct 13 19:41:38 2014 +1100

    powerpc: Reimplement __get_SP() as a function not a define
    
    Li Zhong points out an issue with our current __get_SP()
    implementation. If ftrace function tracing is enabled (ie -pg
    profiling using _mcount) we spill a stack frame on 64bit all the
    time.
    
    If a function calls __get_SP() and later calls a function that is
    tail call optimised, we will pop the stack frame and the value
    returned by __get_SP() is no longer valid. An example from Li can
    be found in save_stack_trace -> save_context_stack:
    
    c0000000000432c0 <.save_stack_trace>:
    c0000000000432c0:       mflr    r0
    c0000000000432c4:       std     r0,16(r1)
    c0000000000432c8:       stdu    r1,-128(r1) <-- stack frame for _mcount
    c0000000000432cc:       std     r3,112(r1)
    c0000000000432d0:       bl      <._mcount>
    c0000000000432d4:       nop
    
    c0000000000432d8:       mr      r4,r1 <-- __get_SP()
    
    c0000000000432dc:       ld      r5,632(r13)
    c0000000000432e0:       ld      r3,112(r1)
    c0000000000432e4:       li      r6,1
    
    c0000000000432e8:       addi    r1,r1,128 <-- pop stack frame
    
    c0000000000432ec:       ld      r0,16(r1)
    c0000000000432f0:       mtlr    r0
    c0000000000432f4:       b       <.save_context_stack> <-- tail call optimized
    
    save_context_stack ends up with a stack pointer below the current
    one, and it is likely to be scribbled over.
    
    Fix this by making __get_SP() a function which returns the
    callers stack frame. Also replace inline assembly which grabs
    the stack pointer in save_stack_trace and show_stack with
    __get_SP().
    
    This also fixes an issue with perf_arch_fetch_caller_regs().
    It currently unwinds the stack once, which will skip a
    valid stack frame on a leaf function. With the __get_SP() fixes
    in this patch, we never need to unwind the stack frame to get
    to the first interesting frame.
    
    We have to export __get_SP() because perf_arch_fetch_caller_regs()
    (which is used in modules) calls it from a header file.
    
    Reported-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index aa1df89c8b2a..3cc643988101 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1545,7 +1545,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		tsk = current;
 	if (sp == 0) {
 		if (tsk == current)
-			asm("mr %0,1" : "=r" (sp));
+			sp = __get_SP();
 		else
 			sp = tsk->thread.ksp;
 	}

commit e1802b065d189cdfa25eaf6d019c222a91618b9c
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 20 08:00:02 2014 +1000

    powerpc: Move more symbol exports next to function definitions
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index bf44ae962ab8..aa1df89c8b2a 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -228,6 +228,7 @@ void giveup_vsx(struct task_struct *tsk)
 	giveup_altivec_maybe_transactional(tsk);
 	__giveup_vsx(tsk);
 }
+EXPORT_SYMBOL(giveup_vsx);
 
 void flush_vsx_to_thread(struct task_struct *tsk)
 {
@@ -1316,6 +1317,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	current->thread.tm_tfiar = 0;
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 }
+EXPORT_SYMBOL(start_thread);
 
 #define PR_FP_ALL_EXCEPT (PR_FP_EXC_DIV | PR_FP_EXC_OVF | PR_FP_EXC_UND \
 		| PR_FP_EXC_RES | PR_FP_EXC_INV)

commit 9be9be2e9a966fe32c1a443c5197f829ecc3028c
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jun 12 16:53:08 2014 +1000

    powerpc: Reduce scariness of interrupt frames in stack traces
    
    Some people see things like "Exception: 501" in stack traces in dmesg
    and assume that means that something has gone badly wrong, when in
    fact "Exception: 501" just means a device interrupt was taken.
    This changes "Exception" to "interrupt" to make it clearer that we
    are just recording the fact of a change in control flow rather than
    some error condition.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9c34327e38ca..bf44ae962ab8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1581,7 +1581,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 			struct pt_regs *regs = (struct pt_regs *)
 				(sp + STACK_FRAME_OVERHEAD);
 			lr = regs->link;
-			printk("--- Exception: %lx at %pS\n    LR = %pS\n",
+			printk("--- interrupt: %lx at %pS\n    LR = %pS\n",
 			       regs->trap, (void *)regs->nip, (void *)lr);
 			firstframe = 1;
 		}

commit cec15488c7e165a10f4c61cfba91c779e6afd19d
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 10 12:29:21 2014 +1000

    powerpc: Pull out ksp_vsid logic into a helper
    
    The previous patch left a bit of a wart in copy_process(). Clean it up a
    bit by moving the logic out into a helper.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e39f388fc25c..9c34327e38ca 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1095,6 +1095,23 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	return 0;
 }
 
+static void setup_ksp_vsid(struct task_struct *p, unsigned long sp)
+{
+#ifdef CONFIG_PPC_STD_MMU_64
+	unsigned long sp_vsid;
+	unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
+
+	if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
+		sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_1T)
+			<< SLB_VSID_SHIFT_1T;
+	else
+		sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_256M)
+			<< SLB_VSID_SHIFT;
+	sp_vsid |= SLB_VSID_KERNEL | llp;
+	p->thread.ksp_vsid = sp_vsid;
+#endif
+}
+
 /*
  * Copy a thread..
  */
@@ -1174,21 +1191,8 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	p->thread.vr_save_area = NULL;
 #endif
 
-#ifdef CONFIG_PPC_STD_MMU_64
-	{
-		unsigned long sp_vsid;
-		unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
+	setup_ksp_vsid(p, sp);
 
-		if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
-			sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_1T)
-				<< SLB_VSID_SHIFT_1T;
-		else
-			sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_256M)
-				<< SLB_VSID_SHIFT;
-		sp_vsid |= SLB_VSID_KERNEL | llp;
-		p->thread.ksp_vsid = sp_vsid;
-	}
-#endif /* CONFIG_PPC_STD_MMU_64 */
 #ifdef CONFIG_PPC64 
 	if (cpu_has_feature(CPU_FTR_DSCR)) {
 		p->thread.dscr_inherit = current->thread.dscr_inherit;

commit 13b3d13b813ab834fac67dc05f8b86dbcc29c134
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 10 12:29:20 2014 +1000

    powerpc: Remove MMU_FTR_SLB
    
    We now only support cpus that use an SLB, so we don't need an MMU
    feature to indicate that.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index be99774d3f44..e39f388fc25c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1175,7 +1175,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 #endif
 
 #ifdef CONFIG_PPC_STD_MMU_64
-	if (mmu_has_feature(MMU_FTR_SLB)) {
+	{
 		unsigned long sp_vsid;
 		unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
 

commit 96d016108640bc2b7fb0ee800737f80923847294
Author: Sam bobroff <sam.bobroff@au1.ibm.com>
Date:   Thu Jun 5 16:19:22 2014 +1000

    powerpc: Correct DSCR during TM context switch
    
    Correct the DSCR SPR becoming temporarily corrupted if a task is
    context switched during a transaction.
    
    The problem occurs while suspending the task and is caused by saving
    the DSCR to thread.dscr after it has already been set to the CPU's
    default value:
    
    __switch_to() calls __switch_to_tm()
            which calls tm_reclaim_task()
            which calls tm_reclaim_thread()
            which calls tm_reclaim()
                    where the DSCR is set to the CPU's default
    __switch_to() calls _switch()
                    where thread.dscr is set to the DSCR
    
    When the task is resumed, it's transaction will be doomed (as usual)
    and the DSCR SPR will be corrupted, although the checkpointed value
    will be correct. Therefore the DSCR will be immediately corrected by
    the transaction aborting, unless it has been suspended. In that case
    the incorrect value can be seen by the task until it resumes the
    transaction.
    
    The fix is to treat the DSCR similarly to the TAR and save it early
    in __switch_to().
    
    A program exposing the problem is added to the kernel self tests as:
    tools/testing/selftests/powerpc/tm/tm-resched-dscr.
    
    Signed-off-by: Sam Bobroff <sam.bobroff@au1.ibm.com>
    CC: <stable@vger.kernel.org> [v3.10+]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8a1edbe26b8f..be99774d3f44 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -755,15 +755,15 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	WARN_ON(!irqs_disabled());
 
-	/* Back up the TAR across context switches.
+	/* Back up the TAR and DSCR across context switches.
 	 * Note that the TAR is not available for use in the kernel.  (To
 	 * provide this, the TAR should be backed up/restored on exception
 	 * entry/exit instead, and be in pt_regs.  FIXME, this should be in
 	 * pt_regs anyway (for debug).)
-	 * Save the TAR here before we do treclaim/trecheckpoint as these
-	 * will change the TAR.
+	 * Save the TAR and DSCR here before we do treclaim/trecheckpoint as
+	 * these will change them.
 	 */
-	save_tar(&prev->thread);
+	save_early_sprs(&prev->thread);
 
 	__switch_to_tm(prev);
 

commit 21f585073d6347651f2262da187606fa1c4ee16d
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Apr 29 15:25:17 2014 -0400

    powerpc: Fix smp_processor_id() in preemptible splat in set_breakpoint
    
    Currently, on 8641D, which doesn't set CONFIG_HAVE_HW_BREAKPOINT
    we get the following splat:
    
    BUG: using smp_processor_id() in preemptible [00000000] code: login/1382
    caller is set_breakpoint+0x1c/0xa0
    CPU: 0 PID: 1382 Comm: login Not tainted 3.15.0-rc3-00041-g2aafe1a4d451 #1
    Call Trace:
    [decd5d80] [c0008dc4] show_stack+0x50/0x158 (unreliable)
    [decd5dc0] [c03c6fa0] dump_stack+0x7c/0xdc
    [decd5de0] [c01f8818] check_preemption_disabled+0xf4/0x104
    [decd5e00] [c00086b8] set_breakpoint+0x1c/0xa0
    [decd5e10] [c00d4530] flush_old_exec+0x2bc/0x588
    [decd5e40] [c011c468] load_elf_binary+0x2ac/0x1164
    [decd5ec0] [c00d35f8] search_binary_handler+0xc4/0x1f8
    [decd5ef0] [c00d4ee8] do_execve+0x3d8/0x4b8
    [decd5f40] [c001185c] ret_from_syscall+0x0/0x38
     --- Exception: c01 at 0xfeee554
        LR = 0xfeee7d4
    
    The call path in this case is:
    
            flush_thread
               --> set_debug_reg_defaults
                 --> set_breakpoint
                   --> __get_cpu_var
    
    Since preemption is enabled in the cleanup of flush thread, and
    there is no need to disable it, introduce the distinction between
    set_breakpoint and __set_breakpoint, leaving only the flush_thread
    instance as the current user of set_breakpoint.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f895a5062287..8a1edbe26b8f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -496,7 +496,7 @@ static inline int set_dawr(struct arch_hw_breakpoint *brk)
 	return 0;
 }
 
-void set_breakpoint(struct arch_hw_breakpoint *brk)
+void __set_breakpoint(struct arch_hw_breakpoint *brk)
 {
 	__get_cpu_var(current_brk) = *brk;
 
@@ -506,6 +506,13 @@ void set_breakpoint(struct arch_hw_breakpoint *brk)
 		set_dabr(brk);
 }
 
+void set_breakpoint(struct arch_hw_breakpoint *brk)
+{
+	preempt_disable();
+	__set_breakpoint(brk);
+	preempt_enable();
+}
+
 #ifdef CONFIG_PPC64
 DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);
 #endif
@@ -835,7 +842,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
 	if (unlikely(!hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))
-		set_breakpoint(&new->thread.hw_brk);
+		__set_breakpoint(&new->thread.hw_brk);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 

commit 04c32a516806ec74b62048baf4cddcbb840927db
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Tue Apr 29 15:25:16 2014 -0400

    powerpc: Drop return value from set_breakpoint as it is unused
    
    None of the callers check the return value, so it might as
    well not have one at all.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2ae1b99166c6..f895a5062287 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -496,14 +496,14 @@ static inline int set_dawr(struct arch_hw_breakpoint *brk)
 	return 0;
 }
 
-int set_breakpoint(struct arch_hw_breakpoint *brk)
+void set_breakpoint(struct arch_hw_breakpoint *brk)
 {
 	__get_cpu_var(current_brk) = *brk;
 
 	if (cpu_has_feature(CPU_FTR_DAWR))
-		return set_dawr(brk);
-
-	return set_dabr(brk);
+		set_dawr(brk);
+	else
+		set_dabr(brk);
 }
 
 #ifdef CONFIG_PPC64

commit 7cedd6014bfe353d4b552ed8d54d63f6e06e26ba
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Feb 4 16:08:51 2014 +1100

    powerpc: Fix kernel thread creation on ABIv2
    
    Change how we setup registers for ret_from_kernel_thread. In
    ABIv1, instead of passing a function descriptor in, dereference
    it and pass the target in directly.
    
    Use ppc_global_function_entry to get it right on both ABIv1 and ABIv2.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 31d021506d21..2ae1b99166c6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -54,6 +54,7 @@
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
 #endif
+#include <asm/code-patching.h>
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 
@@ -1108,7 +1109,9 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		struct thread_info *ti = (void *)task_stack_page(p);
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
-		childregs->gpr[14] = usp;	/* function */
+		/* function */
+		if (usp)
+			childregs->gpr[14] = ppc_function_entry((void *)usp);
 #ifdef CONFIG_PPC64
 		clear_tsk_thread_flag(p, TIF_32BIT);
 		childregs->softe = 1;
@@ -1187,17 +1190,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	if (cpu_has_feature(CPU_FTR_HAS_PPR))
 		p->thread.ppr = INIT_PPR;
 #endif
-	/*
-	 * The PPC64 ABI makes use of a TOC to contain function 
-	 * pointers.  The function (ret_from_except) is actually a pointer
-	 * to the TOC entry.  The first entry is a pointer to the actual
-	 * function.
-	 */
-#ifdef CONFIG_PPC64
-	kregs->nip = *((unsigned long *)f);
-#else
-	kregs->nip = (unsigned long)f;
-#endif
+	kregs->nip = ppc_function_entry(f);
 	return 0;
 }
 

commit e6b8fd028b584ffca7a7255b8971f254932c9fce
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Apr 4 20:19:48 2014 +1100

    powerpc/tm: Disable IRQ in tm_recheckpoint
    
    We can't take an IRQ when we're about to do a trechkpt as our GPR state is set
    to user GPR values.
    
    We've hit this when running some IBM Java stress tests in the lab resulting in
    the following dump:
    
      cpu 0x3f: Vector: 700 (Program Check) at [c000000007eb3d40]
          pc: c000000000050074: restore_gprs+0xc0/0x148
          lr: 00000000b52a8184
          sp: ac57d360
         msr: 8000000100201030
        current = 0xc00000002c500000
        paca    = 0xc000000007dbfc00     softe: 0     irq_happened: 0x00
          pid   = 34535, comm = Pooled Thread #
      R00 = 00000000b52a8184   R16 = 00000000b3e48fda
      R01 = 00000000ac57d360   R17 = 00000000ade79bd8
      R02 = 00000000ac586930   R18 = 000000000fac9bcc
      R03 = 00000000ade60000   R19 = 00000000ac57f930
      R04 = 00000000f6624918   R20 = 00000000ade79be8
      R05 = 00000000f663f238   R21 = 00000000ac218a54
      R06 = 0000000000000002   R22 = 000000000f956280
      R07 = 0000000000000008   R23 = 000000000000007e
      R08 = 000000000000000a   R24 = 000000000000000c
      R09 = 00000000b6e69160   R25 = 00000000b424cf00
      R10 = 0000000000000181   R26 = 00000000f66256d4
      R11 = 000000000f365ec0   R27 = 00000000b6fdcdd0
      R12 = 00000000f66400f0   R28 = 0000000000000001
      R13 = 00000000ada71900   R29 = 00000000ade5a300
      R14 = 00000000ac2185a8   R30 = 00000000f663f238
      R15 = 0000000000000004   R31 = 00000000f6624918
      pc  = c000000000050074 restore_gprs+0xc0/0x148
      cfar= c00000000004fe28 dont_restore_vec+0x1c/0x1a4
      lr  = 00000000b52a8184
      msr = 8000000100201030   cr  = 24804888
      ctr = 0000000000000000   xer = 0000000000000000   trap =  700
    
    This moves tm_recheckpoint to a C function and moves the tm_restore_sprs into
    that function.  It then adds IRQ disabling over the trechkpt critical section.
    It also sets the TEXASR FS in the signals code to ensure this is never set now
    that we explictly write the TM sprs in tm_recheckpoint.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    cc: stable@vger.kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index af064d28b365..31d021506d21 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -610,6 +610,31 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 	tm_save_sprs(thr);
 }
 
+extern void __tm_recheckpoint(struct thread_struct *thread,
+			      unsigned long orig_msr);
+
+void tm_recheckpoint(struct thread_struct *thread,
+		     unsigned long orig_msr)
+{
+	unsigned long flags;
+
+	/* We really can't be interrupted here as the TEXASR registers can't
+	 * change and later in the trecheckpoint code, we have a userspace R1.
+	 * So let's hard disable over this region.
+	 */
+	local_irq_save(flags);
+	hard_irq_disable();
+
+	/* The TM SPRs are restored here, so that TEXASR.FS can be set
+	 * before the trecheckpoint and no explosion occurs.
+	 */
+	tm_restore_sprs(thread);
+
+	__tm_recheckpoint(thread, orig_msr);
+
+	local_irq_restore(flags);
+}
+
 static inline void tm_recheckpoint_new_task(struct task_struct *new)
 {
 	unsigned long msr;
@@ -628,13 +653,10 @@ static inline void tm_recheckpoint_new_task(struct task_struct *new)
 	if (!new->thread.regs)
 		return;
 
-	/* The TM SPRs are restored here, so that TEXASR.FS can be set
-	 * before the trecheckpoint and no explosion occurs.
-	 */
-	tm_restore_sprs(&new->thread);
-
-	if (!MSR_TM_ACTIVE(new->thread.regs->msr))
+	if (!MSR_TM_ACTIVE(new->thread.regs->msr)){
+		tm_restore_sprs(&new->thread);
 		return;
+	}
 	msr = new->thread.tm_orig_msr;
 	/* Recheckpoint to restore original checkpointed register state. */
 	TM_DEBUG("*** tm_recheckpoint of pid %d "

commit 621b5060e823301d0cba4cb52a7ee3491922d291
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon Mar 3 14:21:40 2014 +1100

    powerpc/tm: Fix crash when forking inside a transaction
    
    When we fork/clone we currently don't copy any of the TM state to the new
    thread.  This results in a TM bad thing (program check) when the new process is
    switched in as the kernel does a tmrechkpt with TEXASR FS not set.  Also, since
    R1 is from userspace, we trigger the bad kernel stack pointer detection.  So we
    end up with something like this:
    
       Bad kernel stack pointer 0 at c0000000000404fc
       cpu 0x2: Vector: 700 (Program Check) at [c00000003ffefd40]
           pc: c0000000000404fc: restore_gprs+0xc0/0x148
           lr: 0000000000000000
           sp: 0
          msr: 9000000100201030
         current = 0xc000001dd1417c30
         paca    = 0xc00000000fe00800   softe: 0        irq_happened: 0x01
           pid   = 0, comm = swapper/2
       WARNING: exception is not recoverable, can't continue
    
    The below fixes this by flushing the TM state before we copy the task_struct to
    the clone.  To do this we go through the tmreclaim patch, which removes the
    checkpointed registers from the CPU and transitions the CPU out of TM suspend
    mode.  Hence we need to call tmrechkpt after to restore the checkpointed state
    and the TM mode for the current task.
    
    To make this fail from userspace is simply:
            tbegin
            li      r0, 2
            sc
            <boom>
    
    Kudos to Adhemerval Zanella Neto for finding this.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    cc: Adhemerval Zanella Neto <azanella@br.ibm.com>
    cc: stable@vger.kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8d4c247f1738..af064d28b365 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1048,6 +1048,15 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	flush_altivec_to_thread(src);
 	flush_vsx_to_thread(src);
 	flush_spe_to_thread(src);
+	/*
+	 * Flush TM state out so we can copy it.  __switch_to_tm() does this
+	 * flush but it removes the checkpointed state from the current CPU and
+	 * transitions the CPU out of TM mode.  Hence we need to call
+	 * tm_recheckpoint_new_task() (on the same task) to restore the
+	 * checkpointed state back and the TM mode.
+	 */
+	__switch_to_tm(src);
+	tm_recheckpoint_new_task(src);
 
 	*dst = *src;
 

commit 1c430c06d0ce871f36b2af504d45a07356e44800
Author: Andreas Schwab <schwab@linux-m68k.org>
Date:   Tue Jan 21 23:24:02 2014 +0100

    powerpc: Fix hw breakpoints on !HAVE_HW_BREAKPOINT configurations
    
    This fixes a logic error that caused a failure to update the hw breakpoint
    registers when not using the hw-breakpoint interface.
    
    Signed-off-by: Andreas Schwab <schwab@linux-m68k.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 64b7a6e61dd1..8d4c247f1738 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -811,7 +811,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
  * schedule DABR
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
-	if (unlikely(hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))
+	if (unlikely(!hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))
 		set_breakpoint(&new->thread.hw_brk);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif

commit 1b17366d695c8ab03f98d0155357e97a427e1dce
Merge: d12de1ef5eba 7179ba52889b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 27 21:11:26 2014 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "So here's my next branch for powerpc.  A bit late as I was on vacation
      last week.  It's mostly the same stuff that was in next already, I
      just added two patches today which are the wiring up of lockref for
      powerpc, which for some reason fell through the cracks last time and
      is trivial.
    
      The highlights are, in addition to a bunch of bug fixes:
    
       - Reworked Machine Check handling on kernels running without a
         hypervisor (or acting as a hypervisor).  Provides hooks to handle
         some errors in real mode such as TLB errors, handle SLB errors,
         etc...
    
       - Support for retrieving memory error information from the service
         processor on IBM servers running without a hypervisor and routing
         them to the memory poison infrastructure.
    
       - _PAGE_NUMA support on server processors
    
       - 32-bit BookE relocatable kernel support
    
       - FSL e6500 hardware tablewalk support
    
       - A bunch of new/revived board support
    
       - FSL e6500 deeper idle states and altivec powerdown support
    
      You'll notice a generic mm change here, it has been acked by the
      relevant authorities and is a pre-req for our _PAGE_NUMA support"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (121 commits)
      powerpc: Implement arch_spin_is_locked() using arch_spin_value_unlocked()
      powerpc: Add support for the optimised lockref implementation
      powerpc/powernv: Call OPAL sync before kexec'ing
      powerpc/eeh: Escalate error on non-existing PE
      powerpc/eeh: Handle multiple EEH errors
      powerpc: Fix transactional FP/VMX/VSX unavailable handlers
      powerpc: Don't corrupt transactional state when using FP/VMX in kernel
      powerpc: Reclaim two unused thread_info flag bits
      powerpc: Fix races with irq_work
      Move precessing of MCE queued event out from syscall exit path.
      pseries/cpuidle: Remove redundant call to ppc64_runlatch_off() in cpu idle routines
      powerpc: Make add_system_ram_resources() __init
      powerpc: add SATA_MV to ppc64_defconfig
      powerpc/powernv: Increase candidate fw image size
      powerpc: Add debug checks to catch invalid cpu-to-node mappings
      powerpc: Fix the setup of CPU-to-Node mappings during CPU online
      powerpc/iommu: Don't detach device without IOMMU group
      powerpc/eeh: Hotplug improvement
      powerpc/eeh: Call opal_pci_reinit() on powernv for restoring config space
      powerpc/eeh: Add restore_config operation
      ...

commit fac515db45207718168cb55ca4d0a390e43b61af
Merge: 3ac8ff1c475b d064f30e5063
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jan 15 14:22:35 2014 +1100

    Merge remote-tracking branch 'scott/next' into next
    
    Freescale updates from Scott:
    
    <<
    Highlights include 32-bit booke relocatable support, e6500 hardware
    tablewalk support, various e500 SPE fixes, some new/revived boards, and
    e6500 deeper idle and altivec powerdown modes.
    >>

commit d31626f70b6103f4d9153b75d07e0e8795728cc9
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jan 13 15:56:29 2014 +1100

    powerpc: Don't corrupt transactional state when using FP/VMX in kernel
    
    Currently, when we have a process using the transactional memory
    facilities on POWER8 (that is, the processor is in transactional
    or suspended state), and the process enters the kernel and the
    kernel then uses the floating-point or vector (VMX/Altivec) facility,
    we end up corrupting the user-visible FP/VMX/VSX state.  This
    happens, for example, if a page fault causes a copy-on-write
    operation, because the copy_page function will use VMX to do the
    copy on POWER8.  The test program below demonstrates the bug.
    
    The bug happens because when FP/VMX state for a transactional process
    is stored in the thread_struct, we store the checkpointed state in
    .fp_state/.vr_state and the transactional (current) state in
    .transact_fp/.transact_vr.  However, when the kernel wants to use
    FP/VMX, it calls enable_kernel_fp() or enable_kernel_altivec(),
    which saves the current state in .fp_state/.vr_state.  Furthermore,
    when we return to the user process we return with FP/VMX/VSX
    disabled.  The next time the process uses FP/VMX/VSX, we don't know
    which set of state (the current register values, .fp_state/.vr_state,
    or .transact_fp/.transact_vr) we should be using, since we have no
    way to tell if we are still in the same transaction, and if not,
    whether the previous transaction succeeded or failed.
    
    Thus it is necessary to strictly adhere to the rule that if FP has
    been enabled at any point in a transaction, we must keep FP enabled
    for the user process with the current transactional state in the
    FP registers, until we detect that it is no longer in a transaction.
    Similarly for VMX; once enabled it must stay enabled until the
    process is no longer transactional.
    
    In order to keep this rule, we add a new thread_info flag which we
    test when returning from the kernel to userspace, called TIF_RESTORE_TM.
    This flag indicates that there is FP/VMX/VSX state to be restored
    before entering userspace, and when it is set the .tm_orig_msr field
    in the thread_struct indicates what state needs to be restored.
    The restoration is done by restore_tm_state().  The TIF_RESTORE_TM
    bit is set by new giveup_fpu/altivec_maybe_transactional helpers,
    which are called from enable_kernel_fp/altivec, giveup_vsx, and
    flush_fp/altivec_to_thread instead of giveup_fpu/altivec.
    
    The other thing to be done is to get the transactional FP/VMX/VSX
    state from .fp_state/.vr_state when doing reclaim, if that state
    has been saved there by giveup_fpu/altivec_maybe_transactional.
    Having done this, we set the FP/VMX bit in the thread's MSR after
    reclaim to indicate that that part of the state is now valid
    (having been reclaimed from the processor's checkpointed state).
    
    Finally, in the signal handling code, we move the clearing of the
    transactional state bits in the thread's MSR a bit earlier, before
    calling flush_fp_to_thread(), so that we don't unnecessarily set
    the TIF_RESTORE_TM bit.
    
    This is the test program:
    
    /* Michael Neuling 4/12/2013
     *
     * See if the altivec state is leaked out of an aborted transaction due to
     * kernel vmx copy loops.
     *
     *   gcc -m64 htm_vmxcopy.c -o htm_vmxcopy
     *
     */
    
    /* We don't use all of these, but for reference: */
    
    int main(int argc, char *argv[])
    {
            long double vecin = 1.3;
            long double vecout;
            unsigned long pgsize = getpagesize();
            int i;
            int fd;
            int size = pgsize*16;
            char tmpfile[] = "/tmp/page_faultXXXXXX";
            char buf[pgsize];
            char *a;
            uint64_t aborted = 0;
    
            fd = mkstemp(tmpfile);
            assert(fd >= 0);
    
            memset(buf, 0, pgsize);
            for (i = 0; i < size; i += pgsize)
                    assert(write(fd, buf, pgsize) == pgsize);
    
            unlink(tmpfile);
    
            a = mmap(NULL, size, PROT_READ|PROT_WRITE, MAP_PRIVATE, fd, 0);
            assert(a != MAP_FAILED);
    
            asm __volatile__(
                    "lxvd2x 40,0,%[vecinptr] ; " // set 40 to initial value
                    TBEGIN
                    "beq    3f ;"
                    TSUSPEND
                    "xxlxor 40,40,40 ; " // set 40 to 0
                    "std    5, 0(%[map]) ;" // cause kernel vmx copy page
                    TABORT
                    TRESUME
                    TEND
                    "li     %[res], 0 ;"
                    "b      5f ;"
                    "3: ;" // Abort handler
                    "li     %[res], 1 ;"
                    "5: ;"
                    "stxvd2x 40,0,%[vecoutptr] ; "
                    : [res]"=r"(aborted)
                    : [vecinptr]"r"(&vecin),
                      [vecoutptr]"r"(&vecout),
                      [map]"r"(a)
                    : "memory", "r0", "r3", "r4", "r5", "r6", "r7");
    
            if (aborted && (vecin != vecout)){
                    printf("FAILED: vector state leaked on abort %f != %f\n",
                           (double)vecin, (double)vecout);
                    exit(1);
            }
    
            munmap(a, size);
    
            close(fd);
    
            printf("PASSED!\n");
            return 0;
    }
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index bf8e136316e2..3573d186505f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -73,6 +73,48 @@ struct task_struct *last_task_used_vsx = NULL;
 struct task_struct *last_task_used_spe = NULL;
 #endif
 
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+void giveup_fpu_maybe_transactional(struct task_struct *tsk)
+{
+	/*
+	 * If we are saving the current thread's registers, and the
+	 * thread is in a transactional state, set the TIF_RESTORE_TM
+	 * bit so that we know to restore the registers before
+	 * returning to userspace.
+	 */
+	if (tsk == current && tsk->thread.regs &&
+	    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
+	    !test_thread_flag(TIF_RESTORE_TM)) {
+		tsk->thread.tm_orig_msr = tsk->thread.regs->msr;
+		set_thread_flag(TIF_RESTORE_TM);
+	}
+
+	giveup_fpu(tsk);
+}
+
+void giveup_altivec_maybe_transactional(struct task_struct *tsk)
+{
+	/*
+	 * If we are saving the current thread's registers, and the
+	 * thread is in a transactional state, set the TIF_RESTORE_TM
+	 * bit so that we know to restore the registers before
+	 * returning to userspace.
+	 */
+	if (tsk == current && tsk->thread.regs &&
+	    MSR_TM_ACTIVE(tsk->thread.regs->msr) &&
+	    !test_thread_flag(TIF_RESTORE_TM)) {
+		tsk->thread.tm_orig_msr = tsk->thread.regs->msr;
+		set_thread_flag(TIF_RESTORE_TM);
+	}
+
+	giveup_altivec(tsk);
+}
+
+#else
+#define giveup_fpu_maybe_transactional(tsk)	giveup_fpu(tsk)
+#define giveup_altivec_maybe_transactional(tsk)	giveup_altivec(tsk)
+#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
+
 #ifdef CONFIG_PPC_FPU
 /*
  * Make sure the floating-point register state in the
@@ -101,13 +143,13 @@ void flush_fp_to_thread(struct task_struct *tsk)
 			 */
 			BUG_ON(tsk != current);
 #endif
-			giveup_fpu(tsk);
+			giveup_fpu_maybe_transactional(tsk);
 		}
 		preempt_enable();
 	}
 }
 EXPORT_SYMBOL_GPL(flush_fp_to_thread);
-#endif
+#endif /* CONFIG_PPC_FPU */
 
 void enable_kernel_fp(void)
 {
@@ -115,11 +157,11 @@ void enable_kernel_fp(void)
 
 #ifdef CONFIG_SMP
 	if (current->thread.regs && (current->thread.regs->msr & MSR_FP))
-		giveup_fpu(current);
+		giveup_fpu_maybe_transactional(current);
 	else
 		giveup_fpu(NULL);	/* just enables FP for kernel */
 #else
-	giveup_fpu(last_task_used_math);
+	giveup_fpu_maybe_transactional(last_task_used_math);
 #endif /* CONFIG_SMP */
 }
 EXPORT_SYMBOL(enable_kernel_fp);
@@ -131,11 +173,11 @@ void enable_kernel_altivec(void)
 
 #ifdef CONFIG_SMP
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
-		giveup_altivec(current);
+		giveup_altivec_maybe_transactional(current);
 	else
 		giveup_altivec_notask();
 #else
-	giveup_altivec(last_task_used_altivec);
+	giveup_altivec_maybe_transactional(last_task_used_altivec);
 #endif /* CONFIG_SMP */
 }
 EXPORT_SYMBOL(enable_kernel_altivec);
@@ -152,7 +194,7 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 #ifdef CONFIG_SMP
 			BUG_ON(tsk != current);
 #endif
-			giveup_altivec(tsk);
+			giveup_altivec_maybe_transactional(tsk);
 		}
 		preempt_enable();
 	}
@@ -181,8 +223,8 @@ EXPORT_SYMBOL(enable_kernel_vsx);
 
 void giveup_vsx(struct task_struct *tsk)
 {
-	giveup_fpu(tsk);
-	giveup_altivec(tsk);
+	giveup_fpu_maybe_transactional(tsk);
+	giveup_altivec_maybe_transactional(tsk);
 	__giveup_vsx(tsk);
 }
 
@@ -478,7 +520,48 @@ static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
 		return false;
 	return true;
 }
+
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+static void tm_reclaim_thread(struct thread_struct *thr,
+			      struct thread_info *ti, uint8_t cause)
+{
+	unsigned long msr_diff = 0;
+
+	/*
+	 * If FP/VSX registers have been already saved to the
+	 * thread_struct, move them to the transact_fp array.
+	 * We clear the TIF_RESTORE_TM bit since after the reclaim
+	 * the thread will no longer be transactional.
+	 */
+	if (test_ti_thread_flag(ti, TIF_RESTORE_TM)) {
+		msr_diff = thr->tm_orig_msr & ~thr->regs->msr;
+		if (msr_diff & MSR_FP)
+			memcpy(&thr->transact_fp, &thr->fp_state,
+			       sizeof(struct thread_fp_state));
+		if (msr_diff & MSR_VEC)
+			memcpy(&thr->transact_vr, &thr->vr_state,
+			       sizeof(struct thread_vr_state));
+		clear_ti_thread_flag(ti, TIF_RESTORE_TM);
+		msr_diff &= MSR_FP | MSR_VEC | MSR_VSX | MSR_FE0 | MSR_FE1;
+	}
+
+	tm_reclaim(thr, thr->regs->msr, cause);
+
+	/* Having done the reclaim, we now have the checkpointed
+	 * FP/VSX values in the registers.  These might be valid
+	 * even if we have previously called enable_kernel_fp() or
+	 * flush_fp_to_thread(), so update thr->regs->msr to
+	 * indicate their current validity.
+	 */
+	thr->regs->msr |= msr_diff;
+}
+
+void tm_reclaim_current(uint8_t cause)
+{
+	tm_enable();
+	tm_reclaim_thread(&current->thread, current_thread_info(), cause);
+}
+
 static inline void tm_reclaim_task(struct task_struct *tsk)
 {
 	/* We have to work out if we're switching from/to a task that's in the
@@ -501,9 +584,11 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 
 	/* Stash the original thread MSR, as giveup_fpu et al will
 	 * modify it.  We hold onto it to see whether the task used
-	 * FP & vector regs.
+	 * FP & vector regs.  If the TIF_RESTORE_TM flag is set,
+	 * tm_orig_msr is already set.
 	 */
-	thr->tm_orig_msr = thr->regs->msr;
+	if (!test_ti_thread_flag(task_thread_info(tsk), TIF_RESTORE_TM))
+		thr->tm_orig_msr = thr->regs->msr;
 
 	TM_DEBUG("--- tm_reclaim on pid %d (NIP=%lx, "
 		 "ccr=%lx, msr=%lx, trap=%lx)\n",
@@ -511,7 +596,7 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 		 thr->regs->ccr, thr->regs->msr,
 		 thr->regs->trap);
 
-	tm_reclaim(thr, thr->regs->msr, TM_CAUSE_RESCHED);
+	tm_reclaim_thread(thr, task_thread_info(tsk), TM_CAUSE_RESCHED);
 
 	TM_DEBUG("--- tm_reclaim on pid %d complete\n",
 		 tsk->pid);
@@ -587,6 +672,43 @@ static inline void __switch_to_tm(struct task_struct *prev)
 		tm_reclaim_task(prev);
 	}
 }
+
+/*
+ * This is called if we are on the way out to userspace and the
+ * TIF_RESTORE_TM flag is set.  It checks if we need to reload
+ * FP and/or vector state and does so if necessary.
+ * If userspace is inside a transaction (whether active or
+ * suspended) and FP/VMX/VSX instructions have ever been enabled
+ * inside that transaction, then we have to keep them enabled
+ * and keep the FP/VMX/VSX state loaded while ever the transaction
+ * continues.  The reason is that if we didn't, and subsequently
+ * got a FP/VMX/VSX unavailable interrupt inside a transaction,
+ * we don't know whether it's the same transaction, and thus we
+ * don't know which of the checkpointed state and the transactional
+ * state to use.
+ */
+void restore_tm_state(struct pt_regs *regs)
+{
+	unsigned long msr_diff;
+
+	clear_thread_flag(TIF_RESTORE_TM);
+	if (!MSR_TM_ACTIVE(regs->msr))
+		return;
+
+	msr_diff = current->thread.tm_orig_msr & ~regs->msr;
+	msr_diff &= MSR_FP | MSR_VEC | MSR_VSX;
+	if (msr_diff & MSR_FP) {
+		fp_enable();
+		load_fp_state(&current->thread.fp_state);
+		regs->msr |= current->thread.fpexc_mode;
+	}
+	if (msr_diff & MSR_VEC) {
+		vec_enable();
+		load_vr_state(&current->thread.vr_state);
+	}
+	regs->msr |= msr_diff;
+}
+
 #else
 #define tm_recheckpoint_new_task(new)
 #define __switch_to_tm(prev)

commit c141611fb1ee2cfc374cf9be5327e97f361c4bed
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Jan 9 00:44:29 2014 -0500

    powerpc: Delete non-required instances of include <linux/init.h>
    
    None of these files are actually using any __init type directives
    and hence don't need to include <linux/init.h>.  Most are just a
    left over from __devinit and __cpuinit removal, or simply due to
    code getting copied from one driver to the next.
    
    The one instance where we add an include for init.h covers off
    a case where that file was implicitly getting it from another
    header which itself didn't need it.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3386d8ab7eb0..bf8e136316e2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -25,7 +25,6 @@
 #include <linux/slab.h>
 #include <linux/user.h>
 #include <linux/elf.h>
-#include <linux/init.h>
 #include <linux/prctl.h>
 #include <linux/init_task.h>
 #include <linux/export.h>

commit 640e922501103aaf2e0abb4cf4de5d49fa8342f7
Author: Joseph Myers <joseph@codesourcery.com>
Date:   Tue Dec 10 23:07:45 2013 +0000

    powerpc: fix exception clearing in e500 SPE float emulation
    
    The e500 SPE floating-point emulation code clears existing exceptions
    (__FPU_FPSCR &= ~FP_EX_MASK;) before ORing in the exceptions from the
    emulated operation.  However, these exception bits are the "sticky",
    cumulative exception bits, and should only be cleared by the user
    program setting SPEFSCR, not implicitly by any floating-point
    instruction (whether executed purely by the hardware or emulated).
    The spurious clearing of these bits shows up as missing exceptions in
    glibc testing.
    
    Fixing this, however, is not as simple as just not clearing the bits,
    because while the bits may be from previous floating-point operations
    (in which case they should not be cleared), the processor can also set
    the sticky bits itself before the interrupt for an exception occurs,
    and this can happen in cases when IEEE 754 semantics are that the
    sticky bit should not be set.  Specifically, the "invalid" sticky bit
    is set in various cases with non-finite operands, where IEEE 754
    semantics do not involve raising such an exception, and the
    "underflow" sticky bit is set in cases of exact underflow, whereas
    IEEE 754 semantics are that this flag is set only for inexact
    underflow.  Thus, for correct emulation the kernel needs to know the
    setting of these two sticky bits before the instruction being
    emulated.
    
    When a floating-point operation raises an exception, the kernel can
    note the state of the sticky bits immediately afterwards.  Some
    <fenv.h> functions that affect the state of these bits, such as
    fesetenv and feholdexcept, need to use prctl with PR_GET_FPEXC and
    PR_SET_FPEXC anyway, and so it is natural to record the state of those
    bits during that call into the kernel and so avoid any need for a
    separate call into the kernel to inform it of a change to those bits.
    Thus, the interface I chose to use (in this patch and the glibc port)
    is that one of those prctl calls must be made after any userspace
    change to those sticky bits, other than through a floating-point
    operation that traps into the kernel anyway.  feclearexcept and
    fesetexceptflag duly make those calls, which would not be required
    were it not for this issue.
    
    The previous EGLIBC port, and the uClibc code copied from it, is
    fundamentally broken as regards any use of prctl for floating-point
    exceptions because it didn't use the PR_FP_EXC_SW_ENABLE bit in its
    prctl calls (and did various worse things, such as passing a pointer
    when prctl expected an integer).  If you avoid anything where prctl is
    used, the clearing of sticky bits still means it will never give
    anything approximating correct exception semantics with existing
    kernels.  I don't believe the patch makes things any worse for
    existing code that doesn't try to inform the kernel of changes to
    sticky bits - such code may get incorrect exceptions in some cases,
    but it would have done so anyway in other cases.
    
    Signed-off-by: Joseph Myers <joseph@codesourcery.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3386d8ab7eb0..b08c0d03530f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1175,6 +1175,19 @@ int set_fpexc_mode(struct task_struct *tsk, unsigned int val)
 	if (val & PR_FP_EXC_SW_ENABLE) {
 #ifdef CONFIG_SPE
 		if (cpu_has_feature(CPU_FTR_SPE)) {
+			/*
+			 * When the sticky exception bits are set
+			 * directly by userspace, it must call prctl
+			 * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE
+			 * in the existing prctl settings) or
+			 * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in
+			 * the bits being set).  <fenv.h> functions
+			 * saving and restoring the whole
+			 * floating-point environment need to do so
+			 * anyway to restore the prctl settings from
+			 * the saved environment.
+			 */
+			tsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);
 			tsk->thread.fpexc_mode = val &
 				(PR_FP_EXC_SW_ENABLE | PR_FP_ALL_EXCEPT);
 			return 0;
@@ -1206,9 +1219,22 @@ int get_fpexc_mode(struct task_struct *tsk, unsigned long adr)
 
 	if (tsk->thread.fpexc_mode & PR_FP_EXC_SW_ENABLE)
 #ifdef CONFIG_SPE
-		if (cpu_has_feature(CPU_FTR_SPE))
+		if (cpu_has_feature(CPU_FTR_SPE)) {
+			/*
+			 * When the sticky exception bits are set
+			 * directly by userspace, it must call prctl
+			 * with PR_GET_FPEXC (with PR_FP_EXC_SW_ENABLE
+			 * in the existing prctl settings) or
+			 * PR_SET_FPEXC (with PR_FP_EXC_SW_ENABLE in
+			 * the bits being set).  <fenv.h> functions
+			 * saving and restoring the whole
+			 * floating-point environment need to do so
+			 * anyway to restore the prctl settings from
+			 * the saved environment.
+			 */
+			tsk->thread.spefscr_last = mfspr(SPRN_SPEFSCR);
 			val = tsk->thread.fpexc_mode;
-		else
+		} else
 			return -EINVAL;
 #else
 		return -EINVAL;

commit 5e6d26cf4857a98032a4b71b13d5b33163803523
Merge: 319e2e3f63c3 df9059bb6402
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Dec 20 19:13:58 2013 +0100

    Merge tag 'signed-for-3.13' of git://github.com/agraf/linux-2.6 into kvm-master
    
    Patch queue for 3.13 - 2013-12-18
    
    This fixes some grave issues we've only found after 3.13-rc1:
    
      - Make the modularized HV/PR book3s kvm work well as modules
      - Fix some race conditions
      - Fix compilation with certain compilers (booke)
      - Fix THP for book3s_hv
      - Fix preemption for book3s_pr
    
    Alexander Graf (4):
          KVM: PPC: Book3S: PR: Don't clobber our exit handler id
          KVM: PPC: Book3S: PR: Export kvmppc_copy_to|from_svcpu
          KVM: PPC: Book3S: PR: Make svcpu -> vcpu store preempt savvy
          KVM: PPC: Book3S: PR: Enable interrupts earlier
    
    Aneesh Kumar K.V (1):
          powerpc: book3s: kvm: Don't abuse host r2 in exit path
    
    Paul Mackerras (5):
          KVM: PPC: Book3S HV: Fix physical address calculations
          KVM: PPC: Book3S HV: Refine barriers in guest entry/exit
          KVM: PPC: Book3S HV: Make tbacct_lock irq-safe
          KVM: PPC: Book3S HV: Take SRCU read lock around kvm_read_guest() call
          KVM: PPC: Book3S HV: Don't drop low-order page address bits
    
    Scott Wood (1):
          powerpc/kvm/booke: Fix build break due to stack frame size warning
    
    pingfan liu (1):
          powerpc: kvm: fix rare but potential deadlock scene

commit f5f972102d5c12729f0a35fce266b580aaa03f66
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Nov 22 15:52:29 2013 -0600

    powerpc/kvm/booke: Fix build break due to stack frame size warning
    
    Commit ce11e48b7fdd256ec68b932a89b397a790566031 ("KVM: PPC: E500: Add
    userspace debug stub support") added "struct thread_struct" to the
    stack of kvmppc_vcpu_run().  thread_struct is 1152 bytes on my build,
    compared to 48 bytes for the recently-introduced "struct debug_reg".
    Use the latter instead.
    
    This fixes the following error:
    
    cc1: warnings being treated as errors
    arch/powerpc/kvm/booke.c: In function 'kvmppc_vcpu_run':
    arch/powerpc/kvm/booke.c:760:1: error: the frame size of 1424 bytes is larger than 1024 bytes
    make[2]: *** [arch/powerpc/kvm/booke.o] Error 1
    make[1]: *** [arch/powerpc/kvm] Error 2
    make[1]: *** Waiting for unfinished jobs....
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 75c2d1009985..83530af028b8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -339,7 +339,7 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 #endif
 }
 
-static void prime_debug_regs(struct thread_struct *thread)
+static void prime_debug_regs(struct debug_reg *debug)
 {
 	/*
 	 * We could have inherited MSR_DE from userspace, since
@@ -348,22 +348,22 @@ static void prime_debug_regs(struct thread_struct *thread)
 	 */
 	mtmsr(mfmsr() & ~MSR_DE);
 
-	mtspr(SPRN_IAC1, thread->debug.iac1);
-	mtspr(SPRN_IAC2, thread->debug.iac2);
+	mtspr(SPRN_IAC1, debug->iac1);
+	mtspr(SPRN_IAC2, debug->iac2);
 #if CONFIG_PPC_ADV_DEBUG_IACS > 2
-	mtspr(SPRN_IAC3, thread->debug.iac3);
-	mtspr(SPRN_IAC4, thread->debug.iac4);
+	mtspr(SPRN_IAC3, debug->iac3);
+	mtspr(SPRN_IAC4, debug->iac4);
 #endif
-	mtspr(SPRN_DAC1, thread->debug.dac1);
-	mtspr(SPRN_DAC2, thread->debug.dac2);
+	mtspr(SPRN_DAC1, debug->dac1);
+	mtspr(SPRN_DAC2, debug->dac2);
 #if CONFIG_PPC_ADV_DEBUG_DVCS > 0
-	mtspr(SPRN_DVC1, thread->debug.dvc1);
-	mtspr(SPRN_DVC2, thread->debug.dvc2);
+	mtspr(SPRN_DVC1, debug->dvc1);
+	mtspr(SPRN_DVC2, debug->dvc2);
 #endif
-	mtspr(SPRN_DBCR0, thread->debug.dbcr0);
-	mtspr(SPRN_DBCR1, thread->debug.dbcr1);
+	mtspr(SPRN_DBCR0, debug->dbcr0);
+	mtspr(SPRN_DBCR1, debug->dbcr1);
 #ifdef CONFIG_BOOKE
-	mtspr(SPRN_DBCR2, thread->debug.dbcr2);
+	mtspr(SPRN_DBCR2, debug->dbcr2);
 #endif
 }
 /*
@@ -371,11 +371,11 @@ static void prime_debug_regs(struct thread_struct *thread)
  * debug registers, set the debug registers from the values
  * stored in the new thread.
  */
-void switch_booke_debug_regs(struct thread_struct *new_thread)
+void switch_booke_debug_regs(struct debug_reg *new_debug)
 {
 	if ((current->thread.debug.dbcr0 & DBCR0_IDM)
-		|| (new_thread->debug.dbcr0 & DBCR0_IDM))
-			prime_debug_regs(new_thread);
+		|| (new_debug->dbcr0 & DBCR0_IDM))
+			prime_debug_regs(new_debug);
 }
 EXPORT_SYMBOL_GPL(switch_booke_debug_regs);
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
@@ -683,7 +683,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
-	switch_booke_debug_regs(&new->thread);
+	switch_booke_debug_regs(&new->thread.debug);
 #else
 /*
  * For PPC_BOOK3S_64, we use the hw-breakpoint interfaces that would

commit 3bab0bf045e1cc4880e2cfc9351e52cf7ec8e35e
Merge: a5d6e63323fe c13f20ac4832
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 22 08:07:11 2013 -0800

    Merge branch 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull third set of powerpc updates from Benjamin Herrenschmidt:
     "This is a small collection of random bug fixes and a few improvements
      of Oops output which I deemed valuable enough to include as well.
    
      The fixes are essentially recent build breakage and regressions, and a
      couple of older bugs such as the DTL log duplication, the EEH issue
      with PCI_COMMAND_MASTER and the problem with small contexts passed to
      get/set_context with VSX enabled"
    
    * 'merge' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc:
      powerpc/signals: Mark VSX not saved with small contexts
      powerpc/pseries: Fix SMP=n build of rng.c
      powerpc: Make cpu_to_chip_id() available when SMP=n
      powerpc/vio: Fix a dma_mask issue of vio
      powerpc: booke: Fix build failures
      powerpc: ppc64 address space capped at 32TB, mmap randomisation disabled
      powerpc: Only print PACATMSCRATCH in oops when TM is active
      powerpc/pseries: Duplicate dtl entries sometimes sent to userspace
      powerpc: Remove a few lines of oops output
      powerpc: Print DAR and DSISR on machine check oopses
      powerpc: Fix __get_user_pages_fast() irq handling
      powerpc/eeh: More accurate log
      powerpc/eeh: Enable PCI_COMMAND_MASTER for PCI bridges

commit 6d888d1ab0000dff8ea2901bcdf5d213f2a54e8b
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Nov 18 13:19:17 2013 +1100

    powerpc: Only print PACATMSCRATCH in oops when TM is active
    
    If TM is not active there is no need to print PACATMSCRATCH
    so we can save ourselves a line.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 373980ae52ad..6cd50ac12219 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -871,7 +871,8 @@ void show_regs(struct pt_regs * regs)
 	printk("SOFTE: %ld ", regs->softe);
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-	printk("\nPACATMSCRATCH: %016llx ", get_paca()->tm_scratch);
+	if (MSR_TM_ACTIVE(regs->msr))
+		printk("\nPACATMSCRATCH: %016llx ", get_paca()->tm_scratch);
 #endif
 
 	for (i = 0;  i < 32;  i++) {

commit 9db8bcfd73d4a18c0b3428c30557ccce1171d0af
Author: Anton Blanchard <anton@samba.org>
Date:   Fri Nov 15 15:48:38 2013 +1100

    powerpc: Remove a few lines of oops output
    
    We waste quite a few lines in our oops output:
    
    ...
    MSR: 8000000000009032 <SF,EE,ME,IR,DR,RI>  CR: 28044024  XER: 00000000
    SOFTE: 0
    CFAR: 0000000000009088
    DAR: 000000000000001c, DSISR: 40000000
    
    GPR00: c0000000000c74f0 c00000037cc1b010 c000000000d2bb30 0000000000000000
    ...
    
    We can do a better job here and remove 3 lines:
    
    MSR: 8000000000009032 <SF,EE,ME,IR,DR,RI>  CR: 28044024  XER: 00000000
    CFAR: 0000000000009088 DAR: 0000000000000010, DSISR: 40000000 SOFTE: 1
    GPR00: c0000000000e3d10 c00000037cc2fda0 c000000000d2c3a8 0000000000000001
    
    Also move PACATMSCRATCH up, it doesn't really belong in the stack
    trace section.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 37c4103a8cff..373980ae52ad 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -858,17 +858,20 @@ void show_regs(struct pt_regs * regs)
 	printk("MSR: "REG" ", regs->msr);
 	printbits(regs->msr, msr_bits);
 	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
-#ifdef CONFIG_PPC64
-	printk("SOFTE: %ld\n", regs->softe);
-#endif
 	trap = TRAP(regs);
 	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
-		printk("CFAR: "REG"\n", regs->orig_gpr3);
+		printk("CFAR: "REG" ", regs->orig_gpr3);
 	if (trap == 0x200 || trap == 0x300 || trap == 0x600)
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
-		printk("DEAR: "REG", ESR: "REG"\n", regs->dar, regs->dsisr);
+		printk("DEAR: "REG" ESR: "REG" ", regs->dar, regs->dsisr);
 #else
-		printk("DAR: "REG", DSISR: %08lx\n", regs->dar, regs->dsisr);
+		printk("DAR: "REG" DSISR: %08lx ", regs->dar, regs->dsisr);
+#endif
+#ifdef CONFIG_PPC64
+	printk("SOFTE: %ld ", regs->softe);
+#endif
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	printk("\nPACATMSCRATCH: %016llx ", get_paca()->tm_scratch);
 #endif
 
 	for (i = 0;  i < 32;  i++) {
@@ -886,9 +889,6 @@ void show_regs(struct pt_regs * regs)
 	 */
 	printk("NIP ["REG"] %pS\n", regs->nip, (void *)regs->nip);
 	printk("LR ["REG"] %pS\n", regs->link, (void *)regs->link);
-#endif
-#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-	printk("PACATMSCRATCH [%llx]\n", get_paca()->tm_scratch);
 #endif
 	show_stack(current, (unsigned long *) regs->gpr[1]);
 	if (!user_mode(regs))

commit c54006491dde7d1b8050c5542716b751be92ed80
Author: Anton Blanchard <anton@samba.org>
Date:   Fri Nov 15 15:41:19 2013 +1100

    powerpc: Print DAR and DSISR on machine check oopses
    
    Machine check exceptions set DAR and DSISR, so print them in our
    oops output.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 75c2d1009985..37c4103a8cff 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -864,7 +864,7 @@ void show_regs(struct pt_regs * regs)
 	trap = TRAP(regs);
 	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
 		printk("CFAR: "REG"\n", regs->orig_gpr3);
-	if (trap == 0x300 || trap == 0x600)
+	if (trap == 0x200 || trap == 0x300 || trap == 0x600)
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
 		printk("DEAR: "REG", ESR: "REG"\n", regs->dar, regs->dsisr);
 #else

commit 94af3abf995b17f6a008b00152c94841242ec6c7
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Wed Nov 20 22:15:02 2013 +1100

    powerpc: ELF2 binaries launched directly.
    
    No function descriptor, but we set r12 up and set TIF_RESTOREALL as it
    normally isn't restored on return from syscall.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 75c2d1009985..0650e18169f8 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1086,25 +1086,45 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	regs->msr = MSR_USER;
 #else
 	if (!is_32bit_task()) {
-		unsigned long entry, toc;
+		unsigned long entry;
 
-		/* start is a relocated pointer to the function descriptor for
-		 * the elf _start routine.  The first entry in the function
-		 * descriptor is the entry address of _start and the second
-		 * entry is the TOC value we need to use.
-		 */
-		__get_user(entry, (unsigned long __user *)start);
-		__get_user(toc, (unsigned long __user *)start+1);
+		if (is_elf2_task()) {
+			/* Look ma, no function descriptors! */
+			entry = start;
 
-		/* Check whether the e_entry function descriptor entries
-		 * need to be relocated before we can use them.
-		 */
-		if (load_addr != 0) {
-			entry += load_addr;
-			toc   += load_addr;
+			/*
+			 * Ulrich says:
+			 *   The latest iteration of the ABI requires that when
+			 *   calling a function (at its global entry point),
+			 *   the caller must ensure r12 holds the entry point
+			 *   address (so that the function can quickly
+			 *   establish addressability).
+			 */
+			regs->gpr[12] = start;
+			/* Make sure that's restored on entry to userspace. */
+			set_thread_flag(TIF_RESTOREALL);
+		} else {
+			unsigned long toc;
+
+			/* start is a relocated pointer to the function
+			 * descriptor for the elf _start routine.  The first
+			 * entry in the function descriptor is the entry
+			 * address of _start and the second entry is the TOC
+			 * value we need to use.
+			 */
+			__get_user(entry, (unsigned long __user *)start);
+			__get_user(toc, (unsigned long __user *)start+1);
+
+			/* Check whether the e_entry function descriptor entries
+			 * need to be relocated before we can use them.
+			 */
+			if (load_addr != 0) {
+				entry += load_addr;
+				toc   += load_addr;
+			}
+			regs->gpr[2] = toc;
 		}
 		regs->nip = entry;
-		regs->gpr[2] = toc;
 		regs->msr = MSR_USER64;
 	} else {
 		regs->nip = start;

commit 7ba5fef7d9e1880635cbb2fd698e8a24dc366d0f
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Oct 2 17:15:14 2013 +1000

    powerpc/tm: Remove interrupt disable in __switch_to()
    
    We currently turn IRQs off in __switch_to(0 but this is unnecessary as it's
    already disabled in the caller.
    
    This removes the IRQ disable but adds a check to make sure it is really off
    in case this changes in future.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4d42c4de8b9b..75c2d1009985 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -597,12 +597,13 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {
 	struct thread_struct *new_thread, *old_thread;
-	unsigned long flags;
 	struct task_struct *last;
 #ifdef CONFIG_PPC_BOOK3S_64
 	struct ppc64_tlb_batch *batch;
 #endif
 
+	WARN_ON(!irqs_disabled());
+
 	/* Back up the TAR across context switches.
 	 * Note that the TAR is not available for use in the kernel.  (To
 	 * provide this, the TAR should be backed up/restored on exception
@@ -722,8 +723,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
-	local_irq_save(flags);
-
 	/*
 	 * We can't take a PMU exception inside _switch() since there is a
 	 * window where the kernel stack SLB and the kernel stack are out
@@ -743,8 +742,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
-	local_irq_restore(flags);
-
 	return last;
 }
 

commit 3743c9b8ceb638b6e4b78b42f2262e22aa6359f0
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 12:27:44 2013 +0530

    powerpc: export debug registers save function for KVM
    
    KVM need this function when switching from vcpu to user-space
    thread. My subsequent patch will use this function.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3db9d7e39f39..4d42c4de8b9b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -371,12 +371,13 @@ static void prime_debug_regs(struct thread_struct *thread)
  * debug registers, set the debug registers from the values
  * stored in the new thread.
  */
-static void switch_booke_debug_regs(struct thread_struct *new_thread)
+void switch_booke_debug_regs(struct thread_struct *new_thread)
 {
 	if ((current->thread.debug.dbcr0 & DBCR0_IDM)
 		|| (new_thread->debug.dbcr0 & DBCR0_IDM))
 			prime_debug_regs(new_thread);
 }
+EXPORT_SYMBOL_GPL(switch_booke_debug_regs);
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
 static void set_debug_reg_defaults(struct thread_struct *thread)

commit 51ae8d4a2b9e4aa9a502061b9c39168e08829b94
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 11:45:46 2013 +0530

    powerpc: move debug registers in a structure
    
    This way we can use same data type struct with KVM and
    also help in using other debug related function.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    [scottwood@freescale.com: removed obvious debug_reg comment]
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 83079ef159b9..3db9d7e39f39 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -314,28 +314,28 @@ static DEFINE_PER_CPU(struct arch_hw_breakpoint, current_brk);
  */
 static void set_debug_reg_defaults(struct thread_struct *thread)
 {
-	thread->iac1 = thread->iac2 = 0;
+	thread->debug.iac1 = thread->debug.iac2 = 0;
 #if CONFIG_PPC_ADV_DEBUG_IACS > 2
-	thread->iac3 = thread->iac4 = 0;
+	thread->debug.iac3 = thread->debug.iac4 = 0;
 #endif
-	thread->dac1 = thread->dac2 = 0;
+	thread->debug.dac1 = thread->debug.dac2 = 0;
 #if CONFIG_PPC_ADV_DEBUG_DVCS > 0
-	thread->dvc1 = thread->dvc2 = 0;
+	thread->debug.dvc1 = thread->debug.dvc2 = 0;
 #endif
-	thread->dbcr0 = 0;
+	thread->debug.dbcr0 = 0;
 #ifdef CONFIG_BOOKE
 	/*
 	 * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)
 	 */
-	thread->dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |
+	thread->debug.dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |
 			DBCR1_IAC3US | DBCR1_IAC4US;
 	/*
 	 * Force Data Address Compare User/Supervisor bits to be User-only
 	 * (0b11 MSR[PR]=1) and set all other bits in DBCR2 register to be 0.
 	 */
-	thread->dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;
+	thread->debug.dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;
 #else
-	thread->dbcr1 = 0;
+	thread->debug.dbcr1 = 0;
 #endif
 }
 
@@ -348,22 +348,22 @@ static void prime_debug_regs(struct thread_struct *thread)
 	 */
 	mtmsr(mfmsr() & ~MSR_DE);
 
-	mtspr(SPRN_IAC1, thread->iac1);
-	mtspr(SPRN_IAC2, thread->iac2);
+	mtspr(SPRN_IAC1, thread->debug.iac1);
+	mtspr(SPRN_IAC2, thread->debug.iac2);
 #if CONFIG_PPC_ADV_DEBUG_IACS > 2
-	mtspr(SPRN_IAC3, thread->iac3);
-	mtspr(SPRN_IAC4, thread->iac4);
+	mtspr(SPRN_IAC3, thread->debug.iac3);
+	mtspr(SPRN_IAC4, thread->debug.iac4);
 #endif
-	mtspr(SPRN_DAC1, thread->dac1);
-	mtspr(SPRN_DAC2, thread->dac2);
+	mtspr(SPRN_DAC1, thread->debug.dac1);
+	mtspr(SPRN_DAC2, thread->debug.dac2);
 #if CONFIG_PPC_ADV_DEBUG_DVCS > 0
-	mtspr(SPRN_DVC1, thread->dvc1);
-	mtspr(SPRN_DVC2, thread->dvc2);
+	mtspr(SPRN_DVC1, thread->debug.dvc1);
+	mtspr(SPRN_DVC2, thread->debug.dvc2);
 #endif
-	mtspr(SPRN_DBCR0, thread->dbcr0);
-	mtspr(SPRN_DBCR1, thread->dbcr1);
+	mtspr(SPRN_DBCR0, thread->debug.dbcr0);
+	mtspr(SPRN_DBCR1, thread->debug.dbcr1);
 #ifdef CONFIG_BOOKE
-	mtspr(SPRN_DBCR2, thread->dbcr2);
+	mtspr(SPRN_DBCR2, thread->debug.dbcr2);
 #endif
 }
 /*
@@ -373,8 +373,8 @@ static void prime_debug_regs(struct thread_struct *thread)
  */
 static void switch_booke_debug_regs(struct thread_struct *new_thread)
 {
-	if ((current->thread.dbcr0 & DBCR0_IDM)
-		|| (new_thread->dbcr0 & DBCR0_IDM))
+	if ((current->thread.debug.dbcr0 & DBCR0_IDM)
+		|| (new_thread->debug.dbcr0 & DBCR0_IDM))
 			prime_debug_regs(new_thread);
 }
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */

commit 660970fe97f86c0175576b4a942ebd29fb2ec64e
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 11:45:45 2013 +0530

    powerpc: remove unnecessary line continuations
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8649a3d629e1..83079ef159b9 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -327,7 +327,7 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 	/*
 	 * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)
 	 */
-	thread->dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |	\
+	thread->dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |
 			DBCR1_IAC3US | DBCR1_IAC4US;
 	/*
 	 * Force Data Address Compare User/Supervisor bits to be User-only

commit 18461960cbf50bf345ef0667d45d5f64de8fb893
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 10 20:21:10 2013 +1000

    powerpc: Provide for giveup_fpu/altivec to save state in alternate location
    
    This provides a facility which is intended for use by KVM, where the
    contents of the FP/VSX and VMX (Altivec) registers can be saved away
    to somewhere other than the thread_struct when kernel code wants to
    use floating point or VMX instructions.  This is done by providing a
    pointer in the thread_struct to indicate where the state should be
    saved to.  The giveup_fpu() and giveup_altivec() functions test these
    pointers and save state to the indicated location if they are non-NULL.
    Note that the MSR_FP/VEC bits in task->thread.regs->msr are still used
    to indicate whether the CPU register state is live, even when an
    alternate save location is being used.
    
    This also provides load_fp_state() and load_vr_state() functions, which
    load up FP/VSX and VMX state from memory into the CPU registers, and
    corresponding store_fp_state() and store_vr_state() functions, which
    store FP/VSX and VMX state into memory from the CPU registers.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7a281416affb..8649a3d629e1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1008,6 +1008,11 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	p->thread.ptrace_bps[0] = NULL;
 #endif
 
+	p->thread.fp_save_area = NULL;
+#ifdef CONFIG_ALTIVEC
+	p->thread.vr_save_area = NULL;
+#endif
+
 #ifdef CONFIG_PPC_STD_MMU_64
 	if (mmu_has_feature(MMU_FTR_SLB)) {
 		unsigned long sp_vsid;
@@ -1114,9 +1119,11 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	current->thread.used_vsr = 0;
 #endif
 	memset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));
+	current->thread.fp_save_area = NULL;
 #ifdef CONFIG_ALTIVEC
 	memset(&current->thread.vr_state, 0, sizeof(current->thread.vr_state));
 	current->thread.vr_state.vscr.u[3] = 0x00010000; /* Java mode disabled */
+	current->thread.vr_save_area = NULL;
 	current->thread.vrsave = 0;
 	current->thread.used_vr = 0;
 #endif /* CONFIG_ALTIVEC */

commit de79f7b9f6f92ec1bd6f61fa1f20de60728a5b5e
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 10 20:20:42 2013 +1000

    powerpc: Put FP/VSX and VR state into structures
    
    This creates new 'thread_fp_state' and 'thread_vr_state' structures
    to store FP/VSX state (including FPSCR) and Altivec/VSX state
    (including VSCR), and uses them in the thread_struct.  In the
    thread_fp_state, the FPRs and VSRs are represented as u64 rather
    than double, since we rarely perform floating-point computations
    on the values, and this will enable the structures to be used
    in KVM code as well.  Similarly FPSCR is now a u64 rather than
    a structure of two 32-bit values.
    
    This takes the offsets out of the macros such as SAVE_32FPRS,
    REST_32FPRS, etc.  This enables the same macros to be used for normal
    and transactional state, enabling us to delete the transactional
    versions of the macros.   This also removes the unused do_load_up_fpu
    and do_load_up_altivec, which were in fact buggy since they didn't
    create large enough stack frames to account for the fact that
    load_up_fpu and load_up_altivec are not designed to be called from C
    and assume that their caller's stack frame is an interrupt frame.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 96d2fdf3aa9e..7a281416affb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1113,12 +1113,10 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 #ifdef CONFIG_VSX
 	current->thread.used_vsr = 0;
 #endif
-	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
-	current->thread.fpscr.val = 0;
+	memset(&current->thread.fp_state, 0, sizeof(current->thread.fp_state));
 #ifdef CONFIG_ALTIVEC
-	memset(current->thread.vr, 0, sizeof(current->thread.vr));
-	memset(&current->thread.vscr, 0, sizeof(current->thread.vscr));
-	current->thread.vscr.u[3] = 0x00010000; /* Java mode disabled */
+	memset(&current->thread.vr_state, 0, sizeof(current->thread.vr_state));
+	current->thread.vr_state.vscr.u[3] = 0x00010000; /* Java mode disabled */
 	current->thread.vrsave = 0;
 	current->thread.used_vr = 0;
 #endif /* CONFIG_ALTIVEC */

commit cbc9565ee82694dec31d8137dec975b83175183b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Sep 24 15:17:21 2013 +1000

    powerpc: Remove ksp_limit on ppc64
    
    We've been keeping that field in thread_struct for a while, it contains
    the "limit" of the current stack pointer and is meant to be used for
    detecting stack overflows.
    
    It has a few problems however:
    
     - First, it was never actually *used* on 64-bit. Set and updated but
    not actually exploited
    
     - When switching stack to/from irq and softirq stacks, it's update
    is racy unless we hard disable interrupts, which is costly. This
    is fine on 32-bit as we don't soft-disable there but not on 64-bit.
    
    Thus rather than fixing 2 in order to implement 1 in some hypothetical
    future, let's remove the code completely from 64-bit. In order to avoid
    a clutter of ifdef's, we remove the updates from C code completely
    during interrupt stack switching, and instead maintain it from the
    asm helper that is used to do the stack switching in the first place.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6f428da53e20..96d2fdf3aa9e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1000,9 +1000,10 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	kregs = (struct pt_regs *) sp;
 	sp -= STACK_FRAME_OVERHEAD;
 	p->thread.ksp = sp;
+#ifdef CONFIG_PPC32
 	p->thread.ksp_limit = (unsigned long)task_stack_page(p) +
 				_ALIGN_UP(sizeof(struct thread_info), 16);
-
+#endif
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
 	p->thread.ptrace_bps[0] = NULL;
 #endif

commit 3f1f4311881b330a7b5429dd101e676df191b159
Merge: 5935ff4343a6 28e61cc466d8
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Aug 27 15:03:30 2013 +1000

    Merge branch 'merge' into next
    
    Merge stuff that already went into Linus via "merge" which
    are pre-reqs for subsequent patches

commit 037f0eed57c3f35367ac32275e45f24e297549e9
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sun Jul 14 17:02:05 2013 +0800

    powerpc: Make flush_fp_to_thread() nop when CONFIG_PPC_FPU is disabled
    
    In the current kernel, the function flush_fp_to_thread() is not
    dependent on CONFIG_PPC_FPU. So most invocations of this function
    is not wrapped by CONFIG_PPC_FPU. Even through we don't really
    save the FPRs to the thread struct if CONFIG_PPC_FPU is not enabled,
    but there does have some runtime overhead such as the check for
    tsk->thread.regs and preempt disable and enable. It really make
    no sense to do that. So make it a nop when CONFIG_PPC_FPU is
    disabled. Also remove the wrapped #ifdef CONFIG_PPC_FPU
    when invoking this function.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c517dbe705fd..0ec255a81c66 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -74,6 +74,7 @@ struct task_struct *last_task_used_vsx = NULL;
 struct task_struct *last_task_used_spe = NULL;
 #endif
 
+#ifdef CONFIG_PPC_FPU
 /*
  * Make sure the floating-point register state in the
  * the thread_struct is up to date for task tsk.
@@ -107,6 +108,7 @@ void flush_fp_to_thread(struct task_struct *tsk)
 	}
 }
 EXPORT_SYMBOL_GPL(flush_fp_to_thread);
+#endif
 
 void enable_kernel_fp(void)
 {

commit c2d52644e2da8a07ecab5ca62dd0bc563089e8dc
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Aug 9 17:29:30 2013 +1000

    powerpc: Save the TAR register earlier
    
    This moves us to save the Target Address Register (TAR) a earlier in
    __switch_to.  It introduces a new function save_tar() to do this.
    
    We need to save the TAR earlier as we will overwrite it in the transactional
    memory reclaim/recheckpoint path.  We are going to do this in a subsequent
    patch which will fix saving the TAR register when it's modified inside a
    transaction.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: <stable@vger.kernel.org> [v3.10]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c517dbe705fd..8083be20fe5e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -600,6 +600,16 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	struct ppc64_tlb_batch *batch;
 #endif
 
+	/* Back up the TAR across context switches.
+	 * Note that the TAR is not available for use in the kernel.  (To
+	 * provide this, the TAR should be backed up/restored on exception
+	 * entry/exit instead, and be in pt_regs.  FIXME, this should be in
+	 * pt_regs anyway (for debug).)
+	 * Save the TAR here before we do treclaim/trecheckpoint as these
+	 * will change the TAR.
+	 */
+	save_tar(&prev->thread);
+
 	__switch_to_tm(prev);
 
 #ifdef CONFIG_SMP

commit 24a72acac155576d630cf4304fa9cefb9b62ea1f
Merge: 6e0b8bc965d2 8bb495e3f024
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jul 1 17:57:25 2013 +1000

    Merge tag 'v3.10' into next
    
    Merge 3.10 in order to get some of the last minute powerpc
    changes, resolve conflicts and add additional fixes on top
    of them.

commit 330a1eb7775ba876dbd46b9885556e57f705e3d4
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 28 18:15:16 2013 +1000

    powerpc/perf: Core EBB support for 64-bit book3s
    
    Add support for EBB (Event Based Branches) on 64-bit book3s. See the
    included documentation for more details.
    
    EBBs are a feature which allows the hardware to branch directly to a
    specified user space address when a PMU event overflows. This can be
    used by programs for self-monitoring with no kernel involvement in the
    inner loop.
    
    Most of the logic is in the generic book3s code, primarily to avoid a
    proliferation of PMU callbacks.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b0f3e3f77e72..f8a76e6207bd 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -916,7 +916,11 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	flush_altivec_to_thread(src);
 	flush_vsx_to_thread(src);
 	flush_spe_to_thread(src);
+
 	*dst = *src;
+
+	clear_task_ebb(dst);
+
 	return 0;
 }
 

commit 0e37739b1c96d65e6433998454985de994383019
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jun 13 21:04:56 2013 +1000

    powerpc: Fix stack overflow crash in resume_kernel when ftracing
    
    It's possible for us to crash when running with ftrace enabled, eg:
    
      Bad kernel stack pointer bffffd12 at c00000000000a454
      cpu 0x3: Vector: 300 (Data Access) at [c00000000ffe3d40]
          pc: c00000000000a454: resume_kernel+0x34/0x60
          lr: c00000000000335c: performance_monitor_common+0x15c/0x180
          sp: bffffd12
         msr: 8000000000001032
         dar: bffffd12
       dsisr: 42000000
    
    If we look at current's stack (paca->__current->stack) we see it is
    equal to c0000002ecab0000. Our stack is 16K, and comparing to
    paca->kstack (c0000002ecab3e30) we can see that we have overflowed our
    kernel stack. This leads to us writing over our struct thread_info, and
    in this case we have corrupted thread_info->flags and set
    _TIF_EMULATE_STACK_STORE.
    
    Dumping the stack we see:
    
      3:mon> t c0000002ecab0000
      [c0000002ecab0000] c00000000002131c .performance_monitor_exception+0x5c/0x70
      [c0000002ecab0080] c00000000000335c performance_monitor_common+0x15c/0x180
      --- Exception: f01 (Performance Monitor) at c0000000000fb2ec .trace_hardirqs_off+0x1c/0x30
      [c0000002ecab0370] c00000000016fdb0 .trace_graph_entry+0xb0/0x280 (unreliable)
      [c0000002ecab0410] c00000000003d038 .prepare_ftrace_return+0x98/0x130
      [c0000002ecab04b0] c00000000000a920 .ftrace_graph_caller+0x14/0x28
      [c0000002ecab0520] c0000000000d6b58 .idle_cpu+0x18/0x90
      [c0000002ecab05a0] c00000000000a934 .return_to_handler+0x0/0x34
      [c0000002ecab0620] c00000000001e660 .timer_interrupt+0x160/0x300
      [c0000002ecab06d0] c0000000000025dc decrementer_common+0x15c/0x180
      --- Exception: 901 (Decrementer) at c0000000000104d4 .arch_local_irq_restore+0x74/0xa0
      [c0000002ecab09c0] c0000000000fe044 .trace_hardirqs_on+0x14/0x30 (unreliable)
      [c0000002ecab0fb0] c00000000016fe3c .trace_graph_entry+0x13c/0x280
      [c0000002ecab1050] c00000000003d038 .prepare_ftrace_return+0x98/0x130
      [c0000002ecab10f0] c00000000000a920 .ftrace_graph_caller+0x14/0x28
      [c0000002ecab1160] c0000000000161f0 .__ppc64_runlatch_on+0x10/0x40
      [c0000002ecab11d0] c00000000000a934 .return_to_handler+0x0/0x34
      --- Exception: 901 (Decrementer) at c0000000000104d4 .arch_local_irq_restore+0x74/0xa0
    
      ... and so on
    
    __ppc64_runlatch_on() is called from RUNLATCH_ON in the exception entry
    path. At that point the irq state is not consistent, ie. interrupts are
    hard disabled (by the exception entry), but the paca soft-enabled flag
    may be out of sync.
    
    This leads to the local_irq_restore() in trace_graph_entry() actually
    enabling interrupts, which we do not want. Because we have not yet
    reprogrammed the decrementer we immediately take another decrementer
    exception, and recurse.
    
    The fix is twofold. Firstly make sure we call DISABLE_INTS before
    calling RUNLATCH_ON. The badly named DISABLE_INTS actually reconciles
    the irq state in the paca with the hardware, making it safe again to
    call local_irq_save/restore().
    
    Although that should be sufficient to fix the bug, we also mark the
    runlatch routines as notrace. They are called very early in the
    exception entry and we are asking for trouble tracing them. They are
    also fairly uninteresting and tracing them just adds unnecessary
    overhead.
    
    [ This regression was introduced by fe1952fc0afb9a2e4c79f103c08aef5d13db1873
      "powerpc: Rework runlatch code" by myself --BenH
    ]
    
    CC: <stable@vger.kernel.org> [v3.4+]
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b0f3e3f77e72..076d1242507a 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1369,7 +1369,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 
 #ifdef CONFIG_PPC64
 /* Called with hard IRQs off */
-void __ppc64_runlatch_on(void)
+void notrace __ppc64_runlatch_on(void)
 {
 	struct thread_info *ti = current_thread_info();
 	unsigned long ctrl;
@@ -1382,7 +1382,7 @@ void __ppc64_runlatch_on(void)
 }
 
 /* Called with hard IRQs off */
-void __ppc64_runlatch_off(void)
+void notrace __ppc64_runlatch_off(void)
 {
 	struct thread_info *ti = current_thread_info();
 	unsigned long ctrl;

commit 82a9f16adc12f51c3f8ea59a7c3c120241aff836
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu May 16 20:27:31 2013 +0000

    powerpc/hw_breakpoints: Add DABRX cpu feature to fix 32-bit regression
    
    When introducing support for DABRX in 4474ef0, we broke older 32-bit CPUs
    that don't have that register.
    
    Some CPUs have a DABR but not DABRX.  Configuration are:
    - No 32bit CPUs have DABRX but some have DABR.
    - POWER4+ and below have the DABR but no DABRX.
    - 970 and POWER5 and above have DABR and DABRX.
    - POWER8 has DAWR, hence no DABRX.
    
    This introduces CPU_FTR_DABRX and sets it on appropriate CPUs.  We use
    the top 64 bits for CPU FTR bits since only 64 bit CPUs have this.
    
    Processors that don't have the DABRX will still work as they will fall
    back to software filtering these breakpoints via perf_exclude_event().
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Reported-by: "Gorelik, Jacob (335F)" <jacob.gorelik@jpl.nasa.gov>
    cc: stable@vger.kernel.org (v3.9 only)
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a902723fdc69..b0f3e3f77e72 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -399,7 +399,8 @@ static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
 static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
 {
 	mtspr(SPRN_DABR, dabr);
-	mtspr(SPRN_DABRX, dabrx);
+	if (cpu_has_feature(CPU_FTR_DABRX))
+		mtspr(SPRN_DABRX, dabrx);
 	return 0;
 }
 #else

commit 6cecf76b47ba6bea3c81d170afc2e0b244e5849c
Author: Scott Wood <scottwood@freescale.com>
Date:   Mon May 13 14:14:53 2013 +0000

    powerpc/booke64: Fix kernel hangs at kernel_dbg_exc
    
    MSR_DE is not cleared on entry to the kernel, and we don't clear it
    explicitly outside of debug code.  If we have MSR_DE set in
    prime_debug_regs(), and the new thread has events enabled in DBCR0
    (e.g.  ICMP is set in thread->dbsr0, even though it was cleared in the
    real DBCR0 when the thread got scheduled out), we'll end up taking a
    debug exception in the kernel when DBCR0 is loaded.  DSRR0 will not
    point to an exception vector, and the kernel ends up hanging at
    kernel_dbg_exc.  Fix this by always clearing MSR_DE when we load new
    debug state.
    
    Another observed source of kernel_dbg_exc hangs is with the branch
    taken event.  If this event is active, but we take a non-debug trap
    (e.g. a TLB miss or an asynchronous interrupt) before the next branch.
    We end up taking a branch-taken debug exception on the initial branch
    instruction of the exception vector, but because the debug exception is
    DBSR_BT rather than DBSR_IC we branch to kernel_dbg_exc before even
    checking the DSRR0 address.  Fix this by checking for DBSR_BT as well
    as DBSR_IC, which is what 32-bit does and what the comments suggest was
    intended in the 64-bit code as well.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 21981a89e394..a902723fdc69 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -339,6 +339,13 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 
 static void prime_debug_regs(struct thread_struct *thread)
 {
+	/*
+	 * We could have inherited MSR_DE from userspace, since
+	 * it doesn't get cleared on exception entry.  Make sure
+	 * MSR_DE is clear before we enable any debug events.
+	 */
+	mtmsr(mfmsr() & ~MSR_DE);
+
 	mtspr(SPRN_IAC1, thread->iac1);
 	mtspr(SPRN_IAC2, thread->iac2);
 #if CONFIG_PPC_ADV_DEBUG_IACS > 2

commit af945cf4bfc6586f0739b8c7f67af75576dec25e
Author: Li Zhong <zhong@linux.vnet.ibm.com>
Date:   Mon May 6 22:44:41 2013 +0000

    powerpc: Fix MAX_STACK_TRACE_ENTRIES too low warning again
    
    Saw this warning again, and this time from the ret_from_fork path.
    
    It seems we could clear the back chain earlier in copy_thread(), which
    could cover both path, and also fix potential lockdep usage in
    schedule_tail(), or exception occurred before we clear the back chain.
    
    Signed-off-by: Li Zhong <zhong@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ceb4e7b62cf4..21981a89e394 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -971,6 +971,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	 * do some house keeping and then return from the fork or clone
 	 * system call, using the stack frame created above.
 	 */
+	((unsigned long *)sp)[0] = 0;
 	sp -= sizeof(struct pt_regs);
 	kregs = (struct pt_regs *) sp;
 	sp -= STACK_FRAME_OVERHEAD;

commit 5a148af66932c31814e263366094b5812210b501
Merge: 99c6bcf46d22 54d5999d98f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 2 10:16:16 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc update from Benjamin Herrenschmidt:
     "The main highlights this time around are:
    
       - A pile of addition POWER8 bits and nits, such as updated
         performance counter support (Michael Ellerman), new branch history
         buffer support (Anshuman Khandual), base support for the new PCI
         host bridge when not using the hypervisor (Gavin Shan) and other
         random related bits and fixes from various contributors.
    
       - Some rework of our page table format by Aneesh Kumar which fixes a
         thing or two and paves the way for THP support.  THP itself will
         not make it this time around however.
    
       - More Freescale updates, including Altivec support on the new e6500
         cores, new PCI controller support, and a pile of new boards support
         and updates.
    
       - The usual batch of trivial cleanups & fixes"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (156 commits)
      powerpc: Fix build error for book3e
      powerpc: Context switch the new EBB SPRs
      powerpc: Turn on the EBB H/FSCR bits
      powerpc: Replace CPU_FTR_BCTAR with CPU_FTR_ARCH_207S
      powerpc: Setup BHRB instructions facility in HFSCR for POWER8
      powerpc: Fix interrupt range check on debug exception
      powerpc: Update tlbie/tlbiel as per ISA doc
      powerpc: Print page size info during boot
      powerpc: print both base and actual page size on hash failure
      powerpc: Fix hpte_decode to use the correct decoding for page sizes
      powerpc: Decode the pte-lp-encoding bits correctly.
      powerpc: Use encode avpn where we need only avpn values
      powerpc: Reduce PTE table memory wastage
      powerpc: Move the pte free routines from common header
      powerpc: Reduce the PTE_INDEX_SIZE
      powerpc: Switch 16GB and 16MB explicit hugepages to a different page table format
      powerpc: New hugepage directory format
      powerpc: Don't truncate pgd_index wrongly
      powerpc: Don't hard code the size of pte page
      powerpc: Save DAR and DSISR in pt_regs on MCE
      ...

commit a43cb95d547a061ed5bf1acb28e0f5fd575e26c1
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:17 2013 -0700

    dump_stack: unify debug information printed by show_regs()
    
    show_regs() is inherently arch-dependent but it does make sense to print
    generic debug information and some archs already do albeit in slightly
    different forms.  This patch introduces a generic function to print debug
    information from show_regs() so that different archs print out the same
    information and it's much easier to modify what's printed.
    
    show_regs_print_info() prints out the same debug info as dump_stack()
    does plus task and thread_info pointers.
    
    * Archs which didn't print debug info now do.
    
      alpha, arc, blackfin, c6x, cris, frv, h8300, hexagon, ia64, m32r,
      metag, microblaze, mn10300, openrisc, parisc, score, sh64, sparc,
      um, xtensa
    
    * Already prints debug info.  Replaced with show_regs_print_info().
      The printed information is superset of what used to be there.
    
      arm, arm64, avr32, mips, powerpc, sh32, tile, unicore32, x86
    
    * s390 is special in that it used to print arch-specific information
      along with generic debug info.  Heiko and Martin think that the
      arch-specific extra isn't worth keeping s390 specfic implementation.
      Converted to use the generic version.
    
    Note that now all archs print the debug info before actual register
    dumps.
    
    An example BUG() dump follows.
    
     kernel BUG at /work/os/work/kernel/workqueue.c:4841!
     invalid opcode: 0000 [#1] PREEMPT SMP DEBUG_PAGEALLOC
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.9.0-rc1-work+ #7
     Hardware name: empty empty/S3992, BIOS 080011  10/26/2007
     task: ffff88007c85e040 ti: ffff88007c860000 task.ti: ffff88007c860000
     RIP: 0010:[<ffffffff8234a07e>]  [<ffffffff8234a07e>] init_workqueues+0x4/0x6
     RSP: 0000:ffff88007c861ec8  EFLAGS: 00010246
     RAX: ffff88007c861fd8 RBX: ffffffff824466a8 RCX: 0000000000000001
     RDX: 0000000000000046 RSI: 0000000000000001 RDI: ffffffff8234a07a
     RBP: ffff88007c861ec8 R08: 0000000000000000 R09: 0000000000000000
     R10: 0000000000000001 R11: 0000000000000000 R12: ffffffff8234a07a
     R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
     FS:  0000000000000000(0000) GS:ffff88007dc00000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
     CR2: ffff88015f7ff000 CR3: 00000000021f1000 CR4: 00000000000007f0
     DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
     Stack:
      ffff88007c861ef8 ffffffff81000312 ffffffff824466a8 ffff88007c85e650
      0000000000000003 0000000000000000 ffff88007c861f38 ffffffff82335e5d
      ffff88007c862080 ffffffff8223d8c0 ffff88007c862080 ffffffff81c47760
     Call Trace:
      [<ffffffff81000312>] do_one_initcall+0x122/0x170
      [<ffffffff82335e5d>] kernel_init_freeable+0x9b/0x1c8
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      [<ffffffff81c4776e>] kernel_init+0xe/0xf0
      [<ffffffff81c6be9c>] ret_from_fork+0x7c/0xb0
      [<ffffffff81c47760>] ? rest_init+0x140/0x140
      ...
    
    v2: Typo fix in x86-32.
    
    v3: CPU number dropped from show_regs_print_info() as
        dump_stack_print_info() has been updated to print it.  s390
        specific implementation dropped as requested by s390 maintainers.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Acked-by: Chris Metcalf <cmetcalf@tilera.com>           [tile bits]
    Acked-by: Richard Kuo <rkuo@codeaurora.org>             [hexagon bits]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 624d44bb44dc..13a8d9d0b5cb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -831,6 +831,8 @@ void show_regs(struct pt_regs * regs)
 {
 	int i, trap;
 
+	show_regs_print_info(KERN_DEFAULT);
+
 	printk("NIP: "REG" LR: "REG" CTR: "REG"\n",
 	       regs->nip, regs->link, regs->ctr);
 	printk("REGS: %p TRAP: %04lx   %s  (%s)\n",
@@ -850,12 +852,6 @@ void show_regs(struct pt_regs * regs)
 #else
 		printk("DAR: "REG", DSISR: %08lx\n", regs->dar, regs->dsisr);
 #endif
-	printk("TASK = %p[%d] '%s' THREAD: %p",
-	       current, task_pid_nr(current), current->comm, task_thread_info(current));
-
-#ifdef CONFIG_SMP
-	printk(" CPU: %d", raw_smp_processor_id());
-#endif /* CONFIG_SMP */
 
 	for (i = 0;  i < 32;  i++) {
 		if ((i % REGS_PER_LINE) == 0)

commit 196779b9b4ce1922afabdc20d0270720603bd46c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Apr 30 15:27:12 2013 -0700

    dump_stack: consolidate dump_stack() implementations and unify their behaviors
    
    Both dump_stack() and show_stack() are currently implemented by each
    architecture.  show_stack(NULL, NULL) dumps the backtrace for the
    current task as does dump_stack().  On some archs, dump_stack() prints
    extra information - pid, utsname and so on - in addition to the
    backtrace while the two are identical on other archs.
    
    The usages in arch-independent code of the two functions indicate
    show_stack(NULL, NULL) should print out bare backtrace while
    dump_stack() is used for debugging purposes when something went wrong,
    so it does make sense to print additional information on the task which
    triggered dump_stack().
    
    There's no reason to require archs to implement two separate but mostly
    identical functions.  It leads to unnecessary subtle information.
    
    This patch expands the dummy fallback dump_stack() implementation in
    lib/dump_stack.c such that it prints out debug information (taken from
    x86) and invokes show_stack(NULL, NULL) and drops arch-specific
    dump_stack() implementations in all archs except blackfin.  Blackfin's
    dump_stack() does something wonky that I don't understand.
    
    Debug information can be printed separately by calling
    dump_stack_print_info() so that arch-specific dump_stack()
    implementation can still emit the same debug information.  This is used
    in blackfin.
    
    This patch brings the following behavior changes.
    
    * On some archs, an extra level in backtrace for show_stack() could be
      printed.  This is because the top frame was determined in
      dump_stack() on those archs while generic dump_stack() can't do that
      reliably.  It can be compensated by inlining dump_stack() but not
      sure whether that'd be necessary.
    
    * Most archs didn't use to print debug info on dump_stack().  They do
      now.
    
    An example WARN dump follows.
    
     WARNING: at kernel/workqueue.c:4841 init_workqueues+0x35/0x505()
     Hardware name: empty
     Modules linked in:
     CPU: 0 PID: 1 Comm: swapper/0 Not tainted 3.9.0-rc1-work+ #9
      0000000000000009 ffff88007c861e08 ffffffff81c614dc ffff88007c861e48
      ffffffff8108f50f ffffffff82228240 0000000000000040 ffffffff8234a03c
      0000000000000000 0000000000000000 0000000000000000 ffff88007c861e58
     Call Trace:
      [<ffffffff81c614dc>] dump_stack+0x19/0x1b
      [<ffffffff8108f50f>] warn_slowpath_common+0x7f/0xc0
      [<ffffffff8108f56a>] warn_slowpath_null+0x1a/0x20
      [<ffffffff8234a071>] init_workqueues+0x35/0x505
      ...
    
    v2: CPU number added to the generic debug info as requested by s390
        folks and dropped the s390 specific dump_stack().  This loses %ksp
        from the debug message which the maintainers think isn't important
        enough to keep the s390-specific dump_stack() implementation.
    
        dump_stack_print_info() is moved to kernel/printk.c from
        lib/dump_stack.c.  Because linkage is per objecct file,
        dump_stack_print_info() living in the same lib file as generic
        dump_stack() means that archs which implement custom dump_stack()
        - at this point, only blackfin - can't use dump_stack_print_info()
        as that will bring in the generic version of dump_stack() too.  v1
        The v1 patch broke build on blackfin due to this issue.  The build
        breakage was reported by Fengguang Wu.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Jesper Nilsson <jesper.nilsson@axis.com>
    Acked-by: Vineet Gupta <vgupta@synopsys.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>   [s390 bits]
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Bjorn Helgaas <bhelgaas@google.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Acked-by: Richard Kuo <rkuo@codeaurora.org>             [hexagon bits]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 16e77a81ab4f..624d44bb44dc 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1362,12 +1362,6 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	} while (count++ < kstack_depth_to_print);
 }
 
-void dump_stack(void)
-{
-	show_stack(current, NULL);
-}
-EXPORT_SYMBOL(dump_stack);
-
 #ifdef CONFIG_PPC64
 /* Called with hard IRQs off */
 void __ppc64_runlatch_on(void)

commit 234d15def96ac49027dc869f7bc250d5cb0eb5d7
Merge: 6747e83235ca 60d509fa6a9c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 24 14:43:36 2013 +1000

    Merge remote-tracking branch 'origin/master' into next
    
    Merge upstream to get the audit fixes

commit 28d170abad3d6dfbe7309c0097d7de8a51c6b779
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Apr 21 06:47:59 2013 +0000

    ptrace/powerpc: Don't flush_ptrace_hw_breakpoint() on fork()
    
    arch_dup_task_struct() does flush_ptrace_hw_breakpoint(src), this
    destroys the parent's breakpoints for no reason. We should clear
    child->thread.ptrace_bps[] copied by dup_task_struct() instead.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 59dd545fdde1..834805cf13cb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -910,10 +910,6 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 	flush_altivec_to_thread(src);
 	flush_vsx_to_thread(src);
 	flush_spe_to_thread(src);
-#ifdef CONFIG_HAVE_HW_BREAKPOINT
-	flush_ptrace_hw_breakpoint(src);
-#endif /* CONFIG_HAVE_HW_BREAKPOINT */
-
 	*dst = *src;
 	return 0;
 }
@@ -984,6 +980,10 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	p->thread.ksp_limit = (unsigned long)task_stack_page(p) +
 				_ALIGN_UP(sizeof(struct thread_info), 16);
 
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	p->thread.ptrace_bps[0] = NULL;
+#endif
+
 #ifdef CONFIG_PPC_STD_MMU_64
 	if (mmu_has_feature(MMU_FTR_SLB)) {
 		unsigned long sp_vsid;

commit f110c0c1926028a233830c6166e4d40314420823
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Apr 9 16:18:55 2013 +1000

    powerpc: fix compiling CONFIG_PPC_TRANSACTIONAL_MEM when CONFIG_ALTIVEC=n
    
    We can't compile a kernel with CONFIG_ALTIVEC=n when
    CONFIG_PPC_TRANSACTIONAL_MEM=y.  We currently get:
    
    arch/powerpc/kernel/tm.S:320: Error: unsupported relocation against THREAD_VSCR
    arch/powerpc/kernel/tm.S:323: Error: unsupported relocation against THREAD_VR0
    arch/powerpc/kernel/tm.S:323: Error: unsupported relocation against THREAD_VR0
    etc.
    
    The below fixes this with a sprinkling of #ifdefs.
    
    This was found by mpe with kisskb:
      http://kisskb.ellerman.id.au/kisskb/buildresult/8539442/
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 59dd545fdde1..16e77a81ab4f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -555,10 +555,12 @@ static inline void tm_recheckpoint_new_task(struct task_struct *new)
 		new->thread.regs->msr |=
 			(MSR_FP | new->thread.fpexc_mode);
 	}
+#ifdef CONFIG_ALTIVEC
 	if (msr & MSR_VEC) {
 		do_load_up_transact_altivec(&new->thread);
 		new->thread.regs->msr |= MSR_VEC;
 	}
+#endif
 	/* We may as well turn on VSX too since all the state is restored now */
 	if (msr & MSR_VSX)
 		new->thread.regs->msr |= MSR_VSX;

commit bc2a9408fa65195288b41751016c36fd00a75a85
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:40 2013 +0000

    powerpc: Hook in new transactional memory code
    
    This hooks the new transactional memory code into context switching, FP/VMX/VMX
    unavailable and exception return.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 48a987579e4f..59dd545fdde1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -515,7 +515,7 @@ static inline void tm_reclaim_task(struct task_struct *tsk)
 	tm_save_sprs(thr);
 }
 
-static inline void __maybe_unused tm_recheckpoint_new_task(struct task_struct *new)
+static inline void tm_recheckpoint_new_task(struct task_struct *new)
 {
 	unsigned long msr;
 
@@ -590,6 +590,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	struct ppc64_tlb_batch *batch;
 #endif
 
+	__switch_to_tm(prev);
+
 #ifdef CONFIG_SMP
 	/* avoid complexity of lazy save/restore of fpu
 	 * by just saving it every time we switch out if
@@ -705,6 +707,9 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 * of sync. Hard disable here.
 	 */
 	hard_irq_disable();
+
+	tm_recheckpoint_new_task(new);
+
 	last = _switch(old_thread, new_thread);
 
 #ifdef CONFIG_PPC_BOOK3S_64
@@ -1080,7 +1085,6 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 		regs->msr = MSR_USER32;
 	}
 #endif
-
 	discard_lazy_cpu_state();
 #ifdef CONFIG_VSX
 	current->thread.used_vsr = 0;
@@ -1100,6 +1104,13 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	current->thread.spefscr = 0;
 	current->thread.used_spe = 0;
 #endif /* CONFIG_SPE */
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	if (cpu_has_feature(CPU_FTR_TM))
+		regs->msr |= MSR_TM;
+	current->thread.tm_tfhar = 0;
+	current->thread.tm_texasr = 0;
+	current->thread.tm_tfiar = 0;
+#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 }
 
 #define PR_FP_ALL_EXCEPT (PR_FP_EXC_DIV | PR_FP_EXC_OVF | PR_FP_EXC_UND \

commit fb09692e71f13af7298eb603a1975850b1c7a8d8
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:37 2013 +0000

    powerpc: Add reclaim and recheckpoint functions for context switching transactional memory processes
    
    When we switch out a task, we need to save both the checkpointed and the
    speculated state into the thread struct.
    
    Similarly when we are switching in a task we need to load both the checkpointed
    and speculated state.  If the task was using FP, we non-lazily reload both the
    original and the speculative FP register states.  This is because the kernel
    doesn't see if/when a TM rollback occurs, so if we take an FP unavoidable
    later, we are unable to determine which set of FP regs need to be restored.
    
    This simply adds these functions.  It doesn't hook them into the existing code
    yet.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1cc40533021b..48a987579e4f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -50,6 +50,7 @@
 #include <asm/runlatch.h>
 #include <asm/syscalls.h>
 #include <asm/switch_to.h>
+#include <asm/tm.h>
 #include <asm/debug.h>
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
@@ -467,6 +468,117 @@ static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
 		return false;
 	return true;
 }
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+static inline void tm_reclaim_task(struct task_struct *tsk)
+{
+	/* We have to work out if we're switching from/to a task that's in the
+	 * middle of a transaction.
+	 *
+	 * In switching we need to maintain a 2nd register state as
+	 * oldtask->thread.ckpt_regs.  We tm_reclaim(oldproc); this saves the
+	 * checkpointed (tbegin) state in ckpt_regs and saves the transactional
+	 * (current) FPRs into oldtask->thread.transact_fpr[].
+	 *
+	 * We also context switch (save) TFHAR/TEXASR/TFIAR in here.
+	 */
+	struct thread_struct *thr = &tsk->thread;
+
+	if (!thr->regs)
+		return;
+
+	if (!MSR_TM_ACTIVE(thr->regs->msr))
+		goto out_and_saveregs;
+
+	/* Stash the original thread MSR, as giveup_fpu et al will
+	 * modify it.  We hold onto it to see whether the task used
+	 * FP & vector regs.
+	 */
+	thr->tm_orig_msr = thr->regs->msr;
+
+	TM_DEBUG("--- tm_reclaim on pid %d (NIP=%lx, "
+		 "ccr=%lx, msr=%lx, trap=%lx)\n",
+		 tsk->pid, thr->regs->nip,
+		 thr->regs->ccr, thr->regs->msr,
+		 thr->regs->trap);
+
+	tm_reclaim(thr, thr->regs->msr, TM_CAUSE_RESCHED);
+
+	TM_DEBUG("--- tm_reclaim on pid %d complete\n",
+		 tsk->pid);
+
+out_and_saveregs:
+	/* Always save the regs here, even if a transaction's not active.
+	 * This context-switches a thread's TM info SPRs.  We do it here to
+	 * be consistent with the restore path (in recheckpoint) which
+	 * cannot happen later in _switch().
+	 */
+	tm_save_sprs(thr);
+}
+
+static inline void __maybe_unused tm_recheckpoint_new_task(struct task_struct *new)
+{
+	unsigned long msr;
+
+	if (!cpu_has_feature(CPU_FTR_TM))
+		return;
+
+	/* Recheckpoint the registers of the thread we're about to switch to.
+	 *
+	 * If the task was using FP, we non-lazily reload both the original and
+	 * the speculative FP register states.  This is because the kernel
+	 * doesn't see if/when a TM rollback occurs, so if we take an FP
+	 * unavoidable later, we are unable to determine which set of FP regs
+	 * need to be restored.
+	 */
+	if (!new->thread.regs)
+		return;
+
+	/* The TM SPRs are restored here, so that TEXASR.FS can be set
+	 * before the trecheckpoint and no explosion occurs.
+	 */
+	tm_restore_sprs(&new->thread);
+
+	if (!MSR_TM_ACTIVE(new->thread.regs->msr))
+		return;
+	msr = new->thread.tm_orig_msr;
+	/* Recheckpoint to restore original checkpointed register state. */
+	TM_DEBUG("*** tm_recheckpoint of pid %d "
+		 "(new->msr 0x%lx, new->origmsr 0x%lx)\n",
+		 new->pid, new->thread.regs->msr, msr);
+
+	/* This loads the checkpointed FP/VEC state, if used */
+	tm_recheckpoint(&new->thread, msr);
+
+	/* This loads the speculative FP/VEC state, if used */
+	if (msr & MSR_FP) {
+		do_load_up_transact_fpu(&new->thread);
+		new->thread.regs->msr |=
+			(MSR_FP | new->thread.fpexc_mode);
+	}
+	if (msr & MSR_VEC) {
+		do_load_up_transact_altivec(&new->thread);
+		new->thread.regs->msr |= MSR_VEC;
+	}
+	/* We may as well turn on VSX too since all the state is restored now */
+	if (msr & MSR_VSX)
+		new->thread.regs->msr |= MSR_VSX;
+
+	TM_DEBUG("*** tm_recheckpoint of pid %d complete "
+		 "(kernel msr 0x%lx)\n",
+		 new->pid, mfmsr());
+}
+
+static inline void __switch_to_tm(struct task_struct *prev)
+{
+	if (cpu_has_feature(CPU_FTR_TM)) {
+		tm_enable();
+		tm_reclaim_task(prev);
+	}
+}
+#else
+#define tm_recheckpoint_new_task(new)
+#define __switch_to_tm(prev)
+#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)

commit afc07701ced6463786d09a3b9baf894c1397e991
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:34 2013 +0000

    powerpc: Add transactional memory paca scratch register to show_regs
    
    Add transactional memory paca scratch register to show_regs.  This is useful
    for debugging.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b0a0321e4bb6..1cc40533021b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -753,6 +753,9 @@ void show_regs(struct pt_regs * regs)
 	 */
 	printk("NIP ["REG"] %pS\n", regs->nip, (void *)regs->nip);
 	printk("LR ["REG"] %pS\n", regs->link, (void *)regs->link);
+#endif
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	printk("PACATMSCRATCH [%llx]\n", get_paca()->tm_scratch);
 #endif
 	show_stack(current, (unsigned long *) regs->gpr[1]);
 	if (!user_mode(regs))

commit 8b3c34cf0e0ab334a24aad7367cd06a5ba09a898
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:32 2013 +0000

    powerpc: New macros for transactional memory support
    
    This adds new macros for saving and restoring checkpointed architected state
    from and to the thread_struct.
    
    It also adds some debugging macros for when your brain explodes trying to debug
    your transactional memory enabled kernel.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 96e31de89b43..b0a0321e4bb6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -57,6 +57,13 @@
 #include <linux/kprobes.h>
 #include <linux/kdebug.h>
 
+/* Transactional Memory debug */
+#ifdef TM_DEBUG_SW
+#define TM_DEBUG(x...) printk(KERN_INFO x)
+#else
+#define TM_DEBUG(x...) do { } while(0)
+#endif
+
 extern unsigned long _get_SP(void);
 
 #ifndef CONFIG_SMP

commit 05d694ea0daa2e442191a2128aaec78635823f08
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Jan 24 15:02:58 2013 +0000

    powerpc: Add length setting to set_dawr
    
    Currently we set the length field in the DAWR to 0 which defaults it to one
    double word (64bits) which is the same as the DABR.
    
    Change this so that we can set it to longer values as supported by the DAWR.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 99550d3615c4..96e31de89b43 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -409,7 +409,7 @@ static inline int set_dabr(struct arch_hw_breakpoint *brk)
 
 static inline int set_dawr(struct arch_hw_breakpoint *brk)
 {
-	unsigned long dawr, dawrx;
+	unsigned long dawr, dawrx, mrd;
 
 	dawr = brk->address;
 
@@ -419,6 +419,14 @@ static inline int set_dawr(struct arch_hw_breakpoint *brk)
 		                   << (63 - 59); //* translate */
 	dawrx |= (brk->type & (HW_BRK_TYPE_PRIV_ALL)) \
 		                   >> 3; //* PRIM bits */
+	/* dawr length is stored in field MDR bits 48:53.  Matches range in
+	   doublewords (64 bits) baised by -1 eg. 0b000000=1DW and
+	   0b111111=64DW.
+	   brk->len is in bytes.
+	   This aligns up to double word size, shifts and does the bias.
+	*/
+	mrd = ((brk->len + 7) >> 3) - 1;
+	dawrx |= (mrd & 0x3f) << (63 - 53);
 
 	if (ppc_md.set_dawr)
 		return ppc_md.set_dawr(dawr, dawrx);

commit b9818c3312da66f4b83a4a2e8650628be1237cb5
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Jan 10 14:25:34 2013 +0000

    powerpc: Rename set_break to avoid naming conflict
    
    With allmodconfig we are getting:
      drivers/tty/synclink_gt.c:160:12: error: conflicting types for 'set_break'
      arch/powerpc/include/asm/debug.h:49:5: note: previous declaration of 'set_break' was here
    
      drivers/tty/synclinkmp.c:526:12: error: conflicting types for 'set_break'
      arch/powerpc/include/asm/debug.h:49:5: note: previous declaration of 'set_break' was here
    
    This renames set_break to set_breakpoint to avoid this naming conflict
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8d56452e1dbd..99550d3615c4 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -366,7 +366,7 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 {
 	thread->hw_brk.address = 0;
 	thread->hw_brk.type = 0;
-	set_break(&thread->hw_brk);
+	set_breakpoint(&thread->hw_brk);
 }
 #endif /* !CONFIG_HAVE_HW_BREAKPOINT */
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
@@ -427,7 +427,7 @@ static inline int set_dawr(struct arch_hw_breakpoint *brk)
 	return 0;
 }
 
-int set_break(struct arch_hw_breakpoint *brk)
+int set_breakpoint(struct arch_hw_breakpoint *brk)
 {
 	__get_cpu_var(current_brk) = *brk;
 
@@ -538,7 +538,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
 	if (unlikely(hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))
-		set_break(&new->thread.hw_brk);
+		set_breakpoint(&new->thread.hw_brk);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 

commit bf99de36e48678c61adb697496e0364c610bbbfc
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Dec 20 14:06:45 2012 +0000

    powerpc: Add the DAWR support to the set_break()
    
    This adds DAWR supoprt to the set_break().
    
    It does both bare metal and PAPR versions of setting the DAWR.
    
    There is still some work we can do to make full use of the watchpoint but that
    will come later.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c16c1c2abeea..8d56452e1dbd 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -407,10 +407,33 @@ static inline int set_dabr(struct arch_hw_breakpoint *brk)
 	return __set_dabr(dabr, dabrx);
 }
 
+static inline int set_dawr(struct arch_hw_breakpoint *brk)
+{
+	unsigned long dawr, dawrx;
+
+	dawr = brk->address;
+
+	dawrx  = (brk->type & (HW_BRK_TYPE_READ | HW_BRK_TYPE_WRITE)) \
+		                   << (63 - 58); //* read/write bits */
+	dawrx |= ((brk->type & (HW_BRK_TYPE_TRANSLATE)) >> 2) \
+		                   << (63 - 59); //* translate */
+	dawrx |= (brk->type & (HW_BRK_TYPE_PRIV_ALL)) \
+		                   >> 3; //* PRIM bits */
+
+	if (ppc_md.set_dawr)
+		return ppc_md.set_dawr(dawr, dawrx);
+	mtspr(SPRN_DAWR, dawr);
+	mtspr(SPRN_DAWRX, dawrx);
+	return 0;
+}
+
 int set_break(struct arch_hw_breakpoint *brk)
 {
 	__get_cpu_var(current_brk) = *brk;
 
+	if (cpu_has_feature(CPU_FTR_DAWR))
+		return set_dawr(brk);
+
 	return set_dabr(brk);
 }
 

commit 9422de3e953d0e60eb95f5430a9dd803eec1c6d7
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Dec 20 14:06:44 2012 +0000

    powerpc: Hardware breakpoints rewrite to handle non DABR breakpoint registers
    
    This is a rewrite so that we don't assume we are using the DABR throughout the
    code.  We now use the arch_hw_breakpoint to store the breakpoint in a generic
    manner in the thread_struct, rather than storing the raw DABR value.
    
    The ptrace GET/SET_DEBUGREG interface currently passes the raw DABR in from
    userspace.  We keep this functionality, so that future changes (like the POWER8
    DAWR), will still fake the DABR to userspace.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3065d17f3606..c16c1c2abeea 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -271,7 +271,7 @@ void do_send_trap(struct pt_regs *regs, unsigned long address,
 	force_sig_info(SIGTRAP, &info, current);
 }
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
-void do_dabr(struct pt_regs *regs, unsigned long address,
+void do_break (struct pt_regs *regs, unsigned long address,
 		    unsigned long error_code)
 {
 	siginfo_t info;
@@ -281,11 +281,11 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 			11, SIGSEGV) == NOTIFY_STOP)
 		return;
 
-	if (debugger_dabr_match(regs))
+	if (debugger_break_match(regs))
 		return;
 
-	/* Clear the DABR */
-	set_dabr(0, 0);
+	/* Clear the breakpoint */
+	hw_breakpoint_disable();
 
 	/* Deliver the signal to userspace */
 	info.si_signo = SIGTRAP;
@@ -296,7 +296,7 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 }
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 
-static DEFINE_PER_CPU(unsigned long, current_dabr);
+static DEFINE_PER_CPU(struct arch_hw_breakpoint, current_brk);
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 /*
@@ -364,39 +364,72 @@ static void switch_booke_debug_regs(struct thread_struct *new_thread)
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
 static void set_debug_reg_defaults(struct thread_struct *thread)
 {
-	if (thread->dabr) {
-		thread->dabr = 0;
-		thread->dabrx = 0;
-		set_dabr(0, 0);
-	}
+	thread->hw_brk.address = 0;
+	thread->hw_brk.type = 0;
+	set_break(&thread->hw_brk);
 }
 #endif /* !CONFIG_HAVE_HW_BREAKPOINT */
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 
-int set_dabr(unsigned long dabr, unsigned long dabrx)
-{
-	__get_cpu_var(current_dabr) = dabr;
-
-	if (ppc_md.set_dabr)
-		return ppc_md.set_dabr(dabr, dabrx);
-
-	/* XXX should we have a CPU_FTR_HAS_DABR ? */
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
+static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
+{
 	mtspr(SPRN_DAC1, dabr);
 #ifdef CONFIG_PPC_47x
 	isync();
 #endif
+	return 0;
+}
 #elif defined(CONFIG_PPC_BOOK3S)
+static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
+{
 	mtspr(SPRN_DABR, dabr);
 	mtspr(SPRN_DABRX, dabrx);
-#endif
 	return 0;
 }
+#else
+static inline int __set_dabr(unsigned long dabr, unsigned long dabrx)
+{
+	return -EINVAL;
+}
+#endif
+
+static inline int set_dabr(struct arch_hw_breakpoint *brk)
+{
+	unsigned long dabr, dabrx;
+
+	dabr = brk->address | (brk->type & HW_BRK_TYPE_DABR);
+	dabrx = ((brk->type >> 3) & 0x7);
+
+	if (ppc_md.set_dabr)
+		return ppc_md.set_dabr(dabr, dabrx);
+
+	return __set_dabr(dabr, dabrx);
+}
+
+int set_break(struct arch_hw_breakpoint *brk)
+{
+	__get_cpu_var(current_brk) = *brk;
+
+	return set_dabr(brk);
+}
 
 #ifdef CONFIG_PPC64
 DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);
 #endif
 
+static inline bool hw_brk_match(struct arch_hw_breakpoint *a,
+			      struct arch_hw_breakpoint *b)
+{
+	if (a->address != b->address)
+		return false;
+	if (a->type != b->type)
+		return false;
+	if (a->len != b->len)
+		return false;
+	return true;
+}
+
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {
@@ -481,8 +514,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
  * schedule DABR
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
-	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
-		set_dabr(new->thread.dabr, new->thread.dabrx);
+	if (unlikely(hw_brk_match(&__get_cpu_var(current_brk), &new->thread.hw_brk)))
+		set_break(&new->thread.hw_brk);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 

commit 92779245599bb3d7fb48066b11c4bfd6aa477198
Author: Haren Myneni <haren@linux.vnet.ibm.com>
Date:   Thu Dec 6 21:49:56 2012 +0000

    powerpc: Define ppr in thread_struct
    
    [PATCH 4/6] powerpc: Define ppr in thread_struct
    
    ppr in thread_struct is used to save PPR and restore it before process exits
    from kernel.
    
    This patch sets the default priority to 3 when tasks are created such
    that users can use 4 for higher priority tasks.
    
    Signed-off-by: Haren Myneni <haren@us.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 81430674e71c..3065d17f3606 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -813,6 +813,8 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		p->thread.dscr_inherit = current->thread.dscr_inherit;
 		p->thread.dscr = current->thread.dscr;
 	}
+	if (cpu_has_feature(CPU_FTR_HAS_PPR))
+		p->thread.ppr = INIT_PPR;
 #endif
 	/*
 	 * The PPC64 ABI makes use of a TOC to contain function 

commit afa86fc426ff7e7f5477f15da9c405d08d5cf790
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Oct 22 22:51:14 2012 -0400

    flagday: don't pass regs to copy_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a31437567631..81430674e71c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -733,8 +733,7 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 extern unsigned long dscr_default; /* defined in arch/powerpc/kernel/sysfs.c */
 
 int copy_thread(unsigned long clone_flags, unsigned long usp,
-		unsigned long arg, struct task_struct *p,
-		struct pt_regs *regs)
+		unsigned long arg, struct task_struct *p)
 {
 	struct pt_regs *childregs, *kregs;
 	extern void ret_from_fork(void);
@@ -759,6 +758,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		ti->flags |= _TIF_RESTOREALL;
 		f = ret_from_kernel_thread;
 	} else {
+		struct pt_regs *regs = current_pt_regs();
 		CHECK_FULL_REGS(regs);
 		*childregs = *regs;
 		if (usp)

commit 0bcfe5404962cfb1dd0d4b3755357c12a98ef3d1
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Oct 26 23:08:25 2012 -0400

    powerpc: switch to generic fork/clone/vfork
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f6d244db9203..a31437567631 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1026,29 +1026,6 @@ int get_unalign_ctl(struct task_struct *tsk, unsigned long adr)
 	return put_user(tsk->thread.align_ctl, (unsigned int __user *)adr);
 }
 
-int sys_clone(unsigned long clone_flags, unsigned long usp,
-	      int __user *parent_tidp, void __user *child_threadptr,
-	      int __user *child_tidp, int p6,
-	      struct pt_regs *regs)
-{
- 	return do_fork(clone_flags, usp, regs, 0, parent_tidp, child_tidp);
-}
-
-int sys_fork(unsigned long p1, unsigned long p2, unsigned long p3,
-	     unsigned long p4, unsigned long p5, unsigned long p6,
-	     struct pt_regs *regs)
-{
-	return do_fork(SIGCHLD, 0, regs, 0, NULL, NULL);
-}
-
-int sys_vfork(unsigned long p1, unsigned long p2, unsigned long p3,
-	      unsigned long p4, unsigned long p5, unsigned long p6,
-	      struct pt_regs *regs)
-{
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0,
-			regs, 0, NULL, NULL);
-}
-
 static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,
 				  unsigned long nbytes)
 {

commit ab75819d3942a34d151a34fd43f346d5d8a48148
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Oct 21 22:33:39 2012 -0400

    powerpc: make fork_idle() take the common "kernel thread" path in copy_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3665d287780f..f6d244db9203 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -745,7 +745,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	/* Copy registers */
 	sp -= sizeof(struct pt_regs);
 	childregs = (struct pt_regs *) sp;
-	if (!regs) {
+	if (unlikely(p->flags & PF_KTHREAD)) {
 		struct thread_info *ti = (void *)task_stack_page(p);
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);

commit ea516b11545afa5b1420621981c1411a62bef87e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Oct 21 22:28:43 2012 -0400

    powerpc: put the "zero usp means using parent's stack pointer" to copy_thread()
    
    simplifies callers, at that...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index de9f032116c0..3665d287780f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -761,7 +761,8 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	} else {
 		CHECK_FULL_REGS(regs);
 		*childregs = *regs;
-		childregs->gpr[1] = usp;
+		if (usp)
+			childregs->gpr[1] = usp;
 		p->thread.regs = childregs;
 		childregs->gpr[3] = 0;  /* Result from fork() */
 		if (clone_flags & CLONE_SETTLS) {
@@ -1030,8 +1031,6 @@ int sys_clone(unsigned long clone_flags, unsigned long usp,
 	      int __user *child_tidp, int p6,
 	      struct pt_regs *regs)
 {
-	if (usp == 0)
-		usp = regs->gpr[1];	/* stack pointer for child */
  	return do_fork(clone_flags, usp, regs, 0, parent_tidp, child_tidp);
 }
 
@@ -1039,14 +1038,14 @@ int sys_fork(unsigned long p1, unsigned long p2, unsigned long p3,
 	     unsigned long p4, unsigned long p5, unsigned long p6,
 	     struct pt_regs *regs)
 {
-	return do_fork(SIGCHLD, regs->gpr[1], regs, 0, NULL, NULL);
+	return do_fork(SIGCHLD, 0, regs, 0, NULL, NULL);
 }
 
 int sys_vfork(unsigned long p1, unsigned long p2, unsigned long p3,
 	      unsigned long p4, unsigned long p5, unsigned long p6,
 	      struct pt_regs *regs)
 {
-	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->gpr[1],
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, 0,
 			regs, 0, NULL, NULL);
 }
 

commit 64c2f6596bd84b05a781baf034fdd56ce1192d36
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Oct 21 22:27:13 2012 -0400

    powerpc: don't bother with CHECK_FULL_REGS in sys_fork() et.al.
    
    copy_thread() will do it anyway.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 471c52cfc1fb..de9f032116c0 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1030,7 +1030,6 @@ int sys_clone(unsigned long clone_flags, unsigned long usp,
 	      int __user *child_tidp, int p6,
 	      struct pt_regs *regs)
 {
-	CHECK_FULL_REGS(regs);
 	if (usp == 0)
 		usp = regs->gpr[1];	/* stack pointer for child */
  	return do_fork(clone_flags, usp, regs, 0, parent_tidp, child_tidp);
@@ -1040,7 +1039,6 @@ int sys_fork(unsigned long p1, unsigned long p2, unsigned long p3,
 	     unsigned long p4, unsigned long p5, unsigned long p6,
 	     struct pt_regs *regs)
 {
-	CHECK_FULL_REGS(regs);
 	return do_fork(SIGCHLD, regs->gpr[1], regs, 0, NULL, NULL);
 }
 
@@ -1048,7 +1046,6 @@ int sys_vfork(unsigned long p1, unsigned long p2, unsigned long p3,
 	      unsigned long p4, unsigned long p5, unsigned long p6,
 	      struct pt_regs *regs)
 {
-	CHECK_FULL_REGS(regs);
 	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->gpr[1],
 			regs, 0, NULL, NULL);
 }

commit 9d401279d682280a92db8193ede8415c34588207
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Oct 21 22:25:53 2012 -0400

    powerpc: don't bother with zero-extending arguments in sys_clone()
    
    ... since the syscall glue had been doing that for 9 years already.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8c600c3f6613..471c52cfc1fb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1025,8 +1025,6 @@ int get_unalign_ctl(struct task_struct *tsk, unsigned long adr)
 	return put_user(tsk->thread.align_ctl, (unsigned int __user *)adr);
 }
 
-#define TRUNC_PTR(x)	((typeof(x))(((unsigned long)(x)) & 0xffffffff))
-
 int sys_clone(unsigned long clone_flags, unsigned long usp,
 	      int __user *parent_tidp, void __user *child_threadptr,
 	      int __user *child_tidp, int p6,
@@ -1035,12 +1033,6 @@ int sys_clone(unsigned long clone_flags, unsigned long usp,
 	CHECK_FULL_REGS(regs);
 	if (usp == 0)
 		usp = regs->gpr[1];	/* stack pointer for child */
-#ifdef CONFIG_PPC64
-	if (is_32bit_task()) {
-		parent_tidp = TRUNC_PTR(parent_tidp);
-		child_tidp = TRUNC_PTR(child_tidp);
-	}
-#endif
  	return do_fork(clone_flags, usp, regs, 0, parent_tidp, child_tidp);
 }
 

commit 53b50f9483cce47d1a7aefd1c9f442c094a5b1f7
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Oct 21 16:50:34 2012 -0400

    powerpc: take dereferencing to ret_from_kernel_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7fc70f29edb3..8c600c3f6613 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -749,12 +749,10 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		struct thread_info *ti = (void *)task_stack_page(p);
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
+		childregs->gpr[14] = usp;	/* function */
 #ifdef CONFIG_PPC64
-		childregs->gpr[14] = *(unsigned long *)usp;
 		clear_tsk_thread_flag(p, TIF_32BIT);
 		childregs->softe = 1;
-#else
-		childregs->gpr[14] = usp;	/* function */
 #endif
 		childregs->gpr[15] = arg;
 		p->thread.regs = NULL;	/* no user register state */

commit 40792104b2550ee067f63c3ccc8ea04747dc5037
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Oct 11 10:10:35 2012 -0400

    powerpc: don't mess with r2 in copy_thread() and friends
    
    kernel_thread() callbacks are *not* in modules and are not going to
    be there.  And it's not even read in ppc32 ret_from_kernel_thread(),
    so no need to bother with it there either.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9e685e26c2d1..7fc70f29edb3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -751,12 +751,10 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
 #ifdef CONFIG_PPC64
 		childregs->gpr[14] = *(unsigned long *)usp;
-		childregs->gpr[2] = ((unsigned long *)usp)[1],
 		clear_tsk_thread_flag(p, TIF_32BIT);
 		childregs->softe = 1;
 #else
 		childregs->gpr[14] = usp;	/* function */
-		childregs->gpr[2] = (unsigned long) p;
 #endif
 		childregs->gpr[15] = arg;
 		p->thread.regs = NULL;	/* no user register state */

commit 138d1ce80ed96eff6638f454f0a1500a4aefd17b
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Oct 11 08:41:43 2012 -0400

    powerpc: switch to saner kernel_execve() semantics
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ba48233500f6..9e685e26c2d1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -746,19 +746,21 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	sp -= sizeof(struct pt_regs);
 	childregs = (struct pt_regs *) sp;
 	if (!regs) {
-		/* for kernel thread, set `current' and stackptr in new task */
+		struct thread_info *ti = (void *)task_stack_page(p);
 		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
 #ifdef CONFIG_PPC64
 		childregs->gpr[14] = *(unsigned long *)usp;
 		childregs->gpr[2] = ((unsigned long *)usp)[1],
 		clear_tsk_thread_flag(p, TIF_32BIT);
+		childregs->softe = 1;
 #else
 		childregs->gpr[14] = usp;	/* function */
 		childregs->gpr[2] = (unsigned long) p;
 #endif
 		childregs->gpr[15] = arg;
 		p->thread.regs = NULL;	/* no user register state */
+		ti->flags |= _TIF_RESTOREALL;
 		f = ret_from_kernel_thread;
 	} else {
 		CHECK_FULL_REGS(regs);
@@ -1063,15 +1065,6 @@ int sys_vfork(unsigned long p1, unsigned long p2, unsigned long p3,
 			regs, 0, NULL, NULL);
 }
 
-void __ret_from_kernel_execve(struct pt_regs *normal)
-__noreturn;
-
-void ret_from_kernel_execve(struct pt_regs *normal)
-{
-	set_thread_flag(TIF_RESTOREALL);
-	__ret_from_kernel_execve(normal);
-}
-
 static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,
 				  unsigned long nbytes)
 {

commit 8213a2f3eeafdecf06dd718cb4130372263f6067
Merge: 40924754f2ca 12f79be93d94
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 12 10:49:08 2012 +0900

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal
    
    Pull pile 2 of execve and kernel_thread unification work from Al Viro:
     "Stuff in there: kernel_thread/kernel_execve/sys_execve conversions for
      several more architectures plus assorted signal fixes and cleanups.
    
      There'll be more (in particular, real fixes for the alpha
      do_notify_resume() irq mess)..."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/signal: (43 commits)
      alpha: don't open-code trace_report_syscall_{enter,exit}
      Uninclude linux/freezer.h
      m32r: trim masks
      avr32: trim masks
      tile: don't bother with SIGTRAP in setup_frame
      microblaze: don't bother with SIGTRAP in setup_rt_frame()
      mn10300: don't bother with SIGTRAP in setup_frame()
      frv: no need to raise SIGTRAP in setup_frame()
      x86: get rid of duplicate code in case of CONFIG_VM86
      unicore32: remove pointless test
      h8300: trim _TIF_WORK_MASK
      parisc: decide whether to go to slow path (tracesys) based on thread flags
      parisc: don't bother looping in do_signal()
      parisc: fix double restarts
      bury the rest of TIF_IRET
      sanitize tsk_is_polling()
      bury _TIF_RESTORE_SIGMASK
      unicore32: unobfuscate _TIF_WORK_MASK
      mips: NOTIFY_RESUME is not needed in TIF masks
      mips: merge the identical "return from syscall" per-ABI code
      ...
    
    Conflicts:
            arch/arm/include/asm/thread_info.h

commit 5f3d2f2e1a63679cf1c4a4210f2f1cc2f335bef6
Merge: 283dbd82055e d900bd736646
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Oct 6 03:16:12 2012 +0900

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Benjamin Herrenschmidt:
     "Some highlights in addition to the usual batch of fixes:
    
       - 64TB address space support for 64-bit processes by Aneesh Kumar
    
       - Gavin Shan did a major cleanup & re-organization of our EEH support
         code (IBM fancy PCI error handling & recovery infrastructure) which
         paves the way for supporting different platform backends, along
         with some rework of the PCIe code for the PowerNV platform in order
         to remove home made resource allocations and instead use the
         generic code (which is possible after some small improvements to it
         done by Gavin).
    
       - Uprobes support by Ananth N Mavinakayanahalli
    
       - A pile of embedded updates from Freescale folks, including new SoC
         and board supports, more KVM stuff including preparing for 64-bit
         BookE KVM support, ePAPR 1.1 updates, etc..."
    
    Fixup trivial conflicts in drivers/scsi/ipr.c
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (146 commits)
      powerpc/iommu: Fix multiple issues with IOMMU pools code
      powerpc: Fix VMX fix for memcpy case
      driver/mtd:IFC NAND:Initialise internal SRAM before any write
      powerpc/fsl-pci: use 'Header Type' to identify PCIE mode
      powerpc/eeh: Don't release eeh_mutex in eeh_phb_pe_get
      powerpc: Remove tlb batching hack for nighthawk
      powerpc: Set paca->data_offset = 0 for boot cpu
      powerpc/perf: Sample only if SIAR-Valid bit is set in P7+
      powerpc/fsl-pci: fix warning when CONFIG_SWIOTLB is disabled
      powerpc/mpc85xx: Update interrupt handling for IFC controller
      powerpc/85xx: Enable USB support in p1023rds_defconfig
      powerpc/smp: Do not disable IPI interrupts during suspend
      powerpc/eeh: Fix crash on converting OF node to edev
      powerpc/eeh: Lock module while handling EEH event
      powerpc/kprobe: Don't emulate store when kprobe stwu r1
      powerpc/kprobe: Complete kprobe and migrate exception frame
      powerpc/kprobe: Introduce a new thread flag
      powerpc: Remove unused __get_user64() and __put_user64()
      powerpc/eeh: Global mutex to protect PE tree
      powerpc/eeh: Remove EEH PE for normal PCI hotplug
      ...

commit 0b981cb94bc63a2d0e5eccccdca75fe57643ffce
Merge: 4cba3335826c fdf9c356502a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 1 10:43:39 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Continued quest to clean up and enhance the cputime code by Frederic
      Weisbecker, in preparation for future tickless kernel features.
    
      Other than that, smallish changes."
    
    Fix up trivial conflicts due to additions next to each other in arch/{x86/}Kconfig
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      cputime: Make finegrained irqtime accounting generally available
      cputime: Gather time/stats accounting config options into a single menu
      ia64: Reuse system and user vtime accounting functions on task switch
      ia64: Consolidate user vtime accounting
      vtime: Consolidate system/idle context detection
      cputime: Use a proper subsystem naming for vtime related APIs
      sched: cpu_power: enable ARCH_POWER
      sched/nohz: Clean up select_nohz_load_balancer()
      sched: Fix load avg vs. cpu-hotplug
      sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
      sched: Fix nohz_idle_balance()
      sched: Remove useless code in yield_to()
      sched: Add time unit suffix to sched sysctl knobs
      sched/debug: Limit sd->*_idx range on sysctl
      sched: Remove AFFINE_WAKEUPS feature flag
      s390: Remove leftover account_tick_vtime() header
      cputime: Consolidate vtime handling on context switch
      sched: Move cputime code to its own file
      cputime: Generalize CONFIG_VIRT_CPU_ACCOUNTING
      tile: Remove SD_PREFER_LOCAL leftover
      ...

commit be6abfa769fa07ce89ac73273360b335ae978805
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Aug 31 15:48:05 2012 -0400

    powerpc: switch to generic sys_execve()/kernel_execve()
    
    the only non-obvious part is that current_pt_regs() is really needed
    here - task_pt_regs() is NULL for kernel threads; it's OK for ptrace
    uses (the thing task_pt_regs() is intended for), but not for us.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3b06898fa175..6fdf044f475c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1064,26 +1064,13 @@ int sys_vfork(unsigned long p1, unsigned long p2, unsigned long p3,
 			regs, 0, NULL, NULL);
 }
 
-int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
-	       unsigned long a3, unsigned long a4, unsigned long a5,
-	       struct pt_regs *regs)
+void __ret_from_kernel_execve(struct pt_regs *normal)
+__noreturn;
+
+void ret_from_kernel_execve(struct pt_regs *normal)
 {
-	int error;
-	char *filename;
-
-	filename = getname((const char __user *) a0);
-	error = PTR_ERR(filename);
-	if (IS_ERR(filename))
-		goto out;
-	flush_fp_to_thread(current);
-	flush_altivec_to_thread(current);
-	flush_spe_to_thread(current);
-	error = do_execve(filename,
-			  (const char __user *const __user *) a1,
-			  (const char __user *const __user *) a2, regs);
-	putname(filename);
-out:
-	return error;
+	set_thread_flag(TIF_RESTOREALL);
+	__ret_from_kernel_execve(normal);
 }
 
 static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,

commit 58254e1002a82eb383c5977ad9fd5a451b91fe29
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Wed Sep 12 18:32:42 2012 -0400

    powerpc: split ret_from_fork
    
    ... and get rid of in-kernel syscalls in kernel_thread()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1a1f2ddfb581..3b06898fa175 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -734,30 +734,39 @@ int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 extern unsigned long dscr_default; /* defined in arch/powerpc/kernel/sysfs.c */
 
 int copy_thread(unsigned long clone_flags, unsigned long usp,
-		unsigned long unused, struct task_struct *p,
+		unsigned long arg, struct task_struct *p,
 		struct pt_regs *regs)
 {
 	struct pt_regs *childregs, *kregs;
 	extern void ret_from_fork(void);
+	extern void ret_from_kernel_thread(void);
+	void (*f)(void);
 	unsigned long sp = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 
-	CHECK_FULL_REGS(regs);
 	/* Copy registers */
 	sp -= sizeof(struct pt_regs);
 	childregs = (struct pt_regs *) sp;
-	*childregs = *regs;
-	if ((childregs->msr & MSR_PR) == 0) {
+	if (!regs) {
 		/* for kernel thread, set `current' and stackptr in new task */
+		memset(childregs, 0, sizeof(struct pt_regs));
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
-#ifdef CONFIG_PPC32
-		childregs->gpr[2] = (unsigned long) p;
-#else
+#ifdef CONFIG_PPC64
+		childregs->gpr[14] = *(unsigned long *)usp;
+		childregs->gpr[2] = ((unsigned long *)usp)[1],
 		clear_tsk_thread_flag(p, TIF_32BIT);
+#else
+		childregs->gpr[14] = usp;	/* function */
+		childregs->gpr[2] = (unsigned long) p;
 #endif
+		childregs->gpr[15] = arg;
 		p->thread.regs = NULL;	/* no user register state */
+		f = ret_from_kernel_thread;
 	} else {
+		CHECK_FULL_REGS(regs);
+		*childregs = *regs;
 		childregs->gpr[1] = usp;
 		p->thread.regs = childregs;
+		childregs->gpr[3] = 0;  /* Result from fork() */
 		if (clone_flags & CLONE_SETTLS) {
 #ifdef CONFIG_PPC64
 			if (!is_32bit_task())
@@ -766,8 +775,9 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 #endif
 				childregs->gpr[2] = childregs->gpr[6];
 		}
+
+		f = ret_from_fork;
 	}
-	childregs->gpr[3] = 0;  /* Result from fork() */
 	sp -= STACK_FRAME_OVERHEAD;
 
 	/*
@@ -806,19 +816,17 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		p->thread.dscr = current->thread.dscr;
 	}
 #endif
-
 	/*
 	 * The PPC64 ABI makes use of a TOC to contain function 
 	 * pointers.  The function (ret_from_except) is actually a pointer
 	 * to the TOC entry.  The first entry is a pointer to the actual
 	 * function.
- 	 */
+	 */
 #ifdef CONFIG_PPC64
-	kregs->nip = *((unsigned long *)ret_from_fork);
+	kregs->nip = *((unsigned long *)f);
 #else
-	kregs->nip = (unsigned long)ret_from_fork;
+	kregs->nip = (unsigned long)f;
 #endif
-
 	return 0;
 }
 

commit 4474ef055c5d8cb8eaf002d69e49af71e3aa3a88
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Sep 6 21:24:56 2012 +0000

    powerpc: Rework set_dabr so it can take a DABRX value as well
    
    Rework set_dabr to take a DABRX value as well.
    
    Both the pseries and PS3 hypervisors do some checks on the DABRX
    values that are passed in the hcall.  This patch stops bogus values
    from being passed to hypervisor.  Also, in the case where we are
    clearing the breakpoint, where DABR and DABRX are zero, we modify the
    DABRX value to make it valid so that the hcall won't fail.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2e743de545d0..50e504c29bb9 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -285,7 +285,7 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 		return;
 
 	/* Clear the DABR */
-	set_dabr(0);
+	set_dabr(0, 0);
 
 	/* Deliver the signal to userspace */
 	info.si_signo = SIGTRAP;
@@ -366,18 +366,19 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 {
 	if (thread->dabr) {
 		thread->dabr = 0;
-		set_dabr(0);
+		thread->dabrx = 0;
+		set_dabr(0, 0);
 	}
 }
 #endif /* !CONFIG_HAVE_HW_BREAKPOINT */
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 
-int set_dabr(unsigned long dabr)
+int set_dabr(unsigned long dabr, unsigned long dabrx)
 {
 	__get_cpu_var(current_dabr) = dabr;
 
 	if (ppc_md.set_dabr)
-		return ppc_md.set_dabr(dabr);
+		return ppc_md.set_dabr(dabr, dabrx);
 
 	/* XXX should we have a CPU_FTR_HAS_DABR ? */
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
@@ -387,9 +388,8 @@ int set_dabr(unsigned long dabr)
 #endif
 #elif defined(CONFIG_PPC_BOOK3S)
 	mtspr(SPRN_DABR, dabr);
+	mtspr(SPRN_DABRX, dabrx);
 #endif
-
-
 	return 0;
 }
 
@@ -482,7 +482,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
  */
 #ifndef CONFIG_HAVE_HW_BREAKPOINT
 	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
-		set_dabr(new->thread.dabr);
+		set_dabr(new->thread.dabr, new->thread.dabrx);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 

commit fff34b3412b9401a76ba9d021db1bd91cb0e02b6
Merge: 28e1e58fb668 636802ef96ee
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Sep 7 09:48:59 2012 +1000

    Merge branch 'merge' into next
    
    Brings in various bug fixes from 3.6-rcX

commit 1021cb268b3025573c4811f1dee4a11260c4507b
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Sep 3 16:49:47 2012 +0000

    powerpc: Fix DSCR inheritance in copy_thread()
    
    If the default DSCR is non zero we set thread.dscr_inherit in
    copy_thread() meaning the new thread and all its children will ignore
    future updates to the default DSCR. This is not intended and is
    a change in behaviour that a number of our users have hit.
    
    We just need to inherit thread.dscr and thread.dscr_inherit from
    the parent which ends up being much simpler.
    
    This was found with the following test case:
    
    http://ozlabs.org/~anton/junkcode/dscr_default_test.c
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: <stable@kernel.org> # 3.0+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 710f400476de..1a1f2ddfb581 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -802,16 +802,8 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 #endif /* CONFIG_PPC_STD_MMU_64 */
 #ifdef CONFIG_PPC64 
 	if (cpu_has_feature(CPU_FTR_DSCR)) {
-		if (current->thread.dscr_inherit) {
-			p->thread.dscr_inherit = 1;
-			p->thread.dscr = current->thread.dscr;
-		} else if (0 != dscr_default) {
-			p->thread.dscr_inherit = 1;
-			p->thread.dscr = dscr_default;
-		} else {
-			p->thread.dscr_inherit = 0;
-			p->thread.dscr = 0;
-		}
+		p->thread.dscr_inherit = current->thread.dscr_inherit;
+		p->thread.dscr = current->thread.dscr;
 	}
 #endif
 

commit 41ab5266c3622354353433618edb92ab278025fa
Author: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
Date:   Thu Aug 23 21:27:09 2012 +0000

    powerpc: Add trap_nr to thread_struct
    
    Add thread_struct.trap_nr and use it to store the last exception
    the thread experienced. In this patch, we populate the field at
    various places where we force_sig_info() to the process.
    
    This is also used in uprobes to determine if the probed instruction
    caused an exception.
    
    Signed-off-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 710f400476de..8e701a4ed36b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -258,6 +258,7 @@ void do_send_trap(struct pt_regs *regs, unsigned long address,
 {
 	siginfo_t info;
 
+	current->thread.trap_nr = signal_code;
 	if (notify_die(DIE_DABR_MATCH, "dabr_match", regs, error_code,
 			11, SIGSEGV) == NOTIFY_STOP)
 		return;
@@ -275,6 +276,7 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 {
 	siginfo_t info;
 
+	current->thread.trap_nr = TRAP_HWBKPT;
 	if (notify_die(DIE_DABR_MATCH, "dabr_match", regs, error_code,
 			11, SIGSEGV) == NOTIFY_STOP)
 		return;

commit baa36046d09ea6dbc122c795566992318663d9eb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jun 18 17:54:14 2012 +0200

    cputime: Consolidate vtime handling on context switch
    
    The archs that implement virtual cputime accounting all
    flush the cputime of a task when it gets descheduled
    and sometimes set up some ground initialization for the
    next task to account its cputime.
    
    These archs all put their own hooks in their context
    switch callbacks and handle the off-case themselves.
    
    Consolidate this by creating a new account_switch_vtime()
    callback called in generic code right after a context switch
    and that these archs must implement to flush the prev task
    cputime and initialize the next task cputime related state.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 710f400476de..d73fa999b47b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -514,9 +514,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	local_irq_save(flags);
 
-	account_system_vtime(current);
-	account_process_vtime(current);
-
 	/*
 	 * We can't take a PMU exception inside _switch() since there is a
 	 * window where the kernel stack SLB and the kernel stack are out

commit ec0d7f18ab7b5097d7c0c8f3d909ca1031b9d5cd
Merge: 269af9a1a08d 1dcc8d7ba235
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 10:59:07 2012 -0700

    Merge branch 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull fpu state cleanups from Ingo Molnar:
     "This tree streamlines further aspects of FPU handling by eliminating
      the prepare_to_copy() complication and moving that logic to
      arch_dup_task_struct().
    
      It also fixes the FPU dumps in threaded core dumps, removes and old
      (and now invalid) assumption plus micro-optimizes the exit path by
      avoiding an FPU save for dead tasks."
    
    Fixed up trivial add-add conflict in arch/sh/kernel/process.c that came
    in because we now do the FPU handling in arch_dup_task_struct() rather
    than the legacy (and now gone) prepare_to_copy().
    
    * 'x86-fpu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      x86, fpu: drop the fpu state during thread exit
      x86, xsave: remove thread_has_fpu() bug check in __sanitize_i387_state()
      coredump: ensure the fpu state is flushed for proper multi-threaded core dump
      fork: move the real prepare_to_copy() users to arch_dup_task_struct()

commit 6f73b3629f774c6cba589b15fd095112b25ca923
Merge: 3a8580f82024 2074b1d9d53a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 23 09:02:42 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Benjamin Herrenschmidt:
     "Here are the powerpc goodies for 3.5.  Main highlights are:
    
       - Support for the NX crypto engine in Power7+
       - A bunch of Anton goodness, including some micro optimization of our
         syscall entry on Power7
       - I converted a pile of our thermal control drivers to the new i2c
         APIs (essentially turning the old therm_pm72 into a proper set of
         windfarm drivers).  That's one more step toward removing the
         deprecated i2c APIs, there's still a few drivers to fix, but we are
         getting close
       - kexec/kdump support for 47x embedded cores
    
      The big missing thing here is no updates from Freescale.  Not sure
      what's up here, but with Kumar not working for them anymore things are
      a bit in a state of flux in that area."
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (71 commits)
      powerpc: Fix irq distribution
      Revert "powerpc/hw-breakpoint: Use generic hw-breakpoint interfaces for new PPC ptrace flags"
      powerpc: Fixing a cputhread code documentation
      powerpc/crypto: Enable the PFO-based encryption device
      powerpc/crypto: Build files for the nx device driver
      powerpc/crypto: debugfs routines and docs for the nx device driver
      powerpc/crypto: SHA512 hash routines for nx encryption
      powerpc/crypto: SHA256 hash routines for nx encryption
      powerpc/crypto: AES-XCBC mode routines for nx encryption
      powerpc/crypto: AES-GCM mode routines for nx encryption
      powerpc/crypto: AES-ECB mode routines for nx encryption
      powerpc/crypto: AES-CTR mode routines for nx encryption
      powerpc/crypto: AES-CCM mode routines for nx encryption
      powerpc/crypto: AES-CBC mode routines for nx encryption
      powerpc/crypto: nx driver code supporting nx encryption
      powerpc/pseries: Enable the PFO-based RNG accelerator
      powerpc/pseries/hwrng: PFO-based hwrng driver
      powerpc/pseries: Add PFO support to the VIO bus
      powerpc/pseries: Add pseries update notifier for OFDT prop changes
      powerpc/pseries: Add new hvcall constants to support PFO
      ...

commit 55ccf3fe3f9a3441731aa79cf42a628fc4ecace9
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Wed May 16 15:03:51 2012 -0700

    fork: move the real prepare_to_copy() users to arch_dup_task_struct()
    
    Historical prepare_to_copy() is mostly a no-op, duplicated for majority of
    the architectures and the rest following the x86 model of flushing the extended
    register state like fpu there.
    
    Remove it and use the arch_dup_task_struct() instead.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Link: http://lkml.kernel.org/r/1336692811-30576-1-git-send-email-suresh.b.siddha@intel.com
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Koichi Yasutake <yasutake.koichi@jp.panasonic.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Aurelien Jacquiot <a-jacquiot@ti.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Lennox Wu <lennox.wu@gmail.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4937c9690090..bc129f24e11f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -711,18 +711,21 @@ release_thread(struct task_struct *t)
 }
 
 /*
- * This gets called before we allocate a new thread and copy
- * the current task into it.
+ * this gets called so that we can store coprocessor state into memory and
+ * copy the current task into the new thread.
  */
-void prepare_to_copy(struct task_struct *tsk)
+int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)
 {
-	flush_fp_to_thread(current);
-	flush_altivec_to_thread(current);
-	flush_vsx_to_thread(current);
-	flush_spe_to_thread(current);
+	flush_fp_to_thread(src);
+	flush_altivec_to_thread(src);
+	flush_vsx_to_thread(src);
+	flush_spe_to_thread(src);
 #ifdef CONFIG_HAVE_HW_BREAKPOINT
-	flush_ptrace_hw_breakpoint(tsk);
+	flush_ptrace_hw_breakpoint(src);
 #endif /* CONFIG_HAVE_HW_BREAKPOINT */
+
+	*dst = *src;
+	return 0;
 }
 
 /*

commit 96c951179736eb59c5f66de2ac85af9e7a6a8b15
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat May 5 15:05:45 2012 +0000

    powerpc: Use common threadinfo allocator
    
    The core now has a threadinfo allocator which uses a kmemcache when
    THREAD_SIZE < PAGE_SIZE.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Link: http://lkml.kernel.org/r/20120505150142.059161130@linutronix.de

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4937c9690090..aa05935b6947 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1252,37 +1252,6 @@ void __ppc64_runlatch_off(void)
 }
 #endif /* CONFIG_PPC64 */
 
-#if THREAD_SHIFT < PAGE_SHIFT
-
-static struct kmem_cache *thread_info_cache;
-
-struct thread_info *alloc_thread_info_node(struct task_struct *tsk, int node)
-{
-	struct thread_info *ti;
-
-	ti = kmem_cache_alloc_node(thread_info_cache, GFP_KERNEL, node);
-	if (unlikely(ti == NULL))
-		return NULL;
-#ifdef CONFIG_DEBUG_STACK_USAGE
-	memset(ti, 0, THREAD_SIZE);
-#endif
-	return ti;
-}
-
-void free_thread_info(struct thread_info *ti)
-{
-	kmem_cache_free(thread_info_cache, ti);
-}
-
-void thread_info_cache_init(void)
-{
-	thread_info_cache = kmem_cache_create("thread_info", THREAD_SIZE,
-					      THREAD_SIZE, 0, NULL);
-	BUG_ON(thread_info_cache == NULL);
-}
-
-#endif /* THREAD_SHIFT < PAGE_SHIFT */
-
 unsigned long arch_align_stack(unsigned long sp)
 {
 	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)

commit 35000870fcfbb28757ad47de77b4645072d916b8
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Apr 15 20:56:45 2012 +0000

    powerpc: Optimise enable_kernel_altivec
    
    Add two optimisations to enable_kernel_altivec:
    
    - enable_kernel_altivec has already determined if we need to
    save the previous task's state but we call giveup_altivec
    in both cases, requiring an extra branch in giveup_altivec. Create
    giveup_altivec_notask which only turns on the VMX bit in the
    MSR.
    
    - We write the VMX MSR bit each time we call enable_kernel_altivec
    even it was already set. Check the bit and branch out if we have
    already set it. The classic case for this is vectored IO
    where we have to copy multiple buffers to or from userspace.
    
    The following testcase was used to confirm this patch improves
    performance:
    
    http://ozlabs.org/~anton/junkcode/copy_to_user.c
    
    Since the current breakpoint for using VMX in copy_tofrom_user is
    4096 bytes, I'm using buffers of 4096 + 1 cacheline (4224) bytes.
    A benchmark of 16 entry readvs (-s 16):
    
    time copy_to_user -l 4224 -s 16 -i 1000000
    
    completes 5.2% faster on a POWER7 PS700.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4937c9690090..bb58f41fc045 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -124,7 +124,7 @@ void enable_kernel_altivec(void)
 	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
 		giveup_altivec(current);
 	else
-		giveup_altivec(NULL);	/* just enable AltiVec for kernel - force */
+		giveup_altivec_notask();
 #else
 	giveup_altivec(last_task_used_altivec);
 #endif /* CONFIG_SMP */

commit fae2e0fb24c61ca68c98d854a34732549ebc1854
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 11 10:42:15 2012 +1000

    powerpc: Fix typo in runlatch code
    
    Commit fe1952fc0afb9a2e4c79f103c08aef5d13db1873
    "powerpc: Rework runlatch code" has a nasty typo
    where it uses "TLF_RUNLATCH" instead of "_TLF_RUNLATCH"
    (bit number instead of bit mask), causing some flags to
    be potentially lost such as _TLF_RESTORE_SIGMASK
    
    (Brown paper bag for me ! We should be able to make
    that break at compile time with a bit of magic, any
    volunteer ?)
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f88698c0f332..4937c9690090 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1235,7 +1235,7 @@ void __ppc64_runlatch_on(void)
 	ctrl |= CTRL_RUNLATCH;
 	mtspr(SPRN_CTRLT, ctrl);
 
-	ti->local_flags |= TLF_RUNLATCH;
+	ti->local_flags |= _TLF_RUNLATCH;
 }
 
 /* Called with hard IRQs off */
@@ -1244,7 +1244,7 @@ void __ppc64_runlatch_off(void)
 	struct thread_info *ti = current_thread_info();
 	unsigned long ctrl;
 
-	ti->local_flags &= ~TLF_RUNLATCH;
+	ti->local_flags &= ~_TLF_RUNLATCH;
 
 	ctrl = mfspr(SPRN_CTRLF);
 	ctrl &= ~CTRL_RUNLATCH;

commit ae3a197e3d0bfe3f4bf1693723e82dc018c096f3
Author: David Howells <dhowells@redhat.com>
Date:   Wed Mar 28 18:30:02 2012 +0100

    Disintegrate asm/system.h for PowerPC
    
    Disintegrate asm/system.h for PowerPC.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    cc: linuxppc-dev@lists.ozlabs.org

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e40707032ac3..f88698c0f332 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -41,14 +41,16 @@
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
-#include <asm/system.h>
 #include <asm/io.h>
 #include <asm/processor.h>
 #include <asm/mmu.h>
 #include <asm/prom.h>
 #include <asm/machdep.h>
 #include <asm/time.h>
+#include <asm/runlatch.h>
 #include <asm/syscalls.h>
+#include <asm/switch_to.h>
+#include <asm/debug.h>
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
 #endif

commit 7230c5644188cd9e3fb380cc97dde00c464a3ba7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 6 18:27:59 2012 +1100

    powerpc: Rework lazy-interrupt handling
    
    The current implementation of lazy interrupts handling has some
    issues that this tries to address.
    
    We don't do the various workarounds we need to do when re-enabling
    interrupts in some cases such as when returning from an interrupt
    and thus we may still lose or get delayed decrementer or doorbell
    interrupts.
    
    The current scheme also makes it much harder to handle the external
    "edge" interrupts provided by some BookE processors when using the
    EPR facility (External Proxy) and the Freescale Hypervisor.
    
    Additionally, we tend to keep interrupts hard disabled in a number
    of cases, such as decrementer interrupts, external interrupts, or
    when a masked decrementer interrupt is pending. This is sub-optimal.
    
    This is an attempt at fixing it all in one go by reworking the way
    we do the lazy interrupt disabling from the ground up.
    
    The base idea is to replace the "hard_enabled" field with a
    "irq_happened" field in which we store a bit mask of what interrupt
    occurred while soft-disabled.
    
    When re-enabling, either via arch_local_irq_restore() or when returning
    from an interrupt, we can now decide what to do by testing bits in that
    field.
    
    We then implement replaying of the missed interrupts either by
    re-using the existing exception frame (in exception exit case) or via
    the creation of a new one from an assembly trampoline (in the
    arch_local_irq_enable case).
    
    This removes the need to play with the decrementer to try to create
    fake interrupts, among others.
    
    In addition, this adds a few refinements:
    
     - We no longer  hard disable decrementer interrupts that occur
    while soft-disabled. We now simply bump the decrementer back to max
    (on BookS) or leave it stopped (on BookE) and continue with hard interrupts
    enabled, which means that we'll potentially get better sample quality from
    performance monitor interrupts.
    
     - Timer, decrementer and doorbell interrupts now hard-enable
    shortly after removing the source of the interrupt, which means
    they no longer run entirely hard disabled. Again, this will improve
    perf sample quality.
    
     - On Book3E 64-bit, we now make the performance monitor interrupt
    act as an NMI like Book3S (the necessary C code for that to work
    appear to already be present in the FSL perf code, notably calling
    nmi_enter instead of irq_enter). (This also fixes a bug where BookE
    perfmon interrupts could clobber r14 ... oops)
    
     - We could make "masked" decrementer interrupts act as NMIs when doing
    timer-based perf sampling to improve the sample quality.
    
    Signed-off-by-yet: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    ---
    
    v2:
    
    - Add hard-enable to decrementer, timer and doorbells
    - Fix CR clobber in masked irq handling on BookE
    - Make embedded perf interrupt act as an NMI
    - Add a PACA_HAPPENED_EE_EDGE for use by FSL if they want
      to retrigger an interrupt without preventing hard-enable
    
    v3:
    
     - Fix or vs. ori bug on Book3E
     - Fix enabling of interrupts for some exceptions on Book3E
    
    v4:
    
     - Fix resend of doorbells on return from interrupt on Book3E
    
    v5:
    
     - Rebased on top of my latest series, which involves some significant
    rework of some aspects of the patch.
    
    v6:
     - 32-bit compile fix
     - more compile fixes with various .config combos
     - factor out the asm code to soft-disable interrupts
     - remove the C wrapper around preempt_schedule_irq
    
    v7:
     - Fix a bug with hard irq state tracking on native power7

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index bf80a1d5f8fe..e40707032ac3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -647,6 +647,9 @@ void show_regs(struct pt_regs * regs)
 	printk("MSR: "REG" ", regs->msr);
 	printbits(regs->msr, msr_bits);
 	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
+#ifdef CONFIG_PPC64
+	printk("SOFTE: %ld\n", regs->softe);
+#endif
 	trap = TRAP(regs);
 	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
 		printk("CFAR: "REG"\n", regs->orig_gpr3);

commit fe1952fc0afb9a2e4c79f103c08aef5d13db1873
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Mar 1 12:45:27 2012 +1100

    powerpc: Rework runlatch code
    
    This moves the inlines into system.h and changes the runlatch
    code to use the thread local flags (non-atomic) rather than
    the TIF flags (atomic) to keep track of the latch state.
    
    The code to turn it back on in an asynchronous interrupt is
    now simplified and partially inlined.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index d817ab018486..bf80a1d5f8fe 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1220,34 +1220,32 @@ void dump_stack(void)
 EXPORT_SYMBOL(dump_stack);
 
 #ifdef CONFIG_PPC64
-void ppc64_runlatch_on(void)
+/* Called with hard IRQs off */
+void __ppc64_runlatch_on(void)
 {
+	struct thread_info *ti = current_thread_info();
 	unsigned long ctrl;
 
-	if (cpu_has_feature(CPU_FTR_CTRL) && !test_thread_flag(TIF_RUNLATCH)) {
-		HMT_medium();
-
-		ctrl = mfspr(SPRN_CTRLF);
-		ctrl |= CTRL_RUNLATCH;
-		mtspr(SPRN_CTRLT, ctrl);
+	ctrl = mfspr(SPRN_CTRLF);
+	ctrl |= CTRL_RUNLATCH;
+	mtspr(SPRN_CTRLT, ctrl);
 
-		set_thread_flag(TIF_RUNLATCH);
-	}
+	ti->local_flags |= TLF_RUNLATCH;
 }
 
+/* Called with hard IRQs off */
 void __ppc64_runlatch_off(void)
 {
+	struct thread_info *ti = current_thread_info();
 	unsigned long ctrl;
 
-	HMT_medium();
-
-	clear_thread_flag(TIF_RUNLATCH);
+	ti->local_flags &= ~TLF_RUNLATCH;
 
 	ctrl = mfspr(SPRN_CTRLF);
 	ctrl &= ~CTRL_RUNLATCH;
 	mtspr(SPRN_CTRLT, ctrl);
 }
-#endif
+#endif /* CONFIG_PPC64 */
 
 #if THREAD_SHIFT < PAGE_SHIFT
 

commit 40c8cefaaf12734327db7199a56e60058d98e7b6
Author: Ira Snyder <iws@ovro.caltech.edu>
Date:   Fri Jan 6 12:34:07 2012 +0000

    powerpc: Fix kernel log of oops/panic instruction dump
    
    A kernel oops/panic prints an instruction dump showing several
    instructions before and after the instruction which caused the
    oops/panic.
    
    The code intended that the faulting instruction be enclosed in angle
    brackets, however a bug caused the faulting instruction to be
    interpreted by printk() as the message log level.
    
    To fix this, the KERN_CONT log level is added before the actual text of
    the printed message.
    
    === Before the patch ===
    
    [ 1081.587266] Instruction dump:
    [ 1081.590236] 7c000110 7c0000f8 5400077c 552907f6 7d290378 992b0003 4e800020 38000001
    [ 1081.598034] 3d20c03a 9009a114 7c0004ac 39200000
    [ 1081.602500]  4e800020 3803ffd0 2b800009
    
    <4>[ 1081.587266] Instruction dump:
    <4>[ 1081.590236] 7c000110 7c0000f8 5400077c 552907f6 7d290378 992b0003 4e800020 38000001
    <4>[ 1081.598034] 3d20c03a 9009a114 7c0004ac 39200000
    <98090000>[ 1081.602500]  4e800020 3803ffd0 2b800009
    
    === After the patch ===
    
    [   51.385216] Instruction dump:
    [   51.388186] 7c000110 7c0000f8 5400077c 552907f6 7d290378 992b0003 4e800020 38000001
    [   51.395986] 3d20c03a 9009a114 7c0004ac 39200000 <98090000> 4e800020 3803ffd0 2b800009
    
    <4>[   51.385216] Instruction dump:
    <4>[   51.388186] 7c000110 7c0000f8 5400077c 552907f6 7d290378 992b0003 4e800020 38000001
    <4>[   51.395986] 3d20c03a 9009a114 7c0004ac 39200000 <98090000> 4e800020 3803ffd0 2b800009
    
    Signed-off-by: Ira W. Snyder <iws@ovro.caltech.edu>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ebe5766781aa..d817ab018486 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -566,12 +566,12 @@ static void show_instructions(struct pt_regs *regs)
 		 */
 		if (!__kernel_text_address(pc) ||
 		     __get_user(instr, (unsigned int __user *)pc)) {
-			printk("XXXXXXXX ");
+			printk(KERN_CONT "XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)
-				printk("<%08x> ", instr);
+				printk(KERN_CONT "<%08x> ", instr);
 			else
-				printk("%08x ", instr);
+				printk(KERN_CONT "%08x ", instr);
 		}
 
 		pc += sizeof(int);

commit 3bfd0c9c8f9cd2c09cf3e5376c7113eec3370ebd
Author: Anton Blanchard <anton@samba.org>
Date:   Thu Nov 24 19:35:57 2011 +0000

    powerpc: Decode correct MSR bits in oops output
    
    On a 64bit book3s machine I have an oops from a system reset that
    claims the book3e CE bit was set:
    
    MSR: 8000000000021032 <ME,CE,IR,DR>  CR: 24004082  XER: 00000010
    
    On a book3s machine system reset sets IBM bit 46 and 47 depending on
    the power saving mode. Separate the definitions by type and for
    completeness add the rest of the bits in.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6457574c0b2f..ebe5766781aa 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -584,16 +584,32 @@ static struct regbit {
 	unsigned long bit;
 	const char *name;
 } msr_bits[] = {
+#if defined(CONFIG_PPC64) && !defined(CONFIG_BOOKE)
+	{MSR_SF,	"SF"},
+	{MSR_HV,	"HV"},
+#endif
+	{MSR_VEC,	"VEC"},
+	{MSR_VSX,	"VSX"},
+#ifdef CONFIG_BOOKE
+	{MSR_CE,	"CE"},
+#endif
 	{MSR_EE,	"EE"},
 	{MSR_PR,	"PR"},
 	{MSR_FP,	"FP"},
-	{MSR_VEC,	"VEC"},
-	{MSR_VSX,	"VSX"},
 	{MSR_ME,	"ME"},
-	{MSR_CE,	"CE"},
+#ifdef CONFIG_BOOKE
 	{MSR_DE,	"DE"},
+#else
+	{MSR_SE,	"SE"},
+	{MSR_BE,	"BE"},
+#endif
 	{MSR_IR,	"IR"},
 	{MSR_DR,	"DR"},
+	{MSR_PMM,	"PMM"},
+#ifndef CONFIG_BOOKE
+	{MSR_RI,	"RI"},
+	{MSR_LE,	"LE"},
+#endif
 	{0,		NULL}
 };
 

commit 187b9f2aa769198daa7cf8054abb65a08b8d8b47
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Oct 6 02:53:40 2011 +0000

    powerpc/book3e-64: Fix debug support for userspace
    
    With the introduction of CONFIG_PPC_ADV_DEBUG_REGS user space debug is
    broken on Book-E 64-bit parts that support delayed debug events.  When
    switch_booke_debug_regs() sets DBCR0 we'll start getting debug events as
    MSR_DE is also set and we aren't able to handle debug events from kernel
    space.
    
    We can remove the hack that always enables MSR_DE and loads up DBCR0 and
    just utilize switch_booke_debug_regs() to get user space debug working
    again.
    
    We still need to handle critical/debug exception stacks & proper
    save/restore of state for those exception levles to support debug events
    from kernel space like we have on 32-bit.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ad3612af0a04..6457574c0b2f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -486,28 +486,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	new_thread = &new->thread;
 	old_thread = &current->thread;
 
-#if defined(CONFIG_PPC_BOOK3E_64)
-	/* XXX Current Book3E code doesn't deal with kernel side DBCR0,
-	 * we always hold the user values, so we set it now.
-	 *
-	 * However, we ensure the kernel MSR:DE is appropriately cleared too
-	 * to avoid spurrious single step exceptions in the kernel.
-	 *
-	 * This will have to change to merge with the ppc32 code at some point,
-	 * but I don't like much what ppc32 is doing today so there's some
-	 * thinking needed there
-	 */
-	if ((new_thread->dbcr0 | old_thread->dbcr0) & DBCR0_IDM) {
-		u32 dbcr0;
-
-		mtmsr(mfmsr() & ~MSR_DE);
-		isync();
-		dbcr0 = mfspr(SPRN_DBCR0);
-		dbcr0 = (dbcr0 & DBCR0_EDM) | new_thread->dbcr0;
-		mtspr(SPRN_DBCR0, dbcr0);
-	}
-#endif /* CONFIG_PPC64_BOOK3E */
-
 #ifdef CONFIG_PPC64
 	/*
 	 * Collect processor utilization data per process

commit ba28c9aae26ef7f3651eef6835fae30a979f88ba
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Oct 6 02:53:38 2011 +0000

    powerpc: Revert show_regs() define for readability
    
    We had an existing ifdef for 4xx & BOOKE processors that got changed to
    CONFIG_PPC_ADV_DEBUG_REGS.  The define has nothing to do with
    CONFIG_PPC_ADV_DEBUG_REGS.  The define really should be:
    
     #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
    
    and not
    
     #ifdef CONFIG_PPC_ADV_DEBUG_REGS
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9054ca9ab4f9..ad3612af0a04 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -657,7 +657,7 @@ void show_regs(struct pt_regs * regs)
 	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
 		printk("CFAR: "REG"\n", regs->orig_gpr3);
 	if (trap == 0x300 || trap == 0x600)
-#ifdef CONFIG_PPC_ADV_DEBUG_REGS
+#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
 		printk("DEAR: "REG", ESR: "REG"\n", regs->dar, regs->dsisr);
 #else
 		printk("DAR: "REG", DSISR: %08lx\n", regs->dar, regs->dsisr);

commit 4b16f8e2d6d64249f0ed3ca7fe2a319d0dde2719
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 18:24:23 2011 -0400

    powerpc: various straight conversions from module.h --> export.h
    
    All these files were including module.h just for the basic
    EXPORT_SYMBOL infrastructure.  We can shift them off to the
    export.h header which is a way smaller footprint and thus
    realize some compile time gains.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8f53954e75a3..9054ca9ab4f9 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -28,7 +28,7 @@
 #include <linux/init.h>
 #include <linux/prctl.h>
 #include <linux/init_task.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/kallsyms.h>
 #include <linux/mqueue.h>
 #include <linux/hardirq.h>

commit 184475029a724b6b900d88fc3a5f462a6107d5af
Merge: 3b76eefe0f97 f1f4ee01c0d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 22:59:39 2011 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (99 commits)
      drivers/virt: add missing linux/interrupt.h to fsl_hypervisor.c
      powerpc/85xx: fix mpic configuration in CAMP mode
      powerpc: Copy back TIF flags on return from softirq stack
      powerpc/64: Make server perfmon only built on ppc64 server devices
      powerpc/pseries: Fix hvc_vio.c build due to recent changes
      powerpc: Exporting boot_cpuid_phys
      powerpc: Add CFAR to oops output
      hvc_console: Add kdb support
      powerpc/pseries: Fix hvterm_raw_get_chars to accept < 16 chars, fixing xmon
      powerpc/irq: Quieten irq mapping printks
      powerpc: Enable lockup and hung task detectors in pseries and ppc64 defeconfigs
      powerpc: Add mpt2sas driver to pseries and ppc64 defconfig
      powerpc: Disable IRQs off tracer in ppc64 defconfig
      powerpc: Sync pseries and ppc64 defconfigs
      powerpc/pseries/hvconsole: Fix dropped console output
      hvc_console: Improve tty/console put_chars handling
      powerpc/kdump: Fix timeout in crash_kexec_wait_realmode
      powerpc/mm: Fix output of total_ram.
      powerpc/cpufreq: Add cpufreq driver for Momentum Maple boards
      powerpc: Correct annotations of pmu registration functions
      ...
    
    Fix up trivial Kconfig/Makefile conflicts in arch/powerpc, drivers, and
    drivers/cpufreq

commit 5115a026cebeb5537016497e78f4402e5d4ac54e
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Jul 14 19:25:12 2011 +0000

    powerpc: Add CFAR to oops output
    
    Now we have the CFAR saved add it to the oops output.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 885a2dd2ab80..d1aa3f43a68c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -650,6 +650,8 @@ void show_regs(struct pt_regs * regs)
 	printbits(regs->msr, msr_bits);
 	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
+	if ((regs->trap != 0xc00) && cpu_has_feature(CPU_FTR_CFAR))
+		printk("CFAR: "REG"\n", regs->orig_gpr3);
 	if (trap == 0x300 || trap == 0x600)
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 		printk("DEAR: "REG", ESR: "REG"\n", regs->dar, regs->dsisr);

commit 0e0ebdb9c2ba7b56a82ba36d29ab3d8cb99de9e7
Author: Mathias Krause <minipli@googlemail.com>
Date:   Fri Jun 10 03:10:22 2011 +0000

    powerpc: Remove redundant set_fs(USER_DS)
    
    The address limit is already set in flush_old_exec() so this
    set_fs(USER_DS) is redundant.
    
    Signed-off-by: Mathias Krause <minipli@googlemail.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 91e52df3d81d..885a2dd2ab80 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -831,8 +831,6 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	unsigned long load_addr = regs->gpr[2];	/* saved by ELF_PLAT_INIT */
 #endif
 
-	set_fs(USER_DS);
-
 	/*
 	 * If we exec out of a kernel thread then thread.regs will not be
 	 * set.  Do it now.

commit de56a948b9182fbcf92cb8212f114de096c2d574
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:21:34 2011 +0000

    KVM: PPC: Add support for Book3S processors in hypervisor mode
    
    This adds support for KVM running on 64-bit Book 3S processors,
    specifically POWER7, in hypervisor mode.  Using hypervisor mode means
    that the guest can use the processor's supervisor mode.  That means
    that the guest can execute privileged instructions and access privileged
    registers itself without trapping to the host.  This gives excellent
    performance, but does mean that KVM cannot emulate a processor
    architecture other than the one that the hardware implements.
    
    This code assumes that the guest is running paravirtualized using the
    PAPR (Power Architecture Platform Requirements) interface, which is the
    interface that IBM's PowerVM hypervisor uses.  That means that existing
    Linux distributions that run on IBM pSeries machines will also run
    under KVM without modification.  In order to communicate the PAPR
    hypercalls to qemu, this adds a new KVM_EXIT_PAPR_HCALL exit code
    to include/linux/kvm.h.
    
    Currently the choice between book3s_hv support and book3s_pr support
    (i.e. the existing code, which runs the guest in user mode) has to be
    made at kernel configuration time, so a given kernel binary can only
    do one or the other.
    
    This new book3s_hv code doesn't support MMIO emulation at present.
    Since we are running paravirtualized guests, this isn't a serious
    restriction.
    
    With the guest running in supervisor mode, most exceptions go straight
    to the guest.  We will never get data or instruction storage or segment
    interrupts, alignment interrupts, decrementer interrupts, program
    interrupts, single-step interrupts, etc., coming to the hypervisor from
    the guest.  Therefore this introduces a new KVMTEST_NONHV macro for the
    exception entry path so that we don't have to do the KVM test on entry
    to those exception handlers.
    
    We do however get hypervisor decrementer, hypervisor data storage,
    hypervisor instruction storage, and hypervisor emulation assist
    interrupts, so we have to handle those.
    
    In hypervisor mode, real-mode accesses can access all of RAM, not just
    a limited amount.  Therefore we put all the guest state in the vcpu.arch
    and use the shadow_vcpu in the PACA only for temporary scratch space.
    We allocate the vcpu with kzalloc rather than vzalloc, and we don't use
    anything in the kvmppc_vcpu_book3s struct, so we don't allocate it.
    We don't have a shared page with the guest, but we still need a
    kvm_vcpu_arch_shared struct to store the values of various registers,
    so we include one in the vcpu_arch struct.
    
    The POWER7 processor has a restriction that all threads in a core have
    to be in the same partition.  MMU-on kernel code counts as a partition
    (partition 0), so we have to do a partition switch on every entry to and
    exit from the guest.  At present we require the host and guest to run
    in single-thread mode because of this hardware restriction.
    
    This code allocates a hashed page table for the guest and initializes
    it with HPTEs for the guest's Virtual Real Memory Area (VRMA).  We
    require that the guest memory is allocated using 16MB huge pages, in
    order to simplify the low-level memory management.  This also means that
    we can get away without tracking paging activity in the host for now,
    since huge pages can't be paged or swapped.
    
    This also adds a few new exports needed by the book3s_hv code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 60ac2a9251db..ec2d0edeb134 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -96,6 +96,7 @@ void flush_fp_to_thread(struct task_struct *tsk)
 		preempt_enable();
 	}
 }
+EXPORT_SYMBOL_GPL(flush_fp_to_thread);
 
 void enable_kernel_fp(void)
 {
@@ -145,6 +146,7 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 		preempt_enable();
 	}
 }
+EXPORT_SYMBOL_GPL(flush_altivec_to_thread);
 #endif /* CONFIG_ALTIVEC */
 
 #ifdef CONFIG_VSX
@@ -186,6 +188,7 @@ void flush_vsx_to_thread(struct task_struct *tsk)
 		preempt_enable();
 	}
 }
+EXPORT_SYMBOL_GPL(flush_vsx_to_thread);
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_SPE

commit 685659ee70db0bac47ffd619c726cf600e504fd7
Author: yu liu <yu.liu@freescale.com>
Date:   Tue Jun 14 18:34:25 2011 -0500

    powerpc/e500: Save SPEFCSR in flush_spe_to_thread()
    
    giveup_spe() saves the SPE state which is protected by MSR[SPE].
    However, modifying SPEFSCR does not trap when MSR[SPE]=0.
    And since SPEFSCR is already saved/restored in _switch(),
    not all the callers want to save SPEFSCR again.
    Thus, saving SPEFSCR should not belong to giveup_spe().
    
    This patch moves SPEFSCR saving to flush_spe_to_thread(),
    and cleans up the caller that needs to save SPEFSCR accordingly.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 91e52df3d81d..60ac2a9251db 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -213,6 +213,7 @@ void flush_spe_to_thread(struct task_struct *tsk)
 #ifdef CONFIG_SMP
 			BUG_ON(tsk != current);
 #endif
+			tsk->thread.spefscr = mfspr(SPRN_SPEFSCR);
 			giveup_spe(tsk);
 		}
 		preempt_enable();

commit d6bf29b44ddf3ca915f77b9383bee8b7a209f3fd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 24 17:11:48 2011 -0700

    powerpc: mmu_gather rework
    
    Fix up powerpc to the new mmu_gather stuff.
    
    PPC has an extra batching queue to RCU free the actual pagetable
    allocations, use the ARCH extentions for that for now.
    
    For the ppc64_tlb_batch, which tracks the vaddrs to unhash from the
    hardware hash-table, keep using per-cpu arrays but flush on context switch
    and use a TLF bit to track the lazy_mmu state.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Nick Piggin <npiggin@kernel.dk>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 095043d79946..91e52df3d81d 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -395,6 +395,9 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	struct thread_struct *new_thread, *old_thread;
 	unsigned long flags;
 	struct task_struct *last;
+#ifdef CONFIG_PPC_BOOK3S_64
+	struct ppc64_tlb_batch *batch;
+#endif
 
 #ifdef CONFIG_SMP
 	/* avoid complexity of lazy save/restore of fpu
@@ -513,7 +516,17 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		old_thread->accum_tb += (current_tb - start_tb);
 		new_thread->start_tb = current_tb;
 	}
-#endif
+#endif /* CONFIG_PPC64 */
+
+#ifdef CONFIG_PPC_BOOK3S_64
+	batch = &__get_cpu_var(ppc64_tlb_batch);
+	if (batch->active) {
+		current_thread_info()->local_flags |= _TLF_LAZY_MMU;
+		if (batch->index)
+			__flush_tlb_pending(batch);
+		batch->active = 0;
+	}
+#endif /* CONFIG_PPC_BOOK3S_64 */
 
 	local_irq_save(flags);
 
@@ -528,6 +541,14 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	hard_irq_disable();
 	last = _switch(old_thread, new_thread);
 
+#ifdef CONFIG_PPC_BOOK3S_64
+	if (current_thread_info()->local_flags & _TLF_LAZY_MMU) {
+		current_thread_info()->local_flags &= ~_TLF_LAZY_MMU;
+		batch = &__get_cpu_var(ppc64_tlb_batch);
+		batch->active = 1;
+	}
+#endif /* CONFIG_PPC_BOOK3S_64 */
+
 	local_irq_restore(flags);
 
 	return last;

commit 44ae3ab3358e962039c36ad4ae461ae9fb29596c
Author: Matt Evans <matt@ozlabs.org>
Date:   Wed Apr 6 19:48:50 2011 +0000

    powerpc: Free up some CPU feature bits by moving out MMU-related features
    
    Some of the 64bit PPC CPU features are MMU-related, so this patch moves
    them to MMU_FTR_ bits.  All cpu_has_feature()-style tests are moved to
    mmu_has_feature(), and seven feature bits are freed as a result.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a01c2d93fd2f..095043d79946 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -757,11 +757,11 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 				_ALIGN_UP(sizeof(struct thread_info), 16);
 
 #ifdef CONFIG_PPC_STD_MMU_64
-	if (cpu_has_feature(CPU_FTR_SLB)) {
+	if (mmu_has_feature(MMU_FTR_SLB)) {
 		unsigned long sp_vsid;
 		unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
 
-		if (cpu_has_feature(CPU_FTR_1T_SEGMENT))
+		if (mmu_has_feature(MMU_FTR_1T_SEGMENT))
 			sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_1T)
 				<< SLB_VSID_SHIFT_1T;
 		else

commit efcac6589a277c10060e4be44b9455cf43838dc1
Author: Alexey Kardashevskiy <aik@au1.ibm.com>
Date:   Wed Mar 2 15:18:48 2011 +0000

    powerpc: Per process DSCR + some fixes (try#4)
    
    The DSCR (aka Data Stream Control Register) is supported on some
    server PowerPC chips and allow some control over the prefetch
    of data streams.
    
    This patch allows the value to be specified per thread by emulating
    the corresponding mfspr and mtspr instructions. Children of such
    threads inherit the value. Other threads use a default value that
    can be specified in sysfs - /sys/devices/system/cpu/dscr_default.
    
    If a thread starts with non default value in the sysfs entry,
    all children threads inherit this non default value even if
    the sysfs value is changed later.
    
    Signed-off-by: Alexey Kardashevskiy <aik@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f74f355a9617..a01c2d93fd2f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -702,6 +702,8 @@ void prepare_to_copy(struct task_struct *tsk)
 /*
  * Copy a thread..
  */
+extern unsigned long dscr_default; /* defined in arch/powerpc/kernel/sysfs.c */
+
 int copy_thread(unsigned long clone_flags, unsigned long usp,
 		unsigned long unused, struct task_struct *p,
 		struct pt_regs *regs)
@@ -769,6 +771,20 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		p->thread.ksp_vsid = sp_vsid;
 	}
 #endif /* CONFIG_PPC_STD_MMU_64 */
+#ifdef CONFIG_PPC64 
+	if (cpu_has_feature(CPU_FTR_DSCR)) {
+		if (current->thread.dscr_inherit) {
+			p->thread.dscr_inherit = 1;
+			p->thread.dscr = current->thread.dscr;
+		} else if (0 != dscr_default) {
+			p->thread.dscr_inherit = 1;
+			p->thread.dscr = dscr_default;
+		} else {
+			p->thread.dscr_inherit = 0;
+			p->thread.dscr = 0;
+		}
+	}
+#endif
 
 	/*
 	 * The PPC64 ABI makes use of a TOC to contain function 

commit b6a84016bd2598e35ead635147fa53619982648d
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Mar 22 16:30:42 2011 -0700

    mm: NUMA aware alloc_thread_info_node()
    
    Add a node parameter to alloc_thread_info(), and change its name to
    alloc_thread_info_node()
    
    This change is needed to allow NUMA aware kthread_create_on_cpu()
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Reviewed-by: Andi Kleen <ak@linux.intel.com>
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8303a6c65ef7..f74f355a9617 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1218,11 +1218,11 @@ void __ppc64_runlatch_off(void)
 
 static struct kmem_cache *thread_info_cache;
 
-struct thread_info *alloc_thread_info(struct task_struct *tsk)
+struct thread_info *alloc_thread_info_node(struct task_struct *tsk, int node)
 {
 	struct thread_info *ti;
 
-	ti = kmem_cache_alloc(thread_info_cache, GFP_KERNEL);
+	ti = kmem_cache_alloc_node(thread_info_cache, GFP_KERNEL, node);
 	if (unlikely(ti == NULL))
 		return NULL;
 #ifdef CONFIG_DEBUG_STACK_USAGE

commit e0780b720f75487911e0174ec3dec2da49f7bbfa
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Thu Feb 10 04:44:35 2011 +0000

    powerpc: Fix call to flush_ptrace_hw_breakpoint()
    
    Fix the error in spelling the config option for hw-breakpoints and fix
    the build issue that follows.
    
    Signed-off by: K.Prasad <prasad@linux.vnet.ibm.com>
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7a1d5cb76932..8303a6c65ef7 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -353,6 +353,7 @@ static void switch_booke_debug_regs(struct thread_struct *new_thread)
 			prime_debug_regs(new_thread);
 }
 #else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
+#ifndef CONFIG_HAVE_HW_BREAKPOINT
 static void set_debug_reg_defaults(struct thread_struct *thread)
 {
 	if (thread->dabr) {
@@ -360,6 +361,7 @@ static void set_debug_reg_defaults(struct thread_struct *thread)
 		set_dabr(0);
 	}
 }
+#endif /* !CONFIG_HAVE_HW_BREAKPOINT */
 #endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 
 int set_dabr(unsigned long dabr)
@@ -670,11 +672,11 @@ void flush_thread(void)
 {
 	discard_lazy_cpu_state();
 
-#ifdef CONFIG_HAVE_HW_BREAKPOINTS
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
 	flush_ptrace_hw_breakpoint(current);
-#else /* CONFIG_HAVE_HW_BREAKPOINTS */
+#else /* CONFIG_HAVE_HW_BREAKPOINT */
 	set_debug_reg_defaults(&current->thread);
-#endif /* CONFIG_HAVE_HW_BREAKPOINTS */
+#endif /* CONFIG_HAVE_HW_BREAKPOINT */
 }
 
 void

commit 7071854bb248926b85141d791f9fa17901a6fa4b
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Jan 11 19:44:30 2011 +0000

    powerpc: Print 32 bits of DSISR in show_regs
    
    We were printing 64 bits of DSISR in show_regs even though it is 32 bit.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 84906d3fc860..7a1d5cb76932 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -631,7 +631,7 @@ void show_regs(struct pt_regs * regs)
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 		printk("DEAR: "REG", ESR: "REG"\n", regs->dar, regs->dsisr);
 #else
-		printk("DAR: "REG", DSISR: "REG"\n", regs->dar, regs->dsisr);
+		printk("DAR: "REG", DSISR: %08lx\n", regs->dar, regs->dsisr);
 #endif
 	printk("TASK = %p[%d] '%s' THREAD: %p",
 	       current, task_pid_nr(current), current->comm, task_thread_info(current));

commit cf9efce0ce3136fa076f53e53154e98455229514
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Aug 26 19:56:43 2010 +0000

    powerpc: Account time using timebase rather than PURR
    
    Currently, when CONFIG_VIRT_CPU_ACCOUNTING is enabled, we use the
    PURR register for measuring the user and system time used by
    processes, as well as other related times such as hardirq and
    softirq times.  This turns out to be quite confusing for users
    because it means that a program will often be measured as taking
    less time when run on a multi-threaded processor (SMT2 or SMT4 mode)
    than it does when run on a single-threaded processor (ST mode), even
    though the program takes longer to finish.  The discrepancy is
    accounted for as stolen time, which is also confusing, particularly
    when there are no other partitions running.
    
    This changes the accounting to use the timebase instead, meaning that
    the reported user and system times are the actual number of real-time
    seconds that the program was executing on the processor thread,
    regardless of which SMT mode the processor is in.  Thus a program will
    generally show greater user and system times when run on a
    multi-threaded processor than on a single-threaded processor.
    
    On pSeries systems on POWER5 or later processors, we measure the
    stolen time (time when this partition wasn't running) using the
    hypervisor dispatch trace log.  We check for new entries in the
    log on every entry from user mode and on every transition from
    kernel process context to soft or hard IRQ context (i.e. when
    account_system_vtime() gets called).  So that we can correctly
    distinguish time stolen from user time and time stolen from system
    time, without having to check the log on every exit to user mode,
    we store separate timestamps for exit to user mode and entry from
    user mode.
    
    On systems that have a SPURR (POWER6 and POWER7), we read the SPURR
    in account_system_vtime() (as before), and then apportion the SPURR
    ticks since the last time we read it between scaled user time and
    scaled system time according to the relative proportions of user
    time and system time over the same interval.  This avoids having to
    read the SPURR on every kernel entry and exit.  On systems that have
    PURR but not SPURR (i.e., POWER5), we do the same using the PURR
    rather than the SPURR.
    
    This disables the DTL user interface in /sys/debug/kernel/powerpc/dtl
    for now since it conflicts with the use of the dispatch trace log
    by the time accounting code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 37bc8ff16cac..84906d3fc860 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -517,7 +517,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 	account_system_vtime(current);
 	account_process_vtime(current);
-	calculate_steal_time();
 
 	/*
 	 * We can't take a PMU exception inside _switch() since there is a

commit e1f0ece113fe028593b6869fe191a991322c5d85
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Aug 10 20:02:05 2010 +0000

    powerpc: Move arch_sd_sibling_asym_packing() to smp.c
    
    Simple cleanup by moving arch_sd_sibling_asym_packing from process.c to
    smp.c to save an #ifdef CONFIG_SMP
    
    No functionality change.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b1c648a36b03..37bc8ff16cac 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1298,14 +1298,3 @@ unsigned long randomize_et_dyn(unsigned long base)
 
 	return ret;
 }
-
-#ifdef CONFIG_SMP
-int arch_sd_sibling_asym_packing(void)
-{
-	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
-		printk_once(KERN_INFO "Enabling Asymmetric SMT scheduling\n");
-		return SD_ASYM_PACKING;
-	}
-	return 0;
-}
-#endif

commit 4138d65333fa8961714441ed40229ea8cbeaf7e5
Author: Anton Blanchard <anton@samba.org>
Date:   Fri Aug 6 03:28:19 2010 +0000

    powerpc: Inline ppc64_runlatch_off
    
    I'm sick of seeing ppc64_runlatch_off in our profiles, so inline it
    into the callers. To avoid a mess of circular includes I didn't add
    it as an inline function.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 986fedf7e278..b1c648a36b03 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1199,19 +1199,17 @@ void ppc64_runlatch_on(void)
 	}
 }
 
-void ppc64_runlatch_off(void)
+void __ppc64_runlatch_off(void)
 {
 	unsigned long ctrl;
 
-	if (cpu_has_feature(CPU_FTR_CTRL) && test_thread_flag(TIF_RUNLATCH)) {
-		HMT_medium();
+	HMT_medium();
 
-		clear_thread_flag(TIF_RUNLATCH);
+	clear_thread_flag(TIF_RUNLATCH);
 
-		ctrl = mfspr(SPRN_CTRLF);
-		ctrl &= ~CTRL_RUNLATCH;
-		mtspr(SPRN_CTRLT, ctrl);
-	}
+	ctrl = mfspr(SPRN_CTRLF);
+	ctrl &= ~CTRL_RUNLATCH;
+	mtspr(SPRN_CTRLT, ctrl);
 }
 #endif
 

commit 9904b00593f548156962764f67b1bb23f4da56fc
Author: Denis Kirjanov <dkirjanov@kernel.org>
Date:   Thu Jul 29 22:04:39 2010 +0000

    powerpc: Use is_32bit_task() helper to test 32 bit binary
    
    Use is_32bit_task() helper to test 32 bit binary.
    
    Signed-off-by: Denis Kirjanov <dkirjanov@kernel.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 91356ffda2ca..986fedf7e278 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -728,7 +728,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		p->thread.regs = childregs;
 		if (clone_flags & CLONE_SETTLS) {
 #ifdef CONFIG_PPC64
-			if (!test_thread_flag(TIF_32BIT))
+			if (!is_32bit_task())
 				childregs->gpr[13] = childregs->gpr[6];
 			else
 #endif
@@ -823,7 +823,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	regs->nip = start;
 	regs->msr = MSR_USER;
 #else
-	if (!test_thread_flag(TIF_32BIT)) {
+	if (!is_32bit_task()) {
 		unsigned long entry, toc;
 
 		/* start is a relocated pointer to the function descriptor for
@@ -995,7 +995,7 @@ int sys_clone(unsigned long clone_flags, unsigned long usp,
 	if (usp == 0)
 		usp = regs->gpr[1];	/* stack pointer for child */
 #ifdef CONFIG_PPC64
-	if (test_thread_flag(TIF_32BIT)) {
+	if (is_32bit_task()) {
 		parent_tidp = TRUNC_PTR(parent_tidp);
 		child_tidp = TRUNC_PTR(child_tidp);
 	}

commit d7627467b7a8dd6944885290a03a07ceb28c10eb
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 17 23:52:56 2010 +0100

    Make do_execve() take a const filename pointer
    
    Make do_execve() take a const filename pointer so that kernel_execve() compiles
    correctly on ARM:
    
    arch/arm/kernel/sys_arm.c:88: warning: passing argument 1 of 'do_execve' discards qualifiers from pointer target type
    
    This also requires the argv and envp arguments to be consted twice, once for
    the pointer array and once for the strings the array points to.  This is
    because do_execve() passes a pointer to the filename (now const) to
    copy_strings_kernel().  A simpler alternative would be to cast the filename
    pointer in do_execve() when it's passed to copy_strings_kernel().
    
    do_execve() may not change any of the strings it is passed as part of the argv
    or envp lists as they are some of them in .rodata, so marking these strings as
    const should be fine.
    
    Further kernel_execve() and sys_execve() need to be changed to match.
    
    This has been test built on x86_64, frv, arm and mips.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Ralf Baechle <ralf@linux-mips.org>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index feacfb789686..91356ffda2ca 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1034,8 +1034,9 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	flush_fp_to_thread(current);
 	flush_altivec_to_thread(current);
 	flush_spe_to_thread(current);
-	error = do_execve(filename, (char __user * __user *) a1,
-			  (char __user * __user *) a2, regs);
+	error = do_execve(filename,
+			  (const char __user *const __user *) a1,
+			  (const char __user *const __user *) a2, regs);
 	putname(filename);
 out:
 	return error;

commit c7887325230aec47d47a32562a6e26014a0fafca
Author: David Howells <dhowells@redhat.com>
Date:   Wed Aug 11 11:26:22 2010 +0100

    Mark arguments to certain syscalls as being const
    
    Mark arguments to certain system calls as being const where they should be but
    aren't.  The list includes:
    
     (*) The filename arguments of various stat syscalls, execve(), various utimes
         syscalls and some mount syscalls.
    
     (*) The filename arguments of some syscall helpers relating to the above.
    
     (*) The buffer argument of various write syscalls.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e78a5add7f15..feacfb789686 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1027,7 +1027,7 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	int error;
 	char *filename;
 
-	filename = getname((char __user *) a0);
+	filename = getname((const char __user *) a0);
 	error = PTR_ERR(filename);
 	if (IS_ERR(filename))
 		goto out;

commit c4efd6b569b2646e1346a08a4c40286f8bcb5f11
Merge: 4aed2fd8e318 0bcfe7580794
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 09:39:22 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (27 commits)
      sched: Use correct macro to display sched_child_runs_first in /proc/sched_debug
      sched: No need for bootmem special cases
      sched: Revert nohz_ratelimit() for now
      sched: Reduce update_group_power() calls
      sched: Update rq->clock for nohz balanced cpus
      sched: Fix spelling of sibling
      sched, cpuset: Drop __cpuexit from cpu hotplug callbacks
      sched: Fix the racy usage of thread_group_cputimer() in fastpath_timer_check()
      sched: run_posix_cpu_timers: Don't check ->exit_state, use lock_task_sighand()
      sched: thread_group_cputime: Simplify, document the "alive" check
      sched: Remove the obsolete exit_state/signal hacks
      sched: task_tick_rt: Remove the obsolete ->signal != NULL check
      sched: __sched_setscheduler: Read the RLIMIT_RTPRIO value lockless
      sched: Fix comments to make them DocBook happy
      sched: Fix fix_small_capacity
      powerpc: Exclude arch_sd_sibiling_asym_packing() on UP
      powerpc: Enable asymmetric SMT scheduling on POWER7
      sched: Add asymmetric group packing option for sibling domain
      sched: Fix capacity calculations for SMT4
      sched: Change nohz idle load balancing logic to push model
      ...

commit dca45ad8af54963c005393a484ad117b8ba6150f
Merge: 68c38fc3cb4e cd5b8f8755a8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jul 21 21:45:02 2010 +0200

    Merge branch 'linus' into sched/core
    
    Merge reason: Move from the -rc3 to the almost-rc6 base.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a2e198116f97bb1cd5b37ff33a8cfdfb4010cf5b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jul 9 15:24:47 2010 +1000

    powerpc/book3e: Hack to get gdb moving along on Book3E 64-bit
    
    Our handling of debug interrupts on Book3E 64-bit is not quite
    the way it should be just yet. This is a workaround to let gdb
    work at least for now. We ensure that when context switching,
    we set the appropriate DBCR0 value for the new task. We also
    make sure that we turn off MSR[DE] within the kernel, and set
    it as part of the bits that get set when going back to userspace.
    
    In the long run, we will probably set the userspace DBCR0 on the
    exception exit code path and ensure we have some proper kernel
    value to set on the way into the kernel, a bit like ppc32 does,
    but that will take more work.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1e78453645be..551f6713ff42 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -477,6 +477,28 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	new_thread = &new->thread;
 	old_thread = &current->thread;
 
+#if defined(CONFIG_PPC_BOOK3E_64)
+	/* XXX Current Book3E code doesn't deal with kernel side DBCR0,
+	 * we always hold the user values, so we set it now.
+	 *
+	 * However, we ensure the kernel MSR:DE is appropriately cleared too
+	 * to avoid spurrious single step exceptions in the kernel.
+	 *
+	 * This will have to change to merge with the ppc32 code at some point,
+	 * but I don't like much what ppc32 is doing today so there's some
+	 * thinking needed there
+	 */
+	if ((new_thread->dbcr0 | old_thread->dbcr0) & DBCR0_IDM) {
+		u32 dbcr0;
+
+		mtmsr(mfmsr() & ~MSR_DE);
+		isync();
+		dbcr0 = mfspr(SPRN_DBCR0);
+		dbcr0 = (dbcr0 & DBCR0_EDM) | new_thread->dbcr0;
+		mtspr(SPRN_DBCR0, dbcr0);
+	}
+#endif /* CONFIG_PPC64_BOOK3E */
+
 #ifdef CONFIG_PPC64
 	/*
 	 * Collect processor utilization data per process

commit 5f07aa7524e98d6f68f2bec54f155ef6012e2c9a
Merge: e467e104bb74 d09ec7387184
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jul 9 11:25:48 2010 +1000

    Merge commit 'paulus-perf/master' into next

commit 2ec57d448b2e8fcfba539a46701b43f14f037f17
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Jun 29 12:02:01 2010 +1000

    sched: Fix spelling of sibling
    
    No logic changes, only spelling.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: linuxppc-dev@ozlabs.org
    Cc: David Howells <dhowells@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    LKML-Reference: <15249.1277776921@neuling.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9b41ece010b6..22f08cb7e7d1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1270,7 +1270,7 @@ unsigned long randomize_et_dyn(unsigned long base)
 }
 
 #ifdef CONFIG_SMP
-int arch_sd_sibiling_asym_packing(void)
+int arch_sd_sibling_asym_packing(void)
 {
 	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
 		printk_once(KERN_INFO "Enabling Asymmetric SMT scheduling\n");

commit 5aae8a53708025d4e718f0d2e7c2f766779ddc71
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Tue Jun 15 11:35:19 2010 +0530

    powerpc, hw_breakpoints: Implement hw_breakpoints for 64-bit server processors
    
    Implement perf-events based hw-breakpoint interfaces for PowerPC
    64-bit server (Book III S) processors.  This allows access to a
    given location to be used as an event that can be counted or
    profiled by the perf_events subsystem.
    
    This is done using the DABR (data breakpoint register), which can
    also be used for process debugging via ptrace.  When perf_event
    hw_breakpoint support is configured in, the perf_event subsystem
    manages the DABR and arbitrates access to it, and ptrace then
    creates a perf_event when it is requested to set a data breakpoint.
    
    [Adopted suggestions from Paul Mackerras <paulus@samba.org> to
    - emulate_step() all system-wide breakpoints and single-step only the
      per-task breakpoints
    - perform arch-specific cleanup before unregistration through
      arch_unregister_hw_breakpoint()
    ]
    
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9d255b4f0a0e..cbf3521d50b6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -37,6 +37,7 @@
 #include <linux/kernel_stat.h>
 #include <linux/personality.h>
 #include <linux/random.h>
+#include <linux/hw_breakpoint.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
@@ -462,8 +463,14 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	switch_booke_debug_regs(&new->thread);
 #else
+/*
+ * For PPC_BOOK3S_64, we use the hw-breakpoint interfaces that would
+ * schedule DABR
+ */
+#ifndef CONFIG_HAVE_HW_BREAKPOINT
 	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
 		set_dabr(new->thread.dabr);
+#endif /* CONFIG_HAVE_HW_BREAKPOINT */
 #endif
 
 
@@ -642,7 +649,11 @@ void flush_thread(void)
 {
 	discard_lazy_cpu_state();
 
+#ifdef CONFIG_HAVE_HW_BREAKPOINTS
+	flush_ptrace_hw_breakpoint(current);
+#else /* CONFIG_HAVE_HW_BREAKPOINTS */
 	set_debug_reg_defaults(&current->thread);
+#endif /* CONFIG_HAVE_HW_BREAKPOINTS */
 }
 
 void
@@ -660,6 +671,9 @@ void prepare_to_copy(struct task_struct *tsk)
 	flush_altivec_to_thread(current);
 	flush_vsx_to_thread(current);
 	flush_spe_to_thread(current);
+#ifdef CONFIG_HAVE_HW_BREAKPOINT
+	flush_ptrace_hw_breakpoint(tsk);
+#endif /* CONFIG_HAVE_HW_BREAKPOINT */
 }
 
 /*

commit f1ba9a5b2ab7d3f5a910d93371c4f22b636b7683
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 2 22:24:26 2010 +0000

    powerpc: Unconditionally enabled irq stacks
    
    Irq stacks provide an essential protection from stack overflows through
    external interrupts, at the cost of two additionals stacks per CPU.
    
    Enable them unconditionally to simplify the kernel build and prevent
    people from accidentally disabling them.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9d255b4f0a0e..773424df828a 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1005,7 +1005,6 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	return error;
 }
 
-#ifdef CONFIG_IRQSTACKS
 static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,
 				  unsigned long nbytes)
 {
@@ -1030,10 +1029,6 @@ static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,
 	return 0;
 }
 
-#else
-#define valid_irq_stack(sp, p, nb)	0
-#endif /* CONFIG_IRQSTACKS */
-
 int validate_sp(unsigned long sp, struct task_struct *p,
 		       unsigned long nbytes)
 {

commit 89275d59b572b92b1e2f6ddb63c49deecb801ff9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jun 9 16:20:55 2010 +0200

    powerpc: Exclude arch_sd_sibiling_asym_packing() on UP
    
    Only SMP systems care about load-balance features, plus this
    saves some .text space on UP and also fixes the build.
    
    Reported-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Michael Neuling <mikey@neuling.org>
    LKML-Reference: <tip-76cbd8a8f8b0dddbff89a6708bd5bd13c0d21a00@git.kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a3f347c635b2..9b41ece010b6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1269,6 +1269,7 @@ unsigned long randomize_et_dyn(unsigned long base)
 	return ret;
 }
 
+#ifdef CONFIG_SMP
 int arch_sd_sibiling_asym_packing(void)
 {
 	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
@@ -1277,3 +1278,4 @@ int arch_sd_sibiling_asym_packing(void)
 	}
 	return 0;
 }
+#endif

commit 76cbd8a8f8b0dddbff89a6708bd5bd13c0d21a00
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Jun 8 14:57:02 2010 +1000

    powerpc: Enable asymmetric SMT scheduling on POWER7
    
    The POWER7 core has dynamic SMT mode switching which is controlled by
    the hypervisor.  There are 3 SMT modes:
            SMT1 uses thread  0
            SMT2 uses threads 0 & 1
            SMT4 uses threads 0, 1, 2 & 3
    When in any particular SMT mode, all threads have the same performance
    as each other (ie. at any moment in time, all threads perform the same).
    
    The SMT mode switching works such that when linux has threads 2 & 3 idle
    and 0 & 1 active, it will cede (H_CEDE hypercall) threads 2 and 3 in the
    idle loop and the hypervisor will automatically switch to SMT2 for that
    core (independent of other cores).  The opposite is not true, so if
    threads 0 & 1 are idle and 2 & 3 are active, we will stay in SMT4 mode.
    
    Similarly if thread 0 is active and threads 1, 2 & 3 are idle, we'll go
    into SMT1 mode.
    
    If we can get the core into a lower SMT mode (SMT1 is best), the threads
    will perform better (since they share less core resources).  Hence when
    we have idle threads, we want them to be the higher ones.
    
    This adds a feature bit for asymmetric packing to powerpc and then
    enables it on POWER7.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: linuxppc-dev@ozlabs.org
    LKML-Reference: <20100608045702.31FB5CC8C7@localhost.localdomain>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9d255b4f0a0e..a3f347c635b2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1268,3 +1268,12 @@ unsigned long randomize_et_dyn(unsigned long base)
 
 	return ret;
 }
+
+int arch_sd_sibiling_asym_packing(void)
+{
+	if (cpu_has_feature(CPU_FTR_ASYM_SMT)) {
+		printk_once(KERN_INFO "Enabling Asymmetric SMT scheduling\n");
+		return SD_ASYM_PACKING;
+	}
+	return 0;
+}

commit 221c185d4e11b4061409da5d592779ced484614c
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Fri Mar 5 10:43:24 2010 +0000

    powerpc/476: Add isync after loading mmu and debug spr's
    
    476 requires an isync after loading MMU and debug related SPR's.  Some of
    these are in performance-critical paths and may need to be optimized, but
    initially, we're playing it safe.
    
    Signed-off-by: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e4d71ced97ef..9d255b4f0a0e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -371,6 +371,9 @@ int set_dabr(unsigned long dabr)
 	/* XXX should we have a CPU_FTR_HAS_DABR ? */
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	mtspr(SPRN_DAC1, dabr);
+#ifdef CONFIG_PPC_47x
+	isync();
+#endif
 #elif defined(CONFIG_PPC_BOOK3S)
 	mtspr(SPRN_DABR, dabr);
 #endif

commit 3bffb6529cf10d48a97ac0d6d789986894c25c37
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Mon Feb 8 11:51:18 2010 +0000

    powerpc/booke: Add support for advanced debug registers
    
    powerpc/booke: Add support for advanced debug registers
    
    From: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    
    Based on patches originally written by Torez Smith.
    
    This patch defines context switch and trap related functionality
    for BookE specific Debug Registers. It adds support to ptrace()
    for setting and getting BookE related Debug Registers
    
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Cc: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Gibson <dwg@au1.ibm.com>
    Cc: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Cc: Kumar Gala <galak@kernel.crashing.org>
    Cc: Sergio Durigan Junior <sergiodj@br.ibm.com>
    Cc: Thiago Jung Bauermann <bauerman@br.ibm.com>
    Cc: linuxppc-dev list <Linuxppc-dev@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 9be77e3936fb..e4d71ced97ef 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -245,6 +245,24 @@ void discard_lazy_cpu_state(void)
 }
 #endif /* CONFIG_SMP */
 
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
+void do_send_trap(struct pt_regs *regs, unsigned long address,
+		  unsigned long error_code, int signal_code, int breakpt)
+{
+	siginfo_t info;
+
+	if (notify_die(DIE_DABR_MATCH, "dabr_match", regs, error_code,
+			11, SIGSEGV) == NOTIFY_STOP)
+		return;
+
+	/* Deliver the signal to userspace */
+	info.si_signo = SIGTRAP;
+	info.si_errno = breakpt;	/* breakpoint or watchpoint id */
+	info.si_code = signal_code;
+	info.si_addr = (void __user *)address;
+	force_sig_info(SIGTRAP, &info, current);
+}
+#else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
 void do_dabr(struct pt_regs *regs, unsigned long address,
 		    unsigned long error_code)
 {
@@ -257,12 +275,6 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 	if (debugger_dabr_match(regs))
 		return;
 
-	/* Clear the DAC and struct entries.  One shot trigger */
-#ifdef CONFIG_PPC_ADV_DEBUG_REGS
-	mtspr(SPRN_DBCR0, mfspr(SPRN_DBCR0) & ~(DBSR_DAC1R | DBSR_DAC1W
-							| DBCR0_IDM));
-#endif
-
 	/* Clear the DABR */
 	set_dabr(0);
 
@@ -273,9 +285,82 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 	info.si_addr = (void __user *)address;
 	force_sig_info(SIGTRAP, &info, current);
 }
+#endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
 
 static DEFINE_PER_CPU(unsigned long, current_dabr);
 
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
+/*
+ * Set the debug registers back to their default "safe" values.
+ */
+static void set_debug_reg_defaults(struct thread_struct *thread)
+{
+	thread->iac1 = thread->iac2 = 0;
+#if CONFIG_PPC_ADV_DEBUG_IACS > 2
+	thread->iac3 = thread->iac4 = 0;
+#endif
+	thread->dac1 = thread->dac2 = 0;
+#if CONFIG_PPC_ADV_DEBUG_DVCS > 0
+	thread->dvc1 = thread->dvc2 = 0;
+#endif
+	thread->dbcr0 = 0;
+#ifdef CONFIG_BOOKE
+	/*
+	 * Force User/Supervisor bits to b11 (user-only MSR[PR]=1)
+	 */
+	thread->dbcr1 = DBCR1_IAC1US | DBCR1_IAC2US |	\
+			DBCR1_IAC3US | DBCR1_IAC4US;
+	/*
+	 * Force Data Address Compare User/Supervisor bits to be User-only
+	 * (0b11 MSR[PR]=1) and set all other bits in DBCR2 register to be 0.
+	 */
+	thread->dbcr2 = DBCR2_DAC1US | DBCR2_DAC2US;
+#else
+	thread->dbcr1 = 0;
+#endif
+}
+
+static void prime_debug_regs(struct thread_struct *thread)
+{
+	mtspr(SPRN_IAC1, thread->iac1);
+	mtspr(SPRN_IAC2, thread->iac2);
+#if CONFIG_PPC_ADV_DEBUG_IACS > 2
+	mtspr(SPRN_IAC3, thread->iac3);
+	mtspr(SPRN_IAC4, thread->iac4);
+#endif
+	mtspr(SPRN_DAC1, thread->dac1);
+	mtspr(SPRN_DAC2, thread->dac2);
+#if CONFIG_PPC_ADV_DEBUG_DVCS > 0
+	mtspr(SPRN_DVC1, thread->dvc1);
+	mtspr(SPRN_DVC2, thread->dvc2);
+#endif
+	mtspr(SPRN_DBCR0, thread->dbcr0);
+	mtspr(SPRN_DBCR1, thread->dbcr1);
+#ifdef CONFIG_BOOKE
+	mtspr(SPRN_DBCR2, thread->dbcr2);
+#endif
+}
+/*
+ * Unless neither the old or new thread are making use of the
+ * debug registers, set the debug registers from the values
+ * stored in the new thread.
+ */
+static void switch_booke_debug_regs(struct thread_struct *new_thread)
+{
+	if ((current->thread.dbcr0 & DBCR0_IDM)
+		|| (new_thread->dbcr0 & DBCR0_IDM))
+			prime_debug_regs(new_thread);
+}
+#else	/* !CONFIG_PPC_ADV_DEBUG_REGS */
+static void set_debug_reg_defaults(struct thread_struct *thread)
+{
+	if (thread->dabr) {
+		thread->dabr = 0;
+		set_dabr(0);
+	}
+}
+#endif	/* CONFIG_PPC_ADV_DEBUG_REGS */
+
 int set_dabr(unsigned long dabr)
 {
 	__get_cpu_var(current_dabr) = dabr;
@@ -372,9 +457,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_PPC_ADV_DEBUG_REGS
-	/* If new thread DAC (HW breakpoint) is the same then leave it */
-	if (new->thread.dabr)
-		set_dabr(new->thread.dabr);
+	switch_booke_debug_regs(&new->thread);
 #else
 	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
 		set_dabr(new->thread.dabr);
@@ -556,14 +639,7 @@ void flush_thread(void)
 {
 	discard_lazy_cpu_state();
 
-	if (current->thread.dabr) {
-		current->thread.dabr = 0;
-		set_dabr(0);
-
-#ifdef CONFIG_PPC_ADV_DEBUG_REGS
-		current->thread.dbcr0 &= ~(DBSR_DAC1R | DBSR_DAC1W);
-#endif
-	}
+	set_debug_reg_defaults(&current->thread);
 }
 
 void

commit 172ae2e7f8ff9053905a36672453a6d2ff95b182
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Mon Feb 8 11:50:57 2010 +0000

    powerpc/booke: Introduce new CONFIG options for advanced debug registers
    
    powerpc/booke: Introduce new CONFIG options for advanced debug registers
    
    From: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    
    Introduce new config options to simplify the ifdefs pertaining to the
    advanced debug registers for booke and 40x processors:
    
    CONFIG_PPC_ADV_DEBUG_REGS - boolean: true for dac-based processors
    CONFIG_PPC_ADV_DEBUG_IACS - number of IAC registers
    CONFIG_PPC_ADV_DEBUG_DACS - number of DAC registers
    CONFIG_PPC_ADV_DEBUG_DVCS - number of DVC registers
    CONFIG_PPC_ADV_DEBUG_DAC_RANGE - DAC ranges supported
    
    Beginning conservatively, since I only have the facilities to test 440
    hardware.  I believe all 40x and booke platforms support at least 2 IAC
    and 2 DAC registers.  For 440, 4 IAC and 2 DVC registers are enabled, as
    well as the DAC ranges.
    
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Acked-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7b816daf3eba..9be77e3936fb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -258,7 +258,7 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 		return;
 
 	/* Clear the DAC and struct entries.  One shot trigger */
-#if defined(CONFIG_BOOKE)
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	mtspr(SPRN_DBCR0, mfspr(SPRN_DBCR0) & ~(DBSR_DAC1R | DBSR_DAC1W
 							| DBCR0_IDM));
 #endif
@@ -284,7 +284,7 @@ int set_dabr(unsigned long dabr)
 		return ppc_md.set_dabr(dabr);
 
 	/* XXX should we have a CPU_FTR_HAS_DABR ? */
-#if defined(CONFIG_BOOKE)
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	mtspr(SPRN_DAC1, dabr);
 #elif defined(CONFIG_PPC_BOOK3S)
 	mtspr(SPRN_DABR, dabr);
@@ -371,7 +371,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 #endif /* CONFIG_SMP */
 
-#if defined(CONFIG_BOOKE)
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
 	/* If new thread DAC (HW breakpoint) is the same then leave it */
 	if (new->thread.dabr)
 		set_dabr(new->thread.dabr);
@@ -514,7 +514,7 @@ void show_regs(struct pt_regs * regs)
 	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
 	if (trap == 0x300 || trap == 0x600)
-#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
 		printk("DEAR: "REG", ESR: "REG"\n", regs->dar, regs->dsisr);
 #else
 		printk("DAR: "REG", DSISR: "REG"\n", regs->dar, regs->dsisr);
@@ -560,7 +560,7 @@ void flush_thread(void)
 		current->thread.dabr = 0;
 		set_dabr(0);
 
-#if defined(CONFIG_BOOKE)
+#ifdef CONFIG_PPC_ADV_DEBUG_REGS
 		current->thread.dbcr0 &= ~(DBSR_DAC1R | DBSR_DAC1W);
 #endif
 	}

commit 94f28da8409c6059135e89ac64a0839993124155
Author: Andreas Schwab <schwab@linux-m68k.org>
Date:   Sat Jan 30 10:20:59 2010 +0000

    powerpc: TIF_ABI_PENDING bit removal
    
    Here are the powerpc bits to remove TIF_ABI_PENDING now that
    set_personality() is called at the appropriate place in exec.
    
    Signed-off-by: Andreas Schwab <schwab@linux-m68k.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c930ac38e59f..7b816daf3eba 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -554,18 +554,6 @@ void exit_thread(void)
 
 void flush_thread(void)
 {
-#ifdef CONFIG_PPC64
-	struct thread_info *t = current_thread_info();
-
-	if (test_ti_thread_flag(t, TIF_ABI_PENDING)) {
-		clear_ti_thread_flag(t, TIF_ABI_PENDING);
-		if (test_ti_thread_flag(t, TIF_32BIT))
-			clear_ti_thread_flag(t, TIF_32BIT);
-		else
-			set_ti_thread_flag(t, TIF_32BIT);
-	}
-#endif
-
 	discard_lazy_cpu_state();
 
 	if (current->thread.dabr) {

commit ce7a35c73a308c62f9f0ca9f0821ebe0dc553008
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Fri Oct 16 07:05:17 2009 +0000

    powerpc: Fix compile errors found by new ppc64e_defconfig
    
    Fix the following 3 issues:
    
    arch/powerpc/kernel/process.c: In function 'arch_randomize_brk':
    arch/powerpc/kernel/process.c:1183: error: 'mmu_highuser_ssize' undeclared (first use in this function)
    arch/powerpc/kernel/process.c:1183: error: (Each undeclared identifier is reported only once
    arch/powerpc/kernel/process.c:1183: error: for each function it appears in.)
    arch/powerpc/kernel/process.c:1183: error: 'MMU_SEGSIZE_1T' undeclared (first use in this function)
    
    In file included from arch/powerpc/kernel/setup_64.c:60:
    arch/powerpc/include/asm/mmu-hash64.h:132: error: redefinition of 'struct mmu_psize_def'
    arch/powerpc/include/asm/mmu-hash64.h:159: error: expected identifier or '(' before numeric constant
    arch/powerpc/include/asm/mmu-hash64.h:396: error: conflicting types for 'mm_context_t'
    arch/powerpc/include/asm/mmu-book3e.h:184: error: previous declaration of 'mm_context_t' was here
    
    cc1: warnings being treated as errors
    arch/powerpc/kernel/pci_64.c: In function 'pcibios_unmap_io_space':
    arch/powerpc/kernel/pci_64.c:100: error: unused variable 'res'
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2ec1eaed19ca..c930ac38e59f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1172,7 +1172,7 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 	unsigned long base = mm->brk;
 	unsigned long ret;
 
-#ifdef CONFIG_PPC64
+#ifdef CONFIG_PPC_STD_MMU_64
 	/*
 	 * If we are using 1TB segments and we are allowed to randomise
 	 * the heap, we can put it above 1TB so it is backed by a 1TB

commit 9135c3cc5acf344eb28735681d8bebdb98a2c216
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Sep 15 08:20:15 2009 -0700

    powerpc/ftrace: show real return addresses in modules
    
    When the function graph tracer is enabled, it replaces the return address
    with a hook back to the tracer. This makes back traces see the hook instead
    of the actual return address.
    
    The current code also shows the real address by checking if the return
    address jumps to the return_to_handler. If it is, is also prints out
    the saved real return address.
    
    On powerpc64, some modules may return to mod_return_to_handler, which
    is not checked. This patch will also show the real address if a return
    is to mod_return_to_handler as well.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1168c5f440ab..2ec1eaed19ca 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1016,9 +1016,13 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	int curr_frame = current->curr_ret_stack;
 	extern void return_to_handler(void);
-	unsigned long addr = (unsigned long)return_to_handler;
+	unsigned long rth = (unsigned long)return_to_handler;
+	unsigned long mrth = -1;
 #ifdef CONFIG_PPC64
-	addr = *(unsigned long*)addr;
+	extern void mod_return_to_handler(void);
+	rth = *(unsigned long *)rth;
+	mrth = (unsigned long)mod_return_to_handler;
+	mrth = *(unsigned long *)mrth;
 #endif
 #endif
 
@@ -1044,7 +1048,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		if (!firstframe || ip != lr) {
 			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-			if (ip == addr && curr_frame >= 0) {
+			if ((ip == rth || ip == mrth) && curr_frame >= 0) {
 				printk(" (%pS)",
 				       (void *)current->ret_stack[curr_frame].ret);
 				curr_frame--;

commit 8bbde7a7062facf8af35bcc9a64cbafe8f36f3cf
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Sep 21 16:52:35 2009 +0000

    powerpc: Move 64bit heap above 1TB on machines with 1TB segments
    
    If we are using 1TB segments and we are allowed to randomise the heap, we can
    put it above 1TB so it is backed by a 1TB segment. Otherwise the heap will be
    in the bottom 1TB which always uses 256MB segments and this may result in a
    performance penalty.
    
    This functionality is disabled when heap randomisation is turned off:
    
    echo 1 > /proc/sys/kernel/randomize_va_space
    
    which may be useful when trying to allocate the maximum amount of 16M or 16G
    pages.
    
    On a microbenchmark that repeatedly touches 32GB of memory with a stride of
    256MB + 4kB (designed to stress 256MB segments while still mapping nicely into
    the L1 cache), we see the improvement:
    
    Force malloc to use heap all the time:
    # export MALLOC_MMAP_MAX_=0 MALLOC_TRIM_THRESHOLD_=-1
    
    Disable heap randomization:
    # echo 1 > /proc/sys/kernel/randomize_va_space
    # time ./test
    12.51s
    
    Enable heap randomization:
    # echo 2 > /proc/sys/kernel/randomize_va_space
    # time ./test
    1.70s
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0a3216433051..1168c5f440ab 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1165,7 +1165,22 @@ static inline unsigned long brk_rnd(void)
 
 unsigned long arch_randomize_brk(struct mm_struct *mm)
 {
-	unsigned long ret = PAGE_ALIGN(mm->brk + brk_rnd());
+	unsigned long base = mm->brk;
+	unsigned long ret;
+
+#ifdef CONFIG_PPC64
+	/*
+	 * If we are using 1TB segments and we are allowed to randomise
+	 * the heap, we can put it above 1TB so it is backed by a 1TB
+	 * segment. Otherwise the heap will be in the bottom 1TB
+	 * which always uses 256MB segments and this may result in a
+	 * performance penalty.
+	 */
+	if (!is_32bit_task() && (mmu_highuser_ssize == MMU_SEGSIZE_1T))
+		base = max_t(unsigned long, mm->brk, 1UL << SID_SHIFT_1T);
+#endif
+
+	ret = PAGE_ALIGN(base + brk_rnd());
 
 	if (ret < mm->brk)
 		return mm->brk;

commit c6c9eacef09a94b5866b83556196440aca876702
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Sep 8 14:16:58 2009 +0000

    powerpc/booke: Don't set DABR on 64-bit BookE, use DAC1 instead
    
    Also remove a duplicate setting of it in the context switch path
    on BookE.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 678ff132e8b0..0a3216433051 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -284,14 +284,13 @@ int set_dabr(unsigned long dabr)
 		return ppc_md.set_dabr(dabr);
 
 	/* XXX should we have a CPU_FTR_HAS_DABR ? */
-#if defined(CONFIG_PPC64) || defined(CONFIG_6xx)
-	mtspr(SPRN_DABR, dabr);
-#endif
-
 #if defined(CONFIG_BOOKE)
 	mtspr(SPRN_DAC1, dabr);
+#elif defined(CONFIG_PPC_BOOK3S)
+	mtspr(SPRN_DABR, dabr);
 #endif
 
+
 	return 0;
 }
 
@@ -372,15 +371,16 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 #endif /* CONFIG_SMP */
 
-	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
-		set_dabr(new->thread.dabr);
-
 #if defined(CONFIG_BOOKE)
 	/* If new thread DAC (HW breakpoint) is the same then leave it */
 	if (new->thread.dabr)
 		set_dabr(new->thread.dabr);
+#else
+	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
+		set_dabr(new->thread.dabr);
 #endif
 
+
 	new_thread = &new->thread;
 	old_thread = &current->thread;
 

commit 747bea91b764aefd59091ebff80f182282f1d23c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:27 2009 +0000

    powerpc: Clean ifdef usage in copy_thread()
    
    Currently, a single ifdef covers SLB related bits and more generic ppc64
    related bits, split this in two separate ifdef's since 64-bit BookE will
    need one but not the other.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 892a9f2e6d76..678ff132e8b0 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -664,6 +664,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 		sp_vsid |= SLB_VSID_KERNEL | llp;
 		p->thread.ksp_vsid = sp_vsid;
 	}
+#endif /* CONFIG_PPC_STD_MMU_64 */
 
 	/*
 	 * The PPC64 ABI makes use of a TOC to contain function 
@@ -671,6 +672,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	 * to the TOC entry.  The first entry is a pointer to the actual
 	 * function.
  	 */
+#ifdef CONFIG_PPC64
 	kregs->nip = *((unsigned long *)ret_from_fork);
 #else
 	kregs->nip = (unsigned long)ret_from_fork;

commit a2367194183d6ab6b05e5d7d9b40db6ba48afc06
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Jun 18 22:29:55 2009 +0000

    powerpc: Fix output from show_regs
    
    For some reason we've had an explicit KERN_INFO for GPR dumps.  With
    recent changes we get output like:
    
    <6>GPR00: 00000000 ef855eb0 ef858000 00000001 000000d0 f1000000 ffbc8000 ffffffff
    
    The KERN_INFO is causing the <6>.  Don't see any reason to keep it
    around.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 3e7135bbe40f..892a9f2e6d76 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -528,7 +528,7 @@ void show_regs(struct pt_regs * regs)
 
 	for (i = 0;  i < 32;  i++) {
 		if ((i % REGS_PER_LINE) == 0)
-			printk("\n" KERN_INFO "GPR%02d: ", i);
+			printk("\nGPR%02d: ", i);
 		printk(REG " ", regs->gpr[i]);
 		if (i == LAST_VOLATILE && !FULL_REGS(regs))
 			break;

commit 944916858a430a0627e483657d4cfa2cd2dfb4f7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jun 2 21:17:45 2009 +0000

    powerpc: Shield code specific to 64-bit server processors
    
    This is a random collection of added ifdef's around portions of
    code that only mak sense on server processors. Using either
    CONFIG_PPC_STD_MMU_64 or CONFIG_PPC_BOOK3S as seems appropriate.
    
    This is meant to make the future merging of Book3E 64-bit support
    easier.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7b44a33f03c2..3e7135bbe40f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -650,7 +650,7 @@ int copy_thread(unsigned long clone_flags, unsigned long usp,
 	p->thread.ksp_limit = (unsigned long)task_stack_page(p) +
 				_ALIGN_UP(sizeof(struct thread_info), 16);
 
-#ifdef CONFIG_PPC64
+#ifdef CONFIG_PPC_STD_MMU_64
 	if (cpu_has_feature(CPU_FTR_SLB)) {
 		unsigned long sp_vsid;
 		unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;

commit 6f2c55b843836d26528c56a0968689accaedbc67
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Thu Apr 2 16:56:59 2009 -0700

    Simplify copy_thread()
    
    First argument unused since 2.3.11.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index eac064948780..7b44a33f03c2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -598,7 +598,7 @@ void prepare_to_copy(struct task_struct *tsk)
 /*
  * Copy a thread..
  */
-int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
+int copy_thread(unsigned long clone_flags, unsigned long usp,
 		unsigned long unused, struct task_struct *p,
 		struct pt_regs *regs)
 {

commit 501cb16d3cfdcca99ac26fe122079f2a43b046b8
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Feb 22 01:50:07 2009 +0000

    powerpc: Randomise PIEs
    
    Randomise ELF_ET_DYN_BASE, which is used when loading position independent
    executables.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 30b149ce7598..eac064948780 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1170,3 +1170,13 @@ unsigned long arch_randomize_brk(struct mm_struct *mm)
 
 	return ret;
 }
+
+unsigned long randomize_et_dyn(unsigned long base)
+{
+	unsigned long ret = PAGE_ALIGN(base + brk_rnd());
+
+	if (ret < base)
+		return base;
+
+	return ret;
+}

commit 912f9ee21c836081e3c96dfe61025841ebeb95da
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Feb 22 01:50:04 2009 +0000

    powerpc: Randomise the brk region
    
    Randomize the heap.
    
    before:
    tundro2:~ # sleep 1 & cat /proc/${!}/maps | grep heap
    10017000-10118000 rw-p 10017000 00:00 0                                  [heap]
    10017000-10118000 rw-p 10017000 00:00 0                                  [heap]
    10017000-10118000 rw-p 10017000 00:00 0                                  [heap]
    10017000-10118000 rw-p 10017000 00:00 0                                  [heap]
    10017000-10118000 rw-p 10017000 00:00 0                                  [heap]
    
    after
    tundro2:~ # sleep 1 & cat /proc/${!}/maps | grep heap
    19419000-1951a000 rw-p 19419000 00:00 0                                  [heap]
    325ff000-32700000 rw-p 325ff000 00:00 0                                  [heap]
    1a97c000-1aa7d000 rw-p 1a97c000 00:00 0                                  [heap]
    1cc60000-1cd61000 rw-p 1cc60000 00:00 0                                  [heap]
    1afa9000-1b0aa000 rw-p 1afa9000 00:00 0                                  [heap]
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 69b9d2d3cb84..30b149ce7598 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1147,3 +1147,26 @@ unsigned long arch_align_stack(unsigned long sp)
 		sp -= get_random_int() & ~PAGE_MASK;
 	return sp & ~0xf;
 }
+
+static inline unsigned long brk_rnd(void)
+{
+        unsigned long rnd = 0;
+
+	/* 8MB for 32bit, 1GB for 64bit */
+	if (is_32bit_task())
+		rnd = (long)(get_random_int() % (1<<(23-PAGE_SHIFT)));
+	else
+		rnd = (long)(get_random_int() % (1<<(30-PAGE_SHIFT)));
+
+	return rnd << PAGE_SHIFT;
+}
+
+unsigned long arch_randomize_brk(struct mm_struct *mm)
+{
+	unsigned long ret = PAGE_ALIGN(mm->brk + brk_rnd());
+
+	if (ret < mm->brk)
+		return mm->brk;
+
+	return ret;
+}

commit d839088caec6891a5070f0b1ce61031e458533a9
Author: Anton Blanchard <anton@samba.org>
Date:   Sun Feb 22 01:50:03 2009 +0000

    powerpc: Randomise lower bits of stack address
    
    Randomise the lower bits of the stack address. More randomisation is good for
    security but the scatter can also help with SMT threads that share an L1. A
    quick test case shows this working:
    
    int main()
    {
            int sp;
            printf("%x\n", (unsigned long)&sp & 4095);
    }
    
    before:
    80
    80
    80
    80
    80
    
    after:
    610
    490
    300
    6b0
    d80
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8ede428e76c0..69b9d2d3cb84 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -35,6 +35,8 @@
 #include <linux/utsname.h>
 #include <linux/ftrace.h>
 #include <linux/kernel_stat.h>
+#include <linux/personality.h>
+#include <linux/random.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
@@ -1138,3 +1140,10 @@ void thread_info_cache_init(void)
 }
 
 #endif /* THREAD_SHIFT < PAGE_SHIFT */
+
+unsigned long arch_align_stack(unsigned long sp)
+{
+	if (!(current->personality & ADDR_NO_RANDOMIZE) && randomize_va_space)
+		sp -= get_random_int() & ~PAGE_MASK;
+	return sp & ~0xf;
+}

commit 6794c78243bfda020ab184d6d578944f8e90d26c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Feb 9 21:10:27 2009 -0800

    powerpc64: port of the function graph tracer
    
    This is a port of the function graph tracer that was written by
    Frederic Weisbecker for the x86.
    
    This only works for PPC64 at the moment and only for static tracing.
    PPC32 and dynamic function graph tracing support will come later.
    
    The trace produces a visual calling of functions:
    
     # tracer: function_graph
     #
     # CPU  DURATION                  FUNCTION CALLS
     # |     |   |                     |   |   |   |
      0)   2.224 us    |                        }
      0) ! 271.024 us  |                      }
      0) ! 320.080 us  |                    }
      0) ! 324.656 us  |                  }
      0) ! 329.136 us  |                }
      0)               |                .put_prev_task_fair() {
      0)               |                  .update_curr() {
      0)   2.240 us    |                    .update_min_vruntime();
      0)   6.512 us    |                  }
      0)   2.528 us    |                  .__enqueue_entity();
      0) + 15.536 us   |                }
      0)               |                .pick_next_task_fair() {
      0)   2.032 us    |                  .__pick_next_entity();
      0)   2.064 us    |                  .__clear_buddies();
      0)               |                  .set_next_entity() {
      0)   2.672 us    |                    .__dequeue_entity();
      0)   6.864 us    |                  }
    
    Geoff Lavand tested on PS3.
    
    Tested-by: Geoff Levand <geoffrey.levand@am.sony.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index fb7049c054c0..8ede428e76c0 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -33,6 +33,7 @@
 #include <linux/mqueue.h>
 #include <linux/hardirq.h>
 #include <linux/utsname.h>
+#include <linux/ftrace.h>
 #include <linux/kernel_stat.h>
 
 #include <asm/pgtable.h>
@@ -1008,6 +1009,14 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	unsigned long sp, ip, lr, newsp;
 	int count = 0;
 	int firstframe = 1;
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	int curr_frame = current->curr_ret_stack;
+	extern void return_to_handler(void);
+	unsigned long addr = (unsigned long)return_to_handler;
+#ifdef CONFIG_PPC64
+	addr = *(unsigned long*)addr;
+#endif
+#endif
 
 	sp = (unsigned long) stack;
 	if (tsk == NULL)
@@ -1030,6 +1039,13 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		ip = stack[STACK_FRAME_LR_SAVE];
 		if (!firstframe || ip != lr) {
 			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+			if (ip == addr && curr_frame >= 0) {
+				printk(" (%pS)",
+				       (void *)current->ret_stack[curr_frame].ret);
+				curr_frame--;
+			}
+#endif
 			if (firstframe)
 				printk(" (unreliable)");
 			printk("\n");

commit 79741dd35713ff4f6fd0eafd59fa94e8a4ba922d
Author: Martin Schwidefsky <schwidefsky@de.ibm.com>
Date:   Wed Dec 31 15:11:38 2008 +0100

    [PATCH] idle cputime accounting
    
    The cpu time spent by the idle process actually doing something is
    currently accounted as idle time. This is plain wrong, the architectures
    that support VIRT_CPU_ACCOUNTING=y can do better: distinguish between the
    time spent doing nothing and the time spent by idle doing work. The first
    is accounted with account_idle_time and the second with account_system_time.
    The architectures that use the account_xxx_time interface directly and not
    the account_xxx_ticks interface now need to do the check for the idle
    process in their arch code. In particular to improve the system vs true
    idle time accounting the arch code needs to measure the true idle time
    instead of just testing for the idle process.
    To improve the tick based accounting as well we would need an architecture
    primitive that can tell us if the pt_regs of the interrupted context
    points to the magic instruction that halts the cpu.
    
    In addition idle time is no more added to the stime of the idle process.
    This field now contains the system time of the idle process as it should
    be. On systems without VIRT_CPU_ACCOUNTING this will always be zero as
    every tick that occurs while idle is running will be accounted as idle
    time.
    
    This patch contains the necessary common code changes to be able to
    distinguish idle system time and true idle time. The architectures with
    support for VIRT_CPU_ACCOUNTING need some changes to exploit this.
    
    Signed-off-by: Martin Schwidefsky <schwidefsky@de.ibm.com>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 51b201ddf9a1..fb7049c054c0 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -33,6 +33,7 @@
 #include <linux/mqueue.h>
 #include <linux/hardirq.h>
 #include <linux/utsname.h>
+#include <linux/kernel_stat.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>

commit c4d04be11f99cc9ce4e3801a5da235727db704a9
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Thu Nov 20 03:24:07 2008 +0000

    powerpc: Allow the max stack trace depth to be configured
    
    On my screen, when something crashes, I only have space for maybe 16
    functions of the stack trace before the information above it scrolls
    off the screen.  It's easy to hack the kernel to print out only that
    much, but it's harder to remember to do it.  This introduces a config
    option for it so that I can keep the setting in my config.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b0383236dd07..51b201ddf9a1 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1000,7 +1000,7 @@ unsigned long get_wchan(struct task_struct *p)
 	return 0;
 }
 
-static int kstack_depth_to_print = 64;
+static int kstack_depth_to_print = CONFIG_PRINT_STACK_DEPTH;
 
 void show_stack(struct task_struct *tsk, unsigned long *stack)
 {

commit 1b98326b91eb9eea346945779e1f245aa66b17ce
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Nov 19 04:39:53 2008 +0000

    powerpc: Add MSR[CE, DE] to the MSR bits we print on show_regs()
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 957bded0020d..b0383236dd07 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -467,6 +467,8 @@ static struct regbit {
 	{MSR_VEC,	"VEC"},
 	{MSR_VSX,	"VSX"},
 	{MSR_ME,	"ME"},
+	{MSR_CE,	"CE"},
+	{MSR_DE,	"DE"},
 	{MSR_IR,	"IR"},
 	{MSR_DR,	"DR"},
 	{0,		NULL}

commit 9c4cb82515130c62224e23fdf7c13c8f6c59c614
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Sat Aug 2 02:44:11 2008 +1000

    powerpc: Remove use of CONFIG_PPC_MERGE
    
    Now that arch/ppc is gone and CONFIG_PPC_MERGE is always set, remove
    the dead code associated with !CONFIG_PPC_MERGE from arch/powerpc
    and include/asm-powerpc.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e030f3bd5024..957bded0020d 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -276,10 +276,8 @@ int set_dabr(unsigned long dabr)
 {
 	__get_cpu_var(current_dabr) = dabr;
 
-#ifdef CONFIG_PPC_MERGE		/* XXX for now */
 	if (ppc_md.set_dabr)
 		return ppc_md.set_dabr(dabr);
-#endif
 
 	/* XXX should we have a CPU_FTR_HAS_DABR ? */
 #if defined(CONFIG_PPC64) || defined(CONFIG_6xx)

commit 2325f0a0c3d76bb515f3312ab2b16afdbffcc594
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Sat Jul 26 05:27:33 2008 +1000

    powerpc/booke: Clean up the hardware watchpoint support
    
    * CONFIG_BOOKE is selected by CONFIG_44x so we dont need both
    * Fixed a few comments
    * Go back to only using DBCR0_IDM to determine if we are using
      debug resources.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index db2497ccc111..e030f3bd5024 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -254,7 +254,7 @@ void do_dabr(struct pt_regs *regs, unsigned long address,
 		return;
 
 	/* Clear the DAC and struct entries.  One shot trigger */
-#if (defined(CONFIG_44x) || defined(CONFIG_BOOKE))
+#if defined(CONFIG_BOOKE)
 	mtspr(SPRN_DBCR0, mfspr(SPRN_DBCR0) & ~(DBSR_DAC1R | DBSR_DAC1W
 							| DBCR0_IDM));
 #endif
@@ -286,7 +286,7 @@ int set_dabr(unsigned long dabr)
 	mtspr(SPRN_DABR, dabr);
 #endif
 
-#if defined(CONFIG_44x) || defined(CONFIG_BOOKE)
+#if defined(CONFIG_BOOKE)
 	mtspr(SPRN_DAC1, dabr);
 #endif
 
@@ -373,7 +373,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
 		set_dabr(new->thread.dabr);
 
-#if defined(CONFIG_44x) || defined(CONFIG_BOOKE)
+#if defined(CONFIG_BOOKE)
 	/* If new thread DAC (HW breakpoint) is the same then leave it */
 	if (new->thread.dabr)
 		set_dabr(new->thread.dabr);
@@ -568,7 +568,7 @@ void flush_thread(void)
 		current->thread.dabr = 0;
 		set_dabr(0);
 
-#if defined(CONFIG_44x) || defined(CONFIG_BOOKE)
+#if defined(CONFIG_BOOKE)
 		current->thread.dbcr0 &= ~(DBSR_DAC1R | DBSR_DAC1W);
 #endif
 	}

commit d6a61bfc06d6f2248f3e75f208d64e794082013c
Author: Luis Machado <luisgpm@linux.vnet.ibm.com>
Date:   Thu Jul 24 02:10:41 2008 +1000

    powerpc: BookE hardware watchpoint support
    
    This patch implements support for HW based watchpoint via the
    DBSR_DAC (Data Address Compare) facility of the BookE processors.
    
    It does so by interfacing with the existing DABR breakpoint code
    and adding the necessary bits and pieces for the new bits to
    be properly set or cleared
    
    Signed-off-by: Luis Machado <luisgpm@br.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 219f3634115e..db2497ccc111 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -47,6 +47,8 @@
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
 #endif
+#include <linux/kprobes.h>
+#include <linux/kdebug.h>
 
 extern unsigned long _get_SP(void);
 
@@ -239,6 +241,35 @@ void discard_lazy_cpu_state(void)
 }
 #endif /* CONFIG_SMP */
 
+void do_dabr(struct pt_regs *regs, unsigned long address,
+		    unsigned long error_code)
+{
+	siginfo_t info;
+
+	if (notify_die(DIE_DABR_MATCH, "dabr_match", regs, error_code,
+			11, SIGSEGV) == NOTIFY_STOP)
+		return;
+
+	if (debugger_dabr_match(regs))
+		return;
+
+	/* Clear the DAC and struct entries.  One shot trigger */
+#if (defined(CONFIG_44x) || defined(CONFIG_BOOKE))
+	mtspr(SPRN_DBCR0, mfspr(SPRN_DBCR0) & ~(DBSR_DAC1R | DBSR_DAC1W
+							| DBCR0_IDM));
+#endif
+
+	/* Clear the DABR */
+	set_dabr(0);
+
+	/* Deliver the signal to userspace */
+	info.si_signo = SIGTRAP;
+	info.si_errno = 0;
+	info.si_code = TRAP_HWBKPT;
+	info.si_addr = (void __user *)address;
+	force_sig_info(SIGTRAP, &info, current);
+}
+
 static DEFINE_PER_CPU(unsigned long, current_dabr);
 
 int set_dabr(unsigned long dabr)
@@ -254,6 +285,11 @@ int set_dabr(unsigned long dabr)
 #if defined(CONFIG_PPC64) || defined(CONFIG_6xx)
 	mtspr(SPRN_DABR, dabr);
 #endif
+
+#if defined(CONFIG_44x) || defined(CONFIG_BOOKE)
+	mtspr(SPRN_DAC1, dabr);
+#endif
+
 	return 0;
 }
 
@@ -337,6 +373,12 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
 		set_dabr(new->thread.dabr);
 
+#if defined(CONFIG_44x) || defined(CONFIG_BOOKE)
+	/* If new thread DAC (HW breakpoint) is the same then leave it */
+	if (new->thread.dabr)
+		set_dabr(new->thread.dabr);
+#endif
+
 	new_thread = &new->thread;
 	old_thread = &current->thread;
 
@@ -525,6 +567,10 @@ void flush_thread(void)
 	if (current->thread.dabr) {
 		current->thread.dabr = 0;
 		set_dabr(0);
+
+#if defined(CONFIG_44x) || defined(CONFIG_BOOKE)
+		current->thread.dbcr0 &= ~(DBSR_DAC1R | DBSR_DAC1W);
+#endif
 	}
 }
 

commit 7c29217096d83f657e6ee70479af09b46f4275f6
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Jul 11 16:29:12 2008 +1000

    powerpc: fix giveup_vsx to save registers correctly
    
    giveup_vsx didn't save the FPU and VMX regsiters.  Change it to be
    like giveup_fpr/altivec which save these registers.
    
    Also update call sites where FPU and VMX are already saved to use the
    original giveup_vsx (renamed to __giveup_vsx).
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0a4eb0811590..219f3634115e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -159,6 +159,13 @@ void enable_kernel_vsx(void)
 EXPORT_SYMBOL(enable_kernel_vsx);
 #endif
 
+void giveup_vsx(struct task_struct *tsk)
+{
+	giveup_fpu(tsk);
+	giveup_altivec(tsk);
+	__giveup_vsx(tsk);
+}
+
 void flush_vsx_to_thread(struct task_struct *tsk)
 {
 	if (tsk->thread.regs) {
@@ -290,7 +297,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #endif /* CONFIG_ALTIVEC */
 #ifdef CONFIG_VSX
 	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VSX))
-		giveup_vsx(prev);
+		/* VMX and FPU registers are already save here */
+		__giveup_vsx(prev);
 #endif /* CONFIG_VSX */
 #ifdef CONFIG_SPE
 	/*

commit 058c78f4ba89df7b2de82ac271452f09e2b8fa05
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jul 7 13:44:31 2008 +1000

    powerpc: Use new printk extension %pS to print symbols on oops
    
    This changes the oops and backtrace code to use the new %pS
    printk extension to print out symbols rather than manually
    calling print_symbol.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 85e557300d86..0a4eb0811590 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -485,10 +485,8 @@ void show_regs(struct pt_regs * regs)
 	 * Lookup NIP late so we have the best change of getting the
 	 * above info out without failing
 	 */
-	printk("NIP ["REG"] ", regs->nip);
-	print_symbol("%s\n", regs->nip);
-	printk("LR ["REG"] ", regs->link);
-	print_symbol("%s\n", regs->link);
+	printk("NIP ["REG"] %pS\n", regs->nip, (void *)regs->nip);
+	printk("LR ["REG"] %pS\n", regs->link, (void *)regs->link);
 #endif
 	show_stack(current, (unsigned long *) regs->gpr[1]);
 	if (!user_mode(regs))
@@ -976,8 +974,7 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		newsp = stack[0];
 		ip = stack[STACK_FRAME_LR_SAVE];
 		if (!firstframe || ip != lr) {
-			printk("["REG"] ["REG"] ", sp, ip);
-			print_symbol("%s", ip);
+			printk("["REG"] ["REG"] %pS", sp, ip, (void *)ip);
 			if (firstframe)
 				printk(" (unreliable)");
 			printk("\n");
@@ -992,10 +989,9 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		    && stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {
 			struct pt_regs *regs = (struct pt_regs *)
 				(sp + STACK_FRAME_OVERHEAD);
-			printk("--- Exception: %lx", regs->trap);
-			print_symbol(" at %s\n", regs->nip);
 			lr = regs->link;
-			print_symbol("    LR = %s\n", lr);
+			printk("--- Exception: %lx at %pS\n    LR = %pS\n",
+			       regs->trap, (void *)regs->nip, (void *)lr);
 			firstframe = 1;
 		}
 

commit 138fc1ee06e58f12fc2b755e435ce15bb36a0471
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jul 2 22:51:37 2008 +1000

    powerpc: Remove old dump_task_* functions
    
    Since Roland's ptrace cleanup starting with commit
    f65255e8d51ecbc6c9eef20d39e0377d19b658ca ("[POWERPC] Use user_regset
    accessors for FP regs"), the dump_task_* functions are no longer being
    used.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1924b57bd241..85e557300d86 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -105,29 +105,6 @@ void enable_kernel_fp(void)
 }
 EXPORT_SYMBOL(enable_kernel_fp);
 
-int dump_task_fpu(struct task_struct *tsk, elf_fpregset_t *fpregs)
-{
-#ifdef CONFIG_VSX
-	int i;
-	elf_fpreg_t *reg;
-#endif
-
-	if (!tsk->thread.regs)
-		return 0;
-	flush_fp_to_thread(current);
-
-#ifdef CONFIG_VSX
-	reg = (elf_fpreg_t *)fpregs;
-	for (i = 0; i < ELF_NFPREG - 1; i++, reg++)
-		*reg = tsk->thread.TS_FPR(i);
-	memcpy(reg, &tsk->thread.fpscr, sizeof(elf_fpreg_t));
-#else
-	memcpy(fpregs, &tsk->thread.TS_FPR(0), sizeof(*fpregs));
-#endif
-
-	return 1;
-}
-
 #ifdef CONFIG_ALTIVEC
 void enable_kernel_altivec(void)
 {
@@ -161,35 +138,6 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 		preempt_enable();
 	}
 }
-
-int dump_task_altivec(struct task_struct *tsk, elf_vrregset_t *vrregs)
-{
-	/* ELF_NVRREG includes the VSCR and VRSAVE which we need to save
-	 * separately, see below */
-	const int nregs = ELF_NVRREG - 2;
-	elf_vrreg_t *reg;
-	u32 *dest;
-
-	if (tsk == current)
-		flush_altivec_to_thread(tsk);
-
-	reg = (elf_vrreg_t *)vrregs;
-
-	/* copy the 32 vr registers */
-	memcpy(reg, &tsk->thread.vr[0], nregs * sizeof(*reg));
-	reg += nregs;
-
-	/* copy the vscr */
-	memcpy(reg, &tsk->thread.vscr, sizeof(*reg));
-	reg++;
-
-	/* vrsave is stored in the high 32bit slot of the final 128bits */
-	memset(reg, 0, sizeof(*reg));
-	dest = (u32 *)reg;
-	*dest = tsk->thread.vrsave;
-
-	return 1;
-}
 #endif /* CONFIG_ALTIVEC */
 
 #ifdef CONFIG_VSX
@@ -224,29 +172,6 @@ void flush_vsx_to_thread(struct task_struct *tsk)
 		preempt_enable();
 	}
 }
-
-/*
- * This dumps the lower half 64bits of the first 32 VSX registers.
- * This needs to be called with dump_task_fp and dump_task_altivec to
- * get all the VSX state.
- */
-int dump_task_vsx(struct task_struct *tsk, elf_vrreg_t *vrregs)
-{
-	elf_vrreg_t *reg;
-	double buf[32];
-	int i;
-
-	if (tsk == current)
-		flush_vsx_to_thread(tsk);
-
-	reg = (elf_vrreg_t *)vrregs;
-
-	for (i = 0; i < 32 ; i++)
-		buf[i] = current->thread.fpr[i][TS_VSRLOWOFFSET];
-	memcpy(reg, buf, sizeof(buf));
-
-	return 1;
-}
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_SPE
@@ -279,14 +204,6 @@ void flush_spe_to_thread(struct task_struct *tsk)
 		preempt_enable();
 	}
 }
-
-int dump_spe(struct pt_regs *regs, elf_vrregset_t *evrregs)
-{
-	flush_spe_to_thread(current);
-	/* We copy u32 evr[32] + u64 acc + u32 spefscr -> 35 */
-	memcpy(evrregs, &current->thread.evr[0], sizeof(u32) * 35);
-	return 1;
-}
 #endif /* CONFIG_SPE */
 
 #ifndef CONFIG_SMP

commit f3e909c2750eb20536bacacc867dc9047b70546a
Author: Michael Neuling <mikey@neuling.org>
Date:   Tue Jul 1 14:01:39 2008 +1000

    powerpc: Update for VSX core file and ptrace
    
    This correctly hooks the VSX dump into Roland McGrath core file
    infrastructure.  It adds the VSX dump information as an additional elf
    note in the core file (after talking more to the tool chain/gdb guys).
    This also ensures the formats are consistent between signals, ptrace
    and core files.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index d52ded366f14..1924b57bd241 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -162,7 +162,7 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 	}
 }
 
-int dump_task_altivec(struct task_struct *tsk, elf_vrreg_t *vrregs)
+int dump_task_altivec(struct task_struct *tsk, elf_vrregset_t *vrregs)
 {
 	/* ELF_NVRREG includes the VSCR and VRSAVE which we need to save
 	 * separately, see below */
@@ -249,23 +249,6 @@ int dump_task_vsx(struct task_struct *tsk, elf_vrreg_t *vrregs)
 }
 #endif /* CONFIG_VSX */
 
-int dump_task_vector(struct task_struct *tsk, elf_vrregset_t *vrregs)
-{
-	int rc = 0;
-	elf_vrreg_t *regs = (elf_vrreg_t *)vrregs;
-#ifdef CONFIG_ALTIVEC
-	rc = dump_task_altivec(tsk, regs);
-	if (rc)
-		return rc;
-	regs += ELF_NVRREG;
-#endif
-
-#ifdef CONFIG_VSX
-	rc = dump_task_vsx(tsk, regs);
-#endif
-	return rc;
-}
-
 #ifdef CONFIG_SPE
 
 void enable_kernel_spe(void)

commit ce48b2100785e5ca629fb3aa8e3b50aca808f692
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jun 25 14:07:18 2008 +1000

    powerpc: Add VSX context save/restore, ptrace and signal support
    
    This patch extends the floating point save and restore code to use the
    VSX load/stores when VSX is available.  This will make FP context
    save/restore marginally slower on FP only code, when VSX is available,
    as it has to load/store 128bits rather than just 64bits.
    
    Mixing FP, VMX and VSX code will get constant architected state.
    
    The signals interface is extended to enable access to VSR 0-31
    doubleword 1 after discussions with tool chain maintainers.  Backward
    compatibility is maintained.
    
    The ptrace interface is also extended to allow access to VSR 0-31 full
    registers.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 582df70439cb..d52ded366f14 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -53,6 +53,7 @@ extern unsigned long _get_SP(void);
 #ifndef CONFIG_SMP
 struct task_struct *last_task_used_math = NULL;
 struct task_struct *last_task_used_altivec = NULL;
+struct task_struct *last_task_used_vsx = NULL;
 struct task_struct *last_task_used_spe = NULL;
 #endif
 
@@ -106,11 +107,23 @@ EXPORT_SYMBOL(enable_kernel_fp);
 
 int dump_task_fpu(struct task_struct *tsk, elf_fpregset_t *fpregs)
 {
+#ifdef CONFIG_VSX
+	int i;
+	elf_fpreg_t *reg;
+#endif
+
 	if (!tsk->thread.regs)
 		return 0;
 	flush_fp_to_thread(current);
 
+#ifdef CONFIG_VSX
+	reg = (elf_fpreg_t *)fpregs;
+	for (i = 0; i < ELF_NFPREG - 1; i++, reg++)
+		*reg = tsk->thread.TS_FPR(i);
+	memcpy(reg, &tsk->thread.fpscr, sizeof(elf_fpreg_t));
+#else
 	memcpy(fpregs, &tsk->thread.TS_FPR(0), sizeof(*fpregs));
+#endif
 
 	return 1;
 }
@@ -149,7 +162,7 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 	}
 }
 
-int dump_task_altivec(struct task_struct *tsk, elf_vrregset_t *vrregs)
+int dump_task_altivec(struct task_struct *tsk, elf_vrreg_t *vrregs)
 {
 	/* ELF_NVRREG includes the VSCR and VRSAVE which we need to save
 	 * separately, see below */
@@ -179,6 +192,80 @@ int dump_task_altivec(struct task_struct *tsk, elf_vrregset_t *vrregs)
 }
 #endif /* CONFIG_ALTIVEC */
 
+#ifdef CONFIG_VSX
+#if 0
+/* not currently used, but some crazy RAID module might want to later */
+void enable_kernel_vsx(void)
+{
+	WARN_ON(preemptible());
+
+#ifdef CONFIG_SMP
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VSX))
+		giveup_vsx(current);
+	else
+		giveup_vsx(NULL);	/* just enable vsx for kernel - force */
+#else
+	giveup_vsx(last_task_used_vsx);
+#endif /* CONFIG_SMP */
+}
+EXPORT_SYMBOL(enable_kernel_vsx);
+#endif
+
+void flush_vsx_to_thread(struct task_struct *tsk)
+{
+	if (tsk->thread.regs) {
+		preempt_disable();
+		if (tsk->thread.regs->msr & MSR_VSX) {
+#ifdef CONFIG_SMP
+			BUG_ON(tsk != current);
+#endif
+			giveup_vsx(tsk);
+		}
+		preempt_enable();
+	}
+}
+
+/*
+ * This dumps the lower half 64bits of the first 32 VSX registers.
+ * This needs to be called with dump_task_fp and dump_task_altivec to
+ * get all the VSX state.
+ */
+int dump_task_vsx(struct task_struct *tsk, elf_vrreg_t *vrregs)
+{
+	elf_vrreg_t *reg;
+	double buf[32];
+	int i;
+
+	if (tsk == current)
+		flush_vsx_to_thread(tsk);
+
+	reg = (elf_vrreg_t *)vrregs;
+
+	for (i = 0; i < 32 ; i++)
+		buf[i] = current->thread.fpr[i][TS_VSRLOWOFFSET];
+	memcpy(reg, buf, sizeof(buf));
+
+	return 1;
+}
+#endif /* CONFIG_VSX */
+
+int dump_task_vector(struct task_struct *tsk, elf_vrregset_t *vrregs)
+{
+	int rc = 0;
+	elf_vrreg_t *regs = (elf_vrreg_t *)vrregs;
+#ifdef CONFIG_ALTIVEC
+	rc = dump_task_altivec(tsk, regs);
+	if (rc)
+		return rc;
+	regs += ELF_NVRREG;
+#endif
+
+#ifdef CONFIG_VSX
+	rc = dump_task_vsx(tsk, regs);
+#endif
+	return rc;
+}
+
 #ifdef CONFIG_SPE
 
 void enable_kernel_spe(void)
@@ -233,6 +320,10 @@ void discard_lazy_cpu_state(void)
 	if (last_task_used_altivec == current)
 		last_task_used_altivec = NULL;
 #endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_VSX
+	if (last_task_used_vsx == current)
+		last_task_used_vsx = NULL;
+#endif /* CONFIG_VSX */
 #ifdef CONFIG_SPE
 	if (last_task_used_spe == current)
 		last_task_used_spe = NULL;
@@ -297,6 +388,10 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))
 		giveup_altivec(prev);
 #endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_VSX
+	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VSX))
+		giveup_vsx(prev);
+#endif /* CONFIG_VSX */
 #ifdef CONFIG_SPE
 	/*
 	 * If the previous thread used spe in the last quantum
@@ -317,6 +412,10 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	if (new->thread.regs && last_task_used_altivec == new)
 		new->thread.regs->msr |= MSR_VEC;
 #endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_VSX
+	if (new->thread.regs && last_task_used_vsx == new)
+		new->thread.regs->msr |= MSR_VSX;
+#endif /* CONFIG_VSX */
 #ifdef CONFIG_SPE
 	/* Avoid the trap.  On smp this this never happens since
 	 * we don't set last_task_used_spe
@@ -417,6 +516,8 @@ static struct regbit {
 	{MSR_EE,	"EE"},
 	{MSR_PR,	"PR"},
 	{MSR_FP,	"FP"},
+	{MSR_VEC,	"VEC"},
+	{MSR_VSX,	"VSX"},
 	{MSR_ME,	"ME"},
 	{MSR_IR,	"IR"},
 	{MSR_DR,	"DR"},
@@ -534,6 +635,7 @@ void prepare_to_copy(struct task_struct *tsk)
 {
 	flush_fp_to_thread(current);
 	flush_altivec_to_thread(current);
+	flush_vsx_to_thread(current);
 	flush_spe_to_thread(current);
 }
 
@@ -689,6 +791,9 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 #endif
 
 	discard_lazy_cpu_state();
+#ifdef CONFIG_VSX
+	current->thread.used_vsr = 0;
+#endif
 	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
 	current->thread.fpscr.val = 0;
 #ifdef CONFIG_ALTIVEC

commit 9c75a31c3525a127f70b919856e32be3d8b03755
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Jun 26 17:07:48 2008 +1000

    powerpc: Add macros to access floating point registers in thread_struct.
    
    We are going to change where the floating point registers are stored
    in the thread_struct, so in preparation add some macros to access the
    floating point registers.  Update all code to use these new macros.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7de41c3948ec..582df70439cb 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -110,7 +110,7 @@ int dump_task_fpu(struct task_struct *tsk, elf_fpregset_t *fpregs)
 		return 0;
 	flush_fp_to_thread(current);
 
-	memcpy(fpregs, &tsk->thread.fpr[0], sizeof(*fpregs));
+	memcpy(fpregs, &tsk->thread.TS_FPR(0), sizeof(*fpregs));
 
 	return 1;
 }

commit 85218827cc4ca900867807f19345418164ffc108
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Mon Apr 28 16:21:22 2008 +1000

    [POWERPC] Add IRQSTACKS support on ppc32
    
    This makes it possible to use separate stacks for hard and soft IRQs
    on 32-bit powerpc as well as on 64-bit.  The code for 32-bit is just
    the 32-bit analog of the 64-bit code.
    
    * Added allocation and initialization of the irq stacks.  We limit the
      stacks to be in lowmem for ppc32.
    * Implemented ppc32 versions of call_do_softirq() and call_handle_irq()
      to switch the stack pointers
    * Reworked how we do stack overflow detection.  We now keep around the
      limit of the stack in the thread_struct and compare against the limit
      to see if we've overflowed.  We can now use this on ppc64 if desired.
    
    [ paulus@samba.org: Fixed bug on 6xx where we need to reload r9 with the
      thread_info pointer. ]
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6caad17ea72e..7de41c3948ec 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -589,6 +589,8 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 	kregs = (struct pt_regs *) sp;
 	sp -= STACK_FRAME_OVERHEAD;
 	p->thread.ksp = sp;
+	p->thread.ksp_limit = (unsigned long)task_stack_page(p) +
+				_ALIGN_UP(sizeof(struct thread_info), 16);
 
 #ifdef CONFIG_PPC64
 	if (cpu_has_feature(CPU_FTR_SLB)) {

commit f6a616800e68b61807d0f7bb0d5dc70665ef8046
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Apr 18 16:56:17 2008 +1000

    [POWERPC] Fix kernel stack allocation alignment
    
    The powerpc kernel stacks need to be naturally aligned, as they
    contain the thread info at the bottom, which is obtained by
    clearing the low bits of the stack pointer.
    
    However, when using 64K pages, the stack is smaller than a page,
    so we use kmalloc to allocate it, but that doesn't provide the
    alignment guarantee we need.
    
    It appeared to work so far... until one enables SLUB debugging
    which then returns unaligned pointers.  Ooops...
    
    This fixes it by using a slab cache with enforced alignment.  It
    relies on my previous patch that adds a thread_info_cache_init()
    callback.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 703100d5e458..6caad17ea72e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1033,3 +1033,34 @@ void ppc64_runlatch_off(void)
 	}
 }
 #endif
+
+#if THREAD_SHIFT < PAGE_SHIFT
+
+static struct kmem_cache *thread_info_cache;
+
+struct thread_info *alloc_thread_info(struct task_struct *tsk)
+{
+	struct thread_info *ti;
+
+	ti = kmem_cache_alloc(thread_info_cache, GFP_KERNEL);
+	if (unlikely(ti == NULL))
+		return NULL;
+#ifdef CONFIG_DEBUG_STACK_USAGE
+	memset(ti, 0, THREAD_SIZE);
+#endif
+	return ti;
+}
+
+void free_thread_info(struct thread_info *ti)
+{
+	kmem_cache_free(thread_info_cache, ti);
+}
+
+void thread_info_cache_init(void)
+{
+	thread_info_cache = kmem_cache_create("thread_info", THREAD_SIZE,
+					      THREAD_SIZE, 0, NULL);
+	BUG_ON(thread_info_cache == NULL);
+}
+
+#endif /* THREAD_SHIFT < PAGE_SHIFT */

commit ec2b36b9f23cfbbe94d89724b796b44fd57d5221
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Apr 17 14:34:59 2008 +1000

    [POWERPC] Move stackframe definitions to common header
    
    This moves various definitions used all over the place to parse stack
    frames to ptrace.h so only one definition is needed.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a27910207c7e..703100d5e458 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -914,20 +914,6 @@ int validate_sp(unsigned long sp, struct task_struct *p,
 	return valid_irq_stack(sp, p, nbytes);
 }
 
-#ifdef CONFIG_PPC64
-#define MIN_STACK_FRAME	112	/* same as STACK_FRAME_OVERHEAD, in fact */
-#define FRAME_LR_SAVE	2
-#define INT_FRAME_SIZE	(sizeof(struct pt_regs) + STACK_FRAME_OVERHEAD + 288)
-#define REGS_MARKER	0x7265677368657265ul
-#define FRAME_MARKER	12
-#else
-#define MIN_STACK_FRAME	16
-#define FRAME_LR_SAVE	1
-#define INT_FRAME_SIZE	(sizeof(struct pt_regs) + STACK_FRAME_OVERHEAD)
-#define REGS_MARKER	0x72656773ul
-#define FRAME_MARKER	2
-#endif
-
 EXPORT_SYMBOL(validate_sp);
 
 unsigned long get_wchan(struct task_struct *p)
@@ -939,15 +925,15 @@ unsigned long get_wchan(struct task_struct *p)
 		return 0;
 
 	sp = p->thread.ksp;
-	if (!validate_sp(sp, p, MIN_STACK_FRAME))
+	if (!validate_sp(sp, p, STACK_FRAME_OVERHEAD))
 		return 0;
 
 	do {
 		sp = *(unsigned long *)sp;
-		if (!validate_sp(sp, p, MIN_STACK_FRAME))
+		if (!validate_sp(sp, p, STACK_FRAME_OVERHEAD))
 			return 0;
 		if (count > 0) {
-			ip = ((unsigned long *)sp)[FRAME_LR_SAVE];
+			ip = ((unsigned long *)sp)[STACK_FRAME_LR_SAVE];
 			if (!in_sched_functions(ip))
 				return ip;
 		}
@@ -976,12 +962,12 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 	lr = 0;
 	printk("Call Trace:\n");
 	do {
-		if (!validate_sp(sp, tsk, MIN_STACK_FRAME))
+		if (!validate_sp(sp, tsk, STACK_FRAME_OVERHEAD))
 			return;
 
 		stack = (unsigned long *) sp;
 		newsp = stack[0];
-		ip = stack[FRAME_LR_SAVE];
+		ip = stack[STACK_FRAME_LR_SAVE];
 		if (!firstframe || ip != lr) {
 			printk("["REG"] ["REG"] ", sp, ip);
 			print_symbol("%s", ip);
@@ -995,8 +981,8 @@ void show_stack(struct task_struct *tsk, unsigned long *stack)
 		 * See if this is an exception frame.
 		 * We look for the "regshere" marker in the current frame.
 		 */
-		if (validate_sp(sp, tsk, INT_FRAME_SIZE)
-		    && stack[FRAME_MARKER] == REGS_MARKER) {
+		if (validate_sp(sp, tsk, STACK_INT_FRAME_SIZE)
+		    && stack[STACK_FRAME_MARKER] == STACK_FRAME_REGS_MARKER) {
 			struct pt_regs *regs = (struct pt_regs *)
 				(sp + STACK_FRAME_OVERHEAD);
 			printk("--- Exception: %lx", regs->trap);

commit ac7c5353b189e10cf5dd27399f64f7b013abffc6
Merge: a8f75ea70c58 120dd64cacd4
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Apr 14 21:11:02 2008 +1000

    Merge branch 'linux-2.6'

commit a2ceff5e555e664751bc653a4d9b133efa18c742
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Mar 28 19:11:48 2008 +1100

    [POWERPC] Fix missed hardware breakpoints across multiple threads
    
    There is a bug in the powerpc DABR (data access breakpoint) handling,
    which can result in us missing breakpoints if several threads are trying
    to break on the same address.
    
    The circumstances are that do_page_fault() calls do_dabr(), this clears
    the DABR (sets it to 0) and sets up the signal which will report to
    userspace that the DABR was hit. The do_signal() code will restore the DABR
    value on the way out to userspace.
    
    If we reschedule before calling do_signal(), __switch_to() will check the
    cached DABR value and compare it to the new thread's value, if they match
    we don't set the DABR in hardware.
    
    So if two threads have the same DABR value, and we schedule from one to
    the other after taking the interrupt for the first thread hitting the DABR,
    the second thread will run without the DABR set in hardware.
    
    The cleanest fix is to move the cache update into set_dabr(), that way we
    can't forget to do it.
    
    Reported-by: Jan Kratochvil <jan.kratochvil@redhat.com>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 59311ec0d422..4ec605521504 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -241,8 +241,12 @@ void discard_lazy_cpu_state(void)
 }
 #endif /* CONFIG_SMP */
 
+static DEFINE_PER_CPU(unsigned long, current_dabr);
+
 int set_dabr(unsigned long dabr)
 {
+	__get_cpu_var(current_dabr) = dabr;
+
 #ifdef CONFIG_PPC_MERGE		/* XXX for now */
 	if (ppc_md.set_dabr)
 		return ppc_md.set_dabr(dabr);
@@ -259,8 +263,6 @@ int set_dabr(unsigned long dabr)
 DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);
 #endif
 
-static DEFINE_PER_CPU(unsigned long, current_dabr);
-
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {
@@ -325,10 +327,8 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 #endif /* CONFIG_SMP */
 
-	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr)) {
+	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr))
 		set_dabr(new->thread.dabr);
-		__get_cpu_var(current_dabr) = new->thread.dabr;
-	}
 
 	new_thread = &new->thread;
 	old_thread = &current->thread;

commit 54f53f2b94feb72622bec7a8563fc487d9f97720
Merge: f61fb8a52cdf a4083c9271e0
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Mar 26 08:44:18 2008 +1100

    Merge branch 'linux-2.6'

commit 71e91a0abb839f8d2372236d8fe0513c295ec717
Author: Roland McGrath <roland@redhat.com>
Date:   Mon Mar 17 16:01:20 2008 +1100

    [POWERPC] Don't touch PT_DTRACE in exec
    
    The PT_DTRACE flag is meaningless and obsolete.
    Don't touch it.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4846bf543a8c..7c8e3da23810 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -862,11 +862,6 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	flush_spe_to_thread(current);
 	error = do_execve(filename, (char __user * __user *) a1,
 			  (char __user * __user *) a2, regs);
-	if (error == 0) {
-		task_lock(current);
-		current->ptrace &= ~PT_DTRACE;
-		task_unlock(current);
-	}
 	putname(filename);
 out:
 	return error;

commit 44387e9ff25267c78a99229aca55ed750e9174c7
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Mar 17 15:27:09 2008 +1100

    [POWERPC] Fix PMU + soft interrupt disable bug
    
    Since the PMU is an NMI now, it can come at any time we are only soft
    disabled.  We must hard disable around the two places we allow the kernel
    stack SLB and r1 to go out of sync.  Otherwise the PMU exception can
    force a kernel stack SLB into another slot, which can lead to it
    getting evicted, which can lead to a nasty unrecoverable SLB miss
    in the exception entry code.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 4846bf543a8c..59311ec0d422 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -353,6 +353,12 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	account_process_vtime(current);
 	calculate_steal_time();
 
+	/*
+	 * We can't take a PMU exception inside _switch() since there is a
+	 * window where the kernel stack SLB and the kernel stack are out
+	 * of sync. Hard disable here.
+	 */
+	hard_irq_disable();
 	last = _switch(old_thread, new_thread);
 
 	local_irq_restore(flags);

commit 79ccd1bedc0592602183dad5e3d51d0ab7a9add0
Author: Hugh Dickins <hugh@veritas.com>
Date:   Sat Feb 9 05:25:13 2008 +1100

    [POWERPC] Fix DEBUG_PREEMPT warning when warning
    
    The powerpc show_regs prints CPU using smp_processor_id: change that to
    raw_smp_processor_id, so that when it's showing a WARN_ON backtrace without
    preemption disabled, DEBUG_PREEMPT doesn't mess up that warning with its own.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b9d88374f14f..4846bf543a8c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -462,7 +462,7 @@ void show_regs(struct pt_regs * regs)
 	       current, task_pid_nr(current), current->comm, task_thread_info(current));
 
 #ifdef CONFIG_SMP
-	printk(" CPU: %d", smp_processor_id());
+	printk(" CPU: %d", raw_smp_processor_id());
 #endif /* CONFIG_SMP */
 
 	for (i = 0;  i < 32;  i++) {

commit 81a3843f97cff5fef7b6006fcd2d015d3c4b569f
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Tue Dec 4 16:51:44 2007 +1100

    [POWERPC] Fix hardware IRQ time accounting problem.
    
    The commit fa13a5a1f25f671d084d8884be96fc48d9b68275 (sched: restore
    deterministic CPU accounting on powerpc), unconditionally calls
    update_process_tick() in system context.  In the deterministic
    accounting case this is the correct thing to do.  However, in the
    non-deterministic accounting case we need to not do this, since doing
    this results in the time accounted as hardware irq time being
    artificially elevated.
    
    Also this collapses 2 consecutive '#ifdef CONFIG_VIRT_CPU_ACCOUNTING'
    checks in time.h into one for neatness.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 41e13f4cc6e3..b9d88374f14f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -350,7 +350,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	local_irq_save(flags);
 
 	account_system_vtime(current);
-	account_process_tick(current, 0);
+	account_process_vtime(current);
 	calculate_steal_time();
 
 	last = _switch(old_thread, new_thread);

commit fa13a5a1f25f671d084d8884be96fc48d9b68275
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Nov 9 22:39:38 2007 +0100

    sched: restore deterministic CPU accounting on powerpc
    
    Since powerpc started using CONFIG_GENERIC_CLOCKEVENTS, the
    deterministic CPU accounting (CONFIG_VIRT_CPU_ACCOUNTING) has been
    broken on powerpc, because we end up counting user time twice: once in
    timer_interrupt() and once in update_process_times().
    
    This fixes the problem by pulling the code in update_process_times
    that updates utime and stime into a separate function called
    account_process_tick.  If CONFIG_VIRT_CPU_ACCOUNTING is not defined,
    there is a version of account_process_tick in kernel/timer.c that
    simply accounts a whole tick to either utime or stime as before.  If
    CONFIG_VIRT_CPU_ACCOUNTING is defined, then arch code gets to
    implement account_process_tick.
    
    This also lets us simplify the s390 code a bit; it means that the s390
    timer interrupt can now call update_process_times even when
    CONFIG_VIRT_CPU_ACCOUNTING is turned on, and can just implement a
    suitable account_process_tick().
    
    account_process_tick() now takes the task_struct * as an argument.
    Tested both with and without CONFIG_VIRT_CPU_ACCOUNTING.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index b9d88374f14f..41e13f4cc6e3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -350,7 +350,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	local_irq_save(flags);
 
 	account_system_vtime(current);
-	account_process_vtime(current);
+	account_process_tick(current, 0);
 	calculate_steal_time();
 
 	last = _switch(old_thread, new_thread);

commit 19c5870c0eefd27c6d09d867465e0571262e05d0
Author: Alexey Dobriyan <adobriyan@openvz.org>
Date:   Thu Oct 18 23:40:41 2007 -0700

    Use helpers to obtain task pid in printks (arch code)
    
    One of the easiest things to isolate is the pid printed in kernel log.
    There was a patch, that made this for arch-independent code, this one makes
    so for arch/xxx files.
    
    It took some time to cross-compile it, but hopefully these are all the
    printks in arch code.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@openvz.org>
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ea6ad7a2a7e3..b9d88374f14f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -459,7 +459,7 @@ void show_regs(struct pt_regs * regs)
 		printk("DAR: "REG", DSISR: "REG"\n", regs->dar, regs->dsisr);
 #endif
 	printk("TASK = %p[%d] '%s' THREAD: %p",
-	       current, current->pid, current->comm, task_thread_info(current));
+	       current, task_pid_nr(current), current->comm, task_thread_info(current));
 
 #ifdef CONFIG_SMP
 	printk(" CPU: %d", smp_processor_id());

commit 1f7d6668c29b1dfa307a44844f9bb38356fc989b
Author: Mark Nelson <markn@au1.ibm.com>
Date:   Tue Oct 16 23:25:40 2007 -0700

    powerpc: add Altivec/VMX state to coredumps
    
    Update dump_task_altivec() (which has so far never been put to use) so that
    it dumps the Altivec/VMX registers (VR[0] - VR[31], VSCR and VRSAVE) in the
    same format as the ptrace get_vrregs(), and add the appropriate glue
    typedef and #defines to make it work.
    
    A new note type of NT_PPC_VMX was chosen to be 0x100 (arbitrarily) because
    it allows the low range values to be used for more generic purposes and
    0x100 seems an adequate starting point for PowerPC extensions.
    
    Signed-off-by: Mark Nelson <markn@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Andi Kleen <ak@suse.de>
    Cc: <linux-arch@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7949c203cb89..ea6ad7a2a7e3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -149,10 +149,32 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 	}
 }
 
-int dump_task_altivec(struct pt_regs *regs, elf_vrregset_t *vrregs)
+int dump_task_altivec(struct task_struct *tsk, elf_vrregset_t *vrregs)
 {
-	flush_altivec_to_thread(current);
-	memcpy(vrregs, &current->thread.vr[0], sizeof(*vrregs));
+	/* ELF_NVRREG includes the VSCR and VRSAVE which we need to save
+	 * separately, see below */
+	const int nregs = ELF_NVRREG - 2;
+	elf_vrreg_t *reg;
+	u32 *dest;
+
+	if (tsk == current)
+		flush_altivec_to_thread(tsk);
+
+	reg = (elf_vrreg_t *)vrregs;
+
+	/* copy the 32 vr registers */
+	memcpy(reg, &tsk->thread.vr[0], nregs * sizeof(*reg));
+	reg += nregs;
+
+	/* copy the vscr */
+	memcpy(reg, &tsk->thread.vscr, sizeof(*reg));
+	reg++;
+
+	/* vrsave is stored in the high 32bit slot of the final 128bits */
+	memset(reg, 0, sizeof(*reg));
+	dest = (u32 *)reg;
+	*dest = tsk->thread.vrsave;
+
 	return 1;
 }
 #endif /* CONFIG_ALTIVEC */

commit 1189be6508d45183013ddb82b18f4934193de274
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 11 20:37:10 2007 +1000

    [POWERPC] Use 1TB segments
    
    This makes the kernel use 1TB segments for all kernel mappings and for
    user addresses of 1TB and above, on machines which support them
    (currently POWER5+, POWER6 and PA6T).
    
    We detect that the machine supports 1TB segments by looking at the
    ibm,processor-segment-sizes property in the device tree.
    
    We don't currently use 1TB segments for user addresses < 1T, since
    that would effectively prevent 32-bit processes from using huge pages
    unless we also had a way to revert to using 256MB segments.  That
    would be possible but would involve extra complications (such as
    keeping track of which segment size was used when HPTEs were inserted)
    and is not addressed here.
    
    Parts of this patch were originally written by Ben Herrenschmidt.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 15998b57767c..7949c203cb89 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -564,10 +564,15 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 
 #ifdef CONFIG_PPC64
 	if (cpu_has_feature(CPU_FTR_SLB)) {
-		unsigned long sp_vsid = get_kernel_vsid(sp);
+		unsigned long sp_vsid;
 		unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
 
-		sp_vsid <<= SLB_VSID_SHIFT;
+		if (cpu_has_feature(CPU_FTR_1T_SEGMENT))
+			sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_1T)
+				<< SLB_VSID_SHIFT_1T;
+		else
+			sp_vsid = get_kernel_vsid(sp, MMU_SEGSIZE_256M)
+				<< SLB_VSID_SHIFT;
 		sp_vsid |= SLB_VSID_KERNEL | llp;
 		p->thread.ksp_vsid = sp_vsid;
 	}

commit 70f227d8846a8a9b1f36f71c42e11cc7c6e9408d
Merge: a0c7ce9c877c f778089cb244
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 3 15:33:17 2007 +1000

    Merge branch 'linux-2.6' into for-2.6.24

commit 0de2d820067e03ca93f6bf5320d362d5262fb7a3
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Sep 28 04:38:55 2007 +1000

    [POWERPC] Make instruction dumping work in real mode
    
    On non-book-E, exceptions execute in real mode.  If a fault happens
    that leads to a register dump, the kernel currently prints XXXXXXXX
    because it doesn't realize that PC is a physical address.
    
    This patch checks whether instruction address translation is turned
    on, and if not converts PC into a virtual address.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 57c589c34147..588c0cb8115e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -354,6 +354,14 @@ static void show_instructions(struct pt_regs *regs)
 		if (!(i % 8))
 			printk("\n");
 
+#if !defined(CONFIG_BOOKE)
+		/* If executing with the IMMU off, adjust pc rather
+		 * than print XXXXXXXX.
+		 */
+		if (!(regs->msr & MSR_IR))
+			pc = (unsigned long)phys_to_virt(pc);
+#endif
+
 		/* We use __get_user here *only* to avoid an OOPS on a
 		 * bad address because the pc *should* only be a
 		 * kernel address.

commit 474f81967217f3bec2389ae913da72641f2c40e3
Author: Roland McGrath <roland@redhat.com>
Date:   Mon Sep 24 16:52:44 2007 -0700

    [POWERPC] Ensure FULL_REGS on exec
    
    When PTRACE_O_TRACEEXEC is used, a ptrace call to fetch the registers at
    the PTRACE_EVENT_EXEC stop (PTRACE_PEEKUSR) will oops in CHECK_FULL_REGS.
    With recent versions, "gdb --args /bin/sh -c 'exec /bin/true'" and "run" at
    the (gdb) prompt is sufficient to produce this.  I also have written an
    isolated test case, see https://bugzilla.redhat.com/show_bug.cgi?id=301791#c15.
    
    This change fixes the problem by clearing the low bit of pt_regs.trap in
    start_thread so that FULL_REGS is true again.  This is correct since all of
    the GPRs that "full" refers to are cleared in start_thread.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e477c9d0498b..8a1b001d0b11 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -605,6 +605,13 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	regs->ccr = 0;
 	regs->gpr[1] = sp;
 
+	/*
+	 * We have just cleared all the nonvolatile GPRs, so make
+	 * FULL_REGS(regs) return true.  This is necessary to allow
+	 * ptrace to examine the thread immediately after exec.
+	 */
+	regs->trap &= ~1UL;
+
 #ifdef CONFIG_PPC32
 	regs->mq = 0;
 	regs->nip = start;

commit 5e14d21e3f28a4181dacff0336040e30942f4921
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Sep 13 01:44:20 2007 -0500

    [POWERPC] Add cpu feature for SPE handling
    
    Make it so that SPE support can be determined at runtime.  This is similiar
    to how we handle AltiVec.  This allows us to have SPE support built in and
    work on processors with and without SPE.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e477c9d0498b..57c589c34147 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -669,9 +669,13 @@ int set_fpexc_mode(struct task_struct *tsk, unsigned int val)
 	 * mode (asyn, precise, disabled) for 'Classic' FP. */
 	if (val & PR_FP_EXC_SW_ENABLE) {
 #ifdef CONFIG_SPE
-		tsk->thread.fpexc_mode = val &
-			(PR_FP_EXC_SW_ENABLE | PR_FP_ALL_EXCEPT);
-		return 0;
+		if (cpu_has_feature(CPU_FTR_SPE)) {
+			tsk->thread.fpexc_mode = val &
+				(PR_FP_EXC_SW_ENABLE | PR_FP_ALL_EXCEPT);
+			return 0;
+		} else {
+			return -EINVAL;
+		}
 #else
 		return -EINVAL;
 #endif
@@ -697,7 +701,10 @@ int get_fpexc_mode(struct task_struct *tsk, unsigned long adr)
 
 	if (tsk->thread.fpexc_mode & PR_FP_EXC_SW_ENABLE)
 #ifdef CONFIG_SPE
-		val = tsk->thread.fpexc_mode;
+		if (cpu_has_feature(CPU_FTR_SPE))
+			val = tsk->thread.fpexc_mode;
+		else
+			return -EINVAL;
 #else
 		return -EINVAL;
 #endif

commit 0ee6c15e7ba7b36a217cdadb292eeaf32a057a59
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Tue Aug 28 21:15:53 2007 -0500

    [POWERPC] Flush registers to proper task context
    
    When we flush register state for FP, Altivec, or SPE in flush_*_to_thread
    we need to respect the task_struct that the caller has passed to us.
    
    Most cases we are called with current, however sometimes (ptrace) we may
    be passed a different task_struct.
    
    This showed up when using gdbserver debugging a simple program that used
    floating point. When gdb tried to show the FP regs they all showed up as
    0, because the child's FP registers were never properly flushed to memory.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a83727b308a0..e477c9d0498b 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -83,7 +83,7 @@ void flush_fp_to_thread(struct task_struct *tsk)
 			 */
 			BUG_ON(tsk != current);
 #endif
-			giveup_fpu(current);
+			giveup_fpu(tsk);
 		}
 		preempt_enable();
 	}
@@ -143,7 +143,7 @@ void flush_altivec_to_thread(struct task_struct *tsk)
 #ifdef CONFIG_SMP
 			BUG_ON(tsk != current);
 #endif
-			giveup_altivec(current);
+			giveup_altivec(tsk);
 		}
 		preempt_enable();
 	}
@@ -182,7 +182,7 @@ void flush_spe_to_thread(struct task_struct *tsk)
 #ifdef CONFIG_SMP
 			BUG_ON(tsk != current);
 #endif
-			giveup_spe(current);
+			giveup_spe(tsk);
 		}
 		preempt_enable();
 	}

commit 141707892e92dca69b7b8af65b9367da2d1f8120
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Jul 26 00:46:15 2007 -0500

    [POWERPC] Fix register labels on show_regs() message for 4xx/Book-E
    
    In a show_regs()  message The DEAR and ESR were reported as
    DAR and DSISR which only exist on classic parts.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 84f000a45e36..a83727b308a0 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -423,7 +423,11 @@ void show_regs(struct pt_regs * regs)
 	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
 	if (trap == 0x300 || trap == 0x600)
+#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
+		printk("DEAR: "REG", ESR: "REG"\n", regs->dar, regs->dsisr);
+#else
 		printk("DAR: "REG", DSISR: "REG"\n", regs->dar, regs->dsisr);
+#endif
 	printk("TASK = %p[%d] '%s' THREAD: %p",
 	       current, current->pid, current->comm, task_thread_info(current));
 

commit 791cc501d422be96d6e3098faf6471ba29f4dd33
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jun 4 15:15:48 2007 +1000

    [POWERPC] Always apply DABR changes on context switches
    
    This patch removes the #ifdef CONFIG_PPC64 around setting the DABR.
    
    The actual setting of the SPR inside of the set_dabr() function is dependent
    on CONFIG_PPC64 || CONFIG_6xx but you can always provide a ppc_md hook to
    override that.  We should improve support for different HW breakpoints
    facilities but this is a first step.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 6e2f03566b0d..84f000a45e36 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -219,22 +219,26 @@ void discard_lazy_cpu_state(void)
 }
 #endif /* CONFIG_SMP */
 
-#ifdef CONFIG_PPC_MERGE		/* XXX for now */
 int set_dabr(unsigned long dabr)
 {
+#ifdef CONFIG_PPC_MERGE		/* XXX for now */
 	if (ppc_md.set_dabr)
 		return ppc_md.set_dabr(dabr);
+#endif
 
+	/* XXX should we have a CPU_FTR_HAS_DABR ? */
+#if defined(CONFIG_PPC64) || defined(CONFIG_6xx)
 	mtspr(SPRN_DABR, dabr);
+#endif
 	return 0;
 }
-#endif
 
 #ifdef CONFIG_PPC64
 DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);
-static DEFINE_PER_CPU(unsigned long, current_dabr);
 #endif
 
+static DEFINE_PER_CPU(unsigned long, current_dabr);
+
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
 {
@@ -299,12 +303,10 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 #endif /* CONFIG_SMP */
 
-#ifdef CONFIG_PPC64	/* for now */
 	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr)) {
 		set_dabr(new->thread.dabr);
 		__get_cpu_var(current_dabr) = new->thread.dabr;
 	}
-#endif /* CONFIG_PPC64 */
 
 	new_thread = &new->thread;
 	old_thread = &current->thread;
@@ -473,12 +475,10 @@ void flush_thread(void)
 
 	discard_lazy_cpu_state();
 
-#ifdef CONFIG_PPC64	/* for now */
 	if (current->thread.dabr) {
 		current->thread.dabr = 0;
 		set_dabr(0);
 	}
-#endif
 }
 
 void

commit e63340ae6b6205fef26b40a75673d1c9c0c8bb90
Author: Randy Dunlap <randy.dunlap@oracle.com>
Date:   Tue May 8 00:28:08 2007 -0700

    header cleaning: don't include smp_lock.h when not used
    
    Remove includes of <linux/smp_lock.h> where it is not used/needed.
    Suggested by Al Viro.
    
    Builds cleanly on x86_64, i386, alpha, ia64, powerpc, sparc,
    sparc64, and arm (all 59 defconfigs).
    
    Signed-off-by: Randy Dunlap <randy.dunlap@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e509aae2feb3..6e2f03566b0d 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -19,7 +19,6 @@
 #include <linux/kernel.h>
 #include <linux/mm.h>
 #include <linux/smp.h>
-#include <linux/smp_lock.h>
 #include <linux/stddef.h>
 #include <linux/unistd.h>
 #include <linux/ptrace.h>

commit a741e67969577163a4cfc78d7fd2753219087ef1
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Apr 10 17:09:37 2007 +1000

    [POWERPC] Make tlb flush batch use lazy MMU mode
    
    The current tlb flush code on powerpc 64 bits has a subtle race since we
    lost the page table lock due to the possible faulting in of new PTEs
    after a previous one has been removed but before the corresponding hash
    entry has been evicted, which can leads to all sort of fatal problems.
    
    This patch reworks the batch code completely. It doesn't use the mmu_gather
    stuff anymore. Instead, we use the lazy mmu hooks that were added by the
    paravirt code. They have the nice property that the enter/leave lazy mmu
    mode pair is always fully contained by the PTE lock for a given range
    of PTEs. Thus we can guarantee that all batches are flushed on a given
    CPU before it drops that lock.
    
    We also generalize batching for any PTE update that require a flush.
    
    Batching is now enabled on a CPU by arch_enter_lazy_mmu_mode() and
    disabled by arch_leave_lazy_mmu_mode(). The code epects that this is
    always contained within a PTE lock section so no preemption can happen
    and no PTE insertion in that range from another CPU. When batching
    is enabled on a CPU, every PTE updates that need a hash flush will
    use the batch for that flush.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 949092dccf44..e509aae2feb3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -305,9 +305,7 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		set_dabr(new->thread.dabr);
 		__get_cpu_var(current_dabr) = new->thread.dabr;
 	}
-
-	flush_tlb_pending();
-#endif
+#endif /* CONFIG_PPC64 */
 
 	new_thread = &new->thread;
 	old_thread = &current->thread;

commit e049d1ca3094f3d1d94617f456a9961202f96e3a
Merge: edfac96a92b8 80584ff3b99c
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Apr 13 03:50:03 2007 +1000

    Merge branch 'linux-2.6' into for-2.6.22

commit f6f7dde3f778b318aca71220834482d4ea2d7738
Author: anton@samba.org <anton@samba.org>
Date:   Tue Mar 20 20:38:19 2007 -0500

    [POWERPC] Use lowercase for hex printouts in oops messages.
    
    Use lowercase for hex printouts in oops messages. The number of times I have
    tried to copy and paste from an oops into an objdump search...
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Acked-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 0df049215503..d7d7602e348f 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -402,11 +402,11 @@ static void printbits(unsigned long val, struct regbit *bits)
 }
 
 #ifdef CONFIG_PPC64
-#define REG		"%016lX"
+#define REG		"%016lx"
 #define REGS_PER_LINE	4
 #define LAST_VOLATILE	13
 #else
-#define REG		"%08lX"
+#define REG		"%08lx"
 #define REGS_PER_LINE	8
 #define LAST_VOLATILE	12
 #endif
@@ -421,7 +421,7 @@ void show_regs(struct pt_regs * regs)
 	       regs, regs->trap, print_tainted(), init_utsname()->release);
 	printk("MSR: "REG" ", regs->msr);
 	printbits(regs->msr, msr_bits);
-	printk("  CR: %08lX  XER: %08lX\n", regs->ccr, regs->xer);
+	printk("  CR: %08lx  XER: %08lx\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
 	if (trap == 0x300 || trap == 0x600)
 		printk("DAR: "REG", DSISR: "REG"\n", regs->dar, regs->dsisr);

commit 4002aca771a2aa2848e94a98cf51a2cae4e77ae0
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Mar 20 10:08:33 2007 -0500

    [POWERPC] Remove last_syscall
    
    Remove last_syscall from 32bit powerpc, its been gone in 64bit for years.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 972b2acbe713..0df049215503 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -567,7 +567,6 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 	kregs->nip = *((unsigned long *)ret_from_fork);
 #else
 	kregs->nip = (unsigned long)ret_from_fork;
-	p->thread.last_syscall = -1;
 #endif
 
 	return 0;

commit f144e7c7272bf527c380bffaa5e789dc28a09d8d
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Sat Mar 10 03:23:03 2007 -0500

    [POWERPC] Fix atomicity of TIF update in flush_thread()
    
    Fix atomicity of TIF update in flush_thread() for powerpc
    
    Fixes it correctly with *_ti_thread_flag.
    
    Race :
    
    parent process executing :
    sys_ptrace()
     (lock_kernel())
     (ptrace_get_task_struct(pid))
     arch_ptrace()
       ptrace_detach()
         ptrace_disable(child);
           clear_singlestep(child);
             clear_tsk_thread_flag(child, TIF_SINGLESTEP);
             (which clears the TIF_SINGLESTEP flag atomically from a different
              process)
     (put_task_struct(child))
     (unlock_kernel())
    
    And at the same time, in the child process :
    sys_execve()
     do_execve()
       search_binary_handler()
         load_elf_binary()
           flush_old_exec()
             flush_thread()
               doing a non-atomic thread flag update
    
    Applies on 2.6.20.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f3d4dd580dd6..e53b2988d1bf 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -465,8 +465,13 @@ void flush_thread(void)
 #ifdef CONFIG_PPC64
 	struct thread_info *t = current_thread_info();
 
-	if (t->flags & _TIF_ABI_PENDING)
-		t->flags ^= (_TIF_ABI_PENDING | _TIF_32BIT);
+	if (test_ti_thread_flag(t, TIF_ABI_PENDING)) {
+		clear_ti_thread_flag(t, TIF_ABI_PENDING);
+		if (test_ti_thread_flag(t, TIF_32BIT))
+			clear_ti_thread_flag(t, TIF_32BIT);
+		else
+			set_ti_thread_flag(t, TIF_32BIT);
+	}
 #endif
 
 	discard_lazy_cpu_state();

commit bb72c481e970dc1b4034ddccbe8302ff39e0d948
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Feb 19 11:42:42 2007 +1100

    [POWERPC] Harden validate_sp against stack corruption
    
    If something has overflowed or corrupted the stack and causes an oops,
    and we try to print a stack trace, that will call validate_sp, which
    can itself cause an oops if the cpu field of the thread_info struct at
    the bottom of the stack has been corrupted (if CONFIG_IRQSTACKS is
    set).  This makes debugging harder.
    
    To avoid the second oops, this adds a check to make sure that the cpu
    number is reasonable before using it to check whether the stack is on
    the softirq or hardirq stack.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f3d4dd580dd6..972b2acbe713 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -818,6 +818,35 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	return error;
 }
 
+#ifdef CONFIG_IRQSTACKS
+static inline int valid_irq_stack(unsigned long sp, struct task_struct *p,
+				  unsigned long nbytes)
+{
+	unsigned long stack_page;
+	unsigned long cpu = task_cpu(p);
+
+	/*
+	 * Avoid crashing if the stack has overflowed and corrupted
+	 * task_cpu(p), which is in the thread_info struct.
+	 */
+	if (cpu < NR_CPUS && cpu_possible(cpu)) {
+		stack_page = (unsigned long) hardirq_ctx[cpu];
+		if (sp >= stack_page + sizeof(struct thread_struct)
+		    && sp <= stack_page + THREAD_SIZE - nbytes)
+			return 1;
+
+		stack_page = (unsigned long) softirq_ctx[cpu];
+		if (sp >= stack_page + sizeof(struct thread_struct)
+		    && sp <= stack_page + THREAD_SIZE - nbytes)
+			return 1;
+	}
+	return 0;
+}
+
+#else
+#define valid_irq_stack(sp, p, nb)	0
+#endif /* CONFIG_IRQSTACKS */
+
 int validate_sp(unsigned long sp, struct task_struct *p,
 		       unsigned long nbytes)
 {
@@ -827,19 +856,7 @@ int validate_sp(unsigned long sp, struct task_struct *p,
 	    && sp <= stack_page + THREAD_SIZE - nbytes)
 		return 1;
 
-#ifdef CONFIG_IRQSTACKS
-	stack_page = (unsigned long) hardirq_ctx[task_cpu(p)];
-	if (sp >= stack_page + sizeof(struct thread_struct)
-	    && sp <= stack_page + THREAD_SIZE - nbytes)
-		return 1;
-
-	stack_page = (unsigned long) softirq_ctx[task_cpu(p)];
-	if (sp >= stack_page + sizeof(struct thread_struct)
-	    && sp <= stack_page + THREAD_SIZE - nbytes)
-		return 1;
-#endif
-
-	return 0;
+	return valid_irq_stack(sp, p, nbytes);
 }
 
 #ifdef CONFIG_PPC64

commit 00ae36de49cc718d4122e1c8aac96fd1a5a2553c
Author: Anton Blanchard <anton@samba.org>
Date:   Fri Oct 13 12:17:16 2006 +1000

    [POWERPC] Better check in show_instructions
    
    Instead of just checking that an address is in the right range, use the
    provided __kernel_text_address() helper which covers both the kernel and
    module text sections.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 7b2f6452ba72..f3d4dd580dd6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -341,13 +341,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 
 static int instructions_to_print = 16;
 
-#ifdef CONFIG_PPC64
-#define BAD_PC(pc)	((REGION_ID(pc) != KERNEL_REGION_ID) && \
-		         (REGION_ID(pc) != VMALLOC_REGION_ID))
-#else
-#define BAD_PC(pc)	((pc) < KERNELBASE)
-#endif
-
 static void show_instructions(struct pt_regs *regs)
 {
 	int i;
@@ -366,7 +359,8 @@ static void show_instructions(struct pt_regs *regs)
 		 * bad address because the pc *should* only be a
 		 * kernel address.
 		 */
-		if (BAD_PC(pc) || __get_user(instr, (unsigned int __user *)pc)) {
+		if (!__kernel_text_address(pc) ||
+		     __get_user(instr, (unsigned int __user *)pc)) {
 			printk("XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)

commit 96b644bdec977b97a45133e5b4466ba47a7a5e65
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Oct 2 02:18:13 2006 -0700

    [PATCH] namespaces: utsname: use init_utsname when appropriate
    
    In some places, particularly drivers and __init code, the init utsns is the
    appropriate one to use.  This patch replaces those with a the init_utsname
    helper.
    
    Changes: Removed several uses of init_utsname().  Hope I picked all the
            right ones in net/ipv4/ipconfig.c.  These are now changed to
            utsname() (the per-process namespace utsname) in the previous
            patch (2/7)
    
    [akpm@osdl.org: CIFS fix]
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Cc: Kirill Korotaev <dev@openvz.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Andrey Savochkin <saw@sw.ru>
    Cc: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a127a1e3c097..7b2f6452ba72 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -424,7 +424,7 @@ void show_regs(struct pt_regs * regs)
 	printk("NIP: "REG" LR: "REG" CTR: "REG"\n",
 	       regs->nip, regs->link, regs->ctr);
 	printk("REGS: %p TRAP: %04lx   %s  (%s)\n",
-	       regs, regs->trap, print_tainted(), system_utsname.release);
+	       regs, regs->trap, print_tainted(), init_utsname()->release);
 	printk("MSR: "REG" ", regs->msr);
 	printbits(regs->msr, msr_bits);
 	printk("  CR: %08lX  XER: %08lX\n", regs->ccr, regs->xer);

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jrn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jrn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e4732459c485..a127a1e3c097 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -14,7 +14,6 @@
  *  2 of the License, or (at your option) any later version.
  */
 
-#include <linux/config.h>
 #include <linux/errno.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>

commit e9370ae15dc2f8ba1e1889ce26f13cda565b6ecb
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 7 16:15:39 2006 +1000

    [PATCH] powerpc: Implement PR_[GS]ET_UNALIGN prctls for powerpc
    
    This gives the ability to control whether alignment exceptions get
    fixed up or reported to the process as a SIGBUS, using the existing
    PR_SET_UNALIGN and PR_GET_UNALIGN prctls.  We do not implement the
    option of logging a message on alignment exceptions.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2d35d83961b2..e4732459c485 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -752,6 +752,17 @@ int get_endian(struct task_struct *tsk, unsigned long adr)
 	return put_user(val, (unsigned int __user *)adr);
 }
 
+int set_unalign_ctl(struct task_struct *tsk, unsigned int val)
+{
+	tsk->thread.align_ctl = val;
+	return 0;
+}
+
+int get_unalign_ctl(struct task_struct *tsk, unsigned long adr)
+{
+	return put_user(tsk->thread.align_ctl, (unsigned int __user *)adr);
+}
+
 #define TRUNC_PTR(x)	((typeof(x))(((unsigned long)(x)) & 0xffffffff))
 
 int sys_clone(unsigned long clone_flags, unsigned long usp,

commit fab5db97e44f76461f76b24adfa8ccb14d4df498
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 7 16:14:40 2006 +1000

    [PATCH] powerpc: Implement support for setting little-endian mode via prctl
    
    This adds the PowerPC part of the code to allow processes to change
    their endian mode via prctl.
    
    This also extends the alignment exception handler to be able to fix up
    alignment exceptions that occur in little-endian mode, both for
    "PowerPC" little-endian and true little-endian.
    
    We always enter signal handlers in big-endian mode -- the support for
    little-endian mode does not amount to the creation of a little-endian
    user/kernel ABI.  If the signal handler returns, the endian mode is
    restored to what it was when the signal was delivered.
    
    We have two new kernel CPU feature bits, one for PPC little-endian and
    one for true little-endian.  Most of the classic 32-bit processors
    support PPC little-endian, and this is reflected in the CPU feature
    table.  There are two corresponding feature bits reported to userland
    in the AT_HWCAP aux vector entry.
    
    This is based on an earlier patch by Anton Blanchard.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 2dd47d2dd998..2d35d83961b2 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -708,6 +708,50 @@ int get_fpexc_mode(struct task_struct *tsk, unsigned long adr)
 	return put_user(val, (unsigned int __user *) adr);
 }
 
+int set_endian(struct task_struct *tsk, unsigned int val)
+{
+	struct pt_regs *regs = tsk->thread.regs;
+
+	if ((val == PR_ENDIAN_LITTLE && !cpu_has_feature(CPU_FTR_REAL_LE)) ||
+	    (val == PR_ENDIAN_PPC_LITTLE && !cpu_has_feature(CPU_FTR_PPC_LE)))
+		return -EINVAL;
+
+	if (regs == NULL)
+		return -EINVAL;
+
+	if (val == PR_ENDIAN_BIG)
+		regs->msr &= ~MSR_LE;
+	else if (val == PR_ENDIAN_LITTLE || val == PR_ENDIAN_PPC_LITTLE)
+		regs->msr |= MSR_LE;
+	else
+		return -EINVAL;
+
+	return 0;
+}
+
+int get_endian(struct task_struct *tsk, unsigned long adr)
+{
+	struct pt_regs *regs = tsk->thread.regs;
+	unsigned int val;
+
+	if (!cpu_has_feature(CPU_FTR_PPC_LE) &&
+	    !cpu_has_feature(CPU_FTR_REAL_LE))
+		return -EINVAL;
+
+	if (regs == NULL)
+		return -EINVAL;
+
+	if (regs->msr & MSR_LE) {
+		if (cpu_has_feature(CPU_FTR_REAL_LE))
+			val = PR_ENDIAN_LITTLE;
+		else
+			val = PR_ENDIAN_PPC_LITTLE;
+	} else
+		val = PR_ENDIAN_BIG;
+
+	return put_user(val, (unsigned int __user *)adr);
+}
+
 #define TRUNC_PTR(x)	((typeof(x))(((unsigned long)(x)) & 0xffffffff))
 
 int sys_clone(unsigned long clone_flags, unsigned long usp,

commit 0cb3463f04e771869f481e2dd44f66419e850586
Author: Adrian Bunk <bunk@stusta.de>
Date:   Fri Mar 31 02:32:07 2006 -0800

    [PATCH] unexport get_wchan
    
    The only user of get_wchan is the proc fs - and proc can't be built modular.
    
    Signed-off-by: Adrian Bunk <bunk@stusta.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 706090c99f47..2dd47d2dd998 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -834,7 +834,6 @@ unsigned long get_wchan(struct task_struct *p)
 	} while (count++ < 16);
 	return 0;
 }
-EXPORT_SYMBOL(get_wchan);
 
 static int kstack_depth_to_print = 64;
 

commit 2f25194dbe0c4b2472ce133ea3e9bcbb14936ae7
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Mar 27 11:46:18 2006 +1100

    [PATCH] powerpc: export validate_sp for oprofile calltrace
    
    Export validate_sp so we can use it in the oprofile calltrace code.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index c6e81bbd615c..706090c99f47 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -770,7 +770,7 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	return error;
 }
 
-static int validate_sp(unsigned long sp, struct task_struct *p,
+int validate_sp(unsigned long sp, struct task_struct *p,
 		       unsigned long nbytes)
 {
 	unsigned long stack_page = (unsigned long)task_stack_page(p);
@@ -808,6 +808,8 @@ static int validate_sp(unsigned long sp, struct task_struct *p,
 #define FRAME_MARKER	2
 #endif
 
+EXPORT_SYMBOL(validate_sp);
+
 unsigned long get_wchan(struct task_struct *p)
 {
 	unsigned long ip, sp;

commit a7f31841a40776605c834053ad1eb82d539bd79f
Author: Arnd Bergmann <abergman@de.ibm.com>
Date:   Thu Mar 23 00:00:08 2006 +0100

    [PATCH] powerpc: declare arch syscalls in <asm/syscalls.h>
    
    powerpc currently declares some of its own system calls
    in <asm/unistd.h>, but not all of them. That place also
    contains remainders of the now almost unused kernel syscall
    hack.
    
     - Add a new <asm/syscalls.h> with clean declarations
     - Include that file from every source that implements one
       of these
     - Get rid of old declarations in <asm/unistd.h>
    
    This patch is required as a base for implementing system
    calls from an SPU, but also makes sense as a general
    cleanup.
    
    Signed-off-by: Arnd Bergmann <arnd.bergmann@de.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 995b14688d3a..c6e81bbd615c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -45,6 +45,7 @@
 #include <asm/prom.h>
 #include <asm/machdep.h>
 #include <asm/time.h>
+#include <asm/syscalls.h>
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
 #endif

commit af308377e204e25f1f58627d05fe0f483703b514
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Mar 23 17:38:10 2006 +1100

    [PATCH] powerpc: fix various sparse warnings
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f698aa77127e..995b14688d3a 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -362,7 +362,11 @@ static void show_instructions(struct pt_regs *regs)
 		if (!(i % 8))
 			printk("\n");
 
-		if (BAD_PC(pc) || __get_user(instr, (unsigned int *)pc)) {
+		/* We use __get_user here *only* to avoid an OOPS on a
+		 * bad address because the pc *should* only be a
+		 * kernel address.
+		 */
+		if (BAD_PC(pc) || __get_user(instr, (unsigned int __user *)pc)) {
 			printk("XXXXXXXX ");
 		} else {
 			if (regs->nip == pc)

commit c6fd91f0bdcd294a0ae0ba2b2a7f7456ef4b7144
Author: bibo mao <bibo_mao@linux.intel.com>
Date:   Sun Mar 26 01:38:20 2006 -0800

    [PATCH] kretprobe instance recycled by parent process
    
    When kretprobe probes the schedule() function, if the probed process exits
    then schedule() will never return, so some kretprobe instances will never
    be recycled.
    
    In this patch the parent process will recycle retprobe instances of the
    probed function and there will be no memory leak of kretprobe instances.
    
    Signed-off-by: bibo mao <bibo.mao@intel.com>
    Cc: Masami Hiramatsu <hiramatu@sdl.hitachi.co.jp>
    Cc: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Anil S Keshavamurthy <anil.s.keshavamurthy@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 1770a066c217..f698aa77127e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -35,7 +35,6 @@
 #include <linux/mqueue.h>
 #include <linux/hardirq.h>
 #include <linux/utsname.h>
-#include <linux/kprobes.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
@@ -460,7 +459,6 @@ void show_regs(struct pt_regs * regs)
 
 void exit_thread(void)
 {
-	kprobe_flush_task(current);
 	discard_lazy_cpu_state();
 }
 

commit c6622f63db86fcbd41bf6fe05ddf2e00c1e51ced
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Feb 24 10:06:59 2006 +1100

    powerpc: Implement accurate task and CPU time accounting
    
    This implements accurate task and cpu time accounting for 64-bit
    powerpc kernels.  Instead of accounting a whole jiffy of time to a
    task on a timer interrupt because that task happened to be running at
    the time, we now account time in units of timebase ticks according to
    the actual time spent by the task in user mode and kernel mode.  We
    also count the time spent processing hardware and software interrupts
    accurately.  This is conditional on CONFIG_VIRT_CPU_ACCOUNTING.  If
    that is not set, we do tick-based approximate accounting as before.
    
    To get this accurate information, we read either the PURR (processor
    utilization of resources register) on POWER5 machines, or the timebase
    on other machines on
    
    * each entry to the kernel from usermode
    * each exit to usermode
    * transitions between process context, hard irq context and soft irq
      context in kernel mode
    * context switches.
    
    On POWER5 systems with shared-processor logical partitioning we also
    read both the PURR and the timebase at each timer interrupt and
    context switch in order to determine how much time has been taken by
    the hypervisor to run other partitions ("steal" time).  Unfortunately,
    since we need values of the PURR on both threads at the same time to
    accurately calculate the steal time, and since we can only calculate
    steal time on a per-core basis, the apportioning of the steal time
    between idle time (time which we ceded to the hypervisor in the idle
    loop) and actual stolen time is somewhat approximate at the moment.
    
    This is all based quite heavily on what s390 does, and it uses the
    generic interfaces that were added by the s390 developers,
    i.e. account_system_time(), account_user_time(), etc.
    
    This patch doesn't add any new interfaces between the kernel and
    userspace, and doesn't change the units in which time is reported to
    userspace by things such as /proc/stat, /proc/<pid>/stat, getrusage(),
    times(), etc.  Internally the various task and cpu times are stored in
    timebase units, but they are converted to USER_HZ units (1/100th of a
    second) when reported to userspace.  Some precision is therefore lost
    but there should not be any accumulating error, since the internal
    accumulation is at full precision.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index dd774c3c9302..1770a066c217 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -45,9 +45,9 @@
 #include <asm/mmu.h>
 #include <asm/prom.h>
 #include <asm/machdep.h>
+#include <asm/time.h>
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
-#include <asm/time.h>
 #endif
 
 extern unsigned long _get_SP(void);
@@ -328,6 +328,11 @@ struct task_struct *__switch_to(struct task_struct *prev,
 #endif
 
 	local_irq_save(flags);
+
+	account_system_vtime(current);
+	account_process_vtime(current);
+	calculate_steal_time();
+
 	last = _switch(old_thread, new_thread);
 
 	local_irq_restore(flags);

commit a00428f5b149e36b8225b2a0812742a6dfb07b8c
Merge: 774fee58c465 fb5c594c2acc
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Feb 24 14:05:47 2006 +1100

    Merge ../powerpc-merge

commit cb2c9b2741346eb23b177187a51ff5abf08295bd
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Feb 13 14:48:35 2006 +1100

    [PATCH] powerpc: Fix runlatch performance issues
    
    The runlatch SPR can take a lot of time to write. My original runlatch
    code would set it on every exception entry even though most of the time
    this was not required. It would also continually set it in the idle
    loop, which is an issue on an SMT capable processor.
    
    Now we cache the runlatch value in a threadinfo bit, and only check for
    it in decrementer and hardware interrupt exceptions as well as the idle
    loop. Boot on POWER3, POWER5 and iseries, and compile tested on pmac32.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 57703994a063..c225cf154bfe 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -888,3 +888,35 @@ void dump_stack(void)
 	show_stack(current, NULL);
 }
 EXPORT_SYMBOL(dump_stack);
+
+#ifdef CONFIG_PPC64
+void ppc64_runlatch_on(void)
+{
+	unsigned long ctrl;
+
+	if (cpu_has_feature(CPU_FTR_CTRL) && !test_thread_flag(TIF_RUNLATCH)) {
+		HMT_medium();
+
+		ctrl = mfspr(SPRN_CTRLF);
+		ctrl |= CTRL_RUNLATCH;
+		mtspr(SPRN_CTRLT, ctrl);
+
+		set_thread_flag(TIF_RUNLATCH);
+	}
+}
+
+void ppc64_runlatch_off(void)
+{
+	unsigned long ctrl;
+
+	if (cpu_has_feature(CPU_FTR_CTRL) && test_thread_flag(TIF_RUNLATCH)) {
+		HMT_medium();
+
+		clear_thread_flag(TIF_RUNLATCH);
+
+		ctrl = mfspr(SPRN_CTRLF);
+		ctrl &= ~CTRL_RUNLATCH;
+		mtspr(SPRN_CTRLT, ctrl);
+	}
+}
+#endif

commit 2ef9481e666b4654159ac9f847e6963809e3c470
Author: Jon Mason <jdmason@us.ibm.com>
Date:   Mon Jan 23 10:58:20 2006 -0600

    [PATCH] powerpc: trivial: modify comments to refer to new location of files
    
    This patch removes all self references and fixes references to files
    in the now defunct arch/ppc64 tree.  I think this accomplises
    everything wanted, though there might be a few references I missed.
    
    Signed-off-by: Jon Mason <jdmason@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 57703994a063..1201880cab40 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -1,6 +1,4 @@
 /*
- *  arch/ppc/kernel/process.c
- *
  *  Derived from "arch/i386/kernel/process.c"
  *    Copyright (C) 1995  Linus Torvalds
  *

commit 45bfe98bd790b5ded00462cd582effcfb42263cc
Merge: 9f5974c8734d 624cee31bcb1
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Thu Jan 12 10:21:22 2006 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc-merge
    
    Fix up delete/modify conflict of arch/ppc/kernel/process.c by hand (it's
    gone, gone, gone).
    
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

commit 0cec6fd1370807f98934e84c9e6147335b81d8a5
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Thu Jan 12 01:06:02 2006 -0800

    [PATCH] powerpc: task_stack_page()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index bc03526d25de..1f816f0d7740 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -503,7 +503,7 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 {
 	struct pt_regs *childregs, *kregs;
 	extern void ret_from_fork(void);
-	unsigned long sp = (unsigned long)p->thread_info + THREAD_SIZE;
+	unsigned long sp = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 
 	CHECK_FULL_REGS(regs);
 	/* Copy registers */
@@ -588,10 +588,8 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	 * set.  Do it now.
 	 */
 	if (!current->thread.regs) {
-		unsigned long childregs = (unsigned long)current->thread_info +
-						THREAD_SIZE;
-		childregs -= sizeof(struct pt_regs);
-		current->thread.regs = (struct pt_regs *)childregs;
+		struct pt_regs *regs = task_stack_page(current) + THREAD_SIZE;
+		current->thread.regs = regs - 1;
 	}
 
 	memset(regs->gpr, 0, sizeof(regs->gpr));
@@ -767,7 +765,7 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 static int validate_sp(unsigned long sp, struct task_struct *p,
 		       unsigned long nbytes)
 {
-	unsigned long stack_page = (unsigned long)p->thread_info;
+	unsigned long stack_page = (unsigned long)task_stack_page(p);
 
 	if (sp >= stack_page + sizeof(struct thread_struct)
 	    && sp <= stack_page + THREAD_SIZE - nbytes)

commit b5e2fc1c6259e6f26bc4ae4de697da1f8da0edec
Author: Al Viro <viro@ftp.linux.org.uk>
Date:   Thu Jan 12 01:06:01 2006 -0800

    [PATCH] powerpc: task_thread_info()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 105d5609ff57..bc03526d25de 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -424,7 +424,7 @@ void show_regs(struct pt_regs * regs)
 	if (trap == 0x300 || trap == 0x600)
 		printk("DAR: "REG", DSISR: "REG"\n", regs->dar, regs->dsisr);
 	printk("TASK = %p[%d] '%s' THREAD: %p",
-	       current, current->pid, current->comm, current->thread_info);
+	       current, current->pid, current->comm, task_thread_info(current));
 
 #ifdef CONFIG_SMP
 	printk(" CPU: %d", smp_processor_id());
@@ -516,7 +516,7 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 #ifdef CONFIG_PPC32
 		childregs->gpr[2] = (unsigned long) p;
 #else
-		clear_ti_thread_flag(p->thread_info, TIF_32BIT);
+		clear_tsk_thread_flag(p, TIF_32BIT);
 #endif
 		p->thread.regs = NULL;	/* no user register state */
 	} else {

commit 624cee31bcb14bfd85559fbec5dd7bb833542965
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jan 12 21:22:34 2006 +1100

    powerpc: make ARCH=ppc use arch/powerpc/kernel/process.c
    
    Commit 5388fb1025443ec223ba556b10efc4c5f83f8682 made signal_32.c
    use discard_lazy_cpu_state, which broke ARCH=ppc because that
    uses the common signal_32.c but has its own process.c.  Make ARCH=ppc
    use the common process.c to fix this and to reduce the amount
    of duplicated code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 913f90692a36..9101358cc6b3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -223,6 +223,7 @@ void discard_lazy_cpu_state(void)
 }
 #endif /* CONFIG_SMP */
 
+#ifdef CONFIG_PPC_MERGE		/* XXX for now */
 int set_dabr(unsigned long dabr)
 {
 	if (ppc_md.set_dabr)
@@ -231,6 +232,7 @@ int set_dabr(unsigned long dabr)
 	mtspr(SPRN_DABR, dabr);
 	return 0;
 }
+#endif
 
 #ifdef CONFIG_PPC64
 DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);

commit 5388fb1025443ec223ba556b10efc4c5f83f8682
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jan 11 22:11:39 2006 +1100

    [PATCH] powerpc: Avoid potential FP corruption with preempt and UP
    
    Heikki Lindholm pointed out that there was a potential race with the
    lazy CPU state (FP, VR, EVR) stuff if preempt is enabled.  The race
    is that in the process of restoring FP state on sigreturn, the task
    gets preempted by a user task that wants to use the FPU.  It will take
    an FP unavailable exception, which will write the current FPU state
    to the thread_struct, overwriting the values which sigreturn has
    stored.  Note that this can only happen on UP since we don't implement
    lazy CPU state on SMP.
    
    The fix is to flush the lazy CPU state before updating the
    thread_struct.  To do this we re-use the flush_lazy_cpu_state()
    function from process.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 105d5609ff57..913f90692a36 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -201,13 +201,13 @@ int dump_spe(struct pt_regs *regs, elf_vrregset_t *evrregs)
 }
 #endif /* CONFIG_SPE */
 
+#ifndef CONFIG_SMP
 /*
  * If we are doing lazy switching of CPU state (FP, altivec or SPE),
  * and the current task has some state, discard it.
  */
-static inline void discard_lazy_cpu_state(void)
+void discard_lazy_cpu_state(void)
 {
-#ifndef CONFIG_SMP
 	preempt_disable();
 	if (last_task_used_math == current)
 		last_task_used_math = NULL;
@@ -220,8 +220,8 @@ static inline void discard_lazy_cpu_state(void)
 		last_task_used_spe = NULL;
 #endif
 	preempt_enable();
-#endif /* CONFIG_SMP */
 }
+#endif /* CONFIG_SMP */
 
 int set_dabr(unsigned long dabr)
 {

commit 48abec07cf8063184d397560a6a5f27eaf9caddf
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Nov 30 13:20:54 2005 +1100

    powerpc: Fix bug causing FP registers corruption on UP + preempt
    
    This fixes a bug noticed by Paolo Galtieri and fixed for ARCH=ppc in
    the previous commit (ppc: fix floating point register corruption).
    This fixes the arch/powerpc code by adding preempt_disable/enable,
    and also cleans it up a bit by pulling out the code that discards
    any lazily-switched CPU register state into a new function, rather
    than having that code repeated in three places.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index a5a7542a8ff3..105d5609ff57 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -201,6 +201,28 @@ int dump_spe(struct pt_regs *regs, elf_vrregset_t *evrregs)
 }
 #endif /* CONFIG_SPE */
 
+/*
+ * If we are doing lazy switching of CPU state (FP, altivec or SPE),
+ * and the current task has some state, discard it.
+ */
+static inline void discard_lazy_cpu_state(void)
+{
+#ifndef CONFIG_SMP
+	preempt_disable();
+	if (last_task_used_math == current)
+		last_task_used_math = NULL;
+#ifdef CONFIG_ALTIVEC
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = NULL;
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_SPE
+	if (last_task_used_spe == current)
+		last_task_used_spe = NULL;
+#endif
+	preempt_enable();
+#endif /* CONFIG_SMP */
+}
+
 int set_dabr(unsigned long dabr)
 {
 	if (ppc_md.set_dabr)
@@ -434,19 +456,7 @@ void show_regs(struct pt_regs * regs)
 void exit_thread(void)
 {
 	kprobe_flush_task(current);
-
-#ifndef CONFIG_SMP
-	if (last_task_used_math == current)
-		last_task_used_math = NULL;
-#ifdef CONFIG_ALTIVEC
-	if (last_task_used_altivec == current)
-		last_task_used_altivec = NULL;
-#endif /* CONFIG_ALTIVEC */
-#ifdef CONFIG_SPE
-	if (last_task_used_spe == current)
-		last_task_used_spe = NULL;
-#endif
-#endif /* CONFIG_SMP */
+	discard_lazy_cpu_state();
 }
 
 void flush_thread(void)
@@ -458,18 +468,7 @@ void flush_thread(void)
 		t->flags ^= (_TIF_ABI_PENDING | _TIF_32BIT);
 #endif
 
-#ifndef CONFIG_SMP
-	if (last_task_used_math == current)
-		last_task_used_math = NULL;
-#ifdef CONFIG_ALTIVEC
-	if (last_task_used_altivec == current)
-		last_task_used_altivec = NULL;
-#endif /* CONFIG_ALTIVEC */
-#ifdef CONFIG_SPE
-	if (last_task_used_spe == current)
-		last_task_used_spe = NULL;
-#endif
-#endif /* CONFIG_SMP */
+	discard_lazy_cpu_state();
 
 #ifdef CONFIG_PPC64	/* for now */
 	if (current->thread.dabr) {
@@ -635,18 +634,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	}
 #endif
 
-#ifndef CONFIG_SMP
-	if (last_task_used_math == current)
-		last_task_used_math = NULL;
-#ifdef CONFIG_ALTIVEC
-	if (last_task_used_altivec == current)
-		last_task_used_altivec = NULL;
-#endif
-#ifdef CONFIG_SPE
-	if (last_task_used_spe == current)
-		last_task_used_spe = NULL;
-#endif
-#endif /* CONFIG_SMP */
+	discard_lazy_cpu_state();
 	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
 	current->thread.fpscr.val = 0;
 #ifdef CONFIG_ALTIVEC

commit 8bf1101bd52573e0573e374d56d2feecdbb5e444
Author: Jim Keniston <jkenisto@us.ibm.com>
Date:   Wed Nov 23 13:37:42 2005 -0800

    [PATCH] kprobes: Fix return probes on sys_execve
    
    Fix a bug in kprobes that can cause an Oops or even a crash when a return
    probe is installed on one of the following functions: sys_execve,
    do_execve, load_*_binary, flush_old_exec, or flush_thread.  The fix is to
    remove the call to kprobe_flush_task() in flush_thread().  This fix has
    been tested on all architectures for which the return-probes feature has
    been implemented (i386, x86_64, ppc64, ia64).  Please apply.
    
    BACKGROUND
    
    Up to now, we have called kprobe_flush_task() under two situations: when a
    task exits, and when it execs.  Flushing kretprobe_instances on exit is
    correct because (a) do_exit() doesn't return, and (b) one or more
    return-probed functions may be active when a task calls do_exit().  Neither
    is the case for sys_execve() and its callees.
    
    Initially, the mistaken call to kprobe_flush_task() on exec was harmless
    because we put the "real" return address of each active probed function
    back in the stack, just to be safe, when we recycled its
    kretprobe_instance.  When support for ppc64 and ia64 was added, this safety
    measure couldn't be employed, and was eventually dropped even for i386 and
    x86_64.  sys_execve() and its callees were informally blacklisted for
    return probes until this fix was developed.
    
    Acked-by: Prasanna S Panchamukhi <prasanna@in.ibm.com>
    Signed-off-by: Jim Keniston <jkenisto@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index de69fb37c731..a5a7542a8ff3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -457,7 +457,6 @@ void flush_thread(void)
 	if (t->flags & _TIF_ABI_PENDING)
 		t->flags ^= (_TIF_ABI_PENDING | _TIF_32BIT);
 #endif
-	kprobe_flush_task(current);
 
 #ifndef CONFIG_SMP
 	if (last_task_used_math == current)

commit c6135234550ed89a6fd0e8cb229633967e41d649
Merge: 76032de898f3 0b154bb7d0cc
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Nov 7 14:42:09 2005 +1100

    Merge ../linux-2.6

commit 76032de898f34db55b5048349db56557828a1390
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Mon Nov 7 13:12:03 2005 +1100

    [PATCH] powerpc: Make ppc_md.set_dabr non 64-bit specific
    
    Define ppc_md.set_dabr for both 32 + 64 bit. Cleanup the implementation for
    pSeries also, it was needlessly complex. Now we just do two firmware tests at
    setup time, and use one of two functions, rather than using one function and
    testing on every call.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 96843211cc5c..29f6e875cf1c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -46,10 +46,10 @@
 #include <asm/processor.h>
 #include <asm/mmu.h>
 #include <asm/prom.h>
+#include <asm/machdep.h>
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
 #include <asm/time.h>
-#include <asm/machdep.h>
 #endif
 
 extern unsigned long _get_SP(void);
@@ -203,10 +203,8 @@ int dump_spe(struct pt_regs *regs, elf_vrregset_t *evrregs)
 
 int set_dabr(unsigned long dabr)
 {
-#ifdef CONFIG_PPC64
 	if (ppc_md.set_dabr)
 		return ppc_md.set_dabr(dabr);
-#endif
 
 	mtspr(SPRN_DABR, dabr);
 	return 0;

commit 3c726f8dee6f55e96475574e9f645327e461884c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Nov 7 11:06:55 2005 +1100

    [PATCH] ppc64: support 64k pages
    
    Adds a new CONFIG_PPC_64K_PAGES which, when enabled, changes the kernel
    base page size to 64K.  The resulting kernel still boots on any
    hardware.  On current machines with 4K pages support only, the kernel
    will maintain 16 "subpages" for each 64K page transparently.
    
    Note that while real 64K capable HW has been tested, the current patch
    will not enable it yet as such hardware is not released yet, and I'm
    still verifying with the firmware architects the proper to get the
    information from the newer hypervisors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 96843211cc5c..7f64f0464d44 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -554,12 +554,10 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 #ifdef CONFIG_PPC64
 	if (cpu_has_feature(CPU_FTR_SLB)) {
 		unsigned long sp_vsid = get_kernel_vsid(sp);
+		unsigned long llp = mmu_psize_defs[mmu_linear_psize].sllp;
 
 		sp_vsid <<= SLB_VSID_SHIFT;
-		sp_vsid |= SLB_VSID_KERNEL;
-		if (cpu_has_feature(CPU_FTR_16M_PAGE))
-			sp_vsid |= SLB_VSID_L;
-
+		sp_vsid |= SLB_VSID_KERNEL | llp;
 		p->thread.ksp_vsid = sp_vsid;
 	}
 

commit cab0af98dfbbf8076d1af01f2927af491a76a33f
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Nov 3 15:30:49 2005 +1100

    powerpc: Make set_dabr() a ppc_md function
    
    Move pSeries specific code in set_dabr() into a ppc_md function, this will
    allow us to keep plpar_wrappers.h private to platforms/pseries.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 8f85dabe4df3..96843211cc5c 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -48,8 +48,8 @@
 #include <asm/prom.h>
 #ifdef CONFIG_PPC64
 #include <asm/firmware.h>
-#include <asm/plpar_wrappers.h>
 #include <asm/time.h>
+#include <asm/machdep.h>
 #endif
 
 extern unsigned long _get_SP(void);
@@ -201,27 +201,15 @@ int dump_spe(struct pt_regs *regs, elf_vrregset_t *evrregs)
 }
 #endif /* CONFIG_SPE */
 
-static void set_dabr_spr(unsigned long val)
-{
-	mtspr(SPRN_DABR, val);
-}
-
 int set_dabr(unsigned long dabr)
 {
-	int ret = 0;
-
 #ifdef CONFIG_PPC64
-	if (firmware_has_feature(FW_FEATURE_XDABR)) {
-		/* We want to catch accesses from kernel and userspace */
-		unsigned long flags = H_DABRX_KERNEL|H_DABRX_USER;
-		ret = plpar_set_xdabr(dabr, flags);
-	} else if (firmware_has_feature(FW_FEATURE_DABR)) {
-		ret = plpar_set_dabr(dabr);
-	} else
+	if (ppc_md.set_dabr)
+		return ppc_md.set_dabr(dabr);
 #endif
-		set_dabr_spr(dabr);
 
-	return ret;
+	mtspr(SPRN_DABR, dabr);
+	return 0;
 }
 
 #ifdef CONFIG_PPC64

commit 25c8a78b1e00ac0cc640677eda78b462c2cd4c6e
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Thu Oct 27 16:27:25 2005 +1000

    [PATCH] powerpc: Fix handling of fpscr on 64-bit
    
    The recent merge of fpu.S broken the handling of fpscr for
    ARCH=powerpc and CONFIG_PPC64=y.  FP registers could be corrupted,
    leading to strange random application crashes.
    
    The confusion arises, because the thread_struct has (and requires) a
    64-bit area to save the fpscr, because we use load/store double
    instructions to get it in to/out of the FPU.  However, only the low
    32-bits are actually used, so we want to treat it as a 32-bit quantity
    when manipulating its bits to avoid extra load/stores on 32-bit.  This
    patch replaces the current definition with a structure of two 32-bit
    quantities (pad and val), to clarify things as much as is possible.
    The 'val' field is used when manipulating bits, the structure itself
    is used when obtaining the address for loading/unloading the value
    from the FPU.
    
    While we're at it, consolidate the 4 (!) almost identical versions of
    cvt_fd() and cvt_df() (arch/ppc/kernel/misc.S,
    arch/ppc64/kernel/misc.S, arch/powerpc/kernel/misc_32.S,
    arch/powerpc/kernel/misc_64.S) into a single version in fpu.S.  The
    new version takes a pointer to thread_struct and applies the correct
    offset itself, rather than a pointer to the fpscr field itself, again
    to avoid confusion as to which is the correct field to use.
    
    Finally, this patch makes ARCH=ppc64 also use the consolidated fpu.S
    code, which it previously did not.
    
    Built for G5 (ARCH=ppc64 and ARCH=powerpc), 32-bit powermac (ARCH=ppc
    and ARCH=powerpc) and Walnut (ARCH=ppc, CONFIG_MATH_EMULATION=y).
    Booted on G5 (ARCH=powerpc) and things which previously fell over no
    longer do.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 047da1ae21fe..8f85dabe4df3 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -665,7 +665,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 #endif
 #endif /* CONFIG_SMP */
 	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
-	current->thread.fpscr = 0;
+	current->thread.fpscr.val = 0;
 #ifdef CONFIG_ALTIVEC
 	memset(current->thread.vr, 0, sizeof(current->thread.vr));
 	memset(&current->thread.vscr, 0, sizeof(current->thread.vscr));

commit 90eac727c6d7afbe707ce408edf97c33385fa08c
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Oct 21 16:01:33 2005 +1000

    [PATCH] powerpc: Don't blow away load_addr in start_thread
    
    The patch to make process.c work for 32-bit and 64-bit
    (06d67d54741a5bfefa31945ef195dfa748c29025) broke some 64-bit binaries.
    We were blowing away load_addr in gpr[2], so we weren't properly relocating
    the entry point.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 193c8c1bf132..047da1ae21fe 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -595,6 +595,10 @@ int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
  */
 void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 {
+#ifdef CONFIG_PPC64
+	unsigned long load_addr = regs->gpr[2];	/* saved by ELF_PLAT_INIT */
+#endif
+
 	set_fs(USER_DS);
 
 	/*
@@ -621,7 +625,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	regs->msr = MSR_USER;
 #else
 	if (!test_thread_flag(TIF_32BIT)) {
-		unsigned long entry, toc, load_addr = regs->gpr[2];
+		unsigned long entry, toc;
 
 		/* start is a relocated pointer to the function descriptor for
 		 * the elf _start routine.  The first entry in the function

commit d4bf9a7858a0766cafb21dcb66ff9a5d92c1cd09
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Oct 13 13:40:54 2005 +1000

    ppc64: merge binfmt_elf32.c
    
    and use start_thread for both 32 and 64 bit bineries.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index 92bc75f61ca6..193c8c1bf132 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -620,7 +620,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	regs->nip = start;
 	regs->msr = MSR_USER;
 #else
-	{
+	if (!test_thread_flag(TIF_32BIT)) {
 		unsigned long entry, toc, load_addr = regs->gpr[2];
 
 		/* start is a relocated pointer to the function descriptor for
@@ -641,6 +641,10 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 		regs->nip = entry;
 		regs->gpr[2] = toc;
 		regs->msr = MSR_USER64;
+	} else {
+		regs->nip = start;
+		regs->gpr[2] = 0;
+		regs->msr = MSR_USER32;
 	}
 #endif
 

commit 0f17d0742f27b7a69b0e2dfb21190f06ea3a9087
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Wed Oct 12 23:23:44 2005 +1000

    powerpc: make 64 bit binaries work
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f09908a0beea..92bc75f61ca6 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -620,7 +620,7 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 	regs->nip = start;
 	regs->msr = MSR_USER;
 #else
-	if (test_thread_flag(TIF_32BIT)) {
+	{
 		unsigned long entry, toc, load_addr = regs->gpr[2];
 
 		/* start is a relocated pointer to the function descriptor for
@@ -641,10 +641,6 @@ void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 		regs->nip = entry;
 		regs->gpr[2] = toc;
 		regs->msr = MSR_USER64;
-	} else {
-		regs->nip = start;
-		regs->gpr[2] = 0;
-		regs->msr = MSR_USER32;
 	}
 #endif
 

commit 06d67d54741a5bfefa31945ef195dfa748c29025
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 10 22:29:05 2005 +1000

    powerpc: make process.c suitable for both 32-bit and 64-bit
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index ae316e9ed581..f09908a0beea 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -36,6 +36,8 @@
 #include <linux/kallsyms.h>
 #include <linux/mqueue.h>
 #include <linux/hardirq.h>
+#include <linux/utsname.h>
+#include <linux/kprobes.h>
 
 #include <asm/pgtable.h>
 #include <asm/uaccess.h>
@@ -44,6 +46,11 @@
 #include <asm/processor.h>
 #include <asm/mmu.h>
 #include <asm/prom.h>
+#ifdef CONFIG_PPC64
+#include <asm/firmware.h>
+#include <asm/plpar_wrappers.h>
+#include <asm/time.h>
+#endif
 
 extern unsigned long _get_SP(void);
 
@@ -53,26 +60,6 @@ struct task_struct *last_task_used_altivec = NULL;
 struct task_struct *last_task_used_spe = NULL;
 #endif
 
-static struct fs_struct init_fs = INIT_FS;
-static struct files_struct init_files = INIT_FILES;
-static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
-static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
-struct mm_struct init_mm = INIT_MM(init_mm);
-EXPORT_SYMBOL(init_mm);
-
-/* this is 8kB-aligned so we can get to the thread_info struct
-   at the base of it from the stack pointer with 1 integer instruction. */
-union thread_union init_thread_union
-	__attribute__((__section__(".data.init_task"))) =
-{ INIT_THREAD_INFO(init_task) };
-
-/* initial task structure */
-struct task_struct init_task = INIT_TASK(init_task);
-EXPORT_SYMBOL(init_task);
-
-/* only used to get secondary processor up */
-struct task_struct *current_set[NR_CPUS] = {&init_task, };
-
 /*
  * Make sure the floating-point register state in the
  * the thread_struct is up to date for task tsk.
@@ -237,7 +224,10 @@ int set_dabr(unsigned long dabr)
 	return ret;
 }
 
+#ifdef CONFIG_PPC64
+DEFINE_PER_CPU(struct cpu_usage, cpu_usage_array);
 static DEFINE_PER_CPU(unsigned long, current_dabr);
+#endif
 
 struct task_struct *__switch_to(struct task_struct *prev,
 	struct task_struct *new)
@@ -308,10 +298,27 @@ struct task_struct *__switch_to(struct task_struct *prev,
 		set_dabr(new->thread.dabr);
 		__get_cpu_var(current_dabr) = new->thread.dabr;
 	}
+
+	flush_tlb_pending();
 #endif
 
 	new_thread = &new->thread;
 	old_thread = &current->thread;
+
+#ifdef CONFIG_PPC64
+	/*
+	 * Collect processor utilization data per process
+	 */
+	if (firmware_has_feature(FW_FEATURE_SPLPAR)) {
+		struct cpu_usage *cu = &__get_cpu_var(cpu_usage_array);
+		long unsigned start_tb, current_tb;
+		start_tb = old_thread->start_tb;
+		cu->current_tb = current_tb = mfspr(SPRN_PURR);
+		old_thread->accum_tb += (current_tb - start_tb);
+		new_thread->start_tb = current_tb;
+	}
+#endif
+
 	local_irq_save(flags);
 	last = _switch(old_thread, new_thread);
 
@@ -320,37 +327,106 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	return last;
 }
 
+static int instructions_to_print = 16;
+
+#ifdef CONFIG_PPC64
+#define BAD_PC(pc)	((REGION_ID(pc) != KERNEL_REGION_ID) && \
+		         (REGION_ID(pc) != VMALLOC_REGION_ID))
+#else
+#define BAD_PC(pc)	((pc) < KERNELBASE)
+#endif
+
+static void show_instructions(struct pt_regs *regs)
+{
+	int i;
+	unsigned long pc = regs->nip - (instructions_to_print * 3 / 4 *
+			sizeof(int));
+
+	printk("Instruction dump:");
+
+	for (i = 0; i < instructions_to_print; i++) {
+		int instr;
+
+		if (!(i % 8))
+			printk("\n");
+
+		if (BAD_PC(pc) || __get_user(instr, (unsigned int *)pc)) {
+			printk("XXXXXXXX ");
+		} else {
+			if (regs->nip == pc)
+				printk("<%08x> ", instr);
+			else
+				printk("%08x ", instr);
+		}
+
+		pc += sizeof(int);
+	}
+
+	printk("\n");
+}
+
+static struct regbit {
+	unsigned long bit;
+	const char *name;
+} msr_bits[] = {
+	{MSR_EE,	"EE"},
+	{MSR_PR,	"PR"},
+	{MSR_FP,	"FP"},
+	{MSR_ME,	"ME"},
+	{MSR_IR,	"IR"},
+	{MSR_DR,	"DR"},
+	{0,		NULL}
+};
+
+static void printbits(unsigned long val, struct regbit *bits)
+{
+	const char *sep = "";
+
+	printk("<");
+	for (; bits->bit; ++bits)
+		if (val & bits->bit) {
+			printk("%s%s", sep, bits->name);
+			sep = ",";
+		}
+	printk(">");
+}
+
+#ifdef CONFIG_PPC64
+#define REG		"%016lX"
+#define REGS_PER_LINE	4
+#define LAST_VOLATILE	13
+#else
+#define REG		"%08lX"
+#define REGS_PER_LINE	8
+#define LAST_VOLATILE	12
+#endif
+
 void show_regs(struct pt_regs * regs)
 {
 	int i, trap;
 
-	printk("NIP: %08lX LR: %08lX SP: %08lX REGS: %p TRAP: %04lx    %s\n",
-	       regs->nip, regs->link, regs->gpr[1], regs, regs->trap,
-	       print_tainted());
-	printk("MSR: %08lx EE: %01x PR: %01x FP: %01x ME: %01x IR/DR: %01x%01x\n",
-	       regs->msr, regs->msr&MSR_EE ? 1 : 0, regs->msr&MSR_PR ? 1 : 0,
-	       regs->msr & MSR_FP ? 1 : 0,regs->msr&MSR_ME ? 1 : 0,
-	       regs->msr&MSR_IR ? 1 : 0,
-	       regs->msr&MSR_DR ? 1 : 0);
+	printk("NIP: "REG" LR: "REG" CTR: "REG"\n",
+	       regs->nip, regs->link, regs->ctr);
+	printk("REGS: %p TRAP: %04lx   %s  (%s)\n",
+	       regs, regs->trap, print_tainted(), system_utsname.release);
+	printk("MSR: "REG" ", regs->msr);
+	printbits(regs->msr, msr_bits);
+	printk("  CR: %08lX  XER: %08lX\n", regs->ccr, regs->xer);
 	trap = TRAP(regs);
 	if (trap == 0x300 || trap == 0x600)
-		printk("DAR: %08lX, DSISR: %08lX\n", regs->dar, regs->dsisr);
-	printk("TASK = %p[%d] '%s' THREAD: %p\n",
+		printk("DAR: "REG", DSISR: "REG"\n", regs->dar, regs->dsisr);
+	printk("TASK = %p[%d] '%s' THREAD: %p",
 	       current, current->pid, current->comm, current->thread_info);
-	printk("Last syscall: %ld ", current->thread.last_syscall);
 
 #ifdef CONFIG_SMP
 	printk(" CPU: %d", smp_processor_id());
 #endif /* CONFIG_SMP */
 
 	for (i = 0;  i < 32;  i++) {
-		long r;
-		if ((i % 8) == 0)
+		if ((i % REGS_PER_LINE) == 0)
 			printk("\n" KERN_INFO "GPR%02d: ", i);
-		if (__get_user(r, &regs->gpr[i]))
-			break;
-		printk("%08lX ", r);
-		if (i == 12 && !FULL_REGS(regs))
+		printk(REG " ", regs->gpr[i]);
+		if (i == LAST_VOLATILE && !FULL_REGS(regs))
 			break;
 	}
 	printk("\n");
@@ -359,16 +435,20 @@ void show_regs(struct pt_regs * regs)
 	 * Lookup NIP late so we have the best change of getting the
 	 * above info out without failing
 	 */
-	printk("NIP [%08lx] ", regs->nip);
+	printk("NIP ["REG"] ", regs->nip);
 	print_symbol("%s\n", regs->nip);
-	printk("LR [%08lx] ", regs->link);
+	printk("LR ["REG"] ", regs->link);
 	print_symbol("%s\n", regs->link);
 #endif
 	show_stack(current, (unsigned long *) regs->gpr[1]);
+	if (!user_mode(regs))
+		show_instructions(regs);
 }
 
 void exit_thread(void)
 {
+	kprobe_flush_task(current);
+
 #ifndef CONFIG_SMP
 	if (last_task_used_math == current)
 		last_task_used_math = NULL;
@@ -385,6 +465,14 @@ void exit_thread(void)
 
 void flush_thread(void)
 {
+#ifdef CONFIG_PPC64
+	struct thread_info *t = current_thread_info();
+
+	if (t->flags & _TIF_ABI_PENDING)
+		t->flags ^= (_TIF_ABI_PENDING | _TIF_32BIT);
+#endif
+	kprobe_flush_task(current);
+
 #ifndef CONFIG_SMP
 	if (last_task_used_math == current)
 		last_task_used_math = NULL;
@@ -425,15 +513,13 @@ void prepare_to_copy(struct task_struct *tsk)
 /*
  * Copy a thread..
  */
-int
-copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
-	    unsigned long unused,
-	    struct task_struct *p, struct pt_regs *regs)
+int copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
+		unsigned long unused, struct task_struct *p,
+		struct pt_regs *regs)
 {
 	struct pt_regs *childregs, *kregs;
 	extern void ret_from_fork(void);
 	unsigned long sp = (unsigned long)p->thread_info + THREAD_SIZE;
-	unsigned long childframe;
 
 	CHECK_FULL_REGS(regs);
 	/* Copy registers */
@@ -443,17 +529,26 @@ copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 	if ((childregs->msr & MSR_PR) == 0) {
 		/* for kernel thread, set `current' and stackptr in new task */
 		childregs->gpr[1] = sp + sizeof(struct pt_regs);
+#ifdef CONFIG_PPC32
 		childregs->gpr[2] = (unsigned long) p;
+#else
+		clear_ti_thread_flag(p->thread_info, TIF_32BIT);
+#endif
 		p->thread.regs = NULL;	/* no user register state */
 	} else {
 		childregs->gpr[1] = usp;
 		p->thread.regs = childregs;
-		if (clone_flags & CLONE_SETTLS)
-			childregs->gpr[2] = childregs->gpr[6];
+		if (clone_flags & CLONE_SETTLS) {
+#ifdef CONFIG_PPC64
+			if (!test_thread_flag(TIF_32BIT))
+				childregs->gpr[13] = childregs->gpr[6];
+			else
+#endif
+				childregs->gpr[2] = childregs->gpr[6];
+		}
 	}
 	childregs->gpr[3] = 0;  /* Result from fork() */
 	sp -= STACK_FRAME_OVERHEAD;
-	childframe = sp;
 
 	/*
 	 * The way this works is that at some point in the future
@@ -467,9 +562,30 @@ copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 	kregs = (struct pt_regs *) sp;
 	sp -= STACK_FRAME_OVERHEAD;
 	p->thread.ksp = sp;
-	kregs->nip = (unsigned long)ret_from_fork;
 
+#ifdef CONFIG_PPC64
+	if (cpu_has_feature(CPU_FTR_SLB)) {
+		unsigned long sp_vsid = get_kernel_vsid(sp);
+
+		sp_vsid <<= SLB_VSID_SHIFT;
+		sp_vsid |= SLB_VSID_KERNEL;
+		if (cpu_has_feature(CPU_FTR_16M_PAGE))
+			sp_vsid |= SLB_VSID_L;
+
+		p->thread.ksp_vsid = sp_vsid;
+	}
+
+	/*
+	 * The PPC64 ABI makes use of a TOC to contain function 
+	 * pointers.  The function (ret_from_except) is actually a pointer
+	 * to the TOC entry.  The first entry is a pointer to the actual
+	 * function.
+ 	 */
+	kregs->nip = *((unsigned long *)ret_from_fork);
+#else
+	kregs->nip = (unsigned long)ret_from_fork;
 	p->thread.last_syscall = -1;
+#endif
 
 	return 0;
 }
@@ -477,18 +593,61 @@ copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
 /*
  * Set up a thread for executing a new program
  */
-void start_thread(struct pt_regs *regs, unsigned long nip, unsigned long sp)
+void start_thread(struct pt_regs *regs, unsigned long start, unsigned long sp)
 {
 	set_fs(USER_DS);
+
+	/*
+	 * If we exec out of a kernel thread then thread.regs will not be
+	 * set.  Do it now.
+	 */
+	if (!current->thread.regs) {
+		unsigned long childregs = (unsigned long)current->thread_info +
+						THREAD_SIZE;
+		childregs -= sizeof(struct pt_regs);
+		current->thread.regs = (struct pt_regs *)childregs;
+	}
+
 	memset(regs->gpr, 0, sizeof(regs->gpr));
 	regs->ctr = 0;
 	regs->link = 0;
 	regs->xer = 0;
 	regs->ccr = 0;
-	regs->mq = 0;
-	regs->nip = nip;
 	regs->gpr[1] = sp;
+
+#ifdef CONFIG_PPC32
+	regs->mq = 0;
+	regs->nip = start;
 	regs->msr = MSR_USER;
+#else
+	if (test_thread_flag(TIF_32BIT)) {
+		unsigned long entry, toc, load_addr = regs->gpr[2];
+
+		/* start is a relocated pointer to the function descriptor for
+		 * the elf _start routine.  The first entry in the function
+		 * descriptor is the entry address of _start and the second
+		 * entry is the TOC value we need to use.
+		 */
+		__get_user(entry, (unsigned long __user *)start);
+		__get_user(toc, (unsigned long __user *)start+1);
+
+		/* Check whether the e_entry function descriptor entries
+		 * need to be relocated before we can use them.
+		 */
+		if (load_addr != 0) {
+			entry += load_addr;
+			toc   += load_addr;
+		}
+		regs->nip = entry;
+		regs->gpr[2] = toc;
+		regs->msr = MSR_USER64;
+	} else {
+		regs->nip = start;
+		regs->gpr[2] = 0;
+		regs->msr = MSR_USER32;
+	}
+#endif
+
 #ifndef CONFIG_SMP
 	if (last_task_used_math == current)
 		last_task_used_math = NULL;
@@ -506,6 +665,7 @@ void start_thread(struct pt_regs *regs, unsigned long nip, unsigned long sp)
 #ifdef CONFIG_ALTIVEC
 	memset(current->thread.vr, 0, sizeof(current->thread.vr));
 	memset(&current->thread.vscr, 0, sizeof(current->thread.vscr));
+	current->thread.vscr.u[3] = 0x00010000; /* Java mode disabled */
 	current->thread.vrsave = 0;
 	current->thread.used_vr = 0;
 #endif /* CONFIG_ALTIVEC */
@@ -532,22 +692,23 @@ int set_fpexc_mode(struct task_struct *tsk, unsigned int val)
 #ifdef CONFIG_SPE
 		tsk->thread.fpexc_mode = val &
 			(PR_FP_EXC_SW_ENABLE | PR_FP_ALL_EXCEPT);
+		return 0;
 #else
 		return -EINVAL;
 #endif
-	} else {
-		/* on a CONFIG_SPE this does not hurt us.  The bits that
-		 * __pack_fe01 use do not overlap with bits used for
-		 * PR_FP_EXC_SW_ENABLE.  Additionally, the MSR[FE0,FE1] bits
-		 * on CONFIG_SPE implementations are reserved so writing to
-		 * them does not change anything */
-		if (val > PR_FP_EXC_PRECISE)
-			return -EINVAL;
-		tsk->thread.fpexc_mode = __pack_fe01(val);
-		if (regs != NULL && (regs->msr & MSR_FP) != 0)
-			regs->msr = (regs->msr & ~(MSR_FE0|MSR_FE1))
-				| tsk->thread.fpexc_mode;
 	}
+
+	/* on a CONFIG_SPE this does not hurt us.  The bits that
+	 * __pack_fe01 use do not overlap with bits used for
+	 * PR_FP_EXC_SW_ENABLE.  Additionally, the MSR[FE0,FE1] bits
+	 * on CONFIG_SPE implementations are reserved so writing to
+	 * them does not change anything */
+	if (val > PR_FP_EXC_PRECISE)
+		return -EINVAL;
+	tsk->thread.fpexc_mode = __pack_fe01(val);
+	if (regs != NULL && (regs->msr & MSR_FP) != 0)
+		regs->msr = (regs->msr & ~(MSR_FE0|MSR_FE1))
+			| tsk->thread.fpexc_mode;
 	return 0;
 }
 
@@ -566,6 +727,8 @@ int get_fpexc_mode(struct task_struct *tsk, unsigned long adr)
 	return put_user(val, (unsigned int __user *) adr);
 }
 
+#define TRUNC_PTR(x)	((typeof(x))(((unsigned long)(x)) & 0xffffffff))
+
 int sys_clone(unsigned long clone_flags, unsigned long usp,
 	      int __user *parent_tidp, void __user *child_threadptr,
 	      int __user *child_tidp, int p6,
@@ -574,6 +737,12 @@ int sys_clone(unsigned long clone_flags, unsigned long usp,
 	CHECK_FULL_REGS(regs);
 	if (usp == 0)
 		usp = regs->gpr[1];	/* stack pointer for child */
+#ifdef CONFIG_PPC64
+	if (test_thread_flag(TIF_32BIT)) {
+		parent_tidp = TRUNC_PTR(parent_tidp);
+		child_tidp = TRUNC_PTR(child_tidp);
+	}
+#endif
  	return do_fork(clone_flags, usp, regs, 0, parent_tidp, child_tidp);
 }
 
@@ -599,7 +768,7 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	       struct pt_regs *regs)
 {
 	int error;
-	char * filename;
+	char *filename;
 
 	filename = getname((char __user *) a0);
 	error = PTR_ERR(filename);
@@ -644,67 +813,19 @@ static int validate_sp(unsigned long sp, struct task_struct *p,
 	return 0;
 }
 
-void dump_stack(void)
-{
-	show_stack(current, NULL);
-}
-
-EXPORT_SYMBOL(dump_stack);
-
-void show_stack(struct task_struct *tsk, unsigned long *stack)
-{
-	unsigned long sp, stack_top, prev_sp, ret;
-	int count = 0;
-	unsigned long next_exc = 0;
-	struct pt_regs *regs;
-	extern char ret_from_except, ret_from_except_full, ret_from_syscall;
-
-	sp = (unsigned long) stack;
-	if (tsk == NULL)
-		tsk = current;
-	if (sp == 0) {
-		if (tsk == current)
-			asm("mr %0,1" : "=r" (sp));
-		else
-			sp = tsk->thread.ksp;
-	}
-
-	prev_sp = (unsigned long) (tsk->thread_info + 1);
-	stack_top = (unsigned long) tsk->thread_info + THREAD_SIZE;
-	while (count < 16 && sp > prev_sp && sp < stack_top && (sp & 3) == 0) {
-		if (count == 0) {
-			printk("Call trace:");
-#ifdef CONFIG_KALLSYMS
-			printk("\n");
-#endif
-		} else {
-			if (next_exc) {
-				ret = next_exc;
-				next_exc = 0;
-			} else
-				ret = *(unsigned long *)(sp + 4);
-			printk(" [%08lx] ", ret);
-#ifdef CONFIG_KALLSYMS
-			print_symbol("%s", ret);
-			printk("\n");
-#endif
-			if (ret == (unsigned long) &ret_from_except
-			    || ret == (unsigned long) &ret_from_except_full
-			    || ret == (unsigned long) &ret_from_syscall) {
-				/* sp + 16 points to an exception frame */
-				regs = (struct pt_regs *) (sp + 16);
-				if (sp + 16 + sizeof(*regs) <= stack_top)
-					next_exc = regs->nip;
-			}
-		}
-		++count;
-		sp = *(unsigned long *)sp;
-	}
-#ifndef CONFIG_KALLSYMS
-	if (count > 0)
-		printk("\n");
+#ifdef CONFIG_PPC64
+#define MIN_STACK_FRAME	112	/* same as STACK_FRAME_OVERHEAD, in fact */
+#define FRAME_LR_SAVE	2
+#define INT_FRAME_SIZE	(sizeof(struct pt_regs) + STACK_FRAME_OVERHEAD + 288)
+#define REGS_MARKER	0x7265677368657265ul
+#define FRAME_MARKER	12
+#else
+#define MIN_STACK_FRAME	16
+#define FRAME_LR_SAVE	1
+#define INT_FRAME_SIZE	(sizeof(struct pt_regs) + STACK_FRAME_OVERHEAD)
+#define REGS_MARKER	0x72656773ul
+#define FRAME_MARKER	2
 #endif
-}
 
 unsigned long get_wchan(struct task_struct *p)
 {
@@ -715,15 +836,15 @@ unsigned long get_wchan(struct task_struct *p)
 		return 0;
 
 	sp = p->thread.ksp;
-	if (!validate_sp(sp, p, 16))
+	if (!validate_sp(sp, p, MIN_STACK_FRAME))
 		return 0;
 
 	do {
 		sp = *(unsigned long *)sp;
-		if (!validate_sp(sp, p, 16))
+		if (!validate_sp(sp, p, MIN_STACK_FRAME))
 			return 0;
 		if (count > 0) {
-			ip = *(unsigned long *)(sp + 4);
+			ip = ((unsigned long *)sp)[FRAME_LR_SAVE];
 			if (!in_sched_functions(ip))
 				return ip;
 		}
@@ -731,3 +852,64 @@ unsigned long get_wchan(struct task_struct *p)
 	return 0;
 }
 EXPORT_SYMBOL(get_wchan);
+
+static int kstack_depth_to_print = 64;
+
+void show_stack(struct task_struct *tsk, unsigned long *stack)
+{
+	unsigned long sp, ip, lr, newsp;
+	int count = 0;
+	int firstframe = 1;
+
+	sp = (unsigned long) stack;
+	if (tsk == NULL)
+		tsk = current;
+	if (sp == 0) {
+		if (tsk == current)
+			asm("mr %0,1" : "=r" (sp));
+		else
+			sp = tsk->thread.ksp;
+	}
+
+	lr = 0;
+	printk("Call Trace:\n");
+	do {
+		if (!validate_sp(sp, tsk, MIN_STACK_FRAME))
+			return;
+
+		stack = (unsigned long *) sp;
+		newsp = stack[0];
+		ip = stack[FRAME_LR_SAVE];
+		if (!firstframe || ip != lr) {
+			printk("["REG"] ["REG"] ", sp, ip);
+			print_symbol("%s", ip);
+			if (firstframe)
+				printk(" (unreliable)");
+			printk("\n");
+		}
+		firstframe = 0;
+
+		/*
+		 * See if this is an exception frame.
+		 * We look for the "regshere" marker in the current frame.
+		 */
+		if (validate_sp(sp, tsk, INT_FRAME_SIZE)
+		    && stack[FRAME_MARKER] == REGS_MARKER) {
+			struct pt_regs *regs = (struct pt_regs *)
+				(sp + STACK_FRAME_OVERHEAD);
+			printk("--- Exception: %lx", regs->trap);
+			print_symbol(" at %s\n", regs->nip);
+			lr = regs->link;
+			print_symbol("    LR = %s\n", lr);
+			firstframe = 1;
+		}
+
+		sp = newsp;
+	} while (count++ < kstack_depth_to_print);
+}
+
+void dump_stack(void)
+{
+	show_stack(current, NULL);
+}
+EXPORT_SYMBOL(dump_stack);

commit c0c0d996d08e450164adedc249c1bbbca63524ce
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Oct 1 13:49:08 2005 +1000

    powerpc: Get merged kernel to compile and run on 32-bit SMP powermac.
    
    This updates the powermac SMP code to use the mpic driver instead of
    the openpic driver and fixes the SMP-dependent context switch code.
    We had a subtle bug where we were using interrupt numbers 256-259 for
    IPIs, but ppc32 had NR_IRQS = 256.  Moved the IPIs down to use interrupt
    numbers 252-255 instead.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index e3946769dd8e..ae316e9ed581 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -272,11 +272,6 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 */
 	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))
 		giveup_altivec(prev);
-	/* Avoid the trap.  On smp this this never happens since
-	 * we don't set last_task_used_altivec -- Cort
-	 */
-	if (new->thread.regs && last_task_used_altivec == new)
-		new->thread.regs->msr |= MSR_VEC;
 #endif /* CONFIG_ALTIVEC */
 #ifdef CONFIG_SPE
 	/*
@@ -288,12 +283,24 @@ struct task_struct *__switch_to(struct task_struct *prev,
 	 */
 	if ((prev->thread.regs && (prev->thread.regs->msr & MSR_SPE)))
 		giveup_spe(prev);
+#endif /* CONFIG_SPE */
+
+#else  /* CONFIG_SMP */
+#ifdef CONFIG_ALTIVEC
+	/* Avoid the trap.  On smp this this never happens since
+	 * we don't set last_task_used_altivec -- Cort
+	 */
+	if (new->thread.regs && last_task_used_altivec == new)
+		new->thread.regs->msr |= MSR_VEC;
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_SPE
 	/* Avoid the trap.  On smp this this never happens since
 	 * we don't set last_task_used_spe
 	 */
 	if (new->thread.regs && last_task_used_spe == new)
 		new->thread.regs->msr |= MSR_SPE;
 #endif /* CONFIG_SPE */
+
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_PPC64	/* for now */

commit 20c8c2106305729e7d5e06f6c3d390e965a3dd34
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Sep 28 20:28:14 2005 +1000

    powerpc: Fixes to get the merged kernel to boot on powermac.
    
    This merges ppc_ksyms.c, puts back the actual do_execve call in
    sys_execve, makes init_MMU call find_end_of_memory rather than
    ppc_md.find_end_of_memory (every platform has a device tree
    with a /memory node now, right?) and fixes some problems with the
    mpic initialization on newworld powermacs.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
index f5a9d2a84fa1..e3946769dd8e 100644
--- a/arch/powerpc/kernel/process.c
+++ b/arch/powerpc/kernel/process.c
@@ -601,6 +601,8 @@ int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
 	flush_fp_to_thread(current);
 	flush_altivec_to_thread(current);
 	flush_spe_to_thread(current);
+	error = do_execve(filename, (char __user * __user *) a1,
+			  (char __user * __user *) a2, regs);
 	if (error == 0) {
 		task_lock(current);
 		current->ptrace &= ~PT_DTRACE;

commit 14cf11af6cf608eb8c23e989ddb17a715ddce109
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Sep 26 16:04:21 2005 +1000

    powerpc: Merge enough to start building in arch/powerpc.
    
    This creates the directory structure under arch/powerpc and a bunch
    of Kconfig files.  It does a first-cut merge of arch/powerpc/mm,
    arch/powerpc/lib and arch/powerpc/platforms/powermac.  This is enough
    to build a 32-bit powermac kernel with ARCH=powerpc.
    
    For now we are getting some unmerged files from arch/ppc/kernel and
    arch/ppc/syslib, or arch/ppc64/kernel.  This makes some minor changes
    to files in those directories and files outside arch/powerpc.
    
    The boot directory is still not merged.  That's going to be interesting.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/process.c b/arch/powerpc/kernel/process.c
new file mode 100644
index 000000000000..f5a9d2a84fa1
--- /dev/null
+++ b/arch/powerpc/kernel/process.c
@@ -0,0 +1,724 @@
+/*
+ *  arch/ppc/kernel/process.c
+ *
+ *  Derived from "arch/i386/kernel/process.c"
+ *    Copyright (C) 1995  Linus Torvalds
+ *
+ *  Updated and modified by Cort Dougan (cort@cs.nmt.edu) and
+ *  Paul Mackerras (paulus@cs.anu.edu.au)
+ *
+ *  PowerPC version
+ *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; either version
+ *  2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/config.h>
+#include <linux/errno.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/smp.h>
+#include <linux/smp_lock.h>
+#include <linux/stddef.h>
+#include <linux/unistd.h>
+#include <linux/ptrace.h>
+#include <linux/slab.h>
+#include <linux/user.h>
+#include <linux/elf.h>
+#include <linux/init.h>
+#include <linux/prctl.h>
+#include <linux/init_task.h>
+#include <linux/module.h>
+#include <linux/kallsyms.h>
+#include <linux/mqueue.h>
+#include <linux/hardirq.h>
+
+#include <asm/pgtable.h>
+#include <asm/uaccess.h>
+#include <asm/system.h>
+#include <asm/io.h>
+#include <asm/processor.h>
+#include <asm/mmu.h>
+#include <asm/prom.h>
+
+extern unsigned long _get_SP(void);
+
+#ifndef CONFIG_SMP
+struct task_struct *last_task_used_math = NULL;
+struct task_struct *last_task_used_altivec = NULL;
+struct task_struct *last_task_used_spe = NULL;
+#endif
+
+static struct fs_struct init_fs = INIT_FS;
+static struct files_struct init_files = INIT_FILES;
+static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
+static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
+struct mm_struct init_mm = INIT_MM(init_mm);
+EXPORT_SYMBOL(init_mm);
+
+/* this is 8kB-aligned so we can get to the thread_info struct
+   at the base of it from the stack pointer with 1 integer instruction. */
+union thread_union init_thread_union
+	__attribute__((__section__(".data.init_task"))) =
+{ INIT_THREAD_INFO(init_task) };
+
+/* initial task structure */
+struct task_struct init_task = INIT_TASK(init_task);
+EXPORT_SYMBOL(init_task);
+
+/* only used to get secondary processor up */
+struct task_struct *current_set[NR_CPUS] = {&init_task, };
+
+/*
+ * Make sure the floating-point register state in the
+ * the thread_struct is up to date for task tsk.
+ */
+void flush_fp_to_thread(struct task_struct *tsk)
+{
+	if (tsk->thread.regs) {
+		/*
+		 * We need to disable preemption here because if we didn't,
+		 * another process could get scheduled after the regs->msr
+		 * test but before we have finished saving the FP registers
+		 * to the thread_struct.  That process could take over the
+		 * FPU, and then when we get scheduled again we would store
+		 * bogus values for the remaining FP registers.
+		 */
+		preempt_disable();
+		if (tsk->thread.regs->msr & MSR_FP) {
+#ifdef CONFIG_SMP
+			/*
+			 * This should only ever be called for current or
+			 * for a stopped child process.  Since we save away
+			 * the FP register state on context switch on SMP,
+			 * there is something wrong if a stopped child appears
+			 * to still have its FP state in the CPU registers.
+			 */
+			BUG_ON(tsk != current);
+#endif
+			giveup_fpu(current);
+		}
+		preempt_enable();
+	}
+}
+
+void enable_kernel_fp(void)
+{
+	WARN_ON(preemptible());
+
+#ifdef CONFIG_SMP
+	if (current->thread.regs && (current->thread.regs->msr & MSR_FP))
+		giveup_fpu(current);
+	else
+		giveup_fpu(NULL);	/* just enables FP for kernel */
+#else
+	giveup_fpu(last_task_used_math);
+#endif /* CONFIG_SMP */
+}
+EXPORT_SYMBOL(enable_kernel_fp);
+
+int dump_task_fpu(struct task_struct *tsk, elf_fpregset_t *fpregs)
+{
+	if (!tsk->thread.regs)
+		return 0;
+	flush_fp_to_thread(current);
+
+	memcpy(fpregs, &tsk->thread.fpr[0], sizeof(*fpregs));
+
+	return 1;
+}
+
+#ifdef CONFIG_ALTIVEC
+void enable_kernel_altivec(void)
+{
+	WARN_ON(preemptible());
+
+#ifdef CONFIG_SMP
+	if (current->thread.regs && (current->thread.regs->msr & MSR_VEC))
+		giveup_altivec(current);
+	else
+		giveup_altivec(NULL);	/* just enable AltiVec for kernel - force */
+#else
+	giveup_altivec(last_task_used_altivec);
+#endif /* CONFIG_SMP */
+}
+EXPORT_SYMBOL(enable_kernel_altivec);
+
+/*
+ * Make sure the VMX/Altivec register state in the
+ * the thread_struct is up to date for task tsk.
+ */
+void flush_altivec_to_thread(struct task_struct *tsk)
+{
+	if (tsk->thread.regs) {
+		preempt_disable();
+		if (tsk->thread.regs->msr & MSR_VEC) {
+#ifdef CONFIG_SMP
+			BUG_ON(tsk != current);
+#endif
+			giveup_altivec(current);
+		}
+		preempt_enable();
+	}
+}
+
+int dump_task_altivec(struct pt_regs *regs, elf_vrregset_t *vrregs)
+{
+	flush_altivec_to_thread(current);
+	memcpy(vrregs, &current->thread.vr[0], sizeof(*vrregs));
+	return 1;
+}
+#endif /* CONFIG_ALTIVEC */
+
+#ifdef CONFIG_SPE
+
+void enable_kernel_spe(void)
+{
+	WARN_ON(preemptible());
+
+#ifdef CONFIG_SMP
+	if (current->thread.regs && (current->thread.regs->msr & MSR_SPE))
+		giveup_spe(current);
+	else
+		giveup_spe(NULL);	/* just enable SPE for kernel - force */
+#else
+	giveup_spe(last_task_used_spe);
+#endif /* __SMP __ */
+}
+EXPORT_SYMBOL(enable_kernel_spe);
+
+void flush_spe_to_thread(struct task_struct *tsk)
+{
+	if (tsk->thread.regs) {
+		preempt_disable();
+		if (tsk->thread.regs->msr & MSR_SPE) {
+#ifdef CONFIG_SMP
+			BUG_ON(tsk != current);
+#endif
+			giveup_spe(current);
+		}
+		preempt_enable();
+	}
+}
+
+int dump_spe(struct pt_regs *regs, elf_vrregset_t *evrregs)
+{
+	flush_spe_to_thread(current);
+	/* We copy u32 evr[32] + u64 acc + u32 spefscr -> 35 */
+	memcpy(evrregs, &current->thread.evr[0], sizeof(u32) * 35);
+	return 1;
+}
+#endif /* CONFIG_SPE */
+
+static void set_dabr_spr(unsigned long val)
+{
+	mtspr(SPRN_DABR, val);
+}
+
+int set_dabr(unsigned long dabr)
+{
+	int ret = 0;
+
+#ifdef CONFIG_PPC64
+	if (firmware_has_feature(FW_FEATURE_XDABR)) {
+		/* We want to catch accesses from kernel and userspace */
+		unsigned long flags = H_DABRX_KERNEL|H_DABRX_USER;
+		ret = plpar_set_xdabr(dabr, flags);
+	} else if (firmware_has_feature(FW_FEATURE_DABR)) {
+		ret = plpar_set_dabr(dabr);
+	} else
+#endif
+		set_dabr_spr(dabr);
+
+	return ret;
+}
+
+static DEFINE_PER_CPU(unsigned long, current_dabr);
+
+struct task_struct *__switch_to(struct task_struct *prev,
+	struct task_struct *new)
+{
+	struct thread_struct *new_thread, *old_thread;
+	unsigned long flags;
+	struct task_struct *last;
+
+#ifdef CONFIG_SMP
+	/* avoid complexity of lazy save/restore of fpu
+	 * by just saving it every time we switch out if
+	 * this task used the fpu during the last quantum.
+	 *
+	 * If it tries to use the fpu again, it'll trap and
+	 * reload its fp regs.  So we don't have to do a restore
+	 * every switch, just a save.
+	 *  -- Cort
+	 */
+	if (prev->thread.regs && (prev->thread.regs->msr & MSR_FP))
+		giveup_fpu(prev);
+#ifdef CONFIG_ALTIVEC
+	/*
+	 * If the previous thread used altivec in the last quantum
+	 * (thus changing altivec regs) then save them.
+	 * We used to check the VRSAVE register but not all apps
+	 * set it, so we don't rely on it now (and in fact we need
+	 * to save & restore VSCR even if VRSAVE == 0).  -- paulus
+	 *
+	 * On SMP we always save/restore altivec regs just to avoid the
+	 * complexity of changing processors.
+	 *  -- Cort
+	 */
+	if (prev->thread.regs && (prev->thread.regs->msr & MSR_VEC))
+		giveup_altivec(prev);
+	/* Avoid the trap.  On smp this this never happens since
+	 * we don't set last_task_used_altivec -- Cort
+	 */
+	if (new->thread.regs && last_task_used_altivec == new)
+		new->thread.regs->msr |= MSR_VEC;
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_SPE
+	/*
+	 * If the previous thread used spe in the last quantum
+	 * (thus changing spe regs) then save them.
+	 *
+	 * On SMP we always save/restore spe regs just to avoid the
+	 * complexity of changing processors.
+	 */
+	if ((prev->thread.regs && (prev->thread.regs->msr & MSR_SPE)))
+		giveup_spe(prev);
+	/* Avoid the trap.  On smp this this never happens since
+	 * we don't set last_task_used_spe
+	 */
+	if (new->thread.regs && last_task_used_spe == new)
+		new->thread.regs->msr |= MSR_SPE;
+#endif /* CONFIG_SPE */
+#endif /* CONFIG_SMP */
+
+#ifdef CONFIG_PPC64	/* for now */
+	if (unlikely(__get_cpu_var(current_dabr) != new->thread.dabr)) {
+		set_dabr(new->thread.dabr);
+		__get_cpu_var(current_dabr) = new->thread.dabr;
+	}
+#endif
+
+	new_thread = &new->thread;
+	old_thread = &current->thread;
+	local_irq_save(flags);
+	last = _switch(old_thread, new_thread);
+
+	local_irq_restore(flags);
+
+	return last;
+}
+
+void show_regs(struct pt_regs * regs)
+{
+	int i, trap;
+
+	printk("NIP: %08lX LR: %08lX SP: %08lX REGS: %p TRAP: %04lx    %s\n",
+	       regs->nip, regs->link, regs->gpr[1], regs, regs->trap,
+	       print_tainted());
+	printk("MSR: %08lx EE: %01x PR: %01x FP: %01x ME: %01x IR/DR: %01x%01x\n",
+	       regs->msr, regs->msr&MSR_EE ? 1 : 0, regs->msr&MSR_PR ? 1 : 0,
+	       regs->msr & MSR_FP ? 1 : 0,regs->msr&MSR_ME ? 1 : 0,
+	       regs->msr&MSR_IR ? 1 : 0,
+	       regs->msr&MSR_DR ? 1 : 0);
+	trap = TRAP(regs);
+	if (trap == 0x300 || trap == 0x600)
+		printk("DAR: %08lX, DSISR: %08lX\n", regs->dar, regs->dsisr);
+	printk("TASK = %p[%d] '%s' THREAD: %p\n",
+	       current, current->pid, current->comm, current->thread_info);
+	printk("Last syscall: %ld ", current->thread.last_syscall);
+
+#ifdef CONFIG_SMP
+	printk(" CPU: %d", smp_processor_id());
+#endif /* CONFIG_SMP */
+
+	for (i = 0;  i < 32;  i++) {
+		long r;
+		if ((i % 8) == 0)
+			printk("\n" KERN_INFO "GPR%02d: ", i);
+		if (__get_user(r, &regs->gpr[i]))
+			break;
+		printk("%08lX ", r);
+		if (i == 12 && !FULL_REGS(regs))
+			break;
+	}
+	printk("\n");
+#ifdef CONFIG_KALLSYMS
+	/*
+	 * Lookup NIP late so we have the best change of getting the
+	 * above info out without failing
+	 */
+	printk("NIP [%08lx] ", regs->nip);
+	print_symbol("%s\n", regs->nip);
+	printk("LR [%08lx] ", regs->link);
+	print_symbol("%s\n", regs->link);
+#endif
+	show_stack(current, (unsigned long *) regs->gpr[1]);
+}
+
+void exit_thread(void)
+{
+#ifndef CONFIG_SMP
+	if (last_task_used_math == current)
+		last_task_used_math = NULL;
+#ifdef CONFIG_ALTIVEC
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = NULL;
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_SPE
+	if (last_task_used_spe == current)
+		last_task_used_spe = NULL;
+#endif
+#endif /* CONFIG_SMP */
+}
+
+void flush_thread(void)
+{
+#ifndef CONFIG_SMP
+	if (last_task_used_math == current)
+		last_task_used_math = NULL;
+#ifdef CONFIG_ALTIVEC
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = NULL;
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_SPE
+	if (last_task_used_spe == current)
+		last_task_used_spe = NULL;
+#endif
+#endif /* CONFIG_SMP */
+
+#ifdef CONFIG_PPC64	/* for now */
+	if (current->thread.dabr) {
+		current->thread.dabr = 0;
+		set_dabr(0);
+	}
+#endif
+}
+
+void
+release_thread(struct task_struct *t)
+{
+}
+
+/*
+ * This gets called before we allocate a new thread and copy
+ * the current task into it.
+ */
+void prepare_to_copy(struct task_struct *tsk)
+{
+	flush_fp_to_thread(current);
+	flush_altivec_to_thread(current);
+	flush_spe_to_thread(current);
+}
+
+/*
+ * Copy a thread..
+ */
+int
+copy_thread(int nr, unsigned long clone_flags, unsigned long usp,
+	    unsigned long unused,
+	    struct task_struct *p, struct pt_regs *regs)
+{
+	struct pt_regs *childregs, *kregs;
+	extern void ret_from_fork(void);
+	unsigned long sp = (unsigned long)p->thread_info + THREAD_SIZE;
+	unsigned long childframe;
+
+	CHECK_FULL_REGS(regs);
+	/* Copy registers */
+	sp -= sizeof(struct pt_regs);
+	childregs = (struct pt_regs *) sp;
+	*childregs = *regs;
+	if ((childregs->msr & MSR_PR) == 0) {
+		/* for kernel thread, set `current' and stackptr in new task */
+		childregs->gpr[1] = sp + sizeof(struct pt_regs);
+		childregs->gpr[2] = (unsigned long) p;
+		p->thread.regs = NULL;	/* no user register state */
+	} else {
+		childregs->gpr[1] = usp;
+		p->thread.regs = childregs;
+		if (clone_flags & CLONE_SETTLS)
+			childregs->gpr[2] = childregs->gpr[6];
+	}
+	childregs->gpr[3] = 0;  /* Result from fork() */
+	sp -= STACK_FRAME_OVERHEAD;
+	childframe = sp;
+
+	/*
+	 * The way this works is that at some point in the future
+	 * some task will call _switch to switch to the new task.
+	 * That will pop off the stack frame created below and start
+	 * the new task running at ret_from_fork.  The new task will
+	 * do some house keeping and then return from the fork or clone
+	 * system call, using the stack frame created above.
+	 */
+	sp -= sizeof(struct pt_regs);
+	kregs = (struct pt_regs *) sp;
+	sp -= STACK_FRAME_OVERHEAD;
+	p->thread.ksp = sp;
+	kregs->nip = (unsigned long)ret_from_fork;
+
+	p->thread.last_syscall = -1;
+
+	return 0;
+}
+
+/*
+ * Set up a thread for executing a new program
+ */
+void start_thread(struct pt_regs *regs, unsigned long nip, unsigned long sp)
+{
+	set_fs(USER_DS);
+	memset(regs->gpr, 0, sizeof(regs->gpr));
+	regs->ctr = 0;
+	regs->link = 0;
+	regs->xer = 0;
+	regs->ccr = 0;
+	regs->mq = 0;
+	regs->nip = nip;
+	regs->gpr[1] = sp;
+	regs->msr = MSR_USER;
+#ifndef CONFIG_SMP
+	if (last_task_used_math == current)
+		last_task_used_math = NULL;
+#ifdef CONFIG_ALTIVEC
+	if (last_task_used_altivec == current)
+		last_task_used_altivec = NULL;
+#endif
+#ifdef CONFIG_SPE
+	if (last_task_used_spe == current)
+		last_task_used_spe = NULL;
+#endif
+#endif /* CONFIG_SMP */
+	memset(current->thread.fpr, 0, sizeof(current->thread.fpr));
+	current->thread.fpscr = 0;
+#ifdef CONFIG_ALTIVEC
+	memset(current->thread.vr, 0, sizeof(current->thread.vr));
+	memset(&current->thread.vscr, 0, sizeof(current->thread.vscr));
+	current->thread.vrsave = 0;
+	current->thread.used_vr = 0;
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_SPE
+	memset(current->thread.evr, 0, sizeof(current->thread.evr));
+	current->thread.acc = 0;
+	current->thread.spefscr = 0;
+	current->thread.used_spe = 0;
+#endif /* CONFIG_SPE */
+}
+
+#define PR_FP_ALL_EXCEPT (PR_FP_EXC_DIV | PR_FP_EXC_OVF | PR_FP_EXC_UND \
+		| PR_FP_EXC_RES | PR_FP_EXC_INV)
+
+int set_fpexc_mode(struct task_struct *tsk, unsigned int val)
+{
+	struct pt_regs *regs = tsk->thread.regs;
+
+	/* This is a bit hairy.  If we are an SPE enabled  processor
+	 * (have embedded fp) we store the IEEE exception enable flags in
+	 * fpexc_mode.  fpexc_mode is also used for setting FP exception
+	 * mode (asyn, precise, disabled) for 'Classic' FP. */
+	if (val & PR_FP_EXC_SW_ENABLE) {
+#ifdef CONFIG_SPE
+		tsk->thread.fpexc_mode = val &
+			(PR_FP_EXC_SW_ENABLE | PR_FP_ALL_EXCEPT);
+#else
+		return -EINVAL;
+#endif
+	} else {
+		/* on a CONFIG_SPE this does not hurt us.  The bits that
+		 * __pack_fe01 use do not overlap with bits used for
+		 * PR_FP_EXC_SW_ENABLE.  Additionally, the MSR[FE0,FE1] bits
+		 * on CONFIG_SPE implementations are reserved so writing to
+		 * them does not change anything */
+		if (val > PR_FP_EXC_PRECISE)
+			return -EINVAL;
+		tsk->thread.fpexc_mode = __pack_fe01(val);
+		if (regs != NULL && (regs->msr & MSR_FP) != 0)
+			regs->msr = (regs->msr & ~(MSR_FE0|MSR_FE1))
+				| tsk->thread.fpexc_mode;
+	}
+	return 0;
+}
+
+int get_fpexc_mode(struct task_struct *tsk, unsigned long adr)
+{
+	unsigned int val;
+
+	if (tsk->thread.fpexc_mode & PR_FP_EXC_SW_ENABLE)
+#ifdef CONFIG_SPE
+		val = tsk->thread.fpexc_mode;
+#else
+		return -EINVAL;
+#endif
+	else
+		val = __unpack_fe01(tsk->thread.fpexc_mode);
+	return put_user(val, (unsigned int __user *) adr);
+}
+
+int sys_clone(unsigned long clone_flags, unsigned long usp,
+	      int __user *parent_tidp, void __user *child_threadptr,
+	      int __user *child_tidp, int p6,
+	      struct pt_regs *regs)
+{
+	CHECK_FULL_REGS(regs);
+	if (usp == 0)
+		usp = regs->gpr[1];	/* stack pointer for child */
+ 	return do_fork(clone_flags, usp, regs, 0, parent_tidp, child_tidp);
+}
+
+int sys_fork(unsigned long p1, unsigned long p2, unsigned long p3,
+	     unsigned long p4, unsigned long p5, unsigned long p6,
+	     struct pt_regs *regs)
+{
+	CHECK_FULL_REGS(regs);
+	return do_fork(SIGCHLD, regs->gpr[1], regs, 0, NULL, NULL);
+}
+
+int sys_vfork(unsigned long p1, unsigned long p2, unsigned long p3,
+	      unsigned long p4, unsigned long p5, unsigned long p6,
+	      struct pt_regs *regs)
+{
+	CHECK_FULL_REGS(regs);
+	return do_fork(CLONE_VFORK | CLONE_VM | SIGCHLD, regs->gpr[1],
+			regs, 0, NULL, NULL);
+}
+
+int sys_execve(unsigned long a0, unsigned long a1, unsigned long a2,
+	       unsigned long a3, unsigned long a4, unsigned long a5,
+	       struct pt_regs *regs)
+{
+	int error;
+	char * filename;
+
+	filename = getname((char __user *) a0);
+	error = PTR_ERR(filename);
+	if (IS_ERR(filename))
+		goto out;
+	flush_fp_to_thread(current);
+	flush_altivec_to_thread(current);
+	flush_spe_to_thread(current);
+	if (error == 0) {
+		task_lock(current);
+		current->ptrace &= ~PT_DTRACE;
+		task_unlock(current);
+	}
+	putname(filename);
+out:
+	return error;
+}
+
+static int validate_sp(unsigned long sp, struct task_struct *p,
+		       unsigned long nbytes)
+{
+	unsigned long stack_page = (unsigned long)p->thread_info;
+
+	if (sp >= stack_page + sizeof(struct thread_struct)
+	    && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+
+#ifdef CONFIG_IRQSTACKS
+	stack_page = (unsigned long) hardirq_ctx[task_cpu(p)];
+	if (sp >= stack_page + sizeof(struct thread_struct)
+	    && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+
+	stack_page = (unsigned long) softirq_ctx[task_cpu(p)];
+	if (sp >= stack_page + sizeof(struct thread_struct)
+	    && sp <= stack_page + THREAD_SIZE - nbytes)
+		return 1;
+#endif
+
+	return 0;
+}
+
+void dump_stack(void)
+{
+	show_stack(current, NULL);
+}
+
+EXPORT_SYMBOL(dump_stack);
+
+void show_stack(struct task_struct *tsk, unsigned long *stack)
+{
+	unsigned long sp, stack_top, prev_sp, ret;
+	int count = 0;
+	unsigned long next_exc = 0;
+	struct pt_regs *regs;
+	extern char ret_from_except, ret_from_except_full, ret_from_syscall;
+
+	sp = (unsigned long) stack;
+	if (tsk == NULL)
+		tsk = current;
+	if (sp == 0) {
+		if (tsk == current)
+			asm("mr %0,1" : "=r" (sp));
+		else
+			sp = tsk->thread.ksp;
+	}
+
+	prev_sp = (unsigned long) (tsk->thread_info + 1);
+	stack_top = (unsigned long) tsk->thread_info + THREAD_SIZE;
+	while (count < 16 && sp > prev_sp && sp < stack_top && (sp & 3) == 0) {
+		if (count == 0) {
+			printk("Call trace:");
+#ifdef CONFIG_KALLSYMS
+			printk("\n");
+#endif
+		} else {
+			if (next_exc) {
+				ret = next_exc;
+				next_exc = 0;
+			} else
+				ret = *(unsigned long *)(sp + 4);
+			printk(" [%08lx] ", ret);
+#ifdef CONFIG_KALLSYMS
+			print_symbol("%s", ret);
+			printk("\n");
+#endif
+			if (ret == (unsigned long) &ret_from_except
+			    || ret == (unsigned long) &ret_from_except_full
+			    || ret == (unsigned long) &ret_from_syscall) {
+				/* sp + 16 points to an exception frame */
+				regs = (struct pt_regs *) (sp + 16);
+				if (sp + 16 + sizeof(*regs) <= stack_top)
+					next_exc = regs->nip;
+			}
+		}
+		++count;
+		sp = *(unsigned long *)sp;
+	}
+#ifndef CONFIG_KALLSYMS
+	if (count > 0)
+		printk("\n");
+#endif
+}
+
+unsigned long get_wchan(struct task_struct *p)
+{
+	unsigned long ip, sp;
+	int count = 0;
+
+	if (!p || p == current || p->state == TASK_RUNNING)
+		return 0;
+
+	sp = p->thread.ksp;
+	if (!validate_sp(sp, p, 16))
+		return 0;
+
+	do {
+		sp = *(unsigned long *)sp;
+		if (!validate_sp(sp, p, 16))
+			return 0;
+		if (count > 0) {
+			ip = *(unsigned long *)(sp + 4);
+			if (!in_sched_functions(ip))
+				return ip;
+		}
+	} while (count++ < 16);
+	return 0;
+}
+EXPORT_SYMBOL(get_wchan);
