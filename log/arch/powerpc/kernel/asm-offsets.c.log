commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9b9cde07e396..6657dc6b2336 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -30,7 +30,6 @@
 
 #include <asm/io.h>
 #include <asm/page.h>
-#include <asm/pgtable.h>
 #include <asm/processor.h>
 #include <asm/cputable.h>
 #include <asm/thread_info.h>

commit 4eff2b4f32a309e2171bfe53db3e93b5614f77cb
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Wed May 6 13:40:23 2020 +1000

    powerpc/xmon: Move breakpoints to text section
    
    The instructions for xmon's breakpoint are stored bpt_table[] which is in
    the data section. This is problematic as the data section may be marked
    as no execute. Move bpt_table[] to the text section.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-4-jniethe5@gmail.com

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index fcf24a365fc0..9b9cde07e396 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -70,6 +70,10 @@
 #include <asm/fixmap.h>
 #endif
 
+#ifdef CONFIG_XMON
+#include "../xmon/xmon_bpts.h"
+#endif
+
 #define STACK_PT_REGS_OFFSET(sym, val)	\
 	DEFINE(sym, STACK_FRAME_OVERHEAD + offsetof(struct pt_regs, val))
 
@@ -795,5 +799,9 @@ int main(void)
 	DEFINE(VIRT_IMMR_BASE, (u64)__fix_to_virt(FIX_IMMR_BASE));
 #endif
 
+#ifdef CONFIG_XMON
+	DEFINE(BPT_SIZE, BPT_SIZE);
+#endif
+
 	return 0;
 }

commit 232ca1eecafed8c54491017f0612c33d8c742d74
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Feb 15 10:14:25 2020 +0000

    powerpc/32s: Fix DSI and ISI exceptions for CONFIG_VMAP_STACK
    
    hash_page() needs to read page tables from kernel memory. When entire
    kernel memory is mapped by BATs, which is normally the case when
    CONFIG_STRICT_KERNEL_RWX is not set, it works even if the page hosting
    the page table is not referenced in the MMU hash table.
    
    However, if the page where the page table resides is not covered by
    a BAT, a DSI fault can be encountered from hash_page(), and it loops
    forever. This can happen when CONFIG_STRICT_KERNEL_RWX is selected
    and the alignment of the different regions is too small to allow
    covering the entire memory with BATs. This also happens when
    CONFIG_DEBUG_PAGEALLOC is selected or when booting with 'nobats'
    flag.
    
    Also, if the page containing the kernel stack is not present in the
    MMU hash table, registers cannot be saved and a recursive DSI fault
    is encountered.
    
    To allow hash_page() to properly do its job at all time and load the
    MMU hash table whenever needed, it must run with data MMU disabled.
    This means it must be called before re-enabling data MMU. To allow
    this, registers clobbered by hash_page() and create_hpte() have to
    be saved in the thread struct together with SRR0, SSR1, DAR and DSISR.
    It is also necessary to ensure that DSI prolog doesn't overwrite
    regs saved by prolog of the current running exception. That means:
    - DSI can only use SPRN_SPRG_SCRATCH0
    - Exceptions must free SPRN_SPRG_SCRATCH0 before writing to the stack.
    
    This also fixes the Oops reported by Erhard when create_hpte() is
    called by add_hash_page().
    
    Due to prolog size increase, a few more exceptions had to get split
    in two parts.
    
    Fixes: cd08f109e262 ("powerpc/32s: Enable CONFIG_VMAP_STACK")
    Reported-by: Erhard F. <erhard_f@mailbox.org>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Tested-by: Erhard F. <erhard_f@mailbox.org>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=206501
    Link: https://lore.kernel.org/r/64a4aa44686e9fd4b01333401367029771d9b231.1581761633.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c25e562f1cd9..fcf24a365fc0 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -132,6 +132,18 @@ int main(void)
 	OFFSET(SRR1, thread_struct, srr1);
 	OFFSET(DAR, thread_struct, dar);
 	OFFSET(DSISR, thread_struct, dsisr);
+#ifdef CONFIG_PPC_BOOK3S_32
+	OFFSET(THR0, thread_struct, r0);
+	OFFSET(THR3, thread_struct, r3);
+	OFFSET(THR4, thread_struct, r4);
+	OFFSET(THR5, thread_struct, r5);
+	OFFSET(THR6, thread_struct, r6);
+	OFFSET(THR8, thread_struct, r8);
+	OFFSET(THR9, thread_struct, r9);
+	OFFSET(THR11, thread_struct, r11);
+	OFFSET(THLR, thread_struct, lr);
+	OFFSET(THCTR, thread_struct, ctr);
+#endif
 #endif
 #ifdef CONFIG_SPE
 	OFFSET(THREAD_EVR0, thread_struct, evr[0]);

commit 028474876f472c3b6eee633aed528a1206609657
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Dec 21 08:32:27 2019 +0000

    powerpc/32: prepare for CONFIG_VMAP_STACK
    
    To support CONFIG_VMAP_STACK, the kernel has to activate Data MMU
    Translation for accessing the stack. Before doing that it must save
    SRR0, SRR1 and also DAR and DSISR when relevant, in order to not
    loose them in case there is a Data TLB Miss once the translation is
    reactivated.
    
    This patch adds fields in thread struct for saving those registers.
    It prepares entry_32.S to handle exception entry with
    Data MMU Translation enabled and alters EXCEPTION_PROLOG macros to
    save SRR0, SRR1, DAR and DSISR then reenables Data MMU.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/a775a1fea60f190e0f63503463fb775310a2009b.1576916812.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 90e53d432f2e..c25e562f1cd9 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -127,6 +127,12 @@ int main(void)
 	OFFSET(KSP_VSID, thread_struct, ksp_vsid);
 #else /* CONFIG_PPC64 */
 	OFFSET(PGDIR, thread_struct, pgdir);
+#ifdef CONFIG_VMAP_STACK
+	OFFSET(SRR0, thread_struct, srr0);
+	OFFSET(SRR1, thread_struct, srr1);
+	OFFSET(DAR, thread_struct, dar);
+	OFFSET(DSISR, thread_struct, dsisr);
+#endif
 #ifdef CONFIG_SPE
 	OFFSET(THREAD_EVR0, thread_struct, evr[0]);
 	OFFSET(THREAD_ACC, thread_struct, acc);

commit e33ffc956b086675551b86f5f83cf50ca47aa71c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Dec 2 07:57:33 2019 +0000

    powerpc/vdso32: implement clock_getres entirely
    
    clock_getres returns hrtimer_res for all clocks but coarse ones
    for which it returns KTIME_LOW_RES.
    
    return EINVAL for unknown clocks.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/37f94e47c91070b7606fb3ec3fe6fd2302a475a0.1575273217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0013197d89a6..90e53d432f2e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -413,7 +413,10 @@ int main(void)
 	DEFINE(CLOCK_MONOTONIC, CLOCK_MONOTONIC);
 	DEFINE(CLOCK_REALTIME_COARSE, CLOCK_REALTIME_COARSE);
 	DEFINE(CLOCK_MONOTONIC_COARSE, CLOCK_MONOTONIC_COARSE);
+	DEFINE(CLOCK_MAX, CLOCK_TAI);
 	DEFINE(NSEC_PER_SEC, NSEC_PER_SEC);
+	DEFINE(EINVAL, EINVAL);
+	DEFINE(KTIME_LOW_RES, KTIME_LOW_RES);
 
 #ifdef CONFIG_BUG
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));

commit 2c29eef9fc6f61cfb81b54683be8f116dac80e7a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Dec 2 07:57:31 2019 +0000

    powerpc/vdso32: Don't read cache line size from the datapage on PPC32.
    
    On PPC32, the cache lines have a fixed size known at build time.
    
    Don't read it from the datapage.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/dfa7b35e27e01964fcda84bf1ed8b2b31cf93826.1575273217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 3d47aec7becf..0013197d89a6 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -389,11 +389,11 @@ int main(void)
 	OFFSET(STAMP_XTIME_NSEC, vdso_data, stamp_xtime_nsec);
 	OFFSET(STAMP_SEC_FRAC, vdso_data, stamp_sec_fraction);
 	OFFSET(CLOCK_HRTIMER_RES, vdso_data, hrtimer_res);
+#ifdef CONFIG_PPC64
 	OFFSET(CFG_ICACHE_BLOCKSZ, vdso_data, icache_block_size);
 	OFFSET(CFG_DCACHE_BLOCKSZ, vdso_data, dcache_block_size);
 	OFFSET(CFG_ICACHE_LOGBLOCKSZ, vdso_data, icache_log_block_size);
 	OFFSET(CFG_DCACHE_LOGBLOCKSZ, vdso_data, dcache_log_block_size);
-#ifdef CONFIG_PPC64
 	OFFSET(CFG_SYSCALL_MAP64, vdso_data, syscall_map_64);
 	OFFSET(TVAL64_TV_SEC, __kernel_old_timeval, tv_sec);
 	OFFSET(TVAL64_TV_USEC, __kernel_old_timeval, tv_usec);

commit 552263456215ada7ee8700ce022d12b0cffe4802
Author: Vincenzo Frascino <vincenzo.frascino@arm.com>
Date:   Mon Dec 2 07:57:29 2019 +0000

    powerpc: Fix vDSO clock_getres()
    
    clock_getres in the vDSO library has to preserve the same behaviour
    of posix_get_hrtimer_res().
    
    In particular, posix_get_hrtimer_res() does:
        sec = 0;
        ns = hrtimer_resolution;
    and hrtimer_resolution depends on the enablement of the high
    resolution timers that can happen either at compile or at run time.
    
    Fix the powerpc vdso implementation of clock_getres keeping a copy of
    hrtimer_resolution in vdso data and using that directly.
    
    Fixes: a7f290dad32e ("[PATCH] powerpc: Merge vdso's and add vdso support to 32 bits kernel")
    Cc: stable@vger.kernel.org
    Signed-off-by: Vincenzo Frascino <vincenzo.frascino@arm.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Shuah Khan <skhan@linuxfoundation.org>
    [chleroy: changed CLOCK_REALTIME_RES to CLOCK_HRTIMER_RES]
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/a55eca3a5e85233838c2349783bcb5164dae1d09.1575273217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index f22bd6d1fe93..3d47aec7becf 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -388,6 +388,7 @@ int main(void)
 	OFFSET(STAMP_XTIME_SEC, vdso_data, stamp_xtime_sec);
 	OFFSET(STAMP_XTIME_NSEC, vdso_data, stamp_xtime_nsec);
 	OFFSET(STAMP_SEC_FRAC, vdso_data, stamp_sec_fraction);
+	OFFSET(CLOCK_HRTIMER_RES, vdso_data, hrtimer_res);
 	OFFSET(CFG_ICACHE_BLOCKSZ, vdso_data, icache_block_size);
 	OFFSET(CFG_DCACHE_BLOCKSZ, vdso_data, dcache_block_size);
 	OFFSET(CFG_ICACHE_LOGBLOCKSZ, vdso_data, icache_log_block_size);
@@ -413,7 +414,6 @@ int main(void)
 	DEFINE(CLOCK_REALTIME_COARSE, CLOCK_REALTIME_COARSE);
 	DEFINE(CLOCK_MONOTONIC_COARSE, CLOCK_MONOTONIC_COARSE);
 	DEFINE(NSEC_PER_SEC, NSEC_PER_SEC);
-	DEFINE(CLOCK_REALTIME_RES, MONOTONIC_RES_NSEC);
 
 #ifdef CONFIG_BUG
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));

commit 176ed98c8a76ee08babf99b25b00992c2a5e7bbc
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sun Oct 27 17:26:55 2019 +0100

    y2038: vdso: powerpc: avoid timespec references
    
    As a preparation to stop using 'struct timespec' in the kernel,
    change the powerpc vdso implementation:
    
    - split up the vdso data definition to have equivalent members
       for seconds and nanoseconds instead of an xtime structure
    
    - use timespec64 as an intermediate for the xtime update
    
    - change the asm-offsets definition to be based the appropriate
      fixed-length types
    
    This is only a temporary fix for changing the types, in order
    to actually support a 64-bit safe vdso32 version of clock_gettime(),
    the entire powerpc vdso should be replaced with the generic
    lib/vdso/ implementation. If that happens first, this patch
    becomes obsolete.
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 827f4c354e13..f22bd6d1fe93 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -385,7 +385,8 @@ int main(void)
 	OFFSET(CFG_SYSCALL_MAP32, vdso_data, syscall_map_32);
 	OFFSET(WTOM_CLOCK_SEC, vdso_data, wtom_clock_sec);
 	OFFSET(WTOM_CLOCK_NSEC, vdso_data, wtom_clock_nsec);
-	OFFSET(STAMP_XTIME, vdso_data, stamp_xtime);
+	OFFSET(STAMP_XTIME_SEC, vdso_data, stamp_xtime_sec);
+	OFFSET(STAMP_XTIME_NSEC, vdso_data, stamp_xtime_nsec);
 	OFFSET(STAMP_SEC_FRAC, vdso_data, stamp_sec_fraction);
 	OFFSET(CFG_ICACHE_BLOCKSZ, vdso_data, icache_block_size);
 	OFFSET(CFG_DCACHE_BLOCKSZ, vdso_data, dcache_block_size);
@@ -395,18 +396,13 @@ int main(void)
 	OFFSET(CFG_SYSCALL_MAP64, vdso_data, syscall_map_64);
 	OFFSET(TVAL64_TV_SEC, __kernel_old_timeval, tv_sec);
 	OFFSET(TVAL64_TV_USEC, __kernel_old_timeval, tv_usec);
+#endif
+	OFFSET(TSPC64_TV_SEC, __kernel_timespec, tv_sec);
+	OFFSET(TSPC64_TV_NSEC, __kernel_timespec, tv_nsec);
 	OFFSET(TVAL32_TV_SEC, old_timeval32, tv_sec);
 	OFFSET(TVAL32_TV_USEC, old_timeval32, tv_usec);
-	OFFSET(TSPC64_TV_SEC, timespec, tv_sec);
-	OFFSET(TSPC64_TV_NSEC, timespec, tv_nsec);
 	OFFSET(TSPC32_TV_SEC, old_timespec32, tv_sec);
 	OFFSET(TSPC32_TV_NSEC, old_timespec32, tv_nsec);
-#else
-	OFFSET(TVAL32_TV_SEC, __kernel_old_timeval, tv_sec);
-	OFFSET(TVAL32_TV_USEC, __kernel_old_timeval, tv_usec);
-	OFFSET(TSPC32_TV_SEC, timespec, tv_sec);
-	OFFSET(TSPC32_TV_NSEC, timespec, tv_nsec);
-#endif
 	/* timeval/timezone offsets for use by vdso */
 	OFFSET(TZONE_TZ_MINWEST, timezone, tz_minuteswest);
 	OFFSET(TZONE_TZ_DSTTIME, timezone, tz_dsttime);

commit ddccf40fe82b7ac7c44b186ec4b6d1d1bbc2cbff
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Thu Nov 23 14:29:37 2017 +0100

    y2038: vdso: change timeval to __kernel_old_timeval
    
    The gettimeofday() function in vdso uses the traditional 'timeval'
    structure layout, which will be incompatible with future versions of
    glibc on 32-bit architectures that use a 64-bit time_t.
    
    This interface is problematic for y2038, when time_t overflows on 32-bit
    architectures, but the plan so far is that a libc with 64-bit time_t
    will not call into the gettimeofday() vdso helper at all, and only
    have a method for entering clock_gettime().  This means we don't have
    to fix it here, though we probably want to add a new clock_gettime()
    entry point using a 64-bit version of 'struct timespec' at some point.
    
    Changing the vdso code to use __kernel_old_timeval helps isolate
    this usage from the other ones that still need to be fixed properly,
    and it gets us closer to removing the 'timeval' definition from the
    kernel sources.
    
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 484f54dab247..827f4c354e13 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -393,8 +393,8 @@ int main(void)
 	OFFSET(CFG_DCACHE_LOGBLOCKSZ, vdso_data, dcache_log_block_size);
 #ifdef CONFIG_PPC64
 	OFFSET(CFG_SYSCALL_MAP64, vdso_data, syscall_map_64);
-	OFFSET(TVAL64_TV_SEC, timeval, tv_sec);
-	OFFSET(TVAL64_TV_USEC, timeval, tv_usec);
+	OFFSET(TVAL64_TV_SEC, __kernel_old_timeval, tv_sec);
+	OFFSET(TVAL64_TV_USEC, __kernel_old_timeval, tv_usec);
 	OFFSET(TVAL32_TV_SEC, old_timeval32, tv_sec);
 	OFFSET(TVAL32_TV_USEC, old_timeval32, tv_usec);
 	OFFSET(TSPC64_TV_SEC, timespec, tv_sec);
@@ -402,8 +402,8 @@ int main(void)
 	OFFSET(TSPC32_TV_SEC, old_timespec32, tv_sec);
 	OFFSET(TSPC32_TV_NSEC, old_timespec32, tv_nsec);
 #else
-	OFFSET(TVAL32_TV_SEC, timeval, tv_sec);
-	OFFSET(TVAL32_TV_USEC, timeval, tv_usec);
+	OFFSET(TVAL32_TV_SEC, __kernel_old_timeval, tv_sec);
+	OFFSET(TVAL32_TV_USEC, __kernel_old_timeval, tv_usec);
 	OFFSET(TSPC32_TV_SEC, timespec, tv_sec);
 	OFFSET(TSPC32_TV_NSEC, timespec, tv_nsec);
 #endif

commit 6c85b7bc637b64e681760f62c0eafba2f56745c6
Author: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
Date:   Thu Aug 22 00:48:38 2019 -0300

    powerpc/kvm: Use UV_RETURN ucall to return to ultravisor
    
    When an SVM makes an hypercall or incurs some other exception, the
    Ultravisor usually forwards (a.k.a. reflects) the exceptions to the
    Hypervisor. After processing the exception, Hypervisor uses the
    UV_RETURN ultracall to return control back to the SVM.
    
    The expected register state on entry to this ultracall is:
    
    * Non-volatile registers are restored to their original values.
    * If returning from an hypercall, register R0 contains the return value
      (unlike other ultracalls) and, registers R4 through R12 contain any
      output values of the hypercall.
    * R3 contains the ultracall number, i.e UV_RETURN.
    * If returning with a synthesized interrupt, R2 contains the
      synthesized interrupt number.
    
    Thanks to input from Paul Mackerras, Ram Pai and Mike Anderson.
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Claudio Carvalho <cclaudio@linux.ibm.com>
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190822034838.27876-8-cclaudio@linux.ibm.com

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4ccb6b3a7fbd..484f54dab247 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -506,6 +506,7 @@ int main(void)
 	OFFSET(KVM_VRMA_SLB_V, kvm, arch.vrma_slb_v);
 	OFFSET(KVM_RADIX, kvm, arch.radix);
 	OFFSET(KVM_FWNMI, kvm, arch.fwnmi_enabled);
+	OFFSET(KVM_SECURE_GUEST, kvm, arch.secure_guest);
 	OFFSET(VCPU_DSISR, kvm_vcpu, arch.shregs.dsisr);
 	OFFSET(VCPU_DAR, kvm_vcpu, arch.shregs.dar);
 	OFFSET(VCPU_VPA, kvm_vcpu, arch.vpa.pinned_addr);

commit 192f0f8e9db7efe4ac98d47f5fa4334e43c1204d
Merge: ec9249752465 f5a9e488d623
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 13 16:08:36 2019 -0700

    Merge tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Removal of the NPU DMA code, used by the out-of-tree Nvidia driver,
         as well as some other functions only used by drivers that haven't
         (yet?) made it upstream.
    
       - A fix for a bug in our handling of hardware watchpoints (eg. perf
         record -e mem: ...) which could lead to register corruption and
         kernel crashes.
    
       - Enable HAVE_ARCH_HUGE_VMAP, which allows us to use large pages for
         vmalloc when using the Radix MMU.
    
       - A large but incremental rewrite of our exception handling code to
         use gas macros rather than multiple levels of nested CPP macros.
    
      And the usual small fixes, cleanups and improvements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Andreas Schwab,
      Aneesh Kumar K.V, Anju T Sudhakar, Anton Blanchard, Arnd Bergmann,
      Athira Rajeev, Cédric Le Goater, Christian Lamparter, Christophe
      Leroy, Christophe Lombard, Christoph Hellwig, Daniel Axtens, Denis
      Efremov, Enrico Weigelt, Frederic Barrat, Gautham R. Shenoy, Geert
      Uytterhoeven, Geliang Tang, Gen Zhang, Greg Kroah-Hartman, Greg Kurz,
      Gustavo Romero, Krzysztof Kozlowski, Madhavan Srinivasan, Masahiro
      Yamada, Mathieu Malaterre, Michael Neuling, Nathan Lynch, Naveen N.
      Rao, Nicholas Piggin, Nishad Kamdar, Oliver O'Halloran, Qian Cai, Ravi
      Bangoria, Sachin Sant, Sam Bobroff, Satheesh Rajendran, Segher
      Boessenkool, Shaokun Zhang, Shawn Anastasio, Stewart Smith, Suraj
      Jitindar Singh, Thiago Jung Bauermann, YueHaibing"
    
    * tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (163 commits)
      powerpc/powernv/idle: Fix restore of SPRN_LDBAR for POWER9 stop state.
      powerpc/eeh: Handle hugepages in ioremap space
      ocxl: Update for AFU descriptor template version 1.1
      powerpc/boot: pass CONFIG options in a simpler and more robust way
      powerpc/boot: add {get, put}_unaligned_be32 to xz_config.h
      powerpc/irq: Don't WARN continuously in arch_local_irq_restore()
      powerpc/module64: Use symbolic instructions names.
      powerpc/module32: Use symbolic instructions names.
      powerpc: Move PPC_HA() PPC_HI() and PPC_LO() to ppc-opcode.h
      powerpc/module64: Fix comment in R_PPC64_ENTRY handling
      powerpc/boot: Add lzo support for uImage
      powerpc/boot: Add lzma support for uImage
      powerpc/boot: don't force gzipped uImage
      powerpc/8xx: Add microcode patch to move SMC parameter RAM.
      powerpc/8xx: Use IO accessors in microcode programming.
      powerpc/8xx: replace #ifdefs by IS_ENABLED() in microcode.c
      powerpc/8xx: refactor programming of microcode CPM params.
      powerpc/8xx: refactor printing of microcode patch name.
      powerpc/8xx: Refactor microcode write
      powerpc/8xx: refactor writing of CPM microcode arrays
      ...

commit 0a882e28468f48ab3d9a36dde0a5723ea29ed1ed
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jun 28 16:33:18 2019 +1000

    powerpc/64s/exception: remove bad stack branch
    
    The bad stack test in interrupt handlers has a few problems. For
    performance it is taken in the common case, which is a fetch bubble
    and a waste of i-cache.
    
    For code development and maintainence, it requires yet another stack
    frame setup routine, and that constrains all exception handlers to
    follow the same register save pattern which inhibits future
    optimisation.
    
    Remove the test/branch and replace it with a trap. Teach the program
    check handler to use the emergency stack for this case.
    
    This does not result in quite so nice a message, however the SRR0 and
    SRR1 of the crashed interrupt can be seen in r11 and r12, as is the
    original r1 (adjusted by INT_FRAME_SIZE). These are the most important
    parts to debugging the issue.
    
    The original r9-12 and cr0 is lost, which is the main downside.
    
      kernel BUG at linux/arch/powerpc/kernel/exceptions-64s.S:847!
      Oops: Exception in kernel mode, sig: 5 [#1]
      BE SMP NR_CPUS=2048 NUMA PowerNV
      Modules linked in:
      CPU: 0 PID: 1 Comm: swapper/0 Not tainted
      NIP:  c000000000009108 LR: c000000000cadbcc CTR: c0000000000090f0
      REGS: c0000000fffcbd70 TRAP: 0700   Not tainted
      MSR:  9000000000021032 <SF,HV,ME,IR,DR,RI>  CR: 28222448  XER: 20040000
      CFAR: c000000000009100 IRQMASK: 0
      GPR00: 000000000000003d fffffffffffffd00 c0000000018cfb00 c0000000f02b3166
      GPR04: fffffffffffffffd 0000000000000007 fffffffffffffffb 0000000000000030
      GPR08: 0000000000000037 0000000028222448 0000000000000000 c000000000ca8de0
      GPR12: 9000000002009032 c000000001ae0000 c000000000010a00 0000000000000000
      GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR20: c0000000f00322c0 c000000000f85200 0000000000000004 ffffffffffffffff
      GPR24: fffffffffffffffe 0000000000000000 0000000000000000 000000000000000a
      GPR28: 0000000000000000 0000000000000000 c0000000f02b391c c0000000f02b3167
      NIP [c000000000009108] decrementer_common+0x18/0x160
      LR [c000000000cadbcc] .vsnprintf+0x3ec/0x4f0
      Call Trace:
      Instruction dump:
      996d098a 994d098b 38610070 480246ed 48005518 60000000 38200000 718a4000
      7c2a0b78 3821fd00 41c20008 e82d0970 <0981fd00> f92101a0 f9610170 f9810178
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8e02444e9d3d..524a7bba0ee5 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -270,7 +270,9 @@ int main(void)
 	OFFSET(ACCOUNT_STARTTIME_USER, paca_struct, accounting.starttime_user);
 	OFFSET(ACCOUNT_USER_TIME, paca_struct, accounting.utime);
 	OFFSET(ACCOUNT_SYSTEM_TIME, paca_struct, accounting.stime);
+#ifdef CONFIG_PPC_BOOK3E
 	OFFSET(PACA_TRAP_SAVE, paca_struct, trap_save);
+#endif
 	OFFSET(PACA_SPRG_VDSO, paca_struct, sprg_vdso);
 #else /* CONFIG_PPC64 */
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8e02444e9d3d..31dc7e64cbfc 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * This program is used to generate definitions needed by
  * assembly language modules.
@@ -6,11 +7,6 @@
  * generate asm statements containing #defines,
  * compile this file to assembler, and then extract the
  * #defines from the assembly-language output.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #define GENERATING_ASM_OFFSETS	/* asm/smp.h */

commit bdc7c970bcdce1c018957e0158230bc025682ba2
Merge: 7ae3f6e130e8 e9cef0189c5b
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 30 22:52:03 2019 +1000

    Merge branch 'topic/ppc-kvm' into next
    
    Merge our topic branch shared with KVM. In particular this includes the
    rewrite of the idle code into C.

commit 10d91611f426d4bafd2a83d966c36da811b2f7ad
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Apr 13 00:30:52 2019 +1000

    powerpc/64s: Reimplement book3s idle code in C
    
    Reimplement Book3S idle code in C, moving POWER7/8/9 implementation
    speific HV idle code to the powernv platform code.
    
    Book3S assembly stubs are kept in common code and used only to save
    the stack frame and non-volatile GPRs before executing architected
    idle instructions, and restoring the stack and reloading GPRs then
    returning to C after waking from idle.
    
    The complex logic dealing with threads and subcores, locking, SPRs,
    HMIs, timebase resync, etc., is all done in C which makes it more
    maintainable.
    
    This is not a strict translation to C code, there are some
    significant differences:
    
    - Idle wakeup no longer uses the ->cpu_restore call to reinit SPRs,
      but saves and restores them itself.
    
    - The optimisation where EC=ESL=0 idle modes did not have to save GPRs
      or change MSR is restored, because it's now simple to do. ESL=1
      sleeps that do not lose GPRs can use this optimization too.
    
    - KVM secondary entry and cede is now more of a call/return style
      rather than branchy. nap_state_lost is not required because KVM
      always returns via NVGPR restoring path.
    
    - KVM secondary wakeup from offline sequence is moved entirely into
      the offline wakeup, which avoids a hwsync in the normal idle wakeup
      path.
    
    Performance measured with context switch ping-pong on different
    threads or cores, is possibly improved a small amount, 1-3% depending
    on stop state and core vs thread test for shallow states. Deep states
    it's in the noise compared with other latencies.
    
    KVM improvements:
    
    - Idle sleepers now always return to caller rather than branch out
      to KVM first.
    
    - This allows optimisations like very fast return to caller when no
      state has been lost.
    
    - KVM no longer requires nap_state_lost because it controls NVGPR
      save/restore itself on the way in and out.
    
    - The heavy idle wakeup KVM request check can be moved out of the
      normal host idle code and into the not-performance-critical offline
      code.
    
    - KVM nap code now returns from where it is called, which makes the
      flow a bit easier to follow.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Squash the KVM changes in]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 86a61e5f8285..83ad99f9f05d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -268,7 +268,6 @@ int main(void)
 	OFFSET(ACCOUNT_USER_TIME, paca_struct, accounting.utime);
 	OFFSET(ACCOUNT_SYSTEM_TIME, paca_struct, accounting.stime);
 	OFFSET(PACA_TRAP_SAVE, paca_struct, trap_save);
-	OFFSET(PACA_NAPSTATELOST, paca_struct, nap_state_lost);
 	OFFSET(PACA_SPRG_VDSO, paca_struct, sprg_vdso);
 #else /* CONFIG_PPC64 */
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
@@ -766,23 +765,6 @@ int main(void)
 	OFFSET(VCPU_TIMING_LAST_ENTER_TBL, kvm_vcpu, arch.timing_last_enter.tv32.tbl);
 #endif
 
-#ifdef CONFIG_PPC_POWERNV
-	OFFSET(PACA_CORE_IDLE_STATE_PTR, paca_struct, core_idle_state_ptr);
-	OFFSET(PACA_THREAD_IDLE_STATE, paca_struct, thread_idle_state);
-	OFFSET(PACA_THREAD_MASK, paca_struct, thread_mask);
-	OFFSET(PACA_SUBCORE_SIBLING_MASK, paca_struct, subcore_sibling_mask);
-	OFFSET(PACA_REQ_PSSCR, paca_struct, requested_psscr);
-	OFFSET(PACA_DONT_STOP, paca_struct, dont_stop);
-#define STOP_SPR(x, f)	OFFSET(x, paca_struct, stop_sprs.f)
-	STOP_SPR(STOP_PID, pid);
-	STOP_SPR(STOP_LDBAR, ldbar);
-	STOP_SPR(STOP_FSCR, fscr);
-	STOP_SPR(STOP_HFSCR, hfscr);
-	STOP_SPR(STOP_MMCR1, mmcr1);
-	STOP_SPR(STOP_MMCR2, mmcr2);
-	STOP_SPR(STOP_MMCRA, mmcra);
-#endif
-
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);
 	DEFINE(PPC_DBELL_MSGTYPE, PPC_DBELL_MSGTYPE);
 

commit a68c31fc01ef7863acc0fc74694bf279456a58c4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon Mar 11 08:30:38 2019 +0000

    powerpc/32s: Implement Kernel Userspace Access Protection
    
    This patch implements Kernel Userspace Access Protection for
    book3s/32.
    
    Due to limitations of the processor page protection capabilities,
    the protection is only against writing. read protection cannot be
    achieved using page protection.
    
    The previous patch modifies the page protection so that RW user
    pages are RW for Key 0 and RO for Key 1, and it sets Key 0 for
    both user and kernel.
    
    This patch changes userspace segment registers are set to Ku 0
    and Ks 1. When kernel needs to write to RW pages, the associated
    segment register is then changed to Ks 0 in order to allow write
    access to the kernel.
    
    In order to avoid having the read all segment registers when
    locking/unlocking the access, some data is kept in the thread_struct
    and saved on stack on exceptions. The field identifies both the
    first unlocked segment and the first segment following the last
    unlocked one. When no segment is unlocked, it contains value 0.
    
    As the hash_page() function is not able to easily determine if a
    protfault is due to a bad kernel access to userspace, protfaults
    need to be handled by handle_page_fault when KUAP is set.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Drop allow_read/write_to/from_user() as they're now in kup.h,
          and adapt allow_user_access() to do nothing when to == NULL]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 66202e02fee2..60b82198de7c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -147,6 +147,9 @@ int main(void)
 #if defined(CONFIG_KVM) && defined(CONFIG_BOOKE)
 	OFFSET(THREAD_KVM_VCPU, thread_struct, kvm_vcpu);
 #endif
+#if defined(CONFIG_PPC_BOOK3S_32) && defined(CONFIG_PPC_KUAP)
+	OFFSET(KUAP, thread_struct, kuap);
+#endif
 
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	OFFSET(PACATMSCRATCH, paca_struct, tm_scratch);

commit de78a9c42a790011f179bc94a7da3f5d8721f4cc
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Apr 18 16:51:20 2019 +1000

    powerpc: Add a framework for Kernel Userspace Access Protection
    
    This patch implements a framework for Kernel Userspace Access
    Protection.
    
    Then subarches will have the possibility to provide their own
    implementation by providing setup_kuap() and
    allow/prevent_user_access().
    
    Some platforms will need to know the area accessed and whether it is
    accessed from read, write or both. Therefore source, destination and
    size and handed over to the two functions.
    
    mpe: Rename to allow/prevent rather than unlock/lock, and add
    read/write wrappers. Drop the 32-bit code for now until we have an
    implementation for it. Add kuap to pt_regs for 64-bit as well as
    32-bit. Don't split strings, use pr_crit_ratelimited().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Russell Currey <ruscur@russell.cc>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 86a61e5f8285..66202e02fee2 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -332,6 +332,10 @@ int main(void)
 	STACK_PT_REGS_OFFSET(_PPR, ppr);
 #endif /* CONFIG_PPC64 */
 
+#ifdef CONFIG_PPC_KUAP
+	STACK_PT_REGS_OFFSET(STACK_REGS_KUAP, kuap);
+#endif
+
 #if defined(CONFIG_PPC32)
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
 	DEFINE(EXC_LVL_SIZE, STACK_EXC_LVL_FRAME_SIZE);

commit c911d2e128e8ab7e789a5488dcb63ae9fe130aca
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Jan 12 09:55:50 2019 +0000

    powerpc/64: Replace CURRENT_THREAD_INFO with PACA_THREAD_INFO
    
    Now that current_thread_info is located at the beginning of 'current'
    task struct, CURRENT_THREAD_INFO macro is not really needed any more.
    
    This patch replaces it by loads of the value at PACA_THREAD_INFO(r13).
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Add PACA_THREAD_INFO rather than using PACACURRENT]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8b688b19776a..86a61e5f8285 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -182,6 +182,8 @@ int main(void)
 	OFFSET(PACAPROCSTART, paca_struct, cpu_start);
 	OFFSET(PACAKSAVE, paca_struct, kstack);
 	OFFSET(PACACURRENT, paca_struct, __current);
+	DEFINE(PACA_THREAD_INFO, offsetof(struct paca_struct, __current) +
+				 offsetof(struct task_struct, thread_info));
 	OFFSET(PACASAVEDMSR, paca_struct, saved_msr);
 	OFFSET(PACAR1, paca_struct, saved_r1);
 	OFFSET(PACATOC, paca_struct, kernel_toc);

commit f7354ccac844da7b1af8cc4f09da330fa3e960e4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:09:04 2019 +0000

    powerpc/32: Remove CURRENT_THREAD_INFO and rename TI_CPU
    
    Now that thread_info is similar to task_struct, its address is in r2
    so CURRENT_THREAD_INFO() macro is useless. This patch removes it.
    
    This patch also moves the 'tovirt(r2, r2)' down just before the
    reactivation of MMU translation, so that we keep the physical address
    of 'current' in r2 until then. It avoids a few calls to tophys().
    
    At the same time, as the 'cpu' field is not anymore in thread_info,
    TI_CPU is renamed TASK_CPU by this patch.
    
    It also allows to get rid of a couple of
    '#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE' as ACCOUNT_CPU_USER_ENTRY()
    and ACCOUNT_CPU_USER_EXIT() are empty when
    CONFIG_VIRT_CPU_ACCOUNTING_NATIVE is not defined.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Fix a missed conversion of TI_CPU idle_6xx.S]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1ad0cbcc5f13..8b688b19776a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -99,7 +99,7 @@ int main(void)
 #endif /* CONFIG_PPC64 */
 	OFFSET(TASK_STACK, task_struct, stack);
 #ifdef CONFIG_SMP
-	OFFSET(TI_CPU, task_struct, cpu);
+	OFFSET(TASK_CPU, task_struct, cpu);
 #endif
 
 #ifdef CONFIG_LIVEPATCH

commit a7916a1de526162d73e894b6d3ebd895d4302078
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:09:00 2019 +0000

    powerpc: regain entire stack space
    
    thread_info is not anymore in the stack, so the entire stack
    can now be used.
    
    There is also no risk anymore of corrupting task_cpu(p) with a
    stack overflow so the patch removes the test.
    
    When doing this, an explicit test for NULL stack pointer is
    needed in validate_sp() as it is not anymore implicitely covered
    by the sizeof(thread_info) gap.
    
    In the meantime, with the previous patch all pointers to the stacks
    are not anymore pointers to thread_info so this patch changes them
    to void*
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ca3fb836cbb9..1ad0cbcc5f13 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -92,7 +92,6 @@ int main(void)
 	DEFINE(SIGSEGV, SIGSEGV);
 	DEFINE(NMI_MASK, NMI_MASK);
 #else
-	DEFINE(THREAD_INFO_GAP, _ALIGN_UP(sizeof(struct thread_info), 16));
 	OFFSET(KSP_LIMIT, thread_struct, ksp_limit);
 #ifdef CONFIG_PPC_RTAS
 	OFFSET(RTAS_SP, thread_struct, rtas_sp);

commit ed1cd6deb013a11959d17a94e35ce159197632da
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:58 2019 +0000

    powerpc: Activate CONFIG_THREAD_INFO_IN_TASK
    
    This patch activates CONFIG_THREAD_INFO_IN_TASK which
    moves the thread_info into task_struct.
    
    Moving thread_info into task_struct has the following advantages:
      - It protects thread_info from corruption in the case of stack
        overflows.
      - Its address is harder to determine if stack addresses are leaked,
        making a number of attacks more difficult.
    
    This has the following consequences:
      - thread_info is now located at the beginning of task_struct.
      - The 'cpu' field is now in task_struct, and only exists when
        CONFIG_SMP is active.
      - thread_info doesn't have anymore the 'task' field.
    
    This patch:
      - Removes all recopy of thread_info struct when the stack changes.
      - Changes the CURRENT_THREAD_INFO() macro to point to current.
      - Selects CONFIG_THREAD_INFO_IN_TASK.
      - Modifies raw_smp_processor_id() to get ->cpu from current without
        including linux/sched.h to avoid circular inclusion and without
        including asm/asm-offsets.h to avoid symbol names duplication
        between ASM constants and C constants.
      - Modifies klp_init_thread_info() to take a task_struct pointer
        argument.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Add task_stack.h to livepatch.h to fix build fails]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ca55027f47a4..ca3fb836cbb9 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -13,6 +13,8 @@
  * 2 of the License, or (at your option) any later version.
  */
 
+#define GENERATING_ASM_OFFSETS	/* asm/smp.h */
+
 #include <linux/compat.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
@@ -97,6 +99,9 @@ int main(void)
 #endif
 #endif /* CONFIG_PPC64 */
 	OFFSET(TASK_STACK, task_struct, stack);
+#ifdef CONFIG_SMP
+	OFFSET(TI_CPU, task_struct, cpu);
+#endif
 
 #ifdef CONFIG_LIVEPATCH
 	OFFSET(TI_livepatch_sp, thread_info, livepatch_sp);
@@ -164,8 +169,6 @@ int main(void)
 	OFFSET(TI_FLAGS, thread_info, flags);
 	OFFSET(TI_LOCAL_FLAGS, thread_info, local_flags);
 	OFFSET(TI_PREEMPT, thread_info, preempt_count);
-	OFFSET(TI_TASK, thread_info, task);
-	OFFSET(TI_CPU, thread_info, cpu);
 
 #ifdef CONFIG_PPC64
 	OFFSET(DCACHEL1BLOCKSIZE, ppc64_caches, l1d.block_size);

commit 8c1fc5abdccfb36102fa9647084eeb8c70e32562
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 31 10:08:54 2019 +0000

    powerpc: Rename THREAD_INFO to TASK_STACK
    
    This patch renames THREAD_INFO to TASK_STACK, because it is in fact
    the offset of the pointer to the stack in task_struct so this pointer
    will not be impacted by the move of THREAD_INFO.
    
    Also make it available on 64-bit, as we'll need it there when we
    activate THREAD_INFO_IN_TASK.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Make available on 64-bit]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index d6f9bdb1eb2e..ca55027f47a4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -90,13 +90,13 @@ int main(void)
 	DEFINE(SIGSEGV, SIGSEGV);
 	DEFINE(NMI_MASK, NMI_MASK);
 #else
-	OFFSET(THREAD_INFO, task_struct, stack);
 	DEFINE(THREAD_INFO_GAP, _ALIGN_UP(sizeof(struct thread_info), 16));
 	OFFSET(KSP_LIMIT, thread_struct, ksp_limit);
 #ifdef CONFIG_PPC_RTAS
 	OFFSET(RTAS_SP, thread_struct, rtas_sp);
 #endif
 #endif /* CONFIG_PPC64 */
+	OFFSET(TASK_STACK, task_struct, stack);
 
 #ifdef CONFIG_LIVEPATCH
 	OFFSET(TI_livepatch_sp, thread_info, livepatch_sp);

commit 0df977eafc792a5365a7f81d8d5920132e03afad
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 10:37:54 2019 +0000

    powerpc/6xx: Don't use SPRN_SPRG2 for storing stack pointer while in RTAS
    
    When calling RTAS, the stack pointer is stored in SPRN_SPRG2
    in order to be able to restore it in case of machine check in RTAS.
    
    As machine check is not a perfomance critical path, this patch
    frees SPRN_SPRG2 by using a field in thread struct instead.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9ffc72ded73a..d6f9bdb1eb2e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -93,6 +93,9 @@ int main(void)
 	OFFSET(THREAD_INFO, task_struct, stack);
 	DEFINE(THREAD_INFO_GAP, _ALIGN_UP(sizeof(struct thread_info), 16));
 	OFFSET(KSP_LIMIT, thread_struct, ksp_limit);
+#ifdef CONFIG_PPC_RTAS
+	OFFSET(RTAS_SP, thread_struct, rtas_sp);
+#endif
 #endif /* CONFIG_PPC64 */
 
 #ifdef CONFIG_LIVEPATCH

commit 685f7e4f161425b137056abe35ba8ef7b669d83d
Merge: c7a2c49ea6c9 58cfbac25b1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 26 14:36:21 2018 -0700

    Merge tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - A large series to rewrite our SLB miss handling, replacing a lot of
         fairly complicated asm with much fewer lines of C.
    
       - Following on from that, we now maintain a cache of SLB entries for
         each process and preload them on context switch. Leading to a 27%
         speedup for our context switch benchmark on Power9.
    
       - Improvements to our handling of SLB multi-hit errors. We now print
         more debug information when they occur, and try to continue running
         by flushing the SLB and reloading, rather than treating them as
         fatal.
    
       - Enable THP migration on 64-bit Book3S machines (eg. Power7/8/9).
    
       - Add support for physical memory up to 2PB in the linear mapping on
         64-bit Book3S. We only support up to 512TB as regular system
         memory, otherwise the percpu allocator runs out of vmalloc space.
    
       - Add stack protector support for 32 and 64-bit, with a per-task
         canary.
    
       - Add support for PTRACE_SYSEMU and PTRACE_SYSEMU_SINGLESTEP.
    
       - Support recognising "big cores" on Power9, where two SMT4 cores are
         presented to us as a single SMT8 core.
    
       - A large series to cleanup some of our ioremap handling and PTE
         flags.
    
       - Add a driver for the PAPR SCM (storage class memory) interface,
         allowing guests to operate on SCM devices (acked by Dan).
    
       - Changes to our ftrace code to handle very large kernels, where we
         need to use a trampoline to get to ftrace_caller().
    
      And many other smaller enhancements and cleanups.
    
      Thanks to: Alan Modra, Alistair Popple, Aneesh Kumar K.V, Anton
      Blanchard, Aravinda Prasad, Bartlomiej Zolnierkiewicz, Benjamin
      Herrenschmidt, Breno Leitao, Cédric Le Goater, Christophe Leroy,
      Christophe Lombard, Dan Carpenter, Daniel Axtens, Finn Thain, Gautham
      R. Shenoy, Gustavo Romero, Haren Myneni, Hari Bathini, Jia Hongtao,
      Joel Stanley, John Allen, Laurent Dufour, Madhavan Srinivasan, Mahesh
      Salgaonkar, Mark Hairgrove, Masahiro Yamada, Michael Bringmann,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Nathan
      Fontenot, Naveen N. Rao, Nicholas Piggin, Nick Desaulniers, Oliver
      O'Halloran, Paul Mackerras, Petr Vorel, Rashmica Gupta, Reza Arbab,
      Rob Herring, Sam Bobroff, Samuel Mendoza-Jonas, Scott Wood, Stan
      Johnson, Stephen Rothwell, Stewart Smith, Suraj Jitindar Singh, Tyrel
      Datwyler, Vaibhav Jain, Vasant Hegde, YueHaibing, zhong jiang"
    
    * tag 'powerpc-4.20-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (221 commits)
      Revert "selftests/powerpc: Fix out-of-tree build errors"
      powerpc/msi: Fix compile error on mpc83xx
      powerpc: Fix stack protector crashes on CPU hotplug
      powerpc/traps: restore recoverability of machine_check interrupts
      powerpc/64/module: REL32 relocation range check
      powerpc/64s/radix: Fix radix__flush_tlb_collapsed_pmd double flushing pmd
      selftests/powerpc: Add a test of wild bctr
      powerpc/mm: Fix page table dump to work on Radix
      powerpc/mm/radix: Display if mappings are exec or not
      powerpc/mm/radix: Simplify split mapping logic
      powerpc/mm/radix: Remove the retry in the split mapping logic
      powerpc/mm/radix: Fix small page at boundary when splitting
      powerpc/mm/radix: Fix overuse of small pages in splitting logic
      powerpc/mm/radix: Fix off-by-one in split mapping logic
      powerpc/ftrace: Handle large kernel configs
      powerpc/mm: Fix WARN_ON with THP NUMA migration
      selftests/powerpc: Fix out-of-tree build errors
      powerpc/time: no steal_time when CONFIG_PPC_SPLPAR is not selected
      powerpc/time: Only set CONFIG_ARCH_HAS_SCALED_CPUTIME on PPC64
      powerpc/time: isolate scaled cputime accounting in dedicated functions.
      ...

commit 0d1e8b8d2bcd3150d51754d8d0fdbf44dc88b0d3
Merge: 83c4087ce468 22a7cdcae6a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 25 17:57:35 2018 -0700

    Merge tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "ARM:
       - Improved guest IPA space support (32 to 52 bits)
    
       - RAS event delivery for 32bit
    
       - PMU fixes
    
       - Guest entry hardening
    
       - Various cleanups
    
       - Port of dirty_log_test selftest
    
      PPC:
       - Nested HV KVM support for radix guests on POWER9. The performance
         is much better than with PR KVM. Migration and arbitrary level of
         nesting is supported.
    
       - Disable nested HV-KVM on early POWER9 chips that need a particular
         hardware bug workaround
    
       - One VM per core mode to prevent potential data leaks
    
       - PCI pass-through optimization
    
       - merge ppc-kvm topic branch and kvm-ppc-fixes to get a better base
    
      s390:
       - Initial version of AP crypto virtualization via vfio-mdev
    
       - Improvement for vfio-ap
    
       - Set the host program identifier
    
       - Optimize page table locking
    
      x86:
       - Enable nested virtualization by default
    
       - Implement Hyper-V IPI hypercalls
    
       - Improve #PF and #DB handling
    
       - Allow guests to use Enlightened VMCS
    
       - Add migration selftests for VMCS and Enlightened VMCS
    
       - Allow coalesced PIO accesses
    
       - Add an option to perform nested VMCS host state consistency check
         through hardware
    
       - Automatic tuning of lapic_timer_advance_ns
    
       - Many fixes, minor improvements, and cleanups"
    
    * tag 'kvm-4.20-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      KVM/nVMX: Do not validate that posted_intr_desc_addr is page aligned
      Revert "kvm: x86: optimize dr6 restore"
      KVM: PPC: Optimize clearing TCEs for sparse tables
      x86/kvm/nVMX: tweak shadow fields
      selftests/kvm: add missing executables to .gitignore
      KVM: arm64: Safety check PSTATE when entering guest and handle IL
      KVM: PPC: Book3S HV: Don't use streamlined entry path on early POWER9 chips
      arm/arm64: KVM: Enable 32 bits kvm vcpu events support
      arm/arm64: KVM: Rename function kvm_arch_dev_ioctl_check_extension()
      KVM: arm64: Fix caching of host MDCR_EL2 value
      KVM: VMX: enable nested virtualization by default
      KVM/x86: Use 32bit xor to clear registers in svm.c
      kvm: x86: Introduce KVM_CAP_EXCEPTION_PAYLOAD
      kvm: vmx: Defer setting of DR6 until #DB delivery
      kvm: x86: Defer setting of CR2 until #PF delivery
      kvm: x86: Add payload operands to kvm_multiple_exception
      kvm: x86: Add exception payload fields to kvm_vcpu_events
      kvm: x86: Add has_payload and payload to kvm_queued_exception
      KVM: Documentation: Fix omission in struct kvm_vcpu_events
      KVM: selftests: add Enlightened VMCS test
      ...

commit 126b11b294d15a90e38eb2dcded2433619b2c794
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:53 2018 +1000

    powerpc/64s/hash: Add SLB allocation status bitmaps
    
    Add 32-entry bitmaps to track the allocation status of the first 32
    SLB entries, and whether they are user or kernel entries. These are
    used to allocate free SLB entries first, before resorting to the round
    robin allocator.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 92156c61d21c..10ef2e4db2fd 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -178,7 +178,6 @@ int main(void)
 	OFFSET(PACAKSAVE, paca_struct, kstack);
 	OFFSET(PACACURRENT, paca_struct, __current);
 	OFFSET(PACASAVEDMSR, paca_struct, saved_msr);
-	OFFSET(PACASTABRR, paca_struct, stab_rr);
 	OFFSET(PACAR1, paca_struct, saved_r1);
 	OFFSET(PACATOC, paca_struct, kernel_toc);
 	OFFSET(PACAKBASE, paca_struct, kernelbase);
@@ -217,6 +216,7 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S_64
 	OFFSET(PACASLBCACHE, paca_struct, slb_cache);
 	OFFSET(PACASLBCACHEPTR, paca_struct, slb_cache_ptr);
+	OFFSET(PACASTABRR, paca_struct, stab_rr);
 	OFFSET(PACAVMALLOCSLLP, paca_struct, vmalloc_sllp);
 #ifdef CONFIG_PPC_MM_SLICES
 	OFFSET(MMUPSIZESLLP, mmu_psize_def, sllp);

commit 4c2de74cc8696154b283f241d74ec0bb24438e22
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Oct 13 00:15:16 2018 +1100

    powerpc/64: Interrupts save PPR on stack rather than thread_struct
    
    PPR is the odd register out when it comes to interrupt handling, it is
    saved in current->thread.ppr while all others are saved on the stack.
    
    The difficulty with this is that accessing thread.ppr can cause a SLB
    fault, but the SLB fault handler implementation in C change had
    assumed the normal exception entry handlers would not cause an SLB
    fault.
    
    Fix this by allocating room in the interrupt stack to save PPR.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2eb4923f8468..92156c61d21c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -89,7 +89,6 @@ int main(void)
 #ifdef CONFIG_PPC64
 	DEFINE(SIGSEGV, SIGSEGV);
 	DEFINE(NMI_MASK, NMI_MASK);
-	OFFSET(TASKTHREADPPR, task_struct, thread.ppr);
 #else
 	OFFSET(THREAD_INFO, task_struct, stack);
 	DEFINE(THREAD_INFO_GAP, _ALIGN_UP(sizeof(struct thread_info), 16));
@@ -323,6 +322,7 @@ int main(void)
 	STACK_PT_REGS_OFFSET(_ESR, dsisr);
 #else /* CONFIG_PPC64 */
 	STACK_PT_REGS_OFFSET(SOFTE, softe);
+	STACK_PT_REGS_OFFSET(_PPR, ppr);
 #endif /* CONFIG_PPC64 */
 
 #if defined(CONFIG_PPC32)

commit ed9e84a4d703243a232e6549a13dedfaf0d5d2d8
Author: Joel Stanley <joel@jms.id.au>
Date:   Fri Oct 12 13:14:06 2018 +1030

    powerpc: Use SWITCH_FRAME_SIZE for prom and rtas entry
    
    Commit 6c1719942e19 ("powerpc/of: Remove useless register save/restore
    when calling OF back") removed the saving of srr0 and srr1 when calling
    into OpenFirmware. Commit e31aa453bbc4 ("powerpc: Use LOAD_REG_IMMEDIATE
    only for constants on 64-bit") did the same for rtas.
    
    This means we don't need to save the extra stack space and can use
    the common SWITCH_FRAME_SIZE.
    
    There were already no users of _SRR0 and _SRR1 so we can remove them
    too.
    
    Link: https://github.com/linuxppc/linux/issues/83
    Signed-off-by: Joel Stanley <joel@jms.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index a6d70fd2e499..2eb4923f8468 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -280,11 +280,6 @@ int main(void)
 	/* Interrupt register frame */
 	DEFINE(INT_FRAME_SIZE, STACK_INT_FRAME_SIZE);
 	DEFINE(SWITCH_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
-#ifdef CONFIG_PPC64
-	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
-	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
-	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
-#endif /* CONFIG_PPC64 */
 	STACK_PT_REGS_OFFSET(GPR0, gpr[0]);
 	STACK_PT_REGS_OFFSET(GPR1, gpr[1]);
 	STACK_PT_REGS_OFFSET(GPR2, gpr[2]);
@@ -328,10 +323,6 @@ int main(void)
 	STACK_PT_REGS_OFFSET(_ESR, dsisr);
 #else /* CONFIG_PPC64 */
 	STACK_PT_REGS_OFFSET(SOFTE, softe);
-
-	/* These _only_ to be used with {PROM,RTAS}_FRAME_SIZE!!! */
-	DEFINE(_SRR0, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs));
-	DEFINE(_SRR1, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs)+8);
 #endif /* CONFIG_PPC64 */
 
 #if defined(CONFIG_PPC32)

commit 360cae313702cdd0b90f82c261a8302fecef030a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:31:04 2018 +1100

    KVM: PPC: Book3S HV: Nested guest entry via hypercall
    
    This adds a new hypercall, H_ENTER_NESTED, which is used by a nested
    hypervisor to enter one of its nested guests.  The hypercall supplies
    register values in two structs.  Those values are copied by the level 0
    (L0) hypervisor (the one which is running in hypervisor mode) into the
    vcpu struct of the L1 guest, and then the guest is run until an
    interrupt or error occurs which needs to be reported to L1 via the
    hypercall return value.
    
    Currently this assumes that the L0 and L1 hypervisors are the same
    endianness, and the structs passed as arguments are in native
    endianness.  If they are of different endianness, the version number
    check will fail and the hcall will be rejected.
    
    Nested hypervisors do not support indep_threads_mode=N, so this adds
    code to print a warning message if the administrator has set
    indep_threads_mode=N, and treat it as Y.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 7c3738d890e8..d0abcbbdc700 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -503,6 +503,7 @@ int main(void)
 	OFFSET(VCPU_VPA, kvm_vcpu, arch.vpa.pinned_addr);
 	OFFSET(VCPU_VPA_DIRTY, kvm_vcpu, arch.vpa.dirty);
 	OFFSET(VCPU_HEIR, kvm_vcpu, arch.emul_inst);
+	OFFSET(VCPU_NESTED, kvm_vcpu, arch.nested);
 	OFFSET(VCPU_CPU, kvm_vcpu, cpu);
 	OFFSET(VCPU_THREAD_CPU, kvm_vcpu, arch.thread_cpu);
 #endif

commit fd0944baad806dfb4c777124ec712c55b714ff51
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:58 2018 +1100

    KVM: PPC: Use ccr field in pt_regs struct embedded in vcpu struct
    
    When the 'regs' field was added to struct kvm_vcpu_arch, the code
    was changed to use several of the fields inside regs (e.g., gpr, lr,
    etc.) but not the ccr field, because the ccr field in struct pt_regs
    is 64 bits on 64-bit platforms, but the cr field in kvm_vcpu_arch is
    only 32 bits.  This changes the code to use the regs.ccr field
    instead of cr, and changes the assembly code on 64-bit platforms to
    use 64-bit loads and stores instead of 32-bit ones.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 89cf15566c4e..7c3738d890e8 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -438,7 +438,7 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S
 	OFFSET(VCPU_TAR, kvm_vcpu, arch.tar);
 #endif
-	OFFSET(VCPU_CR, kvm_vcpu, arch.cr);
+	OFFSET(VCPU_CR, kvm_vcpu, arch.regs.ccr);
 	OFFSET(VCPU_PC, kvm_vcpu, arch.regs.nip);
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	OFFSET(VCPU_MSR, kvm_vcpu, arch.shregs.msr);
@@ -695,7 +695,7 @@ int main(void)
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 #else /* CONFIG_PPC_BOOK3S */
-	OFFSET(VCPU_CR, kvm_vcpu, arch.cr);
+	OFFSET(VCPU_CR, kvm_vcpu, arch.regs.ccr);
 	OFFSET(VCPU_XER, kvm_vcpu, arch.regs.xer);
 	OFFSET(VCPU_LR, kvm_vcpu, arch.regs.link);
 	OFFSET(VCPU_CTR, kvm_vcpu, arch.regs.ctr);

commit 06ec27aea9fc84d9c6d879eb64b5bcf28a8a1eb7
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Sep 27 07:05:55 2018 +0000

    powerpc/64: add stack protector support
    
    On PPC64, as register r13 points to the paca_struct at all time,
    this patch adds a copy of the canary there, which is copied at
    task_switch.
    That new canary is then used by using the following GCC options:
    -mstack-protector-guard=tls
    -mstack-protector-guard-reg=r13
    -mstack-protector-guard-offset=offsetof(struct paca_struct, canary))
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9e9ee168e177..a6d70fd2e499 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -81,6 +81,9 @@ int main(void)
 	OFFSET(MM, task_struct, mm);
 #ifdef CONFIG_STACKPROTECTOR
 	OFFSET(TASK_CANARY, task_struct, stack_canary);
+#ifdef CONFIG_PPC64
+	OFFSET(PACA_CANARY, paca_struct, canary);
+#endif
 #endif
 	OFFSET(MMCONTEXTID, mm_struct, context.id);
 #ifdef CONFIG_PPC64

commit c3ff2a5193fa61b1b284cfb1d79628814ed0e95a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Sep 27 07:05:53 2018 +0000

    powerpc/32: add stack protector support
    
    This functionality was tentatively added in the past
    (commit 6533b7c16ee5 ("powerpc: Initial stack protector
    (-fstack-protector) support")) but had to be reverted
    (commit f2574030b0e3 ("powerpc: Revert the initial stack
    protector support") because of GCC implementing it differently
    whether it had been built with libc support or not.
    
    Now, GCC offers the possibility to manually set the
    stack-protector mode (global or tls) regardless of libc support.
    
    This time, the patch selects HAVE_STACKPROTECTOR only if
    -mstack-protector-guard=tls is supported by GCC.
    
    On PPC32, as register r2 points to current task_struct at
    all time, the stack_canary located inside task_struct can be
    used directly by using the following GCC options:
    -mstack-protector-guard=tls
    -mstack-protector-guard-reg=r2
    -mstack-protector-guard-offset=offsetof(struct task_struct, stack_canary))
    
    The protector is disabled for prom_init and bootx_init as
    it is too early to handle it properly.
    
     $ echo CORRUPT_STACK > /sys/kernel/debug/provoke-crash/DIRECT
    [  134.943666] Kernel panic - not syncing: stack-protector: Kernel stack is corrupted in: lkdtm_CORRUPT_STACK+0x64/0x64
    [  134.943666]
    [  134.955414] CPU: 0 PID: 283 Comm: sh Not tainted 4.18.0-s3k-dev-12143-ga3272be41209 #835
    [  134.963380] Call Trace:
    [  134.965860] [c6615d60] [c001f76c] panic+0x118/0x260 (unreliable)
    [  134.971775] [c6615dc0] [c001f654] panic+0x0/0x260
    [  134.976435] [c6615dd0] [c032c368] lkdtm_CORRUPT_STACK_STRONG+0x0/0x64
    [  134.982769] [c6615e00] [ffffffff] 0xffffffff
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 89cf15566c4e..9e9ee168e177 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -79,6 +79,9 @@ int main(void)
 {
 	OFFSET(THREAD, task_struct, thread);
 	OFFSET(MM, task_struct, mm);
+#ifdef CONFIG_STACKPROTECTOR
+	OFFSET(TASK_CANARY, task_struct, stack_canary);
+#endif
 	OFFSET(MMCONTEXTID, mm_struct, context.id);
 #ifdef CONFIG_PPC64
 	DEFINE(SIGSEGV, SIGSEGV);

commit 54be0b9c7c9888ebe63b89a31a17ee3df6a68d61
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Oct 2 23:56:39 2018 +1000

    Revert "convert SLB miss handlers to C" and subsequent commits
    
    This reverts commits:
      5e46e29e6a97 ("powerpc/64s/hash: convert SLB miss handlers to C")
      8fed04d0f6ae ("powerpc/64s/hash: remove user SLB data from the paca")
      655deecf67b2 ("powerpc/64s/hash: SLB allocation status bitmaps")
      2e1626744e8d ("powerpc/64s/hash: provide arch_setup_exec hooks for hash slice setup")
      89ca4e126a3f ("powerpc/64s/hash: Add a SLB preload cache")
    
    This series had a few bugs, and the fixes are not all trivial. So
    revert most of it for now.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ba9d0fc98730..89cf15566c4e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -173,6 +173,7 @@ int main(void)
 	OFFSET(PACAKSAVE, paca_struct, kstack);
 	OFFSET(PACACURRENT, paca_struct, __current);
 	OFFSET(PACASAVEDMSR, paca_struct, saved_msr);
+	OFFSET(PACASTABRR, paca_struct, stab_rr);
 	OFFSET(PACAR1, paca_struct, saved_r1);
 	OFFSET(PACATOC, paca_struct, kernel_toc);
 	OFFSET(PACAKBASE, paca_struct, kernelbase);
@@ -180,6 +181,15 @@ int main(void)
 	OFFSET(PACAIRQSOFTMASK, paca_struct, irq_soft_mask);
 	OFFSET(PACAIRQHAPPENED, paca_struct, irq_happened);
 	OFFSET(PACA_FTRACE_ENABLED, paca_struct, ftrace_enabled);
+#ifdef CONFIG_PPC_BOOK3S
+	OFFSET(PACACONTEXTID, paca_struct, mm_ctx_id);
+#ifdef CONFIG_PPC_MM_SLICES
+	OFFSET(PACALOWSLICESPSIZE, paca_struct, mm_ctx_low_slices_psize);
+	OFFSET(PACAHIGHSLICEPSIZE, paca_struct, mm_ctx_high_slices_psize);
+	OFFSET(PACA_SLB_ADDR_LIMIT, paca_struct, mm_ctx_slb_addr_limit);
+	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
+#endif /* CONFIG_PPC_MM_SLICES */
+#endif
 
 #ifdef CONFIG_PPC_BOOK3E
 	OFFSET(PACAPGD, paca_struct, pgd);
@@ -202,7 +212,6 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S_64
 	OFFSET(PACASLBCACHE, paca_struct, slb_cache);
 	OFFSET(PACASLBCACHEPTR, paca_struct, slb_cache_ptr);
-	OFFSET(PACASTABRR, paca_struct, stab_rr);
 	OFFSET(PACAVMALLOCSLLP, paca_struct, vmalloc_sllp);
 #ifdef CONFIG_PPC_MM_SLICES
 	OFFSET(MMUPSIZESLLP, mmu_psize_def, sllp);

commit 655deecf67b240bf7bb4e73df4e1235900c26a01
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:53 2018 +1000

    powerpc/64s/hash: SLB allocation status bitmaps
    
    Add 32-entry bitmaps to track the allocation status of the first 32
    SLB entries, and whether they are user or kernel entries. These are
    used to allocate free SLB entries first, before resorting to the round
    robin allocator.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ce3ac40fd96e..ba9d0fc98730 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -173,7 +173,6 @@ int main(void)
 	OFFSET(PACAKSAVE, paca_struct, kstack);
 	OFFSET(PACACURRENT, paca_struct, __current);
 	OFFSET(PACASAVEDMSR, paca_struct, saved_msr);
-	OFFSET(PACASTABRR, paca_struct, stab_rr);
 	OFFSET(PACAR1, paca_struct, saved_r1);
 	OFFSET(PACATOC, paca_struct, kernel_toc);
 	OFFSET(PACAKBASE, paca_struct, kernelbase);
@@ -203,6 +202,7 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S_64
 	OFFSET(PACASLBCACHE, paca_struct, slb_cache);
 	OFFSET(PACASLBCACHEPTR, paca_struct, slb_cache_ptr);
+	OFFSET(PACASTABRR, paca_struct, stab_rr);
 	OFFSET(PACAVMALLOCSLLP, paca_struct, vmalloc_sllp);
 #ifdef CONFIG_PPC_MM_SLICES
 	OFFSET(MMUPSIZESLLP, mmu_psize_def, sllp);

commit 8fed04d0f6aedf99b3d811ba58d38bb7f938a47a
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sat Sep 15 01:30:52 2018 +1000

    powerpc/64s/hash: remove user SLB data from the paca
    
    User SLB mappig data is copied into the PACA from the mm->context so
    it can be accessed by the SLB miss handlers.
    
    After the C conversion, SLB miss handlers now run with relocation on,
    and user SLB misses are able to take recursive kernel SLB misses, so
    the user SLB mapping data can be removed from the paca and accessed
    directly.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 89cf15566c4e..ce3ac40fd96e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -181,15 +181,6 @@ int main(void)
 	OFFSET(PACAIRQSOFTMASK, paca_struct, irq_soft_mask);
 	OFFSET(PACAIRQHAPPENED, paca_struct, irq_happened);
 	OFFSET(PACA_FTRACE_ENABLED, paca_struct, ftrace_enabled);
-#ifdef CONFIG_PPC_BOOK3S
-	OFFSET(PACACONTEXTID, paca_struct, mm_ctx_id);
-#ifdef CONFIG_PPC_MM_SLICES
-	OFFSET(PACALOWSLICESPSIZE, paca_struct, mm_ctx_low_slices_psize);
-	OFFSET(PACAHIGHSLICEPSIZE, paca_struct, mm_ctx_high_slices_psize);
-	OFFSET(PACA_SLB_ADDR_LIMIT, paca_struct, mm_ctx_slb_addr_limit);
-	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
-#endif /* CONFIG_PPC_MM_SLICES */
-#endif
 
 #ifdef CONFIG_PPC_BOOK3E
 	OFFSET(PACAPGD, paca_struct, pgd);

commit 9afc5eee65ca7d717a99d6fe8f4adfe32a40940a
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jul 13 12:52:28 2018 +0200

    y2038: globally rename compat_time to old_time32
    
    Christoph Hellwig suggested a slightly different path for handling
    backwards compatibility with the 32-bit time_t based system calls:
    
    Rather than simply reusing the compat_sys_* entry points on 32-bit
    architectures unchanged, we get rid of those entry points and the
    compat_time types by renaming them to something that makes more sense
    on 32-bit architectures (which don't have a compat mode otherwise),
    and then share the entry points under the new name with the 64-bit
    architectures that use them for implementing the compatibility.
    
    The following types and interfaces are renamed here, and moved
    from linux/compat_time.h to linux/time32.h:
    
    old                             new
    ---                             ---
    compat_time_t                   old_time32_t
    struct compat_timeval           struct old_timeval32
    struct compat_timespec          struct old_timespec32
    struct compat_itimerspec        struct old_itimerspec32
    ns_to_compat_timeval()          ns_to_old_timeval32()
    get_compat_itimerspec64()       get_old_itimerspec32()
    put_compat_itimerspec64()       put_old_itimerspec32()
    compat_get_timespec64()         get_old_timespec32()
    compat_put_timespec64()         put_old_timespec32()
    
    As we already have aliases in place, this patch addresses only the
    instances that are relevant to the system call interface in particular,
    not those that occur in device drivers and other modules. Those
    will get handled separately, while providing the 64-bit version
    of the respective interfaces.
    
    I'm not renaming the timex, rusage and itimerval structures, as we are
    still debating what the new interface will look like, and whether we
    will need a replacement at all.
    
    This also doesn't change the names of the syscall entry points, which can
    be done more easily when we actually switch over the 32-bit architectures
    to use them, at that point we need to change COMPAT_SYSCALL_DEFINEx to
    SYSCALL_DEFINEx with a new name, e.g. with a _time32 suffix.
    
    Suggested-by: Christoph Hellwig <hch@infradead.org>
    Link: https://lore.kernel.org/lkml/20180705222110.GA5698@infradead.org/
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 89cf15566c4e..041a115789a1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -387,12 +387,12 @@ int main(void)
 	OFFSET(CFG_SYSCALL_MAP64, vdso_data, syscall_map_64);
 	OFFSET(TVAL64_TV_SEC, timeval, tv_sec);
 	OFFSET(TVAL64_TV_USEC, timeval, tv_usec);
-	OFFSET(TVAL32_TV_SEC, compat_timeval, tv_sec);
-	OFFSET(TVAL32_TV_USEC, compat_timeval, tv_usec);
+	OFFSET(TVAL32_TV_SEC, old_timeval32, tv_sec);
+	OFFSET(TVAL32_TV_USEC, old_timeval32, tv_usec);
 	OFFSET(TSPC64_TV_SEC, timespec, tv_sec);
 	OFFSET(TSPC64_TV_NSEC, timespec, tv_nsec);
-	OFFSET(TSPC32_TV_SEC, compat_timespec, tv_sec);
-	OFFSET(TSPC32_TV_NSEC, compat_timespec, tv_nsec);
+	OFFSET(TSPC32_TV_SEC, old_timespec32, tv_sec);
+	OFFSET(TSPC32_TV_NSEC, old_timespec32, tv_nsec);
 #else
 	OFFSET(TVAL32_TV_SEC, timeval, tv_sec);
 	OFFSET(TVAL32_TV_USEC, timeval, tv_usec);

commit 2bf1071a8d50928a4ae366bb3108833166c2b70c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Jul 5 18:47:00 2018 +1000

    powerpc/64s: Remove POWER9 DD1 support
    
    POWER9 DD1 was never a product. It is no longer supported by upstream
    firmware, and it is not effectively supported in Linux due to lack of
    testing.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Michael Ellerman <mpe@ellerman.id.au>
    [mpe: Remove arch_make_huge_pte() entirely]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0a0544335950..89cf15566c4e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -766,7 +766,6 @@ int main(void)
 	OFFSET(PACA_THREAD_IDLE_STATE, paca_struct, thread_idle_state);
 	OFFSET(PACA_THREAD_MASK, paca_struct, thread_mask);
 	OFFSET(PACA_SUBCORE_SIBLING_MASK, paca_struct, subcore_sibling_mask);
-	OFFSET(PACA_SIBLING_PACA_PTRS, paca_struct, thread_sibling_pacas);
 	OFFSET(PACA_REQ_PSSCR, paca_struct, requested_psscr);
 	OFFSET(PACA_DONT_STOP, paca_struct, dont_stop);
 #define STOP_SPR(x, f)	OFFSET(x, paca_struct, stop_sprs.f)

commit 09027ab73b82ec773f6ab8ae9468d7e15b748dfa
Merge: 273ba45796c1 f61e0d3cc4ae
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 14 17:42:54 2018 +0200

    Merge tag 'kvm-ppc-next-4.18-2' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD

commit c90fca951e90ba470a3dc6087667edffcf8db21b
Merge: c0ab85267e25 ff5bc793e47b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 7 10:23:33 2018 -0700

    Merge tag 'powerpc-4.18-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Support for split PMD page table lock on 64-bit Book3S (Power8/9).
    
       - Add support for HAVE_RELIABLE_STACKTRACE, so we properly support
         live patching again.
    
       - Add support for patching barrier_nospec in copy_from_user() and
         syscall entry.
    
       - A couple of fixes for our data breakpoints on Book3S.
    
       - A series from Nick optimising TLB/mm handling with the Radix MMU.
    
       - Numerous small cleanups to squash sparse/gcc warnings from Mathieu
         Malaterre.
    
       - Several series optimising various parts of the 32-bit code from
         Christophe Leroy.
    
       - Removal of support for two old machines, "SBC834xE" and "C2K"
         ("GEFanuc,C2K"), which is why the diffstat has so many deletions.
    
      And many other small improvements & fixes.
    
      There's a few out-of-area changes. Some minor ftrace changes OK'ed by
      Steve, and a fix to our powernv cpuidle driver. Then there's a series
      touching mm, x86 and fs/proc/task_mmu.c, which cleans up some details
      around pkey support. It was ack'ed/reviewed by Ingo & Dave and has
      been in next for several weeks.
    
      Thanks to: Akshay Adiga, Alastair D'Silva, Alexey Kardashevskiy, Al
      Viro, Andrew Donnellan, Aneesh Kumar K.V, Anju T Sudhakar, Arnd
      Bergmann, Balbir Singh, Cédric Le Goater, Christophe Leroy, Christophe
      Lombard, Colin Ian King, Dave Hansen, Fabio Estevam, Finn Thain,
      Frederic Barrat, Gautham R. Shenoy, Haren Myneni, Hari Bathini, Ingo
      Molnar, Jonathan Neuschäfer, Josh Poimboeuf, Kamalesh Babulal,
      Madhavan Srinivasan, Mahesh Salgaonkar, Mark Greer, Mathieu Malaterre,
      Matthew Wilcox, Michael Neuling, Michal Suchanek, Naveen N. Rao,
      Nicholas Piggin, Nicolai Stange, Olof Johansson, Paul Gortmaker, Paul
      Mackerras, Peter Rosin, Pridhiviraj Paidipeddi, Ram Pai, Rashmica
      Gupta, Ravi Bangoria, Russell Currey, Sam Bobroff, Samuel
      Mendoza-Jonas, Segher Boessenkool, Shilpasri G Bhat, Simon Guo,
      Souptick Joarder, Stewart Smith, Thiago Jung Bauermann, Torsten Duwe,
      Vaibhav Jain, Wei Yongjun, Wolfram Sang, Yisheng Xie, YueHaibing"
    
    * tag 'powerpc-4.18-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (251 commits)
      powerpc/64s/radix: Fix missing ptesync in flush_cache_vmap
      cpuidle: powernv: Fix promotion from snooze if next state disabled
      powerpc: fix build failure by disabling attribute-alias warning in pci_32
      ocxl: Fix missing unlock on error in afu_ioctl_enable_p9_wait()
      powerpc-opal: fix spelling mistake "Uniterrupted" -> "Uninterrupted"
      powerpc: fix spelling mistake: "Usupported" -> "Unsupported"
      powerpc/pkeys: Detach execute_only key on !PROT_EXEC
      powerpc/powernv: copy/paste - Mask SO bit in CR
      powerpc: Remove core support for Marvell mv64x60 hostbridges
      powerpc/boot: Remove core support for Marvell mv64x60 hostbridges
      powerpc/boot: Remove support for Marvell mv64x60 i2c controller
      powerpc/boot: Remove support for Marvell MPSC serial controller
      powerpc/embedded6xx: Remove C2K board support
      powerpc/lib: optimise PPC32 memcmp
      powerpc/lib: optimise 32 bits __clear_user()
      powerpc/time: inline arch_vtime_task_switch()
      powerpc/Makefile: set -mcpu=860 flag for the 8xx
      powerpc: Implement csum_ipv6_magic in assembly
      powerpc/32: Optimise __csum_partial()
      powerpc/lib: Adjust .balign inside string functions for PPC32
      ...

commit 0bbcce5d1ef3f771a349896f1c7574d20dc6f4bd
Merge: 0ef283d4c780 e45e778f078e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 20:27:54 2018 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timers and timekeeping updates from Thomas Gleixner:
    
     - Core infrastucture work for Y2038 to address the COMPAT interfaces:
    
         + Add a new Y2038 safe __kernel_timespec and use it in the core
           code
    
         + Introduce config switches which allow to control the various
           compat mechanisms
    
         + Use the new config switch in the posix timer code to control the
           32bit compat syscall implementation.
    
     - Prevent bogus selection of CPU local clocksources which causes an
       endless reselection loop
    
     - Remove the extra kthread in the clocksource code which has no value
       and just adds another level of indirection
    
     - The usual bunch of trivial updates, cleanups and fixlets all over the
       place
    
     - More SPDX conversions
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      clocksource/drivers/mxs_timer: Switch to SPDX identifier
      clocksource/drivers/timer-imx-tpm: Switch to SPDX identifier
      clocksource/drivers/timer-imx-gpt: Switch to SPDX identifier
      clocksource/drivers/timer-imx-gpt: Remove outdated file path
      clocksource/drivers/arc_timer: Add comments about locking while read GFRC
      clocksource/drivers/mips-gic-timer: Add pr_fmt and reword pr_* messages
      clocksource/drivers/sprd: Fix Kconfig dependency
      clocksource: Move inline keyword to the beginning of function declarations
      timer_list: Remove unused function pointer typedef
      timers: Adjust a kernel-doc comment
      tick: Prefer a lower rating device only if it's CPU local device
      clocksource: Remove kthread
      time: Change nanosleep to safe __kernel_* types
      time: Change types to new y2038 safe __kernel_* types
      time: Fix get_timespec64() for y2038 safe compat interfaces
      time: Add new y2038 safe __kernel_timespec
      posix-timers: Make compat syscalls depend on CONFIG_COMPAT_32BIT_TIME
      time: Introduce CONFIG_COMPAT_32BIT_TIME
      time: Introduce CONFIG_64BIT_TIME in architectures
      compat: Enable compat_get/put_timespec64 always
      ...

commit 173c520a049f57e2af498a3f0557d07797ce1c1b
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 7 14:20:08 2018 +0800

    KVM: PPC: Move nip/ctr/lr/xer registers to pt_regs in kvm_vcpu_arch
    
    This patch moves nip/ctr/lr/xer registers from scattered places in
    kvm_vcpu_arch to pt_regs structure.
    
    cr register is "unsigned long" in pt_regs and u32 in vcpu->arch.
    It will need more consideration and may move in later patches.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 774c6a8ebfb4..70a345c73281 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -431,14 +431,14 @@ int main(void)
 #ifdef CONFIG_ALTIVEC
 	OFFSET(VCPU_VRS, kvm_vcpu, arch.vr.vr);
 #endif
-	OFFSET(VCPU_XER, kvm_vcpu, arch.xer);
-	OFFSET(VCPU_CTR, kvm_vcpu, arch.ctr);
-	OFFSET(VCPU_LR, kvm_vcpu, arch.lr);
+	OFFSET(VCPU_XER, kvm_vcpu, arch.regs.xer);
+	OFFSET(VCPU_CTR, kvm_vcpu, arch.regs.ctr);
+	OFFSET(VCPU_LR, kvm_vcpu, arch.regs.link);
 #ifdef CONFIG_PPC_BOOK3S
 	OFFSET(VCPU_TAR, kvm_vcpu, arch.tar);
 #endif
 	OFFSET(VCPU_CR, kvm_vcpu, arch.cr);
-	OFFSET(VCPU_PC, kvm_vcpu, arch.pc);
+	OFFSET(VCPU_PC, kvm_vcpu, arch.regs.nip);
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	OFFSET(VCPU_MSR, kvm_vcpu, arch.shregs.msr);
 	OFFSET(VCPU_SRR0, kvm_vcpu, arch.shregs.srr0);
@@ -695,10 +695,10 @@ int main(void)
 
 #else /* CONFIG_PPC_BOOK3S */
 	OFFSET(VCPU_CR, kvm_vcpu, arch.cr);
-	OFFSET(VCPU_XER, kvm_vcpu, arch.xer);
-	OFFSET(VCPU_LR, kvm_vcpu, arch.lr);
-	OFFSET(VCPU_CTR, kvm_vcpu, arch.ctr);
-	OFFSET(VCPU_PC, kvm_vcpu, arch.pc);
+	OFFSET(VCPU_XER, kvm_vcpu, arch.regs.xer);
+	OFFSET(VCPU_LR, kvm_vcpu, arch.regs.link);
+	OFFSET(VCPU_CTR, kvm_vcpu, arch.regs.ctr);
+	OFFSET(VCPU_PC, kvm_vcpu, arch.regs.nip);
 	OFFSET(VCPU_SPRG9, kvm_vcpu, arch.sprg9);
 	OFFSET(VCPU_LAST_INST, kvm_vcpu, arch.last_inst);
 	OFFSET(VCPU_FAULT_DEAR, kvm_vcpu, arch.fault_dear);

commit 1143a70665c2175a33a40d8f2dc277978fbf7640
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 7 14:20:07 2018 +0800

    KVM: PPC: Add pt_regs into kvm_vcpu_arch and move vcpu->arch.gpr[] into it
    
    Current regs are scattered at kvm_vcpu_arch structure and it will
    be more neat to organize them into pt_regs structure.
    
    Also it will enable reimplementation of MMIO emulation code with
    analyse_instr() later.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 373dc1d6ef44..774c6a8ebfb4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -425,7 +425,7 @@ int main(void)
 	OFFSET(VCPU_HOST_STACK, kvm_vcpu, arch.host_stack);
 	OFFSET(VCPU_HOST_PID, kvm_vcpu, arch.host_pid);
 	OFFSET(VCPU_GUEST_PID, kvm_vcpu, arch.pid);
-	OFFSET(VCPU_GPRS, kvm_vcpu, arch.gpr);
+	OFFSET(VCPU_GPRS, kvm_vcpu, arch.regs.gpr);
 	OFFSET(VCPU_VRSAVE, kvm_vcpu, arch.vrsave);
 	OFFSET(VCPU_FPRS, kvm_vcpu, arch.fp.fpr);
 #ifdef CONFIG_ALTIVEC

commit 57b8daa70a179bc23cc4240420ab6fbcdd7faf77
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Apr 20 22:51:11 2018 +1000

    KVM: PPC: Book3S HV: Snapshot timebase offset on guest entry
    
    Currently, the HV KVM guest entry/exit code adds the timebase offset
    from the vcore struct to the timebase on guest entry, and subtracts
    it on guest exit.  Which is fine, except that it is possible for
    userspace to change the offset using the SET_ONE_REG interface while
    the vcore is running, as there is only one timebase offset per vcore
    but potentially multiple VCPUs in the vcore.  If that were to happen,
    KVM would subtract a different offset on guest exit from that which
    it had added on guest entry, leading to the timebase being out of sync
    between cores in the host, which then leads to bad things happening
    such as hangs and spurious watchdog timeouts.
    
    To fix this, we add a new field 'tb_offset_applied' to the vcore struct
    which stores the offset that is currently applied to the timebase.
    This value is set from the vcore tb_offset field on guest entry, and
    is what is subtracted from the timebase on guest exit.  Since it is
    zero when the timebase offset is not applied, we can simplify the
    logic in kvmhv_start_timing and kvmhv_accumulate_time.
    
    In addition, we had secondary threads reading the timebase while
    running concurrently with code on the primary thread which would
    eventually add or subtract the timebase offset from the timebase.
    This occurred while saving or restoring the DEC register value on
    the secondary threads.  Although no specific incorrect behaviour has
    been observed, this is a race which should be fixed.  To fix it, we
    move the DEC saving code to just before we call kvmhv_commence_exit,
    and the DEC restoring code to after the point where we have waited
    for the primary thread to switch the MMU context and add the timebase
    offset.  That way we are sure that the timebase contains the guest
    timebase value in both cases.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6bee65f3cfd3..373dc1d6ef44 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -562,6 +562,7 @@ int main(void)
 	OFFSET(VCORE_NAPPING_THREADS, kvmppc_vcore, napping_threads);
 	OFFSET(VCORE_KVM, kvmppc_vcore, kvm);
 	OFFSET(VCORE_TB_OFFSET, kvmppc_vcore, tb_offset);
+	OFFSET(VCORE_TB_OFFSET_APPL, kvmppc_vcore, tb_offset_applied);
 	OFFSET(VCORE_LPCR, kvmppc_vcore, lpcr);
 	OFFSET(VCORE_PCR, kvmppc_vcore, pcr);
 	OFFSET(VCORE_DPDES, kvmppc_vcore, dpdes);

commit ea678ac627e01daf5b4f1da24bf1d0c500e10898
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Thu Apr 19 12:34:00 2018 +0530

    powerpc64/ftrace: Add a field in paca to disable ftrace in unsafe code paths
    
    We have some C code that we call into from real mode where we cannot
    take any exceptions. Though the C functions themselves are mostly safe,
    if these functions are traced, there is a possibility that we may take
    an exception. For instance, in certain conditions, the ftrace code uses
    WARN(), which uses a 'trap' to do its job.
    
    For such scenarios, introduce a new field in paca 'ftrace_enabled',
    which is checked on ftrace entry before continuing. This field can then
    be set to zero to disable/pause ftrace, and set to a non-zero value to
    resume ftrace.
    
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6bee65f3cfd3..262c44a90ea1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -180,6 +180,7 @@ int main(void)
 	OFFSET(PACAKMSR, paca_struct, kernel_msr);
 	OFFSET(PACAIRQSOFTMASK, paca_struct, irq_soft_mask);
 	OFFSET(PACAIRQHAPPENED, paca_struct, irq_happened);
+	OFFSET(PACA_FTRACE_ENABLED, paca_struct, ftrace_enabled);
 #ifdef CONFIG_PPC_BOOK3S
 	OFFSET(PACACONTEXTID, paca_struct, mm_ctx_id);
 #ifdef CONFIG_PPC_MM_SLICES

commit 0d55303c51a4f35f674617e415632d492b596c26
Author: Deepa Dinamani <deepa.kernel@gmail.com>
Date:   Tue Mar 13 21:03:25 2018 -0700

    compat: Move compat_timespec/ timeval to compat_time.h
    
    All the current architecture specific defines for these
    are the same. Refactor these common defines to a common
    header file.
    
    The new common linux/compat_time.h is also useful as it
    will eventually be used to hold all the defines that
    are needed for compat time types that support non y2038
    safe types. New architectures need not have to define these
    new types as they will only use new y2038 safe syscalls.
    This file can be deleted after y2038 when we stop supporting
    non y2038 safe syscalls.
    
    The patch also requires an operation similar to:
    
    git grep "asm/compat\.h" | cut -d ":" -f 1 |  xargs -n 1 sed -i -e "s%asm/compat.h%linux/compat.h%g"
    
    Cc: acme@kernel.org
    Cc: benh@kernel.crashing.org
    Cc: borntraeger@de.ibm.com
    Cc: catalin.marinas@arm.com
    Cc: cmetcalf@mellanox.com
    Cc: cohuck@redhat.com
    Cc: davem@davemloft.net
    Cc: deller@gmx.de
    Cc: devel@driverdev.osuosl.org
    Cc: gerald.schaefer@de.ibm.com
    Cc: gregkh@linuxfoundation.org
    Cc: heiko.carstens@de.ibm.com
    Cc: hoeppner@linux.vnet.ibm.com
    Cc: hpa@zytor.com
    Cc: jejb@parisc-linux.org
    Cc: jwi@linux.vnet.ibm.com
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: linux-s390@vger.kernel.org
    Cc: mark.rutland@arm.com
    Cc: mingo@redhat.com
    Cc: mpe@ellerman.id.au
    Cc: oberpar@linux.vnet.ibm.com
    Cc: oprofile-list@lists.sf.net
    Cc: paulus@samba.org
    Cc: peterz@infradead.org
    Cc: ralf@linux-mips.org
    Cc: rostedt@goodmis.org
    Cc: rric@kernel.org
    Cc: schwidefsky@de.ibm.com
    Cc: sebott@linux.vnet.ibm.com
    Cc: sparclinux@vger.kernel.org
    Cc: sth@linux.vnet.ibm.com
    Cc: ubraun@linux.vnet.ibm.com
    Cc: will.deacon@arm.com
    Cc: x86@kernel.org
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Deepa Dinamani <deepa.kernel@gmail.com>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Acked-by: James Hogan <jhogan@kernel.org>
    Acked-by: Helge Deller <deller@gmx.de>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6bee65f3cfd3..a77528db474c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -13,6 +13,7 @@
  * 2 of the License, or (at your option) any later version.
  */
 
+#include <linux/compat.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
@@ -42,7 +43,6 @@
 #include <asm/paca.h>
 #include <asm/lppaca.h>
 #include <asm/cache.h>
-#include <asm/compat.h>
 #include <asm/mmu.h>
 #include <asm/hvcall.h>
 #include <asm/xics.h>

commit f437c51748fa1dd423a878c870ad203843a51c8d
Merge: 872a100a49c3 29ab6c4708a5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Mar 31 00:11:24 2018 +1100

    Merge branch 'topic/paca' into next
    
    Bring in yet another series that touches KVM code, and might need to
    be merged into the kvm-ppc branch to resolve conflicts.
    
    This required some changes in pnv_power9_force_smt4_catch/release()
    due to the paca array becomming an array of pointers.

commit 8e0b634b132752ec3eba50afb952502b1a87d6ba
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:11 2018 +1000

    powerpc/64s: Do not allocate lppaca if we are not virtualized
    
    The "lppaca" is a structure registered with the hypervisor. This is
    unnecessary when running on non-virtualised platforms. One field from
    the lppaca (pmcregs_in_use) is also used by the host, so move the host
    part out into the paca (lppaca field is still updated in
    guest mode).
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Fix non-pseries build with some #ifdefs]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ea5eb91b836e..bbde55f408c7 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -221,12 +221,17 @@ int main(void)
 	OFFSET(PACA_EXMC, paca_struct, exmc);
 	OFFSET(PACA_EXSLB, paca_struct, exslb);
 	OFFSET(PACA_EXNMI, paca_struct, exnmi);
+#ifdef CONFIG_PPC_PSERIES
 	OFFSET(PACALPPACAPTR, paca_struct, lppaca_ptr);
+#endif
 	OFFSET(PACA_SLBSHADOWPTR, paca_struct, slb_shadow_ptr);
 	OFFSET(SLBSHADOW_STACKVSID, slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid);
 	OFFSET(SLBSHADOW_STACKESID, slb_shadow, save_area[SLB_NUM_BOLTED - 1].esid);
 	OFFSET(SLBSHADOW_SAVEAREA, slb_shadow, save_area);
 	OFFSET(LPPACA_PMCINUSE, lppaca, pmcregs_in_use);
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	OFFSET(PACA_PMCINUSE, paca_struct, pmcregs_in_use);
+#endif
 	OFFSET(LPPACA_DTLIDX, lppaca, dtl_idx);
 	OFFSET(LPPACA_YIELDCOUNT, lppaca, yield_count);
 	OFFSET(PACA_DTL_RIDX, paca_struct, dtl_ridx);

commit 4bb3c7a0208fc13ca70598efd109901a7cd45ae7
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Mar 21 21:32:01 2018 +1100

    KVM: PPC: Book3S HV: Work around transactional memory bugs in POWER9
    
    POWER9 has hardware bugs relating to transactional memory and thread
    reconfiguration (changes to hardware SMT mode).  Specifically, the core
    does not have enough storage to store a complete checkpoint of all the
    architected state for all four threads.  The DD2.2 version of POWER9
    includes hardware modifications designed to allow hypervisor software
    to implement workarounds for these problems.  This patch implements
    those workarounds in KVM code so that KVM guests see a full, working
    transactional memory implementation.
    
    The problems center around the use of TM suspended state, where the
    CPU has a checkpointed state but execution is not transactional.  The
    workaround is to implement a "fake suspend" state, which looks to the
    guest like suspended state but the CPU does not store a checkpoint.
    In this state, any instruction that would cause a transition to
    transactional state (rfid, rfebb, mtmsrd, tresume) or would use the
    checkpointed state (treclaim) causes a "soft patch" interrupt (vector
    0x1500) to the hypervisor so that it can be emulated.  The trechkpt
    instruction also causes a soft patch interrupt.
    
    On POWER9 DD2.2, we avoid returning to the guest in any state which
    would require a checkpoint to be present.  The trechkpt in the guest
    entry path which would normally create that checkpoint is replaced by
    either a transition to fake suspend state, if the guest is in suspend
    state, or a rollback to the pre-transactional state if the guest is in
    transactional state.  Fake suspend state is indicated by a flag in the
    PACA plus a new bit in the PSSCR.  The new PSSCR bit is write-only and
    reads back as 0.
    
    On exit from the guest, if the guest is in fake suspend state, we still
    do the treclaim instruction as we would in real suspend state, in order
    to get into non-transactional state, but we do not save the resulting
    register state since there was no checkpoint.
    
    Emulation of the instructions that cause a softpatch interrupt is
    handled in two paths.  If the guest is in real suspend mode, we call
    kvmhv_p9_tm_emulation_early() to handle the cases where the guest is
    transitioning to transactional state.  This is called before we do the
    treclaim in the guest exit path; because we haven't done treclaim, we
    can get back to the guest with the transaction still active.  If the
    instruction is a case that kvmhv_p9_tm_emulation_early() doesn't
    handle, or if the guest is in fake suspend state, then we proceed to
    do the complete guest exit path and subsequently call
    kvmhv_p9_tm_emulation() in host context with the MMU on.  This handles
    all the cases including the cases that generate program interrupts
    (illegal instruction or TM Bad Thing) and facility unavailable
    interrupts.
    
    The emulation is reasonably straightforward and is mostly concerned
    with checking for exception conditions and updating the state of
    registers such as MSR and CR0.  The treclaim emulation takes care to
    ensure that the TEXASR register gets updated as if it were the guest
    treclaim instruction that had done failure recording, not the treclaim
    done in hypervisor state in the guest exit path.
    
    With this, the KVM_CAP_PPC_HTM capability returns true (1) even if
    transactional memory is not available to host userspace.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index dbefe30d4daa..daf809a9b88e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -568,6 +568,7 @@ int main(void)
 	OFFSET(VCPU_TFHAR, kvm_vcpu, arch.tfhar);
 	OFFSET(VCPU_TFIAR, kvm_vcpu, arch.tfiar);
 	OFFSET(VCPU_TEXASR, kvm_vcpu, arch.texasr);
+	OFFSET(VCPU_ORIG_TEXASR, kvm_vcpu, arch.orig_texasr);
 	OFFSET(VCPU_GPR_TM, kvm_vcpu, arch.gpr_tm);
 	OFFSET(VCPU_FPRS_TM, kvm_vcpu, arch.fp_tm.fpr);
 	OFFSET(VCPU_VRS_TM, kvm_vcpu, arch.vr_tm.vr);
@@ -650,6 +651,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_HOST_IPI, host_ipi);
 	HSTATE_FIELD(HSTATE_PTID, ptid);
 	HSTATE_FIELD(HSTATE_TID, tid);
+	HSTATE_FIELD(HSTATE_FAKE_SUSPEND, fake_suspend);
 	HSTATE_FIELD(HSTATE_MMCR0, host_mmcr[0]);
 	HSTATE_FIELD(HSTATE_MMCR1, host_mmcr[1]);
 	HSTATE_FIELD(HSTATE_MMCRA, host_mmcr[2]);

commit 7672691a08c886e53ccbf8cdca406f8c92ec7a20
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Mar 21 21:32:00 2018 +1100

    powerpc/powernv: Provide a way to force a core into SMT4 mode
    
    POWER9 processors up to and including "Nimbus" v2.2 have hardware
    bugs relating to transactional memory and thread reconfiguration.
    One of these bugs has a workaround which is to get the core into
    SMT4 state temporarily.  This workaround is only needed when
    running bare-metal.
    
    This patch provides a function which gets the core into SMT4 mode
    by preventing threads from going to a stop state, and waking up
    those which are already in a stop state.  Once at least 3 threads
    are not in a stop state, the core will be in SMT4 and we can
    continue.
    
    To do this, we add a "dont_stop" flag to the paca to tell the
    thread not to go into a stop state.  If this flag is set,
    power9_idle_stop() just returns immediately with a return value
    of 0.  The pnv_power9_force_smt4_catch() function does the following:
    
    1. Set the dont_stop flag for each thread in the core, except
       ourselves (in fact we use an atomic_inc() in case more than
       one thread is calling this function concurrently).
    2. See how many threads are awake, indicated by their
       requested_psscr field in the paca being 0.  If this is at
       least 3, skip to step 5.
    3. Send a doorbell interrupt to each thread that was seen as
       being in a stop state in step 2.
    4. Until at least 3 threads are awake, scan the threads to which
       we sent a doorbell interrupt and check if they are awake now.
    
    This relies on the following properties:
    
    - Once dont_stop is non-zero, requested_psccr can't go from zero to
      non-zero, except transiently (and without the thread doing stop).
    - requested_psscr being zero guarantees that the thread isn't in
      a state-losing stop state where thread reconfiguration could occur.
    - Doing stop with a PSSCR value of 0 won't be a state-losing stop
      and thus won't allow thread reconfiguration.
    - Once threads_per_core/2 + 1 (i.e. 3) threads are awake, the core
      must be in SMT4 mode, since SMT modes are powers of 2.
    
    This does add a sync to power9_idle_stop(), which is necessary to
    provide the correct ordering between setting requested_psscr and
    checking dont_stop.  The overhead of the sync should be unnoticeable
    compared to the latency of going into and out of a stop state.
    
    Because some objected to incurring this extra latency on systems where
    the XER[SO] bug is not relevant, I have put the test in
    power9_idle_stop inside a feature section.  This means that
    pnv_power9_force_smt4_catch() WILL NOT WORK correctly on systems
    without the CPU_FTR_P9_TM_XER_SO_BUG feature bit set, and will
    probably hang the system.
    
    In order to cater for uses where the caller has an operation that
    has to be done while the core is in SMT4, the core continues to be
    kept in SMT4 after pnv_power9_force_smt4_catch() function returns,
    until the pnv_power9_force_smt4_release() function is called.
    It undoes the effect of step 1 above and allows the other threads
    to go into a stop state.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ea5eb91b836e..dbefe30d4daa 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -759,6 +759,7 @@ int main(void)
 	OFFSET(PACA_SUBCORE_SIBLING_MASK, paca_struct, subcore_sibling_mask);
 	OFFSET(PACA_SIBLING_PACA_PTRS, paca_struct, thread_sibling_pacas);
 	OFFSET(PACA_REQ_PSSCR, paca_struct, requested_psscr);
+	OFFSET(PACA_DONT_STOP, paca_struct, dont_stop);
 #define STOP_SPR(x, f)	OFFSET(x, paca_struct, stop_sprs.f)
 	STOP_SPR(STOP_PID, pid);
 	STOP_SPR(STOP_LDBAR, ldbar);

commit 15303ba5d1cd9b28d03a980456c0978c0ea3b208
Merge: 9a61df9e5f74 1ab03c072feb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 10 13:16:35 2018 -0800

    Merge tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "ARM:
    
       - icache invalidation optimizations, improving VM startup time
    
       - support for forwarded level-triggered interrupts, improving
         performance for timers and passthrough platform devices
    
       - a small fix for power-management notifiers, and some cosmetic
         changes
    
      PPC:
    
       - add MMIO emulation for vector loads and stores
    
       - allow HPT guests to run on a radix host on POWER9 v2.2 CPUs without
         requiring the complex thread synchronization of older CPU versions
    
       - improve the handling of escalation interrupts with the XIVE
         interrupt controller
    
       - support decrement register migration
    
       - various cleanups and bugfixes.
    
      s390:
    
       - Cornelia Huck passed maintainership to Janosch Frank
    
       - exitless interrupts for emulated devices
    
       - cleanup of cpuflag handling
    
       - kvm_stat counter improvements
    
       - VSIE improvements
    
       - mm cleanup
    
      x86:
    
       - hypervisor part of SEV
    
       - UMIP, RDPID, and MSR_SMI_COUNT emulation
    
       - paravirtualized TLB shootdown using the new KVM_VCPU_PREEMPTED bit
    
       - allow guests to see TOPOEXT, GFNI, VAES, VPCLMULQDQ, and more
         AVX512 features
    
       - show vcpu id in its anonymous inode name
    
       - many fixes and cleanups
    
       - per-VCPU MSR bitmaps (already merged through x86/pti branch)
    
       - stable KVM clock when nesting on Hyper-V (merged through
         x86/hyperv)"
    
    * tag 'kvm-4.16-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (197 commits)
      KVM: PPC: Book3S: Add MMIO emulation for VMX instructions
      KVM: PPC: Book3S HV: Branch inside feature section
      KVM: PPC: Book3S HV: Make HPT resizing work on POWER9
      KVM: PPC: Book3S HV: Fix handling of secondary HPTEG in HPT resizing code
      KVM: PPC: Book3S PR: Fix broken select due to misspelling
      KVM: x86: don't forget vcpu_put() in kvm_arch_vcpu_ioctl_set_sregs()
      KVM: PPC: Book3S PR: Fix svcpu copying with preemption enabled
      KVM: PPC: Book3S HV: Drop locks before reading guest memory
      kvm: x86: remove efer_reload entry in kvm_vcpu_stat
      KVM: x86: AMD Processor Topology Information
      x86/kvm/vmx: do not use vm-exit instruction length for fast MMIO when running nested
      kvm: embed vcpu id to dentry of vcpu anon inode
      kvm: Map PFN-type memory regions as writable (if possible)
      x86/kvm: Make it compile on 32bit and with HYPYERVISOR_GUEST=n
      KVM: arm/arm64: Fixup userspace irqchip static key optimization
      KVM: arm/arm64: Fix userspace_irqchip_in_use counting
      KVM: arm/arm64: Fix incorrect timer_is_pending logic
      MAINTAINERS: update KVM/s390 maintainers
      MAINTAINERS: add Halil as additional vfio-ccw maintainer
      MAINTAINERS: add David as a reviewer for KVM/s390
      ...

commit d2b9b2079e23c1ab80ce1d7670d5e1994468a881
Merge: 7bf14c28ee77 9b9b13a6d153
Author: Radim Krčmář <rkrcmar@redhat.com>
Date:   Thu Feb 1 16:13:07 2018 +0100

    Merge tag 'kvm-ppc-next-4.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    PPC KVM update for 4.16
    
    - Allow HPT guests to run on a radix host on POWER9 v2.2 CPUs
      without requiring the complex thread synchronization that earlier
      CPU versions required.
    
    - A series from Ben Herrenschmidt to improve the handling of
      escalation interrupts with the XIVE interrupt controller.
    
    - Provide for the decrementer register to be copied across on
      migration.
    
    - Various minor cleanups and bugfixes.

commit bdcb1aefc5b3f7d0f1dc8b02673602bca2ff7a4b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Jan 17 23:58:18 2018 +1000

    powerpc/64s: Improve RFI L1-D cache flush fallback
    
    The fallback RFI flush is used when firmware does not provide a way
    to flush the cache. It's a "displacement flush" that evicts useful
    data by displacing it with an uninteresting buffer.
    
    The flush has to take care to work with implementation specific cache
    replacment policies, so the recipe has been in flux. The initial
    slow but conservative approach is to touch all lines of a congruence
    class, with dependencies between each load. It has since been
    determined that a linear pattern of loads without dependencies is
    sufficient, and is significantly faster.
    
    Measuring the speed of a null syscall with RFI fallback flush enabled
    gives the relative improvement:
    
    P8 - 1.83x
    P9 - 1.75x
    
    The flush also becomes simpler and more adaptable to different cache
    geometries.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index fa5c4125f42a..88b84ac76b53 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -239,8 +239,7 @@ int main(void)
 	OFFSET(PACA_IN_NMI, paca_struct, in_nmi);
 	OFFSET(PACA_RFI_FLUSH_FALLBACK_AREA, paca_struct, rfi_flush_fallback_area);
 	OFFSET(PACA_EXRFI, paca_struct, exrfi);
-	OFFSET(PACA_L1D_FLUSH_CONGRUENCE, paca_struct, l1d_flush_congruence);
-	OFFSET(PACA_L1D_FLUSH_SETS, paca_struct, l1d_flush_sets);
+	OFFSET(PACA_L1D_FLUSH_SIZE, paca_struct, l1d_flush_size);
 
 #endif
 	OFFSET(PACAHWCPUID, paca_struct, hw_cpu_id);

commit ebf0b6a8b1e445d2be66087732aafcda12ab9f59
Merge: 5400fc229e60 1b689a95ce74
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jan 21 23:21:14 2018 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch from the 4.15 cycle.
    
    Unusually the fixes branch saw some significant features merged,
    notably the RFI flush patches, so we want the code in next to be
    tested against that, to avoid any surprises when the two are merged.
    
    There's also some other work on the panic handling that was reverted
    in fixes and we now want to do properly in next, which would conflict.
    
    And we also fix a few other minor merge conflicts.

commit 4e26bc4a4ed683c42ba45f09050575a671c6f1f4
Author: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
Date:   Wed Dec 20 09:25:50 2017 +0530

    powerpc/64: Rename soft_enabled to irq_soft_mask
    
    Rename the paca->soft_enabled to paca->irq_soft_mask as it is no
    longer used as a flag for interrupt state, but a mask.
    
    Signed-off-by: Madhavan Srinivasan <maddy@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 3f6316bcde4e..43a6967d48ad 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -178,7 +178,7 @@ int main(void)
 	OFFSET(PACATOC, paca_struct, kernel_toc);
 	OFFSET(PACAKBASE, paca_struct, kernelbase);
 	OFFSET(PACAKMSR, paca_struct, kernel_msr);
-	OFFSET(PACASOFTIRQEN, paca_struct, soft_enabled);
+	OFFSET(PACAIRQSOFTMASK, paca_struct, irq_soft_mask);
 	OFFSET(PACAIRQHAPPENED, paca_struct, irq_happened);
 #ifdef CONFIG_PPC_BOOK3S
 	OFFSET(PACACONTEXTID, paca_struct, mm_ctx_id);

commit 9b9b13a6d1537ddc4caccd6f1c41b78edbc08437
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jan 12 13:37:16 2018 +1100

    KVM: PPC: Book3S HV: Keep XIVE escalation interrupt masked unless ceded
    
    This works on top of the single escalation support. When in single
    escalation, with this change, we will keep the escalation interrupt
    disabled unless the VCPU is in H_CEDE (idle). In any other case, we
    know the VCPU will be rescheduled and thus there is no need to take
    escalation interrupts in the host whenever a guest interrupt fires.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 825089cf3e23..1672dffd94e2 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -734,6 +734,9 @@ int main(void)
 	DEFINE(VCPU_XIVE_CAM_WORD, offsetof(struct kvm_vcpu,
 					    arch.xive_cam_word));
 	DEFINE(VCPU_XIVE_PUSHED, offsetof(struct kvm_vcpu, arch.xive_pushed));
+	DEFINE(VCPU_XIVE_ESC_ON, offsetof(struct kvm_vcpu, arch.xive_esc_on));
+	DEFINE(VCPU_XIVE_ESC_RADDR, offsetof(struct kvm_vcpu, arch.xive_esc_raddr));
+	DEFINE(VCPU_XIVE_ESC_VADDR, offsetof(struct kvm_vcpu, arch.xive_esc_vaddr));
 #endif
 
 #ifdef CONFIG_KVM_EXIT_TIMING

commit 2267ea7661798a42f0da648a2970e2a03f4bc370
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jan 12 13:37:13 2018 +1100

    KVM: PPC: Book3S HV: Don't use existing "prodded" flag for XIVE escalations
    
    The prodded flag is only cleared at the beginning of H_CEDE,
    so every time we have an escalation, we will cause the *next*
    H_CEDE to return immediately.
    
    Instead use a dedicated "irq_pending" flag to indicate that
    a guest interrupt is pending for the VCPU. We don't reuse the
    existing exception bitmap so as to avoid expensive atomic ops.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6b958414b4e0..825089cf3e23 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -514,6 +514,7 @@ int main(void)
 	OFFSET(VCPU_PENDING_EXC, kvm_vcpu, arch.pending_exceptions);
 	OFFSET(VCPU_CEDED, kvm_vcpu, arch.ceded);
 	OFFSET(VCPU_PRODDED, kvm_vcpu, arch.prodded);
+	OFFSET(VCPU_IRQ_PENDING, kvm_vcpu, arch.irq_pending);
 	OFFSET(VCPU_DBELL_REQ, kvm_vcpu, arch.doorbell_request);
 	OFFSET(VCPU_MMCR, kvm_vcpu, arch.mmcr);
 	OFFSET(VCPU_PMC, kvm_vcpu, arch.pmc);

commit aa8a5e0062ac940f7659394f4817c948dc8c0667
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jan 10 03:07:15 2018 +1100

    powerpc/64s: Add support for RFI flush of L1-D cache
    
    On some CPUs we can prevent the Meltdown vulnerability by flushing the
    L1-D cache on exit from kernel to user mode, and from hypervisor to
    guest.
    
    This is known to be the case on at least Power7, Power8 and Power9. At
    this time we do not know the status of the vulnerability on other CPUs
    such as the 970 (Apple G5), pasemi CPUs (AmigaOne X1000) or Freescale
    CPUs. As more information comes to light we can enable this, or other
    mechanisms on those CPUs.
    
    The vulnerability occurs when the load of an architecturally
    inaccessible memory region (eg. userspace load of kernel memory) is
    speculatively executed to the point where its result can influence the
    address of a subsequent speculatively executed load.
    
    In order for that to happen, the first load must hit in the L1,
    because before the load is sent to the L2 the permission check is
    performed. Therefore if no kernel addresses hit in the L1 the
    vulnerability can not occur. We can ensure that is the case by
    flushing the L1 whenever we return to userspace. Similarly for
    hypervisor vs guest.
    
    In order to flush the L1-D cache on exit, we add a section of nops at
    each (h)rfi location that returns to a lower privileged context, and
    patch that with some sequence. Newer firmwares are able to advertise
    to us that there is a special nop instruction that flushes the L1-D.
    If we do not see that advertised, we fall back to doing a displacement
    flush in software.
    
    For guest kernels we support migration between some CPU versions, and
    different CPUs may use different flush instructions. So that we are
    prepared to migrate to a machine with a different flush instruction
    activated, we may have to patch more than one flush instruction at
    boot if the hypervisor tells us to.
    
    In the end this patch is mostly the work of Nicholas Piggin and
    Michael Ellerman. However a cast of thousands contributed to analysis
    of the issue, earlier versions of the patch, back ports testing etc.
    Many thanks to all of them.
    
    Tested-by: Jon Masters <jcm@redhat.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6b958414b4e0..f390d57cf2e1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -237,6 +237,11 @@ int main(void)
 	OFFSET(PACA_NMI_EMERG_SP, paca_struct, nmi_emergency_sp);
 	OFFSET(PACA_IN_MCE, paca_struct, in_mce);
 	OFFSET(PACA_IN_NMI, paca_struct, in_nmi);
+	OFFSET(PACA_RFI_FLUSH_FALLBACK_AREA, paca_struct, rfi_flush_fallback_area);
+	OFFSET(PACA_EXRFI, paca_struct, exrfi);
+	OFFSET(PACA_L1D_FLUSH_CONGRUENCE, paca_struct, l1d_flush_congruence);
+	OFFSET(PACA_L1D_FLUSH_SETS, paca_struct, l1d_flush_sets);
+
 #endif
 	OFFSET(PACAHWCPUID, paca_struct, hw_cpu_id);
 	OFFSET(PACAKEXECSTATE, paca_struct, kexec_state);

commit 5c929885f1bb4b77f85b1769c49405a0e0f154a1
Author: Santosh Sivaraj <santosh@fossix.org>
Date:   Mon Oct 16 11:19:14 2017 +0530

    powerpc/vdso64: Add support for CLOCK_{REALTIME/MONOTONIC}_COARSE
    
    Current vDSO64 implementation does not have support for coarse clocks
    (CLOCK_MONOTONIC_COARSE, CLOCK_REALTIME_COARSE), for which it falls back
    to system call, increasing the response time, vDSO implementation reduces
    the cycle time. Below is a benchmark of the difference in execution times.
    
    (Non-coarse clocks are also included just for completion)
    
    clock-gettime-realtime: syscall: 172 nsec/call
    clock-gettime-realtime:    libc: 28 nsec/call
    clock-gettime-realtime:    vdso: 22 nsec/call
    clock-gettime-monotonic: syscall: 171 nsec/call
    clock-gettime-monotonic:    libc: 30 nsec/call
    clock-gettime-monotonic:    vdso: 25 nsec/call
    clock-gettime-realtime-coarse: syscall: 153 nsec/call
    clock-gettime-realtime-coarse:    libc: 16 nsec/call
    clock-gettime-realtime-coarse:    vdso: 10 nsec/call
    clock-gettime-monotonic-coarse: syscall: 167 nsec/call
    clock-gettime-monotonic-coarse:    libc: 17 nsec/call
    clock-gettime-monotonic-coarse:    vdso: 11 nsec/call
    
    CC: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Reviewed-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Santosh Sivaraj <santosh@fossix.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6b958414b4e0..3f6316bcde4e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -396,6 +396,8 @@ int main(void)
 	/* Other bits used by the vdso */
 	DEFINE(CLOCK_REALTIME, CLOCK_REALTIME);
 	DEFINE(CLOCK_MONOTONIC, CLOCK_MONOTONIC);
+	DEFINE(CLOCK_REALTIME_COARSE, CLOCK_REALTIME_COARSE);
+	DEFINE(CLOCK_MONOTONIC_COARSE, CLOCK_MONOTONIC_COARSE);
 	DEFINE(NSEC_PER_SEC, NSEC_PER_SEC);
 	DEFINE(CLOCK_REALTIME_RES, MONOTONIC_RES_NSEC);
 

commit 974aa5630b318938273d7efe7a2cf031c7b927db
Merge: 441692aafc17 a6014f1ab708
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Nov 16 13:00:24 2017 -0800

    Merge tag 'kvm-4.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Radim Krčmář:
     "First batch of KVM changes for 4.15
    
      Common:
       - Python 3 support in kvm_stat
       - Accounting of slabs to kmemcg
    
      ARM:
       - Optimized arch timer handling for KVM/ARM
       - Improvements to the VGIC ITS code and introduction of an ITS reset
         ioctl
       - Unification of the 32-bit fault injection logic
       - More exact external abort matching logic
    
      PPC:
       - Support for running hashed page table (HPT) MMU mode on a host that
         is using the radix MMU mode; single threaded mode on POWER 9 is
         added as a pre-requisite
       - Resolution of merge conflicts with the last second 4.14 HPT fixes
       - Fixes and cleanups
    
      s390:
       - Some initial preparation patches for exitless interrupts and crypto
       - New capability for AIS migration
       - Fixes
    
      x86:
       - Improved emulation of LAPIC timer mode changes, MCi_STATUS MSRs,
         and after-reset state
       - Refined dependencies for VMX features
       - Fixes for nested SMI injection
       - A lot of cleanups"
    
    * tag 'kvm-4.15-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (89 commits)
      KVM: s390: provide a capability for AIS state migration
      KVM: s390: clear_io_irq() requests are not expected for adapter interrupts
      KVM: s390: abstract conversion between isc and enum irq_types
      KVM: s390: vsie: use common code functions for pinning
      KVM: s390: SIE considerations for AP Queue virtualization
      KVM: s390: document memory ordering for kvm_s390_vcpu_wakeup
      KVM: PPC: Book3S HV: Cosmetic post-merge cleanups
      KVM: arm/arm64: fix the incompatible matching for external abort
      KVM: arm/arm64: Unify 32bit fault injection
      KVM: arm/arm64: vgic-its: Implement KVM_DEV_ARM_ITS_CTRL_RESET
      KVM: arm/arm64: Document KVM_DEV_ARM_ITS_CTRL_RESET
      KVM: arm/arm64: vgic-its: Free caches when GITS_BASER Valid bit is cleared
      KVM: arm/arm64: vgic-its: New helper functions to free the caches
      KVM: arm/arm64: vgic-its: Remove kvm_its_unmap_device
      arm/arm64: KVM: Load the timer state when enabling the timer
      KVM: arm/arm64: Rework kvm_timer_should_fire
      KVM: arm/arm64: Get rid of kvm_timer_flush_hwstate
      KVM: arm/arm64: Avoid phys timer emulation in vcpu entry/exit
      KVM: arm/arm64: Move phys_timer_emulate function
      KVM: arm/arm64: Use kvm_arm_timer_set/get_reg for guest register traps
      ...

commit 4722476bce283149986a3463f61009dec0e7f6a1
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Nov 10 04:27:40 2017 +1100

    powerpc/64s: mm_context.addr_limit is only used on hash
    
    Radix keeps no meaningful state in addr_limit, so remove it from radix
    code and rename to slb_addr_limit to make it clear it applies to hash
    only.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 200623e71474..9aace433491a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -185,7 +185,7 @@ int main(void)
 #ifdef CONFIG_PPC_MM_SLICES
 	OFFSET(PACALOWSLICESPSIZE, paca_struct, mm_ctx_low_slices_psize);
 	OFFSET(PACAHIGHSLICEPSIZE, paca_struct, mm_ctx_high_slices_psize);
-	DEFINE(PACA_ADDR_LIMIT, offsetof(struct paca_struct, addr_limit));
+	OFFSET(PACA_SLB_ADDR_LIMIT, paca_struct, mm_ctx_slb_addr_limit);
 	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
 #endif /* CONFIG_PPC_MM_SLICES */
 #endif

commit 4e003747043d57aa75c9762fa148ef38afe68dd8
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Oct 19 15:08:43 2017 +1100

    powerpc/64s: Replace CONFIG_PPC_STD_MMU_64 with CONFIG_PPC_BOOK3S_64
    
    CONFIG_PPC_STD_MMU_64 indicates support for the "standard" powerpc MMU
    on 64-bit CPUs. The "standard" MMU refers to the hash page table MMU
    found in "server" processors, from IBM mainly.
    
    Currently CONFIG_PPC_STD_MMU_64 is == CONFIG_PPC_BOOK3S_64. While it's
    annoying to have two symbols that always have the same value, it's not
    quite annoying enough to bother removing one.
    
    However with the arrival of Power9, we now have the situation where
    CONFIG_PPC_STD_MMU_64 is enabled, but the kernel is running using the
    Radix MMU - *not* the "standard" MMU. So it is now actively confusing
    to use it, because it implies that code is disabled or inactive when
    the Radix MMU is in use, however that is not necessarily true.
    
    So s/CONFIG_PPC_STD_MMU_64/CONFIG_PPC_BOOK3S_64/, and do some minor
    formatting updates of some of the affected lines.
    
    This will be a pain for backports, but c'est la vie.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8cfb20e38cfe..200623e71474 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -208,7 +208,7 @@ int main(void)
 	OFFSET(TCD_ESEL_FIRST, tlb_core_data, esel_first);
 #endif /* CONFIG_PPC_BOOK3E */
 
-#ifdef CONFIG_PPC_STD_MMU_64
+#ifdef CONFIG_PPC_BOOK3S_64
 	OFFSET(PACASLBCACHE, paca_struct, slb_cache);
 	OFFSET(PACASLBCACHEPTR, paca_struct, slb_cache_ptr);
 	OFFSET(PACAVMALLOCSLLP, paca_struct, vmalloc_sllp);
@@ -230,7 +230,7 @@ int main(void)
 	OFFSET(LPPACA_DTLIDX, lppaca, dtl_idx);
 	OFFSET(LPPACA_YIELDCOUNT, lppaca, yield_count);
 	OFFSET(PACA_DTL_RIDX, paca_struct, dtl_ridx);
-#endif /* CONFIG_PPC_STD_MMU_64 */
+#endif /* CONFIG_PPC_BOOK3S_64 */
 	OFFSET(PACAEMERGSP, paca_struct, emergency_sp);
 #ifdef CONFIG_PPC_BOOK3S_64
 	OFFSET(PACAMCEMERGSP, paca_struct, mc_emergency_sp);

commit c01015091a77035de1939ef106bfbcaf9a21395f
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Oct 19 14:11:23 2017 +1100

    KVM: PPC: Book3S HV: Run HPT guests on POWER9 radix hosts
    
    This patch removes the restriction that a radix host can only run
    radix guests, allowing us to run HPT (hashed page table) guests as
    well.  This is useful because it provides a way to run old guest
    kernels that know about POWER8 but not POWER9.
    
    Unfortunately, POWER9 currently has a restriction that all threads
    in a given code must either all be in HPT mode, or all in radix mode.
    This means that when entering a HPT guest, we have to obtain control
    of all 4 threads in the core and get them to switch their LPIDR and
    LPCR registers, even if they are not going to run a guest.  On guest
    exit we also have to get all threads to switch LPIDR and LPCR back
    to host values.
    
    To make this feasible, we require that KVM not be in the "independent
    threads" mode, and that the CPU cores be in single-threaded mode from
    the host kernel's perspective (only thread 0 online; threads 1, 2 and
    3 offline).  That allows us to use the same code as on POWER8 for
    obtaining control of the secondary threads.
    
    To manage the LPCR/LPIDR changes required, we extend the kvm_split_info
    struct to contain the information needed by the secondary threads.
    All threads perform a barrier synchronization (where all threads wait
    for every other thread to reach the synchronization point) on guest
    entry, both before and after loading LPCR and LPIDR.  On guest exit,
    they all once again perform a barrier synchronization both before
    and after loading host values into LPCR and LPIDR.
    
    Finally, it is also currently necessary to flush the entire TLB every
    time we enter a HPT guest on a radix host.  We do this on thread 0
    with a loop of tlbiel instructions.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8cfb20e38cfe..519fad556113 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -642,6 +642,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_SAVED_XIRR, saved_xirr);
 	HSTATE_FIELD(HSTATE_HOST_IPI, host_ipi);
 	HSTATE_FIELD(HSTATE_PTID, ptid);
+	HSTATE_FIELD(HSTATE_TID, tid);
 	HSTATE_FIELD(HSTATE_MMCR0, host_mmcr[0]);
 	HSTATE_FIELD(HSTATE_MMCR1, host_mmcr[1]);
 	HSTATE_FIELD(HSTATE_MMCRA, host_mmcr[2]);
@@ -667,6 +668,8 @@ int main(void)
 	OFFSET(KVM_SPLIT_LDBAR, kvm_split_mode, ldbar);
 	OFFSET(KVM_SPLIT_DO_NAP, kvm_split_mode, do_nap);
 	OFFSET(KVM_SPLIT_NAPPED, kvm_split_mode, napped);
+	OFFSET(KVM_SPLIT_DO_SET, kvm_split_mode, do_set);
+	OFFSET(KVM_SPLIT_DO_RESTORE, kvm_split_mode, do_restore);
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit e1c1cfed54326fd2b17c78f0c85092167fc0783b
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Fri Jul 21 16:11:37 2017 +0530

    powerpc/powernv: Save/Restore additional SPRs for stop4 cpuidle
    
    The stop4 idle state on POWER9 is a deep idle state which loses
    hypervisor resources, but whose latency is low enough that it can be
    exposed via cpuidle.
    
    Until now, the deep idle states which lose hypervisor resources (eg:
    winkle) were only exposed via CPU-Hotplug.  Hence currently on wakeup
    from such states, barring a few SPRs which need to be restored to
    their older value, rest of the SPRS are reinitialized to their values
    corresponding to that at boot time.
    
    When stop4 is used in the context of cpuidle, we want these additional
    SPRs to be restored to their older value, to ensure that the context
    on the CPU coming back from idle is same as it was before going idle.
    
    In this patch, we define a SPR save area in PACA (since we have used
    up the volatile register space in the stack) and on POWER9, we restore
    SPRN_PID, SPRN_LDBAR, SPRN_FSCR, SPRN_HFSCR, SPRN_MMCRA, SPRN_MMCR1,
    SPRN_MMCR2 to the values they had before entering stop.
    
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6e95c2c19a7e..8cfb20e38cfe 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -746,6 +746,14 @@ int main(void)
 	OFFSET(PACA_SUBCORE_SIBLING_MASK, paca_struct, subcore_sibling_mask);
 	OFFSET(PACA_SIBLING_PACA_PTRS, paca_struct, thread_sibling_pacas);
 	OFFSET(PACA_REQ_PSSCR, paca_struct, requested_psscr);
+#define STOP_SPR(x, f)	OFFSET(x, paca_struct, stop_sprs.f)
+	STOP_SPR(STOP_PID, pid);
+	STOP_SPR(STOP_LDBAR, ldbar);
+	STOP_SPR(STOP_FSCR, fscr);
+	STOP_SPR(STOP_HFSCR, hfscr);
+	STOP_SPR(STOP_MMCR1, mmcr1);
+	STOP_SPR(STOP_MMCR2, mmcr2);
+	STOP_SPR(STOP_MMCRA, mmcra);
 #endif
 
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);

commit d691b7e7d1b5186eae62fd32adee65d3316bfdf6
Merge: b59eea554f57 1e0fc9d1eb2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 7 13:55:45 2017 -0700

    Merge tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for STRICT_KERNEL_RWX on 64-bit server CPUs.
    
       - Platform support for FSP2 (476fpe) board
    
       - Enable ZONE_DEVICE on 64-bit server CPUs.
    
       - Generic & powerpc spin loop primitives to optimise busy waiting
    
       - Convert VDSO update function to use new update_vsyscall() interface
    
       - Optimisations to hypercall/syscall/context-switch paths
    
       - Improvements to the CPU idle code on Power8 and Power9.
    
      As well as many other fixes and improvements.
    
      Thanks to: Akshay Adiga, Andrew Donnellan, Andrew Jeffery, Anshuman
      Khandual, Anton Blanchard, Balbir Singh, Benjamin Herrenschmidt,
      Christophe Leroy, Christophe Lombard, Colin Ian King, Dan Carpenter,
      Gautham R. Shenoy, Hari Bathini, Ian Munsie, Ivan Mikhaylov, Javier
      Martinez Canillas, Madhavan Srinivasan, Masahiro Yamada, Matt Brown,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Naveen N.
      Rao, Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pavel Machek,
      Russell Currey, Santosh Sivaraj, Stephen Rothwell, Thiago Jung
      Bauermann, Yang Li"
    
    * tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (158 commits)
      powerpc/Kconfig: Enable STRICT_KERNEL_RWX for some configs
      powerpc/mm/radix: Implement STRICT_RWX/mark_rodata_ro() for Radix
      powerpc/mm/hash: Implement mark_rodata_ro() for hash
      powerpc/vmlinux.lds: Align __init_begin to 16M
      powerpc/lib/code-patching: Use alternate map for patch_instruction()
      powerpc/xmon: Add patch_instruction() support for xmon
      powerpc/kprobes/optprobes: Use patch_instruction()
      powerpc/kprobes: Move kprobes over to patch_instruction()
      powerpc/mm/radix: Fix execute permissions for interrupt_vectors
      powerpc/pseries: Fix passing of pp0 in updatepp() and updateboltedpp()
      powerpc/64s: Blacklist rtas entry/exit from kprobes
      powerpc/64s: Blacklist functions invoked on a trap
      powerpc/64s: Un-blacklist system_call() from kprobes
      powerpc/64s: Move system_call() symbol to just after setting MSR_EE
      powerpc/64s: Blacklist system_call() and system_call_common() from kprobes
      powerpc/64s: Convert .L__replay_interrupt_return to a local label
      powerpc64/elfv1: Only dereference function descriptor for non-text symbols
      cxl: Export library to support IBM XSL
      powerpc/dts: Use #include "..." to include local DT
      powerpc/perf/hv-24x7: Aggregate result elements on POWER9 SMT8
      ...

commit aa9a95163638bd9acb3e1f61f48cd5a000e79f03
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon May 8 16:23:31 2017 +1000

    powerpc: Fix asm offsets to point to actual FP and VMX regs
    
    The asm code assumes the FP regs are at the start of fp_state. While
    this is true now, it may not always be the case and there is nothing
    enforcing it.
    
    This fixes the asm-offsets to point to the actual FP registers inside
    the fp_state.  Similarly for VMX.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9624851ca276..a7b5af32b89e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -100,12 +100,12 @@ int main(void)
 	OFFSET(THREAD_NORMSAVES, thread_struct, normsave[0]);
 #endif
 	OFFSET(THREAD_FPEXC_MODE, thread_struct, fpexc_mode);
-	OFFSET(THREAD_FPSTATE, thread_struct, fp_state);
+	OFFSET(THREAD_FPSTATE, thread_struct, fp_state.fpr);
 	OFFSET(THREAD_FPSAVEAREA, thread_struct, fp_save_area);
 	OFFSET(FPSTATE_FPSCR, thread_fp_state, fpscr);
 	OFFSET(THREAD_LOAD_FP, thread_struct, load_fp);
 #ifdef CONFIG_ALTIVEC
-	OFFSET(THREAD_VRSTATE, thread_struct, vr_state);
+	OFFSET(THREAD_VRSTATE, thread_struct, vr_state.vr);
 	OFFSET(THREAD_VRSAVEAREA, thread_struct, vr_save_area);
 	OFFSET(THREAD_VRSAVE, thread_struct, vrsave);
 	OFFSET(THREAD_USED_VR, thread_struct, used_vr);
@@ -145,9 +145,9 @@ int main(void)
 	OFFSET(THREAD_TM_PPR, thread_struct, tm_ppr);
 	OFFSET(THREAD_TM_DSCR, thread_struct, tm_dscr);
 	OFFSET(PT_CKPT_REGS, thread_struct, ckpt_regs);
-	OFFSET(THREAD_CKVRSTATE, thread_struct, ckvr_state);
+	OFFSET(THREAD_CKVRSTATE, thread_struct, ckvr_state.vr);
 	OFFSET(THREAD_CKVRSAVE, thread_struct, ckvrsave);
-	OFFSET(THREAD_CKFPSTATE, thread_struct, ckfp_state);
+	OFFSET(THREAD_CKFPSTATE, thread_struct, ckfp_state.fpr);
 	/* Local pt_regs on stack for Transactional Memory funcs. */
 	DEFINE(TM_FRAME_SIZE, STACK_FRAME_OVERHEAD +
 	       sizeof(struct pt_regs) + 16);

commit 134764ed6e12d9f99b3de68b8aaeae1ba842d91c
Author: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
Date:   Thu May 11 16:32:48 2017 +0530

    KVM: PPC: Book3S HV: Add new capability to control MCE behaviour
    
    This introduces a new KVM capability to control how KVM behaves
    on machine check exception (MCE) in HV KVM guests.
    
    If this capability has not been enabled, KVM redirects machine check
    exceptions to guest's 0x200 vector, if the address in error belongs to
    the guest. With this capability enabled, KVM will cause a guest exit
    with the exit reason indicating an NMI.
    
    The new capability is required to avoid problems if a new kernel/KVM
    is used with an old QEMU, running a guest that doesn't issue
    "ibm,nmi-register".  As old QEMU does not understand the NMI exit
    type, it treats it as a fatal error.  However, the guest could have
    handled the machine check error if the exception was delivered to
    guest's 0x200 interrupt vector instead of NMI exit in case of old
    QEMU.
    
    [paulus@ozlabs.org - Reworded the commit message to be clearer,
     enable only on HV KVM.]
    
    Signed-off-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 293fbdf96e7d..ae8e89e0d083 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -485,6 +485,7 @@ int main(void)
 	OFFSET(KVM_ENABLED_HCALLS, kvm, arch.enabled_hcalls);
 	OFFSET(KVM_VRMA_SLB_V, kvm, arch.vrma_slb_v);
 	OFFSET(KVM_RADIX, kvm, arch.radix);
+	OFFSET(KVM_FWNMI, kvm, arch.fwnmi_enabled);
 	OFFSET(VCPU_DSISR, kvm_vcpu, arch.shregs.dsisr);
 	OFFSET(VCPU_DAR, kvm_vcpu, arch.shregs.dar);
 	OFFSET(VCPU_VPA, kvm_vcpu, arch.vpa.pinned_addr);

commit a9af97aa0a12c30178dd7ad9af8887d5b9c4647b
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Jun 13 23:05:48 2017 +1000

    powerpc/64s: msgclr when handling doorbell exceptions from system reset
    
    msgsnd doorbell exceptions are cleared when the doorbell interrupt is
    taken. However if a doorbell exception causes a system reset interrupt
    wake from power saving state, the message is not cleared. Processing
    the doorbell from the system reset interrupt requires msgclr to avoid
    taking the exception again.
    
    Testing this plus the previous wakup direct patch gives:
    
                                    original         wakeup direct     msgclr
    Different threads, same core:   315k/s           264k/s            345k/s
    Different cores:                235k/s           242k/s            242k/s
    
    Net speedup is +10% for same core, and +3% for different core.
    
    Reviewed-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e15c178ba079..9624851ca276 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -746,6 +746,7 @@ int main(void)
 #endif
 
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);
+	DEFINE(PPC_DBELL_MSGTYPE, PPC_DBELL_MSGTYPE);
 
 #ifdef CONFIG_PPC_8xx
 	DEFINE(VIRT_IMMR_BASE, (u64)__fix_to_virt(FIX_IMMR_BASE));

commit 579006944e0d675361e987c646b83d2d1725966c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue May 16 16:41:20 2017 +1000

    KVM: PPC: Book3S HV: Virtualize doorbell facility on POWER9
    
    On POWER9, we no longer have the restriction that we had on POWER8
    where all threads in a core have to be in the same partition, so
    the CPU threads are now independent.  However, we still want to be
    able to run guests with a virtual SMT topology, if only to allow
    migration of guests from POWER8 systems to POWER9.
    
    A guest that has a virtual SMT mode greater than 1 will expect to
    be able to use the doorbell facility; it will expect the msgsndp
    and msgclrp instructions to work appropriately and to be able to read
    sensible values from the TIR (thread identification register) and
    DPDES (directed privileged doorbell exception status) special-purpose
    registers.  However, since each CPU thread is a separate sub-processor
    in POWER9, these instructions and registers can only be used within
    a single CPU thread.
    
    In order for these instructions to appear to act correctly according
    to the guest's virtual SMT mode, we have to trap and emulate them.
    We cause them to trap by clearing the HFSCR_MSGP bit in the HFSCR
    register.  The emulation is triggered by the hypervisor facility
    unavailable interrupt that occurs when the guest uses them.
    
    To cause a doorbell interrupt to occur within the guest, we set the
    DPDES register to 1.  If the guest has interrupts enabled, the CPU
    will generate a doorbell interrupt and clear the DPDES register in
    hardware.  The DPDES hardware register for the guest is saved in the
    vcpu->arch.vcore->dpdes field.  Since this gets written by the guest
    exit code, other VCPUs wishing to cause a doorbell interrupt don't
    write that field directly, but instead set a vcpu->arch.doorbell_request
    flag.  This is consumed and set to 0 by the guest entry code, which
    then sets DPDES to 1.
    
    Emulating reads of the DPDES register is somewhat involved, because
    it requires reading the doorbell pending interrupt status of all of the
    VCPU threads in the virtual core, and if any of those VCPUs are
    running, their doorbell status is only up-to-date in the hardware
    DPDES registers of the CPUs where they are running.  In order to get
    a reasonable approximation of the current doorbell status, we send
    those CPUs an IPI, causing an exit from the guest which will update
    the vcpu->arch.vcore->dpdes field.  We then use that value in
    constructing the emulated DPDES register value.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 562d291f3938..293fbdf96e7d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -513,6 +513,7 @@ int main(void)
 	OFFSET(VCPU_PENDING_EXC, kvm_vcpu, arch.pending_exceptions);
 	OFFSET(VCPU_CEDED, kvm_vcpu, arch.ceded);
 	OFFSET(VCPU_PRODDED, kvm_vcpu, arch.prodded);
+	OFFSET(VCPU_DBELL_REQ, kvm_vcpu, arch.doorbell_request);
 	OFFSET(VCPU_MMCR, kvm_vcpu, arch.mmcr);
 	OFFSET(VCPU_PMC, kvm_vcpu, arch.pmc);
 	OFFSET(VCPU_SPMC, kvm_vcpu, arch.spmc);

commit 769377f77ca2087baeaf97ce0b7026abeba3b581
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Feb 15 14:30:17 2017 +1100

    KVM: PPC: Book3S HV: Context-switch HFSCR between host and guest on POWER9
    
    This adds code to allow us to use a different value for the HFSCR
    (Hypervisor Facilities Status and Control Register) when running the
    guest from that which applies in the host.  The reason for doing this
    is to allow us to trap the msgsndp instruction and related operations
    in future so that they can be virtualized.  We also save the value of
    HFSCR when a hypervisor facility unavailable interrupt occurs, because
    the high byte of HFSCR indicates which facility the guest attempted to
    access.
    
    We save and restore the host value on guest entry/exit because some
    bits of it affect host userspace execution.
    
    We only do all this on POWER9, not on POWER8, because we are not
    intending to virtualize any of the facilities controlled by HFSCR on
    POWER8.  In particular, the HFSCR bit that controls execution of
    msgsndp and related operations does not exist on POWER8.  The HFSCR
    doesn't exist at all on POWER7.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 709e23425317..562d291f3938 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -542,6 +542,7 @@ int main(void)
 	OFFSET(VCPU_WORT, kvm_vcpu, arch.wort);
 	OFFSET(VCPU_TID, kvm_vcpu, arch.tid);
 	OFFSET(VCPU_PSSCR, kvm_vcpu, arch.psscr);
+	OFFSET(VCPU_HFSCR, kvm_vcpu, arch.hfscr);
 	OFFSET(VCORE_ENTRY_EXIT, kvmppc_vcore, entry_exit_map);
 	OFFSET(VCORE_IN_GUEST, kvmppc_vcore, in_guest);
 	OFFSET(VCORE_NAPPING_THREADS, kvmppc_vcore, napping_threads);

commit 22c6663dc69a042a7b4158f162582b4b1ba7a4b7
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Tue May 16 14:19:47 2017 +0530

    powerpc/powernv/idle: Use Requested Level for restoring state on P9 DD1
    
    On Power9 DD1 due to a hardware bug the Power-Saving Level Status
    field (PLS) of the PSSCR for a thread waking up from a deep state can
    under-report if some other thread in the core is in a shallow stop
    state. The scenario in which this can manifest is as follows:
    
       1) All the threads of the core are in deep stop.
       2) One of the threads is woken up. The PLS for this thread will
          correctly reflect that it is waking up from deep stop.
       3) The thread that has woken up now executes a shallow stop.
       4) When some other thread in the core is woken, its PLS will reflect
          the shallow stop state.
    
    Thus, the subsequent thread for which the PLS is under-reporting the
    wakeup state will not restore the hypervisor resources.
    
    Hence, on DD1 systems, use the Requested Level (RL) field as a
    workaround to restore the contents of the hypervisor resources on the
    wakeup from the stop state.
    
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 709e23425317..e15c178ba079 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -742,6 +742,7 @@ int main(void)
 	OFFSET(PACA_THREAD_MASK, paca_struct, thread_mask);
 	OFFSET(PACA_SUBCORE_SIBLING_MASK, paca_struct, subcore_sibling_mask);
 	OFFSET(PACA_SIBLING_PACA_PTRS, paca_struct, thread_sibling_pacas);
+	OFFSET(PACA_REQ_PSSCR, paca_struct, requested_psscr);
 #endif
 
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);

commit 4415b335282591e76762cd9e6dc60932a7595fc3
Merge: 3bed8888edc8 fb7dcf723dd2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 9 11:50:01 2017 +0200

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    The main thing here is a new implementation of the in-kernel
    XICS interrupt controller emulation for POWER9 machines, from Ben
    Herrenschmidt.
    
    POWER9 has a new interrupt controller called XIVE (eXternal Interrupt
    Virtualization Engine) which is able to deliver interrupts directly
    to guest virtual CPUs in hardware without hypervisor intervention.
    With this new code, the guest still sees the old XICS interface but
    performance is better because the XICS emulation in the host uses the
    XIVE directly rather than going through a XICS emulation in firmware.
    
    Conflicts:
            arch/powerpc/kernel/cpu_setup_power.S [cherry-picked fix]
            arch/powerpc/kvm/book3s_xive.c [include asm/debugfs.h]

commit b1ee8a3de5790777f325416ad97340428d8ae25f
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:06 2016 +1000

    powerpc/64s: Dedicated system reset interrupt stack
    
    The system reset interrupt is used for crash/debug situations, so it is
    desirable to have as little impact on the normal state of the system as
    possible.
    
    Currently it uses the current kernel stack to process the exception.
    This stores into the stack which may be involved with the crash. The
    stack pointer may be corrupted, or it may have overflowed.
    
    Avoid or minimise these problems by creating a dedicated NMI stack for
    the system reset interrupt to use.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1466e96d36cf..439c257dec4a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -234,6 +234,7 @@ int main(void)
 	OFFSET(PACAEMERGSP, paca_struct, emergency_sp);
 #ifdef CONFIG_PPC_BOOK3S_64
 	OFFSET(PACAMCEMERGSP, paca_struct, mc_emergency_sp);
+	OFFSET(PACA_NMI_EMERG_SP, paca_struct, nmi_emergency_sp);
 	OFFSET(PACA_IN_MCE, paca_struct, in_mce);
 	OFFSET(PACA_IN_NMI, paca_struct, in_nmi);
 #endif

commit c4f3b52ce7b16824befb16ab3d045c891b08b7db
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:05 2016 +1000

    powerpc/64s: Disallow system reset vs system reset reentrancy
    
    In preparation for using a dedicated stack for system reset interrupts,
    prevent a nested system reset from recovering, in order to simplify
    code that is called in crash/debug path. This allows a system reset
    interrupt to just use the base stack pointer.
    
    Keep an in_nmi nesting counter similarly to the in_mce counter. Consider
    the interrrupt non-recoverable if it is taken inside another system
    reset.
    
    Interrupt nesting could be allowed similarly to MCE, but system reset
    is a special case that's not for normal operation, so simplicity wins
    until there is requirement for nested system reset interrupts.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 46732ee70b43..1466e96d36cf 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -235,6 +235,7 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S_64
 	OFFSET(PACAMCEMERGSP, paca_struct, mc_emergency_sp);
 	OFFSET(PACA_IN_MCE, paca_struct, in_mce);
+	OFFSET(PACA_IN_NMI, paca_struct, in_nmi);
 #endif
 	OFFSET(PACAHWCPUID, paca_struct, hw_cpu_id);
 	OFFSET(PACAKEXECSTATE, paca_struct, kexec_state);

commit a3d96f70c14773d0928c6a54fd278138f0868572
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue Dec 20 04:30:04 2016 +1000

    powerpc/64s: Fix system reset vs general interrupt reentrancy
    
    The system reset interrupt can occur when MSR_EE=0, and it currently
    uses the PACA_EXGEN save area.
    
    Some PACA_EXGEN interrupts have a window where MSR_RI=1 and MSR_EE=0
    when the save area is still in use. A system reset interrupt in this
    window can lead to undetected corruption when the save area gets
    overwritten.
    
    This patch introduces PACA_EXNMI save area for system reset exceptions,
    which closes this corruption window. It's also helpful to retain the
    EXGEN state for debugging situations, even if not considering the
    recoverability aspect.
    
    This patch also moves the PACA_EXMC area down to a less frequently used
    part of the paca with the new save area.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8e1163426ccb..46732ee70b43 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -220,6 +220,7 @@ int main(void)
 	OFFSET(PACA_EXGEN, paca_struct, exgen);
 	OFFSET(PACA_EXMC, paca_struct, exmc);
 	OFFSET(PACA_EXSLB, paca_struct, exslb);
+	OFFSET(PACA_EXNMI, paca_struct, exnmi);
 	OFFSET(PACALPPACAPTR, paca_struct, lppaca_ptr);
 	OFFSET(PACA_SLBSHADOWPTR, paca_struct, slb_shadow_ptr);
 	OFFSET(SLBSHADOW_STACKVSID, slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid);

commit 5af50993850a48ba749b122173d789ea90976c72
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:56 2017 +1000

    KVM: PPC: Book3S HV: Native usage of the XIVE interrupt controller
    
    This patch makes KVM capable of using the XIVE interrupt controller
    to provide the standard PAPR "XICS" style hypercalls. It is necessary
    for proper operations when the host uses XIVE natively.
    
    This has been lightly tested on an actual system, including PCI
    pass-through with a TG3 device.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Cleanup pr_xxx(), unsplit pr_xxx() strings, etc., fix build
     failures by adding KVM_XIVE which depends on KVM_XICS and XIVE, and
     adding empty stubs for the kvm_xive_xxx() routines, fixup subject,
     integrate fixes from Paul for building PR=y HV=n]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4367e7df51a1..1822187813dc 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -630,6 +630,8 @@ int main(void)
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);
 	HSTATE_FIELD(HSTATE_KVM_VCORE, kvm_vcore);
 	HSTATE_FIELD(HSTATE_XICS_PHYS, xics_phys);
+	HSTATE_FIELD(HSTATE_XIVE_TIMA_PHYS, xive_tima_phys);
+	HSTATE_FIELD(HSTATE_XIVE_TIMA_VIRT, xive_tima_virt);
 	HSTATE_FIELD(HSTATE_SAVED_XIRR, saved_xirr);
 	HSTATE_FIELD(HSTATE_HOST_IPI, host_ipi);
 	HSTATE_FIELD(HSTATE_PTID, ptid);
@@ -715,6 +717,14 @@ int main(void)
 	OFFSET(VCPU_HOST_MAS6, kvm_vcpu, arch.host_mas6);
 #endif
 
+#ifdef CONFIG_KVM_XICS
+	DEFINE(VCPU_XIVE_SAVED_STATE, offsetof(struct kvm_vcpu,
+					       arch.xive_saved_state));
+	DEFINE(VCPU_XIVE_CAM_WORD, offsetof(struct kvm_vcpu,
+					    arch.xive_cam_word));
+	DEFINE(VCPU_XIVE_PUSHED, offsetof(struct kvm_vcpu, arch.xive_pushed));
+#endif
+
 #ifdef CONFIG_KVM_EXIT_TIMING
 	OFFSET(VCPU_TIMING_EXIT_TBU, kvm_vcpu, arch.timing_exit.tv32.tbu);
 	OFFSET(VCPU_TIMING_EXIT_TBL, kvm_vcpu, arch.timing_exit.tv32.tbl);

commit 03dfee6d5f824d14e3ecb742518740de69e603cc
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Apr 12 14:56:36 2017 +1000

    powerpc/mm: Fix swapper_pg_dir size on 64-bit hash w/64K pages
    
    Recently in commit f6eedbba7a26 ("powerpc/mm/hash: Increase VA range to 128TB"),
    we increased H_PGD_INDEX_SIZE to 15 when we're building with 64K pages. This
    makes it larger than RADIX_PGD_INDEX_SIZE (13), which means the logic to
    calculate MAX_PGD_INDEX_SIZE in book3s/64/pgtable.h is wrong.
    
    The end result is that the PGD (Page Global Directory, ie top level page table)
    of the kernel (aka. swapper_pg_dir), is too small.
    
    This generally doesn't lead to a crash, as we don't use the full range in normal
    operation. However if we try to dump the kernel pagetables we can trigger a
    crash because we walk off the end of the pgd into other memory and eventually
    try to dereference something bogus:
    
      $ cat /sys/kernel/debug/kernel_pagetables
      Unable to handle kernel paging request for data at address 0xe8fece0000000000
      Faulting instruction address: 0xc000000000072314
      cpu 0xc: Vector: 380 (Data SLB Access) at [c0000000daa13890]
          pc: c000000000072314: ptdump_show+0x164/0x430
          lr: c000000000072550: ptdump_show+0x3a0/0x430
         dar: e802cf0000000000
      seq_read+0xf8/0x560
      full_proxy_read+0x84/0xc0
      __vfs_read+0x6c/0x1d0
      vfs_read+0xbc/0x1b0
      SyS_read+0x6c/0x110
      system_call+0x38/0xfc
    
    The root cause is that MAX_PGD_INDEX_SIZE isn't actually computed to be
    the max of H_PGD_INDEX_SIZE or RADIX_PGD_INDEX_SIZE. To fix that move
    the calculation into asm-offsets.c where we can do it easily using
    max().
    
    Fixes: f6eedbba7a26 ("powerpc/mm/hash: Increase VA range to 128TB")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e7c8229a8812..8e1163426ccb 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -400,8 +400,8 @@ int main(void)
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));
 #endif
 
-#ifdef MAX_PGD_TABLE_SIZE
-	DEFINE(PGD_TABLE_SIZE, MAX_PGD_TABLE_SIZE);
+#ifdef CONFIG_PPC_BOOK3S_64
+	DEFINE(PGD_TABLE_SIZE, (sizeof(pgd_t) << max(RADIX_PGD_INDEX_SIZE, H_PGD_INDEX_SIZE)));
 #else
 	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);
 #endif

commit 17ed4c8f81da2bf340d33a8c875f4d6b1dfd9398
Author: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
Date:   Wed Mar 22 20:34:17 2017 +0530

    powerpc/powernv: Recover correct PACA on wakeup from a stop on P9 DD1
    
    POWER9 DD1.0 hardware has a bug where the SPRs of a thread waking up
    from stop 0,1,2 with ESL=1 can endup being misplaced in the core. Thus
    the HSPRG0 of a thread waking up from can contain the paca pointer of
    its sibling.
    
    This patch implements a context recovery framework within threads of a
    core, by provisioning space in paca_struct for saving every sibling
    threads's paca pointers. Basically, we should be able to arrive at the
    right paca pointer from any of the thread's existing paca pointer.
    
    At bootup, during powernv idle-init, we save the paca address of every
    CPU in each one its siblings paca_struct in the slot corresponding to
    this CPU's index in the core.
    
    On wakeup from a stop, the thread will determine its index in the core
    from the TIR register and recover its PACA pointer by indexing into
    the correct slot in the provisioned space in the current PACA.
    
    Furthermore, ensure that the NVGPRs are restored from the stack on the
    way out by setting the NAPSTATELOST in paca.
    
    [Changelog written with inputs from svaidy@linux.vnet.ibm.com]
    Signed-off-by: Gautham R. Shenoy <ego@linux.vnet.ibm.com>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Call it a bug]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 795505e27535..e7c8229a8812 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -728,6 +728,7 @@ int main(void)
 	OFFSET(PACA_THREAD_IDLE_STATE, paca_struct, thread_idle_state);
 	OFFSET(PACA_THREAD_MASK, paca_struct, thread_mask);
 	OFFSET(PACA_SUBCORE_SIBLING_MASK, paca_struct, subcore_sibling_mask);
+	OFFSET(PACA_SIBLING_PACA_PTRS, paca_struct, thread_sibling_pacas);
 #endif
 
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);

commit bb1832217a859f6dbe4a45ff2ba7fdcab0bb3958
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Wed Mar 22 09:06:59 2017 +0530

    powerpc/mm/hash: Store addr_limit in PACA
    
    We optmize the slice page size array copy to paca by copying only the
    range based on addr_limit. This will require us to not look at page size
    array beyond addr_limit in PACA on slb fault. To enable that copy task
    size to paca which will be used during slb fault.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [mpe: Rename from task_size to addr_limit, consolidate #ifdefs]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4367e7df51a1..795505e27535 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -185,6 +185,7 @@ int main(void)
 #ifdef CONFIG_PPC_MM_SLICES
 	OFFSET(PACALOWSLICESPSIZE, paca_struct, mm_ctx_low_slices_psize);
 	OFFSET(PACAHIGHSLICEPSIZE, paca_struct, mm_ctx_high_slices_psize);
+	DEFINE(PACA_ADDR_LIMIT, offsetof(struct paca_struct, addr_limit));
 	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
 #endif /* CONFIG_PPC_MM_SLICES */
 #endif

commit b286cedd473006b33d5ae076afac509e6b2c3bf4
Merge: 522214d9be9c 9f3768e02335
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 1 10:10:16 2017 -0800

    Merge tag 'powerpc-4.11-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull more powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - an update of the disassembly code used by xmon to the latest
         versions in binutils. We've received permission from all the
         authors of the relevant binutils changes to relicense their changes
         to the relevant files from GPLv3 to GPLv2, for inclusion in Linux.
         Thanks to Peter Bergner for doing the leg work to get permission
         from everyone.
    
       - addition of the "architected" Power9 CPU table entry, allowing us
         to boot in Power9 architected mode under a hypervisor.
    
       - updates to the Power9 PMU code.
    
       - implementation of clear_bit_unlock_is_negative_byte() to optimise
         unlock_page().
    
       - Freescale updates from Scott: "Highlights include 8xx breakpoints
         and perf, t1042rdb display support, and board updates."
    
      Thanks to:
        Al Viro, Andrew Donnellan, Aneesh Kumar K.V, Balbir Singh, Douglas
        Miller, Frédéric Weisbecker, Gavin Shan, Madhavan Srinivasan,
        Michael Roth, Nathan Fontenot, Naveen N. Rao, Nicholas Piggin, Peter
        Bergner, Paul E. McKenney, Rashmica Gupta, Russell Currey, Sahil
        Mehta, Stewart Smith"
    
    * tag 'powerpc-4.11-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (48 commits)
      powerpc: Remove leftover cputime_to_nsecs call causing build error
      powerpc/mm/hash: Always clear UPRT and Host Radix bits when setting up CPU
      powerpc/optprobes: Fix TOC handling in optprobes trampoline
      powerpc/pseries: Advertise Hot Plug Event support to firmware
      cxl: fix nested locking hang during EEH hotplug
      powerpc/xmon: Dump memory in CPU endian format
      powerpc/pseries: Revert 'Auto-online hotplugged memory'
      powerpc/powernv: Make PCI non-optional
      powerpc/64: Implement clear_bit_unlock_is_negative_byte()
      powerpc/powernv: Remove unused variable in pnv_pci_sriov_disable()
      powerpc/kernel: Remove error message in pcibios_setup_phb_resources()
      powerpc/mm: Fix typo in set_pte_at()
      pci/hotplug/pnv-php: Disable MSI and PCI device properly
      pci/hotplug/pnv-php: Disable surprise hotplug capability on conflicts
      pci/hotplug/pnv-php: Remove WARN_ON() in pnv_php_put_slot()
      powerpc: Add POWER9 architected mode to cputable
      powerpc/perf: use is_kernel_addr macro in perf_get_misc_flags()
      powerpc/perf: Avoid FAB_*_MATCH checks for power9
      powerpc/perf: Add restrictions to PMC5 in power9 DD1
      powerpc/perf: Use Instruction Counter value
      ...

commit 38705613b74ab090eee55c327cd0cb77fb10eb26
Merge: ff47d8c05019 438e69b52be7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 22 10:30:38 2017 -0800

    Merge tag 'powerpc-4.11-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for direct mapped LPC on POWER9, giving Linux direct access
         to devices that may be on there such as a UART.
    
       - Memory hotplug support for the Power9 Radix MMU.
    
       - Add new AUX vectors describing the processor's cache geometry, to
         be used by glibc.
    
       - The ability for a guest to ask the hypervisor to resize the guest's
         hash table, and in addition support for doing so automatically when
         memory is hotplugged into/out-of the guest. This allows the hash
         table to be sized based on the current memory usage of the guest,
         rather than the maximum possible memory usage.
    
       - Implementation of optprobes (kprobe optimisation) for powerpc.
    
      In addition there's the topic branch shared with the KVM tree, which
      includes support for guests to use the Radix MMU on Power9.
    
      Thanks to:
        Alistair Popple, Andrew Donnellan, Aneesh Kumar K.V, Anju T, Anton
        Blanchard, Benjamin Herrenschmidt, Chris Packham, Daniel Axtens,
        Daniel Borkmann, David Gibson, Finn Thain, Gautham R. Shenoy, Gavin
        Shan, Greg Kurz, Joel Stanley, John Allen, Madhavan Srinivasan,
        Mahesh Salgaonkar, Markus Elfring, Michael Neuling, Nathan Fontenot,
        Naveen N. Rao, Nicholas Piggin, Paul Mackerras, Ravi Bangoria, Reza
        Arbab, Shailendra Singh, Vaibhav Jain, Wei Yongjun"
    
    * tag 'powerpc-4.11-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (129 commits)
      powerpc/mm/radix: Skip ptesync in pte update helpers
      powerpc/mm/radix: Use ptep_get_and_clear_full when clearing pte for full mm
      powerpc/mm/radix: Update pte update sequence for pte clear case
      powerpc/mm: Update PROTFAULT handling in the page fault path
      powerpc/xmon: Fix data-breakpoint
      powerpc/mm: Fix build break with BOOK3S_64=n and MEMORY_HOTPLUG=y
      powerpc/mm: Fix build break when CMA=n && SPAPR_TCE_IOMMU=y
      powerpc/mm: Fix build break with RADIX=y & HUGETLBFS=n
      powerpc/pseries: Fix typo in parameter description
      powerpc/kprobes: Remove kprobe_exceptions_notify()
      kprobes: Introduce weak variant of kprobe_exceptions_notify()
      powerpc/ftrace: Fix confusing help text for DISABLE_MPROFILE_KERNEL
      powerpc/powernv: Fix opal_exit tracepoint opcode
      powerpc: Add a prototype for mcount() so it can be versioned
      powerpc: Drop GPL from of_node_to_nid() export to match other arches
      powerpc/kprobes: Optimize kprobe in kretprobe_trampoline()
      powerpc/kprobes: Implement Optprobes
      powerpc/kprobes: Fixes for kprobe_lookup_name() on BE
      powerpc: Add helper to check if offset is within relative branch range
      powerpc/bpf: Introduce __PPC_SH64()
      ...

commit 828cad8ea05d194d8a9452e0793261c2024c23a2
Merge: 60c906bab124 bb3bac2ca9a3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Feb 20 12:52:55 2017 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this (fairly busy) cycle were:
    
       - There was a class of scheduler bugs related to forgetting to update
         the rq-clock timestamp which can cause weird and hard to debug
         problems, so there's a new debug facility for this: which uncovered
         a whole lot of bugs which convinced us that we want to keep the
         debug facility.
    
         (Peter Zijlstra, Matt Fleming)
    
       - Various cputime related updates: eliminate cputime and use u64
         nanoseconds directly, simplify and improve the arch interfaces,
         implement delayed accounting more widely, etc. - (Frederic
         Weisbecker)
    
       - Move code around for better structure plus cleanups (Ingo Molnar)
    
       - Move IO schedule accounting deeper into the scheduler plus related
         changes to improve the situation (Tejun Heo)
    
       - ... plus a round of sched/rt and sched/deadline fixes, plus other
         fixes, updats and cleanups"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (85 commits)
      sched/core: Remove unlikely() annotation from sched_move_task()
      sched/autogroup: Rename auto_group.[ch] to autogroup.[ch]
      sched/topology: Split out scheduler topology code from core.c into topology.c
      sched/core: Remove unnecessary #include headers
      sched/rq_clock: Consolidate the ordering of the rq_clock methods
      delayacct: Include <uapi/linux/taskstats.h>
      sched/core: Clean up comments
      sched/rt: Show the 'sched_rr_timeslice' SCHED_RR timeslice tuning knob in milliseconds
      sched/clock: Add dummy clear_sched_clock_stable() stub function
      sched/cputime: Remove generic asm headers
      sched/cputime: Remove unused nsec_to_cputime()
      s390, sched/cputime: Remove unused cputime definitions
      powerpc, sched/cputime: Remove unused cputime definitions
      s390, sched/cputime: Make arch_cpu_idle_time() to return nsecs
      ia64, sched/cputime: Remove unused cputime definitions
      ia64: Convert vtime to use nsec units directly
      ia64, sched/cputime: Move the nsecs based cputime headers to the last arch using it
      sched/cputime: Remove jiffies based cputime
      sched/cputime, vtime: Return nsecs instead of cputime_t to account
      sched/cputime: Complete nsec conversion of tick based accounting
      ...

commit 10d4cf188ab26a4ebeee4c914e82acd5edd9f0ff
Author: Rashmica Gupta <rashmicy@gmail.com>
Date:   Thu Jun 2 14:29:47 2016 +1000

    powerpc/asm: Define STACK_PT_REGS_OFFSET macro in asm-offsets.c
    
    There are quite a few entries in asm-offests.c which look like:
    
      DEFINE(REG, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, reg));
    
    So define a macro to do it once.
    
    Signed-off-by: Rashmica Gupta <rashmicy@gmail.com>
    [mpe: Rename to STACK_PT_REGS_OFFSET for excruciating explicitness]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8b7b2fd7e374..b6709021fee5 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -72,6 +72,9 @@
 #include <asm/fixmap.h>
 #endif
 
+#define STACK_PT_REGS_OFFSET(sym, val)	\
+	DEFINE(sym, STACK_FRAME_OVERHEAD + offsetof(struct pt_regs, val))
+
 int main(void)
 {
 	OFFSET(THREAD, task_struct, thread);
@@ -265,38 +268,38 @@ int main(void)
 	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
 	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
 #endif /* CONFIG_PPC64 */
-	DEFINE(GPR0, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[0]));
-	DEFINE(GPR1, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[1]));
-	DEFINE(GPR2, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[2]));
-	DEFINE(GPR3, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[3]));
-	DEFINE(GPR4, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[4]));
-	DEFINE(GPR5, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[5]));
-	DEFINE(GPR6, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[6]));
-	DEFINE(GPR7, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[7]));
-	DEFINE(GPR8, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[8]));
-	DEFINE(GPR9, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[9]));
-	DEFINE(GPR10, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[10]));
-	DEFINE(GPR11, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[11]));
-	DEFINE(GPR12, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[12]));
-	DEFINE(GPR13, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[13]));
+	STACK_PT_REGS_OFFSET(GPR0, gpr[0]);
+	STACK_PT_REGS_OFFSET(GPR1, gpr[1]);
+	STACK_PT_REGS_OFFSET(GPR2, gpr[2]);
+	STACK_PT_REGS_OFFSET(GPR3, gpr[3]);
+	STACK_PT_REGS_OFFSET(GPR4, gpr[4]);
+	STACK_PT_REGS_OFFSET(GPR5, gpr[5]);
+	STACK_PT_REGS_OFFSET(GPR6, gpr[6]);
+	STACK_PT_REGS_OFFSET(GPR7, gpr[7]);
+	STACK_PT_REGS_OFFSET(GPR8, gpr[8]);
+	STACK_PT_REGS_OFFSET(GPR9, gpr[9]);
+	STACK_PT_REGS_OFFSET(GPR10, gpr[10]);
+	STACK_PT_REGS_OFFSET(GPR11, gpr[11]);
+	STACK_PT_REGS_OFFSET(GPR12, gpr[12]);
+	STACK_PT_REGS_OFFSET(GPR13, gpr[13]);
 #ifndef CONFIG_PPC64
-	DEFINE(GPR14, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[14]));
+	STACK_PT_REGS_OFFSET(GPR14, gpr[14]);
 #endif /* CONFIG_PPC64 */
 	/*
 	 * Note: these symbols include _ because they overlap with special
 	 * register names
 	 */
-	DEFINE(_NIP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, nip));
-	DEFINE(_MSR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, msr));
-	DEFINE(_CTR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, ctr));
-	DEFINE(_LINK, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, link));
-	DEFINE(_CCR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, ccr));
-	DEFINE(_XER, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, xer));
-	DEFINE(_DAR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dar));
-	DEFINE(_DSISR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
-	DEFINE(ORIG_GPR3, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, orig_gpr3));
-	DEFINE(RESULT, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, result));
-	DEFINE(_TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
+	STACK_PT_REGS_OFFSET(_NIP, nip);
+	STACK_PT_REGS_OFFSET(_MSR, msr);
+	STACK_PT_REGS_OFFSET(_CTR, ctr);
+	STACK_PT_REGS_OFFSET(_LINK, link);
+	STACK_PT_REGS_OFFSET(_CCR, ccr);
+	STACK_PT_REGS_OFFSET(_XER, xer);
+	STACK_PT_REGS_OFFSET(_DAR, dar);
+	STACK_PT_REGS_OFFSET(_DSISR, dsisr);
+	STACK_PT_REGS_OFFSET(ORIG_GPR3, orig_gpr3);
+	STACK_PT_REGS_OFFSET(RESULT, result);
+	STACK_PT_REGS_OFFSET(_TRAP, trap);
 #ifndef CONFIG_PPC64
 	/*
 	 * The PowerPC 400-class & Book-E processors have neither the DAR
@@ -304,10 +307,10 @@ int main(void)
 	 * DEAR and ESR SPRs for such processors.  For critical interrupts
 	 * we use them to hold SRR0 and SRR1.
 	 */
-	DEFINE(_DEAR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dar));
-	DEFINE(_ESR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
+	STACK_PT_REGS_OFFSET(_DEAR, dar);
+	STACK_PT_REGS_OFFSET(_ESR, dsisr);
 #else /* CONFIG_PPC64 */
-	DEFINE(SOFTE, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, softe));
+	STACK_PT_REGS_OFFSET(SOFTE, softe);
 
 	/* These _only_ to be used with {PROM,RTAS}_FRAME_SIZE!!! */
 	DEFINE(_SRR0, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs));

commit 454656155110603e88b83eec847d5293a30bb96e
Author: Rashmica Gupta <rashmicy@gmail.com>
Date:   Wed Feb 15 21:41:20 2017 +1100

    powerpc/asm: Use OFFSET macro in asm-offsets.c
    
    A lot of entries in asm-offests.c look like this:
    
      DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
    
    But there is a common macro, OFFSET, which makes this cleaner:
    
      OFFSET(TI_flags, thread_info, flags)
    
    So use it.
    
    Signed-off-by: Rashmica Gupta <rashmicy@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 68657a767f80..8b7b2fd7e374 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -74,206 +74,188 @@
 
 int main(void)
 {
-	DEFINE(THREAD, offsetof(struct task_struct, thread));
-	DEFINE(MM, offsetof(struct task_struct, mm));
-	DEFINE(MMCONTEXTID, offsetof(struct mm_struct, context.id));
+	OFFSET(THREAD, task_struct, thread);
+	OFFSET(MM, task_struct, mm);
+	OFFSET(MMCONTEXTID, mm_struct, context.id);
 #ifdef CONFIG_PPC64
 	DEFINE(SIGSEGV, SIGSEGV);
 	DEFINE(NMI_MASK, NMI_MASK);
-	DEFINE(TASKTHREADPPR, offsetof(struct task_struct, thread.ppr));
+	OFFSET(TASKTHREADPPR, task_struct, thread.ppr);
 #else
-	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
+	OFFSET(THREAD_INFO, task_struct, stack);
 	DEFINE(THREAD_INFO_GAP, _ALIGN_UP(sizeof(struct thread_info), 16));
-	DEFINE(KSP_LIMIT, offsetof(struct thread_struct, ksp_limit));
+	OFFSET(KSP_LIMIT, thread_struct, ksp_limit);
 #endif /* CONFIG_PPC64 */
 
 #ifdef CONFIG_LIVEPATCH
-	DEFINE(TI_livepatch_sp, offsetof(struct thread_info, livepatch_sp));
+	OFFSET(TI_livepatch_sp, thread_info, livepatch_sp);
 #endif
 
 #ifdef CONFIG_CC_STACKPROTECTOR
 	DEFINE(TSK_STACK_CANARY, offsetof(struct task_struct, stack_canary));
 #endif
-	DEFINE(KSP, offsetof(struct thread_struct, ksp));
-	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
+	OFFSET(KSP, thread_struct, ksp);
+	OFFSET(PT_REGS, thread_struct, regs);
 #ifdef CONFIG_BOOKE
-	DEFINE(THREAD_NORMSAVES, offsetof(struct thread_struct, normsave[0]));
+	OFFSET(THREAD_NORMSAVES, thread_struct, normsave[0]);
 #endif
-	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
-	DEFINE(THREAD_FPSTATE, offsetof(struct thread_struct, fp_state));
-	DEFINE(THREAD_FPSAVEAREA, offsetof(struct thread_struct, fp_save_area));
-	DEFINE(FPSTATE_FPSCR, offsetof(struct thread_fp_state, fpscr));
-	DEFINE(THREAD_LOAD_FP, offsetof(struct thread_struct, load_fp));
+	OFFSET(THREAD_FPEXC_MODE, thread_struct, fpexc_mode);
+	OFFSET(THREAD_FPSTATE, thread_struct, fp_state);
+	OFFSET(THREAD_FPSAVEAREA, thread_struct, fp_save_area);
+	OFFSET(FPSTATE_FPSCR, thread_fp_state, fpscr);
+	OFFSET(THREAD_LOAD_FP, thread_struct, load_fp);
 #ifdef CONFIG_ALTIVEC
-	DEFINE(THREAD_VRSTATE, offsetof(struct thread_struct, vr_state));
-	DEFINE(THREAD_VRSAVEAREA, offsetof(struct thread_struct, vr_save_area));
-	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
-	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
-	DEFINE(VRSTATE_VSCR, offsetof(struct thread_vr_state, vscr));
-	DEFINE(THREAD_LOAD_VEC, offsetof(struct thread_struct, load_vec));
+	OFFSET(THREAD_VRSTATE, thread_struct, vr_state);
+	OFFSET(THREAD_VRSAVEAREA, thread_struct, vr_save_area);
+	OFFSET(THREAD_VRSAVE, thread_struct, vrsave);
+	OFFSET(THREAD_USED_VR, thread_struct, used_vr);
+	OFFSET(VRSTATE_VSCR, thread_vr_state, vscr);
+	OFFSET(THREAD_LOAD_VEC, thread_struct, load_vec);
 #endif /* CONFIG_ALTIVEC */
 #ifdef CONFIG_VSX
-	DEFINE(THREAD_USED_VSR, offsetof(struct thread_struct, used_vsr));
+	OFFSET(THREAD_USED_VSR, thread_struct, used_vsr);
 #endif /* CONFIG_VSX */
 #ifdef CONFIG_PPC64
-	DEFINE(KSP_VSID, offsetof(struct thread_struct, ksp_vsid));
+	OFFSET(KSP_VSID, thread_struct, ksp_vsid);
 #else /* CONFIG_PPC64 */
-	DEFINE(PGDIR, offsetof(struct thread_struct, pgdir));
+	OFFSET(PGDIR, thread_struct, pgdir);
 #ifdef CONFIG_SPE
-	DEFINE(THREAD_EVR0, offsetof(struct thread_struct, evr[0]));
-	DEFINE(THREAD_ACC, offsetof(struct thread_struct, acc));
-	DEFINE(THREAD_SPEFSCR, offsetof(struct thread_struct, spefscr));
-	DEFINE(THREAD_USED_SPE, offsetof(struct thread_struct, used_spe));
+	OFFSET(THREAD_EVR0, thread_struct, evr[0]);
+	OFFSET(THREAD_ACC, thread_struct, acc);
+	OFFSET(THREAD_SPEFSCR, thread_struct, spefscr);
+	OFFSET(THREAD_USED_SPE, thread_struct, used_spe);
 #endif /* CONFIG_SPE */
 #endif /* CONFIG_PPC64 */
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
-	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, debug.dbcr0));
+	OFFSET(THREAD_DBCR0, thread_struct, debug.dbcr0);
 #endif
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
-	DEFINE(THREAD_KVM_SVCPU, offsetof(struct thread_struct, kvm_shadow_vcpu));
+	OFFSET(THREAD_KVM_SVCPU, thread_struct, kvm_shadow_vcpu);
 #endif
 #if defined(CONFIG_KVM) && defined(CONFIG_BOOKE)
-	DEFINE(THREAD_KVM_VCPU, offsetof(struct thread_struct, kvm_vcpu));
+	OFFSET(THREAD_KVM_VCPU, thread_struct, kvm_vcpu);
 #endif
 
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-	DEFINE(PACATMSCRATCH, offsetof(struct paca_struct, tm_scratch));
-	DEFINE(THREAD_TM_TFHAR, offsetof(struct thread_struct, tm_tfhar));
-	DEFINE(THREAD_TM_TEXASR, offsetof(struct thread_struct, tm_texasr));
-	DEFINE(THREAD_TM_TFIAR, offsetof(struct thread_struct, tm_tfiar));
-	DEFINE(THREAD_TM_TAR, offsetof(struct thread_struct, tm_tar));
-	DEFINE(THREAD_TM_PPR, offsetof(struct thread_struct, tm_ppr));
-	DEFINE(THREAD_TM_DSCR, offsetof(struct thread_struct, tm_dscr));
-	DEFINE(PT_CKPT_REGS, offsetof(struct thread_struct, ckpt_regs));
-	DEFINE(THREAD_CKVRSTATE, offsetof(struct thread_struct,
-						 ckvr_state));
-	DEFINE(THREAD_CKVRSAVE, offsetof(struct thread_struct,
-					    ckvrsave));
-	DEFINE(THREAD_CKFPSTATE, offsetof(struct thread_struct,
-						 ckfp_state));
+	OFFSET(PACATMSCRATCH, paca_struct, tm_scratch);
+	OFFSET(THREAD_TM_TFHAR, thread_struct, tm_tfhar);
+	OFFSET(THREAD_TM_TEXASR, thread_struct, tm_texasr);
+	OFFSET(THREAD_TM_TFIAR, thread_struct, tm_tfiar);
+	OFFSET(THREAD_TM_TAR, thread_struct, tm_tar);
+	OFFSET(THREAD_TM_PPR, thread_struct, tm_ppr);
+	OFFSET(THREAD_TM_DSCR, thread_struct, tm_dscr);
+	OFFSET(PT_CKPT_REGS, thread_struct, ckpt_regs);
+	OFFSET(THREAD_CKVRSTATE, thread_struct, ckvr_state);
+	OFFSET(THREAD_CKVRSAVE, thread_struct, ckvrsave);
+	OFFSET(THREAD_CKFPSTATE, thread_struct, ckfp_state);
 	/* Local pt_regs on stack for Transactional Memory funcs. */
 	DEFINE(TM_FRAME_SIZE, STACK_FRAME_OVERHEAD +
 	       sizeof(struct pt_regs) + 16);
 #endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
-	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
-	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
-	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
-	DEFINE(TI_TASK, offsetof(struct thread_info, task));
-	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
+	OFFSET(TI_FLAGS, thread_info, flags);
+	OFFSET(TI_LOCAL_FLAGS, thread_info, local_flags);
+	OFFSET(TI_PREEMPT, thread_info, preempt_count);
+	OFFSET(TI_TASK, thread_info, task);
+	OFFSET(TI_CPU, thread_info, cpu);
 
 #ifdef CONFIG_PPC64
-	DEFINE(DCACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, l1d.block_size));
-	DEFINE(DCACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, l1d.log_block_size));
-	DEFINE(DCACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, l1d.blocks_per_page));
-	DEFINE(ICACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, l1i.block_size));
-	DEFINE(ICACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, l1i.log_block_size));
-	DEFINE(ICACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, l1i.blocks_per_page));
+	OFFSET(DCACHEL1BLOCKSIZE, ppc64_caches, l1d.block_size);
+	OFFSET(DCACHEL1LOGBLOCKSIZE, ppc64_caches, l1d.log_block_size);
+	OFFSET(DCACHEL1BLOCKSPERPAGE, ppc64_caches, l1d.blocks_per_page);
+	OFFSET(ICACHEL1BLOCKSIZE, ppc64_caches, l1i.block_size);
+	OFFSET(ICACHEL1LOGBLOCKSIZE, ppc64_caches, l1i.log_block_size);
+	OFFSET(ICACHEL1BLOCKSPERPAGE, ppc64_caches, l1i.blocks_per_page);
 	/* paca */
 	DEFINE(PACA_SIZE, sizeof(struct paca_struct));
-	DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));
-	DEFINE(PACAPROCSTART, offsetof(struct paca_struct, cpu_start));
-	DEFINE(PACAKSAVE, offsetof(struct paca_struct, kstack));
-	DEFINE(PACACURRENT, offsetof(struct paca_struct, __current));
-	DEFINE(PACASAVEDMSR, offsetof(struct paca_struct, saved_msr));
-	DEFINE(PACASTABRR, offsetof(struct paca_struct, stab_rr));
-	DEFINE(PACAR1, offsetof(struct paca_struct, saved_r1));
-	DEFINE(PACATOC, offsetof(struct paca_struct, kernel_toc));
-	DEFINE(PACAKBASE, offsetof(struct paca_struct, kernelbase));
-	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
-	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
-	DEFINE(PACAIRQHAPPENED, offsetof(struct paca_struct, irq_happened));
+	OFFSET(PACAPACAINDEX, paca_struct, paca_index);
+	OFFSET(PACAPROCSTART, paca_struct, cpu_start);
+	OFFSET(PACAKSAVE, paca_struct, kstack);
+	OFFSET(PACACURRENT, paca_struct, __current);
+	OFFSET(PACASAVEDMSR, paca_struct, saved_msr);
+	OFFSET(PACASTABRR, paca_struct, stab_rr);
+	OFFSET(PACAR1, paca_struct, saved_r1);
+	OFFSET(PACATOC, paca_struct, kernel_toc);
+	OFFSET(PACAKBASE, paca_struct, kernelbase);
+	OFFSET(PACAKMSR, paca_struct, kernel_msr);
+	OFFSET(PACASOFTIRQEN, paca_struct, soft_enabled);
+	OFFSET(PACAIRQHAPPENED, paca_struct, irq_happened);
 #ifdef CONFIG_PPC_BOOK3S
-	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, mm_ctx_id));
+	OFFSET(PACACONTEXTID, paca_struct, mm_ctx_id);
 #ifdef CONFIG_PPC_MM_SLICES
-	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,
-					    mm_ctx_low_slices_psize));
-	DEFINE(PACAHIGHSLICEPSIZE, offsetof(struct paca_struct,
-					    mm_ctx_high_slices_psize));
+	OFFSET(PACALOWSLICESPSIZE, paca_struct, mm_ctx_low_slices_psize);
+	OFFSET(PACAHIGHSLICEPSIZE, paca_struct, mm_ctx_high_slices_psize);
 	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
 #endif /* CONFIG_PPC_MM_SLICES */
 #endif
 
 #ifdef CONFIG_PPC_BOOK3E
-	DEFINE(PACAPGD, offsetof(struct paca_struct, pgd));
-	DEFINE(PACA_KERNELPGD, offsetof(struct paca_struct, kernel_pgd));
-	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
-	DEFINE(PACA_EXTLB, offsetof(struct paca_struct, extlb));
-	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
-	DEFINE(PACA_EXCRIT, offsetof(struct paca_struct, excrit));
-	DEFINE(PACA_EXDBG, offsetof(struct paca_struct, exdbg));
-	DEFINE(PACA_MC_STACK, offsetof(struct paca_struct, mc_kstack));
-	DEFINE(PACA_CRIT_STACK, offsetof(struct paca_struct, crit_kstack));
-	DEFINE(PACA_DBG_STACK, offsetof(struct paca_struct, dbg_kstack));
-	DEFINE(PACA_TCD_PTR, offsetof(struct paca_struct, tcd_ptr));
-
-	DEFINE(TCD_ESEL_NEXT,
-		offsetof(struct tlb_core_data, esel_next));
-	DEFINE(TCD_ESEL_MAX,
-		offsetof(struct tlb_core_data, esel_max));
-	DEFINE(TCD_ESEL_FIRST,
-		offsetof(struct tlb_core_data, esel_first));
+	OFFSET(PACAPGD, paca_struct, pgd);
+	OFFSET(PACA_KERNELPGD, paca_struct, kernel_pgd);
+	OFFSET(PACA_EXGEN, paca_struct, exgen);
+	OFFSET(PACA_EXTLB, paca_struct, extlb);
+	OFFSET(PACA_EXMC, paca_struct, exmc);
+	OFFSET(PACA_EXCRIT, paca_struct, excrit);
+	OFFSET(PACA_EXDBG, paca_struct, exdbg);
+	OFFSET(PACA_MC_STACK, paca_struct, mc_kstack);
+	OFFSET(PACA_CRIT_STACK, paca_struct, crit_kstack);
+	OFFSET(PACA_DBG_STACK, paca_struct, dbg_kstack);
+	OFFSET(PACA_TCD_PTR, paca_struct, tcd_ptr);
+
+	OFFSET(TCD_ESEL_NEXT, tlb_core_data, esel_next);
+	OFFSET(TCD_ESEL_MAX, tlb_core_data, esel_max);
+	OFFSET(TCD_ESEL_FIRST, tlb_core_data, esel_first);
 #endif /* CONFIG_PPC_BOOK3E */
 
 #ifdef CONFIG_PPC_STD_MMU_64
-	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
-	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
-	DEFINE(PACAVMALLOCSLLP, offsetof(struct paca_struct, vmalloc_sllp));
+	OFFSET(PACASLBCACHE, paca_struct, slb_cache);
+	OFFSET(PACASLBCACHEPTR, paca_struct, slb_cache_ptr);
+	OFFSET(PACAVMALLOCSLLP, paca_struct, vmalloc_sllp);
 #ifdef CONFIG_PPC_MM_SLICES
-	DEFINE(MMUPSIZESLLP, offsetof(struct mmu_psize_def, sllp));
+	OFFSET(MMUPSIZESLLP, mmu_psize_def, sllp);
 #else
-	DEFINE(PACACONTEXTSLLP, offsetof(struct paca_struct, mm_ctx_sllp));
+	OFFSET(PACACONTEXTSLLP, paca_struct, mm_ctx_sllp);
 #endif /* CONFIG_PPC_MM_SLICES */
-	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
-	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
-	DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));
-	DEFINE(PACALPPACAPTR, offsetof(struct paca_struct, lppaca_ptr));
-	DEFINE(PACA_SLBSHADOWPTR, offsetof(struct paca_struct, slb_shadow_ptr));
-	DEFINE(SLBSHADOW_STACKVSID,
-	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid));
-	DEFINE(SLBSHADOW_STACKESID,
-	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].esid));
-	DEFINE(SLBSHADOW_SAVEAREA, offsetof(struct slb_shadow, save_area));
-	DEFINE(LPPACA_PMCINUSE, offsetof(struct lppaca, pmcregs_in_use));
-	DEFINE(LPPACA_DTLIDX, offsetof(struct lppaca, dtl_idx));
-	DEFINE(LPPACA_YIELDCOUNT, offsetof(struct lppaca, yield_count));
-	DEFINE(PACA_DTL_RIDX, offsetof(struct paca_struct, dtl_ridx));
+	OFFSET(PACA_EXGEN, paca_struct, exgen);
+	OFFSET(PACA_EXMC, paca_struct, exmc);
+	OFFSET(PACA_EXSLB, paca_struct, exslb);
+	OFFSET(PACALPPACAPTR, paca_struct, lppaca_ptr);
+	OFFSET(PACA_SLBSHADOWPTR, paca_struct, slb_shadow_ptr);
+	OFFSET(SLBSHADOW_STACKVSID, slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid);
+	OFFSET(SLBSHADOW_STACKESID, slb_shadow, save_area[SLB_NUM_BOLTED - 1].esid);
+	OFFSET(SLBSHADOW_SAVEAREA, slb_shadow, save_area);
+	OFFSET(LPPACA_PMCINUSE, lppaca, pmcregs_in_use);
+	OFFSET(LPPACA_DTLIDX, lppaca, dtl_idx);
+	OFFSET(LPPACA_YIELDCOUNT, lppaca, yield_count);
+	OFFSET(PACA_DTL_RIDX, paca_struct, dtl_ridx);
 #endif /* CONFIG_PPC_STD_MMU_64 */
-	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
+	OFFSET(PACAEMERGSP, paca_struct, emergency_sp);
 #ifdef CONFIG_PPC_BOOK3S_64
-	DEFINE(PACAMCEMERGSP, offsetof(struct paca_struct, mc_emergency_sp));
-	DEFINE(PACA_IN_MCE, offsetof(struct paca_struct, in_mce));
-#endif
-	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
-	DEFINE(PACAKEXECSTATE, offsetof(struct paca_struct, kexec_state));
-	DEFINE(PACA_DSCR_DEFAULT, offsetof(struct paca_struct, dscr_default));
-	DEFINE(ACCOUNT_STARTTIME,
-	       offsetof(struct paca_struct, accounting.starttime));
-	DEFINE(ACCOUNT_STARTTIME_USER,
-	       offsetof(struct paca_struct, accounting.starttime_user));
-	DEFINE(ACCOUNT_USER_TIME,
-	       offsetof(struct paca_struct, accounting.user_time));
-	DEFINE(ACCOUNT_SYSTEM_TIME,
-	       offsetof(struct paca_struct, accounting.system_time));
-	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
-	DEFINE(PACA_NAPSTATELOST, offsetof(struct paca_struct, nap_state_lost));
-	DEFINE(PACA_SPRG_VDSO, offsetof(struct paca_struct, sprg_vdso));
+	OFFSET(PACAMCEMERGSP, paca_struct, mc_emergency_sp);
+	OFFSET(PACA_IN_MCE, paca_struct, in_mce);
+#endif
+	OFFSET(PACAHWCPUID, paca_struct, hw_cpu_id);
+	OFFSET(PACAKEXECSTATE, paca_struct, kexec_state);
+	OFFSET(PACA_DSCR_DEFAULT, paca_struct, dscr_default);
+	OFFSET(ACCOUNT_STARTTIME, paca_struct, accounting.starttime);
+	OFFSET(ACCOUNT_STARTTIME_USER, paca_struct, accounting.starttime_user);
+	OFFSET(ACCOUNT_USER_TIME, paca_struct, accounting.user_time);
+	OFFSET(ACCOUNT_SYSTEM_TIME, paca_struct, accounting.system_time);
+	OFFSET(PACA_TRAP_SAVE, paca_struct, trap_save);
+	OFFSET(PACA_NAPSTATELOST, paca_struct, nap_state_lost);
+	OFFSET(PACA_SPRG_VDSO, paca_struct, sprg_vdso);
 #else /* CONFIG_PPC64 */
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
-	DEFINE(ACCOUNT_STARTTIME,
-	       offsetof(struct thread_info, accounting.starttime));
-	DEFINE(ACCOUNT_STARTTIME_USER,
-	       offsetof(struct thread_info, accounting.starttime_user));
-	DEFINE(ACCOUNT_USER_TIME,
-	       offsetof(struct thread_info, accounting.user_time));
-	DEFINE(ACCOUNT_SYSTEM_TIME,
-	       offsetof(struct thread_info, accounting.system_time));
+	OFFSET(ACCOUNT_STARTTIME, thread_info, accounting.starttime);
+	OFFSET(ACCOUNT_STARTTIME_USER, thread_info, accounting.starttime_user);
+	OFFSET(ACCOUNT_USER_TIME, thread_info, accounting.user_time);
+	OFFSET(ACCOUNT_SYSTEM_TIME, thread_info, accounting.system_time);
 #endif
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */
-	DEFINE(RTASBASE, offsetof(struct rtas_t, base));
-	DEFINE(RTASENTRY, offsetof(struct rtas_t, entry));
+	OFFSET(RTASBASE, rtas_t, base);
+	OFFSET(RTASENTRY, rtas_t, entry);
 
 	/* Interrupt register frame */
 	DEFINE(INT_FRAME_SIZE, STACK_INT_FRAME_SIZE);
@@ -354,17 +336,17 @@ int main(void)
 #endif
 
 #ifndef CONFIG_PPC64
-	DEFINE(MM_PGD, offsetof(struct mm_struct, pgd));
+	OFFSET(MM_PGD, mm_struct, pgd);
 #endif /* ! CONFIG_PPC64 */
 
 	/* About the CPU features table */
-	DEFINE(CPU_SPEC_FEATURES, offsetof(struct cpu_spec, cpu_features));
-	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
-	DEFINE(CPU_SPEC_RESTORE, offsetof(struct cpu_spec, cpu_restore));
+	OFFSET(CPU_SPEC_FEATURES, cpu_spec, cpu_features);
+	OFFSET(CPU_SPEC_SETUP, cpu_spec, cpu_setup);
+	OFFSET(CPU_SPEC_RESTORE, cpu_spec, cpu_restore);
 
-	DEFINE(pbe_address, offsetof(struct pbe, address));
-	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));
-	DEFINE(pbe_next, offsetof(struct pbe, next));
+	OFFSET(pbe_address, pbe, address);
+	OFFSET(pbe_orig_address, pbe, orig_address);
+	OFFSET(pbe_next, pbe, next);
 
 #ifndef CONFIG_PPC64
 	DEFINE(TASK_SIZE, TASK_SIZE);
@@ -372,40 +354,40 @@ int main(void)
 #endif /* ! CONFIG_PPC64 */
 
 	/* datapage offsets for use by vdso */
-	DEFINE(CFG_TB_ORIG_STAMP, offsetof(struct vdso_data, tb_orig_stamp));
-	DEFINE(CFG_TB_TICKS_PER_SEC, offsetof(struct vdso_data, tb_ticks_per_sec));
-	DEFINE(CFG_TB_TO_XS, offsetof(struct vdso_data, tb_to_xs));
-	DEFINE(CFG_TB_UPDATE_COUNT, offsetof(struct vdso_data, tb_update_count));
-	DEFINE(CFG_TZ_MINUTEWEST, offsetof(struct vdso_data, tz_minuteswest));
-	DEFINE(CFG_TZ_DSTTIME, offsetof(struct vdso_data, tz_dsttime));
-	DEFINE(CFG_SYSCALL_MAP32, offsetof(struct vdso_data, syscall_map_32));
-	DEFINE(WTOM_CLOCK_SEC, offsetof(struct vdso_data, wtom_clock_sec));
-	DEFINE(WTOM_CLOCK_NSEC, offsetof(struct vdso_data, wtom_clock_nsec));
-	DEFINE(STAMP_XTIME, offsetof(struct vdso_data, stamp_xtime));
-	DEFINE(STAMP_SEC_FRAC, offsetof(struct vdso_data, stamp_sec_fraction));
-	DEFINE(CFG_ICACHE_BLOCKSZ, offsetof(struct vdso_data, icache_block_size));
-	DEFINE(CFG_DCACHE_BLOCKSZ, offsetof(struct vdso_data, dcache_block_size));
-	DEFINE(CFG_ICACHE_LOGBLOCKSZ, offsetof(struct vdso_data, icache_log_block_size));
-	DEFINE(CFG_DCACHE_LOGBLOCKSZ, offsetof(struct vdso_data, dcache_log_block_size));
+	OFFSET(CFG_TB_ORIG_STAMP, vdso_data, tb_orig_stamp);
+	OFFSET(CFG_TB_TICKS_PER_SEC, vdso_data, tb_ticks_per_sec);
+	OFFSET(CFG_TB_TO_XS, vdso_data, tb_to_xs);
+	OFFSET(CFG_TB_UPDATE_COUNT, vdso_data, tb_update_count);
+	OFFSET(CFG_TZ_MINUTEWEST, vdso_data, tz_minuteswest);
+	OFFSET(CFG_TZ_DSTTIME, vdso_data, tz_dsttime);
+	OFFSET(CFG_SYSCALL_MAP32, vdso_data, syscall_map_32);
+	OFFSET(WTOM_CLOCK_SEC, vdso_data, wtom_clock_sec);
+	OFFSET(WTOM_CLOCK_NSEC, vdso_data, wtom_clock_nsec);
+	OFFSET(STAMP_XTIME, vdso_data, stamp_xtime);
+	OFFSET(STAMP_SEC_FRAC, vdso_data, stamp_sec_fraction);
+	OFFSET(CFG_ICACHE_BLOCKSZ, vdso_data, icache_block_size);
+	OFFSET(CFG_DCACHE_BLOCKSZ, vdso_data, dcache_block_size);
+	OFFSET(CFG_ICACHE_LOGBLOCKSZ, vdso_data, icache_log_block_size);
+	OFFSET(CFG_DCACHE_LOGBLOCKSZ, vdso_data, dcache_log_block_size);
 #ifdef CONFIG_PPC64
-	DEFINE(CFG_SYSCALL_MAP64, offsetof(struct vdso_data, syscall_map_64));
-	DEFINE(TVAL64_TV_SEC, offsetof(struct timeval, tv_sec));
-	DEFINE(TVAL64_TV_USEC, offsetof(struct timeval, tv_usec));
-	DEFINE(TVAL32_TV_SEC, offsetof(struct compat_timeval, tv_sec));
-	DEFINE(TVAL32_TV_USEC, offsetof(struct compat_timeval, tv_usec));
-	DEFINE(TSPC64_TV_SEC, offsetof(struct timespec, tv_sec));
-	DEFINE(TSPC64_TV_NSEC, offsetof(struct timespec, tv_nsec));
-	DEFINE(TSPC32_TV_SEC, offsetof(struct compat_timespec, tv_sec));
-	DEFINE(TSPC32_TV_NSEC, offsetof(struct compat_timespec, tv_nsec));
+	OFFSET(CFG_SYSCALL_MAP64, vdso_data, syscall_map_64);
+	OFFSET(TVAL64_TV_SEC, timeval, tv_sec);
+	OFFSET(TVAL64_TV_USEC, timeval, tv_usec);
+	OFFSET(TVAL32_TV_SEC, compat_timeval, tv_sec);
+	OFFSET(TVAL32_TV_USEC, compat_timeval, tv_usec);
+	OFFSET(TSPC64_TV_SEC, timespec, tv_sec);
+	OFFSET(TSPC64_TV_NSEC, timespec, tv_nsec);
+	OFFSET(TSPC32_TV_SEC, compat_timespec, tv_sec);
+	OFFSET(TSPC32_TV_NSEC, compat_timespec, tv_nsec);
 #else
-	DEFINE(TVAL32_TV_SEC, offsetof(struct timeval, tv_sec));
-	DEFINE(TVAL32_TV_USEC, offsetof(struct timeval, tv_usec));
-	DEFINE(TSPC32_TV_SEC, offsetof(struct timespec, tv_sec));
-	DEFINE(TSPC32_TV_NSEC, offsetof(struct timespec, tv_nsec));
+	OFFSET(TVAL32_TV_SEC, timeval, tv_sec);
+	OFFSET(TVAL32_TV_USEC, timeval, tv_usec);
+	OFFSET(TSPC32_TV_SEC, timespec, tv_sec);
+	OFFSET(TSPC32_TV_NSEC, timespec, tv_nsec);
 #endif
 	/* timeval/timezone offsets for use by vdso */
-	DEFINE(TZONE_TZ_MINWEST, offsetof(struct timezone, tz_minuteswest));
-	DEFINE(TZONE_TZ_DSTTIME, offsetof(struct timezone, tz_dsttime));
+	OFFSET(TZONE_TZ_MINWEST, timezone, tz_minuteswest);
+	OFFSET(TZONE_TZ_DSTTIME, timezone, tz_dsttime);
 
 	/* Other bits used by the vdso */
 	DEFINE(CLOCK_REALTIME, CLOCK_REALTIME);
@@ -425,170 +407,170 @@ int main(void)
 	DEFINE(PTE_SIZE, sizeof(pte_t));
 
 #ifdef CONFIG_KVM
-	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
-	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
-	DEFINE(VCPU_GUEST_PID, offsetof(struct kvm_vcpu, arch.pid));
-	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
-	DEFINE(VCPU_VRSAVE, offsetof(struct kvm_vcpu, arch.vrsave));
-	DEFINE(VCPU_FPRS, offsetof(struct kvm_vcpu, arch.fp.fpr));
+	OFFSET(VCPU_HOST_STACK, kvm_vcpu, arch.host_stack);
+	OFFSET(VCPU_HOST_PID, kvm_vcpu, arch.host_pid);
+	OFFSET(VCPU_GUEST_PID, kvm_vcpu, arch.pid);
+	OFFSET(VCPU_GPRS, kvm_vcpu, arch.gpr);
+	OFFSET(VCPU_VRSAVE, kvm_vcpu, arch.vrsave);
+	OFFSET(VCPU_FPRS, kvm_vcpu, arch.fp.fpr);
 #ifdef CONFIG_ALTIVEC
-	DEFINE(VCPU_VRS, offsetof(struct kvm_vcpu, arch.vr.vr));
+	OFFSET(VCPU_VRS, kvm_vcpu, arch.vr.vr);
 #endif
-	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
-	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
-	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
+	OFFSET(VCPU_XER, kvm_vcpu, arch.xer);
+	OFFSET(VCPU_CTR, kvm_vcpu, arch.ctr);
+	OFFSET(VCPU_LR, kvm_vcpu, arch.lr);
 #ifdef CONFIG_PPC_BOOK3S
-	DEFINE(VCPU_TAR, offsetof(struct kvm_vcpu, arch.tar));
+	OFFSET(VCPU_TAR, kvm_vcpu, arch.tar);
 #endif
-	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
-	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
+	OFFSET(VCPU_CR, kvm_vcpu, arch.cr);
+	OFFSET(VCPU_PC, kvm_vcpu, arch.pc);
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
-	DEFINE(VCPU_MSR, offsetof(struct kvm_vcpu, arch.shregs.msr));
-	DEFINE(VCPU_SRR0, offsetof(struct kvm_vcpu, arch.shregs.srr0));
-	DEFINE(VCPU_SRR1, offsetof(struct kvm_vcpu, arch.shregs.srr1));
-	DEFINE(VCPU_SPRG0, offsetof(struct kvm_vcpu, arch.shregs.sprg0));
-	DEFINE(VCPU_SPRG1, offsetof(struct kvm_vcpu, arch.shregs.sprg1));
-	DEFINE(VCPU_SPRG2, offsetof(struct kvm_vcpu, arch.shregs.sprg2));
-	DEFINE(VCPU_SPRG3, offsetof(struct kvm_vcpu, arch.shregs.sprg3));
+	OFFSET(VCPU_MSR, kvm_vcpu, arch.shregs.msr);
+	OFFSET(VCPU_SRR0, kvm_vcpu, arch.shregs.srr0);
+	OFFSET(VCPU_SRR1, kvm_vcpu, arch.shregs.srr1);
+	OFFSET(VCPU_SPRG0, kvm_vcpu, arch.shregs.sprg0);
+	OFFSET(VCPU_SPRG1, kvm_vcpu, arch.shregs.sprg1);
+	OFFSET(VCPU_SPRG2, kvm_vcpu, arch.shregs.sprg2);
+	OFFSET(VCPU_SPRG3, kvm_vcpu, arch.shregs.sprg3);
 #endif
 #ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING
-	DEFINE(VCPU_TB_RMENTRY, offsetof(struct kvm_vcpu, arch.rm_entry));
-	DEFINE(VCPU_TB_RMINTR, offsetof(struct kvm_vcpu, arch.rm_intr));
-	DEFINE(VCPU_TB_RMEXIT, offsetof(struct kvm_vcpu, arch.rm_exit));
-	DEFINE(VCPU_TB_GUEST, offsetof(struct kvm_vcpu, arch.guest_time));
-	DEFINE(VCPU_TB_CEDE, offsetof(struct kvm_vcpu, arch.cede_time));
-	DEFINE(VCPU_CUR_ACTIVITY, offsetof(struct kvm_vcpu, arch.cur_activity));
-	DEFINE(VCPU_ACTIVITY_START, offsetof(struct kvm_vcpu, arch.cur_tb_start));
-	DEFINE(TAS_SEQCOUNT, offsetof(struct kvmhv_tb_accumulator, seqcount));
-	DEFINE(TAS_TOTAL, offsetof(struct kvmhv_tb_accumulator, tb_total));
-	DEFINE(TAS_MIN, offsetof(struct kvmhv_tb_accumulator, tb_min));
-	DEFINE(TAS_MAX, offsetof(struct kvmhv_tb_accumulator, tb_max));
-#endif
-	DEFINE(VCPU_SHARED_SPRG3, offsetof(struct kvm_vcpu_arch_shared, sprg3));
-	DEFINE(VCPU_SHARED_SPRG4, offsetof(struct kvm_vcpu_arch_shared, sprg4));
-	DEFINE(VCPU_SHARED_SPRG5, offsetof(struct kvm_vcpu_arch_shared, sprg5));
-	DEFINE(VCPU_SHARED_SPRG6, offsetof(struct kvm_vcpu_arch_shared, sprg6));
-	DEFINE(VCPU_SHARED_SPRG7, offsetof(struct kvm_vcpu_arch_shared, sprg7));
-	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
-	DEFINE(VCPU_SHADOW_PID1, offsetof(struct kvm_vcpu, arch.shadow_pid1));
-	DEFINE(VCPU_SHARED, offsetof(struct kvm_vcpu, arch.shared));
-	DEFINE(VCPU_SHARED_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
-	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
+	OFFSET(VCPU_TB_RMENTRY, kvm_vcpu, arch.rm_entry);
+	OFFSET(VCPU_TB_RMINTR, kvm_vcpu, arch.rm_intr);
+	OFFSET(VCPU_TB_RMEXIT, kvm_vcpu, arch.rm_exit);
+	OFFSET(VCPU_TB_GUEST, kvm_vcpu, arch.guest_time);
+	OFFSET(VCPU_TB_CEDE, kvm_vcpu, arch.cede_time);
+	OFFSET(VCPU_CUR_ACTIVITY, kvm_vcpu, arch.cur_activity);
+	OFFSET(VCPU_ACTIVITY_START, kvm_vcpu, arch.cur_tb_start);
+	OFFSET(TAS_SEQCOUNT, kvmhv_tb_accumulator, seqcount);
+	OFFSET(TAS_TOTAL, kvmhv_tb_accumulator, tb_total);
+	OFFSET(TAS_MIN, kvmhv_tb_accumulator, tb_min);
+	OFFSET(TAS_MAX, kvmhv_tb_accumulator, tb_max);
+#endif
+	OFFSET(VCPU_SHARED_SPRG3, kvm_vcpu_arch_shared, sprg3);
+	OFFSET(VCPU_SHARED_SPRG4, kvm_vcpu_arch_shared, sprg4);
+	OFFSET(VCPU_SHARED_SPRG5, kvm_vcpu_arch_shared, sprg5);
+	OFFSET(VCPU_SHARED_SPRG6, kvm_vcpu_arch_shared, sprg6);
+	OFFSET(VCPU_SHARED_SPRG7, kvm_vcpu_arch_shared, sprg7);
+	OFFSET(VCPU_SHADOW_PID, kvm_vcpu, arch.shadow_pid);
+	OFFSET(VCPU_SHADOW_PID1, kvm_vcpu, arch.shadow_pid1);
+	OFFSET(VCPU_SHARED, kvm_vcpu, arch.shared);
+	OFFSET(VCPU_SHARED_MSR, kvm_vcpu_arch_shared, msr);
+	OFFSET(VCPU_SHADOW_MSR, kvm_vcpu, arch.shadow_msr);
 #if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)
-	DEFINE(VCPU_SHAREDBE, offsetof(struct kvm_vcpu, arch.shared_big_endian));
+	OFFSET(VCPU_SHAREDBE, kvm_vcpu, arch.shared_big_endian);
 #endif
 
-	DEFINE(VCPU_SHARED_MAS0, offsetof(struct kvm_vcpu_arch_shared, mas0));
-	DEFINE(VCPU_SHARED_MAS1, offsetof(struct kvm_vcpu_arch_shared, mas1));
-	DEFINE(VCPU_SHARED_MAS2, offsetof(struct kvm_vcpu_arch_shared, mas2));
-	DEFINE(VCPU_SHARED_MAS7_3, offsetof(struct kvm_vcpu_arch_shared, mas7_3));
-	DEFINE(VCPU_SHARED_MAS4, offsetof(struct kvm_vcpu_arch_shared, mas4));
-	DEFINE(VCPU_SHARED_MAS6, offsetof(struct kvm_vcpu_arch_shared, mas6));
+	OFFSET(VCPU_SHARED_MAS0, kvm_vcpu_arch_shared, mas0);
+	OFFSET(VCPU_SHARED_MAS1, kvm_vcpu_arch_shared, mas1);
+	OFFSET(VCPU_SHARED_MAS2, kvm_vcpu_arch_shared, mas2);
+	OFFSET(VCPU_SHARED_MAS7_3, kvm_vcpu_arch_shared, mas7_3);
+	OFFSET(VCPU_SHARED_MAS4, kvm_vcpu_arch_shared, mas4);
+	OFFSET(VCPU_SHARED_MAS6, kvm_vcpu_arch_shared, mas6);
 
-	DEFINE(VCPU_KVM, offsetof(struct kvm_vcpu, kvm));
-	DEFINE(KVM_LPID, offsetof(struct kvm, arch.lpid));
+	OFFSET(VCPU_KVM, kvm_vcpu, kvm);
+	OFFSET(KVM_LPID, kvm, arch.lpid);
 
 	/* book3s */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
-	DEFINE(KVM_TLB_SETS, offsetof(struct kvm, arch.tlb_sets));
-	DEFINE(KVM_SDR1, offsetof(struct kvm, arch.sdr1));
-	DEFINE(KVM_HOST_LPID, offsetof(struct kvm, arch.host_lpid));
-	DEFINE(KVM_HOST_LPCR, offsetof(struct kvm, arch.host_lpcr));
-	DEFINE(KVM_HOST_SDR1, offsetof(struct kvm, arch.host_sdr1));
-	DEFINE(KVM_NEED_FLUSH, offsetof(struct kvm, arch.need_tlb_flush.bits));
-	DEFINE(KVM_ENABLED_HCALLS, offsetof(struct kvm, arch.enabled_hcalls));
-	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));
-	DEFINE(KVM_RADIX, offsetof(struct kvm, arch.radix));
-	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
-	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
-	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
-	DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
-	DEFINE(VCPU_HEIR, offsetof(struct kvm_vcpu, arch.emul_inst));
-	DEFINE(VCPU_CPU, offsetof(struct kvm_vcpu, cpu));
-	DEFINE(VCPU_THREAD_CPU, offsetof(struct kvm_vcpu, arch.thread_cpu));
+	OFFSET(KVM_TLB_SETS, kvm, arch.tlb_sets);
+	OFFSET(KVM_SDR1, kvm, arch.sdr1);
+	OFFSET(KVM_HOST_LPID, kvm, arch.host_lpid);
+	OFFSET(KVM_HOST_LPCR, kvm, arch.host_lpcr);
+	OFFSET(KVM_HOST_SDR1, kvm, arch.host_sdr1);
+	OFFSET(KVM_NEED_FLUSH, kvm, arch.need_tlb_flush.bits);
+	OFFSET(KVM_ENABLED_HCALLS, kvm, arch.enabled_hcalls);
+	OFFSET(KVM_VRMA_SLB_V, kvm, arch.vrma_slb_v);
+	OFFSET(KVM_RADIX, kvm, arch.radix);
+	OFFSET(VCPU_DSISR, kvm_vcpu, arch.shregs.dsisr);
+	OFFSET(VCPU_DAR, kvm_vcpu, arch.shregs.dar);
+	OFFSET(VCPU_VPA, kvm_vcpu, arch.vpa.pinned_addr);
+	OFFSET(VCPU_VPA_DIRTY, kvm_vcpu, arch.vpa.dirty);
+	OFFSET(VCPU_HEIR, kvm_vcpu, arch.emul_inst);
+	OFFSET(VCPU_CPU, kvm_vcpu, cpu);
+	OFFSET(VCPU_THREAD_CPU, kvm_vcpu, arch.thread_cpu);
 #endif
 #ifdef CONFIG_PPC_BOOK3S
-	DEFINE(VCPU_PURR, offsetof(struct kvm_vcpu, arch.purr));
-	DEFINE(VCPU_SPURR, offsetof(struct kvm_vcpu, arch.spurr));
-	DEFINE(VCPU_IC, offsetof(struct kvm_vcpu, arch.ic));
-	DEFINE(VCPU_DSCR, offsetof(struct kvm_vcpu, arch.dscr));
-	DEFINE(VCPU_AMR, offsetof(struct kvm_vcpu, arch.amr));
-	DEFINE(VCPU_UAMOR, offsetof(struct kvm_vcpu, arch.uamor));
-	DEFINE(VCPU_IAMR, offsetof(struct kvm_vcpu, arch.iamr));
-	DEFINE(VCPU_CTRL, offsetof(struct kvm_vcpu, arch.ctrl));
-	DEFINE(VCPU_DABR, offsetof(struct kvm_vcpu, arch.dabr));
-	DEFINE(VCPU_DABRX, offsetof(struct kvm_vcpu, arch.dabrx));
-	DEFINE(VCPU_DAWR, offsetof(struct kvm_vcpu, arch.dawr));
-	DEFINE(VCPU_DAWRX, offsetof(struct kvm_vcpu, arch.dawrx));
-	DEFINE(VCPU_CIABR, offsetof(struct kvm_vcpu, arch.ciabr));
-	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
-	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
-	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
-	DEFINE(VCPU_PENDING_EXC, offsetof(struct kvm_vcpu, arch.pending_exceptions));
-	DEFINE(VCPU_CEDED, offsetof(struct kvm_vcpu, arch.ceded));
-	DEFINE(VCPU_PRODDED, offsetof(struct kvm_vcpu, arch.prodded));
-	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
-	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
-	DEFINE(VCPU_SPMC, offsetof(struct kvm_vcpu, arch.spmc));
-	DEFINE(VCPU_SIAR, offsetof(struct kvm_vcpu, arch.siar));
-	DEFINE(VCPU_SDAR, offsetof(struct kvm_vcpu, arch.sdar));
-	DEFINE(VCPU_SIER, offsetof(struct kvm_vcpu, arch.sier));
-	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));
-	DEFINE(VCPU_SLB_MAX, offsetof(struct kvm_vcpu, arch.slb_max));
-	DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));
-	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
-	DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
-	DEFINE(VCPU_FAULT_GPA, offsetof(struct kvm_vcpu, arch.fault_gpa));
-	DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
-	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
-	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
-	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
-	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
-	DEFINE(VCPU_FSCR, offsetof(struct kvm_vcpu, arch.fscr));
-	DEFINE(VCPU_PSPB, offsetof(struct kvm_vcpu, arch.pspb));
-	DEFINE(VCPU_EBBHR, offsetof(struct kvm_vcpu, arch.ebbhr));
-	DEFINE(VCPU_EBBRR, offsetof(struct kvm_vcpu, arch.ebbrr));
-	DEFINE(VCPU_BESCR, offsetof(struct kvm_vcpu, arch.bescr));
-	DEFINE(VCPU_CSIGR, offsetof(struct kvm_vcpu, arch.csigr));
-	DEFINE(VCPU_TACR, offsetof(struct kvm_vcpu, arch.tacr));
-	DEFINE(VCPU_TCSCR, offsetof(struct kvm_vcpu, arch.tcscr));
-	DEFINE(VCPU_ACOP, offsetof(struct kvm_vcpu, arch.acop));
-	DEFINE(VCPU_WORT, offsetof(struct kvm_vcpu, arch.wort));
-	DEFINE(VCPU_TID, offsetof(struct kvm_vcpu, arch.tid));
-	DEFINE(VCPU_PSSCR, offsetof(struct kvm_vcpu, arch.psscr));
-	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_map));
-	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
-	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
-	DEFINE(VCORE_KVM, offsetof(struct kvmppc_vcore, kvm));
-	DEFINE(VCORE_TB_OFFSET, offsetof(struct kvmppc_vcore, tb_offset));
-	DEFINE(VCORE_LPCR, offsetof(struct kvmppc_vcore, lpcr));
-	DEFINE(VCORE_PCR, offsetof(struct kvmppc_vcore, pcr));
-	DEFINE(VCORE_DPDES, offsetof(struct kvmppc_vcore, dpdes));
-	DEFINE(VCORE_VTB, offsetof(struct kvmppc_vcore, vtb));
-	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
-	DEFINE(VCPU_SLB_V, offsetof(struct kvmppc_slb, origv));
+	OFFSET(VCPU_PURR, kvm_vcpu, arch.purr);
+	OFFSET(VCPU_SPURR, kvm_vcpu, arch.spurr);
+	OFFSET(VCPU_IC, kvm_vcpu, arch.ic);
+	OFFSET(VCPU_DSCR, kvm_vcpu, arch.dscr);
+	OFFSET(VCPU_AMR, kvm_vcpu, arch.amr);
+	OFFSET(VCPU_UAMOR, kvm_vcpu, arch.uamor);
+	OFFSET(VCPU_IAMR, kvm_vcpu, arch.iamr);
+	OFFSET(VCPU_CTRL, kvm_vcpu, arch.ctrl);
+	OFFSET(VCPU_DABR, kvm_vcpu, arch.dabr);
+	OFFSET(VCPU_DABRX, kvm_vcpu, arch.dabrx);
+	OFFSET(VCPU_DAWR, kvm_vcpu, arch.dawr);
+	OFFSET(VCPU_DAWRX, kvm_vcpu, arch.dawrx);
+	OFFSET(VCPU_CIABR, kvm_vcpu, arch.ciabr);
+	OFFSET(VCPU_HFLAGS, kvm_vcpu, arch.hflags);
+	OFFSET(VCPU_DEC, kvm_vcpu, arch.dec);
+	OFFSET(VCPU_DEC_EXPIRES, kvm_vcpu, arch.dec_expires);
+	OFFSET(VCPU_PENDING_EXC, kvm_vcpu, arch.pending_exceptions);
+	OFFSET(VCPU_CEDED, kvm_vcpu, arch.ceded);
+	OFFSET(VCPU_PRODDED, kvm_vcpu, arch.prodded);
+	OFFSET(VCPU_MMCR, kvm_vcpu, arch.mmcr);
+	OFFSET(VCPU_PMC, kvm_vcpu, arch.pmc);
+	OFFSET(VCPU_SPMC, kvm_vcpu, arch.spmc);
+	OFFSET(VCPU_SIAR, kvm_vcpu, arch.siar);
+	OFFSET(VCPU_SDAR, kvm_vcpu, arch.sdar);
+	OFFSET(VCPU_SIER, kvm_vcpu, arch.sier);
+	OFFSET(VCPU_SLB, kvm_vcpu, arch.slb);
+	OFFSET(VCPU_SLB_MAX, kvm_vcpu, arch.slb_max);
+	OFFSET(VCPU_SLB_NR, kvm_vcpu, arch.slb_nr);
+	OFFSET(VCPU_FAULT_DSISR, kvm_vcpu, arch.fault_dsisr);
+	OFFSET(VCPU_FAULT_DAR, kvm_vcpu, arch.fault_dar);
+	OFFSET(VCPU_FAULT_GPA, kvm_vcpu, arch.fault_gpa);
+	OFFSET(VCPU_INTR_MSR, kvm_vcpu, arch.intr_msr);
+	OFFSET(VCPU_LAST_INST, kvm_vcpu, arch.last_inst);
+	OFFSET(VCPU_TRAP, kvm_vcpu, arch.trap);
+	OFFSET(VCPU_CFAR, kvm_vcpu, arch.cfar);
+	OFFSET(VCPU_PPR, kvm_vcpu, arch.ppr);
+	OFFSET(VCPU_FSCR, kvm_vcpu, arch.fscr);
+	OFFSET(VCPU_PSPB, kvm_vcpu, arch.pspb);
+	OFFSET(VCPU_EBBHR, kvm_vcpu, arch.ebbhr);
+	OFFSET(VCPU_EBBRR, kvm_vcpu, arch.ebbrr);
+	OFFSET(VCPU_BESCR, kvm_vcpu, arch.bescr);
+	OFFSET(VCPU_CSIGR, kvm_vcpu, arch.csigr);
+	OFFSET(VCPU_TACR, kvm_vcpu, arch.tacr);
+	OFFSET(VCPU_TCSCR, kvm_vcpu, arch.tcscr);
+	OFFSET(VCPU_ACOP, kvm_vcpu, arch.acop);
+	OFFSET(VCPU_WORT, kvm_vcpu, arch.wort);
+	OFFSET(VCPU_TID, kvm_vcpu, arch.tid);
+	OFFSET(VCPU_PSSCR, kvm_vcpu, arch.psscr);
+	OFFSET(VCORE_ENTRY_EXIT, kvmppc_vcore, entry_exit_map);
+	OFFSET(VCORE_IN_GUEST, kvmppc_vcore, in_guest);
+	OFFSET(VCORE_NAPPING_THREADS, kvmppc_vcore, napping_threads);
+	OFFSET(VCORE_KVM, kvmppc_vcore, kvm);
+	OFFSET(VCORE_TB_OFFSET, kvmppc_vcore, tb_offset);
+	OFFSET(VCORE_LPCR, kvmppc_vcore, lpcr);
+	OFFSET(VCORE_PCR, kvmppc_vcore, pcr);
+	OFFSET(VCORE_DPDES, kvmppc_vcore, dpdes);
+	OFFSET(VCORE_VTB, kvmppc_vcore, vtb);
+	OFFSET(VCPU_SLB_E, kvmppc_slb, orige);
+	OFFSET(VCPU_SLB_V, kvmppc_slb, origv);
 	DEFINE(VCPU_SLB_SIZE, sizeof(struct kvmppc_slb));
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
-	DEFINE(VCPU_TFHAR, offsetof(struct kvm_vcpu, arch.tfhar));
-	DEFINE(VCPU_TFIAR, offsetof(struct kvm_vcpu, arch.tfiar));
-	DEFINE(VCPU_TEXASR, offsetof(struct kvm_vcpu, arch.texasr));
-	DEFINE(VCPU_GPR_TM, offsetof(struct kvm_vcpu, arch.gpr_tm));
-	DEFINE(VCPU_FPRS_TM, offsetof(struct kvm_vcpu, arch.fp_tm.fpr));
-	DEFINE(VCPU_VRS_TM, offsetof(struct kvm_vcpu, arch.vr_tm.vr));
-	DEFINE(VCPU_VRSAVE_TM, offsetof(struct kvm_vcpu, arch.vrsave_tm));
-	DEFINE(VCPU_CR_TM, offsetof(struct kvm_vcpu, arch.cr_tm));
-	DEFINE(VCPU_XER_TM, offsetof(struct kvm_vcpu, arch.xer_tm));
-	DEFINE(VCPU_LR_TM, offsetof(struct kvm_vcpu, arch.lr_tm));
-	DEFINE(VCPU_CTR_TM, offsetof(struct kvm_vcpu, arch.ctr_tm));
-	DEFINE(VCPU_AMR_TM, offsetof(struct kvm_vcpu, arch.amr_tm));
-	DEFINE(VCPU_PPR_TM, offsetof(struct kvm_vcpu, arch.ppr_tm));
-	DEFINE(VCPU_DSCR_TM, offsetof(struct kvm_vcpu, arch.dscr_tm));
-	DEFINE(VCPU_TAR_TM, offsetof(struct kvm_vcpu, arch.tar_tm));
+	OFFSET(VCPU_TFHAR, kvm_vcpu, arch.tfhar);
+	OFFSET(VCPU_TFIAR, kvm_vcpu, arch.tfiar);
+	OFFSET(VCPU_TEXASR, kvm_vcpu, arch.texasr);
+	OFFSET(VCPU_GPR_TM, kvm_vcpu, arch.gpr_tm);
+	OFFSET(VCPU_FPRS_TM, kvm_vcpu, arch.fp_tm.fpr);
+	OFFSET(VCPU_VRS_TM, kvm_vcpu, arch.vr_tm.vr);
+	OFFSET(VCPU_VRSAVE_TM, kvm_vcpu, arch.vrsave_tm);
+	OFFSET(VCPU_CR_TM, kvm_vcpu, arch.cr_tm);
+	OFFSET(VCPU_XER_TM, kvm_vcpu, arch.xer_tm);
+	OFFSET(VCPU_LR_TM, kvm_vcpu, arch.lr_tm);
+	OFFSET(VCPU_CTR_TM, kvm_vcpu, arch.ctr_tm);
+	OFFSET(VCPU_AMR_TM, kvm_vcpu, arch.amr_tm);
+	OFFSET(VCPU_PPR_TM, kvm_vcpu, arch.ppr_tm);
+	OFFSET(VCPU_DSCR_TM, kvm_vcpu, arch.dscr_tm);
+	OFFSET(VCPU_TAR_TM, kvm_vcpu, arch.tar_tm);
 #endif
 
 #ifdef CONFIG_PPC_BOOK3S_64
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
-	DEFINE(PACA_SVCPU, offsetof(struct paca_struct, shadow_vcpu));
+	OFFSET(PACA_SVCPU, paca_struct, shadow_vcpu);
 # define SVCPU_FIELD(x, f)	DEFINE(x, offsetof(struct paca_struct, shadow_vcpu.f))
 #else
 # define SVCPU_FIELD(x, f)
@@ -671,11 +653,11 @@ int main(void)
 	HSTATE_FIELD(HSTATE_DECEXP, dec_expires);
 	HSTATE_FIELD(HSTATE_SPLIT_MODE, kvm_split_mode);
 	DEFINE(IPI_PRIORITY, IPI_PRIORITY);
-	DEFINE(KVM_SPLIT_RPR, offsetof(struct kvm_split_mode, rpr));
-	DEFINE(KVM_SPLIT_PMMAR, offsetof(struct kvm_split_mode, pmmar));
-	DEFINE(KVM_SPLIT_LDBAR, offsetof(struct kvm_split_mode, ldbar));
-	DEFINE(KVM_SPLIT_DO_NAP, offsetof(struct kvm_split_mode, do_nap));
-	DEFINE(KVM_SPLIT_NAPPED, offsetof(struct kvm_split_mode, napped));
+	OFFSET(KVM_SPLIT_RPR, kvm_split_mode, rpr);
+	OFFSET(KVM_SPLIT_PMMAR, kvm_split_mode, pmmar);
+	OFFSET(KVM_SPLIT_LDBAR, kvm_split_mode, ldbar);
+	OFFSET(KVM_SPLIT_DO_NAP, kvm_split_mode, do_nap);
+	OFFSET(KVM_SPLIT_NAPPED, kvm_split_mode, napped);
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 
 #ifdef CONFIG_PPC_BOOK3S_64
@@ -685,32 +667,27 @@ int main(void)
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 #else /* CONFIG_PPC_BOOK3S */
-	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
-	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
-	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
-	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
-	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
-	DEFINE(VCPU_SPRG9, offsetof(struct kvm_vcpu, arch.sprg9));
-	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
-	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
-	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
-	DEFINE(VCPU_CRIT_SAVE, offsetof(struct kvm_vcpu, arch.crit_save));
+	OFFSET(VCPU_CR, kvm_vcpu, arch.cr);
+	OFFSET(VCPU_XER, kvm_vcpu, arch.xer);
+	OFFSET(VCPU_LR, kvm_vcpu, arch.lr);
+	OFFSET(VCPU_CTR, kvm_vcpu, arch.ctr);
+	OFFSET(VCPU_PC, kvm_vcpu, arch.pc);
+	OFFSET(VCPU_SPRG9, kvm_vcpu, arch.sprg9);
+	OFFSET(VCPU_LAST_INST, kvm_vcpu, arch.last_inst);
+	OFFSET(VCPU_FAULT_DEAR, kvm_vcpu, arch.fault_dear);
+	OFFSET(VCPU_FAULT_ESR, kvm_vcpu, arch.fault_esr);
+	OFFSET(VCPU_CRIT_SAVE, kvm_vcpu, arch.crit_save);
 #endif /* CONFIG_PPC_BOOK3S */
 #endif /* CONFIG_KVM */
 
 #ifdef CONFIG_KVM_GUEST
-	DEFINE(KVM_MAGIC_SCRATCH1, offsetof(struct kvm_vcpu_arch_shared,
-					    scratch1));
-	DEFINE(KVM_MAGIC_SCRATCH2, offsetof(struct kvm_vcpu_arch_shared,
-					    scratch2));
-	DEFINE(KVM_MAGIC_SCRATCH3, offsetof(struct kvm_vcpu_arch_shared,
-					    scratch3));
-	DEFINE(KVM_MAGIC_INT, offsetof(struct kvm_vcpu_arch_shared,
-				       int_pending));
-	DEFINE(KVM_MAGIC_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
-	DEFINE(KVM_MAGIC_CRITICAL, offsetof(struct kvm_vcpu_arch_shared,
-					    critical));
-	DEFINE(KVM_MAGIC_SR, offsetof(struct kvm_vcpu_arch_shared, sr));
+	OFFSET(KVM_MAGIC_SCRATCH1, kvm_vcpu_arch_shared, scratch1);
+	OFFSET(KVM_MAGIC_SCRATCH2, kvm_vcpu_arch_shared, scratch2);
+	OFFSET(KVM_MAGIC_SCRATCH3, kvm_vcpu_arch_shared, scratch3);
+	OFFSET(KVM_MAGIC_INT, kvm_vcpu_arch_shared, int_pending);
+	OFFSET(KVM_MAGIC_MSR, kvm_vcpu_arch_shared, msr);
+	OFFSET(KVM_MAGIC_CRITICAL, kvm_vcpu_arch_shared, critical);
+	OFFSET(KVM_MAGIC_SR, kvm_vcpu_arch_shared, sr);
 #endif
 
 #ifdef CONFIG_44x
@@ -719,45 +696,37 @@ int main(void)
 #endif
 #ifdef CONFIG_PPC_FSL_BOOK3E
 	DEFINE(TLBCAM_SIZE, sizeof(struct tlbcam));
-	DEFINE(TLBCAM_MAS0, offsetof(struct tlbcam, MAS0));
-	DEFINE(TLBCAM_MAS1, offsetof(struct tlbcam, MAS1));
-	DEFINE(TLBCAM_MAS2, offsetof(struct tlbcam, MAS2));
-	DEFINE(TLBCAM_MAS3, offsetof(struct tlbcam, MAS3));
-	DEFINE(TLBCAM_MAS7, offsetof(struct tlbcam, MAS7));
+	OFFSET(TLBCAM_MAS0, tlbcam, MAS0);
+	OFFSET(TLBCAM_MAS1, tlbcam, MAS1);
+	OFFSET(TLBCAM_MAS2, tlbcam, MAS2);
+	OFFSET(TLBCAM_MAS3, tlbcam, MAS3);
+	OFFSET(TLBCAM_MAS7, tlbcam, MAS7);
 #endif
 
 #if defined(CONFIG_KVM) && defined(CONFIG_SPE)
-	DEFINE(VCPU_EVR, offsetof(struct kvm_vcpu, arch.evr[0]));
-	DEFINE(VCPU_ACC, offsetof(struct kvm_vcpu, arch.acc));
-	DEFINE(VCPU_SPEFSCR, offsetof(struct kvm_vcpu, arch.spefscr));
-	DEFINE(VCPU_HOST_SPEFSCR, offsetof(struct kvm_vcpu, arch.host_spefscr));
+	OFFSET(VCPU_EVR, kvm_vcpu, arch.evr[0]);
+	OFFSET(VCPU_ACC, kvm_vcpu, arch.acc);
+	OFFSET(VCPU_SPEFSCR, kvm_vcpu, arch.spefscr);
+	OFFSET(VCPU_HOST_SPEFSCR, kvm_vcpu, arch.host_spefscr);
 #endif
 
 #ifdef CONFIG_KVM_BOOKE_HV
-	DEFINE(VCPU_HOST_MAS4, offsetof(struct kvm_vcpu, arch.host_mas4));
-	DEFINE(VCPU_HOST_MAS6, offsetof(struct kvm_vcpu, arch.host_mas6));
+	OFFSET(VCPU_HOST_MAS4, kvm_vcpu, arch.host_mas4);
+	OFFSET(VCPU_HOST_MAS6, kvm_vcpu, arch.host_mas6);
 #endif
 
 #ifdef CONFIG_KVM_EXIT_TIMING
-	DEFINE(VCPU_TIMING_EXIT_TBU, offsetof(struct kvm_vcpu,
-						arch.timing_exit.tv32.tbu));
-	DEFINE(VCPU_TIMING_EXIT_TBL, offsetof(struct kvm_vcpu,
-						arch.timing_exit.tv32.tbl));
-	DEFINE(VCPU_TIMING_LAST_ENTER_TBU, offsetof(struct kvm_vcpu,
-					arch.timing_last_enter.tv32.tbu));
-	DEFINE(VCPU_TIMING_LAST_ENTER_TBL, offsetof(struct kvm_vcpu,
-					arch.timing_last_enter.tv32.tbl));
+	OFFSET(VCPU_TIMING_EXIT_TBU, kvm_vcpu, arch.timing_exit.tv32.tbu);
+	OFFSET(VCPU_TIMING_EXIT_TBL, kvm_vcpu, arch.timing_exit.tv32.tbl);
+	OFFSET(VCPU_TIMING_LAST_ENTER_TBU, kvm_vcpu, arch.timing_last_enter.tv32.tbu);
+	OFFSET(VCPU_TIMING_LAST_ENTER_TBL, kvm_vcpu, arch.timing_last_enter.tv32.tbl);
 #endif
 
 #ifdef CONFIG_PPC_POWERNV
-	DEFINE(PACA_CORE_IDLE_STATE_PTR,
-			offsetof(struct paca_struct, core_idle_state_ptr));
-	DEFINE(PACA_THREAD_IDLE_STATE,
-			offsetof(struct paca_struct, thread_idle_state));
-	DEFINE(PACA_THREAD_MASK,
-			offsetof(struct paca_struct, thread_mask));
-	DEFINE(PACA_SUBCORE_SIBLING_MASK,
-			offsetof(struct paca_struct, subcore_sibling_mask));
+	OFFSET(PACA_CORE_IDLE_STATE_PTR, paca_struct, core_idle_state_ptr);
+	OFFSET(PACA_THREAD_IDLE_STATE, paca_struct, thread_idle_state);
+	OFFSET(PACA_THREAD_MASK, paca_struct, thread_mask);
+	OFFSET(PACA_SUBCORE_SIBLING_MASK, paca_struct, subcore_sibling_mask);
 #endif
 
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);

commit da0e7e6276968fcc61ac7484d0026cd5fdd94dc3
Merge: a05ef161cdd2 ab9bad0ead9a
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Feb 14 17:18:29 2017 +1100

    Merge branch 'topic/ppc-kvm' into next
    
    Merge the topic branch we're sharing with the kvm-ppc tree.

commit e2827fe5c1566f66a922dd7493cbe4522c50580a
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:47 2017 -0600

    powerpc/64: Clean up ppc64_caches using a struct per cache
    
    We have two set of identical struct members for the I and D sides
    and mostly identical bunches of code to parse the device-tree to
    populate them. Instead make a ppc_cache_info structure with one
    copy for I and one for D
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 125442107b40..b4324db94741 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -163,12 +163,12 @@ int main(void)
 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
 
 #ifdef CONFIG_PPC64
-	DEFINE(DCACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, dblock_size));
-	DEFINE(DCACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, log_dblock_size));
-	DEFINE(DCACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, dblocks_per_page));
-	DEFINE(ICACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, iblock_size));
-	DEFINE(ICACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, log_iblock_size));
-	DEFINE(ICACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, iblocks_per_page));
+	DEFINE(DCACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, l1d.block_size));
+	DEFINE(DCACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, l1d.log_block_size));
+	DEFINE(DCACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, l1d.blocks_per_page));
+	DEFINE(ICACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, l1i.block_size));
+	DEFINE(ICACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, l1i.log_block_size));
+	DEFINE(ICACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, l1i.blocks_per_page));
 	/* paca */
 	DEFINE(PACA_SIZE, sizeof(struct paca_struct));
 	DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));

commit bd067f83b0840e798328d14133ce4542d3bf9e71
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Jan 8 17:31:45 2017 -0600

    powerpc/64: Fix naming of cache block vs. cache line
    
    In a number of places we called "cache line size" what is actually
    the cache block size, which in the powerpc architecture, means the
    effective size to use with cache management instructions (it can
    be different from the actual cache line size).
    
    We fix the naming across the board and properly retrieve both
    pieces of information when available in the device-tree.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0601e6a7297c..125442107b40 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -163,12 +163,12 @@ int main(void)
 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
 
 #ifdef CONFIG_PPC64
-	DEFINE(DCACHEL1LINESIZE, offsetof(struct ppc64_caches, dline_size));
-	DEFINE(DCACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_dline_size));
-	DEFINE(DCACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, dlines_per_page));
-	DEFINE(ICACHEL1LINESIZE, offsetof(struct ppc64_caches, iline_size));
-	DEFINE(ICACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_iline_size));
-	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
+	DEFINE(DCACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, dblock_size));
+	DEFINE(DCACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, log_dblock_size));
+	DEFINE(DCACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, dblocks_per_page));
+	DEFINE(ICACHEL1BLOCKSIZE, offsetof(struct ppc64_caches, iblock_size));
+	DEFINE(ICACHEL1LOGBLOCKSIZE, offsetof(struct ppc64_caches, log_iblock_size));
+	DEFINE(ICACHEL1BLOCKSPERPAGE, offsetof(struct ppc64_caches, iblocks_per_page));
 	/* paca */
 	DEFINE(PACA_SIZE, sizeof(struct paca_struct));
 	DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));

commit f4c51f841d2ac7d36cacb84efbc383190861f87c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:45 2017 +1100

    KVM: PPC: Book3S HV: Modify guest entry/exit paths to handle radix guests
    
    This adds code to  branch around the parts that radix guests don't
    need - clearing and loading the SLB with the guest SLB contents,
    saving the guest SLB contents on exit, and restoring the host SLB
    contents.
    
    Since the host is now using radix, we need to save and restore the
    host value for the PID register.
    
    On hypervisor data/instruction storage interrupts, we don't do the
    guest HPT lookup on radix, but just save the guest physical address
    for the fault (from the ASDR register) in the vcpu struct.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0601e6a7297c..3afa0ad9837f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -498,6 +498,7 @@ int main(void)
 	DEFINE(KVM_NEED_FLUSH, offsetof(struct kvm, arch.need_tlb_flush.bits));
 	DEFINE(KVM_ENABLED_HCALLS, offsetof(struct kvm, arch.enabled_hcalls));
 	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));
+	DEFINE(KVM_RADIX, offsetof(struct kvm, arch.radix));
 	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
@@ -537,6 +538,7 @@ int main(void)
 	DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));
 	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
 	DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
+	DEFINE(VCPU_FAULT_GPA, offsetof(struct kvm_vcpu, arch.fault_gpa));
 	DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));

commit f2574030b0e33263b8a1c28fa3c4fa9292283799
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jan 24 21:37:20 2017 +1100

    powerpc: Revert the initial stack protector support
    
    Unfortunately the stack protector support we merged recently only works
    on some toolchains. If the toolchain is built without glibc support
    everything works fine, but if glibc is built then it leads to a panic
    at boot.
    
    The solution is not rc5 material, so revert the support for now. This
    reverts commits:
    
    6533b7c16ee5 ("powerpc: Initial stack protector (-fstack-protector) support")
    902e06eb86cd ("powerpc/32: Change the stack protector canary value per task")
    
    Fixes: 6533b7c16ee5 ("powerpc: Initial stack protector (-fstack-protector) support")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0601e6a7297c..195a9fc8f81c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -91,9 +91,6 @@ int main(void)
 	DEFINE(TI_livepatch_sp, offsetof(struct thread_info, livepatch_sp));
 #endif
 
-#ifdef CONFIG_CC_STACKPROTECTOR
-	DEFINE(TSK_STACK_CANARY, offsetof(struct task_struct, stack_canary));
-#endif
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
 	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
 #ifdef CONFIG_BOOKE

commit 8c8b73c4811f2b5e458a7418dca07d2ef85c7db1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 5 18:11:45 2017 +0100

    sched/cputime, powerpc: Prepare accounting structure for cputime flush on tick
    
    In order to prepare for CONFIG_VIRT_CPU_ACCOUNTING_NATIVE=y to delay
    cputime accounting to the tick, provide finegrained accumulators to
    powerpc in order to store the cputime until flushing.
    
    While at it, normalize the name of several fields according to common
    cputime naming.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1483636310-6557-6-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0601e6a7297c..e505319574ca 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -252,9 +252,9 @@ int main(void)
 	DEFINE(ACCOUNT_STARTTIME_USER,
 	       offsetof(struct paca_struct, accounting.starttime_user));
 	DEFINE(ACCOUNT_USER_TIME,
-	       offsetof(struct paca_struct, accounting.user_time));
+	       offsetof(struct paca_struct, accounting.utime));
 	DEFINE(ACCOUNT_SYSTEM_TIME,
-	       offsetof(struct paca_struct, accounting.system_time));
+	       offsetof(struct paca_struct, accounting.stime));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 	DEFINE(PACA_NAPSTATELOST, offsetof(struct paca_struct, nap_state_lost));
 	DEFINE(PACA_SPRG_VDSO, offsetof(struct paca_struct, sprg_vdso));
@@ -265,9 +265,9 @@ int main(void)
 	DEFINE(ACCOUNT_STARTTIME_USER,
 	       offsetof(struct thread_info, accounting.starttime_user));
 	DEFINE(ACCOUNT_USER_TIME,
-	       offsetof(struct thread_info, accounting.user_time));
+	       offsetof(struct thread_info, accounting.utime));
 	DEFINE(ACCOUNT_SYSTEM_TIME,
-	       offsetof(struct thread_info, accounting.system_time));
+	       offsetof(struct thread_info, accounting.stime));
 #endif
 #endif /* CONFIG_PPC64 */
 

commit de399813b521ea7e38bbfb5e5b620b5e202e5783
Merge: 57ca04ab4401 c6f6634721c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 09:26:42 2016 -0800

    Merge tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for the kexec_file_load() syscall, which is a prereq for
         secure and trusted boot.
    
       - Prevent kernel execution of userspace on P9 Radix (similar to
         SMEP/PXN).
    
       - Sort the exception tables at build time, to save time at boot, and
         store them as relative offsets to save space in the kernel image &
         memory.
    
       - Allow building the kernel with thin archives, which should allow us
         to build an allyesconfig once some other fixes land.
    
       - Build fixes to allow us to correctly rebuild when changing the
         kernel endian from big to little or vice versa.
    
       - Plumbing so that we can avoid doing a full mm TLB flush on P9
         Radix.
    
       - Initial stack protector support (-fstack-protector).
    
       - Support for dumping the radix (aka. Linux) and hash page tables via
         debugfs.
    
       - Fix an oops in cxl coredump generation when cxl_get_fd() is used.
    
       - Freescale updates from Scott: "Highlights include 8xx hugepage
         support, qbman fixes/cleanup, device tree updates, and some misc
         cleanup."
    
       - Many and varied fixes and minor enhancements as always.
    
      Thanks to:
        Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anshuman
        Khandual, Anton Blanchard, Balbir Singh, Bartlomiej Zolnierkiewicz,
        Christophe Jaillet, Christophe Leroy, Denis Kirjanov, Elimar
        Riesebieter, Frederic Barrat, Gautham R. Shenoy, Geliang Tang, Geoff
        Levand, Jack Miller, Johan Hovold, Lars-Peter Clausen, Libin,
        Madhavan Srinivasan, Michael Neuling, Nathan Fontenot, Naveen N.
        Rao, Nicholas Piggin, Pan Xinhui, Peter Senna Tschudin, Rashmica
        Gupta, Rui Teng, Russell Currey, Scott Wood, Simon Guo, Suraj
        Jitindar Singh, Thiago Jung Bauermann, Tobias Klauser, Vaibhav Jain"
    
    [ And thanks to Michael, who took time off from a new baby to get this
      pull request done.   - Linus ]
    
    * tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (174 commits)
      powerpc/fsl/dts: add FMan node for t1042d4rdb
      powerpc/fsl/dts: add sg_2500_aqr105_phy4 alias on t1024rdb
      powerpc/fsl/dts: add QMan and BMan nodes on t1024
      powerpc/fsl/dts: add QMan and BMan nodes on t1023
      soc/fsl/qman: test: use DEFINE_SPINLOCK()
      powerpc/fsl-lbc: use DEFINE_SPINLOCK()
      powerpc/8xx: Implement support of hugepages
      powerpc: get hugetlbpage handling more generic
      powerpc: port 64 bits pgtable_cache to 32 bits
      powerpc/boot: Request no dynamic linker for boot wrapper
      soc/fsl/bman: Use resource_size instead of computation
      soc/fsl/qe: use builtin_platform_driver
      powerpc/fsl_pmc: use builtin_platform_driver
      powerpc/83xx/suspend: use builtin_platform_driver
      powerpc/ftrace: Fix the comments for ftrace_modify_code
      powerpc/perf: macros for power9 format encoding
      powerpc/perf: power9 raw event format encoding
      powerpc/perf: update attribute_group data structure
      powerpc/perf: factor out the event format field
      powerpc/mm/iommu, vfio/spapr: Put pages on VFIO container shutdown
      ...

commit 7c5b06cadf274f2867523c1130c11387545f808e
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Nov 18 08:28:51 2016 +1100

    KVM: PPC: Book3S HV: Adapt TLB invalidations to work on POWER9
    
    POWER9 adds new capabilities to the tlbie (TLB invalidate entry)
    and tlbiel (local tlbie) instructions.  Both instructions get a
    set of new parameters (RIC, PRS and R) which appear as bits in the
    instruction word.  The tlbiel instruction now has a second register
    operand, which contains a PID and/or LPID value if needed, and
    should otherwise contain 0.
    
    This adapts KVM-HV's usage of tlbie and tlbiel to work on POWER9
    as well as older processors.  Since we only handle HPT guests so
    far, we need RIC=0 PRS=0 R=0, which ends up with the same instruction
    word as on previous processors, so we don't need to conditionally
    execute different instructions depending on the processor.
    
    The local flush on first entry to a guest in book3s_hv_rmhandlers.S
    is a loop which depends on the number of TLB sets.  Rather than
    using feature sections to set the number of iterations based on
    which CPU we're on, we now work out this number at VM creation time
    and store it in the kvm_arch struct.  That will make it possible to
    get the number from the device tree in future, which will help with
    compatibility with future processors.
    
    Since mmu_partition_table_set_entry() does a global flush of the
    whole LPID, we don't need to do the TLB flush on first entry to the
    guest on each processor.  Therefore we don't set all bits in the
    tlb_need_flush bitmap on VM startup on POWER9.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index a4f6d5e32a81..195a9fc8f81c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -487,6 +487,7 @@ int main(void)
 
 	/* book3s */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	DEFINE(KVM_TLB_SETS, offsetof(struct kvm, arch.tlb_sets));
 	DEFINE(KVM_SDR1, offsetof(struct kvm, arch.sdr1));
 	DEFINE(KVM_HOST_LPID, offsetof(struct kvm, arch.host_lpid));
 	DEFINE(KVM_HOST_LPCR, offsetof(struct kvm, arch.host_lpcr));

commit e9cf1e085647b433ccd98582681b17121ecfdc21
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Nov 18 13:11:42 2016 +1100

    KVM: PPC: Book3S HV: Add new POWER9 guest-accessible SPRs
    
    This adds code to handle two new guest-accessible special-purpose
    registers on POWER9: TIDR (thread ID register) and PSSCR (processor
    stop status and control register).  They are context-switched
    between host and guest, and the guest values can be read and set
    via the one_reg interface.
    
    The PSSCR contains some fields which are guest-accessible and some
    which are only accessible in hypervisor mode.  We only allow the
    guest-accessible fields to be read or set by userspace.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c833d88c423d..a4f6d5e32a81 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -548,6 +548,8 @@ int main(void)
 	DEFINE(VCPU_TCSCR, offsetof(struct kvm_vcpu, arch.tcscr));
 	DEFINE(VCPU_ACOP, offsetof(struct kvm_vcpu, arch.acop));
 	DEFINE(VCPU_WORT, offsetof(struct kvm_vcpu, arch.wort));
+	DEFINE(VCPU_TID, offsetof(struct kvm_vcpu, arch.tid));
+	DEFINE(VCPU_PSSCR, offsetof(struct kvm_vcpu, arch.psscr));
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_map));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));

commit 902e06eb86cd62753974c249bd1dedae2825b430
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Nov 22 11:49:32 2016 +0100

    powerpc/32: Change the stack protector canary value per task
    
    Partially copied from commit df0698be14c66 ("ARM: stack protector:
    change the canary value per task")
    
    A new random value for the canary is stored in the task struct whenever
    a new task is forked.  This is meant to allow for different canary values
    per task.  On powerpc, GCC expects the canary value to be found in a global
    variable called __stack_chk_guard.  So this variable has to be updated
    with the value stored in the task struct whenever a task switch occurs.
    
    Because the variable GCC expects is global, this cannot work on SMP
    unfortunately.  So, on SMP, the same initial canary value is kept
    throughout, making this feature a bit less effective although it is still
    useful.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index caec7bf3b99a..5c860302efec 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -91,6 +91,9 @@ int main(void)
 	DEFINE(TI_livepatch_sp, offsetof(struct thread_info, livepatch_sp));
 #endif
 
+#ifdef CONFIG_CC_STACKPROTECTOR
+	DEFINE(TSK_STACK_CANARY, offsetof(struct task_struct, stack_canary));
+#endif
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
 	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
 #ifdef CONFIG_BOOKE

commit 0d808df06a44200f52262b6eb72bcb6042f5a7c5
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Nov 7 15:09:58 2016 +1100

    KVM: PPC: Book3S HV: Save/restore XER in checkpointed register state
    
    When switching from/to a guest that has a transaction in progress,
    we need to save/restore the checkpointed register state.  Although
    XER is part of the CPU state that gets checkpointed, the code that
    does this saving and restoring doesn't save/restore XER.
    
    This fixes it by saving and restoring the XER.  To allow userspace
    to read/write the checkpointed XER value, we also add a new ONE_REG
    specifier.
    
    The visible effect of this bug is that the guest may see its XER
    value being corrupted when it uses transactions.
    
    Fixes: e4e38121507a ("KVM: PPC: Book3S HV: Add transactional memory support")
    Fixes: 0a8eccefcb34 ("KVM: PPC: Book3S HV: Add missing code for transaction reclaim on guest exit")
    Cc: stable@vger.kernel.org # v3.15+
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index caec7bf3b99a..c833d88c423d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -569,6 +569,7 @@ int main(void)
 	DEFINE(VCPU_VRS_TM, offsetof(struct kvm_vcpu, arch.vr_tm.vr));
 	DEFINE(VCPU_VRSAVE_TM, offsetof(struct kvm_vcpu, arch.vrsave_tm));
 	DEFINE(VCPU_CR_TM, offsetof(struct kvm_vcpu, arch.cr_tm));
+	DEFINE(VCPU_XER_TM, offsetof(struct kvm_vcpu, arch.xer_tm));
 	DEFINE(VCPU_LR_TM, offsetof(struct kvm_vcpu, arch.lr_tm));
 	DEFINE(VCPU_CTR_TM, offsetof(struct kvm_vcpu, arch.ctr_tm));
 	DEFINE(VCPU_AMR_TM, offsetof(struct kvm_vcpu, arch.amr_tm));

commit 07021b43597f506cc525d139ed1a94e79cf184f2
Merge: d1f5323370fc b7b7013cac55
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 7 20:19:31 2016 -0700

    Merge tag 'powerpc-4.9-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
       - Major rework of Book3S 64-bit exception vectors (Nicholas Piggin)
       - Use gas sections for arranging exception vectors et. al.
       - Large set of TM cleanups and selftests (Cyril Bur)
       - Enable transactional memory (TM) lazily for userspace (Cyril Bur)
       - Support for XZ compression in the zImage wrapper (Oliver
         O'Halloran)
       - Add support for bpf constant blinding (Naveen N. Rao)
       - Beginnings of upstream support for PA Semi Nemo motherboards
         (Darren Stevens)
    
      Fixes:
       - Ensure .mem(init|exit).text are within _stext/_etext (Michael
         Ellerman)
       - xmon: Don't use ld on 32-bit (Michael Ellerman)
       - vdso64: Use double word compare on pointers (Anton Blanchard)
       - powerpc/nvram: Fix an incorrect partition merge (Pan Xinhui)
       - powerpc: Fix usage of _PAGE_RO in hugepage (Christophe Leroy)
       - powerpc/mm: Update FORCE_MAX_ZONEORDER range to allow hugetlb w/4K
         (Aneesh Kumar K.V)
       - Fix memory leak in queue_hotplug_event() error path (Andrew
         Donnellan)
       - Replay hypervisor maintenance interrupt first (Nicholas Piggin)
    
      Various performance optimisations (Anton Blanchard):
       - Align hot loops of memset() and backwards_memcpy()
       - During context switch, check before setting mm_cpumask
       - Remove static branch prediction in atomic{, 64}_add_unless
       - Only disable HAVE_EFFICIENT_UNALIGNED_ACCESS on POWER7 little
         endian
       - Set default CPU type to POWER8 for little endian builds
    
      Cleanups & features:
       - Sparse fixes/cleanups (Daniel Axtens)
       - Preserve CFAR value on SLB miss caused by access to bogus address
         (Paul Mackerras)
       - Radix MMU fixups for POWER9 (Aneesh Kumar K.V)
       - Support for setting used_(vsr|vr|spe) in sigreturn path (for CRIU)
         (Simon Guo)
       - Optimise syscall entry for virtual, relocatable case (Nicholas
         Piggin)
       - Optimise MSR handling in exception handling (Nicholas Piggin)
       - Support for kexec with Radix MMU (Benjamin Herrenschmidt)
       - powernv EEH fixes (Russell Currey)
       - Suprise PCI hotplug support for powernv (Gavin Shan)
       - Endian/sparse fixes for powernv PCI (Gavin Shan)
       - Defconfig updates (Anton Blanchard)
       - KVM: PPC: Book3S HV: Migrate pinned pages out of CMA (Balbir Singh)
       - cxl: Flush PSL cache before resetting the adapter (Frederic Barrat)
       - cxl: replace loop with for_each_child_of_node(), remove unneeded
         of_node_put() (Andrew Donnellan)
       - Fix HV facility unavailable to use correct handler (Nicholas
         Piggin)
       - Remove unnecessary syscall trampoline (Nicholas Piggin)
       - fadump: Fix build break when CONFIG_PROC_VMCORE=n (Michael
         Ellerman)
       - Quieten EEH message when no adapters are found (Anton Blanchard)
       - powernv: Add PHB register dump debugfs handle (Russell Currey)
       - Use kprobe blacklist for exception handlers & asm functions
         (Nicholas Piggin)
       - Document the syscall ABI (Nicholas Piggin)
       - MAINTAINERS: Update cxl maintainers (Michael Neuling)
       - powerpc: Remove all usages of NO_IRQ (Michael Ellerman)
    
      Minor cleanups:
       - Andrew Donnellan, Christophe Leroy, Colin Ian King, Cyril Bur,
         Frederic Barrat, Pan Xinhui, PrasannaKumar Muralidharan, Rui Teng,
         Simon Guo"
    
    * tag 'powerpc-4.9-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (156 commits)
      powerpc/bpf: Add support for bpf constant blinding
      powerpc/bpf: Implement support for tail calls
      powerpc/bpf: Introduce accessors for using the tmp local stack space
      powerpc/fadump: Fix build break when CONFIG_PROC_VMCORE=n
      powerpc: tm: Enable transactional memory (TM) lazily for userspace
      powerpc/tm: Add TM Unavailable Exception
      powerpc: Remove do_load_up_transact_{fpu,altivec}
      powerpc: tm: Rename transct_(*) to ck(\1)_state
      powerpc: tm: Always use fp_state and vr_state to store live registers
      selftests/powerpc: Add checks for transactional VSXs in signal contexts
      selftests/powerpc: Add checks for transactional VMXs in signal contexts
      selftests/powerpc: Add checks for transactional FPUs in signal contexts
      selftests/powerpc: Add checks for transactional GPRs in signal contexts
      selftests/powerpc: Check that signals always get delivered
      selftests/powerpc: Add TM tcheck helpers in C
      selftests/powerpc: Allow tests to extend their kill timeout
      selftests/powerpc: Introduce GPR asm helper header file
      selftests/powerpc: Move VMX stack frame macros to header file
      selftests/powerpc: Rework FPU stack placement macros and move to header file
      selftests/powerpc: Check for VSX preservation across userspace preemption
      ...

commit 000ec280e3dd5c77a5227db27bfda1511e26db9a
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Fri Sep 23 16:18:25 2016 +1000

    powerpc: tm: Rename transct_(*) to ck(\1)_state
    
    Make the structures being used for checkpointed state named
    consistently with the pt_regs/ckpt_regs.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b89d14c0352c..dd0fc339ba40 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -142,12 +142,12 @@ int main(void)
 	DEFINE(THREAD_TM_PPR, offsetof(struct thread_struct, tm_ppr));
 	DEFINE(THREAD_TM_DSCR, offsetof(struct thread_struct, tm_dscr));
 	DEFINE(PT_CKPT_REGS, offsetof(struct thread_struct, ckpt_regs));
-	DEFINE(THREAD_TRANSACT_VRSTATE, offsetof(struct thread_struct,
-						 transact_vr));
-	DEFINE(THREAD_TRANSACT_VRSAVE, offsetof(struct thread_struct,
-					    transact_vrsave));
-	DEFINE(THREAD_TRANSACT_FPSTATE, offsetof(struct thread_struct,
-						 transact_fp));
+	DEFINE(THREAD_CKVRSTATE, offsetof(struct thread_struct,
+						 ckvr_state));
+	DEFINE(THREAD_CKVRSAVE, offsetof(struct thread_struct,
+					    ckvrsave));
+	DEFINE(THREAD_CKFPSTATE, offsetof(struct thread_struct,
+						 ckfp_state));
 	/* Local pt_regs on stack for Transactional Memory funcs. */
 	DEFINE(TM_FRAME_SIZE, STACK_FRAME_OVERHEAD +
 	       sizeof(struct pt_regs) + 16);

commit 88b02cf97bb7e742db3e31671d54177e3e19fd89
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Sep 15 13:42:52 2016 +1000

    KVM: PPC: Book3S: Treat VTB as a per-subcore register, not per-thread
    
    POWER8 has one virtual timebase (VTB) register per subcore, not one
    per CPU thread.  The HV KVM code currently treats VTB as a per-thread
    register, which can lead to spurious soft lockup messages from guests
    which use the VTB as the time source for the soft lockup detector.
    (CPUs before POWER8 did not have the VTB register.)
    
    For HV KVM, this fixes the problem by making only the primary thread
    in each virtual core save and restore the VTB value.  With this,
    the VTB state becomes part of the kvmppc_vcore structure.  This
    also means that "piggybacking" of multiple virtual cores onto one
    subcore is not possible on POWER8, because then the virtual cores
    would share a single VTB register.
    
    PR KVM emulates a VTB register, which is per-vcpu because PR KVM
    has no notion of CPU threads or SMT.  For PR KVM we move the VTB
    state into the kvmppc_vcpu_book3s struct.
    
    Cc: stable@vger.kernel.org # v3.14+
    Reported-by: Thomas Huth <thuth@redhat.com>
    Tested-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b89d14c0352c..a51ae9b165e0 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -506,7 +506,6 @@ int main(void)
 	DEFINE(VCPU_PURR, offsetof(struct kvm_vcpu, arch.purr));
 	DEFINE(VCPU_SPURR, offsetof(struct kvm_vcpu, arch.spurr));
 	DEFINE(VCPU_IC, offsetof(struct kvm_vcpu, arch.ic));
-	DEFINE(VCPU_VTB, offsetof(struct kvm_vcpu, arch.vtb));
 	DEFINE(VCPU_DSCR, offsetof(struct kvm_vcpu, arch.dscr));
 	DEFINE(VCPU_AMR, offsetof(struct kvm_vcpu, arch.amr));
 	DEFINE(VCPU_UAMOR, offsetof(struct kvm_vcpu, arch.uamor));
@@ -557,6 +556,7 @@ int main(void)
 	DEFINE(VCORE_LPCR, offsetof(struct kvmppc_vcore, lpcr));
 	DEFINE(VCORE_PCR, offsetof(struct kvmppc_vcore, pcr));
 	DEFINE(VCORE_DPDES, offsetof(struct kvmppc_vcore, dpdes));
+	DEFINE(VCORE_VTB, offsetof(struct kvmppc_vcore, vtb));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
 	DEFINE(VCPU_SLB_V, offsetof(struct kvmppc_slb, origv));
 	DEFINE(VCPU_SLB_SIZE, sizeof(struct kvmppc_slb));

commit 9f595fd8b54809fed13fc30906ef1e90a3fcfbc9
Author: Scott Wood <oss@buserror.net>
Date:   Sat Jul 9 03:22:39 2016 -0500

    powerpc/8xx: Force VIRT_IMMR_BASE to be a positive number
    
    The asm-offsets mechanism generates signed numbers, even if the
    input value is explicitly unsigned.  This causes a problem with
    older binutils (e.g. 2.23), which sign-extend a negative number
    when @h is applied.  Thus, this instruction:
    
            cmpli   cr0, r11, VIRT_IMMR_BASE@h
    
    resulted in this:
    
    Error: operand out of range (0xfffffff0 is not between 0x00000000 and
    0x0000ffff)
    
    By casting to a larger type, we can force the output to be expressed
    as a positive number.
    
    Signed-off-by: Scott Wood <oss@buserror.net>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 247f6407c7d8..b89d14c0352c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -754,7 +754,7 @@ int main(void)
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);
 
 #ifdef CONFIG_PPC_8xx
-	DEFINE(VIRT_IMMR_BASE, __fix_to_virt(FIX_IMMR_BASE));
+	DEFINE(VIRT_IMMR_BASE, (u64)__fix_to_virt(FIX_IMMR_BASE));
 #endif
 
 	return 0;

commit f86ef74ed9193c52411277eeac2eec69af553392
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 17 09:02:43 2016 +0200

    powerpc/8xx: Fix vaddr for IMMR early remap
    
    Memory: 124428K/131072K available (3748K kernel code, 188K rwdata,
    648K rodata, 508K init, 290K bss, 6644K reserved)
    Kernel virtual memory layout:
      * 0xfffdf000..0xfffff000  : fixmap
      * 0xfde00000..0xfe000000  : consistent mem
      * 0xfddf6000..0xfde00000  : early ioremap
      * 0xc9000000..0xfddf6000  : vmalloc & ioremap
    SLUB: HWalign=16, Order=0-3, MinObjects=0, CPUs=1, Nodes=1
    
    Today, IMMR is mapped 1:1 at startup
    
    Mapping IMMR 1:1 is just wrong because it may overlap with another
    area. On most mpc8xx boards it is OK as IMMR is set to 0xff000000
    but for instance on EP88xC board, IMMR is at 0xfa200000 which
    overlaps with VM ioremap area
    
    This patch fixes the virtual address for remapping IMMR with the fixmap
    regardless of the value of IMMR.
    
    The size of IMMR area is 256kbytes (CPM at offset 0, security engine
    at offset 128k) so a 512k page is enough
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 047892869257..247f6407c7d8 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -68,6 +68,10 @@
 #include "../mm/mmu_decl.h"
 #endif
 
+#ifdef CONFIG_PPC_8xx
+#include <asm/fixmap.h>
+#endif
+
 int main(void)
 {
 	DEFINE(THREAD, offsetof(struct task_struct, thread));
@@ -749,5 +753,9 @@ int main(void)
 
 	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);
 
+#ifdef CONFIG_PPC_8xx
+	DEFINE(VIRT_IMMR_BASE, __fix_to_virt(FIX_IMMR_BASE));
+#endif
+
 	return 0;
 }

commit c223c90386bc2306510e0ceacd768a0123ff2a2f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 17 08:33:46 2016 +0200

    powerpc32: provide VIRT_CPU_ACCOUNTING
    
    This patch provides VIRT_CPU_ACCOUTING to PPC32 architecture.
    PPC32 doesn't have the PACA structure, so we use the task_info
    structure to store the accounting data.
    
    In order to reuse on PPC32 the PPC64 functions, all u64 data has
    been replaced by 'unsigned long' so that it is u32 on PPC32 and
    u64 on PPC64
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 5b99f956e32f..047892869257 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -240,13 +240,28 @@ int main(void)
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
 	DEFINE(PACAKEXECSTATE, offsetof(struct paca_struct, kexec_state));
 	DEFINE(PACA_DSCR_DEFAULT, offsetof(struct paca_struct, dscr_default));
-	DEFINE(PACA_STARTTIME, offsetof(struct paca_struct, starttime));
-	DEFINE(PACA_STARTTIME_USER, offsetof(struct paca_struct, starttime_user));
-	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
-	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
+	DEFINE(ACCOUNT_STARTTIME,
+	       offsetof(struct paca_struct, accounting.starttime));
+	DEFINE(ACCOUNT_STARTTIME_USER,
+	       offsetof(struct paca_struct, accounting.starttime_user));
+	DEFINE(ACCOUNT_USER_TIME,
+	       offsetof(struct paca_struct, accounting.user_time));
+	DEFINE(ACCOUNT_SYSTEM_TIME,
+	       offsetof(struct paca_struct, accounting.system_time));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 	DEFINE(PACA_NAPSTATELOST, offsetof(struct paca_struct, nap_state_lost));
 	DEFINE(PACA_SPRG_VDSO, offsetof(struct paca_struct, sprg_vdso));
+#else /* CONFIG_PPC64 */
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
+	DEFINE(ACCOUNT_STARTTIME,
+	       offsetof(struct thread_info, accounting.starttime));
+	DEFINE(ACCOUNT_STARTTIME_USER,
+	       offsetof(struct thread_info, accounting.starttime_user));
+	DEFINE(ACCOUNT_USER_TIME,
+	       offsetof(struct thread_info, accounting.user_time));
+	DEFINE(ACCOUNT_SYSTEM_TIME,
+	       offsetof(struct thread_info, accounting.system_time));
+#endif
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */

commit aac6a91fea93e6bdd7ac20365d7ecc9187ca61da
Author: Rashmica Gupta <rashmicy@gmail.com>
Date:   Thu Jun 2 08:56:47 2016 +1000

    powerpc/asm: Remove unused symbols in asm-offsets.c
    
    THREAD_DSCR:
      Added in efcac6589a27 "powerpc: Per process DSCR + some fixes (try#4)"
      Last usage removed in 152d523e6307 "powerpc: Create context switch helpers save_sprs() and restore_sprs()"
    
    THREAD_DSCR_INHERIT:
      Added in 714332858bfd "powerpc: Restore correct DSCR in context switch"
      Last usage removed in 152d523e6307 "powerpc: Create context switch helpers save_sprs() and restore_sprs()"
    
    THREAD_TAR:
      Added in 2468dcf641e4 "powerpc: Add support for context switching the TAR register"
      Last usage removed in 152d523e6307 "powerpc: Create context switch helpers save_sprs() and restore_sprs()"
    
    THREAD_BESCR, THREAD_EBBHR and THREAD_EBBRR:
      Added in 9353374b8e15 "powerpc: Context switch the new EBB SPRs"
      Last usage removed in 152d523e6307 "powerpc: Create context switch helpers save_sprs() and restore_sprs()"
    
    THREAD_SIAR, THREAD_SDAR, THREAD_SIER, THREAD_MMCR0, and THREAD_MMCR2:
      Added in 59affcd3e460 "powerpc: Context switch more PMU related SPRs"
      Last usage removed in b11ae95100f7 "powerpc: Partial revert of "Context switch more PMU related SPRs""
    
    PACA_LOCK_TOKEN:
      Added in 9e368f291560 "KVM: PPC: book3s_hv: Add support for PPC970-family processors"
      Last usage removed in c17b98cf6028 "KVM: PPC: Book3S HV: Remove code for PPC970 processors"
    
    HCALL_STAT_SIZE, HCALL_STAT_CALLS, HCALL_STAT_TB and HCALL_STAT_PURR:
      Added in 57852a853b0d "[POWERPC] powerpc: Instrument Hypervisor Calls"
      Last usage removed in c8cd093a6e9f "powerpc: tracing: Add hypervisor call tracepoints"
    
    VCPU_EPLC:
      Added in d30f6e480055 "KVM: PPC: booke: category E.HV (GS-mode) support"
      Never used.
    
    CPU_DOWN_FLUSH:
      Added in e7affb1dba0e "powerpc/cache: add cache flush operation for various e500"
      Never used.
    
    CFG_STAMP_XSEC:
      Added in 14cf11af6cf6 "powerpc: Merge enough to start building in arch/powerpc."
      Last usage removed in 0e469db8f70c "powerpc: Rework VDSO gettimeofday to prevent time going backwards"
    
    KVM_LPCR:
      Added in aa04b4cc5be6 "KVM: PPC: Allocate RMAs (Real Mode Areas) at boot for use by guests"
      Last usage removed in a0144e2a6b0b "KVM: PPC: Book3S HV: Store LPCR value for each virtual core"
    
    GPR15, GPR16, GPR17, GPR18, GPR19, GPR20, GPR21, GPR22, GPR23, GPR24,
    GPR25, GPR26, GPR27, GPR28, GPR29, GPR30 and GPR31:
      Added in 14cf11af6cf6 "powerpc: Merge enough to start building in arch/powerpc."
      Never used.
    
    VCPU_SHADOW_FSCR:
      Added in 616dff860282 "KVM: PPC: Book3S PR: Handle Facility interrupt and FSCR"
      Never used.
    
    VCPU_SHADOW_SRR1:
      Added in a2d56020d1d9 "KVM: PPC: Book3S PR: Keep volatile reg values in vcpu rather than shadow_vcpu"
      Never used.
    
    KVM_SPLIT_SIZE:
      Added in b4deba5c41e9 "KVM: PPC: Book3S HV: Implement dynamicmicro-threading on POWER8"
      Never used.
    
    VCPU_VCPUID:
      Added in de56a948b918 "KVM: PPC: Add support for Book3S processors in hypervisor mode"
      Last usage removed 1b400ba0cd24 "KVM: PPC: Book3S HV: Improve handling of local vs. global TLB invalidations"
    
    _MQ:
      Added in 14cf11af6cf6 "powerpc: Merge enough to start building in arch/powerpc."
      Never used.
    
    AUDITCONTEXT:
      Added in 14cf11af6cf6 "powerpc: Merge enough to start building in arch/powerpc."
      Last usage removed in 401d1f029beb "[PATCH] syscall entry/exit revamp"
    
    CLONE_VM:
      Added in 14cf11af6cf6 "powerpc: Merge enough to start building in arch/powerpc."
      Currently unused.
    
    CLONE_UNTRACED:
      Added in 14cf11af6cf6 "powerpc: Merge enough to start building in arch/powerpc."
      Currently unused.
    
    Signed-off-by: Rashmica Gupta <rashmicy@gmail.com>
    [mpe: Munge change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9ea09551a2cd..5b99f956e32f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -74,11 +74,8 @@ int main(void)
 	DEFINE(MM, offsetof(struct task_struct, mm));
 	DEFINE(MMCONTEXTID, offsetof(struct mm_struct, context.id));
 #ifdef CONFIG_PPC64
-	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
 	DEFINE(SIGSEGV, SIGSEGV);
 	DEFINE(NMI_MASK, NMI_MASK);
-	DEFINE(THREAD_DSCR, offsetof(struct thread_struct, dscr));
-	DEFINE(THREAD_DSCR_INHERIT, offsetof(struct thread_struct, dscr_inherit));
 	DEFINE(TASKTHREADPPR, offsetof(struct task_struct, thread.ppr));
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
@@ -132,17 +129,6 @@ int main(void)
 	DEFINE(THREAD_KVM_VCPU, offsetof(struct thread_struct, kvm_vcpu));
 #endif
 
-#ifdef CONFIG_PPC_BOOK3S_64
-	DEFINE(THREAD_TAR, offsetof(struct thread_struct, tar));
-	DEFINE(THREAD_BESCR, offsetof(struct thread_struct, bescr));
-	DEFINE(THREAD_EBBHR, offsetof(struct thread_struct, ebbhr));
-	DEFINE(THREAD_EBBRR, offsetof(struct thread_struct, ebbrr));
-	DEFINE(THREAD_SIAR, offsetof(struct thread_struct, siar));
-	DEFINE(THREAD_SDAR, offsetof(struct thread_struct, sdar));
-	DEFINE(THREAD_SIER, offsetof(struct thread_struct, sier));
-	DEFINE(THREAD_MMCR0, offsetof(struct thread_struct, mmcr0));
-	DEFINE(THREAD_MMCR2, offsetof(struct thread_struct, mmcr2));
-#endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	DEFINE(PACATMSCRATCH, offsetof(struct paca_struct, tm_scratch));
 	DEFINE(THREAD_TM_TFHAR, offsetof(struct thread_struct, tm_tfhar));
@@ -178,7 +164,6 @@ int main(void)
 	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
 	/* paca */
 	DEFINE(PACA_SIZE, sizeof(struct paca_struct));
-	DEFINE(PACA_LOCK_TOKEN, offsetof(struct paca_struct, lock_token));
 	DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));
 	DEFINE(PACAPROCSTART, offsetof(struct paca_struct, cpu_start));
 	DEFINE(PACAKSAVE, offsetof(struct paca_struct, kstack));
@@ -275,12 +260,6 @@ int main(void)
 	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
 	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
 	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
-
-	/* hcall statistics */
-	DEFINE(HCALL_STAT_SIZE, sizeof(struct hcall_stats));
-	DEFINE(HCALL_STAT_CALLS, offsetof(struct hcall_stats, num_calls));
-	DEFINE(HCALL_STAT_TB, offsetof(struct hcall_stats, tb_total));
-	DEFINE(HCALL_STAT_PURR, offsetof(struct hcall_stats, purr_total));
 #endif /* CONFIG_PPC64 */
 	DEFINE(GPR0, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[0]));
 	DEFINE(GPR1, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[1]));
@@ -298,23 +277,6 @@ int main(void)
 	DEFINE(GPR13, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[13]));
 #ifndef CONFIG_PPC64
 	DEFINE(GPR14, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[14]));
-	DEFINE(GPR15, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[15]));
-	DEFINE(GPR16, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[16]));
-	DEFINE(GPR17, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[17]));
-	DEFINE(GPR18, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[18]));
-	DEFINE(GPR19, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[19]));
-	DEFINE(GPR20, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[20]));
-	DEFINE(GPR21, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[21]));
-	DEFINE(GPR22, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[22]));
-	DEFINE(GPR23, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[23]));
-	DEFINE(GPR24, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[24]));
-	DEFINE(GPR25, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[25]));
-	DEFINE(GPR26, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[26]));
-	DEFINE(GPR27, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[27]));
-	DEFINE(GPR28, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[28]));
-	DEFINE(GPR29, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[29]));
-	DEFINE(GPR30, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[30]));
-	DEFINE(GPR31, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[31]));
 #endif /* CONFIG_PPC64 */
 	/*
 	 * Note: these symbols include _ because they overlap with special
@@ -332,7 +294,6 @@ int main(void)
 	DEFINE(RESULT, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, result));
 	DEFINE(_TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
 #ifndef CONFIG_PPC64
-	DEFINE(_MQ, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, mq));
 	/*
 	 * The PowerPC 400-class & Book-E processors have neither the DAR
 	 * nor the DSISR SPRs. Hence, we overload them to hold the similar
@@ -369,8 +330,6 @@ int main(void)
 	DEFINE(SAVED_KSP_LIMIT, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, saved_ksp_limit));
 #endif
 #endif
-	DEFINE(CLONE_VM, CLONE_VM);
-	DEFINE(CLONE_UNTRACED, CLONE_UNTRACED);
 
 #ifndef CONFIG_PPC64
 	DEFINE(MM_PGD, offsetof(struct mm_struct, pgd));
@@ -380,7 +339,6 @@ int main(void)
 	DEFINE(CPU_SPEC_FEATURES, offsetof(struct cpu_spec, cpu_features));
 	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
 	DEFINE(CPU_SPEC_RESTORE, offsetof(struct cpu_spec, cpu_restore));
-	DEFINE(CPU_DOWN_FLUSH, offsetof(struct cpu_spec, cpu_down_flush));
 
 	DEFINE(pbe_address, offsetof(struct pbe, address));
 	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));
@@ -395,7 +353,6 @@ int main(void)
 	DEFINE(CFG_TB_ORIG_STAMP, offsetof(struct vdso_data, tb_orig_stamp));
 	DEFINE(CFG_TB_TICKS_PER_SEC, offsetof(struct vdso_data, tb_ticks_per_sec));
 	DEFINE(CFG_TB_TO_XS, offsetof(struct vdso_data, tb_to_xs));
-	DEFINE(CFG_STAMP_XSEC, offsetof(struct vdso_data, stamp_xsec));
 	DEFINE(CFG_TB_UPDATE_COUNT, offsetof(struct vdso_data, tb_update_count));
 	DEFINE(CFG_TZ_MINUTEWEST, offsetof(struct vdso_data, tz_minuteswest));
 	DEFINE(CFG_TZ_DSTTIME, offsetof(struct vdso_data, tz_dsttime));
@@ -517,7 +474,6 @@ int main(void)
 	DEFINE(KVM_HOST_SDR1, offsetof(struct kvm, arch.host_sdr1));
 	DEFINE(KVM_NEED_FLUSH, offsetof(struct kvm, arch.need_tlb_flush.bits));
 	DEFINE(KVM_ENABLED_HCALLS, offsetof(struct kvm, arch.enabled_hcalls));
-	DEFINE(KVM_LPCR, offsetof(struct kvm, arch.lpcr));
 	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));
 	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
@@ -528,7 +484,6 @@ int main(void)
 	DEFINE(VCPU_THREAD_CPU, offsetof(struct kvm_vcpu, arch.thread_cpu));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
-	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
 	DEFINE(VCPU_PURR, offsetof(struct kvm_vcpu, arch.purr));
 	DEFINE(VCPU_SPURR, offsetof(struct kvm_vcpu, arch.spurr));
 	DEFINE(VCPU_IC, offsetof(struct kvm_vcpu, arch.ic));
@@ -566,7 +521,6 @@ int main(void)
 	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
 	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
 	DEFINE(VCPU_FSCR, offsetof(struct kvm_vcpu, arch.fscr));
-	DEFINE(VCPU_SHADOW_FSCR, offsetof(struct kvm_vcpu, arch.shadow_fscr));
 	DEFINE(VCPU_PSPB, offsetof(struct kvm_vcpu, arch.pspb));
 	DEFINE(VCPU_EBBHR, offsetof(struct kvm_vcpu, arch.ebbhr));
 	DEFINE(VCPU_EBBRR, offsetof(struct kvm_vcpu, arch.ebbrr));
@@ -576,7 +530,6 @@ int main(void)
 	DEFINE(VCPU_TCSCR, offsetof(struct kvm_vcpu, arch.tcscr));
 	DEFINE(VCPU_ACOP, offsetof(struct kvm_vcpu, arch.acop));
 	DEFINE(VCPU_WORT, offsetof(struct kvm_vcpu, arch.wort));
-	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_map));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
@@ -693,7 +646,6 @@ int main(void)
 	DEFINE(KVM_SPLIT_RPR, offsetof(struct kvm_split_mode, rpr));
 	DEFINE(KVM_SPLIT_PMMAR, offsetof(struct kvm_split_mode, pmmar));
 	DEFINE(KVM_SPLIT_LDBAR, offsetof(struct kvm_split_mode, ldbar));
-	DEFINE(KVM_SPLIT_SIZE, offsetof(struct kvm_split_mode, subcore_size));
 	DEFINE(KVM_SPLIT_DO_NAP, offsetof(struct kvm_split_mode, do_nap));
 	DEFINE(KVM_SPLIT_NAPPED, offsetof(struct kvm_split_mode, napped));
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
@@ -756,7 +708,6 @@ int main(void)
 #ifdef CONFIG_KVM_BOOKE_HV
 	DEFINE(VCPU_HOST_MAS4, offsetof(struct kvm_vcpu, arch.host_mas4));
 	DEFINE(VCPU_HOST_MAS6, offsetof(struct kvm_vcpu, arch.host_mas6));
-	DEFINE(VCPU_EPLC, offsetof(struct kvm_vcpu, arch.eplc));
 #endif
 
 #ifdef CONFIG_KVM_EXIT_TIMING

commit dd1842a2a448bb66d74aa02a550df6be8c25f20b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:49 2016 +1000

    powerpc/mm: Make page table size a variable
    
    Radix and hash MMU models support different page table sizes. Make
    the #defines a variable so that existing code can work with variable
    sizes.
    
    Slice related code is only used by hash, so use hash constants there. We
    will replicate some of the boundary conditions with resepct to TASK_SIZE
    using radix values too. Right now we do boundary condition check using
    hash constants.
    
    Swapper pgdir size is initialized in asm code. We select the max pgd
    size to keep it simple. For now we select hash pgdir. When adding radix
    we will switch that to radix pgdir which is 64K.
    
    BUILD_BUG_ON check which is removed is already done in hugepage_init()
    using MAYBE_BUILD_BUG_ON().
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c9370d4e36bd..9ea09551a2cd 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -438,7 +438,11 @@ int main(void)
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));
 #endif
 
+#ifdef MAX_PGD_TABLE_SIZE
+	DEFINE(PGD_TABLE_SIZE, MAX_PGD_TABLE_SIZE);
+#else
 	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);
+#endif
 	DEFINE(PTE_SIZE, sizeof(pte_t));
 
 #ifdef CONFIG_KVM

commit 8404410b296095c78ed63f163ac5d417ff0647dd
Merge: 1050e689a63b 85baa095497f
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Apr 18 20:45:32 2016 +1000

    Merge branch 'topic/livepatch' into next
    
    Merge the support for live patching on ppc64le using mprofile-kernel.
    This branch has also been merged into the livepatching tree for v4.7.

commit 85baa095497f3e590df9f6c8932121f123efca5c
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Mar 24 22:04:05 2016 +1100

    powerpc/livepatch: Add live patching support on ppc64le
    
    Add the kconfig logic & assembly support for handling live patched
    functions. This depends on DYNAMIC_FTRACE_WITH_REGS, which in turn
    depends on the new -mprofile-kernel ftrace ABI, which is only supported
    currently on ppc64le.
    
    Live patching is handled by a special ftrace handler. This means it runs
    from ftrace_caller(). The live patch handler modifies the NIP so as to
    redirect the return from ftrace_caller() to the new patched function.
    
    However there is one particularly tricky case we need to handle.
    
    If a function A calls another function B, and it is known at link time
    that they share the same TOC, then A will not save or restore its TOC,
    and will call the local entry point of B.
    
    When we live patch B, we replace it with a new function C, which may
    not have the same TOC as A. At live patch time it's too late to modify A
    to do the TOC save/restore, so the live patching code must interpose
    itself between A and C, and do the TOC save/restore that A omitted.
    
    An additionaly complication is that the livepatch code can not create a
    stack frame in order to save the TOC. That is because if C takes > 8
    arguments, or is varargs, A will have written the arguments for C in
    A's stack frame.
    
    To solve this, we introduce a "livepatch stack" which grows upward from
    the base of the regular stack, and is used to store the TOC & LR when
    calling a live patched function.
    
    When the patched function returns, we retrieve the real LR & TOC from
    the livepatch stack, restore them, and pop the livepatch "stack frame".
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Torsten Duwe <duwe@suse.de>
    Reviewed-by: Balbir Singh <bsingharora@gmail.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 07cebc3514f3..723efac2d917 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -86,6 +86,10 @@ int main(void)
 	DEFINE(KSP_LIMIT, offsetof(struct thread_struct, ksp_limit));
 #endif /* CONFIG_PPC64 */
 
+#ifdef CONFIG_LIVEPATCH
+	DEFINE(TI_livepatch_sp, offsetof(struct thread_info, livepatch_sp));
+#endif
+
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
 	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
 #ifdef CONFIG_BOOKE

commit e7affb1dba0e9068aeb3978e858f39753e0dc20a
Author: chenhui zhao <chenhui.zhao@freescale.com>
Date:   Fri Nov 20 17:13:58 2015 +0800

    powerpc/cache: add cache flush operation for various e500
    
    Various e500 core have different cache architecture, so they
    need different cache flush operations. Therefore, add a callback
    function cpu_flush_caches to the struct cpu_spec. The cache flush
    operation for the specific kind of e500 is selected at init time.
    The callback function will flush all caches inside the current cpu.
    
    Signed-off-by: Chenhui Zhao <chenhui.zhao@freescale.com>
    Signed-off-by: Tang Yuantian <Yuantian.Tang@feescale.com>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 10d5eab19458..0d0183d3180a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -376,6 +376,7 @@ int main(void)
 	DEFINE(CPU_SPEC_FEATURES, offsetof(struct cpu_spec, cpu_features));
 	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
 	DEFINE(CPU_SPEC_RESTORE, offsetof(struct cpu_spec, cpu_restore));
+	DEFINE(CPU_DOWN_FLUSH, offsetof(struct cpu_spec, cpu_down_flush));
 
 	DEFINE(pbe_address, offsetof(struct pbe, address));
 	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));

commit 70fe3d980f5f14d8125869125ba9a0ea95e09c6b
Author: Cyril Bur <cyrilbur@gmail.com>
Date:   Mon Feb 29 17:53:47 2016 +1100

    powerpc: Restore FPU/VEC/VSX if previously used
    
    Currently the FPU, VEC and VSX facilities are lazily loaded. This is not
    a problem unless a process is using these facilities.
    
    Modern versions of GCC are very good at automatically vectorising code,
    new and modernised workloads make use of floating point and vector
    facilities, even the kernel makes use of vectorised memcpy.
    
    All this combined greatly increases the cost of a syscall since the
    kernel uses the facilities sometimes even in syscall fast-path making it
    increasingly common for a thread to take an *_unavailable exception soon
    after a syscall, not to mention potentially taking all three.
    
    The obvious overcompensation to this problem is to simply always load
    all the facilities on every exit to userspace. Loading up all FPU, VEC
    and VSX registers every time can be expensive and if a workload does
    avoid using them, it should not be forced to incur this penalty.
    
    An 8bit counter is used to detect if the registers have been used in the
    past and the registers are always loaded until the value wraps to back
    to zero.
    
    Several versions of the assembly in entry_64.S were tested:
    
      1. Always calling C.
      2. Performing a common case check and then calling C.
      3. A complex check in asm.
    
    After some benchmarking it was determined that avoiding C in the common
    case is a performance benefit (option 2). The full check in asm (option
    3) greatly complicated that codepath for a negligible performance gain
    and the trade-off was deemed not worth it.
    
    Signed-off-by: Cyril Bur <cyrilbur@gmail.com>
    [mpe: Move load_vec in the struct to fill an existing hole, reword change log]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    
    fixup

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 07cebc3514f3..10d5eab19458 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -95,12 +95,14 @@ int main(void)
 	DEFINE(THREAD_FPSTATE, offsetof(struct thread_struct, fp_state));
 	DEFINE(THREAD_FPSAVEAREA, offsetof(struct thread_struct, fp_save_area));
 	DEFINE(FPSTATE_FPSCR, offsetof(struct thread_fp_state, fpscr));
+	DEFINE(THREAD_LOAD_FP, offsetof(struct thread_struct, load_fp));
 #ifdef CONFIG_ALTIVEC
 	DEFINE(THREAD_VRSTATE, offsetof(struct thread_struct, vr_state));
 	DEFINE(THREAD_VRSAVEAREA, offsetof(struct thread_struct, vr_save_area));
 	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
 	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
 	DEFINE(VRSTATE_VSCR, offsetof(struct thread_vr_state, vscr));
+	DEFINE(THREAD_LOAD_VEC, offsetof(struct thread_struct, load_vec));
 #endif /* CONFIG_ALTIVEC */
 #ifdef CONFIG_VSX
 	DEFINE(THREAD_USED_VSR, offsetof(struct thread_struct, used_vsr));

commit 2fc251a8dda56b71ec491bee4c6897e3e12c0739
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Dec 11 09:34:42 2015 +1100

    powerpc: Copy only required pieces of the mm_context_t to the paca
    
    Currently we copy the whole mm_context_t to the paca but only access a
    few bits of it.  This is wasteful of space paca and also takes quite
    some time in the hot path of context switching.
    
    This patch pulls in only the required bits from the mm_context_t to
    the paca and on context switch, copies only those.
    
    Benchmarking this (On top of Anton's recent MSR context switching
    changes [1]) using processes and yield shows an improvement of almost
    3% on POWER8:
    
      http://ozlabs.org/~anton/junkcode/context_switch2.c
      ./context_switch2 --test=yield --process 0 0
    
    1. https://lists.ozlabs.org/pipermail/linuxppc-dev/2015-October/135700.html
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    [mpe: Rename paca fields to be mm_ctx_foo rather than context_foo]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9db7be292bf3..07cebc3514f3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -186,12 +186,12 @@ int main(void)
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
 	DEFINE(PACAIRQHAPPENED, offsetof(struct paca_struct, irq_happened));
 #ifdef CONFIG_PPC_BOOK3S
-	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
+	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, mm_ctx_id));
 #ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,
-					    context.low_slices_psize));
+					    mm_ctx_low_slices_psize));
 	DEFINE(PACAHIGHSLICEPSIZE, offsetof(struct paca_struct,
-					    context.high_slices_psize));
+					    mm_ctx_high_slices_psize));
 	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
 #endif /* CONFIG_PPC_MM_SLICES */
 #endif
@@ -224,7 +224,7 @@ int main(void)
 #ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(MMUPSIZESLLP, offsetof(struct mmu_psize_def, sllp));
 #else
-	DEFINE(PACACONTEXTSLLP, offsetof(struct paca_struct, context.sllp));
+	DEFINE(PACACONTEXTSLLP, offsetof(struct paca_struct, mm_ctx_sllp));
 #endif /* CONFIG_PPC_MM_SLICES */
 	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
 	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));

commit c395465da68bfc3a238d5bc15f862e33e6e9ecec
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Oct 28 15:54:06 2015 +1100

    powerpc: Add function to copy mm_context_t to the paca
    
    This adds a function to copy the mm->context to the paca.  This is
    only a basic conversion for now but will be used more extensively in
    the next patch.
    
    This also adds #ifdef CONFIG_PPC_BOOK3S around this code since it's
    not used elsewhere.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 221d584d089f..9db7be292bf3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -185,6 +185,7 @@ int main(void)
 	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
 	DEFINE(PACAIRQHAPPENED, offsetof(struct paca_struct, irq_happened));
+#ifdef CONFIG_PPC_BOOK3S
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
 #ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,
@@ -193,6 +194,7 @@ int main(void)
 					    context.high_slices_psize));
 	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
 #endif /* CONFIG_PPC_MM_SLICES */
+#endif
 
 #ifdef CONFIG_PPC_BOOK3E
 	DEFINE(PACAPGD, offsetof(struct paca_struct, pgd));

commit 519f526d391b0ef775aeb04c4b6f632ea6b3ee50
Merge: 06ab838c2024 ba60c41ae392
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 10 16:42:49 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull more kvm updates from Paolo Bonzini:
     "ARM:
       - Full debug support for arm64
       - Active state switching for timer interrupts
       - Lazy FP/SIMD save/restore for arm64
       - Generic ARMv8 target
    
      PPC:
       - Book3S: A few bug fixes
       - Book3S: Allow micro-threading on POWER8
    
      x86:
       - Compiler warnings
    
      Generic:
       - Adaptive polling for guest halt"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (49 commits)
      kvm: irqchip: fix memory leak
      kvm: move new trace event outside #ifdef CONFIG_KVM_ASYNC_PF
      KVM: trace kvm_halt_poll_ns grow/shrink
      KVM: dynamic halt-polling
      KVM: make halt_poll_ns per-vCPU
      Silence compiler warning in arch/x86/kvm/emulate.c
      kvm: compile process_smi_save_seg_64() only for x86_64
      KVM: x86: avoid uninitialized variable warning
      KVM: PPC: Book3S: Fix typo in top comment about locking
      KVM: PPC: Book3S: Fix size of the PSPB register
      KVM: PPC: Book3S HV: Exit on H_DOORBELL if HOST_IPI is set
      KVM: PPC: Book3S HV: Fix race in starting secondary threads
      KVM: PPC: Book3S: correct width in XER handling
      KVM: PPC: Book3S HV: Fix preempted vcore stolen time calculation
      KVM: PPC: Book3S HV: Fix preempted vcore list locking
      KVM: PPC: Book3S HV: Implement H_CLEAR_REF and H_CLEAR_MOD
      KVM: PPC: Book3S HV: Fix bug in dirty page tracking
      KVM: PPC: Book3S HV: Fix race in reading change bit when removing HPTE
      KVM: PPC: Book3S HV: Implement dynamic micro-threading on POWER8
      KVM: PPC: Book3S HV: Make use of unused threads when running guests
      ...

commit b4deba5c41e9f6d3239606c9e060853d9decfee1
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jul 2 20:38:16 2015 +1000

    KVM: PPC: Book3S HV: Implement dynamic micro-threading on POWER8
    
    This builds on the ability to run more than one vcore on a physical
    core by using the micro-threading (split-core) modes of the POWER8
    chip.  Previously, only vcores from the same VM could be run together,
    and (on POWER8) only if they had just one thread per core.  With the
    ability to split the core on guest entry and unsplit it on guest exit,
    we can run up to 8 vcpu threads from up to 4 different VMs, and we can
    run multiple vcores with 2 or 4 vcpus per vcore.
    
    Dynamic micro-threading is only available if the static configuration
    of the cores is whole-core mode (unsplit), and only on POWER8.
    
    To manage this, we introduce a new kvm_split_mode struct which is
    shared across all of the subcores in the core, with a pointer in the
    paca on each thread.  In addition we extend the core_info struct to
    have information on each subcore.  When deciding whether to add a
    vcore to the set already on the core, we now have two possibilities:
    (a) piggyback the vcore onto an existing subcore, or (b) start a new
    subcore.
    
    Currently, when any vcpu needs to exit the guest and switch to host
    virtual mode, we interrupt all the threads in all subcores and switch
    the core back to whole-core mode.  It may be possible in future to
    allow some of the subcores to keep executing in the guest while
    subcore 0 switches to the host, but that is not implemented in this
    patch.
    
    This adds a module parameter called dynamic_mt_modes which controls
    which micro-threading (split-core) modes the code will consider, as a
    bitmap.  In other words, if it is 0, no micro-threading mode is
    considered; if it is 2, only 2-way micro-threading is considered; if
    it is 4, only 4-way, and if it is 6, both 2-way and 4-way
    micro-threading mode will be considered.  The default is 6.
    
    With this, we now have secondary threads which are the primary thread
    for their subcore and therefore need to do the MMU switch.  These
    threads will need to be started even if they have no vcpu to run, so
    we use the vcore pointer in the PACA rather than the vcpu pointer to
    trigger them.
    
    It is now possible for thread 0 to find that an exit has been
    requested before it gets to switch the subcore state to the guest.  In
    that case we haven't added the guest's timebase offset to the
    timebase, so we need to be careful not to subtract the offset in the
    guest exit path.  In fact we just skip the whole path that switches
    back to host context, since we haven't switched to the guest context.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index a78cdbf9b622..de62392f093c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -676,7 +676,14 @@ int main(void)
 	HSTATE_FIELD(HSTATE_DSCR, host_dscr);
 	HSTATE_FIELD(HSTATE_DABR, dabr);
 	HSTATE_FIELD(HSTATE_DECEXP, dec_expires);
+	HSTATE_FIELD(HSTATE_SPLIT_MODE, kvm_split_mode);
 	DEFINE(IPI_PRIORITY, IPI_PRIORITY);
+	DEFINE(KVM_SPLIT_RPR, offsetof(struct kvm_split_mode, rpr));
+	DEFINE(KVM_SPLIT_PMMAR, offsetof(struct kvm_split_mode, pmmar));
+	DEFINE(KVM_SPLIT_LDBAR, offsetof(struct kvm_split_mode, ldbar));
+	DEFINE(KVM_SPLIT_SIZE, offsetof(struct kvm_split_mode, subcore_size));
+	DEFINE(KVM_SPLIT_DO_NAP, offsetof(struct kvm_split_mode, do_nap));
+	DEFINE(KVM_SPLIT_NAPPED, offsetof(struct kvm_split_mode, napped));
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit ec257165082616841a354dd915801ed43e3553be
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 24 21:18:03 2015 +1000

    KVM: PPC: Book3S HV: Make use of unused threads when running guests
    
    When running a virtual core of a guest that is configured with fewer
    threads per core than the physical cores have, the extra physical
    threads are currently unused.  This makes it possible to use them to
    run one or more other virtual cores from the same guest when certain
    conditions are met.  This applies on POWER7, and on POWER8 to guests
    with one thread per virtual core.  (It doesn't apply to POWER8 guests
    with multiple threads per vcore because they require a 1-1 virtual to
    physical thread mapping in order to be able to use msgsndp and the
    TIR.)
    
    The idea is that we maintain a list of preempted vcores for each
    physical cpu (i.e. each core, since the host runs single-threaded).
    Then, when a vcore is about to run, it checks to see if there are
    any vcores on the list for its physical cpu that could be
    piggybacked onto this vcore's execution.  If so, those additional
    vcores are put into state VCORE_PIGGYBACK and their runnable VCPU
    threads are started as well as the original vcore, which is called
    the master vcore.
    
    After the vcores have exited the guest, the extra ones are put back
    onto the preempted list if any of their VCPUs are still runnable and
    not idle.
    
    This means that vcpu->arch.ptid is no longer necessarily the same as
    the physical thread that the vcpu runs on.  In order to make it easier
    for code that wants to send an IPI to know which CPU to target, we
    now store that in a new field in struct vcpu_arch, called thread_cpu.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Tested-by: Laurent Vivier <lvivier@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 98230579d99c..a78cdbf9b622 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -512,6 +512,8 @@ int main(void)
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 	DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
 	DEFINE(VCPU_HEIR, offsetof(struct kvm_vcpu, arch.emul_inst));
+	DEFINE(VCPU_CPU, offsetof(struct kvm_vcpu, cpu));
+	DEFINE(VCPU_THREAD_CPU, offsetof(struct kvm_vcpu, arch.thread_cpu));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));

commit e5e55cc08c76134e38526a66b3bd30b1b71bbf63
Author: Kevin Hao <haokexin@gmail.com>
Date:   Thu Aug 13 19:51:35 2015 +0800

    powerpc/e6500: remove the stale TCD_LOCK macro
    
    Since we moved the "lock" to be the first element of
    struct tlb_core_data in commit 82d86de25b9c ("powerpc/e6500: Make TLB
    lock recursive"), this macro is not used by any code. Just delete it.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 98230579d99c..810f433731dc 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -213,7 +213,6 @@ int main(void)
 		offsetof(struct tlb_core_data, esel_max));
 	DEFINE(TCD_ESEL_FIRST,
 		offsetof(struct tlb_core_data, esel_first));
-	DEFINE(TCD_LOCK, offsetof(struct tlb_core_data, lock));
 #endif /* CONFIG_PPC_BOOK3E */
 
 #ifdef CONFIG_PPC_STD_MMU_64

commit 1db365258ad9c3624897f48c764f8c557f492b26
Author: Anshuman Khandual <khandual@linux.vnet.ibm.com>
Date:   Thu May 21 12:13:03 2015 +0530

    powerpc/kernel: Rename PACA_DSCR to PACA_DSCR_DEFAULT
    
    PACA_DSCR offset macro tracks dscr_default element in the paca
    structure. Better change the name of this macro to match that of the
    data element it tracks. Makes the code more readable.
    
    Signed-off-by: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0034b6b3556a..98230579d99c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -247,7 +247,7 @@ int main(void)
 #endif
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
 	DEFINE(PACAKEXECSTATE, offsetof(struct paca_struct, kexec_state));
-	DEFINE(PACA_DSCR, offsetof(struct paca_struct, dscr_default));
+	DEFINE(PACA_DSCR_DEFAULT, offsetof(struct paca_struct, dscr_default));
 	DEFINE(PACA_STARTTIME, offsetof(struct paca_struct, starttime));
 	DEFINE(PACA_STARTTIME_USER, offsetof(struct paca_struct, starttime_user));
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));

commit 66feed61cdf6ee65fd551d3460b1efba6bee55b8
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:12 2015 +1100

    KVM: PPC: Book3S HV: Use msgsnd for signalling threads on POWER8
    
    This uses msgsnd where possible for signalling other threads within
    the same core on POWER8 systems, rather than IPIs through the XICS
    interrupt controller.  This includes waking secondary threads to run
    the guest, the interrupts generated by the virtual XICS, and the
    interrupts to bring the other threads out of the guest when exiting.
    
    Aggregated statistics from debugfs across vcpus for a guest with 32
    vcpus, 8 threads/vcore, running on a POWER8, show this before the
    change:
    
     rm_entry:     3387.6ns (228 - 86600, 1008969 samples)
      rm_exit:     4561.5ns (12 - 3477452, 1009402 samples)
      rm_intr:     1660.0ns (12 - 553050, 3600051 samples)
    
    and this after the change:
    
     rm_entry:     3060.1ns (212 - 65138, 953873 samples)
      rm_exit:     4244.1ns (12 - 9693408, 954331 samples)
      rm_intr:     1342.3ns (12 - 1104718, 3405326 samples)
    
    for a test of booting Fedora 20 big-endian to the login prompt.
    
    The time taken for a H_PROD hcall (which is handled in the host
    kernel) went down from about 35 microseconds to about 16 microseconds
    with this change.
    
    The noinline added to kvmppc_run_core turned out to be necessary for
    good performance, at least with gcc 4.9.2 as packaged with Fedora 21
    and a little-endian POWER8 host.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0d07efbe3fa7..0034b6b3556a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -37,6 +37,7 @@
 #include <asm/thread_info.h>
 #include <asm/rtas.h>
 #include <asm/vdso_datapage.h>
+#include <asm/dbell.h>
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #include <asm/lppaca.h>
@@ -759,5 +760,7 @@ int main(void)
 			offsetof(struct paca_struct, subcore_sibling_mask));
 #endif
 
+	DEFINE(PPC_DBELL_SERVER, PPC_DBELL_SERVER);
+
 	return 0;
 }

commit 7d6c40da198ac18bd5dd2cd18628d5b4c615d842
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:09 2015 +1100

    KVM: PPC: Book3S HV: Use bitmap of active threads rather than count
    
    Currently, the entry_exit_count field in the kvmppc_vcore struct
    contains two 8-bit counts, one of the threads that have started entering
    the guest, and one of the threads that have started exiting the guest.
    This changes it to an entry_exit_map field which contains two bitmaps
    of 8 bits each.  The advantage of doing this is that it gives us a
    bitmap of which threads need to be signalled when exiting the guest.
    That means that we no longer need to use the trick of setting the
    HDEC to 0 to pull the other threads out of the guest, which led in
    some cases to a spurious HDEC interrupt on the next guest entry.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8aa824681d76..0d07efbe3fa7 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -562,7 +562,7 @@ int main(void)
 	DEFINE(VCPU_ACOP, offsetof(struct kvm_vcpu, arch.acop));
 	DEFINE(VCPU_WORT, offsetof(struct kvm_vcpu, arch.wort));
 	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
-	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
+	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_map));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
 	DEFINE(VCORE_KVM, offsetof(struct kvmppc_vcore, kvm));

commit 5d5b99cd6818bdbea287d23ef055bba1a8a9e648
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:06 2015 +1100

    KVM: PPC: Book3S HV: Get rid of vcore nap_count and n_woken
    
    We can tell when a secondary thread has finished running a guest by
    the fact that it clears its kvm_hstate.kvm_vcpu pointer, so there
    is no real need for the nap_count field in the kvmppc_vcore struct.
    This changes kvmppc_wait_for_nap to poll the kvm_hstate.kvm_vcpu
    pointers of the secondary threads rather than polling vc->nap_count.
    Besides reducing the size of the kvmppc_vcore struct by 8 bytes,
    this also means that we can tell which secondary threads have got
    stuck and thus print a more informative error message.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 92ec3fcb3415..8aa824681d76 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -563,7 +563,6 @@ int main(void)
 	DEFINE(VCPU_WORT, offsetof(struct kvm_vcpu, arch.wort));
 	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
-	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
 	DEFINE(VCORE_KVM, offsetof(struct kvmppc_vcore, kvm));

commit 1f09c3ed86287d40fef90611cbbee055313f52cf
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:04 2015 +1100

    KVM: PPC: Book3S HV: Minor cleanups
    
    * Remove unused kvmppc_vcore::n_busy field.
    * Remove setting of RMOR, since it was only used on PPC970 and the
      PPC970 KVM support has been removed.
    * Don't use r1 or r2 in setting the runlatch since they are
      conventionally reserved for other things; use r0 instead.
    * Streamline the code a little and remove the ext_interrupt_to_host
      label.
    * Add some comments about register usage.
    * hcall_try_real_mode doesn't need to be global, and can't be
      called from C code anyway.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 3fea721f0da5..92ec3fcb3415 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -505,7 +505,6 @@ int main(void)
 	DEFINE(KVM_NEED_FLUSH, offsetof(struct kvm, arch.need_tlb_flush.bits));
 	DEFINE(KVM_ENABLED_HCALLS, offsetof(struct kvm, arch.enabled_hcalls));
 	DEFINE(KVM_LPCR, offsetof(struct kvm, arch.lpcr));
-	DEFINE(KVM_RMOR, offsetof(struct kvm, arch.rmor));
 	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));
 	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));

commit b6c295df3131c6fa25f8f29625ee0609506150ad
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Mar 28 14:21:02 2015 +1100

    KVM: PPC: Book3S HV: Accumulate timing information for real-mode code
    
    This reads the timebase at various points in the real-mode guest
    entry/exit code and uses that to accumulate total, minimum and
    maximum time spent in those parts of the code.  Currently these
    times are accumulated per vcpu in 5 parts of the code:
    
    * rm_entry - time taken from the start of kvmppc_hv_entry() until
      just before entering the guest.
    * rm_intr - time from when we take a hypervisor interrupt in the
      guest until we either re-enter the guest or decide to exit to the
      host.  This includes time spent handling hcalls in real mode.
    * rm_exit - time from when we decide to exit the guest until the
      return from kvmppc_hv_entry().
    * guest - time spend in the guest
    * cede - time spent napping in real mode due to an H_CEDE hcall
      while other threads in the same vcore are active.
    
    These times are exposed in debugfs in a directory per vcpu that
    contains a file called "timings".  This file contains one line for
    each of the 5 timings above, with the name followed by a colon and
    4 numbers, which are the count (number of times the code has been
    executed), the total time, the minimum time, and the maximum time,
    all in nanoseconds.
    
    The overhead of the extra code amounts to about 30ns for an hcall that
    is handled in real mode (e.g. H_SET_DABR), which is about 25%.  Since
    production environments may not wish to incur this overhead, the new
    code is conditional on a new config symbol,
    CONFIG_KVM_BOOK3S_HV_EXIT_TIMING.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4717859fdd04..3fea721f0da5 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -458,6 +458,19 @@ int main(void)
 	DEFINE(VCPU_SPRG1, offsetof(struct kvm_vcpu, arch.shregs.sprg1));
 	DEFINE(VCPU_SPRG2, offsetof(struct kvm_vcpu, arch.shregs.sprg2));
 	DEFINE(VCPU_SPRG3, offsetof(struct kvm_vcpu, arch.shregs.sprg3));
+#endif
+#ifdef CONFIG_KVM_BOOK3S_HV_EXIT_TIMING
+	DEFINE(VCPU_TB_RMENTRY, offsetof(struct kvm_vcpu, arch.rm_entry));
+	DEFINE(VCPU_TB_RMINTR, offsetof(struct kvm_vcpu, arch.rm_intr));
+	DEFINE(VCPU_TB_RMEXIT, offsetof(struct kvm_vcpu, arch.rm_exit));
+	DEFINE(VCPU_TB_GUEST, offsetof(struct kvm_vcpu, arch.guest_time));
+	DEFINE(VCPU_TB_CEDE, offsetof(struct kvm_vcpu, arch.cede_time));
+	DEFINE(VCPU_CUR_ACTIVITY, offsetof(struct kvm_vcpu, arch.cur_activity));
+	DEFINE(VCPU_ACTIVITY_START, offsetof(struct kvm_vcpu, arch.cur_tb_start));
+	DEFINE(TAS_SEQCOUNT, offsetof(struct kvmhv_tb_accumulator, seqcount));
+	DEFINE(TAS_TOTAL, offsetof(struct kvmhv_tb_accumulator, tb_total));
+	DEFINE(TAS_MIN, offsetof(struct kvmhv_tb_accumulator, tb_min));
+	DEFINE(TAS_MAX, offsetof(struct kvmhv_tb_accumulator, tb_max));
 #endif
 	DEFINE(VCPU_SHARED_SPRG3, offsetof(struct kvm_vcpu_arch_shared, sprg3));
 	DEFINE(VCPU_SHARED_SPRG4, offsetof(struct kvm_vcpu_arch_shared, sprg4));

commit 9a4fc4eaf111ca960c9f524b850598e9dbc9697f
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 10 19:34:31 2014 +1000

    powerpc/kvm: Create proper names for the kvm_host_state PMU fields
    
    We have two arrays in kvm_host_state that contain register values for
    the PMU. Currently we only create an asm-offsets symbol for the base of
    the arrays, and do the array offset in the assembly code.
    
    Creating an asm-offsets symbol for each field individually makes the
    code much nicer to read, particularly for the MMCRx/SIxR/SDAR fields, and
    might have helped us notice the recent double restore bug we had in this
    code.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Acked-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e624f9646350..4717859fdd04 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -644,8 +644,19 @@ int main(void)
 	HSTATE_FIELD(HSTATE_SAVED_XIRR, saved_xirr);
 	HSTATE_FIELD(HSTATE_HOST_IPI, host_ipi);
 	HSTATE_FIELD(HSTATE_PTID, ptid);
-	HSTATE_FIELD(HSTATE_MMCR, host_mmcr);
-	HSTATE_FIELD(HSTATE_PMC, host_pmc);
+	HSTATE_FIELD(HSTATE_MMCR0, host_mmcr[0]);
+	HSTATE_FIELD(HSTATE_MMCR1, host_mmcr[1]);
+	HSTATE_FIELD(HSTATE_MMCRA, host_mmcr[2]);
+	HSTATE_FIELD(HSTATE_SIAR, host_mmcr[3]);
+	HSTATE_FIELD(HSTATE_SDAR, host_mmcr[4]);
+	HSTATE_FIELD(HSTATE_MMCR2, host_mmcr[5]);
+	HSTATE_FIELD(HSTATE_SIER, host_mmcr[6]);
+	HSTATE_FIELD(HSTATE_PMC1, host_pmc[0]);
+	HSTATE_FIELD(HSTATE_PMC2, host_pmc[1]);
+	HSTATE_FIELD(HSTATE_PMC3, host_pmc[2]);
+	HSTATE_FIELD(HSTATE_PMC4, host_pmc[3]);
+	HSTATE_FIELD(HSTATE_PMC5, host_pmc[4]);
+	HSTATE_FIELD(HSTATE_PMC6, host_pmc[5]);
 	HSTATE_FIELD(HSTATE_PURR, host_purr);
 	HSTATE_FIELD(HSTATE_SPURR, host_spurr);
 	HSTATE_FIELD(HSTATE_DSCR, host_dscr);

commit 34b85e3574424beb30e4cd163e6da2e2282d2683
Merge: d5e80b4b1857 d70a54e2d085
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 19 12:57:45 2014 -0800

    Merge tag 'powerpc-3.19-2' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull second batch of powerpc updates from Michael Ellerman:
     "The highlight is the series that reworks the idle management on
      powernv, which allows us to use deeper idle states on those machines.
    
      There's the fix from Anton for the "BUG at kernel/smpboot.c:134!"
      problem.
    
      An i2c driver for powernv.  This is acked by Wolfram Sang, and he
      asked that we take it through the powerpc tree.
    
      A fix for audit from rgb at Red Hat, acked by Paul Moore who is one of
      the audit maintainers.
    
      A patch from Ben to export the symbol map of our OPAL firmware as a
      sysfs file, so that tools can use it.
    
      Also some CXL fixes, a couple of powerpc perf fixes, a fix for
      smt-enabled, and the patch to add __force to get_user() so we can use
      bitwise types"
    
    * tag 'powerpc-3.19-2' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux:
      powerpc/powernv: Ignore smt-enabled on Power8 and later
      powerpc/uaccess: Allow get_user() with bitwise types
      powerpc/powernv: Expose OPAL firmware symbol map
      powernv/powerpc: Add winkle support for offline cpus
      powernv/cpuidle: Redesign idle states management
      powerpc/powernv: Enable Offline CPUs to enter deep idle states
      powerpc/powernv: Switch off MMU before entering nap/sleep/rvwinkle mode
      i2c: Driver to expose PowerNV platform i2c busses
      powerpc: add little endian flag to syscall_get_arch()
      power/perf/hv-24x7: Use kmem_cache_free() instead of kfree
      powerpc/perf/hv-24x7: Use per-cpu page buffer
      cxl: Unmap MMIO regions when detaching a context
      cxl: Add timeout to process element commands
      cxl: Change contexts_lock to a mutex to fix sleep while atomic bug
      powerpc: Secondary CPUs must set cpu_callin_map after setting active and online

commit 66dcff86ba40eebb5133cccf450878f2bba102ef
Merge: 91ed9e8a32d9 2c4aa55a6af0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 18 16:05:28 2014 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM update from Paolo Bonzini:
     "3.19 changes for KVM:
    
       - spring cleaning: removed support for IA64, and for hardware-
         assisted virtualization on the PPC970
    
       - ARM, PPC, s390 all had only small fixes
    
      For x86:
       - small performance improvements (though only on weird guests)
       - usual round of hardware-compliancy fixes from Nadav
       - APICv fixes
       - XSAVES support for hosts and guests.  XSAVES hosts were broken
         because the (non-KVM) XSAVES patches inadvertently changed the KVM
         userspace ABI whenever XSAVES was enabled; hence, this part is
         going to stable.  Guest support is just a matter of exposing the
         feature and CPUID leaves support"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (179 commits)
      KVM: move APIC types to arch/x86/
      KVM: PPC: Book3S: Enable in-kernel XICS emulation by default
      KVM: PPC: Book3S HV: Improve H_CONFER implementation
      KVM: PPC: Book3S HV: Fix endianness of instruction obtained from HEIR register
      KVM: PPC: Book3S HV: Remove code for PPC970 processors
      KVM: PPC: Book3S HV: Tracepoints for KVM HV guest interactions
      KVM: PPC: Book3S HV: Simplify locking around stolen time calculations
      arch: powerpc: kvm: book3s_paired_singles.c: Remove unused function
      arch: powerpc: kvm: book3s_pr.c: Remove unused function
      arch: powerpc: kvm: book3s.c: Remove some unused functions
      arch: powerpc: kvm: book3s_32_mmu.c: Remove unused function
      KVM: PPC: Book3S HV: Check wait conditions before sleeping in kvmppc_vcore_blocked
      KVM: PPC: Book3S HV: ptes are big endian
      KVM: PPC: Book3S HV: Fix inaccuracies in ICP emulation for H_IPI
      KVM: PPC: Book3S HV: Fix KSM memory corruption
      KVM: PPC: Book3S HV: Fix an issue where guest is paused on receiving HMI
      KVM: PPC: Book3S HV: Fix computation of tlbie operand
      KVM: PPC: Book3S HV: Add missing HPTE unlock
      KVM: PPC: BookE: Improve irq inject tracepoint
      arm/arm64: KVM: Require in-kernel vgic for the arch timers
      ...

commit 4a157d61b48c7cdb8d751001442a14ebac80229f
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 13:30:39 2014 +1100

    KVM: PPC: Book3S HV: Fix endianness of instruction obtained from HEIR register
    
    There are two ways in which a guest instruction can be obtained from
    the guest in the guest exit code in book3s_hv_rmhandlers.S.  If the
    exit was caused by a Hypervisor Emulation interrupt (i.e. an illegal
    instruction), the offending instruction is in the HEIR register
    (Hypervisor Emulation Instruction Register).  If the exit was caused
    by a load or store to an emulated MMIO device, we load the instruction
    from the guest by turning data relocation on and loading the instruction
    with an lwz instruction.
    
    Unfortunately, in the case where the guest has opposite endianness to
    the host, these two methods give results of different endianness, but
    both get put into vcpu->arch.last_inst.  The HEIR value has been loaded
    using guest endianness, whereas the lwz will load the instruction using
    host endianness.  The rest of the code that uses vcpu->arch.last_inst
    assumes it was loaded using host endianness.
    
    To fix this, we define a new vcpu field to store the HEIR value.  Then,
    in kvmppc_handle_exit_hv(), we transfer the value from this new field to
    vcpu->arch.last_inst, doing a byte-swap if the guest and host endianness
    differ.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 815212e9d7ba..b14716bbb7b5 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -498,6 +498,7 @@ int main(void)
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 	DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
+	DEFINE(VCPU_HEIR, offsetof(struct kvm_vcpu, arch.emul_inst));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));

commit c17b98cf6028704e1f953d6a25ed6140425ccfd0
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 13:30:38 2014 +1100

    KVM: PPC: Book3S HV: Remove code for PPC970 processors
    
    This removes the code that was added to enable HV KVM to work
    on PPC970 processors.  The PPC970 is an old CPU that doesn't
    support virtualizing guest memory.  Removing PPC970 support also
    lets us remove the code for allocating and managing contiguous
    real-mode areas, the code for the !kvm->arch.using_mmu_notifiers
    case, the code for pinning pages of guest memory when first
    accessed and keeping track of which pages have been pinned, and
    the code for handling H_ENTER hypercalls in virtual mode.
    
    Book3S HV KVM is now supported only on POWER7 and POWER8 processors.
    The KVM_CAP_PPC_RMA capability now always returns 0.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9d7dede2847c..815212e9d7ba 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -489,7 +489,6 @@ int main(void)
 	DEFINE(KVM_HOST_LPID, offsetof(struct kvm, arch.host_lpid));
 	DEFINE(KVM_HOST_LPCR, offsetof(struct kvm, arch.host_lpcr));
 	DEFINE(KVM_HOST_SDR1, offsetof(struct kvm, arch.host_sdr1));
-	DEFINE(KVM_TLBIE_LOCK, offsetof(struct kvm, arch.tlbie_lock));
 	DEFINE(KVM_NEED_FLUSH, offsetof(struct kvm, arch.need_tlb_flush.bits));
 	DEFINE(KVM_ENABLED_HCALLS, offsetof(struct kvm, arch.enabled_hcalls));
 	DEFINE(KVM_LPCR, offsetof(struct kvm, arch.lpcr));

commit 77b54e9f213f76a23736940cf94bcd765fc00f40
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:53 2014 +0530

    powernv/powerpc: Add winkle support for offline cpus
    
    Winkle is a deep idle state supported in power8 chips. A core enters
    winkle when all the threads of the core enter winkle. In this state
    power supply to the entire chiplet i.e core, private L2 and private L3
    is turned off. As a result it gives higher powersavings compared to
    sleep.
    
    But entering winkle results in a total hypervisor state loss. Hence the
    hypervisor context has to be preserved before entering winkle and
    restored upon wake up.
    
    Power-on Reset Engine (PORE) is a dedicated engine which is responsible
    for powering on the chiplet during wake up. It can be programmed to
    restore the register contests of a few specific registers. This patch
    uses PORE to restore register state wherever possible and uses stack to
    save and restore rest of the necessary registers.
    
    With hypervisor state restore things fall under three categories-
    per-core state, per-subcore state and per-thread state. To manage this,
    extend the infrastructure introduced for sleep. Mainly we add a paca
    variable subcore_sibling_mask. Using this and the core_idle_state we can
    distingush first thread in core and subcore.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index bbd27fe0c039..f68de7a73faa 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -733,6 +733,8 @@ int main(void)
 			offsetof(struct paca_struct, thread_idle_state));
 	DEFINE(PACA_THREAD_MASK,
 			offsetof(struct paca_struct, thread_mask));
+	DEFINE(PACA_SUBCORE_SIBLING_MASK,
+			offsetof(struct paca_struct, subcore_sibling_mask));
 #endif
 
 	return 0;

commit 7cba160ad789a3ad7e68b92bf20eaad6ed171f80
Author: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
Date:   Wed Dec 10 00:26:52 2014 +0530

    powernv/cpuidle: Redesign idle states management
    
    Deep idle states like sleep and winkle are per core idle states. A core
    enters these states only when all the threads enter either the
    particular idle state or a deeper one. There are tasks like fastsleep
    hardware bug workaround and hypervisor core state save which have to be
    done only by the last thread of the core entering deep idle state and
    similarly tasks like timebase resync, hypervisor core register restore
    that have to be done only by the first thread waking up from these
    state.
    
    The current idle state management does not have a way to distinguish the
    first/last thread of the core waking/entering idle states. Tasks like
    timebase resync are done for all the threads. This is not only is
    suboptimal, but can cause functionality issues when subcores and kvm is
    involved.
    
    This patch adds the necessary infrastructure to track idle states of
    threads in a per-core structure. It uses this info to perform tasks like
    fastsleep workaround and timebase resync only once per core.
    
    Signed-off-by: Shreyas B. Prabhu <shreyas@linux.vnet.ibm.com>
    Originally-by: Preeti U. Murthy <preeti@linux.vnet.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: linux-pm@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c161ef3f28a1..bbd27fe0c039 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -726,5 +726,14 @@ int main(void)
 					arch.timing_last_enter.tv32.tbl));
 #endif
 
+#ifdef CONFIG_PPC_POWERNV
+	DEFINE(PACA_CORE_IDLE_STATE_PTR,
+			offsetof(struct paca_struct, core_idle_state_ptr));
+	DEFINE(PACA_THREAD_IDLE_STATE,
+			offsetof(struct paca_struct, thread_idle_state));
+	DEFINE(PACA_THREAD_MASK,
+			offsetof(struct paca_struct, thread_mask));
+#endif
+
 	return 0;
 }

commit 6d626c5ea3d8411cc2a72d7cabe70f01dfc32d1d
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Mon Nov 24 21:59:26 2014 +0530

    powerpc/powernv: Cleanup unused MCE definitions/declarations.
    
    Cleanup OpalMCE_* definitions/declarations and other related code which
    is not used anymore.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Benjamin Herrrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9d7dede2847c..c161ef3f28a1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -726,12 +726,5 @@ int main(void)
 					arch.timing_last_enter.tv32.tbl));
 #endif
 
-#ifdef CONFIG_PPC_POWERNV
-	DEFINE(OPAL_MC_GPR3, offsetof(struct opal_machine_check_event, gpr3));
-	DEFINE(OPAL_MC_SRR0, offsetof(struct opal_machine_check_event, srr0));
-	DEFINE(OPAL_MC_SRR1, offsetof(struct opal_machine_check_event, srr1));
-	DEFINE(PACA_OPAL_MC_EVT, offsetof(struct paca_struct, opal_mc_evt));
-#endif
-
 	return 0;
 }

commit 66bb0aa077978dbb76e6283531eb3cc7a878de38
Merge: e306e3be1cbe c77dcacb3975
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Aug 7 11:35:30 2014 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull second round of KVM changes from Paolo Bonzini:
     "Here are the PPC and ARM changes for KVM, which I separated because
      they had small conflicts (respectively within KVM documentation, and
      with 3.16-rc changes).  Since they were all within the subsystem, I
      took care of them.
    
      Stephen Rothwell reported some snags in PPC builds, but they are all
      fixed now; the latest linux-next report was clean.
    
      New features for ARM include:
       - KVM VGIC v2 emulation on GICv3 hardware
       - Big-Endian support for arm/arm64 (guest and host)
       - Debug Architecture support for arm64 (arm32 is on Christoffer's todo list)
    
      And for PPC:
       - Book3S: Good number of LE host fixes, enable HV on LE
       - Book3S HV: Add in-guest debug support
    
      This release drops support for KVM on the PPC440.  As a result, the
      PPC merge removes more lines than it adds.  :)
    
      I also included an x86 change, since Davidlohr tied it to an
      independent bug report and the reporter quickly provided a Tested-by;
      there was no reason to wait for -rc2"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (122 commits)
      KVM: Move more code under CONFIG_HAVE_KVM_IRQFD
      KVM: nVMX: fix "acknowledge interrupt on exit" when APICv is in use
      KVM: nVMX: Fix nested vmexit ack intr before load vmcs01
      KVM: PPC: Enable IRQFD support for the XICS interrupt controller
      KVM: Give IRQFD its own separate enabling Kconfig option
      KVM: Move irq notifier implementation into eventfd.c
      KVM: Move all accesses to kvm::irq_routing into irqchip.c
      KVM: irqchip: Provide and use accessors for irq routing table
      KVM: Don't keep reference to irq routing table in irqfd struct
      KVM: PPC: drop duplicate tracepoint
      arm64: KVM: fix 64bit CP15 VM access for 32bit guests
      KVM: arm64: GICv3: mandate page-aligned GICV region
      arm64: KVM: GICv3: move system register access to msr_s/mrs_s
      KVM: PPC: PR: Handle FSCR feature deselects
      KVM: PPC: HV: Remove generic instruction emulation
      KVM: PPC: BOOKEHV: rename e500hv_spr to bookehv_spr
      KVM: PPC: Remove DCR handling
      KVM: PPC: Expose helper functions for data/inst faults
      KVM: PPC: Separate loadstore emulation from priv emulation
      KVM: PPC: Handle magic page in kvmppc_ld/st
      ...

commit 99e99d19a86dc596703ed79dcecf9ca6b32a6a8a
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Mon Jul 21 11:23:26 2014 +0530

    kvm: ppc: bookehv: Save restore SPRN_SPRG9 on guest entry exit
    
    SPRN_SPRG is used by debug interrupt handler, so this is required for
    debug support.
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 17ffcb4f27f9..ab9ae0411e8f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -668,6 +668,7 @@ int main(void)
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
 	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
 	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
+	DEFINE(VCPU_SPRG9, offsetof(struct kvm_vcpu, arch.sprg9));
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
 	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));

commit 699a0ea0823d32030b0666b28ff8633960f7ffa7
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 2 11:02:59 2014 +1000

    KVM: PPC: Book3S: Controls for in-kernel sPAPR hypercall handling
    
    This provides a way for userspace controls which sPAPR hcalls get
    handled in the kernel.  Each hcall can be individually enabled or
    disabled for in-kernel handling, except for H_RTAS.  The exception
    for H_RTAS is because userspace can already control whether
    individual RTAS functions are handled in-kernel or not via the
    KVM_PPC_RTAS_DEFINE_TOKEN ioctl, and because the numeric value for
    H_RTAS is out of the normal sequence of hcall numbers.
    
    Hcalls are enabled or disabled using the KVM_ENABLE_CAP ioctl for the
    KVM_CAP_PPC_ENABLE_HCALL capability on the file descriptor for the VM.
    The args field of the struct kvm_enable_cap specifies the hcall number
    in args[0] and the enable/disable flag in args[1]; 0 means disable
    in-kernel handling (so that the hcall will always cause an exit to
    userspace) and 1 means enable.  Enabling or disabling in-kernel
    handling of an hcall is effective across the whole VM.
    
    The ability for KVM_ENABLE_CAP to be used on a VM file descriptor
    on PowerPC is new, added by this commit.  The KVM_CAP_ENABLE_CAP_VM
    capability advertises that this ability exists.
    
    When a VM is created, an initial set of hcalls are enabled for
    in-kernel handling.  The set that is enabled is the set that have
    an in-kernel implementation at this point.  Any new hcall
    implementations from this point onwards should not be added to the
    default set without a good reason.
    
    No distinction is made between real-mode and virtual-mode hcall
    implementations; the one setting controls them both.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index f5995a912213..17ffcb4f27f9 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -493,6 +493,7 @@ int main(void)
 	DEFINE(KVM_HOST_SDR1, offsetof(struct kvm, arch.host_sdr1));
 	DEFINE(KVM_TLBIE_LOCK, offsetof(struct kvm, arch.tlbie_lock));
 	DEFINE(KVM_NEED_FLUSH, offsetof(struct kvm, arch.need_tlb_flush.bits));
+	DEFINE(KVM_ENABLED_HCALLS, offsetof(struct kvm, arch.enabled_hcalls));
 	DEFINE(KVM_LPCR, offsetof(struct kvm, arch.lpcr));
 	DEFINE(KVM_RMOR, offsetof(struct kvm, arch.rmor));
 	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));

commit 376af5947c0e441ccbf98f0212d4ffbf171528f6
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Jul 10 12:29:19 2014 +1000

    powerpc: Remove STAB code
    
    Old cpus didn't have a Segment Lookaside Buffer (SLB), instead they had
    a Segment Table (STAB). Now that we've dropped support for those cpus,
    we can remove the STAB support entirely.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index f5995a912213..e35054054c32 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -216,8 +216,6 @@ int main(void)
 #endif /* CONFIG_PPC_BOOK3E */
 
 #ifdef CONFIG_PPC_STD_MMU_64
-	DEFINE(PACASTABREAL, offsetof(struct paca_struct, stab_real));
-	DEFINE(PACASTABVIRT, offsetof(struct paca_struct, stab_addr));
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
 	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACAVMALLOCSLLP, offsetof(struct paca_struct, vmalloc_sllp));

commit c5aec4c76af1a2d89ee2f2d4d5463b2ad2d85de5
Merge: 2937f5efa575 0c0a3e5a100b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 10 18:54:22 2014 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "Here is the bulk of the powerpc changes for this merge window.  It got
      a bit delayed in part because I wasn't paying attention, and in part
      because I discovered I had a core PCI change without a PCI maintainer
      ack in it.  Bjorn eventually agreed it was ok to merge it though we'll
      probably improve it later and I didn't want to rebase to add his ack.
    
      There is going to be a bit more next week, essentially fixes that I
      still want to sort through and test.
    
      The biggest item this time is the support to build the ppc64 LE kernel
      with our new v2 ABI.  We previously supported v2 userspace but the
      kernel itself was a tougher nut to crack.  This is now sorted mostly
      thanks to Anton and Rusty.
    
      We also have a fairly big series from Cedric that add support for
      64-bit LE zImage boot wrapper.  This was made harder by the fact that
      traditionally our zImage wrapper was always 32-bit, but our new LE
      toolchains don't really support 32-bit anymore (it's somewhat there
      but not really "supported") so we didn't want to rely on it.  This
      meant more churn that just endian fixes.
    
      This brings some more LE bits as well, such as the ability to run in
      LE mode without a hypervisor (ie. under OPAL firmware) by doing the
      right OPAL call to reinitialize the CPU to take HV interrupts in the
      right mode and the usual pile of endian fixes.
    
      There's another series from Gavin adding EEH improvements (one day we
      *will* have a release with less than 20 EEH patches, I promise!).
    
      Another highlight is the support for the "Split core" functionality on
      P8 by Michael.  This allows a P8 core to be split into "sub cores" of
      4 threads which allows the subcores to run different guests under KVM
      (the HW still doesn't support a partition per thread).
    
      And then the usual misc bits and fixes ..."
    
    [ Further delayed by gmail deciding that BenH is a dirty spammer.
      Google knows.  ]
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (155 commits)
      powerpc/powernv: Add missing include to LPC code
      selftests/powerpc: Test the THP bug we fixed in the previous commit
      powerpc/mm: Check paca psize is up to date for huge mappings
      powerpc/powernv: Pass buffer size to OPAL validate flash call
      powerpc/pseries: hcall functions are exported to modules, need _GLOBAL_TOC()
      powerpc: Exported functions __clear_user and copy_page use r2 so need _GLOBAL_TOC()
      powerpc/powernv: Set memory_block_size_bytes to 256MB
      powerpc: Allow ppc_md platform hook to override memory_block_size_bytes
      powerpc/powernv: Fix endian issues in memory error handling code
      powerpc/eeh: Skip eeh sysfs when eeh is disabled
      powerpc: 64bit sendfile is capped at 2GB
      powerpc/powernv: Provide debugfs access to the LPC bus via OPAL
      powerpc/serial: Use saner flags when creating legacy ports
      powerpc: Add cpu family documentation
      powerpc/xmon: Fix up xmon format strings
      powerpc/powernv: Add calls to support little endian host
      powerpc: Document sysfs DSCR interface
      powerpc: Fix regression of per-CPU DSCR setting
      powerpc: Split __SYSFS_SPRSETUP macro
      arch: powerpc/fadump: Cleaning up inconsistent NULL checks
      ...

commit e14e7a1e537d6e18f9c511f25c25c5efb7799fb5
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Apr 22 12:26:58 2014 +0200

    KVM: PPC: Book3S PR: Expose TAR facility to guest
    
    POWER8 implements a new register called TAR. This register has to be
    enabled in FSCR and then from KVM's point of view is mere storage.
    
    This patch enables the guest to use TAR.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e2b86b5c02b3..93e1465c8496 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -446,7 +446,9 @@ int main(void)
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
 	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
+#ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_TAR, offsetof(struct kvm_vcpu, arch.tar));
+#endif
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE

commit 616dff86028298dbc91174fb3d12b8ed8cd74955
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Apr 29 16:48:44 2014 +0200

    KVM: PPC: Book3S PR: Handle Facility interrupt and FSCR
    
    POWER8 introduced a new interrupt type called "Facility unavailable interrupt"
    which contains its status message in a new register called FSCR.
    
    Handle these exits and try to emulate instructions for unhandled facilities.
    Follow-on patches enable KVM to expose specific facilities into the guest.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index bbf3b9a3e2af..e2b86b5c02b3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -537,6 +537,7 @@ int main(void)
 	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
 	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
 	DEFINE(VCPU_FSCR, offsetof(struct kvm_vcpu, arch.fscr));
+	DEFINE(VCPU_SHADOW_FSCR, offsetof(struct kvm_vcpu, arch.shadow_fscr));
 	DEFINE(VCPU_PSPB, offsetof(struct kvm_vcpu, arch.pspb));
 	DEFINE(VCPU_EBBHR, offsetof(struct kvm_vcpu, arch.ebbhr));
 	DEFINE(VCPU_EBBRR, offsetof(struct kvm_vcpu, arch.ebbrr));
@@ -618,6 +619,7 @@ int main(void)
 #ifdef CONFIG_PPC64
 	SVCPU_FIELD(SVCPU_SLB, slb);
 	SVCPU_FIELD(SVCPU_SLB_MAX, slb_max);
+	SVCPU_FIELD(SVCPU_SHADOW_FSCR, shadow_fscr);
 #endif
 
 	HSTATE_FIELD(HSTATE_HOST_R1, host_r1);
@@ -653,6 +655,7 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S_64
 	HSTATE_FIELD(HSTATE_CFAR, cfar);
 	HSTATE_FIELD(HSTATE_PPR, ppr);
+	HSTATE_FIELD(HSTATE_HOST_FSCR, host_fscr);
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 #else /* CONFIG_PPC_BOOK3S */

commit 5deb8e7ad8ac7e3fcdfa042acff617f461b361c2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Apr 24 13:46:24 2014 +0200

    KVM: PPC: Make shared struct aka magic page guest endian
    
    The shared (magic) page is a data structure that contains often used
    supervisor privileged SPRs accessible via memory to the user to reduce
    the number of exits we have to take to read/write them.
    
    When we actually share this structure with the guest we have to maintain
    it in guest endianness, because some of the patch tricks only work with
    native endian load/store operations.
    
    Since we only share the structure with either host or guest in little
    endian on book3s_64 pr mode, we don't have to worry about booke or book3s hv.
    
    For booke, the shared struct stays big endian. For book3s_64 hv we maintain
    the struct in host native endian, since it never gets shared with the guest.
    
    For book3s_64 pr we introduce a variable that tells us which endianness the
    shared struct is in and route every access to it through helper inline
    functions that evaluate this variable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6a4b77d197f3..bbf3b9a3e2af 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -54,6 +54,7 @@
 #endif
 #if defined(CONFIG_KVM) && defined(CONFIG_PPC_BOOK3S)
 #include <asm/kvm_book3s.h>
+#include <asm/kvm_ppc.h>
 #endif
 
 #ifdef CONFIG_PPC32
@@ -467,6 +468,9 @@ int main(void)
 	DEFINE(VCPU_SHARED, offsetof(struct kvm_vcpu, arch.shared));
 	DEFINE(VCPU_SHARED_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
 	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
+#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)
+	DEFINE(VCPU_SHAREDBE, offsetof(struct kvm_vcpu, arch.shared_big_endian));
+#endif
 
 	DEFINE(VCPU_SHARED_MAS0, offsetof(struct kvm_vcpu_arch_shared, mas0));
 	DEFINE(VCPU_SHARED_MAS1, offsetof(struct kvm_vcpu_arch_shared, mas1));

commit e5ee5422f8867d8b8108f8e1f0f47dc59b043f5b
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon May 5 08:39:44 2014 +0530

    KVM: PPC: BOOK3S: PR: Enable Little Endian PR guest
    
    This patch make sure we inherit the LE bit correctly in different case
    so that we can run Little Endian distro in PR mode
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index dba8140ebc20..6a4b77d197f3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -493,7 +493,6 @@ int main(void)
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 	DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
-	DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
@@ -528,6 +527,7 @@ int main(void)
 	DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));
 	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
 	DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
+	DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
 	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));

commit 1739ea9e13e636590dd56c2f4ca85e783da512e7
Author: Sam bobroff <sam.bobroff@au1.ibm.com>
Date:   Wed May 21 16:32:38 2014 +1000

    powerpc: Fix regression of per-CPU DSCR setting
    
    Since commit "efcac65 powerpc: Per process DSCR + some fixes (try#4)"
    it is no longer possible to set the DSCR on a per-CPU basis.
    
    The old behaviour was to minipulate the DSCR SPR directly but this is no
    longer sufficient: the value is quickly overwritten by context switching.
    
    This patch stores the per-CPU DSCR value in a kernel variable rather than
    directly in the SPR and it is used whenever a process has not set the DSCR
    itself. The sysfs interface (/sys/devices/system/cpu/cpuN/dscr) is unchanged.
    
    Writes to the old global default (/sys/devices/system/cpu/dscr_default)
    now set all of the per-CPU values and reads return the last written value.
    
    The new per-CPU default is added to the paca_struct and is used everywhere
    outside of sysfs.c instead of the old global default.
    
    Signed-off-by: Sam Bobroff <sam.bobroff@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index dba8140ebc20..cba2697406b7 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -247,6 +247,7 @@ int main(void)
 #endif
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
 	DEFINE(PACAKEXECSTATE, offsetof(struct paca_struct, kexec_state));
+	DEFINE(PACA_DSCR, offsetof(struct paca_struct, dscr_default));
 	DEFINE(PACA_STARTTIME, offsetof(struct paca_struct, starttime));
 	DEFINE(PACA_STARTTIME_USER, offsetof(struct paca_struct, starttime_user));
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));

commit 9d378dfac885f72b8b369d08fc61bef36e2f2dd1
Author: Scott Wood <scottwood@freescale.com>
Date:   Mon Mar 10 17:29:38 2014 -0500

    powerpc/booke64: Use SPRG7 for VDSO
    
    Previously SPRG3 was marked for use by both VDSO and critical
    interrupts (though critical interrupts were not fully implemented).
    
    In commit 8b64a9dfb091f1eca8b7e58da82f1e7d1d5fe0ad ("powerpc/booke64:
    Use SPRG0/3 scratch for bolted TLB miss & crit int"), Mihai Caraman
    made an attempt to resolve this conflict by restoring the VDSO value
    early in the critical interrupt, but this has some issues:
    
     - It's incompatible with EXCEPTION_COMMON which restores r13 from the
       by-then-overwritten scratch (this cost me some debugging time).
     - It forces critical exceptions to be a special case handled
       differently from even machine check and debug level exceptions.
     - It didn't occur to me that it was possible to make this work at all
       (by doing a final "ld r13, PACA_EXCRIT+EX_R13(r13)") until after
       I made (most of) this patch. :-)
    
    It might be worth investigating using a load rather than SPRG on return
    from all exceptions (except TLB misses where the scratch never leaves
    the SPRG) -- it could save a few cycles.  Until then, let's stick with
    SPRG for all exceptions.
    
    Since we cannot use SPRG4-7 for scratch without corrupting the state of
    a KVM guest, move VDSO to SPRG7 on book3e.  Since neither SPRG4-7 nor
    critical interrupts exist on book3s, SPRG3 is still used for VDSO
    there.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Mihai Caraman <mihai.caraman@freescale.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: kvm-ppc@vger.kernel.org

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b5aacf72ae6f..dba8140ebc20 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -253,7 +253,7 @@ int main(void)
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 	DEFINE(PACA_NAPSTATELOST, offsetof(struct paca_struct, nap_state_lost));
-	DEFINE(PACA_SPRG3, offsetof(struct paca_struct, sprg3));
+	DEFINE(PACA_SPRG_VDSO, offsetof(struct paca_struct, sprg_vdso));
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */

commit e2a0f813e0d53014b78aae76f0359c8a41f05eeb
Merge: e30b82bbe098 b73117c49364
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 31 08:37:32 2014 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull more KVM updates from Paolo Bonzini:
     "Second batch of KVM updates.  Some minor x86 fixes, two s390 guest
      features that need some handling in the host, and all the PPC changes.
    
      The PPC changes include support for little-endian guests and
      enablement for new POWER8 features"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (45 commits)
      x86, kvm: correctly access the KVM_CPUID_FEATURES leaf at 0x40000101
      x86, kvm: cache the base of the KVM cpuid leaves
      kvm: x86: move KVM_CAP_HYPERV_TIME outside #ifdef
      KVM: PPC: Book3S PR: Cope with doorbell interrupts
      KVM: PPC: Book3S HV: Add software abort codes for transactional memory
      KVM: PPC: Book3S HV: Add new state for transactional memory
      powerpc/Kconfig: Make TM select VSX and VMX
      KVM: PPC: Book3S HV: Basic little-endian guest support
      KVM: PPC: Book3S HV: Add support for DABRX register on POWER7
      KVM: PPC: Book3S HV: Prepare for host using hypervisor doorbells
      KVM: PPC: Book3S HV: Handle new LPCR bits on POWER8
      KVM: PPC: Book3S HV: Handle guest using doorbells for IPIs
      KVM: PPC: Book3S HV: Consolidate code that checks reason for wake from nap
      KVM: PPC: Book3S HV: Implement architecture compatibility modes for POWER8
      KVM: PPC: Book3S HV: Add handler for HV facility unavailable
      KVM: PPC: Book3S HV: Flush the correct number of TLB sets on POWER8
      KVM: PPC: Book3S HV: Context-switch new POWER8 SPRs
      KVM: PPC: Book3S HV: Align physical and virtual CPU thread numbers
      KVM: PPC: Book3S HV: Don't set DABR on POWER8
      kvm/ppc: IRQ disabling cleanup
      ...

commit b73117c49364551ff789db7c424a115ac5b77850
Merge: 77f01bdfa5e5 4068890931f6
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jan 29 18:29:01 2014 +0100

    Merge branch 'kvm-ppc-next' of git://github.com/agraf/linux-2.6 into kvm-queue
    
    Conflicts:
            arch/powerpc/kvm/book3s_hv_rmhandlers.S
            arch/powerpc/kvm/booke.c

commit 1b17366d695c8ab03f98d0155357e97a427e1dce
Merge: d12de1ef5eba 7179ba52889b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 27 21:11:26 2014 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "So here's my next branch for powerpc.  A bit late as I was on vacation
      last week.  It's mostly the same stuff that was in next already, I
      just added two patches today which are the wiring up of lockref for
      powerpc, which for some reason fell through the cracks last time and
      is trivial.
    
      The highlights are, in addition to a bunch of bug fixes:
    
       - Reworked Machine Check handling on kernels running without a
         hypervisor (or acting as a hypervisor).  Provides hooks to handle
         some errors in real mode such as TLB errors, handle SLB errors,
         etc...
    
       - Support for retrieving memory error information from the service
         processor on IBM servers running without a hypervisor and routing
         them to the memory poison infrastructure.
    
       - _PAGE_NUMA support on server processors
    
       - 32-bit BookE relocatable kernel support
    
       - FSL e6500 hardware tablewalk support
    
       - A bunch of new/revived board support
    
       - FSL e6500 deeper idle states and altivec powerdown support
    
      You'll notice a generic mm change here, it has been acked by the
      relevant authorities and is a pre-req for our _PAGE_NUMA support"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (121 commits)
      powerpc: Implement arch_spin_is_locked() using arch_spin_value_unlocked()
      powerpc: Add support for the optimised lockref implementation
      powerpc/powernv: Call OPAL sync before kexec'ing
      powerpc/eeh: Escalate error on non-existing PE
      powerpc/eeh: Handle multiple EEH errors
      powerpc: Fix transactional FP/VMX/VSX unavailable handlers
      powerpc: Don't corrupt transactional state when using FP/VMX in kernel
      powerpc: Reclaim two unused thread_info flag bits
      powerpc: Fix races with irq_work
      Move precessing of MCE queued event out from syscall exit path.
      pseries/cpuidle: Remove redundant call to ppc64_runlatch_off() in cpu idle routines
      powerpc: Make add_system_ram_resources() __init
      powerpc: add SATA_MV to ppc64_defconfig
      powerpc/powernv: Increase candidate fw image size
      powerpc: Add debug checks to catch invalid cpu-to-node mappings
      powerpc: Fix the setup of CPU-to-Node mappings during CPU online
      powerpc/iommu: Don't detach device without IOMMU group
      powerpc/eeh: Hotplug improvement
      powerpc/eeh: Call opal_pci_reinit() on powernv for restoring config space
      powerpc/eeh: Add restore_config operation
      ...

commit 7b490411c37f7ab7965cbdfe5e3ec28eadb6db5b
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jan 8 21:25:32 2014 +1100

    KVM: PPC: Book3S HV: Add new state for transactional memory
    
    Add new state for transactional memory (TM) to kvm_vcpu_arch.  Also add
    asm-offset bits that are going to be required.
    
    This also moves the existing TFHAR, TFIAR and TEXASR SPRs into a
    CONFIG_PPC_TRANSACTIONAL_MEM section.  This requires some code changes to
    ensure we still compile with CONFIG_PPC_TRANSACTIONAL_MEM=N.  Much of the added
    the added #ifdefs are removed in a later patch when the bulk of the TM code is
    added.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix merge conflict]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index a60a2fdae5bd..687f2ebf0cce 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -521,9 +521,6 @@ int main(void)
 	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
 	DEFINE(VCPU_FSCR, offsetof(struct kvm_vcpu, arch.fscr));
 	DEFINE(VCPU_PSPB, offsetof(struct kvm_vcpu, arch.pspb));
-	DEFINE(VCPU_TFHAR, offsetof(struct kvm_vcpu, arch.tfhar));
-	DEFINE(VCPU_TFIAR, offsetof(struct kvm_vcpu, arch.tfiar));
-	DEFINE(VCPU_TEXASR, offsetof(struct kvm_vcpu, arch.texasr));
 	DEFINE(VCPU_EBBHR, offsetof(struct kvm_vcpu, arch.ebbhr));
 	DEFINE(VCPU_EBBRR, offsetof(struct kvm_vcpu, arch.ebbrr));
 	DEFINE(VCPU_BESCR, offsetof(struct kvm_vcpu, arch.bescr));
@@ -545,6 +542,22 @@ int main(void)
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
 	DEFINE(VCPU_SLB_V, offsetof(struct kvmppc_slb, origv));
 	DEFINE(VCPU_SLB_SIZE, sizeof(struct kvmppc_slb));
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	DEFINE(VCPU_TFHAR, offsetof(struct kvm_vcpu, arch.tfhar));
+	DEFINE(VCPU_TFIAR, offsetof(struct kvm_vcpu, arch.tfiar));
+	DEFINE(VCPU_TEXASR, offsetof(struct kvm_vcpu, arch.texasr));
+	DEFINE(VCPU_GPR_TM, offsetof(struct kvm_vcpu, arch.gpr_tm));
+	DEFINE(VCPU_FPRS_TM, offsetof(struct kvm_vcpu, arch.fp_tm.fpr));
+	DEFINE(VCPU_VRS_TM, offsetof(struct kvm_vcpu, arch.vr_tm.vr));
+	DEFINE(VCPU_VRSAVE_TM, offsetof(struct kvm_vcpu, arch.vrsave_tm));
+	DEFINE(VCPU_CR_TM, offsetof(struct kvm_vcpu, arch.cr_tm));
+	DEFINE(VCPU_LR_TM, offsetof(struct kvm_vcpu, arch.lr_tm));
+	DEFINE(VCPU_CTR_TM, offsetof(struct kvm_vcpu, arch.ctr_tm));
+	DEFINE(VCPU_AMR_TM, offsetof(struct kvm_vcpu, arch.amr_tm));
+	DEFINE(VCPU_PPR_TM, offsetof(struct kvm_vcpu, arch.ppr_tm));
+	DEFINE(VCPU_DSCR_TM, offsetof(struct kvm_vcpu, arch.dscr_tm));
+	DEFINE(VCPU_TAR_TM, offsetof(struct kvm_vcpu, arch.tar_tm));
+#endif
 
 #ifdef CONFIG_PPC_BOOK3S_64
 #ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE

commit d682916a381ac7c8eb965c10ab64bc7cc2f18647
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Jan 8 21:25:30 2014 +1100

    KVM: PPC: Book3S HV: Basic little-endian guest support
    
    We create a guest MSR from scratch when delivering exceptions in
    a few places.  Instead of extracting LPCR[ILE] and inserting it
    into MSR_LE each time, we simply create a new variable intr_msr which
    contains the entire MSR to use.  For a little-endian guest, userspace
    needs to set the ILE (interrupt little-endian) bit in the LPCR for
    each vcpu (or at least one vcpu in each virtual core).
    
    [paulus@samba.org - removed H_SET_MODE implementation from original
    version of the patch, and made kvmppc_set_lpcr update vcpu->arch.intr_msr.]
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 239a857f1141..a60a2fdae5bd 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -480,6 +480,7 @@ int main(void)
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 	DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
+	DEFINE(VCPU_INTR_MSR, offsetof(struct kvm_vcpu, arch.intr_msr));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));

commit 8563bf52d509213e746295341ab52896b562ca5e
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jan 8 21:25:29 2014 +1100

    KVM: PPC: Book3S HV: Add support for DABRX register on POWER7
    
    The DABRX (DABR extension) register on POWER7 processors provides finer
    control over which accesses cause a data breakpoint interrupt.  It
    contains 3 bits which indicate whether to enable accesses in user,
    kernel and hypervisor modes respectively to cause data breakpoint
    interrupts, plus one bit that enables both real mode and virtual mode
    accesses to cause interrupts.  Currently, KVM sets DABRX to allow
    both kernel and user accesses to cause interrupts while in the guest.
    
    This adds support for the guest to specify other values for DABRX.
    PAPR defines a H_SET_XDABR hcall to allow the guest to set both DABR
    and DABRX with one call.  This adds a real-mode implementation of
    H_SET_XDABR, which shares most of its code with the existing H_SET_DABR
    implementation.  To support this, we add a per-vcpu field to store the
    DABRX value plus code to get and set it via the ONE_REG interface.
    
    For Linux guests to use this new hcall, userspace needs to add
    "hcall-xdabr" to the set of strings in the /chosen/hypertas-functions
    property in the device tree.  If userspace does this and then migrates
    the guest to a host where the kernel doesn't include this patch, then
    userspace will need to implement H_SET_XDABR by writing the specified
    DABR value to the DABR using the ONE_REG interface.  In that case, the
    old kernel will set DABRX to DABRX_USER | DABRX_KERNEL.  That should
    still work correctly, at least for Linux guests, since Linux guests
    cope with getting data breakpoint interrupts in modes that weren't
    requested by just ignoring the interrupt, and Linux guests never set
    DABRX_BTI.
    
    The other thing this does is to make H_SET_DABR and H_SET_XDABR work
    on POWER8, which has the DAWR and DAWRX instead of DABR/X.  Guests that
    know about POWER8 should use H_SET_MODE rather than H_SET_[X]DABR, but
    guests running in POWER7 compatibility mode will still use H_SET_[X]DABR.
    For them, this adds the logic to convert DABR/X values into DAWR/X values
    on POWER8.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 043900abbbb0..239a857f1141 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -493,6 +493,7 @@ int main(void)
 	DEFINE(VCPU_IAMR, offsetof(struct kvm_vcpu, arch.iamr));
 	DEFINE(VCPU_CTRL, offsetof(struct kvm_vcpu, arch.ctrl));
 	DEFINE(VCPU_DABR, offsetof(struct kvm_vcpu, arch.dabr));
+	DEFINE(VCPU_DABRX, offsetof(struct kvm_vcpu, arch.dabrx));
 	DEFINE(VCPU_DAWR, offsetof(struct kvm_vcpu, arch.dawr));
 	DEFINE(VCPU_DAWRX, offsetof(struct kvm_vcpu, arch.dawrx));
 	DEFINE(VCPU_CIABR, offsetof(struct kvm_vcpu, arch.ciabr));

commit b005255e12a311d2c87ea70a7c7b192b2187c22c
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jan 8 21:25:21 2014 +1100

    KVM: PPC: Book3S HV: Context-switch new POWER8 SPRs
    
    This adds fields to the struct kvm_vcpu_arch to store the new
    guest-accessible SPRs on POWER8, adds code to the get/set_one_reg
    functions to allow userspace to access this state, and adds code to
    the guest entry and exit to context-switch these SPRs between host
    and guest.
    
    Note that DPDES (Directed Privileged Doorbell Exception State) is
    shared between threads on a core; hence we store it in struct
    kvmppc_vcore and have the master thread save and restore it.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 332ae66883e4..043900abbbb0 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -432,6 +432,7 @@ int main(void)
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
 	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
+	DEFINE(VCPU_TAR, offsetof(struct kvm_vcpu, arch.tar));
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
@@ -484,11 +485,17 @@ int main(void)
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
 	DEFINE(VCPU_PURR, offsetof(struct kvm_vcpu, arch.purr));
 	DEFINE(VCPU_SPURR, offsetof(struct kvm_vcpu, arch.spurr));
+	DEFINE(VCPU_IC, offsetof(struct kvm_vcpu, arch.ic));
+	DEFINE(VCPU_VTB, offsetof(struct kvm_vcpu, arch.vtb));
 	DEFINE(VCPU_DSCR, offsetof(struct kvm_vcpu, arch.dscr));
 	DEFINE(VCPU_AMR, offsetof(struct kvm_vcpu, arch.amr));
 	DEFINE(VCPU_UAMOR, offsetof(struct kvm_vcpu, arch.uamor));
+	DEFINE(VCPU_IAMR, offsetof(struct kvm_vcpu, arch.iamr));
 	DEFINE(VCPU_CTRL, offsetof(struct kvm_vcpu, arch.ctrl));
 	DEFINE(VCPU_DABR, offsetof(struct kvm_vcpu, arch.dabr));
+	DEFINE(VCPU_DAWR, offsetof(struct kvm_vcpu, arch.dawr));
+	DEFINE(VCPU_DAWRX, offsetof(struct kvm_vcpu, arch.dawrx));
+	DEFINE(VCPU_CIABR, offsetof(struct kvm_vcpu, arch.ciabr));
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
 	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
 	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
@@ -497,8 +504,10 @@ int main(void)
 	DEFINE(VCPU_PRODDED, offsetof(struct kvm_vcpu, arch.prodded));
 	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
 	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
+	DEFINE(VCPU_SPMC, offsetof(struct kvm_vcpu, arch.spmc));
 	DEFINE(VCPU_SIAR, offsetof(struct kvm_vcpu, arch.siar));
 	DEFINE(VCPU_SDAR, offsetof(struct kvm_vcpu, arch.sdar));
+	DEFINE(VCPU_SIER, offsetof(struct kvm_vcpu, arch.sier));
 	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));
 	DEFINE(VCPU_SLB_MAX, offsetof(struct kvm_vcpu, arch.slb_max));
 	DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));
@@ -508,6 +517,19 @@ int main(void)
 	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
 	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
 	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
+	DEFINE(VCPU_FSCR, offsetof(struct kvm_vcpu, arch.fscr));
+	DEFINE(VCPU_PSPB, offsetof(struct kvm_vcpu, arch.pspb));
+	DEFINE(VCPU_TFHAR, offsetof(struct kvm_vcpu, arch.tfhar));
+	DEFINE(VCPU_TFIAR, offsetof(struct kvm_vcpu, arch.tfiar));
+	DEFINE(VCPU_TEXASR, offsetof(struct kvm_vcpu, arch.texasr));
+	DEFINE(VCPU_EBBHR, offsetof(struct kvm_vcpu, arch.ebbhr));
+	DEFINE(VCPU_EBBRR, offsetof(struct kvm_vcpu, arch.ebbrr));
+	DEFINE(VCPU_BESCR, offsetof(struct kvm_vcpu, arch.bescr));
+	DEFINE(VCPU_CSIGR, offsetof(struct kvm_vcpu, arch.csigr));
+	DEFINE(VCPU_TACR, offsetof(struct kvm_vcpu, arch.tacr));
+	DEFINE(VCPU_TCSCR, offsetof(struct kvm_vcpu, arch.tcscr));
+	DEFINE(VCPU_ACOP, offsetof(struct kvm_vcpu, arch.acop));
+	DEFINE(VCPU_WORT, offsetof(struct kvm_vcpu, arch.wort));
 	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
 	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
@@ -517,6 +539,7 @@ int main(void)
 	DEFINE(VCORE_TB_OFFSET, offsetof(struct kvmppc_vcore, tb_offset));
 	DEFINE(VCORE_LPCR, offsetof(struct kvmppc_vcore, lpcr));
 	DEFINE(VCORE_PCR, offsetof(struct kvmppc_vcore, pcr));
+	DEFINE(VCORE_DPDES, offsetof(struct kvmppc_vcore, dpdes));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
 	DEFINE(VCPU_SLB_V, offsetof(struct kvmppc_slb, origv));
 	DEFINE(VCPU_SLB_SIZE, sizeof(struct kvmppc_slb));

commit e0b7ec058c0eb7ba8d5d937d81de2bd16db6970e
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jan 8 21:25:20 2014 +1100

    KVM: PPC: Book3S HV: Align physical and virtual CPU thread numbers
    
    On a threaded processor such as POWER7, we group VCPUs into virtual
    cores and arrange that the VCPUs in a virtual core run on the same
    physical core.  Currently we don't enforce any correspondence between
    virtual thread numbers within a virtual core and physical thread
    numbers.  Physical threads are allocated starting at 0 on a first-come
    first-served basis to runnable virtual threads (VCPUs).
    
    POWER8 implements a new "msgsndp" instruction which guest kernels can
    use to interrupt other threads in the same core or sub-core.  Since
    the instruction takes the destination physical thread ID as a parameter,
    it becomes necessary to align the physical thread IDs with the virtual
    thread IDs, that is, to make sure virtual thread N within a virtual
    core always runs on physical thread N.
    
    This means that it's possible that thread 0, which is where we call
    __kvmppc_vcore_entry, may end up running some other vcpu than the
    one whose task called kvmppc_run_core(), or it may end up running
    no vcpu at all, if for example thread 0 of the virtual core is
    currently executing in userspace.  However, we do need thread 0
    to be responsible for switching the MMU -- a previous version of
    this patch that had other threads switching the MMU was found to
    be responsible for occasional memory corruption and machine check
    interrupts in the guest on POWER7 machines.
    
    To accommodate this, we no longer pass the vcpu pointer to
    __kvmppc_vcore_entry, but instead let the assembly code load it from
    the PACA.  Since the assembly code will need to know the kvm pointer
    and the thread ID for threads which don't have a vcpu, we move the
    thread ID into the PACA and we add a kvm pointer to the virtual core
    structure.
    
    In the case where thread 0 has no vcpu to run, it still calls into
    kvmppc_hv_entry in order to do the MMU switch, and then naps until
    either its vcpu is ready to run in the guest, or some other thread
    needs to exit the guest.  In the latter case, thread 0 jumps to the
    code that switches the MMU back to the host.  This control flow means
    that now we switch the MMU before loading any guest vcpu state.
    Similarly, on guest exit we now save all the guest vcpu state before
    switching the MMU back to the host.  This has required substantial
    code movement, making the diff rather large.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 5e64c3d2149f..332ae66883e4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -506,7 +506,6 @@ int main(void)
 	DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
-	DEFINE(VCPU_PTID, offsetof(struct kvm_vcpu, arch.ptid));
 	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
 	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
 	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
@@ -514,6 +513,7 @@ int main(void)
 	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
+	DEFINE(VCORE_KVM, offsetof(struct kvmppc_vcore, kvm));
 	DEFINE(VCORE_TB_OFFSET, offsetof(struct kvmppc_vcore, tb_offset));
 	DEFINE(VCORE_LPCR, offsetof(struct kvmppc_vcore, lpcr));
 	DEFINE(VCORE_PCR, offsetof(struct kvmppc_vcore, pcr));
@@ -583,6 +583,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_XICS_PHYS, xics_phys);
 	HSTATE_FIELD(HSTATE_SAVED_XIRR, saved_xirr);
 	HSTATE_FIELD(HSTATE_HOST_IPI, host_ipi);
+	HSTATE_FIELD(HSTATE_PTID, ptid);
 	HSTATE_FIELD(HSTATE_MMCR, host_mmcr);
 	HSTATE_FIELD(HSTATE_PMC, host_pmc);
 	HSTATE_FIELD(HSTATE_PURR, host_purr);

commit 28efc35fe68dacbddc4b12c2fa8f2df1593a4ad3
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Oct 11 19:22:38 2013 -0500

    powerpc/e6500: TLB miss handler with hardware tablewalk support
    
    There are a few things that make the existing hw tablewalk handlers
    unsuitable for e6500:
    
     - Indirect entries go in TLB1 (though the resulting direct entries go in
       TLB0).
    
     - It has threads, but no "tlbsrx." -- so we need a spinlock and
       a normal "tlbsx".  Because we need this lock, hardware tablewalk
       is mandatory on e6500 unless we want to add spinlock+tlbsx to
       the normal bolted TLB miss handler.
    
     - TLB1 has no HES (nor next-victim hint) so we need software round robin
       (TODO: integrate this round robin data with hugetlb/KVM)
    
     - The existing tablewalk handlers map half of a page table at a time,
       because IBM hardware has a fixed 1MiB indirect page size.  e6500
       has variable size indirect entries, with a minimum of 2MiB.
       So we can't do the half-page indirect mapping, and even if we
       could it would be less efficient than mapping the full page.
    
     - Like on e5500, the linear mapping is bolted, so we don't need the
       overhead of supporting nested tlb misses.
    
    Note that hardware tablewalk does not work in rev1 of e6500.
    We do not expect to support e6500 rev1 in mainline Linux.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Mihai Caraman <mihai.caraman@freescale.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 41a283956a29..ed8d68ce71f3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -203,6 +203,15 @@ int main(void)
 	DEFINE(PACA_MC_STACK, offsetof(struct paca_struct, mc_kstack));
 	DEFINE(PACA_CRIT_STACK, offsetof(struct paca_struct, crit_kstack));
 	DEFINE(PACA_DBG_STACK, offsetof(struct paca_struct, dbg_kstack));
+	DEFINE(PACA_TCD_PTR, offsetof(struct paca_struct, tcd_ptr));
+
+	DEFINE(TCD_ESEL_NEXT,
+		offsetof(struct tlb_core_data, esel_next));
+	DEFINE(TCD_ESEL_MAX,
+		offsetof(struct tlb_core_data, esel_max));
+	DEFINE(TCD_ESEL_FIRST,
+		offsetof(struct tlb_core_data, esel_first));
+	DEFINE(TCD_LOCK, offsetof(struct tlb_core_data, lock));
 #endif /* CONFIG_PPC_BOOK3E */
 
 #ifdef CONFIG_PPC_STD_MMU_64

commit 595e4f7e697e2e6fb252f6be6a83d5b9460b59a3
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 15 20:43:04 2013 +1100

    KVM: PPC: Book3S HV: Use load/store_fp_state functions in HV guest entry/exit
    
    This modifies kvmppc_load_fp and kvmppc_save_fp to use the generic
    FP/VSX and VMX load/store functions instead of open-coding the
    FP/VSX/VMX load/store instructions.  Since kvmppc_load/save_fp don't
    follow C calling conventions, we make them private symbols within
    book3s_hv_rmhandlers.S.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8403f9031d93..5e64c3d2149f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -426,10 +426,8 @@ int main(void)
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_VRSAVE, offsetof(struct kvm_vcpu, arch.vrsave));
 	DEFINE(VCPU_FPRS, offsetof(struct kvm_vcpu, arch.fp.fpr));
-	DEFINE(VCPU_FPSCR, offsetof(struct kvm_vcpu, arch.fp.fpscr));
 #ifdef CONFIG_ALTIVEC
 	DEFINE(VCPU_VRS, offsetof(struct kvm_vcpu, arch.vr.vr));
-	DEFINE(VCPU_VSCR, offsetof(struct kvm_vcpu, arch.vr.vscr));
 #endif
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
 	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));

commit efff19122315f1431f6b02cd2983b15f5d3957bd
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 15 20:43:02 2013 +1100

    KVM: PPC: Store FP/VSX/VMX state in thread_fp/vr_state structures
    
    This uses struct thread_fp_state and struct thread_vr_state to store
    the floating-point, VMX/Altivec and VSX state, rather than flat arrays.
    This makes transferring the state to/from the thread_struct simpler
    and allows us to unify the get/set_one_reg implementations for the
    VSX registers.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2ea5cc033ec8..8403f9031d93 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -425,14 +425,11 @@ int main(void)
 	DEFINE(VCPU_GUEST_PID, offsetof(struct kvm_vcpu, arch.pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_VRSAVE, offsetof(struct kvm_vcpu, arch.vrsave));
-	DEFINE(VCPU_FPRS, offsetof(struct kvm_vcpu, arch.fpr));
-	DEFINE(VCPU_FPSCR, offsetof(struct kvm_vcpu, arch.fpscr));
+	DEFINE(VCPU_FPRS, offsetof(struct kvm_vcpu, arch.fp.fpr));
+	DEFINE(VCPU_FPSCR, offsetof(struct kvm_vcpu, arch.fp.fpscr));
 #ifdef CONFIG_ALTIVEC
-	DEFINE(VCPU_VRS, offsetof(struct kvm_vcpu, arch.vr));
-	DEFINE(VCPU_VSCR, offsetof(struct kvm_vcpu, arch.vscr));
-#endif
-#ifdef CONFIG_VSX
-	DEFINE(VCPU_VSRS, offsetof(struct kvm_vcpu, arch.vsr));
+	DEFINE(VCPU_VRS, offsetof(struct kvm_vcpu, arch.vr.vr));
+	DEFINE(VCPU_VSCR, offsetof(struct kvm_vcpu, arch.vr.vscr));
 #endif
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
 	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));

commit 36e7bb38028d3d812aa7749208249d600a30c22c
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Nov 11 19:29:47 2013 +0530

    powerpc: book3s: kvm: Don't abuse host r2 in exit path
    
    We don't use PACATOC for PR. Avoid updating HOST_R2 with PR
    KVM mode when both HV and PR are enabled in the kernel. Without this we
    get the below crash
    
    (qemu)
    Unable to handle kernel paging request for data at address 0xffffffffffff8310
    Faulting instruction address: 0xc00000000001d5a4
    cpu 0x2: Vector: 300 (Data Access) at [c0000001dc53aef0]
        pc: c00000000001d5a4: .vtime_delta.isra.1+0x34/0x1d0
        lr: c00000000001d760: .vtime_account_system+0x20/0x60
        sp: c0000001dc53b170
       msr: 8000000000009032
       dar: ffffffffffff8310
     dsisr: 40000000
      current = 0xc0000001d76c62d0
      paca    = 0xc00000000fef1100   softe: 0        irq_happened: 0x01
        pid   = 4472, comm = qemu-system-ppc
    enter ? for help
    [c0000001dc53b200] c00000000001d760 .vtime_account_system+0x20/0x60
    [c0000001dc53b290] c00000000008d050 .kvmppc_handle_exit_pr+0x60/0xa50
    [c0000001dc53b340] c00000000008f51c kvm_start_lightweight+0xb4/0xc4
    [c0000001dc53b510] c00000000008cdf0 .kvmppc_vcpu_run_pr+0x150/0x2e0
    [c0000001dc53b9e0] c00000000008341c .kvmppc_vcpu_run+0x2c/0x40
    [c0000001dc53ba50] c000000000080af4 .kvm_arch_vcpu_ioctl_run+0x54/0x1b0
    [c0000001dc53bae0] c00000000007b4c8 .kvm_vcpu_ioctl+0x478/0x730
    [c0000001dc53bca0] c0000000002140cc .do_vfs_ioctl+0x4ac/0x770
    [c0000001dc53bd80] c0000000002143e8 .SyS_ioctl+0x58/0xb0
    [c0000001dc53be30] c000000000009e58 syscall_exit+0x0/0x98
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2ea5cc033ec8..d3de01066f7d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -576,6 +576,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_VMHANDLER, vmhandler);
 	HSTATE_FIELD(HSTATE_SCRATCH0, scratch0);
 	HSTATE_FIELD(HSTATE_SCRATCH1, scratch1);
+	HSTATE_FIELD(HSTATE_SCRATCH2, scratch2);
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
 	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
 	HSTATE_FIELD(HSTATE_NAPPING, napping);

commit 1e9b4507ed98457edb8a892934282b8f63e17246
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:04:08 2013 +0530

    powerpc/book3s: handle machine check in Linux host.
    
    Move machine check entry point into Linux. So far we were dependent on
    firmware to decode MCE error details and handover the high level info to OS.
    
    This patch introduces early machine check routine that saves the MCE
    information (srr1, srr0, dar and dsisr) to the emergency stack. We allocate
    stack frame on emergency stack and set the r1 accordingly. This allows us to be
    prepared to take another exception without loosing context. One thing to note
    here that, if we get another machine check while ME bit is off then we risk a
    checkstop. Hence we restrict ourselves to save only MCE information and
    register saved on PACA_EXMC save are before we turn the ME bit on. We use
    paca->in_mce flag to differentiate between first entry and nested machine check
    entry which helps proper use of emergency stack. We increment paca->in_mce
    every time we enter in early machine check handler and decrement it while
    leaving. When we enter machine check early handler first time (paca->in_mce ==
    0), we are sure nobody is using MC emergency stack and allocate a stack frame
    at the start of the emergency stack. During subsequent entry (paca->in_mce >
    0), we know that r1 points inside emergency stack and we allocate separate
    stack frame accordingly. This prevents us from clobbering MCE information
    during nested machine checks.
    
    The early machine check handler changes are placed under CPU_FTR_HVMODE
    section. This makes sure that the early machine check handler will get executed
    only in hypervisor kernel.
    
    This is the code flow:
    
                    Machine Check Interrupt
                            |
                            V
                       0x200 vector                           ME=0, IR=0, DR=0
                            |
                            V
            +-----------------------------------------------+
            |machine_check_pSeries_early:                   | ME=0, IR=0, DR=0
            |       Alloc frame on emergency stack          |
            |       Save srr1, srr0, dar and dsisr on stack |
            +-----------------------------------------------+
                            |
                    (ME=1, IR=0, DR=0, RFID)
                            |
                            V
                    machine_check_handle_early                ME=1, IR=0, DR=0
                            |
                            V
            +-----------------------------------------------+
            |       machine_check_early (r3=pt_regs)        | ME=1, IR=0, DR=0
            |       Things to do: (in next patches)         |
            |               Flush SLB for SLB errors        |
            |               Flush TLB for TLB errors        |
            |               Decode and save MCE info        |
            +-----------------------------------------------+
                            |
            (Fall through existing exception handler routine.)
                            |
                            V
                    machine_check_pSerie                      ME=1, IR=0, DR=0
                            |
                    (ME=1, IR=1, DR=1, RFID)
                            |
                            V
                    machine_check_common                      ME=1, IR=1, DR=1
                            .
                            .
                            .
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2ea5cc033ec8..41a283956a29 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -232,6 +232,10 @@ int main(void)
 	DEFINE(PACA_DTL_RIDX, offsetof(struct paca_struct, dtl_ridx));
 #endif /* CONFIG_PPC_STD_MMU_64 */
 	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
+#ifdef CONFIG_PPC_BOOK3S_64
+	DEFINE(PACAMCEMERGSP, offsetof(struct paca_struct, mc_emergency_sp));
+	DEFINE(PACA_IN_MCE, offsetof(struct paca_struct, in_mce));
+#endif
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
 	DEFINE(PACAKEXECSTATE, offsetof(struct paca_struct, kexec_state));
 	DEFINE(PACA_STARTTIME, offsetof(struct paca_struct, starttime));

commit f080480488028bcc25357f85e8ae54ccc3bb7173
Merge: eda670c626a4 e504c9098ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 15 13:51:36 2013 +0900

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Paolo Bonzini:
     "Here are the 3.13 KVM changes.  There was a lot of work on the PPC
      side: the HV and emulation flavors can now coexist in a single kernel
      is probably the most interesting change from a user point of view.
    
      On the x86 side there are nested virtualization improvements and a few
      bugfixes.
    
      ARM got transparent huge page support, improved overcommit, and
      support for big endian guests.
    
      Finally, there is a new interface to connect KVM with VFIO.  This
      helps with devices that use NoSnoop PCI transactions, letting the
      driver in the guest execute WBINVD instructions.  This includes some
      nVidia cards on Windows, that fail to start without these patches and
      the corresponding userspace changes"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (146 commits)
      kvm, vmx: Fix lazy FPU on nested guest
      arm/arm64: KVM: PSCI: propagate caller endianness to the incoming vcpu
      arm/arm64: KVM: MMIO support for BE guest
      kvm, cpuid: Fix sparse warning
      kvm: Delete prototype for non-existent function kvm_check_iopl
      kvm: Delete prototype for non-existent function complete_pio
      hung_task: add method to reset detector
      pvclock: detect watchdog reset at pvclock read
      kvm: optimize out smp_mb after srcu_read_unlock
      srcu: API for barrier after srcu read unlock
      KVM: remove vm mmap method
      KVM: IOMMU: hva align mapping page size
      KVM: x86: trace cpuid emulation when called from emulator
      KVM: emulator: cleanup decode_register_operand() a bit
      KVM: emulator: check rex prefix inside decode_register()
      KVM: x86: fix emulation of "movzbl %bpl, %eax"
      kvm_host: typo fix
      KVM: x86: emulate SAHF instruction
      MAINTAINERS: add tree for kvm.git
      Documentation/kvm: add a 00-INDEX file
      ...

commit 95f328d3ad1a8e4e3175a18546fb35c495e31130
Merge: daf727225b8a a78b55d1c021
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Nov 4 10:20:57 2013 +0200

    Merge branch 'kvm-ppc-queue' of git://github.com/agraf/linux-2.6 into queue
    
    Conflicts:
            arch/powerpc/include/asm/processor.h

commit 51ae8d4a2b9e4aa9a502061b9c39168e08829b94
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Thu Jul 4 11:45:46 2013 +0530

    powerpc: move debug registers in a structure
    
    This way we can use same data type struct with KVM and
    also help in using other debug related function.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    [scottwood@freescale.com: removed obvious debug_reg comment]
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6278edddc3f8..e60a3697932c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -115,7 +115,7 @@ int main(void)
 #endif /* CONFIG_SPE */
 #endif /* CONFIG_PPC64 */
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
-	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
+	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, debug.dbcr0));
 #endif
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	DEFINE(THREAD_KVM_SVCPU, offsetof(struct thread_struct, kvm_shadow_vcpu));

commit 9975f5e3692d320b4259a4d2edd8a979adb1e535
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:52 2013 +0530

    kvm: powerpc: book3s: Add a new config variable CONFIG_KVM_BOOK3S_HV_POSSIBLE
    
    This help ups to select the relevant code in the kernel code
    when we later move HV and PR bits as seperate modules. The patch
    also makes the config options for PR KVM selectable
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 29796559c2fb..1fbb2b63195c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -445,7 +445,7 @@ int main(void)
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	DEFINE(VCPU_MSR, offsetof(struct kvm_vcpu, arch.shregs.msr));
 	DEFINE(VCPU_SRR0, offsetof(struct kvm_vcpu, arch.shregs.srr0));
 	DEFINE(VCPU_SRR1, offsetof(struct kvm_vcpu, arch.shregs.srr1));
@@ -476,7 +476,7 @@ int main(void)
 	DEFINE(KVM_LPID, offsetof(struct kvm, arch.lpid));
 
 	/* book3s */
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	DEFINE(KVM_SDR1, offsetof(struct kvm, arch.sdr1));
 	DEFINE(KVM_HOST_LPID, offsetof(struct kvm, arch.host_lpid));
 	DEFINE(KVM_HOST_LPCR, offsetof(struct kvm, arch.host_lpcr));
@@ -586,7 +586,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
 	HSTATE_FIELD(HSTATE_NAPPING, napping);
 
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	HSTATE_FIELD(HSTATE_HWTHREAD_REQ, hwthread_req);
 	HSTATE_FIELD(HSTATE_HWTHREAD_STATE, hwthread_state);
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);
@@ -602,7 +602,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_DABR, dabr);
 	HSTATE_FIELD(HSTATE_DECEXP, dec_expires);
 	DEFINE(IPI_PRIORITY, IPI_PRIORITY);
-#endif /* CONFIG_KVM_BOOK3S_64_HV */
+#endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 
 #ifdef CONFIG_PPC_BOOK3S_64
 	HSTATE_FIELD(HSTATE_CFAR, cfar);

commit 7aa79938f7d76f5865d0b2a2d9bbe2337560261f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:51 2013 +0530

    kvm: powerpc: book3s: pr: Rename KVM_BOOK3S_PR to KVM_BOOK3S_PR_POSSIBLE
    
    With later patches supporting PR kvm as a kernel module, the changes
    that has to be built into the main kernel binary to enable PR KVM module
    is now selected via KVM_BOOK3S_PR_POSSIBLE
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 95ba8095fc4a..29796559c2fb 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -533,7 +533,7 @@ int main(void)
 	DEFINE(VCPU_SLB_SIZE, sizeof(struct kvmppc_slb));
 
 #ifdef CONFIG_PPC_BOOK3S_64
-#ifdef CONFIG_KVM_BOOK3S_PR
+#ifdef CONFIG_KVM_BOOK3S_PR_POSSIBLE
 	DEFINE(PACA_SVCPU, offsetof(struct paca_struct, shadow_vcpu));
 # define SVCPU_FIELD(x, f)	DEFINE(x, offsetof(struct paca_struct, shadow_vcpu.f))
 #else

commit 95791988fec645d196e746fcc0e329e19f7b1347
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Jun 26 11:12:22 2013 +0530

    powerpc: move debug registers in a structure
    
    This way we can use same data type struct with KVM and
    also help in using other debug related function.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index fda7f4020a33..95ba8095fc4a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -113,7 +113,7 @@ int main(void)
 #endif /* CONFIG_SPE */
 #endif /* CONFIG_PPC64 */
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
-	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
+	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, debug.dbcr0));
 #endif
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	DEFINE(THREAD_KVM_SVCPU, offsetof(struct thread_struct, kvm_shadow_vcpu));

commit a2d56020d1d91934e7bb3e7c8a5a3b5921ce121b
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:43 2013 +1000

    KVM: PPC: Book3S PR: Keep volatile reg values in vcpu rather than shadow_vcpu
    
    Currently PR-style KVM keeps the volatile guest register values
    (R0 - R13, CR, LR, CTR, XER, PC) in a shadow_vcpu struct rather than
    the main kvm_vcpu struct.  For 64-bit, the shadow_vcpu exists in two
    places, a kmalloc'd struct and in the PACA, and it gets copied back
    and forth in kvmppc_core_vcpu_load/put(), because the real-mode code
    can't rely on being able to access the kmalloc'd struct.
    
    This changes the code to copy the volatile values into the shadow_vcpu
    as one of the last things done before entering the guest.  Similarly
    the values are copied back out of the shadow_vcpu to the kvm_vcpu
    immediately after exiting the guest.  We arrange for interrupts to be
    still disabled at this point so that we can't get preempted on 64-bit
    and end up copying values from the wrong PACA.
    
    This means that the accessor functions in kvm_book3s.h for these
    registers are greatly simplified, and are same between PR and HV KVM.
    In places where accesses to shadow_vcpu fields are now replaced by
    accesses to the kvm_vcpu, we can also remove the svcpu_get/put pairs.
    Finally, on 64-bit, we don't need the kmalloc'd struct at all any more.
    
    With this, the time to read the PVR one million times in a loop went
    from 567.7ms to 575.5ms (averages of 6 values), an increase of about
    1.4% for this worse-case test for guest entries and exits.  The
    standard deviation of the measurements is about 11ms, so the
    difference is only marginally significant statistically.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 5a285efba174..fda7f4020a33 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -520,6 +520,7 @@ int main(void)
 	DEFINE(VCPU_PTID, offsetof(struct kvm_vcpu, arch.ptid));
 	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
 	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
+	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
 	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
@@ -527,14 +528,13 @@ int main(void)
 	DEFINE(VCORE_TB_OFFSET, offsetof(struct kvmppc_vcore, tb_offset));
 	DEFINE(VCORE_LPCR, offsetof(struct kvmppc_vcore, lpcr));
 	DEFINE(VCORE_PCR, offsetof(struct kvmppc_vcore, pcr));
-	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
-			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
 	DEFINE(VCPU_SLB_V, offsetof(struct kvmppc_slb, origv));
 	DEFINE(VCPU_SLB_SIZE, sizeof(struct kvmppc_slb));
 
 #ifdef CONFIG_PPC_BOOK3S_64
 #ifdef CONFIG_KVM_BOOK3S_PR
+	DEFINE(PACA_SVCPU, offsetof(struct paca_struct, shadow_vcpu));
 # define SVCPU_FIELD(x, f)	DEFINE(x, offsetof(struct paca_struct, shadow_vcpu.f))
 #else
 # define SVCPU_FIELD(x, f)

commit 388cc6e133132e6c9b64e7d5361114a3a7d57663
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Sep 21 14:35:02 2013 +1000

    KVM: PPC: Book3S HV: Support POWER6 compatibility mode on POWER7
    
    This enables us to use the Processor Compatibility Register (PCR) on
    POWER7 to put the processor into architecture 2.05 compatibility mode
    when running a guest.  In this mode the new instructions and registers
    that were introduced on POWER7 are disabled in user mode.  This
    includes all the VSX facilities plus several other instructions such
    as ldbrx, stdbrx, popcntw, popcntd, etc.
    
    To select this mode, we have a new register accessible through the
    set/get_one_reg interface, called KVM_REG_PPC_ARCH_COMPAT.  Setting
    this to zero gives the full set of capabilities of the processor.
    Setting it to one of the "logical" PVR values defined in PAPR puts
    the vcpu into the compatibility mode for the corresponding
    architecture level.  The supported values are:
    
    0x0f000002      Architecture 2.05 (POWER6)
    0x0f000003      Architecture 2.06 (POWER7)
    0x0f100003      Architecture 2.06+ (POWER7+)
    
    Since the PCR is per-core, the architecture compatibility level and
    the corresponding PCR value are stored in the struct kvmppc_vcore, and
    are therefore shared between all vcpus in a virtual core.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: squash in fix to add missing break statements and documentation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 5fda4ef489ad..5a285efba174 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -526,6 +526,7 @@ int main(void)
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
 	DEFINE(VCORE_TB_OFFSET, offsetof(struct kvmppc_vcore, tb_offset));
 	DEFINE(VCORE_LPCR, offsetof(struct kvmppc_vcore, lpcr));
+	DEFINE(VCORE_PCR, offsetof(struct kvmppc_vcore, pcr));
 	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
 			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));

commit 4b8473c9c19dff1b0c672f182cc50b9952cf42e7
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:39 2013 +1000

    KVM: PPC: Book3S HV: Add support for guest Program Priority Register
    
    POWER7 and later IBM server processors have a register called the
    Program Priority Register (PPR), which controls the priority of
    each hardware CPU SMT thread, and affects how fast it runs compared
    to other SMT threads.  This priority can be controlled by writing to
    the PPR or by use of a set of instructions of the form or rN,rN,rN
    which are otherwise no-ops but have been defined to set the priority
    to particular levels.
    
    This adds code to context switch the PPR when entering and exiting
    guests and to make the PPR value accessible through the SET/GET_ONE_REG
    interface.  When entering the guest, we set the PPR as late as
    possible, because if we are setting a low thread priority it will
    make the code run slowly from that point on.  Similarly, the
    first-level interrupt handlers save the PPR value in the PACA very
    early on, and set the thread priority to the medium level, so that
    the interrupt handling code runs at a reasonable speed.
    
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index fd7513f8014b..5fda4ef489ad 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -519,6 +519,7 @@ int main(void)
 	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
 	DEFINE(VCPU_PTID, offsetof(struct kvm_vcpu, arch.ptid));
 	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
+	DEFINE(VCPU_PPR, offsetof(struct kvm_vcpu, arch.ppr));
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
 	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
@@ -604,6 +605,7 @@ int main(void)
 
 #ifdef CONFIG_PPC_BOOK3S_64
 	HSTATE_FIELD(HSTATE_CFAR, cfar);
+	HSTATE_FIELD(HSTATE_PPR, ppr);
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 #else /* CONFIG_PPC_BOOK3S */

commit a0144e2a6b0b4a137a32f0102354782547bf0935
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 20 14:52:38 2013 +1000

    KVM: PPC: Book3S HV: Store LPCR value for each virtual core
    
    This adds the ability to have a separate LPCR (Logical Partitioning
    Control Register) value relating to a guest for each virtual core,
    rather than only having a single value for the whole VM.  This
    corresponds to what real POWER hardware does, where there is a LPCR
    per CPU thread but most of the fields are required to have the same
    value on all active threads in a core.
    
    The per-virtual-core LPCR can be read and written using the
    GET/SET_ONE_REG interface.  Userspace can can only modify the
    following fields of the LPCR value:
    
    DPFD    Default prefetch depth
    ILE     Interrupt little-endian
    TC      Translation control (secondary HPT hash group search disable)
    
    We still maintain a per-VM default LPCR value in kvm->arch.lpcr, which
    contains bits relating to memory management, i.e. the Virtualized
    Partition Memory (VPM) bits and the bits relating to guest real mode.
    When this default value is updated, the update needs to be propagated
    to the per-vcore values, so we add a kvmppc_update_lpcr() helper to do
    that.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix whitespace]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 34d63d871917..fd7513f8014b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -524,6 +524,7 @@ int main(void)
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
 	DEFINE(VCORE_TB_OFFSET, offsetof(struct kvmppc_vcore, tb_offset));
+	DEFINE(VCORE_LPCR, offsetof(struct kvmppc_vcore, lpcr));
 	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
 			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));

commit 93b0f4dc29c5f077a1c97bd1d484147230c3779a
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 6 13:17:46 2013 +1000

    KVM: PPC: Book3S HV: Implement timebase offset for guests
    
    This allows guests to have a different timebase origin from the host.
    This is needed for migration, where a guest can migrate from one host
    to another and the two hosts might have a different timebase origin.
    However, the timebase seen by the guest must not go backwards, and
    should go forwards only by a small amount corresponding to the time
    taken for the migration.
    
    Therefore this provides a new per-vcpu value accessed via the one_reg
    interface using the new KVM_REG_PPC_TB_OFFSET identifier.  This value
    defaults to 0 and is not modified by KVM.  On entering the guest, this
    value is added onto the timebase, and on exiting the guest, it is
    subtracted from the timebase.
    
    This is only supported for recent POWER hardware which has the TBU40
    (timebase upper 40 bits) register.  Writing to the TBU40 register only
    alters the upper 40 bits of the timebase, leaving the lower 24 bits
    unchanged.  This provides a way to modify the timebase for guest
    migration without disturbing the synchronization of the timebase
    registers across CPU cores.  The kernel rounds up the value given
    to a multiple of 2^24.
    
    Timebase values stored in KVM structures (struct kvm_vcpu, struct
    kvmppc_vcore, etc.) are stored as host timebase values.  The timebase
    values in the dispatch trace log need to be guest timebase values,
    however, since that is read directly by the guest.  This moves the
    setting of vcpu->arch.dec_expires on guest exit to a point after we
    have restored the host timebase so that vcpu->arch.dec_expires is a
    host timebase value.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 19e699d747b7..34d63d871917 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -523,6 +523,7 @@ int main(void)
 	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
+	DEFINE(VCORE_TB_OFFSET, offsetof(struct kvmppc_vcore, tb_offset));
 	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
 			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));

commit 14941789f2a13cd89e2dd567c4f708e571ab714e
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Sep 6 13:11:18 2013 +1000

    KVM: PPC: Book3S HV: Save/restore SIAR and SDAR along with other PMU registers
    
    Currently we are not saving and restoring the SIAR and SDAR registers in
    the PMU (performance monitor unit) on guest entry and exit.  The result
    is that performance monitoring tools in the guest could get false
    information about where a program was executing and what data it was
    accessing at the time of a performance monitor interrupt.  This fixes
    it by saving and restoring these registers along with the other PMU
    registers on guest entry/exit.
    
    This also provides a way for userspace to access these values for a
    vcpu via the one_reg interface.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index d8958be5f31a..19e699d747b7 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -508,6 +508,8 @@ int main(void)
 	DEFINE(VCPU_PRODDED, offsetof(struct kvm_vcpu, arch.prodded));
 	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
 	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
+	DEFINE(VCPU_SIAR, offsetof(struct kvm_vcpu, arch.siar));
+	DEFINE(VCPU_SDAR, offsetof(struct kvm_vcpu, arch.sdar));
 	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));
 	DEFINE(VCPU_SLB_MAX, offsetof(struct kvm_vcpu, arch.slb_max));
 	DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));

commit 18461960cbf50bf345ef0667d45d5f64de8fb893
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 10 20:21:10 2013 +1000

    powerpc: Provide for giveup_fpu/altivec to save state in alternate location
    
    This provides a facility which is intended for use by KVM, where the
    contents of the FP/VSX and VMX (Altivec) registers can be saved away
    to somewhere other than the thread_struct when kernel code wants to
    use floating point or VMX instructions.  This is done by providing a
    pointer in the thread_struct to indicate where the state should be
    saved to.  The giveup_fpu() and giveup_altivec() functions test these
    pointers and save state to the indicated location if they are non-NULL.
    Note that the MSR_FP/VEC bits in task->thread.regs->msr are still used
    to indicate whether the CPU register state is live, even when an
    alternate save location is being used.
    
    This also provides load_fp_state() and load_vr_state() functions, which
    load up FP/VSX and VMX state from memory into the CPU registers, and
    corresponding store_fp_state() and store_vr_state() functions, which
    store FP/VSX and VMX state into memory from the CPU registers.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8d27b61c95b9..6278edddc3f8 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -91,9 +91,11 @@ int main(void)
 #endif
 	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
 	DEFINE(THREAD_FPSTATE, offsetof(struct thread_struct, fp_state));
+	DEFINE(THREAD_FPSAVEAREA, offsetof(struct thread_struct, fp_save_area));
 	DEFINE(FPSTATE_FPSCR, offsetof(struct thread_fp_state, fpscr));
 #ifdef CONFIG_ALTIVEC
 	DEFINE(THREAD_VRSTATE, offsetof(struct thread_struct, vr_state));
+	DEFINE(THREAD_VRSAVEAREA, offsetof(struct thread_struct, vr_save_area));
 	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
 	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
 	DEFINE(VRSTATE_VSCR, offsetof(struct thread_vr_state, vscr));

commit de79f7b9f6f92ec1bd6f61fa1f20de60728a5b5e
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 10 20:20:42 2013 +1000

    powerpc: Put FP/VSX and VR state into structures
    
    This creates new 'thread_fp_state' and 'thread_vr_state' structures
    to store FP/VSX state (including FPSCR) and Altivec/VSX state
    (including VSCR), and uses them in the thread_struct.  In the
    thread_fp_state, the FPRs and VSRs are represented as u64 rather
    than double, since we rarely perform floating-point computations
    on the values, and this will enable the structures to be used
    in KVM code as well.  Similarly FPSCR is now a u64 rather than
    a structure of two 32-bit values.
    
    This takes the offsets out of the macros such as SAVE_32FPRS,
    REST_32FPRS, etc.  This enables the same macros to be used for normal
    and transactional state, enabling us to delete the transactional
    versions of the macros.   This also removes the unused do_load_up_fpu
    and do_load_up_altivec, which were in fact buggy since they didn't
    create large enough stack frames to account for the fact that
    load_up_fpu and load_up_altivec are not designed to be called from C
    and assume that their caller's stack frame is an interrupt frame.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 502c7a4e73f7..8d27b61c95b9 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -90,16 +90,15 @@ int main(void)
 	DEFINE(THREAD_NORMSAVES, offsetof(struct thread_struct, normsave[0]));
 #endif
 	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
-	DEFINE(THREAD_FPR0, offsetof(struct thread_struct, fpr[0]));
-	DEFINE(THREAD_FPSCR, offsetof(struct thread_struct, fpscr));
+	DEFINE(THREAD_FPSTATE, offsetof(struct thread_struct, fp_state));
+	DEFINE(FPSTATE_FPSCR, offsetof(struct thread_fp_state, fpscr));
 #ifdef CONFIG_ALTIVEC
-	DEFINE(THREAD_VR0, offsetof(struct thread_struct, vr[0]));
+	DEFINE(THREAD_VRSTATE, offsetof(struct thread_struct, vr_state));
 	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
-	DEFINE(THREAD_VSCR, offsetof(struct thread_struct, vscr));
 	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
+	DEFINE(VRSTATE_VSCR, offsetof(struct thread_vr_state, vscr));
 #endif /* CONFIG_ALTIVEC */
 #ifdef CONFIG_VSX
-	DEFINE(THREAD_VSR0, offsetof(struct thread_struct, fpr));
 	DEFINE(THREAD_USED_VSR, offsetof(struct thread_struct, used_vsr));
 #endif /* CONFIG_VSX */
 #ifdef CONFIG_PPC64
@@ -143,20 +142,12 @@ int main(void)
 	DEFINE(THREAD_TM_PPR, offsetof(struct thread_struct, tm_ppr));
 	DEFINE(THREAD_TM_DSCR, offsetof(struct thread_struct, tm_dscr));
 	DEFINE(PT_CKPT_REGS, offsetof(struct thread_struct, ckpt_regs));
-	DEFINE(THREAD_TRANSACT_VR0, offsetof(struct thread_struct,
-					 transact_vr[0]));
-	DEFINE(THREAD_TRANSACT_VSCR, offsetof(struct thread_struct,
-					  transact_vscr));
+	DEFINE(THREAD_TRANSACT_VRSTATE, offsetof(struct thread_struct,
+						 transact_vr));
 	DEFINE(THREAD_TRANSACT_VRSAVE, offsetof(struct thread_struct,
 					    transact_vrsave));
-	DEFINE(THREAD_TRANSACT_FPR0, offsetof(struct thread_struct,
-					  transact_fpr[0]));
-	DEFINE(THREAD_TRANSACT_FPSCR, offsetof(struct thread_struct,
-					   transact_fpscr));
-#ifdef CONFIG_VSX
-	DEFINE(THREAD_TRANSACT_VSR0, offsetof(struct thread_struct,
-					  transact_fpr[0]));
-#endif
+	DEFINE(THREAD_TRANSACT_FPSTATE, offsetof(struct thread_struct,
+						 transact_fp));
 	/* Local pt_regs on stack for Transactional Memory funcs. */
 	DEFINE(TM_FRAME_SIZE, STACK_FRAME_OVERHEAD +
 	       sizeof(struct pt_regs) + 16);

commit cbc9565ee82694dec31d8137dec975b83175183b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Sep 24 15:17:21 2013 +1000

    powerpc: Remove ksp_limit on ppc64
    
    We've been keeping that field in thread_struct for a while, it contains
    the "limit" of the current stack pointer and is meant to be used for
    detecting stack overflows.
    
    It has a few problems however:
    
     - First, it was never actually *used* on 64-bit. Set and updated but
    not actually exploited
    
     - When switching stack to/from irq and softirq stacks, it's update
    is racy unless we hard disable interrupts, which is costly. This
    is fine on 32-bit as we don't soft-disable there but not on 64-bit.
    
    Thus rather than fixing 2 in order to implement 1 in some hypothetical
    future, let's remove the code completely from 64-bit. In order to avoid
    a clutter of ifdef's, we remove the updates from C code completely
    during interrupt stack switching, and instead maintain it from the
    asm helper that is used to do the stack switching in the first place.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index d8958be5f31a..502c7a4e73f7 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -80,10 +80,11 @@ int main(void)
 	DEFINE(TASKTHREADPPR, offsetof(struct task_struct, thread.ppr));
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
+	DEFINE(THREAD_INFO_GAP, _ALIGN_UP(sizeof(struct thread_info), 16));
+	DEFINE(KSP_LIMIT, offsetof(struct thread_struct, ksp_limit));
 #endif /* CONFIG_PPC64 */
 
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
-	DEFINE(KSP_LIMIT, offsetof(struct thread_struct, ksp_limit));
 	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
 #ifdef CONFIG_BOOKE
 	DEFINE(THREAD_NORMSAVES, offsetof(struct thread_struct, normsave[0]));

commit ae7a835cc546fc67df90edaaa0c48ae2b22a29fe
Merge: cf39c8e5352b 6b9e4fa07443
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 18:15:06 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Gleb Natapov:
     "The highlights of the release are nested EPT and pv-ticketlocks
      support (hypervisor part, guest part, which is most of the code, goes
      through tip tree).  Apart of that there are many fixes for all arches"
    
    Fix up semantic conflicts as discussed in the pull request thread..
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (88 commits)
      ARM: KVM: Add newlines to panic strings
      ARM: KVM: Work around older compiler bug
      ARM: KVM: Simplify tracepoint text
      ARM: KVM: Fix kvm_set_pte assignment
      ARM: KVM: vgic: Bump VGIC_NR_IRQS to 256
      ARM: KVM: Bugfix: vgic_bytemap_get_reg per cpu regs
      ARM: KVM: vgic: fix GICD_ICFGRn access
      ARM: KVM: vgic: simplify vgic_get_target_reg
      KVM: MMU: remove unused parameter
      KVM: PPC: Book3S PR: Rework kvmppc_mmu_book3s_64_xlate()
      KVM: PPC: Book3S PR: Make instruction fetch fallback work for system calls
      KVM: PPC: Book3S PR: Don't corrupt guest state when kernel uses VMX
      KVM: x86: update masterclock when kvmclock_offset is calculated (v2)
      KVM: PPC: Book3S: Fix compile error in XICS emulation
      KVM: PPC: Book3S PR: return appropriate error when allocation fails
      arch: powerpc: kvm: add signed type cast for comparation
      KVM: x86: add comments where MMIO does not return to the emulator
      KVM: vmx: count exits to userspace during invalid guest emulation
      KVM: rename __kvm_io_bus_sort_cmp to kvm_io_bus_cmp
      kvm: optimize away THP checks in kvm_is_mmio_pfn()
      ...

commit bf550fc93d9855872a95e69e4002256110d89858
Merge: 7e48c101e0c5 cc2df20c7c4c
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Aug 29 00:41:59 2013 +0200

    Merge remote-tracking branch 'origin/next' into kvm-ppc-next
    
    Conflicts:
            mm/Kconfig
    
    CMA DMA split and ZSWAP introduction were conflicting, fix up manually.

commit 28e61cc466d8daace4b0f04ba2b83e0bd68f5832
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Aug 9 17:29:31 2013 +1000

    powerpc/tm: Fix context switching TAR, PPR and DSCR SPRs
    
    If a transaction is rolled back, the Target Address Register (TAR), Processor
    Priority Register (PPR) and Data Stream Control Register (DSCR) should be
    restored to the checkpointed values before the transaction began.  Any changes
    to these SPRs inside the transaction should not be visible in the abort
    handler.
    
    Currently Linux doesn't save or restore the checkpointed TAR, PPR or DSCR.  If
    we preempt a processes inside a transaction which has modified any of these, on
    process restore, that same transaction may be aborted we but we won't see the
    checkpointed versions of these SPRs.
    
    This adds checkpointed versions of these SPRs to the thread_struct and adds the
    save/restore of these three SPRs to the treclaim/trechkpt code.
    
    Without this if any of these SPRs are modified during a transaction, users may
    incorrectly see a speculated SPR value even if the transaction is aborted.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: <stable@vger.kernel.org> [v3.10]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c7e8afc2ead0..8207459efe56 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -138,6 +138,9 @@ int main(void)
 	DEFINE(THREAD_TM_TFHAR, offsetof(struct thread_struct, tm_tfhar));
 	DEFINE(THREAD_TM_TEXASR, offsetof(struct thread_struct, tm_texasr));
 	DEFINE(THREAD_TM_TFIAR, offsetof(struct thread_struct, tm_tfiar));
+	DEFINE(THREAD_TM_TAR, offsetof(struct thread_struct, tm_tar));
+	DEFINE(THREAD_TM_PPR, offsetof(struct thread_struct, tm_ppr));
+	DEFINE(THREAD_TM_DSCR, offsetof(struct thread_struct, tm_dscr));
 	DEFINE(PT_CKPT_REGS, offsetof(struct thread_struct, ckpt_regs));
 	DEFINE(THREAD_TRANSACT_VR0, offsetof(struct thread_struct,
 					 transact_vr[0]));

commit c8ae0ace104ff36d43311c148c8aeed9f194cbf2
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jul 11 21:49:43 2013 +1000

    KVM: PPC: Book3S PR: Load up SPRG3 register with guest value on guest entry
    
    Unlike the other general-purpose SPRs, SPRG3 can be read by usermode
    code, and is used in recent kernels to store the CPU and NUMA node
    numbers so that they can be read by VDSO functions.  Thus we need to
    load the guest's SPRG3 value into the real SPRG3 register when entering
    the guest, and restore the host's value when exiting the guest.  We don't
    need to save the guest SPRG3 value when exiting the guest as usermode
    code can't modify SPRG3.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6f16ffafa6f0..a67c76ef5feb 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -452,6 +452,7 @@ int main(void)
 	DEFINE(VCPU_SPRG2, offsetof(struct kvm_vcpu, arch.shregs.sprg2));
 	DEFINE(VCPU_SPRG3, offsetof(struct kvm_vcpu, arch.shregs.sprg3));
 #endif
+	DEFINE(VCPU_SHARED_SPRG3, offsetof(struct kvm_vcpu_arch_shared, sprg3));
 	DEFINE(VCPU_SHARED_SPRG4, offsetof(struct kvm_vcpu_arch_shared, sprg4));
 	DEFINE(VCPU_SHARED_SPRG5, offsetof(struct kvm_vcpu_arch_shared, sprg5));
 	DEFINE(VCPU_SHARED_SPRG6, offsetof(struct kvm_vcpu_arch_shared, sprg6));

commit 2ac138ca21ad26c988ce7c91d27327f85beb7519
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Jun 28 18:15:15 2013 +1000

    powerpc/perf: Drop MMCRA from thread_struct
    
    In commit 59affcd "Context switch more PMU related SPRs" I added more
    PMU SPRs to thread_struct, later modified in commit b11ae95. To add
    insult to injury it turns out we don't need to switch MMCRA as it's
    only user readable, and the value is recomputed by the PMU code.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index dc684b1af1c4..c7e8afc2ead0 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -132,7 +132,6 @@ int main(void)
 	DEFINE(THREAD_SIER, offsetof(struct thread_struct, sier));
 	DEFINE(THREAD_MMCR0, offsetof(struct thread_struct, mmcr0));
 	DEFINE(THREAD_MMCR2, offsetof(struct thread_struct, mmcr2));
-	DEFINE(THREAD_MMCRA, offsetof(struct thread_struct, mmcra));
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	DEFINE(PACATMSCRATCH, offsetof(struct paca_struct, tm_scratch));

commit 13d543cd79963133cd26748547552c99df8d23a7
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed May 22 09:50:59 2013 +0530

    powerpc: Restore dbcr0 on user space exit
    
    On BookE (Branch taken + Single Step) is as same as Branch Taken
    on BookS and in Linux we simulate BookS behavior for BookE as well.
    When doing so, in Branch taken handling we want to set DBCR0_IC but
    we update the current->thread->dbcr0 and not DBCR0.
    
    Now on 64bit the current->thread.dbcr0 (and other debug registers)
    is synchronized ONLY on context switch flow. But after handling
    Branch taken in debug exception if we return back to user space
    without context switch then single stepping change (DBCR0_ICMP)
    does not get written in h/w DBCR0 and Instruction Complete exception
    does not happen.
    
    This fixes using ptrace reliably on BookE-PowerPC
    
    lmbench latency test (lat_syscall) Results are (they varies a little
    on each run)
    
    1) ./lat_syscall <action> /dev/shm/uImage
    
    action: Open    read    write   stat    fstat   null
    Before: 3.8618  0.2017  0.2851  1.6789  0.2256  0.0856
    After:  3.8580  0.2017  0.2851  1.6955  0.2255  0.0856
    
    1) ./lat_syscall -P 2 -N 10 <action> /dev/shm/uImage
    action: Open    read    write   stat    fstat   null
    Before: 4.1388  0.2238  0.3066  1.7106  0.2256  0.0856
    After:  4.1413  0.2236  0.3062  1.7107  0.2256  0.0856
    
    [ Slightly modified to avoid extra branch in the fast path
      on Book3S and fix build on all non-BookE 64-bit -- BenH
    ]
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6f16ffafa6f0..dc684b1af1c4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -105,9 +105,6 @@ int main(void)
 	DEFINE(KSP_VSID, offsetof(struct thread_struct, ksp_vsid));
 #else /* CONFIG_PPC64 */
 	DEFINE(PGDIR, offsetof(struct thread_struct, pgdir));
-#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
-	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
-#endif
 #ifdef CONFIG_SPE
 	DEFINE(THREAD_EVR0, offsetof(struct thread_struct, evr[0]));
 	DEFINE(THREAD_ACC, offsetof(struct thread_struct, acc));
@@ -115,6 +112,9 @@ int main(void)
 	DEFINE(THREAD_USED_SPE, offsetof(struct thread_struct, used_spe));
 #endif /* CONFIG_SPE */
 #endif /* CONFIG_PPC64 */
+#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
+	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
+#endif
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	DEFINE(THREAD_KVM_SVCPU, offsetof(struct thread_struct, kvm_shadow_vcpu));
 #endif

commit 59affcd3e460b97492bc1aa2b843bafe7c54f596
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue May 21 16:31:12 2013 +0000

    powerpc: Context switch more PMU related SPRs
    
    In commit 9353374 "Context switch the new EBB SPRs" we added support for
    context switching some new EBB SPRs. However despite four of us signing
    off on that patch we missed some. To be fair these are not actually new
    SPRs, but they are now potentially user accessible so need to be context
    switched.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b51a97cfedf8..6f16ffafa6f0 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -127,6 +127,12 @@ int main(void)
 	DEFINE(THREAD_BESCR, offsetof(struct thread_struct, bescr));
 	DEFINE(THREAD_EBBHR, offsetof(struct thread_struct, ebbhr));
 	DEFINE(THREAD_EBBRR, offsetof(struct thread_struct, ebbrr));
+	DEFINE(THREAD_SIAR, offsetof(struct thread_struct, siar));
+	DEFINE(THREAD_SDAR, offsetof(struct thread_struct, sdar));
+	DEFINE(THREAD_SIER, offsetof(struct thread_struct, sier));
+	DEFINE(THREAD_MMCR0, offsetof(struct thread_struct, mmcr0));
+	DEFINE(THREAD_MMCR2, offsetof(struct thread_struct, mmcr2));
+	DEFINE(THREAD_MMCRA, offsetof(struct thread_struct, mmcra));
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	DEFINE(PACATMSCRATCH, offsetof(struct paca_struct, tm_scratch));

commit 01227a889ed56ae53aeebb9f93be9d54dd8b2de8
Merge: 9e6879460c8e db6ae6158186
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun May 5 14:47:31 2013 -0700

    Merge tag 'kvm-3.10-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Gleb Natapov:
     "Highlights of the updates are:
    
      general:
       - new emulated device API
       - legacy device assignment is now optional
       - irqfd interface is more generic and can be shared between arches
    
      x86:
       - VMCS shadow support and other nested VMX improvements
       - APIC virtualization and Posted Interrupt hardware support
       - Optimize mmio spte zapping
    
      ppc:
        - BookE: in-kernel MPIC emulation with irqfd support
        - Book3S: in-kernel XICS emulation (incomplete)
        - Book3S: HV: migration fixes
        - BookE: more debug support preparation
        - BookE: e6500 support
    
      ARM:
       - reworking of Hyp idmaps
    
      s390:
       - ioeventfd for virtio-ccw
    
      And many other bug fixes, cleanups and improvements"
    
    * tag 'kvm-3.10-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (204 commits)
      kvm: Add compat_ioctl for device control API
      KVM: x86: Account for failing enable_irq_window for NMI window request
      KVM: PPC: Book3S: Add API for in-kernel XICS emulation
      kvm/ppc/mpic: fix missing unlock in set_base_addr()
      kvm/ppc: Hold srcu lock when calling kvm_io_bus_read/write
      kvm/ppc/mpic: remove users
      kvm/ppc/mpic: fix mmio region lists when multiple guests used
      kvm/ppc/mpic: remove default routes from documentation
      kvm: KVM_CAP_IOMMU only available with device assignment
      ARM: KVM: iterate over all CPUs for CPU compatibility check
      KVM: ARM: Fix spelling in error message
      ARM: KVM: define KVM_ARM_MAX_VCPUS unconditionally
      KVM: ARM: Fix API documentation for ONE_REG encoding
      ARM: KVM: promote vfp_host pointer to generic host cpu context
      ARM: KVM: add architecture specific hook for capabilities
      ARM: KVM: perform HYP initilization for hotplugged CPUs
      ARM: KVM: switch to a dual-step HYP init code
      ARM: KVM: rework HYP page table freeing
      ARM: KVM: enforce maximum size for identity mapped code
      ARM: KVM: move to a KVM provided HYP idmap
      ...

commit 9353374b8e1585d5fa47a1e5c1d3e9155dd0eb7c
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Apr 30 20:17:04 2013 +0000

    powerpc: Context switch the new EBB SPRs
    
    This context switches the new Event Based Branching (EBB) SPRs.  The three new
    SPRs are:
      - Event Based Branch Handler Register (EBBHR)
      - Event Based Branch Return Register (EBBRR)
      - Branch Event Status and Control Register (BESCR)
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b6c17ec9b169..172233eab799 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -124,6 +124,9 @@ int main(void)
 
 #ifdef CONFIG_PPC_BOOK3S_64
 	DEFINE(THREAD_TAR, offsetof(struct thread_struct, tar));
+	DEFINE(THREAD_BESCR, offsetof(struct thread_struct, bescr));
+	DEFINE(THREAD_EBBHR, offsetof(struct thread_struct, ebbhr));
+	DEFINE(THREAD_EBBRR, offsetof(struct thread_struct, ebbrr));
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	DEFINE(PACATMSCRATCH, offsetof(struct paca_struct, tm_scratch));

commit 54695c3088a74e25474db8eb6b490b45d1aeb0ca
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 17 20:30:50 2013 +0000

    KVM: PPC: Book3S HV: Speed up wakeups of CPUs on HV KVM
    
    Currently, we wake up a CPU by sending a host IPI with
    smp_send_reschedule() to thread 0 of that core, which will take all
    threads out of the guest, and cause them to re-evaluate their
    interrupt status on the way back in.
    
    This adds a mechanism to differentiate real host IPIs from IPIs sent
    by KVM for guest threads to poke each other, in order to target the
    guest threads precisely when possible and avoid that global switch of
    the core to host state.
    
    We then use this new facility in the in-kernel XICS code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index dbfd5498f440..a791229329cf 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -574,6 +574,8 @@ int main(void)
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);
 	HSTATE_FIELD(HSTATE_KVM_VCORE, kvm_vcore);
 	HSTATE_FIELD(HSTATE_XICS_PHYS, xics_phys);
+	HSTATE_FIELD(HSTATE_SAVED_XIRR, saved_xirr);
+	HSTATE_FIELD(HSTATE_HOST_IPI, host_ipi);
 	HSTATE_FIELD(HSTATE_MMCR, host_mmcr);
 	HSTATE_FIELD(HSTATE_PMC, host_pmc);
 	HSTATE_FIELD(HSTATE_PURR, host_purr);

commit c35635efdc0312e013ebda1c8f3b5dd038c0d0e7
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Apr 18 19:51:04 2013 +0000

    KVM: PPC: Book3S HV: Report VPA and DTL modifications in dirty map
    
    At present, the KVM_GET_DIRTY_LOG ioctl doesn't report modifications
    done by the host to the virtual processor areas (VPAs) and dispatch
    trace logs (DTLs) registered by the guest.  This is because those
    modifications are done either in real mode or in the host kernel
    context, and in neither case does the access go through the guest's
    HPT, and thus no change (C) bit gets set in the guest's HPT.
    
    However, the changes done by the host do need to be tracked so that
    the modified pages get transferred when doing live migration.  In
    order to track these modifications, this adds a dirty flag to the
    struct representing the VPA/DTL areas, and arranges to set the flag
    when the VPA/DTL gets modified by the host.  Then, when we are
    collecting the dirty log, we also check the dirty flags for the
    VPA and DTL for each vcpu and set the relevant bit in the dirty log
    if necessary.  Doing this also means we now need to keep track of
    the guest physical address of the VPA/DTL areas.
    
    So as not to lose track of modifications to a VPA/DTL area when it gets
    unregistered, or when a new area gets registered in its place, we need
    to transfer the dirty state to the rmap chain.  This adds code to
    kvmppc_unpin_guest_page() to do that if the area was dirty.  To simplify
    that code, we now require that all VPA, DTL and SLB shadow buffer areas
    fit within a single host page.  Guests already comply with this
    requirement because pHyp requires that these areas not cross a 4k
    boundary.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index d87c90886c75..dbfd5498f440 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -477,6 +477,7 @@ int main(void)
 	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
+	DEFINE(VCPU_VPA_DIRTY, offsetof(struct kvm_vcpu, arch.vpa.dirty));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));

commit 15b708beee6841e0a59ded702c8bfe3042a5b5a4
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Feb 27 18:13:10 2013 +0000

    KVM: PPC: booke: Added debug handler
    
    Installed debug handler will be used for guest debug support
    and debug facility emulation features (patches for these
    features will follow this patch).
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [bharat.bhushan@freescale.com: Substantial changes]
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b6c17ec9b169..d87c90886c75 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -596,6 +596,7 @@ int main(void)
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
 	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
+	DEFINE(VCPU_CRIT_SAVE, offsetof(struct kvm_vcpu, arch.crit_save));
 #endif /* CONFIG_PPC_BOOK3S */
 #endif /* CONFIG_KVM */
 

commit 89f883372fa60f604d136924baf3e89ff1870e9e
Merge: 9e2d59ad580d 6b73a96065e8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Feb 24 13:07:18 2013 -0800

    Merge tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM updates from Marcelo Tosatti:
     "KVM updates for the 3.9 merge window, including x86 real mode
      emulation fixes, stronger memory slot interface restrictions, mmu_lock
      spinlock hold time reduction, improved handling of large page faults
      on shadow, initial APICv HW acceleration support, s390 channel IO
      based virtio, amongst others"
    
    * tag 'kvm-3.9-1' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (143 commits)
      Revert "KVM: MMU: lazily drop large spte"
      x86: pvclock kvm: align allocation size to page size
      KVM: nVMX: Remove redundant get_vmcs12 from nested_vmx_exit_handled_msr
      x86 emulator: fix parity calculation for AAD instruction
      KVM: PPC: BookE: Handle alignment interrupts
      booke: Added DBCR4 SPR number
      KVM: PPC: booke: Allow multiple exception types
      KVM: PPC: booke: use vcpu reference from thread_struct
      KVM: Remove user_alloc from struct kvm_memory_slot
      KVM: VMX: disable apicv by default
      KVM: s390: Fix handling of iscs.
      KVM: MMU: cleanup __direct_map
      KVM: MMU: remove pt_access in mmu_set_spte
      KVM: MMU: cleanup mapping-level
      KVM: MMU: lazily drop large spte
      KVM: VMX: cleanup vmx_set_cr0().
      KVM: VMX: add missing exit names to VMX_EXIT_REASONS array
      KVM: VMX: disable SMEP feature when guest is in non-paging mode
      KVM: Remove duplicate text in api.txt
      Revert "KVM: MMU: split kvm_mmu_free_page"
      ...

commit afc07701ced6463786d09a3b9baf894c1397e991
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:34 2013 +0000

    powerpc: Add transactional memory paca scratch register to show_regs
    
    Add transactional memory paca scratch register to show_regs.  This is useful
    for debugging.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0fdc97496d7c..781190367292 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -126,6 +126,7 @@ int main(void)
 	DEFINE(THREAD_TAR, offsetof(struct thread_struct, tar));
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	DEFINE(PACATMSCRATCH, offsetof(struct paca_struct, tm_scratch));
 	DEFINE(THREAD_TM_TFHAR, offsetof(struct thread_struct, tm_tfhar));
 	DEFINE(THREAD_TM_TEXASR, offsetof(struct thread_struct, tm_texasr));
 	DEFINE(THREAD_TM_TFIAR, offsetof(struct thread_struct, tm_tfiar));

commit 8b3c34cf0e0ab334a24aad7367cd06a5ba09a898
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Feb 13 16:21:32 2013 +0000

    powerpc: New macros for transactional memory support
    
    This adds new macros for saving and restoring checkpointed architected state
    from and to the thread_struct.
    
    It also adds some debugging macros for when your brain explodes trying to debug
    your transactional memory enabled kernel.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e295a09b1f06..0fdc97496d7c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -125,6 +125,29 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S_64
 	DEFINE(THREAD_TAR, offsetof(struct thread_struct, tar));
 #endif
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
+	DEFINE(THREAD_TM_TFHAR, offsetof(struct thread_struct, tm_tfhar));
+	DEFINE(THREAD_TM_TEXASR, offsetof(struct thread_struct, tm_texasr));
+	DEFINE(THREAD_TM_TFIAR, offsetof(struct thread_struct, tm_tfiar));
+	DEFINE(PT_CKPT_REGS, offsetof(struct thread_struct, ckpt_regs));
+	DEFINE(THREAD_TRANSACT_VR0, offsetof(struct thread_struct,
+					 transact_vr[0]));
+	DEFINE(THREAD_TRANSACT_VSCR, offsetof(struct thread_struct,
+					  transact_vscr));
+	DEFINE(THREAD_TRANSACT_VRSAVE, offsetof(struct thread_struct,
+					    transact_vrsave));
+	DEFINE(THREAD_TRANSACT_FPR0, offsetof(struct thread_struct,
+					  transact_fpr[0]));
+	DEFINE(THREAD_TRANSACT_FPSCR, offsetof(struct thread_struct,
+					   transact_fpscr));
+#ifdef CONFIG_VSX
+	DEFINE(THREAD_TRANSACT_VSR0, offsetof(struct thread_struct,
+					  transact_fpr[0]));
+#endif
+	/* Local pt_regs on stack for Transactional Memory funcs. */
+	DEFINE(TM_FRAME_SIZE, STACK_FRAME_OVERHEAD +
+	       sizeof(struct pt_regs) + 16);
+#endif /* CONFIG_PPC_TRANSACTIONAL_MEM */
 
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));

commit 0acb91112a148fbb31678e66839ef757f3be3aa4
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Feb 4 18:10:51 2013 +0000

    powerpc/kvm/book3s_hv: Preserve guest CFAR register value
    
    The CFAR (Come-From Address Register) is a useful debugging aid that
    exists on POWER7 processors.  Currently HV KVM doesn't save or restore
    the CFAR register for guest vcpus, making the CFAR of limited use in
    guests.
    
    This adds the necessary code to capture the CFAR value saved in the
    early exception entry code (it has to be saved before any branch is
    executed), save it in the vcpu.arch struct, and restore it on entry
    to the guest.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index beddba432518..e295a09b1f06 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -479,6 +479,7 @@ int main(void)
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
 	DEFINE(VCPU_PTID, offsetof(struct kvm_vcpu, arch.ptid));
+	DEFINE(VCPU_CFAR, offsetof(struct kvm_vcpu, arch.cfar));
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
 	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
@@ -558,6 +559,10 @@ int main(void)
 	DEFINE(IPI_PRIORITY, IPI_PRIORITY);
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 
+#ifdef CONFIG_PPC_BOOK3S_64
+	HSTATE_FIELD(HSTATE_CFAR, cfar);
+#endif /* CONFIG_PPC_BOOK3S_64 */
+
 #else /* CONFIG_PPC_BOOK3S */
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));

commit ffe129ecd79779221fdb03305049ec8b5a8beb0f
Author: Bharat Bhushan <Bharat.Bhushan@freescale.com>
Date:   Tue Jan 15 22:20:42 2013 +0000

    KVM: PPC: booke: use vcpu reference from thread_struct
    
    Like other places, use thread_struct to get vcpu reference.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4e23ba2f3ca7..46f6afd2172a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -117,7 +117,7 @@ int main(void)
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	DEFINE(THREAD_KVM_SVCPU, offsetof(struct thread_struct, kvm_shadow_vcpu));
 #endif
-#ifdef CONFIG_KVM_BOOKE_HV
+#if defined(CONFIG_KVM) && defined(CONFIG_BOOKE)
 	DEFINE(THREAD_KVM_VCPU, offsetof(struct thread_struct, kvm_vcpu));
 #endif
 

commit 2468dcf641e4f3e1b0153e3e11ca20740b2f4ce8
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Thu Feb 7 15:46:58 2013 +0000

    powerpc: Add support for context switching the TAR register
    
    This patch adds support for enabling and context switching the Target
    Address Register in Power8. The TAR is a new special purpose register
    that can be used for computed branches with the bctar[l] (branch
    conditional to TAR) instruction in the same manner as the count and link
    registers.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e39ca556e87f..beddba432518 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -122,6 +122,10 @@ int main(void)
 	DEFINE(THREAD_KVM_VCPU, offsetof(struct thread_struct, kvm_vcpu));
 #endif
 
+#ifdef CONFIG_PPC_BOOK3S_64
+	DEFINE(THREAD_TAR, offsetof(struct thread_struct, tar));
+#endif
+
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));

commit 92779245599bb3d7fb48066b11c4bfd6aa477198
Author: Haren Myneni <haren@linux.vnet.ibm.com>
Date:   Thu Dec 6 21:49:56 2012 +0000

    powerpc: Define ppr in thread_struct
    
    [PATCH 4/6] powerpc: Define ppr in thread_struct
    
    ppr in thread_struct is used to save PPR and restore it before process exits
    from kernel.
    
    This patch sets the default priority to 3 when tasks are created such
    that users can use 4 for higher priority tasks.
    
    Signed-off-by: Haren Myneni <haren@us.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4e23ba2f3ca7..e39ca556e87f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -77,6 +77,7 @@ int main(void)
 	DEFINE(NMI_MASK, NMI_MASK);
 	DEFINE(THREAD_DSCR, offsetof(struct thread_struct, dscr));
 	DEFINE(THREAD_DSCR_INHERIT, offsetof(struct thread_struct, dscr_inherit));
+	DEFINE(TASKTHREADPPR, offsetof(struct task_struct, thread.ppr));
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
 #endif /* CONFIG_PPC64 */

commit 1b400ba0cd24a5994d792c7cfa0ee24cac266d3c
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Nov 21 23:28:08 2012 +0000

    KVM: PPC: Book3S HV: Improve handling of local vs. global TLB invalidations
    
    When we change or remove a HPT (hashed page table) entry, we can do
    either a global TLB invalidation (tlbie) that works across the whole
    machine, or a local invalidation (tlbiel) that only affects this core.
    Currently we do local invalidations if the VM has only one vcpu or if
    the guest requests it with the H_LOCAL flag, though the guest Linux
    kernel currently doesn't ever use H_LOCAL.  Then, to cope with the
    possibility that vcpus moving around to different physical cores might
    expose stale TLB entries, there is some code in kvmppc_hv_entry to
    flush the whole TLB of entries for this VM if either this vcpu is now
    running on a different physical core from where it last ran, or if this
    physical core last ran a different vcpu.
    
    There are a number of problems on POWER7 with this as it stands:
    
    - The TLB invalidation is done per thread, whereas it only needs to be
      done per core, since the TLB is shared between the threads.
    - With the possibility of the host paging out guest pages, the use of
      H_LOCAL by an SMP guest is dangerous since the guest could possibly
      retain and use a stale TLB entry pointing to a page that had been
      removed from the guest.
    - The TLB invalidations that we do when a vcpu moves from one physical
      core to another are unnecessary in the case of an SMP guest that isn't
      using H_LOCAL.
    - The optimization of using local invalidations rather than global should
      apply to guests with one virtual core, not just one vcpu.
    
    (None of this applies on PPC970, since there we always have to
    invalidate the whole TLB when entering and leaving the guest, and we
    can't support paging out guest memory.)
    
    To fix these problems and simplify the code, we now maintain a simple
    cpumask of which cpus need to flush the TLB on entry to the guest.
    (This is indexed by cpu, though we only ever use the bits for thread
    0 of each core.)  Whenever we do a local TLB invalidation, we set the
    bits for every cpu except the bit for thread 0 of the core that we're
    currently running on.  Whenever we enter a guest, we test and clear the
    bit for our core, and flush the TLB if it was set.
    
    On initial startup of the VM, and when resetting the HPT, we set all the
    bits in the need_tlb_flush cpumask, since any core could potentially have
    stale TLB entries from the previous VM to use the same LPID, or the
    previous contents of the HPT.
    
    Then, we maintain a count of the number of online virtual cores, and use
    that when deciding whether to use a local invalidation rather than the
    number of online vcpus.  The code to make that decision is extracted out
    into a new function, global_invalidates().  For multi-core guests on
    POWER7 (i.e. when we are using mmu notifiers), we now never do local
    invalidations regardless of the H_LOCAL flag.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 7523539cfe9f..4e23ba2f3ca7 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -441,8 +441,7 @@ int main(void)
 	DEFINE(KVM_HOST_LPCR, offsetof(struct kvm, arch.host_lpcr));
 	DEFINE(KVM_HOST_SDR1, offsetof(struct kvm, arch.host_sdr1));
 	DEFINE(KVM_TLBIE_LOCK, offsetof(struct kvm, arch.tlbie_lock));
-	DEFINE(KVM_ONLINE_CPUS, offsetof(struct kvm, online_vcpus.counter));
-	DEFINE(KVM_LAST_VCPU, offsetof(struct kvm, arch.last_vcpu));
+	DEFINE(KVM_NEED_FLUSH, offsetof(struct kvm, arch.need_tlb_flush.bits));
 	DEFINE(KVM_LPCR, offsetof(struct kvm, arch.lpcr));
 	DEFINE(KVM_RMOR, offsetof(struct kvm, arch.rmor));
 	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));
@@ -470,7 +469,6 @@ int main(void)
 	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));
 	DEFINE(VCPU_SLB_MAX, offsetof(struct kvm_vcpu, arch.slb_max));
 	DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));
-	DEFINE(VCPU_LAST_CPU, offsetof(struct kvm_vcpu, arch.last_cpu));
 	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
 	DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));

commit fff34b3412b9401a76ba9d021db1bd91cb0e02b6
Merge: 28e1e58fb668 636802ef96ee
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Sep 7 09:48:59 2012 +1000

    Merge branch 'merge' into next
    
    Brings in various bug fixes from 3.6-rcX

commit 0127262c01f0beb485f917c720d1d95d165dfdbf
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Thu Sep 6 02:49:44 2012 +0000

    powerpc: Restore VDSO information on critical exception om BookE
    
    Critical exception on 64-bit booke uses user-visible SPRG3 as scratch.
    Restore VDSO information in SPRG3 on exception prolog.
    
    Use a common sprg3 field in PACA for all powerpc64 architectures.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 85b05c463fae..72f26166007a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -205,6 +205,7 @@ int main(void)
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 	DEFINE(PACA_NAPSTATELOST, offsetof(struct paca_struct, nap_state_lost));
+	DEFINE(PACA_SPRG3, offsetof(struct paca_struct, sprg3));
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */
@@ -533,7 +534,6 @@ int main(void)
 	HSTATE_FIELD(HSTATE_VMHANDLER, vmhandler);
 	HSTATE_FIELD(HSTATE_SCRATCH0, scratch0);
 	HSTATE_FIELD(HSTATE_SCRATCH1, scratch1);
-	HSTATE_FIELD(HSTATE_SPRG3, sprg3);
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
 	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
 	HSTATE_FIELD(HSTATE_NAPPING, napping);

commit 714332858bfd40dcf8f741498336d93875c23aa7
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Sep 3 16:51:10 2012 +0000

    powerpc: Restore correct DSCR in context switch
    
    During a context switch we always restore the per thread DSCR value.
    If we aren't doing explicit DSCR management
    (ie thread.dscr_inherit == 0) and the default DSCR changed while
    the process has been sleeping we end up with the wrong value.
    
    Check thread.dscr_inherit and select the default DSCR or per thread
    DSCR as required.
    
    This was found with the following test case, when running with
    more threads than CPUs (ie forcing context switching):
    
    http://ozlabs.org/~anton/junkcode/dscr_default_test.c
    
    With the four patches applied I can run a combination of all
    test cases successfully at the same time:
    
    http://ozlabs.org/~anton/junkcode/dscr_default_test.c
    http://ozlabs.org/~anton/junkcode/dscr_explicit_test.c
    http://ozlabs.org/~anton/junkcode/dscr_inherit_test.c
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: <stable@kernel.org> # 3.0+
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 85b05c463fae..e8995727b1c1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -76,6 +76,7 @@ int main(void)
 	DEFINE(SIGSEGV, SIGSEGV);
 	DEFINE(NMI_MASK, NMI_MASK);
 	DEFINE(THREAD_DSCR, offsetof(struct thread_struct, dscr));
+	DEFINE(THREAD_DSCR_INHERIT, offsetof(struct thread_struct, dscr_inherit));
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
 #endif /* CONFIG_PPC64 */

commit 18ad51dd342a7eb09dbcd059d0b451b616d4dafc
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Jul 4 20:37:11 2012 +0000

    powerpc: Add VDSO version of getcpu
    
    We have a request for a fast method of getting CPU and NUMA node IDs
    from userspace. This patch implements a getcpu VDSO function,
    similar to x86.
    
    Ben suggested we use SPRG3 which is userspace readable. SPRG3 can be
    modified by a KVM guest, so we save the SPRG3 value in the paca and
    restore it when transitioning from the guest to the host.
    
    I have a glibc patch that implements sched_getcpu on top of this.
    Testing on a POWER7:
    
    baseline: 538 cycles
    vdso:      30 cycles
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 52c7ad78242e..85b05c463fae 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -533,6 +533,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_VMHANDLER, vmhandler);
 	HSTATE_FIELD(HSTATE_SCRATCH0, scratch0);
 	HSTATE_FIELD(HSTATE_SCRATCH1, scratch1);
+	HSTATE_FIELD(HSTATE_SPRG3, sprg3);
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
 	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
 	HSTATE_FIELD(HSTATE_NAPPING, napping);

commit 07acfc2a9349a8ce45b236c2624dad452001966b
Merge: b5f4035adfff 322728e55aa7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 16:17:30 2012 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull KVM changes from Avi Kivity:
     "Changes include additional instruction emulation, page-crossing MMIO,
      faster dirty logging, preventing the watchdog from killing a stopped
      guest, module autoload, a new MSI ABI, and some minor optimizations
      and fixes.  Outside x86 we have a small s390 and a very large ppc
      update.
    
      Regarding the new (for kvm) rebaseless workflow, some of the patches
      that were merged before we switch trees had to be rebased, while
      others are true pulls.  In either case the signoffs should be correct
      now."
    
    Fix up trivial conflicts in Documentation/feature-removal-schedule.txt
    arch/powerpc/kvm/book3s_segment.S and arch/x86/include/asm/kvm_para.h.
    
    I suspect the kvm_para.h resolution ends up doing the "do I have cpuid"
    check effectively twice (it was done differently in two different
    commits), but better safe than sorry ;)
    
    * 'next' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (125 commits)
      KVM: make asm-generic/kvm_para.h have an ifdef __KERNEL__ block
      KVM: s390: onereg for timer related registers
      KVM: s390: epoch difference and TOD programmable field
      KVM: s390: KVM_GET/SET_ONEREG for s390
      KVM: s390: add capability indicating COW support
      KVM: Fix mmu_reload() clash with nested vmx event injection
      KVM: MMU: Don't use RCU for lockless shadow walking
      KVM: VMX: Optimize %ds, %es reload
      KVM: VMX: Fix %ds/%es clobber
      KVM: x86 emulator: convert bsf/bsr instructions to emulate_2op_SrcV_nobyte()
      KVM: VMX: unlike vmcs on fail path
      KVM: PPC: Emulator: clean up SPR reads and writes
      KVM: PPC: Emulator: clean up instruction parsing
      kvm/powerpc: Add new ioctl to retreive server MMU infos
      kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
      KVM: PPC: bookehv: Fix r8/r13 storing in level exception handler
      KVM: PPC: Book3S: Enable IRQs during exit handling
      KVM: PPC: Fix PR KVM on POWER7 bare metal
      KVM: PPC: Fix stbux emulation
      KVM: PPC: bookehv: Use lwz/stw instead of PPC_LL/PPC_STL for 32-bit fields
      ...

commit 448054a6508d74767ba4c76654b42182e4f750b6
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Apr 10 16:22:12 2012 +0000

    powerpc: Remove iseries specific fields in lppaca
    
    Remove all the iseries specific fields in the lppaca.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 34b8afe94a50..4554dc2fe857 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -188,10 +188,6 @@ int main(void)
 	DEFINE(SLBSHADOW_STACKESID,
 	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].esid));
 	DEFINE(SLBSHADOW_SAVEAREA, offsetof(struct slb_shadow, save_area));
-	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
-	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
-	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
-	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
 	DEFINE(LPPACA_PMCINUSE, offsetof(struct lppaca, pmcregs_in_use));
 	DEFINE(LPPACA_DTLIDX, offsetof(struct lppaca, dtl_idx));
 	DEFINE(LPPACA_YIELDCOUNT, offsetof(struct lppaca, yield_count));

commit 7657f4089b097846cc37bfa2b74fc0bd2bd60e30
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Mar 5 21:42:25 2012 +0000

    KVM: PPC: Book 3S: Fix compilation for !HV configs
    
    Commits 2f5cdd5487 ("KVM: PPC: Book3S HV: Make secondary threads more
    robust against stray IPIs") and 1c2066b0f7 ("KVM: PPC: Book3S HV: Make
    virtual processor area registration more robust") added fields to
    struct kvm_vcpu_arch inside #ifdef CONFIG_KVM_BOOK3S_64_HV regions,
    and added lines to arch/powerpc/kernel/asm-offsets.c to generate
    assembler constants for their offsets.  Unfortunately this led to
    compile errors on Book 3S machines for configs that had KVM enabled
    but not CONFIG_KVM_BOOK3S_64_HV.  This fixes the problem by moving
    the offending lines inside #ifdef CONFIG_KVM_BOOK3S_64_HV regions.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 502e038f8c8a..694af3ebb0e4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -450,6 +450,7 @@ int main(void)
 	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));
 	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
+	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
@@ -466,7 +467,6 @@ int main(void)
 	DEFINE(VCPU_PENDING_EXC, offsetof(struct kvm_vcpu, arch.pending_exceptions));
 	DEFINE(VCPU_CEDED, offsetof(struct kvm_vcpu, arch.ceded));
 	DEFINE(VCPU_PRODDED, offsetof(struct kvm_vcpu, arch.prodded));
-	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
 	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
 	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));
@@ -540,10 +540,10 @@ int main(void)
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
 	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
 	HSTATE_FIELD(HSTATE_NAPPING, napping);
-	HSTATE_FIELD(HSTATE_HWTHREAD_REQ, hwthread_req);
-	HSTATE_FIELD(HSTATE_HWTHREAD_STATE, hwthread_state);
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
+	HSTATE_FIELD(HSTATE_HWTHREAD_REQ, hwthread_req);
+	HSTATE_FIELD(HSTATE_HWTHREAD_STATE, hwthread_state);
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);
 	HSTATE_FIELD(HSTATE_KVM_VCORE, kvm_vcore);
 	HSTATE_FIELD(HSTATE_XICS_PHYS, xics_phys);

commit 2e25aa5f64b18a97f35266e51c71ff4dc644db0c
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Feb 19 17:46:32 2012 +0000

    KVM: PPC: Book3S HV: Make virtual processor area registration more robust
    
    The PAPR API allows three sorts of per-virtual-processor areas to be
    registered (VPA, SLB shadow buffer, and dispatch trace log), and
    furthermore, these can be registered and unregistered for another
    virtual CPU.  Currently we just update the vcpu fields pointing to
    these areas at the time of registration or unregistration.  If this
    is done on another vcpu, there is the possibility that the target vcpu
    is using those fields at the time and could end up using a bogus
    pointer and corrupting memory.
    
    This fixes the race by making the target cpu itself do the update, so
    we can be sure that the update happens at a time when the fields
    aren't being used.  Each area now has a struct kvmppc_vpa which is
    used to manage these updates.  There is also a spinlock which protects
    access to all of the kvmppc_vpa structs, other than to the pinned_addr
    fields.  (We could have just taken the spinlock when using the vpa,
    slb_shadow or dtl fields, but that would mean taking the spinlock on
    every guest entry and exit.)
    
    This also changes 'struct dtl' (which was undefined) to 'struct dtl_entry',
    which is what the rest of the kernel uses.
    
    Thanks to Michael Ellerman <michael@ellerman.id.au> for pointing out
    the need to initialize vcpu->arch.vpa_update_lock.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2abcf7d4b29c..502e038f8c8a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -466,7 +466,7 @@ int main(void)
 	DEFINE(VCPU_PENDING_EXC, offsetof(struct kvm_vcpu, arch.pending_exceptions));
 	DEFINE(VCPU_CEDED, offsetof(struct kvm_vcpu, arch.ceded));
 	DEFINE(VCPU_PRODDED, offsetof(struct kvm_vcpu, arch.prodded));
-	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa));
+	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa.pinned_addr));
 	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
 	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
 	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));

commit f0888f70151c7f53de2b45ee20ff1905837943e8
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Feb 3 00:54:17 2012 +0000

    KVM: PPC: Book3S HV: Make secondary threads more robust against stray IPIs
    
    Currently on POWER7, if we are running the guest on a core and we don't
    need all the hardware threads, we do nothing to ensure that the unused
    threads aren't executing in the kernel (other than checking that they
    are offline).  We just assume they're napping and we don't do anything
    to stop them trying to enter the kernel while the guest is running.
    This means that a stray IPI can wake up the hardware thread and it will
    then try to enter the kernel, but since the core is in guest context,
    it will execute code from the guest in hypervisor mode once it turns the
    MMU on, which tends to lead to crashes or hangs in the host.
    
    This fixes the problem by adding two new one-byte flags in the
    kvmppc_host_state structure in the PACA which are used to interlock
    between the primary thread and the unused secondary threads when entering
    the guest.  With these flags, the primary thread can ensure that the
    unused secondaries are not already in kernel mode (i.e. handling a stray
    IPI) and then indicate that they should not try to enter the kernel
    if they do get woken for any reason.  Instead they will go into KVM code,
    find that there is no vcpu to run, acknowledge and clear the IPI and go
    back to nap mode.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index bbede5882c5b..2abcf7d4b29c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -540,6 +540,8 @@ int main(void)
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
 	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
 	HSTATE_FIELD(HSTATE_NAPPING, napping);
+	HSTATE_FIELD(HSTATE_HWTHREAD_REQ, hwthread_req);
+	HSTATE_FIELD(HSTATE_HWTHREAD_STATE, hwthread_state);
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);

commit d30f6e480055e5be12e7a03fd11ea912a451daa5
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:43 2011 +0000

    KVM: PPC: booke: category E.HV (GS-mode) support
    
    Chips such as e500mc that implement category E.HV in Power ISA 2.06
    provide hardware virtualization features, including a new MSR mode for
    guest state.  The guest OS can perform many operations without trapping
    into the hypervisor, including transitions to and from guest userspace.
    
    Since we can use SRR1[GS] to reliably tell whether an exception came from
    guest state, instead of messing around with IVPR, we use DO_KVM similarly
    to book3s.
    
    Current issues include:
     - Machine checks from guest state are not routed to the host handler.
     - The guest can cause a host oops by executing an emulated instruction
       in a page that lacks read permission.  Existing e500/4xx support has
       the same problem.
    
    Includes work by Ashish Kalra <Ashish.Kalra@freescale.com>,
    Varun Sethi <Varun.Sethi@freescale.com>, and
    Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: remove pt_regs usage]
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 34b8afe94a50..bbede5882c5b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -116,6 +116,9 @@ int main(void)
 #ifdef CONFIG_KVM_BOOK3S_32_HANDLER
 	DEFINE(THREAD_KVM_SVCPU, offsetof(struct thread_struct, kvm_shadow_vcpu));
 #endif
+#ifdef CONFIG_KVM_BOOKE_HV
+	DEFINE(THREAD_KVM_VCPU, offsetof(struct thread_struct, kvm_vcpu));
+#endif
 
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
@@ -387,6 +390,7 @@ int main(void)
 #ifdef CONFIG_KVM
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
+	DEFINE(VCPU_GUEST_PID, offsetof(struct kvm_vcpu, arch.pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_VRSAVE, offsetof(struct kvm_vcpu, arch.vrsave));
 	DEFINE(VCPU_FPRS, offsetof(struct kvm_vcpu, arch.fpr));
@@ -429,9 +433,11 @@ int main(void)
 	DEFINE(VCPU_SHARED_MAS4, offsetof(struct kvm_vcpu_arch_shared, mas4));
 	DEFINE(VCPU_SHARED_MAS6, offsetof(struct kvm_vcpu_arch_shared, mas6));
 
+	DEFINE(VCPU_KVM, offsetof(struct kvm_vcpu, kvm));
+	DEFINE(KVM_LPID, offsetof(struct kvm, arch.lpid));
+
 	/* book3s */
 #ifdef CONFIG_KVM_BOOK3S_64_HV
-	DEFINE(KVM_LPID, offsetof(struct kvm, arch.lpid));
 	DEFINE(KVM_SDR1, offsetof(struct kvm, arch.sdr1));
 	DEFINE(KVM_HOST_LPID, offsetof(struct kvm, arch.host_lpid));
 	DEFINE(KVM_HOST_LPCR, offsetof(struct kvm, arch.host_lpcr));
@@ -446,7 +452,6 @@ int main(void)
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 #endif
 #ifdef CONFIG_PPC_BOOK3S
-	DEFINE(VCPU_KVM, offsetof(struct kvm_vcpu, kvm));
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
 	DEFINE(VCPU_PURR, offsetof(struct kvm_vcpu, arch.purr));
 	DEFINE(VCPU_SPURR, offsetof(struct kvm_vcpu, arch.spurr));
@@ -597,6 +602,12 @@ int main(void)
 	DEFINE(VCPU_HOST_SPEFSCR, offsetof(struct kvm_vcpu, arch.host_spefscr));
 #endif
 
+#ifdef CONFIG_KVM_BOOKE_HV
+	DEFINE(VCPU_HOST_MAS4, offsetof(struct kvm_vcpu, arch.host_mas4));
+	DEFINE(VCPU_HOST_MAS6, offsetof(struct kvm_vcpu, arch.host_mas6));
+	DEFINE(VCPU_EPLC, offsetof(struct kvm_vcpu, arch.eplc));
+#endif
+
 #ifdef CONFIG_KVM_EXIT_TIMING
 	DEFINE(VCPU_TIMING_EXIT_TBU, offsetof(struct kvm_vcpu,
 						arch.timing_exit.tv32.tbu));

commit 2e7580b0e75d771d93e24e681031a165b1d31071
Merge: d25413efa953 cf9eeac46350
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 28 14:35:31 2012 -0700

    Merge branch 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull kvm updates from Avi Kivity:
     "Changes include timekeeping improvements, support for assigning host
      PCI devices that share interrupt lines, s390 user-controlled guests, a
      large ppc update, and random fixes."
    
    This is with the sign-off's fixed, hopefully next merge window we won't
    have rebased commits.
    
    * 'kvm-updates/3.4' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (130 commits)
      KVM: Convert intx_mask_lock to spin lock
      KVM: x86: fix kvm_write_tsc() TSC matching thinko
      x86: kvmclock: abstract save/restore sched_clock_state
      KVM: nVMX: Fix erroneous exception bitmap check
      KVM: Ignore the writes to MSR_K7_HWCR(3)
      KVM: MMU: make use of ->root_level in reset_rsvds_bits_mask
      KVM: PMU: add proper support for fixed counter 2
      KVM: PMU: Fix raw event check
      KVM: PMU: warn when pin control is set in eventsel msr
      KVM: VMX: Fix delayed load of shared MSRs
      KVM: use correct tlbs dirty type in cmpxchg
      KVM: Allow host IRQ sharing for assigned PCI 2.3 devices
      KVM: Ensure all vcpus are consistent with in-kernel irqchip settings
      KVM: x86 emulator: Allow PM/VM86 switch during task switch
      KVM: SVM: Fix CPL updates
      KVM: x86 emulator: VM86 segments must have DPL 3
      KVM: x86 emulator: Fix task switch privilege checks
      arch/powerpc/kvm/book3s_hv.c: included linux/sched.h twice
      KVM: x86 emulator: correctly mask pmc index bits in RDPMC instruction emulation
      KVM: mmu_notifier: Flush TLBs before releasing mmu_lock
      ...

commit 1b041885ae1d9938440fc2cf6a444b70ec0a86c9
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Mar 15 18:20:13 2012 +0000

    powerpc: Remove the remaining CONFIG_PPC_ISERIES pieces
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index cdd0d264415f..cc492e48ddfa 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -46,9 +46,6 @@
 #include <asm/hvcall.h>
 #include <asm/xics.h>
 #endif
-#ifdef CONFIG_PPC_ISERIES
-#include <asm/iseries/alpaca.h>
-#endif
 #ifdef CONFIG_PPC_POWERNV
 #include <asm/opal.h>
 #endif
@@ -384,17 +381,6 @@ int main(void)
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));
 #endif
 
-#ifdef CONFIG_PPC_ISERIES
-	/* the assembler miscalculates the VSID values */
-	DEFINE(PAGE_OFFSET_ESID, GET_ESID(PAGE_OFFSET));
-	DEFINE(PAGE_OFFSET_VSID, KERNEL_VSID(PAGE_OFFSET));
-	DEFINE(VMALLOC_START_ESID, GET_ESID(VMALLOC_START));
-	DEFINE(VMALLOC_START_VSID, KERNEL_VSID(VMALLOC_START));
-
-	/* alpaca */
-	DEFINE(ALPACA_SIZE, sizeof(struct alpaca));
-#endif
-
 	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);
 	DEFINE(PTE_SIZE, sizeof(pte_t));
 

commit 7230c5644188cd9e3fb380cc97dde00c464a3ba7
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 6 18:27:59 2012 +1100

    powerpc: Rework lazy-interrupt handling
    
    The current implementation of lazy interrupts handling has some
    issues that this tries to address.
    
    We don't do the various workarounds we need to do when re-enabling
    interrupts in some cases such as when returning from an interrupt
    and thus we may still lose or get delayed decrementer or doorbell
    interrupts.
    
    The current scheme also makes it much harder to handle the external
    "edge" interrupts provided by some BookE processors when using the
    EPR facility (External Proxy) and the Freescale Hypervisor.
    
    Additionally, we tend to keep interrupts hard disabled in a number
    of cases, such as decrementer interrupts, external interrupts, or
    when a masked decrementer interrupt is pending. This is sub-optimal.
    
    This is an attempt at fixing it all in one go by reworking the way
    we do the lazy interrupt disabling from the ground up.
    
    The base idea is to replace the "hard_enabled" field with a
    "irq_happened" field in which we store a bit mask of what interrupt
    occurred while soft-disabled.
    
    When re-enabling, either via arch_local_irq_restore() or when returning
    from an interrupt, we can now decide what to do by testing bits in that
    field.
    
    We then implement replaying of the missed interrupts either by
    re-using the existing exception frame (in exception exit case) or via
    the creation of a new one from an assembly trampoline (in the
    arch_local_irq_enable case).
    
    This removes the need to play with the decrementer to try to create
    fake interrupts, among others.
    
    In addition, this adds a few refinements:
    
     - We no longer  hard disable decrementer interrupts that occur
    while soft-disabled. We now simply bump the decrementer back to max
    (on BookS) or leave it stopped (on BookE) and continue with hard interrupts
    enabled, which means that we'll potentially get better sample quality from
    performance monitor interrupts.
    
     - Timer, decrementer and doorbell interrupts now hard-enable
    shortly after removing the source of the interrupt, which means
    they no longer run entirely hard disabled. Again, this will improve
    perf sample quality.
    
     - On Book3E 64-bit, we now make the performance monitor interrupt
    act as an NMI like Book3S (the necessary C code for that to work
    appear to already be present in the FSL perf code, notably calling
    nmi_enter instead of irq_enter). (This also fixes a bug where BookE
    perfmon interrupts could clobber r14 ... oops)
    
     - We could make "masked" decrementer interrupts act as NMIs when doing
    timer-based perf sampling to improve the sample quality.
    
    Signed-off-by-yet: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    ---
    
    v2:
    
    - Add hard-enable to decrementer, timer and doorbells
    - Fix CR clobber in masked irq handling on BookE
    - Make embedded perf interrupt act as an NMI
    - Add a PACA_HAPPENED_EE_EDGE for use by FSL if they want
      to retrigger an interrupt without preventing hard-enable
    
    v3:
    
     - Fix or vs. ori bug on Book3E
     - Fix enabling of interrupts for some exceptions on Book3E
    
    v4:
    
     - Fix resend of doorbells on return from interrupt on Book3E
    
    v5:
    
     - Rebased on top of my latest series, which involves some significant
    rework of some aspects of the patch.
    
    v6:
     - 32-bit compile fix
     - more compile fixes with various .config combos
     - factor out the asm code to soft-disable interrupts
     - remove the C wrapper around preempt_schedule_irq
    
    v7:
     - Fix a bug with hard irq state tracking on native power7

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 04caee7d9bc1..cdd0d264415f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -147,7 +147,7 @@ int main(void)
 	DEFINE(PACAKBASE, offsetof(struct paca_struct, kernelbase));
 	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
-	DEFINE(PACAHARDIRQEN, offsetof(struct paca_struct, hard_enabled));
+	DEFINE(PACAIRQHAPPENED, offsetof(struct paca_struct, irq_happened));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
 #ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,

commit 697d3899dcb4bcd918d060a92db57b794e56b077
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:36:37 2011 +0000

    KVM: PPC: Implement MMIO emulation support for Book3S HV guests
    
    This provides the low-level support for MMIO emulation in Book3S HV
    guests.  When the guest tries to map a page which is not covered by
    any memslot, that page is taken to be an MMIO emulation page.  Instead
    of inserting a valid HPTE, we insert an HPTE that has the valid bit
    clear but another hypervisor software-use bit set, which we call
    HPTE_V_ABSENT, to indicate that this is an absent page.  An
    absent page is treated much like a valid page as far as guest hcalls
    (H_ENTER, H_REMOVE, H_READ etc.) are concerned, except of course that
    an absent HPTE doesn't need to be invalidated with tlbie since it
    was never valid as far as the hardware is concerned.
    
    When the guest accesses a page for which there is an absent HPTE, it
    will take a hypervisor data storage interrupt (HDSI) since we now set
    the VPM1 bit in the LPCR.  Our HDSI handler for HPTE-not-present faults
    looks up the hash table and if it finds an absent HPTE mapping the
    requested virtual address, will switch to kernel mode and handle the
    fault in kvmppc_book3s_hv_page_fault(), which at present just calls
    kvmppc_hv_emulate_mmio() to set up the MMIO emulation.
    
    This is based on an earlier patch by Benjamin Herrenschmidt, but since
    heavily reworked.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e7bfcf81b746..8e0db0b12dd0 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -455,6 +455,7 @@ int main(void)
 	DEFINE(KVM_LAST_VCPU, offsetof(struct kvm, arch.last_vcpu));
 	DEFINE(KVM_LPCR, offsetof(struct kvm, arch.lpcr));
 	DEFINE(KVM_RMOR, offsetof(struct kvm, arch.rmor));
+	DEFINE(KVM_VRMA_SLB_V, offsetof(struct kvm, arch.vrma_slb_v));
 	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 #endif

commit b59049720dd95021dfe0d9f4e1fa9458a67cfe29
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Nov 8 18:23:30 2011 -0600

    KVM: PPC: Paravirtualize SPRG4-7, ESR, PIR, MASn
    
    This allows additional registers to be accessed by the guest
    in PR-mode KVM without trapping.
    
    SPRG4-7 are readable from userspace.  On booke, KVM will sync
    these registers when it enters the guest, so that accesses from
    guest userspace will work.  The guest kernel, OTOH, must consistently
    use either the real registers or the shared area between exits.  This
    also applies to the already-paravirted SPRG3.
    
    On non-booke, it's not clear to what extent SPRG4-7 are supported
    (they're not architected for book3s, but exist on at least some classic
    chips).  They are copied in the get/set regs ioctls, but I do not see any
    non-booke emulation.  I also do not see any syncing with real registers
    (in PR-mode) including the user-readable SPRG3.  This patch should not
    make that situation any worse.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 04caee7d9bc1..e7bfcf81b746 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -426,16 +426,23 @@ int main(void)
 	DEFINE(VCPU_SPRG2, offsetof(struct kvm_vcpu, arch.shregs.sprg2));
 	DEFINE(VCPU_SPRG3, offsetof(struct kvm_vcpu, arch.shregs.sprg3));
 #endif
-	DEFINE(VCPU_SPRG4, offsetof(struct kvm_vcpu, arch.sprg4));
-	DEFINE(VCPU_SPRG5, offsetof(struct kvm_vcpu, arch.sprg5));
-	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));
-	DEFINE(VCPU_SPRG7, offsetof(struct kvm_vcpu, arch.sprg7));
+	DEFINE(VCPU_SHARED_SPRG4, offsetof(struct kvm_vcpu_arch_shared, sprg4));
+	DEFINE(VCPU_SHARED_SPRG5, offsetof(struct kvm_vcpu_arch_shared, sprg5));
+	DEFINE(VCPU_SHARED_SPRG6, offsetof(struct kvm_vcpu_arch_shared, sprg6));
+	DEFINE(VCPU_SHARED_SPRG7, offsetof(struct kvm_vcpu_arch_shared, sprg7));
 	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
 	DEFINE(VCPU_SHADOW_PID1, offsetof(struct kvm_vcpu, arch.shadow_pid1));
 	DEFINE(VCPU_SHARED, offsetof(struct kvm_vcpu, arch.shared));
 	DEFINE(VCPU_SHARED_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
 	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
 
+	DEFINE(VCPU_SHARED_MAS0, offsetof(struct kvm_vcpu_arch_shared, mas0));
+	DEFINE(VCPU_SHARED_MAS1, offsetof(struct kvm_vcpu_arch_shared, mas1));
+	DEFINE(VCPU_SHARED_MAS2, offsetof(struct kvm_vcpu_arch_shared, mas2));
+	DEFINE(VCPU_SHARED_MAS7_3, offsetof(struct kvm_vcpu_arch_shared, mas7_3));
+	DEFINE(VCPU_SHARED_MAS4, offsetof(struct kvm_vcpu_arch_shared, mas4));
+	DEFINE(VCPU_SHARED_MAS6, offsetof(struct kvm_vcpu_arch_shared, mas6));
+
 	/* book3s */
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	DEFINE(KVM_LPID, offsetof(struct kvm, arch.lpid));

commit 2fde6d20bb75b53f1ead383b4713f95d0d6d9f59
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 5 19:47:26 2011 +0000

    powerpc: Provide a way for KVM to indicate that NV GPR values are lost
    
    This fixes a problem where a CPU thread coming out of nap mode can
    think it has valid values in the nonvolatile GPRs (r14 - r31) as saved
    away in power7_idle, but in fact the values have been trashed because
    the thread was used for KVM in the mean time.  The result is that the
    thread crashes because code that called power7_idle (e.g.,
    pnv_smp_cpu_kill_self()) goes to use values in registers that have
    been trashed.
    
    The bit field in SRR1 that tells whether state was lost only reflects
    the most recent nap, which may not have been the nap instruction in
    power7_idle.  So we need an extra PACA field to indicate that state
    has been lost even if SRR1 indicates that the most recent nap didn't
    lose state.  We clear this field when saving the state in power7_idle,
    we set it to a non-zero value when we use the thread for KVM, and we
    test it in power7_wakeup_noloss.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 7c5324f1ec9c..04caee7d9bc1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -208,6 +208,7 @@ int main(void)
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
+	DEFINE(PACA_NAPSTATELOST, offsetof(struct paca_struct, nap_state_lost));
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */

commit 1197ab2942f920f261952de0c392ac749a35796b
Merge: ec773e99ab4a 96cc017c5b7e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 17:12:03 2011 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (106 commits)
      powerpc/p3060qds: Add support for P3060QDS board
      powerpc/83xx: Add shutdown request support to MCU handling on MPC8349 MITX
      powerpc/85xx: Make kexec to interate over online cpus
      powerpc/fsl_booke: Fix comment in head_fsl_booke.S
      powerpc/85xx: issue 15 EOI after core reset for FSL CoreNet devices
      powerpc/8xxx: Fix interrupt handling in MPC8xxx GPIO driver
      powerpc/85xx: Add 'fsl,pq3-gpio' compatiable for GPIO driver
      powerpc/86xx: Correct Gianfar support for GE boards
      powerpc/cpm: Clear muram before it is in use.
      drivers/virt: add ioctl for 32-bit compat on 64-bit to fsl-hv-manager
      powerpc/fsl_msi: add support for "msi-address-64" property
      powerpc/85xx: Setup secondary cores PIR with hard SMP id
      powerpc/fsl-booke: Fix settlbcam for 64-bit
      powerpc/85xx: Adding DCSR node to dtsi device trees
      powerpc/85xx: clean up FPGA device tree nodes for Freecsale QorIQ boards
      powerpc/85xx: fix PHYS_64BIT selection for P1022DS
      powerpc/fsl-booke: Fix setup_initial_memory_limit to not blindly map
      powerpc: respect mem= setting for early memory limit setup
      powerpc: Update corenet64_smp_defconfig
      powerpc: Update mpc85xx/corenet 32-bit defconfigs
      ...
    
    Fix up trivial conflicts in:
     - arch/powerpc/configs/40x/hcu4_defconfig
            removed stale file, edited elsewhere
     - arch/powerpc/include/asm/udbg.h, arch/powerpc/kernel/udbg.c:
            added opal and gelic drivers vs added ePAPR driver
     - drivers/tty/serial/8250.c
            moved UPIO_TSI to powerpc vs removed UPIO_DWAPB support

commit 19ccb76a1938ab364a412253daec64613acbf3df
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Jul 23 17:42:46 2011 +1000

    KVM: PPC: Implement H_CEDE hcall for book3s_hv in real-mode code
    
    With a KVM guest operating in SMT4 mode (i.e. 4 hardware threads per
    core), whenever a CPU goes idle, we have to pull all the other
    hardware threads in the core out of the guest, because the H_CEDE
    hcall is handled in the kernel.  This is inefficient.
    
    This adds code to book3s_hv_rmhandlers.S to handle the H_CEDE hcall
    in real mode.  When a guest vcpu does an H_CEDE hcall, we now only
    exit to the kernel if all the other vcpus in the same core are also
    idle.  Otherwise we mark this vcpu as napping, save state that could
    be lost in nap mode (mainly GPRs and FPRs), and execute the nap
    instruction.  When the thread wakes up, because of a decrementer or
    external interrupt, we come back in at kvm_start_guest (from the
    system reset interrupt vector), find the `napping' flag set in the
    paca, and go to the resume path.
    
    This has some other ramifications.  First, when starting a core, we
    now start all the threads, both those that are immediately runnable and
    those that are idle.  This is so that we don't have to pull all the
    threads out of the guest when an idle thread gets a decrementer interrupt
    and wants to start running.  In fact the idle threads will all start
    with the H_CEDE hcall returning; being idle they will just do another
    H_CEDE immediately and go to nap mode.
    
    This required some changes to kvmppc_run_core() and kvmppc_run_vcpu().
    These functions have been restructured to make them simpler and clearer.
    We introduce a level of indirection in the wait queue that gets woken
    when external and decrementer interrupts get generated for a vcpu, so
    that we can have the 4 vcpus in a vcore using the same wait queue.
    We need this because the 4 vcpus are being handled by one thread.
    
    Secondly, when we need to exit from the guest to the kernel, we now
    have to generate an IPI for any napping threads, because an HDEC
    interrupt doesn't wake up a napping thread.
    
    Thirdly, we now need to be able to handle virtual external interrupts
    and decrementer interrupts becoming pending while a thread is napping,
    and deliver those interrupts to the guest when the thread wakes.
    This is done in kvmppc_cede_reentry, just before fast_guest_return.
    
    Finally, since we are not using the generic kvm_vcpu_block for book3s_hv,
    and hence not calling kvm_arch_vcpu_runnable, we can remove the #ifdef
    from kvm_arch_vcpu_runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e069c766695d..69f7ffe7f674 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -44,6 +44,7 @@
 #include <asm/compat.h>
 #include <asm/mmu.h>
 #include <asm/hvcall.h>
+#include <asm/xics.h>
 #endif
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iseries/alpaca.h>
@@ -460,6 +461,8 @@ int main(void)
 	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
 	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
 	DEFINE(VCPU_PENDING_EXC, offsetof(struct kvm_vcpu, arch.pending_exceptions));
+	DEFINE(VCPU_CEDED, offsetof(struct kvm_vcpu, arch.ceded));
+	DEFINE(VCPU_PRODDED, offsetof(struct kvm_vcpu, arch.prodded));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa));
 	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
 	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
@@ -475,6 +478,7 @@ int main(void)
 	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
 	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
 	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
+	DEFINE(VCORE_NAPPING_THREADS, offsetof(struct kvmppc_vcore, napping_threads));
 	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
 			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
@@ -532,6 +536,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_SCRATCH1, scratch1);
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
 	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
+	HSTATE_FIELD(HSTATE_NAPPING, napping);
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);
@@ -544,6 +549,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_DSCR, host_dscr);
 	HSTATE_FIELD(HSTATE_DABR, dabr);
 	HSTATE_FIELD(HSTATE_DECEXP, dec_expires);
+	DEFINE(IPI_PRIORITY, IPI_PRIORITY);
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 
 #else /* CONFIG_PPC_BOOK3S */

commit 02143947603fe90237a0423d34dd8943de229f78
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Jul 23 17:41:44 2011 +1000

    KVM: PPC: book3s_pr: Simplify transitions between virtual and real mode
    
    This simplifies the way that the book3s_pr makes the transition to
    real mode when entering the guest.  We now call kvmppc_entry_trampoline
    (renamed from kvmppc_rmcall) in the base kernel using a normal function
    call instead of doing an indirect call through a pointer in the vcpu.
    If kvm is a module, the module loader takes care of generating a
    trampoline as it does for other calls to functions outside the module.
    
    kvmppc_entry_trampoline then disables interrupts and jumps to
    kvmppc_handler_trampoline_enter in real mode using an rfi[d].
    That then uses the link register as the address to return to
    (potentially in module space) when the guest exits.
    
    This also simplifies the way that we call the Linux interrupt handler
    when we exit the guest due to an external, decrementer or performance
    monitor interrupt.  Instead of turning on the MMU, then deciding that
    we need to call the Linux handler and turning the MMU back off again,
    we now go straight to the handler at the point where we would turn the
    MMU on.  The handler will then return to the virtual-mode code
    (potentially in the module).
    
    Along the way, this moves the setting and clearing of the HID5 DCBZ32
    bit into real-mode interrupts-off code, and also makes sure that
    we clear the MSR[RI] bit before loading values into SRR0/1.
    
    The net result is that we no longer need any code addresses to be
    stored in vcpu->arch.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 5f078bc2063e..e069c766695d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -449,8 +449,6 @@ int main(void)
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_KVM, offsetof(struct kvm_vcpu, kvm));
 	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
-	DEFINE(VCPU_HOST_RETIP, offsetof(struct kvm_vcpu, arch.host_retip));
-	DEFINE(VCPU_HOST_MSR, offsetof(struct kvm_vcpu, arch.host_msr));
 	DEFINE(VCPU_PURR, offsetof(struct kvm_vcpu, arch.purr));
 	DEFINE(VCPU_SPURR, offsetof(struct kvm_vcpu, arch.spurr));
 	DEFINE(VCPU_DSCR, offsetof(struct kvm_vcpu, arch.dscr));
@@ -458,10 +456,6 @@ int main(void)
 	DEFINE(VCPU_UAMOR, offsetof(struct kvm_vcpu, arch.uamor));
 	DEFINE(VCPU_CTRL, offsetof(struct kvm_vcpu, arch.ctrl));
 	DEFINE(VCPU_DABR, offsetof(struct kvm_vcpu, arch.dabr));
-	DEFINE(VCPU_TRAMPOLINE_LOWMEM, offsetof(struct kvm_vcpu, arch.trampoline_lowmem));
-	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
-	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));
-	DEFINE(VCPU_RMCALL, offsetof(struct kvm_vcpu, arch.rmcall));
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
 	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
 	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
@@ -537,6 +531,7 @@ int main(void)
 	HSTATE_FIELD(HSTATE_SCRATCH0, scratch0);
 	HSTATE_FIELD(HSTATE_SCRATCH1, scratch1);
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
+	HSTATE_FIELD(HSTATE_RESTORE_HID5, restore_hid5);
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);

commit ed79ba9e15f84cef05aba5cbfe6e93f9b43c31f4
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Sep 19 17:45:04 2011 +0000

    powerpc/powernv: Machine check and other system interrupts
    
    OPAL can handle various interrupt for us such as Machine Checks (it
    performs all sorts of recovery tasks and passes back control to us with
    informations about the error), Hardware Management Interrupts and Softpatch
    interrupts.
    
    This wires up the mechanisms and prints out specific informations returned
    by HAL when a machine check occurs.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 5f078bc2063e..536ffa897c6c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -48,6 +48,9 @@
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iseries/alpaca.h>
 #endif
+#ifdef CONFIG_PPC_POWERNV
+#include <asm/opal.h>
+#endif
 #if defined(CONFIG_KVM) || defined(CONFIG_KVM_GUEST)
 #include <linux/kvm_host.h>
 #endif
@@ -609,5 +612,12 @@ int main(void)
 					arch.timing_last_enter.tv32.tbl));
 #endif
 
+#ifdef CONFIG_PPC_POWERNV
+	DEFINE(OPAL_MC_GPR3, offsetof(struct opal_machine_check_event, gpr3));
+	DEFINE(OPAL_MC_SRR0, offsetof(struct opal_machine_check_event, srr0));
+	DEFINE(OPAL_MC_SRR1, offsetof(struct opal_machine_check_event, srr1));
+	DEFINE(PACA_OPAL_MC_EVT, offsetof(struct paca_struct, opal_mc_evt));
+#endif
+
 	return 0;
 }

commit 184475029a724b6b900d88fc3a5f462a6107d5af
Merge: 3b76eefe0f97 f1f4ee01c0d3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 22:59:39 2011 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (99 commits)
      drivers/virt: add missing linux/interrupt.h to fsl_hypervisor.c
      powerpc/85xx: fix mpic configuration in CAMP mode
      powerpc: Copy back TIF flags on return from softirq stack
      powerpc/64: Make server perfmon only built on ppc64 server devices
      powerpc/pseries: Fix hvc_vio.c build due to recent changes
      powerpc: Exporting boot_cpuid_phys
      powerpc: Add CFAR to oops output
      hvc_console: Add kdb support
      powerpc/pseries: Fix hvterm_raw_get_chars to accept < 16 chars, fixing xmon
      powerpc/irq: Quieten irq mapping printks
      powerpc: Enable lockup and hung task detectors in pseries and ppc64 defeconfigs
      powerpc: Add mpt2sas driver to pseries and ppc64 defconfig
      powerpc: Disable IRQs off tracer in ppc64 defconfig
      powerpc: Sync pseries and ppc64 defconfigs
      powerpc/pseries/hvconsole: Fix dropped console output
      hvc_console: Improve tty/console put_chars handling
      powerpc/kdump: Fix timeout in crash_kexec_wait_realmode
      powerpc/mm: Fix output of total_ram.
      powerpc/cpufreq: Add cpufreq driver for Momentum Maple boards
      powerpc: Correct annotations of pmu registration functions
      ...
    
    Fix up trivial Kconfig/Makefile conflicts in arch/powerpc, drivers, and
    drivers/cpufreq

commit 9e368f2915601cd5bc7f5fd638b58435b018bbd7
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:40:08 2011 +0000

    KVM: PPC: book3s_hv: Add support for PPC970-family processors
    
    This adds support for running KVM guests in supervisor mode on those
    PPC970 processors that have a usable hypervisor mode.  Unfortunately,
    Apple G5 machines have supervisor mode disabled (MSR[HV] is forced to
    1), but the YDL PowerStation does have a usable hypervisor mode.
    
    There are several differences between the PPC970 and POWER7 in how
    guests are managed.  These differences are accommodated using the
    CPU_FTR_ARCH_201 (PPC970) and CPU_FTR_ARCH_206 (POWER7) CPU feature
    bits.  Notably, on PPC970:
    
    * The LPCR, LPID or RMOR registers don't exist, and the functions of
      those registers are provided by bits in HID4 and one bit in HID0.
    
    * External interrupts can be directed to the hypervisor, but unlike
      POWER7 they are masked by MSR[EE] in non-hypervisor modes and use
      SRR0/1 not HSRR0/1.
    
    * There is no virtual RMA (VRMA) mode; the guest must use an RMO
      (real mode offset) area.
    
    * The TLB entries are not tagged with the LPID, so it is necessary to
      flush the whole TLB on partition switch.  Furthermore, when switching
      partitions we have to ensure that no other CPU is executing the tlbie
      or tlbsync instructions in either the old or the new partition,
      otherwise undefined behaviour can occur.
    
    * The PMU has 8 counters (PMC registers) rather than 6.
    
    * The DSCR, PURR, SPURR, AMR, AMOR, UAMOR registers don't exist.
    
    * The SLB has 64 entries rather than 32.
    
    * There is no mediated external interrupt facility, so if we switch to
      a guest that has a virtual external interrupt pending but the guest
      has MSR[EE] = 0, we have to arrange to have an interrupt pending for
      it so that we can get control back once it re-enables interrupts.  We
      do that by sending ourselves an IPI with smp_send_reschedule after
      hard-disabling interrupts.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index f4aba938166b..54b935f2f5de 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -128,6 +128,7 @@ int main(void)
 	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
 	/* paca */
 	DEFINE(PACA_SIZE, sizeof(struct paca_struct));
+	DEFINE(PACA_LOCK_TOKEN, offsetof(struct paca_struct, lock_token));
 	DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));
 	DEFINE(PACAPROCSTART, offsetof(struct paca_struct, cpu_start));
 	DEFINE(PACAKSAVE, offsetof(struct paca_struct, kstack));

commit aa04b4cc5be64b4fb9ef4e0fdf2418e2f4737fb2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:25:44 2011 +0000

    KVM: PPC: Allocate RMAs (Real Mode Areas) at boot for use by guests
    
    This adds infrastructure which will be needed to allow book3s_hv KVM to
    run on older POWER processors, including PPC970, which don't support
    the Virtual Real Mode Area (VRMA) facility, but only the Real Mode
    Offset (RMO) facility.  These processors require a physically
    contiguous, aligned area of memory for each guest.  When the guest does
    an access in real mode (MMU off), the address is compared against a
    limit value, and if it is lower, the address is ORed with an offset
    value (from the Real Mode Offset Register (RMOR)) and the result becomes
    the real address for the access.  The size of the RMA has to be one of
    a set of supported values, which usually includes 64MB, 128MB, 256MB
    and some larger powers of 2.
    
    Since we are unlikely to be able to allocate 64MB or more of physically
    contiguous memory after the kernel has been running for a while, we
    allocate a pool of RMAs at boot time using the bootmem allocator.  The
    size and number of the RMAs can be set using the kvm_rma_size=xx and
    kvm_rma_count=xx kernel command line options.
    
    KVM exports a new capability, KVM_CAP_PPC_RMA, to signal the availability
    of the pool of preallocated RMAs.  The capability value is 1 if the
    processor can use an RMA but doesn't require one (because it supports
    the VRMA facility), or 2 if the processor requires an RMA for each guest.
    
    This adds a new ioctl, KVM_ALLOCATE_RMA, which allocates an RMA from the
    pool and returns a file descriptor which can be used to map the RMA.  It
    also returns the size of the RMA in the argument structure.
    
    Having an RMA means we will get multiple KMV_SET_USER_MEMORY_REGION
    ioctl calls from userspace.  To cope with this, we now preallocate the
    kvm->arch.ram_pginfo array when the VM is created with a size sufficient
    for up to 64GB of guest memory.  Subsequently we will get rid of this
    array and use memory associated with each memslot instead.
    
    This moves most of the code that translates the user addresses into
    host pfns (page frame numbers) out of kvmppc_prepare_vrma up one level
    to kvmppc_core_prepare_memory_region.  Also, instead of having to look
    up the VMA for each page in order to check the page size, we now check
    that the pages we get are compound pages of 16MB.  However, if we are
    adding memory that is mapped to an RMA, we don't bother with calling
    get_user_pages_fast and instead just offset from the base pfn for the
    RMA.
    
    Typically the RMA gets added after vcpus are created, which makes it
    inconvenient to have the LPCR (logical partition control register) value
    in the vcpu->arch struct, since the LPCR controls whether the processor
    uses RMA or VRMA for the guest.  This moves the LPCR value into the
    kvm->arch struct and arranges for the MER (mediated external request)
    bit, which is the only bit that varies between vcpus, to be set in
    assembly code when going into the guest if there is a pending external
    interrupt request.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index d0f2387fd792..f4aba938166b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -437,6 +437,8 @@ int main(void)
 	DEFINE(KVM_TLBIE_LOCK, offsetof(struct kvm, arch.tlbie_lock));
 	DEFINE(KVM_ONLINE_CPUS, offsetof(struct kvm, online_vcpus.counter));
 	DEFINE(KVM_LAST_VCPU, offsetof(struct kvm, arch.last_vcpu));
+	DEFINE(KVM_LPCR, offsetof(struct kvm, arch.lpcr));
+	DEFINE(KVM_RMOR, offsetof(struct kvm, arch.rmor));
 	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
 	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
 #endif
@@ -459,7 +461,7 @@ int main(void)
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
 	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
 	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
-	DEFINE(VCPU_LPCR, offsetof(struct kvm_vcpu, arch.lpcr));
+	DEFINE(VCPU_PENDING_EXC, offsetof(struct kvm_vcpu, arch.pending_exceptions));
 	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa));
 	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
 	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));

commit 371fefd6f2dc46668e00871930dde613b88d4bde
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:23:08 2011 +0000

    KVM: PPC: Allow book3s_hv guests to use SMT processor modes
    
    This lifts the restriction that book3s_hv guests can only run one
    hardware thread per core, and allows them to use up to 4 threads
    per core on POWER7.  The host still has to run single-threaded.
    
    This capability is advertised to qemu through a new KVM_CAP_PPC_SMT
    capability.  The return value of the ioctl querying this capability
    is the number of vcpus per virtual CPU core (vcore), currently 4.
    
    To use this, the host kernel should be booted with all threads
    active, and then all the secondary threads should be offlined.
    This will put the secondary threads into nap mode.  KVM will then
    wake them from nap mode and use them for running guest code (while
    they are still offline).  To wake the secondary threads, we send
    them an IPI using a new xics_wake_cpu() function, implemented in
    arch/powerpc/sysdev/xics/icp-native.c.  In other words, at this stage
    we assume that the platform has a XICS interrupt controller and
    we are using icp-native.c to drive it.  Since the woken thread will
    need to acknowledge and clear the IPI, we also export the base
    physical address of the XICS registers using kvmppc_set_xics_phys()
    for use in the low-level KVM book3s code.
    
    When a vcpu is created, it is assigned to a virtual CPU core.
    The vcore number is obtained by dividing the vcpu number by the
    number of threads per core in the host.  This number is exported
    to userspace via the KVM_CAP_PPC_SMT capability.  If qemu wishes
    to run the guest in single-threaded mode, it should make all vcpu
    numbers be multiples of the number of threads per core.
    
    We distinguish three states of a vcpu: runnable (i.e., ready to execute
    the guest), blocked (that is, idle), and busy in host.  We currently
    implement a policy that the vcore can run only when all its threads
    are runnable or blocked.  This way, if a vcpu needs to execute elsewhere
    in the kernel or in qemu, it can do so without being starved of CPU
    by the other vcpus.
    
    When a vcore starts to run, it executes in the context of one of the
    vcpu threads.  The other vcpu threads all go to sleep and stay asleep
    until something happens requiring the vcpu thread to return to qemu,
    or to wake up to run the vcore (this can happen when another vcpu
    thread goes from busy in host state to blocked).
    
    It can happen that a vcpu goes from blocked to runnable state (e.g.
    because of an interrupt), and the vcore it belongs to is already
    running.  In that case it can start to run immediately as long as
    the none of the vcpus in the vcore have started to exit the guest.
    We send the next free thread in the vcore an IPI to get it to start
    to execute the guest.  It synchronizes with the other threads via
    the vcore->entry_exit_count field to make sure that it doesn't go
    into the guest if the other vcpus are exiting by the time that it
    is ready to actually enter the guest.
    
    Note that there is no fixed relationship between the hardware thread
    number and the vcpu number.  Hardware threads are assigned to vcpus
    as they become runnable, so we will always use the lower-numbered
    hardware threads in preference to higher-numbered threads if not all
    the vcpus in the vcore are runnable, regardless of which vcpus are
    runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c70d106bf1a4..d0f2387fd792 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -471,6 +471,10 @@ int main(void)
 	DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
+	DEFINE(VCPU_PTID, offsetof(struct kvm_vcpu, arch.ptid));
+	DEFINE(VCORE_ENTRY_EXIT, offsetof(struct kvmppc_vcore, entry_exit_count));
+	DEFINE(VCORE_NAP_COUNT, offsetof(struct kvmppc_vcore, nap_count));
+	DEFINE(VCORE_IN_GUEST, offsetof(struct kvmppc_vcore, in_guest));
 	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
 			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
 	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
@@ -530,6 +534,8 @@ int main(void)
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);
+	HSTATE_FIELD(HSTATE_KVM_VCORE, kvm_vcore);
+	HSTATE_FIELD(HSTATE_XICS_PHYS, xics_phys);
 	HSTATE_FIELD(HSTATE_MMCR, host_mmcr);
 	HSTATE_FIELD(HSTATE_PMC, host_pmc);
 	HSTATE_FIELD(HSTATE_PURR, host_purr);

commit a8606e20e41a8149456bafdf76ad29d47672027c
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:22:05 2011 +0000

    KVM: PPC: Handle some PAPR hcalls in the kernel
    
    This adds the infrastructure for handling PAPR hcalls in the kernel,
    either early in the guest exit path while we are still in real mode,
    or later once the MMU has been turned back on and we are in the full
    kernel context.  The advantage of handling hcalls in real mode if
    possible is that we avoid two partition switches -- and this will
    become more important when we support SMT4 guests, since a partition
    switch means we have to pull all of the threads in the core out of
    the guest.  The disadvantage is that we can only access the kernel
    linear mapping, not anything vmalloced or ioremapped, since the MMU
    is off.
    
    This also adds code to handle the following hcalls in real mode:
    
    H_ENTER       Add an HPTE to the hashed page table
    H_REMOVE      Remove an HPTE from the hashed page table
    H_READ        Read HPTEs from the hashed page table
    H_PROTECT     Change the protection bits in an HPTE
    H_BULK_REMOVE Remove up to 4 HPTEs from the hashed page table
    H_SET_DABR    Set the data address breakpoint register
    
    Plus code to handle the following hcalls in the kernel:
    
    H_CEDE        Idle the vcpu until an interrupt or H_PROD hcall arrives
    H_PROD        Wake up a ceded vcpu
    H_REGISTER_VPA Register a virtual processor area (VPA)
    
    The code that runs in real mode has to be in the base kernel, not in
    the module, if KVM is compiled as a module.  The real-mode code can
    only access the kernel linear mapping, not vmalloc or ioremap space.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 936267462cae..c70d106bf1a4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -189,6 +189,7 @@ int main(void)
 	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
 	DEFINE(LPPACA_PMCINUSE, offsetof(struct lppaca, pmcregs_in_use));
 	DEFINE(LPPACA_DTLIDX, offsetof(struct lppaca, dtl_idx));
+	DEFINE(LPPACA_YIELDCOUNT, offsetof(struct lppaca, yield_count));
 	DEFINE(PACA_DTL_RIDX, offsetof(struct paca_struct, dtl_ridx));
 #endif /* CONFIG_PPC_STD_MMU_64 */
 	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
@@ -459,6 +460,7 @@ int main(void)
 	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
 	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
 	DEFINE(VCPU_LPCR, offsetof(struct kvm_vcpu, arch.lpcr));
+	DEFINE(VCPU_VPA, offsetof(struct kvm_vcpu, arch.vpa));
 	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
 	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
 	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));

commit de56a948b9182fbcf92cb8212f114de096c2d574
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:21:34 2011 +0000

    KVM: PPC: Add support for Book3S processors in hypervisor mode
    
    This adds support for KVM running on 64-bit Book 3S processors,
    specifically POWER7, in hypervisor mode.  Using hypervisor mode means
    that the guest can use the processor's supervisor mode.  That means
    that the guest can execute privileged instructions and access privileged
    registers itself without trapping to the host.  This gives excellent
    performance, but does mean that KVM cannot emulate a processor
    architecture other than the one that the hardware implements.
    
    This code assumes that the guest is running paravirtualized using the
    PAPR (Power Architecture Platform Requirements) interface, which is the
    interface that IBM's PowerVM hypervisor uses.  That means that existing
    Linux distributions that run on IBM pSeries machines will also run
    under KVM without modification.  In order to communicate the PAPR
    hypercalls to qemu, this adds a new KVM_EXIT_PAPR_HCALL exit code
    to include/linux/kvm.h.
    
    Currently the choice between book3s_hv support and book3s_pr support
    (i.e. the existing code, which runs the guest in user mode) has to be
    made at kernel configuration time, so a given kernel binary can only
    do one or the other.
    
    This new book3s_hv code doesn't support MMIO emulation at present.
    Since we are running paravirtualized guests, this isn't a serious
    restriction.
    
    With the guest running in supervisor mode, most exceptions go straight
    to the guest.  We will never get data or instruction storage or segment
    interrupts, alignment interrupts, decrementer interrupts, program
    interrupts, single-step interrupts, etc., coming to the hypervisor from
    the guest.  Therefore this introduces a new KVMTEST_NONHV macro for the
    exception entry path so that we don't have to do the KVM test on entry
    to those exception handlers.
    
    We do however get hypervisor decrementer, hypervisor data storage,
    hypervisor instruction storage, and hypervisor emulation assist
    interrupts, so we have to handle those.
    
    In hypervisor mode, real-mode accesses can access all of RAM, not just
    a limited amount.  Therefore we put all the guest state in the vcpu.arch
    and use the shadow_vcpu in the PACA only for temporary scratch space.
    We allocate the vcpu with kzalloc rather than vzalloc, and we don't use
    anything in the kvmppc_vcpu_book3s struct, so we don't allocate it.
    We don't have a shared page with the guest, but we still need a
    kvm_vcpu_arch_shared struct to store the values of various registers,
    so we include one in the vcpu_arch struct.
    
    The POWER7 processor has a restriction that all threads in a core have
    to be in the same partition.  MMU-on kernel code counts as a partition
    (partition 0), so we have to do a partition switch on every entry to and
    exit from the guest.  At present we require the host and guest to run
    in single-thread mode because of this hardware restriction.
    
    This code allocates a hashed page table for the guest and initializes
    it with HPTEs for the guest's Virtual Real Memory Area (VRMA).  We
    require that the guest memory is allocated using 16MB huge pages, in
    order to simplify the low-level memory management.  This also means that
    we can get away without tracking paging activity in the host for now,
    since huge pages can't be paged or swapped.
    
    This also adds a few new exports needed by the book3s_hv code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index dabfb7346f36..936267462cae 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -187,6 +187,7 @@ int main(void)
 	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
 	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
 	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
+	DEFINE(LPPACA_PMCINUSE, offsetof(struct lppaca, pmcregs_in_use));
 	DEFINE(LPPACA_DTLIDX, offsetof(struct lppaca, dtl_idx));
 	DEFINE(PACA_DTL_RIDX, offsetof(struct paca_struct, dtl_ridx));
 #endif /* CONFIG_PPC_STD_MMU_64 */
@@ -392,6 +393,29 @@ int main(void)
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_VRSAVE, offsetof(struct kvm_vcpu, arch.vrsave));
+	DEFINE(VCPU_FPRS, offsetof(struct kvm_vcpu, arch.fpr));
+	DEFINE(VCPU_FPSCR, offsetof(struct kvm_vcpu, arch.fpscr));
+#ifdef CONFIG_ALTIVEC
+	DEFINE(VCPU_VRS, offsetof(struct kvm_vcpu, arch.vr));
+	DEFINE(VCPU_VSCR, offsetof(struct kvm_vcpu, arch.vscr));
+#endif
+#ifdef CONFIG_VSX
+	DEFINE(VCPU_VSRS, offsetof(struct kvm_vcpu, arch.vsr));
+#endif
+	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
+	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
+	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
+	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
+	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	DEFINE(VCPU_MSR, offsetof(struct kvm_vcpu, arch.shregs.msr));
+	DEFINE(VCPU_SRR0, offsetof(struct kvm_vcpu, arch.shregs.srr0));
+	DEFINE(VCPU_SRR1, offsetof(struct kvm_vcpu, arch.shregs.srr1));
+	DEFINE(VCPU_SPRG0, offsetof(struct kvm_vcpu, arch.shregs.sprg0));
+	DEFINE(VCPU_SPRG1, offsetof(struct kvm_vcpu, arch.shregs.sprg1));
+	DEFINE(VCPU_SPRG2, offsetof(struct kvm_vcpu, arch.shregs.sprg2));
+	DEFINE(VCPU_SPRG3, offsetof(struct kvm_vcpu, arch.shregs.sprg3));
+#endif
 	DEFINE(VCPU_SPRG4, offsetof(struct kvm_vcpu, arch.sprg4));
 	DEFINE(VCPU_SPRG5, offsetof(struct kvm_vcpu, arch.sprg5));
 	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));
@@ -403,17 +427,60 @@ int main(void)
 	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
 
 	/* book3s */
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	DEFINE(KVM_LPID, offsetof(struct kvm, arch.lpid));
+	DEFINE(KVM_SDR1, offsetof(struct kvm, arch.sdr1));
+	DEFINE(KVM_HOST_LPID, offsetof(struct kvm, arch.host_lpid));
+	DEFINE(KVM_HOST_LPCR, offsetof(struct kvm, arch.host_lpcr));
+	DEFINE(KVM_HOST_SDR1, offsetof(struct kvm, arch.host_sdr1));
+	DEFINE(KVM_TLBIE_LOCK, offsetof(struct kvm, arch.tlbie_lock));
+	DEFINE(KVM_ONLINE_CPUS, offsetof(struct kvm, online_vcpus.counter));
+	DEFINE(KVM_LAST_VCPU, offsetof(struct kvm, arch.last_vcpu));
+	DEFINE(VCPU_DSISR, offsetof(struct kvm_vcpu, arch.shregs.dsisr));
+	DEFINE(VCPU_DAR, offsetof(struct kvm_vcpu, arch.shregs.dar));
+#endif
 #ifdef CONFIG_PPC_BOOK3S
+	DEFINE(VCPU_KVM, offsetof(struct kvm_vcpu, kvm));
+	DEFINE(VCPU_VCPUID, offsetof(struct kvm_vcpu, vcpu_id));
 	DEFINE(VCPU_HOST_RETIP, offsetof(struct kvm_vcpu, arch.host_retip));
 	DEFINE(VCPU_HOST_MSR, offsetof(struct kvm_vcpu, arch.host_msr));
+	DEFINE(VCPU_PURR, offsetof(struct kvm_vcpu, arch.purr));
+	DEFINE(VCPU_SPURR, offsetof(struct kvm_vcpu, arch.spurr));
+	DEFINE(VCPU_DSCR, offsetof(struct kvm_vcpu, arch.dscr));
+	DEFINE(VCPU_AMR, offsetof(struct kvm_vcpu, arch.amr));
+	DEFINE(VCPU_UAMOR, offsetof(struct kvm_vcpu, arch.uamor));
+	DEFINE(VCPU_CTRL, offsetof(struct kvm_vcpu, arch.ctrl));
+	DEFINE(VCPU_DABR, offsetof(struct kvm_vcpu, arch.dabr));
 	DEFINE(VCPU_TRAMPOLINE_LOWMEM, offsetof(struct kvm_vcpu, arch.trampoline_lowmem));
 	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
 	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));
 	DEFINE(VCPU_RMCALL, offsetof(struct kvm_vcpu, arch.rmcall));
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
+	DEFINE(VCPU_DEC, offsetof(struct kvm_vcpu, arch.dec));
+	DEFINE(VCPU_DEC_EXPIRES, offsetof(struct kvm_vcpu, arch.dec_expires));
+	DEFINE(VCPU_LPCR, offsetof(struct kvm_vcpu, arch.lpcr));
+	DEFINE(VCPU_MMCR, offsetof(struct kvm_vcpu, arch.mmcr));
+	DEFINE(VCPU_PMC, offsetof(struct kvm_vcpu, arch.pmc));
+	DEFINE(VCPU_SLB, offsetof(struct kvm_vcpu, arch.slb));
+	DEFINE(VCPU_SLB_MAX, offsetof(struct kvm_vcpu, arch.slb_max));
+	DEFINE(VCPU_SLB_NR, offsetof(struct kvm_vcpu, arch.slb_nr));
+	DEFINE(VCPU_LAST_CPU, offsetof(struct kvm_vcpu, arch.last_cpu));
+	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
+	DEFINE(VCPU_FAULT_DAR, offsetof(struct kvm_vcpu, arch.fault_dar));
+	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
+	DEFINE(VCPU_TRAP, offsetof(struct kvm_vcpu, arch.trap));
+	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
+			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
+	DEFINE(VCPU_SLB_E, offsetof(struct kvmppc_slb, orige));
+	DEFINE(VCPU_SLB_V, offsetof(struct kvmppc_slb, origv));
+	DEFINE(VCPU_SLB_SIZE, sizeof(struct kvmppc_slb));
 
 #ifdef CONFIG_PPC_BOOK3S_64
+#ifdef CONFIG_KVM_BOOK3S_PR
 # define SVCPU_FIELD(x, f)	DEFINE(x, offsetof(struct paca_struct, shadow_vcpu.f))
+#else
+# define SVCPU_FIELD(x, f)
+#endif
 # define HSTATE_FIELD(x, f)	DEFINE(x, offsetof(struct paca_struct, kvm_hstate.f))
 #else	/* 32-bit */
 # define SVCPU_FIELD(x, f)	DEFINE(x, offsetof(struct kvmppc_book3s_shadow_vcpu, f))
@@ -453,11 +520,23 @@ int main(void)
 
 	HSTATE_FIELD(HSTATE_HOST_R1, host_r1);
 	HSTATE_FIELD(HSTATE_HOST_R2, host_r2);
+	HSTATE_FIELD(HSTATE_HOST_MSR, host_msr);
 	HSTATE_FIELD(HSTATE_VMHANDLER, vmhandler);
 	HSTATE_FIELD(HSTATE_SCRATCH0, scratch0);
 	HSTATE_FIELD(HSTATE_SCRATCH1, scratch1);
 	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
 
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	HSTATE_FIELD(HSTATE_KVM_VCPU, kvm_vcpu);
+	HSTATE_FIELD(HSTATE_MMCR, host_mmcr);
+	HSTATE_FIELD(HSTATE_PMC, host_pmc);
+	HSTATE_FIELD(HSTATE_PURR, host_purr);
+	HSTATE_FIELD(HSTATE_SPURR, host_spurr);
+	HSTATE_FIELD(HSTATE_DSCR, host_dscr);
+	HSTATE_FIELD(HSTATE_DABR, dabr);
+	HSTATE_FIELD(HSTATE_DECEXP, dec_expires);
+#endif /* CONFIG_KVM_BOOK3S_64_HV */
+
 #else /* CONFIG_PPC_BOOK3S */
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));

commit 3c42bf8a717cb636e0ed2ed77194669e2ac3ed56
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:20:58 2011 +0000

    KVM: PPC: Split host-state fields out of kvmppc_book3s_shadow_vcpu
    
    There are several fields in struct kvmppc_book3s_shadow_vcpu that
    temporarily store bits of host state while a guest is running,
    rather than anything relating to the particular guest or vcpu.
    This splits them out into a new kvmppc_host_state structure and
    modifies the definitions in asm-offsets.c to suit.
    
    On 32-bit, we have a kvmppc_host_state structure inside the
    kvmppc_book3s_shadow_vcpu since the assembly code needs to be able
    to get to them both with one pointer.  On 64-bit they are separate
    fields in the PACA.  This means that on 64-bit we don't need to
    copy the kvmppc_host_state in and out on vcpu load/unload, and
    in future will mean that the book3s_hv code doesn't need a
    shadow_vcpu struct in the PACA at all.  That does mean that we
    have to be careful not to rely on any values persisting in the
    hstate field of the paca across any point where we could block
    or get preempted.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index faf846131f45..dabfb7346f36 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -198,11 +198,6 @@ int main(void)
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
-#ifdef CONFIG_KVM_BOOK3S_64_HANDLER
-	DEFINE(PACA_KVM_SVCPU, offsetof(struct paca_struct, shadow_vcpu));
-	DEFINE(SVCPU_SLB, offsetof(struct kvmppc_book3s_shadow_vcpu, slb));
-	DEFINE(SVCPU_SLB_MAX, offsetof(struct kvmppc_book3s_shadow_vcpu, slb_max));
-#endif
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */
@@ -416,49 +411,54 @@ int main(void)
 	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));
 	DEFINE(VCPU_RMCALL, offsetof(struct kvm_vcpu, arch.rmcall));
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
-	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
-			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
-	DEFINE(SVCPU_CR, offsetof(struct kvmppc_book3s_shadow_vcpu, cr));
-	DEFINE(SVCPU_XER, offsetof(struct kvmppc_book3s_shadow_vcpu, xer));
-	DEFINE(SVCPU_CTR, offsetof(struct kvmppc_book3s_shadow_vcpu, ctr));
-	DEFINE(SVCPU_LR, offsetof(struct kvmppc_book3s_shadow_vcpu, lr));
-	DEFINE(SVCPU_PC, offsetof(struct kvmppc_book3s_shadow_vcpu, pc));
-	DEFINE(SVCPU_R0, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[0]));
-	DEFINE(SVCPU_R1, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[1]));
-	DEFINE(SVCPU_R2, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[2]));
-	DEFINE(SVCPU_R3, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[3]));
-	DEFINE(SVCPU_R4, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[4]));
-	DEFINE(SVCPU_R5, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[5]));
-	DEFINE(SVCPU_R6, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[6]));
-	DEFINE(SVCPU_R7, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[7]));
-	DEFINE(SVCPU_R8, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[8]));
-	DEFINE(SVCPU_R9, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[9]));
-	DEFINE(SVCPU_R10, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[10]));
-	DEFINE(SVCPU_R11, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[11]));
-	DEFINE(SVCPU_R12, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[12]));
-	DEFINE(SVCPU_R13, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[13]));
-	DEFINE(SVCPU_HOST_R1, offsetof(struct kvmppc_book3s_shadow_vcpu, host_r1));
-	DEFINE(SVCPU_HOST_R2, offsetof(struct kvmppc_book3s_shadow_vcpu, host_r2));
-	DEFINE(SVCPU_VMHANDLER, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					 vmhandler));
-	DEFINE(SVCPU_SCRATCH0, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					scratch0));
-	DEFINE(SVCPU_SCRATCH1, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					scratch1));
-	DEFINE(SVCPU_IN_GUEST, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					in_guest));
-	DEFINE(SVCPU_FAULT_DSISR, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					   fault_dsisr));
-	DEFINE(SVCPU_FAULT_DAR, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					 fault_dar));
-	DEFINE(SVCPU_LAST_INST, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					 last_inst));
-	DEFINE(SVCPU_SHADOW_SRR1, offsetof(struct kvmppc_book3s_shadow_vcpu,
-					   shadow_srr1));
+
+#ifdef CONFIG_PPC_BOOK3S_64
+# define SVCPU_FIELD(x, f)	DEFINE(x, offsetof(struct paca_struct, shadow_vcpu.f))
+# define HSTATE_FIELD(x, f)	DEFINE(x, offsetof(struct paca_struct, kvm_hstate.f))
+#else	/* 32-bit */
+# define SVCPU_FIELD(x, f)	DEFINE(x, offsetof(struct kvmppc_book3s_shadow_vcpu, f))
+# define HSTATE_FIELD(x, f)	DEFINE(x, offsetof(struct kvmppc_book3s_shadow_vcpu, hstate.f))
+#endif
+
+	SVCPU_FIELD(SVCPU_CR, cr);
+	SVCPU_FIELD(SVCPU_XER, xer);
+	SVCPU_FIELD(SVCPU_CTR, ctr);
+	SVCPU_FIELD(SVCPU_LR, lr);
+	SVCPU_FIELD(SVCPU_PC, pc);
+	SVCPU_FIELD(SVCPU_R0, gpr[0]);
+	SVCPU_FIELD(SVCPU_R1, gpr[1]);
+	SVCPU_FIELD(SVCPU_R2, gpr[2]);
+	SVCPU_FIELD(SVCPU_R3, gpr[3]);
+	SVCPU_FIELD(SVCPU_R4, gpr[4]);
+	SVCPU_FIELD(SVCPU_R5, gpr[5]);
+	SVCPU_FIELD(SVCPU_R6, gpr[6]);
+	SVCPU_FIELD(SVCPU_R7, gpr[7]);
+	SVCPU_FIELD(SVCPU_R8, gpr[8]);
+	SVCPU_FIELD(SVCPU_R9, gpr[9]);
+	SVCPU_FIELD(SVCPU_R10, gpr[10]);
+	SVCPU_FIELD(SVCPU_R11, gpr[11]);
+	SVCPU_FIELD(SVCPU_R12, gpr[12]);
+	SVCPU_FIELD(SVCPU_R13, gpr[13]);
+	SVCPU_FIELD(SVCPU_FAULT_DSISR, fault_dsisr);
+	SVCPU_FIELD(SVCPU_FAULT_DAR, fault_dar);
+	SVCPU_FIELD(SVCPU_LAST_INST, last_inst);
+	SVCPU_FIELD(SVCPU_SHADOW_SRR1, shadow_srr1);
 #ifdef CONFIG_PPC_BOOK3S_32
-	DEFINE(SVCPU_SR, offsetof(struct kvmppc_book3s_shadow_vcpu, sr));
+	SVCPU_FIELD(SVCPU_SR, sr);
 #endif
-#else
+#ifdef CONFIG_PPC64
+	SVCPU_FIELD(SVCPU_SLB, slb);
+	SVCPU_FIELD(SVCPU_SLB_MAX, slb_max);
+#endif
+
+	HSTATE_FIELD(HSTATE_HOST_R1, host_r1);
+	HSTATE_FIELD(HSTATE_HOST_R2, host_r2);
+	HSTATE_FIELD(HSTATE_VMHANDLER, vmhandler);
+	HSTATE_FIELD(HSTATE_SCRATCH0, scratch0);
+	HSTATE_FIELD(HSTATE_SCRATCH1, scratch1);
+	HSTATE_FIELD(HSTATE_IN_GUEST, in_guest);
+
+#else /* CONFIG_PPC_BOOK3S */
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
@@ -468,7 +468,7 @@ int main(void)
 	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
 	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
 #endif /* CONFIG_PPC_BOOK3S */
-#endif
+#endif /* CONFIG_KVM */
 
 #ifdef CONFIG_KVM_GUEST
 	DEFINE(KVM_MAGIC_SCRATCH1, offsetof(struct kvm_vcpu_arch_shared,

commit dd9ebf1f94354b010f2ac7a98bf69168636cb08e
Author: Liu Yu <yu.liu@freescale.com>
Date:   Tue Jun 14 18:35:14 2011 -0500

    KVM: PPC: e500: Add shadow PID support
    
    Dynamically assign host PIDs to guest PIDs, splitting each guest PID into
    multiple host (shadow) PIDs based on kernel/user and MSR[IS/DS].  Use
    both PID0 and PID1 so that the shadow PIDs for the right mode can be
    selected, that correspond both to guest TID = zero and guest TID = guest
    PID.
    
    This allows us to significantly reduce the frequency of needing to
    invalidate the entire TLB.  When the guest mode or PID changes, we just
    update the host PID0/PID1.  And since the allocation of shadow PIDs is
    global, multiple guests can share the TLB without conflict.
    
    Note that KVM does not yet support the guest setting PID1 or PID2 to
    a value other than zero.  This will need to be fixed for nested KVM
    to work.  Until then, we enforce the requirement for guest PID1/PID2
    to stay zero by failing the emulation if the guest tries to set them
    to something else.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ecd2b3ad7ff6..faf846131f45 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -402,6 +402,7 @@ int main(void)
 	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));
 	DEFINE(VCPU_SPRG7, offsetof(struct kvm_vcpu, arch.sprg7));
 	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
+	DEFINE(VCPU_SHADOW_PID1, offsetof(struct kvm_vcpu, arch.shadow_pid1));
 	DEFINE(VCPU_SHARED, offsetof(struct kvm_vcpu, arch.shared));
 	DEFINE(VCPU_SHARED_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
 	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));

commit 4cd35f675ba41a99a477e28a6add4a66833325f2
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Jun 14 18:34:31 2011 -0500

    KVM: PPC: e500: Save/restore SPE state
    
    This is done lazily.  The SPE save will be done only if the guest has
    used SPE since the last preemption or heavyweight exit.  Restore will be
    done only on demand, when enabling MSR_SPE in the shadow MSR, in response
    to an SPE fault or mtmsr emulation.
    
    For SPEFSCR, Linux already switches it on context switch (non-lazily), so
    the only remaining bit is to save it between qemu and the guest.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 25de8e4808a4..ecd2b3ad7ff6 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -497,6 +497,13 @@ int main(void)
 	DEFINE(TLBCAM_MAS7, offsetof(struct tlbcam, MAS7));
 #endif
 
+#if defined(CONFIG_KVM) && defined(CONFIG_SPE)
+	DEFINE(VCPU_EVR, offsetof(struct kvm_vcpu, arch.evr[0]));
+	DEFINE(VCPU_ACC, offsetof(struct kvm_vcpu, arch.acc));
+	DEFINE(VCPU_SPEFSCR, offsetof(struct kvm_vcpu, arch.spefscr));
+	DEFINE(VCPU_HOST_SPEFSCR, offsetof(struct kvm_vcpu, arch.host_spefscr));
+#endif
+
 #ifdef CONFIG_KVM_EXIT_TIMING
 	DEFINE(VCPU_TIMING_EXIT_TBU, offsetof(struct kvm_vcpu,
 						arch.timing_exit.tv32.tbu));

commit ecee273fc48f7f48f0c2f074335c43aaa790c308
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Jun 14 18:34:29 2011 -0500

    KVM: PPC: booke: use shadow_msr
    
    Keep the guest MSR and the guest-mode true MSR separate, rather than
    modifying the guest MSR on each guest entry to produce a true MSR.
    
    Any bits which should be modified based on guest MSR must be explicitly
    propagated from vcpu->arch.shared->msr to vcpu->arch.shadow_msr in
    kvmppc_set_msr().
    
    While we're modifying the guest entry code, reorder a few instructions
    to bury some load latencies.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 36e1c8a29be8..25de8e4808a4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -404,12 +404,12 @@ int main(void)
 	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
 	DEFINE(VCPU_SHARED, offsetof(struct kvm_vcpu, arch.shared));
 	DEFINE(VCPU_SHARED_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
+	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
 
 	/* book3s */
 #ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_HOST_RETIP, offsetof(struct kvm_vcpu, arch.host_retip));
 	DEFINE(VCPU_HOST_MSR, offsetof(struct kvm_vcpu, arch.host_msr));
-	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
 	DEFINE(VCPU_TRAMPOLINE_LOWMEM, offsetof(struct kvm_vcpu, arch.trampoline_lowmem));
 	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
 	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));

commit 1325a684b553d4b5c41ae0482f8991b43f945746
Author: Ashish Kalra <Ashish.Kalra@freescale.com>
Date:   Fri Apr 22 16:48:27 2011 -0500

    powerpc/85xx: Save scratch registers to thread info instead of using SPRGs.
    
    We expect this is actually faster, and we end up needing more space than we
    can get from the SPRGs in some instances.  This is also useful when running
    as a guest OS - SPRGs4-7 do not have guest versions.
    
    8 slots are allocated in thread_info for this even though we only actually
    use 4 of them - this allows space for future code to have more scratch
    space (and we know we'll need it for things like hugetlb).
    
    Signed-off-by: Ashish Kalra <Ashish.Kalra@freescale.com>
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 36e1c8a29be8..c98144f6f04e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -82,6 +82,9 @@ int main(void)
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
 	DEFINE(KSP_LIMIT, offsetof(struct thread_struct, ksp_limit));
 	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
+#ifdef CONFIG_BOOKE
+	DEFINE(THREAD_NORMSAVES, offsetof(struct thread_struct, normsave[0]));
+#endif
 	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
 	DEFINE(THREAD_FPR0, offsetof(struct thread_struct, fpr[0]));
 	DEFINE(THREAD_FPSCR, offsetof(struct thread_struct, fpscr));

commit f4b10bc60a310916bab5413f821b99ef845cac17
Merge: 53ee7569ce8b c8cfbb555eb3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 23 08:42:08 2011 -0700

    Merge branch 'kvm-updates/2.6.40' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.40' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (131 commits)
      KVM: MMU: Use ptep_user for cmpxchg_gpte()
      KVM: Fix kvm mmu_notifier initialization order
      KVM: Add documentation for KVM_CAP_NR_VCPUS
      KVM: make guest mode entry to be rcu quiescent state
      KVM: x86 emulator: Make jmp far emulation into a separate function
      KVM: x86 emulator: Rename emulate_grpX() to em_grpX()
      KVM: x86 emulator: Remove unused arg from emulate_pop()
      KVM: x86 emulator: Remove unused arg from writeback()
      KVM: x86 emulator: Remove unused arg from read_descriptor()
      KVM: x86 emulator: Remove unused arg from seg_override()
      KVM: Validate userspace_addr of memslot when registered
      KVM: MMU: Clean up gpte reading with copy_from_user()
      KVM: PPC: booke: add sregs support
      KVM: PPC: booke: save/restore VRSAVE (a.k.a. USPRG0)
      KVM: PPC: use ticks, not usecs, for exit timing
      KVM: PPC: fix exit accounting for SPRs, tlbwe, tlbsx
      KVM: PPC: e500: emulate SVR
      KVM: VMX: Cache vmcs segment fields
      KVM: x86 emulator: consolidate segment accessors
      KVM: VMX: Avoid reading %rip unnecessarily when handling exceptions
      ...

commit eab176722f4628b2d9cf76221a43dd3a0e37e632
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Apr 27 17:24:10 2011 -0500

    KVM: PPC: booke: save/restore VRSAVE (a.k.a. USPRG0)
    
    Linux doesn't use USPRG0 (now renamed VRSAVE in the architecture, even
    when Altivec isn't involved), but a guest might.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 23e6a93145ab..cf0d82278951 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -395,6 +395,7 @@ int main(void)
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
+	DEFINE(VCPU_VRSAVE, offsetof(struct kvm_vcpu, arch.vrsave));
 	DEFINE(VCPU_SPRG4, offsetof(struct kvm_vcpu, arch.sprg4));
 	DEFINE(VCPU_SPRG5, offsetof(struct kvm_vcpu, arch.sprg5));
 	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));

commit efcac6589a277c10060e4be44b9455cf43838dc1
Author: Alexey Kardashevskiy <aik@au1.ibm.com>
Date:   Wed Mar 2 15:18:48 2011 +0000

    powerpc: Per process DSCR + some fixes (try#4)
    
    The DSCR (aka Data Stream Control Register) is supported on some
    server PowerPC chips and allow some control over the prefetch
    of data streams.
    
    This patch allows the value to be specified per thread by emulating
    the corresponding mfspr and mtspr instructions. Children of such
    threads inherit the value. Other threads use a default value that
    can be specified in sysfs - /sys/devices/system/cpu/dscr_default.
    
    If a thread starts with non default value in the sysfs entry,
    all children threads inherit this non default value even if
    the sysfs value is changed later.
    
    Signed-off-by: Alexey Kardashevskiy <aik@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 23e6a93145ab..6887661ac072 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -74,6 +74,7 @@ int main(void)
 	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
 	DEFINE(SIGSEGV, SIGSEGV);
 	DEFINE(NMI_MASK, NMI_MASK);
+	DEFINE(THREAD_DSCR, offsetof(struct thread_struct, dscr));
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
 #endif /* CONFIG_PPC64 */

commit 46f5221049bb46b0188aad6b6dfab5dbc778be22
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Nov 18 15:06:17 2010 +0000

    powerpc: Remove second definition of STACK_FRAME_OVERHEAD
    
    Since STACK_FRAME_OVERHEAD is defined in asm/ptrace.h and that
    is ASSEMBER safe, we can just include that instead of going via
    asm-offsets.h.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index bd0df2e6aa8f..23e6a93145ab 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -209,7 +209,6 @@ int main(void)
 	DEFINE(RTASENTRY, offsetof(struct rtas_t, entry));
 
 	/* Interrupt register frame */
-	DEFINE(STACK_FRAME_OVERHEAD, STACK_FRAME_OVERHEAD);
 	DEFINE(INT_FRAME_SIZE, STACK_INT_FRAME_SIZE);
 	DEFINE(SWITCH_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
 #ifdef CONFIG_PPC64

commit 1765a1fe5d6f82c0eceb1ad10594cfc83759b6d0
Merge: bdaf12b41235 2a31339aa014
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 24 12:47:25 2010 -0700

    Merge branch 'kvm-updates/2.6.37' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.37' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (321 commits)
      KVM: Drop CONFIG_DMAR dependency around kvm_iommu_map_pages
      KVM: Fix signature of kvm_iommu_map_pages stub
      KVM: MCE: Send SRAR SIGBUS directly
      KVM: MCE: Add MCG_SER_P into KVM_MCE_CAP_SUPPORTED
      KVM: fix typo in copyright notice
      KVM: Disable interrupts around get_kernel_ns()
      KVM: MMU: Avoid sign extension in mmu_alloc_direct_roots() pae root address
      KVM: MMU: move access code parsing to FNAME(walk_addr) function
      KVM: MMU: audit: check whether have unsync sps after root sync
      KVM: MMU: audit: introduce audit_printk to cleanup audit code
      KVM: MMU: audit: unregister audit tracepoints before module unloaded
      KVM: MMU: audit: fix vcpu's spte walking
      KVM: MMU: set access bit for direct mapping
      KVM: MMU: cleanup for error mask set while walk guest page table
      KVM: MMU: update 'root_hpa' out of loop in PAE shadow path
      KVM: x86 emulator: Eliminate compilation warning in x86_decode_insn()
      KVM: x86: Fix constant type in kvm_get_time_scale
      KVM: VMX: Add AX to list of registers clobbered by guest switch
      KVM guest: Move a printk that's using the clock before it's ready
      KVM: x86: TSC catchup mode
      ...

commit cbe487fac7fc016dbabbcbe83f11384e1803a56d
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Aug 3 10:39:35 2010 +0200

    KVM: PPC: Add mtsrin PV code
    
    This is the guest side of the mtsr acceleration. Using this a guest can now
    call mtsrin with almost no overhead as long as it ensures that it only uses
    it with (MSR_IR|MSR_DR) == 0. Linux does that, so we're good.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 6d92b4e13ebf..7f0d6fcc28a3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -478,6 +478,7 @@ int main(void)
 	DEFINE(KVM_MAGIC_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
 	DEFINE(KVM_MAGIC_CRITICAL, offsetof(struct kvm_vcpu_arch_shared,
 					    critical));
+	DEFINE(KVM_MAGIC_SR, offsetof(struct kvm_vcpu_arch_shared, sr));
 #endif
 
 #ifdef CONFIG_44x

commit 989044ee0fdc6c22a11ea1d22e2a3d17463cb564
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 30 12:01:56 2010 +0200

    KVM: PPC: Fix CONFIG_KVM_GUEST && !CONFIG_KVM case
    
    When CONFIG_KVM_GUEST is selected, but CONFIG_KVM is not, we were missing
    some defines in asm-offsets.c and included too many headers at other places.
    
    This patch makes above configuration work.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 37486cafb69d..6d92b4e13ebf 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -48,11 +48,11 @@
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iseries/alpaca.h>
 #endif
-#ifdef CONFIG_KVM
+#if defined(CONFIG_KVM) || defined(CONFIG_KVM_GUEST)
 #include <linux/kvm_host.h>
-#ifndef CONFIG_BOOKE
-#include <asm/kvm_book3s.h>
 #endif
+#if defined(CONFIG_KVM) && defined(CONFIG_PPC_BOOK3S)
+#include <asm/kvm_book3s.h>
 #endif
 
 #ifdef CONFIG_PPC32

commit d17051cb8d223dffd6bb847b0565ef1654f8e0e1
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:57 2010 +0200

    KVM: PPC: Generic KVM PV guest support
    
    We have all the hypervisor pieces in place now, but the guest parts are still
    missing.
    
    This patch implements basic awareness of KVM when running Linux as guest. It
    doesn't do anything with it yet though.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1221bcdff52f..37486cafb69d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -465,6 +465,21 @@ int main(void)
 	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
 #endif /* CONFIG_PPC_BOOK3S */
 #endif
+
+#ifdef CONFIG_KVM_GUEST
+	DEFINE(KVM_MAGIC_SCRATCH1, offsetof(struct kvm_vcpu_arch_shared,
+					    scratch1));
+	DEFINE(KVM_MAGIC_SCRATCH2, offsetof(struct kvm_vcpu_arch_shared,
+					    scratch2));
+	DEFINE(KVM_MAGIC_SCRATCH3, offsetof(struct kvm_vcpu_arch_shared,
+					    scratch3));
+	DEFINE(KVM_MAGIC_INT, offsetof(struct kvm_vcpu_arch_shared,
+				       int_pending));
+	DEFINE(KVM_MAGIC_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
+	DEFINE(KVM_MAGIC_CRITICAL, offsetof(struct kvm_vcpu_arch_shared,
+					    critical));
+#endif
+
 #ifdef CONFIG_44x
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);
 	DEFINE(PTE_T_LOG2, PTE_T_LOG2);

commit 666e7252a15b7fc4a116e65deaf6da5e4ce660e3
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:43 2010 +0200

    KVM: PPC: Convert MSR to shared page
    
    One of the most obvious registers to share with the guest directly is the
    MSR. The MSR contains the "interrupts enabled" flag which the guest has to
    toggle in critical sections.
    
    So in order to bring the overhead of interrupt en- and disabling down, let's
    put msr into the shared page. Keep in mind that even though you can fully read
    its contents, writing to it doesn't always update all state. There are a few
    safe fields that don't require hypervisor interaction. See the documentation
    for a list of MSR bits that are safe to be set from inside the guest.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 60e7db4c13af..1221bcdff52f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -394,13 +394,13 @@ int main(void)
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
-	DEFINE(VCPU_MSR, offsetof(struct kvm_vcpu, arch.msr));
 	DEFINE(VCPU_SPRG4, offsetof(struct kvm_vcpu, arch.sprg4));
 	DEFINE(VCPU_SPRG5, offsetof(struct kvm_vcpu, arch.sprg5));
 	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));
 	DEFINE(VCPU_SPRG7, offsetof(struct kvm_vcpu, arch.sprg7));
 	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
 	DEFINE(VCPU_SHARED, offsetof(struct kvm_vcpu, arch.shared));
+	DEFINE(VCPU_SHARED_MSR, offsetof(struct kvm_vcpu_arch_shared, msr));
 
 	/* book3s */
 #ifdef CONFIG_PPC_BOOK3S

commit 96bc451a153297bf1f99ef2d633d512ea349ae7a
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:42 2010 +0200

    KVM: PPC: Introduce shared page
    
    For transparent variable sharing between the hypervisor and guest, I introduce
    a shared page. This shared page will contain all the registers the guest can
    read and write safely without exiting guest context.
    
    This patch only implements the stubs required for the basic structure of the
    shared page. The actual register moving follows.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1c0607ddccc0..60e7db4c13af 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -400,6 +400,7 @@ int main(void)
 	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));
 	DEFINE(VCPU_SPRG7, offsetof(struct kvm_vcpu, arch.sprg7));
 	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
+	DEFINE(VCPU_SHARED, offsetof(struct kvm_vcpu, arch.shared));
 
 	/* book3s */
 #ifdef CONFIG_PPC_BOOK3S

commit 55fd766b5fad8240b7a6e994b5779a46d28f73d4
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Fri Oct 16 18:48:40 2009 -0500

    powerpc/fsl-booke64: Use TLB CAMs to cover linear mapping on FSL 64-bit chips
    
    On Freescale parts typically have TLB array for large mappings that we can
    bolt the linear mapping into.  We utilize the code that already exists
    on PPC32 on the 64-bit side to setup the linear mapping to be cover by
    bolted TLB entries.  We utilize a quarter of the variable size TLB array
    for this purpose.
    
    Additionally, we limit the amount of memory to what we can cover via
    bolted entries so we don't get secondary faults in the TLB miss
    handlers.  We should fix this limitation in the future.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c63494090854..c3e01945ad4f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -61,7 +61,7 @@
 #endif
 #endif
 
-#if defined(CONFIG_FSL_BOOKE)
+#if defined(CONFIG_PPC_FSL_BOOK3E)
 #include "../mm/mmu_decl.h"
 #endif
 
@@ -470,7 +470,7 @@ int main(void)
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);
 	DEFINE(PTE_T_LOG2, PTE_T_LOG2);
 #endif
-#ifdef CONFIG_FSL_BOOKE
+#ifdef CONFIG_PPC_FSL_BOOK3E
 	DEFINE(TLBCAM_SIZE, sizeof(struct tlbcam));
 	DEFINE(TLBCAM_MAS0, offsetof(struct tlbcam, MAS0));
 	DEFINE(TLBCAM_MAS1, offsetof(struct tlbcam, MAS1));

commit cf9efce0ce3136fa076f53e53154e98455229514
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Aug 26 19:56:43 2010 +0000

    powerpc: Account time using timebase rather than PURR
    
    Currently, when CONFIG_VIRT_CPU_ACCOUNTING is enabled, we use the
    PURR register for measuring the user and system time used by
    processes, as well as other related times such as hardirq and
    softirq times.  This turns out to be quite confusing for users
    because it means that a program will often be measured as taking
    less time when run on a multi-threaded processor (SMT2 or SMT4 mode)
    than it does when run on a single-threaded processor (ST mode), even
    though the program takes longer to finish.  The discrepancy is
    accounted for as stolen time, which is also confusing, particularly
    when there are no other partitions running.
    
    This changes the accounting to use the timebase instead, meaning that
    the reported user and system times are the actual number of real-time
    seconds that the program was executing on the processor thread,
    regardless of which SMT mode the processor is in.  Thus a program will
    generally show greater user and system times when run on a
    multi-threaded processor than on a single-threaded processor.
    
    On pSeries systems on POWER5 or later processors, we measure the
    stolen time (time when this partition wasn't running) using the
    hypervisor dispatch trace log.  We check for new entries in the
    log on every entry from user mode and on every transition from
    kernel process context to soft or hard IRQ context (i.e. when
    account_system_vtime() gets called).  So that we can correctly
    distinguish time stolen from user time and time stolen from system
    time, without having to check the log on every exit to user mode,
    we store separate timestamps for exit to user mode and entry from
    user mode.
    
    On systems that have a SPURR (POWER6 and POWER7), we read the SPURR
    in account_system_vtime() (as before), and then apportion the SPURR
    ticks since the last time we read it between scaled user time and
    scaled system time according to the relative proportions of user
    time and system time over the same interval.  This avoids having to
    read the SPURR on every kernel entry and exit.  On systems that have
    PURR but not SPURR (i.e., POWER5), we do the same using the PURR
    rather than the SPURR.
    
    This disables the DTL user interface in /sys/debug/kernel/powerpc/dtl
    for now since it conflicts with the use of the dispatch trace log
    by the time accounting code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1c0607ddccc0..c63494090854 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -181,17 +181,19 @@ int main(void)
 	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid));
 	DEFINE(SLBSHADOW_STACKESID,
 	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].esid));
+	DEFINE(SLBSHADOW_SAVEAREA, offsetof(struct slb_shadow, save_area));
 	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
 	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
 	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
 	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
-	DEFINE(SLBSHADOW_SAVEAREA, offsetof(struct slb_shadow, save_area));
+	DEFINE(LPPACA_DTLIDX, offsetof(struct lppaca, dtl_idx));
+	DEFINE(PACA_DTL_RIDX, offsetof(struct paca_struct, dtl_ridx));
 #endif /* CONFIG_PPC_STD_MMU_64 */
 	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
 	DEFINE(PACAKEXECSTATE, offsetof(struct paca_struct, kexec_state));
-	DEFINE(PACA_STARTPURR, offsetof(struct paca_struct, startpurr));
-	DEFINE(PACA_STARTSPURR, offsetof(struct paca_struct, startspurr));
+	DEFINE(PACA_STARTTIME, offsetof(struct paca_struct, starttime));
+	DEFINE(PACA_STARTTIME_USER, offsetof(struct paca_struct, starttime_user));
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));

commit ae01f84b93b274e2f215bdf6d0b46435679b5f9a
Author: Anton Blanchard <anton@samba.org>
Date:   Mon May 31 18:45:11 2010 +0000

    powerpc: Optimise per cpu accesses on 64bit
    
    Now we dynamically allocate the paca array, it takes an extra load
    whenever we want to access another cpu's paca. One place we do that a lot
    is per cpu variables. A simple example:
    
    DEFINE_PER_CPU(unsigned long, vara);
    unsigned long test4(int cpu)
    {
            return per_cpu(vara, cpu);
    }
    
    This takes 4 loads, 5 if you include the actual load of the per cpu variable:
    
        ld r11,-32760(r30)  # load address of paca pointer
        ld r9,-32768(r30)   # load link address of percpu variable
        sldi r3,r29,9       # get offset into paca (each entry is 512 bytes)
        ld r0,0(r11)        # load paca pointer
        add r3,r0,r3        # paca + offset
        ld r11,64(r3)       # load paca[cpu].data_offset
    
        ldx r3,r9,r11       # load per cpu variable
    
    If we remove the ppc64 specific per_cpu_offset(), we get the generic one
    which indexes into a statically allocated array. This removes one load and
    one add:
    
        ld r11,-32760(r30)  # load address of __per_cpu_offset
        ld r9,-32768(r30)   # load link address of percpu variable
        sldi r3,r29,3       # get offset into __per_cpu_offset (each entry 8 bytes)
        ldx r11,r11,r3      # load __per_cpu_offset[cpu]
    
        ldx r3,r9,r11       # load per cpu variable
    
    Having all the offsets in one array also helps when iterating over a per cpu
    variable across a number of cpus, such as in the scheduler. Before we would
    need to load one paca cacheline when calculating each per cpu offset. Now we
    have 16 (128 / sizeof(long)) per cpu offsets in each cacheline.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index acbbac6aaa22..1c0607ddccc0 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -194,7 +194,6 @@ int main(void)
 	DEFINE(PACA_STARTSPURR, offsetof(struct paca_struct, startspurr));
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
-	DEFINE(PACA_DATA_OFFSET, offsetof(struct paca_struct, data_offset));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 #ifdef CONFIG_KVM_BOOK3S_64_HANDLER
 	DEFINE(PACA_KVM_SVCPU, offsetof(struct paca_struct, shadow_vcpu));

commit 8fd63a9ea7528463211a6c88d500c51851d960c8
Author: Paul Mackerras <paulus@samba.org>
Date:   Sun Jun 20 19:03:08 2010 +0000

    powerpc: Rework VDSO gettimeofday to prevent time going backwards
    
    Currently it is possible for userspace to see the result of
    gettimeofday() going backwards by 1 microsecond, assuming that
    userspace is using the gettimeofday() in the VDSO.  The VDSO
    gettimeofday() algorithm computes the time in "xsecs", which are
    units of 2^-20 seconds, or approximately 0.954 microseconds,
    using the algorithm
    
            now = (timebase - tb_orig_stamp) * tb_to_xs + stamp_xsec
    
    and then converts the time in xsecs to seconds and microseconds.
    
    The kernel updates the tb_orig_stamp and stamp_xsec values every
    tick in update_vsyscall().  If the length of the tick is not an
    integer number of xsecs, then some precision is lost in converting
    the current time to xsecs.  For example, with CONFIG_HZ=1000, the
    tick is 1ms long, which is 1048.576 xsecs.  That means that
    stamp_xsec will advance by either 1048 or 1049 on each tick.
    With the right conditions, it is possible for userspace to get
    (timebase - tb_orig_stamp) * tb_to_xs being 1049 if the kernel is
    slightly late in updating the vdso_datapage, and then for stamp_xsec
    to advance by 1048 when the kernel does update it, and for userspace
    to then see (timebase - tb_orig_stamp) * tb_to_xs being zero due to
    integer truncation.  The result is that time appears to go backwards
    by 1 microsecond.
    
    To fix this we change the VDSO gettimeofday to use a new field in the
    VDSO datapage which stores the nanoseconds part of the time as a
    fractional number of seconds in a 0.32 binary fraction format.
    (Or put another way, as a 32-bit number in units of 0.23283 ns.)
    This is convenient because we can use the mulhwu instruction to
    convert it to either microseconds or nanoseconds.
    
    Since it turns out that computing the time of day using this new field
    is simpler than either using stamp_xsec (as gettimeofday does) or
    stamp_xtime.tv_nsec (as clock_gettime does), this converts both
    gettimeofday and clock_gettime to use the new field.  The existing
    __do_get_tspec function is converted to use the new field and take
    a parameter in r7 that indicates the desired resolution, 1,000,000
    for microseconds or 1,000,000,000 for nanoseconds.  The __do_get_xsec
    function is then unused and is deleted.
    
    The new algorithm is
    
            now = ((timebase - tb_orig_stamp) << 12) * tb_to_xs
                    + (stamp_xtime_seconds << 32) + stamp_sec_fraction
    
    with 'now' in units of 2^-32 seconds.  That is then converted to
    seconds and either microseconds or nanoseconds with
    
            seconds = now >> 32
            partseconds = ((now & 0xffffffff) * resolution) >> 32
    
    The 32-bit VDSO code also makes a further simplification: it ignores
    the bottom 32 bits of the tb_to_xs value, which is a 0.64 format binary
    fraction.  Doing so gets rid of 4 multiply instructions.  Assuming
    a timebase frequency of 1GHz or less and an update interval of no
    more than 10ms, the upper 32 bits of tb_to_xs will be at least
    4503599, so the error from ignoring the low 32 bits will be at most
    2.2ns, which is more than an order of magnitude less than the time
    taken to do gettimeofday or clock_gettime on our fastest processors,
    so there is no possibility of seeing inconsistent values due to this.
    
    This also moves update_gtod() down next to its only caller, and makes
    update_vsyscall use the time passed in via the wall_time argument rather
    than accessing xtime directly.  At present, wall_time always points to
    xtime, but that could change in future.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 496cc5b3984f..acbbac6aaa22 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -342,6 +342,7 @@ int main(void)
 	DEFINE(WTOM_CLOCK_SEC, offsetof(struct vdso_data, wtom_clock_sec));
 	DEFINE(WTOM_CLOCK_NSEC, offsetof(struct vdso_data, wtom_clock_nsec));
 	DEFINE(STAMP_XTIME, offsetof(struct vdso_data, stamp_xtime));
+	DEFINE(STAMP_SEC_FRAC, offsetof(struct vdso_data, stamp_sec_fraction));
 	DEFINE(CFG_ICACHE_BLOCKSZ, offsetof(struct vdso_data, icache_block_size));
 	DEFINE(CFG_DCACHE_BLOCKSZ, offsetof(struct vdso_data, dcache_block_size));
 	DEFINE(CFG_ICACHE_LOGBLOCKSZ, offsetof(struct vdso_data, icache_log_block_size));

commit 98edb6ca4174f17a64890a02f44c211c8b44fb3c
Merge: a8251096b427 8fbf065d6256
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 21 17:16:21 2010 -0700

    Merge branch 'kvm-updates/2.6.35' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    * 'kvm-updates/2.6.35' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (269 commits)
      KVM: x86: Add missing locking to arch specific vcpu ioctls
      KVM: PPC: Add missing vcpu_load()/vcpu_put() in vcpu ioctls
      KVM: MMU: Segregate shadow pages with different cr0.wp
      KVM: x86: Check LMA bit before set_efer
      KVM: Don't allow lmsw to clear cr0.pe
      KVM: Add cpuid.txt file
      KVM: x86: Tell the guest we'll warn it about tsc stability
      x86, paravirt: don't compute pvclock adjustments if we trust the tsc
      x86: KVM guest: Try using new kvm clock msrs
      KVM: x86: export paravirtual cpuid flags in KVM_GET_SUPPORTED_CPUID
      KVM: x86: add new KVMCLOCK cpuid feature
      KVM: x86: change msr numbers for kvmclock
      x86, paravirt: Add a global synchronization point for pvclock
      x86, paravirt: Enable pvclock flags in vcpu_time_info structure
      KVM: x86: Inject #GP with the right rip on efer writes
      KVM: SVM: Don't allow nested guest to VMMCALL into host
      KVM: x86: Fix exception reinjection forced to true
      KVM: Fix wallclock version writing race
      KVM: MMU: Don't read pdptrs with mmu spinlock held in mmu_alloc_roots
      KVM: VMX: enable VMXON check with SMX enabled (Intel TXT)
      ...

commit 79c4581262e225a7c96d88b632b05ab3b5e9a52c
Merge: 59534f7298c5 99ec28f183da
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 21 11:17:05 2010 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (92 commits)
      powerpc: Remove unused 'protect4gb' boot parameter
      powerpc: Build-in e1000e for pseries & ppc64_defconfig
      powerpc/pseries: Make request_ras_irqs() available to other pseries code
      powerpc/numa: Use ibm,architecture-vec-5 to detect form 1 affinity
      powerpc/numa: Set a smaller value for RECLAIM_DISTANCE to enable zone reclaim
      powerpc: Use smt_snooze_delay=-1 to always busy loop
      powerpc: Remove check of ibm,smt-snooze-delay OF property
      powerpc/kdump: Fix race in kdump shutdown
      powerpc/kexec: Fix race in kexec shutdown
      powerpc/kexec: Speedup kexec hash PTE tear down
      powerpc/pseries: Add hcall to read 4 ptes at a time in real mode
      powerpc: Use more accurate limit for first segment memory allocations
      powerpc/kdump: Use chip->shutdown to disable IRQs
      powerpc/kdump: CPUs assume the context of the oopsing CPU
      powerpc/crashdump: Do not fail on NULL pointer dereferencing
      powerpc/eeh: Fix oops when probing in early boot
      powerpc/pci: Check devices status property when scanning OF tree
      powerpc/vio: Switch VIO Bus PM to use generic helpers
      powerpc: Avoid bad relocations in iSeries code
      powerpc: Use common cpu_die (fixes SMP+SUSPEND build)
      ...

commit 1fc711f7ffb01089efc58042cfdbac8573d1b59a
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu May 13 19:40:11 2010 +0000

    powerpc/kexec: Fix race in kexec shutdown
    
    In kexec_prepare_cpus, the primary CPU IPIs the secondary CPUs to
    kexec_smp_down().  kexec_smp_down() calls kexec_smp_wait() which sets
    the hw_cpu_id() to -1.  The primary does this while leaving IRQs on
    which means the primary can take a timer interrupt which can lead to
    the IPIing one of the secondary CPUs (say, for a scheduler re-balance)
    but since the secondary CPU now has a hw_cpu_id = -1, we IPI CPU
    -1... Kaboom!
    
    We are hitting this case regularly on POWER7 machines.
    
    There is also a second race, where the primary will tear down the MMU
    mappings before knowing the secondaries have entered real mode.
    
    Also, the secondaries are clearing out any pending IPIs before
    guaranteeing that no more will be received.
    
    This changes kexec_prepare_cpus() so that we turn off IRQs in the
    primary CPU much earlier.  It adds a paca flag to say that the
    secondaries have entered the kexec_smp_down() IPI and turned off IRQs,
    rather than overloading hw_cpu_id with -1.  This new paca flag is
    again used to in indicate when the secondaries has entered real mode.
    
    It also ensures that all CPUs have their IRQs off before we clear out
    any pending IPI requests (in kexec_cpu_down()) to ensure there are no
    trailing IPIs left unacknowledged.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0271b58ec31e..1b784ff92d9d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -184,6 +184,7 @@ int main(void)
 #endif /* CONFIG_PPC_STD_MMU_64 */
 	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
+	DEFINE(PACAKEXECSTATE, offsetof(struct paca_struct, kexec_state));
 	DEFINE(PACA_STARTPURR, offsetof(struct paca_struct, startpurr));
 	DEFINE(PACA_STARTSPURR, offsetof(struct paca_struct, startspurr));
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));

commit 78f622377f7d31d988db350a43c5689dd5f31876
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu May 13 14:38:21 2010 -0500

    powerpc/fsl-booke: Move loadcam_entry back to asm code to fix SMP ftrace
    
    When we build with ftrace enabled its possible that loadcam_entry would
    have used the stack pointer (even though the code doesn't need it).  We
    call loadcam_entry in __secondary_start before the stack is setup.  To
    ensure that loadcam_entry doesn't use the stack pointer the easiest
    solution is to just have it in asm code.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 957ceb7059c5..0271b58ec31e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -448,6 +448,14 @@ int main(void)
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);
 	DEFINE(PTE_T_LOG2, PTE_T_LOG2);
 #endif
+#ifdef CONFIG_FSL_BOOKE
+	DEFINE(TLBCAM_SIZE, sizeof(struct tlbcam));
+	DEFINE(TLBCAM_MAS0, offsetof(struct tlbcam, MAS0));
+	DEFINE(TLBCAM_MAS1, offsetof(struct tlbcam, MAS1));
+	DEFINE(TLBCAM_MAS2, offsetof(struct tlbcam, MAS2));
+	DEFINE(TLBCAM_MAS3, offsetof(struct tlbcam, MAS3));
+	DEFINE(TLBCAM_MAS7, offsetof(struct tlbcam, MAS7));
+#endif
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	DEFINE(VCPU_TIMING_EXIT_TBU, offsetof(struct kvm_vcpu,

commit 218d169c4c856eee7df56ea0fb8cbb32167e63d3
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:55 2010 +0200

    PPC: Export SWITCH_FRAME_SIZE
    
    We need the SWITCH_FRAME_SIZE define on Book3S_32 now too.
    So let's export it unconditionally.
    
    CC: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1804c2cd1dfd..2716c51906d6 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -210,8 +210,8 @@ int main(void)
 	/* Interrupt register frame */
 	DEFINE(STACK_FRAME_OVERHEAD, STACK_FRAME_OVERHEAD);
 	DEFINE(INT_FRAME_SIZE, STACK_INT_FRAME_SIZE);
-#ifdef CONFIG_PPC64
 	DEFINE(SWITCH_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
+#ifdef CONFIG_PPC64
 	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
 	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
 	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);

commit 97e492558f423d99c51eb934506b7a3d7c64613b
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:51 2010 +0200

    KVM: PPC: Add SVCPU to Book3S_32
    
    We need to keep the pointer to the shadow vcpu somewhere accessible from
    within really early interrupt code. The best fit I found was the thread
    struct, as that resides in an SPRG.
    
    So let's put a pointer to the shadow vcpu in the thread struct and add
    an asm-offset so we can find it.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e8003ff81eef..1804c2cd1dfd 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -108,6 +108,9 @@ int main(void)
 	DEFINE(THREAD_USED_SPE, offsetof(struct thread_struct, used_spe));
 #endif /* CONFIG_SPE */
 #endif /* CONFIG_PPC64 */
+#ifdef CONFIG_KVM_BOOK3S_32_HANDLER
+	DEFINE(THREAD_KVM_SVCPU, offsetof(struct thread_struct, kvm_shadow_vcpu));
+#endif
 
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));

commit 0604675fe17f68741730cebe74422605bb79d972
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:44 2010 +0200

    KVM: PPC: Use now shadowed vcpu fields
    
    The shadow vcpu now contains some fields we don't use from the vcpu anymore.
    Access to them happens using inline functions that happily use the shadow
    vcpu fields.
    
    So let's now ifdef them out to booke only and add asm-offsets.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 57a8c49c8830..e8003ff81eef 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -50,6 +50,9 @@
 #endif
 #ifdef CONFIG_KVM
 #include <linux/kvm_host.h>
+#ifndef CONFIG_BOOKE
+#include <asm/kvm_book3s.h>
+#endif
 #endif
 
 #ifdef CONFIG_PPC32
@@ -191,33 +194,9 @@ int main(void)
 	DEFINE(PACA_DATA_OFFSET, offsetof(struct paca_struct, data_offset));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 #ifdef CONFIG_KVM_BOOK3S_64_HANDLER
-	DEFINE(PACA_KVM_IN_GUEST, offsetof(struct paca_struct, kvm_in_guest));
-	DEFINE(PACA_KVM_SLB, offsetof(struct paca_struct, kvm_slb));
-	DEFINE(PACA_KVM_SLB_MAX, offsetof(struct paca_struct, kvm_slb_max));
-	DEFINE(PACA_KVM_CR, offsetof(struct paca_struct, shadow_vcpu.cr));
-	DEFINE(PACA_KVM_XER, offsetof(struct paca_struct, shadow_vcpu.xer));
-	DEFINE(PACA_KVM_R0, offsetof(struct paca_struct, shadow_vcpu.gpr[0]));
-	DEFINE(PACA_KVM_R1, offsetof(struct paca_struct, shadow_vcpu.gpr[1]));
-	DEFINE(PACA_KVM_R2, offsetof(struct paca_struct, shadow_vcpu.gpr[2]));
-	DEFINE(PACA_KVM_R3, offsetof(struct paca_struct, shadow_vcpu.gpr[3]));
-	DEFINE(PACA_KVM_R4, offsetof(struct paca_struct, shadow_vcpu.gpr[4]));
-	DEFINE(PACA_KVM_R5, offsetof(struct paca_struct, shadow_vcpu.gpr[5]));
-	DEFINE(PACA_KVM_R6, offsetof(struct paca_struct, shadow_vcpu.gpr[6]));
-	DEFINE(PACA_KVM_R7, offsetof(struct paca_struct, shadow_vcpu.gpr[7]));
-	DEFINE(PACA_KVM_R8, offsetof(struct paca_struct, shadow_vcpu.gpr[8]));
-	DEFINE(PACA_KVM_R9, offsetof(struct paca_struct, shadow_vcpu.gpr[9]));
-	DEFINE(PACA_KVM_R10, offsetof(struct paca_struct, shadow_vcpu.gpr[10]));
-	DEFINE(PACA_KVM_R11, offsetof(struct paca_struct, shadow_vcpu.gpr[11]));
-	DEFINE(PACA_KVM_R12, offsetof(struct paca_struct, shadow_vcpu.gpr[12]));
-	DEFINE(PACA_KVM_R13, offsetof(struct paca_struct, shadow_vcpu.gpr[13]));
-	DEFINE(PACA_KVM_HOST_R1, offsetof(struct paca_struct, shadow_vcpu.host_r1));
-	DEFINE(PACA_KVM_HOST_R2, offsetof(struct paca_struct, shadow_vcpu.host_r2));
-	DEFINE(PACA_KVM_VMHANDLER, offsetof(struct paca_struct,
-					    shadow_vcpu.vmhandler));
-	DEFINE(PACA_KVM_SCRATCH0, offsetof(struct paca_struct,
-					   shadow_vcpu.scratch0));
-	DEFINE(PACA_KVM_SCRATCH1, offsetof(struct paca_struct,
-					   shadow_vcpu.scratch1));
+	DEFINE(PACA_KVM_SVCPU, offsetof(struct paca_struct, shadow_vcpu));
+	DEFINE(SVCPU_SLB, offsetof(struct kvmppc_book3s_shadow_vcpu, slb));
+	DEFINE(SVCPU_SLB_MAX, offsetof(struct kvmppc_book3s_shadow_vcpu, slb_max));
 #endif
 #endif /* CONFIG_PPC64 */
 
@@ -412,9 +391,6 @@ int main(void)
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
-	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
-	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
-	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
 	DEFINE(VCPU_MSR, offsetof(struct kvm_vcpu, arch.msr));
 	DEFINE(VCPU_SPRG4, offsetof(struct kvm_vcpu, arch.sprg4));
 	DEFINE(VCPU_SPRG5, offsetof(struct kvm_vcpu, arch.sprg5));
@@ -422,26 +398,67 @@ int main(void)
 	DEFINE(VCPU_SPRG7, offsetof(struct kvm_vcpu, arch.sprg7));
 	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
 
-	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
-	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
-	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
-
 	/* book3s */
 #ifdef CONFIG_PPC_BOOK3S
-	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
 	DEFINE(VCPU_HOST_RETIP, offsetof(struct kvm_vcpu, arch.host_retip));
-	DEFINE(VCPU_HOST_R2, offsetof(struct kvm_vcpu, arch.host_r2));
 	DEFINE(VCPU_HOST_MSR, offsetof(struct kvm_vcpu, arch.host_msr));
 	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
-	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
 	DEFINE(VCPU_TRAMPOLINE_LOWMEM, offsetof(struct kvm_vcpu, arch.trampoline_lowmem));
 	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
 	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));
 	DEFINE(VCPU_RMCALL, offsetof(struct kvm_vcpu, arch.rmcall));
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
+	DEFINE(VCPU_SVCPU, offsetof(struct kvmppc_vcpu_book3s, shadow_vcpu) -
+			   offsetof(struct kvmppc_vcpu_book3s, vcpu));
+	DEFINE(SVCPU_CR, offsetof(struct kvmppc_book3s_shadow_vcpu, cr));
+	DEFINE(SVCPU_XER, offsetof(struct kvmppc_book3s_shadow_vcpu, xer));
+	DEFINE(SVCPU_CTR, offsetof(struct kvmppc_book3s_shadow_vcpu, ctr));
+	DEFINE(SVCPU_LR, offsetof(struct kvmppc_book3s_shadow_vcpu, lr));
+	DEFINE(SVCPU_PC, offsetof(struct kvmppc_book3s_shadow_vcpu, pc));
+	DEFINE(SVCPU_R0, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[0]));
+	DEFINE(SVCPU_R1, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[1]));
+	DEFINE(SVCPU_R2, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[2]));
+	DEFINE(SVCPU_R3, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[3]));
+	DEFINE(SVCPU_R4, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[4]));
+	DEFINE(SVCPU_R5, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[5]));
+	DEFINE(SVCPU_R6, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[6]));
+	DEFINE(SVCPU_R7, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[7]));
+	DEFINE(SVCPU_R8, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[8]));
+	DEFINE(SVCPU_R9, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[9]));
+	DEFINE(SVCPU_R10, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[10]));
+	DEFINE(SVCPU_R11, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[11]));
+	DEFINE(SVCPU_R12, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[12]));
+	DEFINE(SVCPU_R13, offsetof(struct kvmppc_book3s_shadow_vcpu, gpr[13]));
+	DEFINE(SVCPU_HOST_R1, offsetof(struct kvmppc_book3s_shadow_vcpu, host_r1));
+	DEFINE(SVCPU_HOST_R2, offsetof(struct kvmppc_book3s_shadow_vcpu, host_r2));
+	DEFINE(SVCPU_VMHANDLER, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					 vmhandler));
+	DEFINE(SVCPU_SCRATCH0, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					scratch0));
+	DEFINE(SVCPU_SCRATCH1, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					scratch1));
+	DEFINE(SVCPU_IN_GUEST, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					in_guest));
+	DEFINE(SVCPU_FAULT_DSISR, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					   fault_dsisr));
+	DEFINE(SVCPU_FAULT_DAR, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					 fault_dar));
+	DEFINE(SVCPU_LAST_INST, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					 last_inst));
+	DEFINE(SVCPU_SHADOW_SRR1, offsetof(struct kvmppc_book3s_shadow_vcpu,
+					   shadow_srr1));
+#ifdef CONFIG_PPC_BOOK3S_32
+	DEFINE(SVCPU_SR, offsetof(struct kvmppc_book3s_shadow_vcpu, sr));
+#endif
 #else
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
+	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
+	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
+	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
+	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
+	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
+	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
 #endif /* CONFIG_PPC_BOOK3S */
 #endif
 #ifdef CONFIG_44x

commit 00c3a37ca332f54f2187720e51f7c0e18e91d7c9
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:42 2010 +0200

    KVM: PPC: Use CONFIG_PPC_BOOK3S define
    
    Upstream recently added a new name for PPC64: Book3S_64.
    
    So instead of using CONFIG_PPC64 we should use CONFIG_PPC_BOOK3S consotently.
    That makes understanding the code easier (I hope).
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 957ceb7059c5..57a8c49c8830 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -426,8 +426,8 @@ int main(void)
 	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
 	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
 
-	/* book3s_64 */
-#ifdef CONFIG_PPC64
+	/* book3s */
+#ifdef CONFIG_PPC_BOOK3S
 	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
 	DEFINE(VCPU_HOST_RETIP, offsetof(struct kvm_vcpu, arch.host_retip));
 	DEFINE(VCPU_HOST_R2, offsetof(struct kvm_vcpu, arch.host_r2));
@@ -442,7 +442,7 @@ int main(void)
 #else
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
 	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
-#endif /* CONFIG_PPC64 */
+#endif /* CONFIG_PPC_BOOK3S */
 #endif
 #ifdef CONFIG_44x
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);

commit 0fe1ac48bef018bed896307cd12f6ca9b5e704ab
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Apr 13 20:46:04 2010 +0000

    powerpc/perf_event: Fix oops due to perf_event_do_pending call
    
    Anton Blanchard found that large POWER systems would occasionally
    crash in the exception exit path when profiling with perf_events.
    The symptom was that an interrupt would occur late in the exit path
    when the MSR[RI] (recoverable interrupt) bit was clear.  Interrupts
    should be hard-disabled at this point but they were enabled.  Because
    the interrupt was not recoverable the system panicked.
    
    The reason is that the exception exit path was calling
    perf_event_do_pending after hard-disabling interrupts, and
    perf_event_do_pending will re-enable interrupts.
    
    The simplest and cleanest fix for this is to use the same mechanism
    that 32-bit powerpc does, namely to cause a self-IPI by setting the
    decrementer to 1.  This means we can remove the tests in the exception
    exit path and raw_local_irq_restore.
    
    This also makes sure that the call to perf_event_do_pending from
    timer_interrupt() happens within irq_enter/irq_exit.  (Note that
    calling perf_event_do_pending from timer_interrupt does not mean that
    there is a possible 1/HZ latency; setting the decrementer to 1 ensures
    that the timer interrupt will happen immediately, i.e. within one
    timebase tick, which is a few nanoseconds or 10s of nanoseconds.)
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: stable@kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 957ceb7059c5..c09138d150d4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -133,7 +133,6 @@ int main(void)
 	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
 	DEFINE(PACAHARDIRQEN, offsetof(struct paca_struct, hard_enabled));
-	DEFINE(PACAPERFPEND, offsetof(struct paca_struct, perf_event_pending));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
 #ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,

commit f7adbba1e5d464b0d449adac1eb2519be6be9728
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 15 14:49:13 2010 +0100

    KVM: PPC: Keep SRR1 flags around in shadow_msr
    
    SRR1 stores more information that just the MSR value. It also stores
    valuable information about the type of interrupt we received, for
    example whether the storage interrupt we just got was because of a
    missing htab entry or not.
    
    We use that information to speed up the exit path.
    
    Now if we get preempted before we can interpret the shadow_msr values,
    we get into vcpu_put which then calls the MSR handler, which then sets
    all the SRR1 information bits in shadow_msr to 0. Great.
    
    So let's preserve the SRR1 specific bits in shadow_msr whenever we set
    the MSR. They don't hurt.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ee9935442f0e..957ceb7059c5 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -433,6 +433,7 @@ int main(void)
 	DEFINE(VCPU_HOST_R2, offsetof(struct kvm_vcpu, arch.host_r2));
 	DEFINE(VCPU_HOST_MSR, offsetof(struct kvm_vcpu, arch.host_msr));
 	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
+	DEFINE(VCPU_SHADOW_SRR1, offsetof(struct kvm_vcpu, arch.shadow_srr1));
 	DEFINE(VCPU_TRAMPOLINE_LOWMEM, offsetof(struct kvm_vcpu, arch.trampoline_lowmem));
 	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
 	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));

commit 021ec9c69f8b7b20f46296cc76cc4cb341b25191
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:06 2010 +0100

    KVM: PPC: Call SLB patching code in interrupt safe manner
    
    Currently we're racy when doing the transition from IR=1 to IR=0, from
    the module memory entry code to the real mode SLB switching code.
    
    To work around that I took a look at the RTAS entry code which is faced
    with a similar problem and did the same thing:
    
      A small helper in linear mapped memory that does mtmsr with IR=0 and
      then RFIs info the actual handler.
    
    Thanks to that trick we can safely take page faults in the entry code
    and only need to be really wary of what to do as of the SLB switching
    part.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1501e77c980c..ee9935442f0e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -214,8 +214,6 @@ int main(void)
 	DEFINE(PACA_KVM_HOST_R2, offsetof(struct paca_struct, shadow_vcpu.host_r2));
 	DEFINE(PACA_KVM_VMHANDLER, offsetof(struct paca_struct,
 					    shadow_vcpu.vmhandler));
-	DEFINE(PACA_KVM_RMHANDLER, offsetof(struct paca_struct,
-					    shadow_vcpu.rmhandler));
 	DEFINE(PACA_KVM_SCRATCH0, offsetof(struct paca_struct,
 					   shadow_vcpu.scratch0));
 	DEFINE(PACA_KVM_SCRATCH1, offsetof(struct paca_struct,
@@ -438,6 +436,7 @@ int main(void)
 	DEFINE(VCPU_TRAMPOLINE_LOWMEM, offsetof(struct kvm_vcpu, arch.trampoline_lowmem));
 	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
 	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));
+	DEFINE(VCPU_RMCALL, offsetof(struct kvm_vcpu, arch.rmcall));
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
 #else
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));

commit 7e57cba06074da84d7c24d8c3f44040d2d8c88ac
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:03 2010 +0100

    KVM: PPC: Use PACA backed shadow vcpu
    
    We're being horribly racy right now. All the entry and exit code hijacks
    random fields from the PACA that could easily be used by different code in
    case we get interrupted, for example by a #MC or even page fault.
    
    After discussing this with Ben, we figured it's best to reserve some more
    space in the PACA and just shove off some vcpu state to there.
    
    That way we can drastically improve the readability of the code, make it
    less racy and less complex.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index a6c2b63227b3..1501e77c980c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -194,6 +194,32 @@ int main(void)
 	DEFINE(PACA_KVM_IN_GUEST, offsetof(struct paca_struct, kvm_in_guest));
 	DEFINE(PACA_KVM_SLB, offsetof(struct paca_struct, kvm_slb));
 	DEFINE(PACA_KVM_SLB_MAX, offsetof(struct paca_struct, kvm_slb_max));
+	DEFINE(PACA_KVM_CR, offsetof(struct paca_struct, shadow_vcpu.cr));
+	DEFINE(PACA_KVM_XER, offsetof(struct paca_struct, shadow_vcpu.xer));
+	DEFINE(PACA_KVM_R0, offsetof(struct paca_struct, shadow_vcpu.gpr[0]));
+	DEFINE(PACA_KVM_R1, offsetof(struct paca_struct, shadow_vcpu.gpr[1]));
+	DEFINE(PACA_KVM_R2, offsetof(struct paca_struct, shadow_vcpu.gpr[2]));
+	DEFINE(PACA_KVM_R3, offsetof(struct paca_struct, shadow_vcpu.gpr[3]));
+	DEFINE(PACA_KVM_R4, offsetof(struct paca_struct, shadow_vcpu.gpr[4]));
+	DEFINE(PACA_KVM_R5, offsetof(struct paca_struct, shadow_vcpu.gpr[5]));
+	DEFINE(PACA_KVM_R6, offsetof(struct paca_struct, shadow_vcpu.gpr[6]));
+	DEFINE(PACA_KVM_R7, offsetof(struct paca_struct, shadow_vcpu.gpr[7]));
+	DEFINE(PACA_KVM_R8, offsetof(struct paca_struct, shadow_vcpu.gpr[8]));
+	DEFINE(PACA_KVM_R9, offsetof(struct paca_struct, shadow_vcpu.gpr[9]));
+	DEFINE(PACA_KVM_R10, offsetof(struct paca_struct, shadow_vcpu.gpr[10]));
+	DEFINE(PACA_KVM_R11, offsetof(struct paca_struct, shadow_vcpu.gpr[11]));
+	DEFINE(PACA_KVM_R12, offsetof(struct paca_struct, shadow_vcpu.gpr[12]));
+	DEFINE(PACA_KVM_R13, offsetof(struct paca_struct, shadow_vcpu.gpr[13]));
+	DEFINE(PACA_KVM_HOST_R1, offsetof(struct paca_struct, shadow_vcpu.host_r1));
+	DEFINE(PACA_KVM_HOST_R2, offsetof(struct paca_struct, shadow_vcpu.host_r2));
+	DEFINE(PACA_KVM_VMHANDLER, offsetof(struct paca_struct,
+					    shadow_vcpu.vmhandler));
+	DEFINE(PACA_KVM_RMHANDLER, offsetof(struct paca_struct,
+					    shadow_vcpu.rmhandler));
+	DEFINE(PACA_KVM_SCRATCH0, offsetof(struct paca_struct,
+					   shadow_vcpu.scratch0));
+	DEFINE(PACA_KVM_SCRATCH1, offsetof(struct paca_struct,
+					   shadow_vcpu.scratch1));
 #endif
 #endif /* CONFIG_PPC64 */
 
@@ -389,8 +415,6 @@ int main(void)
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
-	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
-	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
 	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
 	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
 	DEFINE(VCPU_MSR, offsetof(struct kvm_vcpu, arch.msr));
@@ -415,7 +439,10 @@ int main(void)
 	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
 	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));
 	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
-#endif
+#else
+	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
+	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
+#endif /* CONFIG_PPC64 */
 #endif
 #ifdef CONFIG_44x
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);

commit 8b27f0b61db57f5555fc2d3fc95c3ea9fd1a9d6c
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Oct 15 12:49:01 2009 -0500

    powerpc/fsl-booke: Rework TLB CAM code
    
    Re-write the code so its more standalone and fixed some issues:
    * Bump'd # of CAM entries to 64 to support e500mc
    * Make the code handle MAS7 properly
    * Use pr_cont instead of creating a string as we go
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e2e2082acf29..a6c2b63227b3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -421,9 +421,6 @@ int main(void)
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);
 	DEFINE(PTE_T_LOG2, PTE_T_LOG2);
 #endif
-#ifdef CONFIG_FSL_BOOKE
-	DEFINE(TLBCAM_SIZE, sizeof(struct tlbcam));
-#endif
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	DEFINE(VCPU_TIMING_EXIT_TBU, offsetof(struct kvm_vcpu,

commit 55c758840a2ab0aa31530aca982dd5e9281a169c
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Oct 30 05:47:23 2009 +0000

    Export new PACA constants in asm-offsets
    
    In order to access fields in the PACA from assembly code, we need
    to generate offsets using asm-offsets.c.
    
    So let's add the new PACA related bits, we just introduced!
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index aba3ea64661e..e2e2082acf29 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -190,6 +190,11 @@ int main(void)
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_DATA_OFFSET, offsetof(struct paca_struct, data_offset));
 	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
+#ifdef CONFIG_KVM_BOOK3S_64_HANDLER
+	DEFINE(PACA_KVM_IN_GUEST, offsetof(struct paca_struct, kvm_in_guest));
+	DEFINE(PACA_KVM_SLB, offsetof(struct paca_struct, kvm_slb));
+	DEFINE(PACA_KVM_SLB_MAX, offsetof(struct paca_struct, kvm_slb_max));
+#endif
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */

commit 62908905b2fd98c11bd472c1e617d35eff33fc84
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Oct 30 05:47:18 2009 +0000

    Add Book3s_64 offsets to asm-offsets.c
    
    We need to access some VCPU fields from assembly code. In order to get
    the proper offsets, we have to define them in asm-offsets.c.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0812b0f414bb..aba3ea64661e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -398,6 +398,19 @@ int main(void)
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
 	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
+
+	/* book3s_64 */
+#ifdef CONFIG_PPC64
+	DEFINE(VCPU_FAULT_DSISR, offsetof(struct kvm_vcpu, arch.fault_dsisr));
+	DEFINE(VCPU_HOST_RETIP, offsetof(struct kvm_vcpu, arch.host_retip));
+	DEFINE(VCPU_HOST_R2, offsetof(struct kvm_vcpu, arch.host_r2));
+	DEFINE(VCPU_HOST_MSR, offsetof(struct kvm_vcpu, arch.host_msr));
+	DEFINE(VCPU_SHADOW_MSR, offsetof(struct kvm_vcpu, arch.shadow_msr));
+	DEFINE(VCPU_TRAMPOLINE_LOWMEM, offsetof(struct kvm_vcpu, arch.trampoline_lowmem));
+	DEFINE(VCPU_TRAMPOLINE_ENTER, offsetof(struct kvm_vcpu, arch.trampoline_enter));
+	DEFINE(VCPU_HIGHMEM_HANDLER, offsetof(struct kvm_vcpu, arch.highmem_handler));
+	DEFINE(VCPU_HFLAGS, offsetof(struct kvm_vcpu, arch.hflags));
+#endif
 #endif
 #ifdef CONFIG_44x
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);

commit cdd6c482c9ff9c55475ee7392ec8f672eddb7be6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 21 12:02:48 2009 +0200

    perf: Do the big rename: Performance Counters -> Performance Events
    
    Bye-bye Performance Counters, welcome Performance Events!
    
    In the past few months the perfcounters subsystem has grown out its
    initial role of counting hardware events, and has become (and is
    becoming) a much broader generic event enumeration, reporting, logging,
    monitoring, analysis facility.
    
    Naming its core object 'perf_counter' and naming the subsystem
    'perfcounters' has become more and more of a misnomer. With pending
    code like hw-breakpoints support the 'counter' name is less and
    less appropriate.
    
    All in one, we've decided to rename the subsystem to 'performance
    events' and to propagate this rename through all fields, variables
    and API names. (in an ABI compatible fashion)
    
    The word 'event' is also a bit shorter than 'counter' - which makes
    it slightly more convenient to write/handle as well.
    
    Thanks goes to Stephane Eranian who first observed this misnomer and
    suggested a rename.
    
    User-space tooling and ABI compatibility is not affected - this patch
    should be function-invariant. (Also, defconfigs were not touched to
    keep the size down.)
    
    This patch has been generated via the following script:
    
      FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
      sed -i \
        -e 's/PERF_EVENT_/PERF_RECORD_/g' \
        -e 's/PERF_COUNTER/PERF_EVENT/g' \
        -e 's/perf_counter/perf_event/g' \
        -e 's/nb_counters/nb_events/g' \
        -e 's/swcounter/swevent/g' \
        -e 's/tpcounter_event/tp_event/g' \
        $FILES
    
      for N in $(find . -name perf_counter.[ch]); do
        M=$(echo $N | sed 's/perf_counter/perf_event/g')
        mv $N $M
      done
    
      FILES=$(find . -name perf_event.*)
    
      sed -i \
        -e 's/COUNTER_MASK/REG_MASK/g' \
        -e 's/COUNTER/EVENT/g' \
        -e 's/\<event\>/event_id/g' \
        -e 's/counter/event/g' \
        -e 's/Counter/Event/g' \
        $FILES
    
    ... to keep it as correct as possible. This script can also be
    used by anyone who has pending perfcounters patches - it converts
    a Linux kernel tree over to the new naming. We tried to time this
    change to the point in time where the amount of pending patches
    is the smallest: the end of the merge window.
    
    Namespace clashes were fixed up in a preparatory patch - and some
    stylistic fallout will be fixed up in a subsequent patch.
    
    ( NOTE: 'counters' are still the proper terminology when we deal
      with hardware registers - and these sed scripts are a bit
      over-eager in renaming them. I've undone some of that, but
      in case there's something left where 'counter' would be
      better than 'event' we can undo that on an individual basis
      instead of touching an otherwise nicely automated patch. )
    
    Suggested-by: Stephane Eranian <eranian@google.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Kyle McMartin <kyle@mcmartin.ca>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: <linux-arch@vger.kernel.org>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index f0df285f0f87..0812b0f414bb 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -133,7 +133,7 @@ int main(void)
 	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
 	DEFINE(PACAHARDIRQEN, offsetof(struct paca_struct, hard_enabled));
-	DEFINE(PACAPERFPEND, offsetof(struct paca_struct, perf_counter_pending));
+	DEFINE(PACAPERFPEND, offsetof(struct paca_struct, perf_event_pending));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
 #ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,

commit 4f0dbc2781b9dc457159b676289f874ab2dc3560
Merge: 3c15a6888002 20002ded4d93
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Aug 20 11:07:56 2009 +1000

    Merge commit 'paulus-perf/master' into next

commit dce6670aaa7efece0558010b48d5ef9d421154be
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:42 2009 +0000

    powerpc: Add PACA fields specific to 64-bit Book3E processors
    
    This adds various fields in the PACA that are for use specifically
    by Book3E processors, such as exception save areas, current pgd
    pointer, special exceptions kernel stacks etc...
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0a9f30b54952..b9e010d0fc91 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -140,6 +140,20 @@ int main(void)
 					    context.high_slices_psize));
 	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
 #endif /* CONFIG_PPC_MM_SLICES */
+
+#ifdef CONFIG_PPC_BOOK3E
+	DEFINE(PACAPGD, offsetof(struct paca_struct, pgd));
+	DEFINE(PACA_KERNELPGD, offsetof(struct paca_struct, kernel_pgd));
+	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
+	DEFINE(PACA_EXTLB, offsetof(struct paca_struct, extlb));
+	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
+	DEFINE(PACA_EXCRIT, offsetof(struct paca_struct, excrit));
+	DEFINE(PACA_EXDBG, offsetof(struct paca_struct, exdbg));
+	DEFINE(PACA_MC_STACK, offsetof(struct paca_struct, mc_kstack));
+	DEFINE(PACA_CRIT_STACK, offsetof(struct paca_struct, crit_kstack));
+	DEFINE(PACA_DBG_STACK, offsetof(struct paca_struct, dbg_kstack));
+#endif /* CONFIG_PPC_BOOK3E */
+
 #ifdef CONFIG_PPC_STD_MMU_64
 	DEFINE(PACASTABREAL, offsetof(struct paca_struct, stab_real));
 	DEFINE(PACASTABVIRT, offsetof(struct paca_struct, stab_addr));

commit 57e2a99f74b0d3720c97a6aadb57ae6aad3c61ea
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 28 11:59:34 2009 +1000

    powerpc: Add memory management headers for new 64-bit BookE
    
    This adds the PTE and pgtable format definitions, along with changes
    to the kernel memory map and other definitions related to implementing
    support for 64-bit Book3E. This also shields some asm-offset bits that
    are currently only relevant on 32-bit
    
    We also move the definition of the "linux" page size constants to
    the common mmu.h file and add a few sizes that are relevant to
    embedded processors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 561b64652311..0a9f30b54952 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -52,9 +52,11 @@
 #include <linux/kvm_host.h>
 #endif
 
+#ifdef CONFIG_PPC32
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
 #include "head_booke.h"
 #endif
+#endif
 
 #if defined(CONFIG_FSL_BOOKE)
 #include "../mm/mmu_decl.h"
@@ -260,6 +262,7 @@ int main(void)
 	DEFINE(_SRR1, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs)+8);
 #endif /* CONFIG_PPC64 */
 
+#if defined(CONFIG_PPC32)
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
 	DEFINE(EXC_LVL_SIZE, STACK_EXC_LVL_FRAME_SIZE);
 	DEFINE(MAS0, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas0));
@@ -278,7 +281,7 @@ int main(void)
 	DEFINE(_DSRR1, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, dsrr1));
 	DEFINE(SAVED_KSP_LIMIT, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, saved_ksp_limit));
 #endif
-
+#endif
 	DEFINE(CLONE_VM, CLONE_VM);
 	DEFINE(CLONE_UNTRACED, CLONE_UNTRACED);
 

commit 9c1e105238c474d19905af504f2e7f42d4f71f9e
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Aug 17 15:17:54 2009 +1000

    powerpc: Allow perf_counters to access user memory at interrupt time
    
    This provides a mechanism to allow the perf_counters code to access
    user memory in a PMU interrupt routine.  Such an access can cause
    various kinds of interrupt: SLB miss, MMU hash table miss, segment
    table miss, or TLB miss, depending on the processor.  This commit
    only deals with 64-bit classic/server processors, which use an MMU
    hash table.  32-bit processors are already able to access user memory
    at interrupt time.  Since we don't soft-disable on 32-bit, we avoid
    the possibility of reentering hash_page or the TLB miss handlers,
    since they run with interrupts disabled.
    
    On 64-bit processors, an SLB miss interrupt on a user address will
    update the slb_cache and slb_cache_ptr fields in the paca.  This is
    OK except in the case where a PMU interrupt occurs in switch_slb,
    which also accesses those fields.  To prevent this, we hard-disable
    interrupts in switch_slb.  Interrupts are already soft-disabled at
    this point, and will get hard-enabled when they get soft-enabled
    later.
    
    This also reworks slb_flush_and_rebolt: to avoid hard-disabling twice,
    and to make sure that it clears the slb_cache_ptr when called from
    other callers than switch_slb, the existing routine is renamed to
    __slb_flush_and_rebolt, which is called by switch_slb and the new
    version of slb_flush_and_rebolt.
    
    Similarly, switch_stab (used on POWER3 and RS64 processors) gets a
    hard_irq_disable() to protect the per-cpu variables used there and
    in ste_allocate.
    
    If a MMU hashtable miss interrupt occurs, normally we would call
    hash_page to look up the Linux PTE for the address and create a HPTE.
    However, hash_page is fairly complex and takes some locks, so to
    avoid the possibility of deadlock, we check the preemption count
    to see if we are in a (pseudo-)NMI handler, and if so, we don't call
    hash_page but instead treat it like a bad access that will get
    reported up through the exception table mechanism.  An interrupt
    whose handler runs even though the interrupt occurred when
    soft-disabled (such as the PMU interrupt) is considered a pseudo-NMI
    handler, which should use nmi_enter()/nmi_exit() rather than
    irq_enter()/irq_exit().
    
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 561b64652311..197b15646eeb 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -67,6 +67,8 @@ int main(void)
 	DEFINE(MMCONTEXTID, offsetof(struct mm_struct, context.id));
 #ifdef CONFIG_PPC64
 	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
+	DEFINE(SIGSEGV, SIGSEGV);
+	DEFINE(NMI_MASK, NMI_MASK);
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
 #endif /* CONFIG_PPC64 */

commit bc47ab0241c7c86da4f5e5f82fbca7d45387c18d
Merge: 37f9ef553bed 8ebf975608aa
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Jun 12 16:53:38 2009 +1000

    Merge commit 'origin/master' into next
    
    Manual merge of:
            arch/powerpc/kernel/asm-offsets.c

commit 91c60b5b8209627590b31c07262e40c27d27d272
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jun 2 21:17:41 2009 +0000

    powerpc: Separate PACA fields for server CPUs
    
    This patch has no effect other than re-ordering PACA fields on
    current server CPUs. It however is a pre-requisite for future
    support of BookE 64-bit processors. Various parts of the PACA
    struct are now moved under some ifdef's, either the new
    CONFIG_PPC_BOOK3S or CONFIG_PPC_STD_MMU_64, whatever seems more
    appropriate.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.craashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1e40bc053946..ce90c570cd8e 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -122,8 +122,6 @@ int main(void)
 	DEFINE(PACAKSAVE, offsetof(struct paca_struct, kstack));
 	DEFINE(PACACURRENT, offsetof(struct paca_struct, __current));
 	DEFINE(PACASAVEDMSR, offsetof(struct paca_struct, saved_msr));
-	DEFINE(PACASTABREAL, offsetof(struct paca_struct, stab_real));
-	DEFINE(PACASTABVIRT, offsetof(struct paca_struct, stab_addr));
 	DEFINE(PACASTABRR, offsetof(struct paca_struct, stab_rr));
 	DEFINE(PACAR1, offsetof(struct paca_struct, saved_r1));
 	DEFINE(PACATOC, offsetof(struct paca_struct, kernel_toc));
@@ -131,35 +129,30 @@ int main(void)
 	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
 	DEFINE(PACAHARDIRQEN, offsetof(struct paca_struct, hard_enabled));
-	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
-	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
-	DEFINE(PACAVMALLOCSLLP, offsetof(struct paca_struct, vmalloc_sllp));
 #ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,
 					    context.low_slices_psize));
 	DEFINE(PACAHIGHSLICEPSIZE, offsetof(struct paca_struct,
 					    context.high_slices_psize));
 	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
+#endif /* CONFIG_PPC_MM_SLICES */
+#ifdef CONFIG_PPC_STD_MMU_64
+	DEFINE(PACASTABREAL, offsetof(struct paca_struct, stab_real));
+	DEFINE(PACASTABVIRT, offsetof(struct paca_struct, stab_addr));
+	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
+	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
+	DEFINE(PACAVMALLOCSLLP, offsetof(struct paca_struct, vmalloc_sllp));
+#ifdef CONFIG_PPC_MM_SLICES
 	DEFINE(MMUPSIZESLLP, offsetof(struct mmu_psize_def, sllp));
 #else
 	DEFINE(PACACONTEXTSLLP, offsetof(struct paca_struct, context.sllp));
-
 #endif /* CONFIG_PPC_MM_SLICES */
 	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
 	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
 	DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));
-	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
 	DEFINE(PACALPPACAPTR, offsetof(struct paca_struct, lppaca_ptr));
-	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
-	DEFINE(PACA_STARTPURR, offsetof(struct paca_struct, startpurr));
-	DEFINE(PACA_STARTSPURR, offsetof(struct paca_struct, startspurr));
-	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
-	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_SLBSHADOWPTR, offsetof(struct paca_struct, slb_shadow_ptr));
-	DEFINE(PACA_DATA_OFFSET, offsetof(struct paca_struct, data_offset));
-	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
-
 	DEFINE(SLBSHADOW_STACKVSID,
 	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid));
 	DEFINE(SLBSHADOW_STACKESID,
@@ -169,6 +162,15 @@ int main(void)
 	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
 	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
 	DEFINE(SLBSHADOW_SAVEAREA, offsetof(struct slb_shadow, save_area));
+#endif /* CONFIG_PPC_STD_MMU_64 */
+	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
+	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
+	DEFINE(PACA_STARTPURR, offsetof(struct paca_struct, startpurr));
+	DEFINE(PACA_STARTSPURR, offsetof(struct paca_struct, startspurr));
+	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
+	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
+	DEFINE(PACA_DATA_OFFSET, offsetof(struct paca_struct, data_offset));
+	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */

commit f541ae326fa120fa5c57433e4d9a133df212ce41
Merge: e255357764f9 0221c81b1b8e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Apr 6 09:02:57 2009 +0200

    Merge branch 'linus' into perfcounters/core-v2
    
    Merge reason: we have gathered quite a few conflicts, need to merge upstream
    
    Conflicts:
            arch/powerpc/kernel/Makefile
            arch/x86/ia32/ia32entry.S
            arch/x86/include/asm/hardirq.h
            arch/x86/include/asm/unistd_32.h
            arch/x86/include/asm/unistd_64.h
            arch/x86/kernel/cpu/common.c
            arch/x86/kernel/irq.c
            arch/x86/kernel/syscall_table_32.S
            arch/x86/mm/iomap_32.c
            include/linux/sched.h
            kernel/Makefile
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9ff9a26b786c35ee8d2a66222924a807ec851a9f
Merge: 0a3108beea91 0d34fb8e93ce
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Mar 30 14:04:53 2009 +1100

    Merge commit 'origin/master' into next
    
    Manual merge of:
            arch/powerpc/include/asm/elf.h
            drivers/i2c/busses/i2c-mpc.c

commit 366d4b9b9fdda282006b97316f8038cd36f8ecb7
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:23:08 2009 -0600

    KVM: ppc: No need to include core-header for KVM in asm-offsets.c currently
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 19ee491e9e23..42fe4da4e8ae 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -49,7 +49,7 @@
 #include <asm/iseries/alpaca.h>
 #endif
 #ifdef CONFIG_KVM
-#include <asm/kvm_44x.h>
+#include <linux/kvm_host.h>
 #endif
 
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
@@ -361,8 +361,6 @@ int main(void)
 	DEFINE(PTE_SIZE, sizeof(pte_t));
 
 #ifdef CONFIG_KVM
-	DEFINE(TLBE_BYTES, sizeof(struct kvmppc_44x_tlbe));
-
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));

commit 9e1e3723be3828d6faac03ff6889e78cc0e64286
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Mon Feb 23 17:40:56 2009 +0000

    powerpc: Remove unused asm-offsets entries for cpu_spec
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 19ee491e9e23..10377df38f36 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -284,9 +284,6 @@ int main(void)
 #endif /* ! CONFIG_PPC64 */
 
 	/* About the CPU features table */
-	DEFINE(CPU_SPEC_ENTRY_SIZE, sizeof(struct cpu_spec));
-	DEFINE(CPU_SPEC_PVR_MASK, offsetof(struct cpu_spec, pvr_mask));
-	DEFINE(CPU_SPEC_PVR_VALUE, offsetof(struct cpu_spec, pvr_value));
 	DEFINE(CPU_SPEC_FEATURES, offsetof(struct cpu_spec, cpu_features));
 	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
 	DEFINE(CPU_SPEC_RESTORE, offsetof(struct cpu_spec, cpu_restore));

commit 77835492ed489c0b870f82f4c50687bd267acc0a
Merge: af37501c7921 1de9e8e70f5a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 21 16:37:27 2009 +0100

    Merge commit 'v2.6.29-rc2' into perfcounters/core
    
    Conflicts:
            include/linux/syscalls.h

commit 30aae739a9eb6db31ad7b08dac44bd302f41c709
Merge: 37a76bd4f1b7 6fd8be4bf728
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jan 13 13:59:03 2009 +1100

    Merge commit 'kumar/kumar-next' into next

commit c0d362a832ee70435fc4555a64f820893b1da0bd
Merge: 506c10f26c48 f78628374a13
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Jan 11 02:44:08 2009 +0100

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/perfcounters into perfcounters/core

commit 93a6d3ce6962044fe9badf528fed46b455d58292
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Jan 9 16:52:19 2009 +1100

    powerpc: Provide a way to defer perf counter work until interrupts are enabled
    
    Because 64-bit powerpc uses lazy (soft) interrupt disabling, it is
    possible for a performance monitor exception to come in when the
    kernel thinks interrupts are disabled (i.e. when they are
    soft-disabled but hard-enabled).  In such a situation the performance
    monitor exception handler might have some processing to do (such as
    process wakeups) which can't be done in what is effectively an NMI
    handler.
    
    This provides a way to defer that work until interrupts get enabled,
    either in raw_local_irq_restore() or by returning from an interrupt
    handler to code that had interrupts enabled.  We have a per-processor
    flag that indicates that there is work pending to do when interrupts
    subsequently get re-enabled.  This flag is checked in the interrupt
    return path and in raw_local_irq_restore(), and if it is set,
    perf_counter_do_pending() is called to do the pending work.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 661d07d2146b..cea462900119 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -127,6 +127,7 @@ int main(void)
 	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
 	DEFINE(PACAHARDIRQEN, offsetof(struct paca_struct, hard_enabled));
+	DEFINE(PACAPERFPEND, offsetof(struct paca_struct, perf_counter_pending));
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
 	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));

commit 19f5465e823858a2f0b0e9a92e52816ba3ee70bb
Author: Trent Piepho <tpiepho@freescale.com>
Date:   Mon Dec 8 19:34:55 2008 -0800

    powerpc/fsl-booke: Don't hard-code size of struct tlbcam
    
    Some assembly code in head_fsl_booke.S hard-coded the size of struct tlbcam
    to 20 when it indexed the TLBCAM table.  Anyone changing the size of struct
    tlbcam would not know to expect that.
    
    The kernel already has a system to get the size of C structures into
    assembly language files, asm-offsets, so let's use it.
    
    The definition of the struct gets moved to a header, so that asm-offsets.c
    can include it.
    
    Signed-off-by: Trent Piepho <tpiepho@freescale.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 661d07d2146b..06958da94f17 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -56,6 +56,10 @@
 #include "head_booke.h"
 #endif
 
+#if defined(CONFIG_FSL_BOOKE)
+#include "../mm/mmu_decl.h"
+#endif
+
 int main(void)
 {
 	DEFINE(THREAD, offsetof(struct task_struct, thread));
@@ -384,6 +388,9 @@ int main(void)
 	DEFINE(PGD_T_LOG2, PGD_T_LOG2);
 	DEFINE(PTE_T_LOG2, PTE_T_LOG2);
 #endif
+#ifdef CONFIG_FSL_BOOKE
+	DEFINE(TLBCAM_SIZE, sizeof(struct tlbcam));
+#endif
 
 	return 0;
 }

commit 73e75b416ffcfa3a84952d8e389a0eca080f00e1
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Dec 2 15:51:57 2008 -0600

    KVM: ppc: Implement in-kernel exit timing statistics
    
    Existing KVM statistics are either just counters (kvm_stat) reported for
    KVM generally or trace based aproaches like kvm_trace.
    For KVM on powerpc we had the need to track the timings of the different exit
    types. While this could be achieved parsing data created with a kvm_trace
    extension this adds too much overhead (at least on embedded PowerPC) slowing
    down the workloads we wanted to measure.
    
    Therefore this patch adds a in-kernel exit timing statistic to the powerpc kvm
    code. These statistic is available per vm&vcpu under the kvm debugfs directory.
    As this statistic is low, but still some overhead it can be enabled via a
    .config entry and should be off by default.
    
    Since this patch touched all powerpc kvm_stat code anyway this code is now
    merged and simplified together with the exit timing statistic code (still
    working with exit timing disabled in .config).
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ba39526d3201..9937fe44555f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -383,5 +383,16 @@ int main(void)
 	DEFINE(PTE_T_LOG2, PTE_T_LOG2);
 #endif
 
+#ifdef CONFIG_KVM_EXIT_TIMING
+	DEFINE(VCPU_TIMING_EXIT_TBU, offsetof(struct kvm_vcpu,
+						arch.timing_exit.tv32.tbu));
+	DEFINE(VCPU_TIMING_EXIT_TBL, offsetof(struct kvm_vcpu,
+						arch.timing_exit.tv32.tbl));
+	DEFINE(VCPU_TIMING_LAST_ENTER_TBU, offsetof(struct kvm_vcpu,
+					arch.timing_last_enter.tv32.tbu));
+	DEFINE(VCPU_TIMING_LAST_ENTER_TBL, offsetof(struct kvm_vcpu,
+					arch.timing_last_enter.tv32.tbl));
+#endif
+
 	return 0;
 }

commit 7924bd41097ae8991c6d38cef8b1e4058e30d198
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Dec 2 15:51:55 2008 -0600

    KVM: ppc: directly insert shadow mappings into the hardware TLB
    
    Formerly, we used to maintain a per-vcpu shadow TLB and on every entry to the
    guest would load this array into the hardware TLB. This consumed 1280 bytes of
    memory (64 entries of 16 bytes plus a struct page pointer each), and also
    required some assembly to loop over the array on every entry.
    
    Instead of saving a copy in memory, we can just store shadow mappings directly
    into the hardware TLB, accepting that the host kernel will clobber these as
    part of the normal 440 TLB round robin. When we do that we need less than half
    the memory, and we have decreased the exit handling time for all guest exits,
    at the cost of increased number of TLB misses because the host overwrites some
    guest entries.
    
    These savings will be increased on processors with larger TLBs or which
    implement intelligent flush instructions like tlbivax (which will avoid the
    need to walk arrays in software).
    
    In addition to that and to the code simplification, we have a greater chance of
    leaving other host userspace mappings in the TLB, instead of forcing all
    subsequent tasks to re-fault all their mappings.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 393c7f36a1e8..ba39526d3201 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -359,12 +359,6 @@ int main(void)
 #ifdef CONFIG_KVM
 	DEFINE(TLBE_BYTES, sizeof(struct kvmppc_44x_tlbe));
 
-	DEFINE(VCPU_TO_44X, offsetof(struct kvmppc_vcpu_44x, vcpu));
-	DEFINE(VCPU44x_SHADOW_TLB,
-	       offsetof(struct kvmppc_vcpu_44x, shadow_tlb));
-	DEFINE(VCPU44x_SHADOW_MOD,
-	       offsetof(struct kvmppc_vcpu_44x, shadow_tlb_mod));
-
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));

commit db93f5745d836f81cef0b4101a7c2685eeb55efb
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:18 2008 -0600

    KVM: ppc: create struct kvm_vcpu_44x and introduce container_of() accessor
    
    This patch doesn't yet move all 44x-specific data into the new structure, but
    is the first step down that path. In the future we may also want to create a
    struct kvm_vcpu_booke.
    
    Based on patch from Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0264c97e02b5..393c7f36a1e8 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -23,9 +23,6 @@
 #include <linux/mm.h>
 #include <linux/suspend.h>
 #include <linux/hrtimer.h>
-#ifdef CONFIG_KVM
-#include <linux/kvm_host.h>
-#endif
 #ifdef CONFIG_PPC64
 #include <linux/time.h>
 #include <linux/hardirq.h>
@@ -51,6 +48,9 @@
 #ifdef CONFIG_PPC_ISERIES
 #include <asm/iseries/alpaca.h>
 #endif
+#ifdef CONFIG_KVM
+#include <asm/kvm_44x.h>
+#endif
 
 #if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
 #include "head_booke.h"
@@ -359,10 +359,14 @@ int main(void)
 #ifdef CONFIG_KVM
 	DEFINE(TLBE_BYTES, sizeof(struct kvmppc_44x_tlbe));
 
+	DEFINE(VCPU_TO_44X, offsetof(struct kvmppc_vcpu_44x, vcpu));
+	DEFINE(VCPU44x_SHADOW_TLB,
+	       offsetof(struct kvmppc_vcpu_44x, shadow_tlb));
+	DEFINE(VCPU44x_SHADOW_MOD,
+	       offsetof(struct kvmppc_vcpu_44x, shadow_tlb_mod));
+
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
-	DEFINE(VCPU_SHADOW_TLB, offsetof(struct kvm_vcpu, arch.shadow_tlb));
-	DEFINE(VCPU_SHADOW_MOD, offsetof(struct kvm_vcpu, arch.shadow_tlb_mod));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));

commit 0f55dc481ea5c4f87fc0161cb1b8c6e2cafae8fc
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:12 2008 -0600

    KVM: ppc: Rename "struct tlbe" to "struct kvmppc_44x_tlbe"
    
    This will ease ports to other cores.
    
    Also remove unused "struct kvm_tlb" while we're at it.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 661d07d2146b..0264c97e02b5 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -357,7 +357,7 @@ int main(void)
 	DEFINE(PTE_SIZE, sizeof(pte_t));
 
 #ifdef CONFIG_KVM
-	DEFINE(TLBE_BYTES, sizeof(struct tlbe));
+	DEFINE(TLBE_BYTES, sizeof(struct kvmppc_44x_tlbe));
 
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));

commit ca9153a3a2a7556d091dfe080e42b0e67881fff6
Author: Ilya Yanok <yanok@emcraft.com>
Date:   Thu Dec 11 04:55:41 2008 +0300

    powerpc/44x: Support 16K/64K base page sizes on 44x
    
    This adds support for 16k and 64k page sizes on PowerPC 44x processors.
    
    The PGDIR table is much smaller than a page when using 16k or 64k
    pages (512 and 32 bytes respectively) so we allocate the PGDIR with
    kzalloc() instead of __get_free_pages().
    
    One PTE table covers rather a large memory area when using 16k or 64k
    pages (32MB or 512MB respectively), so we can easily put FIXMAP and
    PKMAP in the area covered by one PTE table.
    
    Signed-off-by: Yuri Tikhonov <yur@emcraft.com>
    Signed-off-by: Vladimir Panfilov <pvr@emcraft.com>
    Signed-off-by: Ilya Yanok <yanok@emcraft.com>
    Acked-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c05ab1d3e620..661d07d2146b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -380,6 +380,10 @@ int main(void)
 	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
 	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
 #endif
+#ifdef CONFIG_44x
+	DEFINE(PGD_T_LOG2, PGD_T_LOG2);
+	DEFINE(PTE_T_LOG2, PTE_T_LOG2);
+#endif
 
 	return 0;
 }

commit 5e696617c425eb97bd943d781f3941fb1e8f0e5b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:24 2008 +0000

    powerpc/mm: Split mmu_context handling
    
    This splits the mmu_context handling between 32-bit hash based
    processors, 64-bit hash based processors and everybody else.  This is
    preliminary work for adding SMP support for BookE processors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 050abfd5c17c..c05ab1d3e620 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -60,6 +60,7 @@ int main(void)
 {
 	DEFINE(THREAD, offsetof(struct task_struct, thread));
 	DEFINE(MM, offsetof(struct task_struct, mm));
+	DEFINE(MMCONTEXTID, offsetof(struct mm_struct, context.id));
 #ifdef CONFIG_PPC64
 	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
 #else

commit 597bc5c00b666fe123abb0af64f6e86f7ab72a90
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 27 23:56:03 2008 +0000

    powerpc: Improve resolution of VDSO clock_gettime
    
    Currently the clock_gettime implementation in the VDSO produces a
    result with microsecond resolution for the cases that are handled
    without a system call, i.e. CLOCK_REALTIME and CLOCK_MONOTONIC.  The
    nanoseconds field of the result is obtained by computing a
    microseconds value and multiplying by 1000.
    
    This changes the code in the VDSO to do the computation for
    clock_gettime with nanosecond resolution.  That means that the
    resolution of the result will ultimately depend on the timebase
    frequency.
    
    Because the timestamp in the VDSO datapage (stamp_xsec, the real time
    corresponding to the timebase count in tb_orig_stamp) is in units of
    2^-20 seconds, it doesn't have sufficient resolution for computing a
    result with nanosecond resolution.  Therefore this adds a copy of
    xtime to the VDSO datapage and updates it in update_gtod() along with
    the other time-related fields.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 75c5dd0138fd..050abfd5c17c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -306,6 +306,7 @@ int main(void)
 	DEFINE(CFG_SYSCALL_MAP32, offsetof(struct vdso_data, syscall_map_32));
 	DEFINE(WTOM_CLOCK_SEC, offsetof(struct vdso_data, wtom_clock_sec));
 	DEFINE(WTOM_CLOCK_NSEC, offsetof(struct vdso_data, wtom_clock_nsec));
+	DEFINE(STAMP_XTIME, offsetof(struct vdso_data, stamp_xtime));
 	DEFINE(CFG_ICACHE_BLOCKSZ, offsetof(struct vdso_data, icache_block_size));
 	DEFINE(CFG_DCACHE_BLOCKSZ, offsetof(struct vdso_data, dcache_block_size));
 	DEFINE(CFG_ICACHE_LOGBLOCKSZ, offsetof(struct vdso_data, icache_log_block_size));

commit 08d19f51f05a68ce89a289320ce4ed96e757df72
Merge: 1c95e1b69073 2381ad241d0b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 16 15:36:00 2008 -0700

    Merge branch 'kvm-updates/2.6.28' of git://git.kernel.org/pub/scm/linux/kernel/git/avi/kvm
    
    * 'kvm-updates/2.6.28' of git://git.kernel.org/pub/scm/linux/kernel/git/avi/kvm: (134 commits)
      KVM: ia64: Add intel iommu support for guests.
      KVM: ia64: add directed mmio range support for kvm guests
      KVM: ia64: Make pmt table be able to hold physical mmio entries.
      KVM: Move irqchip_in_kernel() from ioapic.h to irq.h
      KVM: Separate irq ack notification out of arch/x86/kvm/irq.c
      KVM: Change is_mmio_pfn to kvm_is_mmio_pfn, and make it common for all archs
      KVM: Move device assignment logic to common code
      KVM: Device Assignment: Move vtd.c from arch/x86/kvm/ to virt/kvm/
      KVM: VMX: enable invlpg exiting if EPT is disabled
      KVM: x86: Silence various LAPIC-related host kernel messages
      KVM: Device Assignment: Map mmio pages into VT-d page table
      KVM: PIC: enhance IPI avoidance
      KVM: MMU: add "oos_shadow" parameter to disable oos
      KVM: MMU: speed up mmu_unsync_walk
      KVM: MMU: out of sync shadow core
      KVM: MMU: mmu_convert_notrap helper
      KVM: MMU: awareness of new kvm_mmu_zap_page behaviour
      KVM: MMU: mmu_parent_walk
      KVM: x86: trap invlpg
      KVM: MMU: sync roots on mmu reload
      ...

commit 49dd2c492895828a90ecdf889e7fe9cfb40a82a7
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:53 2008 -0500

    KVM: powerpc: Map guest userspace with TID=0 mappings
    
    When we use TID=N userspace mappings, we must ensure that kernel mappings have
    been destroyed when entering userspace. Using TID=1/TID=0 for kernel/user
    mappings and running userspace with PID=0 means that userspace can't access the
    kernel mappings, but the kernel can directly access userspace.
    
    The net is that we don't need to flush the TLB on privilege switches, but we do
    on guest context switches (which are far more infrequent). Guest boot time
    performance improvement: about 30%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1631d670b9ed..52649da344fb 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -369,7 +369,7 @@ int main(void)
 	DEFINE(VCPU_SPRG5, offsetof(struct kvm_vcpu, arch.sprg5));
 	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));
 	DEFINE(VCPU_SPRG7, offsetof(struct kvm_vcpu, arch.sprg7));
-	DEFINE(VCPU_PID, offsetof(struct kvm_vcpu, arch.pid));
+	DEFINE(VCPU_SHADOW_PID, offsetof(struct kvm_vcpu, arch.shadow_pid));
 
 	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
 	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));

commit 83aae4a8098eb8a40a2e9dab3714354182143b4f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:52 2008 -0500

    KVM: ppc: Write only modified shadow entries into the TLB on exit
    
    Track which TLB entries need to be written, instead of overwriting everything
    below the high water mark. Typically only a single guest TLB entry will be
    modified in a single exit.
    
    Guest boot time performance improvement: about 15%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 594064953951..1631d670b9ed 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -357,6 +357,7 @@ int main(void)
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
 	DEFINE(VCPU_SHADOW_TLB, offsetof(struct kvm_vcpu, arch.shadow_tlb));
+	DEFINE(VCPU_SHADOW_MOD, offsetof(struct kvm_vcpu, arch.shadow_tlb_mod));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
 	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));

commit 20754c2495a791b5b429c0da63394c86ade978e7
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:51 2008 -0500

    KVM: ppc: Stop saving host TLB state
    
    We're saving the host TLB state to memory on every exit, but never using it.
    Originally I had thought that we'd want to restore host TLB for heavyweight
    exits, but that could actually hurt when context switching to an unrelated host
    process (i.e. not qemu).
    
    Since this decreases the performance penalty of all exits, this patch improves
    guest boot time by about 15%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 92768d3006f7..594064953951 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -356,7 +356,6 @@ int main(void)
 
 	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
 	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
-	DEFINE(VCPU_HOST_TLB, offsetof(struct kvm_vcpu, arch.host_tlb));
 	DEFINE(VCPU_SHADOW_TLB, offsetof(struct kvm_vcpu, arch.shadow_tlb));
 	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
 	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));

commit 4ee7084eb11e00eb02dc8435fd18273a61ffa9bf
Author: Becky Bruce <becky.bruce@freescale.com>
Date:   Wed Sep 24 11:01:24 2008 -0500

    POWERPC: Allow 32-bit hashed pgtable code to support 36-bit physical
    
    This rearranges a bit of code, and adds support for
    36-bit physical addressing for configs that use a
    hashed page table.  The 36b physical support is not
    enabled by default on any config - it must be
    explicitly enabled via the config system.
    
    This patch *only* expands the page table code to accomodate
    large physical addresses on 32-bit systems and enables the
    PHYS_64BIT config option for 86xx.  It does *not*
    allow you to boot a board with more than about 3.5GB of
    RAM - for that, SWIOTLB support is also required (and
    coming soon).
    
    Signed-off-by: Becky Bruce <becky.bruce@freescale.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e9c4044012bd..09febc582584 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -352,6 +352,7 @@ int main(void)
 #endif
 
 	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);
+	DEFINE(PTE_SIZE, sizeof(pte_t));
 
 #ifdef CONFIG_KVM
 	DEFINE(TLBE_BYTES, sizeof(struct tlbe));

commit 1f6a93e4c35e75d547b51f56ba8139ab1a91628c
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Aug 30 11:40:24 2008 +1000

    powerpc: Make it possible to move the interrupt handlers away from the kernel
    
    This changes the way that the exception prologs transfer control to
    the handlers in 64-bit kernels with the aim of making it possible to
    have the prologs separate from the main body of the kernel.  Now,
    instead of computing the address of the handler by taking the top
    32 bits of the paca address (to get the 0xc0000000........ part) and
    ORing in something in the bottom 16 bits, we get the base address of
    the kernel by doing a load from the paca and add an offset.
    
    This also replaces an mfmsr and an ori to compute the MSR value for
    the handler with a load from the paca.  That makes it unnecessary to
    have a separate version of EXCEPTION_PROLOG_PSERIES that forces 64-bit
    mode.
    
    We can no longer use a direct branches in the exception prolog code,
    which means that the SLB miss handlers can't branch directly to
    .slb_miss_realmode any more.  Instead we have to compute the address
    and do an indirect branch.  This is conditional on CONFIG_RELOCATABLE;
    for non-relocatable kernels we use a direct branch as before.  (A later
    change will allow CONFIG_RELOCATABLE to be set on 64-bit powerpc.)
    
    Since the secondary CPUs on pSeries start execution in the first 0x100
    bytes of real memory and then have to get to wherever the kernel is,
    we can't use a direct branch to get there.  Instead this changes
    __secondary_hold_spinloop from a flag to a function pointer.  When it
    is set to a non-NULL value, the secondary CPUs jump to the function
    pointed to by that value.
    
    Finally this eliminates one code difference between 32-bit and 64-bit
    by making __secondary_hold be the text address of the secondary CPU
    spinloop rather than a function descriptor for it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 92768d3006f7..e9c4044012bd 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -122,6 +122,8 @@ int main(void)
 	DEFINE(PACASTABRR, offsetof(struct paca_struct, stab_rr));
 	DEFINE(PACAR1, offsetof(struct paca_struct, saved_r1));
 	DEFINE(PACATOC, offsetof(struct paca_struct, kernel_toc));
+	DEFINE(PACAKBASE, offsetof(struct paca_struct, kernelbase));
+	DEFINE(PACAKMSR, offsetof(struct paca_struct, kernel_msr));
 	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
 	DEFINE(PACAHARDIRQEN, offsetof(struct paca_struct, hard_enabled));
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));

commit c6e6771b87d4e339d27f1383c8a808ae9b4ee5b8
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Jun 25 14:07:18 2008 +1000

    powerpc: Introduce VSX thread_struct and CONFIG_VSX
    
    The layout of the new VSR registers and how they overlap on top of the
    legacy FPR and VR registers is:
    
                       VSR doubleword 0               VSR doubleword 1
              ----------------------------------------------------------------
      VSR[0]  |             FPR[0]            |                              |
              ----------------------------------------------------------------
      VSR[1]  |             FPR[1]            |                              |
              ----------------------------------------------------------------
              |              ...              |                              |
              |              ...              |                              |
              ----------------------------------------------------------------
      VSR[30] |             FPR[30]           |                              |
              ----------------------------------------------------------------
      VSR[31] |             FPR[31]           |                              |
              ----------------------------------------------------------------
      VSR[32] |                             VR[0]                            |
              ----------------------------------------------------------------
      VSR[33] |                             VR[1]                            |
              ----------------------------------------------------------------
              |                              ...                             |
              |                              ...                             |
              ----------------------------------------------------------------
      VSR[62] |                             VR[30]                           |
              ----------------------------------------------------------------
      VSR[63] |                             VR[31]                           |
              ----------------------------------------------------------------
    
    VSX has 64 128bit registers.  The first 32 regs overlap with the FP
    registers and hence extend them with and additional 64 bits.  The
    second 32 regs overlap with the VMX registers.
    
    This commit introduces the thread_struct changes required to reflect
    this register layout.  Ptrace and signals code is updated so that the
    floating point registers are correctly accessed from the thread_struct
    when CONFIG_VSX is enabled.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8655c7670350..92768d3006f7 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -78,6 +78,10 @@ int main(void)
 	DEFINE(THREAD_VSCR, offsetof(struct thread_struct, vscr));
 	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
 #endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_VSX
+	DEFINE(THREAD_VSR0, offsetof(struct thread_struct, fpr));
+	DEFINE(THREAD_USED_VSR, offsetof(struct thread_struct, used_vsr));
+#endif /* CONFIG_VSX */
 #ifdef CONFIG_PPC64
 	DEFINE(KSP_VSID, offsetof(struct thread_struct, ksp_vsid));
 #else /* CONFIG_PPC64 */

commit fca622c5b21a259950a2964ceca7b6c2a23c849f
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 30 05:23:21 2008 -0500

    [POWERPC] 40x/Book-E: Save/restore volatile exception registers
    
    On machines with more than one exception level any system register that
    might be modified by the "normal" exception level needs to be saved and
    restored on taking a higher level exception.  We already are saving
    and restoring ESR and DEAR.
    
    For critical level add SRR0/1.
    For debug level add CSRR0/1 and SRR0/1.
    For machine check level add DSRR0/1, CSRR0/1, and SRR0/1.
    
    On FSL Book-E parts we always save/restore the MAS registers for critical,
    debug, and machine check level exceptions.  On 44x we always save/restore
    the MMUCR.
    
    Additionally, we save and restore the ksp_limit since we have to adjust it
    for each exception level.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Acked-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ec9228d687b0..8655c7670350 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -52,6 +52,10 @@
 #include <asm/iseries/alpaca.h>
 #endif
 
+#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
+#include "head_booke.h"
+#endif
+
 int main(void)
 {
 	DEFINE(THREAD, offsetof(struct task_struct, thread));
@@ -242,6 +246,25 @@ int main(void)
 	DEFINE(_SRR1, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs)+8);
 #endif /* CONFIG_PPC64 */
 
+#if defined(CONFIG_BOOKE) || defined(CONFIG_40x)
+	DEFINE(EXC_LVL_SIZE, STACK_EXC_LVL_FRAME_SIZE);
+	DEFINE(MAS0, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas0));
+	/* we overload MMUCR for 44x on MAS0 since they are mutually exclusive */
+	DEFINE(MMUCR, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas0));
+	DEFINE(MAS1, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas1));
+	DEFINE(MAS2, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas2));
+	DEFINE(MAS3, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas3));
+	DEFINE(MAS6, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas6));
+	DEFINE(MAS7, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, mas7));
+	DEFINE(_SRR0, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, srr0));
+	DEFINE(_SRR1, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, srr1));
+	DEFINE(_CSRR0, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, csrr0));
+	DEFINE(_CSRR1, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, csrr1));
+	DEFINE(_DSRR0, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, dsrr0));
+	DEFINE(_DSRR1, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, dsrr1));
+	DEFINE(SAVED_KSP_LIMIT, STACK_INT_FRAME_SIZE+offsetof(struct exception_regs, saved_ksp_limit));
+#endif
+
 	DEFINE(CLONE_VM, CLONE_VM);
 	DEFINE(CLONE_UNTRACED, CLONE_UNTRACED);
 

commit 867a89e0b73af48838c7987e80899a1ff26dd6ff
Merge: 44473d991332 6c39103ce519
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 29 08:19:14 2008 -0700

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc:
      [RAPIDIO] Change RapidIO doorbell source and target ID field to 16-bit
      [RAPIDIO] Add RapidIO connection info print out and re-training for broken connections
      [RAPIDIO] Add serial RapidIO controller support, which includes MPC8548, MPC8641
      [RAPIDIO] Add RapidIO node probing into MPC86xx_HPCN board id table
      [RAPIDIO] Add RapidIO node into MPC8641HPCN dts file
      [RAPIDIO] Auto-probe the RapidIO system size
      [RAPIDIO] Add OF-tree support to RapidIO controller driver
      [RAPIDIO] Add RapidIO multi mport support
      [RAPIDIO] Move include/asm-ppc/rio.h to asm-powerpc
      [RAPIDIO] Add RapidIO option to kernel configuration
      [RAPIDIO] Change RIO function mpc85xx_ to fsl_
      [POWERPC] Provide walk_memory_resource() for powerpc
      [POWERPC] Update lmb data structures for hotplug memory add/remove
      [POWERPC] Hotplug memory remove notifications for powerpc
      [POWERPC] windfarm: Add PowerMac 12,1 support
      [POWERPC] Fix building of pmac32 when CONFIG_NVRAM=m
      [POWERPC] Add IRQSTACKS support on ppc32
      [POWERPC] Use __always_inline for xchg* and cmpxchg*
      [POWERPC] Add fast little-endian switch system call

commit d4d298feeaebb43e0a74e5e2333f1b566c34a37c
Author: Christoph Lameter <clameter@sgi.com>
Date:   Tue Apr 29 01:04:08 2008 -0700

    ppc/powerpc: use kbuild.h instead of defining macros in asm-offsets.c
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 62134845af08..59044e7ed6f4 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -30,6 +30,7 @@
 #include <linux/time.h>
 #include <linux/hardirq.h>
 #endif
+#include <linux/kbuild.h>
 
 #include <asm/io.h>
 #include <asm/page.h>
@@ -51,11 +52,6 @@
 #include <asm/iseries/alpaca.h>
 #endif
 
-#define DEFINE(sym, val) \
-	asm volatile("\n->" #sym " %0 " #val : : "i" (val))
-
-#define BLANK() asm volatile("\n->" : : )
-
 int main(void)
 {
 	DEFINE(THREAD, offsetof(struct task_struct, thread));

commit 85218827cc4ca900867807f19345418164ffc108
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Mon Apr 28 16:21:22 2008 +1000

    [POWERPC] Add IRQSTACKS support on ppc32
    
    This makes it possible to use separate stacks for hard and soft IRQs
    on 32-bit powerpc as well as on 64-bit.  The code for 32-bit is just
    the 32-bit analog of the 64-bit code.
    
    * Added allocation and initialization of the irq stacks.  We limit the
      stacks to be in lowmem for ppc32.
    * Implemented ppc32 versions of call_do_softirq() and call_handle_irq()
      to switch the stack pointers
    * Reworked how we do stack overflow detection.  We now keep around the
      limit of the stack in the thread_struct and compare against the limit
      to see if we've overflowed.  We can now use this on ppc64 if desired.
    
    [ paulus@samba.org: Fixed bug on 6xx where we need to reload r9 with the
      thread_info pointer. ]
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 62134845af08..af1d2c894ee1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -67,6 +67,7 @@ int main(void)
 #endif /* CONFIG_PPC64 */
 
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
+	DEFINE(KSP_LIMIT, offsetof(struct thread_struct, ksp_limit));
 	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
 	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
 	DEFINE(THREAD_FPR0, offsetof(struct thread_struct, fpr[0]));

commit bbf45ba57eaec56569918a8bab96ab653bd45ec1
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Apr 16 23:28:09 2008 -0500

    KVM: ppc: PowerPC 440 KVM implementation
    
    This functionality is definitely experimental, but is capable of running
    unmodified PowerPC 440 Linux kernels as guests on a PowerPC 440 host. (Only
    tested with 440EP "Bamboo" guests so far, but with appropriate userspace
    support other SoC/board combinations should work.)
    
    See Documentation/powerpc/kvm_440.txt for technical details.
    
    [stephen: build fix]
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index adf1d09d726f..62134845af08 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -23,6 +23,9 @@
 #include <linux/mm.h>
 #include <linux/suspend.h>
 #include <linux/hrtimer.h>
+#ifdef CONFIG_KVM
+#include <linux/kvm_host.h>
+#endif
 #ifdef CONFIG_PPC64
 #include <linux/time.h>
 #include <linux/hardirq.h>
@@ -324,5 +327,30 @@ int main(void)
 
 	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);
 
+#ifdef CONFIG_KVM
+	DEFINE(TLBE_BYTES, sizeof(struct tlbe));
+
+	DEFINE(VCPU_HOST_STACK, offsetof(struct kvm_vcpu, arch.host_stack));
+	DEFINE(VCPU_HOST_PID, offsetof(struct kvm_vcpu, arch.host_pid));
+	DEFINE(VCPU_HOST_TLB, offsetof(struct kvm_vcpu, arch.host_tlb));
+	DEFINE(VCPU_SHADOW_TLB, offsetof(struct kvm_vcpu, arch.shadow_tlb));
+	DEFINE(VCPU_GPRS, offsetof(struct kvm_vcpu, arch.gpr));
+	DEFINE(VCPU_LR, offsetof(struct kvm_vcpu, arch.lr));
+	DEFINE(VCPU_CR, offsetof(struct kvm_vcpu, arch.cr));
+	DEFINE(VCPU_XER, offsetof(struct kvm_vcpu, arch.xer));
+	DEFINE(VCPU_CTR, offsetof(struct kvm_vcpu, arch.ctr));
+	DEFINE(VCPU_PC, offsetof(struct kvm_vcpu, arch.pc));
+	DEFINE(VCPU_MSR, offsetof(struct kvm_vcpu, arch.msr));
+	DEFINE(VCPU_SPRG4, offsetof(struct kvm_vcpu, arch.sprg4));
+	DEFINE(VCPU_SPRG5, offsetof(struct kvm_vcpu, arch.sprg5));
+	DEFINE(VCPU_SPRG6, offsetof(struct kvm_vcpu, arch.sprg6));
+	DEFINE(VCPU_SPRG7, offsetof(struct kvm_vcpu, arch.sprg7));
+	DEFINE(VCPU_PID, offsetof(struct kvm_vcpu, arch.pid));
+
+	DEFINE(VCPU_LAST_INST, offsetof(struct kvm_vcpu, arch.last_inst));
+	DEFINE(VCPU_FAULT_DEAR, offsetof(struct kvm_vcpu, arch.fault_dear));
+	DEFINE(VCPU_FAULT_ESR, offsetof(struct kvm_vcpu, arch.fault_esr));
+#endif
+
 	return 0;
 }

commit 91120cc8e07f39078e9a60f1feac7cf665b17c2b
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Apr 24 06:33:49 2008 +1000

    [POWERPC] Cleanup asm-offsets.c
    
    * Removed TI_EXECDOMAIN define as its not used anywhere
    * Use STACK_INT_FRAME_SIZE to allow common define of INT_FRAME_SIZE
    * Define TI_CPU on both ppc32 & ppc64 (removes an ifdef).
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 292c6d8db0e1..adf1d09d726f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -93,10 +93,7 @@ int main(void)
 	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
-#ifdef CONFIG_PPC32
-	DEFINE(TI_EXECDOMAIN, offsetof(struct thread_info, exec_domain));
 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
-#endif /* CONFIG_PPC32 */
 
 #ifdef CONFIG_PPC64
 	DEFINE(DCACHEL1LINESIZE, offsetof(struct ppc64_caches, dline_size));
@@ -165,13 +162,9 @@ int main(void)
 
 	/* Interrupt register frame */
 	DEFINE(STACK_FRAME_OVERHEAD, STACK_FRAME_OVERHEAD);
-#ifndef CONFIG_PPC64
-	DEFINE(INT_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
-#else /* CONFIG_PPC64 */
+	DEFINE(INT_FRAME_SIZE, STACK_INT_FRAME_SIZE);
+#ifdef CONFIG_PPC64
 	DEFINE(SWITCH_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
-	/* 288 = # of volatile regs, int & fp, for leaf routines */
-	/* which do not stack a frame.  See the PPC64 ABI.       */
-	DEFINE(INT_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 288);
 	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
 	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
 	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);

commit 3eb9cf076180ed2003db77bd2c33ac4ed0211089
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Apr 10 16:39:18 2008 +1000

    [POWERPC] iSeries: Use alternate paca structure for booting
    
    The iSeries HV only needs the first two fields of the paca statically
    initialised, so create an alternate paca that contains only those and
    switch to our real paca immediately after boot.
    
    This is in order to make the 1024 cpu patches easier since they will no
    longer have to statically initialise the pacas for iSeries.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e932b43bd82f..292c6d8db0e1 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -44,6 +44,9 @@
 #include <asm/mmu.h>
 #include <asm/hvcall.h>
 #endif
+#ifdef CONFIG_PPC_ISERIES
+#include <asm/iseries/alpaca.h>
+#endif
 
 #define DEFINE(sym, val) \
 	asm volatile("\n->" #sym " %0 " #val : : "i" (val))
@@ -321,6 +324,9 @@ int main(void)
 	DEFINE(PAGE_OFFSET_VSID, KERNEL_VSID(PAGE_OFFSET));
 	DEFINE(VMALLOC_START_ESID, GET_ESID(VMALLOC_START));
 	DEFINE(VMALLOC_START_VSID, KERNEL_VSID(VMALLOC_START));
+
+	/* alpaca */
+	DEFINE(ALPACA_SIZE, sizeof(struct alpaca));
 #endif
 
 	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);

commit 163dab39b5432761437ae59164ab351a8680ca4f
Author: Roland McGrath <roland@redhat.com>
Date:   Thu Mar 20 08:07:51 2008 +1100

    [POWERPC] powerpc32: Remove asm-offsets ptrace cruft
    
    These items in asm-offsets.c are not used anywhere.  This removes them.
    
    Signed-off-by: Roland McGrath <roland@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4b749c416464..e932b43bd82f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -26,8 +26,6 @@
 #ifdef CONFIG_PPC64
 #include <linux/time.h>
 #include <linux/hardirq.h>
-#else
-#include <linux/ptrace.h>
 #endif
 
 #include <asm/io.h>
@@ -60,7 +58,6 @@ int main(void)
 	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
-	DEFINE(PTRACE, offsetof(struct task_struct, ptrace));
 #endif /* CONFIG_PPC64 */
 
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
@@ -80,7 +77,6 @@ int main(void)
 	DEFINE(PGDIR, offsetof(struct thread_struct, pgdir));
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
 	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
-	DEFINE(PT_PTRACED, PT_PTRACED);
 #endif
 #ifdef CONFIG_SPE
 	DEFINE(THREAD_EVR0, offsetof(struct thread_struct, evr[0]));

commit 151db1fc23800875c7ac353b106b7dab77061275
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Fri Feb 8 09:24:52 2008 +1100

    Fix compilation of powerpc asm-offsets.c with old gcc
    
    Commit ad7f71674ad7c3c4467e48f6ab9e85516dae2720 ("[POWERPC] Use a
    sensible default for clock_getres() in the VDSO") corrected the clock
    resolution reported by the VDSO clock_getres() but introduced another
    problem in that older versions of gcc (gcc-4.0 and earlier) fail to
    compile the new code in arch/powerpc/kernel/asm-offsets.c.
    
    This fixes it by introducing a new MONOTONIC_RES_NSEC define in the
    generic code which is equivalent to KTIME_MONOTONIC_RES but is just an
    integer constant, not a ktime union.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e6e49289f788..4b749c416464 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -313,7 +313,7 @@ int main(void)
 	DEFINE(CLOCK_REALTIME, CLOCK_REALTIME);
 	DEFINE(CLOCK_MONOTONIC, CLOCK_MONOTONIC);
 	DEFINE(NSEC_PER_SEC, NSEC_PER_SEC);
-	DEFINE(CLOCK_REALTIME_RES, (KTIME_MONOTONIC_RES).tv64);
+	DEFINE(CLOCK_REALTIME_RES, MONOTONIC_RES_NSEC);
 
 #ifdef CONFIG_BUG
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));

commit ad7f71674ad7c3c4467e48f6ab9e85516dae2720
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Tue Feb 5 16:16:48 2008 +1100

    [POWERPC] Use a sensible default for clock_getres() in the VDSO
    
    This ensures that the syscall and the (fast) vdso versions of
    clock_getres() will return the same resolution.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ed083feaf6f9..e6e49289f788 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -22,6 +22,7 @@
 #include <linux/mman.h>
 #include <linux/mm.h>
 #include <linux/suspend.h>
+#include <linux/hrtimer.h>
 #ifdef CONFIG_PPC64
 #include <linux/time.h>
 #include <linux/hardirq.h>
@@ -312,7 +313,7 @@ int main(void)
 	DEFINE(CLOCK_REALTIME, CLOCK_REALTIME);
 	DEFINE(CLOCK_MONOTONIC, CLOCK_MONOTONIC);
 	DEFINE(NSEC_PER_SEC, NSEC_PER_SEC);
-	DEFINE(CLOCK_REALTIME_RES, TICK_NSEC);
+	DEFINE(CLOCK_REALTIME_RES, (KTIME_MONOTONIC_RES).tv64);
 
 #ifdef CONFIG_BUG
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));

commit bee86f14d51a5a9a3b1897e301da1e415df0ba23
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Dec 6 13:11:04 2007 -0600

    [POWERPC] Fix swapper_pg_dir size when CONFIG_PTE_64BIT=y on FSL_BOOKE
    
    The size of swapper_pg_dir is 8k instead of 4k when using 64-bit PTEs
    (CONFIG_PTE_64BIT).
    
    This was reported by Cedric Hombourger <chombourger@gmail.com>
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index d67bcd84f329..ed083feaf6f9 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -326,8 +326,7 @@ int main(void)
 	DEFINE(VMALLOC_START_VSID, KERNEL_VSID(VMALLOC_START));
 #endif
 
-#ifdef CONFIG_PPC64
 	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);
-#endif
+
 	return 0;
 }

commit fbe481756df57673b6acbcd2e139d0d2658f2188
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Nov 20 12:24:45 2007 +1100

    [POWERPC] vdso: Fixes for cache block sizes
    
    The current VDSO implementation is hardcoded to 128 byte cache blocks,
    which are only used on IBM's 64-bit processors.
    
    Convert it to get the cache block sizes out of vdso_data instead,
    similar to how the ppc64 in-kernel cache flush does it.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2c8e756d19a3..d67bcd84f329 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -284,6 +284,10 @@ int main(void)
 	DEFINE(CFG_SYSCALL_MAP32, offsetof(struct vdso_data, syscall_map_32));
 	DEFINE(WTOM_CLOCK_SEC, offsetof(struct vdso_data, wtom_clock_sec));
 	DEFINE(WTOM_CLOCK_NSEC, offsetof(struct vdso_data, wtom_clock_nsec));
+	DEFINE(CFG_ICACHE_BLOCKSZ, offsetof(struct vdso_data, icache_block_size));
+	DEFINE(CFG_DCACHE_BLOCKSZ, offsetof(struct vdso_data, dcache_block_size));
+	DEFINE(CFG_ICACHE_LOGBLOCKSZ, offsetof(struct vdso_data, icache_log_block_size));
+	DEFINE(CFG_DCACHE_LOGBLOCKSZ, offsetof(struct vdso_data, dcache_log_block_size));
 #ifdef CONFIG_PPC64
 	DEFINE(CFG_SYSCALL_MAP64, offsetof(struct vdso_data, syscall_map_64));
 	DEFINE(TVAL64_TV_SEC, offsetof(struct timeval, tv_sec));

commit 4603ac180a824197c2262747948d0179eb076e9c
Author: Michael Neuling <mikey@neuling.org>
Date:   Thu Oct 18 03:06:37 2007 -0700

    powerpc: add scaled time accounting
    
    This adds POWERPC specific hooks for scaled time accounting.
    
    POWER6 includes a SPURR register.  The SPURR is based off the PURR register
    but is scaled based on CPU frequency and issue rates.  This gives a more
    accurate account of the instructions used per task.  The PURR and timebase
    will be constant relative to the wall clock, irrespective of the CPU
    frequency.
    
    This implementation reads the SPURR register in account_system_vtime which
    is only call called on context witch and hard and soft irq entry and exit.
    The percentage of user and system time is then estimated using the ratio of
    these accounted by the PURR.  If the SPURR is not present, the PURR read.
    
    An earlier implementation of this patch read the SPURR whenever the PURR
    was read, which included the system call entry and exit path.
    Unfortunately this showed a performance regression on lmbench runs, so was
    re-implemented.
    
    I've included the lmbench results here when run bare metal on POWER6.  1st
    column is the unpatch results.  2nd column is the results using the below
    patch and the 3rd is the % diff of these results from the base.  4th and
    5th columns are the results and % differnce from the base using the older
    patch (SPURR read in syscall entry/exit path).
    
                                  Base        Scaled-Acct     SPURR-in-syscall
                                 Result      Result  % diff    Result % diff
    Simple syscall:              0.3086      0.3086  0.0000    0.3452 11.8600
    Simple read:                 0.4591      0.4671  1.7425    0.5044 9.86713
    Simple write:                0.4364      0.4366  0.0458    0.4731 8.40971
    Simple stat:                 2.0055      2.0295  1.1967    2.0669 3.06158
    Simple fstat:                0.5962      0.5876  -1.442    0.6368 6.80979
    Simple open/close:           3.1283      3.1009  -0.875    3.2088 2.57328
    Select on 10 fd's:           0.8554      0.8457  -1.133    0.8667 1.32101
    Select on 100 fd's:          3.5292      3.6329  2.9383    3.6664 3.88756
    Select on 250 fd's:          7.9097      8.1881  3.5197    8.2242 3.97613
    Select on 500 fd's:          15.2659     15.836  3.7357    15.873 3.97814
    Select on 10 tcp fd's:       0.9576      0.9416  -1.670    0.9752 1.83792
    Select on 100 tcp fd's:      7.248       7.2254  -0.311    7.2685 0.28283
    Select on 250 tcp fd's:      17.7742     17.707  -0.375    17.749 -0.1406
    Select on 500 tcp fd's:      35.4258     35.25   -0.496    35.286 -0.3929
    Signal handler installation: 0.6131      0.6075  -0.913    0.647  5.52927
    Signal handler overhead:     2.0919      2.1078  0.7600    2.1831 4.35967
    Protection fault:            0.7345      0.7478  1.8107    0.8031 9.33968
    Pipe latency:                33.006      16.398  -50.31    33.475 1.42368
    AF_UNIX sock stream latency: 14.5093     30.910  113.03    30.715 111.692
    Process fork+exit:           219.8       222.8   1.3648    229.37 4.35623
    Process fork+execve:         876.14      873.28  -0.32     868.66 -0.8533
    Process fork+/bin/sh -c:     2830        2876.5  1.6431    2958   4.52296
    File /var/tmp/XXX write bw:  1193497     1195536 0.1708    118657 -0.5799
    Pagefaults on /var/tmp/XXX:  3.1272      3.2117  2.7020    3.2521 3.99398
    
    Also, kernel compile times show no difference with this patch applied.
    
    [pbadari@us.ibm.com: Avoid unnecessary PURR reading]
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Cc: Balbir Singh <balbir@in.ibm.com>
    Cc: Jay Lan <jlan@engr.sgi.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0ae5d57b9368..2c8e756d19a3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -141,6 +141,7 @@ int main(void)
 	DEFINE(PACALPPACAPTR, offsetof(struct paca_struct, lppaca_ptr));
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
 	DEFINE(PACA_STARTPURR, offsetof(struct paca_struct, startpurr));
+	DEFINE(PACA_STARTSPURR, offsetof(struct paca_struct, startspurr));
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_SLBSHADOWPTR, offsetof(struct paca_struct, slb_shadow_ptr));

commit ee7a76da1ef5e3e5e0e54e84319e435ea25c267c
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Tue Sep 18 17:22:59 2007 +1000

    [POWERPC] Size swapper_pg_dir correctly
    
    David Gibson pointed out that swapper_pg_dir actually need to be
    PGD_TABLE_SIZE bytes long not PAGE_SIZE.  This actually saves 64k in
    the bss for a kernel ppc64_defconfig built with CONFIG_PPC_64K_PAGES.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index a40805328f9b..0ae5d57b9368 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -320,5 +320,9 @@ int main(void)
 	DEFINE(VMALLOC_START_ESID, GET_ESID(VMALLOC_START));
 	DEFINE(VMALLOC_START_VSID, KERNEL_VSID(VMALLOC_START));
 #endif
+
+#ifdef CONFIG_PPC64
+	DEFINE(PGD_TABLE_SIZE, PGD_TABLE_SIZE);
+#endif
 	return 0;
 }

commit 16a15a30f8a09af6ce2dc4fd6eec9b454c1fe488
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Mon Aug 20 14:58:36 2007 +1000

    [POWERPC] iSeries: Clean up lparmap mess
    
    We need to have xLparMap in head_64.S so that it is at a fixed address
    (because the linker will not resolve (address & 0xffffffff) for us).
    But the assembler miscalculates the KERNEL_VSID() expressions.  So put
    the confusing expressions into asm-offsets.c.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2cb1d9487796..a40805328f9b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -312,5 +312,13 @@ int main(void)
 #ifdef CONFIG_BUG
 	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));
 #endif
+
+#ifdef CONFIG_PPC_ISERIES
+	/* the assembler miscalculates the VSID values */
+	DEFINE(PAGE_OFFSET_ESID, GET_ESID(PAGE_OFFSET));
+	DEFINE(PAGE_OFFSET_VSID, KERNEL_VSID(PAGE_OFFSET));
+	DEFINE(VMALLOC_START_ESID, GET_ESID(VMALLOC_START));
+	DEFINE(VMALLOC_START_VSID, KERNEL_VSID(VMALLOC_START));
+#endif
 	return 0;
 }

commit aabded9c3aab5160ae2ca3dd1fa0fa37f3d510e4
Merge: 9a9136e270af f1a1eb299a84
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed May 9 12:56:01 2007 -0700

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    * 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc:
      [POWERPC] Further fixes for the removal of 4level-fixup hack from ppc32
      [POWERPC] EEH: log all PCI-X and PCI-E AER registers
      [POWERPC] EEH: capture and log pci state on error
      [POWERPC] EEH: Split up long error msg
      [POWERPC] EEH: log error only after driver notification.
      [POWERPC] fsl_soc: Make mac_addr const in fs_enet_of_init().
      [POWERPC] Don't use SLAB/SLUB for PTE pages
      [POWERPC] Spufs support for 64K LS mappings on 4K kernels
      [POWERPC] Add ability to 4K kernel to hash in 64K pages
      [POWERPC] Introduce address space "slices"
      [POWERPC] Small fixes & cleanups in segment page size demotion
      [POWERPC] iSeries: Make HVC_ISERIES the default
      [POWERPC] iSeries: suppress build warning in lparmap.c
      [POWERPC] Mark pages that don't exist as nosave
      [POWERPC] swsusp: Introduce register_nosave_region_late

commit f7e4217b007d1f73e7e3cf10ba4fea4a608c603f
Author: Roman Zippel <zippel@linux-m68k.org>
Date:   Wed May 9 02:35:17 2007 -0700

    rename thread_info to stack
    
    This finally renames the thread_info field in task structure to stack, so that
    the assumptions about this field are gone and archs have more freedom about
    placing the thread_info structure.
    
    Nonbroken archs which have a proper thread pointer can do the access to both
    current thread and task structure via a single pointer.
    
    It'll allow for a few more cleanups of the fork code, from which e.g.  ia64
    could benefit.
    
    Signed-off-by: Roman Zippel <zippel@linux-m68k.org>
    [akpm@linux-foundation.org: build fix]
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Ian Molton <spyro@f2s.com>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Roman Zippel <zippel@linux-m68k.org>
    Cc: Greg Ungerer <gerg@uclinux.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Kazumoto Kojima <kkojima@rr.iij4u.or.jp>
    Cc: Richard Curnow <rc@rc0.org.uk>
    Cc: William Lee Irwin III <wli@holomorphy.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Cc: Miles Bader <uclinux-v850@lsi.nec.co.jp>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Chris Zankel <chris@zankel.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8f48560b7ee2..37bc35e69dbe 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -58,7 +58,7 @@ int main(void)
 #ifdef CONFIG_PPC64
 	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
 #else
-	DEFINE(THREAD_INFO, offsetof(struct task_struct, thread_info));
+	DEFINE(THREAD_INFO, offsetof(struct task_struct, stack));
 	DEFINE(PTRACE, offsetof(struct task_struct, ptrace));
 #endif /* CONFIG_PPC64 */
 

commit d0f13e3c20b6fb73ccb467bdca97fa7cf5a574cd
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue May 8 16:27:27 2007 +1000

    [POWERPC] Introduce address space "slices"
    
    The basic issue is to be able to do what hugetlbfs does but with
    different page sizes for some other special filesystems; more
    specifically, my need is:
    
     - Huge pages
    
     - SPE local store mappings using 64K pages on a 4K base page size
    kernel on Cell
    
     - Some special 4K segments in 64K-page kernels for mapping a dodgy
    type of powerpc-specific infiniband hardware that requires 4K MMU
    mappings for various reasons I won't explain here.
    
    The main issues are:
    
     - To maintain/keep track of the page size per "segment" (as we can
    only have one page size per segment on powerpc, which are 256MB
    divisions of the address space).
    
     - To make sure special mappings stay within their allotted
    "segments" (including MAP_FIXED crap)
    
     - To make sure everybody else doesn't mmap/brk/grow_stack into a
    "segment" that is used for a special mapping
    
    Some of the necessary mechanisms to handle that were present in the
    hugetlbfs code, but mostly in ways not suitable for anything else.
    
    The patch relies on some changes to the generic get_unmapped_area()
    that just got merged.  It still hijacks hugetlb callbacks here or
    there as the generic code hasn't been entirely cleaned up yet but
    that shouldn't be a problem.
    
    So what is a slice ?  Well, I re-used the mechanism used formerly by our
    hugetlbfs implementation which divides the address space in
    "meta-segments" which I called "slices".  The division is done using
    256MB slices below 4G, and 1T slices above.  Thus the address space is
    divided currently into 16 "low" slices and 16 "high" slices.  (Special
    case: high slice 0 is the area between 4G and 1T).
    
    Doing so simplifies significantly the tracking of segments and avoids
    having to keep track of all the 256MB segments in the address space.
    
    While I used the "concepts" of hugetlbfs, I mostly re-implemented
    everything in a more generic way and "ported" hugetlbfs to it.
    
    Slices can have an associated page size, which is encoded in the mmu
    context and used by the SLB miss handler to set the segment sizes.  The
    hash code currently doesn't care, it has a specific check for hugepages,
    though I might add a mechanism to provide per-slice hash mapping
    functions in the future.
    
    The slice code provide a pair of "generic" get_unmapped_area() (bottomup
    and topdown) functions that should work with any slice size.  There is
    some trickiness here so I would appreciate people to have a look at the
    implementation of these and let me know if I got something wrong.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8f48560b7ee2..d6803fb7b28b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -122,12 +122,18 @@ int main(void)
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
 	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
-	DEFINE(PACACONTEXTSLLP, offsetof(struct paca_struct, context.sllp));
 	DEFINE(PACAVMALLOCSLLP, offsetof(struct paca_struct, vmalloc_sllp));
-#ifdef CONFIG_HUGETLB_PAGE
-	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
-	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));
-#endif /* CONFIG_HUGETLB_PAGE */
+#ifdef CONFIG_PPC_MM_SLICES
+	DEFINE(PACALOWSLICESPSIZE, offsetof(struct paca_struct,
+					    context.low_slices_psize));
+	DEFINE(PACAHIGHSLICEPSIZE, offsetof(struct paca_struct,
+					    context.high_slices_psize));
+	DEFINE(MMUPSIZEDEFSIZE, sizeof(struct mmu_psize_def));
+	DEFINE(MMUPSIZESLLP, offsetof(struct mmu_psize_def, sllp));
+#else
+	DEFINE(PACACONTEXTSLLP, offsetof(struct paca_struct, context.sllp));
+
+#endif /* CONFIG_PPC_MM_SLICES */
 	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
 	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
 	DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));

commit 543b9fd3528f64c4b20439de0edb453764482de7
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Thu May 3 22:31:38 2007 +1000

    [POWERPC] powermac: Suspend to disk on G5
    
    Powermac G5 suspend to disk implementation.  The code is platform
    agnostic but only tested on powermac, no other 64-bit powerpc
    machines.
    
    Because nvidiafb still breaks suspend I have marked it EXPERIMENTAL on
    powermac and because I can't test it and some lowlevel code will need
    changes it is BROKEN on all other 64-bit platforms.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 0c5150c69175..8f48560b7ee2 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -21,12 +21,12 @@
 #include <linux/types.h>
 #include <linux/mman.h>
 #include <linux/mm.h>
+#include <linux/suspend.h>
 #ifdef CONFIG_PPC64
 #include <linux/time.h>
 #include <linux/hardirq.h>
 #else
 #include <linux/ptrace.h>
-#include <linux/suspend.h>
 #endif
 
 #include <asm/io.h>
@@ -257,11 +257,11 @@ int main(void)
 	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
 	DEFINE(CPU_SPEC_RESTORE, offsetof(struct cpu_spec, cpu_restore));
 
-#ifndef CONFIG_PPC64
 	DEFINE(pbe_address, offsetof(struct pbe, address));
 	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));
 	DEFINE(pbe_next, offsetof(struct pbe, next));
 
+#ifndef CONFIG_PPC64
 	DEFINE(TASK_SIZE, TASK_SIZE);
 	DEFINE(NUM_USER_SEGMENTS, TASK_SIZE>>28);
 #endif /* ! CONFIG_PPC64 */

commit 687304014f7ca8e2fbb3feaefef356b4a0da65ad
Author: Olof Johansson <olof@lixom.net>
Date:   Tue Apr 24 01:11:55 2007 +1000

    [POWERPC] Save trap number in bad_stack
    
    Save the trap number in the case of getting a bad stack in an exception
    handler. It is sometimes useful to know what exception it was that caused
    this to happen. Without this, no trap number is reported.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 9735e828b52d..0c5150c69175 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -139,6 +139,7 @@ int main(void)
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_SLBSHADOWPTR, offsetof(struct paca_struct, slb_shadow_ptr));
 	DEFINE(PACA_DATA_OFFSET, offsetof(struct paca_struct, data_offset));
+	DEFINE(PACA_TRAP_SAVE, offsetof(struct paca_struct, trap_save));
 
 	DEFINE(SLBSHADOW_STACKVSID,
 	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid));

commit 4002aca771a2aa2848e94a98cf51a2cae4e77ae0
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Mar 20 10:08:33 2007 -0500

    [POWERPC] Remove last_syscall
    
    Remove last_syscall from 32bit powerpc, its been gone in 64bit for years.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 030d300cd71c..9735e828b52d 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -77,7 +77,6 @@ int main(void)
 	DEFINE(KSP_VSID, offsetof(struct thread_struct, ksp_vsid));
 #else /* CONFIG_PPC64 */
 	DEFINE(PGDIR, offsetof(struct thread_struct, pgdir));
-	DEFINE(LAST_SYSCALL, offsetof(struct thread_struct, last_syscall));
 #if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
 	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
 	DEFINE(PT_PTRACED, PT_PTRACED);

commit 007d88d042d7b71aa2c9fc615aef97888e20ddf3
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Mon Jan 1 18:45:34 2007 +0000

    [POWERPC] Fix manual assembly WARN_ON() in enter_rtas().
    
    When we switched over to the generic BUG mechanism we forgot to change
    the assembly code which open-codes a WARN_ON() in enter_rtas(), so the
    bug table got corrupted.
    
    This patch provides an EMIT_BUG_ENTRY macro for use in assembly code,
    and uses it in entry_64.S. Tested with CONFIG_DEBUG_BUGVERBOSE on ppc64
    but not without -- I tried to turn it off but it wouldn't go away; I
    suspect Aunt Tillie probably needed it.
    
    This version gets __FILE__ and __LINE__ right in the assembly version --
    rather than saying include/asm-powerpc/bug.h line 21 every time which is
    a little suboptimal.
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index e96521530d21..030d300cd71c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -303,5 +303,8 @@ int main(void)
 	DEFINE(NSEC_PER_SEC, NSEC_PER_SEC);
 	DEFINE(CLOCK_REALTIME_RES, TICK_NSEC);
 
+#ifdef CONFIG_BUG
+	DEFINE(BUG_ENTRY_SIZE, sizeof(struct bug_entry));
+#endif
 	return 0;
 }

commit d04c56f73c30a5e593202ecfcf25ed43d42363a2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 4 16:47:49 2006 +1000

    [POWERPC] Lazy interrupt disabling for 64-bit machines
    
    This implements a lazy strategy for disabling interrupts.  This means
    that local_irq_disable() et al. just clear the 'interrupts are
    enabled' flag in the paca.  If an interrupt comes along, the interrupt
    entry code notices that interrupts are supposed to be disabled, and
    clears the EE bit in SRR1, clears the 'interrupts are hard-enabled'
    flag in the paca, and returns.  This means that interrupts only
    actually get disabled in the processor when an interrupt comes along.
    
    When interrupts are enabled by local_irq_enable() et al., the code
    sets the interrupts-enabled flag in the paca, and then checks whether
    interrupts got hard-disabled.  If so, it also sets the EE bit in the
    MSR to hard-enable the interrupts.
    
    This has the potential to improve performance, and also makes it
    easier to make a kernel that can boot on iSeries and on other 64-bit
    machines, since this lazy-disable strategy is very similar to the
    soft-disable strategy that iSeries already uses.
    
    This version renames paca->proc_enabled to paca->soft_enabled, and
    changes a couple of soft-disables in the kexec code to hard-disables,
    which should fix the crash that Michael Ellerman saw.  This doesn't
    yet use a reserved CR field for the soft_enabled and hard_enabled
    flags.  This applies on top of Stephen Rothwell's patches to make it
    possible to build a combined iSeries/other kernel.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index d06f378597bb..e96521530d21 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -118,7 +118,8 @@ int main(void)
 	DEFINE(PACASTABRR, offsetof(struct paca_struct, stab_rr));
 	DEFINE(PACAR1, offsetof(struct paca_struct, saved_r1));
 	DEFINE(PACATOC, offsetof(struct paca_struct, kernel_toc));
-	DEFINE(PACAPROCENABLED, offsetof(struct paca_struct, proc_enabled));
+	DEFINE(PACASOFTIRQEN, offsetof(struct paca_struct, soft_enabled));
+	DEFINE(PACAHARDIRQEN, offsetof(struct paca_struct, hard_enabled));
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
 	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));

commit f04da0bc36566ad17cf21e4ac8dbae377ca1dc75
Author: Olof Johansson <olof@lixom.net>
Date:   Wed Sep 13 13:32:39 2006 -0500

    [POWERPC] Fix non-smp build
    
    This fixes a compile error that only surfaces on CONFIG_SMP=n builds;
    <asm/hvcall.h> seems to get pulled in through another header file for
    SMP builds.  This problem was introduced by the hvcall stats patch.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c578e7ab8173..d06f378597bb 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -43,6 +43,7 @@
 #include <asm/cache.h>
 #include <asm/compat.h>
 #include <asm/mmu.h>
+#include <asm/hvcall.h>
 #endif
 
 #define DEFINE(sym, val) \

commit 57852a853b0d6761f270be0961d5d8387e98c8bb
Author: Mike Kravetz <kravetz@us.ibm.com>
Date:   Wed Sep 6 16:23:12 2006 -0700

    [POWERPC] powerpc: Instrument Hypervisor Calls
    
    Add instrumentation for hypervisor calls on pseries.  Call statistics
    include number of calls, wall time and cpu cycles (if available) and
    are made available via debugfs.  Instrumentation code is behind the
    HCALL_STATS config option and has no impact if not enabled.
    
    Signed-off-by: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index c53acd2a6dfc..c578e7ab8173 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -137,6 +137,7 @@ int main(void)
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_SLBSHADOWPTR, offsetof(struct paca_struct, slb_shadow_ptr));
+	DEFINE(PACA_DATA_OFFSET, offsetof(struct paca_struct, data_offset));
 
 	DEFINE(SLBSHADOW_STACKVSID,
 	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid));
@@ -165,6 +166,12 @@ int main(void)
 	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
 	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
 	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
+
+	/* hcall statistics */
+	DEFINE(HCALL_STAT_SIZE, sizeof(struct hcall_stats));
+	DEFINE(HCALL_STAT_CALLS, offsetof(struct hcall_stats, num_calls));
+	DEFINE(HCALL_STAT_TB, offsetof(struct hcall_stats, tb_total));
+	DEFINE(HCALL_STAT_PURR, offsetof(struct hcall_stats, purr_total));
 #endif /* CONFIG_PPC64 */
 	DEFINE(GPR0, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[0]));
 	DEFINE(GPR1, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[1]));

commit f39b7a55a84e34e3074b168e30dc73b66e85261d
Author: Olof Johansson <olof@lixom.net>
Date:   Fri Aug 11 00:07:08 2006 -0500

    [POWERPC] Cleanup CPU inits
    
    Cleanup CPU inits a bit more, Geoff Levand already did some earlier.
    
    * Move CPU state save to cpu_setup, since cpu_setup is only ever done
      on cpu 0 on 64-bit and save is never done more than once.
    * Rename __restore_cpu_setup to __restore_cpu_ppc970 and add
      function pointers to the cputable to use instead. Powermac always
      has 970 so no need to check there.
    * Rename __970_cpu_preinit to __cpu_preinit_ppc970 and check PVR before
      calling it instead of in it, it's too early to use cputable.
    * Rename pSeries_secondary_smp_init to generic_secondary_smp_init since
      everyone but powermac and iSeries use it.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index a2f95e467a75..c53acd2a6dfc 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -246,6 +246,7 @@ int main(void)
 	DEFINE(CPU_SPEC_PVR_VALUE, offsetof(struct cpu_spec, pvr_value));
 	DEFINE(CPU_SPEC_FEATURES, offsetof(struct cpu_spec, cpu_features));
 	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
+	DEFINE(CPU_SPEC_RESTORE, offsetof(struct cpu_spec, cpu_restore));
 
 #ifndef CONFIG_PPC64
 	DEFINE(pbe_address, offsetof(struct pbe, address));

commit 11a27ad782fc7ae4b7d6ac8fefad4ceb415300d6
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed Aug 9 17:00:30 2006 +1000

    [POWERPC] SLB shadow buffer cleanup
    
    Cleanup some of the #define magic as suggested by Milton.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 2ef7ea860379..a2f95e467a75 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -42,6 +42,7 @@
 #include <asm/lppaca.h>
 #include <asm/cache.h>
 #include <asm/compat.h>
+#include <asm/mmu.h>
 #endif
 
 #define DEFINE(sym, val) \
@@ -137,6 +138,10 @@ int main(void)
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 	DEFINE(PACA_SLBSHADOWPTR, offsetof(struct paca_struct, slb_shadow_ptr));
 
+	DEFINE(SLBSHADOW_STACKVSID,
+	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].vsid));
+	DEFINE(SLBSHADOW_STACKESID,
+	       offsetof(struct slb_shadow, save_area[SLB_NUM_BOLTED - 1].esid));
 	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
 	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
 	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));

commit 2f6093c84730b4bad65bcd0f2f904a5769b1dfc5
Author: Michael Neuling <mikey@neuling.org>
Date:   Mon Aug 7 16:19:19 2006 +1000

    [POWERPC] Implement SLB shadow buffer
    
    This adds a shadow buffer for the SLBs and regsiters it with PHYP.
    Only the bolted SLB entries (top 3) are shadowed.
    
    The SLB shadow buffer tells the hypervisor what the kernel needs to
    have in the SLB for the kernel to be able to function.  The hypervisor
    can use this information to speed up partition context switches.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ac0631958b20..2ef7ea860379 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -135,11 +135,13 @@ int main(void)
 	DEFINE(PACA_STARTPURR, offsetof(struct paca_struct, startpurr));
 	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
 	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
+	DEFINE(PACA_SLBSHADOWPTR, offsetof(struct paca_struct, slb_shadow_ptr));
 
 	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
 	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
 	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
 	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
+	DEFINE(SLBSHADOW_SAVEAREA, offsetof(struct slb_shadow, save_area));
 #endif /* CONFIG_PPC64 */
 
 	/* RTAS */

commit 54f5cd8afa1c9c9f8b152a946b0a7e0ecdef1631
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Jul 13 18:56:56 2006 +1000

    [POWERPC] iseries: Remove unnecessary include of iseries/hv_lp_event.h
    
    Also remove unnecessary reference to struct HvLpEvent.
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 7ee84968087b..ac0631958b20 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -40,7 +40,6 @@
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #include <asm/lppaca.h>
-#include <asm/iseries/hv_lp_event.h>
 #include <asm/cache.h>
 #include <asm/compat.h>
 #endif

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ff2940548929..7ee84968087b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -13,7 +13,6 @@
  * 2 of the License, or (at your option) any later version.
  */
 
-#include <linux/config.h>
 #include <linux/signal.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>

commit bf72aeba2ffef599d1d386425c9e46b82be657cd
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Jun 15 10:45:18 2006 +1000

    powerpc: Use 64k pages without needing cache-inhibited large pages
    
    Some POWER5+ machines can do 64k hardware pages for normal memory but
    not for cache-inhibited pages.  This patch lets us use 64k hardware
    pages for most user processes on such machines (assuming the kernel
    has been configured with CONFIG_PPC_64K_PAGES=y).  User processes
    start out using 64k pages and get switched to 4k pages if they use any
    non-cacheable mappings.
    
    With this, we use 64k pages for the vmalloc region and 4k pages for
    the imalloc region.  If anything creates a non-cacheable mapping in
    the vmalloc region, the vmalloc region will get switched to 4k pages.
    I don't know of any driver other than the DRM that would do this,
    though, and these machines don't have AGP.
    
    When a region gets switched from 64k pages to 4k pages, we do not have
    to clear out all the 64k HPTEs from the hash table immediately.  We
    use the _PAGE_COMBO bit in the Linux PTE to indicate whether the page
    was hashed in as a 64k page or a set of 4k pages.  If hash_page is
    trying to insert a 4k page for a Linux PTE and it sees that it has
    already been inserted as a 64k page, it first invalidates the 64k HPTE
    before inserting the 4k HPTE.  The hash invalidation routines also use
    the _PAGE_COMBO bit, to determine whether to look for a 64k HPTE or a
    set of 4k HPTEs to remove.  With those two changes, we can tolerate a
    mix of 4k and 64k HPTEs in the hash table, and they will all get
    removed when the address space is torn down.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index aa0486d552a3..ff2940548929 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -122,6 +122,8 @@ int main(void)
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
 	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
+	DEFINE(PACACONTEXTSLLP, offsetof(struct paca_struct, context.sllp));
+	DEFINE(PACAVMALLOCSLLP, offsetof(struct paca_struct, vmalloc_sllp));
 #ifdef CONFIG_HUGETLB_PAGE
 	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
 	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));

commit 430644312810645a6e05855db50a978df9ba3ad3
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 12 18:38:21 2006 +1000

    powerpc: Remove unused paca->pgdir field
    
    The pgdir field in the paca was a leftover from the dynamic VSIDs
    patch, and is not used in the current kernel code.  This removes it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8f85c5e8a55a..aa0486d552a3 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -122,9 +122,6 @@ int main(void)
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
 	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
-#ifdef CONFIG_PPC_64K_PAGES
-	DEFINE(PACAPGDIR, offsetof(struct paca_struct, pgdir));
-#endif
 #ifdef CONFIG_HUGETLB_PAGE
 	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
 	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));

commit f39224a8c1828bdd327539da72a53d8a13595838
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Apr 18 21:49:11 2006 +1000

    powerpc: Use correct sequence for putting CPU into nap mode
    
    We weren't using the recommended sequence for putting the CPU into
    nap mode.  When I changed the idle loop, for some reason 7447A cpus
    started hanging when we put them into nap mode.  Changing to the
    recommended sequence fixes that.
    
    The complexity here is that the recommended sequence is a loop that
    keeps putting the cpu back into nap mode.  Clearly we need some way
    to break out of the loop when an interrupt (external interrupt,
    decrementer, performance monitor) occurs.  Here we use a bit in
    the thread_info struct to indicate that we need this, and the exception
    entry code notices this and arranges for the exception to return
    to the value in the link register, thus breaking out of the loop.
    We use a new `local_flags' field in the thread_info which we can
    alter without needing to use an atomic update sequence.
    
    The PPC970 has the same recommended sequence, so we do the same thing
    there too.
    
    This also fixes a bug in the kernel stack overflow handling code on
    32-bit, since it was causing a value that we needed in a register to
    get trashed.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 54b48f330051..8f85c5e8a55a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -91,6 +91,7 @@ int main(void)
 #endif /* CONFIG_PPC64 */
 
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
+	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
 #ifdef CONFIG_PPC32

commit e8222502ee6157e2713da9e0792c21f4ad458d50
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Mar 28 23:15:54 2006 +1100

    [PATCH] powerpc: Kill _machine and hard-coded platform numbers
    
    This removes statically assigned platform numbers and reworks the
    powerpc platform probe code to use a better mechanism.  With this,
    board support files can simply declare a new machine type with a
    macro, and implement a probe() function that uses the flattened
    device-tree to detect if they apply for a given machine.
    
    We now have a machine_is() macro that replaces the comparisons of
    _machine with the various PLATFORM_* constants.  This commit also
    changes various drivers to use the new macro instead of looking at
    _machine.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 882889b15926..54b48f330051 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -105,8 +105,6 @@ int main(void)
 	DEFINE(ICACHEL1LINESIZE, offsetof(struct ppc64_caches, iline_size));
 	DEFINE(ICACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_iline_size));
 	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
-	DEFINE(PLATFORM_LPAR, PLATFORM_LPAR);
-
 	/* paca */
 	DEFINE(PACA_SIZE, sizeof(struct paca_struct));
 	DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));

commit 516450179454de9e689e0a53ed8f34b896e8651c
Merge: 6749c5507388 0d514f040ac6
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Mar 9 14:32:05 2006 +1100

    Merge ../linux-2.6

commit 1bd79336a426c5e4f3bab142407059ceb12cadf9
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Mar 8 13:24:22 2006 +1100

    powerpc: Fix various syscall/signal/swapcontext bugs
    
    A careful reading of the recent changes to the system call entry/exit
    paths revealed several problems, plus some things that could be
    simplified and improved:
    
    * 32-bit wasn't testing the _TIF_NOERROR bit in the syscall fast exit
      path, so it was only doing anything with it once it saw some other
      bit being set.  In other words, the noerror behaviour would apply to
      the next system call where we had to reschedule or deliver a signal,
      which is not necessarily the current system call.
    
    * 32-bit wasn't doing the call to ptrace_notify in the syscall exit
      path when the _TIF_SINGLESTEP bit was set.
    
    * _TIF_RESTOREALL was in both _TIF_USER_WORK_MASK and
      _TIF_PERSYSCALL_MASK, which is odd since _TIF_RESTOREALL is only set
      by system calls.  I took it out of _TIF_USER_WORK_MASK.
    
    * On 64-bit, _TIF_RESTOREALL wasn't causing the non-volatile registers
      to be restored (unless perhaps a signal was delivered or the syscall
      was traced or single-stepped).  Thus the non-volatile registers
      weren't restored on exit from a signal handler.  We probably got
      away with it mostly because signal handlers written in C wouldn't
      alter the non-volatile registers.
    
    * On 32-bit I simplified the code and made it more like 64-bit by
      making the syscall exit path jump to ret_from_except to handle
      preemption and signal delivery.
    
    * 32-bit was calling do_signal unnecessarily when _TIF_RESTOREALL was
      set - but I think because of that 32-bit was actually restoring the
      non-volatile registers on exit from a signal handler.
    
    * I changed the order of enabling interrupts and saving the
      non-volatile registers before calling do_syscall_trace_leave; now we
      enable interrupts first.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 840aad43a98b..c9a660e4c2db 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -92,7 +92,6 @@ int main(void)
 
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
-	DEFINE(TI_SIGFRAME, offsetof(struct thread_info, nvgprs_frame));
 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
 #ifdef CONFIG_PPC32
 	DEFINE(TI_EXECDOMAIN, offsetof(struct thread_info, exec_domain));

commit c6622f63db86fcbd41bf6fe05ddf2e00c1e51ced
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Feb 24 10:06:59 2006 +1100

    powerpc: Implement accurate task and CPU time accounting
    
    This implements accurate task and cpu time accounting for 64-bit
    powerpc kernels.  Instead of accounting a whole jiffy of time to a
    task on a timer interrupt because that task happened to be running at
    the time, we now account time in units of timebase ticks according to
    the actual time spent by the task in user mode and kernel mode.  We
    also count the time spent processing hardware and software interrupts
    accurately.  This is conditional on CONFIG_VIRT_CPU_ACCOUNTING.  If
    that is not set, we do tick-based approximate accounting as before.
    
    To get this accurate information, we read either the PURR (processor
    utilization of resources register) on POWER5 machines, or the timebase
    on other machines on
    
    * each entry to the kernel from usermode
    * each exit to usermode
    * transitions between process context, hard irq context and soft irq
      context in kernel mode
    * context switches.
    
    On POWER5 systems with shared-processor logical partitioning we also
    read both the PURR and the timebase at each timer interrupt and
    context switch in order to determine how much time has been taken by
    the hypervisor to run other partitions ("steal" time).  Unfortunately,
    since we need values of the PURR on both threads at the same time to
    accurately calculate the steal time, and since we can only calculate
    steal time on a per-core basis, the apportioning of the steal time
    between idle time (time which we ceded to the hypervisor in the idle
    loop) and actual stolen time is somewhat approximate at the moment.
    
    This is all based quite heavily on what s390 does, and it uses the
    generic interfaces that were added by the s390 developers,
    i.e. account_system_time(), account_user_time(), etc.
    
    This patch doesn't add any new interfaces between the kernel and
    userspace, and doesn't change the units in which time is reported to
    userspace by things such as /proc/stat, /proc/<pid>/stat, getrusage(),
    times(), etc.  Internally the various task and cpu times are stored in
    timebase units, but they are converted to USER_HZ units (1/100th of a
    second) when reported to userspace.  Some precision is therefore lost
    but there should not be any accumulating error, since the internal
    accumulation is at full precision.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 840aad43a98b..18810ac55bcc 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -137,6 +137,9 @@ int main(void)
 	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
 	DEFINE(PACALPPACAPTR, offsetof(struct paca_struct, lppaca_ptr));
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
+	DEFINE(PACA_STARTPURR, offsetof(struct paca_struct, startpurr));
+	DEFINE(PACA_USER_TIME, offsetof(struct paca_struct, user_time));
+	DEFINE(PACA_SYSTEM_TIME, offsetof(struct paca_struct, system_time));
 
 	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
 	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));

commit 3356bb9f7ba378a6e2709f9df95f4ea52111f4df
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Fri Jan 13 10:26:42 2006 +1100

    [PATCH] powerpc: Remove lppaca structure from the PACA
    
    At present the lppaca - the structure shared with the iSeries
    hypervisor and phyp - is contained within the PACA, our own low-level
    per-cpu structure.  This doesn't have to be so, the patch below
    removes it, making a separate array of lppaca structures.
    
    This saves approximately 500*NR_CPUS bytes of image size and kernel
    memory, because we don't need aligning gap between the Linux and
    hypervisor portions of every PACA.  On the other hand it means an
    extra level of dereference in many accesses to the lppaca.
    
    The patch also gets rid of several places where we assign the paca
    address to a local variable for no particular reason.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 56399c5c931a..840aad43a98b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -135,7 +135,7 @@ int main(void)
 	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
 	DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));
 	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
-	DEFINE(PACALPPACA, offsetof(struct paca_struct, lppaca));
+	DEFINE(PACALPPACAPTR, offsetof(struct paca_struct, lppaca_ptr));
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
 
 	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));

commit 404849bbd2bfd62e05b36f4753f6e1af6050a824
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Thu Nov 24 16:51:31 2005 +1100

    [PATCH] powerpc: Remove some unneeded fields from the paca
    
    This patch removes several unnecessary fields from the paca:
    
    - next_jiffy_update_tb was simply unused.  Remove trivially.
    
    - The exdsi exception save area was not used.  There were plans to use
      it, but they never seem to have gone anywhere.  If they ever do, we
      can put it back.  Remove from the paca, and from asm-offsets.c
    
    - The default_decr field was used from asm, but was only ever assigned
      the value of tb_ticks_per_jiffy.  Just access tb_ticks_per_jiffy from
      asm directly instead.
    
    Built and booted on POWER5 LPAR and iSeries RS64.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 3bf89d1a2de6..56399c5c931a 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -131,11 +131,9 @@ int main(void)
 	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
 	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));
 #endif /* CONFIG_HUGETLB_PAGE */
-	DEFINE(PACADEFAULTDECR, offsetof(struct paca_struct, default_decr));
 	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
 	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
 	DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));
-	DEFINE(PACA_EXDSI, offsetof(struct paca_struct, exdsi));
 	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
 	DEFINE(PACALPPACA, offsetof(struct paca_struct, lppaca));
 	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));

commit 401d1f029bebb7153ca704997772113dc36d9527
Author: David Woodhouse <dwmw2@infradead.org>
Date:   Tue Nov 15 18:52:18 2005 +0000

    [PATCH] syscall entry/exit revamp
    
    This cleanup patch speeds up the null syscall path on ppc64 by about 3%,
    and brings the ppc32 and ppc64 code slightly closer together.
    
    The ppc64 code was checking current_thread_info()->flags twice in the
    syscall exit path; once for TIF_SYSCALL_T_OR_A before disabling
    interrupts, and then again for TIF_SIGPENDING|TIF_NEED_RESCHED etc after
    disabling interrupts. Now we do the same as ppc32 -- check the flags
    only once in the fast path, and re-enable interrupts if necessary in the
    ptrace case.
    
    The patch abolishes the 'syscall_noerror' member of struct thread_info
    and replaces it with a TIF_NOERROR bit in the flags, which is handled in
    the slow path. This shortens the syscall entry code, which no longer
    needs to clear syscall_noerror.
    
    The patch adds a TIF_SAVE_NVGPRS flag which causes the syscall exit slow
    path to save the non-volatile GPRs into a signal frame. This removes the
    need for the assembly wrappers around sys_sigsuspend(),
    sys_rt_sigsuspend(), et al which existed solely to save those registers
    in advance. It also means I don't have to add new wrappers for ppoll()
    and pselect(), which is what I was supposed to be doing when I got
    distracted into this...
    
    Finally, it unifies the ppc64 and ppc32 methods of handling syscall exit
    directly into a signal handler (as required by sigsuspend et al) by
    introducing a TIF_RESTOREALL flag which causes _all_ the registers to be
    reloaded from the pt_regs by taking the ret_from_exception path, instead
    of the normal syscall exit path which stomps on the callee-saved GPRs.
    
    It appears to pass an LTP test run on ppc64, and passes basic testing on
    ppc32 too. Brief tests of ptrace functionality with strace and gdb also
    appear OK. I wouldn't send it to Linus for 2.6.15 just yet though :)
    
    Signed-off-by: David Woodhouse <dwmw2@infradead.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 91538d2445bf..3bf89d1a2de6 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -92,9 +92,9 @@ int main(void)
 
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
-	DEFINE(TI_SC_NOERR, offsetof(struct thread_info, syscall_noerror));
-#ifdef CONFIG_PPC32
+	DEFINE(TI_SIGFRAME, offsetof(struct thread_info, nvgprs_frame));
 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
+#ifdef CONFIG_PPC32
 	DEFINE(TI_EXECDOMAIN, offsetof(struct thread_info, exec_domain));
 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
 #endif /* CONFIG_PPC32 */

commit 0c37ec2aa88bd8a6aaeb284ff5c86f4c6d8e8469
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Nov 14 14:55:58 2005 +1100

    [PATCH] powerpc: vdso fixes (take #2)
    
    This fixes various errors in the new functions added in the vDSO's,
    I've now verified all functions on both 32 and 64 bits vDSOs. It also
    fix a sign extension bug getting the initial time of day at boot that
    could cause the monotonic clock value to be completely on bogus for
    64 bits applications (with either the vDSO or the syscall) on
    powermacs.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 4550eb4f4fbd..91538d2445bf 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -270,13 +270,15 @@ int main(void)
 	DEFINE(TVAL64_TV_USEC, offsetof(struct timeval, tv_usec));
 	DEFINE(TVAL32_TV_SEC, offsetof(struct compat_timeval, tv_sec));
 	DEFINE(TVAL32_TV_USEC, offsetof(struct compat_timeval, tv_usec));
+	DEFINE(TSPC64_TV_SEC, offsetof(struct timespec, tv_sec));
+	DEFINE(TSPC64_TV_NSEC, offsetof(struct timespec, tv_nsec));
 	DEFINE(TSPC32_TV_SEC, offsetof(struct compat_timespec, tv_sec));
 	DEFINE(TSPC32_TV_NSEC, offsetof(struct compat_timespec, tv_nsec));
 #else
 	DEFINE(TVAL32_TV_SEC, offsetof(struct timeval, tv_sec));
 	DEFINE(TVAL32_TV_USEC, offsetof(struct timeval, tv_usec));
-	DEFINE(TSPEC32_TV_SEC, offsetof(struct timespec, tv_sec));
-	DEFINE(TSPEC32_TV_NSEC, offsetof(struct timespec, tv_nsec));
+	DEFINE(TSPC32_TV_SEC, offsetof(struct timespec, tv_sec));
+	DEFINE(TSPC32_TV_NSEC, offsetof(struct timespec, tv_nsec));
 #endif
 	/* timeval/timezone offsets for use by vdso */
 	DEFINE(TZONE_TZ_MINWEST, offsetof(struct timezone, tz_minuteswest));

commit a7f290dad32ee34d931561b7943c858fe2aae503
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Nov 11 21:15:21 2005 +1100

    [PATCH] powerpc: Merge vdso's and add vdso support to 32 bits kernel
    
    This patch moves the vdso's to arch/powerpc, adds support for the 32
    bits vdso to the 32 bits kernel, rename systemcfg (finally !), and adds
    some new (still untested) routines to both vdso's: clock_gettime() with
    support for CLOCK_REALTIME and CLOCK_MONOTONIC, clock_getres() (same
    clocks) and get_tbfreq() for glibc to retreive the timebase frequency.
    
    Tom,Steve: The implementation of get_tbfreq() I've done for 32 bits
    returns a long long (r3, r4) not a long. This is such that if we ever
    add support for >4Ghz timebases on ppc32, the userland interface won't
    have to change.
    
    I have tested gettimeofday() using some glibc patches in both ppc32 and
    ppc64 kernels using 32 bits userland (I haven't had a chance to test a
    64 bits userland yet, but the implementation didn't change and was
    tested earlier). I haven't tested yet the new functions.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 8793102711a8..4550eb4f4fbd 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -37,12 +37,12 @@
 #include <asm/cputable.h>
 #include <asm/thread_info.h>
 #include <asm/rtas.h>
+#include <asm/vdso_datapage.h>
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #include <asm/lppaca.h>
 #include <asm/iseries/hv_lp_event.h>
 #include <asm/cache.h>
-#include <asm/systemcfg.h>
 #include <asm/compat.h>
 #endif
 
@@ -251,25 +251,42 @@ int main(void)
 
 	DEFINE(TASK_SIZE, TASK_SIZE);
 	DEFINE(NUM_USER_SEGMENTS, TASK_SIZE>>28);
-#else /* CONFIG_PPC64 */
-	/* systemcfg offsets for use by vdso */
-	DEFINE(CFG_TB_ORIG_STAMP, offsetof(struct systemcfg, tb_orig_stamp));
-	DEFINE(CFG_TB_TICKS_PER_SEC, offsetof(struct systemcfg, tb_ticks_per_sec));
-	DEFINE(CFG_TB_TO_XS, offsetof(struct systemcfg, tb_to_xs));
-	DEFINE(CFG_STAMP_XSEC, offsetof(struct systemcfg, stamp_xsec));
-	DEFINE(CFG_TB_UPDATE_COUNT, offsetof(struct systemcfg, tb_update_count));
-	DEFINE(CFG_TZ_MINUTEWEST, offsetof(struct systemcfg, tz_minuteswest));
-	DEFINE(CFG_TZ_DSTTIME, offsetof(struct systemcfg, tz_dsttime));
-	DEFINE(CFG_SYSCALL_MAP32, offsetof(struct systemcfg, syscall_map_32));
-	DEFINE(CFG_SYSCALL_MAP64, offsetof(struct systemcfg, syscall_map_64));
+#endif /* ! CONFIG_PPC64 */
 
-	/* timeval/timezone offsets for use by vdso */
+	/* datapage offsets for use by vdso */
+	DEFINE(CFG_TB_ORIG_STAMP, offsetof(struct vdso_data, tb_orig_stamp));
+	DEFINE(CFG_TB_TICKS_PER_SEC, offsetof(struct vdso_data, tb_ticks_per_sec));
+	DEFINE(CFG_TB_TO_XS, offsetof(struct vdso_data, tb_to_xs));
+	DEFINE(CFG_STAMP_XSEC, offsetof(struct vdso_data, stamp_xsec));
+	DEFINE(CFG_TB_UPDATE_COUNT, offsetof(struct vdso_data, tb_update_count));
+	DEFINE(CFG_TZ_MINUTEWEST, offsetof(struct vdso_data, tz_minuteswest));
+	DEFINE(CFG_TZ_DSTTIME, offsetof(struct vdso_data, tz_dsttime));
+	DEFINE(CFG_SYSCALL_MAP32, offsetof(struct vdso_data, syscall_map_32));
+	DEFINE(WTOM_CLOCK_SEC, offsetof(struct vdso_data, wtom_clock_sec));
+	DEFINE(WTOM_CLOCK_NSEC, offsetof(struct vdso_data, wtom_clock_nsec));
+#ifdef CONFIG_PPC64
+	DEFINE(CFG_SYSCALL_MAP64, offsetof(struct vdso_data, syscall_map_64));
 	DEFINE(TVAL64_TV_SEC, offsetof(struct timeval, tv_sec));
 	DEFINE(TVAL64_TV_USEC, offsetof(struct timeval, tv_usec));
 	DEFINE(TVAL32_TV_SEC, offsetof(struct compat_timeval, tv_sec));
 	DEFINE(TVAL32_TV_USEC, offsetof(struct compat_timeval, tv_usec));
+	DEFINE(TSPC32_TV_SEC, offsetof(struct compat_timespec, tv_sec));
+	DEFINE(TSPC32_TV_NSEC, offsetof(struct compat_timespec, tv_nsec));
+#else
+	DEFINE(TVAL32_TV_SEC, offsetof(struct timeval, tv_sec));
+	DEFINE(TVAL32_TV_USEC, offsetof(struct timeval, tv_usec));
+	DEFINE(TSPEC32_TV_SEC, offsetof(struct timespec, tv_sec));
+	DEFINE(TSPEC32_TV_NSEC, offsetof(struct timespec, tv_nsec));
+#endif
+	/* timeval/timezone offsets for use by vdso */
 	DEFINE(TZONE_TZ_MINWEST, offsetof(struct timezone, tz_minuteswest));
 	DEFINE(TZONE_TZ_DSTTIME, offsetof(struct timezone, tz_dsttime));
-#endif /* CONFIG_PPC64 */
+
+	/* Other bits used by the vdso */
+	DEFINE(CLOCK_REALTIME, CLOCK_REALTIME);
+	DEFINE(CLOCK_MONOTONIC, CLOCK_MONOTONIC);
+	DEFINE(NSEC_PER_SEC, NSEC_PER_SEC);
+	DEFINE(CLOCK_REALTIME_RES, TICK_NSEC);
+
 	return 0;
 }

commit 799d6046d3fb557006e6d7c9767fdb96479b0e0a
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Nov 10 13:37:51 2005 +1100

    [PATCH] powerpc: merge code values for identifying platforms
    
    This patch merges platform codes.  systemcfg->platform is no longer used,
    systemcfg use in general is deprecated as much as possible (and renamed
    _systemcfg before it gets completely moved elsewhere in a future patch),
    _machine is now used on ppc64 along as ppc32.  Platform codes aren't gone
    yet but we are getting a step closer. A bunch of asm code in head[_64].S
    is also turned into C code.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b75757251994..8793102711a8 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -106,7 +106,6 @@ int main(void)
 	DEFINE(ICACHEL1LINESIZE, offsetof(struct ppc64_caches, iline_size));
 	DEFINE(ICACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_iline_size));
 	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
-	DEFINE(PLATFORM, offsetof(struct systemcfg, platform));
 	DEFINE(PLATFORM_LPAR, PLATFORM_LPAR);
 
 	/* paca */

commit 3c726f8dee6f55e96475574e9f645327e461884c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Nov 7 11:06:55 2005 +1100

    [PATCH] ppc64: support 64k pages
    
    Adds a new CONFIG_PPC_64K_PAGES which, when enabled, changes the kernel
    base page size to 64K.  The resulting kernel still boots on any
    hardware.  On current machines with 4K pages support only, the kernel
    will maintain 16 "subpages" for each 64K page transparently.
    
    Note that while real 64K capable HW has been tested, the current patch
    will not enable it yet as such hardware is not released yet, and I'm
    still verifying with the firmware architects the proper to get the
    information from the newer hypervisors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index bc5a3689cc05..b75757251994 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -125,6 +125,9 @@ int main(void)
 	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
 	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
 	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
+#ifdef CONFIG_PPC_64K_PAGES
+	DEFINE(PACAPGDIR, offsetof(struct paca_struct, pgdir));
+#endif
 #ifdef CONFIG_HUGETLB_PAGE
 	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
 	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));

commit e45423eac2e191a6cfdacdf61cb931976d73cc0b
Author: Kelly Daly <kelly@au.ibm.com>
Date:   Wed Nov 2 12:08:31 2005 +1100

    merge filename and modify references to iseries/hv_lp_event.h
    
    Signed-off-by: Kelly Daly <kelly@au.ibm.com>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 330cd783206f..bc5a3689cc05 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -40,7 +40,7 @@
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #include <asm/lppaca.h>
-#include <asm/iSeries/HvLpEvent.h>
+#include <asm/iseries/hv_lp_event.h>
 #include <asm/cache.h>
 #include <asm/systemcfg.h>
 #include <asm/compat.h>

commit d73e0c99f5c45e7b86d38725a4ff49f6746f5353
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Oct 28 22:45:25 2005 +1000

    powerpc: Rename asm offset TRAP to _TRAP for 32-bit
    
    ... for consistency with 64-bit.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 710336a9f0fd..330cd783206f 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -209,6 +209,7 @@ int main(void)
 	DEFINE(_DSISR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
 	DEFINE(ORIG_GPR3, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, orig_gpr3));
 	DEFINE(RESULT, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, result));
+	DEFINE(_TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
 #ifndef CONFIG_PPC64
 	DEFINE(_MQ, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, mq));
 	/*
@@ -219,9 +220,7 @@ int main(void)
 	 */
 	DEFINE(_DEAR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dar));
 	DEFINE(_ESR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
-	DEFINE(TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
 #else /* CONFIG_PPC64 */
-	DEFINE(_TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
 	DEFINE(SOFTE, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, softe));
 
 	/* These _only_ to be used with {PROM,RTAS}_FRAME_SIZE!!! */

commit 033ef338b6e007dc081c6282a4f2a9dd761f8cd2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Oct 26 17:05:24 2005 +1000

    powerpc: Merge rtas.c into arch/powerpc/kernel
    
    This splits arch/ppc64/kernel/rtas.c into arch/powerpc/kernel/rtas.c,
    which contains generic RTAS functions useful on any CHRP platform,
    and arch/powerpc/platforms/pseries/rtas-fw.[ch], which contain
    some pSeries-specific firmware flashing bits.  The parts of rtas.c
    that are to do with pSeries-specific error logging are protected
    by a new CONFIG_RTAS_ERROR_LOGGING symbol.  The inclusion of rtas.o
    is controlled by the CONFIG_PPC_RTAS symbol, and the relevant
    platforms select that.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 1c83abd9f37c..710336a9f0fd 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -36,11 +36,11 @@
 #include <asm/processor.h>
 #include <asm/cputable.h>
 #include <asm/thread_info.h>
+#include <asm/rtas.h>
 #ifdef CONFIG_PPC64
 #include <asm/paca.h>
 #include <asm/lppaca.h>
 #include <asm/iSeries/HvLpEvent.h>
-#include <asm/rtas.h>
 #include <asm/cache.h>
 #include <asm/systemcfg.h>
 #include <asm/compat.h>
@@ -97,7 +97,7 @@ int main(void)
 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
 	DEFINE(TI_EXECDOMAIN, offsetof(struct thread_info, exec_domain));
 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
-#endif /* CONFIG_PPC64 */
+#endif /* CONFIG_PPC32 */
 
 #ifdef CONFIG_PPC64
 	DEFINE(DCACHEL1LINESIZE, offsetof(struct ppc64_caches, dline_size));
@@ -142,11 +142,11 @@ int main(void)
 	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
 	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
 	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
+#endif /* CONFIG_PPC64 */
 
 	/* RTAS */
 	DEFINE(RTASBASE, offsetof(struct rtas_t, base));
 	DEFINE(RTASENTRY, offsetof(struct rtas_t, entry));
-#endif /* CONFIG_PPC64 */
 
 	/* Interrupt register frame */
 	DEFINE(STACK_FRAME_OVERHEAD, STACK_FRAME_OVERHEAD);

commit 6cb7bfebb145af5ea1d052512a2ae7ff07a47202
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Fri Oct 21 15:45:50 2005 +1000

    [PATCH] powerpc: Merge thread_info.h
    
    Merge ppc32 and ppc64 versions of thread_info.h.  They were pretty
    similar already, the chief changes are:
    
            - Instead of inline asm to implement current_thread_info(),
    which needs to be different for ppc32 and ppc64, we use C with an
    asm("r1") register variable.  gcc turns it into the same asm as we
    used to have for both platforms.
            - We replace ppc32's 'local_flags' with the ppc64
    'syscall_noerror' field.  The noerror flag was in fact the only thing
    in the local_flags field anyway, so the ppc64 approach is simpler, and
    means we only need a load-immediate/store instead of load/mask/store
    when clearing the flag.
            - In readiness for 64k pages, when THREAD_SIZE will be less
    than a page, ppc64 used kmalloc() rather than get_free_pages() to
    allocate the kernel stack.  With this patch we do the same for ppc32,
    since there's no strong reason not to.
            - For ppc64, we no longer export THREAD_SHIFT and THREAD_SIZE
    via asm-offsets, thread_info.h can now be safely included in asm, as
    on ppc32.
    
    Built and booted on G4 Powerbook (ARCH=ppc and ARCH=powerpc) and
    Power5 (ARCH=ppc64 and ARCH=powerpc).
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index b0d6a7cd85e9..1c83abd9f37c 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -56,8 +56,6 @@ int main(void)
 	DEFINE(THREAD, offsetof(struct task_struct, thread));
 	DEFINE(MM, offsetof(struct task_struct, mm));
 #ifdef CONFIG_PPC64
-	DEFINE(THREAD_SHIFT, THREAD_SHIFT);
-	DEFINE(THREAD_SIZE, THREAD_SIZE);
 	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
 #else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, thread_info));
@@ -94,12 +92,10 @@ int main(void)
 
 	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
 	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
-#ifdef CONFIG_PPC64
 	DEFINE(TI_SC_NOERR, offsetof(struct thread_info, syscall_noerror));
-#else
+#ifdef CONFIG_PPC32
 	DEFINE(TI_TASK, offsetof(struct thread_info, task));
 	DEFINE(TI_EXECDOMAIN, offsetof(struct thread_info, exec_domain));
-	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
 	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
 #endif /* CONFIG_PPC64 */
 

commit fd582ec88eb8d2d907876603e4ecebe6eab330d9
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 11 22:08:12 2005 +1000

    ppc: Various minor compile fixes
    
    This fixes up a variety of minor problems in compiling with ARCH=ppc
    arising from using the merged versions of various header files.
    A lot of the changes are just adding #include <asm/machdep.h> to
    files that use ppc_md or smp_ops_t.
    
    This also arranges for us to use semaphore.c, vecemu.c, vector.S and
    fpu.S from arch/powerpc/kernel when compiling with ARCH=ppc.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index ddf0c81e1958..b0d6a7cd85e9 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -252,6 +252,7 @@ int main(void)
 	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));
 	DEFINE(pbe_next, offsetof(struct pbe, next));
 
+	DEFINE(TASK_SIZE, TASK_SIZE);
 	DEFINE(NUM_USER_SEGMENTS, TASK_SIZE>>28);
 #else /* CONFIG_PPC64 */
 	/* systemcfg offsets for use by vdso */

commit 40ef8cbc6d360e564573eb19582249c35d8ba330
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 10 22:50:37 2005 +1000

    powerpc: Get 64-bit configs to compile with ARCH=powerpc
    
    This is a bunch of mostly small fixes that are needed to get
    ARCH=powerpc to compile for 64-bit.  This adds setup_64.c from
    arch/ppc64/kernel/setup.c and locks.c from arch/ppc64/lib/locks.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 3a247c033e8b..ddf0c81e1958 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -111,6 +111,7 @@ int main(void)
 	DEFINE(ICACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_iline_size));
 	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
 	DEFINE(PLATFORM, offsetof(struct systemcfg, platform));
+	DEFINE(PLATFORM_LPAR, PLATFORM_LPAR);
 
 	/* paca */
 	DEFINE(PACA_SIZE, sizeof(struct paca_struct));

commit d1dead5c5f016ebadb4b87c2c9fa13dfc2c99bf0
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Thu Sep 29 00:35:31 2005 +1000

    powerpc: merge asm-offsets.c
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
index 16cf0b7ee2b7..3a247c033e8b 100644
--- a/arch/powerpc/kernel/asm-offsets.c
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -20,17 +20,20 @@
 #include <linux/errno.h>
 #include <linux/string.h>
 #include <linux/types.h>
-#include <linux/ptrace.h>
-#include <linux/suspend.h>
 #include <linux/mman.h>
 #include <linux/mm.h>
+#ifdef CONFIG_PPC64
 #include <linux/time.h>
 #include <linux/hardirq.h>
+#else
+#include <linux/ptrace.h>
+#include <linux/suspend.h>
+#endif
+
 #include <asm/io.h>
 #include <asm/page.h>
 #include <asm/pgtable.h>
 #include <asm/processor.h>
-
 #include <asm/cputable.h>
 #include <asm/thread_info.h>
 #ifdef CONFIG_PPC64
@@ -50,63 +53,117 @@
 
 int main(void)
 {
-	/* thread struct on stack */
-	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
-	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
-	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
-#ifdef CONFIG_PPC32
-	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
-#endif
+	DEFINE(THREAD, offsetof(struct task_struct, thread));
+	DEFINE(MM, offsetof(struct task_struct, mm));
 #ifdef CONFIG_PPC64
-	DEFINE(TI_SC_NOERR, offsetof(struct thread_info, syscall_noerror));
 	DEFINE(THREAD_SHIFT, THREAD_SHIFT);
-#endif
 	DEFINE(THREAD_SIZE, THREAD_SIZE);
-
-	/* task_struct->thread */
-	DEFINE(THREAD, offsetof(struct task_struct, thread));
+	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
+#else
 	DEFINE(THREAD_INFO, offsetof(struct task_struct, thread_info));
-	DEFINE(MM, offsetof(struct task_struct, mm));
 	DEFINE(PTRACE, offsetof(struct task_struct, ptrace));
+#endif /* CONFIG_PPC64 */
+
 	DEFINE(KSP, offsetof(struct thread_struct, ksp));
-	DEFINE(PGDIR, offsetof(struct thread_struct, pgdir));
-	DEFINE(LAST_SYSCALL, offsetof(struct thread_struct, last_syscall));
 	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
 	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
 	DEFINE(THREAD_FPR0, offsetof(struct thread_struct, fpr[0]));
 	DEFINE(THREAD_FPSCR, offsetof(struct thread_struct, fpscr));
-#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
-	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
-	DEFINE(PT_PTRACED, PT_PTRACED);
-#endif
-#ifdef CONFIG_PPC64
-	DEFINE(KSP_VSID, offsetof(struct thread_struct, ksp_vsid));
-#endif
-
 #ifdef CONFIG_ALTIVEC
 	DEFINE(THREAD_VR0, offsetof(struct thread_struct, vr[0]));
 	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
 	DEFINE(THREAD_VSCR, offsetof(struct thread_struct, vscr));
 	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
 #endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_PPC64
+	DEFINE(KSP_VSID, offsetof(struct thread_struct, ksp_vsid));
+#else /* CONFIG_PPC64 */
+	DEFINE(PGDIR, offsetof(struct thread_struct, pgdir));
+	DEFINE(LAST_SYSCALL, offsetof(struct thread_struct, last_syscall));
+#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
+	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
+	DEFINE(PT_PTRACED, PT_PTRACED);
+#endif
 #ifdef CONFIG_SPE
 	DEFINE(THREAD_EVR0, offsetof(struct thread_struct, evr[0]));
 	DEFINE(THREAD_ACC, offsetof(struct thread_struct, acc));
 	DEFINE(THREAD_SPEFSCR, offsetof(struct thread_struct, spefscr));
 	DEFINE(THREAD_USED_SPE, offsetof(struct thread_struct, used_spe));
 #endif /* CONFIG_SPE */
+#endif /* CONFIG_PPC64 */
+
+	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
+	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
+#ifdef CONFIG_PPC64
+	DEFINE(TI_SC_NOERR, offsetof(struct thread_info, syscall_noerror));
+#else
+	DEFINE(TI_TASK, offsetof(struct thread_info, task));
+	DEFINE(TI_EXECDOMAIN, offsetof(struct thread_info, exec_domain));
+	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
+	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
+#endif /* CONFIG_PPC64 */
+
+#ifdef CONFIG_PPC64
+	DEFINE(DCACHEL1LINESIZE, offsetof(struct ppc64_caches, dline_size));
+	DEFINE(DCACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_dline_size));
+	DEFINE(DCACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, dlines_per_page));
+	DEFINE(ICACHEL1LINESIZE, offsetof(struct ppc64_caches, iline_size));
+	DEFINE(ICACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_iline_size));
+	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
+	DEFINE(PLATFORM, offsetof(struct systemcfg, platform));
+
+	/* paca */
+	DEFINE(PACA_SIZE, sizeof(struct paca_struct));
+	DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));
+	DEFINE(PACAPROCSTART, offsetof(struct paca_struct, cpu_start));
+	DEFINE(PACAKSAVE, offsetof(struct paca_struct, kstack));
+	DEFINE(PACACURRENT, offsetof(struct paca_struct, __current));
+	DEFINE(PACASAVEDMSR, offsetof(struct paca_struct, saved_msr));
+	DEFINE(PACASTABREAL, offsetof(struct paca_struct, stab_real));
+	DEFINE(PACASTABVIRT, offsetof(struct paca_struct, stab_addr));
+	DEFINE(PACASTABRR, offsetof(struct paca_struct, stab_rr));
+	DEFINE(PACAR1, offsetof(struct paca_struct, saved_r1));
+	DEFINE(PACATOC, offsetof(struct paca_struct, kernel_toc));
+	DEFINE(PACAPROCENABLED, offsetof(struct paca_struct, proc_enabled));
+	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
+	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
+	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
+#ifdef CONFIG_HUGETLB_PAGE
+	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
+	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));
+#endif /* CONFIG_HUGETLB_PAGE */
+	DEFINE(PACADEFAULTDECR, offsetof(struct paca_struct, default_decr));
+	DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
+	DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
+	DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));
+	DEFINE(PACA_EXDSI, offsetof(struct paca_struct, exdsi));
+	DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
+	DEFINE(PACALPPACA, offsetof(struct paca_struct, lppaca));
+	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
+
+	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
+	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
+	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
+	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
+
+	/* RTAS */
+	DEFINE(RTASBASE, offsetof(struct rtas_t, base));
+	DEFINE(RTASENTRY, offsetof(struct rtas_t, entry));
+#endif /* CONFIG_PPC64 */
+
 	/* Interrupt register frame */
 	DEFINE(STACK_FRAME_OVERHEAD, STACK_FRAME_OVERHEAD);
 #ifndef CONFIG_PPC64
 	DEFINE(INT_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
-#else
+#else /* CONFIG_PPC64 */
 	DEFINE(SWITCH_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
-
 	/* 288 = # of volatile regs, int & fp, for leaf routines */
 	/* which do not stack a frame.  See the PPC64 ABI.       */
 	DEFINE(INT_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 288);
-#endif
-	/* in fact we only use gpr0 - gpr9 and gpr20 - gpr23 */
+	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
+	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
+	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
+#endif /* CONFIG_PPC64 */
 	DEFINE(GPR0, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[0]));
 	DEFINE(GPR1, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[1]));
 	DEFINE(GPR2, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[2]));
@@ -121,6 +178,7 @@ int main(void)
 	DEFINE(GPR11, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[11]));
 	DEFINE(GPR12, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[12]));
 	DEFINE(GPR13, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[13]));
+#ifndef CONFIG_PPC64
 	DEFINE(GPR14, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[14]));
 	DEFINE(GPR15, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[15]));
 	DEFINE(GPR16, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[16]));
@@ -139,6 +197,7 @@ int main(void)
 	DEFINE(GPR29, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[29]));
 	DEFINE(GPR30, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[30]));
 	DEFINE(GPR31, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[31]));
+#endif /* CONFIG_PPC64 */
 	/*
 	 * Note: these symbols include _ because they overlap with special
 	 * register names
@@ -148,23 +207,37 @@ int main(void)
 	DEFINE(_CTR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, ctr));
 	DEFINE(_LINK, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, link));
 	DEFINE(_CCR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, ccr));
-	DEFINE(_MQ, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, mq));
 	DEFINE(_XER, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, xer));
 	DEFINE(_DAR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dar));
 	DEFINE(_DSISR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
-	/* The PowerPC 400-class & Book-E processors have neither the DAR nor the DSISR
-	 * SPRs. Hence, we overload them to hold the similar DEAR and ESR SPRs
-	 * for such processors.  For critical interrupts we use them to
-	 * hold SRR0 and SRR1.
+	DEFINE(ORIG_GPR3, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, orig_gpr3));
+	DEFINE(RESULT, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, result));
+#ifndef CONFIG_PPC64
+	DEFINE(_MQ, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, mq));
+	/*
+	 * The PowerPC 400-class & Book-E processors have neither the DAR
+	 * nor the DSISR SPRs. Hence, we overload them to hold the similar
+	 * DEAR and ESR SPRs for such processors.  For critical interrupts
+	 * we use them to hold SRR0 and SRR1.
 	 */
 	DEFINE(_DEAR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dar));
 	DEFINE(_ESR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
-	DEFINE(ORIG_GPR3, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, orig_gpr3));
-	DEFINE(RESULT, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, result));
 	DEFINE(TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
+#else /* CONFIG_PPC64 */
+	DEFINE(_TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
+	DEFINE(SOFTE, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, softe));
+
+	/* These _only_ to be used with {PROM,RTAS}_FRAME_SIZE!!! */
+	DEFINE(_SRR0, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs));
+	DEFINE(_SRR1, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs)+8);
+#endif /* CONFIG_PPC64 */
+
 	DEFINE(CLONE_VM, CLONE_VM);
 	DEFINE(CLONE_UNTRACED, CLONE_UNTRACED);
+
+#ifndef CONFIG_PPC64
 	DEFINE(MM_PGD, offsetof(struct mm_struct, pgd));
+#endif /* ! CONFIG_PPC64 */
 
 	/* About the CPU features table */
 	DEFINE(CPU_SPEC_ENTRY_SIZE, sizeof(struct cpu_spec));
@@ -173,66 +246,13 @@ int main(void)
 	DEFINE(CPU_SPEC_FEATURES, offsetof(struct cpu_spec, cpu_features));
 	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
 
-#ifdef CONFIG_PPC64
-	DEFINE(MM, offsetof(struct task_struct, mm));
-	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
-
-	DEFINE(DCACHEL1LINESIZE, offsetof(struct ppc64_caches, dline_size));
-	DEFINE(DCACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_dline_size));
-	DEFINE(DCACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, dlines_per_page));
-	DEFINE(ICACHEL1LINESIZE, offsetof(struct ppc64_caches, iline_size));
-	DEFINE(ICACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_iline_size));
-	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
-	DEFINE(PLATFORM, offsetof(struct systemcfg, platform));
-
-	/* paca */
-        DEFINE(PACA_SIZE, sizeof(struct paca_struct));
-        DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));
-        DEFINE(PACAPROCSTART, offsetof(struct paca_struct, cpu_start));
-        DEFINE(PACAKSAVE, offsetof(struct paca_struct, kstack));
-	DEFINE(PACACURRENT, offsetof(struct paca_struct, __current));
-        DEFINE(PACASAVEDMSR, offsetof(struct paca_struct, saved_msr));
-        DEFINE(PACASTABREAL, offsetof(struct paca_struct, stab_real));
-        DEFINE(PACASTABVIRT, offsetof(struct paca_struct, stab_addr));
-	DEFINE(PACASTABRR, offsetof(struct paca_struct, stab_rr));
-        DEFINE(PACAR1, offsetof(struct paca_struct, saved_r1));
-	DEFINE(PACATOC, offsetof(struct paca_struct, kernel_toc));
-	DEFINE(PACAPROCENABLED, offsetof(struct paca_struct, proc_enabled));
-	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
-	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
-	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
-#ifdef CONFIG_HUGETLB_PAGE
-	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
-	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));
-#endif /* CONFIG_HUGETLB_PAGE */
-	DEFINE(PACADEFAULTDECR, offsetof(struct paca_struct, default_decr));
-        DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
-        DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
-        DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));
-        DEFINE(PACA_EXDSI, offsetof(struct paca_struct, exdsi));
-        DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
-	DEFINE(PACALPPACA, offsetof(struct paca_struct, lppaca));
-	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
-	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
-	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
-	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
-	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
-
-	/* RTAS */
-	DEFINE(RTASBASE, offsetof(struct rtas_t, base));
-	DEFINE(RTASENTRY, offsetof(struct rtas_t, entry));
-
-	DEFINE(_TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
-	DEFINE(SOFTE, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, softe));
-
-	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
-	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
-	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
-
-	/* These _only_ to be used with {PROM,RTAS}_FRAME_SIZE!!! */
-	DEFINE(_SRR0, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs));
-	DEFINE(_SRR1, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs)+8);
+#ifndef CONFIG_PPC64
+	DEFINE(pbe_address, offsetof(struct pbe, address));
+	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));
+	DEFINE(pbe_next, offsetof(struct pbe, next));
 
+	DEFINE(NUM_USER_SEGMENTS, TASK_SIZE>>28);
+#else /* CONFIG_PPC64 */
 	/* systemcfg offsets for use by vdso */
 	DEFINE(CFG_TB_ORIG_STAMP, offsetof(struct systemcfg, tb_orig_stamp));
 	DEFINE(CFG_TB_TICKS_PER_SEC, offsetof(struct systemcfg, tb_ticks_per_sec));
@@ -251,12 +271,6 @@ int main(void)
 	DEFINE(TVAL32_TV_USEC, offsetof(struct compat_timeval, tv_usec));
 	DEFINE(TZONE_TZ_MINWEST, offsetof(struct timezone, tz_minuteswest));
 	DEFINE(TZONE_TZ_DSTTIME, offsetof(struct timezone, tz_dsttime));
-#endif
-
-	DEFINE(pbe_address, offsetof(struct pbe, address));
-	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));
-	DEFINE(pbe_next, offsetof(struct pbe, next));
-
-	DEFINE(NUM_USER_SEGMENTS, TASK_SIZE>>28);
+#endif /* CONFIG_PPC64 */
 	return 0;
 }

commit 14cf11af6cf608eb8c23e989ddb17a715ddce109
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Sep 26 16:04:21 2005 +1000

    powerpc: Merge enough to start building in arch/powerpc.
    
    This creates the directory structure under arch/powerpc and a bunch
    of Kconfig files.  It does a first-cut merge of arch/powerpc/mm,
    arch/powerpc/lib and arch/powerpc/platforms/powermac.  This is enough
    to build a 32-bit powermac kernel with ARCH=powerpc.
    
    For now we are getting some unmerged files from arch/ppc/kernel and
    arch/ppc/syslib, or arch/ppc64/kernel.  This makes some minor changes
    to files in those directories and files outside arch/powerpc.
    
    The boot directory is still not merged.  That's going to be interesting.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kernel/asm-offsets.c b/arch/powerpc/kernel/asm-offsets.c
new file mode 100644
index 000000000000..16cf0b7ee2b7
--- /dev/null
+++ b/arch/powerpc/kernel/asm-offsets.c
@@ -0,0 +1,262 @@
+/*
+ * This program is used to generate definitions needed by
+ * assembly language modules.
+ *
+ * We use the technique used in the OSF Mach kernel code:
+ * generate asm statements containing #defines,
+ * compile this file to assembler, and then extract the
+ * #defines from the assembly-language output.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/config.h>
+#include <linux/signal.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/ptrace.h>
+#include <linux/suspend.h>
+#include <linux/mman.h>
+#include <linux/mm.h>
+#include <linux/time.h>
+#include <linux/hardirq.h>
+#include <asm/io.h>
+#include <asm/page.h>
+#include <asm/pgtable.h>
+#include <asm/processor.h>
+
+#include <asm/cputable.h>
+#include <asm/thread_info.h>
+#ifdef CONFIG_PPC64
+#include <asm/paca.h>
+#include <asm/lppaca.h>
+#include <asm/iSeries/HvLpEvent.h>
+#include <asm/rtas.h>
+#include <asm/cache.h>
+#include <asm/systemcfg.h>
+#include <asm/compat.h>
+#endif
+
+#define DEFINE(sym, val) \
+	asm volatile("\n->" #sym " %0 " #val : : "i" (val))
+
+#define BLANK() asm volatile("\n->" : : )
+
+int main(void)
+{
+	/* thread struct on stack */
+	DEFINE(TI_FLAGS, offsetof(struct thread_info, flags));
+	DEFINE(TI_PREEMPT, offsetof(struct thread_info, preempt_count));
+	DEFINE(TI_CPU, offsetof(struct thread_info, cpu));
+#ifdef CONFIG_PPC32
+	DEFINE(TI_LOCAL_FLAGS, offsetof(struct thread_info, local_flags));
+#endif
+#ifdef CONFIG_PPC64
+	DEFINE(TI_SC_NOERR, offsetof(struct thread_info, syscall_noerror));
+	DEFINE(THREAD_SHIFT, THREAD_SHIFT);
+#endif
+	DEFINE(THREAD_SIZE, THREAD_SIZE);
+
+	/* task_struct->thread */
+	DEFINE(THREAD, offsetof(struct task_struct, thread));
+	DEFINE(THREAD_INFO, offsetof(struct task_struct, thread_info));
+	DEFINE(MM, offsetof(struct task_struct, mm));
+	DEFINE(PTRACE, offsetof(struct task_struct, ptrace));
+	DEFINE(KSP, offsetof(struct thread_struct, ksp));
+	DEFINE(PGDIR, offsetof(struct thread_struct, pgdir));
+	DEFINE(LAST_SYSCALL, offsetof(struct thread_struct, last_syscall));
+	DEFINE(PT_REGS, offsetof(struct thread_struct, regs));
+	DEFINE(THREAD_FPEXC_MODE, offsetof(struct thread_struct, fpexc_mode));
+	DEFINE(THREAD_FPR0, offsetof(struct thread_struct, fpr[0]));
+	DEFINE(THREAD_FPSCR, offsetof(struct thread_struct, fpscr));
+#if defined(CONFIG_4xx) || defined(CONFIG_BOOKE)
+	DEFINE(THREAD_DBCR0, offsetof(struct thread_struct, dbcr0));
+	DEFINE(PT_PTRACED, PT_PTRACED);
+#endif
+#ifdef CONFIG_PPC64
+	DEFINE(KSP_VSID, offsetof(struct thread_struct, ksp_vsid));
+#endif
+
+#ifdef CONFIG_ALTIVEC
+	DEFINE(THREAD_VR0, offsetof(struct thread_struct, vr[0]));
+	DEFINE(THREAD_VRSAVE, offsetof(struct thread_struct, vrsave));
+	DEFINE(THREAD_VSCR, offsetof(struct thread_struct, vscr));
+	DEFINE(THREAD_USED_VR, offsetof(struct thread_struct, used_vr));
+#endif /* CONFIG_ALTIVEC */
+#ifdef CONFIG_SPE
+	DEFINE(THREAD_EVR0, offsetof(struct thread_struct, evr[0]));
+	DEFINE(THREAD_ACC, offsetof(struct thread_struct, acc));
+	DEFINE(THREAD_SPEFSCR, offsetof(struct thread_struct, spefscr));
+	DEFINE(THREAD_USED_SPE, offsetof(struct thread_struct, used_spe));
+#endif /* CONFIG_SPE */
+	/* Interrupt register frame */
+	DEFINE(STACK_FRAME_OVERHEAD, STACK_FRAME_OVERHEAD);
+#ifndef CONFIG_PPC64
+	DEFINE(INT_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
+#else
+	DEFINE(SWITCH_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs));
+
+	/* 288 = # of volatile regs, int & fp, for leaf routines */
+	/* which do not stack a frame.  See the PPC64 ABI.       */
+	DEFINE(INT_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 288);
+#endif
+	/* in fact we only use gpr0 - gpr9 and gpr20 - gpr23 */
+	DEFINE(GPR0, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[0]));
+	DEFINE(GPR1, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[1]));
+	DEFINE(GPR2, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[2]));
+	DEFINE(GPR3, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[3]));
+	DEFINE(GPR4, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[4]));
+	DEFINE(GPR5, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[5]));
+	DEFINE(GPR6, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[6]));
+	DEFINE(GPR7, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[7]));
+	DEFINE(GPR8, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[8]));
+	DEFINE(GPR9, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[9]));
+	DEFINE(GPR10, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[10]));
+	DEFINE(GPR11, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[11]));
+	DEFINE(GPR12, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[12]));
+	DEFINE(GPR13, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[13]));
+	DEFINE(GPR14, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[14]));
+	DEFINE(GPR15, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[15]));
+	DEFINE(GPR16, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[16]));
+	DEFINE(GPR17, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[17]));
+	DEFINE(GPR18, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[18]));
+	DEFINE(GPR19, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[19]));
+	DEFINE(GPR20, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[20]));
+	DEFINE(GPR21, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[21]));
+	DEFINE(GPR22, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[22]));
+	DEFINE(GPR23, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[23]));
+	DEFINE(GPR24, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[24]));
+	DEFINE(GPR25, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[25]));
+	DEFINE(GPR26, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[26]));
+	DEFINE(GPR27, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[27]));
+	DEFINE(GPR28, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[28]));
+	DEFINE(GPR29, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[29]));
+	DEFINE(GPR30, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[30]));
+	DEFINE(GPR31, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, gpr[31]));
+	/*
+	 * Note: these symbols include _ because they overlap with special
+	 * register names
+	 */
+	DEFINE(_NIP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, nip));
+	DEFINE(_MSR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, msr));
+	DEFINE(_CTR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, ctr));
+	DEFINE(_LINK, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, link));
+	DEFINE(_CCR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, ccr));
+	DEFINE(_MQ, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, mq));
+	DEFINE(_XER, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, xer));
+	DEFINE(_DAR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dar));
+	DEFINE(_DSISR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
+	/* The PowerPC 400-class & Book-E processors have neither the DAR nor the DSISR
+	 * SPRs. Hence, we overload them to hold the similar DEAR and ESR SPRs
+	 * for such processors.  For critical interrupts we use them to
+	 * hold SRR0 and SRR1.
+	 */
+	DEFINE(_DEAR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dar));
+	DEFINE(_ESR, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, dsisr));
+	DEFINE(ORIG_GPR3, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, orig_gpr3));
+	DEFINE(RESULT, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, result));
+	DEFINE(TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
+	DEFINE(CLONE_VM, CLONE_VM);
+	DEFINE(CLONE_UNTRACED, CLONE_UNTRACED);
+	DEFINE(MM_PGD, offsetof(struct mm_struct, pgd));
+
+	/* About the CPU features table */
+	DEFINE(CPU_SPEC_ENTRY_SIZE, sizeof(struct cpu_spec));
+	DEFINE(CPU_SPEC_PVR_MASK, offsetof(struct cpu_spec, pvr_mask));
+	DEFINE(CPU_SPEC_PVR_VALUE, offsetof(struct cpu_spec, pvr_value));
+	DEFINE(CPU_SPEC_FEATURES, offsetof(struct cpu_spec, cpu_features));
+	DEFINE(CPU_SPEC_SETUP, offsetof(struct cpu_spec, cpu_setup));
+
+#ifdef CONFIG_PPC64
+	DEFINE(MM, offsetof(struct task_struct, mm));
+	DEFINE(AUDITCONTEXT, offsetof(struct task_struct, audit_context));
+
+	DEFINE(DCACHEL1LINESIZE, offsetof(struct ppc64_caches, dline_size));
+	DEFINE(DCACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_dline_size));
+	DEFINE(DCACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, dlines_per_page));
+	DEFINE(ICACHEL1LINESIZE, offsetof(struct ppc64_caches, iline_size));
+	DEFINE(ICACHEL1LOGLINESIZE, offsetof(struct ppc64_caches, log_iline_size));
+	DEFINE(ICACHEL1LINESPERPAGE, offsetof(struct ppc64_caches, ilines_per_page));
+	DEFINE(PLATFORM, offsetof(struct systemcfg, platform));
+
+	/* paca */
+        DEFINE(PACA_SIZE, sizeof(struct paca_struct));
+        DEFINE(PACAPACAINDEX, offsetof(struct paca_struct, paca_index));
+        DEFINE(PACAPROCSTART, offsetof(struct paca_struct, cpu_start));
+        DEFINE(PACAKSAVE, offsetof(struct paca_struct, kstack));
+	DEFINE(PACACURRENT, offsetof(struct paca_struct, __current));
+        DEFINE(PACASAVEDMSR, offsetof(struct paca_struct, saved_msr));
+        DEFINE(PACASTABREAL, offsetof(struct paca_struct, stab_real));
+        DEFINE(PACASTABVIRT, offsetof(struct paca_struct, stab_addr));
+	DEFINE(PACASTABRR, offsetof(struct paca_struct, stab_rr));
+        DEFINE(PACAR1, offsetof(struct paca_struct, saved_r1));
+	DEFINE(PACATOC, offsetof(struct paca_struct, kernel_toc));
+	DEFINE(PACAPROCENABLED, offsetof(struct paca_struct, proc_enabled));
+	DEFINE(PACASLBCACHE, offsetof(struct paca_struct, slb_cache));
+	DEFINE(PACASLBCACHEPTR, offsetof(struct paca_struct, slb_cache_ptr));
+	DEFINE(PACACONTEXTID, offsetof(struct paca_struct, context.id));
+#ifdef CONFIG_HUGETLB_PAGE
+	DEFINE(PACALOWHTLBAREAS, offsetof(struct paca_struct, context.low_htlb_areas));
+	DEFINE(PACAHIGHHTLBAREAS, offsetof(struct paca_struct, context.high_htlb_areas));
+#endif /* CONFIG_HUGETLB_PAGE */
+	DEFINE(PACADEFAULTDECR, offsetof(struct paca_struct, default_decr));
+        DEFINE(PACA_EXGEN, offsetof(struct paca_struct, exgen));
+        DEFINE(PACA_EXMC, offsetof(struct paca_struct, exmc));
+        DEFINE(PACA_EXSLB, offsetof(struct paca_struct, exslb));
+        DEFINE(PACA_EXDSI, offsetof(struct paca_struct, exdsi));
+        DEFINE(PACAEMERGSP, offsetof(struct paca_struct, emergency_sp));
+	DEFINE(PACALPPACA, offsetof(struct paca_struct, lppaca));
+	DEFINE(PACAHWCPUID, offsetof(struct paca_struct, hw_cpu_id));
+	DEFINE(LPPACASRR0, offsetof(struct lppaca, saved_srr0));
+	DEFINE(LPPACASRR1, offsetof(struct lppaca, saved_srr1));
+	DEFINE(LPPACAANYINT, offsetof(struct lppaca, int_dword.any_int));
+	DEFINE(LPPACADECRINT, offsetof(struct lppaca, int_dword.fields.decr_int));
+
+	/* RTAS */
+	DEFINE(RTASBASE, offsetof(struct rtas_t, base));
+	DEFINE(RTASENTRY, offsetof(struct rtas_t, entry));
+
+	DEFINE(_TRAP, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, trap));
+	DEFINE(SOFTE, STACK_FRAME_OVERHEAD+offsetof(struct pt_regs, softe));
+
+	/* Create extra stack space for SRR0 and SRR1 when calling prom/rtas. */
+	DEFINE(PROM_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
+	DEFINE(RTAS_FRAME_SIZE, STACK_FRAME_OVERHEAD + sizeof(struct pt_regs) + 16);
+
+	/* These _only_ to be used with {PROM,RTAS}_FRAME_SIZE!!! */
+	DEFINE(_SRR0, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs));
+	DEFINE(_SRR1, STACK_FRAME_OVERHEAD+sizeof(struct pt_regs)+8);
+
+	/* systemcfg offsets for use by vdso */
+	DEFINE(CFG_TB_ORIG_STAMP, offsetof(struct systemcfg, tb_orig_stamp));
+	DEFINE(CFG_TB_TICKS_PER_SEC, offsetof(struct systemcfg, tb_ticks_per_sec));
+	DEFINE(CFG_TB_TO_XS, offsetof(struct systemcfg, tb_to_xs));
+	DEFINE(CFG_STAMP_XSEC, offsetof(struct systemcfg, stamp_xsec));
+	DEFINE(CFG_TB_UPDATE_COUNT, offsetof(struct systemcfg, tb_update_count));
+	DEFINE(CFG_TZ_MINUTEWEST, offsetof(struct systemcfg, tz_minuteswest));
+	DEFINE(CFG_TZ_DSTTIME, offsetof(struct systemcfg, tz_dsttime));
+	DEFINE(CFG_SYSCALL_MAP32, offsetof(struct systemcfg, syscall_map_32));
+	DEFINE(CFG_SYSCALL_MAP64, offsetof(struct systemcfg, syscall_map_64));
+
+	/* timeval/timezone offsets for use by vdso */
+	DEFINE(TVAL64_TV_SEC, offsetof(struct timeval, tv_sec));
+	DEFINE(TVAL64_TV_USEC, offsetof(struct timeval, tv_usec));
+	DEFINE(TVAL32_TV_SEC, offsetof(struct compat_timeval, tv_sec));
+	DEFINE(TVAL32_TV_USEC, offsetof(struct compat_timeval, tv_usec));
+	DEFINE(TZONE_TZ_MINWEST, offsetof(struct timezone, tz_minuteswest));
+	DEFINE(TZONE_TZ_DSTTIME, offsetof(struct timezone, tz_dsttime));
+#endif
+
+	DEFINE(pbe_address, offsetof(struct pbe, address));
+	DEFINE(pbe_orig_address, offsetof(struct pbe, orig_address));
+	DEFINE(pbe_next, offsetof(struct pbe, next));
+
+	DEFINE(NUM_USER_SEGMENTS, TASK_SIZE>>28);
+	return 0;
+}
