commit 8c99d34578628b50233210dae5fc9600eea20b8e
Author: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
Date:   Mon Apr 27 12:35:11 2020 +0800

    KVM: PPC: Clean up redundant 'kvm_run' parameters
    
    In the current kvm version, 'kvm_run' has been included in the 'kvm_vcpu'
    structure. For historical reasons, many kvm-related function parameters
    retain the 'kvm_run' and 'kvm_vcpu' parameters at the same time. This
    patch does a unified cleanup of these remaining redundant parameters.
    
    Signed-off-by: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 27ccff612903..dd7d141e33e8 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -279,7 +279,7 @@ int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvmppc_sanity_check);
 
-int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
+int kvmppc_emulate_mmio(struct kvm_vcpu *vcpu)
 {
 	enum emulation_result er;
 	int r;
@@ -295,7 +295,7 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 		r = RESUME_GUEST;
 		break;
 	case EMULATE_DO_MMIO:
-		run->exit_reason = KVM_EXIT_MMIO;
+		vcpu->run->exit_reason = KVM_EXIT_MMIO;
 		/* We must reload nonvolatiles because "update" load/store
 		 * instructions modify register state. */
 		/* Future optimization: only reload non-volatiles if they were
@@ -1107,9 +1107,9 @@ static inline u32 dp_to_sp(u64 fprd)
 #define dp_to_sp(x)	(x)
 #endif /* CONFIG_PPC_FPU */
 
-static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
-                                      struct kvm_run *run)
+static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu)
 {
+	struct kvm_run *run = vcpu->run;
 	u64 uninitialized_var(gpr);
 
 	if (run->mmio.len > sizeof(gpr)) {
@@ -1219,10 +1219,11 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 	}
 }
 
-static int __kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+static int __kvmppc_handle_load(struct kvm_vcpu *vcpu,
 				unsigned int rt, unsigned int bytes,
 				int is_default_endian, int sign_extend)
 {
+	struct kvm_run *run = vcpu->run;
 	int idx, ret;
 	bool host_swabbed;
 
@@ -1256,7 +1257,7 @@ static int __kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	srcu_read_unlock(&vcpu->kvm->srcu, idx);
 
 	if (!ret) {
-		kvmppc_complete_mmio_load(vcpu, run);
+		kvmppc_complete_mmio_load(vcpu);
 		vcpu->mmio_needed = 0;
 		return EMULATE_DONE;
 	}
@@ -1264,24 +1265,24 @@ static int __kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	return EMULATE_DO_MMIO;
 }
 
-int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+int kvmppc_handle_load(struct kvm_vcpu *vcpu,
 		       unsigned int rt, unsigned int bytes,
 		       int is_default_endian)
 {
-	return __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 0);
+	return __kvmppc_handle_load(vcpu, rt, bytes, is_default_endian, 0);
 }
 EXPORT_SYMBOL_GPL(kvmppc_handle_load);
 
 /* Same as above, but sign extends */
-int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
+int kvmppc_handle_loads(struct kvm_vcpu *vcpu,
 			unsigned int rt, unsigned int bytes,
 			int is_default_endian)
 {
-	return __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 1);
+	return __kvmppc_handle_load(vcpu, rt, bytes, is_default_endian, 1);
 }
 
 #ifdef CONFIG_VSX
-int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+int kvmppc_handle_vsx_load(struct kvm_vcpu *vcpu,
 			unsigned int rt, unsigned int bytes,
 			int is_default_endian, int mmio_sign_extend)
 {
@@ -1292,13 +1293,13 @@ int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		return EMULATE_FAIL;
 
 	while (vcpu->arch.mmio_vsx_copy_nums) {
-		emulated = __kvmppc_handle_load(run, vcpu, rt, bytes,
+		emulated = __kvmppc_handle_load(vcpu, rt, bytes,
 			is_default_endian, mmio_sign_extend);
 
 		if (emulated != EMULATE_DONE)
 			break;
 
-		vcpu->arch.paddr_accessed += run->mmio.len;
+		vcpu->arch.paddr_accessed += vcpu->run->mmio.len;
 
 		vcpu->arch.mmio_vsx_copy_nums--;
 		vcpu->arch.mmio_vsx_offset++;
@@ -1307,9 +1308,10 @@ int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 }
 #endif /* CONFIG_VSX */
 
-int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+int kvmppc_handle_store(struct kvm_vcpu *vcpu,
 			u64 val, unsigned int bytes, int is_default_endian)
 {
+	struct kvm_run *run = vcpu->run;
 	void *data = run->mmio.data;
 	int idx, ret;
 	bool host_swabbed;
@@ -1423,7 +1425,7 @@ static inline int kvmppc_get_vsr_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
 	return result;
 }
 
-int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+int kvmppc_handle_vsx_store(struct kvm_vcpu *vcpu,
 			int rs, unsigned int bytes, int is_default_endian)
 {
 	u64 val;
@@ -1439,13 +1441,13 @@ int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		if (kvmppc_get_vsr_data(vcpu, rs, &val) == -1)
 			return EMULATE_FAIL;
 
-		emulated = kvmppc_handle_store(run, vcpu,
+		emulated = kvmppc_handle_store(vcpu,
 			 val, bytes, is_default_endian);
 
 		if (emulated != EMULATE_DONE)
 			break;
 
-		vcpu->arch.paddr_accessed += run->mmio.len;
+		vcpu->arch.paddr_accessed += vcpu->run->mmio.len;
 
 		vcpu->arch.mmio_vsx_copy_nums--;
 		vcpu->arch.mmio_vsx_offset++;
@@ -1454,19 +1456,19 @@ int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	return emulated;
 }
 
-static int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,
-			struct kvm_run *run)
+static int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu)
 {
+	struct kvm_run *run = vcpu->run;
 	enum emulation_result emulated = EMULATE_FAIL;
 	int r;
 
 	vcpu->arch.paddr_accessed += run->mmio.len;
 
 	if (!vcpu->mmio_is_write) {
-		emulated = kvmppc_handle_vsx_load(run, vcpu, vcpu->arch.io_gpr,
+		emulated = kvmppc_handle_vsx_load(vcpu, vcpu->arch.io_gpr,
 			 run->mmio.len, 1, vcpu->arch.mmio_sign_extend);
 	} else {
-		emulated = kvmppc_handle_vsx_store(run, vcpu,
+		emulated = kvmppc_handle_vsx_store(vcpu,
 			 vcpu->arch.io_gpr, run->mmio.len, 1);
 	}
 
@@ -1490,7 +1492,7 @@ static int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_ALTIVEC
-int kvmppc_handle_vmx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+int kvmppc_handle_vmx_load(struct kvm_vcpu *vcpu,
 		unsigned int rt, unsigned int bytes, int is_default_endian)
 {
 	enum emulation_result emulated = EMULATE_DONE;
@@ -1499,13 +1501,13 @@ int kvmppc_handle_vmx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		return EMULATE_FAIL;
 
 	while (vcpu->arch.mmio_vmx_copy_nums) {
-		emulated = __kvmppc_handle_load(run, vcpu, rt, bytes,
+		emulated = __kvmppc_handle_load(vcpu, rt, bytes,
 				is_default_endian, 0);
 
 		if (emulated != EMULATE_DONE)
 			break;
 
-		vcpu->arch.paddr_accessed += run->mmio.len;
+		vcpu->arch.paddr_accessed += vcpu->run->mmio.len;
 		vcpu->arch.mmio_vmx_copy_nums--;
 		vcpu->arch.mmio_vmx_offset++;
 	}
@@ -1585,7 +1587,7 @@ int kvmppc_get_vmx_byte(struct kvm_vcpu *vcpu, int index, u64 *val)
 	return result;
 }
 
-int kvmppc_handle_vmx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+int kvmppc_handle_vmx_store(struct kvm_vcpu *vcpu,
 		unsigned int rs, unsigned int bytes, int is_default_endian)
 {
 	u64 val = 0;
@@ -1620,12 +1622,12 @@ int kvmppc_handle_vmx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			return EMULATE_FAIL;
 		}
 
-		emulated = kvmppc_handle_store(run, vcpu, val, bytes,
+		emulated = kvmppc_handle_store(vcpu, val, bytes,
 				is_default_endian);
 		if (emulated != EMULATE_DONE)
 			break;
 
-		vcpu->arch.paddr_accessed += run->mmio.len;
+		vcpu->arch.paddr_accessed += vcpu->run->mmio.len;
 		vcpu->arch.mmio_vmx_copy_nums--;
 		vcpu->arch.mmio_vmx_offset++;
 	}
@@ -1633,19 +1635,19 @@ int kvmppc_handle_vmx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	return emulated;
 }
 
-static int kvmppc_emulate_mmio_vmx_loadstore(struct kvm_vcpu *vcpu,
-		struct kvm_run *run)
+static int kvmppc_emulate_mmio_vmx_loadstore(struct kvm_vcpu *vcpu)
 {
+	struct kvm_run *run = vcpu->run;
 	enum emulation_result emulated = EMULATE_FAIL;
 	int r;
 
 	vcpu->arch.paddr_accessed += run->mmio.len;
 
 	if (!vcpu->mmio_is_write) {
-		emulated = kvmppc_handle_vmx_load(run, vcpu,
+		emulated = kvmppc_handle_vmx_load(vcpu,
 				vcpu->arch.io_gpr, run->mmio.len, 1);
 	} else {
-		emulated = kvmppc_handle_vmx_store(run, vcpu,
+		emulated = kvmppc_handle_vmx_store(vcpu,
 				vcpu->arch.io_gpr, run->mmio.len, 1);
 	}
 
@@ -1775,7 +1777,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 	if (vcpu->mmio_needed) {
 		vcpu->mmio_needed = 0;
 		if (!vcpu->mmio_is_write)
-			kvmppc_complete_mmio_load(vcpu, run);
+			kvmppc_complete_mmio_load(vcpu);
 #ifdef CONFIG_VSX
 		if (vcpu->arch.mmio_vsx_copy_nums > 0) {
 			vcpu->arch.mmio_vsx_copy_nums--;
@@ -1783,7 +1785,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		}
 
 		if (vcpu->arch.mmio_vsx_copy_nums > 0) {
-			r = kvmppc_emulate_mmio_vsx_loadstore(vcpu, run);
+			r = kvmppc_emulate_mmio_vsx_loadstore(vcpu);
 			if (r == RESUME_HOST) {
 				vcpu->mmio_needed = 1;
 				goto out;
@@ -1797,7 +1799,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 		}
 
 		if (vcpu->arch.mmio_vmx_copy_nums > 0) {
-			r = kvmppc_emulate_mmio_vmx_loadstore(vcpu, run);
+			r = kvmppc_emulate_mmio_vmx_loadstore(vcpu);
 			if (r == RESUME_HOST) {
 				vcpu->mmio_needed = 1;
 				goto out;
@@ -1830,7 +1832,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 	if (run->immediate_exit)
 		r = -EINTR;
 	else
-		r = kvmppc_vcpu_run(run, vcpu);
+		r = kvmppc_vcpu_run(vcpu);
 
 	kvm_sigset_deactivate(vcpu);
 

commit da4ad88cab5867ee240dfd0585e9d115a8cc47db
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Thu Apr 23 22:48:37 2020 -0700

    kvm: Replace vcpu->swait with rcuwait
    
    The use of any sort of waitqueue (simple or regular) for
    wait/waking vcpus has always been an overkill and semantically
    wrong. Because this is per-vcpu (which is blocked) there is
    only ever a single waiting vcpu, thus no need for any sort of
    queue.
    
    As such, make use of the rcuwait primitive, with the following
    considerations:
    
      - rcuwait already provides the proper barriers that serialize
      concurrent waiter and waker.
    
      - Task wakeup is done in rcu read critical region, with a
      stable task pointer.
    
      - Because there is no concurrency among waiters, we need
      not worry about rcuwait_wait_event() calls corrupting
      the wait->task. As a consequence, this saves the locking
      done in swait when modifying the queue. This also applies
      to per-vcore wait for powerpc kvm-hv.
    
    The x86 tscdeadline_latency test mentioned in 8577370fb0cb
    ("KVM: Use simple waitqueue for vcpu->wq") shows that, on avg,
    latency is reduced by around 15-20% with this change.
    
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: kvmarm@lists.cs.columbia.edu
    Cc: linux-mips@vger.kernel.org
    Reviewed-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Message-Id: <20200424054837.5138-6-dave@stgolabs.net>
    [Avoid extra logic changes. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 052614e9d468..27ccff612903 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -752,7 +752,7 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	if (err)
 		goto out_vcpu_uninit;
 
-	vcpu->arch.wqp = &vcpu->wq;
+	vcpu->arch.waitp = &vcpu->wait;
 	kvmppc_create_vcpu_debugfs(vcpu, vcpu->vcpu_id);
 	return 0;
 

commit 4aef2ec9022b217f74d0f4c9b84081f07cc223d9
Merge: 7c67f54661fc 37486135d3a7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 13 12:14:05 2020 -0400

    Merge branch 'kvm-amd-fixes' into HEAD

commit b9b2782cd55aaf5a65573df99376cf391941cb66
Author: Peter Xu <peterx@redhat.com>
Date:   Tue May 5 11:47:50 2020 -0400

    KVM: X86: Declare KVM_CAP_SET_GUEST_DEBUG properly
    
    KVM_CAP_SET_GUEST_DEBUG should be supported for x86 however it's not declared
    as supported.  My wild guess is that userspaces like QEMU are using "#ifdef
    KVM_CAP_SET_GUEST_DEBUG" to check for the capability instead, but that could be
    wrong because the compilation host may not be the runtime host.
    
    The userspace might still want to keep the old "#ifdef" though to not break the
    guest debug on old kernels.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Message-Id: <20200505154750.126300-1-peterx@redhat.com>
    [Do the same for PPC and s390. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e15166b0a16d..ad2f172c26a6 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -521,6 +521,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
 	case KVM_CAP_IMMEDIATE_EXIT:
+	case KVM_CAP_SET_GUEST_DEBUG:
 		r = 1;
 		break;
 	case KVM_CAP_PPC_GUEST_DEBUG_SSTEP:

commit 1b94f6f81007b4afaea3480ec018bc9236148961
Author: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
Date:   Thu Apr 16 13:10:57 2020 +0800

    KVM: Remove redundant argument to kvm_arch_vcpu_ioctl_run
    
    In earlier versions of kvm, 'kvm_run' was an independent structure
    and was not included in the vcpu structure. At present, 'kvm_run'
    is already included in the vcpu structure, so the parameter
    'kvm_run' is redundant.
    
    This patch simplifies the function definition, removes the extra
    'kvm_run' parameter, and extracts it from the 'kvm_vcpu' structure
    if necessary.
    
    Signed-off-by: Tianjia Zhang <tianjia.zhang@linux.alibaba.com>
    Message-Id: <20200416051057.26526-1-tianjia.zhang@linux.alibaba.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e15166b0a16d..7e24691e138a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1764,8 +1764,9 @@ int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 	return r;
 }
 
-int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu)
 {
+	struct kvm_run *run = vcpu->run;
 	int r;
 
 	vcpu_load(vcpu);

commit b990408537388e9174b642ad36cdef6c47c64d3a
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Sat Mar 21 13:25:55 2020 -0700

    KVM: Pass kvm_init()'s opaque param to additional arch funcs
    
    Pass @opaque to kvm_arch_hardware_setup() and
    kvm_arch_check_processor_compat() to allow architecture specific code to
    reference @opaque without having to stash it away in a temporary global
    variable.  This will enable x86 to separate its vendor specific callback
    ops, which are passed via @opaque, into "init" and "runtime" ops without
    having to stash away the "init" ops.
    
    No functional change intended.
    
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Tested-by: Cornelia Huck <cohuck@redhat.com> #s390
    Acked-by: Marc Zyngier <maz@kernel.org>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Message-Id: <20200321202603.19355-2-sean.j.christopherson@intel.com>
    Reviewed-by: Vitaly Kuznetsov <vkuznets@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index c48862d86adc..e15166b0a16d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -415,12 +415,12 @@ int kvm_arch_hardware_enable(void)
 	return 0;
 }
 
-int kvm_arch_hardware_setup(void)
+int kvm_arch_hardware_setup(void *opaque)
 {
 	return 0;
 }
 
-int kvm_arch_check_processor_compat(void)
+int kvm_arch_check_processor_compat(void *opaque)
 {
 	return kvmppc_core_check_processor_compat();
 }

commit 9a5788c615f52f6d7bf0b61986a632d4ec86791d
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Mar 19 15:29:55 2020 +1100

    KVM: PPC: Book3S HV: Add a capability for enabling secure guests
    
    At present, on Power systems with Protected Execution Facility
    hardware and an ultravisor, a KVM guest can transition to being a
    secure guest at will.  Userspace (QEMU) has no way of knowing
    whether a host system is capable of running secure guests.  This
    will present a problem in future when the ultravisor is capable of
    migrating secure guests from one host to another, because
    virtualization management software will have no way to ensure that
    secure guests only run in domains where all of the hosts can
    support secure guests.
    
    This adds a VM capability which has two functions: (a) userspace
    can query it to find out whether the host can support secure guests,
    and (b) userspace can enable it for a guest, which allows that
    guest to become a secure guest.  If userspace does not enable it,
    KVM will return an error when the ultravisor does the hypercall
    that indicates that the guest is starting to transition to a
    secure guest.  The ultravisor will then abort the transition and
    the guest will terminate.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Ram Pai <linuxram@us.ibm.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e229a81016d0..c48862d86adc 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -668,6 +668,12 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = !!(cur_cpu_spec->cpu_user_features2 & PPC_FEATURE2_HTM) ||
 		     (hv_enabled && cpu_has_feature(CPU_FTR_P9_TM_HV_ASSIST));
 		break;
+#endif
+#if defined(CONFIG_KVM_BOOK3S_HV_POSSIBLE)
+	case KVM_CAP_PPC_SECURE_GUEST:
+		r = hv_enabled && kvmppc_hv_ops->enable_svm &&
+			!kvmppc_hv_ops->enable_svm(NULL);
+		break;
 #endif
 	default:
 		r = 0;
@@ -2166,6 +2172,14 @@ int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 			break;
 		r = kvm->arch.kvm_ops->enable_nested(kvm);
 		break;
+#endif
+#if defined(CONFIG_KVM_BOOK3S_HV_POSSIBLE)
+	case KVM_CAP_PPC_SECURE_GUEST:
+		r = -EINVAL;
+		if (!is_kvmppc_hv_enabled(kvm) || !kvm->arch.kvm_ops->enable_svm)
+			break;
+		r = kvm->arch.kvm_ops->enable_svm(kvm);
+		break;
 #endif
 	default:
 		r = -EINVAL;

commit b2fa4f9088db497fb1b4c6b71e64045b3597ed1f
Author: Greg Kurz <groug@kaod.org>
Date:   Wed Mar 18 18:43:30 2020 +0100

    KVM: PPC: Book3S PR: Fix kernel crash with PR KVM
    
    With PR KVM, shutting down a VM causes the host kernel to crash:
    
    [  314.219284] BUG: Unable to handle kernel data access on read at 0xc00800000176c638
    [  314.219299] Faulting instruction address: 0xc008000000d4ddb0
    cpu 0x0: Vector: 300 (Data Access) at [c00000036da077a0]
        pc: c008000000d4ddb0: kvmppc_mmu_pte_flush_all+0x68/0xd0 [kvm_pr]
        lr: c008000000d4dd94: kvmppc_mmu_pte_flush_all+0x4c/0xd0 [kvm_pr]
        sp: c00000036da07a30
       msr: 900000010280b033
       dar: c00800000176c638
     dsisr: 40000000
      current = 0xc00000036d4c0000
      paca    = 0xc000000001a00000   irqmask: 0x03   irq_happened: 0x01
        pid   = 1992, comm = qemu-system-ppc
    Linux version 5.6.0-master-gku+ (greg@palmb) (gcc version 7.5.0 (Ubuntu 7.5.0-3ubuntu1~18.04)) #17 SMP Wed Mar 18 13:49:29 CET 2020
    enter ? for help
    [c00000036da07ab0] c008000000d4fbe0 kvmppc_mmu_destroy_pr+0x28/0x60 [kvm_pr]
    [c00000036da07ae0] c0080000009eab8c kvmppc_mmu_destroy+0x34/0x50 [kvm]
    [c00000036da07b00] c0080000009e50c0 kvm_arch_vcpu_destroy+0x108/0x140 [kvm]
    [c00000036da07b30] c0080000009d1b50 kvm_vcpu_destroy+0x28/0x80 [kvm]
    [c00000036da07b60] c0080000009e4434 kvm_arch_destroy_vm+0xbc/0x190 [kvm]
    [c00000036da07ba0] c0080000009d9c2c kvm_put_kvm+0x1d4/0x3f0 [kvm]
    [c00000036da07c00] c0080000009da760 kvm_vm_release+0x38/0x60 [kvm]
    [c00000036da07c30] c000000000420be0 __fput+0xe0/0x310
    [c00000036da07c90] c0000000001747a0 task_work_run+0x150/0x1c0
    [c00000036da07cf0] c00000000014896c do_exit+0x44c/0xd00
    [c00000036da07dc0] c0000000001492f4 do_group_exit+0x64/0xd0
    [c00000036da07e00] c000000000149384 sys_exit_group+0x24/0x30
    [c00000036da07e20] c00000000000b9d0 system_call+0x5c/0x68
    
    This is caused by a use-after-free in kvmppc_mmu_pte_flush_all()
    which dereferences vcpu->arch.book3s which was previously freed by
    kvmppc_core_vcpu_free_pr(). This happens because kvmppc_mmu_destroy()
    is called after kvmppc_core_vcpu_free() since commit ff030fdf5573
    ("KVM: PPC: Move kvm_vcpu_init() invocation to common code").
    
    The kvmppc_mmu_destroy() helper calls one of the following depending
    on the KVM backend:
    
    - kvmppc_mmu_destroy_hv() which does nothing (Book3s HV)
    
    - kvmppc_mmu_destroy_pr() which undoes the effects of
      kvmppc_mmu_init() (Book3s PR 32-bit)
    
    - kvmppc_mmu_destroy_pr() which undoes the effects of
      kvmppc_mmu_init() (Book3s PR 64-bit)
    
    - kvmppc_mmu_destroy_e500() which does nothing (BookE e500/e500mc)
    
    It turns out that this is only relevant to PR KVM actually. And both
    32 and 64 backends need vcpu->arch.book3s to be valid when calling
    kvmppc_mmu_destroy_pr(). So instead of calling kvmppc_mmu_destroy()
    from kvm_arch_vcpu_destroy(), call kvmppc_mmu_destroy_pr() at the
    beginning of kvmppc_core_vcpu_free_pr(). This is consistent with
    kvmppc_mmu_init() being the last call in kvmppc_core_vcpu_create_pr().
    
    For the same reason, if kvmppc_core_vcpu_create_pr() returns an
    error then this means that kvmppc_mmu_init() was either not called
    or failed, in which case kvmppc_mmu_destroy() should not be called.
    Drop the line in the error path of kvm_arch_vcpu_create().
    
    Fixes: ff030fdf5573 ("KVM: PPC: Move kvm_vcpu_init() invocation to common code")
    Signed-off-by: Greg Kurz <groug@kaod.org>
    Reviewed-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6729c13161f7..e229a81016d0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -750,7 +750,6 @@ int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 	return 0;
 
 out_vcpu_uninit:
-	kvmppc_mmu_destroy(vcpu);
 	kvmppc_subarch_vcpu_uninit(vcpu);
 	return err;
 }
@@ -783,7 +782,6 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 	kvmppc_core_vcpu_free(vcpu);
 
-	kvmppc_mmu_destroy(vcpu);
 	kvmppc_subarch_vcpu_uninit(vcpu);
 }
 

commit 8fc6ba0a205e9adfaf976077be976f1d4dcd45eb
Author: Joe Perches <joe@perches.com>
Date:   Tue Mar 10 21:51:30 2020 -0700

    KVM: PPC: Use fallthrough;
    
    Convert the various uses of fallthrough comments to fallthrough;
    
    Done via script
    Link: https://lore.kernel.org/lkml/b56602fcf79f849e733e7b521bb0e17895d390fa.1582230379.git.joe.com/
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 62ee66d5eb6f..6729c13161f7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -524,7 +524,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = 1;
 		break;
 	case KVM_CAP_PPC_GUEST_DEBUG_SSTEP:
-		/* fall through */
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:

commit 4d395762599dbab1eb29d9011d5b75ca3cc4f70a
Author: Peter Xu <peterx@redhat.com>
Date:   Fri Feb 28 13:30:20 2020 -0500

    KVM: Remove unnecessary asm/kvm_host.h includes
    
    Remove includes of asm/kvm_host.h from files that already include
    linux/kvm_host.h to make it more obvious that there is no ordering issue
    between the two headers.  linux/kvm_host.h includes asm/kvm_host.h to
    pick up architecture specific settings, and this will never change, i.e.
    including asm/kvm_host.h after linux/kvm_host.h may seem problematic,
    but in practice is simply redundant.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 838cdcd2db12..62ee66d5eb6f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -32,7 +32,6 @@
 #include <asm/plpar_wrappers.h>
 #endif
 #include <asm/ultravisor.h>
-#include <asm/kvm_host.h>
 
 #include "timing.h"
 #include "irq.h"

commit e96c81ee89d80e1a0fe50a0e9be40c1b77e14aaa
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:27 2020 -0800

    KVM: Simplify kvm_free_memslot() and all its descendents
    
    Now that all callers of kvm_free_memslot() pass NULL for @dont, remove
    the param from the top-level routine and all arch's implementations.
    
    No functional change intended.
    
    Tested-by: Christoffer Dall <christoffer.dall@arm.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 768c4a9269be..838cdcd2db12 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -685,10 +685,9 @@ long kvm_arch_dev_ioctl(struct file *filp,
 	return -EINVAL;
 }
 
-void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
-			   struct kvm_memory_slot *dont)
+void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *slot)
 {
-	kvmppc_core_free_memslot(kvm, free, dont);
+	kvmppc_core_free_memslot(kvm, slot);
 }
 
 int kvm_arch_prepare_memory_region(struct kvm *kvm,

commit 9d4c197c0e94c372ceffd2ffc53a23518f301ed9
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:24 2020 -0800

    KVM: Drop "const" attribute from old memslot in commit_memory_region()
    
    Drop the "const" attribute from @old in kvm_arch_commit_memory_region()
    to allow arch specific code to free arch specific resources in the old
    memslot without having to cast away the attribute.  Freeing resources in
    kvm_arch_commit_memory_region() paves the way for simplifying
    kvm_free_memslot() by eliminating the last usage of its @dont param.
    
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 48abf1b9ad58..768c4a9269be 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -701,7 +701,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				   const struct kvm_userspace_memory_region *mem,
-				   const struct kvm_memory_slot *old,
+				   struct kvm_memory_slot *old,
 				   const struct kvm_memory_slot *new,
 				   enum kvm_mr_change change)
 {

commit 414de7abbf809f046511269797d9f2310b88e036
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:20 2020 -0800

    KVM: Drop kvm_arch_create_memslot()
    
    Remove kvm_arch_create_memslot() now that all arch implementations are
    effectively nops.  Removing kvm_arch_create_memslot() eliminates the
    possibility for arch specific code to allocate memory prior to setting
    a memslot, which sets the stage for simplifying kvm_free_memslot().
    
    Cc: Janosch Frank <frankja@linux.ibm.com>
    Acked-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4e0f7a77920d..48abf1b9ad58 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -691,12 +691,6 @@ void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 	kvmppc_core_free_memslot(kvm, free, dont);
 }
 
-int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
-			    unsigned long npages)
-{
-	return 0;
-}
-
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
 				   const struct kvm_userspace_memory_region *mem,

commit 82307e676f9d885871f121e4921b12905a53397d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Tue Feb 18 13:07:18 2020 -0800

    KVM: PPC: Move memslot memory allocation into prepare_memory_region()
    
    Allocate the rmap array during kvm_arch_prepare_memory_region() to pave
    the way for removing kvm_arch_create_memslot() altogether.  Moving PPC's
    memory allocation only changes the order of kernel memory allocations
    between PPC and common KVM code.
    
    No functional change intended.
    
    Acked-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1af96fb5dc6f..4e0f7a77920d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -694,7 +694,7 @@ void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 			    unsigned long npages)
 {
-	return kvmppc_core_create_memslot(kvm, slot, npages);
+	return 0;
 }
 
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
@@ -702,7 +702,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   const struct kvm_userspace_memory_region *mem,
 				   enum kvm_mr_change change)
 {
-	return kvmppc_core_prepare_memory_region(kvm, memslot, mem);
+	return kvmppc_core_prepare_memory_region(kvm, memslot, mem, change);
 }
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,

commit ddd259c9aaba08244dba8877687ee856f79c4f45
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:28 2019 -0800

    KVM: Drop kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit()
    
    Remove kvm_arch_vcpu_init() and kvm_arch_vcpu_uninit() now that all
    arch specific implementations are nops.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4fbf8690b8c5..1af96fb5dc6f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -801,16 +801,6 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 	return kvmppc_core_pending_dec(vcpu);
 }
 
-int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
-{
-	return 0;
-}
-
-void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
-{
-
-}
-
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 #ifdef CONFIG_BOOKE

commit 74ce2e60d4874fc2464e321af1397c6fae984ec9
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:26 2019 -0800

    KVM: PPC: Move all vcpu init code into kvm_arch_vcpu_create()
    
    Fold init() into create() now that the two are called back-to-back by
    common KVM code (kvm_vcpu_init() calls kvm_arch_vcpu_init() as its last
    action, and kvm_vm_ioctl_create_vcpu() calls kvm_arch_vcpu_create()
    immediately thereafter).  Rinse and repeat for kvm_arch_vcpu_uninit()
    and kvm_arch_vcpu_destroy().  This paves the way for removing
    kvm_arch_vcpu_{un}init() entirely.
    
    Note, calling kvmppc_mmu_destroy() if kvmppc_core_vcpu_create() fails
    may or may not be necessary.  Move it along with the more obvious call
    to kvmppc_subarch_vcpu_uninit() so as not to inadvertantly introduce a
    functional change and/or bug.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index fce1b4776e55..4fbf8690b8c5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -725,17 +725,43 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return 0;
 }
 
+static enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)
+{
+	struct kvm_vcpu *vcpu;
+
+	vcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);
+	kvmppc_decrementer_func(vcpu);
+
+	return HRTIMER_NORESTART;
+}
+
 int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
 	int err;
 
-	err = kvmppc_core_vcpu_create(vcpu);
+	hrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
+	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
+	vcpu->arch.dec_expires = get_tb();
+
+#ifdef CONFIG_KVM_EXIT_TIMING
+	mutex_init(&vcpu->arch.exit_timing_lock);
+#endif
+	err = kvmppc_subarch_vcpu_init(vcpu);
 	if (err)
 		return err;
 
+	err = kvmppc_core_vcpu_create(vcpu);
+	if (err)
+		goto out_vcpu_uninit;
+
 	vcpu->arch.wqp = &vcpu->wq;
 	kvmppc_create_vcpu_debugfs(vcpu, vcpu->vcpu_id);
 	return 0;
+
+out_vcpu_uninit:
+	kvmppc_mmu_destroy(vcpu);
+	kvmppc_subarch_vcpu_uninit(vcpu);
+	return err;
 }
 
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
@@ -765,6 +791,9 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	}
 
 	kvmppc_core_vcpu_free(vcpu);
+
+	kvmppc_mmu_destroy(vcpu);
+	kvmppc_subarch_vcpu_uninit(vcpu);
 }
 
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
@@ -772,35 +801,14 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 	return kvmppc_core_pending_dec(vcpu);
 }
 
-static enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)
-{
-	struct kvm_vcpu *vcpu;
-
-	vcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);
-	kvmppc_decrementer_func(vcpu);
-
-	return HRTIMER_NORESTART;
-}
-
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
-	int ret;
-
-	hrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
-	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
-	vcpu->arch.dec_expires = get_tb();
-
-#ifdef CONFIG_KVM_EXIT_TIMING
-	mutex_init(&vcpu->arch.exit_timing_lock);
-#endif
-	ret = kvmppc_subarch_vcpu_init(vcpu);
-	return ret;
+	return 0;
 }
 
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 {
-	kvmppc_mmu_destroy(vcpu);
-	kvmppc_subarch_vcpu_uninit(vcpu);
+
 }
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)

commit e529ef66e6b53b34f9b8caac55950c8a55c79dac
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:15 2019 -0800

    KVM: Move vcpu alloc and init invocation to common code
    
    Now that all architectures tightly couple vcpu allocation/free with the
    mandatory calls to kvm_{un}init_vcpu(), move the sequences verbatim to
    common KVM code.
    
    Move both allocation and initialization in a single patch to eliminate
    thrash in arch specific code.  The bisection benefits of moving the two
    pieces in separate patches is marginal at best, whereas the odds of
    introducing a transient arch specific bug are non-zero.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e3e2b88d3d8b..fce1b4776e55 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -725,32 +725,17 @@ int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
 	return 0;
 }
 
-struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
+int kvm_arch_vcpu_create(struct kvm_vcpu *vcpu)
 {
-	struct kvm_vcpu *vcpu;
 	int err;
 
-	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
-	if (!vcpu)
-		return ERR_PTR(-ENOMEM);
-
-	err = kvm_vcpu_init(vcpu, kvm, id);
-	if (err)
-		goto free_vcpu;
-
 	err = kvmppc_core_vcpu_create(vcpu);
 	if (err)
-		goto uninit_vcpu;
+		return err;
 
 	vcpu->arch.wqp = &vcpu->wq;
-	kvmppc_create_vcpu_debugfs(vcpu, id);
-	return vcpu;
-
-uninit_vcpu:
-	kvm_vcpu_uninit(vcpu);
-free_vcpu:
-	kmem_cache_free(kvm_vcpu_cache, vcpu);
-	return ERR_PTR(err);
+	kvmppc_create_vcpu_debugfs(vcpu, vcpu->vcpu_id);
+	return 0;
 }
 
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
@@ -780,10 +765,6 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 	}
 
 	kvmppc_core_vcpu_free(vcpu);
-
-	kvm_vcpu_uninit(vcpu);
-
-	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)

commit 4543bdc08857e8026475a477e7ba88e461f38271
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:14 2019 -0800

    KVM: Introduce kvm_vcpu_destroy()
    
    Add kvm_vcpu_destroy() and wire up all architectures to call the common
    function instead of their arch specific implementation.  The common
    destruction function will be used by future patches to move allocation
    and initialization of vCPUs to common KVM code, i.e. to free resources
    that are allocated by arch agnostic code.
    
    No functional change intended.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 998ef60ac463..e3e2b88d3d8b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -475,7 +475,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 #endif
 
 	kvm_for_each_vcpu(i, vcpu, kvm)
-		kvm_arch_vcpu_destroy(vcpu);
+		kvm_vcpu_destroy(vcpu);
 
 	mutex_lock(&kvm->lock);
 	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)

commit 897cc38eaab96d006ab17edd0f50a2f432f584cf
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:09 2019 -0800

    KVM: Add kvm_arch_vcpu_precreate() to handle pre-allocation issues
    
    Add a pre-allocation arch hook to handle checks that are currently done
    by arch specific code prior to allocating the vCPU object.  This paves
    the way for moving the allocation to common KVM code.
    
    Acked-by: Christoffer Dall <christoffer.dall@arm.com>
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a2ba708f5cec..998ef60ac463 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -720,6 +720,11 @@ void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
 	kvmppc_core_flush_memslot(kvm, slot);
 }
 
+int kvm_arch_vcpu_precreate(struct kvm *kvm, unsigned int id)
+{
+	return 0;
+}
+
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
 	struct kvm_vcpu *vcpu;

commit d5279f3a882c3c5b57703b602d872d3bc7ffa51d
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:03 2019 -0800

    KVM: PPC: Drop kvm_arch_vcpu_free()
    
    Remove the superfluous kvm_arch_vcpu_free() as it is no longer called
    from commmon KVM code.  Note, kvm_arch_vcpu_destroy() *is* called from
    common code, i.e. choosing which function to whack is not completely
    arbitrary.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 173f57e0a1b5..a2ba708f5cec 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -475,7 +475,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 #endif
 
 	kvm_for_each_vcpu(i, vcpu, kvm)
-		kvm_arch_vcpu_free(vcpu);
+		kvm_arch_vcpu_destroy(vcpu);
 
 	mutex_lock(&kvm->lock);
 	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
@@ -752,7 +752,7 @@ void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
 }
 
-void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
+void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 {
 	/* Make sure we're not using the vcpu anymore */
 	hrtimer_cancel(&vcpu->arch.dec_timer);
@@ -781,11 +781,6 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
-void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
-{
-	kvm_arch_vcpu_free(vcpu);
-}
-
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 {
 	return kvmppc_core_pending_dec(vcpu);

commit ff030fdf55732266c2d35b1a4a0baaf9ce49e9dd
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:55:00 2019 -0800

    KVM: PPC: Move kvm_vcpu_init() invocation to common code
    
    Move the kvm_cpu_{un}init() calls to common PPC code as an intermediate
    step towards removing kvm_cpu_{un}init() altogether.
    
    No functional change intended.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index fd978f681b66..173f57e0a1b5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -729,14 +729,20 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 	if (!vcpu)
 		return ERR_PTR(-ENOMEM);
 
-	err = kvmppc_core_vcpu_create(kvm, vcpu, id);
+	err = kvm_vcpu_init(vcpu, kvm, id);
 	if (err)
 		goto free_vcpu;
 
+	err = kvmppc_core_vcpu_create(vcpu);
+	if (err)
+		goto uninit_vcpu;
+
 	vcpu->arch.wqp = &vcpu->wq;
 	kvmppc_create_vcpu_debugfs(vcpu, id);
 	return vcpu;
 
+uninit_vcpu:
+	kvm_vcpu_uninit(vcpu);
 free_vcpu:
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 	return ERR_PTR(err);
@@ -770,6 +776,8 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 
 	kvmppc_core_vcpu_free(vcpu);
 
+	kvm_vcpu_uninit(vcpu);
+
 	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 

commit c50bfbdc38ec56cf8e53afb4f9ebb600dfcabd49
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Wed Dec 18 13:54:57 2019 -0800

    KVM: PPC: Allocate vcpu struct in common PPC code
    
    Move allocation of all flavors of PPC vCPUs to common PPC code.  All
    variants either allocate 'struct kvm_vcpu' directly, or require that
    the embedded 'struct kvm_vcpu' member be located at offset 0, i.e.
    guarantee that the allocation can be directly interpreted as a 'struct
    kvm_vcpu' object.
    
    Remove the message from the build-time assertion regarding placement of
    the struct, as compatibility with the arch usercopy region is no longer
    the sole dependent on 'struct kvm_vcpu' being at offset zero.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 416fb3d2a1d0..fd978f681b66 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -723,12 +723,23 @@ void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
 	struct kvm_vcpu *vcpu;
-	vcpu = kvmppc_core_vcpu_create(kvm, id);
-	if (!IS_ERR(vcpu)) {
-		vcpu->arch.wqp = &vcpu->wq;
-		kvmppc_create_vcpu_debugfs(vcpu, id);
-	}
+	int err;
+
+	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
+	if (!vcpu)
+		return ERR_PTR(-ENOMEM);
+
+	err = kvmppc_core_vcpu_create(kvm, vcpu, id);
+	if (err)
+		goto free_vcpu;
+
+	vcpu->arch.wqp = &vcpu->wq;
+	kvmppc_create_vcpu_debugfs(vcpu, id);
 	return vcpu;
+
+free_vcpu:
+	kmem_cache_free(kvm_vcpu_cache, vcpu);
+	return ERR_PTR(err);
 }
 
 void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
@@ -758,6 +769,8 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 	}
 
 	kvmppc_core_vcpu_free(vcpu);
+
+	kmem_cache_free(kvm_vcpu_cache, vcpu);
 }
 
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)

commit 22945688acd4d0ec2620b0670a53110401ed9c59
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Mon Nov 25 08:36:30 2019 +0530

    KVM: PPC: Book3S HV: Support reset of secure guest
    
    Add support for reset of secure guest via a new ioctl KVM_PPC_SVM_OFF.
    This ioctl will be issued by QEMU during reset and includes the
    the following steps:
    
    - Release all device pages of the secure guest.
    - Ask UV to terminate the guest via UV_SVM_TERMINATE ucall
    - Unpin the VPA pages so that they can be migrated back to secure
      side when guest becomes secure again. This is required because
      pinned pages can't be migrated.
    - Reinit the partition scoped page tables
    
    After these steps, guest is ready to issue UV_ESM call once again
    to switch to secure mode.
    
    Signed-off-by: Bharata B Rao <bharata@linux.ibm.com>
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
            [Implementation of uv_svm_terminate() and its call from
            guest shutdown path]
    Signed-off-by: Ram Pai <linuxram@us.ibm.com>
            [Unpinning of VPA pages]
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9e085e931d74..416fb3d2a1d0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -31,6 +31,8 @@
 #include <asm/hvcall.h>
 #include <asm/plpar_wrappers.h>
 #endif
+#include <asm/ultravisor.h>
+#include <asm/kvm_host.h>
 
 #include "timing.h"
 #include "irq.h"
@@ -2413,6 +2415,16 @@ long kvm_arch_vm_ioctl(struct file *filp,
 			r = -EFAULT;
 		break;
 	}
+	case KVM_PPC_SVM_OFF: {
+		struct kvm *kvm = filp->private_data;
+
+		r = 0;
+		if (!kvm->arch.kvm_ops->svm_off)
+			goto out;
+
+		r = kvm->arch.kvm_ops->svm_off(kvm);
+		break;
+	}
 	default: {
 		struct kvm *kvm = filp->private_data;
 		r = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);

commit 1a9167a214f560a23c5050ce6dfebae489528f0d
Author: Fabiano Rosas <farosas@linux.ibm.com>
Date:   Wed Jun 19 13:01:27 2019 -0300

    KVM: PPC: Report single stepping capability
    
    When calling the KVM_SET_GUEST_DEBUG ioctl, userspace might request
    the next instruction to be single stepped via the
    KVM_GUESTDBG_SINGLESTEP control bit of the kvm_guest_debug structure.
    
    This patch adds the KVM_CAP_PPC_GUEST_DEBUG_SSTEP capability in order
    to inform userspace about the state of single stepping support.
    
    We currently don't have support for guest single stepping implemented
    in Book3S HV so the capability is only present for Book3S PR and
    BookE.
    
    Signed-off-by: Fabiano Rosas <farosas@linux.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3a77bb643452..9e085e931d74 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -522,6 +522,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_IMMEDIATE_EXIT:
 		r = 1;
 		break;
+	case KVM_CAP_PPC_GUEST_DEBUG_SSTEP:
+		/* fall through */
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:

commit 8146856b0a2693d5f7334f720c9b39a987f7571d
Merge: 16cfacc80857 ff42df49e75f
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Sep 10 16:51:17 2019 +0200

    Merge tag 'kvm-ppc-next-5.4-1' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    PPC KVM update for 5.4
    
    - Some prep for extending the uses of the rmap array
    - Various minor fixes
    - Commits from the powerpc topic/ppc-kvm branch, which fix a problem
      with interrupts arriving after free_irq, causing host hangs and crashes.

commit 2ad7a27deaf6d78545d97ab80874584f6990360e
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Aug 26 16:21:21 2019 +1000

    KVM: PPC: Book3S: Enable XIVE native capability only if OPAL has required functions
    
    There are some POWER9 machines where the OPAL firmware does not support
    the OPAL_XIVE_GET_QUEUE_STATE and OPAL_XIVE_SET_QUEUE_STATE calls.
    The impact of this is that a guest using XIVE natively will not be able
    to be migrated successfully.  On the source side, the get_attr operation
    on the KVM native device for the KVM_DEV_XIVE_GRP_EQ_CONFIG attribute
    will fail; on the destination side, the set_attr operation for the same
    attribute will fail.
    
    This adds tests for the existence of the OPAL get/set queue state
    functions, and if they are not supported, the XIVE-native KVM device
    is not created and the KVM_CAP_PPC_IRQ_XIVE capability returns false.
    Userspace can then either provide a software emulation of XIVE, or
    else tell the guest that it does not have a XIVE controller available
    to it.
    
    Cc: stable@vger.kernel.org # v5.2+
    Fixes: 3fab2d10588e ("KVM: PPC: Book3S HV: XIVE: Activate XIVE exploitation mode")
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: CÃ©dric Le Goater <clg@kaod.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0dba7eb24f92..7012dd709787 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -566,7 +566,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		 * a POWER9 processor) and the PowerNV platform, as
 		 * nested is not yet supported.
 		 */
-		r = xive_enabled() && !!cpu_has_feature(CPU_FTR_HVMODE);
+		r = xive_enabled() && !!cpu_has_feature(CPU_FTR_HVMODE) &&
+			kvmppc_xive_native_supported();
 		break;
 #endif
 

commit 741cbbae0768b828be2d48331eb371a4f08bbea8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Sat Aug 3 08:14:25 2019 +0200

    KVM: remove kvm_arch_has_vcpu_debugfs()
    
    There is no need for this function as all arches have to implement
    kvm_arch_create_vcpu_debugfs() no matter what.  A #define symbol
    let us actually simplify the code.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3e34d5fa6708..3e566c2e6066 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -457,16 +457,6 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	return -EINVAL;
 }
 
-bool kvm_arch_has_vcpu_debugfs(void)
-{
-	return false;
-}
-
-int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
-{
-	return 0;
-}
-
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	unsigned int i;

commit 17e433b54393a6269acbcb792da97791fe1592d8
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Mon Aug 5 10:03:19 2019 +0800

    KVM: Fix leak vCPU's VMCS value into other pCPU
    
    After commit d73eb57b80b (KVM: Boost vCPUs that are delivering interrupts), a
    five years old bug is exposed. Running ebizzy benchmark in three 80 vCPUs VMs
    on one 80 pCPUs Skylake server, a lot of rcu_sched stall warning splatting
    in the VMs after stress testing:
    
     INFO: rcu_sched detected stalls on CPUs/tasks: { 4 41 57 62 77} (detected by 15, t=60004 jiffies, g=899, c=898, q=15073)
     Call Trace:
       flush_tlb_mm_range+0x68/0x140
       tlb_flush_mmu.part.75+0x37/0xe0
       tlb_finish_mmu+0x55/0x60
       zap_page_range+0x142/0x190
       SyS_madvise+0x3cd/0x9c0
       system_call_fastpath+0x1c/0x21
    
    swait_active() sustains to be true before finish_swait() is called in
    kvm_vcpu_block(), voluntarily preempted vCPUs are taken into account
    by kvm_vcpu_on_spin() loop greatly increases the probability condition
    kvm_arch_vcpu_runnable(vcpu) is checked and can be true, when APICv
    is enabled the yield-candidate vCPU's VMCS RVI field leaks(by
    vmx_sync_pir_to_irr()) into spinning-on-a-taken-lock vCPU's current
    VMCS.
    
    This patch fixes it by checking conservatively a subset of events.
    
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Marc Zyngier <Marc.Zyngier@arm.com>
    Cc: stable@vger.kernel.org
    Fixes: 98f4a1467 (KVM: add kvm_arch_vcpu_runnable() test to kvm_vcpu_on_spin() loop)
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0dba7eb24f92..3e34d5fa6708 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -50,6 +50,11 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 	return !!(v->arch.pending_exceptions) || kvm_request_pending(v);
 }
 
+bool kvm_arch_dy_runnable(struct kvm_vcpu *vcpu)
+{
+	return kvm_arch_vcpu_runnable(vcpu);
+}
+
 bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
 {
 	return false;

commit a45ff5994c9cde41af627c46abb9f32beae68943
Merge: 429bb83af8bc 1e0cf16cdad1
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jul 11 15:14:16 2019 +0200

    Merge tag 'kvm-arm-for-5.3' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/arm updates for 5.3
    
    - Add support for chained PMU counters in guests
    - Improve SError handling
    - Handle Neoverse N1 erratum #1349291
    - Allow side-channel mitigation status to be migrated
    - Standardise most AArch64 system register accesses to msr_s/mrs_s
    - Fix host MPIDR corruption on 32bit

commit d94d71cb45fda694a7189839f1c6aacb4f615f95
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 07:12:40 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 266
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation this program is
      distributed in the hope that it will be useful but without any
      warranty without even the implied warranty of merchantability or
      fitness for a particular purpose see the gnu general public license
      for more details you should have received a copy of the gnu general
      public license along with this program if not write to the free
      software foundation 51 franklin street fifth floor boston ma 02110
      1301 usa
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 67 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Richard Fontana <rfontana@redhat.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190529141333.953658117@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index aa3a678711be..6d704ad2472b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1,16 +1,5 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, version 2, as
- * published by the Free Software Foundation.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
  *
  * Copyright IBM Corp. 2007
  *

commit f257d6dcda0187693407e0c2e5dab69bdab3223f
Author: Sean Christopherson <sean.j.christopherson@intel.com>
Date:   Fri Apr 19 22:18:17 2019 -0700

    KVM: Directly return result from kvm_arch_check_processor_compat()
    
    Add a wrapper to invoke kvm_arch_check_processor_compat() so that the
    boilerplate ugliness of checking virtualization support on all CPUs is
    hidden from the arch specific code.  x86's implementation in particular
    is quite heinous, as it unnecessarily propagates the out-param pattern
    into kvm_x86_ops.
    
    While the x86 specific issue could be resolved solely by changing
    kvm_x86_ops, make the change for all architectures as returning a value
    directly is prettier and technically more robust, e.g. s390 doesn't set
    the out param, which could lead to subtle breakage in the (highly
    unlikely) scenario where the out-param was not pre-initialized by the
    caller.
    
    Opportunistically annotate svm_check_processor_compat() with __init.
    
    Signed-off-by: Sean Christopherson <sean.j.christopherson@intel.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index aa3a678711be..628d3c791ad7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -425,9 +425,9 @@ int kvm_arch_hardware_setup(void)
 	return 0;
 }
 
-void kvm_arch_check_processor_compat(void *rtn)
+int kvm_arch_check_processor_compat(void)
 {
-	*(int *)rtn = kvmppc_core_check_processor_compat();
+	return kvmppc_core_check_processor_compat();
 }
 
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)

commit a86cb413f4bf273a9d341a3ab2c2ca44e12eb317
Author: Thomas Huth <thuth@redhat.com>
Date:   Thu May 23 18:43:08 2019 +0200

    KVM: s390: Do not report unusabled IDs via KVM_CAP_MAX_VCPU_ID
    
    KVM_CAP_MAX_VCPU_ID is currently always reporting KVM_MAX_VCPU_ID on all
    architectures. However, on s390x, the amount of usable CPUs is determined
    during runtime - it is depending on the features of the machine the code
    is running on. Since we are using the vcpu_id as an index into the SCA
    structures that are defined by the hardware (see e.g. the sca_add_vcpu()
    function), it is not only the amount of CPUs that is limited by the hard-
    ware, but also the range of IDs that we can use.
    Thus KVM_CAP_MAX_VCPU_ID must be determined during runtime on s390x, too.
    So the handling of KVM_CAP_MAX_VCPU_ID has to be moved from the common
    code into the architecture specific code, and on s390x we have to return
    the same value here as for KVM_CAP_MAX_VCPUS.
    This problem has been discovered with the kvm_create_max_vcpus selftest.
    With this change applied, the selftest now passes on s390x, too.
    
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Message-Id: <20190523164309.13345-9-thuth@redhat.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3393b166817a..aa3a678711be 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -657,6 +657,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
 		break;
+	case KVM_CAP_MAX_VCPU_ID:
+		r = KVM_MAX_VCPU_ID;
+		break;
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_PPC_GET_SMMU_INFO:
 		r = 1;

commit 59c5c58c5b93285753d5c1de34d2e00039c27bc0
Merge: f93f7ede087f 4894fbcce856
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed May 15 23:38:42 2019 +0200

    Merge tag 'kvm-ppc-next-5.2-2' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    PPC KVM update for 5.2
    
    * Support for guests to access the new POWER9 XIVE interrupt controller
      hardware directly, reducing interrupt latency and overhead for guests.
    
    * In-kernel implementation of the H_PAGE_INIT hypercall.
    
    * Reduce memory usage of sparsely-populated IOMMU tables.
    
    * Several bug fixes.
    
    Second PPC KVM update for 5.2
    
    * Fix a bug, fix a spelling mistake, remove some useless code.

commit 3fab2d10588e09abd4bd7b9bacce5e4e435993f6
Author: CÃ©dric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:40 2019 +0200

    KVM: PPC: Book3S HV: XIVE: Activate XIVE exploitation mode
    
    Full support for the XIVE native exploitation mode is now available,
    advertise the capability KVM_CAP_PPC_IRQ_XIVE for guests running on
    PowerNV KVM Hypervisors only. Support for nested guests (pseries KVM
    Hypervisor) is not yet available. XIVE should also have been activated
    which is default setting on POWER9 systems running a recent Linux
    kernel.
    
    Signed-off-by: CÃ©dric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b0858ee61460..f54926c78320 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -573,10 +573,11 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #ifdef CONFIG_KVM_XIVE
 	case KVM_CAP_PPC_IRQ_XIVE:
 		/*
-		 * Return false until all the XIVE infrastructure is
-		 * in place including support for migration.
+		 * We need XIVE to be enabled on the platform (implies
+		 * a POWER9 processor) and the PowerNV platform, as
+		 * nested is not yet supported.
 		 */
-		r = 0;
+		r = xive_enabled() && !!cpu_has_feature(CPU_FTR_HVMODE);
 		break;
 #endif
 

commit eacc56bb9de3e6830ddc169553772cd6de59ee4c
Author: CÃ©dric Le Goater <clg@kaod.org>
Date:   Thu Apr 18 12:39:28 2019 +0200

    KVM: PPC: Book3S HV: XIVE: Introduce a new capability KVM_CAP_PPC_IRQ_XIVE
    
    The user interface exposes a new capability KVM_CAP_PPC_IRQ_XIVE to
    let QEMU connect the vCPU presenters to the XIVE KVM device if
    required. The capability is not advertised for now as the full support
    for the XIVE native exploitation mode is not yet available. When this
    is case, the capability will be advertised on PowerNV Hypervisors
    only. Nested guests (pseries KVM Hypervisor) are not supported.
    
    Internally, the interface to the new KVM device is protected with a
    new interrupt mode: KVMPPC_IRQ_XIVE.
    
    Signed-off-by: CÃ©dric Le Goater <clg@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8885377ec3e0..b0858ee61460 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -570,6 +570,15 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_PPC_GET_CPU_CHAR:
 		r = 1;
 		break;
+#ifdef CONFIG_KVM_XIVE
+	case KVM_CAP_PPC_IRQ_XIVE:
+		/*
+		 * Return false until all the XIVE infrastructure is
+		 * in place including support for migration.
+		 */
+		r = 0;
+		break;
+#endif
 
 	case KVM_CAP_PPC_ALLOC_HTAB:
 		r = hv_enabled;
@@ -753,6 +762,9 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 		else
 			kvmppc_xics_free_icp(vcpu);
 		break;
+	case KVMPPC_IRQ_XIVE:
+		kvmppc_xive_native_cleanup_vcpu(vcpu);
+		break;
 	}
 
 	kvmppc_core_vcpu_free(vcpu);
@@ -1941,6 +1953,30 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		break;
 	}
 #endif /* CONFIG_KVM_XICS */
+#ifdef CONFIG_KVM_XIVE
+	case KVM_CAP_PPC_IRQ_XIVE: {
+		struct fd f;
+		struct kvm_device *dev;
+
+		r = -EBADF;
+		f = fdget(cap->args[0]);
+		if (!f.file)
+			break;
+
+		r = -ENXIO;
+		if (!xive_enabled())
+			break;
+
+		r = -EPERM;
+		dev = kvm_device_from_filp(f.file);
+		if (dev)
+			r = kvmppc_xive_native_connect_vcpu(dev, vcpu,
+							    cap->args[1]);
+
+		fdput(f);
+		break;
+	}
+#endif /* CONFIG_KVM_XIVE */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_FWNMI:
 		r = -EINVAL;

commit c110ae578ca0a10064dfbda3d786d6a733b9fe69
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Mar 28 17:24:03 2019 +0100

    kvm: move KVM_CAP_NR_MEMSLOTS to common code
    
    All architectures except MIPS were defining it in the same way,
    and memory slots are handled entirely by common code so there
    is no point in keeping the definition per-architecture.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8885377ec3e0..92910b7c5bcc 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -644,9 +644,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		else
 			r = num_online_cpus();
 		break;
-	case KVM_CAP_NR_MEMSLOTS:
-		r = KVM_USER_MEM_SLOTS;
-		break;
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
 		break;

commit 2b57ecd0208f7ac0b20b1b171698f027481a39f6
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Mar 1 14:25:16 2019 +1100

    KVM: PPC: Book3S: Add count cache flush parameters to kvmppc_get_cpu_char()
    
    Add KVM_PPC_CPU_CHAR_BCCTR_FLUSH_ASSIST &
    KVM_PPC_CPU_BEHAV_FLUSH_COUNT_CACHE to the characteristics returned
    from the H_GET_CPU_CHARACTERISTICS H-CALL, as queried from either the
    hypervisor or the device tree.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8c69af10f91d..8885377ec3e0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -2189,10 +2189,12 @@ static int pseries_get_cpu_char(struct kvm_ppc_cpu_char *cp)
 			KVM_PPC_CPU_CHAR_L1D_THREAD_PRIV |
 			KVM_PPC_CPU_CHAR_BR_HINT_HONOURED |
 			KVM_PPC_CPU_CHAR_MTTRIG_THR_RECONF |
-			KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS;
+			KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS |
+			KVM_PPC_CPU_CHAR_BCCTR_FLUSH_ASSIST;
 		cp->behaviour_mask = KVM_PPC_CPU_BEHAV_FAVOUR_SECURITY |
 			KVM_PPC_CPU_BEHAV_L1D_FLUSH_PR |
-			KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR;
+			KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR |
+			KVM_PPC_CPU_BEHAV_FLUSH_COUNT_CACHE;
 	}
 	return 0;
 }
@@ -2251,12 +2253,16 @@ static int kvmppc_get_cpu_char(struct kvm_ppc_cpu_char *cp)
 		if (have_fw_feat(fw_features, "enabled",
 				 "fw-count-cache-disabled"))
 			cp->character |= KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS;
+		if (have_fw_feat(fw_features, "enabled",
+				 "fw-count-cache-flush-bcctr2,0,0"))
+			cp->character |= KVM_PPC_CPU_CHAR_BCCTR_FLUSH_ASSIST;
 		cp->character_mask = KVM_PPC_CPU_CHAR_SPEC_BAR_ORI31 |
 			KVM_PPC_CPU_CHAR_BCCTRL_SERIALISED |
 			KVM_PPC_CPU_CHAR_L1D_FLUSH_ORI30 |
 			KVM_PPC_CPU_CHAR_L1D_FLUSH_TRIG2 |
 			KVM_PPC_CPU_CHAR_L1D_THREAD_PRIV |
-			KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS;
+			KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS |
+			KVM_PPC_CPU_CHAR_BCCTR_FLUSH_ASSIST;
 
 		if (have_fw_feat(fw_features, "enabled",
 				 "speculation-policy-favor-security"))
@@ -2267,9 +2273,13 @@ static int kvmppc_get_cpu_char(struct kvm_ppc_cpu_char *cp)
 		if (!have_fw_feat(fw_features, "disabled",
 				  "needs-spec-barrier-for-bound-checks"))
 			cp->behaviour |= KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR;
+		if (have_fw_feat(fw_features, "enabled",
+				 "needs-count-cache-flush-on-context-switch"))
+			cp->behaviour |= KVM_PPC_CPU_BEHAV_FLUSH_COUNT_CACHE;
 		cp->behaviour_mask = KVM_PPC_CPU_BEHAV_FAVOUR_SECURITY |
 			KVM_PPC_CPU_BEHAV_L1D_FLUSH_PR |
-			KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR;
+			KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR |
+			KVM_PPC_CPU_BEHAV_FLUSH_COUNT_CACHE;
 
 		of_node_put(fw_features);
 	}

commit 03f953329bd872b176e825584d8c0b50685f16ee
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 4 22:07:20 2019 +1100

    KVM: PPC: Book3S: Allow XICS emulation to work in nested hosts using XIVE
    
    Currently, the KVM code assumes that if the host kernel is using the
    XIVE interrupt controller (the new interrupt controller that first
    appeared in POWER9 systems), then the in-kernel XICS emulation will
    use the XIVE hardware to deliver interrupts to the guest.  However,
    this only works when the host is running in hypervisor mode and has
    full access to all of the XIVE functionality.  It doesn't work in any
    nested virtualization scenario, either with PR KVM or nested-HV KVM,
    because the XICS-on-XIVE code calls directly into the native-XIVE
    routines, which are not initialized and cannot function correctly
    because they use OPAL calls, and OPAL is not available in a guest.
    
    This means that using the in-kernel XICS emulation in a nested
    hypervisor that is using XIVE as its interrupt controller will cause a
    (nested) host kernel crash.  To fix this, we change most of the places
    where the current code calls xive_enabled() to select between the
    XICS-on-XIVE emulation and the plain XICS emulation to call a new
    function, xics_on_xive(), which returns false in a guest.
    
    However, there is a further twist.  The plain XICS emulation has some
    functions which are used in real mode and access the underlying XICS
    controller (the interrupt controller of the host) directly.  In the
    case of a nested hypervisor, this means doing XICS hypercalls
    directly.  When the nested host is using XIVE as its interrupt
    controller, these hypercalls will fail.  Therefore this also adds
    checks in the places where the XICS emulation wants to access the
    underlying interrupt controller directly, and if that is XIVE, makes
    the code use the virtual mode fallback paths, which call generic
    kernel infrastructure rather than doing direct XICS access.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: CÃ©dric Le Goater <clg@kaod.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b90a7d154180..8c69af10f91d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -748,7 +748,7 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 		kvmppc_mpic_disconnect_vcpu(vcpu->arch.mpic, vcpu);
 		break;
 	case KVMPPC_IRQ_XICS:
-		if (xive_enabled())
+		if (xics_on_xive())
 			kvmppc_xive_cleanup_vcpu(vcpu);
 		else
 			kvmppc_xics_free_icp(vcpu);
@@ -1931,7 +1931,7 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = -EPERM;
 		dev = kvm_device_from_filp(f.file);
 		if (dev) {
-			if (xive_enabled())
+			if (xics_on_xive())
 				r = kvmppc_xive_connect_vcpu(dev, vcpu, cap->args[1]);
 			else
 				r = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);

commit cfdfaf4a86406f8eff7901de09f4910341eb9e31
Merge: e9f2e05a5f19 95d386c2d2e7
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Thu Dec 20 14:54:09 2018 +0100

    Merge tag 'kvm-ppc-next-4.21-1' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    PPC KVM update for 4.21 from Paul Mackerras
    
    The main new feature this time is support in HV nested KVM for passing
    a device that is emulated by a level 0 hypervisor and presented to
    level 1 as a PCI device through to a level 2 guest using VFIO.
    
    Apart from that there are improvements for migration of radix guests
    under HV KVM and some other fixes and cleanups.

commit 873db2cd9a6d7f017d8f4c637cf4166c038c27d6
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Dec 14 16:29:08 2018 +1100

    KVM: PPC: Book3S HV: Allow passthrough of an emulated device to an L2 guest
    
    Allow for a device which is being emulated at L0 (the host) for an L1
    guest to be passed through to a nested (L2) guest.
    
    The existing kvmppc_hv_emulate_mmio function can be used here. The main
    challenge is that for a load the result must be stored into the L2 gpr,
    not an L1 gpr as would normally be the case after going out to qemu to
    complete the operation. This presents a challenge as at this point the
    L2 gpr state has been written back into L1 memory.
    
    To work around this we store the address in L1 memory of the L2 gpr
    where the result of the load is to be stored and use the new io_gpr
    value KVM_MMIO_REG_NESTED_GPR to indicate that this is a nested load for
    which completion must be done when returning back into the kernel. Then
    in kvmppc_complete_mmio_load() the resultant value is written into L1
    memory at the location of the indicated L2 gpr.
    
    Note that we don't currently let an L1 guest emulate a device for an L2
    guest which is then passed through to an L3 guest.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ec9cbf9db364..db61096ffcf7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1208,6 +1208,14 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 				KVMPPC_VMX_COPY_BYTE)
 			kvmppc_set_vmx_byte(vcpu, gpr);
 		break;
+#endif
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	case KVM_MMIO_REG_NESTED_GPR:
+		if (kvmppc_need_byteswap(vcpu))
+			gpr = swab64(gpr);
+		kvm_vcpu_write_guest(vcpu, vcpu->arch.nested_io_gpr, &gpr,
+				     sizeof(gpr));
+		break;
 #endif
 	default:
 		BUG();

commit cc6929cc842cceee714c66cd55346b1a9f114a98
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Dec 14 16:29:07 2018 +1100

    KVM: PPC: Update kvmppc_st and kvmppc_ld to use quadrants
    
    The functions kvmppc_st and kvmppc_ld are used to access guest memory
    from the host using a guest effective address. They do so by translating
    through the process table to obtain a guest real address and then using
    kvm_read_guest or kvm_write_guest to make the access with the guest real
    address.
    
    This method of access however only works for L1 guests and will give the
    incorrect results for a nested guest.
    
    We can however use the store_to_eaddr and load_from_eaddr kvmppc_ops to
    perform the access for a nested guesti (and a L1 guest). So attempt this
    method first and fall back to the old method if this fails and we aren't
    running a nested guest.
    
    At this stage there is no fall back method to perform the access for a
    nested guest and this is left as a future improvement. For now we will
    return to the nested guest and rely on the fact that a translation
    should be faulted in before retrying the access.
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 18dd98f35ac1..ec9cbf9db364 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -331,10 +331,17 @@ int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 {
 	ulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;
 	struct kvmppc_pte pte;
-	int r;
+	int r = -EINVAL;
 
 	vcpu->stat.st++;
 
+	if (vcpu->kvm->arch.kvm_ops && vcpu->kvm->arch.kvm_ops->store_to_eaddr)
+		r = vcpu->kvm->arch.kvm_ops->store_to_eaddr(vcpu, eaddr, ptr,
+							    size);
+
+	if ((!r) || (r == -EAGAIN))
+		return r;
+
 	r = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,
 			 XLATE_WRITE, &pte);
 	if (r < 0)
@@ -367,10 +374,17 @@ int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 {
 	ulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;
 	struct kvmppc_pte pte;
-	int rc;
+	int rc = -EINVAL;
 
 	vcpu->stat.ld++;
 
+	if (vcpu->kvm->arch.kvm_ops && vcpu->kvm->arch.kvm_ops->load_from_eaddr)
+		rc = vcpu->kvm->arch.kvm_ops->load_from_eaddr(vcpu, eaddr, ptr,
+							      size);
+
+	if ((!rc) || (rc == -EAGAIN))
+		return rc;
+
 	rc = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,
 			  XLATE_READ, &pte);
 	if (rc)

commit 693ac10a88a2219bde553b2e8460dbec97e594e6
Author: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
Date:   Fri Dec 14 16:29:03 2018 +1100

    KVM: PPC: Book3S: Only report KVM_CAP_SPAPR_TCE_VFIO on powernv machines
    
    The kvm capability KVM_CAP_SPAPR_TCE_VFIO is used to indicate the
    availability of in kernel tce acceleration for vfio. However it is
    currently the case that this is only available on a powernv machine,
    not for a pseries machine.
    
    Thus make this capability dependent on having the cpu feature
    CPU_FTR_HVMODE.
    
    [paulus@ozlabs.org - fixed compilation for Book E.]
    
    Signed-off-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6a7a6a101efd..18dd98f35ac1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -543,8 +543,11 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_SPAPR_TCE_64:
-		/* fallthrough */
+		r = 1;
+		break;
 	case KVM_CAP_SPAPR_TCE_VFIO:
+		r = !!cpu_has_feature(CPU_FTR_HVMODE);
+		break;
 	case KVM_CAP_PPC_RTAS:
 	case KVM_CAP_PPC_FIXUP_HCALL:
 	case KVM_CAP_PPC_ENABLE_HCALL:

commit f032b73459eed4897bcafee2b1d37a817f1bb596
Author: Bharata B Rao <bharata@linux.ibm.com>
Date:   Wed Dec 12 15:15:30 2018 +1100

    KVM: PPC: Pass change type down to memslot commit function
    
    Currently, kvm_arch_commit_memory_region() gets called with a
    parameter indicating what type of change is being made to the memslot,
    but it doesn't pass it down to the platform-specific memslot commit
    functions.  This adds the `change' parameter to the lower-level
    functions so that they can use it in future.
    
    [paulus@ozlabs.org - fix book E also.]
    
    Signed-off-by: Bharata B Rao <bharata@linux.vnet.ibm.com>
    Reviewed-by: Suraj Jitindar Singh <sjitindarsingh@gmail.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2869a299c4ed..6a7a6a101efd 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -696,7 +696,7 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 				   const struct kvm_memory_slot *new,
 				   enum kvm_mr_change change)
 {
-	kvmppc_core_commit_memory_region(kvm, mem, old, new);
+	kvmppc_core_commit_memory_region(kvm, mem, old, new, change);
 }
 
 void kvm_arch_flush_shadow_memslot(struct kvm *kvm,

commit e5d83c74a5800c2a1fa3ba982c1c4b2b39ae6db2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Feb 16 10:40:56 2017 +0100

    kvm: make KVM_CAP_ENABLE_CAP_VM architecture agnostic
    
    The first such capability to be handled in virt/kvm/ will be manual
    dirty page reprotection.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2869a299c4ed..b1ed31a17a8c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -518,7 +518,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_PPC_IRQ_LEVEL:
 	case KVM_CAP_ENABLE_CAP:
-	case KVM_CAP_ENABLE_CAP_VM:
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
@@ -2084,8 +2083,8 @@ int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,
 }
 
 
-static int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
-				   struct kvm_enable_cap *cap)
+int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
+			    struct kvm_enable_cap *cap)
 {
 	int r;
 
@@ -2273,15 +2272,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
 
 		break;
 	}
-	case KVM_ENABLE_CAP:
-	{
-		struct kvm_enable_cap cap;
-		r = -EFAULT;
-		if (copy_from_user(&cap, argp, sizeof(cap)))
-			goto out;
-		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
-		break;
-	}
 #ifdef CONFIG_SPAPR_TCE_IOMMU
 	case KVM_CREATE_SPAPR_TCE_64: {
 		struct kvm_create_spapr_tce_64 create_tce_64;

commit aa069a996951f3e2e38437ef0316685a5893fc7e
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Sep 21 20:02:01 2018 +1000

    KVM: PPC: Book3S HV: Add a VM capability to enable nested virtualization
    
    With this, userspace can enable a KVM-HV guest to run nested guests
    under it.
    
    The administrator can control whether any nested guests can be run;
    setting the "nested" module parameter to false prevents any guests
    becoming nested hypervisors (that is, any attempt to enable the nested
    capability on a guest will fail).  Guests which are already nested
    hypervisors will continue to be so.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1f4b128894a0..2869a299c4ed 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -597,6 +597,10 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = !!(hv_enabled && cpu_has_feature(CPU_FTR_ARCH_300) &&
 		       cpu_has_feature(CPU_FTR_HVMODE));
 		break;
+	case KVM_CAP_PPC_NESTED_HV:
+		r = !!(hv_enabled && kvmppc_hv_ops->enable_nested &&
+		       !kvmppc_hv_ops->enable_nested(NULL));
+		break;
 #endif
 	case KVM_CAP_SYNC_MMU:
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
@@ -2115,6 +2119,14 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 			r = kvm->arch.kvm_ops->set_smt_mode(kvm, mode, flags);
 		break;
 	}
+
+	case KVM_CAP_PPC_NESTED_HV:
+		r = -EINVAL;
+		if (!is_kvmppc_hv_enabled(kvm) ||
+		    !kvm->arch.kvm_ops->enable_nested)
+			break;
+		r = kvm->arch.kvm_ops->enable_nested(kvm);
+		break;
 #endif
 	default:
 		r = -EINVAL;

commit de760db4d9d7b3e735dbc7e4e3fa2c00909abe7c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:31:16 2018 +1100

    KVM: PPC: Book3S HV: Allow HV module to load without hypervisor mode
    
    With this, the KVM-HV module can be loaded in a guest running under
    KVM-HV, and if the hypervisor supports nested virtualization, this
    guest can now act as a nested hypervisor and run nested guests.
    
    This also adds some checks to inform userspace that HPT guests are not
    supported by nested hypervisors (by returning false for the
    KVM_CAP_PPC_MMU_HASH_V3 capability), and to prevent userspace from
    configuring a guest to use HPT mode.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index eba5756d5b41..1f4b128894a0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -594,7 +594,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = !!(hv_enabled && radix_enabled());
 		break;
 	case KVM_CAP_PPC_MMU_HASH_V3:
-		r = !!(hv_enabled && cpu_has_feature(CPU_FTR_ARCH_300));
+		r = !!(hv_enabled && cpu_has_feature(CPU_FTR_ARCH_300) &&
+		       cpu_has_feature(CPU_FTR_HVMODE));
 		break;
 #endif
 	case KVM_CAP_SYNC_MMU:

commit e61cf2e3a5b452cfefcb145021f5a8ea88735cc1
Merge: 1009aa1205c2 28a1f3ac1d0c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Aug 19 10:38:36 2018 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull first set of KVM updates from Paolo Bonzini:
     "PPC:
       - minor code cleanups
    
      x86:
       - PCID emulation and CR3 caching for shadow page tables
       - nested VMX live migration
       - nested VMCS shadowing
       - optimized IPI hypercall
       - some optimizations
    
      ARM will come next week"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (85 commits)
      kvm: x86: Set highest physical address bits in non-present/reserved SPTEs
      KVM/x86: Use CC_SET()/CC_OUT in arch/x86/kvm/vmx.c
      KVM: X86: Implement PV IPIs in linux guest
      KVM: X86: Add kvm hypervisor init time platform setup callback
      KVM: X86: Implement "send IPI" hypercall
      KVM/x86: Move X86_CR4_OSXSAVE check into kvm_valid_sregs()
      KVM: x86: Skip pae_root shadow allocation if tdp enabled
      KVM/MMU: Combine flushing remote tlb in mmu_set_spte()
      KVM: vmx: skip VMWRITE of HOST_{FS,GS}_BASE when possible
      KVM: vmx: skip VMWRITE of HOST_{FS,GS}_SEL when possible
      KVM: vmx: always initialize HOST_{FS,GS}_BASE to zero during setup
      KVM: vmx: move struct host_state usage to struct loaded_vmcs
      KVM: vmx: compute need to reload FS/GS/LDT on demand
      KVM: nVMX: remove a misleading comment regarding vmcs02 fields
      KVM: vmx: rename __vmx_load_host_state() and vmx_save_host_state()
      KVM: vmx: add dedicated utility to access guest's kernel_gs_base
      KVM: vmx: track host_state.loaded using a loaded_vmcs pointer
      KVM: vmx: refactor segmentation code in vmx_save_host_state()
      kvm: nVMX: Fix fault priority for VMX operations
      kvm: nVMX: Fix fault vector for VMX operation at CPL > 0
      ...

commit 45ef5992e06dcc3a4c7d34d23052289c7676d56c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:25:19 2018 +0000

    powerpc: remove unnecessary inclusion of asm/tlbflush.h
    
    asm/tlbflush.h is only needed for:
    - using functions xxx_flush_tlb_xxx()
    - using MMU_NO_CONTEXT
    - including asm-generic/pgtable.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0e8c20c5eaac..3ccc386b380d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -33,7 +33,6 @@
 #include <asm/cputable.h>
 #include <linux/uaccess.h>
 #include <asm/kvm_ppc.h>
-#include <asm/tlbflush.h>
 #include <asm/cputhreads.h>
 #include <asm/irqflags.h>
 #include <asm/iommu.h>

commit 4eeb85568e5653c71d901f7593d3f3e7e2a5414f
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 28 09:48:26 2018 +0800

    KVM: PPC: Remove mmio_vsx_tx_sx_enabled in KVM MMIO emulation
    
    Originally PPC KVM MMIO emulation uses only 0~31#(5 bits) for VSR
    reg number, and use mmio_vsx_tx_sx_enabled field together for
    0~63# VSR regs.
    
    Currently PPC KVM MMIO emulation is reimplemented with analyse_instr()
    assistance.  analyse_instr() returns 0~63 for VSR register number, so
    it is not necessary to use additional mmio_vsx_tx_sx_enabled field
    any more.
    
    This patch extends related reg bits (expand io_gpr to u16 from u8
    and use 6 bits for VSR reg#), so that mmio_vsx_tx_sx_enabled can
    be removed.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0e8c20c5eaac..b6fd7c5b8d1a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -880,10 +880,10 @@ static inline void kvmppc_set_vsr_dword(struct kvm_vcpu *vcpu,
 	if (offset == -1)
 		return;
 
-	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
-		val.vval = VCPU_VSX_VR(vcpu, index);
+	if (index >= 32) {
+		val.vval = VCPU_VSX_VR(vcpu, index - 32);
 		val.vsxval[offset] = gpr;
-		VCPU_VSX_VR(vcpu, index) = val.vval;
+		VCPU_VSX_VR(vcpu, index - 32) = val.vval;
 	} else {
 		VCPU_VSX_FPR(vcpu, index, offset) = gpr;
 	}
@@ -895,11 +895,11 @@ static inline void kvmppc_set_vsr_dword_dump(struct kvm_vcpu *vcpu,
 	union kvmppc_one_reg val;
 	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
 
-	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
-		val.vval = VCPU_VSX_VR(vcpu, index);
+	if (index >= 32) {
+		val.vval = VCPU_VSX_VR(vcpu, index - 32);
 		val.vsxval[0] = gpr;
 		val.vsxval[1] = gpr;
-		VCPU_VSX_VR(vcpu, index) = val.vval;
+		VCPU_VSX_VR(vcpu, index - 32) = val.vval;
 	} else {
 		VCPU_VSX_FPR(vcpu, index, 0) = gpr;
 		VCPU_VSX_FPR(vcpu, index, 1) = gpr;
@@ -912,12 +912,12 @@ static inline void kvmppc_set_vsr_word_dump(struct kvm_vcpu *vcpu,
 	union kvmppc_one_reg val;
 	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
 
-	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
+	if (index >= 32) {
 		val.vsx32val[0] = gpr;
 		val.vsx32val[1] = gpr;
 		val.vsx32val[2] = gpr;
 		val.vsx32val[3] = gpr;
-		VCPU_VSX_VR(vcpu, index) = val.vval;
+		VCPU_VSX_VR(vcpu, index - 32) = val.vval;
 	} else {
 		val.vsx32val[0] = gpr;
 		val.vsx32val[1] = gpr;
@@ -937,10 +937,10 @@ static inline void kvmppc_set_vsr_word(struct kvm_vcpu *vcpu,
 	if (offset == -1)
 		return;
 
-	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
-		val.vval = VCPU_VSX_VR(vcpu, index);
+	if (index >= 32) {
+		val.vval = VCPU_VSX_VR(vcpu, index - 32);
 		val.vsx32val[offset] = gpr32;
-		VCPU_VSX_VR(vcpu, index) = val.vval;
+		VCPU_VSX_VR(vcpu, index - 32) = val.vval;
 	} else {
 		dword_offset = offset / 2;
 		word_offset = offset % 2;
@@ -1361,10 +1361,10 @@ static inline int kvmppc_get_vsr_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
 			break;
 		}
 
-		if (!vcpu->arch.mmio_vsx_tx_sx_enabled) {
+		if (rs < 32) {
 			*val = VCPU_VSX_FPR(vcpu, rs, vsx_offset);
 		} else {
-			reg.vval = VCPU_VSX_VR(vcpu, rs);
+			reg.vval = VCPU_VSX_VR(vcpu, rs - 32);
 			*val = reg.vsxval[vsx_offset];
 		}
 		break;
@@ -1378,13 +1378,13 @@ static inline int kvmppc_get_vsr_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
 			break;
 		}
 
-		if (!vcpu->arch.mmio_vsx_tx_sx_enabled) {
+		if (rs < 32) {
 			dword_offset = vsx_offset / 2;
 			word_offset = vsx_offset % 2;
 			reg.vsxval[0] = VCPU_VSX_FPR(vcpu, rs, dword_offset);
 			*val = reg.vsx32val[word_offset];
 		} else {
-			reg.vval = VCPU_VSX_VR(vcpu, rs);
+			reg.vval = VCPU_VSX_VR(vcpu, rs - 32);
 			*val = reg.vsx32val[vsx_offset];
 		}
 		break;

commit 09027ab73b82ec773f6ab8ae9468d7e15b748dfa
Merge: 273ba45796c1 f61e0d3cc4ae
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Thu Jun 14 17:42:54 2018 +0200

    Merge tag 'kvm-ppc-next-4.18-2' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD

commit 1499fa809e9e6713952ef84a7e9d51606881681f
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Thu Apr 19 00:49:58 2018 +0530

    kvm: Change return type to vm_fault_t
    
    Use new return type vm_fault_t for fault handler. For
    now, this is just documenting that the function returns
    a VM_FAULT value rather than an errno. Once all instances
    are converted, vm_fault_t will become a distinct type.
    
    commit 1c8f422059ae ("mm: change return type to vm_fault_t")
    
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Reviewed-by: Matthew Wilcox <mawilcox@microsoft.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4e387647b5af..3764d000872e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1830,7 +1830,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	return r;
 }
 
-int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
+vm_fault_t kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 {
 	return VM_FAULT_SIGBUS;
 }

commit c8235c2891d0561960fd5d62fdda58cfb6e503b2
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Wed May 23 15:02:10 2018 +0800

    KVM: PPC: Remove load/put vcpu for KVM_GET/SET_ONE_REG ioctl
    
    Since the vcpu mutex locking/unlock has been moved out of vcpu_load()
    /vcpu_put(), KVM_GET_ONE_REG and KVM_SET_ONE_REG doesn't need to do
    ioctl with loading vcpu anymore. This patch removes vcpu_load()/vcpu_put()
    from KVM_GET_ONE_REG and KVM_SET_ONE_REG ioctl.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index c2c3477af746..4cb377605167 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1998,14 +1998,12 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	{
 		struct kvm_one_reg reg;
 		r = -EFAULT;
-		vcpu_load(vcpu);
 		if (copy_from_user(&reg, argp, sizeof(reg)))
 			goto out;
 		if (ioctl == KVM_SET_ONE_REG)
 			r = kvm_vcpu_ioctl_set_one_reg(vcpu, &reg);
 		else
 			r = kvm_vcpu_ioctl_get_one_reg(vcpu, &reg);
-		vcpu_put(vcpu);
 		break;
 	}
 

commit b3cebfe8c1cadf1817939dcc3688a2504a69c662
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Wed May 23 15:02:09 2018 +0800

    KVM: PPC: Move vcpu_load/vcpu_put down to each ioctl case in kvm_arch_vcpu_ioctl
    
    Although we already have kvm_arch_vcpu_async_ioctl() which doesn't require
    ioctl to load vcpu, the sync ioctl code need to be cleaned up when
    CONFIG_HAVE_KVM_VCPU_ASYNC_IOCTL is not configured.
    
    This patch moves vcpu_load/vcpu_put down to each ioctl switch case so that
    each ioctl can decide to do vcpu_load/vcpu_put or not independently.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b8247fac3e56..c2c3477af746 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1980,16 +1980,16 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 	long r;
 
-	vcpu_load(vcpu);
-
 	switch (ioctl) {
 	case KVM_ENABLE_CAP:
 	{
 		struct kvm_enable_cap cap;
 		r = -EFAULT;
+		vcpu_load(vcpu);
 		if (copy_from_user(&cap, argp, sizeof(cap)))
 			goto out;
 		r = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);
+		vcpu_put(vcpu);
 		break;
 	}
 
@@ -1998,12 +1998,14 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	{
 		struct kvm_one_reg reg;
 		r = -EFAULT;
+		vcpu_load(vcpu);
 		if (copy_from_user(&reg, argp, sizeof(reg)))
 			goto out;
 		if (ioctl == KVM_SET_ONE_REG)
 			r = kvm_vcpu_ioctl_set_one_reg(vcpu, &reg);
 		else
 			r = kvm_vcpu_ioctl_get_one_reg(vcpu, &reg);
+		vcpu_put(vcpu);
 		break;
 	}
 
@@ -2011,9 +2013,11 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	case KVM_DIRTY_TLB: {
 		struct kvm_dirty_tlb dirty;
 		r = -EFAULT;
+		vcpu_load(vcpu);
 		if (copy_from_user(&dirty, argp, sizeof(dirty)))
 			goto out;
 		r = kvm_vcpu_ioctl_dirty_tlb(vcpu, &dirty);
+		vcpu_put(vcpu);
 		break;
 	}
 #endif
@@ -2022,7 +2026,6 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 
 out:
-	vcpu_put(vcpu);
 	return r;
 }
 

commit d234d68eb7464c9ebd11e870404e83d3e9348406
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Wed May 23 15:02:08 2018 +0800

    KVM: PPC: Book3S PR: Enable HTM for PR KVM for KVM_CHECK_EXTENSION ioctl
    
    With current patch set, PR KVM now supports HTM. So this patch turns it
    on for PR KVM.
    
    Tested with:
    https://github.com/justdoitqd/publicFiles/blob/master/test_kvm_htm_cap.c
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 05eccdc10fdd..b8247fac3e56 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -648,9 +648,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #endif
 #ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	case KVM_CAP_PPC_HTM:
-		r = hv_enabled &&
-		    (!!(cur_cpu_spec->cpu_user_features2 & PPC_FEATURE2_HTM) ||
-		     cpu_has_feature(CPU_FTR_P9_TM_HV_ASSIST));
+		r = !!(cur_cpu_spec->cpu_user_features2 & PPC_FEATURE2_HTM) ||
+		     (hv_enabled && cpu_has_feature(CPU_FTR_P9_TM_HV_ASSIST));
 		break;
 #endif
 	default:

commit acc9eb9305fecd958e2877c4e6cd3284d01c2e82
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:26 2018 +0800

    KVM: PPC: Reimplement LOAD_VMX/STORE_VMX instruction mmio emulation with analyse_instr() input
    
    This patch reimplements LOAD_VMX/STORE_VMX MMIO emulation with
    analyse_instr() input. When emulating the store, the VMX reg will need to
    be flushed so that the right reg val can be retrieved before writing to
    IO MEM.
    
    This patch also adds support for lvebx/lvehx/lvewx/stvebx/stvehx/stvewx
    MMIO emulation. To meet the requirement of handling different element
    sizes, kvmppc_handle_load128_by2x64()/kvmppc_handle_store128_by2x64()
    were replaced with kvmppc_handle_vmx_load()/kvmppc_handle_vmx_store().
    
    The framework used is similar to VSX instruction MMIO emulation.
    
    Suggested-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1580bd24bc74..05eccdc10fdd 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -953,30 +953,110 @@ static inline void kvmppc_set_vsr_word(struct kvm_vcpu *vcpu,
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_ALTIVEC
+static inline int kvmppc_get_vmx_offset_generic(struct kvm_vcpu *vcpu,
+		int index, int element_size)
+{
+	int offset;
+	int elts = sizeof(vector128)/element_size;
+
+	if ((index < 0) || (index >= elts))
+		return -1;
+
+	if (kvmppc_need_byteswap(vcpu))
+		offset = elts - index - 1;
+	else
+		offset = index;
+
+	return offset;
+}
+
+static inline int kvmppc_get_vmx_dword_offset(struct kvm_vcpu *vcpu,
+		int index)
+{
+	return kvmppc_get_vmx_offset_generic(vcpu, index, 8);
+}
+
+static inline int kvmppc_get_vmx_word_offset(struct kvm_vcpu *vcpu,
+		int index)
+{
+	return kvmppc_get_vmx_offset_generic(vcpu, index, 4);
+}
+
+static inline int kvmppc_get_vmx_hword_offset(struct kvm_vcpu *vcpu,
+		int index)
+{
+	return kvmppc_get_vmx_offset_generic(vcpu, index, 2);
+}
+
+static inline int kvmppc_get_vmx_byte_offset(struct kvm_vcpu *vcpu,
+		int index)
+{
+	return kvmppc_get_vmx_offset_generic(vcpu, index, 1);
+}
+
+
 static inline void kvmppc_set_vmx_dword(struct kvm_vcpu *vcpu,
-		u64 gpr)
+	u64 gpr)
 {
+	union kvmppc_one_reg val;
+	int offset = kvmppc_get_vmx_dword_offset(vcpu,
+			vcpu->arch.mmio_vmx_offset);
 	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
-	u32 hi, lo;
-	u32 di;
 
-#ifdef __BIG_ENDIAN
-	hi = gpr >> 32;
-	lo = gpr & 0xffffffff;
-#else
-	lo = gpr >> 32;
-	hi = gpr & 0xffffffff;
-#endif
+	if (offset == -1)
+		return;
+
+	val.vval = VCPU_VSX_VR(vcpu, index);
+	val.vsxval[offset] = gpr;
+	VCPU_VSX_VR(vcpu, index) = val.vval;
+}
+
+static inline void kvmppc_set_vmx_word(struct kvm_vcpu *vcpu,
+	u32 gpr32)
+{
+	union kvmppc_one_reg val;
+	int offset = kvmppc_get_vmx_word_offset(vcpu,
+			vcpu->arch.mmio_vmx_offset);
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
+
+	if (offset == -1)
+		return;
+
+	val.vval = VCPU_VSX_VR(vcpu, index);
+	val.vsx32val[offset] = gpr32;
+	VCPU_VSX_VR(vcpu, index) = val.vval;
+}
 
-	di = 2 - vcpu->arch.mmio_vmx_copy_nums;		/* doubleword index */
-	if (di > 1)
+static inline void kvmppc_set_vmx_hword(struct kvm_vcpu *vcpu,
+	u16 gpr16)
+{
+	union kvmppc_one_reg val;
+	int offset = kvmppc_get_vmx_hword_offset(vcpu,
+			vcpu->arch.mmio_vmx_offset);
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
+
+	if (offset == -1)
 		return;
 
-	if (vcpu->arch.mmio_host_swabbed)
-		di = 1 - di;
+	val.vval = VCPU_VSX_VR(vcpu, index);
+	val.vsx16val[offset] = gpr16;
+	VCPU_VSX_VR(vcpu, index) = val.vval;
+}
+
+static inline void kvmppc_set_vmx_byte(struct kvm_vcpu *vcpu,
+	u8 gpr8)
+{
+	union kvmppc_one_reg val;
+	int offset = kvmppc_get_vmx_byte_offset(vcpu,
+			vcpu->arch.mmio_vmx_offset);
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
 
-	VCPU_VSX_VR(vcpu, index).u[di * 2] = hi;
-	VCPU_VSX_VR(vcpu, index).u[di * 2 + 1] = lo;
+	if (offset == -1)
+		return;
+
+	val.vval = VCPU_VSX_VR(vcpu, index);
+	val.vsx8val[offset] = gpr8;
+	VCPU_VSX_VR(vcpu, index) = val.vval;
 }
 #endif /* CONFIG_ALTIVEC */
 
@@ -1097,7 +1177,16 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		if (vcpu->kvm->arch.kvm_ops->giveup_ext)
 			vcpu->kvm->arch.kvm_ops->giveup_ext(vcpu, MSR_VEC);
 
-		kvmppc_set_vmx_dword(vcpu, gpr);
+		if (vcpu->arch.mmio_copy_type == KVMPPC_VMX_COPY_DWORD)
+			kvmppc_set_vmx_dword(vcpu, gpr);
+		else if (vcpu->arch.mmio_copy_type == KVMPPC_VMX_COPY_WORD)
+			kvmppc_set_vmx_word(vcpu, gpr);
+		else if (vcpu->arch.mmio_copy_type ==
+				KVMPPC_VMX_COPY_HWORD)
+			kvmppc_set_vmx_hword(vcpu, gpr);
+		else if (vcpu->arch.mmio_copy_type ==
+				KVMPPC_VMX_COPY_BYTE)
+			kvmppc_set_vmx_byte(vcpu, gpr);
 		break;
 #endif
 	default:
@@ -1376,14 +1465,16 @@ static int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,
 #endif /* CONFIG_VSX */
 
 #ifdef CONFIG_ALTIVEC
-/* handle quadword load access in two halves */
-int kvmppc_handle_load128_by2x64(struct kvm_run *run, struct kvm_vcpu *vcpu,
-		unsigned int rt, int is_default_endian)
+int kvmppc_handle_vmx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+		unsigned int rt, unsigned int bytes, int is_default_endian)
 {
 	enum emulation_result emulated = EMULATE_DONE;
 
+	if (vcpu->arch.mmio_vsx_copy_nums > 2)
+		return EMULATE_FAIL;
+
 	while (vcpu->arch.mmio_vmx_copy_nums) {
-		emulated = __kvmppc_handle_load(run, vcpu, rt, 8,
+		emulated = __kvmppc_handle_load(run, vcpu, rt, bytes,
 				is_default_endian, 0);
 
 		if (emulated != EMULATE_DONE)
@@ -1391,55 +1482,127 @@ int kvmppc_handle_load128_by2x64(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 		vcpu->arch.paddr_accessed += run->mmio.len;
 		vcpu->arch.mmio_vmx_copy_nums--;
+		vcpu->arch.mmio_vmx_offset++;
 	}
 
 	return emulated;
 }
 
-static inline int kvmppc_get_vmx_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
+int kvmppc_get_vmx_dword(struct kvm_vcpu *vcpu, int index, u64 *val)
 {
-	vector128 vrs = VCPU_VSX_VR(vcpu, rs);
-	u32 di;
-	u64 w0, w1;
+	union kvmppc_one_reg reg;
+	int vmx_offset = 0;
+	int result = 0;
+
+	vmx_offset =
+		kvmppc_get_vmx_dword_offset(vcpu, vcpu->arch.mmio_vmx_offset);
 
-	di = 2 - vcpu->arch.mmio_vmx_copy_nums;		/* doubleword index */
-	if (di > 1)
+	if (vmx_offset == -1)
 		return -1;
 
-	if (kvmppc_need_byteswap(vcpu))
-		di = 1 - di;
+	reg.vval = VCPU_VSX_VR(vcpu, index);
+	*val = reg.vsxval[vmx_offset];
 
-	w0 = vrs.u[di * 2];
-	w1 = vrs.u[di * 2 + 1];
+	return result;
+}
 
-#ifdef __BIG_ENDIAN
-	*val = (w0 << 32) | w1;
-#else
-	*val = (w1 << 32) | w0;
-#endif
-	return 0;
+int kvmppc_get_vmx_word(struct kvm_vcpu *vcpu, int index, u64 *val)
+{
+	union kvmppc_one_reg reg;
+	int vmx_offset = 0;
+	int result = 0;
+
+	vmx_offset =
+		kvmppc_get_vmx_word_offset(vcpu, vcpu->arch.mmio_vmx_offset);
+
+	if (vmx_offset == -1)
+		return -1;
+
+	reg.vval = VCPU_VSX_VR(vcpu, index);
+	*val = reg.vsx32val[vmx_offset];
+
+	return result;
+}
+
+int kvmppc_get_vmx_hword(struct kvm_vcpu *vcpu, int index, u64 *val)
+{
+	union kvmppc_one_reg reg;
+	int vmx_offset = 0;
+	int result = 0;
+
+	vmx_offset =
+		kvmppc_get_vmx_hword_offset(vcpu, vcpu->arch.mmio_vmx_offset);
+
+	if (vmx_offset == -1)
+		return -1;
+
+	reg.vval = VCPU_VSX_VR(vcpu, index);
+	*val = reg.vsx16val[vmx_offset];
+
+	return result;
+}
+
+int kvmppc_get_vmx_byte(struct kvm_vcpu *vcpu, int index, u64 *val)
+{
+	union kvmppc_one_reg reg;
+	int vmx_offset = 0;
+	int result = 0;
+
+	vmx_offset =
+		kvmppc_get_vmx_byte_offset(vcpu, vcpu->arch.mmio_vmx_offset);
+
+	if (vmx_offset == -1)
+		return -1;
+
+	reg.vval = VCPU_VSX_VR(vcpu, index);
+	*val = reg.vsx8val[vmx_offset];
+
+	return result;
 }
 
-/* handle quadword store in two halves */
-int kvmppc_handle_store128_by2x64(struct kvm_run *run, struct kvm_vcpu *vcpu,
-		unsigned int rs, int is_default_endian)
+int kvmppc_handle_vmx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+		unsigned int rs, unsigned int bytes, int is_default_endian)
 {
 	u64 val = 0;
+	unsigned int index = rs & KVM_MMIO_REG_MASK;
 	enum emulation_result emulated = EMULATE_DONE;
 
+	if (vcpu->arch.mmio_vsx_copy_nums > 2)
+		return EMULATE_FAIL;
+
 	vcpu->arch.io_gpr = rs;
 
 	while (vcpu->arch.mmio_vmx_copy_nums) {
-		if (kvmppc_get_vmx_data(vcpu, rs, &val) == -1)
+		switch (vcpu->arch.mmio_copy_type) {
+		case KVMPPC_VMX_COPY_DWORD:
+			if (kvmppc_get_vmx_dword(vcpu, index, &val) == -1)
+				return EMULATE_FAIL;
+
+			break;
+		case KVMPPC_VMX_COPY_WORD:
+			if (kvmppc_get_vmx_word(vcpu, index, &val) == -1)
+				return EMULATE_FAIL;
+			break;
+		case KVMPPC_VMX_COPY_HWORD:
+			if (kvmppc_get_vmx_hword(vcpu, index, &val) == -1)
+				return EMULATE_FAIL;
+			break;
+		case KVMPPC_VMX_COPY_BYTE:
+			if (kvmppc_get_vmx_byte(vcpu, index, &val) == -1)
+				return EMULATE_FAIL;
+			break;
+		default:
 			return EMULATE_FAIL;
+		}
 
-		emulated = kvmppc_handle_store(run, vcpu, val, 8,
+		emulated = kvmppc_handle_store(run, vcpu, val, bytes,
 				is_default_endian);
 		if (emulated != EMULATE_DONE)
 			break;
 
 		vcpu->arch.paddr_accessed += run->mmio.len;
 		vcpu->arch.mmio_vmx_copy_nums--;
+		vcpu->arch.mmio_vmx_offset++;
 	}
 
 	return emulated;
@@ -1454,11 +1617,11 @@ static int kvmppc_emulate_mmio_vmx_loadstore(struct kvm_vcpu *vcpu,
 	vcpu->arch.paddr_accessed += run->mmio.len;
 
 	if (!vcpu->mmio_is_write) {
-		emulated = kvmppc_handle_load128_by2x64(run, vcpu,
-				vcpu->arch.io_gpr, 1);
+		emulated = kvmppc_handle_vmx_load(run, vcpu,
+				vcpu->arch.io_gpr, run->mmio.len, 1);
 	} else {
-		emulated = kvmppc_handle_store128_by2x64(run, vcpu,
-				vcpu->arch.io_gpr, 1);
+		emulated = kvmppc_handle_vmx_store(run, vcpu,
+				vcpu->arch.io_gpr, run->mmio.len, 1);
 	}
 
 	switch (emulated) {
@@ -1602,8 +1765,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		}
 #endif
 #ifdef CONFIG_ALTIVEC
-		if (vcpu->arch.mmio_vmx_copy_nums > 0)
+		if (vcpu->arch.mmio_vmx_copy_nums > 0) {
 			vcpu->arch.mmio_vmx_copy_nums--;
+			vcpu->arch.mmio_vmx_offset++;
+		}
 
 		if (vcpu->arch.mmio_vmx_copy_nums > 0) {
 			r = kvmppc_emulate_mmio_vmx_loadstore(vcpu, run);

commit da2a32b876e979d74f84746ae8d066e1d54b568f
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:25 2018 +0800

    KVM: PPC: Expand mmio_vsx_copy_type to cover VMX load/store element types
    
    VSX MMIO emulation uses mmio_vsx_copy_type to represent VSX emulated
    element size/type, such as KVMPPC_VSX_COPY_DWORD_LOAD, etc. This
    patch expands mmio_vsx_copy_type to cover VMX copy type, such as
    KVMPPC_VMX_COPY_BYTE(stvebx/lvebx), etc. As a result,
    mmio_vsx_copy_type is also renamed to mmio_copy_type.
    
    It is a preparation for reimplementing VMX MMIO emulation.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8ce9e7ba5fed..1580bd24bc74 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1080,14 +1080,14 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		if (vcpu->kvm->arch.kvm_ops->giveup_ext)
 			vcpu->kvm->arch.kvm_ops->giveup_ext(vcpu, MSR_VSX);
 
-		if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_DWORD)
+		if (vcpu->arch.mmio_copy_type == KVMPPC_VSX_COPY_DWORD)
 			kvmppc_set_vsr_dword(vcpu, gpr);
-		else if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_WORD)
+		else if (vcpu->arch.mmio_copy_type == KVMPPC_VSX_COPY_WORD)
 			kvmppc_set_vsr_word(vcpu, gpr);
-		else if (vcpu->arch.mmio_vsx_copy_type ==
+		else if (vcpu->arch.mmio_copy_type ==
 				KVMPPC_VSX_COPY_DWORD_LOAD_DUMP)
 			kvmppc_set_vsr_dword_dump(vcpu, gpr);
-		else if (vcpu->arch.mmio_vsx_copy_type ==
+		else if (vcpu->arch.mmio_copy_type ==
 				KVMPPC_VSX_COPY_WORD_LOAD_DUMP)
 			kvmppc_set_vsr_word_dump(vcpu, gpr);
 		break;
@@ -1260,7 +1260,7 @@ static inline int kvmppc_get_vsr_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
 	u32 dword_offset, word_offset;
 	union kvmppc_one_reg reg;
 	int vsx_offset = 0;
-	int copy_type = vcpu->arch.mmio_vsx_copy_type;
+	int copy_type = vcpu->arch.mmio_copy_type;
 	int result = 0;
 
 	switch (copy_type) {

commit 2e6baa46b4ae785e3e954aaf9d2e8a0bb06ad33a
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:22 2018 +0800

    KVM: PPC: Add giveup_ext() hook to PPC KVM ops
    
    Currently HV will save math regs(FP/VEC/VSX) when trap into host. But
    PR KVM will only save math regs when qemu task switch out of CPU, or
    when returning from qemu code.
    
    To emulate FP/VEC/VSX mmio load, PR KVM need to make sure that math
    regs were flushed firstly and then be able to update saved VCPU
    FPR/VEC/VSX area reasonably.
    
    This patch adds giveup_ext() field to KVM ops. Only PR KVM has non-NULL
    giveup_ext() ops. kvmppc_complete_mmio_load() can invoke that hook
    (when not NULL) to flush math regs accordingly, before updating saved
    register vals.
    
    Math regs flush is also necessary for STORE, which will be covered
    in later patch within this patch series.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 45daf3b9d9d2..8ce9e7ba5fed 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1061,6 +1061,9 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
 		break;
 	case KVM_MMIO_REG_FPR:
+		if (vcpu->kvm->arch.kvm_ops->giveup_ext)
+			vcpu->kvm->arch.kvm_ops->giveup_ext(vcpu, MSR_FP);
+
 		VCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;
 		break;
 #ifdef CONFIG_PPC_BOOK3S
@@ -1074,6 +1077,9 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 #endif
 #ifdef CONFIG_VSX
 	case KVM_MMIO_REG_VSX:
+		if (vcpu->kvm->arch.kvm_ops->giveup_ext)
+			vcpu->kvm->arch.kvm_ops->giveup_ext(vcpu, MSR_VSX);
+
 		if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_DWORD)
 			kvmppc_set_vsr_dword(vcpu, gpr);
 		else if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_WORD)
@@ -1088,6 +1094,9 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 #endif
 #ifdef CONFIG_ALTIVEC
 	case KVM_MMIO_REG_VMX:
+		if (vcpu->kvm->arch.kvm_ops->giveup_ext)
+			vcpu->kvm->arch.kvm_ops->giveup_ext(vcpu, MSR_VEC);
+
 		kvmppc_set_vmx_dword(vcpu, gpr);
 		break;
 #endif

commit 94dd7fa1c0b75e909fa54d86ce2d1aaf2c54ceb7
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 21 13:24:20 2018 +0800

    KVM: PPC: Add KVMPPC_VSX_COPY_WORD_LOAD_DUMP type support for mmio emulation
    
    Some VSX instructions like lxvwsx will splat word into VSR. This patch
    adds a new VSX copy type KVMPPC_VSX_COPY_WORD_LOAD_DUMP to support this.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Reviewed-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index bef27b16d233..45daf3b9d9d2 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -907,6 +907,26 @@ static inline void kvmppc_set_vsr_dword_dump(struct kvm_vcpu *vcpu,
 	}
 }
 
+static inline void kvmppc_set_vsr_word_dump(struct kvm_vcpu *vcpu,
+	u32 gpr)
+{
+	union kvmppc_one_reg val;
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
+
+	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
+		val.vsx32val[0] = gpr;
+		val.vsx32val[1] = gpr;
+		val.vsx32val[2] = gpr;
+		val.vsx32val[3] = gpr;
+		VCPU_VSX_VR(vcpu, index) = val.vval;
+	} else {
+		val.vsx32val[0] = gpr;
+		val.vsx32val[1] = gpr;
+		VCPU_VSX_FPR(vcpu, index, 0) = val.vsxval[0];
+		VCPU_VSX_FPR(vcpu, index, 1) = val.vsxval[0];
+	}
+}
+
 static inline void kvmppc_set_vsr_word(struct kvm_vcpu *vcpu,
 	u32 gpr32)
 {
@@ -1061,6 +1081,9 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		else if (vcpu->arch.mmio_vsx_copy_type ==
 				KVMPPC_VSX_COPY_DWORD_LOAD_DUMP)
 			kvmppc_set_vsr_dword_dump(vcpu, gpr);
+		else if (vcpu->arch.mmio_vsx_copy_type ==
+				KVMPPC_VSX_COPY_WORD_LOAD_DUMP)
+			kvmppc_set_vsr_word_dump(vcpu, gpr);
 		break;
 #endif
 #ifdef CONFIG_ALTIVEC

commit f19d1f367a506bc645f8d6695942b8873fc82c84
Author: Simon Guo <wei.guo.simon@gmail.com>
Date:   Mon May 7 14:20:09 2018 +0800

    KVM: PPC: Fix a mmio_host_swabbed uninitialized usage issue
    
    When KVM emulates VMX store, it will invoke kvmppc_get_vmx_data() to
    retrieve VMX reg val. kvmppc_get_vmx_data() will check mmio_host_swabbed
    to decide which double word of vr[] to be used. But the
    mmio_host_swabbed can be uninitialized during VMX store procedure:
    
    kvmppc_emulate_loadstore
            \- kvmppc_handle_store128_by2x64
                    \- kvmppc_get_vmx_data
    
    So vcpu->arch.mmio_host_swabbed is not meant to be used at all for
    emulation of store instructions, and this patch makes that true for
    VMX stores. This patch also initializes mmio_host_swabbed to avoid
    possible future problems.
    
    Signed-off-by: Simon Guo <wei.guo.simon@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4e387647b5af..bef27b16d233 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1374,7 +1374,7 @@ static inline int kvmppc_get_vmx_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
 	if (di > 1)
 		return -1;
 
-	if (vcpu->arch.mmio_host_swabbed)
+	if (kvmppc_need_byteswap(vcpu))
 		di = 1 - di;
 
 	w0 = vrs.u[di * 2];

commit a26cf1c9fe3c2e3b671b490aeb708ea72fb5ac0a
Merge: 78e5dfea84dc 681c617b7c42
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Mar 24 08:43:18 2018 +1100

    Merge branch 'topic/ppc-kvm' into next
    
    This brings in two series from Paul, one of which touches KVM code and
    may need to be merged into the kvm-ppc tree to resolve conflicts.

commit 4bb3c7a0208fc13ca70598efd109901a7cd45ae7
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Mar 21 21:32:01 2018 +1100

    KVM: PPC: Book3S HV: Work around transactional memory bugs in POWER9
    
    POWER9 has hardware bugs relating to transactional memory and thread
    reconfiguration (changes to hardware SMT mode).  Specifically, the core
    does not have enough storage to store a complete checkpoint of all the
    architected state for all four threads.  The DD2.2 version of POWER9
    includes hardware modifications designed to allow hypervisor software
    to implement workarounds for these problems.  This patch implements
    those workarounds in KVM code so that KVM guests see a full, working
    transactional memory implementation.
    
    The problems center around the use of TM suspended state, where the
    CPU has a checkpointed state but execution is not transactional.  The
    workaround is to implement a "fake suspend" state, which looks to the
    guest like suspended state but the CPU does not store a checkpoint.
    In this state, any instruction that would cause a transition to
    transactional state (rfid, rfebb, mtmsrd, tresume) or would use the
    checkpointed state (treclaim) causes a "soft patch" interrupt (vector
    0x1500) to the hypervisor so that it can be emulated.  The trechkpt
    instruction also causes a soft patch interrupt.
    
    On POWER9 DD2.2, we avoid returning to the guest in any state which
    would require a checkpoint to be present.  The trechkpt in the guest
    entry path which would normally create that checkpoint is replaced by
    either a transition to fake suspend state, if the guest is in suspend
    state, or a rollback to the pre-transactional state if the guest is in
    transactional state.  Fake suspend state is indicated by a flag in the
    PACA plus a new bit in the PSSCR.  The new PSSCR bit is write-only and
    reads back as 0.
    
    On exit from the guest, if the guest is in fake suspend state, we still
    do the treclaim instruction as we would in real suspend state, in order
    to get into non-transactional state, but we do not save the resulting
    register state since there was no checkpoint.
    
    Emulation of the instructions that cause a softpatch interrupt is
    handled in two paths.  If the guest is in real suspend mode, we call
    kvmhv_p9_tm_emulation_early() to handle the cases where the guest is
    transitioning to transactional state.  This is called before we do the
    treclaim in the guest exit path; because we haven't done treclaim, we
    can get back to the guest with the transaction still active.  If the
    instruction is a case that kvmhv_p9_tm_emulation_early() doesn't
    handle, or if the guest is in fake suspend state, then we proceed to
    do the complete guest exit path and subsequently call
    kvmhv_p9_tm_emulation() in host context with the MMU on.  This handles
    all the cases including the cases that generate program interrupts
    (illegal instruction or TM Bad Thing) and facility unavailable
    interrupts.
    
    The emulation is reasonably straightforward and is mostly concerned
    with checking for exception conditions and updating the state of
    registers such as MSR and CR0.  The treclaim emulation takes care to
    ensure that the TEXASR register gets updated as if it were the guest
    treclaim instruction that had done failure recording, not the treclaim
    done in hypervisor state in the guest exit path.
    
    With this, the KVM_CAP_PPC_HTM capability returns true (1) even if
    transactional memory is not available to host userspace.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 403e642c78f5..677b98e6650f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -646,10 +646,13 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = hv_enabled;
 		break;
 #endif
+#ifdef CONFIG_PPC_TRANSACTIONAL_MEM
 	case KVM_CAP_PPC_HTM:
 		r = hv_enabled &&
-		    (cur_cpu_spec->cpu_user_features2 & PPC_FEATURE2_HTM_COMP);
+		    (!!(cur_cpu_spec->cpu_user_features2 & PPC_FEATURE2_HTM) ||
+		     cpu_has_feature(CPU_FTR_P9_TM_HV_ASSIST));
 		break;
+#endif
 	default:
 		r = 0;
 		break;

commit 6df3877fc962c2bb3d0438633dfd24a185af6838
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue Feb 13 15:45:21 2018 +1100

    KVM: PPC: Book3S: Fix compile error that occurs with some gcc versions
    
    Some versions of gcc generate a warning that the variable "emulated"
    may be used uninitialized in function kvmppc_handle_load128_by2x64().
    It would be used uninitialized if kvmppc_handle_load128_by2x64 was
    ever called with vcpu->arch.mmio_vmx_copy_nums == 0, but neither of
    the callers ever do that, so there is no actual bug.  When gcc
    generates a warning, it causes the build to fail because arch/powerpc
    is compiled with -Werror.
    
    This silences the warning by initializing "emulated" to EMULATE_DONE.
    
    Fixes: 09f984961c13 ("KVM: PPC: Book3S: Add MMIO emulation for VMX instructions")
    Reported-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0083142c2f84..52c205373986 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1345,7 +1345,7 @@ static int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,
 int kvmppc_handle_load128_by2x64(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		unsigned int rt, int is_default_endian)
 {
-	enum emulation_result emulated;
+	enum emulation_result emulated = EMULATE_DONE;
 
 	while (vcpu->arch.mmio_vmx_copy_nums) {
 		emulated = __kvmppc_handle_load(run, vcpu, rt, 8,

commit c662f77331c98018ed256501557b4dd67133fbd7
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue Feb 13 15:16:01 2018 +1100

    KVM: PPC: Fix compile error that occurs when CONFIG_ALTIVEC=n
    
    Commit accb757d798c ("KVM: Move vcpu_load to arch-specific
    kvm_arch_vcpu_ioctl_run", 2017-12-04) added a "goto out"
    statement and an "out:" label to kvm_arch_vcpu_ioctl_run().
    Since the only "goto out" is inside a CONFIG_VSX block,
    compiling with CONFIG_VSX=n gives a warning that label "out"
    is defined but not used, and because arch/powerpc is compiled
    with -Werror, that becomes a compile error that makes the kernel
    build fail.
    
    Merge commit 1ab03c072feb ("Merge tag 'kvm-ppc-next-4.16-2' of
    git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc",
    2018-02-09) added a similar block of code inside a #ifdef
    CONFIG_ALTIVEC, with a "goto out" statement.
    
    In order to make the build succeed, this adds a #ifdef around the
    "out:" label.  This is a minimal, ugly fix, to be replaced later
    by a refactoring of the code.  Since CONFIG_VSX depends on
    CONFIG_ALTIVEC, it is sufficient to use #ifdef CONFIG_ALTIVEC here.
    
    Fixes: accb757d798c ("KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_run")
    Reported-by: Christian Zigotzky <chzigotzky@xenosoft.de>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 403e642c78f5..0083142c2f84 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1608,7 +1608,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	kvm_sigset_deactivate(vcpu);
 
+#ifdef CONFIG_ALTIVEC
 out:
+#endif
 	vcpu_put(vcpu);
 	return r;
 }

commit 1ab03c072feb579c9fd116de25be2b211e6bff6a
Merge: 80132f4c0cf9 09f984961c13
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Fri Feb 9 21:36:57 2018 +0100

    Merge tag 'kvm-ppc-next-4.16-2' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    Second PPC KVM update for 4.16
    
    Seven fixes that are either trivial or that address bugs that people
    are actually hitting.  The main ones are:
    
    - Drop spinlocks before reading guest memory
    
    - Fix a bug causing corruption of VCPU state in PR KVM with preemption
      enabled
    
    - Make HPT resizing work on POWER9
    
    - Add MMIO emulation for vector loads and stores, because guests now
      use these instructions in memcpy and similar routines.

commit 09f984961c137c4b252c368adab7e1c9f035fa59
Author: Jose Ricardo Ziviani <joserz@linux.vnet.ibm.com>
Date:   Sat Feb 3 18:24:26 2018 -0200

    KVM: PPC: Book3S: Add MMIO emulation for VMX instructions
    
    This patch provides the MMIO load/store vector indexed
    X-Form emulation.
    
    Instructions implemented:
    lvx: the quadword in storage addressed by the result of EA &
    0xffff_ffff_ffff_fff0 is loaded into VRT.
    
    stvx: the contents of VRS are stored into the quadword in storage
    addressed by the result of EA & 0xffff_ffff_ffff_fff0.
    
    Reported-by: Gopesh Kumar Chaudhary <gopchaud@in.ibm.com>
    Reported-by: Balamuruhan S <bala24@linux.vnet.ibm.com>
    Signed-off-by: Jose Ricardo Ziviani <joserz@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index cf86aeb43fcf..47c7a302fd03 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -924,6 +924,34 @@ static inline void kvmppc_set_vsr_word(struct kvm_vcpu *vcpu,
 }
 #endif /* CONFIG_VSX */
 
+#ifdef CONFIG_ALTIVEC
+static inline void kvmppc_set_vmx_dword(struct kvm_vcpu *vcpu,
+		u64 gpr)
+{
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
+	u32 hi, lo;
+	u32 di;
+
+#ifdef __BIG_ENDIAN
+	hi = gpr >> 32;
+	lo = gpr & 0xffffffff;
+#else
+	lo = gpr >> 32;
+	hi = gpr & 0xffffffff;
+#endif
+
+	di = 2 - vcpu->arch.mmio_vmx_copy_nums;		/* doubleword index */
+	if (di > 1)
+		return;
+
+	if (vcpu->arch.mmio_host_swabbed)
+		di = 1 - di;
+
+	VCPU_VSX_VR(vcpu, index).u[di * 2] = hi;
+	VCPU_VSX_VR(vcpu, index).u[di * 2 + 1] = lo;
+}
+#endif /* CONFIG_ALTIVEC */
+
 #ifdef CONFIG_PPC_FPU
 static inline u64 sp_to_dp(u32 fprs)
 {
@@ -1026,6 +1054,11 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 				KVMPPC_VSX_COPY_DWORD_LOAD_DUMP)
 			kvmppc_set_vsr_dword_dump(vcpu, gpr);
 		break;
+#endif
+#ifdef CONFIG_ALTIVEC
+	case KVM_MMIO_REG_VMX:
+		kvmppc_set_vmx_dword(vcpu, gpr);
+		break;
 #endif
 	default:
 		BUG();
@@ -1302,6 +1335,111 @@ static int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,
 }
 #endif /* CONFIG_VSX */
 
+#ifdef CONFIG_ALTIVEC
+/* handle quadword load access in two halves */
+int kvmppc_handle_load128_by2x64(struct kvm_run *run, struct kvm_vcpu *vcpu,
+		unsigned int rt, int is_default_endian)
+{
+	enum emulation_result emulated;
+
+	while (vcpu->arch.mmio_vmx_copy_nums) {
+		emulated = __kvmppc_handle_load(run, vcpu, rt, 8,
+				is_default_endian, 0);
+
+		if (emulated != EMULATE_DONE)
+			break;
+
+		vcpu->arch.paddr_accessed += run->mmio.len;
+		vcpu->arch.mmio_vmx_copy_nums--;
+	}
+
+	return emulated;
+}
+
+static inline int kvmppc_get_vmx_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
+{
+	vector128 vrs = VCPU_VSX_VR(vcpu, rs);
+	u32 di;
+	u64 w0, w1;
+
+	di = 2 - vcpu->arch.mmio_vmx_copy_nums;		/* doubleword index */
+	if (di > 1)
+		return -1;
+
+	if (vcpu->arch.mmio_host_swabbed)
+		di = 1 - di;
+
+	w0 = vrs.u[di * 2];
+	w1 = vrs.u[di * 2 + 1];
+
+#ifdef __BIG_ENDIAN
+	*val = (w0 << 32) | w1;
+#else
+	*val = (w1 << 32) | w0;
+#endif
+	return 0;
+}
+
+/* handle quadword store in two halves */
+int kvmppc_handle_store128_by2x64(struct kvm_run *run, struct kvm_vcpu *vcpu,
+		unsigned int rs, int is_default_endian)
+{
+	u64 val = 0;
+	enum emulation_result emulated = EMULATE_DONE;
+
+	vcpu->arch.io_gpr = rs;
+
+	while (vcpu->arch.mmio_vmx_copy_nums) {
+		if (kvmppc_get_vmx_data(vcpu, rs, &val) == -1)
+			return EMULATE_FAIL;
+
+		emulated = kvmppc_handle_store(run, vcpu, val, 8,
+				is_default_endian);
+		if (emulated != EMULATE_DONE)
+			break;
+
+		vcpu->arch.paddr_accessed += run->mmio.len;
+		vcpu->arch.mmio_vmx_copy_nums--;
+	}
+
+	return emulated;
+}
+
+static int kvmppc_emulate_mmio_vmx_loadstore(struct kvm_vcpu *vcpu,
+		struct kvm_run *run)
+{
+	enum emulation_result emulated = EMULATE_FAIL;
+	int r;
+
+	vcpu->arch.paddr_accessed += run->mmio.len;
+
+	if (!vcpu->mmio_is_write) {
+		emulated = kvmppc_handle_load128_by2x64(run, vcpu,
+				vcpu->arch.io_gpr, 1);
+	} else {
+		emulated = kvmppc_handle_store128_by2x64(run, vcpu,
+				vcpu->arch.io_gpr, 1);
+	}
+
+	switch (emulated) {
+	case EMULATE_DO_MMIO:
+		run->exit_reason = KVM_EXIT_MMIO;
+		r = RESUME_HOST;
+		break;
+	case EMULATE_FAIL:
+		pr_info("KVM: MMIO emulation failed (VMX repeat)\n");
+		run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+		run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+		r = RESUME_HOST;
+		break;
+	default:
+		r = RESUME_GUEST;
+		break;
+	}
+	return r;
+}
+#endif /* CONFIG_ALTIVEC */
+
 int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 {
 	int r = 0;
@@ -1420,6 +1558,18 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 				return r;
 			}
 		}
+#endif
+#ifdef CONFIG_ALTIVEC
+		if (vcpu->arch.mmio_vmx_copy_nums > 0)
+			vcpu->arch.mmio_vmx_copy_nums--;
+
+		if (vcpu->arch.mmio_vmx_copy_nums > 0) {
+			r = kvmppc_emulate_mmio_vmx_loadstore(vcpu, run);
+			if (r == RESUME_HOST) {
+				vcpu->mmio_needed = 1;
+				return r;
+			}
+		}
 #endif
 	} else if (vcpu->arch.osi_needed) {
 		u64 *gprs = run->osi.gprs;

commit 790a9df5fbef982f2a6992194fe497dd2b794a3d
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Fri Feb 2 14:29:08 2018 +1100

    KVM: PPC: Book3S HV: Make HPT resizing work on POWER9
    
    This adds code to enable the HPT resizing code to work on POWER9,
    which uses a slightly modified HPT entry format compared to POWER8.
    On POWER9, we convert HPTEs read from the HPT from the new format to
    the old format so that the rest of the HPT resizing code can work as
    before.  HPTEs written to the new HPT are converted to the new format
    as the last step before writing them into the new HPT.
    
    This takes out the checks added by commit bcd3bb63dbc8 ("KVM: PPC:
    Book3S HV: Disable HPT resizing on POWER9 for now", 2017-02-18),
    now that HPT resizing works on POWER9.
    
    On POWER9, when we pivot to the new HPT, we now call
    kvmppc_setup_partition_table() to update the partition table in order
    to make the hardware use the new HPT.
    
    [paulus@ozlabs.org - added kvmppc_setup_partition_table() call,
     wrote commit message.]
    
    Tested-by: Laurent Vivier <lvivier@redhat.com>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 77eb25abc601..cf86aeb43fcf 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -633,8 +633,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = 1;
 		break;
 	case KVM_CAP_SPAPR_RESIZE_HPT:
-		/* Disable this on POWER9 until code handles new HPTE format */
-		r = !!hv_enabled && !cpu_has_feature(CPU_FTR_ARCH_300);
+		r = !!hv_enabled;
 		break;
 #endif
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE

commit d2b9b2079e23c1ab80ce1d7670d5e1994468a881
Merge: 7bf14c28ee77 9b9b13a6d153
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Thu Feb 1 16:13:07 2018 +0100

    Merge tag 'kvm-ppc-next-4.16-1' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    PPC KVM update for 4.16
    
    - Allow HPT guests to run on a radix host on POWER9 v2.2 CPUs
      without requiring the complex thread synchronization that earlier
      CPU versions required.
    
    - A series from Ben Herrenschmidt to improve the handling of
      escalation interrupts with the XIVE interrupt controller.
    
    - Provide for the decrementer register to be copied across on
      migration.
    
    - Various minor cleanups and bugfixes.

commit 7bf14c28ee776be567855bd39ed8ff795ea19f55
Merge: 87cedc6be559 5fa4ec9cb2e6
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Thu Feb 1 15:04:17 2018 +0100

    Merge branch 'x86/hyperv' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Topic branch for stable KVM clockource under Hyper-V.
    
    Thanks to Christoffer Dall for resolving the ARM conflict.

commit 3214d01f139b7544e870fc0b7fcce8da13c1cb51
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 15 16:06:47 2018 +1100

    KVM: PPC: Book3S: Provide information about hardware/firmware CVE workarounds
    
    This adds a new ioctl, KVM_PPC_GET_CPU_CHAR, that gives userspace
    information about the underlying machine's level of vulnerability
    to the recently announced vulnerabilities CVE-2017-5715,
    CVE-2017-5753 and CVE-2017-5754, and whether the machine provides
    instructions to assist software to work around the vulnerabilities.
    
    The ioctl returns two u64 words describing characteristics of the
    CPU and required software behaviour respectively, plus two mask
    words which indicate which bits have been filled in by the kernel,
    for extensibility.  The bit definitions are the same as for the
    new H_GET_CPU_CHARACTERISTICS hypercall.
    
    There is also a new capability, KVM_CAP_PPC_GET_CPU_CHAR, which
    indicates whether the new ioctl is available.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1915e86cef6f..0a7c88786ec0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -39,6 +39,10 @@
 #include <asm/iommu.h>
 #include <asm/switch_to.h>
 #include <asm/xive.h>
+#ifdef CONFIG_PPC_PSERIES
+#include <asm/hvcall.h>
+#include <asm/plpar_wrappers.h>
+#endif
 
 #include "timing.h"
 #include "irq.h"
@@ -548,6 +552,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #ifdef CONFIG_KVM_XICS
 	case KVM_CAP_IRQ_XICS:
 #endif
+	case KVM_CAP_PPC_GET_CPU_CHAR:
 		r = 1;
 		break;
 
@@ -1759,6 +1764,124 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 	return r;
 }
 
+#ifdef CONFIG_PPC_BOOK3S_64
+/*
+ * These functions check whether the underlying hardware is safe
+ * against attacks based on observing the effects of speculatively
+ * executed instructions, and whether it supplies instructions for
+ * use in workarounds.  The information comes from firmware, either
+ * via the device tree on powernv platforms or from an hcall on
+ * pseries platforms.
+ */
+#ifdef CONFIG_PPC_PSERIES
+static int pseries_get_cpu_char(struct kvm_ppc_cpu_char *cp)
+{
+	struct h_cpu_char_result c;
+	unsigned long rc;
+
+	if (!machine_is(pseries))
+		return -ENOTTY;
+
+	rc = plpar_get_cpu_characteristics(&c);
+	if (rc == H_SUCCESS) {
+		cp->character = c.character;
+		cp->behaviour = c.behaviour;
+		cp->character_mask = KVM_PPC_CPU_CHAR_SPEC_BAR_ORI31 |
+			KVM_PPC_CPU_CHAR_BCCTRL_SERIALISED |
+			KVM_PPC_CPU_CHAR_L1D_FLUSH_ORI30 |
+			KVM_PPC_CPU_CHAR_L1D_FLUSH_TRIG2 |
+			KVM_PPC_CPU_CHAR_L1D_THREAD_PRIV |
+			KVM_PPC_CPU_CHAR_BR_HINT_HONOURED |
+			KVM_PPC_CPU_CHAR_MTTRIG_THR_RECONF |
+			KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS;
+		cp->behaviour_mask = KVM_PPC_CPU_BEHAV_FAVOUR_SECURITY |
+			KVM_PPC_CPU_BEHAV_L1D_FLUSH_PR |
+			KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR;
+	}
+	return 0;
+}
+#else
+static int pseries_get_cpu_char(struct kvm_ppc_cpu_char *cp)
+{
+	return -ENOTTY;
+}
+#endif
+
+static inline bool have_fw_feat(struct device_node *fw_features,
+				const char *state, const char *name)
+{
+	struct device_node *np;
+	bool r = false;
+
+	np = of_get_child_by_name(fw_features, name);
+	if (np) {
+		r = of_property_read_bool(np, state);
+		of_node_put(np);
+	}
+	return r;
+}
+
+static int kvmppc_get_cpu_char(struct kvm_ppc_cpu_char *cp)
+{
+	struct device_node *np, *fw_features;
+	int r;
+
+	memset(cp, 0, sizeof(*cp));
+	r = pseries_get_cpu_char(cp);
+	if (r != -ENOTTY)
+		return r;
+
+	np = of_find_node_by_name(NULL, "ibm,opal");
+	if (np) {
+		fw_features = of_get_child_by_name(np, "fw-features");
+		of_node_put(np);
+		if (!fw_features)
+			return 0;
+		if (have_fw_feat(fw_features, "enabled",
+				 "inst-spec-barrier-ori31,31,0"))
+			cp->character |= KVM_PPC_CPU_CHAR_SPEC_BAR_ORI31;
+		if (have_fw_feat(fw_features, "enabled",
+				 "fw-bcctrl-serialized"))
+			cp->character |= KVM_PPC_CPU_CHAR_BCCTRL_SERIALISED;
+		if (have_fw_feat(fw_features, "enabled",
+				 "inst-l1d-flush-ori30,30,0"))
+			cp->character |= KVM_PPC_CPU_CHAR_L1D_FLUSH_ORI30;
+		if (have_fw_feat(fw_features, "enabled",
+				 "inst-l1d-flush-trig2"))
+			cp->character |= KVM_PPC_CPU_CHAR_L1D_FLUSH_TRIG2;
+		if (have_fw_feat(fw_features, "enabled",
+				 "fw-l1d-thread-split"))
+			cp->character |= KVM_PPC_CPU_CHAR_L1D_THREAD_PRIV;
+		if (have_fw_feat(fw_features, "enabled",
+				 "fw-count-cache-disabled"))
+			cp->character |= KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS;
+		cp->character_mask = KVM_PPC_CPU_CHAR_SPEC_BAR_ORI31 |
+			KVM_PPC_CPU_CHAR_BCCTRL_SERIALISED |
+			KVM_PPC_CPU_CHAR_L1D_FLUSH_ORI30 |
+			KVM_PPC_CPU_CHAR_L1D_FLUSH_TRIG2 |
+			KVM_PPC_CPU_CHAR_L1D_THREAD_PRIV |
+			KVM_PPC_CPU_CHAR_COUNT_CACHE_DIS;
+
+		if (have_fw_feat(fw_features, "enabled",
+				 "speculation-policy-favor-security"))
+			cp->behaviour |= KVM_PPC_CPU_BEHAV_FAVOUR_SECURITY;
+		if (!have_fw_feat(fw_features, "disabled",
+				  "needs-l1d-flush-msr-pr-0-to-1"))
+			cp->behaviour |= KVM_PPC_CPU_BEHAV_L1D_FLUSH_PR;
+		if (!have_fw_feat(fw_features, "disabled",
+				  "needs-spec-barrier-for-bound-checks"))
+			cp->behaviour |= KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR;
+		cp->behaviour_mask = KVM_PPC_CPU_BEHAV_FAVOUR_SECURITY |
+			KVM_PPC_CPU_BEHAV_L1D_FLUSH_PR |
+			KVM_PPC_CPU_BEHAV_BNDS_CHK_SPEC_BAR;
+
+		of_node_put(fw_features);
+	}
+
+	return 0;
+}
+#endif
+
 long kvm_arch_vm_ioctl(struct file *filp,
                        unsigned int ioctl, unsigned long arg)
 {
@@ -1861,6 +1984,14 @@ long kvm_arch_vm_ioctl(struct file *filp,
 			r = -EFAULT;
 		break;
 	}
+	case KVM_PPC_GET_CPU_CHAR: {
+		struct kvm_ppc_cpu_char cpuchar;
+
+		r = kvmppc_get_cpu_char(&cpuchar);
+		if (r >= 0 && copy_to_user(argp, &cpuchar, sizeof(cpuchar)))
+			r = -EFAULT;
+		break;
+	}
 	default: {
 		struct kvm *kvm = filp->private_data;
 		r = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);

commit d27998185da8fbdc35911307ae13518d168778d7
Merge: 00608e1f007e d075745d893c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Jan 19 12:09:57 2018 +1100

    Merge remote-tracking branch 'remotes/powerpc/topic/ppc-kvm' into kvm-ppc-next
    
    This merges in the ppc-kvm topic branch of the powerpc tree to get
    two patches which are prerequisites for the following patch series,
    plus another patch which touches both powerpc and KVM code.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

commit 5855564c8ab2d9cefca7b2933bd19818eb795e40
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Jan 12 20:55:20 2018 +1100

    KVM: PPC: Book3S HV: Enable migration of decrementer register
    
    This adds a register identifier for use with the one_reg interface
    to allow the decrementer expiry time to be read and written by
    userspace.  The decrementer expiry time is in guest timebase units
    and is equal to the sum of the decrementer and the guest timebase.
    (The expiry time is used rather than the decrementer value itself
    because the expiry time is not constantly changing, though the
    decrementer value is, while the guest vcpu is not running.)
    
    Without this, a guest vcpu migrated to a new host will see its
    decrementer set to some random value.  On POWER8 and earlier, the
    decrementer is 32 bits wide and counts down at 512MHz, so the
    guest vcpu will potentially see no decrementer interrupts for up
    to about 4 seconds, which will lead to a stall.  With POWER9, the
    decrementer is now 56 bits side, so the stall can be much longer
    (up to 2.23 years) and more noticeable.
    
    To help work around the problem in cases where userspace has not been
    updated to migrate the decrementer expiry time, we now set the
    default decrementer expiry at vcpu creation time to the current time
    rather than the maximum possible value.  This should mean an
    immediate decrementer interrupt when a migrated vcpu starts
    running.  In cases where the decrementer is 32 bits wide and more
    than 4 seconds elapse between the creation of the vcpu and when it
    first runs, the decrementer would have wrapped around to positive
    values and there may still be a stall - but this is no worse than
    the current situation.  In the large-decrementer case, we are sure
    to get an immediate decrementer interrupt (assuming the time from
    vcpu creation to first run is less than 2.23 years) and we thus
    avoid a very long stall.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index c2c7ef330553..7c9e45f54186 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -758,7 +758,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 
 	hrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
-	vcpu->arch.dec_expires = ~(u64)0;
+	vcpu->arch.dec_expires = get_tb();
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	mutex_init(&vcpu->arch.exit_timing_lock);

commit 5cb0944c0c66004c0d9006a7f0fba5782ae38f69
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Dec 12 17:41:34 2017 +0100

    KVM: introduce kvm_arch_vcpu_async_ioctl
    
    After the vcpu_load/vcpu_put pushdown, the handling of asynchronous VCPU
    ioctl is already much clearer in that it is obvious that they bypass
    vcpu_load and vcpu_put.
    
    However, it is still not perfect in that the different state of the VCPU
    mutex is still hidden in the caller.  Separate those ioctls into a new
    function kvm_arch_vcpu_async_ioctl that returns -ENOIOCTLCMD for more
    "traditional" synchronous ioctls.
    
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Paul Mackerras <paulus@ozlabs.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Reviewed-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    Suggested-by: Cornelia Huck <cohuck@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ba8134a989c1..66a310779de5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1607,12 +1607,11 @@ int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
 	return -EINVAL;
 }
 
-long kvm_arch_vcpu_ioctl(struct file *filp,
-                         unsigned int ioctl, unsigned long arg)
+long kvm_arch_vcpu_async_ioctl(struct file *filp,
+			       unsigned int ioctl, unsigned long arg)
 {
 	struct kvm_vcpu *vcpu = filp->private_data;
 	void __user *argp = (void __user *)arg;
-	long r;
 
 	if (ioctl == KVM_INTERRUPT) {
 		struct kvm_interrupt irq;
@@ -1620,6 +1619,15 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 			return -EFAULT;
 		return kvm_vcpu_ioctl_interrupt(vcpu, &irq);
 	}
+	return -ENOIOCTLCMD;
+}
+
+long kvm_arch_vcpu_ioctl(struct file *filp,
+                         unsigned int ioctl, unsigned long arg)
+{
+	struct kvm_vcpu *vcpu = filp->private_data;
+	void __user *argp = (void __user *)arg;
+	long r;
 
 	vcpu_load(vcpu);
 

commit 9b062471e52a1692c5563ba1535c84d708e2ff6f
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:36 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl
    
    Move the calls to vcpu_load() and vcpu_put() in to the architecture
    specific implementations of kvm_arch_vcpu_ioctl() which dispatches
    further architecture-specific ioctls on to other functions.
    
    Some architectures support asynchronous vcpu ioctls which cannot call
    vcpu_load() or take the vcpu->mutex, because that would prevent
    concurrent execution with a running VCPU, which is the intended purpose
    of these ioctls, for example because they inject interrupts.
    
    We repeat the separate checks for these specifics in the architecture
    code for MIPS, S390 and PPC, and avoid taking the vcpu->mutex and
    calling vcpu_load for these ioctls.
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4e2167a7ae19..ba8134a989c1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1614,16 +1614,16 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 	long r;
 
-	switch (ioctl) {
-	case KVM_INTERRUPT: {
+	if (ioctl == KVM_INTERRUPT) {
 		struct kvm_interrupt irq;
-		r = -EFAULT;
 		if (copy_from_user(&irq, argp, sizeof(irq)))
-			goto out;
-		r = kvm_vcpu_ioctl_interrupt(vcpu, &irq);
-		goto out;
+			return -EFAULT;
+		return kvm_vcpu_ioctl_interrupt(vcpu, &irq);
 	}
 
+	vcpu_load(vcpu);
+
+	switch (ioctl) {
 	case KVM_ENABLE_CAP:
 	{
 		struct kvm_enable_cap cap;
@@ -1663,6 +1663,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 
 out:
+	vcpu_put(vcpu);
 	return r;
 }
 

commit accb757d798c9b4d85cfe3e5972134c586525168
Author: Christoffer Dall <christoffer.dall@linaro.org>
Date:   Mon Dec 4 21:35:25 2017 +0100

    KVM: Move vcpu_load to arch-specific kvm_arch_vcpu_ioctl_run
    
    Move vcpu_load() and vcpu_put() into the architecture specific
    implementations of kvm_arch_vcpu_ioctl_run().
    
    Signed-off-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com> # s390 parts
    Reviewed-by: Cornelia Huck <cohuck@redhat.com>
    [Rebased. - Paolo]
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1915e86cef6f..4e2167a7ae19 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1408,6 +1408,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	int r;
 
+	vcpu_load(vcpu);
+
 	if (vcpu->mmio_needed) {
 		vcpu->mmio_needed = 0;
 		if (!vcpu->mmio_is_write)
@@ -1422,7 +1424,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 			r = kvmppc_emulate_mmio_vsx_loadstore(vcpu, run);
 			if (r == RESUME_HOST) {
 				vcpu->mmio_needed = 1;
-				return r;
+				goto out;
 			}
 		}
 #endif
@@ -1456,6 +1458,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	kvm_sigset_deactivate(vcpu);
 
+out:
+	vcpu_put(vcpu);
 	return r;
 }
 

commit 20b7035c66bacc909ae3ffe92c1a1ea7db99fe4f
Author: Jan H. SchÃ¶nherr <jschoenh@amazon.de>
Date:   Fri Nov 24 22:39:01 2017 +0100

    KVM: Let KVM_SET_SIGNAL_MASK work as advertised
    
    KVM API says for the signal mask you set via KVM_SET_SIGNAL_MASK, that
    "any unblocked signal received [...] will cause KVM_RUN to return with
    -EINTR" and that "the signal will only be delivered if not blocked by
    the original signal mask".
    
    This, however, is only true, when the calling task has a signal handler
    registered for a signal. If not, signal evaluation is short-circuited for
    SIG_IGN and SIG_DFL, and the signal is either ignored without KVM_RUN
    returning or the whole process is terminated.
    
    Make KVM_SET_SIGNAL_MASK behave as advertised by utilizing logic similar
    to that in do_sigtimedwait() to avoid short-circuiting of signals.
    
    Signed-off-by: Jan H. SchÃÂ¶nherr <jschoenh@amazon.de>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6b6c53c42ac9..1915e86cef6f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1407,7 +1407,6 @@ int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	int r;
-	sigset_t sigsaved;
 
 	if (vcpu->mmio_needed) {
 		vcpu->mmio_needed = 0;
@@ -1448,16 +1447,14 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 #endif
 	}
 
-	if (vcpu->sigset_active)
-		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
+	kvm_sigset_activate(vcpu);
 
 	if (run->immediate_exit)
 		r = -EINTR;
 	else
 		r = kvmppc_vcpu_run(run, vcpu);
 
-	if (vcpu->sigset_active)
-		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
+	kvm_sigset_deactivate(vcpu);
 
 	return r;
 }

commit 9aa6825bbb7526a7fdec137b7cc3b042581cd2fc
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Nov 20 19:56:27 2017 +1100

    KVM: PPC: Book3S: Eliminate some unnecessary checks
    
    In an excess of caution, commit 6f63e81bda98 ("KVM: PPC: Book3S: Add
    MMIO emulation for FP and VSX instructions", 2017-02-21) included
    checks for the case that vcpu->arch.mmio_vsx_copy_nums is less than
    zero, even though its type is u8.  This causes a Coverity warning,
    so we remove the check for < 0.  We also adjust the associated
    comment to be more accurate ("4 or less" rather than "less than 4").
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6b6c53c42ac9..c2c7ef330553 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1101,11 +1101,9 @@ int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 {
 	enum emulation_result emulated = EMULATE_DONE;
 
-	/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */
-	if ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||
-		(vcpu->arch.mmio_vsx_copy_nums < 0) ) {
+	/* Currently, mmio_vsx_copy_nums only allowed to be 4 or less */
+	if (vcpu->arch.mmio_vsx_copy_nums > 4)
 		return EMULATE_FAIL;
-	}
 
 	while (vcpu->arch.mmio_vsx_copy_nums) {
 		emulated = __kvmppc_handle_load(run, vcpu, rt, bytes,
@@ -1247,11 +1245,9 @@ int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 	vcpu->arch.io_gpr = rs;
 
-	/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */
-	if ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||
-		(vcpu->arch.mmio_vsx_copy_nums < 0) ) {
+	/* Currently, mmio_vsx_copy_nums only allowed to be 4 or less */
+	if (vcpu->arch.mmio_vsx_copy_nums > 4)
 		return EMULATE_FAIL;
-	}
 
 	while (vcpu->arch.mmio_vsx_copy_nums) {
 		if (kvmppc_get_vsr_data(vcpu, rs, &val) == -1)

commit 072df8130c6b602c8ee219f7b06394680cafad2f
Merge: c01015091a77 38c53af85306
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Nov 9 14:30:24 2017 +1100

    Merge branch 'kvm-ppc-fixes' into kvm-ppc-next
    
    This merges in a couple of fixes from the kvm-ppc-fixes branch that
    modify the same areas of code as some commits from the kvm-ppc-next
    branch, in order to resolve the conflicts.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

commit 18c3640cefc7f1c6986b7be48f5013a8d5e394cb
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Sep 13 16:00:10 2017 +1000

    KVM: PPC: Book3S HV: Add infrastructure for running HPT guests on radix host
    
    This sets up the machinery for switching a guest between HPT (hashed
    page table) and radix MMU modes, so that in future we can run a HPT
    guest on a radix host on POWER9 machines.
    
    * The KVM_PPC_CONFIGURE_V3_MMU ioctl can now specify either HPT or
      radix mode, on a radix host.
    
    * The KVM_CAP_PPC_MMU_HASH_V3 capability now returns 1 on POWER9
      with HV KVM on a radix host.
    
    * The KVM_PPC_GET_SMMU_INFO returns information about the HPT MMU on a
      radix host.
    
    * The KVM_PPC_ALLOCATE_HTAB ioctl on a radix host will switch the
      guest to HPT mode and allocate a HPT.
    
    * For simplicity, we now allocate the rmap array for each memslot,
      even on a radix host, since it will be needed if the guest switches
      to HPT mode.
    
    * Since we cannot yet run a HPT guest on a radix host, the KVM_RUN
      ioctl will return an EINVAL error in that case.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a3746b98ec11..a0b7f094de78 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -590,8 +590,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = !!(hv_enabled && radix_enabled());
 		break;
 	case KVM_CAP_PPC_MMU_HASH_V3:
-		r = !!(hv_enabled && !radix_enabled() &&
-		       cpu_has_feature(CPU_FTR_ARCH_300));
+		r = !!(hv_enabled && cpu_has_feature(CPU_FTR_ARCH_300));
 		break;
 #endif
 	case KVM_CAP_SYNC_MMU:

commit 2a3d6553cbd791da4fb624c2500b4580b3f9d72f
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Oct 12 22:58:54 2017 +1100

    KVM: PPC: Tie KVM_CAP_PPC_HTM to the user-visible TM feature
    
    Currently we use CPU_FTR_TM to decide if the CPU/kernel can support
    TM (Transactional Memory), and if it's true we advertise that to
    Qemu (or similar) via KVM_CAP_PPC_HTM.
    
    PPC_FEATURE2_HTM is the user-visible feature bit, which indicates that
    the CPU and kernel can support TM. Currently CPU_FTR_TM and
    PPC_FEATURE2_HTM always have the same value, either true or false, so
    using the former for KVM_CAP_PPC_HTM is correct.
    
    However some Power9 CPUs can operate in a mode where TM is enabled but
    TM suspended state is disabled. In this mode CPU_FTR_TM is true, but
    PPC_FEATURE2_HTM is false. Instead a different PPC_FEATURE2 bit is
    set, to indicate that this different mode of TM is available.
    
    It is not safe to let guests use TM as-is, when the CPU is in this
    mode. So to prevent that from happening, use PPC_FEATURE2_HTM to
    determine the value of KVM_CAP_PPC_HTM.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3480faaf1ef8..a3746b98ec11 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -644,8 +644,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 #endif
 	case KVM_CAP_PPC_HTM:
-		r = cpu_has_feature(CPU_FTR_TM_COMP) &&
-		    is_kvmppc_hv_enabled(kvm);
+		r = is_kvmppc_hv_enabled(kvm) &&
+		    (cur_cpu_spec->cpu_user_features2 & PPC_FEATURE2_HTM_COMP);
 		break;
 	default:
 		r = 0;

commit ac64115a66c18c01745bbd3c47a36b124e5fd8c0
Author: Greg Kurz <groug@kaod.org>
Date:   Thu Sep 14 23:56:25 2017 +0200

    KVM: PPC: Fix oops when checking KVM_CAP_PPC_HTM
    
    The following program causes a kernel oops:
    
    #include <sys/types.h>
    #include <sys/stat.h>
    #include <fcntl.h>
    #include <sys/ioctl.h>
    #include <linux/kvm.h>
    
    main()
    {
        int fd = open("/dev/kvm", O_RDWR);
        ioctl(fd, KVM_CHECK_EXTENSION, KVM_CAP_PPC_HTM);
    }
    
    This happens because when using the global KVM fd with
    KVM_CHECK_EXTENSION, kvm_vm_ioctl_check_extension() gets
    called with a NULL kvm argument, which gets dereferenced
    in is_kvmppc_hv_enabled(). Spotted while reading the code.
    
    Let's use the hv_enabled fallback variable, like everywhere
    else in this function.
    
    Fixes: 23528bb21ee2 ("KVM: PPC: Introduce KVM_CAP_PPC_HTM")
    Cc: stable@vger.kernel.org # v4.7+
    Signed-off-by: Greg Kurz <groug@kaod.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3480faaf1ef8..ee279c7f4802 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -644,8 +644,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		break;
 #endif
 	case KVM_CAP_PPC_HTM:
-		r = cpu_has_feature(CPU_FTR_TM_COMP) &&
-		    is_kvmppc_hv_enabled(kvm);
+		r = cpu_has_feature(CPU_FTR_TM_COMP) && hv_enabled;
 		break;
 	default:
 		r = 0;

commit 199b5763d329b43c88f6ad539db8a6c6b42f8edb
Author: Longpeng(Mike) <longpeng2@huawei.com>
Date:   Tue Aug 8 12:05:32 2017 +0800

    KVM: add spinlock optimization framework
    
    If a vcpu exits due to request a user mode spinlock, then
    the spinlock-holder may be preempted in user mode or kernel mode.
    (Note that not all architectures trap spin loops in user mode,
    only AMD x86 and ARM/ARM64 currently do).
    
    But if a vcpu exits in kernel mode, then the holder must be
    preempted in kernel mode, so we should choose a vcpu in kernel mode
    as a more likely candidate for the lock holder.
    
    This introduces kvm_arch_vcpu_in_kernel() to decide whether the
    vcpu is in kernel-mode when it's preempted.  kvm_vcpu_on_spin's
    new argument says the same of the spinning VCPU.
    
    Signed-off-by: Longpeng(Mike) <longpeng2@huawei.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1a75c0b5f4ca..3480faaf1ef8 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -58,6 +58,11 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 	return !!(v->arch.pending_exceptions) || kvm_request_pending(v);
 }
 
+bool kvm_arch_vcpu_in_kernel(struct kvm_vcpu *vcpu)
+{
+	return false;
+}
+
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 {
 	return 1;

commit 8a53e7e572252e551fd4b172dc207f8beca1ae20
Merge: 00c14757f6ab 8b24e69fc47e
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Jul 3 10:41:59 2017 +0200

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    - Better machine check handling for HV KVM
    - Ability to support guests with threads=2, 4 or 8 on POWER9
    - Fix for a race that could cause delayed recognition of signals
    - Fix for a bug where POWER9 guests could sleep with interrupts
      pending.

commit 04a7ea04d508b925e7f829305b358157d58b4f82
Merge: c853354429f7 d38338e396ee
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Jun 30 12:38:26 2017 +0200

    Merge tag 'kvmarm-for-4.13' of git://git.kernel.org/pub/scm/linux/kernel/git/kvmarm/kvmarm into HEAD
    
    KVM/ARM updates for 4.13
    
    - vcpu request overhaul
    - allow timer and PMU to have their interrupt number
      selected from userspace
    - workaround for Cavium erratum 30115
    - handling of memory poisonning
    - the usual crop of fixes and cleanups
    
    Conflicts:
            arch/s390/include/asm/kvm_host.h

commit 2ed4f9dd19c0f76f7fb56c4b201696d29149325c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Jun 21 16:01:27 2017 +1000

    KVM: PPC: Book3S HV: Add capability to report possible virtual SMT modes
    
    Now that userspace can set the virtual SMT mode by enabling the
    KVM_CAP_PPC_SMT capability, it is useful for userspace to be able
    to query the set of possible virtual SMT modes.  This provides a
    new capability, KVM_CAP_PPC_SMT_POSSIBLE, to provide this
    information.  The return value is a bitmap of possible modes, with
    bit N set if virtual SMT mode 2^N is available.  That is, 1 indicates
    SMT1 is available, 2 indicates that SMT2 is available, 3 indicates
    that both SMT1 and SMT2 are available, and so on.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ccaa7a407c15..b14736f3870b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -566,6 +566,16 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 				r = threads_per_subcore;
 		}
 		break;
+	case KVM_CAP_PPC_SMT_POSSIBLE:
+		r = 1;
+		if (hv_enabled) {
+			if (!cpu_has_feature(CPU_FTR_ARCH_300))
+				r = ((threads_per_subcore << 1) - 1);
+			else
+				/* P9 can emulate dbells, so allow any mode */
+				r = 8 | 4 | 2 | 1;
+		}
+		break;
 	case KVM_CAP_PPC_RMA:
 		r = 0;
 		break;

commit 134764ed6e12d9f99b3de68b8aaeae1ba842d91c
Author: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
Date:   Thu May 11 16:32:48 2017 +0530

    KVM: PPC: Book3S HV: Add new capability to control MCE behaviour
    
    This introduces a new KVM capability to control how KVM behaves
    on machine check exception (MCE) in HV KVM guests.
    
    If this capability has not been enabled, KVM redirects machine check
    exceptions to guest's 0x200 vector, if the address in error belongs to
    the guest. With this capability enabled, KVM will cause a guest exit
    with the exit reason indicating an NMI.
    
    The new capability is required to avoid problems if a new kernel/KVM
    is used with an old QEMU, running a guest that doesn't issue
    "ibm,nmi-register".  As old QEMU does not understand the NMI exit
    type, it treats it as a fatal error.  However, the guest could have
    handled the machine check error if the exception was delivered to
    guest's 0x200 interrupt vector instead of NMI exit in case of old
    QEMU.
    
    [paulus@ozlabs.org - Reworded the commit message to be clearer,
     enable only on HV KVM.]
    
    Signed-off-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8208c2b95a93..ccaa7a407c15 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -623,6 +623,11 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		/* Disable this on POWER9 until code handles new HPTE format */
 		r = !!hv_enabled && !cpu_has_feature(CPU_FTR_ARCH_300);
 		break;
+#endif
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	case KVM_CAP_PPC_FWNMI:
+		r = hv_enabled;
+		break;
 #endif
 	case KVM_CAP_PPC_HTM:
 		r = cpu_has_feature(CPU_FTR_TM_COMP) &&
@@ -1543,6 +1548,15 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		break;
 	}
 #endif /* CONFIG_KVM_XICS */
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+	case KVM_CAP_PPC_FWNMI:
+		r = -EINVAL;
+		if (!is_kvmppc_hv_enabled(vcpu->kvm))
+			break;
+		r = 0;
+		vcpu->kvm->arch.fwnmi_enabled = true;
+		break;
+#endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 	default:
 		r = -EINVAL;
 		break;

commit 579006944e0d675361e987c646b83d2d1725966c
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Tue May 16 16:41:20 2017 +1000

    KVM: PPC: Book3S HV: Virtualize doorbell facility on POWER9
    
    On POWER9, we no longer have the restriction that we had on POWER8
    where all threads in a core have to be in the same partition, so
    the CPU threads are now independent.  However, we still want to be
    able to run guests with a virtual SMT topology, if only to allow
    migration of guests from POWER8 systems to POWER9.
    
    A guest that has a virtual SMT mode greater than 1 will expect to
    be able to use the doorbell facility; it will expect the msgsndp
    and msgclrp instructions to work appropriately and to be able to read
    sensible values from the TIR (thread identification register) and
    DPDES (directed privileged doorbell exception status) special-purpose
    registers.  However, since each CPU thread is a separate sub-processor
    in POWER9, these instructions and registers can only be used within
    a single CPU thread.
    
    In order for these instructions to appear to act correctly according
    to the guest's virtual SMT mode, we have to trap and emulate them.
    We cause them to trap by clearing the HFSCR_MSGP bit in the HFSCR
    register.  The emulation is triggered by the hypervisor facility
    unavailable interrupt that occurs when the guest uses them.
    
    To cause a doorbell interrupt to occur within the guest, we set the
    DPDES register to 1.  If the guest has interrupts enabled, the CPU
    will generate a doorbell interrupt and clear the DPDES register in
    hardware.  The DPDES hardware register for the guest is saved in the
    vcpu->arch.vcore->dpdes field.  Since this gets written by the guest
    exit code, other VCPUs wishing to cause a doorbell interrupt don't
    write that field directly, but instead set a vcpu->arch.doorbell_request
    flag.  This is consumed and set to 0 by the guest entry code, which
    then sets DPDES to 1.
    
    Emulating reads of the DPDES register is somewhat involved, because
    it requires reading the doorbell pending interrupt status of all of the
    VCPU threads in the virtual core, and if any of those VCPUs are
    running, their doorbell status is only up-to-date in the hardware
    DPDES registers of the CPUs where they are running.  In order to get
    a reasonable approximation of the current doorbell status, we send
    those CPUs an IPI, causing an exit from the guest which will update
    the vcpu->arch.vcore->dpdes field.  We then use that value in
    constructing the emulated DPDES register value.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index da90ae8cd508..8208c2b95a93 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -554,9 +554,12 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:
 		r = 0;
-		if (kvm)
-			r = kvm->arch.smt_mode;
-		else if (hv_enabled) {
+		if (kvm) {
+			if (kvm->arch.emul_smt_mode > 1)
+				r = kvm->arch.emul_smt_mode;
+			else
+				r = kvm->arch.smt_mode;
+		} else if (hv_enabled) {
 			if (cpu_has_feature(CPU_FTR_ARCH_300))
 				r = 1;
 			else

commit 3c313524605a6afd8207448a8e9967f5e8cba734
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Feb 6 13:24:41 2017 +1100

    KVM: PPC: Book3S HV: Allow userspace to set the desired SMT mode
    
    This allows userspace to set the desired virtual SMT (simultaneous
    multithreading) mode for a VM, that is, the number of VCPUs that
    get assigned to each virtual core.  Previously, the virtual SMT mode
    was fixed to the number of threads per subcore, and if userspace
    wanted to have fewer vcpus per vcore, then it would achieve that by
    using a sparse CPU numbering.  This had the disadvantage that the
    vcpu numbers can get quite large, particularly for SMT1 guests on
    a POWER8 with 8 threads per core.  With this patch, userspace can
    set its desired virtual SMT mode and then use contiguous vcpu
    numbering.
    
    On POWER8, where the threading mode is "strict", the virtual SMT mode
    must be less than or equal to the number of threads per subcore.  On
    POWER9, which implements a "loose" threading mode, the virtual SMT
    mode can be any power of 2 between 1 and 8, even though there is
    effectively one thread per subcore, since the threads are independent
    and can all be in different partitions.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7f71ab5fcad1..da90ae8cd508 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -554,7 +554,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:
 		r = 0;
-		if (hv_enabled) {
+		if (kvm)
+			r = kvm->arch.smt_mode;
+		else if (hv_enabled) {
 			if (cpu_has_feature(CPU_FTR_ARCH_300))
 				r = 1;
 			else
@@ -1712,6 +1714,15 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		r = 0;
 		break;
 	}
+	case KVM_CAP_PPC_SMT: {
+		unsigned long mode = cap->args[0];
+		unsigned long flags = cap->args[1];
+
+		r = -EINVAL;
+		if (kvm->arch.kvm_ops->set_smt_mode)
+			r = kvm->arch.kvm_ops->set_smt_mode(kvm, mode, flags);
+		break;
+	}
 #endif
 	default:
 		r = -EINVAL;

commit 2fa6e1e12a024b48b2c7ea39f50205246e027da7
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Sun Jun 4 14:43:52 2017 +0200

    KVM: add kvm_request_pending
    
    A first step in vcpu->requests encapsulation.  Additionally, we now
    use READ_ONCE() when accessing vcpu->requests, which ensures we
    always load vcpu->requests when it's accessed.  This is important as
    other threads can change it any time.  Also, READ_ONCE() documents
    that vcpu->requests is used with other threads, likely requiring
    memory barriers, which it does.
    
    Signed-off-by: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
    [ Documented the new use of READ_ONCE() and converted another check
      in arch/mips/kvm/vz.c ]
    Signed-off-by: Andrew Jones <drjones@redhat.com>
    Acked-by: Christoffer Dall <cdall@linaro.org>
    Signed-off-by: Christoffer Dall <cdall@linaro.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index f7cf2cd564ef..fd64f087737c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -55,8 +55,7 @@ EXPORT_SYMBOL_GPL(kvmppc_pr_ops);
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
-	return !!(v->arch.pending_exceptions) ||
-	       v->requests;
+	return !!(v->arch.pending_exceptions) || kvm_request_pending(v);
 }
 
 int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
@@ -108,7 +107,7 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 		 */
 		smp_mb();
 
-		if (vcpu->requests) {
+		if (kvm_request_pending(vcpu)) {
 			/* Make sure we process requests preemptable */
 			local_irq_enable();
 			trace_kvm_check_requests(vcpu);

commit 76d837a4c0f905f98088877d780169d7a14a6b29
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu May 11 14:31:59 2017 +1000

    KVM: PPC: Book3S PR: Don't include SPAPR TCE code on non-pseries platforms
    
    Commit e91aa8e6ecd5 ("KVM: PPC: Enable IOMMU_API for KVM_BOOK3S_64
    permanently", 2017-03-22) enabled the SPAPR TCE code for all 64-bit
    Book 3S kernel configurations in order to simplify the code and
    reduce #ifdefs.  However, 64-bit Book 3S PPC platforms other than
    pseries and powernv don't implement the necessary IOMMU callbacks,
    leading to build failures like the following (for a pasemi config):
    
    scripts/kconfig/conf  --silentoldconfig Kconfig
    warning: (KVM_BOOK3S_64) selects SPAPR_TCE_IOMMU which has unmet direct dependencies (IOMMU_SUPPORT && (PPC_POWERNV || PPC_PSERIES))
    
    ...
    
      CC [M]  arch/powerpc/kvm/book3s_64_vio.o
    /home/paulus/kernel/kvm/arch/powerpc/kvm/book3s_64_vio.c: In function âkvmppc_clear_tceâ:
    /home/paulus/kernel/kvm/arch/powerpc/kvm/book3s_64_vio.c:363:2: error: implicit declaration of function âiommu_tce_xchgâ [-Werror=implicit-function-declaration]
      iommu_tce_xchg(tbl, entry, &hpa, &dir);
      ^
    
    To fix this, we make the inclusion of the SPAPR TCE support, and the
    code that uses it in book3s_vio.c and book3s_vio_hv.c, depend on
    the inclusion of support for the pseries and/or powernv platforms.
    This means that when running a 'pseries' guest on those platforms,
    the guest won't have in-kernel acceleration of the PAPR TCE hypercalls,
    but at least now they compile.
    
    Reviewed-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index f7cf2cd564ef..7f71ab5fcad1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1749,7 +1749,7 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
 		break;
 	}
-#ifdef CONFIG_PPC_BOOK3S_64
+#ifdef CONFIG_SPAPR_TCE_IOMMU
 	case KVM_CREATE_SPAPR_TCE_64: {
 		struct kvm_create_spapr_tce_64 create_tce_64;
 
@@ -1780,6 +1780,8 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce_64);
 		goto out;
 	}
+#endif
+#ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_PPC_GET_SMMU_INFO: {
 		struct kvm_ppc_smmu_info info;
 		struct kvm *kvm = filp->private_data;

commit 4415b335282591e76762cd9e6dc60932a7595fc3
Merge: 3bed8888edc8 fb7dcf723dd2
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue May 9 11:50:01 2017 +0200

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    The main thing here is a new implementation of the in-kernel
    XICS interrupt controller emulation for POWER9 machines, from Ben
    Herrenschmidt.
    
    POWER9 has a new interrupt controller called XIVE (eXternal Interrupt
    Virtualization Engine) which is able to deliver interrupts directly
    to guest virtual CPUs in hardware without hypervisor intervention.
    With this new code, the guest still sees the old XICS interface but
    performance is better because the XICS emulation in the host uses the
    XIVE directly rather than going through a XICS emulation in firmware.
    
    Conflicts:
            arch/powerpc/kernel/cpu_setup_power.S [cherry-picked fix]
            arch/powerpc/kvm/book3s_xive.c [include asm/debugfs.h]

commit fb7dcf723dd2cb1d5d8f2f49c3023130938848e3
Merge: db4b0dfab7b0 5af50993850a
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Apr 28 08:23:16 2017 +1000

    Merge remote-tracking branch 'remotes/powerpc/topic/xive' into kvm-ppc-next
    
    This merges in the powerpc topic/xive branch to bring in the code for
    the in-kernel XICS interrupt controller emulation to use the new XIVE
    (eXternal Interrupt Virtualization Engine) hardware in the POWER9 chip
    directly, rather than via a XICS emulation in firmware.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

commit 72875d8a4d92f6f37e051be522b2252fd49bd50e
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Wed Apr 26 22:32:19 2017 +0200

    KVM: add kvm_{test,clear}_request to replace {test,clear}_bit
    
    Users were expected to use kvm_check_request() for testing and clearing,
    but request have expanded their use since then and some users want to
    only test or do a faster clear.
    
    Make sure that requests are not directly accessed with bit operations.
    
    Reviewed-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
    Reviewed-by: Andrew Jones <drjones@redhat.com>
    Reviewed-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index cf725c580fc5..1ee22a910074 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -233,7 +233,7 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	case EV_HCALL_TOKEN(EV_IDLE):
 		r = EV_SUCCESS;
 		kvm_vcpu_block(vcpu);
-		clear_bit(KVM_REQ_UNHALT, &vcpu->requests);
+		kvm_clear_request(KVM_REQ_UNHALT, vcpu);
 		break;
 	default:
 		r = EV_UNIMPLEMENTED;

commit 5af50993850a48ba749b122173d789ea90976c72
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 5 17:54:56 2017 +1000

    KVM: PPC: Book3S HV: Native usage of the XIVE interrupt controller
    
    This patch makes KVM capable of using the XIVE interrupt controller
    to provide the standard PAPR "XICS" style hypercalls. It is necessary
    for proper operations when the host uses XIVE natively.
    
    This has been lightly tested on an actual system, including PCI
    pass-through with a TG3 device.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [mpe: Cleanup pr_xxx(), unsplit pr_xxx() strings, etc., fix build
     failures by adding KVM_XIVE which depends on KVM_XICS and XIVE, and
     adding empty stubs for the kvm_xive_xxx() routines, fixup subject,
     integrate fixes from Paul for building PR=y HV=n]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 95c91a9de351..de79bd721ec7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -37,6 +37,8 @@
 #include <asm/cputhreads.h>
 #include <asm/irqflags.h>
 #include <asm/iommu.h>
+#include <asm/xive.h>
+
 #include "timing.h"
 #include "irq.h"
 #include "../mm/mmu_decl.h"
@@ -699,7 +701,10 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 		kvmppc_mpic_disconnect_vcpu(vcpu->arch.mpic, vcpu);
 		break;
 	case KVMPPC_IRQ_XICS:
-		kvmppc_xics_free_icp(vcpu);
+		if (xive_enabled())
+			kvmppc_xive_cleanup_vcpu(vcpu);
+		else
+			kvmppc_xics_free_icp(vcpu);
 		break;
 	}
 
@@ -1219,8 +1224,12 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 
 		r = -EPERM;
 		dev = kvm_device_from_filp(f.file);
-		if (dev)
-			r = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);
+		if (dev) {
+			if (xive_enabled())
+				r = kvmppc_xive_connect_vcpu(dev, vcpu, cap->args[1]);
+			else
+				r = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);
+		}
 
 		fdput(f);
 		break;
@@ -1244,7 +1253,7 @@ bool kvm_arch_intc_initialized(struct kvm *kvm)
 		return true;
 #endif
 #ifdef CONFIG_KVM_XICS
-	if (kvm->arch.xics)
+	if (kvm->arch.xics || kvm->arch.xive)
 		return true;
 #endif
 	return false;

commit 121f80ba68f1a5779a36d7b3247206e60e0a7418
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Wed Mar 22 15:21:56 2017 +1100

    KVM: PPC: VFIO: Add in-kernel acceleration for VFIO
    
    This allows the host kernel to handle H_PUT_TCE, H_PUT_TCE_INDIRECT
    and H_STUFF_TCE requests targeted an IOMMU TCE table used for VFIO
    without passing them to user space which saves time on switching
    to user space and back.
    
    This adds H_PUT_TCE/H_PUT_TCE_INDIRECT/H_STUFF_TCE handlers to KVM.
    KVM tries to handle a TCE request in the real mode, if failed
    it passes the request to the virtual mode to complete the operation.
    If it a virtual mode handler fails, the request is passed to
    the user space; this is not expected to happen though.
    
    To avoid dealing with page use counters (which is tricky in real mode),
    this only accelerates SPAPR TCE IOMMU v2 clients which are required
    to pre-register the userspace memory. The very first TCE request will
    be handled in the VFIO SPAPR TCE driver anyway as the userspace view
    of the TCE table (iommu_table::it_userspace) is not allocated till
    the very first mapping happens and we cannot call vmalloc in real mode.
    
    If we fail to update a hardware IOMMU table unexpected reason, we just
    clear it and move on as there is nothing really we can do about it -
    for example, if we hot plug a VFIO device to a guest, existing TCE tables
    will be mirrored automatically to the hardware and there is no interface
    to report to the guest about possible failures.
    
    This adds new attribute - KVM_DEV_VFIO_GROUP_SET_SPAPR_TCE - to
    the VFIO KVM device. It takes a VFIO group fd and SPAPR TCE table fd
    and associates a physical IOMMU table with the SPAPR TCE table (which
    is a guest view of the hardware IOMMU table). The iommu_table object
    is cached and referenced so we do not have to look up for it in real mode.
    
    This does not implement the UNSET counterpart as there is no use for it -
    once the acceleration is enabled, the existing userspace won't
    disable it unless a VFIO container is destroyed; this adds necessary
    cleanup to the KVM_DEV_VFIO_GROUP_DEL handler.
    
    This advertises the new KVM_CAP_SPAPR_TCE_VFIO capability to the user
    space.
    
    This adds real mode version of WARN_ON_ONCE() as the generic version
    causes problems with rcu_sched. Since we testing what vmalloc_to_phys()
    returns in the code, this also adds a check for already existing
    vmalloc_to_phys() call in kvmppc_rm_h_put_tce_indirect().
    
    This finally makes use of vfio_external_user_iommu_id() which was
    introduced quite some time ago and was considered for removal.
    
    Tests show that this patch increases transmission speed from 220MB/s
    to 750..1020MB/s on 10Gb network (Chelsea CXGB3 10Gb ethernet card).
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Acked-by: Alex Williamson <alex.williamson@redhat.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6c7244879bfe..cf725c580fc5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -534,6 +534,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_SPAPR_TCE_64:
+		/* fallthrough */
+	case KVM_CAP_SPAPR_TCE_VFIO:
 	case KVM_CAP_PPC_RTAS:
 	case KVM_CAP_PPC_FIXUP_HCALL:
 	case KVM_CAP_PPC_ENABLE_HCALL:

commit 6f63e81bda98cbb549b01faf978884692ded438d
Author: Bin Lu <lblulb@linux.vnet.ibm.com>
Date:   Tue Feb 21 21:12:36 2017 +0800

    KVM: PPC: Book3S: Add MMIO emulation for FP and VSX instructions
    
    This patch provides the MMIO load/store emulation for instructions
    of 'double & vector unsigned char & vector signed char & vector
    unsigned short & vector signed short & vector unsigned int & vector
    signed int & vector double '.
    
    The instructions that this adds emulation for are:
    
    - ldx, ldux, lwax,
    - lfs, lfsx, lfsu, lfsux, lfd, lfdx, lfdu, lfdux,
    - stfs, stfsx, stfsu, stfsux, stfd, stfdx, stfdu, stfdux, stfiwx,
    - lxsdx, lxsspx, lxsiwax, lxsiwzx, lxvd2x, lxvw4x, lxvdsx,
    - stxsdx, stxsspx, stxsiwx, stxvd2x, stxvw4x
    
    [paulus@ozlabs.org - some cleanups, fixes and rework, make it
     compile for Book E, fix build when PR KVM is built in]
    
    Signed-off-by: Bin Lu <lblulb@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0e42aa8a279f..6c7244879bfe 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -37,6 +37,7 @@
 #include <asm/cputhreads.h>
 #include <asm/irqflags.h>
 #include <asm/iommu.h>
+#include <asm/switch_to.h>
 #include "timing.h"
 #include "irq.h"
 #include "../mm/mmu_decl.h"
@@ -801,6 +802,129 @@ void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
 		kvm->arch.kvm_ops->irq_bypass_del_producer(cons, prod);
 }
 
+#ifdef CONFIG_VSX
+static inline int kvmppc_get_vsr_dword_offset(int index)
+{
+	int offset;
+
+	if ((index != 0) && (index != 1))
+		return -1;
+
+#ifdef __BIG_ENDIAN
+	offset =  index;
+#else
+	offset = 1 - index;
+#endif
+
+	return offset;
+}
+
+static inline int kvmppc_get_vsr_word_offset(int index)
+{
+	int offset;
+
+	if ((index > 3) || (index < 0))
+		return -1;
+
+#ifdef __BIG_ENDIAN
+	offset = index;
+#else
+	offset = 3 - index;
+#endif
+	return offset;
+}
+
+static inline void kvmppc_set_vsr_dword(struct kvm_vcpu *vcpu,
+	u64 gpr)
+{
+	union kvmppc_one_reg val;
+	int offset = kvmppc_get_vsr_dword_offset(vcpu->arch.mmio_vsx_offset);
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
+
+	if (offset == -1)
+		return;
+
+	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
+		val.vval = VCPU_VSX_VR(vcpu, index);
+		val.vsxval[offset] = gpr;
+		VCPU_VSX_VR(vcpu, index) = val.vval;
+	} else {
+		VCPU_VSX_FPR(vcpu, index, offset) = gpr;
+	}
+}
+
+static inline void kvmppc_set_vsr_dword_dump(struct kvm_vcpu *vcpu,
+	u64 gpr)
+{
+	union kvmppc_one_reg val;
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
+
+	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
+		val.vval = VCPU_VSX_VR(vcpu, index);
+		val.vsxval[0] = gpr;
+		val.vsxval[1] = gpr;
+		VCPU_VSX_VR(vcpu, index) = val.vval;
+	} else {
+		VCPU_VSX_FPR(vcpu, index, 0) = gpr;
+		VCPU_VSX_FPR(vcpu, index, 1) = gpr;
+	}
+}
+
+static inline void kvmppc_set_vsr_word(struct kvm_vcpu *vcpu,
+	u32 gpr32)
+{
+	union kvmppc_one_reg val;
+	int offset = kvmppc_get_vsr_word_offset(vcpu->arch.mmio_vsx_offset);
+	int index = vcpu->arch.io_gpr & KVM_MMIO_REG_MASK;
+	int dword_offset, word_offset;
+
+	if (offset == -1)
+		return;
+
+	if (vcpu->arch.mmio_vsx_tx_sx_enabled) {
+		val.vval = VCPU_VSX_VR(vcpu, index);
+		val.vsx32val[offset] = gpr32;
+		VCPU_VSX_VR(vcpu, index) = val.vval;
+	} else {
+		dword_offset = offset / 2;
+		word_offset = offset % 2;
+		val.vsxval[0] = VCPU_VSX_FPR(vcpu, index, dword_offset);
+		val.vsx32val[word_offset] = gpr32;
+		VCPU_VSX_FPR(vcpu, index, dword_offset) = val.vsxval[0];
+	}
+}
+#endif /* CONFIG_VSX */
+
+#ifdef CONFIG_PPC_FPU
+static inline u64 sp_to_dp(u32 fprs)
+{
+	u64 fprd;
+
+	preempt_disable();
+	enable_kernel_fp();
+	asm ("lfs%U1%X1 0,%1; stfd%U0%X0 0,%0" : "=m" (fprd) : "m" (fprs)
+	     : "fr0");
+	preempt_enable();
+	return fprd;
+}
+
+static inline u32 dp_to_sp(u64 fprd)
+{
+	u32 fprs;
+
+	preempt_disable();
+	enable_kernel_fp();
+	asm ("lfd%U1%X1 0,%1; stfs%U0%X0 0,%0" : "=m" (fprs) : "m" (fprd)
+	     : "fr0");
+	preempt_enable();
+	return fprs;
+}
+
+#else
+#define sp_to_dp(x)	(x)
+#define dp_to_sp(x)	(x)
+#endif /* CONFIG_PPC_FPU */
+
 static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
                                       struct kvm_run *run)
 {
@@ -827,6 +951,10 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		}
 	}
 
+	/* conversion between single and double precision */
+	if ((vcpu->arch.mmio_sp64_extend) && (run->mmio.len == 4))
+		gpr = sp_to_dp(gpr);
+
 	if (vcpu->arch.mmio_sign_extend) {
 		switch (run->mmio.len) {
 #ifdef CONFIG_PPC64
@@ -843,8 +971,6 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		}
 	}
 
-	kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
-
 	switch (vcpu->arch.io_gpr & KVM_MMIO_REG_EXT_MASK) {
 	case KVM_MMIO_REG_GPR:
 		kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
@@ -860,6 +986,17 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		VCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;
 		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
 		break;
+#endif
+#ifdef CONFIG_VSX
+	case KVM_MMIO_REG_VSX:
+		if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_DWORD)
+			kvmppc_set_vsr_dword(vcpu, gpr);
+		else if (vcpu->arch.mmio_vsx_copy_type == KVMPPC_VSX_COPY_WORD)
+			kvmppc_set_vsr_word(vcpu, gpr);
+		else if (vcpu->arch.mmio_vsx_copy_type ==
+				KVMPPC_VSX_COPY_DWORD_LOAD_DUMP)
+			kvmppc_set_vsr_dword_dump(vcpu, gpr);
+		break;
 #endif
 	default:
 		BUG();
@@ -927,6 +1064,35 @@ int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	return __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 1);
 }
 
+#ifdef CONFIG_VSX
+int kvmppc_handle_vsx_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+			unsigned int rt, unsigned int bytes,
+			int is_default_endian, int mmio_sign_extend)
+{
+	enum emulation_result emulated = EMULATE_DONE;
+
+	/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */
+	if ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||
+		(vcpu->arch.mmio_vsx_copy_nums < 0) ) {
+		return EMULATE_FAIL;
+	}
+
+	while (vcpu->arch.mmio_vsx_copy_nums) {
+		emulated = __kvmppc_handle_load(run, vcpu, rt, bytes,
+			is_default_endian, mmio_sign_extend);
+
+		if (emulated != EMULATE_DONE)
+			break;
+
+		vcpu->arch.paddr_accessed += run->mmio.len;
+
+		vcpu->arch.mmio_vsx_copy_nums--;
+		vcpu->arch.mmio_vsx_offset++;
+	}
+	return emulated;
+}
+#endif /* CONFIG_VSX */
+
 int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			u64 val, unsigned int bytes, int is_default_endian)
 {
@@ -952,6 +1118,9 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	vcpu->mmio_needed = 1;
 	vcpu->mmio_is_write = 1;
 
+	if ((vcpu->arch.mmio_sp64_extend) && (bytes == 4))
+		val = dp_to_sp(val);
+
 	/* Store the value at the lowest bytes in 'data'. */
 	if (!host_swabbed) {
 		switch (bytes) {
@@ -985,6 +1154,129 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvmppc_handle_store);
 
+#ifdef CONFIG_VSX
+static inline int kvmppc_get_vsr_data(struct kvm_vcpu *vcpu, int rs, u64 *val)
+{
+	u32 dword_offset, word_offset;
+	union kvmppc_one_reg reg;
+	int vsx_offset = 0;
+	int copy_type = vcpu->arch.mmio_vsx_copy_type;
+	int result = 0;
+
+	switch (copy_type) {
+	case KVMPPC_VSX_COPY_DWORD:
+		vsx_offset =
+			kvmppc_get_vsr_dword_offset(vcpu->arch.mmio_vsx_offset);
+
+		if (vsx_offset == -1) {
+			result = -1;
+			break;
+		}
+
+		if (!vcpu->arch.mmio_vsx_tx_sx_enabled) {
+			*val = VCPU_VSX_FPR(vcpu, rs, vsx_offset);
+		} else {
+			reg.vval = VCPU_VSX_VR(vcpu, rs);
+			*val = reg.vsxval[vsx_offset];
+		}
+		break;
+
+	case KVMPPC_VSX_COPY_WORD:
+		vsx_offset =
+			kvmppc_get_vsr_word_offset(vcpu->arch.mmio_vsx_offset);
+
+		if (vsx_offset == -1) {
+			result = -1;
+			break;
+		}
+
+		if (!vcpu->arch.mmio_vsx_tx_sx_enabled) {
+			dword_offset = vsx_offset / 2;
+			word_offset = vsx_offset % 2;
+			reg.vsxval[0] = VCPU_VSX_FPR(vcpu, rs, dword_offset);
+			*val = reg.vsx32val[word_offset];
+		} else {
+			reg.vval = VCPU_VSX_VR(vcpu, rs);
+			*val = reg.vsx32val[vsx_offset];
+		}
+		break;
+
+	default:
+		result = -1;
+		break;
+	}
+
+	return result;
+}
+
+int kvmppc_handle_vsx_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+			int rs, unsigned int bytes, int is_default_endian)
+{
+	u64 val;
+	enum emulation_result emulated = EMULATE_DONE;
+
+	vcpu->arch.io_gpr = rs;
+
+	/* Currently, mmio_vsx_copy_nums only allowed to be less than 4 */
+	if ( (vcpu->arch.mmio_vsx_copy_nums > 4) ||
+		(vcpu->arch.mmio_vsx_copy_nums < 0) ) {
+		return EMULATE_FAIL;
+	}
+
+	while (vcpu->arch.mmio_vsx_copy_nums) {
+		if (kvmppc_get_vsr_data(vcpu, rs, &val) == -1)
+			return EMULATE_FAIL;
+
+		emulated = kvmppc_handle_store(run, vcpu,
+			 val, bytes, is_default_endian);
+
+		if (emulated != EMULATE_DONE)
+			break;
+
+		vcpu->arch.paddr_accessed += run->mmio.len;
+
+		vcpu->arch.mmio_vsx_copy_nums--;
+		vcpu->arch.mmio_vsx_offset++;
+	}
+
+	return emulated;
+}
+
+static int kvmppc_emulate_mmio_vsx_loadstore(struct kvm_vcpu *vcpu,
+			struct kvm_run *run)
+{
+	enum emulation_result emulated = EMULATE_FAIL;
+	int r;
+
+	vcpu->arch.paddr_accessed += run->mmio.len;
+
+	if (!vcpu->mmio_is_write) {
+		emulated = kvmppc_handle_vsx_load(run, vcpu, vcpu->arch.io_gpr,
+			 run->mmio.len, 1, vcpu->arch.mmio_sign_extend);
+	} else {
+		emulated = kvmppc_handle_vsx_store(run, vcpu,
+			 vcpu->arch.io_gpr, run->mmio.len, 1);
+	}
+
+	switch (emulated) {
+	case EMULATE_DO_MMIO:
+		run->exit_reason = KVM_EXIT_MMIO;
+		r = RESUME_HOST;
+		break;
+	case EMULATE_FAIL:
+		pr_info("KVM: MMIO emulation failed (VSX repeat)\n");
+		run->exit_reason = KVM_EXIT_INTERNAL_ERROR;
+		run->internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
+		r = RESUME_HOST;
+		break;
+	default:
+		r = RESUME_GUEST;
+		break;
+	}
+	return r;
+}
+#endif /* CONFIG_VSX */
+
 int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 {
 	int r = 0;
@@ -1087,13 +1379,24 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	int r;
 	sigset_t sigsaved;
 
-	if (vcpu->sigset_active)
-		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
-
 	if (vcpu->mmio_needed) {
+		vcpu->mmio_needed = 0;
 		if (!vcpu->mmio_is_write)
 			kvmppc_complete_mmio_load(vcpu, run);
-		vcpu->mmio_needed = 0;
+#ifdef CONFIG_VSX
+		if (vcpu->arch.mmio_vsx_copy_nums > 0) {
+			vcpu->arch.mmio_vsx_copy_nums--;
+			vcpu->arch.mmio_vsx_offset++;
+		}
+
+		if (vcpu->arch.mmio_vsx_copy_nums > 0) {
+			r = kvmppc_emulate_mmio_vsx_loadstore(vcpu, run);
+			if (r == RESUME_HOST) {
+				vcpu->mmio_needed = 1;
+				return r;
+			}
+		}
+#endif
 	} else if (vcpu->arch.osi_needed) {
 		u64 *gprs = run->osi.gprs;
 		int i;
@@ -1115,6 +1418,9 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 #endif
 	}
 
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
+
 	if (run->immediate_exit)
 		r = -EINTR;
 	else

commit 3042255899540d4985a616017af9e2b80a84a8cf
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Fri Mar 31 13:53:22 2017 +0200

    kvm: make KVM_CAP_COALESCED_MMIO architecture agnostic
    
    Remove code from architecture files that can be moved to virt/kvm, since there
    is already common code for coalesced MMIO.
    
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    [Removed a pointless 'break' after 'return'.]
    Signed-off-by: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 95c91a9de351..0e42aa8a279f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -524,11 +524,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		/* We support this only for PR */
 		r = !hv_enabled;
 		break;
-#ifdef CONFIG_KVM_MMIO
-	case KVM_CAP_COALESCED_MMIO:
-		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
-		break;
-#endif
 #ifdef CONFIG_KVM_MPIC
 	case KVM_CAP_IRQ_MPIC:
 		r = 1;

commit 174cd4b1e5fbd0d74c68cf3a74f5bd4923485512
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 19:15:33 2017 +0100

    sched/headers: Prepare to move signal wakeup & sigpending methods from <linux/sched.h> into <linux/sched/signal.h>
    
    Fix up affected files that include this signal functionality via sched.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2b38d824e9e5..95c91a9de351 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -23,6 +23,7 @@
 #include <linux/kvm_host.h>
 #include <linux/vmalloc.h>
 #include <linux/hrtimer.h>
+#include <linux/sched/signal.h>
 #include <linux/fs.h>
 #include <linux/slab.h>
 #include <linux/file.h>

commit a26d553400b30b6e0389f5c723c0fc6f7ea473da
Merge: 843574a3ede1 bcd3bb63dbc8
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon Feb 20 11:54:22 2017 +0100

    Merge branch 'kvm-ppc-next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc into HEAD
    
    Paul Mackerras writes:
    "Please do a pull from my kvm-ppc-next branch to get some fixes which I
    would like to have in 4.11.  There are four small commits there; two
    are fixes for potential host crashes in the new HPT resizing code, and
    the other two are changes to printks to make KVM on PPC a little less
    noisy."

commit bcd3bb63dbc87a3bbb21e95a09cd26bb6479c332
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Sat Feb 18 08:30:44 2017 +1100

    KVM: PPC: Book3S HV: Disable HPT resizing on POWER9 for now
    
    The new HPT resizing code added in commit b5baa6877315 ("KVM: PPC:
    Book3S HV: KVM-HV HPT resizing implementation", 2016-12-20) doesn't
    have code to handle the new HPTE format which POWER9 uses.  Thus it
    would be best not to advertise it to userspace on POWER9 systems
    until it works properly.
    
    Also, since resize_hpt_rehash_hpte() contains BUG_ON() calls that
    could be hit on POWER9, let's prevent it from being called on POWER9
    for now.
    
    Acked-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2b3e4e620078..fcb253ba51e5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -613,7 +613,8 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = 1;
 		break;
 	case KVM_CAP_SPAPR_RESIZE_HPT:
-		r = !!hv_enabled;
+		/* Disable this on POWER9 until code handles new HPTE format */
+		r = !!hv_enabled && !cpu_has_feature(CPU_FTR_ARCH_300);
 		break;
 #endif
 	case KVM_CAP_PPC_HTM:

commit 460df4c1fc7c00829050c08d6368dc6e6beef307
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Feb 8 11:50:15 2017 +0100

    KVM: race-free exit from KVM_RUN without POSIX signals
    
    The purpose of the KVM_SET_SIGNAL_MASK API is to let userspace "kick"
    a VCPU out of KVM_RUN through a POSIX signal.  A signal is attached
    to a dummy signal handler; by blocking the signal outside KVM_RUN and
    unblocking it inside, this possible race is closed:
    
              VCPU thread                     service thread
       --------------------------------------------------------------
            check flag
                                              set flag
                                              raise signal
            (signal handler does nothing)
            KVM_RUN
    
    However, one issue with KVM_SET_SIGNAL_MASK is that it has to take
    tsk->sighand->siglock on every KVM_RUN.  This lock is often on a
    remote NUMA node, because it is on the node of a thread's creator.
    Taking this lock can be very expensive if there are many userspace
    exits (as is the case for SMP Windows VMs without Hyper-V reference
    time counter).
    
    As an alternative, we can put the flag directly in kvm_run so that
    KVM can see it:
    
              VCPU thread                     service thread
       --------------------------------------------------------------
                                              raise signal
            signal handler
              set run->immediate_exit
            KVM_RUN
              check run->immediate_exit
    
    Reviewed-by: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2b3e4e620078..1fe1391ba2c2 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -511,6 +511,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
+	case KVM_CAP_IMMEDIATE_EXIT:
 		r = 1;
 		break;
 	case KVM_CAP_PPC_PAIRED_SINGLES:
@@ -1117,7 +1118,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 #endif
 	}
 
-	r = kvmppc_vcpu_run(run, vcpu);
+	if (run->immediate_exit)
+		r = -EINTR;
+	else
+		r = kvmppc_vcpu_run(run, vcpu);
 
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);

commit 050f23390f6bdbfa7dd2800884d32490489851b7
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Dec 20 16:49:07 2016 +1100

    KVM: PPC: Book3S HV: Advertise availablity of HPT resizing on KVM HV
    
    This updates the KVM_CAP_SPAPR_RESIZE_HPT capability to advertise the
    presence of in-kernel HPT resizing on KVM HV.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 40a5b2d75ed1..2b3e4e620078 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -612,6 +612,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_SPAPR_MULTITCE:
 		r = 1;
 		break;
+	case KVM_CAP_SPAPR_RESIZE_HPT:
+		r = !!hv_enabled;
+		break;
 #endif
 	case KVM_CAP_PPC_HTM:
 		r = cpu_has_feature(CPU_FTR_TM_COMP) &&

commit 8cf4ecc0ca9bd9bdc9b4ca0a99f7445a1e74afed
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:53 2017 +1100

    KVM: PPC: Book3S HV: Enable radix guest support
    
    This adds a few last pieces of the support for radix guests:
    
    * Implement the backends for the KVM_PPC_CONFIGURE_V3_MMU and
      KVM_PPC_GET_RMMU_INFO ioctls for radix guests
    
    * On POWER9, allow secondary threads to be on/off-lined while guests
      are running.
    
    * Set up LPCR and the partition table entry for radix guests.
    
    * Don't allocate the rmap array in the kvm_memory_slot structure
      on radix.
    
    * Don't try to initialize the HPT for radix guests, since they don't
      have an HPT.
    
    * Take out the code that prevents the HV KVM module from
      initializing on radix hosts.
    
    At this stage, we only support radix guests if the host is running
    in radix mode, and only support HPT guests if the host is running in
    HPT mode.  Thus a guest cannot switch from one mode to the other,
    which enables some simplifications.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1476a480745e..40a5b2d75ed1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -566,7 +566,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = kvmppc_hwrng_present();
 		break;
 	case KVM_CAP_PPC_MMU_RADIX:
-		r = !!(0 && hv_enabled && radix_enabled());
+		r = !!(hv_enabled && radix_enabled());
 		break;
 	case KVM_CAP_PPC_MMU_HASH_V3:
 		r = !!(hv_enabled && !radix_enabled() &&

commit 468808bd35c4aa3cf7d9fde0ebb010270038734b
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:42 2017 +1100

    KVM: PPC: Book3S HV: Set process table for HPT guests on POWER9
    
    This adds the implementation of the KVM_PPC_CONFIGURE_V3_MMU ioctl
    for HPT guests on POWER9.  With this, we can return 1 for the
    KVM_CAP_PPC_MMU_HASH_V3 capability.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 38c0d154c01e..1476a480745e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -569,7 +569,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = !!(0 && hv_enabled && radix_enabled());
 		break;
 	case KVM_CAP_PPC_MMU_HASH_V3:
-		r = !!(0 && hv_enabled && !radix_enabled() &&
+		r = !!(hv_enabled && !radix_enabled() &&
 		       cpu_has_feature(CPU_FTR_ARCH_300));
 		break;
 #endif

commit c92701322711682de89b2bd0f32affad040b6e86
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Jan 30 21:21:41 2017 +1100

    KVM: PPC: Book3S HV: Add userspace interfaces for POWER9 MMU
    
    This adds two capabilities and two ioctls to allow userspace to
    find out about and configure the POWER9 MMU in a guest.  The two
    capabilities tell userspace whether KVM can support a guest using
    the radix MMU, or using the hashed page table (HPT) MMU with a
    process table and segment tables.  (Note that the MMUs in the
    POWER9 processor cores do not use the process and segment tables
    when in HPT mode, but the nest MMU does).
    
    The KVM_PPC_CONFIGURE_V3_MMU ioctl allows userspace to specify
    whether a guest will use the radix MMU or the HPT MMU, and to
    specify the size and location (in guest space) of the process
    table.
    
    The KVM_PPC_GET_RMMU_INFO ioctl gives userspace information about
    the radix MMU.  It returns a list of supported radix tree geometries
    (base page size and number of bits indexed at each level of the
    radix tree) and the encoding used to specify the various page
    sizes for the TLB invalidate entry instruction.
    
    Initially, both capabilities return 0 and the ioctls return -EINVAL,
    until the necessary infrastructure for them to operate correctly
    is added.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index cd892dec7cb6..38c0d154c01e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -565,6 +565,13 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_PPC_HWRNG:
 		r = kvmppc_hwrng_present();
 		break;
+	case KVM_CAP_PPC_MMU_RADIX:
+		r = !!(0 && hv_enabled && radix_enabled());
+		break;
+	case KVM_CAP_PPC_MMU_HASH_V3:
+		r = !!(0 && hv_enabled && !radix_enabled() &&
+		       cpu_has_feature(CPU_FTR_ARCH_300));
+		break;
 #endif
 	case KVM_CAP_SYNC_MMU:
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
@@ -1468,6 +1475,31 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_rtas_define_token(kvm, argp);
 		break;
 	}
+	case KVM_PPC_CONFIGURE_V3_MMU: {
+		struct kvm *kvm = filp->private_data;
+		struct kvm_ppc_mmuv3_cfg cfg;
+
+		r = -EINVAL;
+		if (!kvm->arch.kvm_ops->configure_mmu)
+			goto out;
+		r = -EFAULT;
+		if (copy_from_user(&cfg, argp, sizeof(cfg)))
+			goto out;
+		r = kvm->arch.kvm_ops->configure_mmu(kvm, &cfg);
+		break;
+	}
+	case KVM_PPC_GET_RMMU_INFO: {
+		struct kvm *kvm = filp->private_data;
+		struct kvm_ppc_rmmu_info info;
+
+		r = -EINVAL;
+		if (!kvm->arch.kvm_ops->get_rmmu_info)
+			goto out;
+		r = kvm->arch.kvm_ops->get_rmmu_info(kvm, &info);
+		if (r >= 0 && copy_to_user(argp, &info, sizeof(info)))
+			r = -EFAULT;
+		break;
+	}
 	default: {
 		struct kvm *kvm = filp->private_data;
 		r = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index efd1183a6b16..cd892dec7cb6 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -30,7 +30,7 @@
 #include <linux/irqbypass.h>
 #include <linux/kvm_irqfd.h>
 #include <asm/cputable.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 #include <asm/kvm_ppc.h>
 #include <asm/tlbflush.h>
 #include <asm/cputhreads.h>

commit a8acaece5d88db234d0b82b8692dea15d602f622
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Nov 23 16:14:07 2016 +1100

    KVM: PPC: Correctly report KVM_CAP_PPC_ALLOC_HTAB
    
    At present KVM on powerpc always reports KVM_CAP_PPC_ALLOC_HTAB as enabled.
    However, the ioctl() it advertises (KVM_PPC_ALLOCATE_HTAB) only actually
    works on KVM HV.  On KVM PR it will fail with ENOTTY.
    
    QEMU already has a workaround for this, so it's not breaking things in
    practice, but it would be better to advertise this correctly.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b5e470571470..efd1183a6b16 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -536,7 +536,6 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_SPAPR_TCE_64:
-	case KVM_CAP_PPC_ALLOC_HTAB:
 	case KVM_CAP_PPC_RTAS:
 	case KVM_CAP_PPC_FIXUP_HCALL:
 	case KVM_CAP_PPC_ENABLE_HCALL:
@@ -545,6 +544,10 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #endif
 		r = 1;
 		break;
+
+	case KVM_CAP_PPC_ALLOC_HTAB:
+		r = hv_enabled;
+		break;
 #endif /* CONFIG_PPC_BOOK3S_64 */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:

commit 45c940ba490df28cb87b993981a5f63df6bbb8db
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Fri Nov 18 17:43:30 2016 +1100

    KVM: PPC: Book3S HV: Treat POWER9 CPU threads as independent subcores
    
    With POWER9, each CPU thread has its own MMU context and can be
    in the host or a guest independently of the other threads; there is
    still however a restriction that all threads must use the same type
    of address translation, either radix tree or hashed page table (HPT).
    
    Since we only support HPT guests on a HPT host at this point, we
    can treat the threads as being independent, and avoid all of the
    work of coordinating the CPU threads.  To make this simpler, we
    introduce a new threads_per_vcore() function that returns 1 on
    POWER9 and threads_per_subcore on POWER7/8, and use that instead
    of threads_per_subcore or threads_per_core in various places.
    
    This also changes the value of the KVM_CAP_PPC_SMT capability on
    POWER9 systems from 4 to 1, so that userspace will not try to
    create VMs with multiple vcpus per vcore.  (If userspace did create
    a VM that thought it was in an SMT mode, the VM might try to use
    the msgsndp instruction, which will not work as expected.  In
    future it may be possible to trap and emulate msgsndp in order to
    allow VMs to think they are in an SMT mode, if only for the purpose
    of allowing migration from POWER8 systems.)
    
    With all this, we can now run guests on POWER9 as long as the host
    is running with HPT translation.  Since userspace currently has no
    way to request radix tree translation for the guest, the guest has
    no choice but to use HPT translation.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 70963c845e96..b5e470571470 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -548,10 +548,13 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 #endif /* CONFIG_PPC_BOOK3S_64 */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:
-		if (hv_enabled)
-			r = threads_per_subcore;
-		else
-			r = 0;
+		r = 0;
+		if (hv_enabled) {
+			if (cpu_has_feature(CPU_FTR_ARCH_300))
+				r = 1;
+			else
+				r = threads_per_subcore;
+		}
 		break;
 	case KVM_CAP_PPC_RMA:
 		r = 0;

commit 235539b48a2357da28f52d66d04bec04f3dcb9dd
Author: Luiz Capitulino <lcapitulino@redhat.com>
Date:   Wed Sep 7 14:47:23 2016 -0400

    kvm: add stubs for arch specific debugfs support
    
    Two stubs are added:
    
     o kvm_arch_has_vcpu_debugfs(): must return true if the arch
       supports creating debugfs entries in the vcpu debugfs dir
       (which will be implemented by the next commit)
    
     o kvm_arch_create_vcpu_debugfs(): code that creates debugfs
       entries in the vcpu debugfs dir
    
    For x86, this commit introduces a new file to avoid growing
    arch/x86/kvm/x86.c even more.
    
    Signed-off-by: Luiz Capitulino <lcapitulino@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0b7d66422650..70963c845e96 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -438,6 +438,16 @@ int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 	return -EINVAL;
 }
 
+bool kvm_arch_has_vcpu_debugfs(void)
+{
+	return false;
+}
+
+int kvm_arch_create_vcpu_debugfs(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	unsigned int i;

commit 9576730d0e6e301343c5aead5418ad53fcecfd14
Author: Suresh Warrier <warrier@linux.vnet.ibm.com>
Date:   Fri Aug 19 15:35:47 2016 +1000

    KVM: PPC: select IRQ_BYPASS_MANAGER
    
    Select IRQ_BYPASS_MANAGER for PPC when CONFIG_KVM is set.
    Add the PPC producer functions for add and del producer.
    
    [paulus@ozlabs.org - Moved new functions from book3s.c to powerpc.c
     so booke compiles; added kvm_arch_has_irq_bypass implementation.]
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ab2b3d544937..0b7d66422650 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -27,6 +27,8 @@
 #include <linux/slab.h>
 #include <linux/file.h>
 #include <linux/module.h>
+#include <linux/irqbypass.h>
+#include <linux/kvm_irqfd.h>
 #include <asm/cputable.h>
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>
@@ -739,6 +741,42 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 #endif
 }
 
+/*
+ * irq_bypass_add_producer and irq_bypass_del_producer are only
+ * useful if the architecture supports PCI passthrough.
+ * irq_bypass_stop and irq_bypass_start are not needed and so
+ * kvm_ops are not defined for them.
+ */
+bool kvm_arch_has_irq_bypass(void)
+{
+	return ((kvmppc_hv_ops && kvmppc_hv_ops->irq_bypass_add_producer) ||
+		(kvmppc_pr_ops && kvmppc_pr_ops->irq_bypass_add_producer));
+}
+
+int kvm_arch_irq_bypass_add_producer(struct irq_bypass_consumer *cons,
+				     struct irq_bypass_producer *prod)
+{
+	struct kvm_kernel_irqfd *irqfd =
+		container_of(cons, struct kvm_kernel_irqfd, consumer);
+	struct kvm *kvm = irqfd->kvm;
+
+	if (kvm->arch.kvm_ops->irq_bypass_add_producer)
+		return kvm->arch.kvm_ops->irq_bypass_add_producer(cons, prod);
+
+	return 0;
+}
+
+void kvm_arch_irq_bypass_del_producer(struct irq_bypass_consumer *cons,
+				      struct irq_bypass_producer *prod)
+{
+	struct kvm_kernel_irqfd *irqfd =
+		container_of(cons, struct kvm_kernel_irqfd, consumer);
+	struct kvm *kvm = irqfd->kvm;
+
+	if (kvm->arch.kvm_ops->irq_bypass_del_producer)
+		kvm->arch.kvm_ops->irq_bypass_del_producer(cons, prod);
+}
+
 static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
                                       struct kvm_run *run)
 {

commit 34a75b0f63356097ae9f706d64a793934891002f
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Aug 10 11:27:27 2016 +1000

    KVM: PPC: Implement kvm_arch_intc_initialized() for PPC
    
    It doesn't make sense to create irqfds for a VM that doesn't have
    in-kernel interrupt controller emulation.  There is an existing
    interface for architecture code to tell the irqfd code whether or
    not any interrupt controller has been initialized, called
    kvm_arch_intc_initialized(), so let's implement that for powerpc.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6ce40dd6fe51..ab2b3d544937 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1167,6 +1167,19 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+bool kvm_arch_intc_initialized(struct kvm *kvm)
+{
+#ifdef CONFIG_KVM_MPIC
+	if (kvm->arch.mpic)
+		return true;
+#endif
+#ifdef CONFIG_KVM_XICS
+	if (kvm->arch.xics)
+		return true;
+#endif
+	return false;
+}
+
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
                                     struct kvm_mp_state *mp_state)
 {

commit 23528bb21ee2c9b27f3feddd77a2a3351a8df148
Author: Sam Bobroff <sam.bobroff@au1.ibm.com>
Date:   Wed Jul 20 13:41:36 2016 +1000

    KVM: PPC: Introduce KVM_CAP_PPC_HTM
    
    Introduce a new KVM capability, KVM_CAP_PPC_HTM, that can be queried to
    determine if a PowerPC KVM guest should use HTM (Hardware Transactional
    Memory).
    
    This will be used by QEMU to populate the pa-features bits in the
    guest's device tree.
    
    Signed-off-by: Sam Bobroff <sam.bobroff@au1.ibm.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1ac036e45ed4..6ce40dd6fe51 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -588,6 +588,10 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		r = 1;
 		break;
 #endif
+	case KVM_CAP_PPC_HTM:
+		r = cpu_has_feature(CPU_FTR_TM_COMP) &&
+		    is_kvmppc_hv_enabled(kvm);
+		break;
 	default:
 		r = 0;
 		break;

commit 6edaa5307f3f51e4e56dc4c63f68a69d88c6ddf5
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Jun 15 15:18:26 2016 +0200

    KVM: remove kvm_guest_enter/exit wrappers
    
    Use the functions from context_tracking.h directly.
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 02416fea7653..1ac036e45ed4 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -119,7 +119,7 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
-		__kvm_guest_enter();
+		guest_enter_irqoff();
 		return 1;
 	}
 

commit eb8b0560160d8363bc40ca05a75ef144cebaae72
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu May 5 16:17:10 2016 +1000

    KVM: PPC: Fix emulated MMIO sign-extension
    
    When the guest does a sign-extending load instruction (such as lha
    or lwa) to an emulated MMIO location, it results in a call to
    kvmppc_handle_loads() in the host.  That function sets the
    vcpu->arch.mmio_sign_extend flag and calls kvmppc_handle_load()
    to do the rest of the work.  However, kvmppc_handle_load() sets
    the mmio_sign_extend flag to 0 unconditionally, so the sign
    extension never gets done.
    
    To fix this, we rename kvmppc_handle_load to __kvmppc_handle_load
    and add an explicit parameter to indicate whether sign extension
    is required.  kvmppc_handle_load() and kvmppc_handle_loads() then
    become 1-line functions that just call __kvmppc_handle_load()
    with the extra parameter.
    
    Reported-by: Bin Lu <lblulb@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6a68730774ee..02416fea7653 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -800,9 +800,9 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 	}
 }
 
-int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
-		       unsigned int rt, unsigned int bytes,
-		       int is_default_endian)
+static int __kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+				unsigned int rt, unsigned int bytes,
+				int is_default_endian, int sign_extend)
 {
 	int idx, ret;
 	bool host_swabbed;
@@ -827,7 +827,7 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	vcpu->arch.mmio_host_swabbed = host_swabbed;
 	vcpu->mmio_needed = 1;
 	vcpu->mmio_is_write = 0;
-	vcpu->arch.mmio_sign_extend = 0;
+	vcpu->arch.mmio_sign_extend = sign_extend;
 
 	idx = srcu_read_lock(&vcpu->kvm->srcu);
 
@@ -844,6 +844,13 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 	return EMULATE_DO_MMIO;
 }
+
+int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+		       unsigned int rt, unsigned int bytes,
+		       int is_default_endian)
+{
+	return __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 0);
+}
 EXPORT_SYMBOL_GPL(kvmppc_handle_load);
 
 /* Same as above, but sign extends */
@@ -851,12 +858,7 @@ int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
 			unsigned int rt, unsigned int bytes,
 			int is_default_endian)
 {
-	int r;
-
-	vcpu->arch.mmio_sign_extend = 1;
-	r = kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian);
-
-	return r;
+	return __kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian, 1);
 }
 
 int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,

commit 489153c746746fc44b1e2fd184248dee8aebbaff
Author: Lan Tianyu <tianyu.lan@intel.com>
Date:   Sun Mar 13 11:10:30 2016 +0800

    KVM/PPC: update the comment of memory barrier in the kvmppc_prepare_to_enter()
    
    The barrier also orders the write to mode from any reads
    to the page tables done and so update the comment.
    
    Signed-off-by: Lan Tianyu <tianyu.lan@intel.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 19aa59b0850c..6a68730774ee 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -96,6 +96,9 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 		 * so we don't miss a request because the requester sees
 		 * OUTSIDE_GUEST_MODE and assumes we'll be checking requests
 		 * before next entering the guest (and thus doesn't IPI).
+		 * This also orders the write to mode from any reads
+		 * to the page tables done while the VCPU is running.
+		 * Please see the comment in kvm_flush_remote_tlbs.
 		 */
 		smp_mb();
 

commit 58ded4201ff028b15f6b317228faa5f154a0663f
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Tue Mar 1 17:54:40 2016 +1100

    KVM: PPC: Add support for 64bit TCE windows
    
    The existing KVM_CREATE_SPAPR_TCE only supports 32bit windows which is not
    enough for directly mapped windows as the guest can get more than 4GB.
    
    This adds KVM_CREATE_SPAPR_TCE_64 ioctl and advertises it
    via KVM_CAP_SPAPR_TCE_64 capability. The table size is checked against
    the locked memory limit.
    
    Since 64bit windows are to support Dynamic DMA windows (DDW), let's add
    @bus_offset and @page_shift which are also required by DDW.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9258675e2ff7..19aa59b0850c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -33,6 +33,7 @@
 #include <asm/tlbflush.h>
 #include <asm/cputhreads.h>
 #include <asm/irqflags.h>
+#include <asm/iommu.h>
 #include "timing.h"
 #include "irq.h"
 #include "../mm/mmu_decl.h"
@@ -519,6 +520,7 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
+	case KVM_CAP_SPAPR_TCE_64:
 	case KVM_CAP_PPC_ALLOC_HTAB:
 	case KVM_CAP_PPC_RTAS:
 	case KVM_CAP_PPC_FIXUP_HCALL:
@@ -1344,13 +1346,34 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		break;
 	}
 #ifdef CONFIG_PPC_BOOK3S_64
+	case KVM_CREATE_SPAPR_TCE_64: {
+		struct kvm_create_spapr_tce_64 create_tce_64;
+
+		r = -EFAULT;
+		if (copy_from_user(&create_tce_64, argp, sizeof(create_tce_64)))
+			goto out;
+		if (create_tce_64.flags) {
+			r = -EINVAL;
+			goto out;
+		}
+		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce_64);
+		goto out;
+	}
 	case KVM_CREATE_SPAPR_TCE: {
 		struct kvm_create_spapr_tce create_tce;
+		struct kvm_create_spapr_tce_64 create_tce_64;
 
 		r = -EFAULT;
 		if (copy_from_user(&create_tce, argp, sizeof(create_tce)))
 			goto out;
-		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);
+
+		create_tce_64.liobn = create_tce.liobn;
+		create_tce_64.page_shift = IOMMU_PAGE_SHIFT_4K;
+		create_tce_64.offset = 0;
+		create_tce_64.size = create_tce.window_size >>
+				IOMMU_PAGE_SHIFT_4K;
+		create_tce_64.flags = 0;
+		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce_64);
 		goto out;
 	}
 	case KVM_PPC_GET_SMMU_INFO: {

commit e17769eb8c897101e2c6df62ec397e450b6e53b4
Author: Suresh E. Warrier <warrier@linux.vnet.ibm.com>
Date:   Mon Dec 21 16:22:51 2015 -0600

    KVM: PPC: Book3S HV: Send IPI to host core to wake VCPU
    
    This patch adds support to real-mode KVM to search for a core
    running in the host partition and send it an IPI message with
    VCPU to be woken. This avoids having to switch to the host
    partition to complete an H_IPI hypercall when the VCPU which
    is the target of the the H_IPI is not loaded (is not running
    in the guest).
    
    The patch also includes the support in the IPI handler running
    in the host to do the wakeup by calling kvmppc_xics_ipi_action
    for the PPC_MSG_RM_HOST_ACTION message.
    
    When a guest is being destroyed, we need to ensure that there
    are no pending IPIs waiting to wake up a VCPU before we free
    the VCPUs of the guest. This is accomplished by:
    - Forces a PPC_MSG_CALL_FUNCTION IPI to be completed by all CPUs
      before freeing any VCPUs in kvm_arch_destroy_vm().
    - Any PPC_MSG_RM_HOST_ACTION messages must be executed first
      before any other PPC_MSG_CALL_FUNCTION messages.
    
    Signed-off-by: Suresh Warrier <warrier@linux.vnet.ibm.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 69f897da782d..9258675e2ff7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -437,6 +437,16 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	unsigned int i;
 	struct kvm_vcpu *vcpu;
 
+#ifdef CONFIG_KVM_XICS
+	/*
+	 * We call kick_all_cpus_sync() to ensure that all
+	 * CPUs have executed any pending IPIs before we
+	 * continue and free VCPUs structures below.
+	 */
+	if (is_kvmppc_hv_enabled(kvm))
+		kick_all_cpus_sync();
+#endif
+
 	kvm_for_each_vcpu(i, vcpu, kvm)
 		kvm_arch_vcpu_free(vcpu);
 

commit d3695aa4f452bc09c834a5010484f65fca37d87c
Author: Alexey Kardashevskiy <aik@ozlabs.ru>
Date:   Mon Feb 15 12:55:09 2016 +1100

    KVM: PPC: Add support for multiple-TCE hcalls
    
    This adds real and virtual mode handlers for the H_PUT_TCE_INDIRECT and
    H_STUFF_TCE hypercalls for user space emulated devices such as IBMVIO
    devices or emulated PCI. These calls allow adding multiple entries
    (up to 512) into the TCE table in one call which saves time on
    transition between kernel and user space.
    
    The current implementation of kvmppc_h_stuff_tce() allows it to be
    executed in both real and virtual modes so there is one helper.
    The kvmppc_rm_h_put_tce_indirect() needs to translate the guest address
    to the host address and since the translation is different, there are
    2 helpers - one for each mode.
    
    This implements the KVM_CAP_PPC_MULTITCE capability. When present,
    the kernel will try handling H_PUT_TCE_INDIRECT and H_STUFF_TCE if these
    are enabled by the userspace via KVM_CAP_PPC_ENABLE_HCALL.
    If they can not be handled by the kernel, they are passed on to
    the user space. The user space still has to have an implementation
    for these.
    
    Both HV and PR-syle KVM are supported.
    
    Signed-off-by: Alexey Kardashevskiy <aik@ozlabs.ru>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a3b182dcb823..69f897da782d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -569,6 +569,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_PPC_GET_SMMU_INFO:
 		r = 1;
 		break;
+	case KVM_CAP_SPAPR_MULTITCE:
+		r = 1;
+		break;
 #endif
 	default:
 		r = 0;

commit b4d7f161feb3015d6306e1d35b565c888ff70c9d
Author: Greg Kurz <gkurz@linux.vnet.ibm.com>
Date:   Wed Jan 13 18:28:17 2016 +0100

    KVM: PPC: Fix ONE_REG AltiVec support
    
    The get and set operations got exchanged by mistake when moving the
    code from book3s.c to powerpc.c.
    
    Fixes: 3840edc8033ad5b86deee309c1c321ca54257452
    Cc: stable@vger.kernel.org # 3.18+
    Signed-off-by: Greg Kurz <gkurz@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6fd2405c7f4a..a3b182dcb823 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -919,21 +919,17 @@ int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 				r = -ENXIO;
 				break;
 			}
-			vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0] = val.vval;
+			val.vval = vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0];
 			break;
 		case KVM_REG_PPC_VSCR:
 			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
 				r = -ENXIO;
 				break;
 			}
-			vcpu->arch.vr.vscr.u[3] = set_reg_val(reg->id, val);
+			val = get_reg_val(reg->id, vcpu->arch.vr.vscr.u[3]);
 			break;
 		case KVM_REG_PPC_VRSAVE:
-			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
-				r = -ENXIO;
-				break;
-			}
-			vcpu->arch.vrsave = set_reg_val(reg->id, val);
+			val = get_reg_val(reg->id, vcpu->arch.vrsave);
 			break;
 #endif /* CONFIG_ALTIVEC */
 		default:
@@ -974,17 +970,21 @@ int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 				r = -ENXIO;
 				break;
 			}
-			val.vval = vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0];
+			vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0] = val.vval;
 			break;
 		case KVM_REG_PPC_VSCR:
 			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
 				r = -ENXIO;
 				break;
 			}
-			val = get_reg_val(reg->id, vcpu->arch.vr.vscr.u[3]);
+			vcpu->arch.vr.vscr.u[3] = set_reg_val(reg->id, val);
 			break;
 		case KVM_REG_PPC_VRSAVE:
-			val = get_reg_val(reg->id, vcpu->arch.vrsave);
+			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
+				r = -ENXIO;
+				break;
+			}
+			vcpu->arch.vrsave = set_reg_val(reg->id, val);
 			break;
 #endif /* CONFIG_ALTIVEC */
 		default:

commit bfec5c2cc0adad3b343281eb4f9b94222c5f594f
Author: Nikunj A Dadhania <nikunj@linux.vnet.ibm.com>
Date:   Fri Oct 16 10:27:53 2015 +0530

    KVM: PPC: Implement extension to report number of memslots
    
    QEMU assumes 32 memslots if this extension is not implemented. Although,
    current value of KVM_USER_MEM_SLOTS is 32, once KVM_USER_MEM_SLOTS
    changes QEMU would take a wrong value.
    
    Signed-off-by: Nikunj A Dadhania <nikunj@linux.vnet.ibm.com>
    Reviewed-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2e51289610e4..6fd2405c7f4a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -559,6 +559,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 		else
 			r = num_online_cpus();
 		break;
+	case KVM_CAP_NR_MEMSLOTS:
+		r = KVM_USER_MEM_SLOTS;
+		break;
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
 		break;

commit 5358a96341a7fba23cbf0eaf01ce1ab4d738fc90
Author: Thomas Huth <thuth@redhat.com>
Date:   Fri May 22 09:25:02 2015 +0200

    KVM: PPC: Fix warnings from sparse
    
    When compiling the KVM code for POWER with "make C=1", sparse
    complains about functions missing proper prototypes and a 64-bit
    constant missing the ULL prefix. Let's fix this by making the
    functions static or by including the proper header with the
    prototypes, and by appending a ULL prefix to the constant
    PPC_MPPE_ADDRESS_MASK.
    
    Signed-off-by: Thomas Huth <thuth@redhat.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e5dde32fe71f..2e51289610e4 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -660,7 +660,7 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 	return kvmppc_core_pending_dec(vcpu);
 }
 
-enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)
+static enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)
 {
 	struct kvm_vcpu *vcpu;
 

commit f36f3f2846b5578d62910ee0b6dbef59fdd1cfa4
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:20:23 2015 +0200

    KVM: add "new" argument to kvm_arch_commit_memory_region
    
    This lets the function access the new memory slot without going through
    kvm_memslots and id_to_memslot.  It will simplify the code when more
    than one address space will be supported.
    
    Unfortunately, the "const"ness of the new argument must be casted
    away in two places.  Fixing KVM to accept const struct kvm_memory_slot
    pointers would require modifications in pretty much all architectures,
    and is left for later.
    
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 5985bb2a332b..e5dde32fe71f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -604,9 +604,10 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				   const struct kvm_userspace_memory_region *mem,
 				   const struct kvm_memory_slot *old,
+				   const struct kvm_memory_slot *new,
 				   enum kvm_mr_change change)
 {
-	kvmppc_core_commit_memory_region(kvm, mem, old);
+	kvmppc_core_commit_memory_region(kvm, mem, old, new);
 }
 
 void kvm_arch_flush_shadow_memslot(struct kvm *kvm,

commit 09170a49422bd786be3eac5cec1955257c5a34b7
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Mon May 18 13:59:39 2015 +0200

    KVM: const-ify uses of struct kvm_userspace_memory_region
    
    Architecture-specific helpers are not supposed to muck with
    struct kvm_userspace_memory_region contents.  Add const to
    enforce this.
    
    In order to eliminate the only write in __kvm_set_memory_region,
    the cleaning of deleted slots is pulled up from update_memslots
    to __kvm_set_memory_region.
    
    Reviewed-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Reviewed-by: Radim Krcmar <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8cd1f80fdc70..5985bb2a332b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -595,14 +595,14 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
-				   struct kvm_userspace_memory_region *mem,
+				   const struct kvm_userspace_memory_region *mem,
 				   enum kvm_mr_change change)
 {
 	return kvmppc_core_prepare_memory_region(kvm, memslot, mem);
 }
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
-				   struct kvm_userspace_memory_region *mem,
+				   const struct kvm_userspace_memory_region *mem,
 				   const struct kvm_memory_slot *old,
 				   enum kvm_mr_change change)
 {

commit ccf73aaf5adfa37f45be12459c17f534e8f2c2c5
Author: Christian Borntraeger <borntraeger@de.ibm.com>
Date:   Thu Apr 30 13:43:31 2015 +0200

    KVM: arm/mips/x86/power use __kvm_guest_{enter|exit}
    
    Use __kvm_guest_{enter|exit} instead of kvm_guest_{enter|exit}
    where interrupts are disabled.
    
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ac3ddf115f3d..8cd1f80fdc70 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -115,7 +115,7 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
-		kvm_guest_enter();
+		__kvm_guest_enter();
 		return 1;
 	}
 

commit eadf16a912b6bdf8bd476bde2f19fb41d06e0c3b
Merge: 4a6554665c62 2fa462f82621
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 26 13:06:22 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm
    
    Pull second batch of KVM changes from Paolo Bonzini:
     "This mostly includes the PPC changes for 4.1, which this time cover
      Book3S HV only (debugging aids, minor performance improvements and
      some cleanups).  But there are also bug fixes and small cleanups for
      ARM, x86 and s390.
    
      The task_migration_notifier revert and real fix is still pending
      review, but I'll send it as soon as possible after -rc1"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/virt/kvm/kvm: (29 commits)
      KVM: arm/arm64: check IRQ number on userland injection
      KVM: arm: irqfd: fix value returned by kvm_irq_map_gsi
      KVM: VMX: Preserve host CR4.MCE value while in guest mode.
      KVM: PPC: Book3S HV: Use msgsnd for signalling threads on POWER8
      KVM: PPC: Book3S HV: Translate kvmhv_commence_exit to C
      KVM: PPC: Book3S HV: Streamline guest entry and exit
      KVM: PPC: Book3S HV: Use bitmap of active threads rather than count
      KVM: PPC: Book3S HV: Use decrementer to wake napping threads
      KVM: PPC: Book3S HV: Don't wake thread with no vcpu on guest IPI
      KVM: PPC: Book3S HV: Get rid of vcore nap_count and n_woken
      KVM: PPC: Book3S HV: Move vcore preemption point up into kvmppc_run_vcpu
      KVM: PPC: Book3S HV: Minor cleanups
      KVM: PPC: Book3S HV: Simplify handling of VCPUs that need a VPA update
      KVM: PPC: Book3S HV: Accumulate timing information for real-mode code
      KVM: PPC: Book3S HV: Create debugfs file for each guest's HPT
      KVM: PPC: Book3S HV: Add ICP real mode counters
      KVM: PPC: Book3S HV: Move virtual mode ICP functions to real-mode
      KVM: PPC: Book3S HV: Convert ICS mutex lock to spin lock
      KVM: PPC: Book3S HV: Add guest->host real mode completion counters
      KVM: PPC: Book3S HV: Add helpers for lock/unlock hpte
      ...

commit e928e9cb3601ce240189bfea05b67ebd391c85ae
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Fri Mar 20 20:39:41 2015 +1100

    KVM: PPC: Book3S HV: Add fast real-mode H_RANDOM implementation.
    
    Some PowerNV systems include a hardware random-number generator.
    This HWRNG is present on POWER7+ and POWER8 chips and is capable of
    generating one 64-bit random number every microsecond.  The random
    numbers are produced by sampling a set of 64 unstable high-frequency
    oscillators and are almost completely entropic.
    
    PAPR defines an H_RANDOM hypercall which guests can use to obtain one
    64-bit random sample from the HWRNG.  This adds a real-mode
    implementation of the H_RANDOM hypercall.  This hypercall was
    implemented in real mode because the latency of reading the HWRNG is
    generally small compared to the latency of a guest exit and entry for
    all the threads in the same virtual core.
    
    Userspace can detect the presence of the HWRNG and the H_RANDOM
    implementation by querying the KVM_CAP_PPC_HWRNG capability.  The
    H_RANDOM hypercall implementation will only be invoked when the guest
    does an H_RANDOM hypercall if userspace first enables the in-kernel
    H_RANDOM implementation using the KVM_CAP_PPC_ENABLE_HCALL capability.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 24bfe401373e..55a4763d6d11 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -529,6 +529,9 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 	case KVM_CAP_PPC_RMA:
 		r = 0;
 		break;
+	case KVM_CAP_PPC_HWRNG:
+		r = kvmppc_hwrng_present();
+		break;
 #endif
 	case KVM_CAP_SYNC_MMU:
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE

commit d19d5efd8c8840aa4f38a6dfbfe500d8cc27de46
Merge: 34c9a0ffc75a 2fe0753d4940
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 16 13:53:32 2015 -0500

    Merge tag 'powerpc-4.1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Numerous minor fixes, cleanups etc.
    
     - More EEH work from Gavin to remove its dependency on device_nodes.
    
     - Memory hotplug implemented entirely in the kernel from Nathan
       Fontenot.
    
     - Removal of redundant CONFIG_PPC_OF by Kevin Hao.
    
     - Rewrite of VPHN parsing logic & tests from Greg Kurz.
    
     - A fix from Nish Aravamudan to reduce memory usage by clamping
       nodes_possible_map.
    
     - Support for pstore on powernv from Hari Bathini.
    
     - Removal of old powerpc specific byte swap routines by David Gibson.
    
     - Fix from Vasant Hegde to prevent the flash driver telling you it was
       flashing your firmware when it wasn't.
    
     - Patch from Ben Herrenschmidt to add an OPAL heartbeat driver.
    
     - Fix for an oops causing get/put_cpu_var() imbalance in perf by Jan
       Stancek.
    
     - Some fixes for migration from Tyrel Datwyler.
    
     - A new syscall to switch the cpu endian by Michael Ellerman.
    
     - Large series from Wei Yang to implement SRIOV, reviewed and acked by
       Bjorn.
    
     - A fix for the OPAL sensor driver from CÃ©dric Le Goater.
    
     - Fixes to get STRICT_MM_TYPECHECKS building again by Michael Ellerman.
    
     - Large series from Daniel Axtens to make our PCI hooks per PHB rather
       than per machine.
    
     - Small patch from Sam Bobroff to explicitly abort non-suspended
       transactions on syscalls, plus a test to exercise it.
    
     - Numerous reworks and fixes for the 24x7 PMU from Sukadev Bhattiprolu.
    
     - Small patch to enable the hard lockup detector from Anton Blanchard.
    
     - Fix from Dave Olson for missing L2 cache information on some CPUs.
    
     - Some fixes from Michael Ellerman to get Cell machines booting again.
    
     - Freescale updates from Scott: Highlights include BMan device tree
       nodes, an MSI erratum workaround, a couple minor performance
       improvements, config updates, and misc fixes/cleanup.
    
    * tag 'powerpc-4.1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/mpe/linux: (196 commits)
      powerpc/powermac: Fix build error seen with powermac smp builds
      powerpc/pseries: Fix compile of memory hotplug without CONFIG_MEMORY_HOTREMOVE
      powerpc: Remove PPC32 code from pseries specific find_and_init_phbs()
      powerpc/cell: Fix iommu breakage caused by controller_ops change
      powerpc/eeh: Fix crash in eeh_add_device_early() on Cell
      powerpc/perf: Cap 64bit userspace backtraces to PERF_MAX_STACK_DEPTH
      powerpc/perf/hv-24x7: Fail 24x7 initcall if create_events_from_catalog() fails
      powerpc/pseries: Correct memory hotplug locking
      powerpc: Fix missing L2 cache size in /sys/devices/system/cpu
      powerpc: Add ppc64 hard lockup detector support
      oprofile: Disable oprofile NMI timer on ppc64
      powerpc/perf/hv-24x7: Add missing put_cpu_var()
      powerpc/perf/hv-24x7: Break up single_24x7_request
      powerpc/perf/hv-24x7: Define update_event_count()
      powerpc/perf/hv-24x7: Whitespace cleanup
      powerpc/perf/hv-24x7: Define add_event_to_24x7_request()
      powerpc/perf/hv-24x7: Rename hv_24x7_event_update
      powerpc/perf/hv-24x7: Move debug prints to separate function
      powerpc/perf/hv-24x7: Drop event_24x7_request()
      powerpc/perf/hv-24x7: Use pr_devel() to log message
      ...
    
    Conflicts:
            tools/testing/selftests/powerpc/Makefile
            tools/testing/selftests/powerpc/tm/Makefile

commit e32edf4fd0fa4897e12ca66118ab67bf257e16e4
Author: Nikolay Nikolaev <n.nikolaev@virtualopensystems.com>
Date:   Thu Mar 26 14:39:28 2015 +0000

    KVM: Redesign kvm_io_bus_ API to pass VCPU structure to the callbacks.
    
    This is needed in e.g. ARM vGIC emulation, where the MMIO handling
    depends on the VCPU that does the access.
    
    Signed-off-by: Nikolay Nikolaev <n.nikolaev@virtualopensystems.com>
    Signed-off-by: Andre Przywara <andre.przywara@arm.com>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Acked-by: Christoffer Dall <christoffer.dall@linaro.org>
    Reviewed-by: Marc Zyngier <marc.zyngier@arm.com>
    Signed-off-by: Marc Zyngier <marc.zyngier@arm.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 27c0face86f4..24bfe401373e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -807,7 +807,7 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 	idx = srcu_read_lock(&vcpu->kvm->srcu);
 
-	ret = kvm_io_bus_read(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
+	ret = kvm_io_bus_read(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
 			      bytes, &run->mmio.data);
 
 	srcu_read_unlock(&vcpu->kvm->srcu, idx);
@@ -880,7 +880,7 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 	idx = srcu_read_lock(&vcpu->kvm->srcu);
 
-	ret = kvm_io_bus_write(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
+	ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, run->mmio.phys_addr,
 			       bytes, &run->mmio.data);
 
 	srcu_read_unlock(&vcpu->kvm->srcu, idx);

commit d078eed35de3866cb4af654db87765f53edbacce
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Feb 3 16:36:24 2015 +1100

    powerpc: Cleanup KVM emulated load/store endian handling
    
    Sometimes the KVM code on powerpc needs to emulate load or store
    instructions from the guest, which can include both normal and byte
    reversed forms.
    
    We currently (AFAICT) handle this correctly, but some variable names are
    very misleading.  In particular we use "is_bigendian" in several places to
    actually mean "is the IO the same endian as the host", but we now support
    little-endian powerpc hosts.  This also ties into the misleadingly named
    ld_le*() and st_le*() functions, which in fact always byteswap, even on
    an LE host.
    
    This patch cleans this up by renaming to more accurate "host_swabbed", and
    uses the generic swab*() functions instead of the powerpc specific and
    misleadingly named ld_le*() and st_le*() functions.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 27c0face86f4..41c5f8f8a20d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -720,7 +720,7 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		return;
 	}
 
-	if (vcpu->arch.mmio_is_bigendian) {
+	if (!vcpu->arch.mmio_host_swabbed) {
 		switch (run->mmio.len) {
 		case 8: gpr = *(u64 *)run->mmio.data; break;
 		case 4: gpr = *(u32 *)run->mmio.data; break;
@@ -728,10 +728,10 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		case 1: gpr = *(u8 *)run->mmio.data; break;
 		}
 	} else {
-		/* Convert BE data from userland back to LE. */
 		switch (run->mmio.len) {
-		case 4: gpr = ld_le32((u32 *)run->mmio.data); break;
-		case 2: gpr = ld_le16((u16 *)run->mmio.data); break;
+		case 8: gpr = swab64(*(u64 *)run->mmio.data); break;
+		case 4: gpr = swab32(*(u32 *)run->mmio.data); break;
+		case 2: gpr = swab16(*(u16 *)run->mmio.data); break;
 		case 1: gpr = *(u8 *)run->mmio.data; break;
 		}
 	}
@@ -780,14 +780,13 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		       int is_default_endian)
 {
 	int idx, ret;
-	int is_bigendian;
+	bool host_swabbed;
 
+	/* Pity C doesn't have a logical XOR operator */
 	if (kvmppc_need_byteswap(vcpu)) {
-		/* Default endianness is "little endian". */
-		is_bigendian = !is_default_endian;
+		host_swabbed = is_default_endian;
 	} else {
-		/* Default endianness is "big endian". */
-		is_bigendian = is_default_endian;
+		host_swabbed = !is_default_endian;
 	}
 
 	if (bytes > sizeof(run->mmio.data)) {
@@ -800,7 +799,7 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	run->mmio.is_write = 0;
 
 	vcpu->arch.io_gpr = rt;
-	vcpu->arch.mmio_is_bigendian = is_bigendian;
+	vcpu->arch.mmio_host_swabbed = host_swabbed;
 	vcpu->mmio_needed = 1;
 	vcpu->mmio_is_write = 0;
 	vcpu->arch.mmio_sign_extend = 0;
@@ -840,14 +839,13 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 {
 	void *data = run->mmio.data;
 	int idx, ret;
-	int is_bigendian;
+	bool host_swabbed;
 
+	/* Pity C doesn't have a logical XOR operator */
 	if (kvmppc_need_byteswap(vcpu)) {
-		/* Default endianness is "little endian". */
-		is_bigendian = !is_default_endian;
+		host_swabbed = is_default_endian;
 	} else {
-		/* Default endianness is "big endian". */
-		is_bigendian = is_default_endian;
+		host_swabbed = !is_default_endian;
 	}
 
 	if (bytes > sizeof(run->mmio.data)) {
@@ -862,7 +860,7 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	vcpu->mmio_is_write = 1;
 
 	/* Store the value at the lowest bytes in 'data'. */
-	if (is_bigendian) {
+	if (!host_swabbed) {
 		switch (bytes) {
 		case 8: *(u64 *)data = val; break;
 		case 4: *(u32 *)data = val; break;
@@ -870,11 +868,11 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		case 1: *(u8  *)data = val; break;
 		}
 	} else {
-		/* Store LE value into 'data'. */
 		switch (bytes) {
-		case 4: st_le32(data, val); break;
-		case 2: st_le16(data, val); break;
-		case 1: *(u8 *)data = val; break;
+		case 8: *(u64 *)data = swab64(val); break;
+		case 4: *(u32 *)data = swab32(val); break;
+		case 2: *(u16 *)data = swab16(val); break;
+		case 1: *(u8  *)data = val; break;
 		}
 	}
 

commit 31928aa5863e71535ee942f506ca9ac8ce1c4315
Author: Dominik Dingel <dingel@linux.vnet.ibm.com>
Date:   Thu Dec 4 15:47:07 2014 +0100

    KVM: remove unneeded return value of vcpu_postcreate
    
    The return value of kvm_arch_vcpu_postcreate is not checked in its
    caller.  This is okay, because only x86 provides vcpu_postcreate right
    now and it could only fail if vcpu_load failed.  But that is not
    possible during KVM_CREATE_VCPU (kvm_arch_vcpu_load is void, too), so
    just get rid of the unchecked return value.
    
    Signed-off-by: Dominik Dingel <dingel@linux.vnet.ibm.com>
    Acked-by: Cornelia Huck <cornelia.huck@de.ibm.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index c45eaab752b0..27c0face86f4 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -623,9 +623,8 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 	return vcpu;
 }
 
-int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
+void kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
 {
-	return 0;
 }
 
 void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)

commit c17b98cf6028704e1f953d6a25ed6140425ccfd0
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 13:30:38 2014 +1100

    KVM: PPC: Book3S HV: Remove code for PPC970 processors
    
    This removes the code that was added to enable HV KVM to work
    on PPC970 processors.  The PPC970 is an old CPU that doesn't
    support virtualizing guest memory.  Removing PPC970 support also
    lets us remove the code for allocating and managing contiguous
    real-mode areas, the code for the !kvm->arch.using_mmu_notifiers
    case, the code for pinning pages of guest memory when first
    accessed and keeping track of which pages have been pinned, and
    the code for handling H_ENTER hypercalls in virtual mode.
    
    Book3S HV KVM is now supported only on POWER7 and POWER8 processors.
    The KVM_CAP_PPC_RMA capability now always returns 0.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index c1f8f53cd312..c45eaab752b0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -527,18 +527,12 @@ int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 			r = 0;
 		break;
 	case KVM_CAP_PPC_RMA:
-		r = hv_enabled;
-		/* PPC970 requires an RMA */
-		if (r && cpu_has_feature(CPU_FTR_ARCH_201))
-			r = 2;
+		r = 0;
 		break;
 #endif
 	case KVM_CAP_SYNC_MMU:
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
-		if (hv_enabled)
-			r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
-		else
-			r = 0;
+		r = hv_enabled;
 #elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 		r = 1;
 #else

commit 8d0eff6385640a9e6eed0b0c09113794b2bb74e9
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Sep 10 14:37:29 2014 +0200

    KVM: PPC: Pass enum to kvmppc_get_last_inst
    
    The kvmppc_get_last_inst function recently received a facelift that allowed
    us to pass an enum of the type of instruction we want to read into it rather
    than an unreadable boolean.
    
    Unfortunately, not all callers ended up passing the enum. This wasn't really
    an issue as "true" and "false" happen to match the two enum values we have,
    but it's still hard to read.
    
    Update all callers of kvmppc_get_last_inst() to follow the new calling
    convention.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ecf057551a2f..c1f8f53cd312 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -294,7 +294,7 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 	{
 		u32 last_inst;
 
-		kvmppc_get_last_inst(vcpu, false, &last_inst);
+		kvmppc_get_last_inst(vcpu, INST_GENERIC, &last_inst);
 		/* XXX Deliver Program interrupt to guest. */
 		pr_emerg("%s: emulation failed (%08x)\n", __func__, last_inst);
 		r = RESUME_HOST;

commit d02d4d156e72baf9a6628c76eb53019124d3c82f
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Mon Sep 1 17:19:56 2014 +0300

    KVM: PPC: Remove the tasklet used by the hrtimer
    
    Powerpc timer implementation is a copycat version of s390. Now that they removed
    the tasklet with commit ea74c0ea1b24a6978a6ebc80ba4dbc7b7848b32d follow this
    optimization.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Bogdan Purcareata <bogdan.purcareata@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0c7d1917c23d..ecf057551a2f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -638,7 +638,6 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 {
 	/* Make sure we're not using the vcpu anymore */
 	hrtimer_cancel(&vcpu->arch.dec_timer);
-	tasklet_kill(&vcpu->arch.tasklet);
 
 	kvmppc_remove_vcpu_debugfs(vcpu);
 
@@ -664,16 +663,12 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 	return kvmppc_core_pending_dec(vcpu);
 }
 
-/*
- * low level hrtimer wake routine. Because this runs in hardirq context
- * we schedule a tasklet to do the real work.
- */
 enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)
 {
 	struct kvm_vcpu *vcpu;
 
 	vcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);
-	tasklet_schedule(&vcpu->arch.tasklet);
+	kvmppc_decrementer_func(vcpu);
 
 	return HRTIMER_NORESTART;
 }
@@ -683,7 +678,6 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	int ret;
 
 	hrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
-	tasklet_init(&vcpu->arch.tasklet, kvmppc_decrementer_func, (ulong)vcpu);
 	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
 	vcpu->arch.dec_expires = ~(u64)0;
 

commit 3840edc8033ad5b86deee309c1c321ca54257452
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Wed Aug 20 16:36:25 2014 +0300

    KVM: PPC: Move ONE_REG AltiVec support to powerpc
    
    Move ONE_REG AltiVec support to powerpc generic layer.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8a26126db482..0c7d1917c23d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -921,6 +921,29 @@ int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 	if (r == -EINVAL) {
 		r = 0;
 		switch (reg->id) {
+#ifdef CONFIG_ALTIVEC
+		case KVM_REG_PPC_VR0 ... KVM_REG_PPC_VR31:
+			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
+				r = -ENXIO;
+				break;
+			}
+			vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0] = val.vval;
+			break;
+		case KVM_REG_PPC_VSCR:
+			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
+				r = -ENXIO;
+				break;
+			}
+			vcpu->arch.vr.vscr.u[3] = set_reg_val(reg->id, val);
+			break;
+		case KVM_REG_PPC_VRSAVE:
+			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
+				r = -ENXIO;
+				break;
+			}
+			vcpu->arch.vrsave = set_reg_val(reg->id, val);
+			break;
+#endif /* CONFIG_ALTIVEC */
 		default:
 			r = -EINVAL;
 			break;
@@ -953,6 +976,25 @@ int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
 	if (r == -EINVAL) {
 		r = 0;
 		switch (reg->id) {
+#ifdef CONFIG_ALTIVEC
+		case KVM_REG_PPC_VR0 ... KVM_REG_PPC_VR31:
+			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
+				r = -ENXIO;
+				break;
+			}
+			val.vval = vcpu->arch.vr.vr[reg->id - KVM_REG_PPC_VR0];
+			break;
+		case KVM_REG_PPC_VSCR:
+			if (!cpu_has_feature(CPU_FTR_ALTIVEC)) {
+				r = -ENXIO;
+				break;
+			}
+			val = get_reg_val(reg->id, vcpu->arch.vr.vscr.u[3]);
+			break;
+		case KVM_REG_PPC_VRSAVE:
+			val = get_reg_val(reg->id, vcpu->arch.vrsave);
+			break;
+#endif /* CONFIG_ALTIVEC */
 		default:
 			r = -EINVAL;
 			break;

commit 8a41ea53b32ffbe7524e3424cf0403fa3b4c73fb
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Wed Aug 20 16:36:24 2014 +0300

    KVM: PPC: Make ONE_REG powerpc generic
    
    Make ONE_REG generic for server and embedded architectures by moving
    kvm_vcpu_ioctl_get_one_reg() and kvm_vcpu_ioctl_set_one_reg() functions
    to powerpc layer.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index da505237a664..8a26126db482 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -907,6 +907,61 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 }
 EXPORT_SYMBOL_GPL(kvmppc_handle_store);
 
+int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
+{
+	int r = 0;
+	union kvmppc_one_reg val;
+	int size;
+
+	size = one_reg_size(reg->id);
+	if (size > sizeof(val))
+		return -EINVAL;
+
+	r = kvmppc_get_one_reg(vcpu, reg->id, &val);
+	if (r == -EINVAL) {
+		r = 0;
+		switch (reg->id) {
+		default:
+			r = -EINVAL;
+			break;
+		}
+	}
+
+	if (r)
+		return r;
+
+	if (copy_to_user((char __user *)(unsigned long)reg->addr, &val, size))
+		r = -EFAULT;
+
+	return r;
+}
+
+int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu, struct kvm_one_reg *reg)
+{
+	int r;
+	union kvmppc_one_reg val;
+	int size;
+
+	size = one_reg_size(reg->id);
+	if (size > sizeof(val))
+		return -EINVAL;
+
+	if (copy_from_user(&val, (char __user *)(unsigned long)reg->addr, size))
+		return -EFAULT;
+
+	r = kvmppc_set_one_reg(vcpu, reg->id, &val);
+	if (r == -EINVAL) {
+		r = 0;
+		switch (reg->id) {
+		default:
+			r = -EINVAL;
+			break;
+		}
+	}
+
+	return r;
+}
+
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	int r;

commit 13a34e067eab24fec882e1834fbf2cc31911d474
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:03 2014 +0200

    KVM: remove garbage arg to *hardware_{en,dis}able
    
    In the beggining was on_each_cpu(), which required an unused argument to
    kvm_arch_ops.hardware_{en,dis}able, but this was soon forgotten.
    
    Remove unnecessary arguments that stem from this.
    
    Signed-off-by: Radim KrÃÂmÃÂ¡Ãâ¢ <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 72c3fc085207..da505237a664 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -384,7 +384,7 @@ int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 }
 EXPORT_SYMBOL_GPL(kvmppc_ld);
 
-int kvm_arch_hardware_enable(void *garbage)
+int kvm_arch_hardware_enable(void)
 {
 	return 0;
 }

commit 0865e636aef751966e6e0f8950a26bc7391e923c
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Thu Aug 28 15:13:02 2014 +0200

    KVM: static inline empty kvm_arch functions
    
    Using static inline is going to save few bytes and cycles.
    For example on powerpc, the difference is 700 B after stripping.
    (5 kB before)
    
    This patch also deals with two overlooked empty functions:
    kvm_arch_flush_shadow was not removed from arch/mips/kvm/mips.c
      2df72e9bc KVM: split kvm_arch_flush_shadow
    and kvm_arch_sched_in never made it into arch/ia64/kvm/kvm-ia64.c.
      e790d9ef6 KVM: add kvm_arch_sched_in
    
    Signed-off-by: Radim KrÃÂmÃÂ¡Ãâ¢ <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index cbc432f4f0a6..72c3fc085207 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -389,19 +389,11 @@ int kvm_arch_hardware_enable(void *garbage)
 	return 0;
 }
 
-void kvm_arch_hardware_disable(void *garbage)
-{
-}
-
 int kvm_arch_hardware_setup(void)
 {
 	return 0;
 }
 
-void kvm_arch_hardware_unsetup(void)
-{
-}
-
 void kvm_arch_check_processor_compat(void *rtn)
 {
 	*(int *)rtn = kvmppc_core_check_processor_compat();
@@ -462,10 +454,6 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	module_put(kvm->arch.kvm_ops->owner);
 }
 
-void kvm_arch_sync_events(struct kvm *kvm)
-{
-}
-
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r;
@@ -608,10 +596,6 @@ int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
 	return kvmppc_core_create_memslot(kvm, slot, npages);
 }
 
-void kvm_arch_memslots_updated(struct kvm *kvm)
-{
-}
-
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
 				   struct kvm_userspace_memory_region *mem,
@@ -628,10 +612,6 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 	kvmppc_core_commit_memory_region(kvm, mem, old);
 }
 
-void kvm_arch_flush_shadow_all(struct kvm *kvm)
-{
-}
-
 void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
 				   struct kvm_memory_slot *slot)
 {
@@ -720,10 +700,6 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 	kvmppc_subarch_vcpu_uninit(vcpu);
 }
 
-void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
-{
-}
-
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 #ifdef CONFIG_BOOKE
@@ -1347,9 +1323,4 @@ int kvm_arch_init(void *opaque)
 	return 0;
 }
 
-void kvm_arch_exit(void)
-{
-
-}
-
 EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_ppc_instr);

commit e790d9ef6405633b007339d746b709aed43a928d
Author: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
Date:   Thu Aug 21 18:08:05 2014 +0200

    KVM: add kvm_arch_sched_in
    
    Introduce preempt notifiers for architecture specific code.
    Advantage over creating a new notifier in every arch is slightly simpler
    code and guaranteed call order with respect to kvm_sched_in.
    
    Signed-off-by: Radim KrÄmÃ¡Å <rkrcmar@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4c79284b58be..cbc432f4f0a6 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -720,6 +720,10 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 	kvmppc_subarch_vcpu_uninit(vcpu);
 }
 
+void kvm_arch_sched_in(struct kvm_vcpu *vcpu, int cpu)
+{
+}
+
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
 #ifdef CONFIG_BOOKE

commit 478d66862559bade81cb653af52b9aa53bee2c8a
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Tue Aug 5 11:29:07 2014 +0200

    KVM: PPC: drop duplicate tracepoint
    
    Commit 29577fc00ba4 ("KVM: PPC: HV: Remove generic instruction emulation")
    caused a build failure with allyesconfig:
    
    arch/powerpc/kvm/kvm-pr.o:(__tracepoints+0xa8): multiple definition of `__tracepoint_kvm_ppc_instr'
    arch/powerpc/kvm/kvm.o:(__tracepoints+0x1c0): first defined here
    
    due to a duplicate definition of the tracepoint in trace.h and
    trace_pr.h.  Because the tracepoint is still used by Book3S HV
    code, and because the PR code does include trace.h, just remove
    the duplicate definition from trace_pr.h, and export it from
    kvm.o.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 288b4bb05cbd..4c79284b58be 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1347,3 +1347,5 @@ void kvm_arch_exit(void)
 {
 
 }
+
+EXPORT_TRACEPOINT_SYMBOL_GPL(kvm_ppc_instr);

commit ce91ddc471b77ec75e5b2a43c803efac605f37b3
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jul 28 19:29:13 2014 +0200

    KVM: PPC: Remove DCR handling
    
    DCR handling was only needed for 440 KVM. Since we removed it, we can also
    remove handling of DCR accesses.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index c14ed15fd60b..288b4bb05cbd 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -743,12 +743,6 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 #endif
 }
 
-static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,
-                                     struct kvm_run *run)
-{
-	kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, run->dcr.data);
-}
-
 static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
                                       struct kvm_run *run)
 {
@@ -945,10 +939,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		if (!vcpu->mmio_is_write)
 			kvmppc_complete_mmio_load(vcpu, run);
 		vcpu->mmio_needed = 0;
-	} else if (vcpu->arch.dcr_needed) {
-		if (!vcpu->arch.dcr_is_write)
-			kvmppc_complete_dcr_load(vcpu, run);
-		vcpu->arch.dcr_needed = 0;
 	} else if (vcpu->arch.osi_needed) {
 		u64 *gprs = run->osi.gprs;
 		int i;

commit d69614a295aef72f8fb22da8e3ccf1a8f19a7ffc
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Jun 18 14:53:49 2014 +0200

    KVM: PPC: Separate loadstore emulation from priv emulation
    
    Today the instruction emulator can get called via 2 separate code paths. It
    can either be called by MMIO emulation detection code or by privileged
    instruction traps.
    
    This is bad, as both code paths prepare the environment differently. For MMIO
    emulation we already know the virtual address we faulted on, so instructions
    there don't have to actually fetch that information.
    
    Split out the two separate use cases into separate files.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 544d1d30c8cc..c14ed15fd60b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -272,7 +272,7 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 	enum emulation_result er;
 	int r;
 
-	er = kvmppc_emulate_instruction(run, vcpu);
+	er = kvmppc_emulate_loadstore(vcpu);
 	switch (er) {
 	case EMULATE_DONE:
 		/* Future optimization: only reload non-volatiles if they were

commit c12fb43c2f6d6a57a4e21afe74ff56485d699ee7
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jun 20 14:43:36 2014 +0200

    KVM: PPC: Handle magic page in kvmppc_ld/st
    
    We use kvmppc_ld and kvmppc_st to emulate load/store instructions that may as
    well access the magic page. Special case it out so that we can properly access
    it.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index be40886e8f0a..544d1d30c8cc 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -312,6 +312,7 @@ EXPORT_SYMBOL_GPL(kvmppc_emulate_mmio);
 int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 	      bool data)
 {
+	ulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;
 	struct kvmppc_pte pte;
 	int r;
 
@@ -327,6 +328,16 @@ int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 	if (!pte.may_write)
 		return -EPERM;
 
+	/* Magic page override */
+	if (kvmppc_supports_magic_page(vcpu) && mp_pa &&
+	    ((pte.raddr & KVM_PAM & PAGE_MASK) == mp_pa) &&
+	    !(kvmppc_get_msr(vcpu) & MSR_PR)) {
+		void *magic = vcpu->arch.shared;
+		magic += pte.eaddr & 0xfff;
+		memcpy(magic, ptr, size);
+		return EMULATE_DONE;
+	}
+
 	if (kvm_write_guest(vcpu->kvm, pte.raddr, ptr, size))
 		return EMULATE_DO_MMIO;
 
@@ -337,6 +348,7 @@ EXPORT_SYMBOL_GPL(kvmppc_st);
 int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 		      bool data)
 {
+	ulong mp_pa = vcpu->arch.magic_page_pa & KVM_PAM & PAGE_MASK;
 	struct kvmppc_pte pte;
 	int rc;
 
@@ -355,6 +367,16 @@ int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 	if (!data && !pte.may_execute)
 		return -ENOEXEC;
 
+	/* Magic page override */
+	if (kvmppc_supports_magic_page(vcpu) && mp_pa &&
+	    ((pte.raddr & KVM_PAM & PAGE_MASK) == mp_pa) &&
+	    !(kvmppc_get_msr(vcpu) & MSR_PR)) {
+		void *magic = vcpu->arch.shared;
+		magic += pte.eaddr & 0xfff;
+		memcpy(ptr, magic, size);
+		return EMULATE_DONE;
+	}
+
 	if (kvm_read_guest(vcpu->kvm, pte.raddr, ptr, size))
 		return EMULATE_DO_MMIO;
 

commit c45c551403f0a7b152e56c53735b954faa36c54c
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jun 20 14:17:30 2014 +0200

    KVM: PPC: Use kvm_read_guest in kvmppc_ld
    
    We have a nice and handy helper to read from guest physical address space,
    so we should make use of it in kvmppc_ld as we already do for its counterpart
    in kvmppc_st.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3d59730a7802..be40886e8f0a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -309,19 +309,6 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvmppc_emulate_mmio);
 
-static hva_t kvmppc_pte_to_hva(struct kvm_vcpu *vcpu, struct kvmppc_pte *pte)
-{
-	hva_t hpage;
-
-	hpage = gfn_to_hva(vcpu->kvm, pte->raddr >> PAGE_SHIFT);
-	if (kvm_is_error_hva(hpage))
-		goto err;
-
-	return hpage | (pte->raddr & ~PAGE_MASK);
-err:
-	return KVM_HVA_ERR_BAD;
-}
-
 int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 	      bool data)
 {
@@ -351,7 +338,6 @@ int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 		      bool data)
 {
 	struct kvmppc_pte pte;
-	hva_t hva = *eaddr;
 	int rc;
 
 	vcpu->stat.ld++;
@@ -369,19 +355,10 @@ int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
 	if (!data && !pte.may_execute)
 		return -ENOEXEC;
 
-	hva = kvmppc_pte_to_hva(vcpu, &pte);
-	if (kvm_is_error_hva(hva))
-		goto mmio;
-
-	if (copy_from_user(ptr, (void __user *)hva, size)) {
-		printk(KERN_INFO "kvmppc_ld at 0x%lx failed\n", hva);
-		goto mmio;
-	}
+	if (kvm_read_guest(vcpu->kvm, pte.raddr, ptr, size))
+		return EMULATE_DO_MMIO;
 
 	return EMULATE_DONE;
-
-mmio:
-	return EMULATE_DO_MMIO;
 }
 EXPORT_SYMBOL_GPL(kvmppc_ld);
 

commit 9897e88a79e1c6a3c5bbe74eccd64c4ba32a3b19
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jun 20 13:59:43 2014 +0200

    KVM: PPC: Remove kvmppc_bad_hva()
    
    We have a proper define for invalid HVA numbers. Use those instead of the
    ppc specific kvmppc_bad_hva().
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2c5a1c3a9db1..3d59730a7802 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -309,11 +309,6 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvmppc_emulate_mmio);
 
-static hva_t kvmppc_bad_hva(void)
-{
-	return PAGE_OFFSET;
-}
-
 static hva_t kvmppc_pte_to_hva(struct kvm_vcpu *vcpu, struct kvmppc_pte *pte)
 {
 	hva_t hpage;
@@ -324,7 +319,7 @@ static hva_t kvmppc_pte_to_hva(struct kvm_vcpu *vcpu, struct kvmppc_pte *pte)
 
 	return hpage | (pte->raddr & ~PAGE_MASK);
 err:
-	return kvmppc_bad_hva();
+	return KVM_HVA_ERR_BAD;
 }
 
 int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,

commit 35c4a7330dbe1ae6f590a5645b185e35ddb3f6d9
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jun 20 13:58:16 2014 +0200

    KVM: PPC: Move kvmppc_ld/st to common code
    
    We have enough common infrastructure now to resolve GVA->GPA mappings at
    runtime. With this we can move our book3s specific helpers to load / store
    in guest virtual address space to common code as well.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index eaa57da7711b..2c5a1c3a9db1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -309,6 +309,87 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvmppc_emulate_mmio);
 
+static hva_t kvmppc_bad_hva(void)
+{
+	return PAGE_OFFSET;
+}
+
+static hva_t kvmppc_pte_to_hva(struct kvm_vcpu *vcpu, struct kvmppc_pte *pte)
+{
+	hva_t hpage;
+
+	hpage = gfn_to_hva(vcpu->kvm, pte->raddr >> PAGE_SHIFT);
+	if (kvm_is_error_hva(hpage))
+		goto err;
+
+	return hpage | (pte->raddr & ~PAGE_MASK);
+err:
+	return kvmppc_bad_hva();
+}
+
+int kvmppc_st(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
+	      bool data)
+{
+	struct kvmppc_pte pte;
+	int r;
+
+	vcpu->stat.st++;
+
+	r = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,
+			 XLATE_WRITE, &pte);
+	if (r < 0)
+		return r;
+
+	*eaddr = pte.raddr;
+
+	if (!pte.may_write)
+		return -EPERM;
+
+	if (kvm_write_guest(vcpu->kvm, pte.raddr, ptr, size))
+		return EMULATE_DO_MMIO;
+
+	return EMULATE_DONE;
+}
+EXPORT_SYMBOL_GPL(kvmppc_st);
+
+int kvmppc_ld(struct kvm_vcpu *vcpu, ulong *eaddr, int size, void *ptr,
+		      bool data)
+{
+	struct kvmppc_pte pte;
+	hva_t hva = *eaddr;
+	int rc;
+
+	vcpu->stat.ld++;
+
+	rc = kvmppc_xlate(vcpu, *eaddr, data ? XLATE_DATA : XLATE_INST,
+			  XLATE_READ, &pte);
+	if (rc)
+		return rc;
+
+	*eaddr = pte.raddr;
+
+	if (!pte.may_read)
+		return -EPERM;
+
+	if (!data && !pte.may_execute)
+		return -ENOEXEC;
+
+	hva = kvmppc_pte_to_hva(vcpu, &pte);
+	if (kvm_is_error_hva(hva))
+		goto mmio;
+
+	if (copy_from_user(ptr, (void __user *)hva, size)) {
+		printk(KERN_INFO "kvmppc_ld at 0x%lx failed\n", hva);
+		goto mmio;
+	}
+
+	return EMULATE_DONE;
+
+mmio:
+	return EMULATE_DO_MMIO;
+}
+EXPORT_SYMBOL_GPL(kvmppc_ld);
+
 int kvm_arch_hardware_enable(void *garbage)
 {
 	return 0;

commit 7a58777a33128b28379851b94070934b17ed9176
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jul 14 18:55:19 2014 +0200

    KVM: PPC: Book3S: Provide different CAPs based on HV or PR mode
    
    With Book3S KVM we can create both PR and HV VMs in parallel on the same
    machine. That gives us new challenges on the CAPs we return - both have
    different capabilities.
    
    When we get asked about CAPs on the kvm fd, there's nothing we can do. We
    can try to be smart and assume we're running HV if HV is available, PR
    otherwise. However with the newly added VM CHECK_EXTENSION we can now ask
    for capabilities directly on a VM which knows whether it's PR or HV.
    
    With this patch I can successfully expose KVM PVINFO data to user space
    in the PR case, fixing magic page mapping for PAPR guests.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index d870bacc2f75..eaa57da7711b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -394,11 +394,17 @@ void kvm_arch_sync_events(struct kvm *kvm)
 int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r;
-	/* FIXME!!
-	 * Should some of this be vm ioctl ? is it possible now ?
-	 */
+	/* Assume we're using HV mode when the HV module is loaded */
 	int hv_enabled = kvmppc_hv_ops ? 1 : 0;
 
+	if (kvm) {
+		/*
+		 * Hooray - we know which VM type we're running on. Depend on
+		 * that rather than the guess above.
+		 */
+		hv_enabled = is_kvmppc_hv_enabled(kvm);
+	}
+
 	switch (ext) {
 #ifdef CONFIG_BOOKE
 	case KVM_CAP_PPC_BOOKE_SREGS:

commit 784aa3d7fb6f729c06d5836c9d9569f58e4d05ae
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Jul 14 18:27:35 2014 +0200

    KVM: Rename and add argument to check_extension
    
    In preparation to make the check_extension function available to VM scope
    we add a struct kvm * argument to the function header and rename the function
    accordingly. It will still be called from the /dev/kvm fd, but with a NULL
    argument for struct kvm *.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8e0356835960..d870bacc2f75 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -391,7 +391,7 @@ void kvm_arch_sync_events(struct kvm *kvm)
 {
 }
 
-int kvm_dev_ioctl_check_extension(long ext)
+int kvm_vm_ioctl_check_extension(struct kvm *kvm, long ext)
 {
 	int r;
 	/* FIXME!!

commit b2677b8dd8de0dc1496ede4da09b9dfd59f15cea
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jul 25 10:38:59 2014 +0200

    KVM: PPC: Remove 440 support
    
    The 440 target hasn't been properly functioning for a few releases and
    before I was the only one who fixes a very serious bug that indicates to
    me that nobody used it before either.
    
    Furthermore KVM on 440 is slow to the extent of unusable.
    
    We don't have to carry along completely unused code. Remove 440 and give
    us one less thing to worry about.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index cfa6cfabf4a3..8e0356835960 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -217,7 +217,6 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	case KVM_HCALL_TOKEN(KVM_HC_FEATURES):
 		r = EV_SUCCESS;
 #if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500V2)
-		/* XXX Missing magic page on 44x */
 		r2 |= (1 << KVM_FEATURE_MAGIC_PAGE);
 #endif
 

commit 51f047261e717b74b226f837a16455994b61ae30
Author: Mihai Caraman <mihai.caraman@freescale.com>
Date:   Wed Jul 23 19:06:21 2014 +0300

    KVM: PPC: Allow kvmppc_get_last_inst() to fail
    
    On book3e, guest last instruction is read on the exit path using load
    external pid (lwepx) dedicated instruction. This load operation may fail
    due to TLB eviction and execute-but-not-read entries.
    
    This patch lay down the path for an alternative solution to read the guest
    last instruction, by allowing kvmppc_get_lat_inst() function to fail.
    Architecture specific implmentations of kvmppc_load_last_inst() may read
    last guest instruction and instruct the emulation layer to re-execute the
    guest in case of failure.
    
    Make kvmppc_get_last_inst() definition common between architectures.
    
    Signed-off-by: Mihai Caraman <mihai.caraman@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index fe0257a8e335..cfa6cfabf4a3 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -280,6 +280,9 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 		 * actually modified. */
 		r = RESUME_GUEST_NV;
 		break;
+	case EMULATE_AGAIN:
+		r = RESUME_GUEST;
+		break;
 	case EMULATE_DO_MMIO:
 		run->exit_reason = KVM_EXIT_MMIO;
 		/* We must reload nonvolatiles because "update" load/store
@@ -289,11 +292,15 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 		r = RESUME_HOST_NV;
 		break;
 	case EMULATE_FAIL:
+	{
+		u32 last_inst;
+
+		kvmppc_get_last_inst(vcpu, false, &last_inst);
 		/* XXX Deliver Program interrupt to guest. */
-		printk(KERN_EMERG "%s: emulation failed (%08x)\n", __func__,
-		       kvmppc_get_last_inst(vcpu));
+		pr_emerg("%s: emulation failed (%08x)\n", __func__, last_inst);
 		r = RESUME_HOST;
 		break;
+	}
 	default:
 		WARN_ON(1);
 		r = RESUME_GUEST;

commit 89b68c96a24f6520c8815f88254c8e7d09aeb40e
Author: Alexander Graf <agraf@suse.de>
Date:   Sun Jul 13 16:37:12 2014 +0200

    KVM: PPC: Book3S: Make magic page properly 4k mappable
    
    The magic page is defined as a 4k page of per-vCPU data that is shared
    between the guest and the host to accelerate accesses to privileged
    registers.
    
    However, when the host is using 64k page size granularity we weren't quite
    as strict about that rule anymore. Instead, we partially treated all of the
    upper 64k as magic page and mapped only the uppermost 4k with the actual
    magic contents.
    
    This works well enough for Linux which doesn't use any memory in kernel
    space in the upper 64k, but Mac OS X got upset. So this patch makes magic
    page actually stay in a 4k range even on 64k page size hosts.
    
    This patch fixes magic page usage with Mac OS X (using MOL) on 64k PAGE_SIZE
    hosts for me.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7efc2b711404..fe0257a8e335 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -190,6 +190,25 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 		vcpu->arch.magic_page_pa = param1 & ~0xfffULL;
 		vcpu->arch.magic_page_ea = param2 & ~0xfffULL;
 
+#ifdef CONFIG_PPC_64K_PAGES
+		/*
+		 * Make sure our 4k magic page is in the same window of a 64k
+		 * page within the guest and within the host's page.
+		 */
+		if ((vcpu->arch.magic_page_pa & 0xf000) !=
+		    ((ulong)vcpu->arch.shared & 0xf000)) {
+			void *old_shared = vcpu->arch.shared;
+			ulong shared = (ulong)vcpu->arch.shared;
+			void *new_shared;
+
+			shared &= PAGE_MASK;
+			shared |= vcpu->arch.magic_page_pa & 0xf000;
+			new_shared = (void*)shared;
+			memcpy(new_shared, old_shared, 0x1000);
+			vcpu->arch.shared = new_shared;
+		}
+#endif
+
 		r2 = KVM_MAGIC_FEAT_SR | KVM_MAGIC_FEAT_MAS0_TO_SPRG7;
 
 		r = EV_SUCCESS;

commit ae2113a4f1a6cd5a3cd3d75f394547922758e9ac
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 2 11:03:00 2014 +1000

    KVM: PPC: Book3S: Allow only implemented hcalls to be enabled or disabled
    
    This adds code to check that when the KVM_CAP_PPC_ENABLE_HCALL
    capability is used to enable or disable in-kernel handling of an
    hcall, that the hcall is actually implemented by the kernel.
    If not an EINVAL error is returned.
    
    This also checks the default-enabled list of hcalls and prints a
    warning if any hcall there is not actually implemented.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3222a4d08a6f..7efc2b711404 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1119,6 +1119,8 @@ static int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
 		if (hcall > MAX_HCALL_OPCODE || (hcall & 3) ||
 		    cap->args[1] > 1)
 			break;
+		if (!kvmppc_book3s_hcall_implemented(kvm, hcall))
+			break;
 		if (cap->args[1])
 			set_bit(hcall / 4, kvm->arch.enabled_hcalls);
 		else

commit 699a0ea0823d32030b0666b28ff8633960f7ffa7
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Jun 2 11:02:59 2014 +1000

    KVM: PPC: Book3S: Controls for in-kernel sPAPR hypercall handling
    
    This provides a way for userspace controls which sPAPR hcalls get
    handled in the kernel.  Each hcall can be individually enabled or
    disabled for in-kernel handling, except for H_RTAS.  The exception
    for H_RTAS is because userspace can already control whether
    individual RTAS functions are handled in-kernel or not via the
    KVM_PPC_RTAS_DEFINE_TOKEN ioctl, and because the numeric value for
    H_RTAS is out of the normal sequence of hcall numbers.
    
    Hcalls are enabled or disabled using the KVM_ENABLE_CAP ioctl for the
    KVM_CAP_PPC_ENABLE_HCALL capability on the file descriptor for the VM.
    The args field of the struct kvm_enable_cap specifies the hcall number
    in args[0] and the enable/disable flag in args[1]; 0 means disable
    in-kernel handling (so that the hcall will always cause an exit to
    userspace) and 1 means enable.  Enabling or disabling in-kernel
    handling of an hcall is effective across the whole VM.
    
    The ability for KVM_ENABLE_CAP to be used on a VM file descriptor
    on PowerPC is new, added by this commit.  The KVM_CAP_ENABLE_CAP_VM
    capability advertises that this ability exists.
    
    When a VM is created, an initial set of hcalls are enabled for
    in-kernel handling.  The set that is enabled is the set that have
    an in-kernel implementation at this point.  Any new hcall
    implementations from this point onwards should not be added to the
    default set without a good reason.
    
    No distinction is made between real-mode and virtual-mode hcall
    implementations; the one setting controls them both.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 61c738ab1283..3222a4d08a6f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -387,6 +387,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_PPC_IRQ_LEVEL:
 	case KVM_CAP_ENABLE_CAP:
+	case KVM_CAP_ENABLE_CAP_VM:
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_IOEVENTFD:
 	case KVM_CAP_DEVICE_CTRL:
@@ -417,6 +418,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_ALLOC_HTAB:
 	case KVM_CAP_PPC_RTAS:
 	case KVM_CAP_PPC_FIXUP_HCALL:
+	case KVM_CAP_PPC_ENABLE_HCALL:
 #ifdef CONFIG_KVM_XICS
 	case KVM_CAP_IRQ_XICS:
 #endif
@@ -1099,6 +1101,40 @@ int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,
 	return 0;
 }
 
+
+static int kvm_vm_ioctl_enable_cap(struct kvm *kvm,
+				   struct kvm_enable_cap *cap)
+{
+	int r;
+
+	if (cap->flags)
+		return -EINVAL;
+
+	switch (cap->cap) {
+#ifdef CONFIG_KVM_BOOK3S_64_HANDLER
+	case KVM_CAP_PPC_ENABLE_HCALL: {
+		unsigned long hcall = cap->args[0];
+
+		r = -EINVAL;
+		if (hcall > MAX_HCALL_OPCODE || (hcall & 3) ||
+		    cap->args[1] > 1)
+			break;
+		if (cap->args[1])
+			set_bit(hcall / 4, kvm->arch.enabled_hcalls);
+		else
+			clear_bit(hcall / 4, kvm->arch.enabled_hcalls);
+		r = 0;
+		break;
+	}
+#endif
+	default:
+		r = -EINVAL;
+		break;
+	}
+
+	return r;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
                        unsigned int ioctl, unsigned long arg)
 {
@@ -1118,6 +1154,15 @@ long kvm_arch_vm_ioctl(struct file *filp,
 
 		break;
 	}
+	case KVM_ENABLE_CAP:
+	{
+		struct kvm_enable_cap cap;
+		r = -EFAULT;
+		if (copy_from_user(&cap, argp, sizeof(cap)))
+			goto out;
+		r = kvm_vm_ioctl_enable_cap(kvm, &cap);
+		break;
+	}
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CREATE_SPAPR_TCE: {
 		struct kvm_create_spapr_tce create_tce;

commit c5aec4c76af1a2d89ee2f2d4d5463b2ad2d85de5
Merge: 2937f5efa575 0c0a3e5a100b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 10 18:54:22 2014 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "Here is the bulk of the powerpc changes for this merge window.  It got
      a bit delayed in part because I wasn't paying attention, and in part
      because I discovered I had a core PCI change without a PCI maintainer
      ack in it.  Bjorn eventually agreed it was ok to merge it though we'll
      probably improve it later and I didn't want to rebase to add his ack.
    
      There is going to be a bit more next week, essentially fixes that I
      still want to sort through and test.
    
      The biggest item this time is the support to build the ppc64 LE kernel
      with our new v2 ABI.  We previously supported v2 userspace but the
      kernel itself was a tougher nut to crack.  This is now sorted mostly
      thanks to Anton and Rusty.
    
      We also have a fairly big series from Cedric that add support for
      64-bit LE zImage boot wrapper.  This was made harder by the fact that
      traditionally our zImage wrapper was always 32-bit, but our new LE
      toolchains don't really support 32-bit anymore (it's somewhat there
      but not really "supported") so we didn't want to rely on it.  This
      meant more churn that just endian fixes.
    
      This brings some more LE bits as well, such as the ability to run in
      LE mode without a hypervisor (ie. under OPAL firmware) by doing the
      right OPAL call to reinitialize the CPU to take HV interrupts in the
      right mode and the usual pile of endian fixes.
    
      There's another series from Gavin adding EEH improvements (one day we
      *will* have a release with less than 20 EEH patches, I promise!).
    
      Another highlight is the support for the "Split core" functionality on
      P8 by Michael.  This allows a P8 core to be split into "sub cores" of
      4 threads which allows the subcores to run different guests under KVM
      (the HW still doesn't support a partition per thread).
    
      And then the usual misc bits and fixes ..."
    
    [ Further delayed by gmail deciding that BenH is a dirty spammer.
      Google knows.  ]
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (155 commits)
      powerpc/powernv: Add missing include to LPC code
      selftests/powerpc: Test the THP bug we fixed in the previous commit
      powerpc/mm: Check paca psize is up to date for huge mappings
      powerpc/powernv: Pass buffer size to OPAL validate flash call
      powerpc/pseries: hcall functions are exported to modules, need _GLOBAL_TOC()
      powerpc: Exported functions __clear_user and copy_page use r2 so need _GLOBAL_TOC()
      powerpc/powernv: Set memory_block_size_bytes to 256MB
      powerpc: Allow ppc_md platform hook to override memory_block_size_bytes
      powerpc/powernv: Fix endian issues in memory error handling code
      powerpc/eeh: Skip eeh sysfs when eeh is disabled
      powerpc: 64bit sendfile is capped at 2GB
      powerpc/powernv: Provide debugfs access to the LPC bus via OPAL
      powerpc/serial: Use saner flags when creating legacy ports
      powerpc: Add cpu family documentation
      powerpc/xmon: Fix up xmon format strings
      powerpc/powernv: Add calls to support little endian host
      powerpc: Document sysfs DSCR interface
      powerpc: Fix regression of per-CPU DSCR setting
      powerpc: Split __SYSFS_SPRSETUP macro
      arch: powerpc/fadump: Cleaning up inconsistent NULL checks
      ...

commit f2e91042a807cbf9b0b0d9776bf37d1ef0bd7ebe
Author: Alexander Graf <agraf@suse.de>
Date:   Thu May 22 17:40:15 2014 +0200

    KVM: PPC: Add CAP to indicate hcall fixes
    
    We worked around some nasty KVM magic page hcall breakages:
    
      1) NX bit not honored, so ignore NX when we detect it
      2) LE guests swizzle hypercall instruction
    
    Without these fixes in place, there's no way it would make sense to expose kvm
    hypercalls to a guest. Chances are immensely high it would trip over and break.
    
    So add a new CAP that gives user space a hint that we have workarounds for the
    bugs above in place. It can use those as hint to disable PV hypercalls when
    the guest CPU is anything POWER7 or higher and the host does not have fixes
    in place.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 154f352c39ae..bab20f410443 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -416,6 +416,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_PPC_ALLOC_HTAB:
 	case KVM_CAP_PPC_RTAS:
+	case KVM_CAP_PPC_FIXUP_HCALL:
 #ifdef CONFIG_KVM_XICS
 	case KVM_CAP_IRQ_XICS:
 #endif

commit f3383cf80e417e86fcc84a2eb4c96bc52842d8d9
Author: Alexander Graf <agraf@suse.de>
Date:   Mon May 12 01:08:32 2014 +0200

    KVM: PPC: Disable NX for old magic page using guests
    
    Old guests try to use the magic page, but map their trampoline code inside
    of an NX region.
    
    Since we can't fix those old kernels, try to detect whether the guest is sane
    or not. If not, just disable NX functionality in KVM so that old guests at
    least work at all. For newer guests, add a bit that we can set to keep NX
    functionality available.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b4e15bf3ff88..154f352c39ae 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -177,8 +177,18 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 		vcpu->arch.shared_big_endian = shared_big_endian;
 #endif
 
-		vcpu->arch.magic_page_pa = param1;
-		vcpu->arch.magic_page_ea = param2;
+		if (!(param2 & MAGIC_PAGE_FLAG_NOT_MAPPED_NX)) {
+			/*
+			 * Older versions of the Linux magic page code had
+			 * a bug where they would map their trampoline code
+			 * NX. If that's the case, remove !PR NX capability.
+			 */
+			vcpu->arch.disable_kernel_nx = true;
+			kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
+		}
+
+		vcpu->arch.magic_page_pa = param1 & ~0xfffULL;
+		vcpu->arch.magic_page_ea = param2 & ~0xfffULL;
 
 		r2 = KVM_MAGIC_FEAT_SR | KVM_MAGIC_FEAT_MAS0_TO_SPRG7;
 

commit 5deb8e7ad8ac7e3fcdfa042acff617f461b361c2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Apr 24 13:46:24 2014 +0200

    KVM: PPC: Make shared struct aka magic page guest endian
    
    The shared (magic) page is a data structure that contains often used
    supervisor privileged SPRs accessible via memory to the user to reduce
    the number of exits we have to take to read/write them.
    
    When we actually share this structure with the guest we have to maintain
    it in guest endianness, because some of the patch tricks only work with
    native endian load/store operations.
    
    Since we only share the structure with either host or guest in little
    endian on book3s_64 pr mode, we don't have to worry about booke or book3s hv.
    
    For booke, the shared struct stays big endian. For book3s_64 hv we maintain
    the struct in host native endian, since it never gets shared with the guest.
    
    For book3s_64 pr we introduce a variable that tells us which endianness the
    shared struct is in and route every access to it through helper inline
    functions that evaluate this variable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a9bd0ff0e173..b4e15bf3ff88 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -125,6 +125,27 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 }
 EXPORT_SYMBOL_GPL(kvmppc_prepare_to_enter);
 
+#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)
+static void kvmppc_swab_shared(struct kvm_vcpu *vcpu)
+{
+	struct kvm_vcpu_arch_shared *shared = vcpu->arch.shared;
+	int i;
+
+	shared->sprg0 = swab64(shared->sprg0);
+	shared->sprg1 = swab64(shared->sprg1);
+	shared->sprg2 = swab64(shared->sprg2);
+	shared->sprg3 = swab64(shared->sprg3);
+	shared->srr0 = swab64(shared->srr0);
+	shared->srr1 = swab64(shared->srr1);
+	shared->dar = swab64(shared->dar);
+	shared->msr = swab64(shared->msr);
+	shared->dsisr = swab32(shared->dsisr);
+	shared->int_pending = swab32(shared->int_pending);
+	for (i = 0; i < ARRAY_SIZE(shared->sr); i++)
+		shared->sr[i] = swab32(shared->sr[i]);
+}
+#endif
+
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 {
 	int nr = kvmppc_get_gpr(vcpu, 11);
@@ -135,7 +156,7 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	unsigned long __maybe_unused param4 = kvmppc_get_gpr(vcpu, 6);
 	unsigned long r2 = 0;
 
-	if (!(vcpu->arch.shared->msr & MSR_SF)) {
+	if (!(kvmppc_get_msr(vcpu) & MSR_SF)) {
 		/* 32 bit mode */
 		param1 &= 0xffffffff;
 		param2 &= 0xffffffff;
@@ -146,6 +167,16 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	switch (nr) {
 	case KVM_HCALL_TOKEN(KVM_HC_PPC_MAP_MAGIC_PAGE):
 	{
+#if defined(CONFIG_PPC_BOOK3S_64) && defined(CONFIG_KVM_BOOK3S_PR_POSSIBLE)
+		/* Book3S can be little endian, find it out here */
+		int shared_big_endian = true;
+		if (vcpu->arch.intr_msr & MSR_LE)
+			shared_big_endian = false;
+		if (shared_big_endian != vcpu->arch.shared_big_endian)
+			kvmppc_swab_shared(vcpu);
+		vcpu->arch.shared_big_endian = shared_big_endian;
+#endif
+
 		vcpu->arch.magic_page_pa = param1;
 		vcpu->arch.magic_page_ea = param2;
 

commit 2743103f91e20d4c4d4f1a8d00821289c4c6ff62
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Apr 24 13:39:16 2014 +0200

    KVM: PPC: PR: Fill pvinfo hcall instructions in big endian
    
    We expose a blob of hypercall instructions to user space that it gives to
    the guest via device tree again. That blob should contain a stream of
    instructions necessary to do a hypercall in big endian, as it just gets
    passed into the guest and old guests use them straight away.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3cf541a53e2a..a9bd0ff0e173 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -1015,10 +1015,10 @@ static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 	u32 inst_nop = 0x60000000;
 #ifdef CONFIG_KVM_BOOKE_HV
 	u32 inst_sc1 = 0x44000022;
-	pvinfo->hcall[0] = inst_sc1;
-	pvinfo->hcall[1] = inst_nop;
-	pvinfo->hcall[2] = inst_nop;
-	pvinfo->hcall[3] = inst_nop;
+	pvinfo->hcall[0] = cpu_to_be32(inst_sc1);
+	pvinfo->hcall[1] = cpu_to_be32(inst_nop);
+	pvinfo->hcall[2] = cpu_to_be32(inst_nop);
+	pvinfo->hcall[3] = cpu_to_be32(inst_nop);
 #else
 	u32 inst_lis = 0x3c000000;
 	u32 inst_ori = 0x60000000;
@@ -1034,10 +1034,10 @@ static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 	 *    sc
 	 *    nop
 	 */
-	pvinfo->hcall[0] = inst_lis | ((KVM_SC_MAGIC_R0 >> 16) & inst_imm_mask);
-	pvinfo->hcall[1] = inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask);
-	pvinfo->hcall[2] = inst_sc;
-	pvinfo->hcall[3] = inst_nop;
+	pvinfo->hcall[0] = cpu_to_be32(inst_lis | ((KVM_SC_MAGIC_R0 >> 16) & inst_imm_mask));
+	pvinfo->hcall[1] = cpu_to_be32(inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask));
+	pvinfo->hcall[2] = cpu_to_be32(inst_sc);
+	pvinfo->hcall[3] = cpu_to_be32(inst_nop);
 #endif
 
 	pvinfo->flags = KVM_PPC_PVINFO_FLAGS_EV_IDLE;

commit 3102f7843c75014fa15d3e6fda3b49f61bc467b4
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 23 18:15:29 2014 +1000

    powerpc/kvm/book3s_hv: Use threads_per_subcore in KVM
    
    To support split core on POWER8 we need to modify various parts of the
    KVM code to use threads_per_subcore instead of threads_per_core. On
    systems that do not support split core threads_per_subcore ==
    threads_per_core and these changes are a nop.
    
    We use threads_per_subcore as the value reported by KVM_CAP_PPC_SMT.
    This communicates to userspace that guests can only be created with
    a value of threads_per_core that is less than or equal to the current
    threads_per_subcore. This ensures that guests can only be created with a
    thread configuration that we are able to run given the current split
    core mode.
    
    Although threads_per_subcore can change during the life of the system,
    the commit that enables that will ensure that threads_per_subcore does
    not change during the life of a KVM VM.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Acked-by: Alexander Graf <agraf@suse.de>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3cf541a53e2a..27919a8715cf 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -384,7 +384,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:
 		if (hv_enabled)
-			r = threads_per_core;
+			r = threads_per_subcore;
 		else
 			r = 0;
 		break;

commit 6c85f52b10fd60e45c6e30c5b85d116406bd3c9b
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Jan 9 19:18:40 2014 -0600

    kvm/ppc: IRQ disabling cleanup
    
    Simplify the handling of lazy EE by going directly from fully-enabled
    to hard-disabled.  This replaces the lazy_irq_pending() check
    (including its misplaced kvm_guest_exit() call).
    
    As suggested by Tiejun Chen, move the interrupt disabling into
    kvmppc_prepare_to_enter() rather than have each caller do it.  Also
    move the IRQ enabling on heavyweight exit into
    kvmppc_prepare_to_enter().
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 026dfaaa4772..3cf541a53e2a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -68,14 +68,16 @@ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
  */
 int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 {
-	int r = 1;
+	int r;
+
+	WARN_ON(irqs_disabled());
+	hard_irq_disable();
 
-	WARN_ON_ONCE(!irqs_disabled());
 	while (true) {
 		if (need_resched()) {
 			local_irq_enable();
 			cond_resched();
-			local_irq_disable();
+			hard_irq_disable();
 			continue;
 		}
 
@@ -101,7 +103,7 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			local_irq_enable();
 			trace_kvm_check_requests(vcpu);
 			r = kvmppc_core_check_requests(vcpu);
-			local_irq_disable();
+			hard_irq_disable();
 			if (r > 0)
 				continue;
 			break;
@@ -113,22 +115,12 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
-#ifdef CONFIG_PPC64
-		/* lazy EE magic */
-		hard_irq_disable();
-		if (lazy_irq_pending()) {
-			/* Got an interrupt in between, try again */
-			local_irq_enable();
-			local_irq_disable();
-			kvm_guest_exit();
-			continue;
-		}
-#endif
-
 		kvm_guest_enter();
-		break;
+		return 1;
 	}
 
+	/* return to host */
+	local_irq_enable();
 	return r;
 }
 EXPORT_SYMBOL_GPL(kvmppc_prepare_to_enter);

commit 736017752d2f6ed0d64f5e15cf48e79779b11c85
Author: CÃ©dric Le Goater <clg@fr.ibm.com>
Date:   Thu Jan 9 11:51:16 2014 +0100

    KVM: PPC: Book3S: MMIO emulation support for little endian guests
    
    MMIO emulation reads the last instruction executed by the guest
    and then emulates. If the guest is running in Little Endian order,
    or more generally in a different endian order of the host, the
    instruction needs to be byte-swapped before being emulated.
    
    This patch adds a helper routine which tests the endian order of
    the host and the guest in order to decide whether a byteswap is
    needed or not. It is then used to byteswap the last instruction
    of the guest in the endian order of the host before MMIO emulation
    is performed.
    
    Finally, kvmppc_handle_load() of kvmppc_handle_store() are modified
    to reverse the endianness of the MMIO if required.
    
    Signed-off-by: CÃ©dric Le Goater <clg@fr.ibm.com>
    [agraf: add booke handling]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7ca9e0a80499..026dfaaa4772 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -673,9 +673,19 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 }
 
 int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
-                       unsigned int rt, unsigned int bytes, int is_bigendian)
+		       unsigned int rt, unsigned int bytes,
+		       int is_default_endian)
 {
 	int idx, ret;
+	int is_bigendian;
+
+	if (kvmppc_need_byteswap(vcpu)) {
+		/* Default endianness is "little endian". */
+		is_bigendian = !is_default_endian;
+	} else {
+		/* Default endianness is "big endian". */
+		is_bigendian = is_default_endian;
+	}
 
 	if (bytes > sizeof(run->mmio.data)) {
 		printk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,
@@ -711,21 +721,31 @@ EXPORT_SYMBOL_GPL(kvmppc_handle_load);
 
 /* Same as above, but sign extends */
 int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
-                        unsigned int rt, unsigned int bytes, int is_bigendian)
+			unsigned int rt, unsigned int bytes,
+			int is_default_endian)
 {
 	int r;
 
 	vcpu->arch.mmio_sign_extend = 1;
-	r = kvmppc_handle_load(run, vcpu, rt, bytes, is_bigendian);
+	r = kvmppc_handle_load(run, vcpu, rt, bytes, is_default_endian);
 
 	return r;
 }
 
 int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
-                        u64 val, unsigned int bytes, int is_bigendian)
+			u64 val, unsigned int bytes, int is_default_endian)
 {
 	void *data = run->mmio.data;
 	int idx, ret;
+	int is_bigendian;
+
+	if (kvmppc_need_byteswap(vcpu)) {
+		/* Default endianness is "little endian". */
+		is_bigendian = !is_default_endian;
+	} else {
+		/* Default endianness is "big endian". */
+		is_bigendian = is_default_endian;
+	}
 
 	if (bytes > sizeof(run->mmio.data)) {
 		printk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,

commit efff19122315f1431f6b02cd2983b15f5d3957bd
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Oct 15 20:43:02 2013 +1100

    KVM: PPC: Store FP/VSX/VMX state in thread_fp/vr_state structures
    
    This uses struct thread_fp_state and struct thread_vr_state to store
    the floating-point, VMX/Altivec and VSX state, rather than flat arrays.
    This makes transferring the state to/from the thread_struct simpler
    and allows us to unify the get/set_one_reg implementations for the
    VSX registers.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9ae97686e9f4..7ca9e0a80499 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -656,14 +656,14 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
 		break;
 	case KVM_MMIO_REG_FPR:
-		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
+		VCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;
 		break;
 #ifdef CONFIG_PPC_BOOK3S
 	case KVM_MMIO_REG_QPR:
 		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
 		break;
 	case KVM_MMIO_REG_FQPR:
-		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
+		VCPU_FPR(vcpu, vcpu->arch.io_gpr & KVM_MMIO_REG_MASK) = gpr;
 		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
 		break;
 #endif

commit a78b55d1c0218b6d91d504941d20e36435c276f5
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:02 2013 +0530

    kvm: powerpc: book3s: drop is_hv_enabled
    
    drop is_hv_enabled, because that should not be a callback property
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0320c1721caa..9ae97686e9f4 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -200,7 +200,7 @@ int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 		goto out;
 
 	/* HV KVM can only do PAPR mode for now */
-	if (!vcpu->arch.papr_enabled && vcpu->kvm->arch.kvm_ops->is_hv_enabled)
+	if (!vcpu->arch.papr_enabled && is_kvmppc_hv_enabled(vcpu->kvm))
 		goto out;
 
 #ifdef CONFIG_KVM_BOOKE_HV

commit cbbc58d4fdfab1a39a6ac1b41fcb17885952157a
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:01 2013 +0530

    kvm: powerpc: book3s: Allow the HV and PR selection per virtual machine
    
    This moves the kvmppc_ops callbacks to be a per VM entity. This
    enables us to select HV and PR mode when creating a VM. We also
    allow both kvm-hv and kvm-pr kernel module to be loaded. To
    achieve this we move /dev/kvm ownership to kvm.ko module. Depending on
    which KVM mode we select during VM creation we take a reference
    count on respective module
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [agraf: fix coding style]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b103d747934a..0320c1721caa 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -26,6 +26,7 @@
 #include <linux/fs.h>
 #include <linux/slab.h>
 #include <linux/file.h>
+#include <linux/module.h>
 #include <asm/cputable.h>
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>
@@ -39,7 +40,11 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
-struct kvmppc_ops *kvmppc_ops;
+struct kvmppc_ops *kvmppc_hv_ops;
+EXPORT_SYMBOL_GPL(kvmppc_hv_ops);
+struct kvmppc_ops *kvmppc_pr_ops;
+EXPORT_SYMBOL_GPL(kvmppc_pr_ops);
+
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
@@ -195,7 +200,7 @@ int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 		goto out;
 
 	/* HV KVM can only do PAPR mode for now */
-	if (!vcpu->arch.papr_enabled && kvmppc_ops->is_hv_enabled)
+	if (!vcpu->arch.papr_enabled && vcpu->kvm->arch.kvm_ops->is_hv_enabled)
 		goto out;
 
 #ifdef CONFIG_KVM_BOOKE_HV
@@ -271,10 +276,35 @@ void kvm_arch_check_processor_compat(void *rtn)
 
 int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
-	if (type)
-		return -EINVAL;
-
+	struct kvmppc_ops *kvm_ops = NULL;
+	/*
+	 * if we have both HV and PR enabled, default is HV
+	 */
+	if (type == 0) {
+		if (kvmppc_hv_ops)
+			kvm_ops = kvmppc_hv_ops;
+		else
+			kvm_ops = kvmppc_pr_ops;
+		if (!kvm_ops)
+			goto err_out;
+	} else	if (type == KVM_VM_PPC_HV) {
+		if (!kvmppc_hv_ops)
+			goto err_out;
+		kvm_ops = kvmppc_hv_ops;
+	} else if (type == KVM_VM_PPC_PR) {
+		if (!kvmppc_pr_ops)
+			goto err_out;
+		kvm_ops = kvmppc_pr_ops;
+	} else
+		goto err_out;
+
+	if (kvm_ops->owner && !try_module_get(kvm_ops->owner))
+		return -ENOENT;
+
+	kvm->arch.kvm_ops = kvm_ops;
 	return kvmppc_core_init_vm(kvm);
+err_out:
+	return -EINVAL;
 }
 
 void kvm_arch_destroy_vm(struct kvm *kvm)
@@ -294,6 +324,9 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 	kvmppc_core_destroy_vm(kvm);
 
 	mutex_unlock(&kvm->lock);
+
+	/* drop the module reference */
+	module_put(kvm->arch.kvm_ops->owner);
 }
 
 void kvm_arch_sync_events(struct kvm *kvm)
@@ -303,6 +336,10 @@ void kvm_arch_sync_events(struct kvm *kvm)
 int kvm_dev_ioctl_check_extension(long ext)
 {
 	int r;
+	/* FIXME!!
+	 * Should some of this be vm ioctl ? is it possible now ?
+	 */
+	int hv_enabled = kvmppc_hv_ops ? 1 : 0;
 
 	switch (ext) {
 #ifdef CONFIG_BOOKE
@@ -329,7 +366,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_SW_TLB:
 #endif
 		/* We support this only for PR */
-		r = !kvmppc_ops->is_hv_enabled;
+		r = !hv_enabled;
 		break;
 #ifdef CONFIG_KVM_MMIO
 	case KVM_CAP_COALESCED_MMIO:
@@ -354,13 +391,13 @@ int kvm_dev_ioctl_check_extension(long ext)
 #endif /* CONFIG_PPC_BOOK3S_64 */
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:
-		if (kvmppc_ops->is_hv_enabled)
+		if (hv_enabled)
 			r = threads_per_core;
 		else
 			r = 0;
 		break;
 	case KVM_CAP_PPC_RMA:
-		r = kvmppc_ops->is_hv_enabled;
+		r = hv_enabled;
 		/* PPC970 requires an RMA */
 		if (r && cpu_has_feature(CPU_FTR_ARCH_201))
 			r = 2;
@@ -368,7 +405,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #endif
 	case KVM_CAP_SYNC_MMU:
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
-		if (kvmppc_ops->is_hv_enabled)
+		if (hv_enabled)
 			r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
 		else
 			r = 0;
@@ -380,7 +417,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 		break;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_HTAB_FD:
-		r = kvmppc_ops->is_hv_enabled;
+		r = hv_enabled;
 		break;
 #endif
 	case KVM_CAP_NR_VCPUS:
@@ -390,7 +427,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 		 * will have secondary threads "offline"), and for other KVM
 		 * implementations just count online CPUs.
 		 */
-		if (kvmppc_ops->is_hv_enabled)
+		if (hv_enabled)
 			r = num_present_cpus();
 		else
 			r = num_online_cpus();
@@ -1039,9 +1076,10 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	}
 	case KVM_PPC_GET_SMMU_INFO: {
 		struct kvm_ppc_smmu_info info;
+		struct kvm *kvm = filp->private_data;
 
 		memset(&info, 0, sizeof(info));
-		r = kvmppc_ops->get_smmu_info(kvm, &info);
+		r = kvm->arch.kvm_ops->get_smmu_info(kvm, &info);
 		if (r >= 0 && copy_to_user(argp, &info, sizeof(info)))
 			r = -EFAULT;
 		break;
@@ -1052,9 +1090,10 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_rtas_define_token(kvm, argp);
 		break;
 	}
-	default:
-		r = kvmppc_ops->arch_vm_ioctl(filp, ioctl, arg);
-
+	default: {
+		struct kvm *kvm = filp->private_data;
+		r = kvm->arch.kvm_ops->arch_vm_ioctl(filp, ioctl, arg);
+	}
 #else /* CONFIG_PPC_BOOK3S_64 */
 	default:
 		r = -ENOTTY;
@@ -1104,15 +1143,10 @@ EXPORT_SYMBOL_GPL(kvmppc_init_lpid);
 
 int kvm_arch_init(void *opaque)
 {
-	if (kvmppc_ops) {
-		printk(KERN_ERR "kvm: already loaded the other module\n");
-		return -EEXIST;
-	}
-	kvmppc_ops = (struct kvmppc_ops *)opaque;
 	return 0;
 }
 
 void kvm_arch_exit(void)
 {
-	kvmppc_ops = NULL;
+
 }

commit 5587027ce9d59a57aecaa190be1c8e560aaff45d
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:18:00 2013 +0530

    kvm: Add struct kvm arg to memslot APIs
    
    We will use that in the later patch to find the kvm ops handler
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9aaa07efa4b6..b103d747934a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -417,15 +417,16 @@ long kvm_arch_dev_ioctl(struct file *filp,
 	return -EINVAL;
 }
 
-void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+void kvm_arch_free_memslot(struct kvm *kvm, struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont)
 {
-	kvmppc_core_free_memslot(free, dont);
+	kvmppc_core_free_memslot(kvm, free, dont);
 }
 
-int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
+int kvm_arch_create_memslot(struct kvm *kvm, struct kvm_memory_slot *slot,
+			    unsigned long npages)
 {
-	return kvmppc_core_create_memslot(slot, npages);
+	return kvmppc_core_create_memslot(kvm, slot, npages);
 }
 
 void kvm_arch_memslots_updated(struct kvm *kvm)

commit 2ba9f0d8875073a2ed802fca0c25c9bfc4338439
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:59 2013 +0530

    kvm: powerpc: book3s: Support building HV and PR KVM as module
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [agraf: squash in compile fix]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 874a2a5e6cec..9aaa07efa4b6 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -126,6 +126,7 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 
 	return r;
 }
+EXPORT_SYMBOL_GPL(kvmppc_prepare_to_enter);
 
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 {
@@ -179,6 +180,7 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 
 	return r;
 }
+EXPORT_SYMBOL_GPL(kvmppc_kvm_pv);
 
 int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 {
@@ -207,6 +209,7 @@ int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 	vcpu->arch.sane = r;
 	return r ? 0 : -EINVAL;
 }
+EXPORT_SYMBOL_GPL(kvmppc_sanity_check);
 
 int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 {
@@ -241,6 +244,7 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 
 	return r;
 }
+EXPORT_SYMBOL_GPL(kvmppc_emulate_mmio);
 
 int kvm_arch_hardware_enable(void *garbage)
 {
@@ -665,6 +669,7 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 	return EMULATE_DO_MMIO;
 }
+EXPORT_SYMBOL_GPL(kvmppc_handle_load);
 
 /* Same as above, but sign extends */
 int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
@@ -726,6 +731,7 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 	return EMULATE_DO_MMIO;
 }
+EXPORT_SYMBOL_GPL(kvmppc_handle_store);
 
 int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
@@ -1074,22 +1080,26 @@ long kvmppc_alloc_lpid(void)
 
 	return lpid;
 }
+EXPORT_SYMBOL_GPL(kvmppc_alloc_lpid);
 
 void kvmppc_claim_lpid(long lpid)
 {
 	set_bit(lpid, lpid_inuse);
 }
+EXPORT_SYMBOL_GPL(kvmppc_claim_lpid);
 
 void kvmppc_free_lpid(long lpid)
 {
 	clear_bit(lpid, lpid_inuse);
 }
+EXPORT_SYMBOL_GPL(kvmppc_free_lpid);
 
 void kvmppc_init_lpid(unsigned long nr_lpids_param)
 {
 	nr_lpids = min_t(unsigned long, KVMPPC_NR_LPIDS, nr_lpids_param);
 	memset(lpid_inuse, 0, sizeof(lpid_inuse));
 }
+EXPORT_SYMBOL_GPL(kvmppc_init_lpid);
 
 int kvm_arch_init(void *opaque)
 {

commit 699cc87641c123128bf3a4e12c0a8d739b1ac2f3
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:56 2013 +0530

    kvm: powerpc: book3s: Add is_hv_enabled to kvmppc_ops
    
    This help us to identify whether we are running with hypervisor mode KVM
    enabled. The change is needed so that we can have both HV and PR kvm
    enabled in the same kernel.
    
    If both HV and PR KVM are included, interrupts come in to the HV version
    of the kvmppc_interrupt code, which then jumps to the PR handler,
    renamed to kvmppc_interrupt_pr, if the guest is a PR guest.
    
    Allowing both PR and HV in the same kernel required some changes to
    kvm_dev_ioctl_check_extension(), since the values returned now can't
    be selected with #ifdefs as much as previously. We look at is_hv_enabled
    to return the right value when checking for capabilities.For capabilities that
    are only provided by HV KVM, we return the HV value only if
    is_hv_enabled is true. For capabilities provided by PR KVM but not HV,
    we return the PR value only if is_hv_enabled is false.
    
    NOTE: in later patch we replace is_hv_enabled with a static inline
    function comparing kvm_ppc_ops
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 69b930550d2e..874a2a5e6cec 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -52,7 +52,6 @@ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
-#ifndef CONFIG_KVM_BOOK3S_64_HV
 /*
  * Common checks before entering the guest world.  Call with interrupts
  * disabled.
@@ -127,7 +126,6 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 
 	return r;
 }
-#endif /* CONFIG_KVM_BOOK3S_64_HV */
 
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 {
@@ -194,11 +192,9 @@ int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 	if ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)
 		goto out;
 
-#ifdef CONFIG_KVM_BOOK3S_64_HV
 	/* HV KVM can only do PAPR mode for now */
-	if (!vcpu->arch.papr_enabled)
+	if (!vcpu->arch.papr_enabled && kvmppc_ops->is_hv_enabled)
 		goto out;
-#endif
 
 #ifdef CONFIG_KVM_BOOKE_HV
 	if (!cpu_has_feature(CPU_FTR_EMB_HV))
@@ -322,22 +318,26 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_DEVICE_CTRL:
 		r = 1;
 		break;
-#ifndef CONFIG_KVM_BOOK3S_64_HV
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:
 #if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB:
 #endif
-#ifdef CONFIG_KVM_MPIC
-	case KVM_CAP_IRQ_MPIC:
-#endif
-		r = 1;
+		/* We support this only for PR */
+		r = !kvmppc_ops->is_hv_enabled;
 		break;
+#ifdef CONFIG_KVM_MMIO
 	case KVM_CAP_COALESCED_MMIO:
 		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 		break;
 #endif
+#ifdef CONFIG_KVM_MPIC
+	case KVM_CAP_IRQ_MPIC:
+		r = 1;
+		break;
+#endif
+
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_PPC_ALLOC_HTAB:
@@ -348,32 +348,37 @@ int kvm_dev_ioctl_check_extension(long ext)
 		r = 1;
 		break;
 #endif /* CONFIG_PPC_BOOK3S_64 */
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_SMT:
-		r = threads_per_core;
+		if (kvmppc_ops->is_hv_enabled)
+			r = threads_per_core;
+		else
+			r = 0;
 		break;
 	case KVM_CAP_PPC_RMA:
-		r = 1;
+		r = kvmppc_ops->is_hv_enabled;
 		/* PPC970 requires an RMA */
-		if (cpu_has_feature(CPU_FTR_ARCH_201))
+		if (r && cpu_has_feature(CPU_FTR_ARCH_201))
 			r = 2;
 		break;
 #endif
 	case KVM_CAP_SYNC_MMU:
-#ifdef CONFIG_KVM_BOOK3S_64_HV
-		r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
+		if (kvmppc_ops->is_hv_enabled)
+			r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
+		else
+			r = 0;
 #elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 		r = 1;
 #else
 		r = 0;
-		break;
 #endif
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+		break;
+#ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	case KVM_CAP_PPC_HTAB_FD:
-		r = 1;
+		r = kvmppc_ops->is_hv_enabled;
 		break;
 #endif
-		break;
 	case KVM_CAP_NR_VCPUS:
 		/*
 		 * Recommending a number of CPUs is somewhat arbitrary; we
@@ -381,11 +386,10 @@ int kvm_dev_ioctl_check_extension(long ext)
 		 * will have secondary threads "offline"), and for other KVM
 		 * implementations just count online CPUs.
 		 */
-#ifdef CONFIG_KVM_BOOK3S_64_HV
-		r = num_present_cpus();
-#else
-		r = num_online_cpus();
-#endif
+		if (kvmppc_ops->is_hv_enabled)
+			r = num_present_cpus();
+		else
+			r = num_online_cpus();
 		break;
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;

commit 3a167beac07cba597856c12b87638a06b0d53db7
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Mon Oct 7 22:17:53 2013 +0530

    kvm: powerpc: Add kvmppc_ops callback
    
    This patch add a new callback kvmppc_ops. This will help us in enabling
    both HV and PR KVM together in the same kernel. The actual change to
    enable them together is done in the later patch in the series.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    [agraf: squash in booke changes]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 07c0106fab76..69b930550d2e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -39,6 +39,8 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
+struct kvmppc_ops *kvmppc_ops;
+
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
 	return !!(v->arch.pending_exceptions) ||
@@ -1024,52 +1026,11 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);
 		goto out;
 	}
-#endif /* CONFIG_PPC_BOOK3S_64 */
-
-#ifdef CONFIG_KVM_BOOK3S_64_HV
-	case KVM_ALLOCATE_RMA: {
-		struct kvm_allocate_rma rma;
-		struct kvm *kvm = filp->private_data;
-
-		r = kvm_vm_ioctl_allocate_rma(kvm, &rma);
-		if (r >= 0 && copy_to_user(argp, &rma, sizeof(rma)))
-			r = -EFAULT;
-		break;
-	}
-
-	case KVM_PPC_ALLOCATE_HTAB: {
-		u32 htab_order;
-
-		r = -EFAULT;
-		if (get_user(htab_order, (u32 __user *)argp))
-			break;
-		r = kvmppc_alloc_reset_hpt(kvm, &htab_order);
-		if (r)
-			break;
-		r = -EFAULT;
-		if (put_user(htab_order, (u32 __user *)argp))
-			break;
-		r = 0;
-		break;
-	}
-
-	case KVM_PPC_GET_HTAB_FD: {
-		struct kvm_get_htab_fd ghf;
-
-		r = -EFAULT;
-		if (copy_from_user(&ghf, argp, sizeof(ghf)))
-			break;
-		r = kvm_vm_ioctl_get_htab_fd(kvm, &ghf);
-		break;
-	}
-#endif /* CONFIG_KVM_BOOK3S_64_HV */
-
-#ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_PPC_GET_SMMU_INFO: {
 		struct kvm_ppc_smmu_info info;
 
 		memset(&info, 0, sizeof(info));
-		r = kvm_vm_ioctl_get_smmu_info(kvm, &info);
+		r = kvmppc_ops->get_smmu_info(kvm, &info);
 		if (r >= 0 && copy_to_user(argp, &info, sizeof(info)))
 			r = -EFAULT;
 		break;
@@ -1080,11 +1041,14 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_rtas_define_token(kvm, argp);
 		break;
 	}
-#endif /* CONFIG_PPC_BOOK3S_64 */
+	default:
+		r = kvmppc_ops->arch_vm_ioctl(filp, ioctl, arg);
+
+#else /* CONFIG_PPC_BOOK3S_64 */
 	default:
 		r = -ENOTTY;
+#endif
 	}
-
 out:
 	return r;
 }
@@ -1125,9 +1089,15 @@ void kvmppc_init_lpid(unsigned long nr_lpids_param)
 
 int kvm_arch_init(void *opaque)
 {
+	if (kvmppc_ops) {
+		printk(KERN_ERR "kvm: already loaded the other module\n");
+		return -EEXIST;
+	}
+	kvmppc_ops = (struct kvmppc_ops *)opaque;
 	return 0;
 }
 
 void kvm_arch_exit(void)
 {
+	kvmppc_ops = NULL;
 }

commit 45d9a2220f6004b47c362cc7fc7cf9a73cb6353a
Merge: 2386a3b0fbb0 02afc27faec9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Sep 5 08:50:26 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs pile 1 from Al Viro:
     "Unfortunately, this merge window it'll have a be a lot of small piles -
      my fault, actually, for not keeping #for-next in anything that would
      resemble a sane shape ;-/
    
      This pile: assorted fixes (the first 3 are -stable fodder, IMO) and
      cleanups + %pd/%pD formats (dentry/file pathname, up to 4 last
      components) + several long-standing patches from various folks.
    
      There definitely will be a lot more (starting with Miklos'
      check_submount_and_drop() series)"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (26 commits)
      direct-io: Handle O_(D)SYNC AIO
      direct-io: Implement generic deferred AIO completions
      add formats for dentry/file pathnames
      kvm eventfd: switch to fdget
      powerpc kvm: use fdget
      switch fchmod() to fdget
      switch epoll_ctl() to fdget
      switch copy_module_from_fd() to fdget
      git simplify nilfs check for busy subtree
      ibmasmfs: don't bother passing superblock when not needed
      don't pass superblock to hypfs_{mkdir,create*}
      don't pass superblock to hypfs_diag_create_files
      don't pass superblock to hypfs_vm_create_files()
      oprofile: get rid of pointless forward declarations of struct super_block
      oprofilefs_create_...() do not need superblock argument
      oprofilefs_mkdir() doesn't need superblock argument
      don't bother with passing superblock to oprofile_create_stats_files()
      oprofile: don't bother with passing superblock to ->create_files()
      don't bother passing sb to oprofile_create_files()
      coh901318: don't open-code simple_read_from_buffer()
      ...

commit 70abadedab73066fae584488640056f83bce5f95
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Aug 30 15:04:22 2013 -0400

    powerpc kvm: use fdget
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6316ee336e88..aca2e8f2e33f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -823,39 +823,39 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 #endif
 #ifdef CONFIG_KVM_MPIC
 	case KVM_CAP_IRQ_MPIC: {
-		struct file *filp;
+		struct fd f;
 		struct kvm_device *dev;
 
 		r = -EBADF;
-		filp = fget(cap->args[0]);
-		if (!filp)
+		f = fdget(cap->args[0]);
+		if (!f.file)
 			break;
 
 		r = -EPERM;
-		dev = kvm_device_from_filp(filp);
+		dev = kvm_device_from_filp(f.file);
 		if (dev)
 			r = kvmppc_mpic_connect_vcpu(dev, vcpu, cap->args[1]);
 
-		fput(filp);
+		fdput(f);
 		break;
 	}
 #endif
 #ifdef CONFIG_KVM_XICS
 	case KVM_CAP_IRQ_XICS: {
-		struct file *filp;
+		struct fd f;
 		struct kvm_device *dev;
 
 		r = -EBADF;
-		filp = fget(cap->args[0]);
-		if (!filp)
+		f = fdget(cap->args[0]);
+		if (!f.file)
 			break;
 
 		r = -EPERM;
-		dev = kvm_device_from_filp(filp);
+		dev = kvm_device_from_filp(f.file);
 		if (dev)
 			r = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);
 
-		fput(filp);
+		fdput(f);
 		break;
 	}
 #endif /* CONFIG_KVM_XICS */

commit bf550fc93d9855872a95e69e4002256110d89858
Merge: 7e48c101e0c5 cc2df20c7c4c
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Aug 29 00:41:59 2013 +0200

    Merge remote-tracking branch 'origin/next' into kvm-ppc-next
    
    Conflicts:
            mm/Kconfig
    
    CMA DMA split and ZSWAP introduction were conflicting, fix up manually.

commit e59dbe09f8e6fb8f6ee19dc79d1a2f14299e4cd2
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Thu Jul 4 13:40:29 2013 +0900

    KVM: Introduce kvm_arch_memslots_updated()
    
    This is called right after the memslots is updated, i.e. when the result
    of update_memslots() gets installed in install_new_memslots().  Since
    the memslots needs to be updated twice when we delete or move a memslot,
    kvm_arch_commit_memory_region() does not correspond to this exactly.
    
    In the following patch, x86 will use this new API to check if the mmio
    generation has reached its maximum value, in which case mmio sptes need
    to be flushed out.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Acked-by: Alexander Graf <agraf@suse.de>
    Reviewed-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6316ee336e88..ae63ae4a1a5f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -420,6 +420,10 @@ int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 	return kvmppc_core_create_memslot(slot, npages);
 }
 
+void kvm_arch_memslots_updated(struct kvm *kvm)
+{
+}
+
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
 				   struct kvm_userspace_memory_region *mem,

commit 5f1c248f52c12e155e0fe6a614178181f7629901
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Jul 10 17:47:39 2013 -0500

    kvm/ppc: Call trace_hardirqs_on before entry
    
    Currently this is only being done on 64-bit.  Rather than just move it
    out of the 64-bit ifdef, move it to kvm_lazy_ee_enable() so that it is
    consistent with lazy ee state, and so that we don't track more host
    code as interrupts-enabled than necessary.
    
    Rename kvm_lazy_ee_enable() to kvm_fix_ee_before_entry() to reflect
    that this function now has a role on 32-bit as well.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6316ee336e88..4e05f8c693b4 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -117,8 +117,6 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			kvm_guest_exit();
 			continue;
 		}
-
-		trace_hardirqs_on();
 #endif
 
 		kvm_guest_enter();

commit 5975a2e0950291a6bfe9fd5880e7952ff87764be
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Apr 27 00:28:37 2013 +0000

    KVM: PPC: Book3S: Add API for in-kernel XICS emulation
    
    This adds the API for userspace to instantiate an XICS device in a VM
    and connect VCPUs to it.  The API consists of a new device type for
    the KVM_CREATE_DEVICE ioctl, a new capability KVM_CAP_IRQ_XICS, which
    functions similarly to KVM_CAP_IRQ_MPIC, and the KVM_IRQ_LINE ioctl,
    which is used to assert and deassert interrupt inputs of the XICS.
    
    The XICS device has one attribute group, KVM_DEV_XICS_GRP_SOURCES.
    Each attribute within this group corresponds to the state of one
    interrupt source.  The attribute number is the same as the interrupt
    source number.
    
    This does not support irq routing or irqfd yet.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Acked-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 270773f0d955..6316ee336e88 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -342,6 +342,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_PPC_ALLOC_HTAB:
 	case KVM_CAP_PPC_RTAS:
+#ifdef CONFIG_KVM_XICS
+	case KVM_CAP_IRQ_XICS:
+#endif
 		r = 1;
 		break;
 #endif /* CONFIG_PPC_BOOK3S_64 */
@@ -837,6 +840,25 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		break;
 	}
 #endif
+#ifdef CONFIG_KVM_XICS
+	case KVM_CAP_IRQ_XICS: {
+		struct file *filp;
+		struct kvm_device *dev;
+
+		r = -EBADF;
+		filp = fget(cap->args[0]);
+		if (!filp)
+			break;
+
+		r = -EPERM;
+		dev = kvm_device_from_filp(filp);
+		if (dev)
+			r = kvmppc_xics_connect_vcpu(dev, vcpu, cap->args[1]);
+
+		fput(filp);
+		break;
+	}
+#endif /* CONFIG_KVM_XICS */
 	default:
 		r = -EINVAL;
 		break;

commit ed840ee9c8f15a726a1e7b98c9f5cfc49c2a0ff2
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 26 14:53:39 2013 +0000

    kvm/ppc: Hold srcu lock when calling kvm_io_bus_read/write
    
    These functions do an srcu_dereference without acquiring the srcu lock
    themselves.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 31084c6335c9..270773f0d955 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -622,6 +622,8 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
                        unsigned int rt, unsigned int bytes, int is_bigendian)
 {
+	int idx, ret;
+
 	if (bytes > sizeof(run->mmio.data)) {
 		printk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,
 		       run->mmio.len);
@@ -637,8 +639,14 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	vcpu->mmio_is_write = 0;
 	vcpu->arch.mmio_sign_extend = 0;
 
-	if (!kvm_io_bus_read(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
-			     bytes, &run->mmio.data)) {
+	idx = srcu_read_lock(&vcpu->kvm->srcu);
+
+	ret = kvm_io_bus_read(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
+			      bytes, &run->mmio.data);
+
+	srcu_read_unlock(&vcpu->kvm->srcu, idx);
+
+	if (!ret) {
 		kvmppc_complete_mmio_load(vcpu, run);
 		vcpu->mmio_needed = 0;
 		return EMULATE_DONE;
@@ -663,6 +671,7 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
                         u64 val, unsigned int bytes, int is_bigendian)
 {
 	void *data = run->mmio.data;
+	int idx, ret;
 
 	if (bytes > sizeof(run->mmio.data)) {
 		printk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,
@@ -692,8 +701,14 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		}
 	}
 
-	if (!kvm_io_bus_write(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
-			      bytes, &run->mmio.data)) {
+	idx = srcu_read_lock(&vcpu->kvm->srcu);
+
+	ret = kvm_io_bus_write(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
+			       bytes, &run->mmio.data);
+
+	srcu_read_unlock(&vcpu->kvm->srcu, idx);
+
+	if (!ret) {
 		vcpu->mmio_needed = 0;
 		return EMULATE_DONE;
 	}

commit bc5ad3f3701116e7db57268e6f89010ec714697e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Apr 17 20:30:26 2013 +0000

    KVM: PPC: Book3S: Add kernel emulation for the XICS interrupt controller
    
    This adds in-kernel emulation of the XICS (eXternal Interrupt
    Controller Specification) interrupt controller specified by PAPR, for
    both HV and PR KVM guests.
    
    The XICS emulation supports up to 1048560 interrupt sources.
    Interrupt source numbers below 16 are reserved; 0 is used to mean no
    interrupt and 2 is used for IPIs.  Internally these are represented in
    blocks of 1024, called ICS (interrupt controller source) entities, but
    that is not visible to userspace.
    
    Each vcpu gets one ICP (interrupt controller presentation) entity,
    used to store the per-vcpu state such as vcpu priority, pending
    interrupt state, IPI request, etc.
    
    This does not include any API or any way to connect vcpus to their
    ICP state; that will be added in later patches.
    
    This is based on an initial implementation by Michael Ellerman
    <michael@ellerman.id.au> reworked by Benjamin Herrenschmidt and
    Paul Mackerras.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix typo, add dependency on !KVM_MPIC]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index d4fd443ae7bd..31084c6335c9 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -471,6 +471,9 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 	case KVMPPC_IRQ_MPIC:
 		kvmppc_mpic_disconnect_vcpu(vcpu->arch.mpic, vcpu);
 		break;
+	case KVMPPC_IRQ_XICS:
+		kvmppc_xics_free_icp(vcpu);
+		break;
 	}
 
 	kvmppc_core_vcpu_free(vcpu);

commit 8e591cb7204739efa8e15967ea334eb367039dde
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Apr 17 20:30:00 2013 +0000

    KVM: PPC: Book3S: Add infrastructure to implement kernel-side RTAS calls
    
    For pseries machine emulation, in order to move the interrupt
    controller code to the kernel, we need to intercept some RTAS
    calls in the kernel itself.  This adds an infrastructure to allow
    in-kernel handlers to be registered for RTAS services by name.
    A new ioctl, KVM_PPC_RTAS_DEFINE_TOKEN, then allows userspace to
    associate token values with those service names.  Then, when the
    guest requests an RTAS service with one of those token values, it
    will be handled by the relevant in-kernel handler rather than being
    passed up to userspace as at present.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix warning]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index d8e81e6c1afe..d4fd443ae7bd 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -341,6 +341,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
 	case KVM_CAP_PPC_ALLOC_HTAB:
+	case KVM_CAP_PPC_RTAS:
 		r = 1;
 		break;
 #endif /* CONFIG_PPC_BOOK3S_64 */
@@ -986,6 +987,7 @@ long kvm_arch_vm_ioctl(struct file *filp,
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	case KVM_ALLOCATE_RMA: {
 		struct kvm_allocate_rma rma;
+		struct kvm *kvm = filp->private_data;
 
 		r = kvm_vm_ioctl_allocate_rma(kvm, &rma);
 		if (r >= 0 && copy_to_user(argp, &rma, sizeof(rma)))
@@ -1030,6 +1032,12 @@ long kvm_arch_vm_ioctl(struct file *filp,
 			r = -EFAULT;
 		break;
 	}
+	case KVM_PPC_RTAS_DEFINE_TOKEN: {
+		struct kvm *kvm = filp->private_data;
+
+		r = kvm_vm_ioctl_rtas_define_token(kvm, argp);
+		break;
+	}
 #endif /* CONFIG_PPC_BOOK3S_64 */
 	default:
 		r = -ENOTTY;

commit 5efdb4be598fc2af6937c3387586635ddf6fd2c8
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Apr 17 00:37:57 2013 +0200

    KVM: PPC: MPIC: Add support for KVM_IRQ_LINE
    
    Now that all pieces are in place for reusing generic irq infrastructure,
    we can copy x86's implementation of KVM_IRQ_LINE irq injection and simply
    reuse it for PPC, as it will work there just as well.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 5d046bbdf11f..d8e81e6c1afe 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -33,6 +33,7 @@
 #include <asm/cputhreads.h>
 #include <asm/irqflags.h>
 #include "timing.h"
+#include "irq.h"
 #include "../mm/mmu_decl.h"
 
 #define CREATE_TRACE_POINTS
@@ -939,6 +940,18 @@ static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 	return 0;
 }
 
+int kvm_vm_ioctl_irq_line(struct kvm *kvm, struct kvm_irq_level *irq_event,
+			  bool line_status)
+{
+	if (!irqchip_in_kernel(kvm))
+		return -ENXIO;
+
+	irq_event->status = kvm_set_irq(kvm, KVM_USERSPACE_IRQ_SOURCE_ID,
+					irq_event->irq, irq_event->level,
+					line_status);
+	return 0;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
                        unsigned int ioctl, unsigned long arg)
 {

commit eb1e4f43e0f47f2655372c7d32c43db9711c278e
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:47 2013 +0000

    kvm/ppc/mpic: add KVM_CAP_IRQ_MPIC
    
    Enabling this capability connects the vcpu to the designated in-kernel
    MPIC.  Using explicit connections between vcpus and irqchips allows
    for flexibility, but the main benefit at the moment is that it
    simplifies the code -- KVM doesn't need vm-global state to remember
    which MPIC object is associated with this vm, and it doesn't need to
    care about ordering between irqchip creation and vcpu creation.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: add stub functions for kvmppc_mpic_{dis,}connect_vcpu]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 88d69cf1f953..5d046bbdf11f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -25,6 +25,7 @@
 #include <linux/hrtimer.h>
 #include <linux/fs.h>
 #include <linux/slab.h>
+#include <linux/file.h>
 #include <asm/cputable.h>
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>
@@ -326,6 +327,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_GET_PVINFO:
 #if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB:
+#endif
+#ifdef CONFIG_KVM_MPIC
+	case KVM_CAP_IRQ_MPIC:
 #endif
 		r = 1;
 		break;
@@ -460,6 +464,13 @@ void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 	tasklet_kill(&vcpu->arch.tasklet);
 
 	kvmppc_remove_vcpu_debugfs(vcpu);
+
+	switch (vcpu->arch.irq_type) {
+	case KVMPPC_IRQ_MPIC:
+		kvmppc_mpic_disconnect_vcpu(vcpu->arch.mpic, vcpu);
+		break;
+	}
+
 	kvmppc_core_vcpu_free(vcpu);
 }
 
@@ -786,6 +797,25 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = kvm_vcpu_ioctl_config_tlb(vcpu, &cfg);
 		break;
 	}
+#endif
+#ifdef CONFIG_KVM_MPIC
+	case KVM_CAP_IRQ_MPIC: {
+		struct file *filp;
+		struct kvm_device *dev;
+
+		r = -EBADF;
+		filp = fget(cap->args[0]);
+		if (!filp)
+			break;
+
+		r = -EPERM;
+		dev = kvm_device_from_filp(filp);
+		if (dev)
+			r = kvmppc_mpic_connect_vcpu(dev, vcpu, cap->args[1]);
+
+		fput(filp);
+		break;
+	}
 #endif
 	default:
 		r = -EINVAL;

commit 5df554ad5b7522ea62b0ff9d5be35183494efc21
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 12 14:08:46 2013 +0000

    kvm/ppc/mpic: in-kernel MPIC emulation
    
    Hook the MPIC code up to the KVM interfaces, add locking, etc.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: add stub function for kvmppc_mpic_set_epr, non-booke, 64bit]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6b8108624851..88d69cf1f953 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -317,6 +317,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_ONE_REG:
 	case KVM_CAP_IOEVENTFD:
+	case KVM_CAP_DEVICE_CTRL:
 		r = 1;
 		break;
 #ifndef CONFIG_KVM_BOOK3S_64_HV
@@ -762,7 +763,10 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		break;
 	case KVM_CAP_PPC_EPR:
 		r = 0;
-		vcpu->arch.epr_enabled = cap->args[0];
+		if (cap->args[0])
+			vcpu->arch.epr_flags |= KVMPPC_EPR_USER;
+		else
+			vcpu->arch.epr_flags &= ~KVMPPC_EPR_USER;
 		break;
 #ifdef CONFIG_BOOKE
 	case KVM_CAP_PPC_BOOKE_WATCHDOG:
@@ -908,6 +912,7 @@ static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 long kvm_arch_vm_ioctl(struct file *filp,
                        unsigned int ioctl, unsigned long arg)
 {
+	struct kvm *kvm __maybe_unused = filp->private_data;
 	void __user *argp = (void __user *)arg;
 	long r;
 
@@ -926,7 +931,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CREATE_SPAPR_TCE: {
 		struct kvm_create_spapr_tce create_tce;
-		struct kvm *kvm = filp->private_data;
 
 		r = -EFAULT;
 		if (copy_from_user(&create_tce, argp, sizeof(create_tce)))
@@ -938,7 +942,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
 
 #ifdef CONFIG_KVM_BOOK3S_64_HV
 	case KVM_ALLOCATE_RMA: {
-		struct kvm *kvm = filp->private_data;
 		struct kvm_allocate_rma rma;
 
 		r = kvm_vm_ioctl_allocate_rma(kvm, &rma);
@@ -948,7 +951,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	}
 
 	case KVM_PPC_ALLOCATE_HTAB: {
-		struct kvm *kvm = filp->private_data;
 		u32 htab_order;
 
 		r = -EFAULT;
@@ -965,7 +967,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	}
 
 	case KVM_PPC_GET_HTAB_FD: {
-		struct kvm *kvm = filp->private_data;
 		struct kvm_get_htab_fd ghf;
 
 		r = -EFAULT;
@@ -978,7 +979,6 @@ long kvm_arch_vm_ioctl(struct file *filp,
 
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_PPC_GET_SMMU_INFO: {
-		struct kvm *kvm = filp->private_data;
 		struct kvm_ppc_smmu_info info;
 
 		memset(&info, 0, sizeof(info));

commit 092d62ee93039bfccbb3a36c69d0c3ee0966a97a
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Mon Apr 8 00:32:12 2013 +0000

    KVM: PPC: debug stub interface parameter defined
    
    This patch defines the interface parameter for KVM_SET_GUEST_DEBUG
    ioctl support. Follow up patches will use this for setting up
    hardware breakpoints, watchpoints and software breakpoints.
    
    Also kvm_arch_vcpu_ioctl_set_guest_debug() is brought one level below.
    This is because I am not sure what is required for book3s. So this ioctl
    behaviour will not change for book3s.
    
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a822659db50a..6b8108624851 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -531,12 +531,6 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 #endif
 }
 
-int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
-                                        struct kvm_guest_debug *dbg)
-{
-	return -EINVAL;
-}
-
 static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,
                                      struct kvm_run *run)
 {

commit be28a27c993ca6f806145d02dbe493baac83a8e9
Author: Scott Wood <scottwood@freescale.com>
Date:   Mon Apr 15 15:07:11 2013 +0000

    kvm/ppc: don't call complete_mmio_load when it's a store
    
    complete_mmio_load writes back the mmio result into the
    destination register.  Doing this on a store results in
    register corruption.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 16b45954511c..a822659db50a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -683,7 +683,6 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 
 	if (!kvm_io_bus_write(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
 			      bytes, &run->mmio.data)) {
-		kvmppc_complete_mmio_load(vcpu, run);
 		vcpu->mmio_needed = 0;
 		return EMULATE_DONE;
 	}

commit 4fe27d2addda8af7714546a69369fb92dddcf9a3
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Feb 14 14:00:25 2013 +0000

    KVM: PPC: Remove unused argument to kvmppc_core_dequeue_external
    
    Currently kvmppc_core_dequeue_external() takes a struct kvm_interrupt *
    argument and does nothing with it, in any of its implementations.
    This removes it in order to make things easier for forthcoming
    in-kernel interrupt controller emulation code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7b5d4d20cdc5..16b45954511c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -739,7 +739,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 {
 	if (irq->irq == KVM_INTERRUPT_UNSET) {
-		kvmppc_core_dequeue_external(vcpu, irq);
+		kvmppc_core_dequeue_external(vcpu);
 		return 0;
 	}
 

commit 8482644aea11e0647867732319ccf35879a9acc2
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:45:25 2013 +0900

    KVM: set_memory_region: Refactor commit_memory_region()
    
    This patch makes the parameter old a const pointer to the old memory
    slot and adds a new parameter named change to know the change being
    requested: the former is for removing extra copying and the latter is
    for cleaning up the code.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8aa51cd67c28..7b5d4d20cdc5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -420,7 +420,8 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
 				   struct kvm_userspace_memory_region *mem,
-				   struct kvm_memory_slot old)
+				   const struct kvm_memory_slot *old,
+				   enum kvm_mr_change change)
 {
 	kvmppc_core_commit_memory_region(kvm, mem, old);
 }

commit 7b6195a91d60909a2834ab7181e2b9476e6fe749
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:44:34 2013 +0900

    KVM: set_memory_region: Refactor prepare_memory_region()
    
    This patch drops the parameter old, a copy of the old memory slot, and
    adds a new parameter named change to know the change being requested.
    
    This not only cleans up the code but also removes extra copying of the
    memory slot structure.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 22b33159fbc4..8aa51cd67c28 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -412,8 +412,8 @@ int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
 				   struct kvm_memory_slot *memslot,
-				   struct kvm_memory_slot old,
-				   struct kvm_userspace_memory_region *mem)
+				   struct kvm_userspace_memory_region *mem,
+				   enum kvm_mr_change change)
 {
 	return kvmppc_core_prepare_memory_region(kvm, memslot, mem);
 }

commit 462fce46065ec4b200c08619c047b9e5a8fd154a
Author: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
Date:   Wed Feb 27 19:41:56 2013 +0900

    KVM: set_memory_region: Drop user_alloc from prepare/commit_memory_region()
    
    X86 does not use this any more.  The remaining user, s390's !user_alloc
    check, can be simply removed since KVM_SET_MEMORY_REGION ioctl is no
    longer supported.
    
    Note: fixed powerpc's indentations with spaces to suppress checkpatch
    errors.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa_takuya_b1@lab.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 934413cd3a1b..22b33159fbc4 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -411,18 +411,16 @@ int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 }
 
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
-                                   struct kvm_memory_slot *memslot,
-                                   struct kvm_memory_slot old,
-                                   struct kvm_userspace_memory_region *mem,
-                                   bool user_alloc)
+				   struct kvm_memory_slot *memslot,
+				   struct kvm_memory_slot old,
+				   struct kvm_userspace_memory_region *mem)
 {
 	return kvmppc_core_prepare_memory_region(kvm, memslot, mem);
 }
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
-               struct kvm_userspace_memory_region *mem,
-               struct kvm_memory_slot old,
-               bool user_alloc)
+				   struct kvm_userspace_memory_region *mem,
+				   struct kvm_memory_slot old)
 {
 	kvmppc_core_commit_memory_region(kvm, mem, old);
 }

commit 1c810636556c8d53a37406b34a64d9b9b0161aa6
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 4 18:12:48 2013 +0100

    KVM: PPC: BookE: Implement EPR exit
    
    The External Proxy Facility in FSL BookE chips allows the interrupt
    controller to automatically acknowledge an interrupt as soon as a
    core gets its pending external interrupt delivered.
    
    Today, user space implements the interrupt controller, so we need to
    check on it during such a cycle.
    
    This patch implements logic for user space to enable EPR exiting,
    disable EPR exiting and EPR exiting itself, so that user space can
    acknowledge an interrupt when an external interrupt has successfully
    been delivered into the guest vcpu.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e2225e5b8a4c..934413cd3a1b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -306,6 +306,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #ifdef CONFIG_BOOKE
 	case KVM_CAP_PPC_BOOKE_SREGS:
 	case KVM_CAP_PPC_BOOKE_WATCHDOG:
+	case KVM_CAP_PPC_EPR:
 #else
 	case KVM_CAP_PPC_SEGSTATE:
 	case KVM_CAP_PPC_HIOR:
@@ -721,6 +722,11 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		for (i = 0; i < 9; ++i)
 			kvmppc_set_gpr(vcpu, 4 + i, run->papr_hcall.args[i]);
 		vcpu->arch.hcall_needed = 0;
+#ifdef CONFIG_BOOKE
+	} else if (vcpu->arch.epr_needed) {
+		kvmppc_set_epr(vcpu, run->epr.epr);
+		vcpu->arch.epr_needed = 0;
+#endif
 	}
 
 	r = kvmppc_vcpu_run(run, vcpu);
@@ -762,6 +768,10 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = 0;
 		vcpu->arch.papr_enabled = true;
 		break;
+	case KVM_CAP_PPC_EPR:
+		r = 0;
+		vcpu->arch.epr_enabled = cap->args[0];
+		break;
 #ifdef CONFIG_BOOKE
 	case KVM_CAP_PPC_BOOKE_WATCHDOG:
 		r = 0;

commit 5a33169ed29060df71627103e6968078b42de945
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Dec 14 23:46:03 2012 +0100

    KVM: PPC: Only WARN on invalid emulation
    
    When we hit an emulation result that we didn't expect, that is an error,
    but it's nothing that warrants a BUG(), because it can be guest triggered.
    
    So instead, let's only WARN() the user that this happened.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index be83fca2e8fd..e2225e5b8a4c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -237,7 +237,8 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 		r = RESUME_HOST;
 		break;
 	default:
-		BUG();
+		WARN_ON(1);
+		r = RESUME_GUEST;
 	}
 
 	return r;

commit f82a8cfe9354f5cdea55ebeceba3fd19051d3ee8
Author: Alex Williamson <alex.williamson@redhat.com>
Date:   Mon Dec 10 10:33:21 2012 -0700

    KVM: struct kvm_memory_slot.user_alloc -> bool
    
    There's no need for this to be an int, it holds a boolean.
    Move to the end of the struct for alignment.
    
    Reviewed-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Alex Williamson <alex.williamson@redhat.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 70739a089560..be83fca2e8fd 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -412,7 +412,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
                                    struct kvm_memory_slot *memslot,
                                    struct kvm_memory_slot old,
                                    struct kvm_userspace_memory_region *mem,
-                                   int user_alloc)
+                                   bool user_alloc)
 {
 	return kvmppc_core_prepare_memory_region(kvm, memslot, mem);
 }
@@ -420,7 +420,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
 void kvm_arch_commit_memory_region(struct kvm *kvm,
                struct kvm_userspace_memory_region *mem,
                struct kvm_memory_slot old,
-               int user_alloc)
+               bool user_alloc)
 {
 	kvmppc_core_commit_memory_region(kvm, mem, old);
 }

commit a2932923ccf63c419c77aaa18ac09be98f2c94d8
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Nov 19 22:57:20 2012 +0000

    KVM: PPC: Book3S HV: Provide a method for userspace to read and write the HPT
    
    A new ioctl, KVM_PPC_GET_HTAB_FD, returns a file descriptor.  Reads on
    this fd return the contents of the HPT (hashed page table), writes
    create and/or remove entries in the HPT.  There is a new capability,
    KVM_CAP_PPC_HTAB_FD, to indicate the presence of the ioctl.  The ioctl
    takes an argument structure with the index of the first HPT entry to
    read out and a set of flags.  The flags indicate whether the user is
    intending to read or write the HPT, and whether to return all entries
    or only the "bolted" entries (those with the bolted bit, 0x10, set in
    the first doubleword).
    
    This is intended for use in implementing qemu's savevm/loadvm and for
    live migration.  Therefore, on reads, the first pass returns information
    about all HPTEs (or all bolted HPTEs).  When the first pass reaches the
    end of the HPT, it returns from the read.  Subsequent reads only return
    information about HPTEs that have changed since they were last read.
    A read that finds no changed HPTEs in the HPT following where the last
    read finished will return 0 bytes.
    
    The format of the data provides a simple run-length compression of the
    invalid entries.  Each block of data starts with a header that indicates
    the index (position in the HPT, which is just an array), the number of
    valid entries starting at that index (may be zero), and the number of
    invalid entries following those valid entries.  The valid entries, 16
    bytes each, follow the header.  The invalid entries are not explicitly
    represented.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix documentation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index d583ea15e151..70739a089560 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -354,6 +354,12 @@ int kvm_dev_ioctl_check_extension(long ext)
 		r = 1;
 #else
 		r = 0;
+		break;
+#endif
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	case KVM_CAP_PPC_HTAB_FD:
+		r = 1;
+		break;
 #endif
 		break;
 	case KVM_CAP_NR_VCPUS:
@@ -954,6 +960,17 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = 0;
 		break;
 	}
+
+	case KVM_PPC_GET_HTAB_FD: {
+		struct kvm *kvm = filp->private_data;
+		struct kvm_get_htab_fd ghf;
+
+		r = -EFAULT;
+		if (copy_from_user(&ghf, argp, sizeof(ghf)))
+			break;
+		r = kvm_vm_ioctl_get_htab_fd(kvm, &ghf);
+		break;
+	}
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit 0e673fb679027600cad45bd61a4cc9ebd2ed2bb1
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Oct 9 00:06:20 2012 +0200

    KVM: PPC: Support eventfd
    
    In order to support the generic eventfd infrastructure on PPC, we need
    to call into the generic KVM in-kernel device mmio code.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index f9ab12aea829..d583ea15e151 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -314,6 +314,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_IRQ_LEVEL:
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_ONE_REG:
+	case KVM_CAP_IOEVENTFD:
 		r = 1;
 		break;
 #ifndef CONFIG_KVM_BOOK3S_64_HV
@@ -618,6 +619,13 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	vcpu->mmio_is_write = 0;
 	vcpu->arch.mmio_sign_extend = 0;
 
+	if (!kvm_io_bus_read(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
+			     bytes, &run->mmio.data)) {
+		kvmppc_complete_mmio_load(vcpu, run);
+		vcpu->mmio_needed = 0;
+		return EMULATE_DONE;
+	}
+
 	return EMULATE_DO_MMIO;
 }
 
@@ -627,8 +635,8 @@ int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
 {
 	int r;
 
-	r = kvmppc_handle_load(run, vcpu, rt, bytes, is_bigendian);
 	vcpu->arch.mmio_sign_extend = 1;
+	r = kvmppc_handle_load(run, vcpu, rt, bytes, is_bigendian);
 
 	return r;
 }
@@ -666,6 +674,13 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 		}
 	}
 
+	if (!kvm_io_bus_write(vcpu->kvm, KVM_MMIO_BUS, run->mmio.phys_addr,
+			      bytes, &run->mmio.data)) {
+		kvmppc_complete_mmio_load(vcpu, run);
+		vcpu->mmio_needed = 0;
+		return EMULATE_DONE;
+	}
+
 	return EMULATE_DO_MMIO;
 }
 

commit 42897d866b120547777ae1fd316680ec53356d9c
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Nov 27 23:29:02 2012 -0200

    KVM: x86: add kvm_arch_vcpu_postcreate callback, move TSC initialization
    
    TSC initialization will soon make use of online_vcpus.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index deb0d596d815..f9ab12aea829 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -439,6 +439,11 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 	return vcpu;
 }
 
+int kvm_arch_vcpu_postcreate(struct kvm_vcpu *vcpu)
+{
+	return 0;
+}
+
 void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 {
 	/* Make sure we're not using the vcpu anymore */

commit 5bd1cf118533aba41b3fbd4834e6362a9237db71
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Aug 22 15:03:50 2012 +0000

    KVM: PPC: set IN_GUEST_MODE before checking requests
    
    Avoid a race as described in the code comment.
    
    Also remove a related smp_wmb() from booke's kvmppc_prepare_to_enter().
    I can't see any reason for it, and the book3s_pr version doesn't have it.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6002ea938a48..deb0d596d815 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -78,7 +78,16 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			break;
 		}
 
+		vcpu->mode = IN_GUEST_MODE;
+
+		/*
+		 * Reading vcpu->requests must happen after setting vcpu->mode,
+		 * so we don't miss a request because the requester sees
+		 * OUTSIDE_GUEST_MODE and assumes we'll be checking requests
+		 * before next entering the guest (and thus doesn't IPI).
+		 */
 		smp_mb();
+
 		if (vcpu->requests) {
 			/* Make sure we process requests preemptable */
 			local_irq_enable();
@@ -111,11 +120,6 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 #endif
 
 		kvm_guest_enter();
-
-		/* Going into guest context! Yay! */
-		vcpu->mode = IN_GUEST_MODE;
-		smp_wmb();
-
 		break;
 	}
 

commit a47d72f3613d5edfd8e752c9b804d7df35810649
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Sep 20 19:35:51 2012 +0000

    KVM: PPC: Book3S HV: Fix updates of vcpu->cpu
    
    This removes the powerpc "generic" updates of vcpu->cpu in load and
    put, and moves them to the various backends.
    
    The reason is that "HV" KVM does its own sauce with that field
    and the generic updates might corrupt it. The field contains the
    CPU# of the -first- HW CPU of the core always for all the VCPU
    threads of a core (the one that's online from a host Linux
    perspective).
    
    However, the preempt notifiers are going to be called on the
    threads VCPUs when they are running (due to them sleeping on our
    private waitqueue) causing unload to be called, potentially
    clobbering the value.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8443e23f3605..6002ea938a48 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -504,7 +504,6 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	mtspr(SPRN_VRSAVE, vcpu->arch.vrsave);
 #endif
 	kvmppc_core_vcpu_load(vcpu, cpu);
-	vcpu->cpu = smp_processor_id();
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
@@ -513,7 +512,6 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 #ifdef CONFIG_BOOKE
 	vcpu->arch.vrsave = mfspr(SPRN_VRSAVE);
 #endif
-	vcpu->cpu = -1;
 }
 
 int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,

commit dfe49dbd1fc7310a4e0e2f83ae737cd7d34fa0cd
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 11 13:28:18 2012 +0000

    KVM: PPC: Book3S HV: Handle memory slot deletion and modification correctly
    
    This adds an implementation of kvm_arch_flush_shadow_memslot for
    Book3S HV, and arranges for kvmppc_core_commit_memory_region to
    flush the dirty log when modifying an existing slot.  With this,
    we can handle deletion and modification of memory slots.
    
    kvm_arch_flush_shadow_memslot calls kvmppc_core_flush_memslot, which
    on Book3S HV now traverses the reverse map chains to remove any HPT
    (hashed page table) entries referring to pages in the memslot.  This
    gets called by generic code whenever deleting a memslot or changing
    the guest physical address for a memslot.
    
    We flush the dirty log in kvmppc_core_commit_memory_region for
    consistency with what x86 does.  We only need to flush when an
    existing memslot is being modified, because for a new memslot the
    rmap array (which stores the dirty bits) is all zero, meaning that
    every page is considered clean already, and when deleting a memslot
    we obviously don't care about the dirty bits any more.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 33122dd89da9..8443e23f3605 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -411,7 +411,7 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
                struct kvm_memory_slot old,
                int user_alloc)
 {
-	kvmppc_core_commit_memory_region(kvm, mem);
+	kvmppc_core_commit_memory_region(kvm, mem, old);
 }
 
 void kvm_arch_flush_shadow_all(struct kvm *kvm)
@@ -421,6 +421,7 @@ void kvm_arch_flush_shadow_all(struct kvm *kvm)
 void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
 				   struct kvm_memory_slot *slot)
 {
+	kvmppc_core_flush_memslot(kvm, slot);
 }
 
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)

commit a66b48c3a39fa1c4223d4f847fdc7a04ed1618de
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Sep 11 13:27:46 2012 +0000

    KVM: PPC: Move kvm->arch.slot_phys into memslot.arch
    
    Now that we have an architecture-specific field in the kvm_memory_slot
    structure, we can use it to store the array of page physical addresses
    that we need for Book3S HV KVM on PPC970 processors.  This reduces the
    size of struct kvm_arch for Book3S HV, and also reduces the size of
    struct kvm_arch_memory_slot for other PPC KVM variants since the fields
    in it are now only compiled in for Book3S HV.
    
    This necessitates making the kvm_arch_create_memslot and
    kvm_arch_free_memslot operations specific to each PPC KVM variant.
    That in turn means that we now don't allocate the rmap arrays on
    Book3S PR and Book E.
    
    Since we now unpin pages and free the slot_phys array in
    kvmppc_core_free_memslot, we no longer need to do it in
    kvmppc_core_destroy_vm, since the generic code takes care to free
    all the memslots when destroying a VM.
    
    We now need the new memslot to be passed in to
    kvmppc_core_prepare_memory_region, since we need to initialize its
    arch.slot_phys member on Book3S HV.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0ffd7d17adc7..33122dd89da9 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -389,19 +389,12 @@ long kvm_arch_dev_ioctl(struct file *filp,
 void kvm_arch_free_memslot(struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont)
 {
-	if (!dont || free->arch.rmap != dont->arch.rmap) {
-		vfree(free->arch.rmap);
-		free->arch.rmap = NULL;
-	}
+	kvmppc_core_free_memslot(free, dont);
 }
 
 int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 {
-	slot->arch.rmap = vzalloc(npages * sizeof(*slot->arch.rmap));
-	if (!slot->arch.rmap)
-		return -ENOMEM;
-
-	return 0;
+	return kvmppc_core_create_memslot(slot, npages);
 }
 
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
@@ -410,7 +403,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
                                    struct kvm_userspace_memory_region *mem,
                                    int user_alloc)
 {
-	return kvmppc_core_prepare_memory_region(kvm, mem);
+	return kvmppc_core_prepare_memory_region(kvm, memslot, mem);
 }
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,

commit f61c94bb99ca4253ac5dd57750e1af209a4beb7a
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Wed Aug 8 20:38:19 2012 +0000

    KVM: PPC: booke: Add watchdog emulation
    
    This patch adds the watchdog emulation in KVM. The watchdog
    emulation is enabled by KVM_ENABLE_CAP(KVM_CAP_PPC_BOOKE_WATCHDOG) ioctl.
    The kernel timer are used for watchdog emulation and emulates
    h/w watchdog state machine. On watchdog timer expiry, it exit to QEMU
    if TCR.WRC is non ZERO. QEMU can reset/shutdown etc depending upon how
    it is configured.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [bharat.bhushan@freescale.com: reworked patch]
    Signed-off-by: Bharat Bhushan <bharat.bhushan@freescale.com>
    [agraf: adjust to new request framework]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 54b12af577d0..0ffd7d17adc7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -300,6 +300,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	switch (ext) {
 #ifdef CONFIG_BOOKE
 	case KVM_CAP_PPC_BOOKE_SREGS:
+	case KVM_CAP_PPC_BOOKE_WATCHDOG:
 #else
 	case KVM_CAP_PPC_SEGSTATE:
 	case KVM_CAP_PPC_HIOR:
@@ -476,6 +477,8 @@ enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)
 
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
+	int ret;
+
 	hrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 	tasklet_init(&vcpu->arch.tasklet, kvmppc_decrementer_func, (ulong)vcpu);
 	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
@@ -484,13 +487,14 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 #ifdef CONFIG_KVM_EXIT_TIMING
 	mutex_init(&vcpu->arch.exit_timing_lock);
 #endif
-
-	return 0;
+	ret = kvmppc_subarch_vcpu_init(vcpu);
+	return ret;
 }
 
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 {
 	kvmppc_mmu_destroy(vcpu);
+	kvmppc_subarch_vcpu_uninit(vcpu);
 }
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
@@ -735,6 +739,12 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = 0;
 		vcpu->arch.papr_enabled = true;
 		break;
+#ifdef CONFIG_BOOKE
+	case KVM_CAP_PPC_BOOKE_WATCHDOG:
+		r = 0;
+		vcpu->arch.watchdog_enabled = true;
+		break;
+#endif
 #if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB: {
 		struct kvm_config_tlb cfg;

commit 7c973a2ebb8fb9c8ee2ae9647f9ad7b0ad58a3e6
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 13 12:50:35 2012 +0200

    KVM: PPC: Add return value to core_check_requests
    
    Requests may want to tell us that we need to go back into host state,
    so add a return value for the checks.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0e2a98ab6a77..54b12af577d0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -83,9 +83,11 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			/* Make sure we process requests preemptable */
 			local_irq_enable();
 			trace_kvm_check_requests(vcpu);
-			kvmppc_core_check_requests(vcpu);
+			r = kvmppc_core_check_requests(vcpu);
 			local_irq_disable();
-			continue;
+			if (r > 0)
+				continue;
+			break;
 		}
 
 		if (kvmppc_core_prepare_to_enter(vcpu)) {

commit 7ee788556bf395a8ef413bea33494df29a3409e0
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 13 12:44:41 2012 +0200

    KVM: PPC: Add return value in prepare_to_enter
    
    Our prepare_to_enter helper wants to be able to return in more circumstances
    to the host than only when an interrupt is pending. Broaden the interface a
    bit and move even more generic code to the generic helper.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index dc86371b9953..0e2a98ab6a77 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -53,11 +53,14 @@ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
  * Common checks before entering the guest world.  Call with interrupts
  * disabled.
  *
- * returns !0 if a signal is pending and check_signal is true
+ * returns:
+ *
+ * == 1 if we're ready to go into guest state
+ * <= 0 if we need to go back to the host with return value
  */
 int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 {
-	int r = 0;
+	int r = 1;
 
 	WARN_ON_ONCE(!irqs_disabled());
 	while (true) {
@@ -69,7 +72,9 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 		}
 
 		if (signal_pending(current)) {
-			r = 1;
+			kvmppc_account_exit(vcpu, SIGNAL_EXITS);
+			vcpu->run->exit_reason = KVM_EXIT_INTR;
+			r = -EINTR;
 			break;
 		}
 

commit 206c2ed7f1ea55222bde2954ee3d65c2e9cfb750
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 13 12:43:33 2012 +0200

    KVM: PPC: Ignore EXITING_GUEST_MODE mode
    
    We don't need to do anything when mode is EXITING_GUEST_MODE, because
    we essentially are outside of guest mode and did everything it asked
    us to do by the time we check it.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6646574bf930..dc86371b9953 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -89,11 +89,6 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			continue;
 		}
 
-		if (vcpu->mode == EXITING_GUEST_MODE) {
-			r = 1;
-			break;
-		}
-
 #ifdef CONFIG_PPC64
 		/* lazy EE magic */
 		hard_irq_disable();

commit 3766a4c693358cff33441310413e3776dbbf8ef0
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 13 01:24:01 2012 +0200

    KVM: PPC: Move kvm_guest_enter call into generic code
    
    We need to call kvm_guest_enter in booke and book3s, so move its
    call to generic code.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 266549979e9f..6646574bf930 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -101,12 +101,15 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			/* Got an interrupt in between, try again */
 			local_irq_enable();
 			local_irq_disable();
+			kvm_guest_exit();
 			continue;
 		}
 
 		trace_hardirqs_on();
 #endif
 
+		kvm_guest_enter();
+
 		/* Going into guest context! Yay! */
 		vcpu->mode = IN_GUEST_MODE;
 		smp_wmb();

commit bd2be6836ee493d41fe42367a2b129aa771185c1
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 13 01:04:19 2012 +0200

    KVM: PPC: Book3S: PR: Rework irq disabling
    
    Today, we disable preemption while inside guest context, because we need
    to expose to the world that we are not in a preemptible context. However,
    during that time we already have interrupts disabled, which would indicate
    that we are in a non-preemptible context.
    
    The reason the checks for irqs_disabled() fail for us though is that we
    manually control hard IRQs and ignore all the lazy EE framework. Let's
    stop doing that. Instead, let's always use lazy EE to indicate when we
    want to disable IRQs, but do a special final switch that gets us into
    EE disabled, but soft enabled state. That way when we get back out of
    guest state, we are immediately ready to process interrupts.
    
    This simplifies the code drastically and reduces the time that we appear
    as preempt disabled.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 153a26abc915..266549979e9f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -30,6 +30,7 @@
 #include <asm/kvm_ppc.h>
 #include <asm/tlbflush.h>
 #include <asm/cputhreads.h>
+#include <asm/irqflags.h>
 #include "timing.h"
 #include "../mm/mmu_decl.h"
 
@@ -93,6 +94,19 @@ int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
 			break;
 		}
 
+#ifdef CONFIG_PPC64
+		/* lazy EE magic */
+		hard_irq_disable();
+		if (lazy_irq_pending()) {
+			/* Got an interrupt in between, try again */
+			local_irq_enable();
+			local_irq_disable();
+			continue;
+		}
+
+		trace_hardirqs_on();
+#endif
+
 		/* Going into guest context! Yay! */
 		vcpu->mode = IN_GUEST_MODE;
 		smp_wmb();

commit 03d25c5bd5c3125055bd36f4813ddb817def19dd
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Aug 10 12:28:50 2012 +0200

    KVM: PPC: Use same kvmppc_prepare_to_enter code for booke and book3s_pr
    
    We need to do the same things when preparing to enter a guest for booke and
    book3s_pr cores. Fold the generic code into a generic function that both call.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 45fe433316ea..153a26abc915 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -47,6 +47,63 @@ int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
 	return 1;
 }
 
+#ifndef CONFIG_KVM_BOOK3S_64_HV
+/*
+ * Common checks before entering the guest world.  Call with interrupts
+ * disabled.
+ *
+ * returns !0 if a signal is pending and check_signal is true
+ */
+int kvmppc_prepare_to_enter(struct kvm_vcpu *vcpu)
+{
+	int r = 0;
+
+	WARN_ON_ONCE(!irqs_disabled());
+	while (true) {
+		if (need_resched()) {
+			local_irq_enable();
+			cond_resched();
+			local_irq_disable();
+			continue;
+		}
+
+		if (signal_pending(current)) {
+			r = 1;
+			break;
+		}
+
+		smp_mb();
+		if (vcpu->requests) {
+			/* Make sure we process requests preemptable */
+			local_irq_enable();
+			trace_kvm_check_requests(vcpu);
+			kvmppc_core_check_requests(vcpu);
+			local_irq_disable();
+			continue;
+		}
+
+		if (kvmppc_core_prepare_to_enter(vcpu)) {
+			/* interrupts got enabled in between, so we
+			   are back at square 1 */
+			continue;
+		}
+
+		if (vcpu->mode == EXITING_GUEST_MODE) {
+			r = 1;
+			break;
+		}
+
+		/* Going into guest context! Yay! */
+		vcpu->mode = IN_GUEST_MODE;
+		smp_wmb();
+
+		break;
+	}
+
+	return r;
+}
+#endif /* CONFIG_KVM_BOOK3S_64_HV */
+
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 {
 	int nr = kvmppc_get_gpr(vcpu, 11);

commit f4800b1f4d23156e9080a08d6114e5d8bb767964
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Aug 7 10:24:14 2012 +0200

    KVM: PPC: Expose SYNC cap based on mmu notifiers
    
    Semantically, the "SYNC" cap means that we have mmu notifiers available.
    Express this in our #ifdef'ery around the feature, so that we can be sure
    we don't miss out on ppc targets when they get their implementation.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index dbf56e173c25..45fe433316ea 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -264,10 +264,16 @@ int kvm_dev_ioctl_check_extension(long ext)
 		if (cpu_has_feature(CPU_FTR_ARCH_201))
 			r = 2;
 		break;
+#endif
 	case KVM_CAP_SYNC_MMU:
+#ifdef CONFIG_KVM_BOOK3S_64_HV
 		r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
-		break;
+#elif defined(KVM_ARCH_WANT_MMU_NOTIFIER)
+		r = 1;
+#else
+		r = 0;
 #endif
+		break;
 	case KVM_CAP_NR_VCPUS:
 		/*
 		 * Recommending a number of CPUs is somewhat arbitrary; we

commit 9202e07636f0c4858ba6c30773a3f160b2b5659a
Author: Liu Yu-B13201 <Yu.Liu@freescale.com>
Date:   Tue Jul 3 05:48:52 2012 +0000

    KVM: PPC: Add support for ePAPR idle hcall in host kernel
    
    And add a new flag definition in kvm_ppc_pvinfo to indicate
    whether the host supports the EV_IDLE hcall.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [stuart.yoder@freescale.com: cleanup,fixes for conditions allowing idle]
    Signed-off-by: Stuart Yoder <stuart.yoder@freescale.com>
    [agraf: fix typo]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a478e662b2bc..dbf56e173c25 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -38,8 +38,7 @@
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
-	return !(v->arch.shared->msr & MSR_WE) ||
-	       !!(v->arch.pending_exceptions) ||
+	return !!(v->arch.pending_exceptions) ||
 	       v->requests;
 }
 
@@ -86,6 +85,11 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 
 		/* Second return value is in r4 */
 		break;
+	case EV_HCALL_TOKEN(EV_IDLE):
+		r = EV_SUCCESS;
+		kvm_vcpu_block(vcpu);
+		clear_bit(KVM_REQ_UNHALT, &vcpu->requests);
+		break;
 	default:
 		r = EV_UNIMPLEMENTED;
 		break;
@@ -779,6 +783,8 @@ static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 	pvinfo->hcall[3] = inst_nop;
 #endif
 
+	pvinfo->flags = KVM_PPC_PVINFO_FLAGS_EV_IDLE;
+
 	return 0;
 }
 

commit 784bafac79e7646e56f40998a6dde0e1ed5595f8
Author: Stuart Yoder <stuart.yoder@freescale.com>
Date:   Tue Jul 3 05:48:51 2012 +0000

    KVM: PPC: add pvinfo for hcall opcodes on e500mc/e5500
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [stuart: factored this out from idle hcall support in host patch]
    Signed-off-by: Stuart Yoder <stuart.yoder@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0368a9391b21..a478e662b2bc 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -751,9 +751,16 @@ int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
 
 static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 {
+	u32 inst_nop = 0x60000000;
+#ifdef CONFIG_KVM_BOOKE_HV
+	u32 inst_sc1 = 0x44000022;
+	pvinfo->hcall[0] = inst_sc1;
+	pvinfo->hcall[1] = inst_nop;
+	pvinfo->hcall[2] = inst_nop;
+	pvinfo->hcall[3] = inst_nop;
+#else
 	u32 inst_lis = 0x3c000000;
 	u32 inst_ori = 0x60000000;
-	u32 inst_nop = 0x60000000;
 	u32 inst_sc = 0x44000002;
 	u32 inst_imm_mask = 0xffff;
 
@@ -770,6 +777,7 @@ static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 	pvinfo->hcall[1] = inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask);
 	pvinfo->hcall[2] = inst_sc;
 	pvinfo->hcall[3] = inst_nop;
+#endif
 
 	return 0;
 }

commit fdcf8bd7e711d4c0fe3ef624cfb5e3808149ff7f
Author: Stuart Yoder <stuart.yoder@freescale.com>
Date:   Tue Jul 3 05:48:50 2012 +0000

    KVM: PPC: use definitions in epapr header for hcalls
    
    Signed-off-by: Stuart Yoder <stuart.yoder@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4d213b8b0fb5..0368a9391b21 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -67,18 +67,18 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	}
 
 	switch (nr) {
-	case HC_VENDOR_KVM | KVM_HC_PPC_MAP_MAGIC_PAGE:
+	case KVM_HCALL_TOKEN(KVM_HC_PPC_MAP_MAGIC_PAGE):
 	{
 		vcpu->arch.magic_page_pa = param1;
 		vcpu->arch.magic_page_ea = param2;
 
 		r2 = KVM_MAGIC_FEAT_SR | KVM_MAGIC_FEAT_MAS0_TO_SPRG7;
 
-		r = HC_EV_SUCCESS;
+		r = EV_SUCCESS;
 		break;
 	}
-	case HC_VENDOR_KVM | KVM_HC_FEATURES:
-		r = HC_EV_SUCCESS;
+	case KVM_HCALL_TOKEN(KVM_HC_FEATURES):
+		r = EV_SUCCESS;
 #if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500V2)
 		/* XXX Missing magic page on 44x */
 		r2 |= (1 << KVM_FEATURE_MAGIC_PAGE);
@@ -87,7 +87,7 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 		/* Second return value is in r4 */
 		break;
 	default:
-		r = HC_EV_UNIMPLEMENTED;
+		r = EV_UNIMPLEMENTED;
 		break;
 	}
 

commit 2df72e9bc4c505d8357012f2924589f3d16f9d44
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Fri Aug 24 15:54:57 2012 -0300

    KVM: split kvm_arch_flush_shadow
    
    Introducing kvm_arch_flush_shadow_memslot, to invalidate the
    translations of a single memory slot.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 879b14a61403..4d213b8b0fb5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -334,8 +334,12 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
 	kvmppc_core_commit_memory_region(kvm, mem);
 }
 
+void kvm_arch_flush_shadow_all(struct kvm *kvm)
+{
+}
 
-void kvm_arch_flush_shadow(struct kvm *kvm)
+void kvm_arch_flush_shadow_memslot(struct kvm *kvm,
+				   struct kvm_memory_slot *slot)
 {
 }
 

commit d89cc617b954aff4030fce178f7d86f59aaf713d
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Aug 1 18:03:28 2012 +0900

    KVM: Push rmap into kvm_arch_memory_slot
    
    Two reasons:
     - x86 can integrate rmap and rmap_pde and remove heuristics in
       __gfn_to_rmap().
     - Some architectures do not need rmap.
    
    Since rmap is one of the most memory consuming stuff in KVM, ppc'd
    better restrict the allocation to Book3S HV.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 87f4dc886076..879b14a61403 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -302,10 +302,18 @@ long kvm_arch_dev_ioctl(struct file *filp,
 void kvm_arch_free_memslot(struct kvm_memory_slot *free,
 			   struct kvm_memory_slot *dont)
 {
+	if (!dont || free->arch.rmap != dont->arch.rmap) {
+		vfree(free->arch.rmap);
+		free->arch.rmap = NULL;
+	}
 }
 
 int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
 {
+	slot->arch.rmap = vzalloc(npages * sizeof(*slot->arch.rmap));
+	if (!slot->arch.rmap)
+		return -ENOMEM;
+
 	return 0;
 }
 

commit 32fad281c0680ed0ccade7dda85a2121cf9b1d06
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri May 4 02:32:53 2012 +0000

    KVM: PPC: Book3S HV: Make the guest hash table size configurable
    
    This adds a new ioctl to enable userspace to control the size of the guest
    hashed page table (HPT) and to clear it out when resetting the guest.
    The KVM_PPC_ALLOCATE_HTAB ioctl is a VM ioctl and takes as its parameter
    a pointer to a u32 containing the desired order of the HPT (log base 2
    of the size in bytes), which is updated on successful return to the
    actual order of the HPT which was allocated.
    
    There must be no vcpus running at the time of this ioctl.  To enforce
    this, we now keep a count of the number of vcpus running in
    kvm->arch.vcpus_running.
    
    If the ioctl is called when a HPT has already been allocated, we don't
    reallocate the HPT but just clear it out.  We first clear the
    kvm->arch.rma_setup_done flag, which has two effects: (a) since we hold
    the kvm->lock mutex, it will prevent any vcpus from starting to run until
    we're done, and (b) it means that the first vcpu to run after we're done
    will re-establish the VRMA if necessary.
    
    If userspace doesn't call this ioctl before running the first vcpu, the
    kernel will allocate a default-sized HPT at that point.  We do it then
    rather than when creating the VM, as the code did previously, so that
    userspace has a chance to do the ioctl if it wants.
    
    When allocating the HPT, we can allocate either from the kernel page
    allocator, or from the preallocated pool.  If userspace is asking for
    a different size from the preallocated HPTs, we first try to allocate
    using the kernel page allocator.  Then we try to allocate from the
    preallocated pool, and then if that fails, we try allocating decreasing
    sizes from the kernel page allocator, down to the minimum size allowed
    (256kB).  Note that the kernel page allocator limits allocations to
    1 << CONFIG_FORCE_MAX_ZONEORDER pages, which by default corresponds to
    16MB (on 64-bit powerpc, at least).
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix module compilation]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1493c8de947b..87f4dc886076 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -246,6 +246,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #endif
 #ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
+	case KVM_CAP_PPC_ALLOC_HTAB:
 		r = 1;
 		break;
 #endif /* CONFIG_PPC_BOOK3S_64 */
@@ -802,6 +803,23 @@ long kvm_arch_vm_ioctl(struct file *filp,
 			r = -EFAULT;
 		break;
 	}
+
+	case KVM_PPC_ALLOCATE_HTAB: {
+		struct kvm *kvm = filp->private_data;
+		u32 htab_order;
+
+		r = -EFAULT;
+		if (get_user(htab_order, (u32 __user *)argp))
+			break;
+		r = kvmppc_alloc_reset_hpt(kvm, &htab_order);
+		if (r)
+			break;
+		r = -EFAULT;
+		if (put_user(htab_order, (u32 __user *)argp))
+			break;
+		r = 0;
+		break;
+	}
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 
 #ifdef CONFIG_PPC_BOOK3S_64

commit 5b74716ebab10e7bce960d148fe6d8f6920451e5
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Apr 26 19:43:42 2012 +0000

    kvm/powerpc: Add new ioctl to retreive server MMU infos
    
    This is necessary for qemu to be able to pass the right information
    to the guest, such as the supported page sizes and corresponding
    encodings in the SLB and hash table, which can vary depending
    on the processor type, the type of KVM used (PR vs HV) and the
    version of KVM
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [agraf: fix compilation on hv, adjust for newer ioctl numbers]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6ac31154d170..1493c8de947b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -279,6 +279,11 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_MAX_VCPUS:
 		r = KVM_MAX_VCPUS;
 		break;
+#ifdef CONFIG_PPC_BOOK3S_64
+	case KVM_CAP_PPC_GET_SMMU_INFO:
+		r = 1;
+		break;
+#endif
 	default:
 		r = 0;
 		break;
@@ -718,7 +723,6 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		break;
 	}
 #endif
-
 	default:
 		r = -EINVAL;
 	}
@@ -800,6 +804,18 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	}
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 
+#ifdef CONFIG_PPC_BOOK3S_64
+	case KVM_PPC_GET_SMMU_INFO: {
+		struct kvm *kvm = filp->private_data;
+		struct kvm_ppc_smmu_info info;
+
+		memset(&info, 0, sizeof(info));
+		r = kvm_vm_ioctl_get_smmu_info(kvm, &info);
+		if (r >= 0 && copy_to_user(argp, &info, sizeof(info)))
+			r = -EFAULT;
+		break;
+	}
+#endif /* CONFIG_PPC_BOOK3S_64 */
 	default:
 		r = -ENOTTY;
 	}

commit f31e65e1170edba4a86bd8cba0318e251d3746d0
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Mar 15 21:58:34 2012 +0000

    kvm/book3s: Make kernel emulated H_PUT_TCE available for "PR" KVM
    
    There is nothing in the code for emulating TCE tables in the kernel
    that prevents it from working on "PR" KVM... other than ifdef's and
    location of the code.
    
    This and moves the bulk of the code there to a new file called
    book3s_64_vio.c.
    
    This speeds things up a bit on my G5.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    [agraf: fix for hv kvm, 32bit, whitespace]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 58ad8609bb43..6ac31154d170 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -244,10 +244,12 @@ int kvm_dev_ioctl_check_extension(long ext)
 		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 		break;
 #endif
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CAP_SPAPR_TCE:
 		r = 1;
 		break;
+#endif /* CONFIG_PPC_BOOK3S_64 */
+#ifdef CONFIG_KVM_BOOK3S_64_HV
 	case KVM_CAP_PPC_SMT:
 		r = threads_per_core;
 		break;
@@ -773,7 +775,7 @@ long kvm_arch_vm_ioctl(struct file *filp,
 
 		break;
 	}
-#ifdef CONFIG_KVM_BOOK3S_64_HV
+#ifdef CONFIG_PPC_BOOK3S_64
 	case KVM_CREATE_SPAPR_TCE: {
 		struct kvm_create_spapr_tce create_tce;
 		struct kvm *kvm = filp->private_data;
@@ -784,7 +786,9 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);
 		goto out;
 	}
+#endif /* CONFIG_PPC_BOOK3S_64 */
 
+#ifdef CONFIG_KVM_BOOK3S_64_HV
 	case KVM_ALLOCATE_RMA: {
 		struct kvm *kvm = filp->private_data;
 		struct kvm_allocate_rma rma;

commit bf7ca4bdcb8f1eb15d11879efa824b45443ddb69
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Feb 15 23:40:00 2012 +0000

    KVM: PPC: rename CONFIG_KVM_E500 -> CONFIG_KVM_E500V2
    
    The CONFIG_KVM_E500 option really indicates that we're running on a V2 machine,
    not on a machine of the generic E500 class. So indicate that properly and
    change the config name accordingly.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 14bf8d59bc2a..58ad8609bb43 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -79,7 +79,7 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	}
 	case HC_VENDOR_KVM | KVM_HC_FEATURES:
 		r = HC_EV_SUCCESS;
-#if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500)
+#if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500V2)
 		/* XXX Missing magic page on 44x */
 		r2 |= (1 << KVM_FEATURE_MAGIC_PAGE);
 #endif
@@ -235,7 +235,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:
-#if defined(CONFIG_KVM_E500) || defined(CONFIG_KVM_E500MC)
+#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB:
 #endif
 		r = 1;
@@ -629,7 +629,7 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = 0;
 		vcpu->arch.papr_enabled = true;
 		break;
-#if defined(CONFIG_KVM_E500) || defined(CONFIG_KVM_E500MC)
+#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB: {
 		struct kvm_config_tlb cfg;
 		void __user *user_ptr = (void __user *)(uintptr_t)cap->args[0];
@@ -706,7 +706,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		break;
 	}
 
-#if defined(CONFIG_KVM_E500) || defined(CONFIG_KVM_E500MC)
+#if defined(CONFIG_KVM_E500V2) || defined(CONFIG_KVM_E500MC)
 	case KVM_DIRTY_TLB: {
 		struct kvm_dirty_tlb dirty;
 		r = -EFAULT;

commit 73196cd364a2d972d73fa08da9d81ca3215bed68
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:47 2011 +0000

    KVM: PPC: e500mc support
    
    Add processor support for e500mc, using hardware virtualization support
    (GS-mode).
    
    Current issues include:
     - No support for external proxy (coreint) interrupt mode in the guest.
    
    Includes work by Ashish Kalra <Ashish.Kalra@freescale.com>,
    Varun Sethi <Varun.Sethi@freescale.com>, and
    Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6a530e4b3e7c..14bf8d59bc2a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -235,7 +235,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:
-#ifdef CONFIG_KVM_E500
+#if defined(CONFIG_KVM_E500) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB:
 #endif
 		r = 1;
@@ -629,7 +629,7 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = 0;
 		vcpu->arch.papr_enabled = true;
 		break;
-#ifdef CONFIG_KVM_E500
+#if defined(CONFIG_KVM_E500) || defined(CONFIG_KVM_E500MC)
 	case KVM_CAP_SW_TLB: {
 		struct kvm_config_tlb cfg;
 		void __user *user_ptr = (void __user *)(uintptr_t)cap->args[0];
@@ -706,7 +706,7 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		break;
 	}
 
-#ifdef CONFIG_KVM_E500
+#if defined(CONFIG_KVM_E500) || defined(CONFIG_KVM_E500MC)
 	case KVM_DIRTY_TLB: {
 		struct kvm_dirty_tlb dirty;
 		r = -EFAULT;

commit d30f6e480055e5be12e7a03fd11ea912a451daa5
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:43 2011 +0000

    KVM: PPC: booke: category E.HV (GS-mode) support
    
    Chips such as e500mc that implement category E.HV in Power ISA 2.06
    provide hardware virtualization features, including a new MSR mode for
    guest state.  The guest OS can perform many operations without trapping
    into the hypervisor, including transitions to and from guest userspace.
    
    Since we can use SRR1[GS] to reliably tell whether an exception came from
    guest state, instead of messing around with IVPR, we use DO_KVM similarly
    to book3s.
    
    Current issues include:
     - Machine checks from guest state are not routed to the host handler.
     - The guest can cause a host oops by executing an emulated instruction
       in a page that lacks read permission.  Existing e500/4xx support has
       the same problem.
    
    Includes work by Ashish Kalra <Ashish.Kalra@freescale.com>,
    Varun Sethi <Varun.Sethi@freescale.com>, and
    Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    [agraf: remove pt_regs usage]
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index cd53e08403b3..6a530e4b3e7c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -114,6 +114,11 @@ int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
 		goto out;
 #endif
 
+#ifdef CONFIG_KVM_BOOKE_HV
+	if (!cpu_has_feature(CPU_FTR_EMB_HV))
+		goto out;
+#endif
+
 	r = true;
 
 out:

commit 043cc4d724da6bb9e4f417c735accec58dfa40bf
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Dec 20 15:34:20 2011 +0000

    KVM: PPC: factor out lpid allocator from book3s_64_mmu_hv
    
    We'll use it on e500mc as well.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b5e9046462fd..cd53e08403b3 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -799,6 +799,40 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	return r;
 }
 
+static unsigned long lpid_inuse[BITS_TO_LONGS(KVMPPC_NR_LPIDS)];
+static unsigned long nr_lpids;
+
+long kvmppc_alloc_lpid(void)
+{
+	long lpid;
+
+	do {
+		lpid = find_first_zero_bit(lpid_inuse, KVMPPC_NR_LPIDS);
+		if (lpid >= nr_lpids) {
+			pr_err("%s: No LPIDs free\n", __func__);
+			return -ENOMEM;
+		}
+	} while (test_and_set_bit(lpid, lpid_inuse));
+
+	return lpid;
+}
+
+void kvmppc_claim_lpid(long lpid)
+{
+	set_bit(lpid, lpid_inuse);
+}
+
+void kvmppc_free_lpid(long lpid)
+{
+	clear_bit(lpid, lpid_inuse);
+}
+
+void kvmppc_init_lpid(unsigned long nr_lpids_param)
+{
+	nr_lpids = min_t(unsigned long, KVMPPC_NR_LPIDS, nr_lpids_param);
+	memset(lpid_inuse, 0, sizeof(lpid_inuse));
+}
+
 int kvm_arch_init(void *opaque)
 {
 	return 0;

commit b6d33834bd4e8bdf4a199812e31b3e36da53c794
Author: Christoffer Dall <c.dall@virtualopensystems.com>
Date:   Thu Mar 8 16:44:24 2012 -0500

    KVM: Factor out kvm_vcpu_kick to arch-generic code
    
    The kvm_vcpu_kick function performs roughly the same funcitonality on
    most all architectures, so we shouldn't have separate copies.
    
    PowerPC keeps a pointer to interchanging waitqueues on the vcpu_arch
    structure and to accomodate this special need a
    __KVM_HAVE_ARCH_VCPU_GET_WQ define and accompanying function
    kvm_arch_vcpu_wq have been defined. For all other architectures this
    is a generic inline that just returns &vcpu->wq;
    
    Acked-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Christoffer Dall <c.dall@virtualopensystems.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 00d7e345b3fe..b5e9046462fd 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -43,6 +43,11 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 	       v->requests;
 }
 
+int kvm_arch_vcpu_should_kick(struct kvm_vcpu *vcpu)
+{
+	return 1;
+}
+
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 {
 	int nr = kvmppc_get_gpr(vcpu, 11);
@@ -588,21 +593,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return r;
 }
 
-void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
-{
-	int me;
-	int cpu = vcpu->cpu;
-
-	me = get_cpu();
-	if (waitqueue_active(vcpu->arch.wqp)) {
-		wake_up_interruptible(vcpu->arch.wqp);
-		vcpu->stat.halt_wakeup++;
-	} else if (cpu != me && cpu != -1) {
-		smp_send_reschedule(vcpu->cpu);
-	}
-	put_cpu();
-}
-
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 {
 	if (irq->irq == KVM_INTERRUPT_UNSET) {
@@ -611,6 +601,7 @@ int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 	}
 
 	kvmppc_core_queue_external(vcpu, irq);
+
 	kvm_vcpu_kick(vcpu);
 
 	return 0;

commit db3fe4eb45f3555d91a7124e18cf3a2f2a30eb90
Author: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
Date:   Wed Feb 8 13:02:18 2012 +0900

    KVM: Introduce kvm_memory_slot::arch and move lpage_info into it
    
    Some members of kvm_memory_slot are not used by every architecture.
    
    This patch is the first step to make this difference clear by
    introducing kvm_memory_slot::arch;  lpage_info is moved into it.
    
    Signed-off-by: Takuya Yoshikawa <yoshikawa.takuya@oss.ntt.co.jp>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0e21d155eea7..00d7e345b3fe 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -281,6 +281,16 @@ long kvm_arch_dev_ioctl(struct file *filp,
 	return -EINVAL;
 }
 
+void kvm_arch_free_memslot(struct kvm_memory_slot *free,
+			   struct kvm_memory_slot *dont)
+{
+}
+
+int kvm_arch_create_memslot(struct kvm_memory_slot *slot, unsigned long npages)
+{
+	return 0;
+}
+
 int kvm_arch_prepare_memory_region(struct kvm *kvm,
                                    struct kvm_memory_slot *memslot,
                                    struct kvm_memory_slot old,

commit b3c5d3c2a49602c370de6d02fdb923bc48cd1abc
Author: Alexander Graf <agraf@suse.de>
Date:   Sat Jan 7 02:07:38 2012 +0100

    KVM: PPC: Rename MMIO register identifiers
    
    We need the KVM_REG namespace for generic register settings now, so
    let's rename the existing users to something different, enabling
    us to reuse the namespace for more visible interfaces.
    
    While at it, also move these private constants to a private header.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e23270779ff5..0e21d155eea7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -448,20 +448,20 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 
 	kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
 
-	switch (vcpu->arch.io_gpr & KVM_REG_EXT_MASK) {
-	case KVM_REG_GPR:
+	switch (vcpu->arch.io_gpr & KVM_MMIO_REG_EXT_MASK) {
+	case KVM_MMIO_REG_GPR:
 		kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
 		break;
-	case KVM_REG_FPR:
-		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
+	case KVM_MMIO_REG_FPR:
+		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
 		break;
 #ifdef CONFIG_PPC_BOOK3S
-	case KVM_REG_QPR:
-		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
+	case KVM_MMIO_REG_QPR:
+		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
 		break;
-	case KVM_REG_FQPR:
-		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
-		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
+	case KVM_MMIO_REG_FQPR:
+		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
+		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_MMIO_REG_MASK] = gpr;
 		break;
 #endif
 	default:

commit 31f3438eca2fc90dc892e0e9963ba4b93a2c8383
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:26:50 2011 +0000

    KVM: PPC: Move kvm_vcpu_ioctl_[gs]et_one_reg down to platform-specific code
    
    This moves the get/set_one_reg implementation down from powerpc.c into
    booke.c, book3s_pr.c and book3s_hv.c.  This avoids #ifdefs in C code,
    but more importantly, it fixes a bug on Book3s HV where we were
    accessing beyond the end of the kvm_vcpu struct (via the to_book3s()
    macro) and corrupting memory, causing random crashes and file corruption.
    
    On Book3s HV we only accept setting the HIOR to zero, since the guest
    runs in supervisor mode and its vectors are never offset from zero.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    [agraf update to apply on top of changed ONE_REG patches]
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 59852091b389..e23270779ff5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -647,44 +647,6 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	return r;
 }
 
-static int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
-				      struct kvm_one_reg *reg)
-{
-	int r = -EINVAL;
-
-	switch (reg->id) {
-#ifdef CONFIG_PPC_BOOK3S
-	case KVM_REG_PPC_HIOR:
-		r = put_user(to_book3s(vcpu)->hior, (u64 __user *)reg->addr);
-		break;
-#endif
-	default:
-		break;
-	}
-
-	return r;
-}
-
-static int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
-				      struct kvm_one_reg *reg)
-{
-	int r = -EINVAL;
-
-	switch (reg->id) {
-#ifdef CONFIG_PPC_BOOK3S
-	case KVM_ONE_REG_PPC_HIOR:
-		r = get_user(to_book3s(vcpu)->hior, (u64 __user *)reg->addr);
-		if (!r)
-			to_book3s(vcpu)->hior_explicit = true;
-		break;
-#endif
-	default:
-		break;
-	}
-
-	return r;
-}
-
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
                                     struct kvm_mp_state *mp_state)
 {

commit 1022fc3d3bfaca09d5d6bfcc93a168de16840814
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Sep 14 21:45:23 2011 +0200

    KVM: PPC: Add support for explicit HIOR setting
    
    Until now, we always set HIOR based on the PVR, but this is just wrong.
    Instead, we should be setting HIOR explicitly, so user space can decide
    what the initial HIOR value is - just like on real hardware.
    
    We keep the old PVR based way around for backwards compatibility, but
    once user space uses the SET_ONE_REG based method, we drop the PVR logic.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 089c61bf0e16..59852091b389 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -212,6 +212,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_BOOKE_SREGS:
 #else
 	case KVM_CAP_PPC_SEGSTATE:
+	case KVM_CAP_PPC_HIOR:
 	case KVM_CAP_PPC_PAPR:
 #endif
 	case KVM_CAP_PPC_UNSET_IRQ:
@@ -652,6 +653,11 @@ static int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
 	int r = -EINVAL;
 
 	switch (reg->id) {
+#ifdef CONFIG_PPC_BOOK3S
+	case KVM_REG_PPC_HIOR:
+		r = put_user(to_book3s(vcpu)->hior, (u64 __user *)reg->addr);
+		break;
+#endif
 	default:
 		break;
 	}
@@ -665,6 +671,13 @@ static int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
 	int r = -EINVAL;
 
 	switch (reg->id) {
+#ifdef CONFIG_PPC_BOOK3S
+	case KVM_ONE_REG_PPC_HIOR:
+		r = get_user(to_book3s(vcpu)->hior, (u64 __user *)reg->addr);
+		if (!r)
+			to_book3s(vcpu)->hior_explicit = true;
+		break;
+#endif
 	default:
 		break;
 	}

commit e24ed81fedd551e80378be62fa0b0532480ea7d4
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Sep 14 10:02:41 2011 +0200

    KVM: PPC: Add generic single register ioctls
    
    Right now we transfer a static struct every time we want to get or set
    registers. Unfortunately, over time we realize that there are more of
    these than we thought of before and the extensibility and flexibility of
    transferring a full struct every time is limited.
    
    So this is a new approach to the problem. With these new ioctls, we can
    get and set a single register that is identified by an ID. This allows for
    very precise and limited transmittal of data. When we later realize that
    it's a better idea to shove over multiple registers at once, we can reuse
    most of the infrastructure and simply implement a GET_MANY_REGS / SET_MANY_REGS
    interface.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index f4380cb264e0..089c61bf0e16 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -217,6 +217,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_PPC_IRQ_LEVEL:
 	case KVM_CAP_ENABLE_CAP:
+	case KVM_CAP_ONE_REG:
 		r = 1;
 		break;
 #ifndef CONFIG_KVM_BOOK3S_64_HV
@@ -645,6 +646,32 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 	return r;
 }
 
+static int kvm_vcpu_ioctl_get_one_reg(struct kvm_vcpu *vcpu,
+				      struct kvm_one_reg *reg)
+{
+	int r = -EINVAL;
+
+	switch (reg->id) {
+	default:
+		break;
+	}
+
+	return r;
+}
+
+static int kvm_vcpu_ioctl_set_one_reg(struct kvm_vcpu *vcpu,
+				      struct kvm_one_reg *reg)
+{
+	int r = -EINVAL;
+
+	switch (reg->id) {
+	default:
+		break;
+	}
+
+	return r;
+}
+
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
                                     struct kvm_mp_state *mp_state)
 {
@@ -684,6 +711,20 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		break;
 	}
 
+	case KVM_SET_ONE_REG:
+	case KVM_GET_ONE_REG:
+	{
+		struct kvm_one_reg reg;
+		r = -EFAULT;
+		if (copy_from_user(&reg, argp, sizeof(reg)))
+			goto out;
+		if (ioctl == KVM_SET_ONE_REG)
+			r = kvm_vcpu_ioctl_set_one_reg(vcpu, &reg);
+		else
+			r = kvm_vcpu_ioctl_get_one_reg(vcpu, &reg);
+		break;
+	}
+
 #ifdef CONFIG_KVM_E500
 	case KVM_DIRTY_TLB: {
 		struct kvm_dirty_tlb dirty;

commit b5434032fcfd7490c6453feb397fb781762b6f09
Author: Matt Evans <matt@ozlabs.org>
Date:   Wed Dec 7 16:55:57 2011 +0000

    KVM: PPC: Add KVM_CAP_NR_VCPUS and KVM_CAP_MAX_VCPUS
    
    PPC KVM lacks these two capabilities, and as such a userland system must assume
    a max of 4 VCPUs (following api.txt).  With these, a userland can determine
    a more realistic limit.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 25171037a36c..f4380cb264e0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -249,6 +249,22 @@ int kvm_dev_ioctl_check_extension(long ext)
 		r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
 		break;
 #endif
+	case KVM_CAP_NR_VCPUS:
+		/*
+		 * Recommending a number of CPUs is somewhat arbitrary; we
+		 * return the number of present CPUs for -HV (since a host
+		 * will have secondary threads "offline"), and for other KVM
+		 * implementations just count online CPUs.
+		 */
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+		r = num_present_cpus();
+#else
+		r = num_online_cpus();
+#endif
+		break;
+	case KVM_CAP_MAX_VCPUS:
+		r = KVM_MAX_VCPUS;
+		break;
 	default:
 		r = 0;
 		break;

commit 03cdab5340b423ec88fc18eb158a62a8a7b94d7f
Author: Matt Evans <matt@ozlabs.org>
Date:   Tue Dec 6 21:19:42 2011 +0000

    KVM: PPC: Fix vcpu_create dereference before validity check.
    
    Fix usage of vcpu struct before check that it's actually valid.
    
    Signed-off-by: Matt Evans <matt@ozlabs.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 06e955b5837e..25171037a36c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -289,9 +289,10 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
 	struct kvm_vcpu *vcpu;
 	vcpu = kvmppc_core_vcpu_create(kvm, id);
-	vcpu->arch.wqp = &vcpu->wq;
-	if (!IS_ERR(vcpu))
+	if (!IS_ERR(vcpu)) {
+		vcpu->arch.wqp = &vcpu->wq;
 		kvmppc_create_vcpu_debugfs(vcpu, id);
+	}
 	return vcpu;
 }
 

commit 342d3db763f2621ed4546ebf8f6c61cb29d7fbdb
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:38:05 2011 +0000

    KVM: PPC: Implement MMU notifiers for Book3S HV guests
    
    This adds the infrastructure to enable us to page out pages underneath
    a Book3S HV guest, on processors that support virtualized partition
    memory, that is, POWER7.  Instead of pinning all the guest's pages,
    we now look in the host userspace Linux page tables to find the
    mapping for a given guest page.  Then, if the userspace Linux PTE
    gets invalidated, kvm_unmap_hva() gets called for that address, and
    we replace all the guest HPTEs that refer to that page with absent
    HPTEs, i.e. ones with the valid bit clear and the HPTE_V_ABSENT bit
    set, which will cause an HDSI when the guest tries to access them.
    Finally, the page fault handler is extended to reinstantiate the
    guest HPTE when the guest tries to access a page which has been paged
    out.
    
    Since we can't intercept the guest DSI and ISI interrupts on PPC970,
    we still have to pin all the guest pages on PPC970.  We have a new flag,
    kvm->arch.using_mmu_notifiers, that indicates whether we can page
    guest pages out.  If it is not set, the MMU notifier callbacks do
    nothing and everything operates as before.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4f85ac32258a..06e955b5837e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -245,6 +245,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 		if (cpu_has_feature(CPU_FTR_ARCH_201))
 			r = 2;
 		break;
+	case KVM_CAP_SYNC_MMU:
+		r = cpu_has_feature(CPU_FTR_ARCH_206) ? 1 : 0;
+		break;
 #endif
 	default:
 		r = 0;

commit 4e72dbe13528394a413889d73e5025dbdf6cab70
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Dec 12 12:24:48 2011 +0000

    KVM: PPC: Make wakeups work again for Book3S HV guests
    
    When commit f43fdc15fa ("KVM: PPC: booke: Improve timer register
    emulation") factored out some code in arch/powerpc/kvm/powerpc.c
    into a new helper function, kvm_vcpu_kick(), an error crept in
    which causes Book3s HV guest vcpus to stall.  This fixes it.
    On POWER7 machines, guest vcpus are grouped together into virtual
    CPU cores that share a single waitqueue, so it's important to use
    vcpu->arch.wqp rather than &vcpu->wq.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e1ef4d6d972a..4f85ac32258a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -562,7 +562,7 @@ void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 	int cpu = vcpu->cpu;
 
 	me = get_cpu();
-	if (waitqueue_active(&vcpu->wq)) {
+	if (waitqueue_active(vcpu->arch.wqp)) {
 		wake_up_interruptible(vcpu->arch.wqp);
 		vcpu->stat.halt_wakeup++;
 	} else if (cpu != me && cpu != -1) {

commit ae21216bece0a623d09980c120b9c98790a860b9
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Dec 9 15:20:46 2011 +0100

    KVM: PPC: align vcpu_kick with x86
    
    Our vcpu kick implementation differs a bit from x86 which resulted in us not
    disabling preemption during the kick. Get it a bit closer to what x86 does.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index fd8d3b16eaf3..e1ef4d6d972a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -558,12 +558,17 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
 {
+	int me;
+	int cpu = vcpu->cpu;
+
+	me = get_cpu();
 	if (waitqueue_active(&vcpu->wq)) {
 		wake_up_interruptible(vcpu->arch.wqp);
 		vcpu->stat.halt_wakeup++;
-	} else if (vcpu->cpu != -1) {
+	} else if (cpu != me && cpu != -1) {
 		smp_send_reschedule(vcpu->cpu);
 	}
+	put_cpu();
 }
 
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)

commit dfd4d47e9a71c5a35eb67a44cd311efbe1846b7e
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Nov 17 12:39:59 2011 +0000

    KVM: PPC: booke: Improve timer register emulation
    
    Decrementers are now properly driven by TCR/TSR, and the guest
    has full read/write access to these registers.
    
    The decrementer keeps ticking (and setting the TSR bit) regardless of
    whether the interrupts are enabled with TCR.
    
    The decrementer stops at zero, rather than going negative.
    
    Decrementers (and FITs, once implemented) are delivered as
    level-triggered interrupts -- dequeued when the TSR bit is cleared, not
    on delivery.
    
    Signed-off-by: Liu Yu <yu.liu@freescale.com>
    [scottwood@freescale.com: significant changes]
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index d02e4c84e213..fd8d3b16eaf3 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -39,7 +39,8 @@
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
 	return !(v->arch.shared->msr & MSR_WE) ||
-	       !!(v->arch.pending_exceptions);
+	       !!(v->arch.pending_exceptions) ||
+	       v->requests;
 }
 
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
@@ -311,18 +312,6 @@ int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 	return kvmppc_core_pending_dec(vcpu);
 }
 
-static void kvmppc_decrementer_func(unsigned long data)
-{
-	struct kvm_vcpu *vcpu = (struct kvm_vcpu *)data;
-
-	kvmppc_core_queue_dec(vcpu);
-
-	if (waitqueue_active(vcpu->arch.wqp)) {
-		wake_up_interruptible(vcpu->arch.wqp);
-		vcpu->stat.halt_wakeup++;
-	}
-}
-
 /*
  * low level hrtimer wake routine. Because this runs in hardirq context
  * we schedule a tasklet to do the real work.
@@ -567,6 +556,16 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	return r;
 }
 
+void kvm_vcpu_kick(struct kvm_vcpu *vcpu)
+{
+	if (waitqueue_active(&vcpu->wq)) {
+		wake_up_interruptible(vcpu->arch.wqp);
+		vcpu->stat.halt_wakeup++;
+	} else if (vcpu->cpu != -1) {
+		smp_send_reschedule(vcpu->cpu);
+	}
+}
+
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 {
 	if (irq->irq == KVM_INTERRUPT_UNSET) {
@@ -575,13 +574,7 @@ int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 	}
 
 	kvmppc_core_queue_external(vcpu, irq);
-
-	if (waitqueue_active(vcpu->arch.wqp)) {
-		wake_up_interruptible(vcpu->arch.wqp);
-		vcpu->stat.halt_wakeup++;
-	} else if (vcpu->cpu != -1) {
-		smp_send_reschedule(vcpu->cpu);
-	}
+	kvm_vcpu_kick(vcpu);
 
 	return 0;
 }

commit b59049720dd95021dfe0d9f4e1fa9458a67cfe29
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Nov 8 18:23:30 2011 -0600

    KVM: PPC: Paravirtualize SPRG4-7, ESR, PIR, MASn
    
    This allows additional registers to be accessed by the guest
    in PR-mode KVM without trapping.
    
    SPRG4-7 are readable from userspace.  On booke, KVM will sync
    these registers when it enters the guest, so that accesses from
    guest userspace will work.  The guest kernel, OTOH, must consistently
    use either the real registers or the shared area between exits.  This
    also applies to the already-paravirted SPRG3.
    
    On non-booke, it's not clear to what extent SPRG4-7 are supported
    (they're not architected for book3s, but exist on at least some classic
    chips).  They are copied in the get/set regs ioctls, but I do not see any
    non-booke emulation.  I also do not see any syncing with real registers
    (in PR-mode) including the user-readable SPRG3.  This patch should not
    make that situation any worse.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7411bdd8ff6f..d02e4c84e213 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -66,7 +66,7 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 		vcpu->arch.magic_page_pa = param1;
 		vcpu->arch.magic_page_ea = param2;
 
-		r2 = KVM_MAGIC_FEAT_SR;
+		r2 = KVM_MAGIC_FEAT_SR | KVM_MAGIC_FEAT_MAS0_TO_SPRG7;
 
 		r = HC_EV_SUCCESS;
 		break;

commit 25051b5a5aff0bb71435421b4b80279b789fa0dc
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Nov 8 18:23:23 2011 -0600

    KVM: PPC: Move prepare_to_enter call site into subarch code
    
    This function should be called with interrupts disabled, to avoid
    a race where an exception is delivered after we check, but the
    resched kick is received before we disable interrupts (and thus doesn't
    actually trigger the exit code that would recheck exceptions).
    
    booke already does this properly in the lightweight exit case, but
    not on initial entry.
    
    For now, move the call of prepare_to_enter into subarch-specific code so
    that booke can do the right thing here.  Ideally book3s would do the same
    thing, but I'm having a hard time seeing where it does any interrupt
    disabling of this sort (plus it has several additional call sites), so
    I'm deferring the book3s fix to someone more familiar with that code.
    book3s behavior should be unchanged by this patch.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6186ec0d939b..7411bdd8ff6f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -559,8 +559,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		vcpu->arch.hcall_needed = 0;
 	}
 
-	kvmppc_core_prepare_to_enter(vcpu);
-
 	r = kvmppc_vcpu_run(run, vcpu);
 
 	if (vcpu->sigset_active)

commit 7e28e60ef974d0eeb43112ef264d8c130f7b7bf4
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Nov 8 18:23:20 2011 -0600

    KVM: PPC: Rename deliver_interrupts to prepare_to_enter
    
    This function also updates paravirt int_pending, so rename it
    to be more obvious that this is a collection of checks run prior
    to (re)entering a guest.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3cf6fba513ac..6186ec0d939b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -559,7 +559,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		vcpu->arch.hcall_needed = 0;
 	}
 
-	kvmppc_core_deliver_interrupts(vcpu);
+	kvmppc_core_prepare_to_enter(vcpu);
 
 	r = kvmppc_vcpu_run(run, vcpu);
 

commit dc83b8bc0256ee682506ed83853a98eaba529c6f
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Aug 18 15:25:21 2011 -0500

    KVM: PPC: e500: MMU API
    
    This implements a shared-memory API for giving host userspace access to
    the guest's TLB.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a5671616af86..3cf6fba513ac 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -222,6 +222,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:
+#ifdef CONFIG_KVM_E500
+	case KVM_CAP_SW_TLB:
+#endif
 		r = 1;
 		break;
 	case KVM_CAP_COALESCED_MMIO:
@@ -602,6 +605,19 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = 0;
 		vcpu->arch.papr_enabled = true;
 		break;
+#ifdef CONFIG_KVM_E500
+	case KVM_CAP_SW_TLB: {
+		struct kvm_config_tlb cfg;
+		void __user *user_ptr = (void __user *)(uintptr_t)cap->args[0];
+
+		r = -EFAULT;
+		if (copy_from_user(&cfg, user_ptr, sizeof(cfg)))
+			break;
+
+		r = kvm_vcpu_ioctl_config_tlb(vcpu, &cfg);
+		break;
+	}
+#endif
 	default:
 		r = -EINVAL;
 		break;
@@ -651,6 +667,18 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		r = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);
 		break;
 	}
+
+#ifdef CONFIG_KVM_E500
+	case KVM_DIRTY_TLB: {
+		struct kvm_dirty_tlb dirty;
+		r = -EFAULT;
+		if (copy_from_user(&dirty, argp, sizeof(dirty)))
+			goto out;
+		r = kvm_vcpu_ioctl_dirty_tlb(vcpu, &dirty);
+		break;
+	}
+#endif
+
 	default:
 		r = -EINVAL;
 	}

commit 5b1c1493afe8d69909f9df3221bb2fffdf479f4a
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:23 2012 +0100

    KVM: s390: ucontrol: export SIE control block to user
    
    This patch exports the s390 SIE hardware control block to userspace
    via the mapping of the vcpu file descriptor. In order to do so,
    a new arch callback named kvm_arch_vcpu_fault  is introduced for all
    architectures. It allows to map architecture specific pages.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 83f244569874..a5671616af86 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -659,6 +659,11 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	return r;
 }
 
+int kvm_arch_vcpu_fault(struct kvm_vcpu *vcpu, struct vm_fault *vmf)
+{
+	return VM_FAULT_SIGBUS;
+}
+
 static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
 {
 	u32 inst_lis = 0x3c000000;

commit e08b96371625aaa84cb03f51acc4c8e0be27403a
Author: Carsten Otte <cotte@de.ibm.com>
Date:   Wed Jan 4 10:25:20 2012 +0100

    KVM: s390: add parameter for KVM_CREATE_VM
    
    This patch introduces a new config option for user controlled kernel
    virtual machines. It introduces a parameter to KVM_CREATE_VM that
    allows to set bits that alter the capabilities of the newly created
    virtual machine.
    The parameter is passed to kvm_arch_init_vm for all architectures.
    The only valid modifier bit for now is KVM_VM_S390_UCONTROL.
    This requires CAP_SYS_ADMIN privileges and creates a user controlled
    virtual machine on s390 architectures.
    
    Signed-off-by: Carsten Otte <cotte@de.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 607fbdf24b84..83f244569874 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -171,8 +171,11 @@ void kvm_arch_check_processor_compat(void *rtn)
 	*(int *)rtn = kvmppc_core_check_processor_compat();
 }
 
-int kvm_arch_init_vm(struct kvm *kvm)
+int kvm_arch_init_vm(struct kvm *kvm, unsigned long type)
 {
+	if (type)
+		return -EINVAL;
+
 	return kvmppc_core_init_vm(kvm);
 }
 

commit bb75c627fb0dfb8c0ab75d3033709ff928896e16
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Nov 17 15:26:35 2011 +0100

    Revert "KVM: PPC: Add support for explicit HIOR setting"
    
    This reverts commit a15bd354f083f20f257db450488db52ac27df439.
    
    It exceeded the padding on the SREGS struct, rendering the ABI
    backwards-incompatible.
    
    Conflicts:
    
            arch/powerpc/kvm/powerpc.c
            include/linux/kvm.h
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index efbf9ad87203..607fbdf24b84 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -208,7 +208,6 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_BOOKE_SREGS:
 #else
 	case KVM_CAP_PPC_SEGSTATE:
-	case KVM_CAP_PPC_HIOR:
 	case KVM_CAP_PPC_PAPR:
 #endif
 	case KVM_CAP_PPC_UNSET_IRQ:

commit ead53f22dc646d91a1b6201b9f44dd47d7d88c34
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 14:24:04 2011 -0400

    powerpc: remove non-required uses of include <linux/module.h>
    
    None of the files touched here are modules, and they are not
    exporting any symbols either -- so there is no need to be including
    the module.h.  Builds of all the files remains successful.
    
    Even kernel/module.c does not need to include it, since it includes
    linux/moduleloader.h instead.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0d843c6ba315..efbf9ad87203 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -21,7 +21,6 @@
 #include <linux/errno.h>
 #include <linux/err.h>
 #include <linux/kvm_host.h>
-#include <linux/module.h>
 #include <linux/vmalloc.h>
 #include <linux/hrtimer.h>
 #include <linux/fs.h>

commit 19ccb76a1938ab364a412253daec64613acbf3df
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Jul 23 17:42:46 2011 +1000

    KVM: PPC: Implement H_CEDE hcall for book3s_hv in real-mode code
    
    With a KVM guest operating in SMT4 mode (i.e. 4 hardware threads per
    core), whenever a CPU goes idle, we have to pull all the other
    hardware threads in the core out of the guest, because the H_CEDE
    hcall is handled in the kernel.  This is inefficient.
    
    This adds code to book3s_hv_rmhandlers.S to handle the H_CEDE hcall
    in real mode.  When a guest vcpu does an H_CEDE hcall, we now only
    exit to the kernel if all the other vcpus in the same core are also
    idle.  Otherwise we mark this vcpu as napping, save state that could
    be lost in nap mode (mainly GPRs and FPRs), and execute the nap
    instruction.  When the thread wakes up, because of a decrementer or
    external interrupt, we come back in at kvm_start_guest (from the
    system reset interrupt vector), find the `napping' flag set in the
    paca, and go to the resume path.
    
    This has some other ramifications.  First, when starting a core, we
    now start all the threads, both those that are immediately runnable and
    those that are idle.  This is so that we don't have to pull all the
    threads out of the guest when an idle thread gets a decrementer interrupt
    and wants to start running.  In fact the idle threads will all start
    with the H_CEDE hcall returning; being idle they will just do another
    H_CEDE immediately and go to nap mode.
    
    This required some changes to kvmppc_run_core() and kvmppc_run_vcpu().
    These functions have been restructured to make them simpler and clearer.
    We introduce a level of indirection in the wait queue that gets woken
    when external and decrementer interrupts get generated for a vcpu, so
    that we can have the 4 vcpus in a vcore using the same wait queue.
    We need this because the 4 vcpus are being handled by one thread.
    
    Secondly, when we need to exit from the guest to the kernel, we now
    have to generate an IPI for any napping threads, because an HDEC
    interrupt doesn't wake up a napping thread.
    
    Thirdly, we now need to be able to handle virtual external interrupts
    and decrementer interrupts becoming pending while a thread is napping,
    and deliver those interrupts to the guest when the thread wakes.
    This is done in kvmppc_cede_reentry, just before fast_guest_return.
    
    Finally, since we are not using the generic kvm_vcpu_block for book3s_hv,
    and hence not calling kvm_arch_vcpu_runnable, we can remove the #ifdef
    from kvm_arch_vcpu_runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a8000ce562b0..0d843c6ba315 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -39,12 +39,8 @@
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
-#ifndef CONFIG_KVM_BOOK3S_64_HV
 	return !(v->arch.shared->msr & MSR_WE) ||
 	       !!(v->arch.pending_exceptions);
-#else
-	return !(v->arch.ceded) || !!(v->arch.pending_exceptions);
-#endif
 }
 
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
@@ -285,6 +281,7 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
 	struct kvm_vcpu *vcpu;
 	vcpu = kvmppc_core_vcpu_create(kvm, id);
+	vcpu->arch.wqp = &vcpu->wq;
 	if (!IS_ERR(vcpu))
 		kvmppc_create_vcpu_debugfs(vcpu, id);
 	return vcpu;
@@ -316,8 +313,8 @@ static void kvmppc_decrementer_func(unsigned long data)
 
 	kvmppc_core_queue_dec(vcpu);
 
-	if (waitqueue_active(&vcpu->wq)) {
-		wake_up_interruptible(&vcpu->wq);
+	if (waitqueue_active(vcpu->arch.wqp)) {
+		wake_up_interruptible(vcpu->arch.wqp);
 		vcpu->stat.halt_wakeup++;
 	}
 }
@@ -570,13 +567,15 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 {
-	if (irq->irq == KVM_INTERRUPT_UNSET)
+	if (irq->irq == KVM_INTERRUPT_UNSET) {
 		kvmppc_core_dequeue_external(vcpu, irq);
-	else
-		kvmppc_core_queue_external(vcpu, irq);
+		return 0;
+	}
+
+	kvmppc_core_queue_external(vcpu, irq);
 
-	if (waitqueue_active(&vcpu->wq)) {
-		wake_up_interruptible(&vcpu->wq);
+	if (waitqueue_active(vcpu->arch.wqp)) {
+		wake_up_interruptible(vcpu->arch.wqp);
 		vcpu->stat.halt_wakeup++;
 	} else if (vcpu->cpu != -1) {
 		smp_send_reschedule(vcpu->cpu);

commit af8f38b3499f0d4a3c354df2435f0fb2dded250a
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Aug 10 13:57:08 2011 +0200

    KVM: PPC: Add sanity checking to vcpu_run
    
    There are multiple features in PowerPC KVM that can now be enabled
    depending on the user's wishes. Some of the combinations don't make
    sense or don't work though.
    
    So this patch adds a way to check if the executing environment would
    actually be able to run the guest properly. It also adds sanity
    checks if PVR is set (should always be true given the current code
    flow), if PAPR is only used with book3s_64 where it works and that
    HV KVM is only used in PAPR mode.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 13bc798a4441..a8000ce562b0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -95,6 +95,31 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	return r;
 }
 
+int kvmppc_sanity_check(struct kvm_vcpu *vcpu)
+{
+	int r = false;
+
+	/* We have to know what CPU to virtualize */
+	if (!vcpu->arch.pvr)
+		goto out;
+
+	/* PAPR only works with book3s_64 */
+	if ((vcpu->arch.cpu_type != KVM_CPU_3S_64) && vcpu->arch.papr_enabled)
+		goto out;
+
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	/* HV KVM can only do PAPR mode for now */
+	if (!vcpu->arch.papr_enabled)
+		goto out;
+#endif
+
+	r = true;
+
+out:
+	vcpu->arch.sane = r;
+	return r ? 0 : -EINVAL;
+}
+
 int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 {
 	enum emulation_result er;
@@ -582,6 +607,9 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		break;
 	}
 
+	if (!r)
+		r = kvmppc_sanity_check(vcpu);
+
 	return r;
 }
 

commit 930b412a005bde2ea3f05911eaaeeb10f11d79ab
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 8 17:29:42 2011 +0200

    KVM: PPC: Enable the PAPR CAP for Book3S
    
    Now that Book3S PV mode can also run PAPR guests, we can add a PAPR cap and
    enable it for all Book3S targets. Enabling that CAP switches KVM into PAPR
    mode.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 17a5c83e1ccc..13bc798a4441 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -189,6 +189,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 #else
 	case KVM_CAP_PPC_SEGSTATE:
 	case KVM_CAP_PPC_HIOR:
+	case KVM_CAP_PPC_PAPR:
 #endif
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_PPC_IRQ_LEVEL:
@@ -572,6 +573,10 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		r = 0;
 		vcpu->arch.osi_enabled = true;
 		break;
+	case KVM_CAP_PPC_PAPR:
+		r = 0;
+		vcpu->arch.papr_enabled = true;
+		break;
 	default:
 		r = -EINVAL;
 		break;

commit a15bd354f083f20f257db450488db52ac27df439
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 8 17:17:09 2011 +0200

    KVM: PPC: Add support for explicit HIOR setting
    
    Until now, we always set HIOR based on the PVR, but this is just wrong.
    Instead, we should be setting HIOR explicitly, so user space can decide
    what the initial HIOR value is - just like on real hardware.
    
    We keep the old PVR based way around for backwards compatibility, but
    once user space uses the SREGS based method, we drop the PVR logic.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a107c9be0fb1..17a5c83e1ccc 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -188,6 +188,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_BOOKE_SREGS:
 #else
 	case KVM_CAP_PPC_SEGSTATE:
+	case KVM_CAP_PPC_HIOR:
 #endif
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_PPC_IRQ_LEVEL:

commit 9e368f2915601cd5bc7f5fd638b58435b018bbd7
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:40:08 2011 +0000

    KVM: PPC: book3s_hv: Add support for PPC970-family processors
    
    This adds support for running KVM guests in supervisor mode on those
    PPC970 processors that have a usable hypervisor mode.  Unfortunately,
    Apple G5 machines have supervisor mode disabled (MSR[HV] is forced to
    1), but the YDL PowerStation does have a usable hypervisor mode.
    
    There are several differences between the PPC970 and POWER7 in how
    guests are managed.  These differences are accommodated using the
    CPU_FTR_ARCH_201 (PPC970) and CPU_FTR_ARCH_206 (POWER7) CPU feature
    bits.  Notably, on PPC970:
    
    * The LPCR, LPID or RMOR registers don't exist, and the functions of
      those registers are provided by bits in HID4 and one bit in HID0.
    
    * External interrupts can be directed to the hypervisor, but unlike
      POWER7 they are masked by MSR[EE] in non-hypervisor modes and use
      SRR0/1 not HSRR0/1.
    
    * There is no virtual RMA (VRMA) mode; the guest must use an RMO
      (real mode offset) area.
    
    * The TLB entries are not tagged with the LPID, so it is necessary to
      flush the whole TLB on partition switch.  Furthermore, when switching
      partitions we have to ensure that no other CPU is executing the tlbie
      or tlbsync instructions in either the old or the new partition,
      otherwise undefined behaviour can occur.
    
    * The PMU has 8 counters (PMC registers) rather than 6.
    
    * The DSCR, PURR, SPURR, AMR, AMOR, UAMOR registers don't exist.
    
    * The SLB has 64 entries rather than 32.
    
    * There is no mediated external interrupt facility, so if we switch to
      a guest that has a virtual external interrupt pending but the guest
      has MSR[EE] = 0, we have to arrange to have an interrupt pending for
      it so that we can get control back once it re-enables interrupts.  We
      do that by sending ourselves an IPI with smp_send_reschedule after
      hard-disabling interrupts.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 72c506505fa4..a107c9be0fb1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -213,6 +213,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 		break;
 	case KVM_CAP_PPC_RMA:
 		r = 1;
+		/* PPC970 requires an RMA */
+		if (cpu_has_feature(CPU_FTR_ARCH_201))
+			r = 2;
 		break;
 #endif
 	default:

commit aa04b4cc5be64b4fb9ef4e0fdf2418e2f4737fb2
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:25:44 2011 +0000

    KVM: PPC: Allocate RMAs (Real Mode Areas) at boot for use by guests
    
    This adds infrastructure which will be needed to allow book3s_hv KVM to
    run on older POWER processors, including PPC970, which don't support
    the Virtual Real Mode Area (VRMA) facility, but only the Real Mode
    Offset (RMO) facility.  These processors require a physically
    contiguous, aligned area of memory for each guest.  When the guest does
    an access in real mode (MMU off), the address is compared against a
    limit value, and if it is lower, the address is ORed with an offset
    value (from the Real Mode Offset Register (RMOR)) and the result becomes
    the real address for the access.  The size of the RMA has to be one of
    a set of supported values, which usually includes 64MB, 128MB, 256MB
    and some larger powers of 2.
    
    Since we are unlikely to be able to allocate 64MB or more of physically
    contiguous memory after the kernel has been running for a while, we
    allocate a pool of RMAs at boot time using the bootmem allocator.  The
    size and number of the RMAs can be set using the kvm_rma_size=xx and
    kvm_rma_count=xx kernel command line options.
    
    KVM exports a new capability, KVM_CAP_PPC_RMA, to signal the availability
    of the pool of preallocated RMAs.  The capability value is 1 if the
    processor can use an RMA but doesn't require one (because it supports
    the VRMA facility), or 2 if the processor requires an RMA for each guest.
    
    This adds a new ioctl, KVM_ALLOCATE_RMA, which allocates an RMA from the
    pool and returns a file descriptor which can be used to map the RMA.  It
    also returns the size of the RMA in the argument structure.
    
    Having an RMA means we will get multiple KMV_SET_USER_MEMORY_REGION
    ioctl calls from userspace.  To cope with this, we now preallocate the
    kvm->arch.ram_pginfo array when the VM is created with a size sufficient
    for up to 64GB of guest memory.  Subsequently we will get rid of this
    array and use memory associated with each memslot instead.
    
    This moves most of the code that translates the user addresses into
    host pfns (page frame numbers) out of kvmppc_prepare_vrma up one level
    to kvmppc_core_prepare_memory_region.  Also, instead of having to look
    up the VMA for each page in order to check the page size, we now check
    that the pages we get are compound pages of 16MB.  However, if we are
    adding memory that is mapped to an RMA, we don't bother with calling
    get_user_pages_fast and instead just offset from the base pfn for the
    RMA.
    
    Typically the RMA gets added after vcpus are created, which makes it
    inconvenient to have the LPCR (logical partition control register) value
    in the vcpu->arch struct, since the LPCR controls whether the processor
    uses RMA or VRMA for the guest.  This moves the LPCR value into the
    kvm->arch struct and arranges for the MER (mediated external request)
    bit, which is the only bit that varies between vcpus, to be set in
    assembly code when going into the guest if there is a pending external
    interrupt request.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4c549664c987..72c506505fa4 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -211,6 +211,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_SMT:
 		r = threads_per_core;
 		break;
+	case KVM_CAP_PPC_RMA:
+		r = 1;
+		break;
 #endif
 	default:
 		r = 0;
@@ -673,6 +676,16 @@ long kvm_arch_vm_ioctl(struct file *filp,
 		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);
 		goto out;
 	}
+
+	case KVM_ALLOCATE_RMA: {
+		struct kvm *kvm = filp->private_data;
+		struct kvm_allocate_rma rma;
+
+		r = kvm_vm_ioctl_allocate_rma(kvm, &rma);
+		if (r >= 0 && copy_to_user(argp, &rma, sizeof(rma)))
+			r = -EFAULT;
+		break;
+	}
 #endif /* CONFIG_KVM_BOOK3S_64_HV */
 
 	default:

commit 371fefd6f2dc46668e00871930dde613b88d4bde
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:23:08 2011 +0000

    KVM: PPC: Allow book3s_hv guests to use SMT processor modes
    
    This lifts the restriction that book3s_hv guests can only run one
    hardware thread per core, and allows them to use up to 4 threads
    per core on POWER7.  The host still has to run single-threaded.
    
    This capability is advertised to qemu through a new KVM_CAP_PPC_SMT
    capability.  The return value of the ioctl querying this capability
    is the number of vcpus per virtual CPU core (vcore), currently 4.
    
    To use this, the host kernel should be booted with all threads
    active, and then all the secondary threads should be offlined.
    This will put the secondary threads into nap mode.  KVM will then
    wake them from nap mode and use them for running guest code (while
    they are still offline).  To wake the secondary threads, we send
    them an IPI using a new xics_wake_cpu() function, implemented in
    arch/powerpc/sysdev/xics/icp-native.c.  In other words, at this stage
    we assume that the platform has a XICS interrupt controller and
    we are using icp-native.c to drive it.  Since the woken thread will
    need to acknowledge and clear the IPI, we also export the base
    physical address of the XICS registers using kvmppc_set_xics_phys()
    for use in the low-level KVM book3s code.
    
    When a vcpu is created, it is assigned to a virtual CPU core.
    The vcore number is obtained by dividing the vcpu number by the
    number of threads per core in the host.  This number is exported
    to userspace via the KVM_CAP_PPC_SMT capability.  If qemu wishes
    to run the guest in single-threaded mode, it should make all vcpu
    numbers be multiples of the number of threads per core.
    
    We distinguish three states of a vcpu: runnable (i.e., ready to execute
    the guest), blocked (that is, idle), and busy in host.  We currently
    implement a policy that the vcore can run only when all its threads
    are runnable or blocked.  This way, if a vcpu needs to execute elsewhere
    in the kernel or in qemu, it can do so without being starved of CPU
    by the other vcpus.
    
    When a vcore starts to run, it executes in the context of one of the
    vcpu threads.  The other vcpu threads all go to sleep and stay asleep
    until something happens requiring the vcpu thread to return to qemu,
    or to wake up to run the vcore (this can happen when another vcpu
    thread goes from busy in host state to blocked).
    
    It can happen that a vcpu goes from blocked to runnable state (e.g.
    because of an interrupt), and the vcore it belongs to is already
    running.  In that case it can start to run immediately as long as
    the none of the vcpus in the vcore have started to exit the guest.
    We send the next free thread in the vcore an IPI to get it to start
    to execute the guest.  It synchronizes with the other threads via
    the vcore->entry_exit_count field to make sure that it doesn't go
    into the guest if the other vcpus are exiting by the time that it
    is ready to actually enter the guest.
    
    Note that there is no fixed relationship between the hardware thread
    number and the vcpu number.  Hardware threads are assigned to vcpus
    as they become runnable, so we will always use the lower-numbered
    hardware threads in preference to higher-numbered threads if not all
    the vcpus in the vcore are runnable, regardless of which vcpus are
    runnable.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index c78ceb9d5605..4c549664c987 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -30,6 +30,7 @@
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>
 #include <asm/tlbflush.h>
+#include <asm/cputhreads.h>
 #include "timing.h"
 #include "../mm/mmu_decl.h"
 
@@ -207,6 +208,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_SPAPR_TCE:
 		r = 1;
 		break;
+	case KVM_CAP_PPC_SMT:
+		r = threads_per_core;
+		break;
 #endif
 	default:
 		r = 0;

commit 54738c097163c3f01e67ccc85462b78d4d4f495f
Author: David Gibson <dwg@au1.ibm.com>
Date:   Wed Jun 29 00:22:41 2011 +0000

    KVM: PPC: Accelerate H_PUT_TCE by implementing it in real mode
    
    This improves I/O performance for guests using the PAPR
    paravirtualization interface by making the H_PUT_TCE hcall faster, by
    implementing it in real mode.  H_PUT_TCE is used for updating virtual
    IOMMU tables, and is used both for virtual I/O and for real I/O in the
    PAPR interface.
    
    Since this moves the IOMMU tables into the kernel, we define a new
    KVM_CREATE_SPAPR_TCE ioctl to allow qemu to create the tables.  The
    ioctl returns a file descriptor which can be used to mmap the newly
    created table.  The qemu driver models use them in the same way as
    userspace managed tables, but they can be updated directly by the
    guest with a real-mode H_PUT_TCE implementation, reducing the number
    of host/guest context switches during guest IO.
    
    There are certain circumstances where it is useful for userland qemu
    to write to the TCE table even if the kernel H_PUT_TCE path is used
    most of the time.  Specifically, allowing this will avoid awkwardness
    when we need to reset the table.  More importantly, we will in the
    future need to write the table in order to restore its state after a
    checkpoint resume or migration.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6fc9ee499b61..c78ceb9d5605 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -202,6 +202,11 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_COALESCED_MMIO:
 		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 		break;
+#endif
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	case KVM_CAP_SPAPR_TCE:
+		r = 1;
+		break;
 #endif
 	default:
 		r = 0;
@@ -653,6 +658,19 @@ long kvm_arch_vm_ioctl(struct file *filp,
 
 		break;
 	}
+#ifdef CONFIG_KVM_BOOK3S_64_HV
+	case KVM_CREATE_SPAPR_TCE: {
+		struct kvm_create_spapr_tce create_tce;
+		struct kvm *kvm = filp->private_data;
+
+		r = -EFAULT;
+		if (copy_from_user(&create_tce, argp, sizeof(create_tce)))
+			goto out;
+		r = kvm_vm_ioctl_create_spapr_tce(kvm, &create_tce);
+		goto out;
+	}
+#endif /* CONFIG_KVM_BOOK3S_64_HV */
+
 	default:
 		r = -ENOTTY;
 	}

commit a8606e20e41a8149456bafdf76ad29d47672027c
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:22:05 2011 +0000

    KVM: PPC: Handle some PAPR hcalls in the kernel
    
    This adds the infrastructure for handling PAPR hcalls in the kernel,
    either early in the guest exit path while we are still in real mode,
    or later once the MMU has been turned back on and we are in the full
    kernel context.  The advantage of handling hcalls in real mode if
    possible is that we avoid two partition switches -- and this will
    become more important when we support SMT4 guests, since a partition
    switch means we have to pull all of the threads in the core out of
    the guest.  The disadvantage is that we can only access the kernel
    linear mapping, not anything vmalloced or ioremapped, since the MMU
    is off.
    
    This also adds code to handle the following hcalls in real mode:
    
    H_ENTER       Add an HPTE to the hashed page table
    H_REMOVE      Remove an HPTE from the hashed page table
    H_READ        Read HPTEs from the hashed page table
    H_PROTECT     Change the protection bits in an HPTE
    H_BULK_REMOVE Remove up to 4 HPTEs from the hashed page table
    H_SET_DABR    Set the data address breakpoint register
    
    Plus code to handle the following hcalls in the kernel:
    
    H_CEDE        Idle the vcpu until an interrupt or H_PROD hcall arrives
    H_PROD        Wake up a ceded vcpu
    H_REGISTER_VPA Register a virtual processor area (VPA)
    
    The code that runs in real mode has to be in the base kernel, not in
    the module, if KVM is compiled as a module.  The real-mode code can
    only access the kernel linear mapping, not vmalloc or ioremap space.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3a4f379ee70f..6fc9ee499b61 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -42,7 +42,7 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 	return !(v->arch.shared->msr & MSR_WE) ||
 	       !!(v->arch.pending_exceptions);
 #else
-	return 1;
+	return !(v->arch.ceded) || !!(v->arch.pending_exceptions);
 #endif
 }
 

commit de56a948b9182fbcf92cb8212f114de096c2d574
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:21:34 2011 +0000

    KVM: PPC: Add support for Book3S processors in hypervisor mode
    
    This adds support for KVM running on 64-bit Book 3S processors,
    specifically POWER7, in hypervisor mode.  Using hypervisor mode means
    that the guest can use the processor's supervisor mode.  That means
    that the guest can execute privileged instructions and access privileged
    registers itself without trapping to the host.  This gives excellent
    performance, but does mean that KVM cannot emulate a processor
    architecture other than the one that the hardware implements.
    
    This code assumes that the guest is running paravirtualized using the
    PAPR (Power Architecture Platform Requirements) interface, which is the
    interface that IBM's PowerVM hypervisor uses.  That means that existing
    Linux distributions that run on IBM pSeries machines will also run
    under KVM without modification.  In order to communicate the PAPR
    hypercalls to qemu, this adds a new KVM_EXIT_PAPR_HCALL exit code
    to include/linux/kvm.h.
    
    Currently the choice between book3s_hv support and book3s_pr support
    (i.e. the existing code, which runs the guest in user mode) has to be
    made at kernel configuration time, so a given kernel binary can only
    do one or the other.
    
    This new book3s_hv code doesn't support MMIO emulation at present.
    Since we are running paravirtualized guests, this isn't a serious
    restriction.
    
    With the guest running in supervisor mode, most exceptions go straight
    to the guest.  We will never get data or instruction storage or segment
    interrupts, alignment interrupts, decrementer interrupts, program
    interrupts, single-step interrupts, etc., coming to the hypervisor from
    the guest.  Therefore this introduces a new KVMTEST_NONHV macro for the
    exception entry path so that we don't have to do the KVM test on entry
    to those exception handlers.
    
    We do however get hypervisor decrementer, hypervisor data storage,
    hypervisor instruction storage, and hypervisor emulation assist
    interrupts, so we have to handle those.
    
    In hypervisor mode, real-mode accesses can access all of RAM, not just
    a limited amount.  Therefore we put all the guest state in the vcpu.arch
    and use the shadow_vcpu in the PACA only for temporary scratch space.
    We allocate the vcpu with kzalloc rather than vzalloc, and we don't use
    anything in the kvmppc_vcpu_book3s struct, so we don't allocate it.
    We don't have a shared page with the guest, but we still need a
    kvm_vcpu_arch_shared struct to store the values of various registers,
    so we include one in the vcpu_arch struct.
    
    The POWER7 processor has a restriction that all threads in a core have
    to be in the same partition.  MMU-on kernel code counts as a partition
    (partition 0), so we have to do a partition switch on every entry to and
    exit from the guest.  At present we require the host and guest to run
    in single-thread mode because of this hardware restriction.
    
    This code allocates a hashed page table for the guest and initializes
    it with HPTEs for the guest's Virtual Real Memory Area (VRMA).  We
    require that the guest memory is allocated using 16MB huge pages, in
    order to simplify the low-level memory management.  This also means that
    we can get away without tracking paging activity in the host for now,
    since huge pages can't be paged or swapped.
    
    This also adds a few new exports needed by the book3s_hv code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 026036efcde0..3a4f379ee70f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -38,8 +38,12 @@
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
+#ifndef CONFIG_KVM_BOOK3S_64_HV
 	return !(v->arch.shared->msr & MSR_WE) ||
 	       !!(v->arch.pending_exceptions);
+#else
+	return 1;
+#endif
 }
 
 int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
@@ -184,10 +188,13 @@ int kvm_dev_ioctl_check_extension(long ext)
 #else
 	case KVM_CAP_PPC_SEGSTATE:
 #endif
-	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_PPC_IRQ_LEVEL:
 	case KVM_CAP_ENABLE_CAP:
+		r = 1;
+		break;
+#ifndef CONFIG_KVM_BOOK3S_64_HV
+	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:
 		r = 1;
@@ -195,6 +202,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_COALESCED_MMIO:
 		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 		break;
+#endif
 	default:
 		r = 0;
 		break;
@@ -291,6 +299,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	hrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
 	tasklet_init(&vcpu->arch.tasklet, kvmppc_decrementer_func, (ulong)vcpu);
 	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
+	vcpu->arch.dec_expires = ~(u64)0;
 
 #ifdef CONFIG_KVM_EXIT_TIMING
 	mutex_init(&vcpu->arch.exit_timing_lock);
@@ -317,6 +326,7 @@ void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 	mtspr(SPRN_VRSAVE, vcpu->arch.vrsave);
 #endif
 	kvmppc_core_vcpu_load(vcpu, cpu);
+	vcpu->cpu = smp_processor_id();
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
@@ -325,6 +335,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 #ifdef CONFIG_BOOKE
 	vcpu->arch.vrsave = mfspr(SPRN_VRSAVE);
 #endif
+	vcpu->cpu = -1;
 }
 
 int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
@@ -496,6 +507,13 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		for (i = 0; i < 32; i++)
 			kvmppc_set_gpr(vcpu, i, gprs[i]);
 		vcpu->arch.osi_needed = 0;
+	} else if (vcpu->arch.hcall_needed) {
+		int i;
+
+		kvmppc_set_gpr(vcpu, 3, run->papr_hcall.ret);
+		for (i = 0; i < 9; ++i)
+			kvmppc_set_gpr(vcpu, 4 + i, run->papr_hcall.args[i]);
+		vcpu->arch.hcall_needed = 0;
 	}
 
 	kvmppc_core_deliver_interrupts(vcpu);
@@ -518,6 +536,8 @@ int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 	if (waitqueue_active(&vcpu->wq)) {
 		wake_up_interruptible(&vcpu->wq);
 		vcpu->stat.halt_wakeup++;
+	} else if (vcpu->cpu != -1) {
+		smp_send_reschedule(vcpu->cpu);
 	}
 
 	return 0;

commit df6909e5d52f67be01862c5cb453e509aee661f1
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:19:50 2011 +0000

    KVM: PPC: Move guest enter/exit down into subarch-specific code
    
    Instead of doing the kvm_guest_enter/exit() and local_irq_dis/enable()
    calls in powerpc.c, this moves them down into the subarch-specific
    book3s_pr.c and booke.c.  This eliminates an extra local_irq_enable()
    call in book3s_pr.c, and will be needed for when we do SMT4 guest
    support in the book3s hypervisor mode code.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0c80e159c138..026036efcde0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -500,11 +500,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 	kvmppc_core_deliver_interrupts(vcpu);
 
-	local_irq_disable();
-	kvm_guest_enter();
-	r = __kvmppc_vcpu_run(run, vcpu);
-	kvm_guest_exit();
-	local_irq_enable();
+	r = kvmppc_vcpu_run(run, vcpu);
 
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);

commit f9e0554deca54a42fb2cf7f68c05a4a37461c205
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Jun 29 00:19:22 2011 +0000

    KVM: PPC: Pass init/destroy vm and prepare/commit memory region ops down
    
    This arranges for the top-level arch/powerpc/kvm/powerpc.c file to
    pass down some of the calls it gets to the lower-level subarchitecture
    specific code.  The lower-level implementations (in booke.c and book3s.c)
    are no-ops.  The coming book3s_hv.c will need this.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 24e2b64b6a48..0c80e159c138 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -148,7 +148,7 @@ void kvm_arch_check_processor_compat(void *rtn)
 
 int kvm_arch_init_vm(struct kvm *kvm)
 {
-	return 0;
+	return kvmppc_core_init_vm(kvm);
 }
 
 void kvm_arch_destroy_vm(struct kvm *kvm)
@@ -164,6 +164,9 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 		kvm->vcpus[i] = NULL;
 
 	atomic_set(&kvm->online_vcpus, 0);
+
+	kvmppc_core_destroy_vm(kvm);
+
 	mutex_unlock(&kvm->lock);
 }
 
@@ -212,7 +215,7 @@ int kvm_arch_prepare_memory_region(struct kvm *kvm,
                                    struct kvm_userspace_memory_region *mem,
                                    int user_alloc)
 {
-	return 0;
+	return kvmppc_core_prepare_memory_region(kvm, mem);
 }
 
 void kvm_arch_commit_memory_region(struct kvm *kvm,
@@ -220,7 +223,7 @@ void kvm_arch_commit_memory_region(struct kvm *kvm,
                struct kvm_memory_slot old,
                int user_alloc)
 {
-       return;
+	kvmppc_core_commit_memory_region(kvm, mem);
 }
 
 

commit a4cd8b23ac5786943202c0174c717956947db43c
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Jun 14 18:34:41 2011 -0500

    KVM: PPC: e500: enable magic page
    
    This is a shared page used for paravirtualization.  It is always present
    in the guest kernel's effective address space at the address indicated
    by the hypercall that enables it.
    
    The physical address specified by the hypercall is not used, as
    e500 does not have real mode.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 616dd516ca1f..24e2b64b6a48 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -73,7 +73,8 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	}
 	case HC_VENDOR_KVM | KVM_HC_FEATURES:
 		r = HC_EV_SUCCESS;
-#if defined(CONFIG_PPC_BOOK3S) /* XXX Missing magic page on BookE */
+#if defined(CONFIG_PPC_BOOK3S) || defined(CONFIG_KVM_E500)
+		/* XXX Missing magic page on 44x */
 		r2 |= (1 << KVM_FEATURE_MAGIC_PAGE);
 #endif
 

commit 5ce941ee4258b836cf818d2ac159d8cf3ebad648
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Apr 27 17:24:21 2011 -0500

    KVM: PPC: booke: add sregs support
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9e6aa8bfd160..616dd516ca1f 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -175,7 +175,11 @@ int kvm_dev_ioctl_check_extension(long ext)
 	int r;
 
 	switch (ext) {
+#ifdef CONFIG_BOOKE
+	case KVM_CAP_PPC_BOOKE_SREGS:
+#else
 	case KVM_CAP_PPC_SEGSTATE:
+#endif
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_PPC_IRQ_LEVEL:

commit eab176722f4628b2d9cf76221a43dd3a0e37e632
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Apr 27 17:24:10 2011 -0500

    KVM: PPC: booke: save/restore VRSAVE (a.k.a. USPRG0)
    
    Linux doesn't use USPRG0 (now renamed VRSAVE in the architecture, even
    when Altivec isn't involved), but a guest might.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ec3d2e75c0a8..9e6aa8bfd160 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -298,12 +298,25 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
+#ifdef CONFIG_BOOKE
+	/*
+	 * vrsave (formerly usprg0) isn't used by Linux, but may
+	 * be used by the guest.
+	 *
+	 * On non-booke this is associated with Altivec and
+	 * is handled by code in book3s.c.
+	 */
+	mtspr(SPRN_VRSAVE, vcpu->arch.vrsave);
+#endif
 	kvmppc_core_vcpu_load(vcpu, cpu);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	kvmppc_core_vcpu_put(vcpu);
+#ifdef CONFIG_BOOKE
+	vcpu->arch.vrsave = mfspr(SPRN_VRSAVE);
+#endif
 }
 
 int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,

commit 09000adb86550d2895b64faa52e64eaec3cae7b2
Author: Bharat Bhushan <r65777@freescale.com>
Date:   Fri Mar 25 10:32:13 2011 +0530

    KVM: PPC: Fix issue clearing exit timing counters
    
    Following dump is observed on host when clearing the exit timing counters
    
    [root@p1021mds kvm]# echo -n 'c' > vm1200_vcpu0_timing
    INFO: task echo:1276 blocked for more than 120 seconds.
    "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
    echo          D 0ff5bf94     0  1276   1190 0x00000000
    Call Trace:
    [c2157e40] [c0007908] __switch_to+0x9c/0xc4
    [c2157e50] [c040293c] schedule+0x1b4/0x3bc
    [c2157e90] [c04032dc] __mutex_lock_slowpath+0x74/0xc0
    [c2157ec0] [c00369e4] kvmppc_init_timing_stats+0x20/0xb8
    [c2157ed0] [c0036b00] kvmppc_exit_timing_write+0x84/0x98
    [c2157ef0] [c00b9f90] vfs_write+0xc0/0x16c
    [c2157f10] [c00ba284] sys_write+0x4c/0x90
    [c2157f40] [c000e320] ret_from_syscall+0x0/0x3c
    
            The vcpu->mutex is used by kvm_ioctl_* (KVM_RUN etc) and same was
    used when clearing the stats (in kvmppc_init_timing_stats()). What happens
    is that when the guest is idle then it held the vcpu->mutx. While the
    exiting timing process waits for guest to release the vcpu->mutex and
    a hang state is reached.
    
            Now using seprate lock for exit timing stats.
    
    Signed-off-by: Bharat Bhushan <Bharat.Bhushan@freescale.com>
    Acked-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 99758460efde..ec3d2e75c0a8 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -284,6 +284,10 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 	tasklet_init(&vcpu->arch.tasklet, kvmppc_decrementer_func, (ulong)vcpu);
 	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
 
+#ifdef CONFIG_KVM_EXIT_TIMING
+	mutex_init(&vcpu->arch.exit_timing_lock);
+#endif
+
 	return 0;
 }
 

commit d89f5eff70a31237ffa1e21c51d23ca532110aea
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Tue Nov 9 17:02:49 2010 +0100

    KVM: Clean up vm creation and release
    
    IA64 support forces us to abstract the allocation of the kvm structure.
    But instead of mixing this up with arch-specific initialization and
    doing the same on destruction, split both steps. This allows to move
    generic destruction calls into generic code.
    
    It also fixes error clean-up on failures of kvm_create_vm for IA64.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 38f756f25053..99758460efde 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -145,18 +145,12 @@ void kvm_arch_check_processor_compat(void *rtn)
 	*(int *)rtn = kvmppc_core_check_processor_compat();
 }
 
-struct kvm *kvm_arch_create_vm(void)
+int kvm_arch_init_vm(struct kvm *kvm)
 {
-	struct kvm *kvm;
-
-	kvm = kzalloc(sizeof(struct kvm), GFP_KERNEL);
-	if (!kvm)
-		return ERR_PTR(-ENOMEM);
-
-	return kvm;
+	return 0;
 }
 
-static void kvmppc_free_vcpus(struct kvm *kvm)
+void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	unsigned int i;
 	struct kvm_vcpu *vcpu;
@@ -176,14 +170,6 @@ void kvm_arch_sync_events(struct kvm *kvm)
 {
 }
 
-void kvm_arch_destroy_vm(struct kvm *kvm)
-{
-	kvmppc_free_vcpus(kvm);
-	kvm_free_physmem(kvm);
-	cleanup_srcu_struct(&kvm->srcu);
-	kfree(kvm);
-}
-
 int kvm_dev_ioctl_check_extension(long ext)
 {
 	int r;

commit d8cdddcd645766cd4d80fa222226ae6ebfb706af
Author: Vasiliy Kulikov <segooon@gmail.com>
Date:   Sat Oct 30 13:04:24 2010 +0400

    KVM: PPC: fix information leak to userland
    
    Structure kvm_ppc_pvinfo is copied to userland with flags and
    pad fields unitialized.  It leads to leaking of contents of
    kernel stack memory.
    
    Signed-off-by: Vasiliy Kulikov <segooon@gmail.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2f87a1627f6c..38f756f25053 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -617,6 +617,7 @@ long kvm_arch_vm_ioctl(struct file *filp,
 	switch (ioctl) {
 	case KVM_PPC_GET_PVINFO: {
 		struct kvm_ppc_pvinfo pvinfo;
+		memset(&pvinfo, 0, sizeof(pvinfo));
 		r = kvm_vm_ioctl_get_pvinfo(&pvinfo);
 		if (copy_to_user(argp, &pvinfo, sizeof(pvinfo))) {
 			r = -EFAULT;

commit 7b4203e8cb5c5d9bc49da62b7a6fa4ba876a1b3f
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Aug 30 13:50:45 2010 +0200

    KVM: PPC: Expose level based interrupt cap
    
    Now that we have all the level interrupt magic in place, let's
    expose the capability to user space, so it can make use of it!
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 028891c0baff..2f87a1627f6c 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -192,6 +192,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_SEGSTATE:
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_UNSET_IRQ:
+	case KVM_CAP_PPC_IRQ_LEVEL:
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_PPC_OSI:
 	case KVM_CAP_PPC_GET_PVINFO:

commit df1bfa25d81f9451715ccbbb67551e0f792ceec8
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Aug 3 02:29:27 2010 +0200

    KVM: PPC: Put segment registers in shared page
    
    Now that the actual mtsr doesn't do anything anymore, we can move the sr
    contents over to the shared page, so a guest can directly read and write
    its sr contents from guest context.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 496d7a5200dc..028891c0baff 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -66,7 +66,7 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 		vcpu->arch.magic_page_pa = param1;
 		vcpu->arch.magic_page_ea = param2;
 
-		r2 = 0;
+		r2 = KVM_MAGIC_FEAT_SR;
 
 		r = HC_EV_SUCCESS;
 		break;

commit 7508e16c9f2a20f7721d7bc47c33a7b34c873a2c
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Aug 3 11:32:56 2010 +0200

    KVM: PPC: Add feature bitmap for magic page
    
    We will soon add SR PV support to the shared page, so we need some
    infrastructure that allows the guest to query for features KVM exports.
    
    This patch adds a second return value to the magic mapping that
    indicated to the guest which features are available.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 6a53a3f86dae..496d7a5200dc 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -66,6 +66,8 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 		vcpu->arch.magic_page_pa = param1;
 		vcpu->arch.magic_page_ea = param2;
 
+		r2 = 0;
+
 		r = HC_EV_SUCCESS;
 		break;
 	}
@@ -76,13 +78,14 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 #endif
 
 		/* Second return value is in r4 */
-		kvmppc_set_gpr(vcpu, 4, r2);
 		break;
 	default:
 		r = HC_EV_UNIMPLEMENTED;
 		break;
 	}
 
+	kvmppc_set_gpr(vcpu, 4, r2);
+
 	return r;
 }
 

commit 15711e9c927bfc08e66791cbf0ca7887c0880768
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:48:08 2010 +0200

    KVM: PPC: Add get_pvinfo interface to query hypercall instructions
    
    We need to tell the guest the opcodes that make up a hypercall through
    interfaces that are controlled by userspace. So we need to add a call
    for userspace to allow it to query those opcodes so it can pass them
    on.
    
    This is required because the hypercall opcodes can change based on
    the hypervisor conditions. If we're running in hardware accelerated
    hypervisor mode, a hypercall looks different from when we're running
    without hardware acceleration.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index fecfe043458d..6a53a3f86dae 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -191,6 +191,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_ENABLE_CAP:
 	case KVM_CAP_PPC_OSI:
+	case KVM_CAP_PPC_GET_PVINFO:
 		r = 1;
 		break;
 	case KVM_CAP_COALESCED_MMIO:
@@ -578,16 +579,53 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	return r;
 }
 
+static int kvm_vm_ioctl_get_pvinfo(struct kvm_ppc_pvinfo *pvinfo)
+{
+	u32 inst_lis = 0x3c000000;
+	u32 inst_ori = 0x60000000;
+	u32 inst_nop = 0x60000000;
+	u32 inst_sc = 0x44000002;
+	u32 inst_imm_mask = 0xffff;
+
+	/*
+	 * The hypercall to get into KVM from within guest context is as
+	 * follows:
+	 *
+	 *    lis r0, r0, KVM_SC_MAGIC_R0@h
+	 *    ori r0, KVM_SC_MAGIC_R0@l
+	 *    sc
+	 *    nop
+	 */
+	pvinfo->hcall[0] = inst_lis | ((KVM_SC_MAGIC_R0 >> 16) & inst_imm_mask);
+	pvinfo->hcall[1] = inst_ori | (KVM_SC_MAGIC_R0 & inst_imm_mask);
+	pvinfo->hcall[2] = inst_sc;
+	pvinfo->hcall[3] = inst_nop;
+
+	return 0;
+}
+
 long kvm_arch_vm_ioctl(struct file *filp,
                        unsigned int ioctl, unsigned long arg)
 {
+	void __user *argp = (void __user *)arg;
 	long r;
 
 	switch (ioctl) {
+	case KVM_PPC_GET_PVINFO: {
+		struct kvm_ppc_pvinfo pvinfo;
+		r = kvm_vm_ioctl_get_pvinfo(&pvinfo);
+		if (copy_to_user(argp, &pvinfo, sizeof(pvinfo))) {
+			r = -EFAULT;
+			goto out;
+		}
+
+		break;
+	}
 	default:
 		r = -ENOTTY;
 	}
 
+out:
 	return r;
 }
 

commit 5fc87407b55f5799418f4dc5931232c2bc06d077
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:55 2010 +0200

    KVM: PPC: Expose magic page support to guest
    
    Now that we have the shared page in place and the MMU code knows about
    the magic page, we can expose that capability to the guest!
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a4cf4b47e232..fecfe043458d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -61,8 +61,19 @@ int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
 	}
 
 	switch (nr) {
+	case HC_VENDOR_KVM | KVM_HC_PPC_MAP_MAGIC_PAGE:
+	{
+		vcpu->arch.magic_page_pa = param1;
+		vcpu->arch.magic_page_ea = param2;
+
+		r = HC_EV_SUCCESS;
+		break;
+	}
 	case HC_VENDOR_KVM | KVM_HC_FEATURES:
 		r = HC_EV_SUCCESS;
+#if defined(CONFIG_PPC_BOOK3S) /* XXX Missing magic page on BookE */
+		r2 |= (1 << KVM_FEATURE_MAGIC_PAGE);
+#endif
 
 		/* Second return value is in r4 */
 		kvmppc_set_gpr(vcpu, 4, r2);

commit 2a342ed57756ad5d8af5456959433884367e5ab2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:48 2010 +0200

    KVM: PPC: Implement hypervisor interface
    
    To communicate with KVM directly we need to plumb some sort of interface
    between the guest and KVM. Usually those interfaces use hypercalls.
    
    This hypercall implementation is described in the last patch of the series
    in a special documentation file. Please read that for further information.
    
    This patch implements stubs to handle KVM PPC hypercalls on the host and
    guest side alike.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 22f6fa2982f2..a4cf4b47e232 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -42,6 +42,38 @@ int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 	       !!(v->arch.pending_exceptions);
 }
 
+int kvmppc_kvm_pv(struct kvm_vcpu *vcpu)
+{
+	int nr = kvmppc_get_gpr(vcpu, 11);
+	int r;
+	unsigned long __maybe_unused param1 = kvmppc_get_gpr(vcpu, 3);
+	unsigned long __maybe_unused param2 = kvmppc_get_gpr(vcpu, 4);
+	unsigned long __maybe_unused param3 = kvmppc_get_gpr(vcpu, 5);
+	unsigned long __maybe_unused param4 = kvmppc_get_gpr(vcpu, 6);
+	unsigned long r2 = 0;
+
+	if (!(vcpu->arch.shared->msr & MSR_SF)) {
+		/* 32 bit mode */
+		param1 &= 0xffffffff;
+		param2 &= 0xffffffff;
+		param3 &= 0xffffffff;
+		param4 &= 0xffffffff;
+	}
+
+	switch (nr) {
+	case HC_VENDOR_KVM | KVM_HC_FEATURES:
+		r = HC_EV_SUCCESS;
+
+		/* Second return value is in r4 */
+		kvmppc_set_gpr(vcpu, 4, r2);
+		break;
+	default:
+		r = HC_EV_UNIMPLEMENTED;
+		break;
+	}
+
+	return r;
+}
 
 int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 {

commit 666e7252a15b7fc4a116e65deaf6da5e4ce660e3
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Jul 29 14:47:43 2010 +0200

    KVM: PPC: Convert MSR to shared page
    
    One of the most obvious registers to share with the guest directly is the
    MSR. The MSR contains the "interrupts enabled" flag which the guest has to
    toggle in critical sections.
    
    So in order to bring the overhead of interrupt en- and disabling down, let's
    put msr into the shared page. Keep in mind that even though you can fully read
    its contents, writing to it doesn't always update all state. There are a few
    safe fields that don't require hypervisor interaction. See the documentation
    for a list of MSR bits that are safe to be set from inside the guest.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 72a4ad86ee91..22f6fa2982f2 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -38,7 +38,8 @@
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
-	return !(v->arch.msr & MSR_WE) || !!(v->arch.pending_exceptions);
+	return !(v->arch.shared->msr & MSR_WE) ||
+	       !!(v->arch.pending_exceptions);
 }
 
 

commit a1f4d39500ad8ed61825eff061debff42386ab5b
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Jun 21 11:44:20 2010 +0300

    KVM: Remove memory alias support
    
    As advertised in feature-removal-schedule.txt.  Equivalent support is provided
    by overlapping memory regions.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b5ebdfbed20b..72a4ad86ee91 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -36,11 +36,6 @@
 #define CREATE_TRACE_POINTS
 #include "trace.h"
 
-gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
-{
-	return gfn;
-}
-
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
 	return !(v->arch.msr & MSR_WE) || !!(v->arch.pending_exceptions);

commit 69b61833f7ce6e61e196c8a724e4d1278b24ac02
Author: Denis Kirjanov <dkirjanov@hera.kernel.org>
Date:   Fri Jun 11 11:23:26 2010 +0000

    KVM: PPC: fix build warning in kvm_arch_vcpu_ioctl_run
    
    Fix compile warning:
      CC [M]  arch/powerpc/kvm/powerpc.o
      arch/powerpc/kvm/powerpc.c: In function 'kvm_arch_vcpu_ioctl_run':
      arch/powerpc/kvm/powerpc.c:290: warning: 'gpr' may be used uninitialized in this function
      arch/powerpc/kvm/powerpc.c:290: note: 'gpr' was declared here
    
    Signed-off-by: Denis Kirjanov <dkirjanov@kernel.org>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a1d87508892e..b5ebdfbed20b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -287,7 +287,7 @@ static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,
 static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
                                       struct kvm_run *run)
 {
-	u64 gpr;
+	u64 uninitialized_var(gpr);
 
 	if (run->mmio.len > sizeof(gpr)) {
 		printk(KERN_ERR "bad MMIO length: %d\n", run->mmio.len);

commit 93736624635235cc5372ffca6d62816d02170724
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 13 12:35:17 2010 +0300

    KVM: Consolidate arch specific vcpu ioctl locking
    
    Now that all arch specific ioctls have centralized locking, it is easy to
    move it to the central dispatcher.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index caeed7b31bf9..a1d87508892e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -512,17 +512,16 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 	long r;
 
-	if (ioctl == KVM_INTERRUPT) {
+	switch (ioctl) {
+	case KVM_INTERRUPT: {
 		struct kvm_interrupt irq;
 		r = -EFAULT;
 		if (copy_from_user(&irq, argp, sizeof(irq)))
-			goto out_nolock;
+			goto out;
 		r = kvm_vcpu_ioctl_interrupt(vcpu, &irq);
-		goto out_nolock;
+		goto out;
 	}
 
-	vcpu_load(vcpu);
-	switch (ioctl) {
 	case KVM_ENABLE_CAP:
 	{
 		struct kvm_enable_cap cap;
@@ -537,8 +536,6 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 
 out:
-	vcpu_put(vcpu);
-out_nolock:
 	return r;
 }
 

commit 19483d144023f7f4817dedafe26d5fe9ff2e7087
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 13 12:30:43 2010 +0300

    KVM: PPC: Centralize locking of arch specific vcpu ioctls
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index e0fae7a618ca..caeed7b31bf9 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -512,15 +512,17 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	void __user *argp = (void __user *)arg;
 	long r;
 
-	switch (ioctl) {
-	case KVM_INTERRUPT: {
+	if (ioctl == KVM_INTERRUPT) {
 		struct kvm_interrupt irq;
 		r = -EFAULT;
 		if (copy_from_user(&irq, argp, sizeof(irq)))
-			goto out;
+			goto out_nolock;
 		r = kvm_vcpu_ioctl_interrupt(vcpu, &irq);
-		break;
+		goto out_nolock;
 	}
+
+	vcpu_load(vcpu);
+	switch (ioctl) {
 	case KVM_ENABLE_CAP:
 	{
 		struct kvm_enable_cap cap;
@@ -535,6 +537,8 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	}
 
 out:
+	vcpu_put(vcpu);
+out_nolock:
 	return r;
 }
 

commit 2122ff5eab8faec853e43f6de886e8dc8f31e317
Author: Avi Kivity <avi@redhat.com>
Date:   Thu May 13 11:25:04 2010 +0300

    KVM: move vcpu locking to dispatcher for generic vcpu ioctls
    
    All vcpu ioctls need to be locked, so instead of locking each one specifically
    we lock at the generic dispatcher.
    
    This patch only updates generic ioctls and leaves arch specific ioctls alone.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9b8683f39e05..e0fae7a618ca 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -423,8 +423,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	int r;
 	sigset_t sigsaved;
 
-	vcpu_load(vcpu);
-
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
@@ -456,8 +454,6 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
 
-	vcpu_put(vcpu);
-
 	return r;
 }
 

commit c7f38f46f2a98d232147e47284cb4e7363296a3e
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Apr 16 00:11:40 2010 +0200

    KVM: PPC: Improve indirect svcpu accessors
    
    We already have some inline fuctions we use to access vcpu or svcpu structs,
    depending on whether we're on booke or book3s. Since we just put a few more
    registers into the svcpu, we also need to make sure the respective callbacks
    are available and get used.
    
    So this patch moves direct use of the now in the svcpu struct fields to
    inline function calls. While at it, it also moves the definition of those
    inline function calls to respective header files for booke and book3s,
    greatly improving readability.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ffbe4cac5b15..9b8683f39e05 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -70,7 +70,7 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 	case EMULATE_FAIL:
 		/* XXX Deliver Program interrupt to guest. */
 		printk(KERN_EMERG "%s: emulation failed (%08x)\n", __func__,
-		       vcpu->arch.last_inst);
+		       kvmppc_get_last_inst(vcpu));
 		r = RESUME_HOST;
 		break;
 	default:

commit 287d5611fab5a42214d028b6f67fbd2ee977dcf2
Author: Alexander Graf <agraf@suse.de>
Date:   Thu Apr 1 15:33:21 2010 +0200

    KVM: PPC: Only use QPRs when available
    
    BookE KVM doesn't know about QPRs, so let's not try to access then.
    
    This fixes a build error on BookE KVM.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9a4dd8146d39..ffbe4cac5b15 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -335,6 +335,7 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 	case KVM_REG_FPR:
 		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
 		break;
+#ifdef CONFIG_PPC_BOOK3S
 	case KVM_REG_QPR:
 		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
 		break;
@@ -342,6 +343,7 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
 		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
 		break;
+#endif
 	default:
 		BUG();
 	}

commit ad0a048b096ac819f28667602285453468a8d8f9
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Mar 24 21:48:30 2010 +0100

    KVM: PPC: Add OSI hypercall interface
    
    MOL uses its own hypercall interface to call back into userspace when
    the guest wants to do something.
    
    So let's implement that as an exit reason, specify it with a CAP and
    only really use it when userspace wants us to.
    
    The only user of it so far is MOL.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 646bfd256e5d..9a4dd8146d39 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -151,6 +151,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_UNSET_IRQ:
 	case KVM_CAP_ENABLE_CAP:
+	case KVM_CAP_PPC_OSI:
 		r = 1;
 		break;
 	case KVM_CAP_COALESCED_MMIO:
@@ -433,6 +434,13 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		if (!vcpu->arch.dcr_is_write)
 			kvmppc_complete_dcr_load(vcpu, run);
 		vcpu->arch.dcr_needed = 0;
+	} else if (vcpu->arch.osi_needed) {
+		u64 *gprs = run->osi.gprs;
+		int i;
+
+		for (i = 0; i < 32; i++)
+			kvmppc_set_gpr(vcpu, i, gprs[i]);
+		vcpu->arch.osi_needed = 0;
 	}
 
 	kvmppc_core_deliver_interrupts(vcpu);
@@ -475,6 +483,10 @@ static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
 		return -EINVAL;
 
 	switch (cap->cap) {
+	case KVM_CAP_PPC_OSI:
+		r = 0;
+		vcpu->arch.osi_enabled = true;
+		break;
 	default:
 		r = -EINVAL;
 		break;

commit 71fbfd5f38f73515f1516a68fbe04dba198b70f0
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Mar 24 21:48:29 2010 +0100

    KVM: Add support for enabling capabilities per-vcpu
    
    Some times we don't want all capabilities to be available to all
    our vcpus. One example for that is the OSI interface, implemented
    in the next patch.
    
    In order to have a generic mechanism in how to enable capabilities
    individually, this patch introduces a new ioctl that can be used
    for this purpose. That way features we don't want in all guests or
    userspace configurations can just not be enabled and we're good.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0bb6a7e826d1..646bfd256e5d 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -150,6 +150,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_PPC_SEGSTATE:
 	case KVM_CAP_PPC_PAIRED_SINGLES:
 	case KVM_CAP_PPC_UNSET_IRQ:
+	case KVM_CAP_ENABLE_CAP:
 		r = 1;
 		break;
 	case KVM_CAP_COALESCED_MMIO:
@@ -465,6 +466,23 @@ int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 	return 0;
 }
 
+static int kvm_vcpu_ioctl_enable_cap(struct kvm_vcpu *vcpu,
+				     struct kvm_enable_cap *cap)
+{
+	int r;
+
+	if (cap->flags)
+		return -EINVAL;
+
+	switch (cap->cap) {
+	default:
+		r = -EINVAL;
+		break;
+	}
+
+	return r;
+}
+
 int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
                                     struct kvm_mp_state *mp_state)
 {
@@ -493,6 +511,15 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 		r = kvm_vcpu_ioctl_interrupt(vcpu, &irq);
 		break;
 	}
+	case KVM_ENABLE_CAP:
+	{
+		struct kvm_enable_cap cap;
+		r = -EFAULT;
+		if (copy_from_user(&cap, argp, sizeof(cap)))
+			goto out;
+		r = kvm_vcpu_ioctl_enable_cap(vcpu, &cap);
+		break;
+	}
 	default:
 		r = -EINVAL;
 	}

commit 18978768d89f638165646718c50ced19f2a10164
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Mar 24 21:48:18 2010 +0100

    KVM: PPC: Allow userspace to unset the IRQ line
    
    Userspace can tell us that it wants to trigger an interrupt. But
    so far it can't tell us that it wants to stop triggering one.
    
    So let's interpret the parameter to the ioctl that we have anyways
    to tell us if we want to raise or lower the interrupt line.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    
    v2 -> v3:
    
     - Add CAP for unset irq
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 3f8677e9d8f9..0bb6a7e826d1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -149,6 +149,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 	switch (ext) {
 	case KVM_CAP_PPC_SEGSTATE:
 	case KVM_CAP_PPC_PAIRED_SINGLES:
+	case KVM_CAP_PPC_UNSET_IRQ:
 		r = 1;
 		break;
 	case KVM_CAP_COALESCED_MMIO:
@@ -451,7 +452,10 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 {
-	kvmppc_core_queue_external(vcpu, irq);
+	if (irq->irq == KVM_INTERRUPT_UNSET)
+		kvmppc_core_dequeue_external(vcpu, irq);
+	else
+		kvmppc_core_queue_external(vcpu, irq);
 
 	if (waitqueue_active(&vcpu->wq)) {
 		wake_up_interruptible(&vcpu->wq);

commit 06056bfb944a0302a8f22eb45f09123de7fb417b
Author: Wei Yongjun <yjwei@cn.fujitsu.com>
Date:   Tue Mar 9 14:13:43 2010 +0800

    KVM: PPC: Do not create debugfs if fail to create vcpu
    
    If fail to create the vcpu, we should not create the debugfs
    for it.
    
    Signed-off-by: Wei Yongjun <yjwei@cn.fujitsu.com>
    Acked-by: Alexander Graf <agraf@suse.de>
    Cc: stable@kernel.org
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ace31ca05245..3f8677e9d8f9 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -194,7 +194,8 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
 	struct kvm_vcpu *vcpu;
 	vcpu = kvmppc_core_vcpu_create(kvm, id);
-	kvmppc_create_vcpu_debugfs(vcpu, id);
+	if (!IS_ERR(vcpu))
+		kvmppc_create_vcpu_debugfs(vcpu, id);
 	return vcpu;
 }
 

commit a595405df9efb89710cd555d29df0e4902f90613
Author: Alexander Graf <alex@csgraf.de>
Date:   Mon Feb 22 16:52:14 2010 +0100

    KVM: PPC: Destory timer on vcpu destruction
    
    When we destory a vcpu, we should also make sure to kill all pending
    timers that could still be up. When not doing this, hrtimers might
    dereference null pointers trying to call our code.
    
    This patch fixes spontanious kernel panics seen after closing VMs.
    
    Signed-off-by: Alexander Graf <alex@csgraf.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index ad2b6275acb7..ace31ca05245 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -200,6 +200,10 @@ struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 
 void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 {
+	/* Make sure we're not using the vcpu anymore */
+	hrtimer_cancel(&vcpu->arch.dec_timer);
+	tasklet_kill(&vcpu->arch.tasklet);
+
 	kvmppc_remove_vcpu_debugfs(vcpu);
 	kvmppc_core_vcpu_free(vcpu);
 }

commit c10207fe86b1761c3ad135eb922fdb41bbde3025
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:45 2010 +0100

    KVM: PPC: Add capability for paired singles
    
    We need to tell userspace that we can emulate paired single instructions.
    So let's add a capability export.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1266ed02b471..ad2b6275acb7 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -148,6 +148,7 @@ int kvm_dev_ioctl_check_extension(long ext)
 
 	switch (ext) {
 	case KVM_CAP_PPC_SEGSTATE:
+	case KVM_CAP_PPC_PAIRED_SINGLES:
 		r = 1;
 		break;
 	case KVM_CAP_COALESCED_MMIO:

commit 3587d5348ced089666c51411bd9d771fb0b072cf
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:30 2010 +0100

    KVM: PPC: Teach MMIO Signedness
    
    The guest I was trying to get to run uses the LHA and LHAU instructions.
    Those instructions basically do a load, but also sign extend the result.
    
    Since we need to fill our registers by hand when doing MMIO, we also need
    to sign extend manually.
    
    This patch implements sign extended MMIO and the LHA(U) instructions.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b7858b1e15ec..1266ed02b471 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -301,6 +301,22 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 		}
 	}
 
+	if (vcpu->arch.mmio_sign_extend) {
+		switch (run->mmio.len) {
+#ifdef CONFIG_PPC64
+		case 4:
+			gpr = (s64)(s32)gpr;
+			break;
+#endif
+		case 2:
+			gpr = (s64)(s16)gpr;
+			break;
+		case 1:
+			gpr = (s64)(s8)gpr;
+			break;
+		}
+	}
+
 	kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
 
 	switch (vcpu->arch.io_gpr & KVM_REG_EXT_MASK) {
@@ -338,10 +354,23 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	vcpu->arch.mmio_is_bigendian = is_bigendian;
 	vcpu->mmio_needed = 1;
 	vcpu->mmio_is_write = 0;
+	vcpu->arch.mmio_sign_extend = 0;
 
 	return EMULATE_DO_MMIO;
 }
 
+/* Same as above, but sign extends */
+int kvmppc_handle_loads(struct kvm_run *run, struct kvm_vcpu *vcpu,
+                        unsigned int rt, unsigned int bytes, int is_bigendian)
+{
+	int r;
+
+	r = kvmppc_handle_load(run, vcpu, rt, bytes, is_bigendian);
+	vcpu->arch.mmio_sign_extend = 1;
+
+	return r;
+}
+
 int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
                         u64 val, unsigned int bytes, int is_bigendian)
 {

commit b104d06632d08957f384ff7403f609fb5dfb9cbd
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Feb 19 11:00:29 2010 +0100

    KVM: PPC: Enable MMIO to do 64 bits, fprs and qprs
    
    Right now MMIO access can only happen for GPRs and is at most 32 bit wide.
    That's actually enough for almost all types of hardware out there.
    
    Unfortunately, the guest I was using used FPU writes to MMIO regions, so
    it ended up writing 64 bit MMIOs using FPRs and QPRs.
    
    So let's add code to handle those odd cases too.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 297fcd2ff7d0..b7858b1e15ec 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -278,7 +278,7 @@ static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,
 static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
                                       struct kvm_run *run)
 {
-	ulong gpr;
+	u64 gpr;
 
 	if (run->mmio.len > sizeof(gpr)) {
 		printk(KERN_ERR "bad MMIO length: %d\n", run->mmio.len);
@@ -287,6 +287,7 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 
 	if (vcpu->arch.mmio_is_bigendian) {
 		switch (run->mmio.len) {
+		case 8: gpr = *(u64 *)run->mmio.data; break;
 		case 4: gpr = *(u32 *)run->mmio.data; break;
 		case 2: gpr = *(u16 *)run->mmio.data; break;
 		case 1: gpr = *(u8 *)run->mmio.data; break;
@@ -301,6 +302,24 @@ static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
 	}
 
 	kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
+
+	switch (vcpu->arch.io_gpr & KVM_REG_EXT_MASK) {
+	case KVM_REG_GPR:
+		kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
+		break;
+	case KVM_REG_FPR:
+		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
+		break;
+	case KVM_REG_QPR:
+		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
+		break;
+	case KVM_REG_FQPR:
+		vcpu->arch.fpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
+		vcpu->arch.qpr[vcpu->arch.io_gpr & KVM_REG_MASK] = gpr;
+		break;
+	default:
+		BUG();
+	}
 }
 
 int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
@@ -324,7 +343,7 @@ int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
 }
 
 int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
-                        u32 val, unsigned int bytes, int is_bigendian)
+                        u64 val, unsigned int bytes, int is_bigendian)
 {
 	void *data = run->mmio.data;
 
@@ -342,6 +361,7 @@ int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
 	/* Store the value at the lowest bytes in 'data'. */
 	if (is_bigendian) {
 		switch (bytes) {
+		case 8: *(u64 *)data = val; break;
 		case 4: *(u32 *)data = val; break;
 		case 2: *(u16 *)data = val; break;
 		case 1: *(u8  *)data = val; break;

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 51aedd7f16bc..297fcd2ff7d0 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -25,6 +25,7 @@
 #include <linux/vmalloc.h>
 #include <linux/hrtimer.h>
 #include <linux/fs.h>
+#include <linux/slab.h>
 #include <asm/cputable.h>
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>

commit 647492047763c3ee8fe51ecf9a04f39040aa495b
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Tue Jan 19 12:45:23 2010 -0200

    KVM: fix cleanup_srcu_struct on vm destruction
    
    cleanup_srcu_struct on VM destruction remains broken:
    
    BUG: unable to handle kernel paging request at ffffffffffffffff
    IP: [<ffffffff802533d2>] srcu_read_lock+0x16/0x21
    RIP: 0010:[<ffffffff802533d2>]  [<ffffffff802533d2>] srcu_read_lock+0x16/0x21
    Call Trace:
     [<ffffffffa05354c4>] kvm_arch_vcpu_uninit+0x1b/0x48 [kvm]
     [<ffffffffa05339c6>] kvm_vcpu_uninit+0x9/0x15 [kvm]
     [<ffffffffa0569f7d>] vmx_free_vcpu+0x7f/0x8f [kvm_intel]
     [<ffffffffa05357b5>] kvm_arch_destroy_vm+0x78/0x111 [kvm]
     [<ffffffffa053315b>] kvm_put_kvm+0xd4/0xfe [kvm]
    
    Move it to kvm_arch_destroy_vm.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Reported-by: Jan Kiszka <jan.kiszka@siemens.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2c291161df89..51aedd7f16bc 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -137,6 +137,7 @@ void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvmppc_free_vcpus(kvm);
 	kvm_free_physmem(kvm);
+	cleanup_srcu_struct(&kvm->srcu);
 	kfree(kvm);
 }
 

commit 8e5b26b55a8b6aee2c789b1d20ec715f9e4bea5c
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Jan 8 02:58:01 2010 +0100

    KVM: PPC: Use accessor functions for GPR access
    
    All code in PPC KVM currently accesses gprs in the vcpu struct directly.
    
    While there's nothing wrong with that wrt the current way gprs are stored
    and loaded, it doesn't suffice for the PACA acceleration that will follow
    in this patchset.
    
    So let's just create little wrapper inline functions that we call whenever
    a GPR needs to be read from or written to. The compiled code shouldn't really
    change at all for now.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 4633e7850dd2..2c291161df89 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -270,34 +270,35 @@ int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
 static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,
                                      struct kvm_run *run)
 {
-	ulong *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
-	*gpr = run->dcr.data;
+	kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, run->dcr.data);
 }
 
 static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
                                       struct kvm_run *run)
 {
-	ulong *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
+	ulong gpr;
 
-	if (run->mmio.len > sizeof(*gpr)) {
+	if (run->mmio.len > sizeof(gpr)) {
 		printk(KERN_ERR "bad MMIO length: %d\n", run->mmio.len);
 		return;
 	}
 
 	if (vcpu->arch.mmio_is_bigendian) {
 		switch (run->mmio.len) {
-		case 4: *gpr = *(u32 *)run->mmio.data; break;
-		case 2: *gpr = *(u16 *)run->mmio.data; break;
-		case 1: *gpr = *(u8 *)run->mmio.data; break;
+		case 4: gpr = *(u32 *)run->mmio.data; break;
+		case 2: gpr = *(u16 *)run->mmio.data; break;
+		case 1: gpr = *(u8 *)run->mmio.data; break;
 		}
 	} else {
 		/* Convert BE data from userland back to LE. */
 		switch (run->mmio.len) {
-		case 4: *gpr = ld_le32((u32 *)run->mmio.data); break;
-		case 2: *gpr = ld_le16((u16 *)run->mmio.data); break;
-		case 1: *gpr = *(u8 *)run->mmio.data; break;
+		case 4: gpr = ld_le32((u32 *)run->mmio.data); break;
+		case 2: gpr = ld_le16((u16 *)run->mmio.data); break;
+		case 1: gpr = *(u8 *)run->mmio.data; break;
 		}
 	}
+
+	kvmppc_set_gpr(vcpu, vcpu->arch.io_gpr, gpr);
 }
 
 int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,

commit f7784b8ec9b6a041fa828cfbe9012fe51933f5ac
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Wed Dec 23 14:35:18 2009 -0200

    KVM: split kvm_arch_set_memory_region into prepare and commit
    
    Required for SRCU convertion later.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index f06cf93b178e..4633e7850dd2 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -165,14 +165,24 @@ long kvm_arch_dev_ioctl(struct file *filp,
 	return -EINVAL;
 }
 
-int kvm_arch_set_memory_region(struct kvm *kvm,
-                               struct kvm_userspace_memory_region *mem,
-                               struct kvm_memory_slot old,
-                               int user_alloc)
+int kvm_arch_prepare_memory_region(struct kvm *kvm,
+                                   struct kvm_memory_slot *memslot,
+                                   struct kvm_memory_slot old,
+                                   struct kvm_userspace_memory_region *mem,
+                                   int user_alloc)
 {
 	return 0;
 }
 
+void kvm_arch_commit_memory_region(struct kvm *kvm,
+               struct kvm_userspace_memory_region *mem,
+               struct kvm_memory_slot old,
+               int user_alloc)
+{
+       return;
+}
+
+
 void kvm_arch_flush_shadow(struct kvm *kvm)
 {
 }

commit bcd6acd51f3d4d1ada201e9bc5c40a31d6d80c71
Merge: 11c34c7deaee 3ff6a468b45b
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Dec 9 17:14:38 2009 +1100

    Merge commit 'origin/master' into next
    
    Conflicts:
            include/linux/kvm.h

commit e15a113700324f7fdcee95589875daed2b98a2fe
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Nov 30 03:02:02 2009 +0000

    powerpc/kvm: Sync guest visible MMU state
    
    Currently userspace has no chance to find out which virtual address space we're
    in and resolve addresses. While that is a big problem for migration, it's also
    unpleasent when debugging, as gdb and the monitor don't work on virtual
    addresses.
    
    This patch exports enough of the MMU segment state to userspace to make
    debugging work and thus also includes the groundwork for migration.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 692c3709011e..d82551efbfbf 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -144,6 +144,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	int r;
 
 	switch (ext) {
+	case KVM_CAP_PPC_SEGSTATE:
+		r = 1;
+		break;
 	case KVM_CAP_COALESCED_MMIO:
 		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 		break;

commit 10474ae8945ce08622fd1f3464e55bd817bf2376
Author: Alexander Graf <agraf@suse.de>
Date:   Tue Sep 15 11:37:46 2009 +0200

    KVM: Activate Virtualization On Demand
    
    X86 CPUs need to have some magic happening to enable the virtualization
    extensions on them. This magic can result in unpleasant results for
    users, like blocking other VMMs from working (vmx) or using invalid TLB
    entries (svm).
    
    Currently KVM activates virtualization when the respective kernel module
    is loaded. This blocks us from autoloading KVM modules without breaking
    other VMMs.
    
    To circumvent this problem at least a bit, this patch introduces on
    demand activation of virtualization. This means, that instead
    virtualization is enabled on creation of the first virtual machine
    and disabled on destruction of the last one.
    
    So using this, KVM can be easily autoloaded, while keeping other
    hypervisors usable.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 95af62217b6b..5902bbc2411e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -78,8 +78,9 @@ int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
 	return r;
 }
 
-void kvm_arch_hardware_enable(void *garbage)
+int kvm_arch_hardware_enable(void *garbage)
 {
+	return 0;
 }
 
 void kvm_arch_hardware_disable(void *garbage)

commit 367e1319b229110a27c53221c2fa32a6aa86d4a9
Author: Avi Kivity <avi@redhat.com>
Date:   Wed Aug 26 14:57:07 2009 +0300

    KVM: Return -ENOTTY on unrecognized ioctls
    
    Not the incorrect -EINVAL.
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2a4551f78f60..95af62217b6b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -421,7 +421,7 @@ long kvm_arch_vm_ioctl(struct file *filp,
 
 	switch (ioctl) {
 	default:
-		r = -EINVAL;
+		r = -ENOTTY;
 	}
 
 	return r;

commit 544c6761bb05a1dd19a39cb9bed096273f9bdb36
Author: Alexander Graf <agraf@suse.de>
Date:   Mon Nov 2 12:02:31 2009 +0000

    Use hrtimers for the decrementer
    
    Following S390's good example we should use hrtimers for the decrementer too!
    This patch converts the timer from the old mechanism to hrtimers.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index a06ecc3401fd..692c3709011e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -23,6 +23,7 @@
 #include <linux/kvm_host.h>
 #include <linux/module.h>
 #include <linux/vmalloc.h>
+#include <linux/hrtimer.h>
 #include <linux/fs.h>
 #include <asm/cputable.h>
 #include <asm/uaccess.h>
@@ -208,10 +209,25 @@ static void kvmppc_decrementer_func(unsigned long data)
 	}
 }
 
+/*
+ * low level hrtimer wake routine. Because this runs in hardirq context
+ * we schedule a tasklet to do the real work.
+ */
+enum hrtimer_restart kvmppc_decrementer_wakeup(struct hrtimer *timer)
+{
+	struct kvm_vcpu *vcpu;
+
+	vcpu = container_of(timer, struct kvm_vcpu, arch.dec_timer);
+	tasklet_schedule(&vcpu->arch.tasklet);
+
+	return HRTIMER_NORESTART;
+}
+
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 {
-	setup_timer(&vcpu->arch.dec_timer, kvmppc_decrementer_func,
-	            (unsigned long)vcpu);
+	hrtimer_init(&vcpu->arch.dec_timer, CLOCK_REALTIME, HRTIMER_MODE_ABS);
+	tasklet_init(&vcpu->arch.tasklet, kvmppc_decrementer_func, (ulong)vcpu);
+	vcpu->arch.dec_timer.function = kvmppc_decrementer_wakeup;
 
 	return 0;
 }

commit 4e755758cbbeb0afe44556c1ce52ea6f590ed5bf
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Oct 30 05:47:01 2009 +0000

    Move dirty logging code to sub-arch
    
    PowerPC code handles dirty logging in the generic parts atm. While this
    is great for "return -ENOTSUPP", we need to be rather target specific
    when actually implementing it.
    
    So let's split it to implementation specific code, so we can implement
    it for book3s.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2a4551f78f60..a06ecc3401fd 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -409,11 +409,6 @@ long kvm_arch_vcpu_ioctl(struct file *filp,
 	return r;
 }
 
-int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
-{
-	return -ENOTSUPP;
-}
-
 long kvm_arch_vm_ioctl(struct file *filp,
                        unsigned int ioctl, unsigned long arg)
 {

commit a1b37100d9e29c1f8dc3e2f5490a205c80180e01
Author: Gleb Natapov <gleb@redhat.com>
Date:   Thu Jul 9 15:33:52 2009 +0300

    KVM: Reduce runnability interface with arch support code
    
    Remove kvm_cpu_has_interrupt() and kvm_arch_interrupt_allowed() from
    interface between general code and arch code. kvm_arch_vcpu_runnable()
    checks for interrupts instead.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0341391eff12..2a4551f78f60 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -39,20 +39,9 @@ gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
 	return gfn;
 }
 
-int kvm_cpu_has_interrupt(struct kvm_vcpu *v)
-{
-	return !!(v->arch.pending_exceptions);
-}
-
-int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu)
-{
-	/* do real check here */
-	return 1;
-}
-
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
-	return !(v->arch.msr & MSR_WE);
+	return !(v->arch.msr & MSR_WE) || !!(v->arch.pending_exceptions);
 }
 
 

commit 46f43c6ee022c3aeb9686b104234b9f27fac03c2
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jun 18 11:47:27 2009 -0300

    KVM: powerpc: convert marker probes to event trace
    
    [avi: make it build]
    [avi: fold trace-arch.h into trace.h]
    
    CC: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7ad30e0a1b9a..0341391eff12 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -31,6 +31,9 @@
 #include "timing.h"
 #include "../mm/mmu_decl.h"
 
+#define CREATE_TRACE_POINTS
+#include "trace.h"
+
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
 {
 	return gfn;

commit 988a2cae6a3c0dea6df59808a935a9a697bfc28c
Author: Gleb Natapov <gleb@redhat.com>
Date:   Tue Jun 9 15:56:29 2009 +0300

    KVM: Use macro to iterate over vcpus.
    
    [christian: remove unused variables on s390]
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Christian Borntraeger <borntraeger@de.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2cf915e51e7e..7ad30e0a1b9a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -122,13 +122,17 @@ struct kvm *kvm_arch_create_vm(void)
 static void kvmppc_free_vcpus(struct kvm *kvm)
 {
 	unsigned int i;
+	struct kvm_vcpu *vcpu;
 
-	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
-		if (kvm->vcpus[i]) {
-			kvm_arch_vcpu_free(kvm->vcpus[i]);
-			kvm->vcpus[i] = NULL;
-		}
-	}
+	kvm_for_each_vcpu(i, vcpu, kvm)
+		kvm_arch_vcpu_free(vcpu);
+
+	mutex_lock(&kvm->lock);
+	for (i = 0; i < atomic_read(&kvm->online_vcpus); i++)
+		kvm->vcpus[i] = NULL;
+
+	atomic_set(&kvm->online_vcpus, 0);
+	mutex_unlock(&kvm->lock);
 }
 
 void kvm_arch_sync_events(struct kvm *kvm)

commit 78646121e9a2fcf7977cc15966420e572a450bc3
Author: Gleb Natapov <gleb@redhat.com>
Date:   Mon Mar 23 12:12:11 2009 +0200

    KVM: Fix interrupt unhalting a vcpu when it shouldn't
    
    kvm_vcpu_block() unhalts vpu on an interrupt/timer without checking
    if interrupt window is actually opened.
    
    Signed-off-by: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 9057335fdc61..2cf915e51e7e 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -41,6 +41,12 @@ int kvm_cpu_has_interrupt(struct kvm_vcpu *v)
 	return !!(v->arch.pending_exceptions);
 }
 
+int kvm_arch_interrupt_allowed(struct kvm_vcpu *vcpu)
+{
+	/* do real check here */
+	return 1;
+}
+
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
 	return !(v->arch.msr & MSR_WE);

commit f5d0906b5bafd7faea553ed1cc92bd06755b66b9
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sun Jan 4 13:51:09 2009 -0600

    KVM: ppc: remove debug support broken by KVM debug rewrite
    
    After the rewrite of KVM's debug support, this code doesn't even build any
    more.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index f30be9ef2314..9057335fdc61 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -221,41 +221,18 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
-	if (vcpu->guest_debug.enabled)
-		kvmppc_core_load_guest_debugstate(vcpu);
-
 	kvmppc_core_vcpu_load(vcpu, cpu);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
-	if (vcpu->guest_debug.enabled)
-		kvmppc_core_load_host_debugstate(vcpu);
-
-	/* Don't leave guest TLB entries resident when being de-scheduled. */
-	/* XXX It would be nice to differentiate between heavyweight exit and
-	 * sched_out here, since we could avoid the TLB flush for heavyweight
-	 * exits. */
-	_tlbil_all();
 	kvmppc_core_vcpu_put(vcpu);
 }
 
 int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
-					struct kvm_guest_debug *dbg)
+                                        struct kvm_guest_debug *dbg)
 {
-	int i;
-
-	vcpu->guest_debug.enabled = dbg->enabled;
-	if (vcpu->guest_debug.enabled) {
-		for (i=0; i < ARRAY_SIZE(vcpu->guest_debug.bp); i++) {
-			if (dbg->breakpoints[i].enabled)
-				vcpu->guest_debug.bp[i] = dbg->breakpoints[i].address;
-			else
-				vcpu->guest_debug.bp[i] = 0;
-		}
-	}
-
-	return 0;
+	return -EINVAL;
 }
 
 static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,

commit ecc0981ff07cbe7cdf95de20be5b24fee8e49cb5
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Sat Jan 3 16:22:59 2009 -0600

    KVM: ppc: cosmetic changes to mmu hook names
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7c2ad4017d6a..f30be9ef2314 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -216,7 +216,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 {
-	kvmppc_core_destroy_mmu(vcpu);
+	kvmppc_mmu_destroy(vcpu);
 }
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)

commit d0bfb940ecabf0b44fb1fd80d8d60594e569e5ec
Author: Jan Kiszka <jan.kiszka@siemens.com>
Date:   Mon Dec 15 13:52:10 2008 +0100

    KVM: New guest debug interface
    
    This rips out the support for KVM_DEBUG_GUEST and introduces a new IOCTL
    instead: KVM_SET_GUEST_DEBUG. The IOCTL payload consists of a generic
    part, controlling the "main switch" and the single-step feature. The
    arch specific part adds an x86 interface for intercepting both types of
    debug exceptions separately and re-injecting them when the host was not
    interested. Moveover, the foundation for guest debugging via debug
    registers is layed.
    
    To signal breakpoint events properly back to userland, an arch-specific
    data block is now returned along KVM_EXIT_DEBUG. For x86, the arch block
    contains the PC, the debug exception, and relevant debug registers to
    tell debug events properly apart.
    
    The availability of this new interface is signaled by
    KVM_CAP_SET_GUEST_DEBUG. Empty stubs for not yet supported archs are
    provided.
    
    Note that both SVM and VTX are supported, but only the latter was tested
    yet. Based on the experience with all those VTX corner case, I would be
    fairly surprised if SVM will work out of the box.
    
    Signed-off-by: Jan Kiszka <jan.kiszka@siemens.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 5f81256287f5..7c2ad4017d6a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -240,8 +240,8 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	kvmppc_core_vcpu_put(vcpu);
 }
 
-int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
-                                    struct kvm_debug_guest *dbg)
+int kvm_arch_vcpu_ioctl_set_guest_debug(struct kvm_vcpu *vcpu,
+					struct kvm_guest_debug *dbg)
 {
 	int i;
 

commit ad8ba2cd44d4d39fb3fe55d5dcc565b19fc3a7fb
Author: Sheng Yang <sheng@linux.intel.com>
Date:   Tue Jan 6 10:03:02 2009 +0800

    KVM: Add kvm_arch_sync_events to sync with asynchronize events
    
    kvm_arch_sync_events is introduced to quiet down all other events may happen
    contemporary with VM destroy process, like IRQ handler and work struct for
    assigned device.
    
    For kvm_arch_sync_events is called at the very beginning of kvm_destroy_vm(), so
    the state of KVM here is legal and can provide a environment to quiet down other
    events.
    
    Signed-off-by: Sheng Yang <sheng@linux.intel.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 2822c8ccfaaf..5f81256287f5 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -125,6 +125,10 @@ static void kvmppc_free_vcpus(struct kvm *kvm)
 	}
 }
 
+void kvm_arch_sync_events(struct kvm *kvm)
+{
+}
+
 void kvm_arch_destroy_vm(struct kvm *kvm)
 {
 	kvmppc_free_vcpus(kvm);

commit ca9edaee1aea34ebd9adb48910aba0b3d64b1b22
Author: Avi Kivity <avi@redhat.com>
Date:   Mon Dec 8 18:29:29 2008 +0200

    KVM: Consolidate userspace memory capability reporting into common code
    
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 1deda37cb771..2822c8ccfaaf 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -137,9 +137,6 @@ int kvm_dev_ioctl_check_extension(long ext)
 	int r;
 
 	switch (ext) {
-	case KVM_CAP_USER_MEMORY:
-		r = 1;
-		break;
 	case KVM_CAP_COALESCED_MMIO:
 		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
 		break;

commit 73e75b416ffcfa3a84952d8e389a0eca080f00e1
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Tue Dec 2 15:51:57 2008 -0600

    KVM: ppc: Implement in-kernel exit timing statistics
    
    Existing KVM statistics are either just counters (kvm_stat) reported for
    KVM generally or trace based aproaches like kvm_trace.
    For KVM on powerpc we had the need to track the timings of the different exit
    types. While this could be achieved parsing data created with a kvm_trace
    extension this adds too much overhead (at least on embedded PowerPC) slowing
    down the workloads we wanted to measure.
    
    Therefore this patch adds a in-kernel exit timing statistic to the powerpc kvm
    code. These statistic is available per vm&vcpu under the kvm debugfs directory.
    As this statistic is low, but still some overhead it can be enabled via a
    .config entry and should be off by default.
    
    Since this patch touched all powerpc kvm_stat code anyway this code is now
    merged and simplified together with the exit timing statistic code (still
    working with exit timing disabled in .config).
    
    Signed-off-by: Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 7ad150e0fbbf..1deda37cb771 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -28,9 +28,9 @@
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>
 #include <asm/tlbflush.h>
+#include "timing.h"
 #include "../mm/mmu_decl.h"
 
-
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
 {
 	return gfn;
@@ -171,11 +171,15 @@ void kvm_arch_flush_shadow(struct kvm *kvm)
 
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
-	return kvmppc_core_vcpu_create(kvm, id);
+	struct kvm_vcpu *vcpu;
+	vcpu = kvmppc_core_vcpu_create(kvm, id);
+	kvmppc_create_vcpu_debugfs(vcpu, id);
+	return vcpu;
 }
 
 void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 {
+	kvmppc_remove_vcpu_debugfs(vcpu);
 	kvmppc_core_vcpu_free(vcpu);
 }
 

commit 5cf8ca22146fa106f3bb865631ec04f5b499508f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:19 2008 -0600

    KVM: ppc: adjust vcpu types to support 64-bit cores
    
    However, some of these fields could be split into separate per-core structures
    in the future.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 237f3ba68d27..7ad150e0fbbf 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -256,14 +256,14 @@ int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
 static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,
                                      struct kvm_run *run)
 {
-	u32 *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
+	ulong *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
 	*gpr = run->dcr.data;
 }
 
 static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
                                       struct kvm_run *run)
 {
-	u32 *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
+	ulong *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
 
 	if (run->mmio.len > sizeof(*gpr)) {
 		printk(KERN_ERR "bad MMIO length: %d\n", run->mmio.len);

commit db93f5745d836f81cef0b4101a7c2685eeb55efb
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:18 2008 -0600

    KVM: ppc: create struct kvm_vcpu_44x and introduce container_of() accessor
    
    This patch doesn't yet move all 44x-specific data into the new structure, but
    is the first step down that path. In the future we may also want to create a
    struct kvm_vcpu_booke.
    
    Based on patch from Liu Yu <yu.liu@freescale.com>.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8d0aaf96d838..237f3ba68d27 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -171,31 +171,12 @@ void kvm_arch_flush_shadow(struct kvm *kvm)
 
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
-	struct kvm_vcpu *vcpu;
-	int err;
-
-	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
-	if (!vcpu) {
-		err = -ENOMEM;
-		goto out;
-	}
-
-	err = kvm_vcpu_init(vcpu, kvm, id);
-	if (err)
-		goto free_vcpu;
-
-	return vcpu;
-
-free_vcpu:
-	kmem_cache_free(kvm_vcpu_cache, vcpu);
-out:
-	return ERR_PTR(err);
+	return kvmppc_core_vcpu_create(kvm, id);
 }
 
 void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
 {
-	kvm_vcpu_uninit(vcpu);
-	kmem_cache_free(kvm_vcpu_cache, vcpu);
+	kvmppc_core_vcpu_free(vcpu);
 }
 
 void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)

commit 9dd921cfea734409a931ccc6eafd7f09850311e9
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Nov 5 09:36:14 2008 -0600

    KVM: ppc: Refactor powerpc.c to relocate 440-specific code
    
    This introduces a set of core-provided hooks. For 440, some of these are
    implemented by booke.c, with the rest in (the new) 44x.c.
    
    Note that these hooks are link-time, not run-time. Since it is not possible to
    build a single kernel for both e500 and 440 (for example), using function
    pointers would only add overhead.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 8bef0efcdfe1..8d0aaf96d838 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -99,14 +99,7 @@ void kvm_arch_hardware_unsetup(void)
 
 void kvm_arch_check_processor_compat(void *rtn)
 {
-	int r;
-
-	if (strcmp(cur_cpu_spec->platform, "ppc440") == 0)
-		r = 0;
-	else
-		r = -ENOTSUPP;
-
-	*(int *)rtn = r;
+	*(int *)rtn = kvmppc_core_check_processor_compat();
 }
 
 struct kvm *kvm_arch_create_vm(void)
@@ -212,16 +205,14 @@ void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
 
 int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
 {
-	unsigned int priority = exception_priority[BOOKE_INTERRUPT_DECREMENTER];
-
-	return test_bit(priority, &vcpu->arch.pending_exceptions);
+	return kvmppc_core_pending_dec(vcpu);
 }
 
 static void kvmppc_decrementer_func(unsigned long data)
 {
 	struct kvm_vcpu *vcpu = (struct kvm_vcpu *)data;
 
-	kvmppc_queue_exception(vcpu, BOOKE_INTERRUPT_DECREMENTER);
+	kvmppc_core_queue_dec(vcpu);
 
 	if (waitqueue_active(&vcpu->wq)) {
 		wake_up_interruptible(&vcpu->wq);
@@ -242,96 +233,25 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 	kvmppc_core_destroy_mmu(vcpu);
 }
 
-/* Note: clearing MSR[DE] just means that the debug interrupt will not be
- * delivered *immediately*. Instead, it simply sets the appropriate DBSR bits.
- * If those DBSR bits are still set when MSR[DE] is re-enabled, the interrupt
- * will be delivered as an "imprecise debug event" (which is indicated by
- * DBSR[IDE].
- */
-static void kvmppc_disable_debug_interrupts(void)
-{
-	mtmsr(mfmsr() & ~MSR_DE);
-}
-
-static void kvmppc_restore_host_debug_state(struct kvm_vcpu *vcpu)
-{
-	kvmppc_disable_debug_interrupts();
-
-	mtspr(SPRN_IAC1, vcpu->arch.host_iac[0]);
-	mtspr(SPRN_IAC2, vcpu->arch.host_iac[1]);
-	mtspr(SPRN_IAC3, vcpu->arch.host_iac[2]);
-	mtspr(SPRN_IAC4, vcpu->arch.host_iac[3]);
-	mtspr(SPRN_DBCR1, vcpu->arch.host_dbcr1);
-	mtspr(SPRN_DBCR2, vcpu->arch.host_dbcr2);
-	mtspr(SPRN_DBCR0, vcpu->arch.host_dbcr0);
-	mtmsr(vcpu->arch.host_msr);
-}
-
-static void kvmppc_load_guest_debug_registers(struct kvm_vcpu *vcpu)
-{
-	struct kvm_guest_debug *dbg = &vcpu->guest_debug;
-	u32 dbcr0 = 0;
-
-	vcpu->arch.host_msr = mfmsr();
-	kvmppc_disable_debug_interrupts();
-
-	/* Save host debug register state. */
-	vcpu->arch.host_iac[0] = mfspr(SPRN_IAC1);
-	vcpu->arch.host_iac[1] = mfspr(SPRN_IAC2);
-	vcpu->arch.host_iac[2] = mfspr(SPRN_IAC3);
-	vcpu->arch.host_iac[3] = mfspr(SPRN_IAC4);
-	vcpu->arch.host_dbcr0 = mfspr(SPRN_DBCR0);
-	vcpu->arch.host_dbcr1 = mfspr(SPRN_DBCR1);
-	vcpu->arch.host_dbcr2 = mfspr(SPRN_DBCR2);
-
-	/* set registers up for guest */
-
-	if (dbg->bp[0]) {
-		mtspr(SPRN_IAC1, dbg->bp[0]);
-		dbcr0 |= DBCR0_IAC1 | DBCR0_IDM;
-	}
-	if (dbg->bp[1]) {
-		mtspr(SPRN_IAC2, dbg->bp[1]);
-		dbcr0 |= DBCR0_IAC2 | DBCR0_IDM;
-	}
-	if (dbg->bp[2]) {
-		mtspr(SPRN_IAC3, dbg->bp[2]);
-		dbcr0 |= DBCR0_IAC3 | DBCR0_IDM;
-	}
-	if (dbg->bp[3]) {
-		mtspr(SPRN_IAC4, dbg->bp[3]);
-		dbcr0 |= DBCR0_IAC4 | DBCR0_IDM;
-	}
-
-	mtspr(SPRN_DBCR0, dbcr0);
-	mtspr(SPRN_DBCR1, 0);
-	mtspr(SPRN_DBCR2, 0);
-}
-
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
-	int i;
-
 	if (vcpu->guest_debug.enabled)
-		kvmppc_load_guest_debug_registers(vcpu);
+		kvmppc_core_load_guest_debugstate(vcpu);
 
-	/* Mark every guest entry in the shadow TLB entry modified, so that they
-	 * will all be reloaded on the next vcpu run (instead of being
-	 * demand-faulted). */
-	for (i = 0; i <= tlb_44x_hwater; i++)
-		kvmppc_tlbe_set_modified(vcpu, i);
+	kvmppc_core_vcpu_load(vcpu, cpu);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->guest_debug.enabled)
-		kvmppc_restore_host_debug_state(vcpu);
+		kvmppc_core_load_host_debugstate(vcpu);
 
 	/* Don't leave guest TLB entries resident when being de-scheduled. */
 	/* XXX It would be nice to differentiate between heavyweight exit and
 	 * sched_out here, since we could avoid the TLB flush for heavyweight
 	 * exits. */
 	_tlbil_all();
+	kvmppc_core_vcpu_put(vcpu);
 }
 
 int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
@@ -460,7 +380,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 		vcpu->arch.dcr_needed = 0;
 	}
 
-	kvmppc_check_and_deliver_interrupts(vcpu);
+	kvmppc_core_deliver_interrupts(vcpu);
 
 	local_irq_disable();
 	kvm_guest_enter();
@@ -478,7 +398,7 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 {
-	kvmppc_queue_exception(vcpu, BOOKE_INTERRUPT_EXTERNAL);
+	kvmppc_core_queue_external(vcpu, irq);
 
 	if (waitqueue_active(&vcpu->wq)) {
 		wake_up_interruptible(&vcpu->wq);

commit fad7b9b51e21a97e9e2662f1b51869ed2d209097
Author: Paul Mackerras <paulus@samba.org>
Date:   Tue Dec 23 14:57:26 2008 +1100

    powerpc: Fix KVM build on ppc440
    
    Commit 2a4aca1144394653269720ffbb5a325a77abd5fa ("powerpc/mm: Split
    low level tlb invalidate for nohash processors") changed a call to
    _tlbia to _tlbil_all but didn't include the header that defines
    _tlbil_all, leading to a build failure on 440 if KVM is enabled.
    This fixes it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index eb955d755c9a..8bef0efcdfe1 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -28,6 +28,7 @@
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>
 #include <asm/tlbflush.h>
+#include "../mm/mmu_decl.h"
 
 
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)

commit 2a4aca1144394653269720ffbb5a325a77abd5fa
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:42 2008 +0000

    powerpc/mm: Split low level tlb invalidate for nohash processors
    
    Currently, the various forms of low level TLB invalidations are all
    implemented in misc_32.S for 32-bit processors, in a fairly scary
    mess of #ifdef's and with interesting duplication such as a whole
    bunch of code for FSL _tlbie and _tlbia which are no longer used.
    
    This moves things around such that _tlbie is now defined in
    hash_low_32.S and is only used by the 32-bit hash code, and all
    nohash CPUs use the various _tlbil_* forms that are now moved to
    a new file, tlb_nohash_low.S.
    
    I moved all the definitions for that stuff out of
    include/asm/tlbflush.h as they are really internal mm stuff, into
    mm/mmu_decl.h
    
    The code should have no functional changes.  I kept some variants
    inline for trivial forms on things like 40x and 8xx.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index fda9baada132..eb955d755c9a 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -330,7 +330,7 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 	/* XXX It would be nice to differentiate between heavyweight exit and
 	 * sched_out here, since we could avoid the TLB flush for heavyweight
 	 * exits. */
-	_tlbia();
+	_tlbil_all();
 }
 
 int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,

commit c30f8a6c6d74f67bc2107726cc61a1e7c71e9740
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Mon Nov 24 11:37:38 2008 -0600

    KVM: ppc: stop leaking host memory on VM exit
    
    When the VM exits, we must call put_page() for every page referenced in the
    shadow TLB.
    
    Without this patch, we usually leak 30-50 host pages (120 - 200 KiB with 4 KiB
    pages). The maximum number of pages leaked is the size of our shadow TLB, 64
    pages.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@redhat.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 90a6fc422b23..fda9baada132 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -238,6 +238,7 @@ int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 {
+	kvmppc_core_destroy_mmu(vcpu);
 }
 
 /* Note: clearing MSR[DE] just means that the debug interrupt will not be

commit 83aae4a8098eb8a40a2e9dab3714354182143b4f
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:52 2008 -0500

    KVM: ppc: Write only modified shadow entries into the TLB on exit
    
    Track which TLB entries need to be written, instead of overwriting everything
    below the high water mark. Typically only a single guest TLB entry will be
    modified in a single exit.
    
    Guest boot time performance improvement: about 15%.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b75607180ddb..90a6fc422b23 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -27,6 +27,7 @@
 #include <asm/cputable.h>
 #include <asm/uaccess.h>
 #include <asm/kvm_ppc.h>
+#include <asm/tlbflush.h>
 
 
 gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
@@ -307,14 +308,28 @@ static void kvmppc_load_guest_debug_registers(struct kvm_vcpu *vcpu)
 
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
+	int i;
+
 	if (vcpu->guest_debug.enabled)
 		kvmppc_load_guest_debug_registers(vcpu);
+
+	/* Mark every guest entry in the shadow TLB entry modified, so that they
+	 * will all be reloaded on the next vcpu run (instead of being
+	 * demand-faulted). */
+	for (i = 0; i <= tlb_44x_hwater; i++)
+		kvmppc_tlbe_set_modified(vcpu, i);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 	if (vcpu->guest_debug.enabled)
 		kvmppc_restore_host_debug_state(vcpu);
+
+	/* Don't leave guest TLB entries resident when being de-scheduled. */
+	/* XXX It would be nice to differentiate between heavyweight exit and
+	 * sched_out here, since we could avoid the TLB flush for heavyweight
+	 * exits. */
+	_tlbia();
 }
 
 int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,

commit 6a0ab738ef42d87951b3980f61b1f4cbb14d4171
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Jul 25 13:54:49 2008 -0500

    KVM: ppc: guest breakpoint support
    
    Allow host userspace to program hardware debug registers to set breakpoints
    inside guests.
    
    Signed-off-by: Jerone Young <jyoung5@us.ibm.com>
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 53826a5f6c06..b75607180ddb 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -239,18 +239,100 @@ void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
 {
 }
 
+/* Note: clearing MSR[DE] just means that the debug interrupt will not be
+ * delivered *immediately*. Instead, it simply sets the appropriate DBSR bits.
+ * If those DBSR bits are still set when MSR[DE] is re-enabled, the interrupt
+ * will be delivered as an "imprecise debug event" (which is indicated by
+ * DBSR[IDE].
+ */
+static void kvmppc_disable_debug_interrupts(void)
+{
+	mtmsr(mfmsr() & ~MSR_DE);
+}
+
+static void kvmppc_restore_host_debug_state(struct kvm_vcpu *vcpu)
+{
+	kvmppc_disable_debug_interrupts();
+
+	mtspr(SPRN_IAC1, vcpu->arch.host_iac[0]);
+	mtspr(SPRN_IAC2, vcpu->arch.host_iac[1]);
+	mtspr(SPRN_IAC3, vcpu->arch.host_iac[2]);
+	mtspr(SPRN_IAC4, vcpu->arch.host_iac[3]);
+	mtspr(SPRN_DBCR1, vcpu->arch.host_dbcr1);
+	mtspr(SPRN_DBCR2, vcpu->arch.host_dbcr2);
+	mtspr(SPRN_DBCR0, vcpu->arch.host_dbcr0);
+	mtmsr(vcpu->arch.host_msr);
+}
+
+static void kvmppc_load_guest_debug_registers(struct kvm_vcpu *vcpu)
+{
+	struct kvm_guest_debug *dbg = &vcpu->guest_debug;
+	u32 dbcr0 = 0;
+
+	vcpu->arch.host_msr = mfmsr();
+	kvmppc_disable_debug_interrupts();
+
+	/* Save host debug register state. */
+	vcpu->arch.host_iac[0] = mfspr(SPRN_IAC1);
+	vcpu->arch.host_iac[1] = mfspr(SPRN_IAC2);
+	vcpu->arch.host_iac[2] = mfspr(SPRN_IAC3);
+	vcpu->arch.host_iac[3] = mfspr(SPRN_IAC4);
+	vcpu->arch.host_dbcr0 = mfspr(SPRN_DBCR0);
+	vcpu->arch.host_dbcr1 = mfspr(SPRN_DBCR1);
+	vcpu->arch.host_dbcr2 = mfspr(SPRN_DBCR2);
+
+	/* set registers up for guest */
+
+	if (dbg->bp[0]) {
+		mtspr(SPRN_IAC1, dbg->bp[0]);
+		dbcr0 |= DBCR0_IAC1 | DBCR0_IDM;
+	}
+	if (dbg->bp[1]) {
+		mtspr(SPRN_IAC2, dbg->bp[1]);
+		dbcr0 |= DBCR0_IAC2 | DBCR0_IDM;
+	}
+	if (dbg->bp[2]) {
+		mtspr(SPRN_IAC3, dbg->bp[2]);
+		dbcr0 |= DBCR0_IAC3 | DBCR0_IDM;
+	}
+	if (dbg->bp[3]) {
+		mtspr(SPRN_IAC4, dbg->bp[3]);
+		dbcr0 |= DBCR0_IAC4 | DBCR0_IDM;
+	}
+
+	mtspr(SPRN_DBCR0, dbcr0);
+	mtspr(SPRN_DBCR1, 0);
+	mtspr(SPRN_DBCR2, 0);
+}
+
 void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
 {
+	if (vcpu->guest_debug.enabled)
+		kvmppc_load_guest_debug_registers(vcpu);
 }
 
 void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
+	if (vcpu->guest_debug.enabled)
+		kvmppc_restore_host_debug_state(vcpu);
 }
 
 int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
                                     struct kvm_debug_guest *dbg)
 {
-	return -ENOTSUPP;
+	int i;
+
+	vcpu->guest_debug.enabled = dbg->enabled;
+	if (vcpu->guest_debug.enabled) {
+		for (i=0; i < ARRAY_SIZE(vcpu->guest_debug.bp); i++) {
+			if (dbg->breakpoints[i].enabled)
+				vcpu->guest_debug.bp[i] = dbg->breakpoints[i].address;
+			else
+				vcpu->guest_debug.bp[i] = 0;
+		}
+	}
+
+	return 0;
 }
 
 static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,

commit 34d4cb8fca1f2a31be152b74797e6cd160ec9de6
Author: Marcelo Tosatti <mtosatti@redhat.com>
Date:   Thu Jul 10 20:49:31 2008 -0300

    KVM: MMU: nuke shadowed pgtable pages and ptes on memslot destruction
    
    Flush the shadow mmu before removing regions to avoid stale entries.
    
    Signed-off-by: Marcelo Tosatti <mtosatti@redhat.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index b850d2497027..53826a5f6c06 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -170,6 +170,10 @@ int kvm_arch_set_memory_region(struct kvm *kvm,
 	return 0;
 }
 
+void kvm_arch_flush_shadow(struct kvm *kvm)
+{
+}
+
 struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
 {
 	struct kvm_vcpu *vcpu;

commit 588968b6b7d34e6a88f538d1db9aca47b203623e
Author: Laurent Vivier <Laurent.Vivier@bull.net>
Date:   Fri May 30 16:05:56 2008 +0200

    KVM: Add coalesced MMIO support (powerpc part)
    
    This patch enables coalesced MMIO for powerpc architecture.
    It defines KVM_MMIO_PAGE_OFFSET and KVM_CAP_COALESCED_MMIO.
    It enables the compilation of coalesced_mmio.c.
    
    Signed-off-by: Laurent Vivier <Laurent.Vivier@bull.net>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 0513b359851b..b850d2497027 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -145,6 +145,9 @@ int kvm_dev_ioctl_check_extension(long ext)
 	case KVM_CAP_USER_MEMORY:
 		r = 1;
 		break;
+	case KVM_CAP_COALESCED_MMIO:
+		r = KVM_COALESCED_MMIO_PAGE_OFFSET;
+		break;
 	default:
 		r = 0;
 		break;

commit 7cc8883074b040aa8c1ebd3a17463b0ea3a9ef16
Author: Avi Kivity <avi@qumranet.com>
Date:   Tue May 13 16:29:20 2008 +0300

    KVM: Remove decache_vcpus_on_cpu() and related callbacks
    
    Obsoleted by the vmx-specific per-cpu list.
    
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index 777e0f34e0ea..0513b359851b 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -240,10 +240,6 @@ void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
 {
 }
 
-void decache_vcpus_on_cpu(int cpu)
-{
-}
-
 int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
                                     struct kvm_debug_guest *dbg)
 {

commit 45c5eb67da5a668abe79c23a7e64dbc87a600f90
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Fri Apr 25 17:55:49 2008 -0500

    KVM: ppc: Handle guest idle by emulating MSR[WE] writes
    
    This reduces host CPU usage when the guest is idle. However, the guest must
    set MSR[WE] in its idle loop, which Linux did not do until 2.6.26.
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Signed-off-by: Jerone Young <jyoung5@us.ibm.com>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
index bad40bd2d3ac..777e0f34e0ea 100644
--- a/arch/powerpc/kvm/powerpc.c
+++ b/arch/powerpc/kvm/powerpc.c
@@ -36,13 +36,12 @@ gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
 
 int kvm_cpu_has_interrupt(struct kvm_vcpu *v)
 {
-	/* XXX implement me */
-	return 0;
+	return !!(v->arch.pending_exceptions);
 }
 
 int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
 {
-	return 1;
+	return !(v->arch.msr & MSR_WE);
 }
 
 
@@ -214,6 +213,11 @@ static void kvmppc_decrementer_func(unsigned long data)
 	struct kvm_vcpu *vcpu = (struct kvm_vcpu *)data;
 
 	kvmppc_queue_exception(vcpu, BOOKE_INTERRUPT_DECREMENTER);
+
+	if (waitqueue_active(&vcpu->wq)) {
+		wake_up_interruptible(&vcpu->wq);
+		vcpu->stat.halt_wakeup++;
+	}
 }
 
 int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
@@ -339,6 +343,8 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	int r;
 	sigset_t sigsaved;
 
+	vcpu_load(vcpu);
+
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
 
@@ -363,12 +369,20 @@ int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
 	if (vcpu->sigset_active)
 		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
 
+	vcpu_put(vcpu);
+
 	return r;
 }
 
 int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
 {
 	kvmppc_queue_exception(vcpu, BOOKE_INTERRUPT_EXTERNAL);
+
+	if (waitqueue_active(&vcpu->wq)) {
+		wake_up_interruptible(&vcpu->wq);
+		vcpu->stat.halt_wakeup++;
+	}
+
 	return 0;
 }
 

commit bbf45ba57eaec56569918a8bab96ab653bd45ec1
Author: Hollis Blanchard <hollisb@us.ibm.com>
Date:   Wed Apr 16 23:28:09 2008 -0500

    KVM: ppc: PowerPC 440 KVM implementation
    
    This functionality is definitely experimental, but is capable of running
    unmodified PowerPC 440 Linux kernels as guests on a PowerPC 440 host. (Only
    tested with 440EP "Bamboo" guests so far, but with appropriate userspace
    support other SoC/board combinations should work.)
    
    See Documentation/powerpc/kvm_440.txt for technical details.
    
    [stephen: build fix]
    
    Signed-off-by: Hollis Blanchard <hollisb@us.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Avi Kivity <avi@qumranet.com>

diff --git a/arch/powerpc/kvm/powerpc.c b/arch/powerpc/kvm/powerpc.c
new file mode 100644
index 000000000000..bad40bd2d3ac
--- /dev/null
+++ b/arch/powerpc/kvm/powerpc.c
@@ -0,0 +1,436 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, version 2, as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+ *
+ * Copyright IBM Corp. 2007
+ *
+ * Authors: Hollis Blanchard <hollisb@us.ibm.com>
+ *          Christian Ehrhardt <ehrhardt@linux.vnet.ibm.com>
+ */
+
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/kvm_host.h>
+#include <linux/module.h>
+#include <linux/vmalloc.h>
+#include <linux/fs.h>
+#include <asm/cputable.h>
+#include <asm/uaccess.h>
+#include <asm/kvm_ppc.h>
+
+
+gfn_t unalias_gfn(struct kvm *kvm, gfn_t gfn)
+{
+	return gfn;
+}
+
+int kvm_cpu_has_interrupt(struct kvm_vcpu *v)
+{
+	/* XXX implement me */
+	return 0;
+}
+
+int kvm_arch_vcpu_runnable(struct kvm_vcpu *v)
+{
+	return 1;
+}
+
+
+int kvmppc_emulate_mmio(struct kvm_run *run, struct kvm_vcpu *vcpu)
+{
+	enum emulation_result er;
+	int r;
+
+	er = kvmppc_emulate_instruction(run, vcpu);
+	switch (er) {
+	case EMULATE_DONE:
+		/* Future optimization: only reload non-volatiles if they were
+		 * actually modified. */
+		r = RESUME_GUEST_NV;
+		break;
+	case EMULATE_DO_MMIO:
+		run->exit_reason = KVM_EXIT_MMIO;
+		/* We must reload nonvolatiles because "update" load/store
+		 * instructions modify register state. */
+		/* Future optimization: only reload non-volatiles if they were
+		 * actually modified. */
+		r = RESUME_HOST_NV;
+		break;
+	case EMULATE_FAIL:
+		/* XXX Deliver Program interrupt to guest. */
+		printk(KERN_EMERG "%s: emulation failed (%08x)\n", __func__,
+		       vcpu->arch.last_inst);
+		r = RESUME_HOST;
+		break;
+	default:
+		BUG();
+	}
+
+	return r;
+}
+
+void kvm_arch_hardware_enable(void *garbage)
+{
+}
+
+void kvm_arch_hardware_disable(void *garbage)
+{
+}
+
+int kvm_arch_hardware_setup(void)
+{
+	return 0;
+}
+
+void kvm_arch_hardware_unsetup(void)
+{
+}
+
+void kvm_arch_check_processor_compat(void *rtn)
+{
+	int r;
+
+	if (strcmp(cur_cpu_spec->platform, "ppc440") == 0)
+		r = 0;
+	else
+		r = -ENOTSUPP;
+
+	*(int *)rtn = r;
+}
+
+struct kvm *kvm_arch_create_vm(void)
+{
+	struct kvm *kvm;
+
+	kvm = kzalloc(sizeof(struct kvm), GFP_KERNEL);
+	if (!kvm)
+		return ERR_PTR(-ENOMEM);
+
+	return kvm;
+}
+
+static void kvmppc_free_vcpus(struct kvm *kvm)
+{
+	unsigned int i;
+
+	for (i = 0; i < KVM_MAX_VCPUS; ++i) {
+		if (kvm->vcpus[i]) {
+			kvm_arch_vcpu_free(kvm->vcpus[i]);
+			kvm->vcpus[i] = NULL;
+		}
+	}
+}
+
+void kvm_arch_destroy_vm(struct kvm *kvm)
+{
+	kvmppc_free_vcpus(kvm);
+	kvm_free_physmem(kvm);
+	kfree(kvm);
+}
+
+int kvm_dev_ioctl_check_extension(long ext)
+{
+	int r;
+
+	switch (ext) {
+	case KVM_CAP_USER_MEMORY:
+		r = 1;
+		break;
+	default:
+		r = 0;
+		break;
+	}
+	return r;
+
+}
+
+long kvm_arch_dev_ioctl(struct file *filp,
+                        unsigned int ioctl, unsigned long arg)
+{
+	return -EINVAL;
+}
+
+int kvm_arch_set_memory_region(struct kvm *kvm,
+                               struct kvm_userspace_memory_region *mem,
+                               struct kvm_memory_slot old,
+                               int user_alloc)
+{
+	return 0;
+}
+
+struct kvm_vcpu *kvm_arch_vcpu_create(struct kvm *kvm, unsigned int id)
+{
+	struct kvm_vcpu *vcpu;
+	int err;
+
+	vcpu = kmem_cache_zalloc(kvm_vcpu_cache, GFP_KERNEL);
+	if (!vcpu) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	err = kvm_vcpu_init(vcpu, kvm, id);
+	if (err)
+		goto free_vcpu;
+
+	return vcpu;
+
+free_vcpu:
+	kmem_cache_free(kvm_vcpu_cache, vcpu);
+out:
+	return ERR_PTR(err);
+}
+
+void kvm_arch_vcpu_free(struct kvm_vcpu *vcpu)
+{
+	kvm_vcpu_uninit(vcpu);
+	kmem_cache_free(kvm_vcpu_cache, vcpu);
+}
+
+void kvm_arch_vcpu_destroy(struct kvm_vcpu *vcpu)
+{
+	kvm_arch_vcpu_free(vcpu);
+}
+
+int kvm_cpu_has_pending_timer(struct kvm_vcpu *vcpu)
+{
+	unsigned int priority = exception_priority[BOOKE_INTERRUPT_DECREMENTER];
+
+	return test_bit(priority, &vcpu->arch.pending_exceptions);
+}
+
+static void kvmppc_decrementer_func(unsigned long data)
+{
+	struct kvm_vcpu *vcpu = (struct kvm_vcpu *)data;
+
+	kvmppc_queue_exception(vcpu, BOOKE_INTERRUPT_DECREMENTER);
+}
+
+int kvm_arch_vcpu_init(struct kvm_vcpu *vcpu)
+{
+	setup_timer(&vcpu->arch.dec_timer, kvmppc_decrementer_func,
+	            (unsigned long)vcpu);
+
+	return 0;
+}
+
+void kvm_arch_vcpu_uninit(struct kvm_vcpu *vcpu)
+{
+}
+
+void kvm_arch_vcpu_load(struct kvm_vcpu *vcpu, int cpu)
+{
+}
+
+void kvm_arch_vcpu_put(struct kvm_vcpu *vcpu)
+{
+}
+
+void decache_vcpus_on_cpu(int cpu)
+{
+}
+
+int kvm_arch_vcpu_ioctl_debug_guest(struct kvm_vcpu *vcpu,
+                                    struct kvm_debug_guest *dbg)
+{
+	return -ENOTSUPP;
+}
+
+static void kvmppc_complete_dcr_load(struct kvm_vcpu *vcpu,
+                                     struct kvm_run *run)
+{
+	u32 *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
+	*gpr = run->dcr.data;
+}
+
+static void kvmppc_complete_mmio_load(struct kvm_vcpu *vcpu,
+                                      struct kvm_run *run)
+{
+	u32 *gpr = &vcpu->arch.gpr[vcpu->arch.io_gpr];
+
+	if (run->mmio.len > sizeof(*gpr)) {
+		printk(KERN_ERR "bad MMIO length: %d\n", run->mmio.len);
+		return;
+	}
+
+	if (vcpu->arch.mmio_is_bigendian) {
+		switch (run->mmio.len) {
+		case 4: *gpr = *(u32 *)run->mmio.data; break;
+		case 2: *gpr = *(u16 *)run->mmio.data; break;
+		case 1: *gpr = *(u8 *)run->mmio.data; break;
+		}
+	} else {
+		/* Convert BE data from userland back to LE. */
+		switch (run->mmio.len) {
+		case 4: *gpr = ld_le32((u32 *)run->mmio.data); break;
+		case 2: *gpr = ld_le16((u16 *)run->mmio.data); break;
+		case 1: *gpr = *(u8 *)run->mmio.data; break;
+		}
+	}
+}
+
+int kvmppc_handle_load(struct kvm_run *run, struct kvm_vcpu *vcpu,
+                       unsigned int rt, unsigned int bytes, int is_bigendian)
+{
+	if (bytes > sizeof(run->mmio.data)) {
+		printk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,
+		       run->mmio.len);
+	}
+
+	run->mmio.phys_addr = vcpu->arch.paddr_accessed;
+	run->mmio.len = bytes;
+	run->mmio.is_write = 0;
+
+	vcpu->arch.io_gpr = rt;
+	vcpu->arch.mmio_is_bigendian = is_bigendian;
+	vcpu->mmio_needed = 1;
+	vcpu->mmio_is_write = 0;
+
+	return EMULATE_DO_MMIO;
+}
+
+int kvmppc_handle_store(struct kvm_run *run, struct kvm_vcpu *vcpu,
+                        u32 val, unsigned int bytes, int is_bigendian)
+{
+	void *data = run->mmio.data;
+
+	if (bytes > sizeof(run->mmio.data)) {
+		printk(KERN_ERR "%s: bad MMIO length: %d\n", __func__,
+		       run->mmio.len);
+	}
+
+	run->mmio.phys_addr = vcpu->arch.paddr_accessed;
+	run->mmio.len = bytes;
+	run->mmio.is_write = 1;
+	vcpu->mmio_needed = 1;
+	vcpu->mmio_is_write = 1;
+
+	/* Store the value at the lowest bytes in 'data'. */
+	if (is_bigendian) {
+		switch (bytes) {
+		case 4: *(u32 *)data = val; break;
+		case 2: *(u16 *)data = val; break;
+		case 1: *(u8  *)data = val; break;
+		}
+	} else {
+		/* Store LE value into 'data'. */
+		switch (bytes) {
+		case 4: st_le32(data, val); break;
+		case 2: st_le16(data, val); break;
+		case 1: *(u8 *)data = val; break;
+		}
+	}
+
+	return EMULATE_DO_MMIO;
+}
+
+int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *run)
+{
+	int r;
+	sigset_t sigsaved;
+
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &vcpu->sigset, &sigsaved);
+
+	if (vcpu->mmio_needed) {
+		if (!vcpu->mmio_is_write)
+			kvmppc_complete_mmio_load(vcpu, run);
+		vcpu->mmio_needed = 0;
+	} else if (vcpu->arch.dcr_needed) {
+		if (!vcpu->arch.dcr_is_write)
+			kvmppc_complete_dcr_load(vcpu, run);
+		vcpu->arch.dcr_needed = 0;
+	}
+
+	kvmppc_check_and_deliver_interrupts(vcpu);
+
+	local_irq_disable();
+	kvm_guest_enter();
+	r = __kvmppc_vcpu_run(run, vcpu);
+	kvm_guest_exit();
+	local_irq_enable();
+
+	if (vcpu->sigset_active)
+		sigprocmask(SIG_SETMASK, &sigsaved, NULL);
+
+	return r;
+}
+
+int kvm_vcpu_ioctl_interrupt(struct kvm_vcpu *vcpu, struct kvm_interrupt *irq)
+{
+	kvmppc_queue_exception(vcpu, BOOKE_INTERRUPT_EXTERNAL);
+	return 0;
+}
+
+int kvm_arch_vcpu_ioctl_get_mpstate(struct kvm_vcpu *vcpu,
+                                    struct kvm_mp_state *mp_state)
+{
+	return -EINVAL;
+}
+
+int kvm_arch_vcpu_ioctl_set_mpstate(struct kvm_vcpu *vcpu,
+                                    struct kvm_mp_state *mp_state)
+{
+	return -EINVAL;
+}
+
+long kvm_arch_vcpu_ioctl(struct file *filp,
+                         unsigned int ioctl, unsigned long arg)
+{
+	struct kvm_vcpu *vcpu = filp->private_data;
+	void __user *argp = (void __user *)arg;
+	long r;
+
+	switch (ioctl) {
+	case KVM_INTERRUPT: {
+		struct kvm_interrupt irq;
+		r = -EFAULT;
+		if (copy_from_user(&irq, argp, sizeof(irq)))
+			goto out;
+		r = kvm_vcpu_ioctl_interrupt(vcpu, &irq);
+		break;
+	}
+	default:
+		r = -EINVAL;
+	}
+
+out:
+	return r;
+}
+
+int kvm_vm_ioctl_get_dirty_log(struct kvm *kvm, struct kvm_dirty_log *log)
+{
+	return -ENOTSUPP;
+}
+
+long kvm_arch_vm_ioctl(struct file *filp,
+                       unsigned int ioctl, unsigned long arg)
+{
+	long r;
+
+	switch (ioctl) {
+	default:
+		r = -EINVAL;
+	}
+
+	return r;
+}
+
+int kvm_arch_init(void *opaque)
+{
+	return 0;
+}
+
+void kvm_arch_exit(void)
+{
+}
