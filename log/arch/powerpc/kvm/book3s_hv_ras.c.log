commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 8c24c3bea0bf..79f7d07ef674 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -1,7 +1,5 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License, version 2, as
- * published by the Free Software Foundation.
  *
  * Copyright 2012 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>
  */

commit 884dfb722db899e36d8c382783347aab57f96caa
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Feb 21 13:38:49 2019 +1100

    KVM: PPC: Book3S HV: Simplify machine check handling
    
    This makes the handling of machine check interrupts that occur inside
    a guest simpler and more robust, with less done in assembler code and
    in real mode.
    
    Now, when a machine check occurs inside a guest, we always get the
    machine check event struct and put a copy in the vcpu struct for the
    vcpu where the machine check occurred.  We no longer call
    machine_check_queue_event() from kvmppc_realmode_mc_power7(), because
    on POWER8, when a vcpu is running on an offline secondary thread and
    we call machine_check_queue_event(), that calls irq_work_queue(),
    which doesn't work because the CPU is offline, but instead triggers
    the WARN_ON(lazy_irq_pending()) in pnv_smp_cpu_kill_self() (which
    fires again and again because nothing clears the condition).
    
    All that machine_check_queue_event() actually does is to cause the
    event to be printed to the console.  For a machine check occurring in
    the guest, we now print the event in kvmppc_handle_exit_hv()
    instead.
    
    The assembly code at label machine_check_realmode now just calls C
    code and then continues exiting the guest.  We no longer either
    synthesize a machine check for the guest in assembly code or return
    to the guest without a machine check.
    
    The code in kvmppc_handle_exit_hv() is extended to handle the case
    where the guest is not FWNMI-capable.  In that case we now always
    synthesize a machine check interrupt for the guest.  Previously, if
    the host thinks it has recovered the machine check fully, it would
    return to the guest without any notification that the machine check
    had occurred.  If the machine check was caused by some action of the
    guest (such as creating duplicate SLB entries), it is much better to
    tell the guest that it has caused a problem.  Therefore we now always
    generate a machine check interrupt for guests that are not
    FWNMI-capable.
    
    Reviewed-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 0787f12c1a1b..8c24c3bea0bf 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -66,10 +66,8 @@ static void reload_slb(struct kvm_vcpu *vcpu)
 /*
  * On POWER7, see if we can handle a machine check that occurred inside
  * the guest in real mode, without switching to the host partition.
- *
- * Returns: 0 => exit guest, 1 => deliver machine check to guest
  */
-static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
+static void kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 {
 	unsigned long srr1 = vcpu->arch.shregs.msr;
 	struct machine_check_event mce_evt;
@@ -111,52 +109,24 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 	}
 
 	/*
-	 * See if we have already handled the condition in the linux host.
-	 * We assume that if the condition is recovered then linux host
-	 * will have generated an error log event that we will pick
-	 * up and log later.
-	 * Don't release mce event now. We will queue up the event so that
-	 * we can log the MCE event info on host console.
+	 * Now get the event and stash it in the vcpu struct so it can
+	 * be handled by the primary thread in virtual mode.  We can't
+	 * call machine_check_queue_event() here if we are running on
+	 * an offline secondary thread.
 	 */
-	if (!get_mce_event(&mce_evt, MCE_EVENT_DONTRELEASE))
-		goto out;
-
-	if (mce_evt.version == MCE_V1 &&
-	    (mce_evt.severity == MCE_SEV_NO_ERROR ||
-	     mce_evt.disposition == MCE_DISPOSITION_RECOVERED))
-		handled = 1;
-
-out:
-	/*
-	 * For guest that supports FWNMI capability, hook the MCE event into
-	 * vcpu structure. We are going to exit the guest with KVM_EXIT_NMI
-	 * exit reason. On our way to exit we will pull this event from vcpu
-	 * structure and print it from thread 0 of the core/subcore.
-	 *
-	 * For guest that does not support FWNMI capability (old QEMU):
-	 * We are now going enter guest either through machine check
-	 * interrupt (for unhandled errors) or will continue from
-	 * current HSRR0 (for handled errors) in guest. Hence
-	 * queue up the event so that we can log it from host console later.
-	 */
-	if (vcpu->kvm->arch.fwnmi_enabled) {
-		/*
-		 * Hook up the mce event on to vcpu structure.
-		 * First clear the old event.
-		 */
-		memset(&vcpu->arch.mce_evt, 0, sizeof(vcpu->arch.mce_evt));
-		if (get_mce_event(&mce_evt, MCE_EVENT_RELEASE)) {
-			vcpu->arch.mce_evt = mce_evt;
-		}
-	} else
-		machine_check_queue_event();
+	if (get_mce_event(&mce_evt, MCE_EVENT_RELEASE)) {
+		if (handled && mce_evt.version == MCE_V1)
+			mce_evt.disposition = MCE_DISPOSITION_RECOVERED;
+	} else {
+		memset(&mce_evt, 0, sizeof(mce_evt));
+	}
 
-	return handled;
+	vcpu->arch.mce_evt = mce_evt;
 }
 
-long kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu)
+void kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu)
 {
-	return kvmppc_realmode_mc_power7(vcpu);
+	kvmppc_realmode_mc_power7(vcpu);
 }
 
 /* Check if dynamic split is in force and return subcore size accordingly. */

commit 95a6432ce903858a2f285d611275340aa574c6ac
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:55 2018 +1100

    KVM: PPC: Book3S HV: Streamlined guest entry/exit path on P9 for radix guests
    
    This creates an alternative guest entry/exit path which is used for
    radix guests on POWER9 systems when we have indep_threads_mode=Y.  In
    these circumstances there is exactly one vcpu per vcore and there is
    no coordination required between vcpus or vcores; the vcpu can enter
    the guest without needing to synchronize with anything else.
    
    The new fast path is implemented almost entirely in C in book3s_hv.c
    and runs with the MMU on until the guest is entered.  On guest exit
    we use the existing path until the point where we are committed to
    exiting the guest (as distinct from handling an interrupt in the
    low-level code and returning to the guest) and we have pulled the
    guest context from the XIVE.  At that point we check a flag in the
    stack frame to see whether we came in via the old path and the new
    path; if we came in via the new path then we go back to C code to do
    the rest of the process of saving the guest context and restoring the
    host context.
    
    The C code is split into separate functions for handling the
    OS-accessible state and the hypervisor state, with the idea that the
    latter can be replaced by a hypercall when we implement nested
    virtualization.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    [mpe: Fix CONFIG_ALTIVEC=n build]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index ee564b682f0c..0787f12c1a1b 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -177,6 +177,7 @@ void kvmppc_subcore_enter_guest(void)
 
 	local_paca->sibling_subcore_state->in_guest[subcore_id] = 1;
 }
+EXPORT_SYMBOL_GPL(kvmppc_subcore_enter_guest);
 
 void kvmppc_subcore_exit_guest(void)
 {
@@ -187,6 +188,7 @@ void kvmppc_subcore_exit_guest(void)
 
 	local_paca->sibling_subcore_state->in_guest[subcore_id] = 0;
 }
+EXPORT_SYMBOL_GPL(kvmppc_subcore_exit_guest);
 
 static bool kvmppc_tb_resync_required(void)
 {

commit df709a296ef7c493a074609019b0e074cde5c0d0
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Mon Oct 8 16:30:52 2018 +1100

    KVM: PPC: Book3S HV: Simplify real-mode interrupt handling
    
    This streamlines the first part of the code that handles a hypervisor
    interrupt that occurred in the guest.  With this, all of the real-mode
    handling that occurs is done before the "guest_exit_cont" label; once
    we get to that label we are committed to exiting to host virtual mode.
    Thus the machine check and HMI real-mode handling is moved before that
    label.
    
    Also, the code to handle external interrupts is moved out of line, as
    is the code that calls kvmppc_realmode_hmi_handler().
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index b11043b23c18..ee564b682f0c 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -331,5 +331,13 @@ long kvmppc_realmode_hmi_handler(void)
 	} else {
 		wait_for_tb_resync();
 	}
+
+	/*
+	 * Reset tb_offset_applied so the guest exit code won't try
+	 * to subtract the previous timebase offset from the timebase.
+	 */
+	if (local_paca->kvm_hstate.kvm_vcore)
+		local_paca->kvm_hstate.kvm_vcore->tb_offset_applied = 0;
+
 	return 0;
 }

commit 5400fc229e6078a6964b15fb98e9a994df3d642a
Merge: 02ef6dd8109b 76b03dc07eeb
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jan 21 22:43:43 2018 +1100

    Merge branch 'topic/ppc-kvm' into next
    
    Merge the topic branch we share with kvm-ppc, this brings in two xive
    commits, one from Paul to rework HMI handling, and a minor cleanup to
    drop an unused flag.

commit d075745d893c78730e4a3b7a60fca23c2f764081
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Wed Jan 17 20:51:13 2018 +1100

    KVM: PPC: Book3S HV: Improve handling of debug-trigger HMIs on POWER9
    
    Hypervisor maintenance interrupts (HMIs) are generated by various
    causes, signalled by bits in the hypervisor maintenance exception
    register (HMER).  In most cases calling OPAL to handle the interrupt
    is the correct thing to do, but the "debug trigger" HMIs signalled by
    PPC bit 17 (bit 46) of HMER are used to invoke software workarounds
    for hardware bugs, and OPAL does not have any code to handle this
    cause.  The debug trigger HMI is used in POWER9 DD2.0 and DD2.1 chips
    to work around a hardware bug in executing vector load instructions to
    cache inhibited memory.  In POWER9 DD2.2 chips, it is generated when
    conditions are detected relating to threads being in TM (transactional
    memory) suspended mode when the core SMT configuration needs to be
    reconfigured.
    
    The kernel currently has code to detect the vector CI load condition,
    but only when the HMI occurs in the host, not when it occurs in a
    guest.  If a HMI occurs in the guest, it is always passed to OPAL, and
    then we always re-sync the timebase, because the HMI cause might have
    been a timebase error, for which OPAL would re-sync the timebase, thus
    removing the timebase offset which KVM applied for the guest.  Since
    we don't know what OPAL did, we don't know whether to subtract the
    timebase offset from the timebase, so instead we re-sync the timebase.
    
    This adds code to determine explicitly what the cause of a debug
    trigger HMI will be.  This is based on a new device-tree property
    under the CPU nodes called ibm,hmi-special-triggers, if it is
    present, or otherwise based on the PVR (processor version register).
    The handling of debug trigger HMIs is pulled out into a separate
    function which can be called from the KVM guest exit code.  If this
    function handles and clears the HMI, and no other HMI causes remain,
    then we skip calling OPAL and we proceed to subtract the guest
    timebase offset from the timebase.
    
    The overall handling for HMIs that occur in the host (i.e. not in a
    KVM guest) is largely unchanged, except that we now don't set the flag
    for the vector CI load workaround on DD2.2 processors.
    
    This also removes a BUG_ON in the KVM code.  BUG_ON is generally not
    useful in KVM guest entry/exit code since it is difficult to handle
    the resulting trap gracefully.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index c356f9a40b24..c296343d0dcc 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -268,17 +268,19 @@ static void kvmppc_tb_resync_done(void)
  *   secondary threads to proceed.
  * - All secondary threads will eventually call opal hmi handler on
  *   their exit path.
+ *
+ * Returns 1 if the timebase offset should be applied, 0 if not.
  */
 
 long kvmppc_realmode_hmi_handler(void)
 {
-	int ptid = local_paca->kvm_hstate.ptid;
 	bool resync_req;
 
-	/* This is only called on primary thread. */
-	BUG_ON(ptid != 0);
 	__this_cpu_inc(irq_stat.hmi_exceptions);
 
+	if (hmi_handle_debugtrig(NULL) >= 0)
+		return 1;
+
 	/*
 	 * By now primary thread has already completed guest->host
 	 * partition switch but haven't signaled secondaries yet.

commit d4748276ae14ce951a3254852dddc3675797c277
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Sun Dec 24 01:15:50 2017 +1000

    powerpc/64s: Improve local TLB flush for boot and MCE on POWER9
    
    There are several cases outside the normal address space management
    where a CPU's entire local TLB is to be flushed:
    
      1. Booting the kernel, in case something has left stale entries in
         the TLB (e.g., kexec).
    
      2. Machine check, to clean corrupted TLB entries.
    
    One other place where the TLB is flushed, is waking from deep idle
    states. The flush is a side-effect of calling ->cpu_restore with the
    intention of re-setting various SPRs. The flush itself is unnecessary
    because in the first case, the TLB should not acquire new corrupted
    TLB entries as part of sleep/wake (though they may be lost).
    
    This type of TLB flush is coded inflexibly, several times for each CPU
    type, and they have a number of problems with ISA v3.0B:
    
    - The current radix mode of the MMU is not taken into account, it is
      always done as a hash flushn For IS=2 (LPID-matching flush from host)
      and IS=3 with HV=0 (guest kernel flush), tlbie(l) is undefined if
      the R field does not match the current radix mode.
    
    - ISA v3.0B hash must flush the partition and process table caches as
      well.
    
    - ISA v3.0B radix must flush partition and process scoped translations,
      partition and process table caches, and also the page walk cache.
    
    So consolidate the flushing code and implement it in C and inline asm
    under the mm/ directory with the rest of the flush code. Add ISA v3.0B
    cases for radix and hash, and use the radix flush in radix environment.
    
    Provide a way for IS=2 (LPID flush) to specify the radix mode of the
    partition. Have KVM pass in the radix mode of the guest.
    
    Take out the flushes from early cputable/dt_cpu_ftrs detection hooks,
    and move it later in the boot process after, the MMU registers are set
    up and before relocation is first turned on.
    
    The TLB flush is no longer called when restoring from deep idle states.
    This was not be done as a separate step because booting secondaries
    uses the same cpu_restore as idle restore, which needs the TLB flush.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index c356f9a40b24..e61066bb6725 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -87,8 +87,7 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 				   DSISR_MC_SLB_PARITY | DSISR_MC_DERAT_MULTI);
 		}
 		if (dsisr & DSISR_MC_TLB_MULTI) {
-			if (cur_cpu_spec && cur_cpu_spec->flush_tlb)
-				cur_cpu_spec->flush_tlb(TLB_INVAL_SCOPE_LPID);
+			tlbiel_all_lpid(vcpu->kvm->arch.radix);
 			dsisr &= ~DSISR_MC_TLB_MULTI;
 		}
 		/* Any other errors we don't understand? */
@@ -105,8 +104,7 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 		reload_slb(vcpu);
 		break;
 	case SRR1_MC_IFETCH_TLBMULTI:
-		if (cur_cpu_spec && cur_cpu_spec->flush_tlb)
-			cur_cpu_spec->flush_tlb(TLB_INVAL_SCOPE_LPID);
+		tlbiel_all_lpid(vcpu->kvm->arch.radix);
 		break;
 	default:
 		handled = 0;

commit e20bbd3d8d5c4432db8fd6f091b096963236064f
Author: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
Date:   Thu May 11 16:33:37 2017 +0530

    KVM: PPC: Book3S HV: Exit guest upon MCE when FWNMI capability is enabled
    
    Enhance KVM to cause a guest exit with KVM_EXIT_NMI
    exit reason upon a machine check exception (MCE) in
    the guest address space if the KVM_CAP_PPC_FWNMI
    capability is enabled (instead of delivering a 0x200
    interrupt to guest). This enables QEMU to build error
    log and deliver machine check exception to guest via
    guest registered machine check handler.
    
    This approach simplifies the delivery of machine
    check exception to guest OS compared to the earlier
    approach of KVM directly invoking 0x200 guest interrupt
    vector.
    
    This design/approach is based on the feedback for the
    QEMU patches to handle machine check exception. Details
    of earlier approach of handling machine check exception
    in QEMU and related discussions can be found at:
    
    https://lists.nongnu.org/archive/html/qemu-devel/2014-11/msg00813.html
    
    Note:
    
    This patch now directly invokes machine_check_print_event_info()
    from kvmppc_handle_exit_hv() to print the event to host console
    at the time of guest exit before the exception is passed on to the
    guest. Hence, the host-side handling which was performed earlier
    via machine_check_fwnmi is removed.
    
    The reasons for this approach is (i) it is not possible
    to distinguish whether the exception occurred in the
    guest or the host from the pt_regs passed on the
    machine_check_exception(). Hence machine_check_exception()
    calls panic, instead of passing on the exception to
    the guest, if the machine check exception is not
    recoverable. (ii) the approach introduced in this
    patch gives opportunity to the host kernel to perform
    actions in virtual mode before passing on the exception
    to the guest. This approach does not require complex
    tweaks to machine_check_fwnmi and friends.
    
    Signed-off-by: Aravinda Prasad <aravinda@linux.vnet.ibm.com>
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 7ef0993214f3..c356f9a40b24 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -130,12 +130,28 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 
 out:
 	/*
+	 * For guest that supports FWNMI capability, hook the MCE event into
+	 * vcpu structure. We are going to exit the guest with KVM_EXIT_NMI
+	 * exit reason. On our way to exit we will pull this event from vcpu
+	 * structure and print it from thread 0 of the core/subcore.
+	 *
+	 * For guest that does not support FWNMI capability (old QEMU):
 	 * We are now going enter guest either through machine check
 	 * interrupt (for unhandled errors) or will continue from
 	 * current HSRR0 (for handled errors) in guest. Hence
 	 * queue up the event so that we can log it from host console later.
 	 */
-	machine_check_queue_event();
+	if (vcpu->kvm->arch.fwnmi_enabled) {
+		/*
+		 * Hook up the mce event on to vcpu structure.
+		 * First clear the old event.
+		 */
+		memset(&vcpu->arch.mce_evt, 0, sizeof(vcpu->arch.mce_evt));
+		if (get_mce_event(&mce_evt, MCE_EVENT_RELEASE)) {
+			vcpu->arch.mce_evt = mce_evt;
+		}
+	} else
+		machine_check_queue_event();
 
 	return handled;
 }

commit e34af7849014f1d80899b811cf9021588cb8dd88
Author: Paul Mackerras <paulus@ozlabs.org>
Date:   Thu Dec 1 14:03:46 2016 +1100

    KVM: PPC: Book3S: Move prototypes for KVM functions into kvm_ppc.h
    
    This moves the prototypes for functions that are only called from
    assembler code out of asm/asm-prototypes.h into asm/kvm_ppc.h.
    The prototypes were added in commit ebe4535fbe7a ("KVM: PPC:
    Book3S HV: sparse: prototypes for functions called from assembler",
    2016-10-10), but given that the functions are KVM functions,
    having them in a KVM header will be better for long-term
    maintenance.
    
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index be1cee5dc032..7ef0993214f3 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -16,7 +16,7 @@
 #include <asm/machdep.h>
 #include <asm/cputhreads.h>
 #include <asm/hmi.h>
-#include <asm/asm-prototypes.h>
+#include <asm/kvm_ppc.h>
 
 /* SRR1 bits for machine check on POWER7 */
 #define SRR1_MC_LDSTERR		(1ul << (63-42))

commit ebe4535fbe7a190e13c0e175e7e7a02898dbac33
Author: Daniel Axtens <dja@axtens.net>
Date:   Mon Oct 10 11:31:20 2016 +1100

    KVM: PPC: Book3S HV: sparse: prototypes for functions called from assembler
    
    A bunch of KVM functions are only called from assembler.
    Give them prototypes in asm-prototypes.h
    This reduces sparse warnings.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 0fa70a9618d7..be1cee5dc032 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -16,6 +16,7 @@
 #include <asm/machdep.h>
 #include <asm/cputhreads.h>
 #include <asm/hmi.h>
+#include <asm/asm-prototypes.h>
 
 /* SRR1 bits for machine check on POWER7 */
 #define SRR1_MC_LDSTERR		(1ul << (63-42))

commit fd7bacbca47a86a6f418440d8a5d7b7edbb2f8f9
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Sun May 15 09:44:26 2016 +0530

    KVM: PPC: Book3S HV: Fix TB corruption in guest exit path on HMI interrupt
    
    When a guest is assigned to a core it converts the host Timebase (TB)
    into guest TB by adding guest timebase offset before entering into
    guest. During guest exit it restores the guest TB to host TB. This means
    under certain conditions (Guest migration) host TB and guest TB can differ.
    
    When we get an HMI for TB related issues the opal HMI handler would
    try fixing errors and restore the correct host TB value. With no guest
    running, we don't have any issues. But with guest running on the core
    we run into TB corruption issues.
    
    If we get an HMI while in the guest, the current HMI handler invokes opal
    hmi handler before forcing guest to exit. The guest exit path subtracts
    the guest TB offset from the current TB value which may have already
    been restored with host value by opal hmi handler. This leads to incorrect
    host and guest TB values.
    
    With split-core, things become more complex. With split-core, TB also gets
    split and each subcore gets its own TB register. When a hmi handler fixes
    a TB error and restores the TB value, it affects all the TB values of
    sibling subcores on the same core. On TB errors all the thread in the core
    gets HMI. With existing code, the individual threads call opal hmi handle
    independently which can easily throw TB out of sync if we have guest
    running on subcores. Hence we will need to co-ordinate with all the
    threads before making opal hmi handler call followed by TB resync.
    
    This patch introduces a sibling subcore state structure (shared by all
    threads in the core) in paca which holds information about whether sibling
    subcores are in Guest mode or host mode. An array in_guest[] of size
    MAX_SUBCORE_PER_CORE=4 is used to maintain the state of each subcore.
    The subcore id is used as index into in_guest[] array. Only primary
    thread entering/exiting the guest is responsible to set/unset its
    designated array element.
    
    On TB error, we get HMI interrupt on every thread on the core. Upon HMI,
    this patch will now force guest to vacate the core/subcore. Primary
    thread from each subcore will then turn off its respective bit
    from the above bitmap during the guest exit path just after the
    guest->host partition switch is complete.
    
    All other threads that have just exited the guest OR were already in host
    will wait until all other subcores clears their respective bit.
    Once all the subcores turn off their respective bit, all threads will
    will make call to opal hmi handler.
    
    It is not necessary that opal hmi handler would resync the TB value for
    every HMI interrupts. It would do so only for the HMI caused due to
    TB errors. For rest, it would not touch TB value. Hence to make things
    simpler, primary thread would call TB resync explicitly once for each
    core immediately after opal hmi handler instead of subtracting guest
    offset from TB. TB resync call will restore the TB with host value.
    Thus we can be sure about the TB state.
    
    One of the primary threads exiting the guest will take up the
    responsibility of calling TB resync. It will use one of the top bits
    (bit 63) from subcore state flags bitmap to make the decision. The first
    primary thread (among the subcores) that is able to set the bit will
    have to call the TB resync. Rest all other threads will wait until TB
    resync is complete.  Once TB resync is complete all threads will then
    proceed.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@ozlabs.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 93b5f5c9b445..0fa70a9618d7 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -13,6 +13,9 @@
 #include <linux/kernel.h>
 #include <asm/opal.h>
 #include <asm/mce.h>
+#include <asm/machdep.h>
+#include <asm/cputhreads.h>
+#include <asm/hmi.h>
 
 /* SRR1 bits for machine check on POWER7 */
 #define SRR1_MC_LDSTERR		(1ul << (63-42))
@@ -140,3 +143,176 @@ long kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu)
 {
 	return kvmppc_realmode_mc_power7(vcpu);
 }
+
+/* Check if dynamic split is in force and return subcore size accordingly. */
+static inline int kvmppc_cur_subcore_size(void)
+{
+	if (local_paca->kvm_hstate.kvm_split_mode)
+		return local_paca->kvm_hstate.kvm_split_mode->subcore_size;
+
+	return threads_per_subcore;
+}
+
+void kvmppc_subcore_enter_guest(void)
+{
+	int thread_id, subcore_id;
+
+	thread_id = cpu_thread_in_core(local_paca->paca_index);
+	subcore_id = thread_id / kvmppc_cur_subcore_size();
+
+	local_paca->sibling_subcore_state->in_guest[subcore_id] = 1;
+}
+
+void kvmppc_subcore_exit_guest(void)
+{
+	int thread_id, subcore_id;
+
+	thread_id = cpu_thread_in_core(local_paca->paca_index);
+	subcore_id = thread_id / kvmppc_cur_subcore_size();
+
+	local_paca->sibling_subcore_state->in_guest[subcore_id] = 0;
+}
+
+static bool kvmppc_tb_resync_required(void)
+{
+	if (test_and_set_bit(CORE_TB_RESYNC_REQ_BIT,
+				&local_paca->sibling_subcore_state->flags))
+		return false;
+
+	return true;
+}
+
+static void kvmppc_tb_resync_done(void)
+{
+	clear_bit(CORE_TB_RESYNC_REQ_BIT,
+			&local_paca->sibling_subcore_state->flags);
+}
+
+/*
+ * kvmppc_realmode_hmi_handler() is called only by primary thread during
+ * guest exit path.
+ *
+ * There are multiple reasons why HMI could occur, one of them is
+ * Timebase (TB) error. If this HMI is due to TB error, then TB would
+ * have been in stopped state. The opal hmi handler Will fix it and
+ * restore the TB value with host timebase value. For HMI caused due
+ * to non-TB errors, opal hmi handler will not touch/restore TB register
+ * and hence there won't be any change in TB value.
+ *
+ * Since we are not sure about the cause of this HMI, we can't be sure
+ * about the content of TB register whether it holds guest or host timebase
+ * value. Hence the idea is to resync the TB on every HMI, so that we
+ * know about the exact state of the TB value. Resync TB call will
+ * restore TB to host timebase.
+ *
+ * Things to consider:
+ * - On TB error, HMI interrupt is reported on all the threads of the core
+ *   that has encountered TB error irrespective of split-core mode.
+ * - The very first thread on the core that get chance to fix TB error
+ *   would rsync the TB with local chipTOD value.
+ * - The resync TB is a core level action i.e. it will sync all the TBs
+ *   in that core independent of split-core mode. This means if we trigger
+ *   TB sync from a thread from one subcore, it would affect TB values of
+ *   sibling subcores of the same core.
+ *
+ * All threads need to co-ordinate before making opal hmi handler.
+ * All threads will use sibling_subcore_state->in_guest[] (shared by all
+ * threads in the core) in paca which holds information about whether
+ * sibling subcores are in Guest mode or host mode. The in_guest[] array
+ * is of size MAX_SUBCORE_PER_CORE=4, indexed using subcore id to set/unset
+ * subcore status. Only primary threads from each subcore is responsible
+ * to set/unset its designated array element while entering/exiting the
+ * guset.
+ *
+ * After invoking opal hmi handler call, one of the thread (of entire core)
+ * will need to resync the TB. Bit 63 from subcore state bitmap flags
+ * (sibling_subcore_state->flags) will be used to co-ordinate between
+ * primary threads to decide who takes up the responsibility.
+ *
+ * This is what we do:
+ * - Primary thread from each subcore tries to set resync required bit[63]
+ *   of paca->sibling_subcore_state->flags.
+ * - The first primary thread that is able to set the flag takes the
+ *   responsibility of TB resync. (Let us call it as thread leader)
+ * - All other threads which are in host will call
+ *   wait_for_subcore_guest_exit() and wait for in_guest[0-3] from
+ *   paca->sibling_subcore_state to get cleared.
+ * - All the primary thread will clear its subcore status from subcore
+ *   state in_guest[] array respectively.
+ * - Once all primary threads clear in_guest[0-3], all of them will invoke
+ *   opal hmi handler.
+ * - Now all threads will wait for TB resync to complete by invoking
+ *   wait_for_tb_resync() except the thread leader.
+ * - Thread leader will do a TB resync by invoking opal_resync_timebase()
+ *   call and the it will clear the resync required bit.
+ * - All other threads will now come out of resync wait loop and proceed
+ *   with individual execution.
+ * - On return of this function, primary thread will signal all
+ *   secondary threads to proceed.
+ * - All secondary threads will eventually call opal hmi handler on
+ *   their exit path.
+ */
+
+long kvmppc_realmode_hmi_handler(void)
+{
+	int ptid = local_paca->kvm_hstate.ptid;
+	bool resync_req;
+
+	/* This is only called on primary thread. */
+	BUG_ON(ptid != 0);
+	__this_cpu_inc(irq_stat.hmi_exceptions);
+
+	/*
+	 * By now primary thread has already completed guest->host
+	 * partition switch but haven't signaled secondaries yet.
+	 * All the secondary threads on this subcore is waiting
+	 * for primary thread to signal them to go ahead.
+	 *
+	 * For threads from subcore which isn't in guest, they all will
+	 * wait until all other subcores on this core exit the guest.
+	 *
+	 * Now set the resync required bit. If you are the first to
+	 * set this bit then kvmppc_tb_resync_required() function will
+	 * return true. For rest all other subcores
+	 * kvmppc_tb_resync_required() will return false.
+	 *
+	 * If resync_req == true, then this thread is responsible to
+	 * initiate TB resync after hmi handler has completed.
+	 * All other threads on this core will wait until this thread
+	 * clears the resync required bit flag.
+	 */
+	resync_req = kvmppc_tb_resync_required();
+
+	/* Reset the subcore status to indicate it has exited guest */
+	kvmppc_subcore_exit_guest();
+
+	/*
+	 * Wait for other subcores on this core to exit the guest.
+	 * All the primary threads and threads from subcore that are
+	 * not in guest will wait here until all subcores are out
+	 * of guest context.
+	 */
+	wait_for_subcore_guest_exit();
+
+	/*
+	 * At this point we are sure that primary threads from each
+	 * subcore on this core have completed guest->host partition
+	 * switch. Now it is safe to call HMI handler.
+	 */
+	if (ppc_md.hmi_exception_early)
+		ppc_md.hmi_exception_early(NULL);
+
+	/*
+	 * Check if this thread is responsible to resync TB.
+	 * All other threads will wait until this thread completes the
+	 * TB resync.
+	 */
+	if (resync_req) {
+		opal_resync_timebase();
+		/* Reset TB resync req bit */
+		kvmppc_tb_resync_done();
+	} else {
+		wait_for_tb_resync();
+	}
+	return 0;
+}

commit 45706bb53d118b5340a12926e26444d73b6491f9
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Fri Dec 19 08:41:05 2014 +0530

    powerpc/book3s: Fix flush_tlb cpu_spec hook to take a generic argument.
    
    The flush_tlb hook in cpu_spec was introduced as a generic function hook
    to invalidate TLBs. But the current implementation of flush_tlb hook
    takes IS (invalidation selector) as an argument which is architecture
    dependent. Hence, It is not right to have a generic routine where caller
    has to pass non-generic argument.
    
    This patch fixes this and makes flush_tlb hook as high level API.
    
    Reported-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 60081bd75847..93b5f5c9b445 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -84,7 +84,7 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 		}
 		if (dsisr & DSISR_MC_TLB_MULTI) {
 			if (cur_cpu_spec && cur_cpu_spec->flush_tlb)
-				cur_cpu_spec->flush_tlb(TLBIEL_INVAL_SET_LPID);
+				cur_cpu_spec->flush_tlb(TLB_INVAL_SCOPE_LPID);
 			dsisr &= ~DSISR_MC_TLB_MULTI;
 		}
 		/* Any other errors we don't understand? */
@@ -102,7 +102,7 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 		break;
 	case SRR1_MC_IFETCH_TLBMULTI:
 		if (cur_cpu_spec && cur_cpu_spec->flush_tlb)
-			cur_cpu_spec->flush_tlb(TLBIEL_INVAL_SET_LPID);
+			cur_cpu_spec->flush_tlb(TLB_INVAL_SCOPE_LPID);
 		break;
 	default:
 		handled = 0;

commit c17b98cf6028704e1f953d6a25ed6140425ccfd0
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Dec 3 13:30:38 2014 +1100

    KVM: PPC: Book3S HV: Remove code for PPC970 processors
    
    This removes the code that was added to enable HV KVM to work
    on PPC970 processors.  The PPC970 is an old CPU that doesn't
    support virtualizing guest memory.  Removing PPC970 support also
    lets us remove the code for allocating and managing contiguous
    real-mode areas, the code for the !kvm->arch.using_mmu_notifiers
    case, the code for pinning pages of guest memory when first
    accessed and keeping track of which pages have been pinned, and
    the code for handling H_ENTER hypercalls in virtual mode.
    
    Book3S HV KVM is now supported only on POWER7 and POWER8 processors.
    The KVM_CAP_PPC_RMA capability now always returns 0.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index d562c8e2bc30..60081bd75847 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -138,8 +138,5 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 
 long kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu)
 {
-	if (cpu_has_feature(CPU_FTR_ARCH_206))
-		return kvmppc_realmode_mc_power7(vcpu);
-
-	return 0;
+	return kvmppc_realmode_mc_power7(vcpu);
 }

commit 02407552256111479fbfd23a3e01218b399aaa35
Author: Alexander Graf <agraf@suse.de>
Date:   Wed Jun 11 10:34:19 2014 +0200

    KVM: PPC: Book3S HV: Access guest VPA in BE
    
    There are a few shared data structures between the host and the guest. Most
    of them get registered through the VPA interface.
    
    These data structures are defined to always be in big endian byte order, so
    let's make sure we always access them in big endian.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 3a5c568b1e89..d562c8e2bc30 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -45,14 +45,14 @@ static void reload_slb(struct kvm_vcpu *vcpu)
 		return;
 
 	/* Sanity check */
-	n = min_t(u32, slb->persistent, SLB_MIN_SIZE);
+	n = min_t(u32, be32_to_cpu(slb->persistent), SLB_MIN_SIZE);
 	if ((void *) &slb->save_area[n] > vcpu->arch.slb_shadow.pinned_end)
 		return;
 
 	/* Load up the SLB from that */
 	for (i = 0; i < n; ++i) {
-		unsigned long rb = slb->save_area[i].esid;
-		unsigned long rs = slb->save_area[i].vsid;
+		unsigned long rb = be64_to_cpu(slb->save_area[i].esid);
+		unsigned long rs = be64_to_cpu(slb->save_area[i].vsid);
 
 		rb = (rb & ~0xFFFul) | i;	/* insert entry number */
 		asm volatile("slbmte %0,%1" : : "r" (rs), "r" (rb));

commit 74845bc2fa9c0e6b218821cd4e1eb7a552d3e503
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Jun 11 14:18:21 2014 +0530

    powerpc/book3s: Fix guest MC delivery mechanism to avoid soft lockups in guest.
    
    Currently we forward MCEs to guest which have been recovered by guest.
    And for unhandled errors we do not deliver the MCE to guest. It looks like
    with no support of FWNMI in qemu, guest just panics whenever we deliver the
    recovered MCEs to guest. Also, the existig code used to return to host for
    unhandled errors which was casuing guest to hang with soft lockups inside
    guest and makes it difficult to recover guest instance.
    
    This patch now forwards all fatal MCEs to guest causing guest to crash/panic.
    And, for recovered errors we just go back to normal functioning of guest
    instead of returning to host. This fixes soft lockup issues in guest.
    This patch also fixes an issue where guest MCE events were not logged to
    host console.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 768a9f977c00..3a5c568b1e89 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -113,10 +113,8 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 	 * We assume that if the condition is recovered then linux host
 	 * will have generated an error log event that we will pick
 	 * up and log later.
-	 * Don't release mce event now. In case if condition is not
-	 * recovered we do guest exit and go back to linux host machine
-	 * check handler. Hence we need make sure that current mce event
-	 * is available for linux host to consume.
+	 * Don't release mce event now. We will queue up the event so that
+	 * we can log the MCE event info on host console.
 	 */
 	if (!get_mce_event(&mce_evt, MCE_EVENT_DONTRELEASE))
 		goto out;
@@ -128,11 +126,12 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 
 out:
 	/*
-	 * If we have handled the error, then release the mce event because
-	 * we will be delivering machine check to guest.
+	 * We are now going enter guest either through machine check
+	 * interrupt (for unhandled errors) or will continue from
+	 * current HSRR0 (for handled errors) in guest. Hence
+	 * queue up the event so that we can log it from host console later.
 	 */
-	if (handled)
-		release_mce_event();
+	machine_check_queue_event();
 
 	return handled;
 }

commit 36df96f8acaf51992177645eb2d781f766ce97dc
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:05:40 2013 +0530

    powerpc/book3s: Decode and save machine check event.
    
    Now that we handle machine check in linux, the MCE decoding should also
    take place in linux host. This info is crucial to log before we go down
    in case we can not handle the machine check errors. This patch decodes
    and populates a machine check event which contain high level meaning full
    MCE information.
    
    We do this in real mode C code with ME bit on. The MCE information is still
    available on emergency stack (in pt_regs structure format). Even if we take
    another exception at this point the MCE early handler will allocate a new
    stack frame on top of current one. So when we return back here we still have
    our MCE information safe on current stack.
    
    We use per cpu buffer to save high level MCE information. Each per cpu buffer
    is an array of machine check event structure indexed by per cpu counter
    mce_nest_count. The mce_nest_count is incremented every time we enter
    machine check early handler in real mode to get the current free slot
    (index = mce_nest_count - 1). The mce_nest_count is decremented once the
    MCE info is consumed by virtual mode machine exception handler.
    
    This patch provides save_mce_event(), get_mce_event() and release_mce_event()
    generic routines that can be used by machine check handlers to populate and
    retrieve the event. The routine release_mce_event() will free the event slot so
    that it can be reused. Caller can invoke get_mce_event() with a release flag
    either to release the event slot immediately OR keep it so that it can be
    fetched again. The event slot can be also released anytime by invoking
    release_mce_event().
    
    This patch also updates kvm code to invoke get_mce_event to retrieve generic
    mce event rather than paca->opal_mce_evt.
    
    The KVM code always calls get_mce_event() with release flags set to false so
    that event is available for linus host machine
    
    If machine check occurs while we are in guest, KVM tries to handle the error.
    If KVM is able to handle MC error successfully, it enters the guest and
    delivers the machine check to guest. If KVM is not able to handle MC error, it
    exists the guest and passes the control to linux host machine check handler
    which then logs MC event and decides how to handle it in linux host. In failure
    case, KVM needs to make sure that the MC event is available for linux host to
    consume. Hence KVM always calls get_mce_event() with release flags set to false
    and later it invokes release_mce_event() only if it succeeds to handle error.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 5c427b41a2f5..768a9f977c00 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -12,6 +12,7 @@
 #include <linux/kvm_host.h>
 #include <linux/kernel.h>
 #include <asm/opal.h>
+#include <asm/mce.h>
 
 /* SRR1 bits for machine check on POWER7 */
 #define SRR1_MC_LDSTERR		(1ul << (63-42))
@@ -67,9 +68,7 @@ static void reload_slb(struct kvm_vcpu *vcpu)
 static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 {
 	unsigned long srr1 = vcpu->arch.shregs.msr;
-#ifdef CONFIG_PPC_POWERNV
-	struct opal_machine_check_event *opal_evt;
-#endif
+	struct machine_check_event mce_evt;
 	long handled = 1;
 
 	if (srr1 & SRR1_MC_LDSTERR) {
@@ -109,22 +108,31 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 		handled = 0;
 	}
 
-#ifdef CONFIG_PPC_POWERNV
 	/*
-	 * See if OPAL has already handled the condition.
-	 * We assume that if the condition is recovered then OPAL
+	 * See if we have already handled the condition in the linux host.
+	 * We assume that if the condition is recovered then linux host
 	 * will have generated an error log event that we will pick
 	 * up and log later.
+	 * Don't release mce event now. In case if condition is not
+	 * recovered we do guest exit and go back to linux host machine
+	 * check handler. Hence we need make sure that current mce event
+	 * is available for linux host to consume.
 	 */
-	opal_evt = local_paca->opal_mc_evt;
-	if (opal_evt->version == OpalMCE_V1 &&
-	    (opal_evt->severity == OpalMCE_SEV_NO_ERROR ||
-	     opal_evt->disposition == OpalMCE_DISPOSITION_RECOVERED))
+	if (!get_mce_event(&mce_evt, MCE_EVENT_DONTRELEASE))
+		goto out;
+
+	if (mce_evt.version == MCE_V1 &&
+	    (mce_evt.severity == MCE_SEV_NO_ERROR ||
+	     mce_evt.disposition == MCE_DISPOSITION_RECOVERED))
 		handled = 1;
 
+out:
+	/*
+	 * If we have handled the error, then release the mce event because
+	 * we will be delivering machine check to guest.
+	 */
 	if (handled)
-		opal_evt->in_use = 0;
-#endif
+		release_mce_event();
 
 	return handled;
 }

commit 0440705049b041d84268ea57f6e90e2f16618897
Author: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
Date:   Wed Oct 30 20:04:56 2013 +0530

    powerpc/book3s: Add flush_tlb operation in cpu_spec.
    
    This patch introduces flush_tlb operation in cpu_spec structure. This will
    help us to invoke appropriate CPU-side flush tlb routine. This patch
    adds the foundation to invoke CPU specific flush routine for respective
    architectures. Currently this patch introduce flush_tlb for p7 and p8.
    
    Signed-off-by: Mahesh Salgaonkar <mahesh@linux.vnet.ibm.com>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index a353c485808c..5c427b41a2f5 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -58,18 +58,6 @@ static void reload_slb(struct kvm_vcpu *vcpu)
 	}
 }
 
-/* POWER7 TLB flush */
-static void flush_tlb_power7(struct kvm_vcpu *vcpu)
-{
-	unsigned long i, rb;
-
-	rb = TLBIEL_INVAL_SET_LPID;
-	for (i = 0; i < POWER7_TLB_SETS; ++i) {
-		asm volatile("tlbiel %0" : : "r" (rb));
-		rb += 1 << TLBIEL_INVAL_SET_SHIFT;
-	}
-}
-
 /*
  * On POWER7, see if we can handle a machine check that occurred inside
  * the guest in real mode, without switching to the host partition.
@@ -96,7 +84,8 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 				   DSISR_MC_SLB_PARITY | DSISR_MC_DERAT_MULTI);
 		}
 		if (dsisr & DSISR_MC_TLB_MULTI) {
-			flush_tlb_power7(vcpu);
+			if (cur_cpu_spec && cur_cpu_spec->flush_tlb)
+				cur_cpu_spec->flush_tlb(TLBIEL_INVAL_SET_LPID);
 			dsisr &= ~DSISR_MC_TLB_MULTI;
 		}
 		/* Any other errors we don't understand? */
@@ -113,7 +102,8 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 		reload_slb(vcpu);
 		break;
 	case SRR1_MC_IFETCH_TLBMULTI:
-		flush_tlb_power7(vcpu);
+		if (cur_cpu_spec && cur_cpu_spec->flush_tlb)
+			cur_cpu_spec->flush_tlb(TLBIEL_INVAL_SET_LPID);
 		break;
 	default:
 		handled = 0;

commit d591390da94234d5b8eb1bbd9f2a25d2e780d528
Author: Andreas Schwab <schwab@linux-m68k.org>
Date:   Sat Dec 22 04:09:17 2012 +0000

    KVM: PPC: Book3S HV: Fix compilation without CONFIG_PPC_POWERNV
    
    Fixes this build breakage:
    
    arch/powerpc/kvm/book3s_hv_ras.c: In function ‘kvmppc_realmode_mc_power7’:
    arch/powerpc/kvm/book3s_hv_ras.c:126:23: error: ‘struct paca_struct’ has no member named ‘opal_mc_evt’
    
    Signed-off-by: Andreas Schwab <schwab@linux-m68k.org>
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
index 35f3cf0269b3..a353c485808c 100644
--- a/arch/powerpc/kvm/book3s_hv_ras.c
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -79,7 +79,9 @@ static void flush_tlb_power7(struct kvm_vcpu *vcpu)
 static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 {
 	unsigned long srr1 = vcpu->arch.shregs.msr;
+#ifdef CONFIG_PPC_POWERNV
 	struct opal_machine_check_event *opal_evt;
+#endif
 	long handled = 1;
 
 	if (srr1 & SRR1_MC_LDSTERR) {
@@ -117,6 +119,7 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 		handled = 0;
 	}
 
+#ifdef CONFIG_PPC_POWERNV
 	/*
 	 * See if OPAL has already handled the condition.
 	 * We assume that if the condition is recovered then OPAL
@@ -131,6 +134,7 @@ static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
 
 	if (handled)
 		opal_evt->in_use = 0;
+#endif
 
 	return handled;
 }

commit b4072df4076c4f33ac9f518052c318c979bca533
Author: Paul Mackerras <paulus@samba.org>
Date:   Fri Nov 23 22:37:50 2012 +0000

    KVM: PPC: Book3S HV: Handle guest-caused machine checks on POWER7 without panicking
    
    Currently, if a machine check interrupt happens while we are in the
    guest, we exit the guest and call the host's machine check handler,
    which tends to cause the host to panic.  Some machine checks can be
    triggered by the guest; for example, if the guest creates two entries
    in the SLB that map the same effective address, and then accesses that
    effective address, the CPU will take a machine check interrupt.
    
    To handle this better, when a machine check happens inside the guest,
    we call a new function, kvmppc_realmode_machine_check(), while still in
    real mode before exiting the guest.  On POWER7, it handles the cases
    that the guest can trigger, either by flushing and reloading the SLB,
    or by flushing the TLB, and then it delivers the machine check interrupt
    directly to the guest without going back to the host.  On POWER7, the
    OPAL firmware patches the machine check interrupt vector so that it
    gets control first, and it leaves behind its analysis of the situation
    in a structure pointed to by the opal_mc_evt field of the paca.  The
    kvmppc_realmode_machine_check() function looks at this, and if OPAL
    reports that there was no error, or that it has handled the error, we
    also go straight back to the guest with a machine check.  We have to
    deliver a machine check to the guest since the machine check interrupt
    might have trashed valid values in SRR0/1.
    
    If the machine check is one we can't handle in real mode, and one that
    OPAL hasn't already handled, or on PPC970, we exit the guest and call
    the host's machine check handler.  We do this by jumping to the
    machine_check_fwnmi label, rather than absolute address 0x200, because
    we don't want to re-execute OPAL's handler on POWER7.  On PPC970, the
    two are equivalent because address 0x200 just contains a branch.
    
    Then, if the host machine check handler decides that the system can
    continue executing, kvmppc_handle_exit() delivers a machine check
    interrupt to the guest -- once again to let the guest know that SRR0/1
    have been modified.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    [agraf: fix checkpatch warnings]
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/kvm/book3s_hv_ras.c b/arch/powerpc/kvm/book3s_hv_ras.c
new file mode 100644
index 000000000000..35f3cf0269b3
--- /dev/null
+++ b/arch/powerpc/kvm/book3s_hv_ras.c
@@ -0,0 +1,144 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License, version 2, as
+ * published by the Free Software Foundation.
+ *
+ * Copyright 2012 Paul Mackerras, IBM Corp. <paulus@au1.ibm.com>
+ */
+
+#include <linux/types.h>
+#include <linux/string.h>
+#include <linux/kvm.h>
+#include <linux/kvm_host.h>
+#include <linux/kernel.h>
+#include <asm/opal.h>
+
+/* SRR1 bits for machine check on POWER7 */
+#define SRR1_MC_LDSTERR		(1ul << (63-42))
+#define SRR1_MC_IFETCH_SH	(63-45)
+#define SRR1_MC_IFETCH_MASK	0x7
+#define SRR1_MC_IFETCH_SLBPAR		2	/* SLB parity error */
+#define SRR1_MC_IFETCH_SLBMULTI		3	/* SLB multi-hit */
+#define SRR1_MC_IFETCH_SLBPARMULTI	4	/* SLB parity + multi-hit */
+#define SRR1_MC_IFETCH_TLBMULTI		5	/* I-TLB multi-hit */
+
+/* DSISR bits for machine check on POWER7 */
+#define DSISR_MC_DERAT_MULTI	0x800		/* D-ERAT multi-hit */
+#define DSISR_MC_TLB_MULTI	0x400		/* D-TLB multi-hit */
+#define DSISR_MC_SLB_PARITY	0x100		/* SLB parity error */
+#define DSISR_MC_SLB_MULTI	0x080		/* SLB multi-hit */
+#define DSISR_MC_SLB_PARMULTI	0x040		/* SLB parity + multi-hit */
+
+/* POWER7 SLB flush and reload */
+static void reload_slb(struct kvm_vcpu *vcpu)
+{
+	struct slb_shadow *slb;
+	unsigned long i, n;
+
+	/* First clear out SLB */
+	asm volatile("slbmte %0,%0; slbia" : : "r" (0));
+
+	/* Do they have an SLB shadow buffer registered? */
+	slb = vcpu->arch.slb_shadow.pinned_addr;
+	if (!slb)
+		return;
+
+	/* Sanity check */
+	n = min_t(u32, slb->persistent, SLB_MIN_SIZE);
+	if ((void *) &slb->save_area[n] > vcpu->arch.slb_shadow.pinned_end)
+		return;
+
+	/* Load up the SLB from that */
+	for (i = 0; i < n; ++i) {
+		unsigned long rb = slb->save_area[i].esid;
+		unsigned long rs = slb->save_area[i].vsid;
+
+		rb = (rb & ~0xFFFul) | i;	/* insert entry number */
+		asm volatile("slbmte %0,%1" : : "r" (rs), "r" (rb));
+	}
+}
+
+/* POWER7 TLB flush */
+static void flush_tlb_power7(struct kvm_vcpu *vcpu)
+{
+	unsigned long i, rb;
+
+	rb = TLBIEL_INVAL_SET_LPID;
+	for (i = 0; i < POWER7_TLB_SETS; ++i) {
+		asm volatile("tlbiel %0" : : "r" (rb));
+		rb += 1 << TLBIEL_INVAL_SET_SHIFT;
+	}
+}
+
+/*
+ * On POWER7, see if we can handle a machine check that occurred inside
+ * the guest in real mode, without switching to the host partition.
+ *
+ * Returns: 0 => exit guest, 1 => deliver machine check to guest
+ */
+static long kvmppc_realmode_mc_power7(struct kvm_vcpu *vcpu)
+{
+	unsigned long srr1 = vcpu->arch.shregs.msr;
+	struct opal_machine_check_event *opal_evt;
+	long handled = 1;
+
+	if (srr1 & SRR1_MC_LDSTERR) {
+		/* error on load/store */
+		unsigned long dsisr = vcpu->arch.shregs.dsisr;
+
+		if (dsisr & (DSISR_MC_SLB_PARMULTI | DSISR_MC_SLB_MULTI |
+			     DSISR_MC_SLB_PARITY | DSISR_MC_DERAT_MULTI)) {
+			/* flush and reload SLB; flushes D-ERAT too */
+			reload_slb(vcpu);
+			dsisr &= ~(DSISR_MC_SLB_PARMULTI | DSISR_MC_SLB_MULTI |
+				   DSISR_MC_SLB_PARITY | DSISR_MC_DERAT_MULTI);
+		}
+		if (dsisr & DSISR_MC_TLB_MULTI) {
+			flush_tlb_power7(vcpu);
+			dsisr &= ~DSISR_MC_TLB_MULTI;
+		}
+		/* Any other errors we don't understand? */
+		if (dsisr & 0xffffffffUL)
+			handled = 0;
+	}
+
+	switch ((srr1 >> SRR1_MC_IFETCH_SH) & SRR1_MC_IFETCH_MASK) {
+	case 0:
+		break;
+	case SRR1_MC_IFETCH_SLBPAR:
+	case SRR1_MC_IFETCH_SLBMULTI:
+	case SRR1_MC_IFETCH_SLBPARMULTI:
+		reload_slb(vcpu);
+		break;
+	case SRR1_MC_IFETCH_TLBMULTI:
+		flush_tlb_power7(vcpu);
+		break;
+	default:
+		handled = 0;
+	}
+
+	/*
+	 * See if OPAL has already handled the condition.
+	 * We assume that if the condition is recovered then OPAL
+	 * will have generated an error log event that we will pick
+	 * up and log later.
+	 */
+	opal_evt = local_paca->opal_mc_evt;
+	if (opal_evt->version == OpalMCE_V1 &&
+	    (opal_evt->severity == OpalMCE_SEV_NO_ERROR ||
+	     opal_evt->disposition == OpalMCE_DISPOSITION_RECOVERED))
+		handled = 1;
+
+	if (handled)
+		opal_evt->in_use = 0;
+
+	return handled;
+}
+
+long kvmppc_realmode_machine_check(struct kvm_vcpu *vcpu)
+{
+	if (cpu_has_feature(CPU_FTR_ARCH_206))
+		return kvmppc_realmode_mc_power7(vcpu);
+
+	return 0;
+}
