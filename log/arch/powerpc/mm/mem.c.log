commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e2d6a6236aa7..c2c11eb8dcfc 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -38,7 +38,6 @@
 #include <asm/prom.h>
 #include <asm/io.h>
 #include <asm/mmu_context.h>
-#include <asm/pgtable.h>
 #include <asm/mmu.h>
 #include <asm/smp.h>
 #include <asm/machdep.h>

commit 885f7f8e30468705926b3de53d32925bd7a51a03
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Jun 7 21:42:22 2020 -0700

    mm: rename flush_icache_user_range to flush_icache_user_page
    
    The function currently known as flush_icache_user_range only operates on
    a single page.  Rename it to flush_icache_user_page as we'll need the
    name flush_icache_user_range for something else soon.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Stefan Kristiansson <stefan.kristiansson@saunalahti.fi>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Albert Ou <aou@eecs.berkeley.edu>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Link: http://lkml.kernel.org/r/20200515143646.3857579-20-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3f1a70c6781b..e2d6a6236aa7 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -577,7 +577,7 @@ void copy_user_page(void *vto, void *vfrom, unsigned long vaddr,
 	flush_dcache_page(pg);
 }
 
-void flush_icache_user_range(struct vm_area_struct *vma, struct page *page,
+void flush_icache_user_page(struct vm_area_struct *vma, struct page *page,
 			     unsigned long addr, int len)
 {
 	unsigned long maddr;

commit e292e7403e3fe048abec1a5fd83b3181748dcb1d
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Jun 7 21:41:32 2020 -0700

    powerpc: unexport flush_icache_user_range
    
    flush_icache_user_range is only used by copy_to_user_page, which is only
    used by core VM code.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Link: http://lkml.kernel.org/r/20200515143646.3857579-4-hch@lst.de
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5f7fe13211e9..3f1a70c6781b 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -586,7 +586,6 @@ void flush_icache_user_range(struct vm_area_struct *vma, struct page *page,
 	flush_icache_range(maddr, maddr + len);
 	kunmap(page);
 }
-EXPORT_SYMBOL(flush_icache_user_range);
 
 /*
  * System memory should not be in /proc/iomem but various tools expect it

commit 7ae77150d94d3b535c7b85e6b3647113095e79bf
Merge: 084623e468d5 1395375c5927
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 5 12:39:30 2020 -0700

    Merge tag 'powerpc-5.8-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
    
     - Support for userspace to send requests directly to the on-chip GZIP
       accelerator on Power9.
    
     - Rework of our lockless page table walking (__find_linux_pte()) to
       make it safe against parallel page table manipulations without
       relying on an IPI for serialisation.
    
     - A series of fixes & enhancements to make our machine check handling
       more robust.
    
     - Lots of plumbing to add support for "prefixed" (64-bit) instructions
       on Power10.
    
     - Support for using huge pages for the linear mapping on 8xx (32-bit).
    
     - Remove obsolete Xilinx PPC405/PPC440 support, and an associated sound
       driver.
    
     - Removal of some obsolete 40x platforms and associated cruft.
    
     - Initial support for booting on Power10.
    
     - Lots of other small features, cleanups & fixes.
    
    Thanks to: Alexey Kardashevskiy, Alistair Popple, Andrew Donnellan,
    Andrey Abramov, Aneesh Kumar K.V, Balamuruhan S, Bharata B Rao, Bulent
    Abali, CÃ©dric Le Goater, Chen Zhou, Christian Zigotzky, Christophe
    JAILLET, Christophe Leroy, Dmitry Torokhov, Emmanuel Nicolet, Erhard F.,
    Gautham R. Shenoy, Geoff Levand, George Spelvin, Greg Kurz, Gustavo A.
    R. Silva, Gustavo Walbon, Haren Myneni, Hari Bathini, Joel Stanley,
    Jordan Niethe, Kajol Jain, Kees Cook, Leonardo Bras, Madhavan
    Srinivasan., Mahesh Salgaonkar, Markus Elfring, Michael Neuling, Michal
    Simek, Nathan Chancellor, Nathan Lynch, Naveen N. Rao, Nicholas Piggin,
    Oliver O'Halloran, Paul Mackerras, Pingfan Liu, Qian Cai, Ram Pai,
    Raphael Moreira Zinsly, Ravi Bangoria, Sam Bobroff, Sandipan Das, Segher
    Boessenkool, Stephen Rothwell, Sukadev Bhattiprolu, Tyrel Datwyler,
    Wolfram Sang, Xiongfeng Wang.
    
    * tag 'powerpc-5.8-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (299 commits)
      powerpc/pseries: Make vio and ibmebus initcalls pseries specific
      cxl: Remove dead Kconfig options
      powerpc: Add POWER10 architected mode
      powerpc/dt_cpu_ftrs: Add MMA feature
      powerpc/dt_cpu_ftrs: Enable Prefixed Instructions
      powerpc/dt_cpu_ftrs: Advertise support for ISA v3.1 if selected
      powerpc: Add support for ISA v3.1
      powerpc: Add new HWCAP bits
      powerpc/64s: Don't set FSCR bits in INIT_THREAD
      powerpc/64s: Save FSCR to init_task.thread.fscr after feature init
      powerpc/64s: Don't let DT CPU features set FSCR_DSCR
      powerpc/64s: Don't init FSCR_DSCR in __init_FSCR()
      powerpc/32s: Fix another build failure with CONFIG_PPC_KUAP_DEBUG
      powerpc/module_64: Use special stub for _mcount() with -mprofile-kernel
      powerpc/module_64: Simplify check for -mprofile-kernel ftrace relocations
      powerpc/module_64: Consolidate ftrace code
      powerpc/32: Disable KASAN with pages bigger than 16k
      powerpc/uaccess: Don't set KUEP by default on book3s/32
      powerpc/uaccess: Don't set KUAP by default on book3s/32
      powerpc/8xx: Reduce time spent in allow_user_access() and friends
      ...

commit db458d73fa35e256bba90b59a6776810800c8bb6
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Thu Jun 4 16:47:50 2020 -0700

    arch/kmap: ensure kmap_prot visibility
    
    We want to support kmap_atomic_prot() on all architectures and it makes
    sense to define kmap_atomic() to use the default kmap_prot.
    
    So we ensure all arch's have a globally available kmap_prot either as a
    define or exported symbol.
    
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Christian KÃ¶nig <christian.koenig@amd.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20200507150004.1423069-9-ira.weiny@intel.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0fcea21f26b4..7cebb9c818d3 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -64,8 +64,6 @@ bool init_mem_is_free;
 #ifdef CONFIG_HIGHMEM
 pte_t *kmap_pte;
 EXPORT_SYMBOL(kmap_pte);
-pgprot_t kmap_prot;
-EXPORT_SYMBOL(kmap_prot);
 #endif
 
 pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
@@ -245,7 +243,6 @@ void __init paging_init(void)
 	pkmap_page_table = virt_to_kpte(PKMAP_BASE);
 
 	kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
-	kmap_prot = PAGE_KERNEL;
 #endif /* CONFIG_HIGHMEM */
 
 	printk(KERN_DEBUG "Top of RAM: 0x%llx, Total RAM: 0x%llx\n",

commit 9691a071aa26a21fc8dac804a2b98d3c24f76f9a
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Jun 3 15:57:10 2020 -0700

    mm: use free_area_init() instead of free_area_init_nodes()
    
    free_area_init() has effectively became a wrapper for
    free_area_init_nodes() and there is no point of keeping it.  Still
    free_area_init() name is shorter and more general as it does not imply
    necessity to initialize multiple nodes.
    
    Rename free_area_init_nodes() to free_area_init(), update the callers and
    drop old version of free_area_init().
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Tested-by: Hoan Tran <hoan@os.amperecomputing.com>      [arm64]
    Reviewed-by: Baoquan He <bhe@redhat.com>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: "James E.J. Bottomley" <James.Bottomley@HansenPartnership.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200412194859.12663-6-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 041ed7cfd341..0fcea21f26b4 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -271,7 +271,7 @@ void __init paging_init(void)
 	max_zone_pfns[ZONE_HIGHMEM] = max_pfn;
 #endif
 
-	free_area_init_nodes(max_zone_pfns);
+	free_area_init(max_zone_pfns);
 
 	mark_nonram_nosave();
 }

commit a64371b5d4fb37199dcd04cb7bf0132894018e33
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Mar 31 16:03:40 2020 +0000

    powerpc/mem: Blacklist flush_dcache_icache_phys() for kprobe
    
    kprobe does not handle events happening in real mode, all
    functions running with MMU disabled have to be blacklisted.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/eaab3bff961c3bfe149f1d0bd3593291ef939dcc.1585670437.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 041ed7cfd341..f9c20cc23718 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -32,6 +32,7 @@
 #include <linux/vmalloc.h>
 #include <linux/memremap.h>
 #include <linux/dma-direct.h>
+#include <linux/kprobes.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -468,6 +469,7 @@ static void flush_dcache_icache_phys(unsigned long physaddr)
 		: "r" (nb), "r" (msr), "i" (bytes), "r" (msr0)
 		: "ctr", "memory");
 }
+NOKPROBE_SYMBOL(flush_dcache_icache_phys)
 #endif // !defined(CONFIG_PPC_8xx) && !defined(CONFIG_PPC64)
 
 /*

commit bfeb022f8fe4c5afdcfd7a3d868fac9765f9bcad
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Apr 10 14:33:36 2020 -0700

    mm/memory_hotplug: add pgprot_t to mhp_params
    
    devm_memremap_pages() is currently used by the PCI P2PDMA code to create
    struct page mappings for IO memory.  At present, these mappings are
    created with PAGE_KERNEL which implies setting the PAT bits to be WB.
    However, on x86, an mtrr register will typically override this and force
    the cache type to be UC-.  In the case firmware doesn't set this
    register it is effectively WB and will typically result in a machine
    check exception when it's accessed.
    
    Other arches are not currently likely to function correctly seeing they
    don't have any MTRR registers to fall back on.
    
    To solve this, provide a way to specify the pgprot value explicitly to
    arch_add_memory().
    
    Of the arches that support MEMORY_HOTPLUG: x86_64, and arm64 need a
    simple change to pass the pgprot_t down to their respective functions
    which set up the page tables.  For x86_32, set the page tables
    explicitly using _set_memory_prot() (seeing they are already mapped).
    
    For ia64, s390 and sh, reject anything but PAGE_KERNEL settings -- this
    should be fine, for now, seeing these architectures don't support
    ZONE_DEVICE.
    
    A check in __add_pages() is also added to ensure the pgprot parameter
    was set for all arches.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Badger <ebadger@gigaio.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200306170846.9333-7-logang@deltatee.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index bf63ab04db63..041ed7cfd341 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -132,7 +132,8 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 	resize_hpt_for_hotplug(memblock_phys_mem_size());
 
 	start = (unsigned long)__va(start);
-	rc = create_section_mapping(start, start + size, nid, PAGE_KERNEL);
+	rc = create_section_mapping(start, start + size, nid,
+				    params->pgprot);
 	if (rc) {
 		pr_warn("Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
 			start, start + size, rc);

commit 4e00c5affdd4b04e6392001716333971932f3d0c
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Apr 10 14:33:32 2020 -0700

    powerpc/mm: thread pgprot_t through create_section_mapping()
    
    In prepartion to support a pgprot_t argument for arch_add_memory().
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Eric Badger <ebadger@gigaio.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200306170846.9333-6-logang@deltatee.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e1cc58115816..bf63ab04db63 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -90,7 +90,8 @@ int memory_add_physaddr_to_nid(u64 start)
 }
 #endif
 
-int __weak create_section_mapping(unsigned long start, unsigned long end, int nid)
+int __weak create_section_mapping(unsigned long start, unsigned long end,
+				  int nid, pgprot_t prot)
 {
 	return -ENODEV;
 }
@@ -131,7 +132,7 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 	resize_hpt_for_hotplug(memblock_phys_mem_size());
 
 	start = (unsigned long)__va(start);
-	rc = create_section_mapping(start, start + size, nid);
+	rc = create_section_mapping(start, start + size, nid, PAGE_KERNEL);
 	if (rc) {
 		pr_warn("Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
 			start, start + size, rc);

commit f5637d3b42ab0465ef71d5fb8461bce97fba95e8
Author: Logan Gunthorpe <logang@deltatee.com>
Date:   Fri Apr 10 14:33:21 2020 -0700

    mm/memory_hotplug: rename mhp_restrictions to mhp_params
    
    The mhp_restrictions struct really doesn't specify anything resembling a
    restriction anymore so rename it to be mhp_params as it is a list of
    extended parameters.
    
    Signed-off-by: Logan Gunthorpe <logang@deltatee.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Eric Badger <ebadger@gigaio.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will@kernel.org>
    Link: http://lkml.kernel.org/r/20200306170846.9333-3-logang@deltatee.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 9b4f5fb719e0..e1cc58115816 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -122,7 +122,7 @@ static void flush_dcache_range_chunked(unsigned long start, unsigned long stop,
 }
 
 int __ref arch_add_memory(int nid, u64 start, u64 size,
-			struct mhp_restrictions *restrictions)
+			  struct mhp_params *params)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
@@ -138,7 +138,7 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 		return -EFAULT;
 	}
 
-	return __add_pages(nid, start_pfn, nr_pages, restrictions);
+	return __add_pages(nid, start_pfn, nr_pages, params);
 }
 
 void __ref arch_remove_memory(int nid, u64 start, u64 size,

commit 819723a8a2fb66678fe15d177ae56af38163f364
Merge: 247257b03b04 59bee45b9712
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Mar 10 15:16:42 2020 +1100

    Merge branch 'fixes' into next
    
    Merge in our fixes branch. In particular we want to merge the TM and KUAP fixes,
    so we can add selftests for them in next.

commit 59bee45b9712c759ea4d3dcc4eff1752f3a66558
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Mar 3 23:28:47 2020 +1100

    powerpc/mm: Fix missing KUAP disable in flush_coherent_icache()
    
    Stefan reported a strange kernel fault which turned out to be due to a
    missing KUAP disable in flush_coherent_icache() called from
    flush_icache_range().
    
    The fault looks like:
    
      Kernel attempted to access user page (7fffc30d9c00) - exploit attempt? (uid: 1009)
      BUG: Unable to handle kernel data access on read at 0x7fffc30d9c00
      Faulting instruction address: 0xc00000000007232c
      Oops: Kernel access of bad area, sig: 11 [#1]
      LE PAGE_SIZE=64K MMU=Radix SMP NR_CPUS=2048 NUMA PowerNV
      CPU: 35 PID: 5886 Comm: sigtramp Not tainted 5.6.0-rc2-gcc-8.2.0-00003-gfc37a1632d40 #79
      NIP:  c00000000007232c LR: c00000000003b7fc CTR: 0000000000000000
      REGS: c000001e11093940 TRAP: 0300   Not tainted  (5.6.0-rc2-gcc-8.2.0-00003-gfc37a1632d40)
      MSR:  900000000280b033 <SF,HV,VEC,VSX,EE,FP,ME,IR,DR,RI,LE>  CR: 28000884  XER: 00000000
      CFAR: c0000000000722fc DAR: 00007fffc30d9c00 DSISR: 08000000 IRQMASK: 0
      GPR00: c00000000003b7fc c000001e11093bd0 c0000000023ac200 00007fffc30d9c00
      GPR04: 00007fffc30d9c18 0000000000000000 c000001e11093bd4 0000000000000000
      GPR08: 0000000000000000 0000000000000001 0000000000000000 c000001e1104ed80
      GPR12: 0000000000000000 c000001fff6ab380 c0000000016be2d0 4000000000000000
      GPR16: c000000000000000 bfffffffffffffff 0000000000000000 0000000000000000
      GPR20: 00007fffc30d9c00 00007fffc30d8f58 00007fffc30d9c18 00007fffc30d9c20
      GPR24: 00007fffc30d9c18 0000000000000000 c000001e11093d90 c000001e1104ed80
      GPR28: c000001e11093e90 0000000000000000 c0000000023d9d18 00007fffc30d9c00
      NIP flush_icache_range+0x5c/0x80
      LR  handle_rt_signal64+0x95c/0xc2c
      Call Trace:
        0xc000001e11093d90 (unreliable)
        handle_rt_signal64+0x93c/0xc2c
        do_notify_resume+0x310/0x430
        ret_from_except_lite+0x70/0x74
      Instruction dump:
      409e002c 7c0802a6 3c62ff31 3863f6a0 f8010080 48195fed 60000000 48fe4c8d
      60000000 e8010080 7c0803a6 7c0004ac <7c00ffac> 7c0004ac 4c00012c 38210070
    
    This path through handle_rt_signal64() to setup_trampoline() and
    flush_icache_range() is only triggered by 64-bit processes that have
    unmapped their VDSO, which is rare.
    
    flush_icache_range() takes a range of addresses to flush. In
    flush_coherent_icache() we implement an optimisation for CPUs where we
    know we don't actually have to flush the whole range, we just need to
    do a single icbi.
    
    However we still execute the icbi on the user address of the start of
    the range we're flushing. On CPUs that also implement KUAP (Power9)
    that leads to the spurious fault above.
    
    We should be able to pass any address, including a kernel address, to
    the icbi on these CPUs, which would avoid any interaction with KUAP.
    But I don't want to make that change in a bug fix, just in case it
    surfaces some strange behaviour on some CPU.
    
    So for now just disable KUAP around the icbi. Note the icbi is treated
    as a load, so we allow read access, not write as you'd expect.
    
    Fixes: 890274c2dc4c ("powerpc/64s: Implement KUAP for Radix MMU")
    Cc: stable@vger.kernel.org # v5.2+
    Reported-by: Stefan Berger <stefanb@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200303235708.26004-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index ef7b1119b2e2..1c07d5a3f543 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -373,7 +373,9 @@ static inline bool flush_coherent_icache(unsigned long addr)
 	 */
 	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE)) {
 		mb(); /* sync */
+		allow_read_from_user((const void __user *)addr, L1_CACHE_BYTES);
 		icbi((void *)addr);
+		prevent_read_from_user((const void __user *)addr, L1_CACHE_BYTES);
 		mb(); /* sync */
 		isync();
 		return true;

commit 2efc7c085f05870eda6f29ac71eeb83f3bd54415
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 9 08:25:26 2020 +0000

    powerpc/32: drop get_pteptr()
    
    Commit 8d30c14cab30 ("powerpc/mm: Rework I$/D$ coherency (v3)") and
    commit 90ac19a8b21b ("[POWERPC] Abolish iopa(), mm_ptov(),
    io_block_mapping() from arch/powerpc") removed the use of get_pteptr()
    outside of mm/pgtable_32.c
    
    In mm/pgtable_32.c, the only user of get_pteptr() is change_page_attr()
    which operates on kernel context and on lowmem pages only.
    
    Make virt_to_kpte() available outside of mm/mem.c and use it instead
    of get_pteptr(), and drop get_pteptr()
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/788378c6c3ba5c5298caab7c7f95e6c3c88244b8.1578558199.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index b7325bb4c890..4f852d2a62f5 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -66,11 +66,6 @@ pte_t *kmap_pte;
 EXPORT_SYMBOL(kmap_pte);
 pgprot_t kmap_prot;
 EXPORT_SYMBOL(kmap_prot);
-
-static inline pte_t *virt_to_kpte(unsigned long vaddr)
-{
-	return pte_offset_kernel(pmd_ptr_k(vaddr), vaddr);
-}
 #endif
 
 pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,

commit 0b1c524caaae2428b20e714297243e5551251eb5
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jan 9 08:25:25 2020 +0000

    powerpc/32: refactor pmd_offset(pud_offset(pgd_offset...
    
    At several places pmd pointer is retrieved through the same action:
    
            pmd = pmd_offset(pud_offset(pgd_offset(mm, addr), addr), addr);
    
    or
    
            pmd = pmd_offset(pud_offset(pgd_offset_k(addr), addr), addr);
    
    Refactor this by implementing two helpers pmd_ptr() and pmd_ptr_k()
    
    This will help when adding the p4d level.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/7b065c5be35726af4066cab238ee35cabceda1fa.1578558199.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index ef7b1119b2e2..b7325bb4c890 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -69,8 +69,7 @@ EXPORT_SYMBOL(kmap_prot);
 
 static inline pte_t *virt_to_kpte(unsigned long vaddr)
 {
-	return pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(vaddr),
-			vaddr), vaddr), vaddr);
+	return pte_offset_kernel(pmd_ptr_k(vaddr), vaddr);
 }
 #endif
 

commit 3d4247fcc938d0ab5cf6fdb752dae07fdeab9736
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Jan 14 17:54:00 2020 +0000

    powerpc/32: Add support of KASAN_VMALLOC
    
    Add support of KASAN_VMALLOC on PPC32.
    
    To allow this, the early shadow covering the VMALLOC space
    need to be removed once high_memory var is set and before
    freeing memblock.
    
    And the VMALLOC area need to be aligned such that boundaries
    are covered by a full shadow page.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/031dec5487bde9b2181c8b3c9800e1879cf98c1a.1579024426.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f5535eae637f..ef7b1119b2e2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -49,6 +49,7 @@
 #include <asm/fixmap.h>
 #include <asm/swiotlb.h>
 #include <asm/rtas.h>
+#include <asm/kasan.h>
 
 #include <mm/mmu_decl.h>
 
@@ -301,6 +302,9 @@ void __init mem_init(void)
 
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 	set_max_mapnr(max_pfn);
+
+	kasan_late_init();
+
 	memblock_free_all();
 
 #ifdef CONFIG_HIGHMEM

commit feee6b2989165631b17ac6d4ccdbf6759254e85a
Author: David Hildenbrand <david@redhat.com>
Date:   Sat Jan 4 12:59:33 2020 -0800

    mm/memory_hotplug: shrink zones when offlining memory
    
    We currently try to shrink a single zone when removing memory.  We use
    the zone of the first page of the memory we are removing.  If that
    memmap was never initialized (e.g., memory was never onlined), we will
    read garbage and can trigger kernel BUGs (due to a stale pointer):
    
        BUG: unable to handle page fault for address: 000000000000353d
        #PF: supervisor write access in kernel mode
        #PF: error_code(0x0002) - not-present page
        PGD 0 P4D 0
        Oops: 0002 [#1] SMP PTI
        CPU: 1 PID: 7 Comm: kworker/u8:0 Not tainted 5.3.0-rc5-next-20190820+ #317
        Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.12.1-0-ga5cab58e9a3f-prebuilt.qemu.4
        Workqueue: kacpi_hotplug acpi_hotplug_work_fn
        RIP: 0010:clear_zone_contiguous+0x5/0x10
        Code: 48 89 c6 48 89 c3 e8 2a fe ff ff 48 85 c0 75 cf 5b 5d c3 c6 85 fd 05 00 00 01 5b 5d c3 0f 1f 840
        RSP: 0018:ffffad2400043c98 EFLAGS: 00010246
        RAX: 0000000000000000 RBX: 0000000200000000 RCX: 0000000000000000
        RDX: 0000000000200000 RSI: 0000000000140000 RDI: 0000000000002f40
        RBP: 0000000140000000 R08: 0000000000000000 R09: 0000000000000001
        R10: 0000000000000000 R11: 0000000000000000 R12: 0000000000140000
        R13: 0000000000140000 R14: 0000000000002f40 R15: ffff9e3e7aff3680
        FS:  0000000000000000(0000) GS:ffff9e3e7bb00000(0000) knlGS:0000000000000000
        CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
        CR2: 000000000000353d CR3: 0000000058610000 CR4: 00000000000006e0
        DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
        DR3: 0000000000000000 DR6: 00000000fffe0ff0 DR7: 0000000000000400
        Call Trace:
         __remove_pages+0x4b/0x640
         arch_remove_memory+0x63/0x8d
         try_remove_memory+0xdb/0x130
         __remove_memory+0xa/0x11
         acpi_memory_device_remove+0x70/0x100
         acpi_bus_trim+0x55/0x90
         acpi_device_hotplug+0x227/0x3a0
         acpi_hotplug_work_fn+0x1a/0x30
         process_one_work+0x221/0x550
         worker_thread+0x50/0x3b0
         kthread+0x105/0x140
         ret_from_fork+0x3a/0x50
        Modules linked in:
        CR2: 000000000000353d
    
    Instead, shrink the zones when offlining memory or when onlining failed.
    Introduce and use remove_pfn_range_from_zone(() for that.  We now
    properly shrink the zones, even if we have DIMMs whereby
    
     - Some memory blocks fall into no zone (never onlined)
    
     - Some memory blocks fall into multiple zones (offlined+re-onlined)
    
     - Multiple memory blocks that fall into different zones
    
    Drop the zone parameter (with a potential dubious value) from
    __remove_pages() and __remove_section().
    
    Link: http://lkml.kernel.org/r/20191006085646.5768-6-david@redhat.com
    Fixes: f1dd2cd13c4b ("mm, memory_hotplug: do not associate hotadded memory to zones until online")      [visible after d0dc12e86b319]
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: "Matthew Wilcox (Oracle)" <willy@infradead.org>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.ibm.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: <stable@vger.kernel.org>    [5.0+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 617c2777926f..f5535eae637f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -151,10 +151,9 @@ void __ref arch_remove_memory(int nid, u64 start, u64 size,
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
-	struct page *page = pfn_to_page(start_pfn) + vmem_altmap_offset(altmap);
 	int ret;
 
-	__remove_pages(page_zone(page), start_pfn, nr_pages, altmap);
+	__remove_pages(start_pfn, nr_pages, altmap);
 
 	/* Remove htab bolted mappings for this section of memory */
 	start = (unsigned long)__va(start);

commit 8fabc623238e68b3ac63c0dd1657bf86c1fa33af
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Wed Dec 4 14:35:24 2019 +0200

    powerpc: Ensure that swiotlb buffer is allocated from low memory
    
    Some powerpc platforms (e.g. 85xx) limit DMA-able memory way below 4G.
    If a system has more physical memory than this limit, the swiotlb
    buffer is not addressable because it is allocated from memblock using
    top-down mode.
    
    Force memblock to bottom-up mode before calling swiotlb_init() to
    ensure that the swiotlb buffer is DMA-able.
    
    Reported-by: Christian Zigotzky <chzigotzky@xenosoft.de>
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191204123524.22919-1-rppt@kernel.org

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 9488b63dfc87..617c2777926f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -289,6 +289,14 @@ void __init mem_init(void)
 	BUILD_BUG_ON(MMU_PAGE_COUNT > 16);
 
 #ifdef CONFIG_SWIOTLB
+	/*
+	 * Some platforms (e.g. 85xx) limit DMA-able memory way below
+	 * 4G. We force memblock to bottom-up mode to ensure that the
+	 * memory allocated in swiotlb_init() is DMA-able.
+	 * As it's the last memblock allocation, no need to reset it
+	 * back to to-down.
+	 */
+	memblock_set_bottom_up(true);
 	swiotlb_init(0);
 #endif
 

commit 6f4679b956741d2da6ad3ebb738cbe1264ac8781
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Dec 4 10:59:09 2019 +0530

    powerpc/pmem: Fix kernel crash due to wrong range value usage in flush_dcache_range
    
    This patch fix the below kernel crash.
    
     BUG: Unable to handle kernel data access on read at 0xc000000380000000
     Faulting instruction address: 0xc00000000008b6f0
    cpu 0x5: Vector: 300 (Data Access) at [c0000000d8587790]
        pc: c00000000008b6f0: arch_remove_memory+0x150/0x210
        lr: c00000000008b720: arch_remove_memory+0x180/0x210
        sp: c0000000d8587a20
       msr: 800000000280b033
       dar: c000000380000000
     dsisr: 40000000
      current = 0xc0000000d8558600
      paca    = 0xc00000000fff8f00   irqmask: 0x03   irq_happened: 0x01
        pid   = 1220, comm = ndctl
    enter ? for help
     memunmap_pages+0x33c/0x410
     devm_action_release+0x30/0x50
     release_nodes+0x30c/0x3a0
     device_release_driver_internal+0x178/0x240
     unbind_store+0x74/0x190
     drv_attr_store+0x44/0x60
     sysfs_kf_write+0x74/0xa0
     kernfs_fop_write+0x1b0/0x260
     __vfs_write+0x3c/0x70
     vfs_write+0xe4/0x200
     ksys_write+0x7c/0x140
     system_call+0x5c/0x68
    
    Fixes: 076265907cf9 ("powerpc: Chunk calls to flush_dcache_range in arch_*_memory")
    Reported-by: Sachin Sant <sachinp@linux.vnet.ibm.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191204052909.59145-1-aneesh.kumar@linux.ibm.com

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index ad299e72ec30..9488b63dfc87 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -121,7 +121,7 @@ static void flush_dcache_range_chunked(unsigned long start, unsigned long stop,
 	unsigned long i;
 
 	for (i = start; i < stop; i += chunk) {
-		flush_dcache_range(i, min(stop, start + chunk));
+		flush_dcache_range(i, min(stop, i + chunk));
 		cond_resched();
 	}
 }

commit 7794b1d4185e2587af46435e3e2f6696dae314c7
Merge: 9dd0013824fc 2807273f5e88
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 30 14:35:43 2019 -0800

    Merge tag 'powerpc-5.5-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
    
       - Infrastructure for secure boot on some bare metal Power9 machines.
         The firmware support is still in development, so the code here
         won't actually activate secure boot on any existing systems.
    
       - A change to xmon (our crash handler / pseudo-debugger) to restrict
         it to read-only mode when the kernel is lockdown'ed, otherwise it's
         trivial to drop into xmon and modify kernel data, such as the
         lockdown state.
    
       - Support for KASLR on 32-bit BookE machines (Freescale / NXP).
    
       - Fixes for our flush_icache_range() and __kernel_sync_dicache()
         (VDSO) to work with memory ranges >4GB.
    
       - Some reworks of the pseries CMM (Cooperative Memory Management)
         driver to make it behave more like other balloon drivers and enable
         some cleanups of generic mm code.
    
       - A series of fixes to our hardware breakpoint support to properly
         handle unaligned watchpoint addresses.
    
      Plus a bunch of other smaller improvements, fixes and cleanups.
    
      Thanks to: Alastair D'Silva, Andrew Donnellan, Aneesh Kumar K.V,
      Anthony Steinhauser, CÃ©dric Le Goater, Chris Packham, Chris Smart,
      Christophe Leroy, Christopher M. Riedl, Christoph Hellwig, Claudio
      Carvalho, Daniel Axtens, David Hildenbrand, Deb McLemore, Diana
      Craciun, Eric Richter, Geert Uytterhoeven, Greg Kroah-Hartman, Greg
      Kurz, Gustavo L. F. Walbon, Hari Bathini, Harish, Jason Yan, Krzysztof
      Kozlowski, Leonardo Bras, Mathieu Malaterre, Mauro S. M. Rodrigues,
      Michal Suchanek, Mimi Zohar, Nathan Chancellor, Nathan Lynch, Nayna
      Jain, Nick Desaulniers, Oliver O'Halloran, Qian Cai, Rasmus Villemoes,
      Ravi Bangoria, Sam Bobroff, Santosh Sivaraj, Scott Wood, Thomas Huth,
      Tyrel Datwyler, Vaibhav Jain, Valentin Longchamp, YueHaibing"
    
    * tag 'powerpc-5.5-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (144 commits)
      powerpc/fixmap: fix crash with HIGHMEM
      x86/efi: remove unused variables
      powerpc: Define arch_is_kernel_initmem_freed() for lockdep
      powerpc/prom_init: Use -ffreestanding to avoid a reference to bcmp
      powerpc: Avoid clang warnings around setjmp and longjmp
      powerpc: Don't add -mabi= flags when building with Clang
      powerpc: Fix Kconfig indentation
      powerpc/fixmap: don't clear fixmap area in paging_init()
      selftests/powerpc: spectre_v2 test must be built 64-bit
      powerpc/powernv: Disable native PCIe port management
      powerpc/kexec: Move kexec files into a dedicated subdir.
      powerpc/32: Split kexec low level code out of misc_32.S
      powerpc/sysdev: drop simple gpio
      powerpc/83xx: map IMMR with a BAT.
      powerpc/32s: automatically allocate BAT in setbat()
      powerpc/ioremap: warn on early use of ioremap()
      powerpc: Add support for GENERIC_EARLY_IOREMAP
      powerpc/fixmap: Use __fix_to_virt() instead of fix_to_virt()
      powerpc/8xx: use the fixmapped IMMR in cpm_reset()
      powerpc/8xx: add __init to cpm1 init functions
      ...

commit 2807273f5e88ed086d7d5d838fdee71e11e5085f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Nov 28 07:59:22 2019 +0000

    powerpc/fixmap: fix crash with HIGHMEM
    
    Commit f2bb86937d86 ("powerpc/fixmap: don't clear fixmap area in
    paging_init()") removed the clearing of fixmap area in order to
    avoid clearing fixmapped areas set earlier.
    
    However unlike all other users of fixmap which use __set_fixmap(),
    HIGHMEM functions directly use __set_pte_at(). This means
    the page table must pre-exist, otherwise the following crash
    can be encoutered due to the lack of entry in the PGD.
    
    Oops: Kernel access of bad area, sig: 11 [#1]
    BE PAGE_SIZE=4K MMU=Hash PowerMac
    Modules linked in:
    CPU: 0 PID: 1 Comm: swapper Not tainted 5.4.0+ #2528
    NIP:  c0144ce8 LR: c0144ccc CTR: 00000080
    REGS: ef0b5aa0 TRAP: 0300   Not tainted  (5.4.0+)
    MSR:  00009032 <EE,ME,IR,DR,RI>  CR: 44282842  XER: 00000000
    DAR: fffdf000 DSISR: 42000000
    GPR00: c0144ccc ef0b5b58 ef0b0000 fffdf000 fffdf000 00000000 c0000f7c 00000000
    GPR08: c0833000 fffdf000 00000000 ef1c53c9 24042842 00000000 00000000 00000000
    GPR16: 00000000 00000000 ef7e7358 effe8160 00000000 c08a9660 c0851644 00000004
    GPR24: c08c70a8 00002dc2 00000000 00000001 00000201 effe8160 effe8160 00000000
    NIP [c0144ce8] prep_new_page+0x138/0x178
    LR [c0144ccc] prep_new_page+0x11c/0x178
    Call Trace:
    [ef0b5b58] [c0144ccc] prep_new_page+0x11c/0x178 (unreliable)
    [ef0b5b88] [c0147218] get_page_from_freelist+0x1fc/0xd88
    [ef0b5c38] [c0148328] __alloc_pages_nodemask+0xd4/0xbb4
    [ef0b5cf8] [c0142ba8] __vmalloc_node_range+0x1b4/0x2e0
    [ef0b5d38] [c0142dd0] vzalloc+0x48/0x58
    [ef0b5d58] [c0301c8c] check_partition+0x58/0x244
    [ef0b5d78] [c02ffe80] blk_add_partitions+0x44/0x2cc
    [ef0b5db8] [c01a32d8] bdev_disk_changed+0x68/0xfc
    [ef0b5de8] [c01a4494] __blkdev_get+0x290/0x460
    [ef0b5e28] [c02fdd40] __device_add_disk+0x480/0x4d8
    [ef0b5e68] [c0810688] brd_init+0xc0/0x188
    [ef0b5e88] [c0005194] do_one_initcall+0x40/0x19c
    [ef0b5ee8] [c07dd4dc] kernel_init_freeable+0x164/0x230
    [ef0b5f28] [c0005408] kernel_init+0x18/0x10c
    [ef0b5f38] [c0014274] ret_from_kernel_thread+0x14/0x1c
    
    Partially revert that commit to still clear the fixmap area dedicated
    to HIGHMEM.
    
    Fixes: f2bb86937d86 ("powerpc/fixmap: don't clear fixmap area in paging_init()")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/d42fa9747df5afa41e67b08e374c98d3b40529c9.1574927918.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index bf0c54b1f161..b5835a71f10f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -238,6 +238,12 @@ void __init paging_init(void)
 	phys_addr_t top_of_ram = memblock_end_of_DRAM();
 
 #ifdef CONFIG_HIGHMEM
+	unsigned long v = __fix_to_virt(FIX_KMAP_END);
+	unsigned long end = __fix_to_virt(FIX_KMAP_BEGIN);
+
+	for (; v < end; v += PAGE_SIZE)
+		map_kernel_page(v, 0, __pgprot(0)); /* XXX gross */
+
 	map_kernel_page(PKMAP_BASE, 0, __pgprot(0));	/* XXX gross */
 	pkmap_page_table = virt_to_kpte(PKMAP_BASE);
 

commit f2bb86937d86ebcb0e52f95b6d19aba1d850e601
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Sep 12 13:49:41 2019 +0000

    powerpc/fixmap: don't clear fixmap area in paging_init()
    
    fixmap is intended to map things permanently like the IMMR region on
    FSL SOC (8xx, 83xx, ...), so don't clear it when initialising paging()
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/41c99bc06394a6bc2888631cb98a3ed2ae281ddb.1568295907.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 7573002077a6..bf0c54b1f161 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -237,14 +237,6 @@ void __init paging_init(void)
 	unsigned long long total_ram = memblock_phys_mem_size();
 	phys_addr_t top_of_ram = memblock_end_of_DRAM();
 
-#ifdef CONFIG_PPC32
-	unsigned long v = __fix_to_virt(__end_of_fixed_addresses - 1);
-	unsigned long end = __fix_to_virt(FIX_HOLE);
-
-	for (; v < end; v += PAGE_SIZE)
-		map_kernel_page(v, 0, __pgprot(0)); /* XXX gross */
-#endif
-
 #ifdef CONFIG_HIGHMEM
 	map_kernel_page(PKMAP_BASE, 0, __pgprot(0));	/* XXX gross */
 	pkmap_page_table = virt_to_kpte(PKMAP_BASE);

commit ea458effa88e4f4739551d76fe3f702daf607995
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Mon Nov 4 13:32:58 2019 +1100

    powerpc: Don't flush caches when adding memory
    
    This operation takes a significant amount of time when hotplugging
    large amounts of memory (~50 seconds with 890GB of persistent memory).
    
    This was orignally in commit fb5924fddf9e
    ("powerpc/mm: Flush cache on memory hot(un)plug") to support memtrace,
    but the flush on add is not needed as it is flushed on remove.
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191104023305.9581-7-alastair@au1.ibm.com

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 634e5ea55b6b..7573002077a6 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -142,8 +142,6 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 		return -EFAULT;
 	}
 
-	flush_dcache_range_chunked(start, start + size, FLUSH_CHUNK_SIZE);
-
 	return __add_pages(nid, start_pfn, nr_pages, restrictions);
 }
 

commit 076265907cf9633bbef861c7c2a1c26a8209f283
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Mon Nov 4 13:32:57 2019 +1100

    powerpc: Chunk calls to flush_dcache_range in arch_*_memory
    
    When presented with large amounts of memory being hotplugged
    (in my test case, ~890GB), the call to flush_dcache_range takes
    a while (~50 seconds), triggering RCU stalls.
    
    This patch breaks up the call into 1GB chunks, calling
    cond_resched() inbetween to allow the scheduler to run.
    
    Fixes: fb5924fddf9e ("powerpc/mm: Flush cache on memory hot(un)plug")
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20191104023305.9581-6-alastair@au1.ibm.com

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3392cacabe60..634e5ea55b6b 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -104,6 +104,27 @@ int __weak remove_section_mapping(unsigned long start, unsigned long end)
 	return -ENODEV;
 }
 
+#define FLUSH_CHUNK_SIZE SZ_1G
+/**
+ * flush_dcache_range_chunked(): Write any modified data cache blocks out to
+ * memory and invalidate them, in chunks of up to FLUSH_CHUNK_SIZE
+ * Does not invalidate the corresponding instruction cache blocks.
+ *
+ * @start: the start address
+ * @stop: the stop address (exclusive)
+ * @chunk: the max size of the chunks
+ */
+static void flush_dcache_range_chunked(unsigned long start, unsigned long stop,
+				       unsigned long chunk)
+{
+	unsigned long i;
+
+	for (i = start; i < stop; i += chunk) {
+		flush_dcache_range(i, min(stop, start + chunk));
+		cond_resched();
+	}
+}
+
 int __ref arch_add_memory(int nid, u64 start, u64 size,
 			struct mhp_restrictions *restrictions)
 {
@@ -120,7 +141,8 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 			start, start + size, rc);
 		return -EFAULT;
 	}
-	flush_dcache_range(start, start + size);
+
+	flush_dcache_range_chunked(start, start + size, FLUSH_CHUNK_SIZE);
 
 	return __add_pages(nid, start_pfn, nr_pages, restrictions);
 }
@@ -137,7 +159,8 @@ void __ref arch_remove_memory(int nid, u64 start, u64 size,
 
 	/* Remove htab bolted mappings for this section of memory */
 	start = (unsigned long)__va(start);
-	flush_dcache_range(start, start + size);
+	flush_dcache_range_chunked(start, start + size, FLUSH_CHUNK_SIZE);
+
 	ret = remove_section_mapping(start, start + size);
 	WARN_ON_ONCE(ret);
 

commit 23eb7f560a2a6a1b0dbaaaae8685da75314347e4
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Mon Nov 4 13:32:56 2019 +1100

    powerpc: Convert flush_icache_range & friends to C
    
    Similar to commit 22e9c88d486a
    ("powerpc/64: reuse PPC32 static inline flush_dcache_range()")
    this patch converts the following ASM symbols to C:
        flush_icache_range()
        __flush_dcache_icache()
        __flush_dcache_icache_phys()
    
    This was done as we discovered a long-standing bug where the length of the
    range was truncated due to using a 32 bit shift instead of a 64 bit one.
    
    By converting these functions to C, it becomes easier to maintain.
    
    flush_dcache_icache_phys() retains a critical assembler section as we must
    ensure there are no memory accesses while the data MMU is disabled
    (authored by Christophe Leroy). Since this has no external callers, it has
    also been made static, allowing the compiler to inline it within
    flush_dcache_icache_page().
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    [mpe: Minor fixups, don't export __flush_dcache_icache()]
    Link: https://lore.kernel.org/r/20191104023305.9581-5-alastair@au1.ibm.com

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index be941d382c8d..3392cacabe60 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -318,6 +318,120 @@ void free_initmem(void)
 	free_initmem_default(POISON_FREE_INITMEM);
 }
 
+/**
+ * flush_coherent_icache() - if a CPU has a coherent icache, flush it
+ * @addr: The base address to use (can be any valid address, the whole cache will be flushed)
+ * Return true if the cache was flushed, false otherwise
+ */
+static inline bool flush_coherent_icache(unsigned long addr)
+{
+	/*
+	 * For a snooping icache, we still need a dummy icbi to purge all the
+	 * prefetched instructions from the ifetch buffers. We also need a sync
+	 * before the icbi to order the the actual stores to memory that might
+	 * have modified instructions with the icbi.
+	 */
+	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE)) {
+		mb(); /* sync */
+		icbi((void *)addr);
+		mb(); /* sync */
+		isync();
+		return true;
+	}
+
+	return false;
+}
+
+/**
+ * invalidate_icache_range() - Flush the icache by issuing icbi across an address range
+ * @start: the start address
+ * @stop: the stop address (exclusive)
+ */
+static void invalidate_icache_range(unsigned long start, unsigned long stop)
+{
+	unsigned long shift = l1_icache_shift();
+	unsigned long bytes = l1_icache_bytes();
+	char *addr = (char *)(start & ~(bytes - 1));
+	unsigned long size = stop - (unsigned long)addr + (bytes - 1);
+	unsigned long i;
+
+	for (i = 0; i < size >> shift; i++, addr += bytes)
+		icbi(addr);
+
+	mb(); /* sync */
+	isync();
+}
+
+/**
+ * flush_icache_range: Write any modified data cache blocks out to memory
+ * and invalidate the corresponding blocks in the instruction cache
+ *
+ * Generic code will call this after writing memory, before executing from it.
+ *
+ * @start: the start address
+ * @stop: the stop address (exclusive)
+ */
+void flush_icache_range(unsigned long start, unsigned long stop)
+{
+	if (flush_coherent_icache(start))
+		return;
+
+	clean_dcache_range(start, stop);
+
+	if (IS_ENABLED(CONFIG_44x)) {
+		/*
+		 * Flash invalidate on 44x because we are passed kmapped
+		 * addresses and this doesn't work for userspace pages due to
+		 * the virtually tagged icache.
+		 */
+		iccci((void *)start);
+		mb(); /* sync */
+		isync();
+	} else
+		invalidate_icache_range(start, stop);
+}
+EXPORT_SYMBOL(flush_icache_range);
+
+#if !defined(CONFIG_PPC_8xx) && !defined(CONFIG_PPC64)
+/**
+ * flush_dcache_icache_phys() - Flush a page by it's physical address
+ * @physaddr: the physical address of the page
+ */
+static void flush_dcache_icache_phys(unsigned long physaddr)
+{
+	unsigned long bytes = l1_dcache_bytes();
+	unsigned long nb = PAGE_SIZE / bytes;
+	unsigned long addr = physaddr & PAGE_MASK;
+	unsigned long msr, msr0;
+	unsigned long loop1 = addr, loop2 = addr;
+
+	msr0 = mfmsr();
+	msr = msr0 & ~MSR_DR;
+	/*
+	 * This must remain as ASM to prevent potential memory accesses
+	 * while the data MMU is disabled
+	 */
+	asm volatile(
+		"   mtctr %2;\n"
+		"   mtmsr %3;\n"
+		"   isync;\n"
+		"0: dcbst   0, %0;\n"
+		"   addi    %0, %0, %4;\n"
+		"   bdnz    0b;\n"
+		"   sync;\n"
+		"   mtctr %2;\n"
+		"1: icbi    0, %1;\n"
+		"   addi    %1, %1, %4;\n"
+		"   bdnz    1b;\n"
+		"   sync;\n"
+		"   mtmsr %5;\n"
+		"   isync;\n"
+		: "+&r" (loop1), "+&r" (loop2)
+		: "r" (nb), "r" (msr), "i" (bytes), "r" (msr0)
+		: "ctr", "memory");
+}
+#endif // !defined(CONFIG_PPC_8xx) && !defined(CONFIG_PPC64)
+
 /*
  * This is called when a page has been modified by the kernel.
  * It just marks the page as not i-cache clean.  We do the i-cache
@@ -350,12 +464,46 @@ void flush_dcache_icache_page(struct page *page)
 		__flush_dcache_icache(start);
 		kunmap_atomic(start);
 	} else {
-		__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
+		unsigned long addr = page_to_pfn(page) << PAGE_SHIFT;
+
+		if (flush_coherent_icache(addr))
+			return;
+		flush_dcache_icache_phys(addr);
 	}
 #endif
 }
 EXPORT_SYMBOL(flush_dcache_icache_page);
 
+/**
+ * __flush_dcache_icache(): Flush a particular page from the data cache to RAM.
+ * Note: this is necessary because the instruction cache does *not*
+ * snoop from the data cache.
+ *
+ * @page: the address of the page to flush
+ */
+void __flush_dcache_icache(void *p)
+{
+	unsigned long addr = (unsigned long)p;
+
+	if (flush_coherent_icache(addr))
+		return;
+
+	clean_dcache_range(addr, addr + PAGE_SIZE);
+
+	/*
+	 * We don't flush the icache on 44x. Those have a virtual icache and we
+	 * don't have access to the virtual address here (it's not the page
+	 * vaddr but where it's mapped in user space). The flushing of the
+	 * icache on these is handled elsewhere, when a change in the address
+	 * space occurs, before returning to user space.
+	 */
+
+	if (cpu_has_feature(MMU_FTR_TYPE_44x))
+		return;
+
+	invalidate_icache_range(addr, addr + PAGE_SIZE);
+}
+
 void clear_user_page(void *page, unsigned long vaddr, struct page *pg)
 {
 	clear_page(page);

commit 8b5369ea580964dbc982781bfb9fb93459fc5e8d
Author: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
Date:   Mon Oct 14 20:31:03 2019 +0200

    dma/direct: turn ARCH_ZONE_DMA_BITS into a variable
    
    Some architectures, notably ARM, are interested in tweaking this
    depending on their runtime DMA addressing limitations.
    
    Acked-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Nicolas Saenz Julienne <nsaenzjulienne@suse.de>
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index be941d382c8d..c95b7fe9f298 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -31,6 +31,7 @@
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
 #include <linux/memremap.h>
+#include <linux/dma-direct.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -201,10 +202,10 @@ static int __init mark_nonram_nosave(void)
  * everything else. GFP_DMA32 page allocations automatically fall back to
  * ZONE_DMA.
  *
- * By using 31-bit unconditionally, we can exploit ARCH_ZONE_DMA_BITS to
- * inform the generic DMA mapping code.  32-bit only devices (if not handled
- * by an IOMMU anyway) will take a first dip into ZONE_NORMAL and get
- * otherwise served by ZONE_DMA.
+ * By using 31-bit unconditionally, we can exploit zone_dma_bits to inform the
+ * generic DMA mapping code.  32-bit only devices (if not handled by an IOMMU
+ * anyway) will take a first dip into ZONE_NORMAL and get otherwise served by
+ * ZONE_DMA.
  */
 static unsigned long max_zone_pfns[MAX_NR_ZONES];
 
@@ -237,9 +238,18 @@ void __init paging_init(void)
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (long int)((top_of_ram - total_ram) >> 20));
 
+	/*
+	 * Allow 30-bit DMA for very limited Broadcom wifi chips on many
+	 * powerbooks.
+	 */
+	if (IS_ENABLED(CONFIG_PPC32))
+		zone_dma_bits = 30;
+	else
+		zone_dma_bits = 31;
+
 #ifdef CONFIG_ZONE_DMA
 	max_zone_pfns[ZONE_DMA]	= min(max_low_pfn,
-				      1UL << (ARCH_ZONE_DMA_BITS - PAGE_SHIFT));
+				      1UL << (zone_dma_bits - PAGE_SHIFT));
 #endif
 	max_zone_pfns[ZONE_NORMAL] = max_low_pfn;
 #ifdef CONFIG_HIGHMEM

commit f2902a2fb40c589b886d21518ef8a1ee87f76b0c
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 14 15:22:30 2019 +0200

    powerpc: use the generic dma coherent remap allocator
    
    This switches to using common code for the DMA allocations, including
    potential use of the CMA allocator if configured.
    
    Switching to the generic code enables DMA allocations from atomic
    context, which is required by the DMA API documentation, and also
    adds various other minor features drivers start relying upon.  It
    also makes sure we have on tested code base for all architectures
    that require uncached pte bits for coherent DMA allocations.
    
    Another advantage is that consistent memory allocations now share
    the general vmalloc pool instead of needing an explicit careout
    from it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Christophe Leroy <christophe.leroy@c-s.fr> # tested on 8xx
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190814132230.31874-2-hch@lst.de

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 69f99128a8d6..be941d382c8d 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -302,10 +302,6 @@ void __init mem_init(void)
 	pr_info("  * 0x%08lx..0x%08lx  : highmem PTEs\n",
 		PKMAP_BASE, PKMAP_ADDR(LAST_PKMAP));
 #endif /* CONFIG_HIGHMEM */
-#ifdef CONFIG_NOT_COHERENT_CACHE
-	pr_info("  * 0x%08lx..0x%08lx  : consistent mem\n",
-		IOREMAP_TOP, IOREMAP_TOP + CONFIG_CONSISTENT_SIZE);
-#endif /* CONFIG_NOT_COHERENT_CACHE */
 	if (ioremap_bot != IOREMAP_TOP)
 		pr_info("  * 0x%08lx..0x%08lx  : early ioremap\n",
 			ioremap_bot, IOREMAP_TOP);

commit e5a1edb9fe4cfa07e37a59475f8f7d0a8939c73e
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Aug 16 05:41:42 2019 +0000

    powerpc/mm: move update_mmu_cache() into book3s hash utils.
    
    update_mmu_cache() is only for BOOK3S, and can be simplified for
    BOOK3S32.
    
    Move it out of mem.c into respective BOOK3S32 and BOOK3S64 files
    containing hash utils.
    
    BOOK3S64 version of hash_preload() is only used locally, declare it
    static.
    
    Remove the radix_enabled() stuff in BOOK3S32 version.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/107aaf43583a5f5d09e0d4e84c4c4390ecfcd512.1565933217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 58d11362eee4..69f99128a8d6 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -407,58 +407,6 @@ void flush_icache_user_range(struct vm_area_struct *vma, struct page *page,
 }
 EXPORT_SYMBOL(flush_icache_user_range);
 
-/*
- * This is called at the end of handling a user page fault, when the
- * fault has been handled by updating a PTE in the linux page tables.
- * We use it to preload an HPTE into the hash table corresponding to
- * the updated linux PTE.
- * 
- * This must always be called with the pte lock held.
- */
-#ifdef CONFIG_PPC_BOOK3S
-void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
-		      pte_t *ptep)
-{
-	/*
-	 * We don't need to worry about _PAGE_PRESENT here because we are
-	 * called with either mm->page_table_lock held or ptl lock held
-	 */
-	unsigned long trap;
-	bool is_exec;
-
-	if (radix_enabled()) {
-		prefetch((void *)address);
-		return;
-	}
-
-	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
-	if (!pte_young(*ptep) || address >= TASK_SIZE)
-		return;
-
-	/* We try to figure out if we are coming from an instruction
-	 * access fault and pass that down to __hash_page so we avoid
-	 * double-faulting on execution of fresh text. We have to test
-	 * for regs NULL since init will get here first thing at boot
-	 *
-	 * We also avoid filling the hash if not coming from a fault
-	 */
-
-	trap = current->thread.regs ? TRAP(current->thread.regs) : 0UL;
-	switch (trap) {
-	case 0x300:
-		is_exec = false;
-		break;
-	case 0x400:
-		is_exec = true;
-		break;
-	default:
-		return;
-	}
-
-	hash_preload(vma->vm_mm, address, is_exec, trap);
-}
-#endif /* CONFIG_PPC_BOOK3S */
-
 /*
  * System memory should not be in /proc/iomem but various tools expect it
  * (eg kdump).

commit 4c1616ef036ffaaea95a29d7b6abf9d3e8eb9d92
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Aug 16 05:41:41 2019 +0000

    powerpc/mm: move FSL_BOOK3 version of update_mmu_cache()
    
    Move FSL_BOOK3E version of update_mmu_cache() at the same
    place as book3e_hugetlb_preload() as update_mmu_cache() is
    the only user of book3e_hugetlb_preload().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/4d69fdc86df9c74adc71a60331a86f6afb8b5e9e.1565933217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index ee8225cd2cd2..58d11362eee4 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -458,14 +458,6 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	hash_preload(vma->vm_mm, address, is_exec, trap);
 }
 #endif /* CONFIG_PPC_BOOK3S */
-#if defined(CONFIG_PPC_FSL_BOOK3E) && defined(CONFIG_HUGETLB_PAGE)
-void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
-		      pte_t *ptep)
-{
-	if (is_vm_hugetlb_page(vma))
-		book3e_hugetlb_preload(vma, address, *ptep);
-}
-#endif
 
 /*
  * System memory should not be in /proc/iomem but various tools expect it

commit d9642117914c9d3f800b3bacc19d7e388b04edb4
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Aug 16 05:41:40 2019 +0000

    powerpc/mm: define empty update_mmu_cache() as static inline
    
    Only BOOK3S and FSL_BOOK3E have a usefull update_mmu_cache().
    
    For the others, just define it static inline.
    
    In the meantime, simplify the FSL_BOOK3E related ifdef as
    book3e_hugetlb_preload() only exists when CONFIG_PPC_FSL_BOOK3E
    is selected.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/668aba4db6b9af6d8a151174e11a4289f1a6bbcd.1565933217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 828c535e8fd2..ee8225cd2cd2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -415,10 +415,10 @@ EXPORT_SYMBOL(flush_icache_user_range);
  * 
  * This must always be called with the pte lock held.
  */
+#ifdef CONFIG_PPC_BOOK3S
 void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		      pte_t *ptep)
 {
-#ifdef CONFIG_PPC_BOOK3S
 	/*
 	 * We don't need to worry about _PAGE_PRESENT here because we are
 	 * called with either mm->page_table_lock held or ptl lock held
@@ -456,13 +456,16 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	}
 
 	hash_preload(vma->vm_mm, address, is_exec, trap);
+}
 #endif /* CONFIG_PPC_BOOK3S */
-#if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
-	&& defined(CONFIG_HUGETLB_PAGE)
+#if defined(CONFIG_PPC_FSL_BOOK3E) && defined(CONFIG_HUGETLB_PAGE)
+void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
+		      pte_t *ptep)
+{
 	if (is_vm_hugetlb_page(vma))
 		book3e_hugetlb_preload(vma, address, *ptep);
-#endif
 }
+#endif
 
 /*
  * System memory should not be in /proc/iomem but various tools expect it

commit ad628a34ec4e3558bf838195f60bbaa4c2b68f2a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Aug 14 14:36:10 2019 +0000

    powerpc/mm: don't display empty early ioremap area
    
    On the 8xx, the layout displayed at boot is:
    
    [    0.000000] Memory: 121856K/131072K available (5728K kernel code, 592K rwdata, 1248K rodata, 560K init, 448K bss, 9216K reserved, 0K cma-reserved)
    [    0.000000] Kernel virtual memory layout:
    [    0.000000]   * 0xffefc000..0xffffc000  : fixmap
    [    0.000000]   * 0xffefc000..0xffefc000  : early ioremap
    [    0.000000]   * 0xc9000000..0xffefc000  : vmalloc & ioremap
    [    0.000000] SLUB: HWalign=16, Order=0-3, MinObjects=0, CPUs=1, Nodes=1
    
    Remove display of an empty early ioremap.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/f6267226038cb25a839b567319e240576e3f8565.1565793287.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 9191a66b3bc5..828c535e8fd2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -306,8 +306,9 @@ void __init mem_init(void)
 	pr_info("  * 0x%08lx..0x%08lx  : consistent mem\n",
 		IOREMAP_TOP, IOREMAP_TOP + CONFIG_CONSISTENT_SIZE);
 #endif /* CONFIG_NOT_COHERENT_CACHE */
-	pr_info("  * 0x%08lx..0x%08lx  : early ioremap\n",
-		ioremap_bot, IOREMAP_TOP);
+	if (ioremap_bot != IOREMAP_TOP)
+		pr_info("  * 0x%08lx..0x%08lx  : early ioremap\n",
+			ioremap_bot, IOREMAP_TOP);
 	pr_info("  * 0x%08lx..0x%08lx  : vmalloc & ioremap\n",
 		VMALLOC_START, VMALLOC_END);
 #endif /* CONFIG_PPC32 */

commit bed38c3e2dca01b358a62b5e73b46e875742fd75
Merge: 76260774575c 3a855b7ac7d5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 24 09:58:39 2019 -0700

    Merge tag 'powerpc-5.3-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
     "An assortment of non-regression fixes that have accumulated since the
      start of the merge window.
    
       - A fix for a user triggerable oops on machines where transactional
         memory is disabled, eg. Power9 bare metal, Power8 with TM disabled
         on the command line, or all Power7 or earlier machines.
    
       - Three fixes for handling of PMU and power saving registers when
         running nested KVM on Power9.
    
       - Two fixes for bugs found while stress testing the XIVE interrupt
         controller code, also on Power9.
    
       - A fix to allow guests to boot under Qemu/KVM on Power9 using the
         the Hash MMU with >= 1TB of memory.
    
       - Two fixes for bugs in the recent DMA cleanup, one of which could
         lead to checkstops.
    
       - And finally three fixes for the PAPR SCM nvdimm driver.
    
      Thanks to: Alexey Kardashevskiy, Andrea Arcangeli, CÃ©dric Le Goater,
      Christoph Hellwig, David Gibson, Gautham R. Shenoy, Michael Neuling,
      Oliver O'Halloran, Satheesh Rajendran, Shawn Anastasio, Suraj Jitindar
      Singh, Vaibhav Jain"
    
    * tag 'powerpc-5.3-2' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      powerpc/papr_scm: Force a scm-unbind if initial scm-bind fails
      powerpc/papr_scm: Update drc_pmem_unbind() to use H_SCM_UNBIND_ALL
      powerpc/pseries: Update SCM hcall op-codes in hvcall.h
      powerpc/tm: Fix oops on sigreturn on systems without TM
      powerpc/dma: Fix invalid DMA mmap behavior
      KVM: PPC: Book3S HV: XIVE: fix rollback when kvmppc_xive_create fails
      powerpc/xive: Fix loop exit-condition in xive_find_target_in_mask()
      powerpc: fix off by one in max_zone_pfn initialization for ZONE_DMA
      KVM: PPC: Book3S HV: Save and restore guest visible PSSCR bits on pseries
      powerpc/pmu: Set pmcregs_in_use in paca when running as LPAR
      KVM: PPC: Book3S HV: Always save guest pmu for guest capable of nesting
      powerpc/mm: Limit rma_size to 1TB when running without HV mode

commit 80ec922dbd87fd38d15719c86a94457204648aeb
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jul 18 15:56:51 2019 -0700

    mm/memory_hotplug: allow arch_remove_memory() without CONFIG_MEMORY_HOTREMOVE
    
    We want to improve error handling while adding memory by allowing to use
    arch_remove_memory() and __remove_pages() even if
    CONFIG_MEMORY_HOTREMOVE is not set to e.g., implement something like:
    
            arch_add_memory()
            rc = do_something();
            if (rc) {
                    arch_remove_memory();
            }
    
    We won't get rid of CONFIG_MEMORY_HOTREMOVE for now, as it will require
    quite some dependencies for memory offlining.
    
    Link: http://lkml.kernel.org/r/20190527111152.16324-7-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Alex Deucher <alexander.deucher@amd.com>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Mark Brown <broonie@kernel.org>
    Cc: Chris Wilson <chris@chris-wilson.co.uk>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: "mike.travis@hpe.com" <mike.travis@hpe.com>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Baoquan He <bhe@redhat.com>
    Cc: Logan Gunthorpe <logang@deltatee.com>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chintan Pandya <cpandya@codeaurora.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Jun Yao <yaojun8558363@gmail.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yu Zhao <yuzhao@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 26a8da3723bb..9259337d7374 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -125,7 +125,6 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 	return __add_pages(nid, start_pfn, nr_pages, restrictions);
 }
 
-#ifdef CONFIG_MEMORY_HOTREMOVE
 void __ref arch_remove_memory(int nid, u64 start, u64 size,
 			     struct vmem_altmap *altmap)
 {
@@ -151,7 +150,6 @@ void __ref arch_remove_memory(int nid, u64 start, u64 size,
 		pr_warn("Hash collision while resizing HPT\n");
 }
 #endif
-#endif /* CONFIG_MEMORY_HOTPLUG */
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 void __init mem_topology_setup(void)

commit 03800e0526ee25ed7c843ca1e57b69ac2a5af642
Author: Andrea Arcangeli <aarcange@redhat.com>
Date:   Tue Jun 25 10:17:27 2019 -0400

    powerpc: fix off by one in max_zone_pfn initialization for ZONE_DMA
    
    25078dc1f74be16b858e914f52cc8f4d03c2271a first introduced an off by
    one error in the ZONE_DMA initialization of PPC_BOOK3E_64=y and since
    9739ab7eda459f0669ec9807e0d9be5020bab88c the off by one applies to
    PPC32=y too. This simply corrects the off by one and should resolve
    crashes like below:
    
    [   65.179101] page 0x7fff outside node 0 zone DMA [ 0x0 - 0x7fff ]
    
    Unfortunately in various MM places "max" means a non inclusive end of
    range. free_area_init_nodes max_zone_pfn parameter is one case and
    MAX_ORDER is another one (unrelated) that comes by memory.
    
    Reported-by: Zorro Lang <zlang@redhat.com>
    Fixes: 25078dc1f74b ("powerpc: use mm zones more sensibly")
    Fixes: 9739ab7eda45 ("powerpc: enable a 30-bit ZONE_DMA for 32-bit pmac")
    Signed-off-by: Andrea Arcangeli <aarcange@redhat.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20190625141727.2883-1-aarcange@redhat.com

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 6d5f0fc76666..b6beb86e5300 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -249,7 +249,7 @@ void __init paging_init(void)
 
 #ifdef CONFIG_ZONE_DMA
 	max_zone_pfns[ZONE_DMA]	= min(max_low_pfn,
-			((1UL << ARCH_ZONE_DMA_BITS) - 1) >> PAGE_SHIFT);
+				      1UL << (ARCH_ZONE_DMA_BITS - PAGE_SHIFT));
 #endif
 	max_zone_pfns[ZONE_NORMAL] = max_low_pfn;
 #ifdef CONFIG_HIGHMEM

commit fec88ab0af9706b2201e5daf377c5031c62d11f7
Merge: fa6e951a2a44 cc5dfd59e375
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jul 14 19:42:11 2019 -0700

    Merge tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull HMM updates from Jason Gunthorpe:
     "Improvements and bug fixes for the hmm interface in the kernel:
    
       - Improve clarity, locking and APIs related to the 'hmm mirror'
         feature merged last cycle. In linux-next we now see AMDGPU and
         nouveau to be using this API.
    
       - Remove old or transitional hmm APIs. These are hold overs from the
         past with no users, or APIs that existed only to manage cross tree
         conflicts. There are still a few more of these cleanups that didn't
         make the merge window cut off.
    
       - Improve some core mm APIs:
           - export alloc_pages_vma() for driver use
           - refactor into devm_request_free_mem_region() to manage
             DEVICE_PRIVATE resource reservations
           - refactor duplicative driver code into the core dev_pagemap
             struct
    
       - Remove hmm wrappers of improved core mm APIs, instead have drivers
         use the simplified API directly
    
       - Remove DEVICE_PUBLIC
    
       - Simplify the kconfig flow for the hmm users and core code"
    
    * tag 'for-linus-hmm' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (42 commits)
      mm: don't select MIGRATE_VMA_HELPER from HMM_MIRROR
      mm: remove the HMM config option
      mm: sort out the DEVICE_PRIVATE Kconfig mess
      mm: simplify ZONE_DEVICE page private data
      mm: remove hmm_devmem_add
      mm: remove hmm_vma_alloc_locked_page
      nouveau: use devm_memremap_pages directly
      nouveau: use alloc_page_vma directly
      PCI/P2PDMA: use the dev_pagemap internal refcount
      device-dax: use the dev_pagemap internal refcount
      memremap: provide an optional internal refcount in struct dev_pagemap
      memremap: replace the altmap_valid field with a PGMAP_ALTMAP_VALID flag
      memremap: remove the data field in struct dev_pagemap
      memremap: add a migrate_to_ram method to struct dev_pagemap_ops
      memremap: lift the devmap_enable manipulation into devm_memremap_pages
      memremap: pass a struct dev_pagemap to ->kill and ->cleanup
      memremap: move dev_pagemap callbacks into a separate structure
      memremap: validate the pagemap type passed to devm_memremap_pages
      mm: factor out a devm_request_free_mem_region helper
      mm: export alloc_pages_vma
      ...

commit 192f0f8e9db7efe4ac98d47f5fa4334e43c1204d
Merge: ec9249752465 f5a9e488d623
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 13 16:08:36 2019 -0700

    Merge tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Removal of the NPU DMA code, used by the out-of-tree Nvidia driver,
         as well as some other functions only used by drivers that haven't
         (yet?) made it upstream.
    
       - A fix for a bug in our handling of hardware watchpoints (eg. perf
         record -e mem: ...) which could lead to register corruption and
         kernel crashes.
    
       - Enable HAVE_ARCH_HUGE_VMAP, which allows us to use large pages for
         vmalloc when using the Radix MMU.
    
       - A large but incremental rewrite of our exception handling code to
         use gas macros rather than multiple levels of nested CPP macros.
    
      And the usual small fixes, cleanups and improvements.
    
      Thanks to: Alastair D'Silva, Alexey Kardashevskiy, Andreas Schwab,
      Aneesh Kumar K.V, Anju T Sudhakar, Anton Blanchard, Arnd Bergmann,
      Athira Rajeev, CÃ©dric Le Goater, Christian Lamparter, Christophe
      Leroy, Christophe Lombard, Christoph Hellwig, Daniel Axtens, Denis
      Efremov, Enrico Weigelt, Frederic Barrat, Gautham R. Shenoy, Geert
      Uytterhoeven, Geliang Tang, Gen Zhang, Greg Kroah-Hartman, Greg Kurz,
      Gustavo Romero, Krzysztof Kozlowski, Madhavan Srinivasan, Masahiro
      Yamada, Mathieu Malaterre, Michael Neuling, Nathan Lynch, Naveen N.
      Rao, Nicholas Piggin, Nishad Kamdar, Oliver O'Halloran, Qian Cai, Ravi
      Bangoria, Sachin Sant, Sam Bobroff, Satheesh Rajendran, Segher
      Boessenkool, Shaokun Zhang, Shawn Anastasio, Stewart Smith, Suraj
      Jitindar Singh, Thiago Jung Bauermann, YueHaibing"
    
    * tag 'powerpc-5.3-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (163 commits)
      powerpc/powernv/idle: Fix restore of SPRN_LDBAR for POWER9 stop state.
      powerpc/eeh: Handle hugepages in ioremap space
      ocxl: Update for AFU descriptor template version 1.1
      powerpc/boot: pass CONFIG options in a simpler and more robust way
      powerpc/boot: add {get, put}_unaligned_be32 to xz_config.h
      powerpc/irq: Don't WARN continuously in arch_local_irq_restore()
      powerpc/module64: Use symbolic instructions names.
      powerpc/module32: Use symbolic instructions names.
      powerpc: Move PPC_HA() PPC_HI() and PPC_LO() to ppc-opcode.h
      powerpc/module64: Fix comment in R_PPC64_ENTRY handling
      powerpc/boot: Add lzo support for uImage
      powerpc/boot: Add lzma support for uImage
      powerpc/boot: don't force gzipped uImage
      powerpc/8xx: Add microcode patch to move SMC parameter RAM.
      powerpc/8xx: Use IO accessors in microcode programming.
      powerpc/8xx: replace #ifdefs by IS_ENABLED() in microcode.c
      powerpc/8xx: refactor programming of microcode CPM params.
      powerpc/8xx: refactor printing of microcode patch name.
      powerpc/8xx: Refactor microcode write
      powerpc/8xx: refactor writing of CPM microcode arrays
      ...

commit 1cfb725fb1899dc6fdc88f8b5354a65e8ad260c6
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 14 09:05:13 2019 +0000

    powerpc/64: flush_inval_dcache_range() becomes flush_dcache_range()
    
    On most arches having function flush_dcache_range(), including PPC32,
    this function does a writeback and invalidation of the cache bloc.
    
    On PPC64, flush_dcache_range() only does a writeback while
    flush_inval_dcache_range() does the invalidation in addition.
    
    In addition it looks like within arch/powerpc/, there are no PPC64
    platforms using flush_dcache_range()
    
    This patch drops the existing 64 bits version of flush_dcache_range()
    and renames flush_inval_dcache_range() into flush_dcache_range().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 40bd4153ab09..096c87836e29 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -125,7 +125,7 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 			start, start + size, rc);
 		return -EFAULT;
 	}
-	flush_inval_dcache_range(start, start + size);
+	flush_dcache_range(start, start + size);
 
 	return __add_pages(nid, start_pfn, nr_pages, restrictions);
 }
@@ -151,7 +151,7 @@ void __ref arch_remove_memory(int nid, u64 start, u64 size,
 
 	/* Remove htab bolted mappings for this section of memory */
 	start = (unsigned long)__va(start);
-	flush_inval_dcache_range(start, start + size);
+	flush_dcache_range(start, start + size);
 	ret = remove_section_mapping(start, start + size);
 	WARN_ON_ONCE(ret);
 

commit 514caf23a70fd697fa2ece238b2cd8dcc73fb16f
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 26 14:27:13 2019 +0200

    memremap: replace the altmap_valid field with a PGMAP_ALTMAP_VALID flag
    
    Add a flags field to struct dev_pagemap to replace the altmap_valid
    boolean to be a little more extensible.  Also add a pgmap_altmap() helper
    to find the optional altmap and clean up the code using the altmap using
    it.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2540d3b2588c..a2923c5c1982 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -131,17 +131,9 @@ void __ref arch_remove_memory(int nid, u64 start, u64 size,
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
-	struct page *page;
+	struct page *page = pfn_to_page(start_pfn) + vmem_altmap_offset(altmap);
 	int ret;
 
-	/*
-	 * If we have an altmap then we need to skip over any reserved PFNs
-	 * when querying the zone.
-	 */
-	page = pfn_to_page(start_pfn);
-	if (altmap)
-		page += vmem_altmap_offset(altmap);
-
 	__remove_pages(page_zone(page), start_pfn, nr_pages, altmap);
 
 	/* Remove htab bolted mappings for this section of memory */

commit a8282bf087bcfb348ad97c8ed1f457bc11fd9709
Merge: 693cd8ce3f88 500871125920
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jun 22 09:09:42 2019 -0700

    Merge tag 'powerpc-5.2-5' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc fixes from Michael Ellerman:
     "This is a frustratingly large batch at rc5. Some of these were sent
      earlier but were missed by me due to being distracted by other things,
      and some took a while to track down due to needing manual bisection on
      old hardware. But still we clearly need to improve our testing of KVM,
      and of 32-bit, so that we catch these earlier.
    
      Summary: seven fixes, all for bugs introduced this cycle.
    
       - The commit to add KASAN support broke booting on 32-bit SMP
         machines, due to a refactoring that moved some setup out of the
         secondary CPU path.
    
       - A fix for another 32-bit SMP bug introduced by the fast syscall
         entry implementation for 32-bit BOOKE. And a build fix for the same
         commit.
    
       - Our change to allow the DAWR to be force enabled on Power9
         introduced a bug in KVM, where we clobber r3 leading to a host
         crash.
    
       - The same commit also exposed a previously unreachable bug in the
         nested KVM handling of DAWR, which could lead to an oops in a
         nested host.
    
       - One of the DMA reworks broke the b43legacy WiFi driver on some
         people's powermacs, fix it by enabling a 30-bit ZONE_DMA on 32-bit.
    
       - A fix for TLB flushing in KVM introduced a new bug, as it neglected
         to also flush the ERAT, this could lead to memory corruption in the
         guest.
    
      Thanks to: Aaro Koskinen, Christoph Hellwig, Christophe Leroy, Larry
      Finger, Michael Neuling, Suraj Jitindar Singh"
    
    * tag 'powerpc-5.2-5' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux:
      KVM: PPC: Book3S HV: Invalidate ERAT when flushing guest TLB entries
      powerpc: enable a 30-bit ZONE_DMA for 32-bit pmac
      KVM: PPC: Book3S HV: Only write DAWR[X] when handling h_set_dawr in real mode
      KVM: PPC: Book3S HV: Fix r3 corruption in h_set_dabr()
      powerpc/32: fix build failure on book3e with KVM
      powerpc/booke: fix fast syscall entry on SMP
      powerpc/32s: fix initial setup of segment registers on secondary CPU

commit 9739ab7eda459f0669ec9807e0d9be5020bab88c
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jun 13 10:24:46 2019 +0200

    powerpc: enable a 30-bit ZONE_DMA for 32-bit pmac
    
    With the strict dma mask checking introduced with the switch to
    the generic DMA direct code common wifi chips on 32-bit powerbooks
    stopped working.  Add a 30-bit ZONE_DMA to the 32-bit pmac builds
    to allow them to reliably allocate dma coherent memory.
    
    Fixes: 65a21b71f948 ("powerpc/dma: remove dma_nommu_dma_supported")
    Reported-by: Aaro Koskinen <aaro.koskinen@iki.fi>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Larry Finger <Larry.Finger@lwfinger.net>
    Acked-by: Larry Finger <Larry.Finger@lwfinger.net>
    Tested-by: Aaro Koskinen <aaro.koskinen@iki.fi>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e885fe2aafcc..40bd4153ab09 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -253,7 +253,8 @@ void __init paging_init(void)
 	       (long int)((top_of_ram - total_ram) >> 20));
 
 #ifdef CONFIG_ZONE_DMA
-	max_zone_pfns[ZONE_DMA]	= min(max_low_pfn, 0x7fffffffUL >> PAGE_SHIFT);
+	max_zone_pfns[ZONE_DMA]	= min(max_low_pfn,
+			((1UL << ARCH_ZONE_DMA_BITS) - 1) >> PAGE_SHIFT);
 #endif
 	max_zone_pfns[ZONE_NORMAL] = max_low_pfn;
 #ifdef CONFIG_HIGHMEM

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e885fe2aafcc..cba29131bccc 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  *  PowerPC version
  *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
@@ -9,12 +10,6 @@
  *
  *  Derived from "arch/i386/mm/init.c"
  *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds
- *
- *  This program is free software; you can redistribute it and/or
- *  modify it under the terms of the GNU General Public License
- *  as published by the Free Software Foundation; either version
- *  2 of the License, or (at your option) any later version.
- *
  */
 
 #include <linux/export.h>

commit ac5c94264580f498e484c854031d0226b3c1038f
Author: David Hildenbrand <david@redhat.com>
Date:   Mon May 13 17:21:46 2019 -0700

    mm/memory_hotplug: make __remove_pages() and arch_remove_memory() never fail
    
    All callers of arch_remove_memory() ignore errors.  And we should really
    try to remove any errors from the memory removal path.  No more errors are
    reported from __remove_pages().  BUG() in s390x code in case
    arch_remove_memory() is triggered.  We may implement that properly later.
    WARN in case powerpc code failed to remove the section mapping, which is
    better than ignoring the error completely right now.
    
    Link: http://lkml.kernel.org/r/20190409100148.24703-5-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Oscar Salvador <osalvador@suse.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Stefan Agner <stefan@agner.ch>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Vasily Gorbik <gor@linux.ibm.com>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Masahiro Yamada <yamada.masahiro@socionext.com>
    Cc: Rob Herring <robh@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Mathieu Malaterre <malat@debian.org>
    Cc: Andrew Banman <andrew.banman@hpe.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Mike Travis <mike.travis@hpe.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index de5c591a550d..e885fe2aafcc 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -131,7 +131,7 @@ int __ref arch_add_memory(int nid, u64 start, u64 size,
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-int __ref arch_remove_memory(int nid, u64 start, u64 size,
+void __ref arch_remove_memory(int nid, u64 start, u64 size,
 			     struct vmem_altmap *altmap)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
@@ -147,14 +147,13 @@ int __ref arch_remove_memory(int nid, u64 start, u64 size,
 	if (altmap)
 		page += vmem_altmap_offset(altmap);
 
-	ret = __remove_pages(page_zone(page), start_pfn, nr_pages, altmap);
-	if (ret)
-		return ret;
+	__remove_pages(page_zone(page), start_pfn, nr_pages, altmap);
 
 	/* Remove htab bolted mappings for this section of memory */
 	start = (unsigned long)__va(start);
 	flush_inval_dcache_range(start, start + size);
 	ret = remove_section_mapping(start, start + size);
+	WARN_ON_ONCE(ret);
 
 	/* Ensure all vmalloc mappings are flushed in case they also
 	 * hit that section of memory
@@ -163,8 +162,6 @@ int __ref arch_remove_memory(int nid, u64 start, u64 size,
 
 	if (resize_hpt_for_hotplug(memblock_phys_mem_size()) == -ENOSPC)
 		pr_warn("Hash collision while resizing HPT\n");
-
-	return ret;
 }
 #endif
 #endif /* CONFIG_MEMORY_HOTPLUG */

commit 940519f0c8b757fdcbc5d14c93cdaada20ded14c
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon May 13 17:21:26 2019 -0700

    mm, memory_hotplug: provide a more generic restrictions for memory hotplug
    
    arch_add_memory, __add_pages take a want_memblock which controls whether
    the newly added memory should get the sysfs memblock user API (e.g.
    ZONE_DEVICE users do not want/need this interface).  Some callers even
    want to control where do we allocate the memmap from by configuring
    altmap.
    
    Add a more generic hotplug context for arch_add_memory and __add_pages.
    struct mhp_restrictions contains flags which contains additional features
    to be enabled by the memory hotplug (MHP_MEMBLOCK_API currently) and
    altmap for alternative memmap allocator.
    
    This patch shouldn't introduce any functional change.
    
    [akpm@linux-foundation.org: build fix]
    Link: http://lkml.kernel.org/r/20190408082633.2864-3-osalvador@suse.de
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 20266898f3a8..de5c591a550d 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -109,8 +109,8 @@ int __weak remove_section_mapping(unsigned long start, unsigned long end)
 	return -ENODEV;
 }
 
-int __ref arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
-			  bool want_memblock)
+int __ref arch_add_memory(int nid, u64 start, u64 size,
+			struct mhp_restrictions *restrictions)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
@@ -127,7 +127,7 @@ int __ref arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altm
 	}
 	flush_inval_dcache_range(start, start + size);
 
-	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
+	return __add_pages(nid, start_pfn, nr_pages, restrictions);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit 4afd58e14dd415e456fd236755373f52e6055ec7
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon May 13 17:18:34 2019 -0700

    initramfs: provide a generic free_initrd_mem implementation
    
    For most architectures free_initrd_mem just expands to the same
    free_reserved_area call.  Provide that as a generic implementation marked
    __weak.
    
    Link: http://lkml.kernel.org/r/20190213174621.29297-8-hch@lst.de
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>     [m68k]
    Acked-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>   [arm64]
    Cc: Steven Price <steven.price@arm.com>
    Cc: Alexander Viro <viro@zeniv.linux.org.uk>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index cd525d709072..20266898f3a8 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -338,13 +338,6 @@ void free_initmem(void)
 	free_initmem_default(POISON_FREE_INITMEM);
 }
 
-#ifdef CONFIG_BLK_DEV_INITRD
-void __init free_initrd_mem(unsigned long start, unsigned long end)
-{
-	free_reserved_area((void *)start, (void *)end, -1, "initrd");
-}
-#endif
-
 /*
  * This is called when a page has been modified by the kernel.
  * It just marks the page as not i-cache clean.  We do the i-cache

commit b4abe38fd698ace6942edeeb79a5b8a60a7af4fa
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:23:32 2019 +0000

    powerpc/32: prepare shadow area for KASAN
    
    This patch prepares a shadow area for KASAN.
    
    The shadow area will be at the top of the kernel virtual
    memory space above the fixmap area and will occupy one
    eighth of the total kernel virtual memory space.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 105c58f8900a..cd525d709072 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -310,6 +310,10 @@ void __init mem_init(void)
 	mem_init_print_info(NULL);
 #ifdef CONFIG_PPC32
 	pr_info("Kernel virtual memory layout:\n");
+#ifdef CONFIG_KASAN
+	pr_info("  * 0x%08lx..0x%08lx  : kasan shadow mem\n",
+		KASAN_SHADOW_START, KASAN_SHADOW_END);
+#endif
 	pr_info("  * 0x%08lx..0x%08lx  : fixmap\n", FIXADDR_START, FIXADDR_TOP);
 #ifdef CONFIG_HIGHMEM
 	pr_info("  * 0x%08lx..0x%08lx  : highmem PTEs\n",

commit 9d9f2cccde952126185e3336af0d4dc62eb254ad
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Mar 29 09:59:59 2019 +0000

    powerpc/mm: change #include "mmu_decl.h" to <mm/mmu_decl.h>
    
    This patch make inclusion of mmu_decl.h independant of the location
    of the file including it.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e12bec98366f..105c58f8900a 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -54,7 +54,7 @@
 #include <asm/swiotlb.h>
 #include <asm/rtas.h>
 
-#include "mmu_decl.h"
+#include <mm/mmu_decl.h>
 
 #ifndef CPU_FTR_COHERENT_ICACHE
 #define CPU_FTR_COHERENT_ICACHE	0	/* XXX for now */

commit 26ad26718dfaa7cf49d106d212ebf2370076c253
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Sat Mar 30 11:13:45 2019 +0530

    powerpc/mm: Fix section mismatch warning
    
    This patch fix the below section mismatch warnings.
    
    WARNING: vmlinux.o(.text+0x2d1f44): Section mismatch in reference from the function devm_memremap_pages_release() to the function .meminit.text:arch_remove_memory()
    WARNING: vmlinux.o(.text+0x2d265c): Section mismatch in reference from the function devm_memremap_pages() to the function .meminit.text:arch_add_memory()
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3665602a9dfa..e12bec98366f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -109,8 +109,8 @@ int __weak remove_section_mapping(unsigned long start, unsigned long end)
 	return -ENODEV;
 }
 
-int __meminit arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
-		bool want_memblock)
+int __ref arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+			  bool want_memblock)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
@@ -131,8 +131,8 @@ int __meminit arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-int __meminit arch_remove_memory(int nid, u64 start, u64 size,
-					struct vmem_altmap *altmap)
+int __ref arch_remove_memory(int nid, u64 start, u64 size,
+			     struct vmem_altmap *altmap)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;

commit f172acbfae1a78b1a3c775f78e8d0dcd15b9d768
Author: Laurent Vivier <lvivier@redhat.com>
Date:   Wed Mar 13 11:25:28 2019 +0100

    powerpc/mm: move warning from resize_hpt_for_hotplug()
    
    resize_hpt_for_hotplug() reports a warning when it cannot
    resize the hash page table ("Unable to resize hash page
    table to target order") but in some cases it's not a problem
    and can make user thinks something has not worked properly.
    
    This patch moves the warning to arch_remove_memory() to
    only report the problem when it is needed.
    
    Reviewed-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Laurent Vivier <lvivier@redhat.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f6787f90e158..3665602a9dfa 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -161,7 +161,8 @@ int __meminit arch_remove_memory(int nid, u64 start, u64 size,
 	 */
 	vm_unmap_aliases();
 
-	resize_hpt_for_hotplug(memblock_phys_mem_size());
+	if (resize_hpt_for_hotplug(memblock_phys_mem_size()) == -ENOSPC)
+		pr_warn("Hash collision while resizing HPT\n");
 
 	return ret;
 }

commit d0055df0c9c1471c389197a69f43e300185a75aa
Merge: 637cfeb9f99c 4a605e2d1a69
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Thu Feb 21 23:15:10 2019 +1100

    Merge branch 'topic/dma' into next
    
    Merge hch's big DMA rework series. This is in a topic branch in case he
    wants to merge it to minimise conflicts.

commit 31f940afda6add7a7bb182adde97e615e5355c6d
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Feb 13 08:01:28 2019 +0100

    powerpc/dma: use the dma-direct allocator for coherent platforms
    
    The generic code allows a few nice things such as node local allocations
    and dipping into the CMA area.  The lookup of the right zone for a given
    dma mask works a little different, but the results should be the same.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Christian Zigotzky <chzigotzky@xenosoft.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 33cc6f676fa6..a10ee3645a6c 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -69,15 +69,12 @@ pte_t *kmap_pte;
 EXPORT_SYMBOL(kmap_pte);
 pgprot_t kmap_prot;
 EXPORT_SYMBOL(kmap_prot);
-#define TOP_ZONE ZONE_HIGHMEM
 
 static inline pte_t *virt_to_kpte(unsigned long vaddr)
 {
 	return pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(vaddr),
 			vaddr), vaddr), vaddr);
 }
-#else
-#define TOP_ZONE ZONE_NORMAL
 #endif
 
 int page_is_ram(unsigned long pfn)
@@ -261,25 +258,6 @@ static int __init mark_nonram_nosave(void)
  */
 static unsigned long max_zone_pfns[MAX_NR_ZONES];
 
-/*
- * Find the least restrictive zone that is entirely below the
- * specified pfn limit.  Returns < 0 if no suitable zone is found.
- *
- * pfn_limit must be u64 because it can exceed 32 bits even on 32-bit
- * systems -- the DMA limit can be higher than any possible real pfn.
- */
-int dma_pfn_limit_to_zone(u64 pfn_limit)
-{
-	int i;
-
-	for (i = TOP_ZONE; i >= 0; i--) {
-		if (max_zone_pfns[i] <= pfn_limit)
-			return i;
-	}
-
-	return -EPERM;
-}
-
 /*
  * paging_init() sets up the page tables - in fact we've already done this.
  */

commit 26b523356f49a0117c8f9e32ca98aa6d6e496e1a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Feb 1 10:46:52 2019 +0000

    powerpc: Drop page_is_ram() and walk_system_ram_range()
    
    Since commit c40dd2f76644 ("powerpc: Add System RAM to /proc/iomem")
    it is possible to use the generic walk_system_ram_range() and
    the generic page_is_ram().
    
    To enable the use of walk_system_ram_range() by the IBM EHEA ethernet
    driver, we still need an export of the generic function.
    
    As powerpc was the only user of CONFIG_ARCH_HAS_WALK_MEMORY, the
    ifdef around the generic walk_system_ram_range() has become useless
    and can be dropped.
    
    Fixes: c40dd2f76644 ("powerpc: Add System RAM to /proc/iomem")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    [mpe: Keep the EXPORT_SYMBOL_GPL in powerpc code]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 33cc6f676fa6..81f251fc4169 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -80,11 +80,6 @@ static inline pte_t *virt_to_kpte(unsigned long vaddr)
 #define TOP_ZONE ZONE_NORMAL
 #endif
 
-int page_is_ram(unsigned long pfn)
-{
-	return memblock_is_memory(__pfn_to_phys(pfn));
-}
-
 pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 			      unsigned long size, pgprot_t vma_prot)
 {
@@ -176,34 +171,6 @@ int __meminit arch_remove_memory(int nid, u64 start, u64 size,
 #endif
 #endif /* CONFIG_MEMORY_HOTPLUG */
 
-/*
- * walk_memory_resource() needs to make sure there is no holes in a given
- * memory range.  PPC64 does not maintain the memory layout in /proc/iomem.
- * Instead it maintains it in memblock.memory structures.  Walk through the
- * memory regions, find holes and callback for contiguous regions.
- */
-int
-walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
-		void *arg, int (*func)(unsigned long, unsigned long, void *))
-{
-	struct memblock_region *reg;
-	unsigned long end_pfn = start_pfn + nr_pages;
-	unsigned long tstart, tend;
-	int ret = -1;
-
-	for_each_memblock(memory, reg) {
-		tstart = max(start_pfn, memblock_region_memory_base_pfn(reg));
-		tend = min(end_pfn, memblock_region_memory_end_pfn(reg));
-		if (tstart >= tend)
-			continue;
-		ret = (*func)(tstart, tend - tstart, arg);
-		if (ret)
-			break;
-	}
-	return ret;
-}
-EXPORT_SYMBOL_GPL(walk_system_ram_range);
-
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 void __init mem_topology_setup(void)
 {
@@ -585,3 +552,9 @@ int devmem_is_allowed(unsigned long pfn)
 	return 0;
 }
 #endif /* CONFIG_STRICT_DEVMEM */
+
+/*
+ * This is defined in kernel/resource.c but only powerpc needs to export it, for
+ * the EHEA driver. Drop this when drivers/net/ethernet/ibm/ehea is removed.
+ */
+EXPORT_SYMBOL_GPL(walk_system_ram_range);

commit 2c2a5af6fed20cf74401c9d64319c76c5ff81309
Author: Oscar Salvador <osalvador@suse.com>
Date:   Fri Dec 28 00:36:22 2018 -0800

    mm, memory_hotplug: add nid parameter to arch_remove_memory
    
    Patch series "Do not touch pages in hot-remove path", v2.
    
    This patchset aims for two things:
    
     1) A better definition about offline and hot-remove stage
     2) Solving bugs where we can access non-initialized pages
        during hot-remove operations [2] [3].
    
    This is achieved by moving all page/zone handling to the offline
    stage, so we do not need to access pages when hot-removing memory.
    
    [1] https://patchwork.kernel.org/cover/10691415/
    [2] https://patchwork.kernel.org/patch/10547445/
    [3] https://www.spinics.net/lists/linux-mm/msg161316.html
    
    This patch (of 5):
    
    This is a preparation for the following-up patches.  The idea of passing
    the nid is that it will allow us to get rid of the zone parameter
    afterwards.
    
    Link: http://lkml.kernel.org/r/20181127162005.15833-2-osalvador@suse.de
    Signed-off-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Jonathan Cameron <Jonathan.Cameron@huawei.com>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 20394e52fe27..33cc6f676fa6 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -139,7 +139,8 @@ int __meminit arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-int __meminit arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap)
+int __meminit arch_remove_memory(int nid, u64 start, u64 size,
+					struct vmem_altmap *altmap)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;

commit 25078dc1f74be16b858e914f52cc8f4d03c2271a
Author: Christoph Hellwig <hch@lst.de>
Date:   Sun Dec 16 17:53:49 2018 +0100

    powerpc: use mm zones more sensibly
    
    Powerpc has somewhat odd usage where ZONE_DMA is used for all memory on
    common 64-bit configfs, and ZONE_DMA32 is used for 31-bit schemes.
    
    Move to a scheme closer to what other architectures use (and I dare to
    say the intent of the system):
    
     - ZONE_DMA: optionally for memory < 31-bit (64-bit embedded only)
     - ZONE_NORMAL: everything addressable by the kernel
     - ZONE_HIGHMEM: memory > 32-bit for 32-bit kernels
    
    Also provide information on how ZONE_DMA is used by defining
    ARCH_ZONE_DMA_BITS.
    
    Contains various fixes from Benjamin Herrenschmidt.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 065c37d54b49..20394e52fe27 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -246,35 +246,19 @@ static int __init mark_nonram_nosave(void)
 }
 #endif
 
-static bool zone_limits_final;
-
 /*
- * The memory zones past TOP_ZONE are managed by generic mm code.
- * These should be set to zero since that's what every other
- * architecture does.
+ * Zones usage:
+ *
+ * We setup ZONE_DMA to be 31-bits on all platforms and ZONE_NORMAL to be
+ * everything else. GFP_DMA32 page allocations automatically fall back to
+ * ZONE_DMA.
+ *
+ * By using 31-bit unconditionally, we can exploit ARCH_ZONE_DMA_BITS to
+ * inform the generic DMA mapping code.  32-bit only devices (if not handled
+ * by an IOMMU anyway) will take a first dip into ZONE_NORMAL and get
+ * otherwise served by ZONE_DMA.
  */
-static unsigned long max_zone_pfns[MAX_NR_ZONES] = {
-	[0            ... TOP_ZONE        ] = ~0UL,
-	[TOP_ZONE + 1 ... MAX_NR_ZONES - 1] = 0
-};
-
-/*
- * Restrict the specified zone and all more restrictive zones
- * to be below the specified pfn.  May not be called after
- * paging_init().
- */
-void __init limit_zone_pfn(enum zone_type zone, unsigned long pfn_limit)
-{
-	int i;
-
-	if (WARN_ON(zone_limits_final))
-		return;
-
-	for (i = zone; i >= 0; i--) {
-		if (max_zone_pfns[i] > pfn_limit)
-			max_zone_pfns[i] = pfn_limit;
-	}
-}
+static unsigned long max_zone_pfns[MAX_NR_ZONES];
 
 /*
  * Find the least restrictive zone that is entirely below the
@@ -324,11 +308,14 @@ void __init paging_init(void)
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (long int)((top_of_ram - total_ram) >> 20));
 
+#ifdef CONFIG_ZONE_DMA
+	max_zone_pfns[ZONE_DMA]	= min(max_low_pfn, 0x7fffffffUL >> PAGE_SHIFT);
+#endif
+	max_zone_pfns[ZONE_NORMAL] = max_low_pfn;
 #ifdef CONFIG_HIGHMEM
-	limit_zone_pfn(ZONE_NORMAL, lowmem_end_addr >> PAGE_SHIFT);
+	max_zone_pfns[ZONE_HIGHMEM] = max_pfn;
 #endif
-	limit_zone_pfn(TOP_ZONE, top_of_ram >> PAGE_SHIFT);
-	zone_limits_final = true;
+
 	free_area_init_nodes(max_zone_pfns);
 
 	mark_nonram_nosave();

commit 5b3e84fc10ddb77193d6ac1d2be991d47264c716
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Nov 17 10:25:04 2018 +0000

    powerpc: change CONFIG_PPC_STD_MMU to CONFIG_PPC_BOOK3S
    
    Today we have:
    
    config PPC_BOOK3S
            def_bool y
            depends on PPC_BOOK3S_32 || PPC_BOOK3S_64
    
    config PPC_STD_MMU
            def_bool y
            depends on PPC_BOOK3S
    
    PPC_STD_MMU is therefore redundant with PPC_BOOK3S. Lets remove it.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0a64fffabee1..065c37d54b49 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -503,7 +503,7 @@ EXPORT_SYMBOL(flush_icache_user_range);
 void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		      pte_t *ptep)
 {
-#ifdef CONFIG_PPC_STD_MMU
+#ifdef CONFIG_PPC_BOOK3S
 	/*
 	 * We don't need to worry about _PAGE_PRESENT here because we are
 	 * called with either mm->page_table_lock held or ptl lock held
@@ -541,7 +541,7 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	}
 
 	hash_preload(vma->vm_mm, address, is_exec, trap);
-#endif /* CONFIG_PPC_STD_MMU */
+#endif /* CONFIG_PPC_BOOK3S */
 #if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
 	&& defined(CONFIG_HUGETLB_PAGE)
 	if (is_vm_hugetlb_page(vma))

commit 57c8a661d95dff48dd9c2f2496139082bbaf241a
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:49 2018 -0700

    mm: remove include/linux/bootmem.h
    
    Move remaining definitions and declarations from include/linux/bootmem.h
    into include/linux/memblock.h and remove the redundant header.
    
    The includes were replaced with the semantic patch below and then
    semi-automated removal of duplicated '#include <linux/memblock.h>
    
    @@
    @@
    - #include <linux/bootmem.h>
    + #include <linux/memblock.h>
    
    [sfr@canb.auug.org.au: dma-direct: fix up for the removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181002185342.133d1680@canb.auug.org.au
    [sfr@canb.auug.org.au: powerpc: fix up for removal of linux/bootmem.h]
      Link: http://lkml.kernel.org/r/20181005161406.73ef8727@canb.auug.org.au
    [sfr@canb.auug.org.au: x86/kaslr, ACPI/NUMA: fix for linux/bootmem.h removal]
      Link: http://lkml.kernel.org/r/20181008190341.5e396491@canb.auug.org.au
    Link: http://lkml.kernel.org/r/1536927045-23536-30-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index b3fe79064a69..0a64fffabee1 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -27,12 +27,11 @@
 #include <linux/mm.h>
 #include <linux/stddef.h>
 #include <linux/init.h>
-#include <linux/bootmem.h>
+#include <linux/memblock.h>
 #include <linux/highmem.h>
 #include <linux/initrd.h>
 #include <linux/pagemap.h>
 #include <linux/suspend.h>
-#include <linux/memblock.h>
 #include <linux/hugetlb.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>

commit c6ffc5ca8fb311a89cb6de5c31b6511308ddac8d
Author: Mike Rapoport <rppt@linux.vnet.ibm.com>
Date:   Tue Oct 30 15:09:30 2018 -0700

    memblock: rename free_all_bootmem to memblock_free_all
    
    The conversion is done using
    
    sed -i 's@free_all_bootmem@memblock_free_all@' \
        $(git grep -l free_all_bootmem)
    
    Link: http://lkml.kernel.org/r/1536927045-23536-26-git-send-email-rppt@linux.vnet.ibm.com
    Signed-off-by: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Ley Foon Tan <lftan@altera.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Paul Burton <paul.burton@mips.com>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Serge Semin <fancer.lancer@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index dd949d6649a2..b3fe79064a69 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -349,7 +349,7 @@ void __init mem_init(void)
 
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 	set_max_mapnr(max_pfn);
-	free_all_bootmem();
+	memblock_free_all();
 
 #ifdef CONFIG_HIGHMEM
 	{

commit 34eb138ed74dc95285478903148a53bd034829be
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Oct 9 13:51:54 2018 +0000

    powerpc/mm: don't use _PAGE_EXEC for calling hash_preload()
    
    The 'access' parameter of hash_preload() is either 0 or _PAGE_EXEC.
    Among the two versions of hash_preload(), only the PPC64 one is
    doing something with this 'access' parameter.
    
    In order to remove the use of _PAGE_EXEC outside platform code,
    'access' parameter is replaced by 'is_exec' which will be either
    true of false, and the PPC64 version of hash_preload() creates
    the access flag based on 'is_exec'.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index cb421aeb7674..dd949d6649a2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -509,7 +509,8 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	 * We don't need to worry about _PAGE_PRESENT here because we are
 	 * called with either mm->page_table_lock held or ptl lock held
 	 */
-	unsigned long access, trap;
+	unsigned long trap;
+	bool is_exec;
 
 	if (radix_enabled()) {
 		prefetch((void *)address);
@@ -531,16 +532,16 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	trap = current->thread.regs ? TRAP(current->thread.regs) : 0UL;
 	switch (trap) {
 	case 0x300:
-		access = 0UL;
+		is_exec = false;
 		break;
 	case 0x400:
-		access = _PAGE_EXEC;
+		is_exec = true;
 		break;
 	default:
 		return;
 	}
 
-	hash_preload(vma->vm_mm, address, access, trap);
+	hash_preload(vma->vm_mm, address, is_exec, trap);
 #endif /* CONFIG_PPC_STD_MMU */
 #if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
 	&& defined(CONFIG_HUGETLB_PAGE)

commit c766ee72235d09b0080f77474085fc17d6ae2fb1
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Oct 9 13:51:45 2018 +0000

    powerpc: handover page flags with a pgprot_t parameter
    
    In order to avoid multiple conversions, handover directly a
    pgprot_t to map_kernel_page() as already done for radix.
    
    Do the same for __ioremap_caller() and __ioremap_at().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 04ccb274a620..cb421aeb7674 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -309,11 +309,11 @@ void __init paging_init(void)
 	unsigned long end = __fix_to_virt(FIX_HOLE);
 
 	for (; v < end; v += PAGE_SIZE)
-		map_kernel_page(v, 0, 0); /* XXX gross */
+		map_kernel_page(v, 0, __pgprot(0)); /* XXX gross */
 #endif
 
 #ifdef CONFIG_HIGHMEM
-	map_kernel_page(PKMAP_BASE, 0, 0);	/* XXX gross */
+	map_kernel_page(PKMAP_BASE, 0, __pgprot(0));	/* XXX gross */
 	pkmap_page_table = virt_to_kpte(PKMAP_BASE);
 
 	kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));

commit 51c3c62b58b357e8d35e4cc32f7b4ec907426fe3
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Sep 14 11:14:11 2018 +1000

    powerpc: Avoid code patching freed init sections
    
    This stops us from doing code patching in init sections after they've
    been freed.
    
    In this chain:
      kvm_guest_init() ->
        kvm_use_magic_page() ->
          fault_in_pages_readable() ->
             __get_user() ->
               __get_user_nocheck() ->
                 barrier_nospec();
    
    We have a code patching location at barrier_nospec() and
    kvm_guest_init() is an init function. This whole chain gets inlined,
    so when we free the init section (hence kvm_guest_init()), this code
    goes away and hence should no longer be patched.
    
    We seen this as userspace memory corruption when using a memory
    checker while doing partition migration testing on powervm (this
    starts the code patching post migration via
    /sys/kernel/mobility/migration). In theory, it could also happen when
    using /sys/kernel/debug/powerpc/barrier_nospec.
    
    Cc: stable@vger.kernel.org # 4.13+
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Reviewed-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5c8530d0c611..04ccb274a620 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -63,6 +63,7 @@
 #endif
 
 unsigned long long memory_limit;
+bool init_mem_is_free;
 
 #ifdef CONFIG_HIGHMEM
 pte_t *kmap_pte;
@@ -396,6 +397,7 @@ void free_initmem(void)
 {
 	ppc_md.progress = ppc_printk_progress;
 	mark_initmem_nx();
+	init_mem_is_free = true;
 	free_initmem_default(POISON_FREE_INITMEM);
 }
 

commit d7dc899abefb4412388a5d3ec690070197d07d20
Author: Stefan Agner <stefan@agner.ch>
Date:   Thu Jun 14 15:28:02 2018 -0700

    treewide: use PHYS_ADDR_MAX to avoid type casting ULLONG_MAX
    
    With PHYS_ADDR_MAX there is now a type safe variant for all bits set.
    Make use of it.
    
    Patch created using a semantic patch as follows:
    
    // <smpl>
    @@
    typedef phys_addr_t;
    @@
    -(phys_addr_t)ULLONG_MAX
    +PHYS_ADDR_MAX
    // </smpl>
    
    Link: http://lkml.kernel.org/r/20180419214204.19322-1-stefan@agner.ch
    Signed-off-by: Stefan Agner <stefan@agner.ch>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Acked-by: Catalin Marinas <catalin.marinas@arm.com>     [arm64]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 8cecda4bd66a..5c8530d0c611 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -215,7 +215,7 @@ void __init mem_topology_setup(void)
 	/* Place all memblock_regions in the same node and merge contiguous
 	 * memblock_regions
 	 */
-	memblock_set_node(0, (phys_addr_t)ULLONG_MAX, &memblock.memory, 0);
+	memblock_set_node(0, PHYS_ADDR_MAX, &memblock.memory, 0);
 }
 
 void __init initmem_init(void)

commit 68662f85f3c3ea8a6fb61586a05955ab51bbb9e6
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Fri Jun 1 20:01:18 2018 +1000

    powerpc/64s/radix: prefetch user address in update_mmu_cache
    
    Prefetch the faulting address in update_mmu_cache to give the page
    table walker perhaps 100 cycles head start as locks are dropped and
    the interrupt completed.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index c3c39b02b2ba..8cecda4bd66a 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -509,8 +509,10 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	 */
 	unsigned long access, trap;
 
-	if (radix_enabled())
+	if (radix_enabled()) {
+		prefetch((void *)address);
 		return;
+	}
 
 	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
 	if (!pte_young(*ptep) || address >= TASK_SIZE)

commit fb5924fddf9ee31db04da7ad4e8c3434a387101b
Author: Balbir Singh <bsingharora@gmail.com>
Date:   Fri Apr 6 15:24:23 2018 +1000

    powerpc/mm: Flush cache on memory hot(un)plug
    
    This patch adds support for flushing potentially dirty cache lines
    when memory is hot-plugged/hot-un-plugged. The support is currently
    limited to 64 bit systems.
    
    The bug was exposed when mappings for a device were actually
    hot-unplugged and plugged in back later. A similar issue was observed
    during the development of memtrace, but memtrace does it's own
    flushing of region via a custom routine.
    
    These patches do a flush both on hotplug/unplug to clear any stale
    data in the cache w.r.t mappings, there is a small race window where a
    clean cache line may be created again just prior to tearing down the
    mapping.
    
    The patches were tested by disabling the flush routines in memtrace
    and doing I/O on the trace file. The system immediately
    checkstops (quite reliablly if prior to the hot-unplug of the memtrace
    region, we memset the regions we are about to hot unplug). After these
    patches no custom flushing is needed in the memtrace code.
    
    Fixes: 9d5171a8f248 ("powerpc/powernv: Enable removal of memory for in memory tracing")
    Cc: stable@vger.kernel.org # v4.14+
    Signed-off-by: Balbir Singh <bsingharora@gmail.com>
    Acked-by: Reza Arbab <arbab@linux.ibm.com>
    Reviewed-by: Rashmica Gupta <rashmica.g@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 737f8a4632cc..c3c39b02b2ba 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -133,6 +133,7 @@ int __meminit arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *
 			start, start + size, rc);
 		return -EFAULT;
 	}
+	flush_inval_dcache_range(start, start + size);
 
 	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
 }
@@ -159,6 +160,7 @@ int __meminit arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap
 
 	/* Remove htab bolted mappings for this section of memory */
 	start = (unsigned long)__va(start);
+	flush_inval_dcache_range(start, start + size);
 	ret = remove_section_mapping(start, start + size);
 
 	/* Ensure all vmalloc mappings are flushed in case they also

commit f65e67c7e3308c0af08080782d79a8cb95c44929
Author: Jonathan NeuschÃ¤fer <j.neuschaefer@gmx.net>
Date:   Wed Mar 28 02:25:41 2018 +0200

    powerpc/mm: Use memblock API for PPC32 page_is_ram
    
    To support accurate checking for different blocks of memory on PPC32,
    use the same memblock-based approach that's already used on PPC64 also
    on PPC32.
    
    Signed-off-by: Jonathan NeuschÃ¤fer <j.neuschaefer@gmx.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 8f335cf052f8..737f8a4632cc 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -82,11 +82,7 @@ static inline pte_t *virt_to_kpte(unsigned long vaddr)
 
 int page_is_ram(unsigned long pfn)
 {
-#ifndef CONFIG_PPC64	/* XXX for now */
-	return pfn < max_pfn;
-#else
 	return memblock_is_memory(__pfn_to_phys(pfn));
-#endif
 }
 
 pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,

commit 2615c93e5f52db62586112793d889face99eb905
Author: Jonathan NeuschÃ¤fer <j.neuschaefer@gmx.net>
Date:   Wed Mar 28 02:25:40 2018 +0200

    powerpc/mm: Simplify page_is_ram by using memblock_is_memory
    
    Instead of open-coding the search in page_is_ram, call memblock_is_memory.
    
    Signed-off-by: Jonathan NeuschÃ¤fer <j.neuschaefer@gmx.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e2f5025b03b0..8f335cf052f8 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -85,13 +85,7 @@ int page_is_ram(unsigned long pfn)
 #ifndef CONFIG_PPC64	/* XXX for now */
 	return pfn < max_pfn;
 #else
-	unsigned long paddr = (pfn << PAGE_SHIFT);
-	struct memblock_region *reg;
-
-	for_each_memblock(memory, reg)
-		if (paddr >= reg->base && paddr < (reg->base + reg->size))
-			return 1;
-	return 0;
+	return memblock_is_memory(__pfn_to_phys(pfn));
 #endif
 }
 

commit f437c51748fa1dd423a878c870ad203843a51c8d
Merge: 872a100a49c3 29ab6c4708a5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Mar 31 00:11:24 2018 +1100

    Merge branch 'topic/paca' into next
    
    Bring in yet another series that touches KVM code, and might need to
    be merged into the kvm-ppc branch to resolve conflicts.
    
    This required some changes in pnv_power9_force_smt4_catch/release()
    due to the paca array becomming an array of pointers.

commit 29ab6c4708a587bc27ea0c765ac36aef9c1a77c9
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:22 2018 +1000

    powerpc/mm: Pass node id into create_section_mapping
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    [mpe: Move __map_kernel_page_nid() inside #ifdef SPARSEMEM_VMEMMAP]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 4eee46ea4d96..f50ce66dd6bd 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -117,7 +117,7 @@ int memory_add_physaddr_to_nid(u64 start)
 }
 #endif
 
-int __weak create_section_mapping(unsigned long start, unsigned long end)
+int __weak create_section_mapping(unsigned long start, unsigned long end, int nid)
 {
 	return -ENODEV;
 }
@@ -137,7 +137,7 @@ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
 	resize_hpt_for_hotplug(memblock_phys_mem_size());
 
 	start = (unsigned long)__va(start);
-	rc = create_section_mapping(start, start + size);
+	rc = create_section_mapping(start, start + size, nid);
 	if (rc) {
 		pr_warn("Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
 			start, start + size, rc);

commit 9bd9be006c8ec0ccf7cb0422d35033af39d3f969
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Wed Feb 14 01:08:16 2018 +1000

    powerpc/mm/numa: move numa topology discovery earlier
    
    Split sparsemem initialisation from basic numa topology discovery.
    Move the parsing earlier in boot, before pacas are allocated.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index fe8c61149fb8..4eee46ea4d96 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -212,7 +212,7 @@ walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 EXPORT_SYMBOL_GPL(walk_system_ram_range);
 
 #ifndef CONFIG_NEED_MULTIPLE_NODES
-void __init initmem_init(void)
+void __init mem_topology_setup(void)
 {
 	max_low_pfn = max_pfn = memblock_end_of_DRAM() >> PAGE_SHIFT;
 	min_low_pfn = MEMORY_START >> PAGE_SHIFT;
@@ -224,7 +224,10 @@ void __init initmem_init(void)
 	 * memblock_regions
 	 */
 	memblock_set_node(0, (phys_addr_t)ULLONG_MAX, &memblock.memory, 0);
+}
 
+void __init initmem_init(void)
+{
 	/* XXX need to clip this if using highmem? */
 	sparse_memory_present_with_active_regions(0);
 	sparse_init();

commit bde709a70884bfc790da6fbc4467c91e8d41c51b
Author: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
Date:   Fri Mar 9 17:45:58 2018 -0300

    powerpc/mm: Fix section mismatch warning in stop_machine_change_mapping()
    
    Fix the warning messages for stop_machine_change_mapping(), and a number
    of other affected functions in its call chain.
    
    All modified functions are under CONFIG_MEMORY_HOTPLUG, so __meminit
    is okay (keeps them / does not discard them).
    
    Boot-tested on powernv/power9/radix-mmu and pseries/power8/hash-mmu.
    
        $ make -j$(nproc) CONFIG_DEBUG_SECTION_MISMATCH=y vmlinux
        ...
          MODPOST vmlinux.o
        WARNING: vmlinux.o(.text+0x6b130): Section mismatch in reference from the function stop_machine_change_mapping() to the function .meminit.text:create_physical_mapping()
        The function stop_machine_change_mapping() references
        the function __meminit create_physical_mapping().
        This is often because stop_machine_change_mapping lacks a __meminit
        annotation or the annotation of create_physical_mapping is wrong.
    
        WARNING: vmlinux.o(.text+0x6b13c): Section mismatch in reference from the function stop_machine_change_mapping() to the function .meminit.text:create_physical_mapping()
        The function stop_machine_change_mapping() references
        the function __meminit create_physical_mapping().
        This is often because stop_machine_change_mapping lacks a __meminit
        annotation or the annotation of create_physical_mapping is wrong.
        ...
    
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index fe8c61149fb8..85245ef97e72 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -127,7 +127,7 @@ int __weak remove_section_mapping(unsigned long start, unsigned long end)
 	return -ENODEV;
 }
 
-int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+int __meminit arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
 		bool want_memblock)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
@@ -148,7 +148,7 @@ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-int arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap)
+int __meminit arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;

commit 3ff1b28caaff1d66d2be7e6eb7c56f78e9046fbb
Merge: 105cf3c8c626 ee95f4059a83
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 6 10:41:33 2018 -0800

    Merge tag 'libnvdimm-for-4.16' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Ross Zwisler:
    
     - Require struct page by default for filesystem DAX to remove a number
       of surprising failure cases. This includes failures with direct I/O,
       gdb and fork(2).
    
     - Add support for the new Platform Capabilities Structure added to the
       NFIT in ACPI 6.2a. This new table tells us whether the platform
       supports flushing of CPU and memory controller caches on unexpected
       power loss events.
    
     - Revamp vmem_altmap and dev_pagemap handling to clean up code and
       better support future future PCI P2P uses.
    
     - Deprecate the ND_IOCTL_SMART_THRESHOLD command whose payload has
       become out-of-sync with recent versions of the NVDIMM_FAMILY_INTEL
       spec, and instead rely on the generic ND_CMD_CALL approach used by
       the two other IOCTL families, NVDIMM_FAMILY_{HPE,MSFT}.
    
     - Enhance nfit_test so we can test some of the new things added in
       version 1.6 of the DSM specification. This includes testing firmware
       download and simulating the Last Shutdown State (LSS) status.
    
    * tag 'libnvdimm-for-4.16' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (37 commits)
      libnvdimm, namespace: remove redundant initialization of 'nd_mapping'
      acpi, nfit: fix register dimm error handling
      libnvdimm, namespace: make min namespace size 4K
      tools/testing/nvdimm: force nfit_test to depend on instrumented modules
      libnvdimm/nfit_test: adding support for unit testing enable LSS status
      libnvdimm/nfit_test: add firmware download emulation
      nfit-test: Add platform cap support from ACPI 6.2a to test
      libnvdimm: expose platform persistence attribute for nd_region
      acpi: nfit: add persistent memory control flag for nd_region
      acpi: nfit: Add support for detect platform CPU cache flush on power loss
      device-dax: Fix trailing semicolon
      libnvdimm, btt: fix uninitialized err_lock
      dax: require 'struct page' by default for filesystem dax
      ext2: auto disable dax instead of failing mount
      ext4: auto disable dax instead of failing mount
      mm, dax: introduce pfn_t_special()
      mm: Fix devm_memremap_pages() collision handling
      mm: Fix memory size alignment in devm_memremap_pages_release()
      memremap: merge find_dev_pagemap into get_dev_pagemap
      memremap: change devm_memremap_pages interface to use struct dev_pagemap
      ...

commit da024512a1fa5c979257e442130ee1d468285057
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:55 2017 +0100

    mm: pass the vmem_altmap to arch_remove_memory and __remove_pages
    
    We can just pass this on instead of having to do a radix tree lookup
    without proper locking 2 levels into the callchain.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e670cfc2766e..22aa528b78a2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -149,11 +149,10 @@ int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
-int arch_remove_memory(u64 start, u64 size)
+int arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
-	struct vmem_altmap *altmap;
 	struct page *page;
 	int ret;
 
@@ -162,11 +161,10 @@ int arch_remove_memory(u64 start, u64 size)
 	 * when querying the zone.
 	 */
 	page = pfn_to_page(start_pfn);
-	altmap = to_vmem_altmap((unsigned long) page);
 	if (altmap)
 		page += vmem_altmap_offset(altmap);
 
-	ret = __remove_pages(page_zone(page), start_pfn, nr_pages);
+	ret = __remove_pages(page_zone(page), start_pfn, nr_pages, altmap);
 	if (ret)
 		return ret;
 

commit 24e6d5a59ac7d31adc0322de2d0117dfa370936f
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Dec 29 08:53:53 2017 +0100

    mm: pass the vmem_altmap to arch_add_memory and __add_pages
    
    We can just pass this on instead of having to do a radix tree lookup
    without proper locking 2 levels into the callchain.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 4362b86ef84c..e670cfc2766e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -127,7 +127,8 @@ int __weak remove_section_mapping(unsigned long start, unsigned long end)
 	return -ENODEV;
 }
 
-int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock)
+int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,
+		bool want_memblock)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
@@ -144,7 +145,7 @@ int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock)
 		return -EFAULT;
 	}
 
-	return __add_pages(nid, start_pfn, nr_pages, want_memblock);
+	return __add_pages(nid, start_pfn, nr_pages, altmap, want_memblock);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit f2c2cbcc35d47f1471a04155ac357521f5170371
Author: Joe Perches <joe@perches.com>
Date:   Mon Oct 24 21:00:08 2016 -0700

    powerpc: Use pr_warn instead of pr_warning
    
    At some point, pr_warning will be removed so all logging messages use
    a consistent <prefix>_warn style.
    
    Update arch/powerpc/
    
    Miscellanea:
    
    o Coalesce formats
    o Realign arguments
    o Use %s, __func__ instead of embedded function names
    o Remove unnecessary line continuations
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: Geoff Levand <geoff@infradead.org>
    [mpe: Rebase due to some %pOF changes.]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 4362b86ef84c..1281c6eb3a85 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -138,8 +138,7 @@ int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock)
 	start = (unsigned long)__va(start);
 	rc = create_section_mapping(start, start + size);
 	if (rc) {
-		pr_warning(
-			"Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
+		pr_warn("Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
 			start, start + size, rc);
 		return -EFAULT;
 	}

commit 968159c0031ac1e07ab4426397e786c9c483f068
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Aug 8 13:58:54 2017 +0200

    powerpc/8xx: Getting rid of remaining use of CONFIG_8xx
    
    Two config options exist to define powerpc MPC8xx:
    * CONFIG_PPC_8xx
    * CONFIG_8xx
    
    arch/powerpc/platforms/Kconfig.cputype has contained the following
    comment about CONFIG_8xx item for some years:
    "# this is temp to handle compat with arch=ppc"
    
    arch/powerpc is now the only place with remaining use of
    CONFIG_8xx: get rid of them.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 46b4e67d2372..4362b86ef84c 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -436,7 +436,7 @@ void flush_dcache_icache_page(struct page *page)
 		return;
 	}
 #endif
-#if defined(CONFIG_8xx) || defined(CONFIG_PPC64)
+#if defined(CONFIG_PPC_8xx) || defined(CONFIG_PPC64)
 	/* On 8xx there is no need to kmap since highmem is not supported */
 	__flush_dcache_icache(page_address(page));
 #else

commit 029d9252b116fa52a95150819e62af1f6e420fe5
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri Jul 14 16:51:23 2017 +1000

    powerpc/mm: Mark __init memory no-execute when STRICT_KERNEL_RWX=y
    
    Currently even with STRICT_KERNEL_RWX we leave the __init text marked
    executable after init, which is bad.
    
    Add a hook to mark it NX (no-execute) before we free it, and implement
    it for radix and hash.
    
    Note that we use __init_end as the end address, not _einittext,
    because overlaps_kernel_text() uses __init_end, because there are
    additional executable sections other than .init.text between
    __init_begin and __init_end.
    
    Tested on radix and hash with:
    
      0:mon> p $__init_begin
      *** 400 exception occurred
    
    Fixes: 1e0fc9d1eb2b ("powerpc/Kconfig: Enable STRICT_KERNEL_RWX for some configs")
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 8541f18694a4..46b4e67d2372 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -402,6 +402,7 @@ void __init mem_init(void)
 void free_initmem(void)
 {
 	ppc_md.progress = ppc_printk_progress;
+	mark_initmem_nx();
 	free_initmem_default(POISON_FREE_INITMEM);
 }
 

commit d691b7e7d1b5186eae62fd32adee65d3316bfdf6
Merge: b59eea554f57 1e0fc9d1eb2b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 7 13:55:45 2017 -0700

    Merge tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for STRICT_KERNEL_RWX on 64-bit server CPUs.
    
       - Platform support for FSP2 (476fpe) board
    
       - Enable ZONE_DEVICE on 64-bit server CPUs.
    
       - Generic & powerpc spin loop primitives to optimise busy waiting
    
       - Convert VDSO update function to use new update_vsyscall() interface
    
       - Optimisations to hypercall/syscall/context-switch paths
    
       - Improvements to the CPU idle code on Power8 and Power9.
    
      As well as many other fixes and improvements.
    
      Thanks to: Akshay Adiga, Andrew Donnellan, Andrew Jeffery, Anshuman
      Khandual, Anton Blanchard, Balbir Singh, Benjamin Herrenschmidt,
      Christophe Leroy, Christophe Lombard, Colin Ian King, Dan Carpenter,
      Gautham R. Shenoy, Hari Bathini, Ian Munsie, Ivan Mikhaylov, Javier
      Martinez Canillas, Madhavan Srinivasan, Masahiro Yamada, Matt Brown,
      Michael Neuling, Michal Suchanek, Murilo Opsfelder Araujo, Naveen N.
      Rao, Nicholas Piggin, Oliver O'Halloran, Paul Mackerras, Pavel Machek,
      Russell Currey, Santosh Sivaraj, Stephen Rothwell, Thiago Jung
      Bauermann, Yang Li"
    
    * tag 'powerpc-4.13-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (158 commits)
      powerpc/Kconfig: Enable STRICT_KERNEL_RWX for some configs
      powerpc/mm/radix: Implement STRICT_RWX/mark_rodata_ro() for Radix
      powerpc/mm/hash: Implement mark_rodata_ro() for hash
      powerpc/vmlinux.lds: Align __init_begin to 16M
      powerpc/lib/code-patching: Use alternate map for patch_instruction()
      powerpc/xmon: Add patch_instruction() support for xmon
      powerpc/kprobes/optprobes: Use patch_instruction()
      powerpc/kprobes: Move kprobes over to patch_instruction()
      powerpc/mm/radix: Fix execute permissions for interrupt_vectors
      powerpc/pseries: Fix passing of pp0 in updatepp() and updateboltedpp()
      powerpc/64s: Blacklist rtas entry/exit from kprobes
      powerpc/64s: Blacklist functions invoked on a trap
      powerpc/64s: Un-blacklist system_call() from kprobes
      powerpc/64s: Move system_call() symbol to just after setting MSR_EE
      powerpc/64s: Blacklist system_call() and system_call_common() from kprobes
      powerpc/64s: Convert .L__replay_interrupt_return to a local label
      powerpc64/elfv1: Only dereference function descriptor for non-text symbols
      cxl: Export library to support IBM XSL
      powerpc/dts: Use #include "..." to include local DT
      powerpc/perf/hv-24x7: Aggregate result elements on POWER9 SMT8
      ...

commit 3d79a728f9b2e6ddcce4e02c91c4de1076548a4c
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:21 2017 -0700

    mm, memory_hotplug: replace for_device by want_memblock in arch_add_memory
    
    arch_add_memory gets for_device argument which then controls whether we
    want to create memblocks for created memory sections.  Simplify the
    logic by telling whether we want memblocks directly rather than going
    through pointless negation.  This also makes the api easier to
    understand because it is clear what we want rather than nothing telling
    for_device which can mean anything.
    
    This shouldn't introduce any functional change.
    
    Link: http://lkml.kernel.org/r/20170515085827.16474-13-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 72c46eb53215..de5a90e1ceaa 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -126,7 +126,7 @@ int __weak remove_section_mapping(unsigned long start, unsigned long end)
 	return -ENODEV;
 }
 
-int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
+int arch_add_memory(int nid, u64 start, u64 size, bool want_memblock)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
@@ -143,7 +143,7 @@ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 		return -EFAULT;
 	}
 
-	return __add_pages(nid, start_pfn, nr_pages, !for_device);
+	return __add_pages(nid, start_pfn, nr_pages, want_memblock);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit f1dd2cd13c4bbbc9a7c4617b3b034fa643de98fe
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:11 2017 -0700

    mm, memory_hotplug: do not associate hotadded memory to zones until online
    
    The current memory hotplug implementation relies on having all the
    struct pages associate with a zone/node during the physical hotplug
    phase (arch_add_memory->__add_pages->__add_section->__add_zone).  In the
    vast majority of cases this means that they are added to ZONE_NORMAL.
    This has been so since 9d99aaa31f59 ("[PATCH] x86_64: Support memory
    hotadd without sparsemem") and it wasn't a big deal back then because
    movable onlining didn't exist yet.
    
    Much later memory hotplug wanted to (ab)use ZONE_MOVABLE for movable
    onlining 511c2aba8f07 ("mm, memory-hotplug: dynamic configure movable
    memory and portion memory") and then things got more complicated.
    Rather than reconsidering the zone association which was no longer
    needed (because the memory hotplug already depended on SPARSEMEM) a
    convoluted semantic of zone shifting has been developed.  Only the
    currently last memblock or the one adjacent to the zone_movable can be
    onlined movable.  This essentially means that the online type changes as
    the new memblocks are added.
    
    Let's simulate memory hot online manually
      $ echo 0x100000000 > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory32/valid_zones
      Normal Movable
    
      $ echo $((0x100000000+(128<<20))) > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
    
      $ echo $((0x100000000+2*(128<<20))) > /sys/devices/system/memory/probe
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal
      /sys/devices/system/memory/memory34/valid_zones:Normal Movable
    
      $ echo online_movable > /sys/devices/system/memory/memory34/state
      $ grep . /sys/devices/system/memory/memory3?/valid_zones
      /sys/devices/system/memory/memory32/valid_zones:Normal
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable Normal
    
    This is an awkward semantic because an udev event is sent as soon as the
    block is onlined and an udev handler might want to online it based on
    some policy (e.g.  association with a node) but it will inherently race
    with new blocks showing up.
    
    This patch changes the physical online phase to not associate pages with
    any zone at all.  All the pages are just marked reserved and wait for
    the onlining phase to be associated with the zone as per the online
    request.  There are only two requirements
    
            - existing ZONE_NORMAL and ZONE_MOVABLE cannot overlap
    
            - ZONE_NORMAL precedes ZONE_MOVABLE in physical addresses
    
    the latter one is not an inherent requirement and can be changed in the
    future.  It preserves the current behavior and made the code slightly
    simpler.  This is subject to change in future.
    
    This means that the same physical online steps as above will lead to the
    following state: Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Normal Movable
    
      /sys/devices/system/memory/memory32/valid_zones:Normal Movable
      /sys/devices/system/memory/memory33/valid_zones:Normal Movable
      /sys/devices/system/memory/memory34/valid_zones:Movable
    
    Implementation:
    The current move_pfn_range is reimplemented to check the above
    requirements (allow_online_pfn_range) and then updates the respective
    zone (move_pfn_range_to_zone), the pgdat and links all the pages in the
    pfn range with the zone/node.  __add_pages is updated to not require the
    zone and only initializes sections in the range.  This allowed to
    simplify the arch_add_memory code (s390 could get rid of quite some of
    code).
    
    devm_memremap_pages is the only user of arch_add_memory which relies on
    the zone association because it only hooks into the memory hotplug only
    half way.  It uses it to associate the new memory with ZONE_DEVICE but
    doesn't allow it to be {on,off}lined via sysfs.  This means that this
    particular code path has to call move_pfn_range_to_zone explicitly.
    
    The original zone shifting code is kept in place and will be removed in
    the follow up patch for an easier review.
    
    Please note that this patch also changes the original behavior when
    offlining a memory block adjacent to another zone (Normal vs.  Movable)
    used to allow to change its movable type.  This will be handled later.
    
    [richard.weiyang@gmail.com: simplify zone_intersects()]
      Link: http://lkml.kernel.org/r/20170616092335.5177-1-richard.weiyang@gmail.com
    [richard.weiyang@gmail.com: remove duplicate call for set_page_links]
      Link: http://lkml.kernel.org/r/20170616092335.5177-2-richard.weiyang@gmail.com
    [akpm@linux-foundation.org: remove unused local `i']
    Link: http://lkml.kernel.org/r/20170515085827.16474-12-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Signed-off-by: Wei Yang <richard.weiyang@gmail.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Tested-by: Reza Arbab <arbab@linux.vnet.ibm.com>
    Acked-by: Heiko Carstens <heiko.carstens@de.ibm.com> # For s390 bits
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e6b2e6618b6c..72c46eb53215 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -128,16 +128,12 @@ int __weak remove_section_mapping(unsigned long start, unsigned long end)
 
 int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 {
-	struct pglist_data *pgdata;
-	struct zone *zone;
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 	int rc;
 
 	resize_hpt_for_hotplug(memblock_phys_mem_size());
 
-	pgdata = NODE_DATA(nid);
-
 	start = (unsigned long)__va(start);
 	rc = create_section_mapping(start, start + size);
 	if (rc) {
@@ -147,11 +143,7 @@ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 		return -EFAULT;
 	}
 
-	/* this should work for most non-highmem platforms */
-	zone = pgdata->node_zones +
-		zone_for_memory(nid, start, size, 0, for_device);
-
-	return __add_pages(nid, zone, start_pfn, nr_pages, !for_device);
+	return __add_pages(nid, start_pfn, nr_pages, !for_device);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit 1b862aecfbd419cdc4553645bf86d07554279bed
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:37:45 2017 -0700

    mm, memory_hotplug: get rid of is_zone_device_section
    
    Device memory hotplug hooks into regular memory hotplug only half way.
    It needs memory sections to track struct pages but there is no
    need/desire to associate those sections with memory blocks and export
    them to the userspace via sysfs because they cannot be onlined anyway.
    
    This is currently expressed by for_device argument to arch_add_memory
    which then makes sure to associate the given memory range with
    ZONE_DEVICE.  register_new_memory then relies on is_zone_device_section
    to distinguish special memory hotplug from the regular one.  While this
    works now, later patches in this series want to move __add_zone outside
    of arch_add_memory path so we have to come up with something else.
    
    Add want_memblock down the __add_pages path and use it to control
    whether the section->memblock association should be done.
    arch_add_memory then just trivially want memblock for everything but
    for_device hotplug.
    
    remove_memory_section doesn't need is_zone_device_section either.  We
    can simply skip all the memblock specific cleanup if there is no
    memblock for the given section.
    
    This shouldn't introduce any functional change.
    
    Link: http://lkml.kernel.org/r/20170515085827.16474-5-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Dan Williams <dan.j.williams@intel.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 9ee536ec0739..e6b2e6618b6c 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -151,7 +151,7 @@ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 	zone = pgdata->node_zones +
 		zone_for_memory(nid, start, size, 0, for_device);
 
-	return __add_pages(nid, zone, start_pfn, nr_pages);
+	return __add_pages(nid, zone, start_pfn, nr_pages, !for_device);
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE

commit b584c2544041707ea041748dbfbb1081289c6cf5
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Wed Jun 28 11:32:33 2017 +1000

    powerpc/vmemmap: Add altmap support
    
    Adds support to powerpc for the altmap feature of ZONE_DEVICE memory. An
    altmap is a driver provided region that is used to provide the backing
    storage for the struct pages of ZONE_DEVICE memory. In situations where
    large amount of ZONE_DEVICE memory is being added to the system the
    altmap reduces pressure on main system memory by allowing the mm/
    metadata to be stored on the device itself rather in main memory.
    
    Reviewed-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 04f4c9852791..6f71d259de68 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -36,6 +36,7 @@
 #include <linux/hugetlb.h>
 #include <linux/slab.h>
 #include <linux/vmalloc.h>
+#include <linux/memremap.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -159,11 +160,20 @@ int arch_remove_memory(u64 start, u64 size)
 {
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
-	struct zone *zone;
+	struct vmem_altmap *altmap;
+	struct page *page;
 	int ret;
 
-	zone = page_zone(pfn_to_page(start_pfn));
-	ret = __remove_pages(zone, start_pfn, nr_pages);
+	/*
+	 * If we have an altmap then we need to skip over any reserved PFNs
+	 * when querying the zone.
+	 */
+	page = pfn_to_page(start_pfn);
+	altmap = to_vmem_altmap((unsigned long) page);
+	if (altmap)
+		page += vmem_altmap_offset(altmap);
+
+	ret = __remove_pages(page_zone(page), start_pfn, nr_pages);
 	if (ret)
 		return ret;
 

commit 4386c096c2ffa1b3232d701e9d7ff82a1378e1c5
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon May 29 17:31:56 2017 +0200

    powerpc/mm: Rename map_page() to map_kernel_page() on 32-bit
    
    These two functions implement the same semantics, so unify their naming so we
    can share code that calls them. The longer name is more descriptive so use it.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 9ee536ec0739..04f4c9852791 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -313,11 +313,11 @@ void __init paging_init(void)
 	unsigned long end = __fix_to_virt(FIX_HOLE);
 
 	for (; v < end; v += PAGE_SIZE)
-		map_page(v, 0, 0); /* XXX gross */
+		map_kernel_page(v, 0, 0); /* XXX gross */
 #endif
 
 #ifdef CONFIG_HIGHMEM
-	map_page(PKMAP_BASE, 0, 0);	/* XXX gross */
+	map_kernel_page(PKMAP_BASE, 0, 0);	/* XXX gross */
 	pkmap_page_table = virt_to_kpte(PKMAP_BASE);
 
 	kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));

commit 438cc81a41e8c2905d0f4f300d4690445b2ab240
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Fri Dec 9 11:07:38 2016 +1100

    powerpc/pseries: Automatically resize HPT for memory hot add/remove
    
    We've now implemented code in the pseries platform to use the new PAPR
    interface to allow resizing the hash page table (HPT) at runtime.
    
    This patch uses that interface to automatically attempt to resize the HPT
    when memory is hot added or removed.  This tries to always keep the HPT at
    a reasonable size for our current memory size.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5f844337de21..9ee536ec0739 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -134,6 +134,8 @@ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 	int rc;
 
+	resize_hpt_for_hotplug(memblock_phys_mem_size());
+
 	pgdata = NODE_DATA(nid);
 
 	start = (unsigned long)__va(start);
@@ -174,6 +176,8 @@ int arch_remove_memory(u64 start, u64 size)
 	 */
 	vm_unmap_aliases();
 
+	resize_hpt_for_hotplug(memblock_phys_mem_size());
+
 	return ret;
 }
 #endif

commit fecbfabe1dc940525f26eb1683cf64137befc3c2
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:07:54 2016 +1000

    powerpc: Fix build with CONFIG_MEMORY_HOTPLUG on some configs
    
    For memory hotplug to work, the MMU code needs to provide the functions
    create_section_mapping() and remove_section_mapping() to respectively
    map and unmap portions of the linear mapping.
    
    At the moment only hash64 provides these, so we provide weak stubs that
    just error out. This fixes the build with configurations such as 64-bit
    BookE with CONFIG_MEMORY_HOTPLUG enabled.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 9eac9d46f54b..5f844337de21 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -116,6 +116,16 @@ int memory_add_physaddr_to_nid(u64 start)
 }
 #endif
 
+int __weak create_section_mapping(unsigned long start, unsigned long end)
+{
+	return -ENODEV;
+}
+
+int __weak remove_section_mapping(unsigned long start, unsigned long end)
+{
+	return -ENODEV;
+}
+
 int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 {
 	struct pglist_data *pgdata;

commit 3079abe555511031e2ba5d1e21ddc52edc9af349
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Wed May 11 19:22:18 2016 +1000

    powerpc/mm: Ensure "special" zones are empty
    
    The mm zone mechanism was traditionally used by arch specific code to
    partition memory into allocation zones. However there are several zones
    that are managed by the mm subsystem rather than the architecture. Most
    architectures set the max PFN of these special zones to zero, however on
    powerpc we set them to ~0ul. This, in conjunction with a bug in
    free_area_init_nodes() results in all of system memory being placed in
    ZONE_DEVICE when enabled. Device memory cannot be used for regular kernel
    memory allocations so this will cause a kernel panic at boot. Given the
    planned addition of more mm managed zones (ZONE_CMA) we should aim to be
    consistent with every other architecture and set the max PFN for these
    zones to zero.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Reviewed-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2fd57fa48429..9eac9d46f54b 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -239,8 +239,14 @@ static int __init mark_nonram_nosave(void)
 
 static bool zone_limits_final;
 
+/*
+ * The memory zones past TOP_ZONE are managed by generic mm code.
+ * These should be set to zero since that's what every other
+ * architecture does.
+ */
 static unsigned long max_zone_pfns[MAX_NR_ZONES] = {
-	[0 ... MAX_NR_ZONES - 1] = ~0UL
+	[0            ... TOP_ZONE        ] = ~0UL,
+	[TOP_ZONE + 1 ... MAX_NR_ZONES - 1] = 0
 };
 
 /*

commit 171cb719dacd0f102b39bd5ab5eea0ce11813853
Author: Gavin Shan <gwshan@linux.vnet.ibm.com>
Date:   Fri Feb 26 11:26:26 2016 +1100

    powerpc/mm: Improve readability of update_mmu_cache()
    
    The function is used to update the MMU with software PTE. It can
    be called by data access exception handler (0x300) or instruction
    access exception handler (0x400). If the function is called by
    0x400 handler, the local variable @access is set to _PAGE_EXEC
    to indicate the software PTE should have that flag set. When the
    function is called by 0x300 handler, @access is set to zero.
    
    This improves the readability of the function by replacing if
    statements with switch. No logical changes introduced.
    
    Signed-off-by: Gavin Shan <gwshan@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 879e0bc6f82e..2fd57fa48429 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -491,7 +491,8 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	 * We don't need to worry about _PAGE_PRESENT here because we are
 	 * called with either mm->page_table_lock held or ptl lock held
 	 */
-	unsigned long access = 0, trap;
+	unsigned long access, trap;
+
 	if (radix_enabled())
 		return;
 
@@ -506,13 +507,19 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	 *
 	 * We also avoid filling the hash if not coming from a fault
 	 */
-	if (current->thread.regs == NULL)
-		return;
-	trap = TRAP(current->thread.regs);
-	if (trap == 0x400)
-		access |= _PAGE_EXEC;
-	else if (trap != 0x300)
+
+	trap = current->thread.regs ? TRAP(current->thread.regs) : 0UL;
+	switch (trap) {
+	case 0x300:
+		access = 0UL;
+		break;
+	case 0x400:
+		access = _PAGE_EXEC;
+		break;
+	default:
 		return;
+	}
+
 	hash_preload(vma->vm_mm, address, access, trap);
 #endif /* CONFIG_PPC_STD_MMU */
 #if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \

commit dd0b52c47a2d71b7960c9a5ce7d51f17ed03d9f5
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Thu May 5 17:54:08 2016 +1000

    powerpc/mm: define TOP_ZONE as a constant
    
    The zone that contains the top of memory will be either ZONE_NORMAL
    or ZONE_HIGHMEM depending on the kernel config. There are two functions
    that require this information and both of them use an #ifdef to set
    a local variable (top_zone). This is a little silly so lets just make it
    a constant.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Cc: linux-mm@kvack.org
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 89677884ecc8..879e0bc6f82e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -68,12 +68,15 @@ pte_t *kmap_pte;
 EXPORT_SYMBOL(kmap_pte);
 pgprot_t kmap_prot;
 EXPORT_SYMBOL(kmap_prot);
+#define TOP_ZONE ZONE_HIGHMEM
 
 static inline pte_t *virt_to_kpte(unsigned long vaddr)
 {
 	return pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(vaddr),
 			vaddr), vaddr), vaddr);
 }
+#else
+#define TOP_ZONE ZONE_NORMAL
 #endif
 
 int page_is_ram(unsigned long pfn)
@@ -267,14 +270,9 @@ void __init limit_zone_pfn(enum zone_type zone, unsigned long pfn_limit)
  */
 int dma_pfn_limit_to_zone(u64 pfn_limit)
 {
-	enum zone_type top_zone = ZONE_NORMAL;
 	int i;
 
-#ifdef CONFIG_HIGHMEM
-	top_zone = ZONE_HIGHMEM;
-#endif
-
-	for (i = top_zone; i >= 0; i--) {
+	for (i = TOP_ZONE; i >= 0; i--) {
 		if (max_zone_pfns[i] <= pfn_limit)
 			return i;
 	}
@@ -289,7 +287,6 @@ void __init paging_init(void)
 {
 	unsigned long long total_ram = memblock_phys_mem_size();
 	phys_addr_t top_of_ram = memblock_end_of_DRAM();
-	enum zone_type top_zone;
 
 #ifdef CONFIG_PPC32
 	unsigned long v = __fix_to_virt(__end_of_fixed_addresses - 1);
@@ -313,13 +310,9 @@ void __init paging_init(void)
 	       (long int)((top_of_ram - total_ram) >> 20));
 
 #ifdef CONFIG_HIGHMEM
-	top_zone = ZONE_HIGHMEM;
 	limit_zone_pfn(ZONE_NORMAL, lowmem_end_addr >> PAGE_SHIFT);
-#else
-	top_zone = ZONE_NORMAL;
 #endif
-
-	limit_zone_pfn(top_zone, top_of_ram >> PAGE_SHIFT);
+	limit_zone_pfn(TOP_ZONE, top_of_ram >> PAGE_SHIFT);
 	zone_limits_final = true;
 	free_area_init_nodes(max_zone_pfns);
 

commit a3dece6d69b0ad21b64104dff508c67a1a1f14dd
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:26:22 2016 +1000

    powerpc/radix: Update MMU cache
    
    With radix there is no MMU cache. Hence we don't need to do anything in
    update_mmu_cache().
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index ac79dbde1015..89677884ecc8 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -499,6 +499,8 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	 * called with either mm->page_table_lock held or ptl lock held
 	 */
 	unsigned long access = 0, trap;
+	if (radix_enabled())
+		return;
 
 	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
 	if (!pte_young(*ptep) || address >= TASK_SIZE)

commit d5e2d00898bdfed9586472679760fc81a2ca2d02
Merge: 31e182363b39 6e669f085d59
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 15:38:41 2016 -0700

    Merge tag 'powerpc-4.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "This was delayed a day or two by some build-breakage on old toolchains
      which we've now fixed.
    
      There's two PCI commits both acked by Bjorn.
    
      There's one commit to mm/hugepage.c which is (co)authored by Kirill.
    
      Highlights:
       - Restructure Linux PTE on Book3S/64 to Radix format from Paul
         Mackerras
       - Book3s 64 MMU cleanup in preparation for Radix MMU from Aneesh
         Kumar K.V
       - Add POWER9 cputable entry from Michael Neuling
       - FPU/Altivec/VSX save/restore optimisations from Cyril Bur
       - Add support for new ftrace ABI on ppc64le from Torsten Duwe
    
      Various cleanups & minor fixes from:
       - Adam Buchbinder, Andrew Donnellan, Balbir Singh, Christophe Leroy,
         Cyril Bur, Luis Henriques, Madhavan Srinivasan, Pan Xinhui, Russell
         Currey, Sukadev Bhattiprolu, Suraj Jitindar Singh.
    
      General:
       - atomics: Allow architectures to define their own __atomic_op_*
         helpers from Boqun Feng
       - Implement atomic{, 64}_*_return_* variants and acquire/release/
         relaxed variants for (cmp)xchg from Boqun Feng
       - Add powernv_defconfig from Jeremy Kerr
       - Fix BUG_ON() reporting in real mode from Balbir Singh
       - Add xmon command to dump OPAL msglog from Andrew Donnellan
       - Add xmon command to dump process/task similar to ps(1) from Douglas
         Miller
       - Clean up memory hotplug failure paths from David Gibson
    
      pci/eeh:
       - Redesign SR-IOV on PowerNV to give absolute isolation between VFs
         from Wei Yang.
       - EEH Support for SRIOV VFs from Wei Yang and Gavin Shan.
       - PCI/IOV: Rename and export virtfn_{add, remove} from Wei Yang
       - PCI: Add pcibios_bus_add_device() weak function from Wei Yang
       - MAINTAINERS: Update EEH details and maintainership from Russell
         Currey
    
      cxl:
       - Support added to the CXL driver for running on both bare-metal and
         hypervisor systems, from Christophe Lombard and Frederic Barrat.
       - Ignore probes for virtual afu pci devices from Vaibhav Jain
    
      perf:
       - Export Power8 generic and cache events to sysfs from Sukadev
         Bhattiprolu
       - hv-24x7: Fix usage with chip events, display change in counter
         values, display domain indices in sysfs, eliminate domain suffix in
         event names, from Sukadev Bhattiprolu
    
      Freescale:
       - Updates from Scott: "Highlights include 8xx optimizations, 32-bit
         checksum optimizations, 86xx consolidation, e5500/e6500 cpu
         hotplug, more fman and other dt bits, and minor fixes/cleanup"
    
    * tag 'powerpc-4.6-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (179 commits)
      powerpc: Fix unrecoverable SLB miss during restore_math()
      powerpc/8xx: Fix do_mtspr_cpu6() build on older compilers
      powerpc/rcpm: Fix build break when SMP=n
      powerpc/book3e-64: Use hardcoded mttmr opcode
      powerpc/fsl/dts: Add "jedec,spi-nor" flash compatible
      powerpc/T104xRDB: add tdm riser card node to device tree
      powerpc32: PAGE_EXEC required for inittext
      powerpc/mpc85xx: Add pcsphy nodes to FManV3 device tree
      powerpc/mpc85xx: Add MDIO bus muxing support to the board device tree(s)
      powerpc/86xx: Introduce and use common dtsi
      powerpc/86xx: Update device tree
      powerpc/86xx: Move dts files to fsl directory
      powerpc/86xx: Switch to kconfig fragments approach
      powerpc/86xx: Update defconfigs
      powerpc/86xx: Consolidate common platform code
      powerpc32: Remove one insn in mulhdu
      powerpc32: small optimisation in flush_icache_range()
      powerpc: Simplify test in __dma_sync()
      powerpc32: move xxxxx_dcache_range() functions inline
      powerpc32: Remove clear_pages() and define clear_page() inline
      ...

commit 1dace6c665ec59bdc4eeafa4db7228c1a673e2e2
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Feb 9 13:32:42 2016 +1000

    powerpc/mm: Clean up memory hotplug failure paths
    
    This makes a number of cleanups to handling of mapping failures during
    memory hotplug on Power:
    
    For errors creating the linear mapping for the hot-added region:
      * This is now reported with EFAULT which is more appropriate than the
        previous EINVAL (the failure is unlikely to be related to the
        function's parameters)
      * An error in this path now prints a warning message, rather than just
        silently failing to add the extra memory.
      * Previously a failure here could result in the region being partially
        mapped.  We now clean up any partial mapping before failing.
    
    For errors creating the vmemmap for the hot-added region:
       * This is now reported with EFAULT instead of causing a BUG() - this
         could happen for external reason (e.g. full hash table) so it's better
         to handle this non-fatally
       * An error message is also printed, so the failure won't be silent
       * As above a failure could cause a partially mapped region, we now
         clean this up. [mpe: move htab_remove_mapping() out of #ifdef
         CONFIG_MEMORY_HOTPLUG to enable this]
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Reviewed-by: Paul Mackerras <paulus@samba.org>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index d0f0a514b04e..f980da6d7569 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -119,12 +119,18 @@ int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 	struct zone *zone;
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
+	int rc;
 
 	pgdata = NODE_DATA(nid);
 
 	start = (unsigned long)__va(start);
-	if (create_section_mapping(start, start + size))
-		return -EINVAL;
+	rc = create_section_mapping(start, start + size);
+	if (rc) {
+		pr_warning(
+			"Unable to create mapping for hot added memory 0x%llx..0x%llx: %d\n",
+			start, start + size, rc);
+		return -EFAULT;
+	}
 
 	/* this should work for most non-highmem platforms */
 	zone = pgdata->node_zones +

commit 35d98e93fe6a7ab612f6b389ce42c1dc135d6eef
Author: Toshi Kani <toshi.kani@hpe.com>
Date:   Tue Jan 26 21:57:22 2016 +0100

    arch: Set IORESOURCE_SYSTEM_RAM flag for System RAM
    
    Set IORESOURCE_SYSTEM_RAM in flags of resource ranges with
    "System RAM", "Kernel code", "Kernel data", and "Kernel bss".
    
    Note that:
    
     - IORESOURCE_SYSRAM (i.e. modifier bit) is set in flags when
       IORESOURCE_MEM is already set. IORESOURCE_SYSTEM_RAM is defined
       as (IORESOURCE_MEM|IORESOURCE_SYSRAM).
    
     - Some archs do not set 'flags' for children nodes, such as
       "Kernel code".  This patch does not change 'flags' in this
       case.
    
    Signed-off-by: Toshi Kani <toshi.kani@hpe.com>
    Signed-off-by: Borislav Petkov <bp@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis R. Rodriguez <mcgrof@suse.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: linux-arch@vger.kernel.org
    Cc: linux-arm-kernel@lists.infradead.org
    Cc: linux-mips@linux-mips.org
    Cc: linux-mm <linux-mm@kvack.org>
    Cc: linux-parisc@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-sh@vger.kernel.org
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: sparclinux@vger.kernel.org
    Link: http://lkml.kernel.org/r/1453841853-11383-7-git-send-email-bp@alien8.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index d0f0a514b04e..f078a1f94fc2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -541,7 +541,7 @@ static int __init add_system_ram_resources(void)
 			res->name = "System RAM";
 			res->start = base;
 			res->end = base + size - 1;
-			res->flags = IORESOURCE_MEM | IORESOURCE_BUSY;
+			res->flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
 			WARN_ON(request_resource(&iomem_resource, res) < 0);
 		}
 	}

commit e256caa7d0515e301f8c8c6e7d1204a2b67b1381
Author: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
Date:   Thu Jan 21 21:45:31 2016 +0530

    powerpc/mm: Allow user space to map rtas_rmo_buf
    
    With commit 90a545e9 (restrict /dev/mem to idle io memory ranges) mapping
    rtas_rmo_buf from user space is failing. Hence we are not able to make
    RTAS syscall.
    
    This patch calls page_is_rtas_user_buf before calling iomem_is_exclusive
    in  devmem_is_allowed(). This will allow user space to map rtas_rmo_buf
    and we are able to make RTAS syscall.
    
    Reported-by: Bharata B Rao <bharata@linux.vnet.ibm.com>
    CC: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Vasant Hegde <hegdevasant@linux.vnet.ibm.com>
    Acked-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 22d94c3e6fc4..d0f0a514b04e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -560,12 +560,12 @@ subsys_initcall(add_system_ram_resources);
  */
 int devmem_is_allowed(unsigned long pfn)
 {
+	if (page_is_rtas_user_buf(pfn))
+		return 1;
 	if (iomem_is_exclusive(PFN_PHYS(pfn)))
 		return 0;
 	if (!page_is_ram(pfn))
 		return 1;
-	if (page_is_rtas_user_buf(pfn))
-		return 1;
 	return 0;
 }
 #endif /* CONFIG_STRICT_DEVMEM */

commit 12f03ee606914317e7e6a0815e53a48205c31dae
Merge: d9241b22b58e 004f1afbe199
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 8 14:35:59 2015 -0700

    Merge tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm updates from Dan Williams:
     "This update has successfully completed a 0day-kbuild run and has
      appeared in a linux-next release.  The changes outside of the typical
      drivers/nvdimm/ and drivers/acpi/nfit.[ch] paths are related to the
      removal of IORESOURCE_CACHEABLE, the introduction of memremap(), and
      the introduction of ZONE_DEVICE + devm_memremap_pages().
    
      Summary:
    
       - Introduce ZONE_DEVICE and devm_memremap_pages() as a generic
         mechanism for adding device-driver-discovered memory regions to the
         kernel's direct map.
    
         This facility is used by the pmem driver to enable pfn_to_page()
         operations on the page frames returned by DAX ('direct_access' in
         'struct block_device_operations').
    
         For now, the 'memmap' allocation for these "device" pages comes
         from "System RAM".  Support for allocating the memmap from device
         memory will arrive in a later kernel.
    
       - Introduce memremap() to replace usages of ioremap_cache() and
         ioremap_wt().  memremap() drops the __iomem annotation for these
         mappings to memory that do not have i/o side effects.  The
         replacement of ioremap_cache() with memremap() is limited to the
         pmem driver to ease merging the api change in v4.3.
    
         Completion of the conversion is targeted for v4.4.
    
       - Similar to the usage of memcpy_to_pmem() + wmb_pmem() in the pmem
         driver, update the VFS DAX implementation and PMEM api to provide
         persistence guarantees for kernel operations on a DAX mapping.
    
       - Convert the ACPI NFIT 'BLK' driver to map the block apertures as
         cacheable to improve performance.
    
       - Miscellaneous updates and fixes to libnvdimm including support for
         issuing "address range scrub" commands, clarifying the optimal
         'sector size' of pmem devices, a clarification of the usage of the
         ACPI '_STA' (status) property for DIMM devices, and other minor
         fixes"
    
    * tag 'libnvdimm-for-4.3' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (34 commits)
      libnvdimm, pmem: direct map legacy pmem by default
      libnvdimm, pmem: 'struct page' for pmem
      libnvdimm, pfn: 'struct page' provider infrastructure
      x86, pmem: clarify that ARCH_HAS_PMEM_API implies PMEM mapped WB
      add devm_memremap_pages
      mm: ZONE_DEVICE for "device memory"
      mm: move __phys_to_pfn and __pfn_to_phys to asm/generic/memory_model.h
      dax: drop size parameter to ->direct_access()
      nd_blk: change aperture mapping from WC to WB
      nvdimm: change to use generic kvfree()
      pmem, dax: have direct_access use __pmem annotation
      dax: update I/O path to do proper PMEM flushing
      pmem: add copy_from_iter_pmem() and clear_pmem()
      pmem, x86: clean up conditional pmem includes
      pmem: remove layer when calling arch_has_wmb_pmem()
      pmem, x86: move x86 PMEM API to new pmem.h header
      libnvdimm, e820: make CONFIG_X86_PMEM_LEGACY a tristate option
      pmem: switch to devm_ allocations
      devres: add devm_memremap
      libnvdimm, btt: write and validate parent_uuid
      ...

commit 033fbae988fcb67e5077203512181890848b8e90
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Sun Aug 9 15:29:06 2015 -0400

    mm: ZONE_DEVICE for "device memory"
    
    While pmem is usable as a block device or via DAX mappings to userspace
    there are several usage scenarios that can not target pmem due to its
    lack of struct page coverage. In preparation for "hot plugging" pmem
    into the vmemmap add ZONE_DEVICE as a new zone to tag these pages
    separately from the ones that are subject to standard page allocations.
    Importantly "device memory" can be removed at will by userspace
    unbinding the driver of the device.
    
    Having a separate zone prevents allocation and otherwise marks these
    pages that are distinct from typical uniform memory.  Device memory has
    different lifetime and performance characteristics than RAM.  However,
    since we have run out of ZONES_SHIFT bits this functionality currently
    depends on sacrificing ZONE_DMA.
    
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Jerome Glisse <j.glisse@gmail.com>
    [hch: various simplifications in the arch interface]
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0f11819d8f1d..6571cfb05668 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -113,7 +113,7 @@ int memory_add_physaddr_to_nid(u64 start)
 }
 #endif
 
-int arch_add_memory(int nid, u64 start, u64 size)
+int arch_add_memory(int nid, u64 start, u64 size, bool for_device)
 {
 	struct pglist_data *pgdata;
 	struct zone *zone;
@@ -128,7 +128,7 @@ int arch_add_memory(int nid, u64 start, u64 size)
 
 	/* this should work for most non-highmem platforms */
 	zone = pgdata->node_zones +
-		zone_for_memory(nid, start, size, 0);
+		zone_for_memory(nid, start, size, 0, for_device);
 
 	return __add_pages(nid, zone, start_pfn, nr_pages);
 }

commit 2f7d2b74a9dd9140053f143e1c94da0fef3c1109
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Apr 15 19:40:23 2015 -0500

    powerpc/mm: Don't call __flush_dcache_icache_phys() with PA>VA
    
    __flush_dcache_icache_phys() requires the ability to access the
    memory with the MMU disabled, which means that on a 32-bit system
    any memory above 4 GiB is inaccessible.  In particular, mpc86xx is
    32-bit and can have more than 4 GiB of RAM.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0f11819d8f1d..e1fe333da946 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -414,17 +414,17 @@ void flush_dcache_icache_page(struct page *page)
 		return;
 	}
 #endif
-#ifdef CONFIG_BOOKE
-	{
+#if defined(CONFIG_8xx) || defined(CONFIG_PPC64)
+	/* On 8xx there is no need to kmap since highmem is not supported */
+	__flush_dcache_icache(page_address(page));
+#else
+	if (IS_ENABLED(CONFIG_BOOKE) || sizeof(phys_addr_t) > sizeof(void *)) {
 		void *start = kmap_atomic(page);
 		__flush_dcache_icache(start);
 		kunmap_atomic(start);
+	} else {
+		__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
 	}
-#elif defined(CONFIG_8xx) || defined(CONFIG_PPC64)
-	/* On 8xx there is no need to kmap since highmem is not supported */
-	__flush_dcache_icache(page_address(page)); 
-#else
-	__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
 #endif
 }
 EXPORT_SYMBOL(flush_dcache_icache_page);

commit 6c0cc62715bfd0b26f7d1a79e6e8143085950ca7
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Apr 17 16:17:14 2015 -0500

    powerpc/mm: Use PFN_PHYS() in devmem_is_allowed()
    
    This function can run on systems where physical addresses don't
    fit in unsigned long, so make sure to use the macro that contains the
    proper cast.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 45fda71feb27..0f11819d8f1d 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -560,7 +560,7 @@ subsys_initcall(add_system_ram_resources);
  */
 int devmem_is_allowed(unsigned long pfn)
 {
-	if (iomem_is_exclusive(pfn << PAGE_SHIFT))
+	if (iomem_is_exclusive(PFN_PHYS(pfn)))
 		return 0;
 	if (!page_is_ram(pfn))
 		return 1;

commit f691fa1080c37c48da0cdfeae082c3bef5df2643
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Mar 30 14:10:37 2015 +1100

    powerpc: Replace mem_init_done with slab_is_available()
    
    We have a powerpc specific global called mem_init_done which is "set on
    boot once kmalloc can be called".
    
    But that's not *quite* true. We set it at the bottom of mem_init(), and
    rely on the fact that mm_init() calls kmem_cache_init() immediately
    after that, and nothing is running in parallel.
    
    So replace it with the generic and 100% correct slab_is_available().
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index b7285a5870f8..45fda71feb27 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -61,7 +61,6 @@
 #define CPU_FTR_NOEXECUTE	0
 #endif
 
-int mem_init_done;
 unsigned long long memory_limit;
 
 #ifdef CONFIG_HIGHMEM
@@ -377,8 +376,6 @@ void __init mem_init(void)
 	pr_info("  * 0x%08lx..0x%08lx  : vmalloc & ioremap\n",
 		VMALLOC_START, VMALLOC_END);
 #endif /* CONFIG_PPC32 */
-
-	mem_init_done = 1;
 }
 
 void free_initmem(void)

commit 21098b9e07476deb3f40acd7e51cffbffb4ef865
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Sep 17 22:15:36 2014 +1000

    powerpc: Move sparse_init() into initmem_init
    
    We did part of sparse initialisation in setup_arch and part in
    initmem_init. Put them together.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Tested-by: Emil Medve <Emilian.Medve@Freescale.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e076d19c095b..b7285a5870f8 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -205,6 +205,7 @@ void __init initmem_init(void)
 
 	/* XXX need to clip this if using highmem? */
 	sparse_memory_present_with_active_regions(0);
+	sparse_init();
 }
 
 /* mark pages that don't exist as nosave */

commit 10239733ee8617bac3f1c1769af43a88ed979324
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Sep 17 22:15:33 2014 +1000

    powerpc: Remove bootmem allocator
    
    At the moment we transition from the memblock alloctor to the bootmem
    allocator. Gitting rid of the bootmem allocator removes a bunch of
    complicated code (most of which I owe the dubious honour of being
    responsible for writing).
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Tested-by: Emil Medve <Emilian.Medve@Freescale.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2add0b7b9f6d..e076d19c095b 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -61,7 +61,6 @@
 #define CPU_FTR_NOEXECUTE	0
 #endif
 
-int init_bootmem_done;
 int mem_init_done;
 unsigned long long memory_limit;
 
@@ -190,70 +189,22 @@ walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 }
 EXPORT_SYMBOL_GPL(walk_system_ram_range);
 
-/*
- * Initialize the bootmem system and give it all the memory we
- * have available.  If we are using highmem, we only put the
- * lowmem into the bootmem system.
- */
 #ifndef CONFIG_NEED_MULTIPLE_NODES
-void __init do_init_bootmem(void)
+void __init initmem_init(void)
 {
-	unsigned long start, bootmap_pages;
-	unsigned long total_pages;
-	struct memblock_region *reg;
-	int boot_mapsize;
-
 	max_low_pfn = max_pfn = memblock_end_of_DRAM() >> PAGE_SHIFT;
-	total_pages = (memblock_end_of_DRAM() - memstart_addr) >> PAGE_SHIFT;
+	min_low_pfn = MEMORY_START >> PAGE_SHIFT;
 #ifdef CONFIG_HIGHMEM
-	total_pages = total_lowmem >> PAGE_SHIFT;
 	max_low_pfn = lowmem_end_addr >> PAGE_SHIFT;
 #endif
 
-	/*
-	 * Find an area to use for the bootmem bitmap.  Calculate the size of
-	 * bitmap required as (Total Memory) / PAGE_SIZE / BITS_PER_BYTE.
-	 * Add 1 additional page in case the address isn't page-aligned.
-	 */
-	bootmap_pages = bootmem_bootmap_pages(total_pages);
-
-	start = memblock_alloc(bootmap_pages << PAGE_SHIFT, PAGE_SIZE);
-
-	min_low_pfn = MEMORY_START >> PAGE_SHIFT;
-	boot_mapsize = init_bootmem_node(NODE_DATA(0), start >> PAGE_SHIFT, min_low_pfn, max_low_pfn);
-
 	/* Place all memblock_regions in the same node and merge contiguous
 	 * memblock_regions
 	 */
 	memblock_set_node(0, (phys_addr_t)ULLONG_MAX, &memblock.memory, 0);
 
-	/* Add all physical memory to the bootmem map, mark each area
-	 * present.
-	 */
-#ifdef CONFIG_HIGHMEM
-	free_bootmem_with_active_regions(0, lowmem_end_addr >> PAGE_SHIFT);
-
-	/* reserve the sections we're already using */
-	for_each_memblock(reserved, reg) {
-		unsigned long top = reg->base + reg->size - 1;
-		if (top < lowmem_end_addr)
-			reserve_bootmem(reg->base, reg->size, BOOTMEM_DEFAULT);
-		else if (reg->base < lowmem_end_addr) {
-			unsigned long trunc_size = lowmem_end_addr - reg->base;
-			reserve_bootmem(reg->base, trunc_size, BOOTMEM_DEFAULT);
-		}
-	}
-#else
-	free_bootmem_with_active_regions(0, max_pfn);
-
-	/* reserve the sections we're already using */
-	for_each_memblock(reserved, reg)
-		reserve_bootmem(reg->base, reg->size, BOOTMEM_DEFAULT);
-#endif
 	/* XXX need to clip this if using highmem? */
 	sparse_memory_present_with_active_regions(0);
-
-	init_bootmem_done = 1;
 }
 
 /* mark pages that don't exist as nosave */
@@ -369,14 +320,6 @@ void __init paging_init(void)
 	mark_nonram_nosave();
 }
 
-static void __init register_page_bootmem_info(void)
-{
-	int i;
-
-	for_each_online_node(i)
-		register_page_bootmem_info_node(NODE_DATA(i));
-}
-
 void __init mem_init(void)
 {
 	/*
@@ -389,7 +332,6 @@ void __init mem_init(void)
 	swiotlb_init(0);
 #endif
 
-	register_page_bootmem_info();
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 	set_max_mapnr(max_pfn);
 	free_all_bootmem();

commit 16d0f5c4af76b0c3424290937bf1ac22adf439b1
Author: Anton Blanchard <anton@samba.org>
Date:   Tue Oct 14 22:17:47 2014 +1100

    powerpc: Remove ppc_md.remove_memory
    
    We have an extra level of indirection on memory hot remove which is not
    matched on memory hot add. Memory hotplug is book3s only, so there is
    no need for it.
    
    This also enables means remove_memory() (ie memory hot unplug) works
    on powernv.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 8ebaac75c940..2add0b7b9f6d 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -35,6 +35,7 @@
 #include <linux/memblock.h>
 #include <linux/hugetlb.h>
 #include <linux/slab.h>
+#include <linux/vmalloc.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -144,8 +145,17 @@ int arch_remove_memory(u64 start, u64 size)
 
 	zone = page_zone(pfn_to_page(start_pfn));
 	ret = __remove_pages(zone, start_pfn, nr_pages);
-	if (!ret && (ppc_md.remove_memory))
-		ret = ppc_md.remove_memory(start, size);
+	if (ret)
+		return ret;
+
+	/* Remove htab bolted mappings for this section of memory */
+	start = (unsigned long)__va(start);
+	ret = remove_section_mapping(start, start + size);
+
+	/* Ensure all vmalloc mappings are flushed in case they also
+	 * hit that section of memory
+	 */
+	vm_unmap_aliases();
 
 	return ret;
 }

commit 75d43b2d0a323ba894d85060888f039e41b441ca
Merge: d0b7abb2c7c0 cb0446c1b625
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Oct 4 08:59:06 2014 +1000

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/scottwood/linux.git
    
    Freescale updates from Scott (27 commits):
    
      "Highlights include DMA32 zone support (SATA, USB, etc now works on 64-bit
       FSL kernels), MSI changes, 8xx optimizations and cleanup, t104x board
       support, and PrPMC PCI enumeration."

commit 6db35ad2373eed5deb3b105ae7c1e9de3e34ae94
Author: Scott Wood <scottwood@freescale.com>
Date:   Thu Sep 18 14:05:02 2014 -0500

    powerpc/mm: Use common paging_init() for NUMA
    
    Commit 1c98025c6c95bc057a25e2c6596de23288c68160 "powerpc: Dynamic DMA
    zone limits" updated how zones are created in paging_init(), but missed
    the NUMA version of paging_init().  This was noticed via a linker
    error, since dma_pfn_limit_to_zone() was, like the non-NUMA
    paging_init(), limited by #ifndef CONFIG_NEED_MULTIPLE_NODES.
    
    It turns out that the NUMA paging_init() was not actually doing
    anything different from the standard paging_init(), other than a couple
    debug prints, a couple 32-bit-only ifdef sections, and a call to
    mark_nonram_nosave().  It's not clear whether mark_nonram_nosave() is
    inherently wrong to do for NUMA, or just not useful on targets that
    have NUMA, but for now I'm preserving the existing behavior.
    
    Fixes: 1c98025c6c9 "powerpc: Dynamic DMA zone limits"
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 687e7f7f7751..420dfff22ba5 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -259,6 +259,12 @@ static int __init mark_nonram_nosave(void)
 	}
 	return 0;
 }
+#else /* CONFIG_NEED_MULTIPLE_NODES */
+static int __init mark_nonram_nosave(void)
+{
+	return 0;
+}
+#endif
 
 static bool zone_limits_final;
 
@@ -351,7 +357,6 @@ void __init paging_init(void)
 
 	mark_nonram_nosave();
 }
-#endif /* ! CONFIG_NEED_MULTIPLE_NODES */
 
 static void __init register_page_bootmem_info(void)
 {

commit 1c98025c6c95bc057a25e2c6596de23288c68160
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Aug 8 18:40:42 2014 -0500

    powerpc: Dynamic DMA zone limits
    
    Platform code can call limit_zone_pfn() to set appropriate limits
    for ZONE_DMA and ZONE_DMA32, and dma_direct_alloc_coherent() will
    select a suitable zone based on a device's mask and the pfn limits that
    platform code has configured.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Shaohui Xie <Shaohui.Xie@freescale.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2c8e90f5789e..687e7f7f7751 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -260,6 +260,54 @@ static int __init mark_nonram_nosave(void)
 	return 0;
 }
 
+static bool zone_limits_final;
+
+static unsigned long max_zone_pfns[MAX_NR_ZONES] = {
+	[0 ... MAX_NR_ZONES - 1] = ~0UL
+};
+
+/*
+ * Restrict the specified zone and all more restrictive zones
+ * to be below the specified pfn.  May not be called after
+ * paging_init().
+ */
+void __init limit_zone_pfn(enum zone_type zone, unsigned long pfn_limit)
+{
+	int i;
+
+	if (WARN_ON(zone_limits_final))
+		return;
+
+	for (i = zone; i >= 0; i--) {
+		if (max_zone_pfns[i] > pfn_limit)
+			max_zone_pfns[i] = pfn_limit;
+	}
+}
+
+/*
+ * Find the least restrictive zone that is entirely below the
+ * specified pfn limit.  Returns < 0 if no suitable zone is found.
+ *
+ * pfn_limit must be u64 because it can exceed 32 bits even on 32-bit
+ * systems -- the DMA limit can be higher than any possible real pfn.
+ */
+int dma_pfn_limit_to_zone(u64 pfn_limit)
+{
+	enum zone_type top_zone = ZONE_NORMAL;
+	int i;
+
+#ifdef CONFIG_HIGHMEM
+	top_zone = ZONE_HIGHMEM;
+#endif
+
+	for (i = top_zone; i >= 0; i--) {
+		if (max_zone_pfns[i] <= pfn_limit)
+			return i;
+	}
+
+	return -EPERM;
+}
+
 /*
  * paging_init() sets up the page tables - in fact we've already done this.
  */
@@ -267,7 +315,7 @@ void __init paging_init(void)
 {
 	unsigned long long total_ram = memblock_phys_mem_size();
 	phys_addr_t top_of_ram = memblock_end_of_DRAM();
-	unsigned long max_zone_pfns[MAX_NR_ZONES];
+	enum zone_type top_zone;
 
 #ifdef CONFIG_PPC32
 	unsigned long v = __fix_to_virt(__end_of_fixed_addresses - 1);
@@ -289,13 +337,16 @@ void __init paging_init(void)
 	       (unsigned long long)top_of_ram, total_ram);
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (long int)((top_of_ram - total_ram) >> 20));
-	memset(max_zone_pfns, 0, sizeof(max_zone_pfns));
+
 #ifdef CONFIG_HIGHMEM
-	max_zone_pfns[ZONE_DMA] = lowmem_end_addr >> PAGE_SHIFT;
-	max_zone_pfns[ZONE_HIGHMEM] = top_of_ram >> PAGE_SHIFT;
+	top_zone = ZONE_HIGHMEM;
+	limit_zone_pfn(ZONE_NORMAL, lowmem_end_addr >> PAGE_SHIFT);
 #else
-	max_zone_pfns[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
+	top_zone = ZONE_NORMAL;
 #endif
+
+	limit_zone_pfn(top_zone, top_of_ram >> PAGE_SHIFT);
+	zone_limits_final = true;
 	free_area_init_nodes(max_zone_pfns);
 
 	mark_nonram_nosave();

commit f51202de8ffd9dc8ca76ea81a916349138e234ca
Author: Wang Nan <wangnan0@huawei.com>
Date:   Wed Aug 6 16:07:45 2014 -0700

    memory-hotplug: ppc: suitable memory should go to ZONE_MOVABLE
    
    This patch introduces zone_for_memory() to arch_add_memory() on powerpc
    to ensure new, higher memory added into ZONE_MOVABLE if movable zone has
    already setup.
    
    Signed-off-by: Wang Nan <wangnan0@huawei.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: "Mel Gorman" <mgorman@suse.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2c8e90f5789e..e0f7a189c48e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -128,7 +128,8 @@ int arch_add_memory(int nid, u64 start, u64 size)
 		return -EINVAL;
 
 	/* this should work for most non-highmem platforms */
-	zone = pgdata->node_zones;
+	zone = pgdata->node_zones +
+		zone_for_memory(nid, start, size, 0);
 
 	return __add_pages(nid, zone, start_pfn, nr_pages);
 }

commit 9ac8cde938481cd0e3f700b8f071c4eca989c9f6
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Mon Jan 27 10:54:06 2014 -0600

    powerpc/pseries: Use remove_memory() to remove memory
    
    The memory remove code for powerpc/pseries should call remove_memory()
    so that we are holding the hotplug_memory lock during memory remove
    operations.
    
    This patch updates the memory node remove handler to call remove_memory()
    and adds a ppc_md.remove_memory() entry to handle pseries specific work
    that is called from arch_remove_memory().
    
    During memory remove in pseries_remove_memblock() we have to stay with
    removing memory one section at a time. This is needed because of how memory
    resources are handled. During memory add for pseries (via the probe file in
    sysfs) we add memory one section at a time which gives us a memory resource
    for each section. Future patches will aim to address this so will not have
    to remove memory one section at a time.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 4b5cd5c2594d..2c8e90f5789e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -139,9 +139,14 @@ int arch_remove_memory(u64 start, u64 size)
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 	struct zone *zone;
+	int ret;
 
 	zone = page_zone(pfn_to_page(start_pfn));
-	return __remove_pages(zone, start_pfn, nr_pages);
+	ret = __remove_pages(zone, start_pfn, nr_pages);
+	if (!ret && (ppc_md.remove_memory))
+		ret = ppc_md.remove_memory(start, size);
+
+	return ret;
 }
 #endif
 #endif /* CONFIG_MEMORY_HOTPLUG */

commit 1b17366d695c8ab03f98d0155357e97a427e1dce
Merge: d12de1ef5eba 7179ba52889b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 27 21:11:26 2014 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "So here's my next branch for powerpc.  A bit late as I was on vacation
      last week.  It's mostly the same stuff that was in next already, I
      just added two patches today which are the wiring up of lockref for
      powerpc, which for some reason fell through the cracks last time and
      is trivial.
    
      The highlights are, in addition to a bunch of bug fixes:
    
       - Reworked Machine Check handling on kernels running without a
         hypervisor (or acting as a hypervisor).  Provides hooks to handle
         some errors in real mode such as TLB errors, handle SLB errors,
         etc...
    
       - Support for retrieving memory error information from the service
         processor on IBM servers running without a hypervisor and routing
         them to the memory poison infrastructure.
    
       - _PAGE_NUMA support on server processors
    
       - 32-bit BookE relocatable kernel support
    
       - FSL e6500 hardware tablewalk support
    
       - A bunch of new/revived board support
    
       - FSL e6500 deeper idle states and altivec powerdown support
    
      You'll notice a generic mm change here, it has been acked by the
      relevant authorities and is a pre-req for our _PAGE_NUMA support"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (121 commits)
      powerpc: Implement arch_spin_is_locked() using arch_spin_value_unlocked()
      powerpc: Add support for the optimised lockref implementation
      powerpc/powernv: Call OPAL sync before kexec'ing
      powerpc/eeh: Escalate error on non-existing PE
      powerpc/eeh: Handle multiple EEH errors
      powerpc: Fix transactional FP/VMX/VSX unavailable handlers
      powerpc: Don't corrupt transactional state when using FP/VMX in kernel
      powerpc: Reclaim two unused thread_info flag bits
      powerpc: Fix races with irq_work
      Move precessing of MCE queued event out from syscall exit path.
      pseries/cpuidle: Remove redundant call to ppc64_runlatch_off() in cpu idle routines
      powerpc: Make add_system_ram_resources() __init
      powerpc: add SATA_MV to ppc64_defconfig
      powerpc/powernv: Increase candidate fw image size
      powerpc: Add debug checks to catch invalid cpu-to-node mappings
      powerpc: Fix the setup of CPU-to-Node mappings during CPU online
      powerpc/iommu: Don't detach device without IOMMU group
      powerpc/eeh: Hotplug improvement
      powerpc/eeh: Call opal_pci_reinit() on powernv for restoring config space
      powerpc/eeh: Add restore_config operation
      ...

commit e7e8de5918dd6a07cbddae559600d7765ad6a56e
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Tue Jan 21 15:49:26 2014 -0800

    memblock: make memblock_set_node() support different memblock_type
    
    [sfr@canb.auug.org.au: fix powerpc build]
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Reviewed-by: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Rafael J . Wysocki" <rjw@sisk.pl>
    Cc: Chen Tang <imtangchen@gmail.com>
    Cc: Gong Chen <gong.chen@linux.intel.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Len Brown <lenb@kernel.org>
    Cc: Liu Jiang <jiang.liu@huawei.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Taku Izumi <izumi.taku@jp.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Thomas Renninger <trenn@suse.de>
    Cc: Toshi Kani <toshi.kani@hp.com>
    Cc: Vasilis Liaskovitis <vasilis.liaskovitis@profitbricks.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3fa93dc7fe75..8c1dd23652a1 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -209,7 +209,7 @@ void __init do_init_bootmem(void)
 	/* Place all memblock_regions in the same node and merge contiguous
 	 * memblock_regions
 	 */
-	memblock_set_node(0, (phys_addr_t)ULLONG_MAX, 0);
+	memblock_set_node(0, (phys_addr_t)ULLONG_MAX, &memblock.memory, 0);
 
 	/* Add all physical memory to the bootmem map, mark each area
 	 * present.

commit fac515db45207718168cb55ca4d0a390e43b61af
Merge: 3ac8ff1c475b d064f30e5063
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jan 15 14:22:35 2014 +1100

    Merge remote-tracking branch 'scott/next' into next
    
    Freescale updates from Scott:
    
    <<
    Highlights include 32-bit booke relocatable support, e6500 hardware
    tablewalk support, various e500 SPE fixes, some new/revived boards, and
    e6500 deeper idle and altivec powerdown modes.
    >>

commit 4f7709248db7402507a2a5164f86ad4c75a1926b
Author: Geert Uytterhoeven <geert@linux-m68k.org>
Date:   Sun Sep 15 11:39:36 2013 +0200

    powerpc: Make add_system_ram_resources() __init
    
    add_system_ram_resources() is a subsys_initcall.
    
    Signed-off-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3fa93dc7fe75..9c25feba7c40 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -507,7 +507,7 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
  * System memory should not be in /proc/iomem but various tools expect it
  * (eg kdump).
  */
-static int add_system_ram_resources(void)
+static int __init add_system_ram_resources(void)
 {
 	struct memblock_region *reg;
 

commit 28efc35fe68dacbddc4b12c2fa8f2df1593a4ad3
Author: Scott Wood <scottwood@freescale.com>
Date:   Fri Oct 11 19:22:38 2013 -0500

    powerpc/e6500: TLB miss handler with hardware tablewalk support
    
    There are a few things that make the existing hw tablewalk handlers
    unsuitable for e6500:
    
     - Indirect entries go in TLB1 (though the resulting direct entries go in
       TLB0).
    
     - It has threads, but no "tlbsrx." -- so we need a spinlock and
       a normal "tlbsx".  Because we need this lock, hardware tablewalk
       is mandatory on e6500 unless we want to add spinlock+tlbsx to
       the normal bolted TLB miss handler.
    
     - TLB1 has no HES (nor next-victim hint) so we need software round robin
       (TODO: integrate this round robin data with hugetlb/KVM)
    
     - The existing tablewalk handlers map half of a page table at a time,
       because IBM hardware has a fixed 1MiB indirect page size.  e6500
       has variable size indirect entries, with a minimum of 2MiB.
       So we can't do the half-page indirect mapping, and even if we
       could it would be less efficient than mapping the full page.
    
     - Like on e5500, the linear mapping is bolted, so we don't need the
       overhead of supporting nested tlb misses.
    
    Note that hardware tablewalk does not work in rev1 of e6500.
    We do not expect to support e6500 rev1 in mainline Linux.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Cc: Mihai Caraman <mihai.caraman@freescale.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3fa93dc7fe75..94448cd4b444 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -307,6 +307,12 @@ static void __init register_page_bootmem_info(void)
 
 void __init mem_init(void)
 {
+	/*
+	 * book3s is limited to 16 page sizes due to encoding this in
+	 * a 4-bit field for slices.
+	 */
+	BUILD_BUG_ON(MMU_PAGE_COUNT > 16);
+
 #ifdef CONFIG_SWIOTLB
 	swiotlb_init(0);
 #endif

commit f7e3334a6bcb42e7295a9bd9cb36ca4e6e4e66b4
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Fri Sep 27 10:18:09 2013 -0500

    powerpc: Fix memory hotplug with sparse vmemmap
    
    Previous commit 46723bfa540... introduced a new config option
    HAVE_BOOTMEM_INFO_NODE that ended up breaking memory hot-remove for ppc
    when sparse vmemmap is not defined.
    
    This patch defines HAVE_BOOTMEM_INFO_NODE for ppc and adds the call to
    register_page_bootmem_info_node. Without this we get a BUG_ON for memory
    hot remove in put_page_bootmem().
    
    This also adds a stub for register_page_bootmem_memmap to allow ppc to build
    with sparse vmemmap defined. Leaving this as a stub is fine since the same
    vmemmap addresses are also handled in vmemmap_populate and as such are
    properly mapped.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    CC: <stable@vger.kernel.org> [v3.9+]

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 1cf9c5b67f24..3fa93dc7fe75 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -297,12 +297,21 @@ void __init paging_init(void)
 }
 #endif /* ! CONFIG_NEED_MULTIPLE_NODES */
 
+static void __init register_page_bootmem_info(void)
+{
+	int i;
+
+	for_each_online_node(i)
+		register_page_bootmem_info_node(NODE_DATA(i));
+}
+
 void __init mem_init(void)
 {
 #ifdef CONFIG_SWIOTLB
 	swiotlb_init(0);
 #endif
 
+	register_page_bootmem_info();
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 	set_max_mapnr(max_pfn);
 	free_all_bootmem();

commit f748edafac858404be6c8dd67160c2032d97ddf5
Author: Nathan Fontenot <nfont@linux.vnet.ibm.com>
Date:   Mon Aug 19 21:52:20 2013 -0500

    powerpc/mm: Mark Memory Resources as busy
    
    Memory I/O resources need to be marked as busy or else we cannot remove
    them when doing memory hot remove.
    
    Signed-off-by: Nathan Fontenot <nfont@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 7f4bea162026..1cf9c5b67f24 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -514,7 +514,7 @@ static int add_system_ram_resources(void)
 			res->name = "System RAM";
 			res->start = base;
 			res->end = base + size - 1;
-			res->flags = IORESOURCE_MEM;
+			res->flags = IORESOURCE_MEM | IORESOURCE_BUSY;
 			WARN_ON(request_resource(&iomem_resource, res) < 0);
 		}
 	}

commit 65b97fb7303050fc826e518cf67fc283da23314f
Merge: ddcf6600b133 1d8b368ab4aa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 4 10:29:23 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc updates from Ben Herrenschmidt:
     "This is the powerpc changes for the 3.11 merge window.  In addition to
      the usual bug fixes and small updates, the main highlights are:
    
       - Support for transparent huge pages by Aneesh Kumar for 64-bit
         server processors.  This allows the use of 16M pages as transparent
         huge pages on kernels compiled with a 64K base page size.
    
       - Base VFIO support for KVM on power by Alexey Kardashevskiy
    
       - Wiring up of our nvram to the pstore infrastructure, including
         putting compressed oopses in there by Aruna Balakrishnaiah
    
       - Move, rework and improve our "EEH" (basically PCI error handling
         and recovery) infrastructure.  It is no longer specific to pseries
         but is now usable by the new "powernv" platform as well (no
         hypervisor) by Gavin Shan.
    
       - I fixed some bugs in our math-emu instruction decoding and made it
         usable to emulate some optional FP instructions on processors with
         hard FP that lack them (such as fsqrt on Freescale embedded
         processors).
    
       - Support for Power8 "Event Based Branch" facility by Michael
         Ellerman.  This facility allows what is basically "userspace
         interrupts" for performance monitor events.
    
       - A bunch of Transactional Memory vs.  Signals bug fixes and HW
         breakpoint/watchpoint fixes by Michael Neuling.
    
      And more ...  I appologize in advance if I've failed to highlight
      something that somebody deemed worth it."
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (156 commits)
      pstore: Add hsize argument in write_buf call of pstore_ftrace_call
      powerpc/fsl: add MPIC timer wakeup support
      powerpc/mpic: create mpic subsystem object
      powerpc/mpic: add global timer support
      powerpc/mpic: add irq_set_wake support
      powerpc/85xx: enable coreint for all the 64bit boards
      powerpc/8xx: Erroneous double irq_eoi() on CPM IRQ in MPC8xx
      powerpc/fsl: Enable CONFIG_E1000E in mpc85xx_smp_defconfig
      powerpc/mpic: Add get_version API both for internal and external use
      powerpc: Handle both new style and old style reserve maps
      powerpc/hw_brk: Fix off by one error when validating DAWR region end
      powerpc/pseries: Support compression of oops text via pstore
      powerpc/pseries: Re-organise the oops compression code
      pstore: Pass header size in the pstore write callback
      powerpc/powernv: Fix iommu initialization again
      powerpc/pseries: Inform the hypervisor we are using EBB regs
      powerpc/perf: Add power8 EBB support
      powerpc/perf: Core EBB support for 64-bit book3s
      powerpc/perf: Drop MMCRA from thread_struct
      powerpc/perf: Don't enable if we have zero events
      ...

commit 602ddc70ecf710e0cf512862473eac52524ec081
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:04:32 2013 -0700

    mm/PPC: prepare for killing free_all_bootmem_node()
    
    Prepare for killing free_all_bootmem_node() by using free_all_bootmem().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: "Suzuki K. Poulose" <suzuki@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 49c18b60bbb4..1cb1ea133a2c 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -304,22 +304,8 @@ void __init mem_init(void)
 #endif
 
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
-
-#ifdef CONFIG_NEED_MULTIPLE_NODES
-	{
-		pg_data_t *pgdat;
-
-		for_each_online_pgdat(pgdat)
-			if (pgdat->node_spanned_pages != 0) {
-				printk("freeing bootmem node %d\n",
-					pgdat->node_id);
-				free_all_bootmem_node(pgdat);
-			}
-	}
-#else
-	max_mapnr = max_pfn;
+	set_max_mapnr(max_pfn);
 	free_all_bootmem();
-#endif
 
 #ifdef CONFIG_HIGHMEM
 	{

commit 369a9d8523dc7317eccb64b7aee6e9641d84cc8b
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:04:09 2013 -0700

    mm/ppc: prepare for removing num_physpages and simplify mem_init()
    
    Prepare for removing num_physpages and simplify mem_init().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3bcfc0d0d322..49c18b60bbb4 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -299,46 +299,27 @@ void __init paging_init(void)
 
 void __init mem_init(void)
 {
-#ifdef CONFIG_NEED_MULTIPLE_NODES
-	int nid;
-#endif
-	pg_data_t *pgdat;
-	unsigned long i;
-	struct page *page;
-	unsigned long reservedpages = 0, codesize, initsize, datasize, bsssize;
-
 #ifdef CONFIG_SWIOTLB
 	swiotlb_init(0);
 #endif
 
-	num_physpages = memblock_phys_mem_size() >> PAGE_SHIFT;
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 
 #ifdef CONFIG_NEED_MULTIPLE_NODES
-        for_each_online_node(nid) {
-		if (NODE_DATA(nid)->node_spanned_pages != 0) {
-			printk("freeing bootmem node %d\n", nid);
-			free_all_bootmem_node(NODE_DATA(nid));
-		}
+	{
+		pg_data_t *pgdat;
+
+		for_each_online_pgdat(pgdat)
+			if (pgdat->node_spanned_pages != 0) {
+				printk("freeing bootmem node %d\n",
+					pgdat->node_id);
+				free_all_bootmem_node(pgdat);
+			}
 	}
 #else
 	max_mapnr = max_pfn;
 	free_all_bootmem();
 #endif
-	for_each_online_pgdat(pgdat) {
-		for (i = 0; i < pgdat->node_spanned_pages; i++) {
-			if (!pfn_valid(pgdat->node_start_pfn + i))
-				continue;
-			page = pgdat_page_nr(pgdat, i);
-			if (PageReserved(page))
-				reservedpages++;
-		}
-	}
-
-	codesize = (unsigned long)&_sdata - (unsigned long)&_stext;
-	datasize = (unsigned long)&_edata - (unsigned long)&_sdata;
-	initsize = (unsigned long)&__init_end - (unsigned long)&__init_begin;
-	bsssize = (unsigned long)&__bss_stop - (unsigned long)&__bss_start;
 
 #ifdef CONFIG_HIGHMEM
 	{
@@ -348,13 +329,9 @@ void __init mem_init(void)
 		for (pfn = highmem_mapnr; pfn < max_mapnr; ++pfn) {
 			phys_addr_t paddr = (phys_addr_t)pfn << PAGE_SHIFT;
 			struct page *page = pfn_to_page(pfn);
-			if (memblock_is_reserved(paddr))
-				continue;
-			free_highmem_page(page);
-			reservedpages--;
+			if (!memblock_is_reserved(paddr))
+				free_highmem_page(page);
 		}
-		printk(KERN_DEBUG "High memory: %luk\n",
-		       totalhigh_pages << (PAGE_SHIFT-10));
 	}
 #endif /* CONFIG_HIGHMEM */
 
@@ -367,16 +344,7 @@ void __init mem_init(void)
 		(mfspr(SPRN_TLB1CFG) & TLBnCFG_N_ENTRY) - 1;
 #endif
 
-	printk(KERN_INFO "Memory: %luk/%luk available (%luk kernel code, "
-	       "%luk reserved, %luk data, %luk bss, %luk init)\n",
-		nr_free_pages() << (PAGE_SHIFT-10),
-		num_physpages << (PAGE_SHIFT-10),
-		codesize >> 10,
-		reservedpages << (PAGE_SHIFT-10),
-		datasize >> 10,
-		bsssize >> 10,
-		initsize >> 10);
-
+	mem_init_print_info(NULL);
 #ifdef CONFIG_PPC32
 	pr_info("Kernel virtual memory layout:\n");
 	pr_info("  * 0x%08lx..0x%08lx  : fixmap\n", FIXADDR_START, FIXADDR_TOP);

commit 0c988534737a358fdff42fcce78f0ff1a12dbfc5
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:03:24 2013 -0700

    mm: concentrate modification of totalram_pages into the mm core
    
    Concentrate code to modify totalram_pages into the mm core, so the arch
    memory initialized code doesn't need to take care of it.  With these
    changes applied, only following functions from mm core modify global
    variable totalram_pages: free_bootmem_late(), free_all_bootmem(),
    free_all_bootmem_node(), adjust_managed_page_count().
    
    With this patch applied, it will be much more easier for us to keep
    totalram_pages and zone->managed_pages in consistence.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 7f47a05f55af..3bcfc0d0d322 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -318,13 +318,12 @@ void __init mem_init(void)
         for_each_online_node(nid) {
 		if (NODE_DATA(nid)->node_spanned_pages != 0) {
 			printk("freeing bootmem node %d\n", nid);
-			totalram_pages +=
-				free_all_bootmem_node(NODE_DATA(nid));
+			free_all_bootmem_node(NODE_DATA(nid));
 		}
 	}
 #else
 	max_mapnr = max_pfn;
-	totalram_pages += free_all_bootmem();
+	free_all_bootmem();
 #endif
 	for_each_online_pgdat(pgdat) {
 		for (i = 0; i < pgdat->node_spanned_pages; i++) {

commit dbe67df4ba78c79db547c7864e1120981c144c97
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:02:51 2013 -0700

    mm: enhance free_reserved_area() to support poisoning memory with zero
    
    Address more review comments from last round of code review.
    1) Enhance free_reserved_area() to support poisoning freed memory with
       pattern '0'. This could be used to get rid of poison_init_mem()
       on ARM64.
    2) A previous patch has disabled memory poison for initmem on s390
       by mistake, so restore to the original behavior.
    3) Remove redundant PAGE_ALIGN() when calling free_reserved_area().
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 347c5b1bbd62..7f47a05f55af 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -407,7 +407,7 @@ void free_initmem(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
-	free_reserved_area((void *)start, (void *)end, 0, "initrd");
+	free_reserved_area((void *)start, (void *)end, -1, "initrd");
 }
 #endif
 

commit 11199692d83dd3fe1511203024fb9853d176ec4c
Author: Jiang Liu <liuj97@gmail.com>
Date:   Wed Jul 3 15:02:48 2013 -0700

    mm: change signature of free_reserved_area() to fix building warnings
    
    Change signature of free_reserved_area() according to Russell King's
    suggestion to fix following build warnings:
    
      arch/arm/mm/init.c: In function 'mem_init':
      arch/arm/mm/init.c:603:2: warning: passing argument 1 of 'free_reserved_area' makes integer from pointer without a cast [enabled by default]
        free_reserved_area(__va(PHYS_PFN_OFFSET), swapper_pg_dir, 0, NULL);
        ^
      In file included from include/linux/mman.h:4:0,
                       from arch/arm/mm/init.c:15:
      include/linux/mm.h:1301:22: note: expected 'long unsigned int' but argument is of type 'void *'
       extern unsigned long free_reserved_area(unsigned long start, unsigned long end,
    
       mm/page_alloc.c: In function 'free_reserved_area':
    >> mm/page_alloc.c:5134:3: warning: passing argument 1 of 'virt_to_phys' makes pointer from integer without a cast [enabled by default]
       In file included from arch/mips/include/asm/page.h:49:0,
                        from include/linux/mmzone.h:20,
                        from include/linux/gfp.h:4,
                        from include/linux/mm.h:8,
                        from mm/page_alloc.c:18:
       arch/mips/include/asm/io.h:119:29: note: expected 'const volatile void *' but argument is of type 'long unsigned int'
       mm/page_alloc.c: In function 'free_area_init_nodes':
       mm/page_alloc.c:5030:34: warning: array subscript is below array bounds [-Warray-bounds]
    
    Also address some minor code review comments.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Michael S. Tsirkin" <mst@redhat.com>
    Cc: <sworddragon2@aol.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Yinghai Lu <yinghai@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0988a26e0413..347c5b1bbd62 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -407,7 +407,7 @@ void free_initmem(void)
 #ifdef CONFIG_BLK_DEV_INITRD
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
-	free_reserved_area(start, end, 0, "initrd");
+	free_reserved_area((void *)start, (void *)end, 0, "initrd");
 }
 #endif
 

commit 0ac52dd7666d5c0d0147d73a8e4b1d1ffd81cdf3
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Thu Jun 20 14:30:22 2013 +0530

    powerpc: Make linux pagetable walk safe with THP enabled
    
    We need to have irqs disabled to handle all the possible parallel update for
    linux page table without holding locks.
    
    Events that we are intersted in while walking page tables are
    1) Page fault
    2) umap
    3) THP split
    4) THP collapse
    
    A) local_irq_disabled:
    ------------------------
    1) page fault:
    A none to valid transition via page fault is not an issue because we
    would either see a none or valid. If it is none, we would error out
    the page table walk. We may need to use on stack values when checking for
    type of page table elements, because if we do
    
    if (!is_hugepd()) {
        if (!pmd_none() {
           if (pmd_bad() {
    
    We could take that bad condition because the pmd got converted to a hugepd
    after the !is_hugepd check via a hugetlb fault.
    
    The right way would be to check for pmd_none higher up or use on stack value.
    
    2) A valid to none conversion via unmap:
    We can safely walk the upper level table, because we don't remove the the
    page table entries until rcu grace period. So even if we followed a
    wrong pointer we still have the pointer valid till the grace period.
    
    A PTE pointer returned need to be atomically checked for _PAGE_PRESENT and
     _PAGE_BUSY. A valid pointer returned could becoming none later. To prevent
    pte_clear we take _PAGE_BUSY.
    
    3) THP split:
    A valid transparent hugepage is converted to nomal page. Before we split we
    do pmd_splitting_flush, which sets the hugepage PTE to _PAGE_SPLITTING
    So when walking page table we need to check for pmd_trans_splitting and
    handle that. The pte returned should also need to be checked for
    _PAGE_SPLITTING before setting _PAGE_BUSY similar to _PAGE_PRESENT. We save
    the value of PTE on stack and check for the flag in the local pte value.
    If we don't have the value set we can safely operate on the local pte value
    and we atomicaly set _PAGE_BUSY.
    
    4) THP collapse:
    A normal page gets converted to hugepage. In the collapse path, we
    mark the pmd none early (pmdp_clear_flush). With irq disabled, if we
    are aleady walking page table we would see the pmd_none and won't continue.
    If we see a valid PMD, we should still check for _PAGE_PRESENT before
    setting _PAGE_BUSY, to make sure we didn't collapse the PTE to a Huge PTE.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0988a26e0413..ccd49f9503a9 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -508,6 +508,10 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		      pte_t *ptep)
 {
 #ifdef CONFIG_PPC_STD_MMU
+	/*
+	 * We don't need to worry about _PAGE_PRESENT here because we are
+	 * called with either mm->page_table_lock held or ptl lock held
+	 */
 	unsigned long access = 0, trap;
 
 	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */

commit 5a148af66932c31814e263366094b5812210b501
Merge: 99c6bcf46d22 54d5999d98f2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 2 10:16:16 2013 -0700

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    Pull powerpc update from Benjamin Herrenschmidt:
     "The main highlights this time around are:
    
       - A pile of addition POWER8 bits and nits, such as updated
         performance counter support (Michael Ellerman), new branch history
         buffer support (Anshuman Khandual), base support for the new PCI
         host bridge when not using the hypervisor (Gavin Shan) and other
         random related bits and fixes from various contributors.
    
       - Some rework of our page table format by Aneesh Kumar which fixes a
         thing or two and paves the way for THP support.  THP itself will
         not make it this time around however.
    
       - More Freescale updates, including Altivec support on the new e6500
         cores, new PCI controller support, and a pile of new boards support
         and updates.
    
       - The usual batch of trivial cleanups & fixes"
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (156 commits)
      powerpc: Fix build error for book3e
      powerpc: Context switch the new EBB SPRs
      powerpc: Turn on the EBB H/FSCR bits
      powerpc: Replace CPU_FTR_BCTAR with CPU_FTR_ARCH_207S
      powerpc: Setup BHRB instructions facility in HFSCR for POWER8
      powerpc: Fix interrupt range check on debug exception
      powerpc: Update tlbie/tlbiel as per ISA doc
      powerpc: Print page size info during boot
      powerpc: print both base and actual page size on hash failure
      powerpc: Fix hpte_decode to use the correct decoding for page sizes
      powerpc: Decode the pte-lp-encoding bits correctly.
      powerpc: Use encode avpn where we need only avpn values
      powerpc: Reduce PTE table memory wastage
      powerpc: Move the pte free routines from common header
      powerpc: Reduce the PTE_INDEX_SIZE
      powerpc: Switch 16GB and 16MB explicit hugepages to a different page table format
      powerpc: New hugepage directory format
      powerpc: Don't truncate pgd_index wrongly
      powerpc: Don't hard code the size of pte page
      powerpc: Save DAR and DSISR in pt_regs on MCE
      ...

commit 7db6f78cfe683cb8ce508d6cecdfc6638ac35a3f
Author: Jiang Liu <liuj97@gmail.com>
Date:   Mon Apr 29 15:07:08 2013 -0700

    mm/PPC: use free_highmem_page() to free highmem pages into buddy system
    
    Use helper function free_highmem_page() to free highmem pages into
    the buddy system.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: "Suzuki K. Poulose" <suzuki@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index c756713aaaba..cd76c454942f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -352,13 +352,9 @@ void __init mem_init(void)
 			struct page *page = pfn_to_page(pfn);
 			if (memblock_is_reserved(paddr))
 				continue;
-			ClearPageReserved(page);
-			init_page_count(page);
-			__free_page(page);
-			totalhigh_pages++;
+			free_highmem_page(page);
 			reservedpages--;
 		}
-		totalram_pages += totalhigh_pages;
 		printk(KERN_DEBUG "High memory: %luk\n",
 		       totalhigh_pages << (PAGE_SHIFT-10));
 	}

commit 5d585e5c4840381112011aa257c7a94b2b66f9de
Author: Jiang Liu <liuj97@gmail.com>
Date:   Mon Apr 29 15:06:47 2013 -0700

    mm/ppc: use common help functions to free reserved pages
    
    Use common help functions to free reserved pages.
    
    Signed-off-by: Jiang Liu <jiang.liu@huawei.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Anatolij Gustschin <agust@denx.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f1f7409a4183..c756713aaaba 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -405,39 +405,14 @@ void __init mem_init(void)
 
 void free_initmem(void)
 {
-	unsigned long addr;
-
 	ppc_md.progress = ppc_printk_progress;
-
-	addr = (unsigned long)__init_begin;
-	for (; addr < (unsigned long)__init_end; addr += PAGE_SIZE) {
-		memset((void *)addr, POISON_FREE_INITMEM, PAGE_SIZE);
-		ClearPageReserved(virt_to_page(addr));
-		init_page_count(virt_to_page(addr));
-		free_page(addr);
-		totalram_pages++;
-	}
-	pr_info("Freeing unused kernel memory: %luk freed\n",
-		((unsigned long)__init_end -
-		(unsigned long)__init_begin) >> 10);
+	free_initmem_default(POISON_FREE_INITMEM);
 }
 
 #ifdef CONFIG_BLK_DEV_INITRD
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {
-	if (start >= end)
-		return;
-
-	start = _ALIGN_DOWN(start, PAGE_SIZE);
-	end = _ALIGN_UP(end, PAGE_SIZE);
-	pr_info("Freeing initrd memory: %ldk freed\n", (end - start) >> 10);
-
-	for (; start < end; start += PAGE_SIZE) {
-		ClearPageReserved(virt_to_page(start));
-		init_page_count(virt_to_page(start));
-		free_page(start);
-		totalram_pages++;
-	}
+	free_reserved_area(start, end, 0, "initrd");
 }
 #endif
 

commit 8040bda31ab229e931d872a51c89ddeae4012b00
Author: Valentina Manea <valentina.manea.m@gmail.com>
Date:   Sun Mar 10 03:22:39 2013 +0000

    powerpc: place EXPORT_SYMBOL macro right after declaration
    
    This fixes the following checkpatch.pl warnings:
    
    WARNING: EXPORT_SYMBOL(foo); should immediately follow its function/variable
    +EXPORT_SYMBOL(kmap_prot);
    
    WARNING: EXPORT_SYMBOL(foo); should immediately follow its function/variable
    +EXPORT_SYMBOL(kmap_pte);
    
    Signed-off-by: Valentina Manea <valentina.manea.m@gmail.com>
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f1f7409a4183..056732e8ca38 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -66,10 +66,9 @@ unsigned long long memory_limit;
 
 #ifdef CONFIG_HIGHMEM
 pte_t *kmap_pte;
+EXPORT_SYMBOL(kmap_pte);
 pgprot_t kmap_prot;
-
 EXPORT_SYMBOL(kmap_prot);
-EXPORT_SYMBOL(kmap_pte);
 
 static inline pte_t *virt_to_kpte(unsigned long vaddr)
 {

commit 5ce1a70e2f00f0bce0cab57f798ca354b9496169
Merge: 9d3cae26acb4 ef53d16cded7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 23 17:50:35 2013 -0800

    Merge branch 'akpm' (more incoming from Andrew)
    
    Merge second patch-bomb from Andrew Morton:
    
     - A little DM fix
    
     - the MM queue
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (154 commits)
      ksm: allocate roots when needed
      mm: cleanup "swapcache" in do_swap_page
      mm,ksm: swapoff might need to copy
      mm,ksm: FOLL_MIGRATION do migration_entry_wait
      ksm: shrink 32-bit rmap_item back to 32 bytes
      ksm: treat unstable nid like in stable tree
      ksm: add some comments
      tmpfs: fix mempolicy object leaks
      tmpfs: fix use-after-free of mempolicy object
      mm/fadvise.c: drain all pagevecs if POSIX_FADV_DONTNEED fails to discard all pages
      mm: export mmu notifier invalidates
      mm: accelerate mm_populate() treatment of THP pages
      mm: use long type for page counts in mm_populate() and get_user_pages()
      mm: accurately document nr_free_*_pages functions with code comments
      HWPOISON: change order of error_states[]'s elements
      HWPOISON: fix misjudgement of page_action() for errors on mlocked pages
      memcg: stop warning on memcg_propagate_kmem
      net: change type of virtio_chan->p9_max_pages
      vmscan: change type of vm_total_pages to unsigned long
      fs/nfsd: change type of max_delegations, nfsd_drc_max_mem and nfsd_drc_mem_used
      ...

commit 24d335ca3606b610ec69c66a1e42760c96d89470
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Fri Feb 22 16:32:58 2013 -0800

    memory-hotplug: introduce new arch_remove_memory() for removing page table
    
    For removing memory, we need to remove page tables.  But it depends on
    architecture.  So the patch introduce arch_remove_memory() for removing
    page table.  Now it only calls __remove_pages().
    
    Note: __remove_pages() for some archtecuture is not implemented
          (I don't know how to implement it for s390).
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Jiang Liu <jiang.liu@huawei.com>
    Cc: Jianguo Wu <wujianguo@huawei.com>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Wu Jianguo <wujianguo@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0dba5066c22a..09c64518b4d0 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -133,6 +133,18 @@ int arch_add_memory(int nid, u64 start, u64 size)
 
 	return __add_pages(nid, zone, start_pfn, nr_pages);
 }
+
+#ifdef CONFIG_MEMORY_HOTREMOVE
+int arch_remove_memory(u64 start, u64 size)
+{
+	unsigned long start_pfn = start >> PAGE_SHIFT;
+	unsigned long nr_pages = size >> PAGE_SHIFT;
+	struct zone *zone;
+
+	zone = page_zone(pfn_to_page(start_pfn));
+	return __remove_pages(zone, start_pfn, nr_pages);
+}
+#endif
 #endif /* CONFIG_MEMORY_HOTPLUG */
 
 /*

commit 4e8309baedfef8360f5b06ccaff8c45cff73cc4f
Author: Cody P Schafer <cody@linux.vnet.ibm.com>
Date:   Wed Jan 9 12:40:18 2013 +0000

    powerpc/mm: Eliminate unneeded for_each_memblock
    
    The only persistent change made by this loop is calling
    memblock_set_node() once for each memblock, which is not useful (and has
    no effect) as memblock_set_node() is not called with any
    memblock-specific parameters.
    
    Subsistute a single memblock_set_node().
    
    Signed-off-by: Cody P Schafer <cody@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0dba5066c22a..40df7c8f2096 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -195,13 +195,10 @@ void __init do_init_bootmem(void)
 	min_low_pfn = MEMORY_START >> PAGE_SHIFT;
 	boot_mapsize = init_bootmem_node(NODE_DATA(0), start >> PAGE_SHIFT, min_low_pfn, max_low_pfn);
 
-	/* Add active regions with valid PFNs */
-	for_each_memblock(memory, reg) {
-		unsigned long start_pfn, end_pfn;
-		start_pfn = memblock_region_memory_base_pfn(reg);
-		end_pfn = memblock_region_memory_end_pfn(reg);
-		memblock_set_node(0, (phys_addr_t)ULLONG_MAX, 0);
-	}
+	/* Place all memblock_regions in the same node and merge contiguous
+	 * memblock_regions
+	 */
+	memblock_set_node(0, (phys_addr_t)ULLONG_MAX, 0);
 
 	/* Add all physical memory to the bootmem map, mark each area
 	 * present.

commit 688ba1dbee8a49a2efe507cd9ae69634d92bb640
Author: Jia Hongtao <B38951@freescale.com>
Date:   Fri Aug 3 18:14:10 2012 +0800

    powerpc/swiotlb: Enable at early stage and disable if not necessary
    
    Remove the dependency on PCI initialization for SWIOTLB initialization.
    So that PCI can be initialized at proper time.
    
    SWIOTLB is partly determined by PCI inbound/outbound map which is assigned
    in PCI initialization. But swiotlb_init() should be done at the stage of
    mem_init() which is much earlier than PCI initialization. So we reserve the
    memory for SWIOTLB first and free it if not necessary.
    
    All boards are converted to fit this change.
    
    Signed-off-by: Jia Hongtao <B38951@freescale.com>
    Signed-off-by: Li Yang <leoli@freescale.com>
    Acked-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 44cf2b20503d..0dba5066c22a 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -300,8 +300,7 @@ void __init mem_init(void)
 	unsigned long reservedpages = 0, codesize, initsize, datasize, bsssize;
 
 #ifdef CONFIG_SWIOTLB
-	if (ppc_swiotlb_enable)
-		swiotlb_init(1);
+	swiotlb_init(0);
 #endif
 
 	num_physpages = memblock_phys_mem_size() >> PAGE_SHIFT;

commit a84fcd46870113e92523e1ebb9a0ec75f66e03a2
Author: Suzuki Poulose <suzuki@in.ibm.com>
Date:   Tue Aug 21 01:42:33 2012 +0000

    powerpc: Change memory_limit from phys_addr_t to unsigned long long
    
    There are some device-tree nodes, whose values are of type phys_addr_t.
    The phys_addr_t is variable sized based on the CONFIG_PHSY_T_64BIT.
    
    Change these to a fixed unsigned long long for consistency.
    
    This patch does the change only for memory_limit.
    
    The following is a list of such variables which need the change:
    
     1) kernel_end, crashk_size - in arch/powerpc/kernel/machine_kexec.c
    
     2) (struct resource *)crashk_res.start - We could export a local static
        variable from machine_kexec.c.
    
    Changing the above values might break the kexec-tools. So, I will
    fix kexec-tools first to handle the different sized values and then change
     the above.
    
    Suggested-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Suzuki K. Poulose <suzuki@in.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index fbdad0e3929a..44cf2b20503d 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -62,7 +62,7 @@
 
 int init_bootmem_done;
 int mem_init_done;
-phys_addr_t memory_limit;
+unsigned long long memory_limit;
 
 #ifdef CONFIG_HIGHMEM
 pte_t *kmap_pte;

commit 249ba1ee0f8fcb4e40caa5fbea11dafde201cc46
Author: Alexander Graf <agraf@suse.de>
Date:   Fri Aug 3 13:56:33 2012 +0200

    KVM: PPC: Add cache flush on page map
    
    When we map a page that wasn't icache cleared before, do so when first
    mapping it in KVM using the same information bits as the Linux mapping
    logic. That way we are 100% sure that any page we map does not have stale
    entries in the icache.
    
    Signed-off-by: Alexander Graf <agraf@suse.de>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index baaafde7d135..fbdad0e3929a 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -469,6 +469,7 @@ void flush_dcache_icache_page(struct page *page)
 	__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
 #endif
 }
+EXPORT_SYMBOL(flush_dcache_icache_page);
 
 void clear_user_page(void *page, unsigned long vaddr, struct page *pg)
 {

commit 2480b2089210de3353b43419a80d9fa298d76f0e
Author: Cong Wang <amwang@redhat.com>
Date:   Fri Nov 25 23:14:16 2011 +0800

    powerpc: remove the second argument of k[un]map_atomic()
    
    Signed-off-by: Cong Wang <amwang@redhat.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index d974b79a3068..baaafde7d135 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -458,9 +458,9 @@ void flush_dcache_icache_page(struct page *page)
 #endif
 #ifdef CONFIG_BOOKE
 	{
-		void *start = kmap_atomic(page, KM_PPC_SYNC_ICACHE);
+		void *start = kmap_atomic(page);
 		__flush_dcache_icache(start);
-		kunmap_atomic(start, KM_PPC_SYNC_ICACHE);
+		kunmap_atomic(start);
 	}
 #elif defined(CONFIG_8xx) || defined(CONFIG_PPC64)
 	/* On 8xx there is no need to kmap since highmem is not supported */

commit e4e88f31bcb5f05f24b9ae518d4ecb44e1a7774d
Merge: 9753dfe19a85 ef88e3911c0e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jan 6 17:58:22 2012 -0800

    Merge branch 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc
    
    * 'next' of git://git.kernel.org/pub/scm/linux/kernel/git/benh/powerpc: (185 commits)
      powerpc: fix compile error with 85xx/p1010rdb.c
      powerpc: fix compile error with 85xx/p1023_rds.c
      powerpc/fsl: add MSI support for the Freescale hypervisor
      arch/powerpc/sysdev/fsl_rmu.c: introduce missing kfree
      powerpc/fsl: Add support for Integrated Flash Controller
      powerpc/fsl: update compatiable on fsl 16550 uart nodes
      powerpc/85xx: fix PCI and localbus properties in p1022ds.dts
      powerpc/85xx: re-enable ePAPR byte channel driver in corenet32_smp_defconfig
      powerpc/fsl: Update defconfigs to enable some standard FSL HW features
      powerpc: Add TBI PHY node to first MDIO bus
      sbc834x: put full compat string in board match check
      powerpc/fsl-pci: Allow 64-bit PCIe devices to DMA to any memory address
      powerpc: Fix unpaired probe_hcall_entry and probe_hcall_exit
      offb: Fix setting of the pseudo-palette for >8bpp
      offb: Add palette hack for qemu "standard vga" framebuffer
      offb: Fix bug in calculating requested vram size
      powerpc/boot: Change the WARN to INFO for boot wrapper overlap message
      powerpc/44x: Fix build error on currituck platform
      powerpc/boot: Change the load address for the wrapper to fit the kernel
      powerpc/44x: Enable CRASH_DUMP for 440x
      ...
    
    Fix up a trivial conflict in arch/powerpc/include/asm/cputime.h due to
    the additional sparse-checking code for cputime_t.

commit 1d7cfe18ec2eb2d0480a9b29465af66b61291202
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Dec 8 10:22:08 2011 -0800

    powerpc: Use HAVE_MEMBLOCK_NODE_MAP
    
    powerpc doesn't access early_node_map[] directly and enabling
    HAVE_MEMBLOCK_NODE_MAP is trivial - replacing add_active_range() calls
    with memblock_set_node() and selecting HAVE_MEMBLOCK_NODE_MAP is
    enough.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Yinghai Lu <yinghai@kernel.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2dd6bdd31fe1..8e2eb6611b0b 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -199,7 +199,7 @@ void __init do_init_bootmem(void)
 		unsigned long start_pfn, end_pfn;
 		start_pfn = memblock_region_memory_base_pfn(reg);
 		end_pfn = memblock_region_memory_end_pfn(reg);
-		add_active_range(0, start_pfn, end_pfn);
+		memblock_set_node(0, (phys_addr_t)ULLONG_MAX, 0);
 	}
 
 	/* Add all physical memory to the bootmem map, mark each area

commit 8a3e3d31d13ece3003e8f750178a7db02c5cd123
Author: sukadev@linux.vnet.ibm.com <sukadev@linux.vnet.ibm.com>
Date:   Fri Dec 2 12:26:23 2011 +0000

    powerpc: Punch a hole in /dev/mem for librtas
    
    With CONFIG_STRICT_DEVMEM=y, user space cannot read any part of /dev/mem.
    Since this breaks librtas, punch a hole in /dev/mem to allow access to the
    rmo_buffer that librtas needs.
    
    Anton Blanchard reported the problem and helped with the fix.
    
    A quick test for this patch:
    
           # cat /proc/rtas/rmo_buffer
           000000000f190000 10000
    
           # python -c "print 0x000000000f190000 / 0x10000"
           3865
    
           # dd if=/dev/mem of=/tmp/foo count=1 bs=64k skip=3865
           1+0 records in
           1+0 records out
           65536 bytes (66 kB) copied, 0.000205235 s, 319 MB/s
    
           # dd if=/dev/mem of=/tmp/foo
           dd: reading `/dev/mem': Operation not permitted
           0+0 records in
           0+0 records out
           0 bytes (0 B) copied, 0.00022519 s, 0.0 kB/s
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 83d819f30864..395f42d5f785 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -51,6 +51,7 @@
 #include <asm/vdso.h>
 #include <asm/fixmap.h>
 #include <asm/swiotlb.h>
+#include <asm/rtas.h>
 
 #include "mmu_decl.h"
 
@@ -600,6 +601,8 @@ int devmem_is_allowed(unsigned long pfn)
 		return 0;
 	if (!page_is_ram(pfn))
 		return 1;
+	if (page_is_rtas_user_buf(pfn))
+		return 1;
 	return 0;
 }
 #endif /* CONFIG_STRICT_DEVMEM */

commit d93e4d7d72037d8c9405e8d404ecb2ee162adc25
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Mon Nov 28 14:43:33 2011 +0000

    powerpc/book3e: Change hugetlb preload to take vma argument
    
    This avoids an extra find_vma() and is less error-prone.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 22563b9664c3..83d819f30864 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -553,7 +553,7 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 #if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
 	&& defined(CONFIG_HUGETLB_PAGE)
 	if (is_vm_hugetlb_page(vma))
-		book3e_hugetlb_preload(vma->vm_mm, address, *ptep);
+		book3e_hugetlb_preload(vma, address, *ptep);
 #endif
 }
 

commit 1d54cf2b973a6265789b382b7d305771321b9b57
Author: sukadev@linux.vnet.ibm.com <sukadev@linux.vnet.ibm.com>
Date:   Tue Aug 30 09:19:17 2011 +0000

    powerpc: Implement CONFIG_STRICT_DEVMEM
    
    As described in the help text in the patch, this token restricts general
    access to /dev/mem as a way of increasing the security. Specifically, access
    to exclusive IOMEM and kernel RAM is denied unless CONFIG_STRICT_DEVMEM is
    set to 'n'.
    
    Implement the 'devmem_is_allowed()' interface for Powerpc. It will be
    called from range_is_allowed() when userpsace attempts to access /dev/mem.
    
    This patch is based on an earlier patch from Steve Best and with input from
    Paul Mackerras and Scott Wood.
    
    [BenH] Fixed a typo or two and removed the generic change which should
           be submitted as a separate patch
    
    Signed-off-by: Sukadev Bhattiprolu <sukadev@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2dd6bdd31fe1..22563b9664c3 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -585,3 +585,21 @@ static int add_system_ram_resources(void)
 	return 0;
 }
 subsys_initcall(add_system_ram_resources);
+
+#ifdef CONFIG_STRICT_DEVMEM
+/*
+ * devmem_is_allowed(): check to see if /dev/mem access to a certain address
+ * is valid. The argument is a physical page number.
+ *
+ * Access has to be given to non-kernel-ram areas as well, these contain the
+ * PCI mmio resources as well as potential bios/acpi data regions.
+ */
+int devmem_is_allowed(unsigned long pfn)
+{
+	if (iomem_is_exclusive(pfn << PAGE_SHIFT))
+		return 0;
+	if (!page_is_ram(pfn))
+		return 1;
+	return 0;
+}
+#endif /* CONFIG_STRICT_DEVMEM */

commit c40dd2f76644016ca7677545fc846ec2470d70a1
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Nov 2 14:56:12 2011 +0000

    powerpc: Add System RAM to /proc/iomem
    
    We've resisted adding System RAM to /proc/iomem because it is
    the wrong place for it. Unfortunately we continue to find tools
    that rely on this behaviour so give up and add it in.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 16da595ff402..2dd6bdd31fe1 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -34,6 +34,7 @@
 #include <linux/suspend.h>
 #include <linux/memblock.h>
 #include <linux/hugetlb.h>
+#include <linux/slab.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -555,3 +556,32 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		book3e_hugetlb_preload(vma->vm_mm, address, *ptep);
 #endif
 }
+
+/*
+ * System memory should not be in /proc/iomem but various tools expect it
+ * (eg kdump).
+ */
+static int add_system_ram_resources(void)
+{
+	struct memblock_region *reg;
+
+	for_each_memblock(memory, reg) {
+		struct resource *res;
+		unsigned long base = reg->base;
+		unsigned long size = reg->size;
+
+		res = kzalloc(sizeof(struct resource), GFP_KERNEL);
+		WARN_ON(!res);
+
+		if (res) {
+			res->name = "System RAM";
+			res->start = base;
+			res->end = base + size - 1;
+			res->flags = IORESOURCE_MEM;
+			WARN_ON(request_resource(&iomem_resource, res) < 0);
+		}
+	}
+
+	return 0;
+}
+subsys_initcall(add_system_ram_resources);

commit 32aaeffbd4a7457bf2f7448b33b5946ff2a960eb
Merge: 208bca086040 67b84999b1a8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 19:44:47 2011 -0800

    Merge branch 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux
    
    * 'modsplit-Oct31_2011' of git://git.kernel.org/pub/scm/linux/kernel/git/paulg/linux: (230 commits)
      Revert "tracing: Include module.h in define_trace.h"
      irq: don't put module.h into irq.h for tracking irqgen modules.
      bluetooth: macroize two small inlines to avoid module.h
      ip_vs.h: fix implicit use of module_get/module_put from module.h
      nf_conntrack.h: fix up fallout from implicit moduleparam.h presence
      include: replace linux/module.h with "struct module" wherever possible
      include: convert various register fcns to macros to avoid include chaining
      crypto.h: remove unused crypto_tfm_alg_modname() inline
      uwb.h: fix implicit use of asm/page.h for PAGE_SIZE
      pm_runtime.h: explicitly requires notifier.h
      linux/dmaengine.h: fix implicit use of bitmap.h and asm/page.h
      miscdevice.h: fix up implicit use of lists and types
      stop_machine.h: fix implicit use of smp.h for smp_processor_id
      of: fix implicit use of errno.h in include/linux/of.h
      of_platform.h: delete needless include <linux/module.h>
      acpi: remove module.h include from platform/aclinux.h
      miscdevice.h: delete unnecessary inclusion of module.h
      device_cgroup.h: delete needless include <linux/module.h>
      net: sch_generic remove redundant use of <linux/module.h>
      net: inet_timewait_sock doesnt need <linux/module.h>
      ...
    
    Fix up trivial conflicts (other header files, and  removal of the ab3550 mfd driver) in
     - drivers/media/dvb/frontends/dibx000_common.c
     - drivers/media/video/{mt9m111.c,ov6650.c}
     - drivers/mfd/ab3550-core.c
     - include/linux/dmaengine.h

commit 4b16f8e2d6d64249f0ed3ca7fe2a319d0dde2719
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Jul 22 18:24:23 2011 -0400

    powerpc: various straight conversions from module.h --> export.h
    
    All these files were including module.h just for the basic
    EXPORT_SYMBOL infrastructure.  We can shift them off to the
    export.h header which is a way smaller footprint and thus
    realize some compile time gains.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index c781bbcf7338..507963ee9aa4 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -17,7 +17,7 @@
  *
  */
 
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>
 #include <linux/errno.h>

commit a11940978bd598e65996b4f807cf4904793f7025
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 10 20:44:24 2011 +0000

    powerpc: Fix oops when echoing bad values to /sys/devices/system/memory/probe
    
    If we echo an address the hypervisor doesn't like to
    /sys/devices/system/memory/probe we oops the box:
    
    # echo 0x10000000000 > /sys/devices/system/memory/probe
    
    kernel BUG at arch/powerpc/mm/hash_utils_64.c:541!
    
    The backtrace is:
    
    create_section_mapping
    arch_add_memory
    add_memory
    memory_probe_store
    sysdev_class_store
    sysfs_write_file
    vfs_write
    SyS_write
    
    In create_section_mapping we BUG if htab_bolt_mapping returned
    an error. A better approach is to return an error which will
    propagate back to userspace.
    
    Rerunning the test with this patch applied:
    
    # echo 0x10000000000 > /sys/devices/system/memory/probe
    -bash: echo: write error: Invalid argument
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: stable@kernel.org
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index ad9cf49dfb89..5db316cad47b 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -123,7 +123,8 @@ int arch_add_memory(int nid, u64 start, u64 size)
 	pgdata = NODE_DATA(nid);
 
 	start = (unsigned long)__va(start);
-	create_section_mapping(start, start + size);
+	if (create_section_mapping(start, start + size))
+		return -EINVAL;
 
 	/* this should work for most non-highmem platforms */
 	zone = pgdata->node_zones;

commit 41151e77a4d96ea138cede6d84c955aa4769ce74
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 09:54:48 2011 +0000

    powerpc: Hugetlb for BookE
    
    Enable hugepages on Freescale BookE processors.  This allows the kernel to
    use huge TLB entries to map pages, which can greatly reduce the number of
    TLB misses and the amount of TLB thrashing experienced by applications with
    large memory footprints.  Care should be taken when using this on FSL
    processors, as the number of large TLB entries supported by the core is low
    (16-64) on current processors.
    
    The supported set of hugepage sizes include 4m, 16m, 64m, 256m, and 1g.
    Page sizes larger than the max zone size are called "gigantic" pages and
    must be allocated on the command line (and cannot be deallocated).
    
    This is currently only fully implemented for Freescale 32-bit BookE
    processors, but there is some infrastructure in the code for
    64-bit BooKE.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index c781bbcf7338..ad9cf49dfb89 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -548,4 +548,9 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		return;
 	hash_preload(vma->vm_mm, address, access, trap);
 #endif /* CONFIG_PPC_STD_MMU */
+#if (defined(CONFIG_PPC_BOOK3E_64) || defined(CONFIG_PPC_FSL_BOOK3E)) \
+	&& defined(CONFIG_HUGETLB_PAGE)
+	if (is_vm_hugetlb_page(vma))
+		book3e_hugetlb_preload(vma->vm_mm, address, *ptep);
+#endif
 }

commit f7ba2991e9063fa8cf668ee57c08e2842ad04f11
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Mon Jul 4 18:44:19 2011 +0000

    powerpc/mm: Fix output of total_ram.
    
    On 32bit platforms that support >= 4GB memory total_ram was truncated.
    This creates a confusing printk:
            Top of RAM: 0x100000000, Total RAM: 0x0
    Fix that:
            Top of RAM: 0x100000000, Total RAM: 0x100000000
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Acked-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 457dc84154ba..c781bbcf7338 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -249,7 +249,7 @@ static int __init mark_nonram_nosave(void)
  */
 void __init paging_init(void)
 {
-	unsigned long total_ram = memblock_phys_mem_size();
+	unsigned long long total_ram = memblock_phys_mem_size();
 	phys_addr_t top_of_ram = memblock_end_of_DRAM();
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
 
@@ -269,7 +269,7 @@ void __init paging_init(void)
 	kmap_prot = PAGE_KERNEL;
 #endif /* CONFIG_HIGHMEM */
 
-	printk(KERN_DEBUG "Top of RAM: 0x%llx, Total RAM: 0x%lx\n",
+	printk(KERN_DEBUG "Top of RAM: 0x%llx, Total RAM: 0x%llx\n",
 	       (unsigned long long)top_of_ram, total_ram);
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (long int)((top_of_ram - total_ram) >> 20));

commit 3160b09796129abc9523ea3cd1633b0faba64a02
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 14:54:47 2011 -0500

    powerpc: Create next_tlbcam_idx percpu variable for FSL_BOOKE
    
    This is used to round-robin TLBCAM entries.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index a3841bb75cbb..457dc84154ba 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -353,6 +353,15 @@ void __init mem_init(void)
 	}
 #endif /* CONFIG_HIGHMEM */
 
+#if defined(CONFIG_PPC_FSL_BOOK3E) && !defined(CONFIG_SMP)
+	/*
+	 * If smp is enabled, next_tlbcam_idx is initialized in the cpu up
+	 * functions.... do it here for the non-smp case.
+	 */
+	per_cpu(next_tlbcam_idx, smp_processor_id()) =
+		(mfspr(SPRN_TLB1CFG) & TLBnCFG_N_ENTRY) - 1;
+#endif
+
 	printk(KERN_INFO "Memory: %luk/%luk available (%luk kernel code, "
 	       "%luk reserved, %luk data, %luk bss, %luk init)\n",
 		nr_free_pages() << (PAGE_SHIFT-10),

commit a9c0f41b3a64955fd6f4e9d66ae1df1cbdee0cd0
Author: Dave Carroll <dcarroll@astekcorp.com>
Date:   Sat Jun 18 07:36:40 2011 +0000

    powerpc: Add printk companion for ppc_md.progress
    
    This patch adds a printk companion to replace the udbg progress function
    when initmem is freed.
    
    Suggested-by: Milton Miller <miltonm@bga.com>
    Suggested-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Dave Carroll <dcarroll@astekcorp.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e9379ce6f3be..a3841bb75cbb 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -387,7 +387,7 @@ void free_initmem(void)
 {
 	unsigned long addr;
 
-	ppc_md.progress = NULL;
+	ppc_md.progress = ppc_printk_progress;
 
 	addr = (unsigned long)__init_begin;
 	for (; addr < (unsigned long)__init_end; addr += PAGE_SIZE) {

commit 2773fcc8c48b947c997ff345f3e453917883cdb5
Author: Dave Carroll <dcarroll@astekcorp.com>
Date:   Sat Jun 18 07:36:39 2011 +0000

    powerpc: Move free_initmem to common code
    
    The free_initmem function is basically duplicated in mm/init_32,
    and init_64, and is moved to the common 32/64-bit mm/mem.c.
    
    All other sections except init were removed in v2.6.15 by
    6c45ab992e4299c869fb26427944a8f8ea177024 (powerpc: Remove section
    free() and linker script bits), and therefore the bulk of the executed
    code is identical.
    
    This patch also removes updating ppc_md.progress to NULL in the powermac
    late_initcall.
    
    Suggested-by: Milton Miller <miltonm@bga.com>
    Suggested-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Dave Carroll <dcarroll@astekcorp.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 89cbfef5a7dd..e9379ce6f3be 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -383,6 +383,25 @@ void __init mem_init(void)
 	mem_init_done = 1;
 }
 
+void free_initmem(void)
+{
+	unsigned long addr;
+
+	ppc_md.progress = NULL;
+
+	addr = (unsigned long)__init_begin;
+	for (; addr < (unsigned long)__init_end; addr += PAGE_SIZE) {
+		memset((void *)addr, POISON_FREE_INITMEM, PAGE_SIZE);
+		ClearPageReserved(virt_to_page(addr));
+		init_page_count(virt_to_page(addr));
+		free_page(addr);
+		totalram_pages++;
+	}
+	pr_info("Freeing unused kernel memory: %luk freed\n",
+		((unsigned long)__init_end -
+		(unsigned long)__init_begin) >> 10);
+}
+
 #ifdef CONFIG_BLK_DEV_INITRD
 void __init free_initrd_mem(unsigned long start, unsigned long end)
 {

commit 6da49a292534d31ca391928ea97f8225774dc1e0
Merge: 17bdc6c0e979 ca56a95eedcc
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jun 30 15:23:59 2011 +1000

    Merge remote branch 'origin/master' into next

commit 3d41e0f6d91cedcab08dff2f5e148b20c87a7432
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Tue Jun 28 09:54:46 2011 +0000

    powerpc: mem_init should call memblock_is_reserved with phys_addr_t
    
    This has been broken for a while but hasn't been an issue until
    now because nobody was reserving regions at high addresses.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 57e545b84bf1..097b288779e2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -337,8 +337,9 @@ void __init mem_init(void)
 
 		highmem_mapnr = lowmem_end_addr >> PAGE_SHIFT;
 		for (pfn = highmem_mapnr; pfn < max_mapnr; ++pfn) {
+			phys_addr_t paddr = (phys_addr_t)pfn << PAGE_SHIFT;
 			struct page *page = pfn_to_page(pfn);
-			if (memblock_is_reserved(pfn << PAGE_SHIFT))
+			if (memblock_is_reserved(paddr))
 				continue;
 			ClearPageReserved(page);
 			init_page_count(page);

commit 307cfe715344e15eda12dad3bb14f794115ca823
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jun 9 16:52:38 2011 +1000

    powerpc: Force page alignment for initrd reserved memory
    
    When using 64K pages with a separate cpio rootfs, U-Boot will align
    the rootfs on a 4K page boundary. When the memory is reserved, and
    subsequent early memblock_alloc is called, it will allocate memory
    between the 64K page alignment and reserved memory. When the reserved
    memory is subsequently freed, it is done so by pages, causing the
    early memblock_alloc requests to be re-used, which in my case, caused
    the device-tree to be clobbered.
    
    This patch forces the reserved memory for initrd to be kernel page
    aligned, and will move the device tree if it overlaps with the range
    extension of initrd. This patch will also consolidate the identical
    function free_initrd_mem() from mm/init_32.c, init_64.c to mm/mem.c,
    and adds the same range extension when freeing initrd. free_initrd_mem()
    is also moved to the __init section.
    
    Many thanks to Milton Miller for his input on this patch.
    
    [BenH: Fixed build without CONFIG_BLK_DEV_INITRD]
    
    Signed-off-by: Dave Carroll <dcarroll@astekcorp.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 57e545b84bf1..29d4dde65c45 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -382,6 +382,25 @@ void __init mem_init(void)
 	mem_init_done = 1;
 }
 
+#ifdef CONFIG_BLK_DEV_INITRD
+void __init free_initrd_mem(unsigned long start, unsigned long end)
+{
+	if (start >= end)
+		return;
+
+	start = _ALIGN_DOWN(start, PAGE_SIZE);
+	end = _ALIGN_UP(end, PAGE_SIZE);
+	pr_info("Freeing initrd memory: %ldk freed\n", (end - start) >> 10);
+
+	for (; start < end; start += PAGE_SIZE) {
+		ClearPageReserved(virt_to_page(start));
+		init_page_count(virt_to_page(start));
+		free_page(start);
+		totalram_pages++;
+	}
+}
+#endif
+
 /*
  * This is called when a page has been modified by the kernel.
  * It just marks the page as not i-cache clean.  We do the i-cache

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index a66499650909..57e545b84bf1 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -424,7 +424,7 @@ void clear_user_page(void *page, unsigned long vaddr, struct page *pg)
 	clear_page(page);
 
 	/*
-	 * We shouldnt have to do this, but some versions of glibc
+	 * We shouldn't have to do this, but some versions of glibc
 	 * require it (ld.so assumes zero filled pages are icache clean)
 	 * - Anton
 	 */

commit c7fc2de0c83dbd2eaf759c5cd0e2b9cf1eb4df3a
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Tue Oct 12 14:07:09 2010 -0700

    memblock, bootmem: Round pfn properly for memory and reserved regions
    
    We need to round memory regions correctly -- specifically, we need to
    round reserved region in the more expansive direction (lower limit
    down, upper limit up) whereas usable memory regions need to be rounded
    in the more restrictive direction (lower limit up, upper limit down).
    
    This introduces two set of inlines:
    
            memblock_region_memory_base_pfn()
            memblock_region_memory_end_pfn()
            memblock_region_reserved_base_pfn()
            memblock_region_reserved_end_pfn()
    
    Although they are antisymmetric (and therefore are technically
    duplicates) the use of the different inlines explicitly documents the
    programmer's intention.
    
    The lack of proper rounding caused a bug on ARM, which was then found
    to also affect other architectures.
    
    Reported-by: Russell King <rmk@arm.linux.org.uk>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    LKML-Reference: <4CB4CDFD.4020105@kernel.org>
    Cc: Jeremy Fitzhardinge <jeremy@goop.org>
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f661f6c527da..a66499650909 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -148,8 +148,8 @@ walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 	int ret = -1;
 
 	for_each_memblock(memory, reg) {
-		tstart = max(start_pfn, memblock_region_base_pfn(reg));
-		tend = min(end_pfn, memblock_region_end_pfn(reg));
+		tstart = max(start_pfn, memblock_region_memory_base_pfn(reg));
+		tend = min(end_pfn, memblock_region_memory_end_pfn(reg));
 		if (tstart >= tend)
 			continue;
 		ret = (*func)(tstart, tend - tstart, arg);
@@ -195,8 +195,8 @@ void __init do_init_bootmem(void)
 	/* Add active regions with valid PFNs */
 	for_each_memblock(memory, reg) {
 		unsigned long start_pfn, end_pfn;
-		start_pfn = memblock_region_base_pfn(reg);
-		end_pfn = memblock_region_end_pfn(reg);
+		start_pfn = memblock_region_memory_base_pfn(reg);
+		end_pfn = memblock_region_memory_end_pfn(reg);
 		add_active_range(0, start_pfn, end_pfn);
 	}
 
@@ -236,9 +236,9 @@ static int __init mark_nonram_nosave(void)
 
 	for_each_memblock(memory, reg) {
 		if (prev &&
-		    memblock_region_end_pfn(prev) < memblock_region_base_pfn(reg))
-			register_nosave_region(memblock_region_end_pfn(prev),
-					       memblock_region_base_pfn(reg));
+		    memblock_region_memory_end_pfn(prev) < memblock_region_memory_base_pfn(reg))
+			register_nosave_region(memblock_region_memory_end_pfn(prev),
+					       memblock_region_memory_base_pfn(reg));
 		prev = reg;
 	}
 	return 0;

commit 4734b594c6ca1be796d30c82d93fdf5160f45124
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Jul 28 14:31:29 2010 +1000

    memblock: Remove memblock_type.size and add memblock.memory_size instead
    
    Right now, both the "memory" and "reserved" memblock_type structures have
    a "size" member. It represents the calculated memory size in the former
    case and is unused in the latter.
    
    This moves it out to the main memblock structure instead
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 52df5428ece4..f661f6c527da 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -301,7 +301,7 @@ void __init mem_init(void)
 		swiotlb_init(1);
 #endif
 
-	num_physpages = memblock.memory.size >> PAGE_SHIFT;
+	num_physpages = memblock_phys_mem_size() >> PAGE_SHIFT;
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 
 #ifdef CONFIG_NEED_MULTIPLE_NODES

commit 28be7072ce54b82642ebff6a80d474d4c6a6a7fd
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 4 13:43:53 2010 +1000

    memblock/powerpc: Use new accessors
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index a33f5c186fb7..52df5428ece4 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -82,18 +82,11 @@ int page_is_ram(unsigned long pfn)
 	return pfn < max_pfn;
 #else
 	unsigned long paddr = (pfn << PAGE_SHIFT);
-	int i;
-	for (i=0; i < memblock.memory.cnt; i++) {
-		unsigned long base;
+	struct memblock_region *reg;
 
-		base = memblock.memory.regions[i].base;
-
-		if ((paddr >= base) &&
-			(paddr < (base + memblock.memory.regions[i].size))) {
+	for_each_memblock(memory, reg)
+		if (paddr >= reg->base && paddr < (reg->base + reg->size))
 			return 1;
-		}
-	}
-
 	return 0;
 #endif
 }
@@ -149,23 +142,19 @@ int
 walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 		void *arg, int (*func)(unsigned long, unsigned long, void *))
 {
-	struct memblock_region res;
-	unsigned long pfn, len;
-	u64 end;
+	struct memblock_region *reg;
+	unsigned long end_pfn = start_pfn + nr_pages;
+	unsigned long tstart, tend;
 	int ret = -1;
 
-	res.base = (u64) start_pfn << PAGE_SHIFT;
-	res.size = (u64) nr_pages << PAGE_SHIFT;
-
-	end = res.base + res.size - 1;
-	while ((res.base < end) && (memblock_find(&res) >= 0)) {
-		pfn = (unsigned long)(res.base >> PAGE_SHIFT);
-		len = (unsigned long)(res.size >> PAGE_SHIFT);
-		ret = (*func)(pfn, len, arg);
+	for_each_memblock(memory, reg) {
+		tstart = max(start_pfn, memblock_region_base_pfn(reg));
+		tend = min(end_pfn, memblock_region_end_pfn(reg));
+		if (tstart >= tend)
+			continue;
+		ret = (*func)(tstart, tend - tstart, arg);
 		if (ret)
 			break;
-		res.base += (res.size + 1);
-		res.size = (end - res.base + 1);
 	}
 	return ret;
 }
@@ -179,9 +168,9 @@ EXPORT_SYMBOL_GPL(walk_system_ram_range);
 #ifndef CONFIG_NEED_MULTIPLE_NODES
 void __init do_init_bootmem(void)
 {
-	unsigned long i;
 	unsigned long start, bootmap_pages;
 	unsigned long total_pages;
+	struct memblock_region *reg;
 	int boot_mapsize;
 
 	max_low_pfn = max_pfn = memblock_end_of_DRAM() >> PAGE_SHIFT;
@@ -204,10 +193,10 @@ void __init do_init_bootmem(void)
 	boot_mapsize = init_bootmem_node(NODE_DATA(0), start >> PAGE_SHIFT, min_low_pfn, max_low_pfn);
 
 	/* Add active regions with valid PFNs */
-	for (i = 0; i < memblock.memory.cnt; i++) {
+	for_each_memblock(memory, reg) {
 		unsigned long start_pfn, end_pfn;
-		start_pfn = memblock.memory.regions[i].base >> PAGE_SHIFT;
-		end_pfn = start_pfn + memblock_size_pages(&memblock.memory, i);
+		start_pfn = memblock_region_base_pfn(reg);
+		end_pfn = memblock_region_end_pfn(reg);
 		add_active_range(0, start_pfn, end_pfn);
 	}
 
@@ -218,29 +207,21 @@ void __init do_init_bootmem(void)
 	free_bootmem_with_active_regions(0, lowmem_end_addr >> PAGE_SHIFT);
 
 	/* reserve the sections we're already using */
-	for (i = 0; i < memblock.reserved.cnt; i++) {
-		unsigned long addr = memblock.reserved.regions[i].base +
-				     memblock_size_bytes(&memblock.reserved, i) - 1;
-		if (addr < lowmem_end_addr)
-			reserve_bootmem(memblock.reserved.regions[i].base,
-					memblock_size_bytes(&memblock.reserved, i),
-					BOOTMEM_DEFAULT);
-		else if (memblock.reserved.regions[i].base < lowmem_end_addr) {
-			unsigned long adjusted_size = lowmem_end_addr -
-				      memblock.reserved.regions[i].base;
-			reserve_bootmem(memblock.reserved.regions[i].base,
-					adjusted_size, BOOTMEM_DEFAULT);
+	for_each_memblock(reserved, reg) {
+		unsigned long top = reg->base + reg->size - 1;
+		if (top < lowmem_end_addr)
+			reserve_bootmem(reg->base, reg->size, BOOTMEM_DEFAULT);
+		else if (reg->base < lowmem_end_addr) {
+			unsigned long trunc_size = lowmem_end_addr - reg->base;
+			reserve_bootmem(reg->base, trunc_size, BOOTMEM_DEFAULT);
 		}
 	}
 #else
 	free_bootmem_with_active_regions(0, max_pfn);
 
 	/* reserve the sections we're already using */
-	for (i = 0; i < memblock.reserved.cnt; i++)
-		reserve_bootmem(memblock.reserved.regions[i].base,
-				memblock_size_bytes(&memblock.reserved, i),
-				BOOTMEM_DEFAULT);
-
+	for_each_memblock(reserved, reg)
+		reserve_bootmem(reg->base, reg->size, BOOTMEM_DEFAULT);
 #endif
 	/* XXX need to clip this if using highmem? */
 	sparse_memory_present_with_active_regions(0);
@@ -251,22 +232,15 @@ void __init do_init_bootmem(void)
 /* mark pages that don't exist as nosave */
 static int __init mark_nonram_nosave(void)
 {
-	unsigned long memblock_next_region_start_pfn,
-		      memblock_region_max_pfn;
-	int i;
-
-	for (i = 0; i < memblock.memory.cnt - 1; i++) {
-		memblock_region_max_pfn =
-			(memblock.memory.regions[i].base >> PAGE_SHIFT) +
-			(memblock.memory.regions[i].size >> PAGE_SHIFT);
-		memblock_next_region_start_pfn =
-			memblock.memory.regions[i+1].base >> PAGE_SHIFT;
-
-		if (memblock_region_max_pfn < memblock_next_region_start_pfn)
-			register_nosave_region(memblock_region_max_pfn,
-					       memblock_next_region_start_pfn);
+	struct memblock_region *reg, *prev = NULL;
+
+	for_each_memblock(memory, reg) {
+		if (prev &&
+		    memblock_region_end_pfn(prev) < memblock_region_base_pfn(reg))
+			register_nosave_region(memblock_region_end_pfn(prev),
+					       memblock_region_base_pfn(reg));
+		prev = reg;
 	}
-
 	return 0;
 }
 

commit e3239ff92a17976ac5d26fa0fe40ef3a9daf2523
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 4 14:06:41 2010 +1000

    memblock: Rename memblock_region to memblock_type and memblock_property to memblock_region
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 1a84a8d00005..a33f5c186fb7 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -86,10 +86,10 @@ int page_is_ram(unsigned long pfn)
 	for (i=0; i < memblock.memory.cnt; i++) {
 		unsigned long base;
 
-		base = memblock.memory.region[i].base;
+		base = memblock.memory.regions[i].base;
 
 		if ((paddr >= base) &&
-			(paddr < (base + memblock.memory.region[i].size))) {
+			(paddr < (base + memblock.memory.regions[i].size))) {
 			return 1;
 		}
 	}
@@ -149,7 +149,7 @@ int
 walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 		void *arg, int (*func)(unsigned long, unsigned long, void *))
 {
-	struct memblock_property res;
+	struct memblock_region res;
 	unsigned long pfn, len;
 	u64 end;
 	int ret = -1;
@@ -206,7 +206,7 @@ void __init do_init_bootmem(void)
 	/* Add active regions with valid PFNs */
 	for (i = 0; i < memblock.memory.cnt; i++) {
 		unsigned long start_pfn, end_pfn;
-		start_pfn = memblock.memory.region[i].base >> PAGE_SHIFT;
+		start_pfn = memblock.memory.regions[i].base >> PAGE_SHIFT;
 		end_pfn = start_pfn + memblock_size_pages(&memblock.memory, i);
 		add_active_range(0, start_pfn, end_pfn);
 	}
@@ -219,16 +219,16 @@ void __init do_init_bootmem(void)
 
 	/* reserve the sections we're already using */
 	for (i = 0; i < memblock.reserved.cnt; i++) {
-		unsigned long addr = memblock.reserved.region[i].base +
+		unsigned long addr = memblock.reserved.regions[i].base +
 				     memblock_size_bytes(&memblock.reserved, i) - 1;
 		if (addr < lowmem_end_addr)
-			reserve_bootmem(memblock.reserved.region[i].base,
+			reserve_bootmem(memblock.reserved.regions[i].base,
 					memblock_size_bytes(&memblock.reserved, i),
 					BOOTMEM_DEFAULT);
-		else if (memblock.reserved.region[i].base < lowmem_end_addr) {
+		else if (memblock.reserved.regions[i].base < lowmem_end_addr) {
 			unsigned long adjusted_size = lowmem_end_addr -
-				      memblock.reserved.region[i].base;
-			reserve_bootmem(memblock.reserved.region[i].base,
+				      memblock.reserved.regions[i].base;
+			reserve_bootmem(memblock.reserved.regions[i].base,
 					adjusted_size, BOOTMEM_DEFAULT);
 		}
 	}
@@ -237,7 +237,7 @@ void __init do_init_bootmem(void)
 
 	/* reserve the sections we're already using */
 	for (i = 0; i < memblock.reserved.cnt; i++)
-		reserve_bootmem(memblock.reserved.region[i].base,
+		reserve_bootmem(memblock.reserved.regions[i].base,
 				memblock_size_bytes(&memblock.reserved, i),
 				BOOTMEM_DEFAULT);
 
@@ -257,10 +257,10 @@ static int __init mark_nonram_nosave(void)
 
 	for (i = 0; i < memblock.memory.cnt - 1; i++) {
 		memblock_region_max_pfn =
-			(memblock.memory.region[i].base >> PAGE_SHIFT) +
-			(memblock.memory.region[i].size >> PAGE_SHIFT);
+			(memblock.memory.regions[i].base >> PAGE_SHIFT) +
+			(memblock.memory.regions[i].size >> PAGE_SHIFT);
 		memblock_next_region_start_pfn =
-			memblock.memory.region[i+1].base >> PAGE_SHIFT;
+			memblock.memory.regions[i+1].base >> PAGE_SHIFT;
 
 		if (memblock_region_max_pfn < memblock_next_region_start_pfn)
 			register_nosave_region(memblock_region_max_pfn,

commit 95f72d1ed41a66f1c1c29c24d479de81a0bea36f
Author: Yinghai Lu <yinghai@kernel.org>
Date:   Mon Jul 12 14:36:09 2010 +1000

    lmb: rename to memblock
    
    via following scripts
    
          FILES=$(find * -type f | grep -vE 'oprofile|[^K]config')
    
          sed -i \
            -e 's/lmb/memblock/g' \
            -e 's/LMB/MEMBLOCK/g' \
            $FILES
    
          for N in $(find . -name lmb.[ch]); do
            M=$(echo $N | sed 's/lmb/memblock/g')
            mv $N $M
          done
    
    and remove some wrong change like lmbench and dlmb etc.
    
    also move memblock.c from lib/ to mm/
    
    Suggested-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: "H. Peter Anvin" <hpa@zytor.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Yinghai Lu <yinghai@kernel.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0f594d774bf7..1a84a8d00005 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -32,7 +32,7 @@
 #include <linux/initrd.h>
 #include <linux/pagemap.h>
 #include <linux/suspend.h>
-#include <linux/lmb.h>
+#include <linux/memblock.h>
 #include <linux/hugetlb.h>
 
 #include <asm/pgalloc.h>
@@ -83,13 +83,13 @@ int page_is_ram(unsigned long pfn)
 #else
 	unsigned long paddr = (pfn << PAGE_SHIFT);
 	int i;
-	for (i=0; i < lmb.memory.cnt; i++) {
+	for (i=0; i < memblock.memory.cnt; i++) {
 		unsigned long base;
 
-		base = lmb.memory.region[i].base;
+		base = memblock.memory.region[i].base;
 
 		if ((paddr >= base) &&
-			(paddr < (base + lmb.memory.region[i].size))) {
+			(paddr < (base + memblock.memory.region[i].size))) {
 			return 1;
 		}
 	}
@@ -142,14 +142,14 @@ int arch_add_memory(int nid, u64 start, u64 size)
 /*
  * walk_memory_resource() needs to make sure there is no holes in a given
  * memory range.  PPC64 does not maintain the memory layout in /proc/iomem.
- * Instead it maintains it in lmb.memory structures.  Walk through the
+ * Instead it maintains it in memblock.memory structures.  Walk through the
  * memory regions, find holes and callback for contiguous regions.
  */
 int
 walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 		void *arg, int (*func)(unsigned long, unsigned long, void *))
 {
-	struct lmb_property res;
+	struct memblock_property res;
 	unsigned long pfn, len;
 	u64 end;
 	int ret = -1;
@@ -158,7 +158,7 @@ walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
 	res.size = (u64) nr_pages << PAGE_SHIFT;
 
 	end = res.base + res.size - 1;
-	while ((res.base < end) && (lmb_find(&res) >= 0)) {
+	while ((res.base < end) && (memblock_find(&res) >= 0)) {
 		pfn = (unsigned long)(res.base >> PAGE_SHIFT);
 		len = (unsigned long)(res.size >> PAGE_SHIFT);
 		ret = (*func)(pfn, len, arg);
@@ -184,8 +184,8 @@ void __init do_init_bootmem(void)
 	unsigned long total_pages;
 	int boot_mapsize;
 
-	max_low_pfn = max_pfn = lmb_end_of_DRAM() >> PAGE_SHIFT;
-	total_pages = (lmb_end_of_DRAM() - memstart_addr) >> PAGE_SHIFT;
+	max_low_pfn = max_pfn = memblock_end_of_DRAM() >> PAGE_SHIFT;
+	total_pages = (memblock_end_of_DRAM() - memstart_addr) >> PAGE_SHIFT;
 #ifdef CONFIG_HIGHMEM
 	total_pages = total_lowmem >> PAGE_SHIFT;
 	max_low_pfn = lowmem_end_addr >> PAGE_SHIFT;
@@ -198,16 +198,16 @@ void __init do_init_bootmem(void)
 	 */
 	bootmap_pages = bootmem_bootmap_pages(total_pages);
 
-	start = lmb_alloc(bootmap_pages << PAGE_SHIFT, PAGE_SIZE);
+	start = memblock_alloc(bootmap_pages << PAGE_SHIFT, PAGE_SIZE);
 
 	min_low_pfn = MEMORY_START >> PAGE_SHIFT;
 	boot_mapsize = init_bootmem_node(NODE_DATA(0), start >> PAGE_SHIFT, min_low_pfn, max_low_pfn);
 
 	/* Add active regions with valid PFNs */
-	for (i = 0; i < lmb.memory.cnt; i++) {
+	for (i = 0; i < memblock.memory.cnt; i++) {
 		unsigned long start_pfn, end_pfn;
-		start_pfn = lmb.memory.region[i].base >> PAGE_SHIFT;
-		end_pfn = start_pfn + lmb_size_pages(&lmb.memory, i);
+		start_pfn = memblock.memory.region[i].base >> PAGE_SHIFT;
+		end_pfn = start_pfn + memblock_size_pages(&memblock.memory, i);
 		add_active_range(0, start_pfn, end_pfn);
 	}
 
@@ -218,17 +218,17 @@ void __init do_init_bootmem(void)
 	free_bootmem_with_active_regions(0, lowmem_end_addr >> PAGE_SHIFT);
 
 	/* reserve the sections we're already using */
-	for (i = 0; i < lmb.reserved.cnt; i++) {
-		unsigned long addr = lmb.reserved.region[i].base +
-				     lmb_size_bytes(&lmb.reserved, i) - 1;
+	for (i = 0; i < memblock.reserved.cnt; i++) {
+		unsigned long addr = memblock.reserved.region[i].base +
+				     memblock_size_bytes(&memblock.reserved, i) - 1;
 		if (addr < lowmem_end_addr)
-			reserve_bootmem(lmb.reserved.region[i].base,
-					lmb_size_bytes(&lmb.reserved, i),
+			reserve_bootmem(memblock.reserved.region[i].base,
+					memblock_size_bytes(&memblock.reserved, i),
 					BOOTMEM_DEFAULT);
-		else if (lmb.reserved.region[i].base < lowmem_end_addr) {
+		else if (memblock.reserved.region[i].base < lowmem_end_addr) {
 			unsigned long adjusted_size = lowmem_end_addr -
-				      lmb.reserved.region[i].base;
-			reserve_bootmem(lmb.reserved.region[i].base,
+				      memblock.reserved.region[i].base;
+			reserve_bootmem(memblock.reserved.region[i].base,
 					adjusted_size, BOOTMEM_DEFAULT);
 		}
 	}
@@ -236,9 +236,9 @@ void __init do_init_bootmem(void)
 	free_bootmem_with_active_regions(0, max_pfn);
 
 	/* reserve the sections we're already using */
-	for (i = 0; i < lmb.reserved.cnt; i++)
-		reserve_bootmem(lmb.reserved.region[i].base,
-				lmb_size_bytes(&lmb.reserved, i),
+	for (i = 0; i < memblock.reserved.cnt; i++)
+		reserve_bootmem(memblock.reserved.region[i].base,
+				memblock_size_bytes(&memblock.reserved, i),
 				BOOTMEM_DEFAULT);
 
 #endif
@@ -251,20 +251,20 @@ void __init do_init_bootmem(void)
 /* mark pages that don't exist as nosave */
 static int __init mark_nonram_nosave(void)
 {
-	unsigned long lmb_next_region_start_pfn,
-		      lmb_region_max_pfn;
+	unsigned long memblock_next_region_start_pfn,
+		      memblock_region_max_pfn;
 	int i;
 
-	for (i = 0; i < lmb.memory.cnt - 1; i++) {
-		lmb_region_max_pfn =
-			(lmb.memory.region[i].base >> PAGE_SHIFT) +
-			(lmb.memory.region[i].size >> PAGE_SHIFT);
-		lmb_next_region_start_pfn =
-			lmb.memory.region[i+1].base >> PAGE_SHIFT;
+	for (i = 0; i < memblock.memory.cnt - 1; i++) {
+		memblock_region_max_pfn =
+			(memblock.memory.region[i].base >> PAGE_SHIFT) +
+			(memblock.memory.region[i].size >> PAGE_SHIFT);
+		memblock_next_region_start_pfn =
+			memblock.memory.region[i+1].base >> PAGE_SHIFT;
 
-		if (lmb_region_max_pfn < lmb_next_region_start_pfn)
-			register_nosave_region(lmb_region_max_pfn,
-					       lmb_next_region_start_pfn);
+		if (memblock_region_max_pfn < memblock_next_region_start_pfn)
+			register_nosave_region(memblock_region_max_pfn,
+					       memblock_next_region_start_pfn);
 	}
 
 	return 0;
@@ -275,8 +275,8 @@ static int __init mark_nonram_nosave(void)
  */
 void __init paging_init(void)
 {
-	unsigned long total_ram = lmb_phys_mem_size();
-	phys_addr_t top_of_ram = lmb_end_of_DRAM();
+	unsigned long total_ram = memblock_phys_mem_size();
+	phys_addr_t top_of_ram = memblock_end_of_DRAM();
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
 
 #ifdef CONFIG_PPC32
@@ -327,7 +327,7 @@ void __init mem_init(void)
 		swiotlb_init(1);
 #endif
 
-	num_physpages = lmb.memory.size >> PAGE_SHIFT;
+	num_physpages = memblock.memory.size >> PAGE_SHIFT;
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 
 #ifdef CONFIG_NEED_MULTIPLE_NODES
@@ -364,7 +364,7 @@ void __init mem_init(void)
 		highmem_mapnr = lowmem_end_addr >> PAGE_SHIFT;
 		for (pfn = highmem_mapnr; pfn < max_mapnr; ++pfn) {
 			struct page *page = pfn_to_page(pfn);
-			if (lmb_is_reserved(pfn << PAGE_SHIFT))
+			if (memblock_is_reserved(pfn << PAGE_SHIFT))
 				continue;
 			ClearPageReserved(page);
 			init_page_count(page);

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 448f972b22f5..0f594d774bf7 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -22,6 +22,7 @@
 #include <linux/kernel.h>
 #include <linux/errno.h>
 #include <linux/string.h>
+#include <linux/gfp.h>
 #include <linux/types.h>
 #include <linux/mm.h>
 #include <linux/stddef.h>

commit a93272969c6b1d59883fcbb04845420bd72c9a20
Author: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
Date:   Tue Mar 16 13:16:25 2010 +0000

    powerpc: Fix swiotlb to respect the boot option
    
    powerpc initializes swiotlb before parsing the kernel boot options so
    swiotlb options (e.g. specifying the swiotlb buffer size) are ignored.
    
    Any time before freeing bootmem works for swiotlb so this patch moves
    powerpc's swiotlb initialization after parsing the kernel boot
    options, mem_init (as x86 does).
    
    Signed-off-by: FUJITA Tomonori <fujita.tomonori@lab.ntt.co.jp>
    Tested-by: Becky Bruce <beckyb@kernel.crashing.org>
    Tested-by: Albert Herranz <albert_herranz@yahoo.es>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 311224cdb7ad..448f972b22f5 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -48,6 +48,7 @@
 #include <asm/sparsemem.h>
 #include <asm/vdso.h>
 #include <asm/fixmap.h>
+#include <asm/swiotlb.h>
 
 #include "mmu_decl.h"
 
@@ -320,6 +321,11 @@ void __init mem_init(void)
 	struct page *page;
 	unsigned long reservedpages = 0, codesize, initsize, datasize, bsssize;
 
+#ifdef CONFIG_SWIOTLB
+	if (ppc_swiotlb_enable)
+		swiotlb_init(1);
+#endif
+
 	num_physpages = lmb.memory.size >> PAGE_SHIFT;
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 

commit 4b3073e1c53a256275f1079c0fbfbe85883d9275
Author: Russell King <rmk+kernel@arm.linux.org.uk>
Date:   Fri Dec 18 16:40:18 2009 +0000

    MM: Pass a PTE pointer to update_mmu_cache() rather than the PTE itself
    
    On VIVT ARM, when we have multiple shared mappings of the same file
    in the same MM, we need to ensure that we have coherency across all
    copies.  We do this via make_coherent() by making the pages
    uncacheable.
    
    This used to work fine, until we allowed highmem with highpte - we
    now have a page table which is mapped as required, and is not available
    for modification via update_mmu_cache().
    
    Ralf Beache suggested getting rid of the PTE value passed to
    update_mmu_cache():
    
      On MIPS update_mmu_cache() calls __update_tlb() which walks pagetables
      to construct a pointer to the pte again.  Passing a pte_t * is much
      more elegant.  Maybe we might even replace the pte argument with the
      pte_t?
    
    Ben Herrenschmidt would also like the pte pointer for PowerPC:
    
      Passing the ptep in there is exactly what I want.  I want that
      -instead- of the PTE value, because I have issue on some ppc cases,
      for I$/D$ coherency, where set_pte_at() may decide to mask out the
      _PAGE_EXEC.
    
    So, pass in the mapped page table pointer into update_mmu_cache(), and
    remove the PTE value, updating all implementations and call sites to
    suit.
    
    Includes a fix from Stephen Rothwell:
    
      sparc: fix fallout from update_mmu_cache API change
    
      Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Russell King <rmk+kernel@arm.linux.org.uk>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index b9b152558f9c..311224cdb7ad 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -494,13 +494,13 @@ EXPORT_SYMBOL(flush_icache_user_range);
  * This must always be called with the pte lock held.
  */
 void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
-		      pte_t pte)
+		      pte_t *ptep)
 {
 #ifdef CONFIG_PPC_STD_MMU
 	unsigned long access = 0, trap;
 
 	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
-	if (!pte_young(pte) || address >= TASK_SIZE)
+	if (!pte_young(*ptep) || address >= TASK_SIZE)
 		return;
 
 	/* We try to figure out if we are coming from an instruction

commit 0895ecda79428df48501e48dd0a868e0c8e1aae2
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Oct 26 19:24:31 2009 +0000

    powerpc/mm: Bring hugepage PTE accessor functions back into sync with normal accessors
    
    The hugepage arch code provides a number of hook functions/macros
    which mirror the functionality of various normal page pte access
    functions.  Various changes in the normal page accessors (in
    particular BenH's recent changes to the handling of lazy icache
    flushing and PAGE_EXEC) have caused the hugepage versions to get out
    of sync with the originals.  In some cases, this is a bug, at least on
    some MMU types.
    
    One of the reasons that some hooks were not identical to the normal
    page versions, is that the fact we're dealing with a hugepage needed
    to be passed down do use the correct dcache-icache flush function.
    This patch makes the main flush_dcache_icache_page() function hugepage
    aware (by checking for the PageCompound flag).  That in turn means we
    can make set_huge_pte_at() just a call to set_pte_at() bringing it
    back into sync.  As a bonus, this lets us remove the
    hash_huge_page_do_lazy_icache() function, replacing it with a call to
    the hash_page_do_lazy_icache() function it was based on.
    
    Some other hugepage pte access hooks - huge_ptep_get_and_clear() and
    huge_ptep_clear_flush() - are not so easily unified, but this patch at
    least brings them back into sync with the current versions of the
    corresponding normal page functions.
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 59736317bf0e..b9b152558f9c 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -32,6 +32,7 @@
 #include <linux/pagemap.h>
 #include <linux/suspend.h>
 #include <linux/lmb.h>
+#include <linux/hugetlb.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -417,18 +418,26 @@ EXPORT_SYMBOL(flush_dcache_page);
 
 void flush_dcache_icache_page(struct page *page)
 {
+#ifdef CONFIG_HUGETLB_PAGE
+	if (PageCompound(page)) {
+		flush_dcache_icache_hugepage(page);
+		return;
+	}
+#endif
 #ifdef CONFIG_BOOKE
-	void *start = kmap_atomic(page, KM_PPC_SYNC_ICACHE);
-	__flush_dcache_icache(start);
-	kunmap_atomic(start, KM_PPC_SYNC_ICACHE);
+	{
+		void *start = kmap_atomic(page, KM_PPC_SYNC_ICACHE);
+		__flush_dcache_icache(start);
+		kunmap_atomic(start, KM_PPC_SYNC_ICACHE);
+	}
 #elif defined(CONFIG_8xx) || defined(CONFIG_PPC64)
 	/* On 8xx there is no need to kmap since highmem is not supported */
 	__flush_dcache_icache(page_address(page)); 
 #else
 	__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
 #endif
-
 }
+
 void clear_user_page(void *page, unsigned long vaddr, struct page *pg)
 {
 	clear_page(page);

commit 908eedc6168bd92e89f90d89fa389065a36358fa
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Sep 22 16:45:46 2009 -0700

    walk system ram range
    
    Originally, walk_memory_resource() was introduced to traverse all memory
    of "System RAM" for detecting memory hotplug/unplug range.  For doing so,
    flags of IORESOUCE_MEM|IORESOURCE_BUSY was used and this was enough for
    memory hotplug.
    
    But for using other purpose, /proc/kcore, this may includes some firmware
    area marked as IORESOURCE_BUSY | IORESOUCE_MEM.  This patch makes the
    check strict to find out busy "System RAM".
    
    Note: PPC64 keeps their own walk_memory_resouce(), which walk through
    ppc64's lmb informaton.  Because old kclist_add() is called per lmb, this
    patch makes no difference in behavior, finally.
    
    And this patch removes CONFIG_MEMORY_HOTPLUG check from this function.
    Because pfn_valid() just show "there is memmap or not* and cannot be used
    for "there is physical memory or not", this function is useful in generic
    to scan physical memory range.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: AmÃ©rico Wang <xiyou.wangcong@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Roland Dreier <rolandd@cisco.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0e5c59b995ef..59736317bf0e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -143,8 +143,8 @@ int arch_add_memory(int nid, u64 start, u64 size)
  * memory regions, find holes and callback for contiguous regions.
  */
 int
-walk_memory_resource(unsigned long start_pfn, unsigned long nr_pages, void *arg,
-			int (*func)(unsigned long, unsigned long, void *))
+walk_system_ram_range(unsigned long start_pfn, unsigned long nr_pages,
+		void *arg, int (*func)(unsigned long, unsigned long, void *))
 {
 	struct lmb_property res;
 	unsigned long pfn, len;
@@ -166,7 +166,7 @@ walk_memory_resource(unsigned long start_pfn, unsigned long nr_pages, void *arg,
 	}
 	return ret;
 }
-EXPORT_SYMBOL_GPL(walk_memory_resource);
+EXPORT_SYMBOL_GPL(walk_system_ram_range);
 
 /*
  * Initialize the bootmem system and give it all the memory we

commit cc013a88906bad9d2832d6316de1c7dbc1c2a794
Author: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
Date:   Mon Sep 21 17:02:36 2009 -0700

    arches: drop superfluous casts in nr_free_pages() callers
    
    Commit 96177299416dbccb73b54e6b344260154a445375 ("Drop free_pages()")
    modified nr_free_pages() to return 'unsigned long' instead of 'unsigned
    int'.  This made the casts to 'unsigned long' in most callers superfluous,
    so remove them.
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Russell King <rmk+kernel@arm.linux.org.uk>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Kyle McMartin <kyle@mcmartin.ca>
    Acked-by: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Haavard Skinnemoen <hskinnemoen@atmel.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: David Howells <dhowells@redhat.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Chris Zankel <zankel@tensilica.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 579382c163a9..0e5c59b995ef 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -372,7 +372,7 @@ void __init mem_init(void)
 
 	printk(KERN_INFO "Memory: %luk/%luk available (%luk kernel code, "
 	       "%luk reserved, %luk data, %luk bss, %luk init)\n",
-		(unsigned long)nr_free_pages() << (PAGE_SHIFT-10),
+		nr_free_pages() << (PAGE_SHIFT-10),
 		num_physpages << (PAGE_SHIFT-10),
 		codesize >> 10,
 		reservedpages << (PAGE_SHIFT-10),

commit 8b31e49d1d75729c1da9009664ba52abd1adc628
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed May 27 13:50:33 2009 +1000

    powerpc: Fix up dma_alloc_coherent() on platforms without cache coherency.
    
    The implementation we just revived has issues, such as using a
    Kconfig-defined virtual address area in kernel space that nothing
    actually carves out (and thus will overlap whatever is there),
    or having some dependencies on being self contained in a single
    PTE page which adds unnecessary constraints on the kernel virtual
    address space.
    
    This fixes it by using more classic PTE accessors and automatically
    locating the area for consistent memory, carving an appropriate hole
    in the kernel virtual address space, leaving only the size of that
    area as a Kconfig option. It also brings some dma-mask related fixes
    from the ARM implementation which was almost identical initially but
    grew its own fixes.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index d3a4e67561fa..579382c163a9 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -387,6 +387,10 @@ void __init mem_init(void)
 	pr_info("  * 0x%08lx..0x%08lx  : highmem PTEs\n",
 		PKMAP_BASE, PKMAP_ADDR(LAST_PKMAP));
 #endif /* CONFIG_HIGHMEM */
+#ifdef CONFIG_NOT_COHERENT_CACHE
+	pr_info("  * 0x%08lx..0x%08lx  : consistent mem\n",
+		IOREMAP_TOP, IOREMAP_TOP + CONFIG_CONSISTENT_SIZE);
+#endif /* CONFIG_NOT_COHERENT_CACHE */
 	pr_info("  * 0x%08lx..0x%08lx  : early ioremap\n",
 		ioremap_bot, IOREMAP_TOP);
 	pr_info("  * 0x%08lx..0x%08lx  : vmalloc & ioremap\n",

commit f637a49e507c88354ab32b5d914e06acfb7ee00d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed May 27 13:44:50 2009 +1000

    powerpc: Minor cleanups of kernel virt address space definitions
    
    Make FIXADDR_TOP a compile time constant and cleanup a
    couple of definitions relative to the layout of the kernel
    address space on ppc32. We also print out that layout at
    boot time for debugging purposes.
    
    This is a pre-requisite for properly fixing non-coherent
    DMA allocactions.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index d0602a76bf7f..d3a4e67561fa 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -380,6 +380,19 @@ void __init mem_init(void)
 		bsssize >> 10,
 		initsize >> 10);
 
+#ifdef CONFIG_PPC32
+	pr_info("Kernel virtual memory layout:\n");
+	pr_info("  * 0x%08lx..0x%08lx  : fixmap\n", FIXADDR_START, FIXADDR_TOP);
+#ifdef CONFIG_HIGHMEM
+	pr_info("  * 0x%08lx..0x%08lx  : highmem PTEs\n",
+		PKMAP_BASE, PKMAP_ADDR(LAST_PKMAP));
+#endif /* CONFIG_HIGHMEM */
+	pr_info("  * 0x%08lx..0x%08lx  : early ioremap\n",
+		ioremap_bot, IOREMAP_TOP);
+	pr_info("  * 0x%08lx..0x%08lx  : vmalloc & ioremap\n",
+		VMALLOC_START, VMALLOC_END);
+#endif /* CONFIG_PPC32 */
+
 	mem_init_done = 1;
 }
 

commit 49a849652513235a244dfbf5e58c54f796bd1148
Author: Becky Bruce <beckyb@kernel.crashing.org>
Date:   Fri May 8 12:19:27 2009 +0000

    powerpc: Allow mem=x cmdline to work with 4G+
    
    We're currently choking on mem=4g (and above) due to memory_limit
    being specified as an unsigned long. Make memory_limit
    phys_addr_t to fix this.
    
    Signed-off-by: Becky Bruce <beckyb@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f668fa9ba804..d0602a76bf7f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -57,7 +57,7 @@
 
 int init_bootmem_done;
 int mem_init_done;
-unsigned long memory_limit;
+phys_addr_t memory_limit;
 
 #ifdef CONFIG_HIGHMEM
 pte_t *kmap_pte;

commit 8d30c14cab30d405a05f2aaceda1e9ad57800f36
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Feb 10 16:02:37 2009 +0000

    powerpc/mm: Rework I$/D$ coherency (v3)
    
    This patch reworks the way we do I and D cache coherency on PowerPC.
    
    The "old" way was split in 3 different parts depending on the processor type:
    
       - Hash with per-page exec support (64-bit and >= POWER4 only) does it
    at hashing time, by preventing exec on unclean pages and cleaning pages
    on exec faults.
    
       - Everything without per-page exec support (32-bit hash, 8xx, and
    64-bit < POWER4) does it for all page going to user space in update_mmu_cache().
    
       - Embedded with per-page exec support does it from do_page_fault() on
    exec faults, in a way similar to what the hash code does.
    
    That leads to confusion, and bugs. For example, the method using update_mmu_cache()
    is racy on SMP where another processor can see the new PTE and hash it in before
    we have cleaned the cache, and then blow trying to execute. This is hard to hit but
    I think it has bitten us in the past.
    
    Also, it's inefficient for embedded where we always end up having to do at least
    one more page fault.
    
    This reworks the whole thing by moving the cache sync into two main call sites,
    though we keep different behaviours depending on the HW capability. The call
    sites are set_pte_at() which is now made out of line, and ptep_set_access_flags()
    which joins the former in pgtable.c
    
    The base idea for Embedded with per-page exec support, is that we now do the
    flush at set_pte_at() time when coming from an exec fault, which allows us
    to avoid the double fault problem completely (we can even improve the situation
    more by implementing TLB preload in update_mmu_cache() but that's for later).
    
    If for some reason we didn't do it there and we try to execute, we'll hit
    the page fault, which will do a minor fault, which will hit ptep_set_access_flags()
    to do things like update _PAGE_ACCESSED or _PAGE_DIRTY if needed, we just make
    this guys also perform the I/D cache sync for exec faults now. This second path
    is the catch all for things that weren't cleaned at set_pte_at() time.
    
    For cpus without per-pag exec support, we always do the sync at set_pte_at(),
    thus guaranteeing that when the PTE is visible to other processors, the cache
    is clean.
    
    For the 64-bit hash with per-page exec support case, we keep the old mechanism
    for now. I'll look into changing it later, once I've reworked a bit how we
    use _PAGE_EXEC.
    
    This is also a first step for adding _PAGE_EXEC support for embedded platforms
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f00f09a77f12..f668fa9ba804 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -472,40 +472,7 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 {
 #ifdef CONFIG_PPC_STD_MMU
 	unsigned long access = 0, trap;
-#endif
-	unsigned long pfn = pte_pfn(pte);
-
-	/* handle i-cache coherency */
-	if (!cpu_has_feature(CPU_FTR_COHERENT_ICACHE) &&
-	    !cpu_has_feature(CPU_FTR_NOEXECUTE) &&
-	    pfn_valid(pfn)) {
-		struct page *page = pfn_to_page(pfn);
-#ifdef CONFIG_8xx
-		/* On 8xx, cache control instructions (particularly
-		 * "dcbst" from flush_dcache_icache) fault as write
-		 * operation if there is an unpopulated TLB entry
-		 * for the address in question. To workaround that,
-		 * we invalidate the TLB here, thus avoiding dcbst
-		 * misbehaviour.
-		 */
-		_tlbil_va(address, 0 /* 8xx doesn't care about PID */);
-#endif
-		/* The _PAGE_USER test should really be _PAGE_EXEC, but
-		 * older glibc versions execute some code from no-exec
-		 * pages, which for now we are supporting.  If exec-only
-		 * pages are ever implemented, this will have to change.
-		 */
-		if (!PageReserved(page) && (pte_val(pte) & _PAGE_USER)
-		    && !test_bit(PG_arch_1, &page->flags)) {
-			if (vma->vm_mm == current->active_mm) {
-				__flush_dcache_icache((void *) address);
-			} else
-				flush_dcache_icache_page(page);
-			set_bit(PG_arch_1, &page->flags);
-		}
-	}
 
-#ifdef CONFIG_PPC_STD_MMU
 	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
 	if (!pte_young(pte) || address >= TASK_SIZE)
 		return;

commit c04fc586c1a480ba198f03ae7b6cbd7b57380b91
Author: Gary Hade <garyhade@us.ibm.com>
Date:   Tue Jan 6 14:39:14 2009 -0800

    mm: show node to memory section relationship with symlinks in sysfs
    
    Show node to memory section relationship with symlinks in sysfs
    
    Add /sys/devices/system/node/nodeX/memoryY symlinks for all
    the memory sections located on nodeX.  For example:
    /sys/devices/system/node/node1/memory135 -> ../../memory/memory135
    indicates that memory section 135 resides on node1.
    
    Also revises documentation to cover this change as well as updating
    Documentation/ABI/testing/sysfs-devices-memory to include descriptions
    of memory hotremove files 'phys_device', 'phys_index', and 'state'
    that were previously not described there.
    
    In addition to it always being a good policy to provide users with
    the maximum possible amount of physical location information for
    resources that can be hot-added and/or hot-removed, the following
    are some (but likely not all) of the user benefits provided by
    this change.
    Immediate:
      - Provides information needed to determine the specific node
        on which a defective DIMM is located.  This will reduce system
        downtime when the node or defective DIMM is swapped out.
      - Prevents unintended onlining of a memory section that was
        previously offlined due to a defective DIMM.  This could happen
        during node hot-add when the user or node hot-add assist script
        onlines _all_ offlined sections due to user or script inability
        to identify the specific memory sections located on the hot-added
        node.  The consequences of reintroducing the defective memory
        could be ugly.
      - Provides information needed to vary the amount and distribution
        of memory on specific nodes for testing or debugging purposes.
    Future:
      - Will provide information needed to identify the memory
        sections that need to be offlined prior to physical removal
        of a specific node.
    
    Symlink creation during boot was tested on 2-node x86_64, 2-node
    ppc64, and 2-node ia64 systems.  Symlink creation during physical
    memory hot-add tested on a 2-node x86_64 system.
    
    Signed-off-by: Gary Hade <garyhade@us.ibm.com>
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 53b06ebb3f2f..f00f09a77f12 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -132,7 +132,7 @@ int arch_add_memory(int nid, u64 start, u64 size)
 	/* this should work for most non-highmem platforms */
 	zone = pgdata->node_zones;
 
-	return __add_pages(zone, start_pfn, nr_pages);
+	return __add_pages(nid, zone, start_pfn, nr_pages);
 }
 #endif /* CONFIG_MEMORY_HOTPLUG */
 

commit 64b3d0e8122b422e879b23d42f9e0e8efbbf9744
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:51 2008 +0000

    powerpc/mm: Rework usage of _PAGE_COHERENT/NO_CACHE/GUARDED
    
    Currently, we never set _PAGE_COHERENT in the PTEs, we just OR it in
    in the hash code based on some CPU feature bit.  We also manipulate
    _PAGE_NO_CACHE and _PAGE_GUARDED by hand in all sorts of places.
    
    This changes the logic so that instead, the PTE now contains
    _PAGE_COHERENT for all normal RAM pages thay have I = 0 on platforms
    that need it.  The hash code clears it if the feature bit is not set.
    
    It also adds some clean accessors to setup various valid combinations
    of access flags and change various bits of code to use them instead.
    
    This should help having the PTE actually containing the bit
    combinations that we really want.
    
    I also removed _PAGE_GUARDED from _PAGE_BASE on 44x and instead
    set it explicitely from the TLB miss.  I will ultimately remove it
    completely as it appears that it might not be needed after all
    but in the meantime, having it in the TLB miss makes things a
    lot easier.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 8fee696fb795..53b06ebb3f2f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -102,8 +102,8 @@ pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 		return ppc_md.phys_mem_access_prot(file, pfn, size, vma_prot);
 
 	if (!page_is_ram(pfn))
-		vma_prot = __pgprot(pgprot_val(vma_prot)
-				    | _PAGE_GUARDED | _PAGE_NO_CACHE);
+		vma_prot = pgprot_noncached(vma_prot);
+
 	return vma_prot;
 }
 EXPORT_SYMBOL(phys_mem_access_prot);

commit f048aace29e007f2b642097e2da8231e0e9cce2d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:38 2008 +0000

    powerpc/mm: Add SMP support to no-hash TLB handling
    
    This commit moves the whole no-hash TLB handling out of line into a
    new tlb_nohash.c file, and implements some basic SMP support using
    IPIs and/or broadcast tlbivax instructions.
    
    Note that I'm using local invalidations for D->I cache coherency.
    
    At worst, if another processor is trying to execute the same and
    has the old entry in its TLB, it will just take a fault and re-do
    the TLB flush locally (it won't re-do the cache flush in any case).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index b9e1a1da6e52..8fee696fb795 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -488,7 +488,7 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		 * we invalidate the TLB here, thus avoiding dcbst
 		 * misbehaviour.
 		 */
-		_tlbie(address, 0 /* 8xx doesn't care about PID */);
+		_tlbil_va(address, 0 /* 8xx doesn't care about PID */);
 #endif
 		/* The _PAGE_USER test should really be _PAGE_EXEC, but
 		 * older glibc versions execute some code from no-exec

commit 71088785c6bc68fddb450063d57b1bd1c78e0ea1
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Sat Oct 18 20:25:58 2008 -0700

    mm: cleanup to make remove_memory() arch-neutral
    
    There is nothing architecture specific about remove_memory().
    remove_memory() function is common for all architectures which support
    hotplug memory remove.  Instead of duplicating it in every architecture,
    collapse them into arch neutral function.
    
    [akpm@linux-foundation.org: fix the export]
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Gary Hade <garyhade@us.ibm.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 98d7bf99533a..b9e1a1da6e52 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -134,23 +134,6 @@ int arch_add_memory(int nid, u64 start, u64 size)
 
 	return __add_pages(zone, start_pfn, nr_pages);
 }
-
-#ifdef CONFIG_MEMORY_HOTREMOVE
-int remove_memory(u64 start, u64 size)
-{
-	unsigned long start_pfn, end_pfn;
-	int ret;
-
-	start_pfn = start >> PAGE_SHIFT;
-	end_pfn = start_pfn + (size >> PAGE_SHIFT);
-	ret = offline_pages(start_pfn, end_pfn, 120 * HZ);
-	if (ret)
-		goto out;
-	/* Arch-specific calls go here - next patch */
-out:
-	return ret;
-}
-#endif /* CONFIG_MEMORY_HOTREMOVE */
 #endif /* CONFIG_MEMORY_HOTPLUG */
 
 /*

commit a880e7623397bcb44877b012cd65baa11ad1bbf8
Author: Roland Dreier <rdreier@cisco.com>
Date:   Mon Sep 15 10:43:35 2008 +0000

    powerpc: Avoid integer overflow in page_is_ram()
    
    Commit 8b150478 ("ppc: make phys_mem_access_prot() work with pfns
    instead of addresses") fixed page_is_ram() in arch/ppc to avoid overflow
    for addresses above 4G on 32-bit kernels.  However arch/powerpc's
    page_is_ram() is missing the same fix -- it computes a physical address
    by doing pfn << PAGE_SHIFT, which overflows if pfn corresponds to a page
    above 4G.
    
    In particular this causes pages above 4G to be mapped with the wrong
    caching attribute; for example many ppc440-based SoCs have PCI space
    above 4G, and mmap()ing MMIO space may end up with a mapping that has
    caching enabled.
    
    Fix this by working with the pfn and avoiding the conversion to
    physical address that causes the overflow.  This patch compares the
    pfn to max_pfn, which is a semantic change from the old code -- that
    code compared the physical address to high_memory, which corresponds
    to max_low_pfn.  However, I think that was is another bug, since
    highmem pages are still RAM.
    
    Reported-by: vb <vb@vsbe.com>
    Signed-off-by: Roland Dreier <rolandd@cisco.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 1c93c255873b..98d7bf99533a 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -75,11 +75,10 @@ static inline pte_t *virt_to_kpte(unsigned long vaddr)
 
 int page_is_ram(unsigned long pfn)
 {
-	unsigned long paddr = (pfn << PAGE_SHIFT);
-
 #ifndef CONFIG_PPC64	/* XXX for now */
-	return paddr < __pa(high_memory);
+	return pfn < max_pfn;
 #else
+	unsigned long paddr = (pfn << PAGE_SHIFT);
 	int i;
 	for (i=0; i < lmb.memory.cnt; i++) {
 		unsigned long base;

commit fb61063587982b52304d62cdbb6a0a88d26ae7ef
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Thu Jul 31 13:51:42 2008 +1000

    powerpc: Fix compiler warning in arch/powerpc/mm/mem.c
    
    Explicitly cast to unsigned long long, rather than u64.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 702691cb9e82..1c93c255873b 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -311,7 +311,7 @@ void __init paging_init(void)
 #endif /* CONFIG_HIGHMEM */
 
 	printk(KERN_DEBUG "Top of RAM: 0x%llx, Total RAM: 0x%lx\n",
-	       (u64)top_of_ram, total_ram);
+	       (unsigned long long)top_of_ram, total_ram);
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (long int)((top_of_ram - total_ram) >> 20));
 	memset(max_zone_pfns, 0, sizeof(max_zone_pfns));

commit bda2fa535564ace56a395d5b65c6dc81305401fa
Author: Johannes Weiner <hannes@saeurebad.de>
Date:   Fri Jul 25 19:46:14 2008 -0700

    powerpc: use generic show_mem()
    
    Remove arch-specific show_mem() in favor of the generic version.
    
    This also removes the following redundant information display:
    
            - pages in swapcache, printed by show_swap_cache_info()
    
    where show_mem() calls show_free_areas(), which calls
    show_swap_cache_info().
    
    Signed-off-by: Johannes Weiner <hannes@saeurebad.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 1ca2235f0965..702691cb9e82 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -186,45 +186,6 @@ walk_memory_resource(unsigned long start_pfn, unsigned long nr_pages, void *arg,
 }
 EXPORT_SYMBOL_GPL(walk_memory_resource);
 
-void show_mem(void)
-{
-	unsigned long total = 0, reserved = 0;
-	unsigned long shared = 0, cached = 0;
-	unsigned long highmem = 0;
-	struct page *page;
-	pg_data_t *pgdat;
-	unsigned long i;
-
-	printk("Mem-info:\n");
-	show_free_areas();
-	for_each_online_pgdat(pgdat) {
-		unsigned long flags;
-		pgdat_resize_lock(pgdat, &flags);
-		for (i = 0; i < pgdat->node_spanned_pages; i++) {
-			if (!pfn_valid(pgdat->node_start_pfn + i))
-				continue;
-			page = pgdat_page_nr(pgdat, i);
-			total++;
-			if (PageHighMem(page))
-				highmem++;
-			if (PageReserved(page))
-				reserved++;
-			else if (PageSwapCache(page))
-				cached++;
-			else if (page_count(page))
-				shared += page_count(page) - 1;
-		}
-		pgdat_resize_unlock(pgdat, &flags);
-	}
-	printk("%ld pages of RAM\n", total);
-#ifdef CONFIG_HIGHMEM
-	printk("%ld pages of HIGHMEM\n", highmem);
-#endif
-	printk("%ld reserved pages\n", reserved);
-	printk("%ld pages shared\n", shared);
-	printk("%ld pages swap cached\n", cached);
-}
-
 /*
  * Initialize the bootmem system and give it all the memory we
  * have available.  If we are using highmem, we only put the

commit 2bf3016f89344d4cd8b2c96bbec2b642a2bde413
Author: Stefan Roese <sr@denx.de>
Date:   Thu Jul 10 01:09:23 2008 +1000

    powerpc: Fix problems with 32bit PPC's running with >= 4GB of RAM
    
    This patch enables 32bit PPC's (with 36bit physical address space, e.g.
    IBM/AMCC PPC44x) to run with >= 4GB of RAM. Mostly its just replacing types
    (unsigned long -> phys_addr_t).
    
    Tested on an AMCC Katmai with 4GB of DDR2.
    
    Signed-off-by: Stefan Roese <sr@denx.de>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 776ba6ad5e1e..1ca2235f0965 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -330,7 +330,7 @@ static int __init mark_nonram_nosave(void)
 void __init paging_init(void)
 {
 	unsigned long total_ram = lmb_phys_mem_size();
-	unsigned long top_of_ram = lmb_end_of_DRAM();
+	phys_addr_t top_of_ram = lmb_end_of_DRAM();
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
 
 #ifdef CONFIG_PPC32
@@ -349,10 +349,10 @@ void __init paging_init(void)
 	kmap_prot = PAGE_KERNEL;
 #endif /* CONFIG_HIGHMEM */
 
-	printk(KERN_DEBUG "Top of RAM: 0x%lx, Total RAM: 0x%lx\n",
-	       top_of_ram, total_ram);
+	printk(KERN_DEBUG "Top of RAM: 0x%llx, Total RAM: 0x%lx\n",
+	       (u64)top_of_ram, total_ram);
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
-	       (top_of_ram - total_ram) >> 20);
+	       (long int)((top_of_ram - total_ram) >> 20));
 	memset(max_zone_pfns, 0, sizeof(max_zone_pfns));
 #ifdef CONFIG_HIGHMEM
 	max_zone_pfns[ZONE_DMA] = lowmem_end_addr >> PAGE_SHIFT;

commit db7f37de2c8346c33cf9279fa9d8e8316e8d821c
Author: Tony Breeds <tony@bakeyournoodle.com>
Date:   Tue Jul 1 11:30:06 2008 +1000

    powerpc: Fix building of arch/powerpc/mm/mem.o when MEMORY_HOTPLUG=y and SPARSEMEM=n
    
    Currently the kernel fails to build with the above config options with:
      CC      arch/powerpc/mm/mem.o
    arch/powerpc/mm/mem.c: In function 'arch_add_memory':
    arch/powerpc/mm/mem.c:130: error: implicit declaration of function 'create_section_mapping'
    
    This explicitly includes asm/sparsemem.h in arch/powerpc/mm/mem.c and
    moves the guards in include/asm-powerpc/sparsemem.h to protect the
    SPARSEMEM specific portions only.
    
    Signed-off-by: Tony Breeds <tony@bakeyournoodle.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 51f82d83bf14..776ba6ad5e1e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -44,6 +44,7 @@
 #include <asm/btext.h>
 #include <asm/tlb.h>
 #include <asm/sections.h>
+#include <asm/sparsemem.h>
 #include <asm/vdso.h>
 #include <asm/fixmap.h>
 

commit 0d5799449f0f373ca12681d86c941ae464146a37
Author: Nathan Lynch <ntl@pobox.com>
Date:   Wed Jun 4 08:30:54 2008 +1000

    [POWERPC] Make walk_memory_resource available with MEMORY_HOTPLUG=n
    
    The ehea driver was recently changed[1] to use walk_memory_resource() to
    detect the system's memory layout.  However, walk_memory_resource() is
    available only when memory hotplug is enabled.  So CONFIG_EHEA was
    made to depend on MEMORY_HOTPLUG [2], but it is inappropriate for a
    network driver to have such a dependency.
    
    Make the declaration of walk_memory_resource() and its powerpc
    implementation (ehea is powerpc-specific) unconditionally available.
    
    [1] 48cfb14f8b89d4d5b3df6c16f08b258686fb12ad
        "ehea: Add DLPAR memory remove support"
    
    [2] fb7b6ca2b6b7c23b52be143bdd5f55a23b9780c8
        "ehea: Add dependency to Kconfig"
    
    Signed-off-by: Nathan Lynch <ntl@pobox.com>
    Acked-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f67e118116fa..51f82d83bf14 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -151,6 +151,7 @@ int remove_memory(u64 start, u64 size)
 	return ret;
 }
 #endif /* CONFIG_MEMORY_HOTREMOVE */
+#endif /* CONFIG_MEMORY_HOTPLUG */
 
 /*
  * walk_memory_resource() needs to make sure there is no holes in a given
@@ -184,8 +185,6 @@ walk_memory_resource(unsigned long start_pfn, unsigned long nr_pages, void *arg,
 }
 EXPORT_SYMBOL_GPL(walk_memory_resource);
 
-#endif /* CONFIG_MEMORY_HOTPLUG */
-
 void show_mem(void)
 {
 	unsigned long total = 0, reserved = 0;

commit 9d88a2eb6e05c07aa0d484b8fa1372722fa921d0
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Fri Apr 18 13:33:53 2008 -0700

    [POWERPC] Provide walk_memory_resource() for powerpc
    
    Provide walk_memory_resource() for 64-bit powerpc.  PowerPC maintains
    logical memory region mapping in the lmb.memory structure.  Walk
    through these structures and do the callbacks for the contiguous
    chunks.
    
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index d9e37f365b54..f67e118116fa 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -154,19 +154,35 @@ int remove_memory(u64 start, u64 size)
 
 /*
  * walk_memory_resource() needs to make sure there is no holes in a given
- * memory range. On PPC64, since this range comes from /sysfs, the range
- * is guaranteed to be valid, non-overlapping and can not contain any
- * holes. By the time we get here (memory add or remove), /proc/device-tree
- * is updated and correct. Only reason we need to check against device-tree
- * would be if we allow user-land to specify a memory range through a
- * system call/ioctl etc. instead of doing offline/online through /sysfs.
+ * memory range.  PPC64 does not maintain the memory layout in /proc/iomem.
+ * Instead it maintains it in lmb.memory structures.  Walk through the
+ * memory regions, find holes and callback for contiguous regions.
  */
 int
 walk_memory_resource(unsigned long start_pfn, unsigned long nr_pages, void *arg,
 			int (*func)(unsigned long, unsigned long, void *))
 {
-	return  (*func)(start_pfn, nr_pages, arg);
+	struct lmb_property res;
+	unsigned long pfn, len;
+	u64 end;
+	int ret = -1;
+
+	res.base = (u64) start_pfn << PAGE_SHIFT;
+	res.size = (u64) nr_pages << PAGE_SHIFT;
+
+	end = res.base + res.size - 1;
+	while ((res.base < end) && (lmb_find(&res) >= 0)) {
+		pfn = (unsigned long)(res.base >> PAGE_SHIFT);
+		len = (unsigned long)(res.size >> PAGE_SHIFT);
+		ret = (*func)(pfn, len, arg);
+		if (ret)
+			break;
+		res.base += (res.size + 1);
+		res.size = (end - res.base + 1);
+	}
+	return ret;
 }
+EXPORT_SYMBOL_GPL(walk_memory_resource);
 
 #endif /* CONFIG_MEMORY_HOTPLUG */
 

commit 180c06efce691f2b721dd0d965079827bdd7ee03
Author: Jeremy Fitzhardinge <jeremy@goop.org>
Date:   Mon Apr 28 02:12:03 2008 -0700

    hotplug-memory: make online_page() common
    
    All architectures use an effectively identical definition of online_page(), so
    just make it common code.  x86-64, ia64, powerpc and sh are actually
    identical; x86-32 is slightly different.
    
    x86-32's differences arise because it puts its hotplug pages in the highmem
    zone.  We can handle this in the generic code by inspecting the page to see if
    its in highmem, and update the totalhigh_pages count appropriately.  This
    leaves init_32.c:free_new_highpage with a single caller, so I folded it into
    add_one_highpage_init.
    
    I also removed an incorrect comment referring to the NUMA case; any NUMA
    details have already been dealt with by the time online_page() is called.
    
    [akpm@linux-foundation.org: fix indenting]
    Signed-off-by: Jeremy Fitzhardinge <jeremy.fitzhardinge@citrix.com>
    Acked-by: Dave Hansen <dave@linux.vnet.ibm.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamez.hiroyu@jp.fujitsu.com>
    Tested-by: KAMEZAWA Hiroyuki <kamez.hiroyu@jp.fujitsu.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Christoph Lameter <clameter@sgi.com>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5ccb579b81e4..d9e37f365b54 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -110,15 +110,6 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 
 #ifdef CONFIG_MEMORY_HOTPLUG
 
-void online_page(struct page *page)
-{
-	ClearPageReserved(page);
-	init_page_count(page);
-	__free_page(page);
-	totalram_pages++;
-	num_physpages++;
-}
-
 #ifdef CONFIG_NUMA
 int memory_add_physaddr_to_nid(u64 start)
 {

commit 2c419bdeca1d958bb02228b5141695f312d8c633
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 23 23:05:20 2008 +1000

    [POWERPC] Port fixmap from x86 and use for kmap_atomic
    
    The fixmap code from x86 allows us to have compile time virtual addresses
    that we change the physical addresses of at run time.
    
    This is useful for applications like kmap_atomic, PCI config that is done
    via direct memory map, kexec/kdump.
    
    We got ride of CONFIG_HIGHMEM_START as we can now determine a more optimal
    location for PKMAP_BASE based on where the fixmap addresses start and
    working back from there.
    
    Additionally, the kmap code in asm-powerpc/highmem.h always had debug
    enabled.  Moved to using CONFIG_DEBUG_HIGHMEM to determine if we should
    have the extra debug checking.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0062e6b1c555..5ccb579b81e4 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -45,6 +45,7 @@
 #include <asm/tlb.h>
 #include <asm/sections.h>
 #include <asm/vdso.h>
+#include <asm/fixmap.h>
 
 #include "mmu_decl.h"
 
@@ -57,6 +58,20 @@ int init_bootmem_done;
 int mem_init_done;
 unsigned long memory_limit;
 
+#ifdef CONFIG_HIGHMEM
+pte_t *kmap_pte;
+pgprot_t kmap_prot;
+
+EXPORT_SYMBOL(kmap_prot);
+EXPORT_SYMBOL(kmap_pte);
+
+static inline pte_t *virt_to_kpte(unsigned long vaddr)
+{
+	return pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k(vaddr),
+			vaddr), vaddr), vaddr);
+}
+#endif
+
 int page_is_ram(unsigned long pfn)
 {
 	unsigned long paddr = (pfn << PAGE_SHIFT);
@@ -311,14 +326,19 @@ void __init paging_init(void)
 	unsigned long top_of_ram = lmb_end_of_DRAM();
 	unsigned long max_zone_pfns[MAX_NR_ZONES];
 
+#ifdef CONFIG_PPC32
+	unsigned long v = __fix_to_virt(__end_of_fixed_addresses - 1);
+	unsigned long end = __fix_to_virt(FIX_HOLE);
+
+	for (; v < end; v += PAGE_SIZE)
+		map_page(v, 0, 0); /* XXX gross */
+#endif
+
 #ifdef CONFIG_HIGHMEM
 	map_page(PKMAP_BASE, 0, 0);	/* XXX gross */
-	pkmap_page_table = pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k
-			(PKMAP_BASE), PKMAP_BASE), PKMAP_BASE), PKMAP_BASE);
-	map_page(KMAP_FIX_BEGIN, 0, 0);	/* XXX gross */
-	kmap_pte = pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k
-			(KMAP_FIX_BEGIN), KMAP_FIX_BEGIN), KMAP_FIX_BEGIN),
-			 KMAP_FIX_BEGIN);
+	pkmap_page_table = virt_to_kpte(PKMAP_BASE);
+
+	kmap_pte = virt_to_kpte(__fix_to_virt(FIX_KMAP_BEGIN));
 	kmap_prot = PAGE_KERNEL;
 #endif /* CONFIG_HIGHMEM */
 

commit 37dd2badcfcec35f5e21a0926968d77a404f03c3
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Tue Apr 22 04:22:34 2008 +1000

    [POWERPC] 85xx: Add support for relocatable kernel (and booting at non-zero)
    
    Added support to allow an 85xx kernel to be run from a non-zero physical
    address (useful for cooperative asymmetric multiprocessing situations and
    kdump).  The support can be configured at compile time by setting
    CONFIG_PAGE_OFFSET, CONFIG_KERNEL_START, and CONFIG_PHYSICAL_START as
    desired.
    
    Alternatively, the kernel build can set CONFIG_RELOCATABLE.  Setting this
    config option causes the kernel to determine at runtime the physical
    addresses of CONFIG_PAGE_OFFSET and CONFIG_KERNEL_START.  If
    CONFIG_RELOCATABLE is set, then CONFIG_PHYSICAL_START has no meaning.
    However, CONFIG_PHYSICAL_START will always be used to set the LOAD program
    header physical address field in the resulting ELF image.
    
    Currently we are limited to running at a physical address that is a
    multiple of 256M.  This is due to how we map TLBs to cover
    lowmem.  This should be fixed to allow 64M or maybe even 16M alignment
    in the future.  It is considered an error to try and run a kernel at a
    non-aligned physical address.
    
    All the magic for this support is accomplished by proper initialization
    of the kernel memory subsystem and use of ARCH_PFN_OFFSET.
    
    The use of ARCH_PFN_OFFSET only affects normal memory and not IO mappings.
    ioremap uses map_page and isn't affected by ARCH_PFN_OFFSET.
    
    /dev/mem continues to allow access to any physical address in the system
    regardless of how CONFIG_PHYSICAL_START is set.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 16def4dcff6d..0062e6b1c555 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -216,7 +216,7 @@ void __init do_init_bootmem(void)
 	unsigned long total_pages;
 	int boot_mapsize;
 
-	max_pfn = lmb_end_of_DRAM() >> PAGE_SHIFT;
+	max_low_pfn = max_pfn = lmb_end_of_DRAM() >> PAGE_SHIFT;
 	total_pages = (lmb_end_of_DRAM() - memstart_addr) >> PAGE_SHIFT;
 #ifdef CONFIG_HIGHMEM
 	total_pages = total_lowmem >> PAGE_SHIFT;
@@ -232,7 +232,8 @@ void __init do_init_bootmem(void)
 
 	start = lmb_alloc(bootmap_pages << PAGE_SHIFT, PAGE_SIZE);
 
-	boot_mapsize = init_bootmem(start >> PAGE_SHIFT, total_pages);
+	min_low_pfn = MEMORY_START >> PAGE_SHIFT;
+	boot_mapsize = init_bootmem_node(NODE_DATA(0), start >> PAGE_SHIFT, min_low_pfn, max_low_pfn);
 
 	/* Add active regions with valid PFNs */
 	for (i = 0; i < lmb.memory.cnt; i++) {

commit d7917ba7051e3fd12ebe2d5a09b29fb3a2b38190
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 16 05:52:22 2008 +1000

    [POWERPC] Introduce lowmem_end_addr to distinguish from total_lowmem
    
    total_lowmem represents the amount of low memory, not the physical
    address that low memory ends at.  If the start of memory is at 0 it
    happens that total_lowmem can be used as both the size and the address
    that lowmem ends at (or more specifically one byte beyond the end).
    
    To make the code a bit more clear and deal with the case when the start of
    memory isn't at physical 0, we introduce lowmem_end_addr that represents
    one byte beyond the last physical address in the lowmem region.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e3349ead3959..16def4dcff6d 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -216,9 +216,11 @@ void __init do_init_bootmem(void)
 	unsigned long total_pages;
 	int boot_mapsize;
 
-	max_pfn = total_pages = lmb_end_of_DRAM() >> PAGE_SHIFT;
+	max_pfn = lmb_end_of_DRAM() >> PAGE_SHIFT;
+	total_pages = (lmb_end_of_DRAM() - memstart_addr) >> PAGE_SHIFT;
 #ifdef CONFIG_HIGHMEM
 	total_pages = total_lowmem >> PAGE_SHIFT;
+	max_low_pfn = lowmem_end_addr >> PAGE_SHIFT;
 #endif
 
 	/*
@@ -244,18 +246,18 @@ void __init do_init_bootmem(void)
 	 * present.
 	 */
 #ifdef CONFIG_HIGHMEM
-	free_bootmem_with_active_regions(0, total_lowmem >> PAGE_SHIFT);
+	free_bootmem_with_active_regions(0, lowmem_end_addr >> PAGE_SHIFT);
 
 	/* reserve the sections we're already using */
 	for (i = 0; i < lmb.reserved.cnt; i++) {
 		unsigned long addr = lmb.reserved.region[i].base +
 				     lmb_size_bytes(&lmb.reserved, i) - 1;
-		if (addr < total_lowmem)
+		if (addr < lowmem_end_addr)
 			reserve_bootmem(lmb.reserved.region[i].base,
 					lmb_size_bytes(&lmb.reserved, i),
 					BOOTMEM_DEFAULT);
-		else if (lmb.reserved.region[i].base < total_lowmem) {
-			unsigned long adjusted_size = total_lowmem -
+		else if (lmb.reserved.region[i].base < lowmem_end_addr) {
+			unsigned long adjusted_size = lowmem_end_addr -
 				      lmb.reserved.region[i].base;
 			reserve_bootmem(lmb.reserved.region[i].base,
 					adjusted_size, BOOTMEM_DEFAULT);
@@ -325,7 +327,7 @@ void __init paging_init(void)
 	       (top_of_ram - total_ram) >> 20);
 	memset(max_zone_pfns, 0, sizeof(max_zone_pfns));
 #ifdef CONFIG_HIGHMEM
-	max_zone_pfns[ZONE_DMA] = total_lowmem >> PAGE_SHIFT;
+	max_zone_pfns[ZONE_DMA] = lowmem_end_addr >> PAGE_SHIFT;
 	max_zone_pfns[ZONE_HIGHMEM] = top_of_ram >> PAGE_SHIFT;
 #else
 	max_zone_pfns[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
@@ -380,7 +382,7 @@ void __init mem_init(void)
 	{
 		unsigned long pfn, highmem_mapnr;
 
-		highmem_mapnr = total_lowmem >> PAGE_SHIFT;
+		highmem_mapnr = lowmem_end_addr >> PAGE_SHIFT;
 		for (pfn = highmem_mapnr; pfn < max_mapnr; ++pfn) {
 			struct page *page = pfn_to_page(pfn);
 			if (lmb_is_reserved(pfn << PAGE_SHIFT))

commit 745681a524cfc9c972dd89dc7df46f825217c22b
Author: Johannes Weiner <hannes@saeurebad.de>
Date:   Sat Mar 29 08:21:09 2008 +1100

    [POWERPC] Remove redundant display of free swap space in show_mem()
    
    show_mem() has no need to print the amount of free swap space manually
    because show_free_areas() does this already and is called by the
    former.
    
    The two outputs only differ in text formatting:
    
      printk("Free swap  = %lukB\n", ...);
      printk("Free swap:       %6ldkB\n", ...);
    
    Signed-off-by: Johannes Weiner <hannes@saeurebad.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 1b119b92a826..e3349ead3959 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -175,7 +175,6 @@ void show_mem(void)
 
 	printk("Mem-info:\n");
 	show_free_areas();
-	printk("Free swap:       %6ldkB\n", nr_swap_pages<<(PAGE_SHIFT-10));
 	for_each_online_pgdat(pgdat) {
 		unsigned long flags;
 		pgdat_resize_lock(pgdat, &flags);

commit fa90f70a8ed338d1afb43da21163752c6558a7d5
Author: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
Date:   Sat Mar 29 03:10:50 2008 +1100

    [POWERPC] arch_add_memory() cannot be __devinit
    
    WARNING: vmlinux.o(.text+0xb41b0): Section mismatch in reference from the
    function .add_memory() to the function .devinit.text:.arch_add_memory()
    The function .add_memory() references
    the function __devinit .arch_add_memory().
    This is often because .add_memory lacks a __devinit
    annotation or the annotation of .arch_add_memory is wrong.
    
    arch_add_memory() is also not __devinit on other architectures
    
    Signed-off-by: Geert Uytterhoeven <Geert.Uytterhoeven@sonycom.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 60c019cdc69f..1b119b92a826 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -111,7 +111,7 @@ int memory_add_physaddr_to_nid(u64 start)
 }
 #endif
 
-int __devinit arch_add_memory(int nid, u64 start, u64 size)
+int arch_add_memory(int nid, u64 start, u64 size)
 {
 	struct pglist_data *pgdata;
 	struct zone *zone;

commit d9b2b2a277219d4812311d995054ce4f95067725
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Feb 13 16:56:49 2008 -0800

    [LIB]: Make PowerPC LMB code generic so sparc64 can use it too.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index be5c506779a7..60c019cdc69f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -31,6 +31,7 @@
 #include <linux/initrd.h>
 #include <linux/pagemap.h>
 #include <linux/suspend.h>
+#include <linux/lmb.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -42,7 +43,6 @@
 #include <asm/machdep.h>
 #include <asm/btext.h>
 #include <asm/tlb.h>
-#include <asm/lmb.h>
 #include <asm/sections.h>
 #include <asm/vdso.h>
 

commit a99824f327c748b2753f4fa570eb1fefcd6a9c4d
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Tue Feb 5 00:10:18 2008 -0800

    [POWERPC] Add arch-specific walk_memory_remove() for 64-bit powerpc
    
    walk_memory_resource() verifies if there are holes in a given memory
    range, by checking against /proc/iomem.  On x86/ia64 system memory is
    represented in /proc/iomem.  On powerpc, we don't show system memory as
    IO resource in /proc/iomem - instead it's maintained in
    /proc/device-tree.
    
    This provides a way for an architecture to provide its own
    walk_memory_resource() function.  On powerpc, the memory region is
    small (16MB), contiguous and non-overlapping.  So extra checking
    against the device-tree is not needed.
    
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Kumar Gala <galak@gate.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f0a1fd268b7a..be5c506779a7 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -145,6 +145,23 @@ int remove_memory(u64 start, u64 size)
 	return ret;
 }
 #endif /* CONFIG_MEMORY_HOTREMOVE */
+
+/*
+ * walk_memory_resource() needs to make sure there is no holes in a given
+ * memory range. On PPC64, since this range comes from /sysfs, the range
+ * is guaranteed to be valid, non-overlapping and can not contain any
+ * holes. By the time we get here (memory add or remove), /proc/device-tree
+ * is updated and correct. Only reason we need to check against device-tree
+ * would be if we allow user-land to specify a memory range through a
+ * system call/ioctl etc. instead of doing offline/online through /sysfs.
+ */
+int
+walk_memory_resource(unsigned long start_pfn, unsigned long nr_pages, void *arg,
+			int (*func)(unsigned long, unsigned long, void *))
+{
+	return  (*func)(start_pfn, nr_pages, arg);
+}
+
 #endif /* CONFIG_MEMORY_HOTPLUG */
 
 void show_mem(void)

commit aa620abe756207222d234f785d41435fac486e06
Author: Badari Pulavarty <pbadari@us.ibm.com>
Date:   Tue Feb 5 00:10:16 2008 -0800

    [POWERPC] Add remove_memory() for 64-bit powerpc
    
    Supply remove_memory() function for 64-bit powerpc.  This is still
    not quite complete as it needs to do some more arch-specific stuff,
    which will be added in a later patch.
    
    Signed-off-by: Badari Pulavarty <pbadari@us.ibm.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 93a5c53e3423..f0a1fd268b7a 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -129,6 +129,22 @@ int __devinit arch_add_memory(int nid, u64 start, u64 size)
 	return __add_pages(zone, start_pfn, nr_pages);
 }
 
+#ifdef CONFIG_MEMORY_HOTREMOVE
+int remove_memory(u64 start, u64 size)
+{
+	unsigned long start_pfn, end_pfn;
+	int ret;
+
+	start_pfn = start >> PAGE_SHIFT;
+	end_pfn = start_pfn + (size >> PAGE_SHIFT);
+	ret = offline_pages(start_pfn, end_pfn, 120 * HZ);
+	if (ret)
+		goto out;
+	/* Arch-specific calls go here - next patch */
+out:
+	return ret;
+}
+#endif /* CONFIG_MEMORY_HOTREMOVE */
 #endif /* CONFIG_MEMORY_HOTPLUG */
 
 void show_mem(void)

commit 37969581301e50872a1ae84dc73962b5f7ee6b76
Merge: 80ff8a805113 24f1a849614b
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Thu Feb 7 09:02:26 2008 -0800

    Merge branch 'for-2.6.25' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    * 'for-2.6.25' of git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc: (69 commits)
      [POWERPC] Add SPE registers to core dumps
      [POWERPC] Use regset code for compat PTRACE_*REGS* calls
      [POWERPC] Use generic compat_sys_ptrace
      [POWERPC] Use generic compat_ptrace_request
      [POWERPC] Use generic ptrace peekdata/pokedata
      [POWERPC] Use regset code for PTRACE_*REGS* requests
      [POWERPC] Switch to generic compat_binfmt_elf code
      [POWERPC] Switch to using user_regset-based core dumps
      [POWERPC] Add user_regset compat support
      [POWERPC] Add user_regset_view definitions
      [POWERPC] Use user_regset accessors for GPRs
      [POWERPC] ptrace accessors for special regs MSR and TRAP
      [POWERPC] Use user_regset accessors for SPE regs
      [POWERPC] Use user_regset accessors for altivec regs
      [POWERPC] Use user_regset accessors for FP regs
      [POWERPC] mpc52xx: fix compile error introduce when rebasing patch
      [POWERPC] 4xx: PCIe indirect DCR spinlock fix.
      [POWERPC] Add missing native dcr dcr_ind_lock spinlock
      [POWERPC] 4xx: Fix offset value on Warp board
      [POWERPC] 4xx: Add 440EPx Sequoia ehci dts entry
      ...

commit 72a7fe3967dbf86cb34e24fbf1d957fe24d2f246
Author: Bernhard Walle <bwalle@suse.de>
Date:   Thu Feb 7 00:15:17 2008 -0800

    Introduce flags for reserve_bootmem()
    
    This patchset adds a flags variable to reserve_bootmem() and uses the
    BOOTMEM_EXCLUSIVE flag in crashkernel reservation code to detect collisions
    between crashkernel area and already used memory.
    
    This patch:
    
    Change the reserve_bootmem() function to accept a new flag BOOTMEM_EXCLUSIVE.
    If that flag is set, the function returns with -EBUSY if the memory already
    has been reserved in the past.  This is to avoid conflicts.
    
    Because that code runs before SMP initialisation, there's no race condition
    inside reserve_bootmem_core().
    
    [akpm@linux-foundation.org: coding-style fixes]
    [akpm@linux-foundation.org: fix powerpc build]
    Signed-off-by: Bernhard Walle <bwalle@suse.de>
    Cc: <linux-arch@vger.kernel.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Vivek Goyal <vgoyal@in.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e8122447f019..ff5debf5eedd 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -220,12 +220,13 @@ void __init do_init_bootmem(void)
 				     lmb_size_bytes(&lmb.reserved, i) - 1;
 		if (addr < total_lowmem)
 			reserve_bootmem(lmb.reserved.region[i].base,
-					lmb_size_bytes(&lmb.reserved, i));
+					lmb_size_bytes(&lmb.reserved, i),
+					BOOTMEM_DEFAULT);
 		else if (lmb.reserved.region[i].base < total_lowmem) {
 			unsigned long adjusted_size = total_lowmem -
 				      lmb.reserved.region[i].base;
 			reserve_bootmem(lmb.reserved.region[i].base,
-					adjusted_size);
+					adjusted_size, BOOTMEM_DEFAULT);
 		}
 	}
 #else
@@ -234,7 +235,8 @@ void __init do_init_bootmem(void)
 	/* reserve the sections we're already using */
 	for (i = 0; i < lmb.reserved.cnt; i++)
 		reserve_bootmem(lmb.reserved.region[i].base,
-				lmb_size_bytes(&lmb.reserved, i));
+				lmb_size_bytes(&lmb.reserved, i),
+				BOOTMEM_DEFAULT);
 
 #endif
 	/* XXX need to clip this if using highmem? */

commit 551ed332da2f52daf5d01fc76d2a894aba79d316
Author: Scott Wood <scottwood@freescale.com>
Date:   Wed Feb 6 06:43:26 2008 +1100

    [POWERPC] update_mmu_cache: Don't cache-flush non-readable pages
    
    Currently, update_mmu_cache will crash if given a no-access PTE.
    There's no need to synchronize dcache/icache unless it's an exec
    mapping -- however, due to the existence of older glibc versions that
    execute out of a read-but-no-exec page, readability is tested instead.
    
    This assumes no exec-only mappings; if such mappings become supported,
    they will need to go through the kmap_atomic() version of
    dcache/icache synchronization.
    
    This fixes a bug reported by some users where the kernel would crash
    while dumping core on a threaded program.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e8122447f019..c7d7bd43a251 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -483,7 +483,12 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		 */
 		_tlbie(address, 0 /* 8xx doesn't care about PID */);
 #endif
-		if (!PageReserved(page)
+		/* The _PAGE_USER test should really be _PAGE_EXEC, but
+		 * older glibc versions execute some code from no-exec
+		 * pages, which for now we are supporting.  If exec-only
+		 * pages are ever implemented, this will have to change.
+		 */
+		if (!PageReserved(page) && (pte_val(pte) & _PAGE_USER)
 		    && !test_bit(PG_arch_1, &page->flags)) {
 			if (vma->vm_mm == current->active_mm) {
 				__flush_dcache_icache((void *) address);

commit f98eeb4eb1c52de89dcefeb538029bcecc6dd42d
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Jan 9 11:27:23 2008 -0600

    [POWERPC] Fix handling of memreserve if the range lands in highmem
    
    There were several issues if a memreserve range existed and happened
    to be in highmem:
    
    * The bootmem allocator is only aware of lowmem so calling
      reserve_bootmem with a highmem address would cause a BUG_ON
    * All highmem pages were provided to the buddy allocator
    
    Added a lmb_is_reserved() api that we now use to determine if a highem
    page should continue to be PageReserved or provided to the buddy
    allocator.
    
    Also, we incorrectly reported the amount of pages reserved since all
    highmem pages are initally marked reserved and we clear the
    PageReserved flag as we "free" up the highmem pages.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5402fb6b3aae..e8122447f019 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -213,15 +213,30 @@ void __init do_init_bootmem(void)
 	 */
 #ifdef CONFIG_HIGHMEM
 	free_bootmem_with_active_regions(0, total_lowmem >> PAGE_SHIFT);
+
+	/* reserve the sections we're already using */
+	for (i = 0; i < lmb.reserved.cnt; i++) {
+		unsigned long addr = lmb.reserved.region[i].base +
+				     lmb_size_bytes(&lmb.reserved, i) - 1;
+		if (addr < total_lowmem)
+			reserve_bootmem(lmb.reserved.region[i].base,
+					lmb_size_bytes(&lmb.reserved, i));
+		else if (lmb.reserved.region[i].base < total_lowmem) {
+			unsigned long adjusted_size = total_lowmem -
+				      lmb.reserved.region[i].base;
+			reserve_bootmem(lmb.reserved.region[i].base,
+					adjusted_size);
+		}
+	}
 #else
 	free_bootmem_with_active_regions(0, max_pfn);
-#endif
 
 	/* reserve the sections we're already using */
 	for (i = 0; i < lmb.reserved.cnt; i++)
 		reserve_bootmem(lmb.reserved.region[i].base,
 				lmb_size_bytes(&lmb.reserved, i));
 
+#endif
 	/* XXX need to clip this if using highmem? */
 	sparse_memory_present_with_active_regions(0);
 
@@ -334,11 +349,13 @@ void __init mem_init(void)
 		highmem_mapnr = total_lowmem >> PAGE_SHIFT;
 		for (pfn = highmem_mapnr; pfn < max_mapnr; ++pfn) {
 			struct page *page = pfn_to_page(pfn);
-
+			if (lmb_is_reserved(pfn << PAGE_SHIFT))
+				continue;
 			ClearPageReserved(page);
 			init_page_count(page);
 			__free_page(page);
 			totalhigh_pages++;
+			reservedpages--;
 		}
 		totalram_pages += totalhigh_pages;
 		printk(KERN_DEBUG "High memory: %luk\n",

commit 0b47759db54f82df68ed179ddc5cb2becea56158
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Nov 20 18:32:12 2007 +1100

    [POWERPC] Fix 8xx build breakage due to _tlbie changes
    
    My changes to _tlbie to fix 4xx unfortunately broke 8xx build in a
    couple of places.  This fixes it.
    
    Spotted by Olof Johansson.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Vitaly Bordug <vitb@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 81eb96ec13b2..5402fb6b3aae 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -464,7 +464,7 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		 * we invalidate the TLB here, thus avoiding dcbst
 		 * misbehaviour.
 		 */
-		_tlbie(address);
+		_tlbie(address, 0 /* 8xx doesn't care about PID */);
 #endif
 		if (!PageReserved(page)
 		    && !test_bit(PG_arch_1, &page->flags)) {

commit 48e94196a533dbee17c252bf80d0310fb8c8c2eb
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 16 01:26:14 2007 -0700

    fix memory hot remove not configured case.
    
    Now, arch dependent code around CONFIG_MEMORY_HOTREMOVE is a mess.
    This patch cleans up them. This is against 2.6.23-rc6-mm1.
    
     - fix compile failure on ia64/ CONFIG_MEMORY_HOTPLUG && !CONFIG_MEMORY_HOTREMOVE case.
     - For !CONFIG_MEMORY_HOTREMOVE, add generic no-op remove_memory(),
       which returns -EINVAL.
     - removed remove_pages() only used in powerpc.
     - removed no-op remove_memory() in i386, sh, sparc64, x86_64.
    
     - only powerpc returns -ENOSYS at memory hot remove(no-op). changes it
       to return -EINVAL.
    
    Note:
    Currently, only ia64 supports CONFIG_MEMORY_HOTREMOVE. I welcome other
    archs if there are requirements and testers.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 32dcfc9b0082..81eb96ec13b2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -129,51 +129,6 @@ int __devinit arch_add_memory(int nid, u64 start, u64 size)
 	return __add_pages(zone, start_pfn, nr_pages);
 }
 
-/*
- * First pass at this code will check to determine if the remove
- * request is within the RMO.  Do not allow removal within the RMO.
- */
-int __devinit remove_memory(u64 start, u64 size)
-{
-	struct zone *zone;
-	unsigned long start_pfn, end_pfn, nr_pages;
-
-	start_pfn = start >> PAGE_SHIFT;
-	nr_pages = size >> PAGE_SHIFT;
-	end_pfn = start_pfn + nr_pages;
-
-	printk("%s(): Attempting to remove memoy in range "
-			"%lx to %lx\n", __func__, start, start+size);
-	/*
-	 * check for range within RMO
-	 */
-	zone = page_zone(pfn_to_page(start_pfn));
-
-	printk("%s(): memory will be removed from "
-			"the %s zone\n", __func__, zone->name);
-
-	/*
-	 * not handling removing memory ranges that
-	 * overlap multiple zones yet
-	 */
-	if (end_pfn > (zone->zone_start_pfn + zone->spanned_pages))
-		goto overlap;
-
-	/* make sure it is NOT in RMO */
-	if ((start < lmb.rmo_size) || ((start+size) < lmb.rmo_size)) {
-		printk("%s(): range to be removed must NOT be in RMO!\n",
-			__func__);
-		goto in_rmo;
-	}
-
-	return __remove_pages(zone, start_pfn, nr_pages);
-
-overlap:
-	printk("%s(): memory range to be removed overlaps "
-		"multiple zones!!!\n", __func__);
-in_rmo:
-	return -1;
-}
 #endif /* CONFIG_MEMORY_HOTPLUG */
 
 void show_mem(void)

commit 9420dc65ff9e6b67c032286efde823aeb8684670
Author: Jesper Juhl <jesper.juhl@gmail.com>
Date:   Mon Jul 30 08:18:25 2007 +1000

    [POWERPC] Clean out a bunch of duplicate includes
    
    This removes several duplicate includes from arch/powerpc/.
    
    Signed-off-by: Jesper Juhl <jesper.juhl@gmail.com>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index f0e7eedb1ba3..32dcfc9b0082 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -42,7 +42,6 @@
 #include <asm/machdep.h>
 #include <asm/btext.h>
 #include <asm/tlb.h>
-#include <asm/prom.h>
 #include <asm/lmb.h>
 #include <asm/sections.h>
 #include <asm/vdso.h>

commit b3e998ee05773a3ac33d540aa341295acef4c56b
Author: Manish Ahuja <ahuja@austin.ibm.com>
Date:   Sat Jul 7 07:59:55 2007 +1000

    [POWERPC] Remove extra return statement
    
    Found 2 instances of return one right after each other in
    arch_add_memory().  This removes the superfluous one.
    
    Signed-off-by: Manish Ahuja <mahuja@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2c8790f36be2..f0e7eedb1ba3 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -128,8 +128,6 @@ int __devinit arch_add_memory(int nid, u64 start, u64 size)
 	zone = pgdata->node_zones;
 
 	return __add_pages(zone, start_pfn, nr_pages);
-
-	return 0;
 }
 
 /*

commit f21f49ea639ac3f24824177dac1268af75a2d373
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Jun 13 14:52:54 2007 +1000

    [POWERPC] Remove the dregs of APUS support from arch/powerpc
    
    APUS (the Amiga Power-Up System) is not supported under arch/powerpc
    and it's unlikely it ever will be.  Therefore, this patch removes the
    fragments of APUS support code from arch/powerpc which have been
    copied from arch/ppc.
    
    A few APUS references are left in asm-powerpc in .h files which are
    still used from arch/ppc.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0266a94d83b6..2c8790f36be2 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -5,7 +5,6 @@
  *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)
  *  and Cort Dougan (PReP) (cort@cs.nmt.edu)
  *    Copyright (C) 1996 Paul Mackerras
- *  Amiga/APUS changes by Jesper Skov (jskov@cygnus.co.uk).
  *  PPC44x/36-bit changes by Matt Porter (mporter@mvista.com)
  *
  *  Derived from "arch/i386/mm/init.c"

commit 5453e7723b95958f4591b2e0063573d8d53e7699
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue May 22 16:25:51 2007 +1000

    [POWERPC] Fix warning in 32-bit builds with CONFIG_HIGHMEM
    
    Some missing fixup for the removal of 4 level fixup header.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 246eeea40ece..0266a94d83b6 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -310,11 +310,12 @@ void __init paging_init(void)
 
 #ifdef CONFIG_HIGHMEM
 	map_page(PKMAP_BASE, 0, 0);	/* XXX gross */
-	pkmap_page_table = pte_offset_kernel(pmd_offset(pgd_offset_k
-			(PKMAP_BASE), PKMAP_BASE), PKMAP_BASE);
+	pkmap_page_table = pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k
+			(PKMAP_BASE), PKMAP_BASE), PKMAP_BASE), PKMAP_BASE);
 	map_page(KMAP_FIX_BEGIN, 0, 0);	/* XXX gross */
-	kmap_pte = pte_offset_kernel(pmd_offset(pgd_offset_k
-			(KMAP_FIX_BEGIN), KMAP_FIX_BEGIN), KMAP_FIX_BEGIN);
+	kmap_pte = pte_offset_kernel(pmd_offset(pud_offset(pgd_offset_k
+			(KMAP_FIX_BEGIN), KMAP_FIX_BEGIN), KMAP_FIX_BEGIN),
+			 KMAP_FIX_BEGIN);
 	kmap_prot = PAGE_KERNEL;
 #endif /* CONFIG_HIGHMEM */
 

commit 4e8ad3e816cca76ff8550e531103fc5e20471654
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Tue May 8 19:25:00 2007 +1000

    [POWERPC] Mark pages that don't exist as nosave
    
    On some powerpc architectures (notably 64-bit powermac) there is a memory
    hole, for example on powermacs between 2G and 4G. Since we use the flat
    memory model regardless, these pages must be marked as nosave (for suspend
    to disk.)
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 1a6e08f3298f..246eeea40ece 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -31,6 +31,7 @@
 #include <linux/highmem.h>
 #include <linux/initrd.h>
 #include <linux/pagemap.h>
+#include <linux/suspend.h>
 
 #include <asm/pgalloc.h>
 #include <asm/prom.h>
@@ -276,6 +277,28 @@ void __init do_init_bootmem(void)
 	init_bootmem_done = 1;
 }
 
+/* mark pages that don't exist as nosave */
+static int __init mark_nonram_nosave(void)
+{
+	unsigned long lmb_next_region_start_pfn,
+		      lmb_region_max_pfn;
+	int i;
+
+	for (i = 0; i < lmb.memory.cnt - 1; i++) {
+		lmb_region_max_pfn =
+			(lmb.memory.region[i].base >> PAGE_SHIFT) +
+			(lmb.memory.region[i].size >> PAGE_SHIFT);
+		lmb_next_region_start_pfn =
+			lmb.memory.region[i+1].base >> PAGE_SHIFT;
+
+		if (lmb_region_max_pfn < lmb_next_region_start_pfn)
+			register_nosave_region(lmb_region_max_pfn,
+					       lmb_next_region_start_pfn);
+	}
+
+	return 0;
+}
+
 /*
  * paging_init() sets up the page tables - in fact we've already done this.
  */
@@ -307,6 +330,8 @@ void __init paging_init(void)
 	max_zone_pfns[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
 #endif
 	free_area_init_nodes(max_zone_pfns);
+
+	mark_nonram_nosave();
 }
 #endif /* ! CONFIG_NEED_MULTIPLE_NODES */
 

commit a3cf4bdef090530cf2e07dcd45265107db6e4752
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Fri Feb 9 00:27:44 2007 +1100

    [POWERPC] Remove unneeded page_is_ram export
    
    arch/powerpc/mm/mem.c exports page_is_ram, which is not used anywhere
    that could be modular.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index c4bcd7546424..1a6e08f3298f 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -80,7 +80,6 @@ int page_is_ram(unsigned long pfn)
 	return 0;
 #endif
 }
-EXPORT_SYMBOL(page_is_ram);
 
 pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 			      unsigned long size, pgprot_t vma_prot)

commit ee4f2ea48674b6c9d91bc854edc51a3e6a7168c4
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Apr 12 15:30:22 2007 +1000

    [POWERPC] Fix 32-bit mm operations when not using BATs
    
    On hash table based 32 bits powerpc's, the hash management code runs with
    a big spinlock. It's thus important that it never causes itself a hash
    fault. That code is generally safe (it does memory accesses in real mode
    among other things) with the exception of the actual access to the code
    itself. That is, the kernel text needs to be accessible without taking
    a hash miss exceptions.
    
    This is currently guaranteed by having a BAT register mapping part of the
    linear mapping permanently, which includes the kernel text. But this is
    not true if using the "nobats" kernel command line option (which can be
    useful for debugging) and will not be true when using DEBUG_PAGEALLOC
    implemented in a subsequent patch.
    
    This patch fixes this by pre-faulting in the hash table pages that hit
    the kernel text, and making sure we never evict such a page under hash
    pressure.
    
    Signed-off-by: Benjamin Herrenchmidt <benh@kernel.crashing.org>
    
     arch/powerpc/mm/hash_low_32.S |   22 ++++++++++++++++++++--
     arch/powerpc/mm/mem.c         |    3 ---
     arch/powerpc/mm/mmu_decl.h    |    4 ++++
     arch/powerpc/mm/pgtable_32.c  |   11 +++++++----
     4 files changed, 31 insertions(+), 9 deletions(-)
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 52f397c108a7..c4bcd7546424 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -58,9 +58,6 @@ int init_bootmem_done;
 int mem_init_done;
 unsigned long memory_limit;
 
-extern void hash_preload(struct mm_struct *mm, unsigned long ea,
-			 unsigned long access, unsigned long trap);
-
 int page_is_ram(unsigned long pfn)
 {
 	unsigned long paddr = (pfn << PAGE_SHIFT);

commit 7ac9a13717c10c5ee074a6b23096c8d277fa5712
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Feb 12 13:31:08 2007 +1100

    [POWERPC] Fix vDSO page count calculation
    
    The recent vDSO consolidation patches broke powerpc due to a mistake
    in the definition of MAXPAGES constants. This fixes it by moving to
    a dynamically allocated array of pages instead as I don't like much
    hard coded size limits. Also move the vdso initialisation to an initcall
    since it doesn't really need to be done -that- early.
    
    Applogies for not catching the breakage earlier, Roland _did_ CC me on
    his patches a while ago, I got busy with other things and forgot to test
    them.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 77b4637097e9..52f397c108a7 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -384,9 +384,6 @@ void __init mem_init(void)
 		initsize >> 10);
 
 	mem_init_done = 1;
-
-	/* Initialize the vDSO */
-	vdso_init();
 }
 
 /*

commit bcff4948c64e1af09e9a986e324626ee873d3a07
Author: Johannes Berg <johannes@sipsolutions.net>
Date:   Wed Feb 7 13:45:45 2007 +0100

    [POWERPC] Remove bogus comment about page_is_ram
    
    arch/powerpc/mm/mem.c states that page_is_ram is called by the code that
    implements /dev/mem, which isn't true.  Remove the comment.
    
    Signed-off-by: Johannes Berg <johannes@sipsolutions.net>
    Cc: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index c85eda63d2b3..77b4637097e9 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -61,10 +61,6 @@ unsigned long memory_limit;
 extern void hash_preload(struct mm_struct *mm, unsigned long ea,
 			 unsigned long access, unsigned long trap);
 
-/*
- * This is called by /dev/mem to know if a given address has to
- * be mapped non-cacheable or not
- */
 int page_is_ram(unsigned long pfn)
 {
 	unsigned long paddr = (pfn << PAGE_SHIFT);

commit dbbb06b7f6ae8037a5f6b4498e492791e1929635
Author: Vitaly Bordug <vbordug@ru.mvista.com>
Date:   Wed Jan 24 22:40:57 2007 +0300

    [POWERPC] 8xx: platform specific mmu updates
    
    This is just a straight port of the same done in arch/ppc
    by Marcelo Tosatti. One used to be
    [PATCH] ppc32 8xx: update_mmu_cache() needs unconditional tlbie,
    commit eb07d964b4491d1bb5864cd3d7e7633ccdda9a53
    
    In a nutshell, the board is nearly stuck without this, yet without any
    visible failure - being just very slow.
    
    Signed-off-by: Vitaly Bordug <vbordug@ru.mvista.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index d1c0758c5611..c85eda63d2b3 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -490,19 +490,19 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	    !cpu_has_feature(CPU_FTR_NOEXECUTE) &&
 	    pfn_valid(pfn)) {
 		struct page *page = pfn_to_page(pfn);
+#ifdef CONFIG_8xx
+		/* On 8xx, cache control instructions (particularly
+		 * "dcbst" from flush_dcache_icache) fault as write
+		 * operation if there is an unpopulated TLB entry
+		 * for the address in question. To workaround that,
+		 * we invalidate the TLB here, thus avoiding dcbst
+		 * misbehaviour.
+		 */
+		_tlbie(address);
+#endif
 		if (!PageReserved(page)
 		    && !test_bit(PG_arch_1, &page->flags)) {
 			if (vma->vm_mm == current->active_mm) {
-#ifdef CONFIG_8xx
-			/* On 8xx, cache control instructions (particularly 
-		 	 * "dcbst" from flush_dcache_icache) fault as write 
-			 * operation if there is an unpopulated TLB entry 
-			 * for the address in question. To workaround that, 
-			 * we invalidate the TLB here, thus avoiding dcbst 
-			 * misbehaviour.
-			 */
-				_tlbie(address);
-#endif
 				__flush_dcache_icache((void *) address);
 			} else
 				flush_dcache_icache_page(page);

commit 6391af174ad75f72e92043c1dd8302660a2fec58
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Oct 11 01:20:39 2006 -0700

    [PATCH] mm: use symbolic names instead of indices for zone initialisation
    
    Arch-independent zone-sizing is using indices instead of symbolic names to
    offset within an array related to zones (max_zone_pfns).  The unintended
    impact is that ZONE_DMA and ZONE_NORMAL is initialised on powerpc instead
    of ZONE_DMA and ZONE_HIGHMEM when CONFIG_HIGHMEM is set.  As a result, the
    the machine fails to boot but will boot with CONFIG_HIGHMEM turned off.
    
    The following patch properly initialises the max_zone_pfns[] array and uses
    symbolic names instead of indices in each architecture using
    arch-independent zone-sizing.  Two users have successfully booted their
    powerpcs with it (one an ibook G4).  It has also been boot tested on x86,
    x86_64, ppc64 and ia64.  Please merge for 2.6.19-rc2.
    
    Credit to Benjamin Herrenschmidt for identifying the bug and rolling the
    first fix.  Additional credit to Johannes Berg and Andreas Schwab for
    reporting the problem and testing on powerpc.
    
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 16fe027bbc12..d1c0758c5611 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -307,11 +307,12 @@ void __init paging_init(void)
 	       top_of_ram, total_ram);
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (top_of_ram - total_ram) >> 20);
+	memset(max_zone_pfns, 0, sizeof(max_zone_pfns));
 #ifdef CONFIG_HIGHMEM
-	max_zone_pfns[0] = total_lowmem >> PAGE_SHIFT;
-	max_zone_pfns[1] = top_of_ram >> PAGE_SHIFT;
+	max_zone_pfns[ZONE_DMA] = total_lowmem >> PAGE_SHIFT;
+	max_zone_pfns[ZONE_HIGHMEM] = top_of_ram >> PAGE_SHIFT;
 #else
-	max_zone_pfns[0] = top_of_ram >> PAGE_SHIFT;
+	max_zone_pfns[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
 #endif
 	free_area_init_nodes(max_zone_pfns);
 }

commit c67c3cb4c99fb2ee63c8733943c353d745f45b84
Author: Mel Gorman <mel@csn.ul.ie>
Date:   Wed Sep 27 01:49:49 2006 -0700

    [PATCH] Have Power use add_active_range() and free_area_init_nodes()
    
    Size zones and holes in an architecture independent manner for Power.
    
    [judith@osdl.org: build fix]
    Signed-off-by: Mel Gorman <mel@csn.ul.ie>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: Andy Whitcroft <apw@shadowen.org>
    Cc: Andi Kleen <ak@muc.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: "Keith Mannthey" <kmannth@gmail.com>
    Cc: "Luck, Tony" <tony.luck@intel.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index eebd8b83a6b0..16fe027bbc12 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -256,20 +256,22 @@ void __init do_init_bootmem(void)
 
 	boot_mapsize = init_bootmem(start >> PAGE_SHIFT, total_pages);
 
+	/* Add active regions with valid PFNs */
+	for (i = 0; i < lmb.memory.cnt; i++) {
+		unsigned long start_pfn, end_pfn;
+		start_pfn = lmb.memory.region[i].base >> PAGE_SHIFT;
+		end_pfn = start_pfn + lmb_size_pages(&lmb.memory, i);
+		add_active_range(0, start_pfn, end_pfn);
+	}
+
 	/* Add all physical memory to the bootmem map, mark each area
 	 * present.
 	 */
-	for (i = 0; i < lmb.memory.cnt; i++) {
-		unsigned long base = lmb.memory.region[i].base;
-		unsigned long size = lmb_size_bytes(&lmb.memory, i);
 #ifdef CONFIG_HIGHMEM
-		if (base >= total_lowmem)
-			continue;
-		if (base + size > total_lowmem)
-			size = total_lowmem - base;
+	free_bootmem_with_active_regions(0, total_lowmem >> PAGE_SHIFT);
+#else
+	free_bootmem_with_active_regions(0, max_pfn);
 #endif
-		free_bootmem(base, size);
-	}
 
 	/* reserve the sections we're already using */
 	for (i = 0; i < lmb.reserved.cnt; i++)
@@ -277,9 +279,8 @@ void __init do_init_bootmem(void)
 				lmb_size_bytes(&lmb.reserved, i));
 
 	/* XXX need to clip this if using highmem? */
-	for (i = 0; i < lmb.memory.cnt; i++)
-		memory_present(0, lmb_start_pfn(&lmb.memory, i),
-			       lmb_end_pfn(&lmb.memory, i));
+	sparse_memory_present_with_active_regions(0);
+
 	init_bootmem_done = 1;
 }
 
@@ -288,10 +289,9 @@ void __init do_init_bootmem(void)
  */
 void __init paging_init(void)
 {
-	unsigned long zones_size[MAX_NR_ZONES];
-	unsigned long zholes_size[MAX_NR_ZONES];
 	unsigned long total_ram = lmb_phys_mem_size();
 	unsigned long top_of_ram = lmb_end_of_DRAM();
+	unsigned long max_zone_pfns[MAX_NR_ZONES];
 
 #ifdef CONFIG_HIGHMEM
 	map_page(PKMAP_BASE, 0, 0);	/* XXX gross */
@@ -307,26 +307,13 @@ void __init paging_init(void)
 	       top_of_ram, total_ram);
 	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (top_of_ram - total_ram) >> 20);
-	/*
-	 * All pages are DMA-able so we put them all in the DMA zone.
-	 */
-	memset(zones_size, 0, sizeof(zones_size));
-	memset(zholes_size, 0, sizeof(zholes_size));
-
-	zones_size[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
-	zholes_size[ZONE_DMA] = (top_of_ram - total_ram) >> PAGE_SHIFT;
-
 #ifdef CONFIG_HIGHMEM
-	zones_size[ZONE_DMA] = total_lowmem >> PAGE_SHIFT;
-	zones_size[ZONE_HIGHMEM] = (total_memory - total_lowmem) >> PAGE_SHIFT;
-	zholes_size[ZONE_HIGHMEM] = (top_of_ram - total_ram) >> PAGE_SHIFT;
+	max_zone_pfns[0] = total_lowmem >> PAGE_SHIFT;
+	max_zone_pfns[1] = top_of_ram >> PAGE_SHIFT;
 #else
-	zones_size[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
-	zholes_size[ZONE_DMA] = (top_of_ram - total_ram) >> PAGE_SHIFT;
-#endif /* CONFIG_HIGHMEM */
-
-	free_area_init_node(0, NODE_DATA(0), zones_size,
-			    __pa(PAGE_OFFSET) >> PAGE_SHIFT, zholes_size);
+	max_zone_pfns[0] = top_of_ram >> PAGE_SHIFT;
+#endif
+	free_area_init_nodes(max_zone_pfns);
 }
 #endif /* ! CONFIG_NEED_MULTIPLE_NODES */
 

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: JÃ¶rn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: JÃ¶rn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 089d939a0b3e..eebd8b83a6b0 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -18,7 +18,6 @@
  *
  */
 
-#include <linux/config.h>
 #include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/kernel.h>

commit bc02af93dd2bbddce1b55e0a493f833a1b7cf140
Author: Yasunori Goto <y-goto@jp.fujitsu.com>
Date:   Tue Jun 27 02:53:30 2006 -0700

    [PATCH] pgdat allocation for new node add (specify node id)
    
    Change the name of old add_memory() to arch_add_memory.  And use node id to
    get pgdat for the node at NODE_DATA().
    
    Note: Powerpc's old add_memory() is defined as __devinit. However,
          add_memory() is usually called only after bootup.
          I suppose it may be redundant. But, I'm not well known about powerpc.
          So, I keep it. (But, __meminit is better at least.)
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Cc: Dave Hansen <haveblue@us.ibm.com>
    Cc: "Brown, Len" <len.brown@intel.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 69f3b9a20beb..089d939a0b3e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -114,15 +114,20 @@ void online_page(struct page *page)
 	num_physpages++;
 }
 
-int __devinit add_memory(u64 start, u64 size)
+#ifdef CONFIG_NUMA
+int memory_add_physaddr_to_nid(u64 start)
+{
+	return hot_add_scn_to_nid(start);
+}
+#endif
+
+int __devinit arch_add_memory(int nid, u64 start, u64 size)
 {
 	struct pglist_data *pgdata;
 	struct zone *zone;
-	int nid;
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 
-	nid = hot_add_scn_to_nid(start);
 	pgdata = NODE_DATA(nid);
 
 	start = (unsigned long)__va(start);

commit e110b281dc93e3b4587a3d0440bb7ae38daddfde
Author: Olof Johansson <olof@lixom.net>
Date:   Wed Apr 12 15:25:01 2006 -0500

    [PATCH] powerpc: Less verbose mem configuration output
    
    Quieten some of the debug ram config output. we already print out available
    memory at KERN_INFO level.
    
    Signed-off-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 741dd8802d49..69f3b9a20beb 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -299,9 +299,9 @@ void __init paging_init(void)
 	kmap_prot = PAGE_KERNEL;
 #endif /* CONFIG_HIGHMEM */
 
-	printk(KERN_INFO "Top of RAM: 0x%lx, Total RAM: 0x%lx\n",
+	printk(KERN_DEBUG "Top of RAM: 0x%lx, Total RAM: 0x%lx\n",
 	       top_of_ram, total_ram);
-	printk(KERN_INFO "Memory hole size: %ldMB\n",
+	printk(KERN_DEBUG "Memory hole size: %ldMB\n",
 	       (top_of_ram - total_ram) >> 20);
 	/*
 	 * All pages are DMA-able so we put them all in the DMA zone.
@@ -380,7 +380,7 @@ void __init mem_init(void)
 			totalhigh_pages++;
 		}
 		totalram_pages += totalhigh_pages;
-		printk(KERN_INFO "High memory: %luk\n",
+		printk(KERN_DEBUG "High memory: %luk\n",
 		       totalhigh_pages << (PAGE_SHIFT-10));
 	}
 #endif /* CONFIG_HIGHMEM */

commit bac30d1a78d0f11c613968fc8b351a91ed465386
Merge: e8222502ee61 ca9ba4471c12
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Mar 29 13:24:50 2006 +1100

    Merge ../linux-2.6

commit ec936fc563715a9e2b2e363eb060655b49529325
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Mon Mar 27 01:15:59 2006 -0800

    [PATCH] for_each_online_pgdat: renaming for_each_pgdat
    
    Replace for_each_pgdat() with for_each_online_pgdat().
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index badac10d700c..5e435a9c3431 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -195,7 +195,7 @@ void show_mem(void)
 	printk("Mem-info:\n");
 	show_free_areas();
 	printk("Free swap:       %6ldkB\n", nr_swap_pages<<(PAGE_SHIFT-10));
-	for_each_pgdat(pgdat) {
+	for_each_online_pgdat(pgdat) {
 		unsigned long flags;
 		pgdat_resize_lock(pgdat, &flags);
 		for (i = 0; i < pgdat->node_spanned_pages; i++) {
@@ -351,7 +351,7 @@ void __init mem_init(void)
 	max_mapnr = max_pfn;
 	totalram_pages += free_all_bootmem();
 #endif
-	for_each_pgdat(pgdat) {
+	for_each_online_pgdat(pgdat) {
 		for (i = 0; i < pgdat->node_spanned_pages; i++) {
 			if (!pfn_valid(pgdat->node_start_pfn + i))
 				continue;

commit c258dd40ab8851c41873516ef90125e3bcd2e3e1
Author: Anton Blanchard <anton@samba.org>
Date:   Sat Mar 25 17:27:09 2006 +1100

    [PATCH] powerpc: Consistent printing of node id
    
    We were printing node ids in hex in one spot. Lets be consistent and
    always print them in decimal.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index badac10d700c..4737f84bb19e 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -342,7 +342,7 @@ void __init mem_init(void)
 #ifdef CONFIG_NEED_MULTIPLE_NODES
         for_each_online_node(nid) {
 		if (NODE_DATA(nid)->node_spanned_pages != 0) {
-			printk("freeing bootmem node %x\n", nid);
+			printk("freeing bootmem node %d\n", nid);
 			totalram_pages +=
 				free_all_bootmem_node(NODE_DATA(nid));
 		}

commit 2e6e33bab6e1996a5dec9108fb467b52b841e7a8
Merge: b7ad6d75028d b88a0b1d5560
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Wed Mar 22 22:20:46 2006 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/paulus/powerpc: (78 commits)
      [PATCH] powerpc: Add FSL SEC node to documentation
      [PATCH] macintosh: tidy-up driver_register() return values
      [PATCH] powerpc: tidy-up of_register_driver()/driver_register() return values
      [PATCH] powerpc: via-pmu warning fix
      [PATCH] macintosh: cleanup the use of i2c headers
      [PATCH] powerpc: dont allow old RTC to be selected
      [PATCH] powerpc: make powerbook_sleep_grackle static
      [PATCH] powerpc: Fix warning in add_memory
      [PATCH] powerpc: update mailing list addresses
      [PATCH] powerpc: Remove calculation of io hole
      [PATCH] powerpc: iseries: Add bootargs to /chosen
      [PATCH] powerpc: iseries: Add /system-id, /model and /compatible
      [PATCH] powerpc: Add strne2a() to convert a string from EBCDIC to ASCII
      [PATCH] powerpc: iseries: Make more stuff static in platforms/iseries/mf.c
      [PATCH] powerpc: iseries: Remove pointless iSeries_(restart|power_off|halt)
      [PATCH] powerpc: iseries: mf related cleanups
      [PATCH] powerpc: Replace platform_is_lpar() with a firmware feature
      [PATCH] powerpc: trivial: Cleanup whitespace in cputable.h
      [PATCH] powerpc: Remove unused iommu_off logic from pSeries_init_early()
      [PATCH] powerpc: Unconfuse htab_bolt_mapping() callers
      ...

commit 2d0eee14b23f20c501a6c2536edf68f93c56edcd
Author: Andrew Morton <akpm@osdl.org>
Date:   Tue Mar 21 23:00:05 2006 -0800

    [PATCH] powerpc: Fix warning in add_memory
    
    arch/powerpc/mm/mem.c: In function `add_memory':
    arch/powerpc/mm/mem.c:128: warning: assignment makes integer from pointer without a cast
    
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 6809cdba6e94..80550b284505 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -125,7 +125,7 @@ int __devinit add_memory(u64 start, u64 size)
 	nid = hot_add_scn_to_nid(start);
 	pgdata = NODE_DATA(nid);
 
-	start = __va(start);
+	start = (unsigned long)__va(start);
 	create_section_mapping(start, start + size);
 
 	/* this should work for most non-highmem platforms */

commit 7835e98b2e3c66dba79cb0ff8ebb90a2fe030c29
Author: Nick Piggin <npiggin@suse.de>
Date:   Wed Mar 22 00:08:40 2006 -0800

    [PATCH] remove set_page_count() outside mm/
    
    set_page_count usage outside mm/ is limited to setting the refcount to 1.
    Remove set_page_count from outside mm/, and replace those users with
    init_page_count() and set_page_refcounted().
    
    This allows more debug checking, and tighter control on how code is allowed
    to play around with page->_count.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 6ae5c130d0db..454cac01d8cc 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -108,7 +108,7 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 void online_page(struct page *page)
 {
 	ClearPageReserved(page);
-	set_page_count(page, 1);
+	init_page_count(page);
 	__free_page(page);
 	totalram_pages++;
 	num_physpages++;
@@ -376,7 +376,7 @@ void __init mem_init(void)
 			struct page *page = pfn_to_page(pfn);
 
 			ClearPageReserved(page);
-			set_page_count(page, 1);
+			init_page_count(page);
 			__free_page(page);
 			totalhigh_pages++;
 		}

commit 70dc991d66cac40fdb07346dba2b5d862d732c34
Author: Nick Piggin <npiggin@suse.de>
Date:   Wed Mar 22 00:08:35 2006 -0800

    [PATCH] remove set_page_count(page, 0) users (outside mm)
    
    A couple of places set_page_count(page, 1) that don't need to.
    
    Signed-off-by: Nick Piggin <npiggin@suse.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Paolo 'Blaisorblade' Giarrusso <blaisorblade@yahoo.it>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 550517c2dd42..6ae5c130d0db 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -108,8 +108,8 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 void online_page(struct page *page)
 {
 	ClearPageReserved(page);
-	set_page_count(page, 0);
-	free_cold_page(page);
+	set_page_count(page, 1);
+	__free_page(page);
 	totalram_pages++;
 	num_physpages++;
 }

commit d7a5b2ffa1352f0310630934a56aecbdfb617b72
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Wed Jan 25 21:31:28 2006 +1300

    [PATCH] powerpc: Always panic if lmb_alloc() fails
    
    Currently most callers of lmb_alloc() don't check if it worked or not, if it
    ever does weird bad things will probably happen. The few callers who do check
    just panic or BUG_ON.
    
    So make lmb_alloc() panic internally, to catch bugs at the source. The few
    callers who did check the result no longer need to.
    
    The only caller that did anything interesting with the return result was
    careful_allocation(). For it we create __lmb_alloc_base() which _doesn't_ panic
    automatically, a little messy, but passable.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 550517c2dd42..6809cdba6e94 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -249,7 +249,6 @@ void __init do_init_bootmem(void)
 	bootmap_pages = bootmem_bootmap_pages(total_pages);
 
 	start = lmb_alloc(bootmap_pages << PAGE_SHIFT, PAGE_SIZE);
-	BUG_ON(!start);
 
 	boot_mapsize = init_bootmem(start >> PAGE_SHIFT, total_pages);
 

commit 09f5dc44aeff9f792a807a7dca64c1791d98d8c8
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Feb 6 13:24:53 2006 +1100

    [PATCH] powerpc: Cleanup, consolidating icache dirtying logic
    
    The code to mark a page as icache dirty (so that it will later be
    icache-dcache flushed when we try to execute from it) is duplicated in
    three places: flush_dcache_page() does this marking and nothing else,
    but clear_user_page() and copy_user_page() duplicate it, since those
    functions make the page icache dirty themselves.
    
    This patch makes those other functions call flush_dcache_page()
    instead, so the logic's all in one place.  This will make life less
    confusing if we ever need to tweak the details of the the lazy icache
    flush mechanism.
    
     arch/powerpc/mm/mem.c |   14 ++------------
     1 file changed, 2 insertions(+), 12 deletions(-)
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 15aac0d78dfa..550517c2dd42 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -435,17 +435,12 @@ void clear_user_page(void *page, unsigned long vaddr, struct page *pg)
 {
 	clear_page(page);
 
-	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
-		return;
 	/*
 	 * We shouldnt have to do this, but some versions of glibc
 	 * require it (ld.so assumes zero filled pages are icache clean)
 	 * - Anton
 	 */
-
-	/* avoid an atomic op if possible */
-	if (test_bit(PG_arch_1, &pg->flags))
-		clear_bit(PG_arch_1, &pg->flags);
+	flush_dcache_page(pg);
 }
 EXPORT_SYMBOL(clear_user_page);
 
@@ -469,12 +464,7 @@ void copy_user_page(void *vto, void *vfrom, unsigned long vaddr,
 		return;
 #endif
 
-	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
-		return;
-
-	/* avoid an atomic op if possible */
-	if (test_bit(PG_arch_1, &pg->flags))
-		clear_bit(PG_arch_1, &pg->flags);
+	flush_dcache_page(pg);
 }
 
 void flush_icache_user_range(struct vm_area_struct *vma, struct page *page,

commit e0fa93d6e6f0f36f39b813e561dbb890c58da58f
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Mon Jan 9 18:19:49 2006 +1100

    [PATCH] powerpc: Don't use KERNELBASE in add_memory()
    
    In add_memory() we should be using __va() to get a virtual address.
    Spotted by Mike Kravetz.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5e5bff5616a8..15aac0d78dfa 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -125,7 +125,7 @@ int __devinit add_memory(u64 start, u64 size)
 	nid = hot_add_scn_to_nid(start);
 	pgdata = NODE_DATA(nid);
 
-	start += KERNELBASE;
+	start = __va(start);
 	create_section_mapping(start, start + size);
 
 	/* this should work for most non-highmem platforms */

commit 237a0989e2902b7d43c4228a36d82f8691fb2118
Author: Mike Kravetz <kravetz@us.ibm.com>
Date:   Mon Dec 5 12:06:42 2005 -0800

    [PATCH] powerpc: numa placement for dynamically added memory
    
    This places dynamically added memory within the appropriate
    numa node.  A new routine hot_add_scn_to_nid() replicates most of
    the memory scanning code in parse_numa_properties().
    
    Signed-off-by: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index ed6ed2e30dac..5e5bff5616a8 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -114,18 +114,17 @@ void online_page(struct page *page)
 	num_physpages++;
 }
 
-/*
- * This works only for the non-NUMA case.  Later, we'll need a lookup
- * to convert from real physical addresses to nid, that doesn't use
- * pfn_to_nid().
- */
 int __devinit add_memory(u64 start, u64 size)
 {
-	struct pglist_data *pgdata = NODE_DATA(0);
+	struct pglist_data *pgdata;
 	struct zone *zone;
+	int nid;
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 
+	nid = hot_add_scn_to_nid(start);
+	pgdata = NODE_DATA(nid);
+
 	start += KERNELBASE;
 	create_section_mapping(start, start + size);
 

commit 01edcd891c3e9f4bb992ff2ceb69836bf76f8ddf
Author: Hugh Dickins <hugh@veritas.com>
Date:   Wed Nov 23 13:37:39 2005 -0800

    [PATCH] mm: powerpc ptlock comments
    
    Update comments (only) on page_table_lock and mmap_sem in arch/powerpc.
    Removed the comment on page_table_lock from hash_huge_page: since it's no
    longer taking page_table_lock itself, it's irrelevant whether others are; but
    how it is safe (even against huge file truncation?) I can't say.
    
    Signed-off-by: Hugh Dickins <hugh@veritas.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 4bd7b0a70996..ed6ed2e30dac 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -495,7 +495,7 @@ EXPORT_SYMBOL(flush_icache_user_range);
  * We use it to preload an HPTE into the hash table corresponding to
  * the updated linux PTE.
  * 
- * This must always be called with the mm->page_table_lock held
+ * This must always be called with the pte lock held.
  */
 void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		      pte_t pte)

commit fb6d73d3014babb69f5cc2d1d78b31e9d09fc5df
Author: Paul Mackerras <paulus@samba.org>
Date:   Wed Nov 16 11:43:26 2005 +1100

    [PATCH] powerpc: Fix sparsemem with memory holes [was Re: ppc64 oops..]
    
    This patch should fix the crashes we have been seeing on 64-bit
    powerpc systems with a memory hole when sparsemem is enabled.
    I'd appreciate it if people who know more about NUMA and sparsemem
    than me could look over it.
    
    There were two bugs.  The first was that if NUMA was enabled but there
    was no NUMA information for the machine, the setup_nonnuma() function
    was adding a single region, assuming memory was contiguous.  The
    second was that the loops in mem_init() and show_mem() assumed that
    all pages within the span of a pgdat were valid (had a valid struct
    page).
    
    I also fixed the incorrect setting of num_physpages that Mike Kravetz
    pointed out.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index e2c95fcb8055..4bd7b0a70996 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -200,6 +200,8 @@ void show_mem(void)
 		unsigned long flags;
 		pgdat_resize_lock(pgdat, &flags);
 		for (i = 0; i < pgdat->node_spanned_pages; i++) {
+			if (!pfn_valid(pgdat->node_start_pfn + i))
+				continue;
 			page = pgdat_page_nr(pgdat, i);
 			total++;
 			if (PageHighMem(page))
@@ -336,7 +338,7 @@ void __init mem_init(void)
 	struct page *page;
 	unsigned long reservedpages = 0, codesize, initsize, datasize, bsssize;
 
-	num_physpages = max_pfn;	/* RAM is assumed contiguous */
+	num_physpages = lmb.memory.size >> PAGE_SHIFT;
 	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
 
 #ifdef CONFIG_NEED_MULTIPLE_NODES
@@ -348,11 +350,13 @@ void __init mem_init(void)
 		}
 	}
 #else
-	max_mapnr = num_physpages;
+	max_mapnr = max_pfn;
 	totalram_pages += free_all_bootmem();
 #endif
 	for_each_pgdat(pgdat) {
 		for (i = 0; i < pgdat->node_spanned_pages; i++) {
+			if (!pfn_valid(pgdat->node_start_pfn + i))
+				continue;
 			page = pgdat_page_nr(pgdat, i);
 			if (PageReserved(page))
 				reservedpages++;

commit a7f290dad32ee34d931561b7943c858fe2aae503
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Nov 11 21:15:21 2005 +1100

    [PATCH] powerpc: Merge vdso's and add vdso support to 32 bits kernel
    
    This patch moves the vdso's to arch/powerpc, adds support for the 32
    bits vdso to the 32 bits kernel, rename systemcfg (finally !), and adds
    some new (still untested) routines to both vdso's: clock_gettime() with
    support for CLOCK_REALTIME and CLOCK_MONOTONIC, clock_getres() (same
    clocks) and get_tbfreq() for glibc to retreive the timebase frequency.
    
    Tom,Steve: The implementation of get_tbfreq() I've done for 32 bits
    returns a long long (r3, r4) not a long. This is such that if we ever
    add support for >4Ghz timebases on ppc32, the userland interface won't
    have to change.
    
    I have tested gettimeofday() using some glibc patches in both ppc32 and
    ppc64 kernels using 32 bits userland (I haven't had a chance to test a
    64 bits userland yet, but the implementation didn't change and was
    tested earlier). I haven't tested yet the new functions.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 1dd3cc69a490..e2c95fcb8055 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -46,9 +46,7 @@
 #include <asm/prom.h>
 #include <asm/lmb.h>
 #include <asm/sections.h>
-#ifdef CONFIG_PPC64
 #include <asm/vdso.h>
-#endif
 
 #include "mmu_decl.h"
 
@@ -397,10 +395,8 @@ void __init mem_init(void)
 
 	mem_init_done = 1;
 
-#ifdef CONFIG_PPC64
 	/* Initialize the vDSO */
 	vdso_init();
-#endif
 }
 
 /*

commit dd7ccbd3eed52a42dc8bba57717dac3ece39b58e
Author: Mike Kravetz <kravetz@us.ibm.com>
Date:   Fri Nov 4 15:19:32 2005 -0800

    [PATCH] Memory Add Fixes for ppc64
    
    memmap_init_zone() sets page count to 1.  Before 'freeing' the
    page, we need to clear the count.  This is the same that is done
    on free_all_bootmem_core() for memory discovered at boot time.
    
    Signed-off-by: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 2ead0951823a..1dd3cc69a490 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -110,6 +110,7 @@ EXPORT_SYMBOL(phys_mem_access_prot);
 void online_page(struct page *page)
 {
 	ClearPageReserved(page);
+	set_page_count(page, 0);
 	free_cold_page(page);
 	totalram_pages++;
 	num_physpages++;

commit 54b79248b24610663298e122c6ba3804468271fe
Author: Mike Kravetz <kravetz@us.ibm.com>
Date:   Mon Nov 7 16:25:48 2005 -0800

    [PATCH] revised Memory Add Fixes for ppc64
    
    Add the create_section_mapping() routine to create hptes for memory
    sections dynamically added after system boot.
    
    Signed-off-by: Mike Kravetz <kravetz@us.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 6f55efd9be95..2ead0951823a 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -127,6 +127,9 @@ int __devinit add_memory(u64 start, u64 size)
 	unsigned long start_pfn = start >> PAGE_SHIFT;
 	unsigned long nr_pages = size >> PAGE_SHIFT;
 
+	start += KERNELBASE;
+	create_section_mapping(start, start + size);
+
 	/* this should work for most non-highmem platforms */
 	zone = pgdata->node_zones;
 

commit bcb3557694d4e880051795ad97d609d255bcb658
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Nov 7 17:43:07 2005 +1100

    [PATCH] ppc64: fix Memory: summary line
    
    On ppc64 we end up with a negative value for the data size in the memory
    boot message:
    
    Memory: 2035560k/2097152k available (5792k kernel code, 89564k reserved,
    18014398509481632k data, 870k bss, 352k init)
    
    It turns out the section ordering of the linker script is different on
    ppc32 and ppc64, so just count data as _edata - _sdata which should work
    on both.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 7faa46b71f21..6f55efd9be95 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -358,7 +358,7 @@ void __init mem_init(void)
 	}
 
 	codesize = (unsigned long)&_sdata - (unsigned long)&_stext;
-	datasize = (unsigned long)&__init_begin - (unsigned long)&_sdata;
+	datasize = (unsigned long)&_edata - (unsigned long)&_sdata;
 	initsize = (unsigned long)&__init_end - (unsigned long)&__init_begin;
 	bsssize = (unsigned long)&__bss_stop - (unsigned long)&__bss_start;
 

commit 3c726f8dee6f55e96475574e9f645327e461884c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Nov 7 11:06:55 2005 +1100

    [PATCH] ppc64: support 64k pages
    
    Adds a new CONFIG_PPC_64K_PAGES which, when enabled, changes the kernel
    base page size to 64K.  The resulting kernel still boots on any
    hardware.  On current machines with 4K pages support only, the kernel
    will maintain 16 "subpages" for each 64K page transparently.
    
    Note that while real 64K capable HW has been tested, the current patch
    will not enable it yet as such hardware is not released yet, and I'm
    still verifying with the firmware architects the proper to get the
    information from the newer hypervisors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 117b00012e14..7faa46b71f21 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -61,6 +61,9 @@ int init_bootmem_done;
 int mem_init_done;
 unsigned long memory_limit;
 
+extern void hash_preload(struct mm_struct *mm, unsigned long ea,
+			 unsigned long access, unsigned long trap);
+
 /*
  * This is called by /dev/mem to know if a given address has to
  * be mapped non-cacheable or not
@@ -493,18 +496,10 @@ EXPORT_SYMBOL(flush_icache_user_range);
 void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 		      pte_t pte)
 {
-	/* handle i-cache coherency */
-	unsigned long pfn = pte_pfn(pte);
-#ifdef CONFIG_PPC32
-	pmd_t *pmd;
-#else
-	unsigned long vsid;
-	void *pgdir;
-	pte_t *ptep;
-	int local = 0;
-	cpumask_t tmp;
-	unsigned long flags;
+#ifdef CONFIG_PPC_STD_MMU
+	unsigned long access = 0, trap;
 #endif
+	unsigned long pfn = pte_pfn(pte);
 
 	/* handle i-cache coherency */
 	if (!cpu_has_feature(CPU_FTR_COHERENT_ICACHE) &&
@@ -535,30 +530,21 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
 	if (!pte_young(pte) || address >= TASK_SIZE)
 		return;
-#ifdef CONFIG_PPC32
-	if (Hash == 0)
-		return;
-	pmd = pmd_offset(pgd_offset(vma->vm_mm, address), address);
-	if (!pmd_none(*pmd))
-		add_hash_page(vma->vm_mm->context, address, pmd_val(*pmd));
-#else
-	pgdir = vma->vm_mm->pgd;
-	if (pgdir == NULL)
-		return;
 
-	ptep = find_linux_pte(pgdir, address);
-	if (!ptep)
+	/* We try to figure out if we are coming from an instruction
+	 * access fault and pass that down to __hash_page so we avoid
+	 * double-faulting on execution of fresh text. We have to test
+	 * for regs NULL since init will get here first thing at boot
+	 *
+	 * We also avoid filling the hash if not coming from a fault
+	 */
+	if (current->thread.regs == NULL)
 		return;
-
-	vsid = get_vsid(vma->vm_mm->context.id, address);
-
-	local_irq_save(flags);
-	tmp = cpumask_of_cpu(smp_processor_id());
-	if (cpus_equal(vma->vm_mm->cpu_vm_mask, tmp))
-		local = 1;
-
-	__hash_page(address, 0, vsid, ptep, 0x300, local);
-	local_irq_restore(flags);
-#endif
-#endif
+	trap = TRAP(current->thread.regs);
+	if (trap == 0x400)
+		access |= _PAGE_EXEC;
+	else if (trap != 0x300)
+		return;
+	hash_preload(vma->vm_mm, address, access, trap);
+#endif /* CONFIG_PPC_STD_MMU */
 }

commit 23fd07750a789a66fe88cf173d52a18f1a387da4
Merge: bd787d438a59 ed28f96ac196
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 31 13:37:12 2005 +1100

    Merge ../linux-2.6 by hand

commit cf00a8d18b9a1c2d55b2728e89125c234e821db5
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 31 13:07:02 2005 +1100

    powerpc: Fix bug arising from having multiple memory_limit variables
    
    We had a static memory_limit in prom.c, and then another one defined
    in setup_64.c and used in numa.c, which resulted in the kernel crashing
    when mem=xxx was given on the command line.  This puts the declaration
    in system.h and the definition in mem.c.  This also moves the
    definition of tce_alloc_start/end out of setup_64.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 3ca331728d21..e43e8ef70088 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -59,6 +59,7 @@
 
 int init_bootmem_done;
 int mem_init_done;
+unsigned long memory_limit;
 
 /*
  * This is called by /dev/mem to know if a given address has to

commit 8b150478aeb1a8edb9015c2f7ac4da637ff65c45
Author: Roland Dreier <rolandd@cisco.com>
Date:   Fri Oct 28 17:46:18 2005 -0700

    [PATCH] ppc: make phys_mem_access_prot() work with pfns instead of addresses
    
    Change the phys_mem_access_prot() function to take a pfn instead of an
    address.  This allows mmap64() to work on /dev/mem for addresses above 4G
    on 32-bit architectures.  We start with a pfn in mmap_mem(), so there's no
    need to convert to an address; in fact, it's actively bad, since the
    conversion can overflow when the address is above 4G.
    
    Similarly fix the ppc32 page_is_ram() function to avoid a conversion to an
    address by directly comparing to max_pfn.  Working with max_pfn instead of
    high_memory fixes page_is_ram() to give the right answer for highmem pages.
    
    Signed-off-by: Roland Dreier <rolandd@cisco.com>
    Cc: Anton Blanchard <anton@samba.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 695db6a588ce..3ca331728d21 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -88,13 +88,13 @@ int page_is_ram(unsigned long pfn)
 }
 EXPORT_SYMBOL(page_is_ram);
 
-pgprot_t phys_mem_access_prot(struct file *file, unsigned long addr,
+pgprot_t phys_mem_access_prot(struct file *file, unsigned long pfn,
 			      unsigned long size, pgprot_t vma_prot)
 {
 	if (ppc_md.phys_mem_access_prot)
-		return ppc_md.phys_mem_access_prot(file, addr, size, vma_prot);
+		return ppc_md.phys_mem_access_prot(file, pfn, size, vma_prot);
 
-	if (!page_is_ram(addr >> PAGE_SHIFT))
+	if (!page_is_ram(pfn))
 		vma_prot = __pgprot(pgprot_val(vma_prot)
 				    | _PAGE_GUARDED | _PAGE_NO_CACHE);
 	return vma_prot;

commit e37bc5df8e96c72f27ec3579499726b656e4e641
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Oct 24 11:41:33 2005 +1000

    [PATCH] powerpc: Purge bootinfo.h
    
    With ARCH=powerpc we assume the presence of a device tree, so we don't
    require any support for the old bi_recs method of passing boot
    parameters.  Likewise, we've never needed it for ppc64, but we still
    had an include/asm-ppc64/bootinfo.h from which nothing was used.  This
    patch removes that file, and all references to it in arch/ppc64 and
    arch/powerpc.  A related, unused variable 'boot_mem_size' is also
    removed from setup_32.c.  The bootinfo stuff remains in ARCH=ppc for
    the time being.
    
    Built and booted on Power5 (ARCH=ppc64 and ARCH=powerpc), built for
    32-bit powermac (ARCH=powerpc and ARCH=ppc).
    
    Signed-off-by: David Gibson <dwg@au1.ibm.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 5e9206715f09..695db6a588ce 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -43,7 +43,6 @@
 #include <asm/machdep.h>
 #include <asm/btext.h>
 #include <asm/tlb.h>
-#include <asm/bootinfo.h>
 #include <asm/prom.h>
 #include <asm/lmb.h>
 #include <asm/sections.h>

commit 985990137e81ca9fd6561cd0f7d1a9695ec57d5a
Merge: 834289447542 63172cb3d5ef
Author: Paul Mackerras <paulus@samba.org>
Date:   Sat Oct 22 16:51:34 2005 +1000

    Merge changes from linux-2.6 by hand

commit ab1f9dac6eea25ee59e4c8e1cf0b7476afbbfe07
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 10 21:58:35 2005 +1000

    powerpc: Merge arch/ppc64/mm to arch/powerpc/mm
    
    This moves the remaining files in arch/ppc64/mm to arch/powerpc/mm,
    and arranges that we use them when compiling with ARCH=ppc64.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 0650de74d0b3..55b5860ed3c9 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -47,6 +47,9 @@
 #include <asm/prom.h>
 #include <asm/lmb.h>
 #include <asm/sections.h>
+#ifdef CONFIG_PPC64
+#include <asm/vdso.h>
+#endif
 
 #include "mmu_decl.h"
 
@@ -334,7 +337,7 @@ void flush_dcache_icache_page(struct page *page)
 	void *start = kmap_atomic(page, KM_PPC_SYNC_ICACHE);
 	__flush_dcache_icache(start);
 	kunmap_atomic(start, KM_PPC_SYNC_ICACHE);
-#elif defined(CONFIG_8xx)
+#elif defined(CONFIG_8xx) || defined(CONFIG_PPC64)
 	/* On 8xx there is no need to kmap since highmem is not supported */
 	__flush_dcache_icache(page_address(page)); 
 #else
@@ -463,18 +466,18 @@ void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
 	if (pgdir == NULL)
 		return;
 
-	ptep = find_linux_pte(pgdir, ea);
+	ptep = find_linux_pte(pgdir, address);
 	if (!ptep)
 		return;
 
-	vsid = get_vsid(vma->vm_mm->context.id, ea);
+	vsid = get_vsid(vma->vm_mm->context.id, address);
 
 	local_irq_save(flags);
 	tmp = cpumask_of_cpu(smp_processor_id());
 	if (cpus_equal(vma->vm_mm->cpu_vm_mask, tmp))
 		local = 1;
 
-	__hash_page(ea, pte_val(pte) & (_PAGE_USER|_PAGE_RW), vsid, ptep,
+	__hash_page(address, pte_val(pte) & (_PAGE_USER|_PAGE_RW), vsid, ptep,
 		    0x300, local);
 	local_irq_restore(flags);
 #endif

commit 7c8c6b9776fb41134d87ef50706a777a45d61cd4
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 6 12:23:33 2005 +1000

    powerpc: Merge lmb.c and make MM initialization use it.
    
    This also creates merged versions of do_init_bootmem, paging_init
    and mem_init and moves them to arch/powerpc/mm/mem.c.  It gets rid
    of the mem_pieces stuff.
    
    I made memory_limit a parameter to lmb_enforce_memory_limit rather
    than a global referenced by that function.  This will require some
    small changes to ppc64 if we want to continue building ARCH=ppc64
    using the merged lmb.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
index 345db08e5d20..0650de74d0b3 100644
--- a/arch/powerpc/mm/mem.c
+++ b/arch/powerpc/mm/mem.c
@@ -45,8 +45,9 @@
 #include <asm/tlb.h>
 #include <asm/bootinfo.h>
 #include <asm/prom.h>
+#include <asm/lmb.h>
+#include <asm/sections.h>
 
-#include "mem_pieces.h"
 #include "mmu_decl.h"
 
 #ifndef CPU_FTR_COHERENT_ICACHE
@@ -54,6 +55,9 @@
 #define CPU_FTR_NOEXECUTE	0
 #endif
 
+int init_bootmem_done;
+int mem_init_done;
+
 /*
  * This is called by /dev/mem to know if a given address has to
  * be mapped non-cacheable or not
@@ -130,6 +134,185 @@ void show_mem(void)
 	printk("%ld pages swap cached\n", cached);
 }
 
+/*
+ * Initialize the bootmem system and give it all the memory we
+ * have available.  If we are using highmem, we only put the
+ * lowmem into the bootmem system.
+ */
+#ifndef CONFIG_NEED_MULTIPLE_NODES
+void __init do_init_bootmem(void)
+{
+	unsigned long i;
+	unsigned long start, bootmap_pages;
+	unsigned long total_pages;
+	int boot_mapsize;
+
+	max_pfn = total_pages = lmb_end_of_DRAM() >> PAGE_SHIFT;
+#ifdef CONFIG_HIGHMEM
+	total_pages = total_lowmem >> PAGE_SHIFT;
+#endif
+
+	/*
+	 * Find an area to use for the bootmem bitmap.  Calculate the size of
+	 * bitmap required as (Total Memory) / PAGE_SIZE / BITS_PER_BYTE.
+	 * Add 1 additional page in case the address isn't page-aligned.
+	 */
+	bootmap_pages = bootmem_bootmap_pages(total_pages);
+
+	start = lmb_alloc(bootmap_pages << PAGE_SHIFT, PAGE_SIZE);
+	BUG_ON(!start);
+
+	boot_mapsize = init_bootmem(start >> PAGE_SHIFT, total_pages);
+
+	/* Add all physical memory to the bootmem map, mark each area
+	 * present.
+	 */
+	for (i = 0; i < lmb.memory.cnt; i++) {
+		unsigned long base = lmb.memory.region[i].base;
+		unsigned long size = lmb_size_bytes(&lmb.memory, i);
+#ifdef CONFIG_HIGHMEM
+		if (base >= total_lowmem)
+			continue;
+		if (base + size > total_lowmem)
+			size = total_lowmem - base;
+#endif
+		free_bootmem(base, size);
+	}
+
+	/* reserve the sections we're already using */
+	for (i = 0; i < lmb.reserved.cnt; i++)
+		reserve_bootmem(lmb.reserved.region[i].base,
+				lmb_size_bytes(&lmb.reserved, i));
+
+	/* XXX need to clip this if using highmem? */
+	for (i = 0; i < lmb.memory.cnt; i++)
+		memory_present(0, lmb_start_pfn(&lmb.memory, i),
+			       lmb_end_pfn(&lmb.memory, i));
+	init_bootmem_done = 1;
+}
+
+/*
+ * paging_init() sets up the page tables - in fact we've already done this.
+ */
+void __init paging_init(void)
+{
+	unsigned long zones_size[MAX_NR_ZONES];
+	unsigned long zholes_size[MAX_NR_ZONES];
+	unsigned long total_ram = lmb_phys_mem_size();
+	unsigned long top_of_ram = lmb_end_of_DRAM();
+
+#ifdef CONFIG_HIGHMEM
+	map_page(PKMAP_BASE, 0, 0);	/* XXX gross */
+	pkmap_page_table = pte_offset_kernel(pmd_offset(pgd_offset_k
+			(PKMAP_BASE), PKMAP_BASE), PKMAP_BASE);
+	map_page(KMAP_FIX_BEGIN, 0, 0);	/* XXX gross */
+	kmap_pte = pte_offset_kernel(pmd_offset(pgd_offset_k
+			(KMAP_FIX_BEGIN), KMAP_FIX_BEGIN), KMAP_FIX_BEGIN);
+	kmap_prot = PAGE_KERNEL;
+#endif /* CONFIG_HIGHMEM */
+
+	printk(KERN_INFO "Top of RAM: 0x%lx, Total RAM: 0x%lx\n",
+	       top_of_ram, total_ram);
+	printk(KERN_INFO "Memory hole size: %ldMB\n",
+	       (top_of_ram - total_ram) >> 20);
+	/*
+	 * All pages are DMA-able so we put them all in the DMA zone.
+	 */
+	memset(zones_size, 0, sizeof(zones_size));
+	memset(zholes_size, 0, sizeof(zholes_size));
+
+	zones_size[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
+	zholes_size[ZONE_DMA] = (top_of_ram - total_ram) >> PAGE_SHIFT;
+
+#ifdef CONFIG_HIGHMEM
+	zones_size[ZONE_DMA] = total_lowmem >> PAGE_SHIFT;
+	zones_size[ZONE_HIGHMEM] = (total_memory - total_lowmem) >> PAGE_SHIFT;
+	zholes_size[ZONE_HIGHMEM] = (top_of_ram - total_ram) >> PAGE_SHIFT;
+#else
+	zones_size[ZONE_DMA] = top_of_ram >> PAGE_SHIFT;
+	zholes_size[ZONE_DMA] = (top_of_ram - total_ram) >> PAGE_SHIFT;
+#endif /* CONFIG_HIGHMEM */
+
+	free_area_init_node(0, NODE_DATA(0), zones_size,
+			    __pa(PAGE_OFFSET) >> PAGE_SHIFT, zholes_size);
+}
+#endif /* ! CONFIG_NEED_MULTIPLE_NODES */
+
+void __init mem_init(void)
+{
+#ifdef CONFIG_NEED_MULTIPLE_NODES
+	int nid;
+#endif
+	pg_data_t *pgdat;
+	unsigned long i;
+	struct page *page;
+	unsigned long reservedpages = 0, codesize, initsize, datasize, bsssize;
+
+	num_physpages = max_pfn;	/* RAM is assumed contiguous */
+	high_memory = (void *) __va(max_low_pfn * PAGE_SIZE);
+
+#ifdef CONFIG_NEED_MULTIPLE_NODES
+        for_each_online_node(nid) {
+		if (NODE_DATA(nid)->node_spanned_pages != 0) {
+			printk("freeing bootmem node %x\n", nid);
+			totalram_pages +=
+				free_all_bootmem_node(NODE_DATA(nid));
+		}
+	}
+#else
+	max_mapnr = num_physpages;
+	totalram_pages += free_all_bootmem();
+#endif
+	for_each_pgdat(pgdat) {
+		for (i = 0; i < pgdat->node_spanned_pages; i++) {
+			page = pgdat_page_nr(pgdat, i);
+			if (PageReserved(page))
+				reservedpages++;
+		}
+	}
+
+	codesize = (unsigned long)&_sdata - (unsigned long)&_stext;
+	datasize = (unsigned long)&__init_begin - (unsigned long)&_sdata;
+	initsize = (unsigned long)&__init_end - (unsigned long)&__init_begin;
+	bsssize = (unsigned long)&__bss_stop - (unsigned long)&__bss_start;
+
+#ifdef CONFIG_HIGHMEM
+	{
+		unsigned long pfn, highmem_mapnr;
+
+		highmem_mapnr = total_lowmem >> PAGE_SHIFT;
+		for (pfn = highmem_mapnr; pfn < max_mapnr; ++pfn) {
+			struct page *page = pfn_to_page(pfn);
+
+			ClearPageReserved(page);
+			set_page_count(page, 1);
+			__free_page(page);
+			totalhigh_pages++;
+		}
+		totalram_pages += totalhigh_pages;
+		printk(KERN_INFO "High memory: %luk\n",
+		       totalhigh_pages << (PAGE_SHIFT-10));
+	}
+#endif /* CONFIG_HIGHMEM */
+
+	printk(KERN_INFO "Memory: %luk/%luk available (%luk kernel code, "
+	       "%luk reserved, %luk data, %luk bss, %luk init)\n",
+		(unsigned long)nr_free_pages() << (PAGE_SHIFT-10),
+		num_physpages << (PAGE_SHIFT-10),
+		codesize >> 10,
+		reservedpages << (PAGE_SHIFT-10),
+		datasize >> 10,
+		bsssize >> 10,
+		initsize >> 10);
+
+	mem_init_done = 1;
+
+#ifdef CONFIG_PPC64
+	/* Initialize the vDSO */
+	vdso_init();
+#endif
+}
+
 /*
  * This is called when a page has been modified by the kernel.
  * It just marks the page as not i-cache clean.  We do the i-cache

commit 14cf11af6cf608eb8c23e989ddb17a715ddce109
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Sep 26 16:04:21 2005 +1000

    powerpc: Merge enough to start building in arch/powerpc.
    
    This creates the directory structure under arch/powerpc and a bunch
    of Kconfig files.  It does a first-cut merge of arch/powerpc/mm,
    arch/powerpc/lib and arch/powerpc/platforms/powermac.  This is enough
    to build a 32-bit powermac kernel with ARCH=powerpc.
    
    For now we are getting some unmerged files from arch/ppc/kernel and
    arch/ppc/syslib, or arch/ppc64/kernel.  This makes some minor changes
    to files in those directories and files outside arch/powerpc.
    
    The boot directory is still not merged.  That's going to be interesting.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mem.c b/arch/powerpc/mm/mem.c
new file mode 100644
index 000000000000..345db08e5d20
--- /dev/null
+++ b/arch/powerpc/mm/mem.c
@@ -0,0 +1,299 @@
+/*
+ *  PowerPC version
+ *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
+ *
+ *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)
+ *  and Cort Dougan (PReP) (cort@cs.nmt.edu)
+ *    Copyright (C) 1996 Paul Mackerras
+ *  Amiga/APUS changes by Jesper Skov (jskov@cygnus.co.uk).
+ *  PPC44x/36-bit changes by Matt Porter (mporter@mvista.com)
+ *
+ *  Derived from "arch/i386/mm/init.c"
+ *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; either version
+ *  2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/mm.h>
+#include <linux/stddef.h>
+#include <linux/init.h>
+#include <linux/bootmem.h>
+#include <linux/highmem.h>
+#include <linux/initrd.h>
+#include <linux/pagemap.h>
+
+#include <asm/pgalloc.h>
+#include <asm/prom.h>
+#include <asm/io.h>
+#include <asm/mmu_context.h>
+#include <asm/pgtable.h>
+#include <asm/mmu.h>
+#include <asm/smp.h>
+#include <asm/machdep.h>
+#include <asm/btext.h>
+#include <asm/tlb.h>
+#include <asm/bootinfo.h>
+#include <asm/prom.h>
+
+#include "mem_pieces.h"
+#include "mmu_decl.h"
+
+#ifndef CPU_FTR_COHERENT_ICACHE
+#define CPU_FTR_COHERENT_ICACHE	0	/* XXX for now */
+#define CPU_FTR_NOEXECUTE	0
+#endif
+
+/*
+ * This is called by /dev/mem to know if a given address has to
+ * be mapped non-cacheable or not
+ */
+int page_is_ram(unsigned long pfn)
+{
+	unsigned long paddr = (pfn << PAGE_SHIFT);
+
+#ifndef CONFIG_PPC64	/* XXX for now */
+	return paddr < __pa(high_memory);
+#else
+	int i;
+	for (i=0; i < lmb.memory.cnt; i++) {
+		unsigned long base;
+
+		base = lmb.memory.region[i].base;
+
+		if ((paddr >= base) &&
+			(paddr < (base + lmb.memory.region[i].size))) {
+			return 1;
+		}
+	}
+
+	return 0;
+#endif
+}
+EXPORT_SYMBOL(page_is_ram);
+
+pgprot_t phys_mem_access_prot(struct file *file, unsigned long addr,
+			      unsigned long size, pgprot_t vma_prot)
+{
+	if (ppc_md.phys_mem_access_prot)
+		return ppc_md.phys_mem_access_prot(file, addr, size, vma_prot);
+
+	if (!page_is_ram(addr >> PAGE_SHIFT))
+		vma_prot = __pgprot(pgprot_val(vma_prot)
+				    | _PAGE_GUARDED | _PAGE_NO_CACHE);
+	return vma_prot;
+}
+EXPORT_SYMBOL(phys_mem_access_prot);
+
+void show_mem(void)
+{
+	unsigned long total = 0, reserved = 0;
+	unsigned long shared = 0, cached = 0;
+	unsigned long highmem = 0;
+	struct page *page;
+	pg_data_t *pgdat;
+	unsigned long i;
+
+	printk("Mem-info:\n");
+	show_free_areas();
+	printk("Free swap:       %6ldkB\n", nr_swap_pages<<(PAGE_SHIFT-10));
+	for_each_pgdat(pgdat) {
+		for (i = 0; i < pgdat->node_spanned_pages; i++) {
+			page = pgdat_page_nr(pgdat, i);
+			total++;
+			if (PageHighMem(page))
+				highmem++;
+			if (PageReserved(page))
+				reserved++;
+			else if (PageSwapCache(page))
+				cached++;
+			else if (page_count(page))
+				shared += page_count(page) - 1;
+		}
+	}
+	printk("%ld pages of RAM\n", total);
+#ifdef CONFIG_HIGHMEM
+	printk("%ld pages of HIGHMEM\n", highmem);
+#endif
+	printk("%ld reserved pages\n", reserved);
+	printk("%ld pages shared\n", shared);
+	printk("%ld pages swap cached\n", cached);
+}
+
+/*
+ * This is called when a page has been modified by the kernel.
+ * It just marks the page as not i-cache clean.  We do the i-cache
+ * flush later when the page is given to a user process, if necessary.
+ */
+void flush_dcache_page(struct page *page)
+{
+	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
+		return;
+	/* avoid an atomic op if possible */
+	if (test_bit(PG_arch_1, &page->flags))
+		clear_bit(PG_arch_1, &page->flags);
+}
+EXPORT_SYMBOL(flush_dcache_page);
+
+void flush_dcache_icache_page(struct page *page)
+{
+#ifdef CONFIG_BOOKE
+	void *start = kmap_atomic(page, KM_PPC_SYNC_ICACHE);
+	__flush_dcache_icache(start);
+	kunmap_atomic(start, KM_PPC_SYNC_ICACHE);
+#elif defined(CONFIG_8xx)
+	/* On 8xx there is no need to kmap since highmem is not supported */
+	__flush_dcache_icache(page_address(page)); 
+#else
+	__flush_dcache_icache_phys(page_to_pfn(page) << PAGE_SHIFT);
+#endif
+
+}
+void clear_user_page(void *page, unsigned long vaddr, struct page *pg)
+{
+	clear_page(page);
+
+	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
+		return;
+	/*
+	 * We shouldnt have to do this, but some versions of glibc
+	 * require it (ld.so assumes zero filled pages are icache clean)
+	 * - Anton
+	 */
+
+	/* avoid an atomic op if possible */
+	if (test_bit(PG_arch_1, &pg->flags))
+		clear_bit(PG_arch_1, &pg->flags);
+}
+EXPORT_SYMBOL(clear_user_page);
+
+void copy_user_page(void *vto, void *vfrom, unsigned long vaddr,
+		    struct page *pg)
+{
+	copy_page(vto, vfrom);
+
+	/*
+	 * We should be able to use the following optimisation, however
+	 * there are two problems.
+	 * Firstly a bug in some versions of binutils meant PLT sections
+	 * were not marked executable.
+	 * Secondly the first word in the GOT section is blrl, used
+	 * to establish the GOT address. Until recently the GOT was
+	 * not marked executable.
+	 * - Anton
+	 */
+#if 0
+	if (!vma->vm_file && ((vma->vm_flags & VM_EXEC) == 0))
+		return;
+#endif
+
+	if (cpu_has_feature(CPU_FTR_COHERENT_ICACHE))
+		return;
+
+	/* avoid an atomic op if possible */
+	if (test_bit(PG_arch_1, &pg->flags))
+		clear_bit(PG_arch_1, &pg->flags);
+}
+
+void flush_icache_user_range(struct vm_area_struct *vma, struct page *page,
+			     unsigned long addr, int len)
+{
+	unsigned long maddr;
+
+	maddr = (unsigned long) kmap(page) + (addr & ~PAGE_MASK);
+	flush_icache_range(maddr, maddr + len);
+	kunmap(page);
+}
+EXPORT_SYMBOL(flush_icache_user_range);
+
+/*
+ * This is called at the end of handling a user page fault, when the
+ * fault has been handled by updating a PTE in the linux page tables.
+ * We use it to preload an HPTE into the hash table corresponding to
+ * the updated linux PTE.
+ * 
+ * This must always be called with the mm->page_table_lock held
+ */
+void update_mmu_cache(struct vm_area_struct *vma, unsigned long address,
+		      pte_t pte)
+{
+	/* handle i-cache coherency */
+	unsigned long pfn = pte_pfn(pte);
+#ifdef CONFIG_PPC32
+	pmd_t *pmd;
+#else
+	unsigned long vsid;
+	void *pgdir;
+	pte_t *ptep;
+	int local = 0;
+	cpumask_t tmp;
+	unsigned long flags;
+#endif
+
+	/* handle i-cache coherency */
+	if (!cpu_has_feature(CPU_FTR_COHERENT_ICACHE) &&
+	    !cpu_has_feature(CPU_FTR_NOEXECUTE) &&
+	    pfn_valid(pfn)) {
+		struct page *page = pfn_to_page(pfn);
+		if (!PageReserved(page)
+		    && !test_bit(PG_arch_1, &page->flags)) {
+			if (vma->vm_mm == current->active_mm) {
+#ifdef CONFIG_8xx
+			/* On 8xx, cache control instructions (particularly 
+		 	 * "dcbst" from flush_dcache_icache) fault as write 
+			 * operation if there is an unpopulated TLB entry 
+			 * for the address in question. To workaround that, 
+			 * we invalidate the TLB here, thus avoiding dcbst 
+			 * misbehaviour.
+			 */
+				_tlbie(address);
+#endif
+				__flush_dcache_icache((void *) address);
+			} else
+				flush_dcache_icache_page(page);
+			set_bit(PG_arch_1, &page->flags);
+		}
+	}
+
+#ifdef CONFIG_PPC_STD_MMU
+	/* We only want HPTEs for linux PTEs that have _PAGE_ACCESSED set */
+	if (!pte_young(pte) || address >= TASK_SIZE)
+		return;
+#ifdef CONFIG_PPC32
+	if (Hash == 0)
+		return;
+	pmd = pmd_offset(pgd_offset(vma->vm_mm, address), address);
+	if (!pmd_none(*pmd))
+		add_hash_page(vma->vm_mm->context, address, pmd_val(*pmd));
+#else
+	pgdir = vma->vm_mm->pgd;
+	if (pgdir == NULL)
+		return;
+
+	ptep = find_linux_pte(pgdir, ea);
+	if (!ptep)
+		return;
+
+	vsid = get_vsid(vma->vm_mm->context.id, ea);
+
+	local_irq_save(flags);
+	tmp = cpumask_of_cpu(smp_processor_id());
+	if (cpus_equal(vma->vm_mm->cpu_vm_mask, tmp))
+		local = 1;
+
+	__hash_page(ea, pte_val(pte) & (_PAGE_USER|_PAGE_RW), vsid, ptep,
+		    0x300, local);
+	local_irq_restore(flags);
+#endif
+#endif
+}
