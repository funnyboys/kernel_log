commit 136a9a0f74d2e0d9de5515190fe80344b86b45cf
Author: Christophe Leroy <christophe.leroy@csgroup.eu>
Date:   Tue May 19 05:49:14 2020 +0000

    powerpc/8xx: Don't set IMMR map anymore at boot
    
    Only early debug requires IMMR to be mapped early.
    
    No need to set it up and pin it in assembly. Map it
    through page tables at udbg init when necessary.
    
    If CONFIG_PIN_TLB_IMMR is selected, pin it once we
    don't need the 32 Mb pinned RAM anymore.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@csgroup.eu>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/13c1e8539fdf363d3146f4884e5c3c76c6c308b5.1589866984.git.christophe.leroy@csgroup.eu

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 7097e07a209a..1b6d39e9baed 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -182,6 +182,10 @@ static inline void mmu_mark_initmem_nx(void) { }
 static inline void mmu_mark_rodata_ro(void) { }
 #endif
 
+#ifdef CONFIG_PPC_8xx
+void __init mmu_mapin_immr(void);
+#endif
+
 #ifdef CONFIG_PPC_DEBUG_WX
 void ptdump_check_wx(void);
 #else

commit 1e1c8b2cc37afb333c1829e8e0360321813bf220
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Jan 14 07:14:40 2020 +0000

    powerpc/ptdump: don't entirely rebuild kernel when selecting CONFIG_PPC_DEBUG_WX
    
    Selecting CONFIG_PPC_DEBUG_WX only impacts ptdump and pgtable_32/64
    init calls. Declaring related functions in asm/pgtable.h implies
    rebuilding almost everything.
    
    Move ptdump_check_wx() declaration in mm/mmu_decl.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/bf34fd9dca61eadf9a134a9f89ebbc162cfd5f86.1578986011.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 8e99649c24fc..7097e07a209a 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -181,3 +181,9 @@ void mmu_mark_rodata_ro(void);
 static inline void mmu_mark_initmem_nx(void) { }
 static inline void mmu_mark_rodata_ro(void) { }
 #endif
+
+#ifdef CONFIG_PPC_DEBUG_WX
+void ptdump_check_wx(void);
+#else
+static inline void ptdump_check_wx(void) { }
+#endif

commit b39609720069f5a6eed2b3e3f618c23587021ff5
Author: Jason Yan <yanaijie@huawei.com>
Date:   Fri Sep 20 17:45:42 2019 +0800

    powerpc/fsl_booke/kaslr: clear the original kernel if randomized
    
    The original kernel still exists in the memory, clear it now.
    
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Diana Craciun <diana.craciun@nxp.com>
    Tested-by: Diana Craciun <diana.craciun@nxp.com>
    Signed-off-by: Scott Wood <oss@buserror.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index ae06c5675abb..8e99649c24fc 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -148,8 +148,10 @@ extern void loadcam_multi(int first_idx, int num, int tmp_idx);
 
 #ifdef CONFIG_RANDOMIZE_BASE
 void kaslr_early_init(void *dt_ptr, phys_addr_t size);
+void kaslr_late_init(void);
 #else
 static inline void kaslr_early_init(void *dt_ptr, phys_addr_t size) {}
+static inline void kaslr_late_init(void) {}
 #endif
 
 struct tlbcam {

commit 2b0e86cc5de6dabadc2d64cefa429fc227c8a756
Author: Jason Yan <yanaijie@huawei.com>
Date:   Fri Sep 20 17:45:40 2019 +0800

    powerpc/fsl_booke/32: implement KASLR infrastructure
    
    This patch add support to boot kernel from places other than KERNELBASE.
    Since CONFIG_RELOCATABLE has already supported, what we need to do is
    map or copy kernel to a proper place and relocate. Freescale Book-E
    parts expect lowmem to be mapped by fixed TLB entries(TLB1). The TLB1
    entries are not suitable to map the kernel directly in a randomized
    region, so we chose to copy the kernel to a proper place and restart to
    relocate.
    
    The offset of the kernel was not randomized yet(a fixed 64M is set). We
    will randomize it in the next patch.
    
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Tested-by: Diana Craciun <diana.craciun@nxp.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>
    [mpe: Use PTRRELOC() in early_init()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index bbfd185fb99a..ae06c5675abb 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -141,10 +141,17 @@ extern int switch_to_as1(void);
 extern void restore_to_as0(int esel, int offset, void *dt_ptr, int bootcpu);
 void create_kaslr_tlb_entry(int entry, unsigned long virt, phys_addr_t phys);
 void reloc_kernel_entry(void *fdt, int addr);
+extern int is_second_reloc;
 #endif
 extern void loadcam_entry(unsigned int index);
 extern void loadcam_multi(int first_idx, int num, int tmp_idx);
 
+#ifdef CONFIG_RANDOMIZE_BASE
+void kaslr_early_init(void *dt_ptr, phys_addr_t size);
+#else
+static inline void kaslr_early_init(void *dt_ptr, phys_addr_t size) {}
+#endif
+
 struct tlbcam {
 	u32	MAS0;
 	u32	MAS1;

commit c061b38a3e48663c29611e3b60afffe624d7c830
Author: Jason Yan <yanaijie@huawei.com>
Date:   Fri Sep 20 17:45:39 2019 +0800

    powerpc/fsl_booke/32: introduce reloc_kernel_entry() helper
    
    Add a new helper reloc_kernel_entry() to jump back to the start of the
    new kernel. After we put the new kernel in a randomized place we can use
    this new helper to enter the kernel and begin to relocate again.
    
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Diana Craciun <diana.craciun@nxp.com>
    Tested-by: Diana Craciun <diana.craciun@nxp.com>
    Signed-off-by: Scott Wood <oss@buserror.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 9da261ad54c3..bbfd185fb99a 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -140,6 +140,7 @@ extern void adjust_total_lowmem(void);
 extern int switch_to_as1(void);
 extern void restore_to_as0(int esel, int offset, void *dt_ptr, int bootcpu);
 void create_kaslr_tlb_entry(int entry, unsigned long virt, phys_addr_t phys);
+void reloc_kernel_entry(void *fdt, int addr);
 #endif
 extern void loadcam_entry(unsigned int index);
 extern void loadcam_multi(int first_idx, int num, int tmp_idx);

commit aa1d2090e69311c65f69c0fa2311d1d0f01c55f8
Author: Jason Yan <yanaijie@huawei.com>
Date:   Fri Sep 20 17:45:38 2019 +0800

    powerpc/fsl_booke/32: introduce create_kaslr_tlb_entry() helper
    
    Add a new helper create_kaslr_tlb_entry() to create a tlb entry by the
    virtual and physical address. This is a preparation to support boot kernel
    at a randomized address.
    
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Reviewed-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Diana Craciun <diana.craciun@nxp.com>
    Tested-by: Diana Craciun <diana.craciun@nxp.com>
    Signed-off-by: Scott Wood <oss@buserror.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index c750ac9ec713..9da261ad54c3 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -139,6 +139,7 @@ extern unsigned long calc_cam_sz(unsigned long ram, unsigned long virt,
 extern void adjust_total_lowmem(void);
 extern int switch_to_as1(void);
 extern void restore_to_as0(int esel, int offset, void *dt_ptr, int bootcpu);
+void create_kaslr_tlb_entry(int entry, unsigned long virt, phys_addr_t phys);
 #endif
 extern void loadcam_entry(unsigned int index);
 extern void loadcam_multi(int first_idx, int num, int tmp_idx);

commit 7cd9b317b630683b0c8eb2dfcfb046003ad6b97b
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Aug 20 14:07:16 2019 +0000

    powerpc/mm: make ioremap_bot common to all
    
    Drop multiple definitions of ioremap_bot and make one common to
    all subarches.
    
    Only CONFIG_PPC_BOOK3E_64 had a global static init value for
    ioremap_bot. Now ioremap_bot is set in early_init_mmu_global().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/920eebfd9f36f14c79d1755847f5bf7c83703bdd.1566309262.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index adbaf2167214..c750ac9ec713 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -106,7 +106,6 @@ extern u8 early_hash[];
 
 #endif /* CONFIG_PPC32 */
 
-extern unsigned long ioremap_bot;
 extern unsigned long __max_low_memory;
 extern phys_addr_t __initial_memory_limit_addr;
 extern phys_addr_t total_memory;

commit f49f4e2b68b683491263e92c229ff344d44759a7
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Aug 16 05:41:43 2019 +0000

    powerpc/mm: Simplify update_mmu_cache() on BOOK3S32
    
    On BOOK3S32, hash_preload() neither use is_exec nor trap,
    so drop those parameters and simplify update_mmu_cached().
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/35f143c6fe29f9fd25c7f3cd4448ae401029ce3c.1565933217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 9f325a7a09cb..adbaf2167214 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -91,8 +91,7 @@ void print_system_hash_info(void);
 
 #ifdef CONFIG_PPC32
 
-void hash_preload(struct mm_struct *mm, unsigned long ea,
-		  bool is_exec, unsigned long trap);
+void hash_preload(struct mm_struct *mm, unsigned long ea);
 
 extern void mapin_ram(void);
 extern void setbat(int index, unsigned long virt, phys_addr_t phys,

commit e5a1edb9fe4cfa07e37a59475f8f7d0a8939c73e
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Aug 16 05:41:42 2019 +0000

    powerpc/mm: move update_mmu_cache() into book3s hash utils.
    
    update_mmu_cache() is only for BOOK3S, and can be simplified for
    BOOK3S32.
    
    Move it out of mem.c into respective BOOK3S32 and BOOK3S64 files
    containing hash utils.
    
    BOOK3S64 version of hash_preload() is only used locally, declare it
    static.
    
    Remove the radix_enabled() stuff in BOOK3S32 version.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/107aaf43583a5f5d09e0d4e84c4c4390ecfcd512.1565933217.git.christophe.leroy@c-s.fr

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 32c1a191c28a..9f325a7a09cb 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -82,10 +82,6 @@ static inline void print_system_hash_info(void) {}
 
 #else /* CONFIG_PPC_MMU_NOHASH */
 
-extern void hash_preload(struct mm_struct *mm, unsigned long ea,
-			 bool is_exec, unsigned long trap);
-
-
 extern void _tlbie(unsigned long address);
 extern void _tlbia(void);
 
@@ -95,6 +91,9 @@ void print_system_hash_info(void);
 
 #ifdef CONFIG_PPC32
 
+void hash_preload(struct mm_struct *mm, unsigned long ea,
+		  bool is_exec, unsigned long trap);
+
 extern void mapin_ram(void);
 extern void setbat(int index, unsigned long virt, phys_addr_t phys,
 		   unsigned int size, pgprot_t prot);

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 7bac0aa2026a..32c1a191c28a 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
 /*
  * Declarations of procedures and variables shared between files
  * in arch/ppc/mm/.
@@ -11,12 +12,6 @@
  *
  *  Derived from "arch/i386/mm/init.c"
  *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds
- *
- *  This program is free software; you can redistribute it and/or
- *  modify it under the terms of the GNU General Public License
- *  as published by the Free Software Foundation; either version
- *  2 of the License, or (at your option) any later version.
- *
  */
 #include <linux/mm.h>
 #include <asm/mmu.h>

commit e4dccf9092ab48a6f902003b9558c0e45d0e849a
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:36:39 2019 +0000

    powerpc/mm: print hash info in a helper
    
    Reduce #ifdef mess by defining a helper to print
    hash info at startup.
    
    In the meantime, remove the display of hash table address
    to reduce leak of non necessary information.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 7b8833d695d1..7bac0aa2026a 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -83,6 +83,8 @@ static inline void _tlbivax_bcast(unsigned long address, unsigned int pid,
 }
 #endif
 
+static inline void print_system_hash_info(void) {}
+
 #else /* CONFIG_PPC_MMU_NOHASH */
 
 extern void hash_preload(struct mm_struct *mm, unsigned long ea,
@@ -92,6 +94,8 @@ extern void hash_preload(struct mm_struct *mm, unsigned long ea,
 extern void _tlbie(unsigned long address);
 extern void _tlbia(void);
 
+void print_system_hash_info(void);
+
 #endif /* CONFIG_PPC_MMU_NOHASH */
 
 #ifdef CONFIG_PPC32
@@ -105,7 +109,6 @@ extern unsigned int rtas_data, rtas_size;
 
 struct hash_pte;
 extern struct hash_pte *Hash;
-extern unsigned long Hash_size, Hash_mask;
 extern u8 early_hash[];
 
 #endif /* CONFIG_PPC32 */

commit 57e0491b58fa2a217029b696511499008852a642
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:36:36 2019 +0000

    powerpc/32s: drop Hash_end
    
    Hash_end has never been used, drop it.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 31fce3914ddc..7b8833d695d1 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -104,7 +104,7 @@ extern int __map_without_bats;
 extern unsigned int rtas_data, rtas_size;
 
 struct hash_pte;
-extern struct hash_pte *Hash, *Hash_end;
+extern struct hash_pte *Hash;
 extern unsigned long Hash_size, Hash_mask;
 extern u8 early_hash[];
 

commit 215b823707ce4e8e52b106915f70357fa474c669
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:23:36 2019 +0000

    powerpc/32s: set up an early static hash table for KASAN.
    
    KASAN requires early activation of hash table, before memblock()
    functions are available.
    
    This patch implements an early hash_table statically defined in
    __initdata.
    
    During early boot, a single page table is used.
    
    For hash32, when doing the final init, one page table is allocated
    for each PGD entry because of the _PAGE_HASHPTE flag which can't be
    common to several virt pages. This is done after memblock get
    available but before switching to the final hash table, otherwise
    there are issues with TLB flushing due to the shared entries.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index d726ff776054..31fce3914ddc 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -106,6 +106,7 @@ extern unsigned int rtas_data, rtas_size;
 struct hash_pte;
 extern struct hash_pte *Hash, *Hash_end;
 extern unsigned long Hash_size, Hash_mask;
+extern u8 early_hash[];
 
 #endif /* CONFIG_PPC32 */
 

commit 72f208c6a8f7bc78ef5248babd9e6ed6302bd2a0
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Apr 26 16:23:35 2019 +0000

    powerpc/32s: move hash code patching out of MMU_init_hw()
    
    For KASAN, hash table handling will be activated early for
    accessing to KASAN shadow areas.
    
    In order to avoid any modification of the hash functions while
    they are still used with the early hash table, the code patching
    is moved out of MMU_init_hw() and put close to the big-bang switch
    to the final hash table.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 74ff61dabcb1..d726ff776054 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -130,6 +130,7 @@ extern void wii_memory_fixups(void);
  */
 #ifdef CONFIG_PPC32
 extern void MMU_init_hw(void);
+void MMU_init_hw_patch(void);
 unsigned long mmu_mapin_ram(unsigned long base, unsigned long top);
 #endif
 

commit d5f17ee96447736a84bc44ffc4b0dddb1b519222
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 19:08:51 2019 +0000

    powerpc/8xx: don't disable large TLBs with CONFIG_STRICT_KERNEL_RWX
    
    This patch implements handling of STRICT_KERNEL_RWX with
    large TLBs directly in the TLB miss handlers.
    
    To do so, etext and sinittext are aligned on 512kB boundaries
    and the miss handlers use 512kB pages instead of 8Mb pages for
    addresses close to the boundaries.
    
    It sets RO PP flags for addresses under sinittext.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 98fc94affc29..74ff61dabcb1 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -166,7 +166,7 @@ static inline phys_addr_t v_block_mapped(unsigned long va) { return 0; }
 static inline unsigned long p_block_mapped(phys_addr_t pa) { return 0; }
 #endif
 
-#if defined(CONFIG_PPC_BOOK3S_32)
+#if defined(CONFIG_PPC_BOOK3S_32) || defined(CONFIG_PPC_8xx)
 void mmu_mark_initmem_nx(void);
 void mmu_mark_rodata_ro(void);
 #else

commit 63b2bc619565ef7078e7b12fafb82f51867f002b
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 19:08:49 2019 +0000

    powerpc/mm/32s: Use BATs for STRICT_KERNEL_RWX
    
    Today, STRICT_KERNEL_RWX is based on the use of regular pages
    to map kernel pages.
    
    On Book3s 32, it has three consequences:
    - Using pages instead of BAT for mapping kernel linear memory severely
    impacts performance.
    - Exec protection is not effective because no-execute cannot be set at
    page level (except on 603 which doesn't have hash tables)
    - Write protection is not effective because PP bits do not provide RO
    mode for kernel-only pages (except on 603 which handles it in software
    via PAGE_DIRTY)
    
    On the 603+, we have:
    - Independent IBAT and DBAT allowing limitation of exec parts.
    - NX bit can be set in segment registers to forbit execution on memory
    mapped by pages.
    - RO mode on DBATs even for kernel-only blocks.
    
    On the 601, there is nothing much we can do other than warn the user
    about it, because:
    - BATs are common to instructions and data.
    - BAT do not provide RO mode for kernel-only blocks.
    - segment registers don't have the NX bit.
    
    In order to use IBAT for exec protection, this patch:
    - Aligns _etext to BAT block sizes (128kb)
    - Set NX bit in kernel segment register (Except on vmalloc area when
    CONFIG_MODULES is selected)
    - Maps kernel text with IBATs.
    
    In order to use DBAT for exec protection, this patch:
    - Aligns RW DATA to BAT block sizes (4M)
    - Maps kernel RO area with write prohibited DBATs
    - Maps remaining memory with remaining DBATs
    
    Here is what we get with this patch on a 832x when activating
    STRICT_KERNEL_RWX:
    
    Symbols:
    c0000000 T _stext
    c0680000 R __start_rodata
    c0680000 R _etext
    c0800000 T __init_begin
    c0800000 T _sinittext
    
    ~# cat /sys/kernel/debug/block_address_translation
    ---[ Instruction Block Address Translation ]---
    0: 0xc0000000-0xc03fffff 0x00000000 Kernel EXEC coherent
    1: 0xc0400000-0xc05fffff 0x00400000 Kernel EXEC coherent
    2: 0xc0600000-0xc067ffff 0x00600000 Kernel EXEC coherent
    3:         -
    4:         -
    5:         -
    6:         -
    7:         -
    
    ---[ Data Block Address Translation ]---
    0: 0xc0000000-0xc07fffff 0x00000000 Kernel RO coherent
    1: 0xc0800000-0xc0ffffff 0x00800000 Kernel RW coherent
    2: 0xc1000000-0xc1ffffff 0x01000000 Kernel RW coherent
    3: 0xc2000000-0xc3ffffff 0x02000000 Kernel RW coherent
    4: 0xc4000000-0xc7ffffff 0x04000000 Kernel RW coherent
    5: 0xc8000000-0xcfffffff 0x08000000 Kernel RW coherent
    6: 0xd0000000-0xdfffffff 0x10000000 Kernel RW coherent
    7:         -
    
    ~# cat /sys/kernel/debug/segment_registers
    ---[ User Segments ]---
    0x00000000-0x0fffffff Kern key 1 User key 1 VSID 0xa085d0
    0x10000000-0x1fffffff Kern key 1 User key 1 VSID 0xa086e1
    0x20000000-0x2fffffff Kern key 1 User key 1 VSID 0xa087f2
    0x30000000-0x3fffffff Kern key 1 User key 1 VSID 0xa08903
    0x40000000-0x4fffffff Kern key 1 User key 1 VSID 0xa08a14
    0x50000000-0x5fffffff Kern key 1 User key 1 VSID 0xa08b25
    0x60000000-0x6fffffff Kern key 1 User key 1 VSID 0xa08c36
    0x70000000-0x7fffffff Kern key 1 User key 1 VSID 0xa08d47
    0x80000000-0x8fffffff Kern key 1 User key 1 VSID 0xa08e58
    0x90000000-0x9fffffff Kern key 1 User key 1 VSID 0xa08f69
    0xa0000000-0xafffffff Kern key 1 User key 1 VSID 0xa0907a
    0xb0000000-0xbfffffff Kern key 1 User key 1 VSID 0xa0918b
    
    ---[ Kernel Segments ]---
    0xc0000000-0xcfffffff Kern key 0 User key 1 No Exec VSID 0x000ccc
    0xd0000000-0xdfffffff Kern key 0 User key 1 No Exec VSID 0x000ddd
    0xe0000000-0xefffffff Kern key 0 User key 1 No Exec VSID 0x000eee
    0xf0000000-0xffffffff Kern key 0 User key 1 No Exec VSID 0x000fff
    
    Aligning _etext to 128kb allows to map up to 32Mb text with 8 IBATs:
    16Mb + 8Mb + 4Mb + 2Mb + 1Mb + 512kb + 256kb + 128kb (+ 128kb) = 32Mb
    (A 9th IBAT is unneeded as 32Mb would need only a single 32Mb block)
    
    Aligning data to 4M allows to map up to 512Mb data with 8 DBATs:
    16Mb + 8Mb + 4Mb + 4Mb + 32Mb + 64Mb + 128Mb + 256Mb = 512Mb
    
    Because some processors only have 4 BATs and because some targets need
    DBATs for mapping other areas, the following patch will allow to
    modify _etext and data alignment.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 61730023dde3..98fc94affc29 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -165,3 +165,11 @@ unsigned long p_block_mapped(phys_addr_t pa);
 static inline phys_addr_t v_block_mapped(unsigned long va) { return 0; }
 static inline unsigned long p_block_mapped(phys_addr_t pa) { return 0; }
 #endif
+
+#if defined(CONFIG_PPC_BOOK3S_32)
+void mmu_mark_initmem_nx(void);
+void mmu_mark_rodata_ro(void);
+#else
+static inline void mmu_mark_initmem_nx(void) { }
+static inline void mmu_mark_rodata_ro(void) { }
+#endif

commit 14e609d693ef678a211a8dcd0e13463a2581ed85
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Feb 21 19:08:38 2019 +0000

    powerpc/mm/32: add base address to mmu_mapin_ram()
    
    At the time being, mmu_mapin_ram() always maps RAM from the beginning.
    But some platforms like the WII have to map a second block of RAM.
    
    This patch adds to mmu_mapin_ram() the base address of the block.
    At the moment, only base address 0 is supported.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index c4a717da65eb..61730023dde3 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -130,7 +130,7 @@ extern void wii_memory_fixups(void);
  */
 #ifdef CONFIG_PPC32
 extern void MMU_init_hw(void);
-extern unsigned long mmu_mapin_ram(unsigned long top);
+unsigned long mmu_mapin_ram(unsigned long base, unsigned long top);
 #endif
 
 #ifdef CONFIG_PPC_FSL_BOOK3E

commit d7cceda96badc1bd444cff27ab9c375a1277c1e3
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Nov 17 10:24:56 2018 +0000

    powerpc: change CONFIG_6xx to CONFIG_PPC_BOOK3S_32
    
    Today we have:
    
    config PPC_BOOK3S_32
            bool "512x/52xx/6xx/7xx/74xx/82xx/83xx/86xx"
            [depends on PPC32 within a choice]
    
    config PPC_BOOK3S
            def_bool y
            depends on PPC_BOOK3S_32 || PPC_BOOK3S_64
    
    config 6xx
            def_bool y
            depends on PPC32 && PPC_BOOK3S
    
    6xx is therefore redundant with PPC_BOOK3S_32.
    
    In order to make the code clearer, lets use preferably PPC_BOOK3S_32.
    This will allow to remove CONFIG_6xx in a later patch.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 8574fbbc45e0..c4a717da65eb 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -155,7 +155,7 @@ struct tlbcam {
 };
 #endif
 
-#if defined(CONFIG_6xx) || defined(CONFIG_FSL_BOOKE) || defined(CONFIG_PPC_8xx)
+#if defined(CONFIG_PPC_BOOK3S_32) || defined(CONFIG_FSL_BOOKE) || defined(CONFIG_PPC_8xx)
 /* 6xx have BATS */
 /* FSL_BOOKE have TLBCAM */
 /* 8xx have LTLB */

commit 8114c36ea6486aba2269d0590c5d553108ee9558
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Mar 21 15:17:00 2018 +0100

    powerpc/mm: Trace tlbia instruction
    
    Add a trace point for tlbia (Translation Lookaside Buffer Invalidate
    All) instruction.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 1db2027a0110..8574fbbc45e0 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -31,10 +31,12 @@
 static inline void _tlbil_all(void)
 {
 	asm volatile ("sync; tlbia; isync" : : : "memory");
+	trace_tlbia(MMU_NO_CONTEXT);
 }
 static inline void _tlbil_pid(unsigned int pid)
 {
 	asm volatile ("sync; tlbia; isync" : : : "memory");
+	trace_tlbia(pid);
 }
 #define _tlbil_pid_noind(pid)	_tlbil_pid(pid)
 

commit cf4a6085151ae3f4e78dd91981833e65aaae8bc6
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Wed Mar 21 15:16:58 2018 +0100

    powerpc/mm: Add missing tracepoint for tlbie
    
    commit 0428491cba927 ("powerpc/mm: Trace tlbie(l) instructions")
    added tracepoints for tlbie calls, but _tlbil_va() was forgotten
    
    Fixes: 0428491cba927 ("powerpc/mm: Trace tlbie(l) instructions")
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index dd7f9b951d25..1db2027a0110 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -22,6 +22,7 @@
 #include <asm/mmu.h>
 
 #ifdef CONFIG_PPC_MMU_NOHASH
+#include <asm/trace.h>
 
 /*
  * On 40x and 8xx, we directly inline tlbia and tlbivax
@@ -55,6 +56,7 @@ static inline void _tlbil_va(unsigned long address, unsigned int pid,
 			     unsigned int tsize, unsigned int ind)
 {
 	asm volatile ("tlbie %0; sync" : : "r" (address) : "memory");
+	trace_tlbie(0, 0, address, pid, 0, 0, 0);
 }
 #elif defined(CONFIG_PPC_BOOK3E)
 extern void _tlbil_va(unsigned long address, unsigned int pid,

commit 34eb138ed74dc95285478903148a53bd034829be
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Oct 9 13:51:54 2018 +0000

    powerpc/mm: don't use _PAGE_EXEC for calling hash_preload()
    
    The 'access' parameter of hash_preload() is either 0 or _PAGE_EXEC.
    Among the two versions of hash_preload(), only the PPC64 one is
    doing something with this 'access' parameter.
    
    In order to remove the use of _PAGE_EXEC outside platform code,
    'access' parameter is replaced by 'is_exec' which will be either
    true of false, and the PPC64 version of hash_preload() creates
    the access flag based on 'is_exec'.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index e5d779eed181..dd7f9b951d25 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -82,7 +82,7 @@ static inline void _tlbivax_bcast(unsigned long address, unsigned int pid,
 #else /* CONFIG_PPC_MMU_NOHASH */
 
 extern void hash_preload(struct mm_struct *mm, unsigned long ea,
-			 unsigned long access, unsigned long trap);
+			 bool is_exec, unsigned long trap);
 
 
 extern void _tlbie(unsigned long address);

commit 45ef5992e06dcc3a4c7d34d23052289c7676d56c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Thu Jul 5 16:25:19 2018 +0000

    powerpc: remove unnecessary inclusion of asm/tlbflush.h
    
    asm/tlbflush.h is only needed for:
    - using functions xxx_flush_tlb_xxx()
    - using MMU_NO_CONTEXT
    - including asm-generic/pgtable.h
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index c4c0a09a7775..e5d779eed181 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -19,7 +19,6 @@
  *
  */
 #include <linux/mm.h>
-#include <asm/tlbflush.h>
 #include <asm/mmu.h>
 
 #ifdef CONFIG_PPC_MMU_NOHASH

commit 7e1405917c145edbb7d4cd520e890e44161dd7be
Author: Jonathan Neuschäfer <j.neuschaefer@gmx.net>
Date:   Wed Mar 28 02:25:44 2018 +0200

    powerpc/mm/32: Remove the reserved memory hack
    
    This hack, introduced in commit c5df7f775148 ("powerpc: allow ioremap
    within reserved memory regions") is now unnecessary.
    
    Signed-off-by: Jonathan Neuschäfer <j.neuschaefer@gmx.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 57fbc554c785..c4c0a09a7775 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -98,7 +98,6 @@ extern void setbat(int index, unsigned long virt, phys_addr_t phys,
 		   unsigned int size, pgprot_t prot);
 
 extern int __map_without_bats;
-extern int __allow_ioremap_reserved;
 extern unsigned int rtas_data, rtas_size;
 
 struct hash_pte;

commit 968159c0031ac1e07ab4426397e786c9c483f068
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Aug 8 13:58:54 2017 +0200

    powerpc/8xx: Getting rid of remaining use of CONFIG_8xx
    
    Two config options exist to define powerpc MPC8xx:
    * CONFIG_PPC_8xx
    * CONFIG_8xx
    
    arch/powerpc/platforms/Kconfig.cputype has contained the following
    comment about CONFIG_8xx item for some years:
    "# this is temp to handle compat with arch=ppc"
    
    arch/powerpc is now the only place with remaining use of
    CONFIG_8xx: get rid of them.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index d46128b22150..57fbc554c785 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -27,7 +27,7 @@
 /*
  * On 40x and 8xx, we directly inline tlbia and tlbivax
  */
-#if defined(CONFIG_40x) || defined(CONFIG_8xx)
+#if defined(CONFIG_40x) || defined(CONFIG_PPC_8xx)
 static inline void _tlbil_all(void)
 {
 	asm volatile ("sync; tlbia; isync" : : : "memory");
@@ -38,7 +38,7 @@ static inline void _tlbil_pid(unsigned int pid)
 }
 #define _tlbil_pid_noind(pid)	_tlbil_pid(pid)
 
-#else /* CONFIG_40x || CONFIG_8xx */
+#else /* CONFIG_40x || CONFIG_PPC_8xx */
 extern void _tlbil_all(void);
 extern void _tlbil_pid(unsigned int pid);
 #ifdef CONFIG_PPC_BOOK3E
@@ -46,12 +46,12 @@ extern void _tlbil_pid_noind(unsigned int pid);
 #else
 #define _tlbil_pid_noind(pid)	_tlbil_pid(pid)
 #endif
-#endif /* !(CONFIG_40x || CONFIG_8xx) */
+#endif /* !(CONFIG_40x || CONFIG_PPC_8xx) */
 
 /*
  * On 8xx, we directly inline tlbie, on others, it's extern
  */
-#ifdef CONFIG_8xx
+#ifdef CONFIG_PPC_8xx
 static inline void _tlbil_va(unsigned long address, unsigned int pid,
 			     unsigned int tsize, unsigned int ind)
 {
@@ -67,7 +67,7 @@ static inline void _tlbil_va(unsigned long address, unsigned int pid,
 {
 	__tlbil_va(address, pid);
 }
-#endif /* CONFIG_8xx */
+#endif /* CONFIG_PPC_8xx */
 
 #if defined(CONFIG_PPC_BOOK3E) || defined(CONFIG_PPC_47x)
 extern void _tlbivax_bcast(unsigned long address, unsigned int pid,

commit 4386c096c2ffa1b3232d701e9d7ff82a1378e1c5
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Mon May 29 17:31:56 2017 +0200

    powerpc/mm: Rename map_page() to map_kernel_page() on 32-bit
    
    These two functions implement the same semantics, so unify their naming so we
    can share code that calls them. The longer name is more descriptive so use it.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Acked-by: Balbir Singh <bsingharora@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index f988db655e5b..d46128b22150 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -94,7 +94,6 @@ extern void _tlbia(void);
 #ifdef CONFIG_PPC32
 
 extern void mapin_ram(void);
-extern int map_page(unsigned long va, phys_addr_t pa, int flags);
 extern void setbat(int index, unsigned long virt, phys_addr_t phys,
 		   unsigned int size, pgprot_t prot);
 

commit 4badd43ae44109c88438cc6421d208f513cf537f
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue May 17 09:02:45 2016 +0200

    powerpc/8xx: Map IMMR area with 512k page at a fixed address
    
    Once the linear memory space has been mapped with 8Mb pages, as
    seen in the related commit, we get 11 millions DTLB missed during
    the reference 600s period. 77% of the misses are on user addresses
    and 23% are on kernel addresses (1 fourth for linear address space
    and 3 fourth for virtual address space)
    
    Traditionaly, each driver manages one computer board which has its
    own components with its own memory maps.
    But on embedded chips like the MPC8xx, the SOC has all registers
    located in the same IO area.
    
    When looking at ioremaps done during startup, we see that
    many drivers are re-mapping small parts of the IMMR for their own use
    and all those small pieces gets their own 4k page, amplifying the
    number of TLB misses: in our system we get 0xff000000 mapped 31 times
    and 0xff003000 mapped 9 times.
    
    Even if each part of IMMR was mapped only once with 4k pages, it would
    still be several small mappings towards linear area.
    
    This patch maps the IMMR with a single 512k page.
    
    With this patch applied, the number of DTLB misses during the 10 min
    period is reduced to 11.8 millions for a duration of 5.8s, which
    represents 2% of the non-idle time hence yet another 10% reduction.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 6af65327c993..f988db655e5b 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -154,9 +154,10 @@ struct tlbcam {
 };
 #endif
 
-#if defined(CONFIG_6xx) || defined(CONFIG_FSL_BOOKE)
+#if defined(CONFIG_6xx) || defined(CONFIG_FSL_BOOKE) || defined(CONFIG_PPC_8xx)
 /* 6xx have BATS */
 /* FSL_BOOKE have TLBCAM */
+/* 8xx have LTLB */
 phys_addr_t v_block_mapped(unsigned long va);
 unsigned long p_block_mapped(phys_addr_t pa);
 #else

commit 31a14fae92b29b7a7fcc65600f072bf448d3b0eb
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Fri Apr 29 23:25:59 2016 +1000

    powerpc/mm: Abstraction for vmemmap and map_kernel_page()
    
    For hash we create vmemmap mapping using bolted hash page table entries.
    For radix we fill the radix page table. The next patch will add the
    radix details for creating vmemmap mappings.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index bfb7c0bcabd5..6af65327c993 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -108,11 +108,6 @@ extern unsigned long Hash_size, Hash_mask;
 
 #endif /* CONFIG_PPC32 */
 
-#ifdef CONFIG_PPC64
-extern int map_kernel_page(unsigned long ea, unsigned long pa,
-			   unsigned long flags);
-#endif /* CONFIG_PPC64 */
-
 extern unsigned long ioremap_bot;
 extern unsigned long __max_low_memory;
 extern phys_addr_t __initial_memory_limit_addr;

commit e974cd4be0be8de0d370ee4dbf181d614c0de386
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Feb 9 17:08:10 2016 +0100

    powerpc32: remove ioremap_base
    
    ioremap_base is not initialised and is nowhere used so remove it
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 4b85077d4828..bfb7c0bcabd5 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -100,7 +100,6 @@ extern void setbat(int index, unsigned long virt, phys_addr_t phys,
 
 extern int __map_without_bats;
 extern int __allow_ioremap_reserved;
-extern unsigned long ioremap_base;
 extern unsigned int rtas_data, rtas_size;
 
 struct hash_pte;

commit 3084cdb7cd6a1609d0a4480291f5e4da80765d03
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Feb 9 17:07:58 2016 +0100

    powerpc32: refactor x_mapped_by_bats() and x_mapped_by_tlbcam() together
    
    x_mapped_by_bats() and x_mapped_by_tlbcam() serve the same kind of
    purpose, and are never defined at the same time.
    So rename them x_block_mapped() and define them in the relevant
    places
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 718076ff0b8a..4b85077d4828 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -159,3 +159,13 @@ struct tlbcam {
 	u32	MAS7;
 };
 #endif
+
+#if defined(CONFIG_6xx) || defined(CONFIG_FSL_BOOKE)
+/* 6xx have BATS */
+/* FSL_BOOKE have TLBCAM */
+phys_addr_t v_block_mapped(unsigned long va);
+unsigned long p_block_mapped(phys_addr_t pa);
+#else
+static inline phys_addr_t v_block_mapped(unsigned long va) { return 0; }
+static inline unsigned long p_block_mapped(phys_addr_t pa) { return 0; }
+#endif

commit a372acfac51e0d5858f8f6f84da52defcabf054b
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Tue Feb 9 17:07:50 2016 +0100

    powerpc/8xx: Map linear kernel RAM with 8M pages
    
    On a live running system (VoIP gateway for Air Trafic Control), over
    a 10 minutes period (with 277s idle), we get 87 millions DTLB misses
    and approximatly 35 secondes are spent in DTLB handler.
    This represents 5.8% of the overall time and even 10.8% of the
    non-idle time.
    Among those 87 millions DTLB misses, 15% are on user addresses and
    85% are on kernel addresses. And within the kernel addresses, 93%
    are on addresses from the linear address space and only 7% are on
    addresses from the virtual address space.
    
    MPC8xx has no BATs but it has 8Mb page size. This patch implements
    mapping of kernel RAM using 8Mb pages, on the same model as what is
    done on the 40x.
    
    In 4k pages mode, each PGD entry maps a 4Mb area: we map every two
    entries to the same 8Mb physical page. In each second entry, we add
    4Mb to the page physical address to ease life of the FixupDAR
    routine. This is just ignored by HW.
    
    In 16k pages mode, each PGD entry maps a 64Mb area: each PGD entry
    will point to the first page of the area. The DTLB handler adds
    the 3 bits from EPN to map the correct page.
    
    With this patch applied, we now get only 13 millions TLB misses
    during the 10 minutes period. The idle time has increased to 313s
    and the overall time spent in DTLB miss handler is 6.3s, which
    represents 1% of the overall time and 2.2% of non-idle time.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Scott Wood <oss@buserror.net>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 898d63365cdd..718076ff0b8a 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -133,22 +133,17 @@ extern void wii_memory_fixups(void);
 /* ...and now those things that may be slightly different between processor
  * architectures.  -- Dan
  */
-#if defined(CONFIG_8xx)
-#define MMU_init_hw()		do { } while(0)
-#define mmu_mapin_ram(top)	(0UL)
-
-#elif defined(CONFIG_4xx)
+#ifdef CONFIG_PPC32
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
+#endif
 
-#elif defined(CONFIG_PPC_FSL_BOOK3E)
+#ifdef CONFIG_PPC_FSL_BOOK3E
 extern unsigned long map_mem_in_cams(unsigned long ram, int max_cam_idx,
 				     bool dryrun);
 extern unsigned long calc_cam_sz(unsigned long ram, unsigned long virt,
 				 phys_addr_t phys);
 #ifdef CONFIG_PPC32
-extern void MMU_init_hw(void);
-extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void adjust_total_lowmem(void);
 extern int switch_to_as1(void);
 extern void restore_to_as0(int esel, int offset, void *dt_ptr, int bootcpu);
@@ -163,8 +158,4 @@ struct tlbcam {
 	u32	MAS3;
 	u32	MAS7;
 };
-#elif defined(CONFIG_PPC32)
-/* anything 32-bit except 4xx or 8xx */
-extern void MMU_init_hw(void);
-extern unsigned long mmu_mapin_ram(unsigned long top);
 #endif

commit 849f86a630e9c84bf4c9d5dcbfe59dc94b2e15ce
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Feb 22 13:41:15 2016 +1100

    powerpc/mm/book3s-64: Move _PAGE_PRESENT to the most significant bit
    
    This changes _PAGE_PRESENT for 64-bit Book 3S processors from 0x2 to
    0x8000_0000_0000_0000, because that is where PowerISA v3.0 CPUs in
    radix mode will expect to find it.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 9f58ff44a075..898d63365cdd 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -110,7 +110,8 @@ extern unsigned long Hash_size, Hash_mask;
 #endif /* CONFIG_PPC32 */
 
 #ifdef CONFIG_PPC64
-extern int map_kernel_page(unsigned long ea, unsigned long pa, int flags);
+extern int map_kernel_page(unsigned long ea, unsigned long pa,
+			   unsigned long flags);
 #endif /* CONFIG_PPC64 */
 
 extern unsigned long ioremap_bot;

commit eba5de8dc163fac4d20f0afed8183431491b570f
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Oct 6 22:48:10 2015 -0500

    powerpc/fsl-booke-64: Don't limit ppc64_rma_size to one TLB entry
    
    This is required for kdump to work when loaded at at an address that
    does not fall within the first TLB entry -- which can easily happen
    because while the lower limit is enforced via reserved memory, which
    doesn't affect how much is mapped, the upper limit is enforced via a
    different mechanism that does.  Thus, more TLB entries are needed than
    would normally be used, as the total memory to be mapped might not be a
    power of two.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 27c3a2d3a4f1..9f58ff44a075 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -141,7 +141,8 @@ extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
 
 #elif defined(CONFIG_PPC_FSL_BOOK3E)
-extern unsigned long map_mem_in_cams(unsigned long ram, int max_cam_idx);
+extern unsigned long map_mem_in_cams(unsigned long ram, int max_cam_idx,
+				     bool dryrun);
 extern unsigned long calc_cam_sz(unsigned long ram, unsigned long virt,
 				 phys_addr_t phys);
 #ifdef CONFIG_PPC32

commit d9e1831a420267a7ced708bb259d65b0a3c0344d
Author: Scott Wood <scottwood@freescale.com>
Date:   Tue Oct 6 22:48:09 2015 -0500

    powerpc/85xx: Load all early TLB entries at once
    
    Use an AS=1 trampoline TLB entry to allow all normal TLB1 entries to
    be loaded at once.  This avoids the need to keep the translation that
    code is executing from in the same TLB entry in the final TLB
    configuration as during early boot, which in turn is helpful for
    relocatable kernels (e.g. kdump) where the kernel is not running from
    what would be the first TLB entry.
    
    On e6500, we limit map_mem_in_cams() to the primary hwthread of a
    core (the boot cpu is always considered primary, as a kdump kernel
    can be entered on any cpu).  Each TLB only needs to be set up once,
    and when we do, we don't want another thread to be running when we
    create a temporary trampoline TLB1 entry.
    
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 085b66b10891..27c3a2d3a4f1 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -152,6 +152,7 @@ extern int switch_to_as1(void);
 extern void restore_to_as0(int esel, int offset, void *dt_ptr, int bootcpu);
 #endif
 extern void loadcam_entry(unsigned int index);
+extern void loadcam_multi(int first_idx, int num, int tmp_idx);
 
 struct tlbcam {
 	u32	MAS0;

commit 5dd4e4f6fe9495f02d4594bd460b84008a3e8e93
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Mar 25 20:11:55 2015 +1100

    powerpc/mm: Change setbat() to take a pgprot_t rather than flags
    
    The callers of setbat() are actually passing a pgprot_t for the flags
    parameter. This doesn't matter unless STRICT_MM_TYPECHECKS is enabled.
    So we can turn that on without breaking the build, change setbat() to
    take a pgprot_t and have it convert it to an unsigned long internally.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 78c45f392f5b..085b66b10891 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -96,7 +96,7 @@ extern void _tlbia(void);
 extern void mapin_ram(void);
 extern int map_page(unsigned long va, phys_addr_t pa, int flags);
 extern void setbat(int index, unsigned long virt, phys_addr_t phys,
-		   unsigned int size, int flags);
+		   unsigned int size, pgprot_t prot);
 
 extern int __map_without_bats;
 extern int __allow_ioremap_reserved;

commit b823982cd78ee4c62fa58aa844583738003fbf75
Author: Paul Bolle <pebolle@tiscali.nl>
Date:   Fri Sep 26 19:49:12 2014 +0200

    powerpc: Fix comment typo 'CONIFG_8xx'
    
    Signed-off-by: Paul Bolle <pebolle@tiscali.nl>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 9615d82919b8..78c45f392f5b 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -67,7 +67,7 @@ static inline void _tlbil_va(unsigned long address, unsigned int pid,
 {
 	__tlbil_va(address, pid);
 }
-#endif /* CONIFG_8xx */
+#endif /* CONFIG_8xx */
 
 #if defined(CONFIG_PPC_BOOK3E) || defined(CONFIG_PPC_47x)
 extern void _tlbivax_bcast(unsigned long address, unsigned int pid,

commit 0be7d969b0efef085ed6497d462ba16a875ca737
Author: Kevin Hao <haokexin@gmail.com>
Date:   Tue Dec 24 15:12:11 2013 +0800

    powerpc/fsl_booke: smp support for booting a relocatable kernel above 64M
    
    When booting above the 64M for a secondary cpu, we also face the
    same issue as the boot cpu that the PAGE_OFFSET map two different
    physical address for the init tlb and the final map. So we have to use
    switch_to_as1/restore_to_as0 between the conversion of these two
    maps. When restoring to as0 for a secondary cpu, we only need to
    return to the caller. So add a new parameter for function
    restore_to_as0 for this purpose.
    
    Use LOAD_REG_ADDR_PIC to get the address of variables which may
    be used before we set the final map in cams for the secondary cpu.
    Move the setting of cams a bit earlier in order to avoid the
    unnecessary using of LOAD_REG_ADDR_PIC.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 91da910210cb..9615d82919b8 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -149,7 +149,7 @@ extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void adjust_total_lowmem(void);
 extern int switch_to_as1(void);
-extern void restore_to_as0(int esel, int offset, void *dt_ptr);
+extern void restore_to_as0(int esel, int offset, void *dt_ptr, int bootcpu);
 #endif
 extern void loadcam_entry(unsigned int index);
 

commit 7d2471f9fa85089beb1cb9436ffc28f9e11e518d
Author: Kevin Hao <haokexin@gmail.com>
Date:   Tue Dec 24 15:12:10 2013 +0800

    powerpc/fsl_booke: make sure PAGE_OFFSET map to memstart_addr for relocatable kernel
    
    This is always true for a non-relocatable kernel. Otherwise the kernel
    would get stuck. But for a relocatable kernel, it seems a little
    complicated. When booting a relocatable kernel, we just align the
    kernel start addr to 64M and map the PAGE_OFFSET from there. The
    relocation will base on this virtual address. But if this address
    is not the same as the memstart_addr, we will have to change the
    map of PAGE_OFFSET to the real memstart_addr and do another relocation
    again.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    [scottwood@freescale.com: make offset long and non-negative in simple case]
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index eefbf7bb4331..91da910210cb 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -149,7 +149,7 @@ extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void adjust_total_lowmem(void);
 extern int switch_to_as1(void);
-extern void restore_to_as0(int esel);
+extern void restore_to_as0(int esel, int offset, void *dt_ptr);
 #endif
 extern void loadcam_entry(unsigned int index);
 

commit 78a235efdc42ff363de81fdbc171385e8b86b69b
Author: Kevin Hao <haokexin@gmail.com>
Date:   Tue Dec 24 15:12:07 2013 +0800

    powerpc/fsl_booke: set the tlb entry for the kernel address in AS1
    
    We use the tlb1 entries to map low mem to the kernel space. In the
    current code, it assumes that the first tlb entry would cover the
    kernel image. But this is not true for some special cases, such as
    when we run a relocatable kernel above the 64M or set
    CONFIG_KERNEL_START above 64M. So we choose to switch to address
    space 1 before setting these tlb entries.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Scott Wood <scottwood@freescale.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 83eb5d5f53d5..eefbf7bb4331 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -148,6 +148,8 @@ extern unsigned long calc_cam_sz(unsigned long ram, unsigned long virt,
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void adjust_total_lowmem(void);
+extern int switch_to_as1(void);
+extern void restore_to_as0(int esel);
 #endif
 extern void loadcam_entry(unsigned int index);
 

commit 1dc91c3eb374ca01ec99dc0ca2a38babc509beb3
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Fri Sep 16 10:39:59 2011 -0500

    powerpc/fsl-booke: Fix setup_initial_memory_limit to not blindly map
    
    On FSL Book-E devices we support multiple large TLB sizes and so we can
    get into situations in which the initial 1G TLB size is too big and
    we're asked for a size that is not mappable by a single entry (like
    512M).  The single entry is important because when we bring up secondary
    cores they need to ensure any data structure they need to access (eg
    PACA or stack) is always mapped.
    
    So we really need to determine what size will actually be mapped by the
    first TLB entry to ensure we limit early memory references to that
    region.  We refactor the map_mem_in_cams() code to provider a helper
    function that we can utilize to determine the size of the first TLB
    entry while taking into account size and alignment constraints.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index dd0a2589591d..83eb5d5f53d5 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -142,6 +142,8 @@ extern unsigned long mmu_mapin_ram(unsigned long top);
 
 #elif defined(CONFIG_PPC_FSL_BOOK3E)
 extern unsigned long map_mem_in_cams(unsigned long ram, int max_cam_idx);
+extern unsigned long calc_cam_sz(unsigned long ram, unsigned long virt,
+				 phys_addr_t phys);
 #ifdef CONFIG_PPC32
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);

commit 55fd766b5fad8240b7a6e994b5779a46d28f73d4
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Fri Oct 16 18:48:40 2009 -0500

    powerpc/fsl-booke64: Use TLB CAMs to cover linear mapping on FSL 64-bit chips
    
    On Freescale parts typically have TLB array for large mappings that we can
    bolt the linear mapping into.  We utilize the code that already exists
    on PPC32 on the 64-bit side to setup the linear mapping to be cover by
    bolted TLB entries.  We utilize a quarter of the variable size TLB array
    for this purpose.
    
    Additionally, we limit the amount of memory to what we can cover via
    bolted entries so we don't get secondary faults in the TLB miss
    handlers.  We should fix this limitation in the future.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 63b84a0d3b10..dd0a2589591d 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -140,10 +140,13 @@ extern void wii_memory_fixups(void);
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
 
-#elif defined(CONFIG_FSL_BOOKE)
+#elif defined(CONFIG_PPC_FSL_BOOK3E)
+extern unsigned long map_mem_in_cams(unsigned long ram, int max_cam_idx);
+#ifdef CONFIG_PPC32
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void adjust_total_lowmem(void);
+#endif
 extern void loadcam_entry(unsigned int index);
 
 struct tlbcam {

commit 78f622377f7d31d988db350a43c5689dd5f31876
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu May 13 14:38:21 2010 -0500

    powerpc/fsl-booke: Move loadcam_entry back to asm code to fix SMP ftrace
    
    When we build with ftrace enabled its possible that loadcam_entry would
    have used the stack pointer (even though the code doesn't need it).  We
    call loadcam_entry in __secondary_start before the stack is setup.  To
    ensure that loadcam_entry doesn't use the stack pointer the easiest
    solution is to just have it in asm code.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index eb11d5d2aa94..63b84a0d3b10 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -144,7 +144,15 @@ extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void adjust_total_lowmem(void);
-
+extern void loadcam_entry(unsigned int index);
+
+struct tlbcam {
+	u32	MAS0;
+	u32	MAS1;
+	unsigned long	MAS2;
+	u32	MAS3;
+	u32	MAS7;
+};
 #elif defined(CONFIG_PPC32)
 /* anything 32-bit except 4xx or 8xx */
 extern void MMU_init_hw(void);

commit e7f75ad01d590243904c2d95ab47e6b2e9ef6dad
Author: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
Date:   Fri Mar 5 10:43:12 2010 +0000

    powerpc/47x: Base ppc476 support
    
    This patch adds the base support for the 476 processor.  The code was
    primarily written by Ben Herrenschmidt and Torez Smith, but I've been
    maintaining it for a while.
    
    The goal is to have a single binary that will run on 44x and 47x, but
    we still have some details to work out.  The biggest is that the L1 cache
    line size differs on the two platforms, but it's currently a compile-time
    option.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Torez Smith  <lnxtorez@linux.vnet.ibm.com>
    Signed-off-by: Dave Kleikamp <shaggy@linux.vnet.ibm.com>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index d49a77503e19..eb11d5d2aa94 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -69,12 +69,7 @@ static inline void _tlbil_va(unsigned long address, unsigned int pid,
 }
 #endif /* CONIFG_8xx */
 
-/*
- * As of today, we don't support tlbivax broadcast on any
- * implementation. When that becomes the case, this will be
- * an extern.
- */
-#ifdef CONFIG_PPC_BOOK3E
+#if defined(CONFIG_PPC_BOOK3E) || defined(CONFIG_PPC_47x)
 extern void _tlbivax_bcast(unsigned long address, unsigned int pid,
 			   unsigned int tsize, unsigned int ind);
 #else

commit a73611b6aafa3b902524dad2d68e378c4ec9f4db
Merge: 5fa3577b1a12 ae4cec473696
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 16 13:26:53 2009 -0800

    Merge branch 'next' of git://git.secretlab.ca/git/linux-2.6
    
    * 'next' of git://git.secretlab.ca/git/linux-2.6: (23 commits)
      powerpc: fix up for mmu_mapin_ram api change
      powerpc: wii: allow ioremap within the memory hole
      powerpc: allow ioremap within reserved memory regions
      wii: use both mem1 and mem2 as ram
      wii: bootwrapper: add fixup to calc useable mem2
      powerpc: gamecube/wii: early debugging using usbgecko
      powerpc: reserve fixmap entries for early debug
      powerpc: wii: default config
      powerpc: wii: platform support
      powerpc: wii: hollywood interrupt controller support
      powerpc: broadway processor support
      powerpc: wii: bootwrapper bits
      powerpc: wii: device tree
      powerpc: gamecube: default config
      powerpc: gamecube: platform support
      powerpc: gamecube/wii: flipper interrupt controller support
      powerpc: gamecube/wii: udbg support for usbgecko
      powerpc: gamecube/wii: do not include PCI support
      powerpc: gamecube/wii: declare as non-coherent platforms
      powerpc: gamecube/wii: introduce GAMECUBE_COMMON
      ...
    
    Fix up conflicts in arch/powerpc/mm/fsl_booke_mmu.c.
    
    Hopefully even close to correctly.

commit ae4cec4736969ec2196a6bbce4ab263ff7cb7eef
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Mon Dec 14 09:04:24 2009 -0700

    powerpc: fix up for mmu_mapin_ram api change
    
    Today's linux-next build (powerpc ppc44x_defconfig) failed like this:
    
    arch/powerpc/mm/pgtable_32.c: In function 'mapin_ram':
    arch/powerpc/mm/pgtable_32.c:318: error: too many arguments to function 'mmu_mapin_ram'
    
    Casued by commit de32400dd26e743c5d500aa42d8d6818b79edb73 ("wii: use both
    mem1 and mem2 as ram").
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 34dacc32250d..06f708c38f1b 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -150,15 +150,15 @@ extern void wii_memory_fixups(void);
  */
 #if defined(CONFIG_8xx)
 #define MMU_init_hw()		do { } while(0)
-#define mmu_mapin_ram()		(0UL)
+#define mmu_mapin_ram(top)	(0UL)
 
 #elif defined(CONFIG_4xx)
 extern void MMU_init_hw(void);
-extern unsigned long mmu_mapin_ram(void);
+extern unsigned long mmu_mapin_ram(unsigned long top);
 
 #elif defined(CONFIG_FSL_BOOKE)
 extern void MMU_init_hw(void);
-extern unsigned long mmu_mapin_ram(void);
+extern unsigned long mmu_mapin_ram(unsigned long top);
 extern void adjust_total_lowmem(void);
 
 #elif defined(CONFIG_PPC32)

commit c5df7f775148723de39274537a886e9502eef336
Author: Albert Herranz <albert_herranz@yahoo.es>
Date:   Sat Dec 12 06:31:54 2009 +0000

    powerpc: allow ioremap within reserved memory regions
    
    Add a flag to let a platform ioremap memory regions marked as reserved.
    
    This flag will be used later by the Nintendo Wii support code to allow
    ioremapping the I/O region sitting between MEM1 and MEM2 and marked
    as reserved RAM in the patch "wii: use both mem1 and mem2 as ram".
    
    This will no longer be needed when proper discontig memory support
    for 32-bit PowerPC is added to the kernel.
    
    Signed-off-by: Albert Herranz <albert_herranz@yahoo.es>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 9aa39fe74f8a..34dacc32250d 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -115,6 +115,7 @@ extern void settlbcam(int index, unsigned long virt, phys_addr_t phys,
 extern void invalidate_tlbcam_entry(int index);
 
 extern int __map_without_bats;
+extern int __allow_ioremap_reserved;
 extern unsigned long ioremap_base;
 extern unsigned int rtas_data, rtas_size;
 

commit de32400dd26e743c5d500aa42d8d6818b79edb73
Author: Albert Herranz <albert_herranz@yahoo.es>
Date:   Sat Dec 12 06:31:53 2009 +0000

    wii: use both mem1 and mem2 as ram
    
    The Nintendo Wii video game console has two discontiguous RAM regions:
    - MEM1: 24MB @ 0x00000000
    - MEM2: 64MB @ 0x10000000
    
    Unfortunately, the kernel currently does not support discontiguous RAM
    memory regions on 32-bit PowerPC platforms.
    
    This patch adds a series of workarounds to allow the use of the second
    memory region (MEM2) as RAM by the kernel.
    Basically, a single range of memory from the beginning of MEM1 to the
    end of MEM2 is reported to the kernel, and a memory reservation is
    created for the hole between MEM1 and MEM2.
    
    With this patch the system is able to use all the available RAM and not
    just ~27% of it.
    
    This will no longer be needed when proper discontig memory support
    for 32-bit PowerPC is added to the kernel.
    
    Signed-off-by: Albert Herranz <albert_herranz@yahoo.es>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Grant Likely <grant.likely@secretlab.ca>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index d2e5321d5ea6..9aa39fe74f8a 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -136,6 +136,14 @@ extern phys_addr_t total_lowmem;
 extern phys_addr_t memstart_addr;
 extern phys_addr_t lowmem_end_addr;
 
+#ifdef CONFIG_WII
+extern unsigned long wii_hole_start;
+extern unsigned long wii_hole_size;
+
+extern unsigned long wii_mmu_mapin_mem2(unsigned long top);
+extern void wii_memory_fixups(void);
+#endif
+
 /* ...and now those things that may be slightly different between processor
  * architectures.  -- Dan
  */
@@ -155,5 +163,5 @@ extern void adjust_total_lowmem(void);
 #elif defined(CONFIG_PPC32)
 /* anything 32-bit except 4xx or 8xx */
 extern void MMU_init_hw(void);
-extern unsigned long mmu_mapin_ram(void);
+extern unsigned long mmu_mapin_ram(unsigned long top);
 #endif

commit 8b27f0b61db57f5555fc2d3fc95c3ea9fd1a9d6c
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Thu Oct 15 12:49:01 2009 -0500

    powerpc/fsl-booke: Rework TLB CAM code
    
    Re-write the code so its more standalone and fixed some issues:
    * Bump'd # of CAM entries to 64 to support e500mc
    * Make the code handle MAS7 properly
    * Use pr_cont instead of creating a string as we go
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index d2e5321d5ea6..e27a990af42d 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -98,21 +98,10 @@ extern void _tlbia(void);
 
 #ifdef CONFIG_PPC32
 
-struct tlbcam {
-	u32	MAS0;
-	u32	MAS1;
-	u32	MAS2;
-	u32	MAS3;
-	u32	MAS7;
-};
-
 extern void mapin_ram(void);
 extern int map_page(unsigned long va, phys_addr_t pa, int flags);
 extern void setbat(int index, unsigned long virt, phys_addr_t phys,
 		   unsigned int size, int flags);
-extern void settlbcam(int index, unsigned long virt, phys_addr_t phys,
-		      unsigned int size, int flags, unsigned int pid);
-extern void invalidate_tlbcam_entry(int index);
 
 extern int __map_without_bats;
 extern unsigned long ioremap_base;

commit 32a74949b7337726e76d69f51c48715431126c6c
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:58 2009 +0000

    powerpc/mm: Add support for SPARSEMEM_VMEMMAP on 64-bit Book3E
    
    The base TLB support didn't include support for SPARSEMEM_VMEMMAP, though
    we did carve out some virtual space for it, the necessary support code
    wasn't there. This implements it by using 16M pages for now, though the
    page size could easily be changed at runtime if necessary.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 5961c6b739dd..d2e5321d5ea6 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -121,7 +121,12 @@ extern unsigned int rtas_data, rtas_size;
 struct hash_pte;
 extern struct hash_pte *Hash, *Hash_end;
 extern unsigned long Hash_size, Hash_mask;
-#endif
+
+#endif /* CONFIG_PPC32 */
+
+#ifdef CONFIG_PPC64
+extern int map_kernel_page(unsigned long ea, unsigned long pa, int flags);
+#endif /* CONFIG_PPC64 */
 
 extern unsigned long ioremap_bot;
 extern unsigned long __max_low_memory;

commit 25d21ad6e799cccd097b9df2a2fefe19a7e1dfcf
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:47 2009 +0000

    powerpc: Add TLB management code for 64-bit Book3E
    
    This adds the TLB miss handler assembly, the low level TLB flush routines
    along with the necessary hook for dealing with our virtual page tables
    or indirect TLB entries that need to be flushes when PTE pages are freed.
    
    There is currently no support for hugetlbfs
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 3871dceee2dd..5961c6b739dd 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -41,7 +41,11 @@ static inline void _tlbil_pid(unsigned int pid)
 #else /* CONFIG_40x || CONFIG_8xx */
 extern void _tlbil_all(void);
 extern void _tlbil_pid(unsigned int pid);
+#ifdef CONFIG_PPC_BOOK3E
+extern void _tlbil_pid_noind(unsigned int pid);
+#else
 #define _tlbil_pid_noind(pid)	_tlbil_pid(pid)
+#endif
 #endif /* !(CONFIG_40x || CONFIG_8xx) */
 
 /*
@@ -53,7 +57,10 @@ static inline void _tlbil_va(unsigned long address, unsigned int pid,
 {
 	asm volatile ("tlbie %0; sync" : : "r" (address) : "memory");
 }
-#else /* CONFIG_8xx */
+#elif defined(CONFIG_PPC_BOOK3E)
+extern void _tlbil_va(unsigned long address, unsigned int pid,
+		      unsigned int tsize, unsigned int ind);
+#else
 extern void __tlbil_va(unsigned long address, unsigned int pid);
 static inline void _tlbil_va(unsigned long address, unsigned int pid,
 			     unsigned int tsize, unsigned int ind)
@@ -67,11 +74,16 @@ static inline void _tlbil_va(unsigned long address, unsigned int pid,
  * implementation. When that becomes the case, this will be
  * an extern.
  */
+#ifdef CONFIG_PPC_BOOK3E
+extern void _tlbivax_bcast(unsigned long address, unsigned int pid,
+			   unsigned int tsize, unsigned int ind);
+#else
 static inline void _tlbivax_bcast(unsigned long address, unsigned int pid,
 				   unsigned int tsize, unsigned int ind)
 {
 	BUG();
 }
+#endif
 
 #else /* CONFIG_PPC_MMU_NOHASH */
 

commit d4e167da4cb60910f6ac305aee03714937f70b71
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Jul 23 23:15:24 2009 +0000

    powerpc/mm: Make low level TLB flush ops on BookE take additional args
    
    We need to pass down whether the page is direct or indirect and we'll
    need to pass the page size to _tlbil_va and _tlbivax_bcast
    
    We also add a new low level _tlbil_pid_noind() which does a TLB flush
    by PID but avoids flushing indirect entries if possible
    
    This implements those new prototypes but defines them with inlines
    or macros so that no additional arguments are actually passed on current
    processors.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index d1f9c62dc177..3871dceee2dd 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -36,21 +36,30 @@ static inline void _tlbil_pid(unsigned int pid)
 {
 	asm volatile ("sync; tlbia; isync" : : : "memory");
 }
+#define _tlbil_pid_noind(pid)	_tlbil_pid(pid)
+
 #else /* CONFIG_40x || CONFIG_8xx */
 extern void _tlbil_all(void);
 extern void _tlbil_pid(unsigned int pid);
+#define _tlbil_pid_noind(pid)	_tlbil_pid(pid)
 #endif /* !(CONFIG_40x || CONFIG_8xx) */
 
 /*
  * On 8xx, we directly inline tlbie, on others, it's extern
  */
 #ifdef CONFIG_8xx
-static inline void _tlbil_va(unsigned long address, unsigned int pid)
+static inline void _tlbil_va(unsigned long address, unsigned int pid,
+			     unsigned int tsize, unsigned int ind)
 {
 	asm volatile ("tlbie %0; sync" : : "r" (address) : "memory");
 }
 #else /* CONFIG_8xx */
-extern void _tlbil_va(unsigned long address, unsigned int pid);
+extern void __tlbil_va(unsigned long address, unsigned int pid);
+static inline void _tlbil_va(unsigned long address, unsigned int pid,
+			     unsigned int tsize, unsigned int ind)
+{
+	__tlbil_va(address, pid);
+}
 #endif /* CONIFG_8xx */
 
 /*
@@ -58,7 +67,8 @@ extern void _tlbil_va(unsigned long address, unsigned int pid);
  * implementation. When that becomes the case, this will be
  * an extern.
  */
-static inline void _tlbivax_bcast(unsigned long address, unsigned int pid)
+static inline void _tlbivax_bcast(unsigned long address, unsigned int pid,
+				   unsigned int tsize, unsigned int ind)
 {
 	BUG();
 }

commit 30aae739a9eb6db31ad7b08dac44bd302f41c709
Merge: 37a76bd4f1b7 6fd8be4bf728
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jan 13 13:59:03 2009 +1100

    Merge commit 'kumar/kumar-next' into next

commit 4a0826824beb28390651a962987b0681b9e7fe93
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jan 6 17:56:51 2009 +0000

    powerpc: Fix missing semicolons in mmu_decl.h
    
    This is a brown paper bag from one of my earlier patches that
    breaks build on 40x and 8xx.
    
    And yes, I've now added 40x and 8xx to my list of test configs :-)
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 4314b39b6faf..ad123bced404 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -30,11 +30,11 @@
 #if defined(CONFIG_40x) || defined(CONFIG_8xx)
 static inline void _tlbil_all(void)
 {
-	asm volatile ("sync; tlbia; isync" : : : "memory")
+	asm volatile ("sync; tlbia; isync" : : : "memory");
 }
 static inline void _tlbil_pid(unsigned int pid)
 {
-	asm volatile ("sync; tlbia; isync" : : : "memory")
+	asm volatile ("sync; tlbia; isync" : : : "memory");
 }
 #else /* CONFIG_40x || CONFIG_8xx */
 extern void _tlbil_all(void);
@@ -47,7 +47,7 @@ extern void _tlbil_pid(unsigned int pid);
 #ifdef CONFIG_8xx
 static inline void _tlbil_va(unsigned long address, unsigned int pid)
 {
-	asm volatile ("tlbie %0; sync" : : "r" (address) : "memory")
+	asm volatile ("tlbie %0; sync" : : "r" (address) : "memory");
 }
 #else /* CONFIG_8xx */
 extern void _tlbil_va(unsigned long address, unsigned int pid);

commit 6fd8be4bf72879b3039654388e985cabf8449af5
Author: Trent Piepho <tpiepho@freescale.com>
Date:   Mon Dec 8 19:34:56 2008 -0800

    powerpc/fsl-booke: Remove num_tlbcam_entries
    
    This is a global variable defined in fsl_booke_mmu.c with a value that gets
    initialized in assembly code in head_fsl_booke.S.
    
    It's never used.
    
    If some code ever does want to know the number of entries in TLB1, then
    "numcams = mfspr(SPRN_TLB1CFG) & 0xfff", is a whole lot simpler than a
    global initialized during kernel boot from assembly.
    
    Signed-off-by: Trent Piepho <tpiepho@freescale.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 6f6ee62c2a04..d0bb69dc6278 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -99,8 +99,6 @@ extern unsigned int rtas_data, rtas_size;
 struct hash_pte;
 extern struct hash_pte *Hash, *Hash_end;
 extern unsigned long Hash_size, Hash_mask;
-
-extern unsigned int num_tlbcam_entries;
 #endif
 
 extern unsigned long ioremap_bot;

commit 19f5465e823858a2f0b0e9a92e52816ba3ee70bb
Author: Trent Piepho <tpiepho@freescale.com>
Date:   Mon Dec 8 19:34:55 2008 -0800

    powerpc/fsl-booke: Don't hard-code size of struct tlbcam
    
    Some assembly code in head_fsl_booke.S hard-coded the size of struct tlbcam
    to 20 when it indexed the TLBCAM table.  Anyone changing the size of struct
    tlbcam would not know to expect that.
    
    The kernel already has a system to get the size of C structures into
    assembly language files, asm-offsets, so let's use it.
    
    The definition of the struct gets moved to a header, so that asm-offsets.c
    can include it.
    
    Signed-off-by: Trent Piepho <tpiepho@freescale.com>
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 4314b39b6faf..6f6ee62c2a04 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -75,6 +75,15 @@ extern void _tlbia(void);
 #endif /* CONFIG_PPC_MMU_NOHASH */
 
 #ifdef CONFIG_PPC32
+
+struct tlbcam {
+	u32	MAS0;
+	u32	MAS1;
+	u32	MAS2;
+	u32	MAS3;
+	u32	MAS7;
+};
+
 extern void mapin_ram(void);
 extern int map_page(unsigned long va, phys_addr_t pa, int flags);
 extern void setbat(int index, unsigned long virt, phys_addr_t phys,

commit 2a4aca1144394653269720ffbb5a325a77abd5fa
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Dec 18 19:13:42 2008 +0000

    powerpc/mm: Split low level tlb invalidate for nohash processors
    
    Currently, the various forms of low level TLB invalidations are all
    implemented in misc_32.S for 32-bit processors, in a fairly scary
    mess of #ifdef's and with interesting duplication such as a whole
    bunch of code for FSL _tlbie and _tlbia which are no longer used.
    
    This moves things around such that _tlbie is now defined in
    hash_low_32.S and is only used by the 32-bit hash code, and all
    nohash CPUs use the various _tlbil_* forms that are now moved to
    a new file, tlb_nohash_low.S.
    
    I moved all the definitions for that stuff out of
    include/asm/tlbflush.h as they are really internal mm stuff, into
    mm/mmu_decl.h
    
    The code should have no functional changes.  I kept some variants
    inline for trivial forms on things like 40x and 8xx.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index b4344fd30f2a..4314b39b6faf 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -22,10 +22,58 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu.h>
 
+#ifdef CONFIG_PPC_MMU_NOHASH
+
+/*
+ * On 40x and 8xx, we directly inline tlbia and tlbivax
+ */
+#if defined(CONFIG_40x) || defined(CONFIG_8xx)
+static inline void _tlbil_all(void)
+{
+	asm volatile ("sync; tlbia; isync" : : : "memory")
+}
+static inline void _tlbil_pid(unsigned int pid)
+{
+	asm volatile ("sync; tlbia; isync" : : : "memory")
+}
+#else /* CONFIG_40x || CONFIG_8xx */
+extern void _tlbil_all(void);
+extern void _tlbil_pid(unsigned int pid);
+#endif /* !(CONFIG_40x || CONFIG_8xx) */
+
+/*
+ * On 8xx, we directly inline tlbie, on others, it's extern
+ */
+#ifdef CONFIG_8xx
+static inline void _tlbil_va(unsigned long address, unsigned int pid)
+{
+	asm volatile ("tlbie %0; sync" : : "r" (address) : "memory")
+}
+#else /* CONFIG_8xx */
+extern void _tlbil_va(unsigned long address, unsigned int pid);
+#endif /* CONIFG_8xx */
+
+/*
+ * As of today, we don't support tlbivax broadcast on any
+ * implementation. When that becomes the case, this will be
+ * an extern.
+ */
+static inline void _tlbivax_bcast(unsigned long address, unsigned int pid)
+{
+	BUG();
+}
+
+#else /* CONFIG_PPC_MMU_NOHASH */
+
 extern void hash_preload(struct mm_struct *mm, unsigned long ea,
 			 unsigned long access, unsigned long trap);
 
 
+extern void _tlbie(unsigned long address);
+extern void _tlbia(void);
+
+#endif /* CONFIG_PPC_MMU_NOHASH */
+
 #ifdef CONFIG_PPC32
 extern void mapin_ram(void);
 extern int map_page(unsigned long va, phys_addr_t pa, int flags);

commit f63837f0581fe580168ae1a7d178ded935411747
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Sun Dec 14 19:44:51 2008 +0000

    powerpc/mm: Remove flush_HPTE()
    
    The function flush_HPTE() is used in only one place, the implementation
    of DEBUG_PAGEALLOC on ppc32.
    
    It's actually a dup of flush_tlb_page() though it's -slightly- more
    efficient on hash based processors.  We remove it and replace it by
    a direct call to the hash flush code on those processors and to
    flush_tlb_page() for everybody else.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index fab3cfad4099..b4344fd30f2a 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -58,17 +58,14 @@ extern phys_addr_t lowmem_end_addr;
  * architectures.  -- Dan
  */
 #if defined(CONFIG_8xx)
-#define flush_HPTE(X, va, pg)	_tlbie(va, 0 /* 8xx doesn't care about PID */)
 #define MMU_init_hw()		do { } while(0)
 #define mmu_mapin_ram()		(0UL)
 
 #elif defined(CONFIG_4xx)
-#define flush_HPTE(pid, va, pg)	_tlbie(va, pid)
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(void);
 
 #elif defined(CONFIG_FSL_BOOKE)
-#define flush_HPTE(pid, va, pg)	_tlbie(va, pid)
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(void);
 extern void adjust_total_lowmem(void);
@@ -77,18 +74,4 @@ extern void adjust_total_lowmem(void);
 /* anything 32-bit except 4xx or 8xx */
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(void);
-
-/* Be careful....this needs to be updated if we ever encounter 603 SMPs,
- * which includes all new 82xx processors.  We need tlbie/tlbsync here
- * in that case (I think). -- Dan.
- */
-static inline void flush_HPTE(unsigned context, unsigned long va,
-			      unsigned long pdval)
-{
-	if ((Hash != 0) &&
-	    cpu_has_feature(CPU_FTR_HPTE_TABLE))
-		flush_hash_pages(0, va, pdval, 1);
-	else
-		_tlbie(va);
-}
 #endif

commit 2bf3016f89344d4cd8b2c96bbec2b642a2bde413
Author: Stefan Roese <sr@denx.de>
Date:   Thu Jul 10 01:09:23 2008 +1000

    powerpc: Fix problems with 32bit PPC's running with >= 4GB of RAM
    
    This patch enables 32bit PPC's (with 36bit physical address space, e.g.
    IBM/AMCC PPC44x) to run with >= 4GB of RAM. Mostly its just replacing types
    (unsigned long -> phys_addr_t).
    
    Tested on an AMCC Katmai with 4GB of DDR2.
    
    Signed-off-by: Stefan Roese <sr@denx.de>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 46585b7bb194..fab3cfad4099 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -49,8 +49,8 @@ extern unsigned int num_tlbcam_entries;
 extern unsigned long ioremap_bot;
 extern unsigned long __max_low_memory;
 extern phys_addr_t __initial_memory_limit_addr;
-extern unsigned long total_memory;
-extern unsigned long total_lowmem;
+extern phys_addr_t total_memory;
+extern phys_addr_t total_lowmem;
 extern phys_addr_t memstart_addr;
 extern phys_addr_t lowmem_end_addr;
 

commit 7c5c4325d2d911fe54db3bc14149bfa558ae0acb
Author: Becky Bruce <becky.bruce@freescale.com>
Date:   Sat Jun 14 09:41:42 2008 +1000

    powerpc: Change BAT code to use phys_addr_t
    
    Currently, the physical address is an unsigned long, but it should
    be phys_addr_t in set_bat, [v/p]_mapped_by_bat.  Also, create a
    macro that can convert a large physical address into the correct
    format for programming the BAT registers.
    
    Signed-off-by: Becky Bruce <becky.bruce@freescale.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 04802252a64f..46585b7bb194 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -29,7 +29,7 @@ extern void hash_preload(struct mm_struct *mm, unsigned long ea,
 #ifdef CONFIG_PPC32
 extern void mapin_ram(void);
 extern int map_page(unsigned long va, phys_addr_t pa, int flags);
-extern void setbat(int index, unsigned long virt, unsigned long phys,
+extern void setbat(int index, unsigned long virt, phys_addr_t phys,
 		   unsigned int size, int flags);
 extern void settlbcam(int index, unsigned long virt, phys_addr_t phys,
 		      unsigned int size, int flags, unsigned int pid);

commit 09b5e63f827016732d956abb7a4c74a312d20521
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 16 05:52:25 2008 +1000

    [POWERPC] Rename __initial_memory_limit to __initial_memory_limit_addr
    
    We always use __initial_memory_limit as an address so rename it
    to be clear.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 67477e772958..04802252a64f 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -48,7 +48,7 @@ extern unsigned int num_tlbcam_entries;
 
 extern unsigned long ioremap_bot;
 extern unsigned long __max_low_memory;
-extern unsigned long __initial_memory_limit;
+extern phys_addr_t __initial_memory_limit_addr;
 extern unsigned long total_memory;
 extern unsigned long total_lowmem;
 extern phys_addr_t memstart_addr;

commit d7917ba7051e3fd12ebe2d5a09b29fb3a2b38190
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 16 05:52:22 2008 +1000

    [POWERPC] Introduce lowmem_end_addr to distinguish from total_lowmem
    
    total_lowmem represents the amount of low memory, not the physical
    address that low memory ends at.  If the start of memory is at 0 it
    happens that total_lowmem can be used as both the size and the address
    that lowmem ends at (or more specifically one byte beyond the end).
    
    To make the code a bit more clear and deal with the case when the start of
    memory isn't at physical 0, we introduce lowmem_end_addr that represents
    one byte beyond the last physical address in the lowmem region.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 5bc11f5933a9..67477e772958 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -52,6 +52,7 @@ extern unsigned long __initial_memory_limit;
 extern unsigned long total_memory;
 extern unsigned long total_lowmem;
 extern phys_addr_t memstart_addr;
+extern phys_addr_t lowmem_end_addr;
 
 /* ...and now those things that may be slightly different between processor
  * architectures.  -- Dan

commit 99c62dd773797b68f3b1ca6bb3274725d1852fa2
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Apr 16 05:52:21 2008 +1000

    [POWERPC] Remove and replace uses of PPC_MEMSTART with memstart_addr
    
    A number of users of PPC_MEMSTART (40x, ppc_mmu_32) can just always
    use 0 as we don't support booting these kernels at non-zero physical
    addresses since their exception vectors must be at 0 (or 0xfffx_xxxx).
    
    For the sub-arches that support relocatable interrupt vectors
    (book-e), it's reasonable to have memory start at a non-zero physical
    address.  For those cases use the variable memstart_addr instead of
    the #define PPC_MEMSTART since the only uses of PPC_MEMSTART are for
    initialization and in the future we can set memstart_addr at runtime
    to have a relocatable kernel.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index ebfd13dc9d19..5bc11f5933a9 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -51,6 +51,7 @@ extern unsigned long __max_low_memory;
 extern unsigned long __initial_memory_limit;
 extern unsigned long total_memory;
 extern unsigned long total_lowmem;
+extern phys_addr_t memstart_addr;
 
 /* ...and now those things that may be slightly different between processor
  * architectures.  -- Dan

commit 0b47759db54f82df68ed179ddc5cb2becea56158
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Nov 20 18:32:12 2007 +1100

    [POWERPC] Fix 8xx build breakage due to _tlbie changes
    
    My changes to _tlbie to fix 4xx unfortunately broke 8xx build in a
    couple of places.  This fixes it.
    
    Spotted by Olof Johansson.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Vitaly Bordug <vitb@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index eb3a732e91db..ebfd13dc9d19 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -56,7 +56,7 @@ extern unsigned long total_lowmem;
  * architectures.  -- Dan
  */
 #if defined(CONFIG_8xx)
-#define flush_HPTE(X, va, pg)	_tlbie(va)
+#define flush_HPTE(X, va, pg)	_tlbie(va, 0 /* 8xx doesn't care about PID */)
 #define MMU_init_hw()		do { } while(0)
 #define mmu_mapin_ram()		(0UL)
 

commit e701d269aa28996f3502780951fe1b12d5d66b49
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Oct 30 09:46:06 2007 +1100

    [POWERPC] 4xx: Fix 4xx flush_tlb_page()
    
    On 4xx CPUs, the current implementation of flush_tlb_page() uses
    a low level _tlbie() assembly function that only works for the
    current PID. Thus, invalidations caused by, for example, a COW
    fault triggered by get_user_pages() from a different context will
    not work properly, causing among other things, gdb breakpoints
    to fail.
    
    This patch adds a "pid" argument to _tlbie() on 4xx processors,
    and uses it to flush entries in the right context. FSL BookE
    also gets the argument but it seems they don't need it (their
    tlbivax form ignores the PID when invalidating according to the
    document I have).
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Josh Boyer <jwboyer@linux.vnet.ibm.com>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index c94a64fd3c01..eb3a732e91db 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -61,12 +61,12 @@ extern unsigned long total_lowmem;
 #define mmu_mapin_ram()		(0UL)
 
 #elif defined(CONFIG_4xx)
-#define flush_HPTE(X, va, pg)	_tlbie(va)
+#define flush_HPTE(pid, va, pg)	_tlbie(va, pid)
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(void);
 
 #elif defined(CONFIG_FSL_BOOKE)
-#define flush_HPTE(X, va, pg)	_tlbie(va)
+#define flush_HPTE(pid, va, pg)	_tlbie(va, pid)
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(void);
 extern void adjust_total_lowmem(void);

commit 8e561e7eda02819c711a75b64a000bf34948cdbb
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Jun 13 14:52:56 2007 +1000

    [POWERPC] Kill typedef-ed structs for hash PTEs and BATs
    
    Using typedefs to rename structure types if frowned on by CodingStyle.
    However, we do so for the hash PTE structure on both ppc32 (where it's
    called "PTE") and ppc64 (where it's called "hpte_t").  On ppc32 we
    also have such a typedef for the BATs ("BAT").
    
    This removes this unhelpful use of typedefs, in the process
    bringing ppc32 and ppc64 closer together, by using the name "struct
    hash_pte" in both cases.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 69cd1c617cdc..c94a64fd3c01 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -39,8 +39,8 @@ extern int __map_without_bats;
 extern unsigned long ioremap_base;
 extern unsigned int rtas_data, rtas_size;
 
-struct _PTE;
-extern struct _PTE *Hash, *Hash_end;
+struct hash_pte;
+extern struct hash_pte *Hash, *Hash_end;
 extern unsigned long Hash_size, Hash_mask;
 
 extern unsigned int num_tlbcam_entries;

commit f21f49ea639ac3f24824177dac1268af75a2d373
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Jun 13 14:52:54 2007 +1000

    [POWERPC] Remove the dregs of APUS support from arch/powerpc
    
    APUS (the Amiga Power-Up System) is not supported under arch/powerpc
    and it's unlikely it ever will be.  Therefore, this patch removes the
    fragments of APUS support code from arch/powerpc which have been
    copied from arch/ppc.
    
    A few APUS references are left in asm-powerpc in .h files which are
    still used from arch/ppc.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index f7a4066a57ea..69cd1c617cdc 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -8,7 +8,6 @@
  *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)
  *  and Cort Dougan (PReP) (cort@cs.nmt.edu)
  *    Copyright (C) 1996 Paul Mackerras
- *  Amiga/APUS changes by Jesper Skov (jskov@cygnus.co.uk).
  *
  *  Derived from "arch/i386/mm/init.c"
  *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds

commit 3d5134ee8341bffc4f539049abb9e90d469b448d
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Mon Jun 4 15:15:36 2007 +1000

    [POWERPC] Rewrite IO allocation & mapping on powerpc64
    
    This rewrites pretty much from scratch the handling of MMIO and PIO
    space allocations on powerpc64.  The main goals are:
    
     - Get rid of imalloc and use more common code where possible
     - Simplify the current mess so that PIO space is allocated and
       mapped in a single place for PCI bridges
     - Handle allocation constraints of PIO for all bridges including
       hot plugged ones within the 2GB space reserved for IO ports,
       so that devices on hotplugged busses will now work with drivers
       that assume IO ports fit in an int.
     - Cleanup and separate tracking of the ISA space in the reserved
       low 64K of IO space. No ISA -> Nothing mapped there.
    
    I booted a cell blade with IDE on PIO and MMIO and a dual G5 so
    far, that's it :-)
    
    With this patch, all allocations are done using the code in
    mm/vmalloc.c, though we use the low level __get_vm_area with
    explicit start/stop constraints in order to manage separate
    areas for vmalloc/vmap, ioremap, and PCI IOs.
    
    This greatly simplifies a lot of things, as you can see in the
    diffstat of that patch :-)
    
    A new pair of functions pcibios_map/unmap_io_space() now replace
    all of the previous code that used to manipulate PCI IOs space.
    The allocation is done at mapping time, which is now called from
    scan_phb's, just before the devices are probed (instead of after,
    which is by itself a bug fix). The only other caller is the PCI
    hotplug code for hot adding PCI-PCI bridges (slots).
    
    imalloc is gone, as is the "sub-allocation" thing, but I do beleive
    that hotplug should still work in the sense that the space allocation
    is always done by the PHB, but if you unmap a child bus of this PHB
    (which seems to be possible), then the code should properly tear
    down all the HPTE mappings for that area of the PHB allocated IO space.
    
    I now always reserve the first 64K of IO space for the bridge with
    the ISA bus on it. I have moved the code for tracking ISA in a separate
    file which should also make it smarter if we ever are capable of
    hot unplugging or re-plugging an ISA bridge.
    
    This should have a side effect on platforms like powermac where VGA IOs
    will no longer work. This is done on purpose though as they would have
    worked semi-randomly before. The idea at this point is to isolate drivers
    that might need to access those and fix them by providing a proper
    function to obtain an offset to the legacy IOs of a given bus.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 2558c34eedaa..f7a4066a57ea 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -90,16 +90,4 @@ static inline void flush_HPTE(unsigned context, unsigned long va,
 	else
 		_tlbie(va);
 }
-#else /* CONFIG_PPC64 */
-/* imalloc region types */
-#define IM_REGION_UNUSED	0x1
-#define IM_REGION_SUBSET	0x2
-#define IM_REGION_EXISTS	0x4
-#define IM_REGION_OVERLAP	0x8
-#define IM_REGION_SUPERSET	0x10
-
-extern struct vm_struct * im_get_free_area(unsigned long size);
-extern struct vm_struct * im_get_area(unsigned long v_addr, unsigned long size,
-				      int region_type);
-extern void im_free(void *addr);
 #endif

commit 57d7909e0d2dd54567ae775e22b14076b777042a
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Mon Apr 30 14:06:25 2007 +1000

    [POWERPC] Revise PPC44x MMU code for arch/powerpc
    
    This patch takes the definitions for the PPC44x MMU (a software loaded
    TLB) from asm-ppc/mmu.h, cleans them up of things no longer necessary
    in arch/powerpc and puts them in a new asm-powerpc/mmu_44x.h file.  It
    also substantially simplifies arch/powerpc/mm/44x_mmu.c and makes a
    couple of small fixes necessary for the 44x MMU code to build and work
    properly in arch/powerpc.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 9c4538bb04b0..2558c34eedaa 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -40,7 +40,8 @@ extern int __map_without_bats;
 extern unsigned long ioremap_base;
 extern unsigned int rtas_data, rtas_size;
 
-extern PTE *Hash, *Hash_end;
+struct _PTE;
+extern struct _PTE *Hash, *Hash_end;
 extern unsigned long Hash_size, Hash_mask;
 
 extern unsigned int num_tlbcam_entries;

commit 621023072524fc0155ed16490255e1ea3aa11585
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Tue Apr 24 13:09:12 2007 +1000

    [POWERPC] Cleanup and fix breakage in tlbflush.h
    
    BenH's commit a741e67969577163a4cfc78d7fd2753219087ef1 in powerpc.git,
    although (AFAICT) only intended to affect ppc64, also has side-effects
    which break 44x.  I think 40x, 8xx and Freescale Book E are also
    affected, though I haven't tested them.
    
    The problem lies in unconditionally removing flush_tlb_pending() from
    the versions of flush_tlb_mm(), flush_tlb_range() and
    flush_tlb_kernel_range() used on ppc64 - which are also used the
    embedded platforms mentioned above.
    
    The patch below cleans up the convoluted #ifdef logic in tlbflush.h,
    in the process restoring the necessary flushes for the software TLB
    platforms.  There are three sets of definitions for the flushing
    hooks: the software TLB versions (revised to avoid using names which
    appear to related to TLB batching), the 32-bit hash based versions
    (external functions) amd the 64-bit hash based versions (which
    implement batching).
    
    It also moves the declaration of update_mmu_cache() to always be in
    tlbflush.h (previously it was in tlbflush.h except for PPC64, where it
    was in pgtable.h).
    
    Booted on Ebony (440GP) and compiled for 64-bit and 32-bit
    multiplatform.
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Acked-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index ee55e0bb28bc..9c4538bb04b0 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -19,6 +19,7 @@
  *  2 of the License, or (at your option) any later version.
  *
  */
+#include <linux/mm.h>
 #include <asm/tlbflush.h>
 #include <asm/mmu.h>
 

commit ee4f2ea48674b6c9d91bc854edc51a3e6a7168c4
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Thu Apr 12 15:30:22 2007 +1000

    [POWERPC] Fix 32-bit mm operations when not using BATs
    
    On hash table based 32 bits powerpc's, the hash management code runs with
    a big spinlock. It's thus important that it never causes itself a hash
    fault. That code is generally safe (it does memory accesses in real mode
    among other things) with the exception of the actual access to the code
    itself. That is, the kernel text needs to be accessible without taking
    a hash miss exceptions.
    
    This is currently guaranteed by having a BAT register mapping part of the
    linear mapping permanently, which includes the kernel text. But this is
    not true if using the "nobats" kernel command line option (which can be
    useful for debugging) and will not be true when using DEBUG_PAGEALLOC
    implemented in a subsequent patch.
    
    This patch fixes this by pre-faulting in the hash table pages that hit
    the kernel text, and making sure we never evict such a page under hash
    pressure.
    
    Signed-off-by: Benjamin Herrenchmidt <benh@kernel.crashing.org>
    
     arch/powerpc/mm/hash_low_32.S |   22 ++++++++++++++++++++--
     arch/powerpc/mm/mem.c         |    3 ---
     arch/powerpc/mm/mmu_decl.h    |    4 ++++
     arch/powerpc/mm/pgtable_32.c  |   11 +++++++----
     4 files changed, 31 insertions(+), 9 deletions(-)
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index bea2d21ac6f7..ee55e0bb28bc 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -22,6 +22,10 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu.h>
 
+extern void hash_preload(struct mm_struct *mm, unsigned long ea,
+			 unsigned long access, unsigned long trap);
+
+
 #ifdef CONFIG_PPC32
 extern void mapin_ram(void);
 extern int map_page(unsigned long va, phys_addr_t pa, int flags);

commit 800fc3eeb0eed3bf98d621c0da24d68cabcf6526
Author: David Gibson <david@gibson.dropbear.id.au>
Date:   Wed Nov 16 15:43:48 2005 +1100

    [PATCH] powerpc: Remove imalloc.h
    
    asm-ppc64/imalloc.h is only included from files in arch/powerpc/mm.
    We already have a header for mm local definitions,
    arch/powerpc/mm/mmu_decl.h.  Thus, this patch moves the contents of
    imalloc.h into mmu_decl.h.  The only exception are the definitions of
    PHBS_IO_BASE, IMALLOC_BASE and IMALLOC_END.  Those are moved into
    pgtable.h, next to similar definitions of VMALLOC_START and
    VMALLOC_SIZE.
    
    Built for multiplatform 32bit and 64bit (ARCH=powerpc).
    
    Signed-off-by: David Gibson <david@gibson.dropbear.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index a4d7a327c0e5..bea2d21ac6f7 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -33,7 +33,6 @@ extern void invalidate_tlbcam_entry(int index);
 
 extern int __map_without_bats;
 extern unsigned long ioremap_base;
-extern unsigned long ioremap_bot;
 extern unsigned int rtas_data, rtas_size;
 
 extern PTE *Hash, *Hash_end;
@@ -42,6 +41,7 @@ extern unsigned long Hash_size, Hash_mask;
 extern unsigned int num_tlbcam_entries;
 #endif
 
+extern unsigned long ioremap_bot;
 extern unsigned long __max_low_memory;
 extern unsigned long __initial_memory_limit;
 extern unsigned long total_memory;
@@ -84,4 +84,16 @@ static inline void flush_HPTE(unsigned context, unsigned long va,
 	else
 		_tlbie(va);
 }
+#else /* CONFIG_PPC64 */
+/* imalloc region types */
+#define IM_REGION_UNUSED	0x1
+#define IM_REGION_SUBSET	0x2
+#define IM_REGION_EXISTS	0x4
+#define IM_REGION_OVERLAP	0x8
+#define IM_REGION_SUPERSET	0x10
+
+extern struct vm_struct * im_get_free_area(unsigned long size);
+extern struct vm_struct * im_get_area(unsigned long v_addr, unsigned long size,
+				      int region_type);
+extern void im_free(void *addr);
 #endif

commit ab1f9dac6eea25ee59e4c8e1cf0b7476afbbfe07
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Oct 10 21:58:35 2005 +1000

    powerpc: Merge arch/ppc64/mm to arch/powerpc/mm
    
    This moves the remaining files in arch/ppc64/mm to arch/powerpc/mm,
    and arranges that we use them when compiling with ARCH=ppc64.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 06fe8af3af55..a4d7a327c0e5 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -22,11 +22,11 @@
 #include <asm/tlbflush.h>
 #include <asm/mmu.h>
 
+#ifdef CONFIG_PPC32
 extern void mapin_ram(void);
 extern int map_page(unsigned long va, phys_addr_t pa, int flags);
 extern void setbat(int index, unsigned long virt, unsigned long phys,
 		   unsigned int size, int flags);
-extern void reserve_phys_mem(unsigned long start, unsigned long size);
 extern void settlbcam(int index, unsigned long virt, phys_addr_t phys,
 		      unsigned int size, int flags, unsigned int pid);
 extern void invalidate_tlbcam_entry(int index);
@@ -36,16 +36,16 @@ extern unsigned long ioremap_base;
 extern unsigned long ioremap_bot;
 extern unsigned int rtas_data, rtas_size;
 
-extern unsigned long __max_low_memory;
-extern unsigned long __initial_memory_limit;
-extern unsigned long total_memory;
-extern unsigned long total_lowmem;
-extern int mem_init_done;
-
 extern PTE *Hash, *Hash_end;
 extern unsigned long Hash_size, Hash_mask;
 
 extern unsigned int num_tlbcam_entries;
+#endif
+
+extern unsigned long __max_low_memory;
+extern unsigned long __initial_memory_limit;
+extern unsigned long total_memory;
+extern unsigned long total_lowmem;
 
 /* ...and now those things that may be slightly different between processor
  * architectures.  -- Dan
@@ -66,8 +66,8 @@ extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(void);
 extern void adjust_total_lowmem(void);
 
-#else
-/* anything except 4xx or 8xx */
+#elif defined(CONFIG_PPC32)
+/* anything 32-bit except 4xx or 8xx */
 extern void MMU_init_hw(void);
 extern unsigned long mmu_mapin_ram(void);
 

commit 7c8c6b9776fb41134d87ef50706a777a45d61cd4
Author: Paul Mackerras <paulus@samba.org>
Date:   Thu Oct 6 12:23:33 2005 +1000

    powerpc: Merge lmb.c and make MM initialization use it.
    
    This also creates merged versions of do_init_bootmem, paging_init
    and mem_init and moves them to arch/powerpc/mm/mem.c.  It gets rid
    of the mem_pieces stuff.
    
    I made memory_limit a parameter to lmb_enforce_memory_limit rather
    than a global referenced by that function.  This will require some
    small changes to ppc64 if we want to continue building ARCH=ppc64
    using the merged lmb.c.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
index 540f3292b229..06fe8af3af55 100644
--- a/arch/powerpc/mm/mmu_decl.h
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -36,6 +36,8 @@ extern unsigned long ioremap_base;
 extern unsigned long ioremap_bot;
 extern unsigned int rtas_data, rtas_size;
 
+extern unsigned long __max_low_memory;
+extern unsigned long __initial_memory_limit;
 extern unsigned long total_memory;
 extern unsigned long total_lowmem;
 extern int mem_init_done;

commit 14cf11af6cf608eb8c23e989ddb17a715ddce109
Author: Paul Mackerras <paulus@samba.org>
Date:   Mon Sep 26 16:04:21 2005 +1000

    powerpc: Merge enough to start building in arch/powerpc.
    
    This creates the directory structure under arch/powerpc and a bunch
    of Kconfig files.  It does a first-cut merge of arch/powerpc/mm,
    arch/powerpc/lib and arch/powerpc/platforms/powermac.  This is enough
    to build a 32-bit powermac kernel with ARCH=powerpc.
    
    For now we are getting some unmerged files from arch/ppc/kernel and
    arch/ppc/syslib, or arch/ppc64/kernel.  This makes some minor changes
    to files in those directories and files outside arch/powerpc.
    
    The boot directory is still not merged.  That's going to be interesting.
    
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/mm/mmu_decl.h b/arch/powerpc/mm/mmu_decl.h
new file mode 100644
index 000000000000..540f3292b229
--- /dev/null
+++ b/arch/powerpc/mm/mmu_decl.h
@@ -0,0 +1,85 @@
+/*
+ * Declarations of procedures and variables shared between files
+ * in arch/ppc/mm/.
+ *
+ *  Derived from arch/ppc/mm/init.c:
+ *    Copyright (C) 1995-1996 Gary Thomas (gdt@linuxppc.org)
+ *
+ *  Modifications by Paul Mackerras (PowerMac) (paulus@cs.anu.edu.au)
+ *  and Cort Dougan (PReP) (cort@cs.nmt.edu)
+ *    Copyright (C) 1996 Paul Mackerras
+ *  Amiga/APUS changes by Jesper Skov (jskov@cygnus.co.uk).
+ *
+ *  Derived from "arch/i386/mm/init.c"
+ *    Copyright (C) 1991, 1992, 1993, 1994  Linus Torvalds
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; either version
+ *  2 of the License, or (at your option) any later version.
+ *
+ */
+#include <asm/tlbflush.h>
+#include <asm/mmu.h>
+
+extern void mapin_ram(void);
+extern int map_page(unsigned long va, phys_addr_t pa, int flags);
+extern void setbat(int index, unsigned long virt, unsigned long phys,
+		   unsigned int size, int flags);
+extern void reserve_phys_mem(unsigned long start, unsigned long size);
+extern void settlbcam(int index, unsigned long virt, phys_addr_t phys,
+		      unsigned int size, int flags, unsigned int pid);
+extern void invalidate_tlbcam_entry(int index);
+
+extern int __map_without_bats;
+extern unsigned long ioremap_base;
+extern unsigned long ioremap_bot;
+extern unsigned int rtas_data, rtas_size;
+
+extern unsigned long total_memory;
+extern unsigned long total_lowmem;
+extern int mem_init_done;
+
+extern PTE *Hash, *Hash_end;
+extern unsigned long Hash_size, Hash_mask;
+
+extern unsigned int num_tlbcam_entries;
+
+/* ...and now those things that may be slightly different between processor
+ * architectures.  -- Dan
+ */
+#if defined(CONFIG_8xx)
+#define flush_HPTE(X, va, pg)	_tlbie(va)
+#define MMU_init_hw()		do { } while(0)
+#define mmu_mapin_ram()		(0UL)
+
+#elif defined(CONFIG_4xx)
+#define flush_HPTE(X, va, pg)	_tlbie(va)
+extern void MMU_init_hw(void);
+extern unsigned long mmu_mapin_ram(void);
+
+#elif defined(CONFIG_FSL_BOOKE)
+#define flush_HPTE(X, va, pg)	_tlbie(va)
+extern void MMU_init_hw(void);
+extern unsigned long mmu_mapin_ram(void);
+extern void adjust_total_lowmem(void);
+
+#else
+/* anything except 4xx or 8xx */
+extern void MMU_init_hw(void);
+extern unsigned long mmu_mapin_ram(void);
+
+/* Be careful....this needs to be updated if we ever encounter 603 SMPs,
+ * which includes all new 82xx processors.  We need tlbie/tlbsync here
+ * in that case (I think). -- Dan.
+ */
+static inline void flush_HPTE(unsigned context, unsigned long va,
+			      unsigned long pdval)
+{
+	if ((Hash != 0) &&
+	    cpu_has_feature(CPU_FTR_HPTE_TABLE))
+		flush_hash_pages(0, va, pdval, 1);
+	else
+		_tlbie(va);
+}
+#endif
