commit c5ff46d69c410f7fac173e4fde3eea484b4b4eda
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Fri May 22 23:33:18 2020 +1000

    powerpc: Add ppc_inst_next()
    
    In a few places we want to calculate the address of the next
    instruction. Previously that was simple, we just added 4 bytes, or if
    using a u32 * we incremented that pointer by 1.
    
    But prefixed instructions make it more complicated, we need to advance
    by either 4 or 8 bytes depending on the actual instruction. We also
    can't do pointer arithmetic using struct ppc_inst, because it is
    always 8 bytes in size on 64-bit, even though we might only need to
    advance by 4 bytes.
    
    So add a ppc_inst_next() helper which calculates the location of the
    next instruction, if the given instruction was located at the given
    address. Note the instruction doesn't need to actually be at the
    address in memory.
    
    Although it would seem natural for the value to be passed by value,
    that makes it too easy to write a loop that will read off the end of a
    page, eg:
    
            for (; src < end; src = ppc_inst_next(src, *src),
                              dest = ppc_inst_next(dest, *dest))
    
    As noticed by Christophe and Jordan, if end is the exact end of a
    page, and the next page is not mapped, this will fault, because *dest
    will read 8 bytes, 4 bytes into the next page.
    
    So value is passed by reference, so the helper can be careful to use
    ppc_inst_read() on it.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Jordan Niethe <jniethe5@gmail.com>
    Link: https://lore.kernel.org/r/20200522133318.1681406-1-mpe@ellerman.id.au

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 80f320c2e189..4c0a7ee9fa00 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -68,7 +68,7 @@ static int patch_alt_instruction(struct ppc_inst *src, struct ppc_inst *dest,
 
 static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
 {
-	struct ppc_inst *start, *end, *alt_start, *alt_end, *src, *dest;
+	struct ppc_inst *start, *end, *alt_start, *alt_end, *src, *dest, nop;
 
 	start = calc_addr(fcur, fcur->start_off);
 	end = calc_addr(fcur, fcur->end_off);
@@ -84,14 +84,15 @@ static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
 	src = alt_start;
 	dest = start;
 
-	for (; src < alt_end; src = (void *)src + ppc_inst_len(ppc_inst_read(src)),
-	     (dest = (void *)dest + ppc_inst_len(ppc_inst_read(dest)))) {
+	for (; src < alt_end; src = ppc_inst_next(src, src),
+			      dest = ppc_inst_next(dest, dest)) {
 		if (patch_alt_instruction(src, dest, alt_start, alt_end))
 			return 1;
 	}
 
-	for (; dest < end; dest = (void *)dest + ppc_inst_len(ppc_inst(PPC_INST_NOP)))
-		raw_patch_instruction(dest, ppc_inst(PPC_INST_NOP));
+	nop = ppc_inst(PPC_INST_NOP);
+	for (; dest < end; dest = ppc_inst_next(dest, &nop))
+		raw_patch_instruction(dest, nop);
 
 	return 0;
 }
@@ -405,8 +406,8 @@ static void do_final_fixups(void)
 	while (src < end) {
 		inst = ppc_inst_read(src);
 		raw_patch_instruction(dest, inst);
-		src = (void *)src + ppc_inst_len(inst);
-		dest = (void *)dest + ppc_inst_len(inst);
+		src = ppc_inst_next(src, src);
+		dest = ppc_inst_next(dest, dest);
 	}
 #endif
 }

commit 785b79d1e02873c2088ee1301154c66dace66ce5
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Wed May 6 13:40:45 2020 +1000

    powerpc: Test prefixed instructions in feature fixups
    
    Expand the feature-fixups self-tests to includes tests for prefixed
    instructions.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    [mpe: Use CONFIG_PPC64 not __powerpc64__, add empty inlines]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-26-jniethe5@gmail.com

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 1fb845f60f43..80f320c2e189 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -689,6 +689,78 @@ static void test_lwsync_macros(void)
 	}
 }
 
+#ifdef CONFIG_PPC64
+static void __init test_prefix_patching(void)
+{
+	extern unsigned int ftr_fixup_prefix1[];
+	extern unsigned int end_ftr_fixup_prefix1[];
+	extern unsigned int ftr_fixup_prefix1_orig[];
+	extern unsigned int ftr_fixup_prefix1_expected[];
+	int size = sizeof(unsigned int) * (end_ftr_fixup_prefix1 - ftr_fixup_prefix1);
+
+	fixup.value = fixup.mask = 8;
+	fixup.start_off = calc_offset(&fixup, ftr_fixup_prefix1 + 1);
+	fixup.end_off = calc_offset(&fixup, ftr_fixup_prefix1 + 3);
+	fixup.alt_start_off = fixup.alt_end_off = 0;
+
+	/* Sanity check */
+	check(memcmp(ftr_fixup_prefix1, ftr_fixup_prefix1_orig, size) == 0);
+
+	patch_feature_section(0, &fixup);
+	check(memcmp(ftr_fixup_prefix1, ftr_fixup_prefix1_expected, size) == 0);
+	check(memcmp(ftr_fixup_prefix1, ftr_fixup_prefix1_orig, size) != 0);
+}
+
+static void __init test_prefix_alt_patching(void)
+{
+	extern unsigned int ftr_fixup_prefix2[];
+	extern unsigned int end_ftr_fixup_prefix2[];
+	extern unsigned int ftr_fixup_prefix2_orig[];
+	extern unsigned int ftr_fixup_prefix2_expected[];
+	extern unsigned int ftr_fixup_prefix2_alt[];
+	int size = sizeof(unsigned int) * (end_ftr_fixup_prefix2 - ftr_fixup_prefix2);
+
+	fixup.value = fixup.mask = 8;
+	fixup.start_off = calc_offset(&fixup, ftr_fixup_prefix2 + 1);
+	fixup.end_off = calc_offset(&fixup, ftr_fixup_prefix2 + 3);
+	fixup.alt_start_off = calc_offset(&fixup, ftr_fixup_prefix2_alt);
+	fixup.alt_end_off = calc_offset(&fixup, ftr_fixup_prefix2_alt + 2);
+	/* Sanity check */
+	check(memcmp(ftr_fixup_prefix2, ftr_fixup_prefix2_orig, size) == 0);
+
+	patch_feature_section(0, &fixup);
+	check(memcmp(ftr_fixup_prefix2, ftr_fixup_prefix2_expected, size) == 0);
+	check(memcmp(ftr_fixup_prefix2, ftr_fixup_prefix2_orig, size) != 0);
+}
+
+static void __init test_prefix_word_alt_patching(void)
+{
+	extern unsigned int ftr_fixup_prefix3[];
+	extern unsigned int end_ftr_fixup_prefix3[];
+	extern unsigned int ftr_fixup_prefix3_orig[];
+	extern unsigned int ftr_fixup_prefix3_expected[];
+	extern unsigned int ftr_fixup_prefix3_alt[];
+	int size = sizeof(unsigned int) * (end_ftr_fixup_prefix3 - ftr_fixup_prefix3);
+
+	fixup.value = fixup.mask = 8;
+	fixup.start_off = calc_offset(&fixup, ftr_fixup_prefix3 + 1);
+	fixup.end_off = calc_offset(&fixup, ftr_fixup_prefix3 + 4);
+	fixup.alt_start_off = calc_offset(&fixup, ftr_fixup_prefix3_alt);
+	fixup.alt_end_off = calc_offset(&fixup, ftr_fixup_prefix3_alt + 3);
+	/* Sanity check */
+	check(memcmp(ftr_fixup_prefix3, ftr_fixup_prefix3_orig, size) == 0);
+
+	patch_feature_section(0, &fixup);
+	check(memcmp(ftr_fixup_prefix3, ftr_fixup_prefix3_expected, size) == 0);
+	patch_feature_section(0, &fixup);
+	check(memcmp(ftr_fixup_prefix3, ftr_fixup_prefix3_orig, size) != 0);
+}
+#else
+static inline void test_prefix_patching(void) {}
+static inline void test_prefix_alt_patching(void) {}
+static inline void test_prefix_word_alt_patching(void) {}
+#endif /* CONFIG_PPC64 */
+
 static int __init test_feature_fixups(void)
 {
 	printk(KERN_DEBUG "Running feature fixup self-tests ...\n");
@@ -703,6 +775,9 @@ static int __init test_feature_fixups(void)
 	test_cpu_macros();
 	test_fw_macros();
 	test_lwsync_macros();
+	test_prefix_patching();
+	test_prefix_alt_patching();
+	test_prefix_word_alt_patching();
 
 	return 0;
 }

commit 650b55b707fdfa764e9f2b81314d3eb4216fb962
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Fri May 15 12:12:55 2020 +1000

    powerpc: Add prefixed instructions to instruction data type
    
    For powerpc64, redefine the ppc_inst type so both word and prefixed
    instructions can be represented. On powerpc32 the type will remain the
    same. Update places which had assumed instructions to be 4 bytes long.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    Reviewed-by: Alistair Popple <alistair@popple.id.au>
    [mpe: Rework the get_user_inst() macros to be parameterised, and don't
          assign to the dest if an error occurred. Use CONFIG_PPC64 not
          __powerpc64__ in a few places. Address other comments from
          Christophe. Fix some sparse complaints.]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-24-jniethe5@gmail.com

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 0c9ffdef8096..1fb845f60f43 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -84,12 +84,13 @@ static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
 	src = alt_start;
 	dest = start;
 
-	for (; src < alt_end; src++, dest++) {
+	for (; src < alt_end; src = (void *)src + ppc_inst_len(ppc_inst_read(src)),
+	     (dest = (void *)dest + ppc_inst_len(ppc_inst_read(dest)))) {
 		if (patch_alt_instruction(src, dest, alt_start, alt_end))
 			return 1;
 	}
 
-	for (; dest < end; dest++)
+	for (; dest < end; dest = (void *)dest + ppc_inst_len(ppc_inst(PPC_INST_NOP)))
 		raw_patch_instruction(dest, ppc_inst(PPC_INST_NOP));
 
 	return 0;

commit 622cf6f436a12338bbcfbb3474629755547fd112
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Wed May 6 13:40:37 2020 +1000

    powerpc: Introduce a function for reporting instruction length
    
    Currently all instructions have the same length, but in preparation for
    prefixed instructions introduce a function for returning instruction
    length.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Alistair Popple <alistair@popple.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-18-jniethe5@gmail.com

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 3c55097d406d..0c9ffdef8096 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -392,20 +392,20 @@ void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 static void do_final_fixups(void)
 {
 #if defined(CONFIG_PPC64) && defined(CONFIG_RELOCATABLE)
-	struct ppc_inst *src, *dest;
-	unsigned long length;
+	struct ppc_inst inst, *src, *dest, *end;
 
 	if (PHYSICAL_START == 0)
 		return;
 
 	src = (struct ppc_inst *)(KERNELBASE + PHYSICAL_START);
 	dest = (struct ppc_inst *)KERNELBASE;
-	length = (__end_interrupts - _stext) / sizeof(struct ppc_inst);
+	end = (void *)src + (__end_interrupts - _stext);
 
-	while (length--) {
-		raw_patch_instruction(dest, ppc_inst_read(src));
-		src++;
-		dest++;
+	while (src < end) {
+		inst = ppc_inst_read(src);
+		raw_patch_instruction(dest, inst);
+		src = (void *)src + ppc_inst_len(inst);
+		dest = (void *)dest + ppc_inst_len(inst);
 	}
 #endif
 }

commit f8faaffaa7d99028e457ef2d1dcb43a98f736938
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Wed May 6 13:40:32 2020 +1000

    powerpc: Use a function for reading instructions
    
    Prefixed instructions will mean there are instructions of different
    length. As a result dereferencing a pointer to an instruction will not
    necessarily give the desired result. Introduce a function for reading
    instructions from memory into the instruction data type.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Alistair Popple <alistair@popple.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-13-jniethe5@gmail.com

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index fd978b8ee6d6..3c55097d406d 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -48,7 +48,7 @@ static int patch_alt_instruction(struct ppc_inst *src, struct ppc_inst *dest,
 	int err;
 	struct ppc_inst instr;
 
-	instr = *src;
+	instr = ppc_inst_read(src);
 
 	if (instr_is_relative_branch(*src)) {
 		struct ppc_inst *target = (struct ppc_inst *)branch_target(src);
@@ -403,7 +403,7 @@ static void do_final_fixups(void)
 	length = (__end_interrupts - _stext) / sizeof(struct ppc_inst);
 
 	while (length--) {
-		raw_patch_instruction(dest, *src);
+		raw_patch_instruction(dest, ppc_inst_read(src));
 		src++;
 		dest++;
 	}

commit 94afd069d937d84fb4f696eb9a78db4084e43d21
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Wed May 6 13:40:31 2020 +1000

    powerpc: Use a datatype for instructions
    
    Currently unsigned ints are used to represent instructions on powerpc.
    This has worked well as instructions have always been 4 byte words.
    
    However, ISA v3.1 introduces some changes to instructions that mean
    this scheme will no longer work as well. This change is Prefixed
    Instructions. A prefixed instruction is made up of a word prefix
    followed by a word suffix to make an 8 byte double word instruction.
    No matter the endianness of the system the prefix always comes first.
    Prefixed instructions are only planned for powerpc64.
    
    Introduce a ppc_inst type to represent both prefixed and word
    instructions on powerpc64 while keeping it possible to exclusively
    have word instructions on powerpc32.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    [mpe: Fix compile error in emulate_spe()]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-12-jniethe5@gmail.com

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 6e7479b8887a..fd978b8ee6d6 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -32,26 +32,26 @@ struct fixup_entry {
 	long		alt_end_off;
 };
 
-static unsigned int *calc_addr(struct fixup_entry *fcur, long offset)
+static struct ppc_inst *calc_addr(struct fixup_entry *fcur, long offset)
 {
 	/*
 	 * We store the offset to the code as a negative offset from
 	 * the start of the alt_entry, to support the VDSO. This
 	 * routine converts that back into an actual address.
 	 */
-	return (unsigned int *)((unsigned long)fcur + offset);
+	return (struct ppc_inst *)((unsigned long)fcur + offset);
 }
 
-static int patch_alt_instruction(unsigned int *src, unsigned int *dest,
-				 unsigned int *alt_start, unsigned int *alt_end)
+static int patch_alt_instruction(struct ppc_inst *src, struct ppc_inst *dest,
+				 struct ppc_inst *alt_start, struct ppc_inst *alt_end)
 {
 	int err;
-	unsigned int instr;
+	struct ppc_inst instr;
 
 	instr = *src;
 
 	if (instr_is_relative_branch(*src)) {
-		unsigned int *target = (unsigned int *)branch_target(src);
+		struct ppc_inst *target = (struct ppc_inst *)branch_target(src);
 
 		/* Branch within the section doesn't need translating */
 		if (target < alt_start || target > alt_end) {
@@ -68,7 +68,7 @@ static int patch_alt_instruction(unsigned int *src, unsigned int *dest,
 
 static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
 {
-	unsigned int *start, *end, *alt_start, *alt_end, *src, *dest;
+	struct ppc_inst *start, *end, *alt_start, *alt_end, *src, *dest;
 
 	start = calc_addr(fcur, fcur->start_off);
 	end = calc_addr(fcur, fcur->end_off);
@@ -147,15 +147,17 @@ static void do_stf_entry_barrier_fixups(enum stf_barrier_type types)
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
 
-		patch_instruction(dest, ppc_inst(instrs[0]));
+		patch_instruction((struct ppc_inst *)dest, ppc_inst(instrs[0]));
 
 		if (types & STF_BARRIER_FALLBACK)
-			patch_branch(dest + 1, (unsigned long)&stf_barrier_fallback,
+			patch_branch((struct ppc_inst *)(dest + 1),
+				     (unsigned long)&stf_barrier_fallback,
 				     BRANCH_SET_LINK);
 		else
-			patch_instruction(dest + 1, ppc_inst(instrs[1]));
+			patch_instruction((struct ppc_inst *)(dest + 1),
+					  ppc_inst(instrs[1]));
 
-		patch_instruction(dest + 2, ppc_inst(instrs[2]));
+		patch_instruction((struct ppc_inst *)(dest + 2), ppc_inst(instrs[2]));
 	}
 
 	printk(KERN_DEBUG "stf-barrier: patched %d entry locations (%s barrier)\n", i,
@@ -208,12 +210,12 @@ static void do_stf_exit_barrier_fixups(enum stf_barrier_type types)
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
 
-		patch_instruction(dest, ppc_inst(instrs[0]));
-		patch_instruction(dest + 1, ppc_inst(instrs[1]));
-		patch_instruction(dest + 2, ppc_inst(instrs[2]));
-		patch_instruction(dest + 3, ppc_inst(instrs[3]));
-		patch_instruction(dest + 4, ppc_inst(instrs[4]));
-		patch_instruction(dest + 5, ppc_inst(instrs[5]));
+		patch_instruction((struct ppc_inst *)dest, ppc_inst(instrs[0]));
+		patch_instruction((struct ppc_inst *)(dest + 1), ppc_inst(instrs[1]));
+		patch_instruction((struct ppc_inst *)(dest + 2), ppc_inst(instrs[2]));
+		patch_instruction((struct ppc_inst *)(dest + 3), ppc_inst(instrs[3]));
+		patch_instruction((struct ppc_inst *)(dest + 4), ppc_inst(instrs[4]));
+		patch_instruction((struct ppc_inst *)(dest + 5), ppc_inst(instrs[5]));
 	}
 	printk(KERN_DEBUG "stf-barrier: patched %d exit locations (%s barrier)\n", i,
 		(types == STF_BARRIER_NONE)                  ? "no" :
@@ -261,9 +263,9 @@ void do_rfi_flush_fixups(enum l1d_flush_type types)
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
 
-		patch_instruction(dest, ppc_inst(instrs[0]));
-		patch_instruction(dest + 1, ppc_inst(instrs[1]));
-		patch_instruction(dest + 2, ppc_inst(instrs[2]));
+		patch_instruction((struct ppc_inst *)dest, ppc_inst(instrs[0]));
+		patch_instruction((struct ppc_inst *)(dest + 1), ppc_inst(instrs[1]));
+		patch_instruction((struct ppc_inst *)(dest + 2), ppc_inst(instrs[2]));
 	}
 
 	printk(KERN_DEBUG "rfi-flush: patched %d locations (%s flush)\n", i,
@@ -296,7 +298,7 @@ void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_
 		dest = (void *)start + *start;
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
-		patch_instruction(dest, ppc_inst(instr));
+		patch_instruction((struct ppc_inst *)dest, ppc_inst(instr));
 	}
 
 	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
@@ -339,8 +341,8 @@ void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_
 		dest = (void *)start + *start;
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
-		patch_instruction(dest, ppc_inst(instr[0]));
-		patch_instruction(dest + 1, ppc_inst(instr[1]));
+		patch_instruction((struct ppc_inst *)dest, ppc_inst(instr[0]));
+		patch_instruction((struct ppc_inst *)(dest + 1), ppc_inst(instr[1]));
 	}
 
 	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
@@ -354,7 +356,7 @@ static void patch_btb_flush_section(long *curr)
 	end = (void *)curr + *(curr + 1);
 	for (; start < end; start++) {
 		pr_devel("patching dest %lx\n", (unsigned long)start);
-		patch_instruction(start, ppc_inst(PPC_INST_NOP));
+		patch_instruction((struct ppc_inst *)start, ppc_inst(PPC_INST_NOP));
 	}
 }
 
@@ -373,7 +375,7 @@ void do_btb_flush_fixups(void)
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 {
 	long *start, *end;
-	unsigned int *dest;
+	struct ppc_inst *dest;
 
 	if (!(value & CPU_FTR_LWSYNC))
 		return ;
@@ -390,18 +392,18 @@ void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 static void do_final_fixups(void)
 {
 #if defined(CONFIG_PPC64) && defined(CONFIG_RELOCATABLE)
-	int *src, *dest;
+	struct ppc_inst *src, *dest;
 	unsigned long length;
 
 	if (PHYSICAL_START == 0)
 		return;
 
-	src = (int *)(KERNELBASE + PHYSICAL_START);
-	dest = (int *)KERNELBASE;
-	length = (__end_interrupts - _stext) / sizeof(int);
+	src = (struct ppc_inst *)(KERNELBASE + PHYSICAL_START);
+	dest = (struct ppc_inst *)KERNELBASE;
+	length = (__end_interrupts - _stext) / sizeof(struct ppc_inst);
 
 	while (length--) {
-		raw_patch_instruction(dest, ppc_inst(*src));
+		raw_patch_instruction(dest, *src);
 		src++;
 		dest++;
 	}

commit 753462512868674a788ecc77bb96752efb818785
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Wed May 6 13:40:26 2020 +1000

    powerpc: Use a macro for creating instructions from u32s
    
    In preparation for instructions having a more complex data type start
    using a macro, ppc_inst(), for making an instruction out of a u32.  A
    macro is used so that instructions can be used as initializer elements.
    Currently this does nothing, but it will allow for creating a data type
    that can represent prefixed instructions.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    [mpe: Change include guard to _ASM_POWERPC_INST_H]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Alistair Popple <alistair@popple.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-7-jniethe5@gmail.com

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index b129d7b4e7dd..6e7479b8887a 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -21,6 +21,7 @@
 #include <asm/setup.h>
 #include <asm/security_features.h>
 #include <asm/firmware.h>
+#include <asm/inst.h>
 
 struct fixup_entry {
 	unsigned long	mask;
@@ -89,7 +90,7 @@ static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
 	}
 
 	for (; dest < end; dest++)
-		raw_patch_instruction(dest, PPC_INST_NOP);
+		raw_patch_instruction(dest, ppc_inst(PPC_INST_NOP));
 
 	return 0;
 }
@@ -146,15 +147,15 @@ static void do_stf_entry_barrier_fixups(enum stf_barrier_type types)
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
 
-		patch_instruction(dest, instrs[0]);
+		patch_instruction(dest, ppc_inst(instrs[0]));
 
 		if (types & STF_BARRIER_FALLBACK)
 			patch_branch(dest + 1, (unsigned long)&stf_barrier_fallback,
 				     BRANCH_SET_LINK);
 		else
-			patch_instruction(dest + 1, instrs[1]);
+			patch_instruction(dest + 1, ppc_inst(instrs[1]));
 
-		patch_instruction(dest + 2, instrs[2]);
+		patch_instruction(dest + 2, ppc_inst(instrs[2]));
 	}
 
 	printk(KERN_DEBUG "stf-barrier: patched %d entry locations (%s barrier)\n", i,
@@ -207,12 +208,12 @@ static void do_stf_exit_barrier_fixups(enum stf_barrier_type types)
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
 
-		patch_instruction(dest, instrs[0]);
-		patch_instruction(dest + 1, instrs[1]);
-		patch_instruction(dest + 2, instrs[2]);
-		patch_instruction(dest + 3, instrs[3]);
-		patch_instruction(dest + 4, instrs[4]);
-		patch_instruction(dest + 5, instrs[5]);
+		patch_instruction(dest, ppc_inst(instrs[0]));
+		patch_instruction(dest + 1, ppc_inst(instrs[1]));
+		patch_instruction(dest + 2, ppc_inst(instrs[2]));
+		patch_instruction(dest + 3, ppc_inst(instrs[3]));
+		patch_instruction(dest + 4, ppc_inst(instrs[4]));
+		patch_instruction(dest + 5, ppc_inst(instrs[5]));
 	}
 	printk(KERN_DEBUG "stf-barrier: patched %d exit locations (%s barrier)\n", i,
 		(types == STF_BARRIER_NONE)                  ? "no" :
@@ -260,9 +261,9 @@ void do_rfi_flush_fixups(enum l1d_flush_type types)
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
 
-		patch_instruction(dest, instrs[0]);
-		patch_instruction(dest + 1, instrs[1]);
-		patch_instruction(dest + 2, instrs[2]);
+		patch_instruction(dest, ppc_inst(instrs[0]));
+		patch_instruction(dest + 1, ppc_inst(instrs[1]));
+		patch_instruction(dest + 2, ppc_inst(instrs[2]));
 	}
 
 	printk(KERN_DEBUG "rfi-flush: patched %d locations (%s flush)\n", i,
@@ -295,7 +296,7 @@ void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_
 		dest = (void *)start + *start;
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
-		patch_instruction(dest, instr);
+		patch_instruction(dest, ppc_inst(instr));
 	}
 
 	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
@@ -338,8 +339,8 @@ void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_
 		dest = (void *)start + *start;
 
 		pr_devel("patching dest %lx\n", (unsigned long)dest);
-		patch_instruction(dest, instr[0]);
-		patch_instruction(dest + 1, instr[1]);
+		patch_instruction(dest, ppc_inst(instr[0]));
+		patch_instruction(dest + 1, ppc_inst(instr[1]));
 	}
 
 	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
@@ -353,7 +354,7 @@ static void patch_btb_flush_section(long *curr)
 	end = (void *)curr + *(curr + 1);
 	for (; start < end; start++) {
 		pr_devel("patching dest %lx\n", (unsigned long)start);
-		patch_instruction(start, PPC_INST_NOP);
+		patch_instruction(start, ppc_inst(PPC_INST_NOP));
 	}
 }
 
@@ -382,7 +383,7 @@ void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 
 	for (; start < end; start++) {
 		dest = (void *)start + *start;
-		raw_patch_instruction(dest, PPC_INST_LWSYNC);
+		raw_patch_instruction(dest, ppc_inst(PPC_INST_LWSYNC));
 	}
 }
 
@@ -400,7 +401,7 @@ static void do_final_fixups(void)
 	length = (__end_interrupts - _stext) / sizeof(int);
 
 	while (length--) {
-		raw_patch_instruction(dest, *src);
+		raw_patch_instruction(dest, ppc_inst(*src));
 		src++;
 		dest++;
 	}

commit 7c95d8893fb55869882c9f68f4c94840dc43f18f
Author: Jordan Niethe <jniethe5@gmail.com>
Date:   Wed May 6 13:40:25 2020 +1000

    powerpc: Change calling convention for create_branch() et. al.
    
    create_branch(), create_cond_branch() and translate_branch() return the
    instruction that they create, or return 0 to signal an error. Separate
    these concerns in preparation for an instruction type that is not just
    an unsigned int.  Fill the created instruction to a pointer passed as
    the first parameter to the function and use a non-zero return value to
    signify an error.
    
    Signed-off-by: Jordan Niethe <jniethe5@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Reviewed-by: Alistair Popple <alistair@popple.id.au>
    Link: https://lore.kernel.org/r/20200506034050.24806-6-jniethe5@gmail.com

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 4ba634b89ce5..b129d7b4e7dd 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -44,6 +44,7 @@ static unsigned int *calc_addr(struct fixup_entry *fcur, long offset)
 static int patch_alt_instruction(unsigned int *src, unsigned int *dest,
 				 unsigned int *alt_start, unsigned int *alt_end)
 {
+	int err;
 	unsigned int instr;
 
 	instr = *src;
@@ -53,8 +54,8 @@ static int patch_alt_instruction(unsigned int *src, unsigned int *dest,
 
 		/* Branch within the section doesn't need translating */
 		if (target < alt_start || target > alt_end) {
-			instr = translate_branch(dest, src);
-			if (!instr)
+			err = translate_branch(&instr, dest, src);
+			if (err)
 				return 1;
 		}
 	}

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 5169cc805464..4ba634b89ce5 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  *  Copyright (C) 2001 Ben. Herrenschmidt (benh@kernel.crashing.org)
  *
@@ -5,11 +6,6 @@
  *      Copyright (C) 2003 Dave Engebretsen <engebret@us.ibm.com>
  *
  *  Copyright 2008 Michael Ellerman, IBM Corporation.
- *
- *  This program is free software; you can redistribute it and/or
- *  modify it under the terms of the GNU General Public License
- *  as published by the Free Software Foundation; either version
- *  2 of the License, or (at your option) any later version.
  */
 
 #include <linux/types.h>

commit 76a5eaa38b15dda92cd6964248c39b5a6f3a4e9d
Author: Diana Craciun <diana.craciun@nxp.com>
Date:   Wed Dec 12 16:03:00 2018 +0200

    powerpc/fsl: Add infrastructure to fixup branch predictor flush
    
    In order to protect against speculation attacks (Spectre
    variant 2) on NXP PowerPC platforms, the branch predictor
    should be flushed when the privillege level is changed.
    This patch is adding the infrastructure to fixup at runtime
    the code sections that are performing the branch predictor flush
    depending on a boot arg parameter which is added later in a
    separate patch.
    
    Signed-off-by: Diana Craciun <diana.craciun@nxp.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index ca8478503f18..5169cc805464 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -347,6 +347,29 @@ void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_
 
 	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
 }
+
+static void patch_btb_flush_section(long *curr)
+{
+	unsigned int *start, *end;
+
+	start = (void *)curr + *curr;
+	end = (void *)curr + *(curr + 1);
+	for (; start < end; start++) {
+		pr_devel("patching dest %lx\n", (unsigned long)start);
+		patch_instruction(start, PPC_INST_NOP);
+	}
+}
+
+void do_btb_flush_fixups(void)
+{
+	long *start, *end;
+
+	start = PTRRELOC(&__start__btb_flush_fixup);
+	end = PTRRELOC(&__stop__btb_flush_fixup);
+
+	for (; start < end; start += 2)
+		patch_btb_flush_section(start);
+}
 #endif /* CONFIG_PPC_FSL_BOOK3E */
 
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)

commit 3b30c6e8b962c5d4d8d8b6ae7688380dadc03174
Author: Breno Leitao <leitao@debian.org>
Date:   Mon Oct 22 11:54:17 2018 -0300

    powerpc/lib: Declare static methods
    
    Functions do_stf_{entry,exit}_barrier_fixups are static but not declared as
    such. This was detected by `sparse` tool with the following warning:
    
            arch/powerpc/lib/feature-fixups.c:121:6: warning: symbol 'do_stf_entry_barrier_fixups' was not declared. Should it be static?
    
            arch/powerpc/lib/feature-fixups.c:171:6: warning: symbol 'do_stf_exit_barrier_fixups' was not declared. Should it be static?
    
    This patch declares both functions as static, as they are only called by
    do_stf_barrier_fixups(), which is in the same source code file.
    
    Signed-off-by: Breno Leitao <leitao@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index e613b02bb2f0..ca8478503f18 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -118,7 +118,7 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 }
 
 #ifdef CONFIG_PPC_BOOK3S_64
-void do_stf_entry_barrier_fixups(enum stf_barrier_type types)
+static void do_stf_entry_barrier_fixups(enum stf_barrier_type types)
 {
 	unsigned int instrs[3], *dest;
 	long *start, *end;
@@ -168,7 +168,7 @@ void do_stf_entry_barrier_fixups(enum stf_barrier_type types)
 		                                           : "unknown");
 }
 
-void do_stf_exit_barrier_fixups(enum stf_barrier_type types)
+static void do_stf_exit_barrier_fixups(enum stf_barrier_type types)
 {
 	unsigned int instrs[6], *dest;
 	long *start, *end;

commit ebcd1bfc33c7a90df941df68a6e5d4018c022fba
Author: Diana Craciun <diana.craciun@nxp.com>
Date:   Sat Jul 28 09:06:37 2018 +1000

    powerpc/fsl: Add barrier_nospec implementation for NXP PowerPC Book3E
    
    Implement the barrier_nospec as a isync;sync instruction sequence.
    The implementation uses the infrastructure built for BOOK3S 64.
    
    Signed-off-by: Diana Craciun <diana.craciun@nxp.com>
    [mpe: Split out of larger patch]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 0e604b41b5d1..e613b02bb2f0 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -318,6 +318,37 @@ void do_barrier_nospec_fixups(bool enable)
 }
 #endif /* CONFIG_PPC_BARRIER_NOSPEC */
 
+#ifdef CONFIG_PPC_FSL_BOOK3E
+void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_end)
+{
+	unsigned int instr[2], *dest;
+	long *start, *end;
+	int i;
+
+	start = fixup_start;
+	end = fixup_end;
+
+	instr[0] = PPC_INST_NOP;
+	instr[1] = PPC_INST_NOP;
+
+	if (enable) {
+		pr_info("barrier-nospec: using isync; sync as speculation barrier\n");
+		instr[0] = PPC_INST_ISYNC;
+		instr[1] = PPC_INST_SYNC;
+	}
+
+	for (i = 0; start < end; start++, i++) {
+		dest = (void *)start + *start;
+
+		pr_devel("patching dest %lx\n", (unsigned long)dest);
+		patch_instruction(dest, instr[0]);
+		patch_instruction(dest + 1, instr[1]);
+	}
+
+	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
+}
+#endif /* CONFIG_PPC_FSL_BOOK3E */
+
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 {
 	long *start, *end;

commit 179ab1cbf883575c3a585bcfc0f2160f1d22a149
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sat Jul 28 09:06:34 2018 +1000

    powerpc/64: Add CONFIG_PPC_BARRIER_NOSPEC
    
    Add a config symbol to encode which platforms support the
    barrier_nospec speculation barrier. Currently this is just Book3S 64
    but we will add Book3E in a future patch.
    
    Signed-off-by: Diana Craciun <diana.craciun@nxp.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 8b69f868298c..0e604b41b5d1 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -304,6 +304,9 @@ void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_
 	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
 }
 
+#endif /* CONFIG_PPC_BOOK3S_64 */
+
+#ifdef CONFIG_PPC_BARRIER_NOSPEC
 void do_barrier_nospec_fixups(bool enable)
 {
 	void *start, *end;
@@ -313,8 +316,7 @@ void do_barrier_nospec_fixups(bool enable)
 
 	do_barrier_nospec_fixups_range(enable, start, end);
 }
-
-#endif /* CONFIG_PPC_BOOK3S_64 */
+#endif /* CONFIG_PPC_BARRIER_NOSPEC */
 
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 {

commit c90fca951e90ba470a3dc6087667edffcf8db21b
Merge: c0ab85267e25 ff5bc793e47b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 7 10:23:33 2018 -0700

    Merge tag 'powerpc-4.18-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Notable changes:
    
       - Support for split PMD page table lock on 64-bit Book3S (Power8/9).
    
       - Add support for HAVE_RELIABLE_STACKTRACE, so we properly support
         live patching again.
    
       - Add support for patching barrier_nospec in copy_from_user() and
         syscall entry.
    
       - A couple of fixes for our data breakpoints on Book3S.
    
       - A series from Nick optimising TLB/mm handling with the Radix MMU.
    
       - Numerous small cleanups to squash sparse/gcc warnings from Mathieu
         Malaterre.
    
       - Several series optimising various parts of the 32-bit code from
         Christophe Leroy.
    
       - Removal of support for two old machines, "SBC834xE" and "C2K"
         ("GEFanuc,C2K"), which is why the diffstat has so many deletions.
    
      And many other small improvements & fixes.
    
      There's a few out-of-area changes. Some minor ftrace changes OK'ed by
      Steve, and a fix to our powernv cpuidle driver. Then there's a series
      touching mm, x86 and fs/proc/task_mmu.c, which cleans up some details
      around pkey support. It was ack'ed/reviewed by Ingo & Dave and has
      been in next for several weeks.
    
      Thanks to: Akshay Adiga, Alastair D'Silva, Alexey Kardashevskiy, Al
      Viro, Andrew Donnellan, Aneesh Kumar K.V, Anju T Sudhakar, Arnd
      Bergmann, Balbir Singh, Cédric Le Goater, Christophe Leroy, Christophe
      Lombard, Colin Ian King, Dave Hansen, Fabio Estevam, Finn Thain,
      Frederic Barrat, Gautham R. Shenoy, Haren Myneni, Hari Bathini, Ingo
      Molnar, Jonathan Neuschäfer, Josh Poimboeuf, Kamalesh Babulal,
      Madhavan Srinivasan, Mahesh Salgaonkar, Mark Greer, Mathieu Malaterre,
      Matthew Wilcox, Michael Neuling, Michal Suchanek, Naveen N. Rao,
      Nicholas Piggin, Nicolai Stange, Olof Johansson, Paul Gortmaker, Paul
      Mackerras, Peter Rosin, Pridhiviraj Paidipeddi, Ram Pai, Rashmica
      Gupta, Ravi Bangoria, Russell Currey, Sam Bobroff, Samuel
      Mendoza-Jonas, Segher Boessenkool, Shilpasri G Bhat, Simon Guo,
      Souptick Joarder, Stewart Smith, Thiago Jung Bauermann, Torsten Duwe,
      Vaibhav Jain, Wei Yongjun, Wolfram Sang, Yisheng Xie, YueHaibing"
    
    * tag 'powerpc-4.18-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (251 commits)
      powerpc/64s/radix: Fix missing ptesync in flush_cache_vmap
      cpuidle: powernv: Fix promotion from snooze if next state disabled
      powerpc: fix build failure by disabling attribute-alias warning in pci_32
      ocxl: Fix missing unlock on error in afu_ioctl_enable_p9_wait()
      powerpc-opal: fix spelling mistake "Uniterrupted" -> "Uninterrupted"
      powerpc: fix spelling mistake: "Usupported" -> "Unsupported"
      powerpc/pkeys: Detach execute_only key on !PROT_EXEC
      powerpc/powernv: copy/paste - Mask SO bit in CR
      powerpc: Remove core support for Marvell mv64x60 hostbridges
      powerpc/boot: Remove core support for Marvell mv64x60 hostbridges
      powerpc/boot: Remove support for Marvell mv64x60 i2c controller
      powerpc/boot: Remove support for Marvell MPSC serial controller
      powerpc/embedded6xx: Remove C2K board support
      powerpc/lib: optimise PPC32 memcmp
      powerpc/lib: optimise 32 bits __clear_user()
      powerpc/time: inline arch_vtime_task_switch()
      powerpc/Makefile: set -mcpu=860 flag for the 8xx
      powerpc: Implement csum_ipv6_magic in assembly
      powerpc/32: Optimise __csum_partial()
      powerpc/lib: Adjust .balign inside string functions for PPC32
      ...

commit 815069ca57c142eb71d27439bc27f41a433a67b3
Author: Michal Suchanek <msuchanek@suse.de>
Date:   Tue Apr 24 14:15:56 2018 +1000

    powerpc/64s: Patch barrier_nospec in modules
    
    Note that unlike RFI which is patched only in kernel the nospec state
    reflects settings at the time the module was loaded.
    
    Iterating all modules and re-patching every time the settings change
    is not implemented.
    
    Based on lwsync patching.
    
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index ae911dad9b16..2b9173d09f24 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -163,14 +163,14 @@ void do_rfi_flush_fixups(enum l1d_flush_type types)
 						: "unknown");
 }
 
-void do_barrier_nospec_fixups(bool enable)
+void do_barrier_nospec_fixups_range(bool enable, void *fixup_start, void *fixup_end)
 {
 	unsigned int instr, *dest;
 	long *start, *end;
 	int i;
 
-	start = PTRRELOC(&__start___barrier_nospec_fixup),
-	end = PTRRELOC(&__stop___barrier_nospec_fixup);
+	start = fixup_start;
+	end = fixup_end;
 
 	instr = 0x60000000; /* nop */
 
@@ -189,6 +189,16 @@ void do_barrier_nospec_fixups(bool enable)
 	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
 }
 
+void do_barrier_nospec_fixups(bool enable)
+{
+	void *start, *end;
+
+	start = PTRRELOC(&__start___barrier_nospec_fixup),
+	end = PTRRELOC(&__stop___barrier_nospec_fixup);
+
+	do_barrier_nospec_fixups_range(enable, start, end);
+}
+
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)

commit 2eea7f067f495e33b8b116b35b5988ab2b8aec55
Author: Michal Suchanek <msuchanek@suse.de>
Date:   Tue Apr 24 14:15:55 2018 +1000

    powerpc/64s: Add support for ori barrier_nospec patching
    
    Based on the RFI patching. This is required to be able to disable the
    speculation barrier.
    
    Only one barrier type is supported and it does nothing when the
    firmware does not enable it. Also re-patching modules is not supported
    So the only meaningful thing that can be done is patching out the
    speculation barrier at boot when the user says it is not wanted.
    
    Signed-off-by: Michal Suchanek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index f3e46d4edd72..ae911dad9b16 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -162,6 +162,33 @@ void do_rfi_flush_fixups(enum l1d_flush_type types)
 		(types &  L1D_FLUSH_MTTRIG)     ? "mttrig type"
 						: "unknown");
 }
+
+void do_barrier_nospec_fixups(bool enable)
+{
+	unsigned int instr, *dest;
+	long *start, *end;
+	int i;
+
+	start = PTRRELOC(&__start___barrier_nospec_fixup),
+	end = PTRRELOC(&__stop___barrier_nospec_fixup);
+
+	instr = 0x60000000; /* nop */
+
+	if (enable) {
+		pr_info("barrier-nospec: using ORI speculation barrier\n");
+		instr = 0x63ff0000; /* ori 31,31,0 speculation barrier */
+	}
+
+	for (i = 0; start < end; start++, i++) {
+		dest = (void *)start + *start;
+
+		pr_devel("patching dest %lx\n", (unsigned long)dest);
+		patch_instruction(dest, instr);
+	}
+
+	printk(KERN_DEBUG "barrier-nospec: patched %d locations\n", i);
+}
+
 #endif /* CONFIG_PPC_BOOK3S_64 */
 
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)

commit a048a07d7f4535baa4cbad6bc024f175317ab938
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Tue May 22 09:00:00 2018 +1000

    powerpc/64s: Add support for a store forwarding barrier at kernel entry/exit
    
    On some CPUs we can prevent a vulnerability related to store-to-load
    forwarding by preventing store forwarding between privilege domains,
    by inserting a barrier in kernel entry and exit paths.
    
    This is known to be the case on at least Power7, Power8 and Power9
    powerpc CPUs.
    
    Barriers must be inserted generally before the first load after moving
    to a higher privilege, and after the last store before moving to a
    lower privilege, HV and PR privilege transitions must be protected.
    
    Barriers are added as patch sections, with all kernel/hypervisor entry
    points patched, and the exit points to lower privilge levels patched
    similarly to the RFI flush patching.
    
    Firmware advertisement is not implemented yet, so CPU flush types
    are hard coded.
    
    Thanks to Michal Suchánek for bug fixes and review.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michal Suchánek <msuchanek@suse.de>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 288fe4f0db4e..e1bcdc32a851 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -23,6 +23,7 @@
 #include <asm/page.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
+#include <asm/security_features.h>
 #include <asm/firmware.h>
 
 struct fixup_entry {
@@ -117,6 +118,120 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 }
 
 #ifdef CONFIG_PPC_BOOK3S_64
+void do_stf_entry_barrier_fixups(enum stf_barrier_type types)
+{
+	unsigned int instrs[3], *dest;
+	long *start, *end;
+	int i;
+
+	start = PTRRELOC(&__start___stf_entry_barrier_fixup),
+	end = PTRRELOC(&__stop___stf_entry_barrier_fixup);
+
+	instrs[0] = 0x60000000; /* nop */
+	instrs[1] = 0x60000000; /* nop */
+	instrs[2] = 0x60000000; /* nop */
+
+	i = 0;
+	if (types & STF_BARRIER_FALLBACK) {
+		instrs[i++] = 0x7d4802a6; /* mflr r10		*/
+		instrs[i++] = 0x60000000; /* branch patched below */
+		instrs[i++] = 0x7d4803a6; /* mtlr r10		*/
+	} else if (types & STF_BARRIER_EIEIO) {
+		instrs[i++] = 0x7e0006ac; /* eieio + bit 6 hint */
+	} else if (types & STF_BARRIER_SYNC_ORI) {
+		instrs[i++] = 0x7c0004ac; /* hwsync		*/
+		instrs[i++] = 0xe94d0000; /* ld r10,0(r13)	*/
+		instrs[i++] = 0x63ff0000; /* ori 31,31,0 speculation barrier */
+	}
+
+	for (i = 0; start < end; start++, i++) {
+		dest = (void *)start + *start;
+
+		pr_devel("patching dest %lx\n", (unsigned long)dest);
+
+		patch_instruction(dest, instrs[0]);
+
+		if (types & STF_BARRIER_FALLBACK)
+			patch_branch(dest + 1, (unsigned long)&stf_barrier_fallback,
+				     BRANCH_SET_LINK);
+		else
+			patch_instruction(dest + 1, instrs[1]);
+
+		patch_instruction(dest + 2, instrs[2]);
+	}
+
+	printk(KERN_DEBUG "stf-barrier: patched %d entry locations (%s barrier)\n", i,
+		(types == STF_BARRIER_NONE)                  ? "no" :
+		(types == STF_BARRIER_FALLBACK)              ? "fallback" :
+		(types == STF_BARRIER_EIEIO)                 ? "eieio" :
+		(types == (STF_BARRIER_SYNC_ORI))            ? "hwsync"
+		                                           : "unknown");
+}
+
+void do_stf_exit_barrier_fixups(enum stf_barrier_type types)
+{
+	unsigned int instrs[6], *dest;
+	long *start, *end;
+	int i;
+
+	start = PTRRELOC(&__start___stf_exit_barrier_fixup),
+	end = PTRRELOC(&__stop___stf_exit_barrier_fixup);
+
+	instrs[0] = 0x60000000; /* nop */
+	instrs[1] = 0x60000000; /* nop */
+	instrs[2] = 0x60000000; /* nop */
+	instrs[3] = 0x60000000; /* nop */
+	instrs[4] = 0x60000000; /* nop */
+	instrs[5] = 0x60000000; /* nop */
+
+	i = 0;
+	if (types & STF_BARRIER_FALLBACK || types & STF_BARRIER_SYNC_ORI) {
+		if (cpu_has_feature(CPU_FTR_HVMODE)) {
+			instrs[i++] = 0x7db14ba6; /* mtspr 0x131, r13 (HSPRG1) */
+			instrs[i++] = 0x7db04aa6; /* mfspr r13, 0x130 (HSPRG0) */
+		} else {
+			instrs[i++] = 0x7db243a6; /* mtsprg 2,r13	*/
+			instrs[i++] = 0x7db142a6; /* mfsprg r13,1    */
+	        }
+		instrs[i++] = 0x7c0004ac; /* hwsync		*/
+		instrs[i++] = 0xe9ad0000; /* ld r13,0(r13)	*/
+		instrs[i++] = 0x63ff0000; /* ori 31,31,0 speculation barrier */
+		if (cpu_has_feature(CPU_FTR_HVMODE)) {
+			instrs[i++] = 0x7db14aa6; /* mfspr r13, 0x131 (HSPRG1) */
+		} else {
+			instrs[i++] = 0x7db242a6; /* mfsprg r13,2 */
+		}
+	} else if (types & STF_BARRIER_EIEIO) {
+		instrs[i++] = 0x7e0006ac; /* eieio + bit 6 hint */
+	}
+
+	for (i = 0; start < end; start++, i++) {
+		dest = (void *)start + *start;
+
+		pr_devel("patching dest %lx\n", (unsigned long)dest);
+
+		patch_instruction(dest, instrs[0]);
+		patch_instruction(dest + 1, instrs[1]);
+		patch_instruction(dest + 2, instrs[2]);
+		patch_instruction(dest + 3, instrs[3]);
+		patch_instruction(dest + 4, instrs[4]);
+		patch_instruction(dest + 5, instrs[5]);
+	}
+	printk(KERN_DEBUG "stf-barrier: patched %d exit locations (%s barrier)\n", i,
+		(types == STF_BARRIER_NONE)                  ? "no" :
+		(types == STF_BARRIER_FALLBACK)              ? "fallback" :
+		(types == STF_BARRIER_EIEIO)                 ? "eieio" :
+		(types == (STF_BARRIER_SYNC_ORI))            ? "hwsync"
+		                                           : "unknown");
+}
+
+
+void do_stf_barrier_fixups(enum stf_barrier_type types)
+{
+	do_stf_entry_barrier_fixups(types);
+	do_stf_exit_barrier_fixups(types);
+}
+
 void do_rfi_flush_fixups(enum l1d_flush_type types)
 {
 	unsigned int instrs[3], *dest;

commit 6158faed7c80256fb6b370781cb3cd6815f4ba69
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 17 00:39:05 2018 +1000

    powerpc/lib: Add alt patching test of branching past the last instruction
    
    Add a test of the relative branch patching logic in the alternate
    section feature fixup code. This tests that if we branch past the last
    instruction of the alternate section, the branch is not patched.
    That's because the assembler will have created a branch that already
    points to the first instruction after the patched section, which is
    correct and needs no further patching.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 097b45bd9de4..f3e46d4edd72 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -425,6 +425,16 @@ static void test_alternative_case_with_external_branch(void)
 	check(memcmp(ftr_fixup_test6, ftr_fixup_test6_expected, size) == 0);
 }
 
+static void test_alternative_case_with_branch_to_end(void)
+{
+	extern unsigned int ftr_fixup_test7[];
+	extern unsigned int end_ftr_fixup_test7[];
+	extern unsigned int ftr_fixup_test7_expected[];
+	int size = 4 * (end_ftr_fixup_test7 - ftr_fixup_test7);
+
+	check(memcmp(ftr_fixup_test7, ftr_fixup_test7_expected, size) == 0);
+}
+
 static void test_cpu_macros(void)
 {
 	extern u8 ftr_fixup_test_FTR_macros[];
@@ -480,6 +490,7 @@ static int __init test_feature_fixups(void)
 	test_alternative_case_too_small();
 	test_alternative_case_with_branch();
 	test_alternative_case_with_external_branch();
+	test_alternative_case_with_branch_to_end();
 	test_cpu_macros();
 	test_fw_macros();
 	test_lwsync_macros();

commit cad0e39023b43d94d5e38dfd55c103e15bdd093d
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Apr 17 00:39:03 2018 +1000

    powerpc/lib: Fix the feature fixup tests to actually work
    
    The code patching code has always been a bit confused about whether
    it's best to use void *, unsigned int *, char *, etc. to point to
    instructions. In fact in the feature fixups tests we use both unsigned
    int[] and u8[] in different places.
    
    Unfortunately the tests that use unsigned int[] calculate the size of
    the code blocks using subtraction of those unsigned int pointers, and
    then pass the result to memcmp(). This means we're only comparing 1/4
    of the bytes we need to, because we need to multiply by
    sizeof(unsigned int) to get the number of *bytes*.
    
    The result is that the tests do all the patching and then only compare
    some of the resulting code, so patching bugs that only effect that
    last 3/4 of the code could slip through undetected. It turns out that
    hasn't been happening, although one test had a bad expected case (see
    previous commit).
    
    Fix it for now by multiplying the size by 4 in the affected functions.
    
    Fixes: 362e7701fd18 ("powerpc: Add self-tests of the feature fixup code")
    Epic-brown-paper-bag-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 288fe4f0db4e..097b45bd9de4 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -285,7 +285,7 @@ static void test_basic_patching(void)
 	extern unsigned int end_ftr_fixup_test1[];
 	extern unsigned int ftr_fixup_test1_orig[];
 	extern unsigned int ftr_fixup_test1_expected[];
-	int size = end_ftr_fixup_test1 - ftr_fixup_test1;
+	int size = 4 * (end_ftr_fixup_test1 - ftr_fixup_test1);
 
 	fixup.value = fixup.mask = 8;
 	fixup.start_off = calc_offset(&fixup, ftr_fixup_test1 + 1);
@@ -317,7 +317,7 @@ static void test_alternative_patching(void)
 	extern unsigned int ftr_fixup_test2_orig[];
 	extern unsigned int ftr_fixup_test2_alt[];
 	extern unsigned int ftr_fixup_test2_expected[];
-	int size = end_ftr_fixup_test2 - ftr_fixup_test2;
+	int size = 4 * (end_ftr_fixup_test2 - ftr_fixup_test2);
 
 	fixup.value = fixup.mask = 0xF;
 	fixup.start_off = calc_offset(&fixup, ftr_fixup_test2 + 1);
@@ -349,7 +349,7 @@ static void test_alternative_case_too_big(void)
 	extern unsigned int end_ftr_fixup_test3[];
 	extern unsigned int ftr_fixup_test3_orig[];
 	extern unsigned int ftr_fixup_test3_alt[];
-	int size = end_ftr_fixup_test3 - ftr_fixup_test3;
+	int size = 4 * (end_ftr_fixup_test3 - ftr_fixup_test3);
 
 	fixup.value = fixup.mask = 0xC;
 	fixup.start_off = calc_offset(&fixup, ftr_fixup_test3 + 1);
@@ -376,7 +376,7 @@ static void test_alternative_case_too_small(void)
 	extern unsigned int ftr_fixup_test4_orig[];
 	extern unsigned int ftr_fixup_test4_alt[];
 	extern unsigned int ftr_fixup_test4_expected[];
-	int size = end_ftr_fixup_test4 - ftr_fixup_test4;
+	int size = 4 * (end_ftr_fixup_test4 - ftr_fixup_test4);
 	unsigned long flag;
 
 	/* Check a high-bit flag */
@@ -410,7 +410,7 @@ static void test_alternative_case_with_branch(void)
 	extern unsigned int ftr_fixup_test5[];
 	extern unsigned int end_ftr_fixup_test5[];
 	extern unsigned int ftr_fixup_test5_expected[];
-	int size = end_ftr_fixup_test5 - ftr_fixup_test5;
+	int size = 4 * (end_ftr_fixup_test5 - ftr_fixup_test5);
 
 	check(memcmp(ftr_fixup_test5, ftr_fixup_test5_expected, size) == 0);
 }
@@ -420,7 +420,7 @@ static void test_alternative_case_with_external_branch(void)
 	extern unsigned int ftr_fixup_test6[];
 	extern unsigned int end_ftr_fixup_test6[];
 	extern unsigned int ftr_fixup_test6_expected[];
-	int size = end_ftr_fixup_test6 - ftr_fixup_test6;
+	int size = 4 * (end_ftr_fixup_test6 - ftr_fixup_test6);
 
 	check(memcmp(ftr_fixup_test6, ftr_fixup_test6_expected, size) == 0);
 }

commit b8858581febb050688e276b956796bc4a78299ed
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Mon Apr 16 23:25:19 2018 +1000

    powerpc/lib: Fix off-by-one in alternate feature patching
    
    When we patch an alternate feature section, we have to adjust any
    relative branches that branch out of the alternate section.
    
    But currently we have a bug if we have a branch that points to past
    the last instruction of the alternate section, eg:
    
      FTR_SECTION_ELSE
      1:     b       2f
             or      6,6,6
      2:
      ALT_FTR_SECTION_END(...)
             nop
    
    This will result in a relative branch at 1 with a target that equals
    the end of the alternate section.
    
    That branch does not need adjusting when it's moved to the non-else
    location. Currently we do adjust it, resulting in a branch that goes
    off into the link-time location of the else section, which is junk.
    
    The fix is to not patch branches that have a target == end of the
    alternate section.
    
    Fixes: d20fe50a7b3c ("KVM: PPC: Book3S HV: Branch inside feature section")
    Fixes: 9b1a735de64c ("powerpc: Add logic to patch alternative feature sections")
    Cc: stable@vger.kernel.org # v2.6.27+
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 35f80ab7cbd8..288fe4f0db4e 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -55,7 +55,7 @@ static int patch_alt_instruction(unsigned int *src, unsigned int *dest,
 		unsigned int *target = (unsigned int *)branch_target(src);
 
 		/* Branch within the section doesn't need translating */
-		if (target < alt_start || target >= alt_end) {
+		if (target < alt_start || target > alt_end) {
 			instr = translate_branch(dest, src);
 			if (!instr)
 				return 1;

commit 0063d61ccfc011f379a31acaeba6de7c926fed2c
Author: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
Date:   Wed Mar 14 19:40:41 2018 -0300

    powerpc/rfi-flush: Differentiate enabled and patched flush types
    
    Currently the rfi-flush messages print 'Using <type> flush' for all
    enabled_flush_types, but that is not necessarily true -- as now the
    fallback flush is always enabled on pseries, but the fixup function
    overwrites its nop/branch slot with other flush types, if available.
    
    So, replace the 'Using <type> flush' messages with '<type> flush is
    available'.
    
    Also, print the patched flush types in the fixup function, so users
    can know what is (not) being used (e.g., the slower, fallback flush,
    or no flush type at all if flush is disabled via the debugfs switch).
    
    Suggested-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Mauricio Faria de Oliveira <mauricfo@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 73697c4e3468..35f80ab7cbd8 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -153,7 +153,14 @@ void do_rfi_flush_fixups(enum l1d_flush_type types)
 		patch_instruction(dest + 2, instrs[2]);
 	}
 
-	printk(KERN_DEBUG "rfi-flush: patched %d locations\n", i);
+	printk(KERN_DEBUG "rfi-flush: patched %d locations (%s flush)\n", i,
+		(types == L1D_FLUSH_NONE)       ? "no" :
+		(types == L1D_FLUSH_FALLBACK)   ? "fallback displacement" :
+		(types &  L1D_FLUSH_ORI)        ? (types & L1D_FLUSH_MTTRIG)
+							? "ori+mttrig type"
+							: "ori type" :
+		(types &  L1D_FLUSH_MTTRIG)     ? "mttrig type"
+						: "unknown");
 }
 #endif /* CONFIG_PPC_BOOK3S_64 */
 

commit ebf0b6a8b1e445d2be66087732aafcda12ab9f59
Merge: 5400fc229e60 1b689a95ce74
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Sun Jan 21 23:21:14 2018 +1100

    Merge branch 'fixes' into next
    
    Merge our fixes branch from the 4.15 cycle.
    
    Unusually the fixes branch saw some significant features merged,
    notably the RFI flush patches, so we want the code in next to be
    tested against that, to avoid any surprises when the two are merged.
    
    There's also some other work on the panic handling that was reverted
    in fixes and we now want to do properly in next, which would conflict.
    
    And we also fix a few other minor merge conflicts.

commit 8183d99f4a22c2abbc543847a588df3666ef0c0c
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Fri Nov 24 08:31:09 2017 +0100

    powerpc/lib/feature-fixups: use raw_patch_instruction()
    
    feature fixups need to use patch_instruction() early in the boot,
    even before the code is relocated to its final address, requiring
    patch_instruction() to use PTRRELOC() in order to address data.
    
    But feature fixups applies on code before it is set to read only,
    even for modules. Therefore, feature fixups can use
    raw_patch_instruction() instead.
    
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 41cf5ae273cf..0872d60ede10 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -62,7 +62,7 @@ static int patch_alt_instruction(unsigned int *src, unsigned int *dest,
 		}
 	}
 
-	patch_instruction(dest, instr);
+	raw_patch_instruction(dest, instr);
 
 	return 0;
 }
@@ -91,7 +91,7 @@ static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
 	}
 
 	for (; dest < end; dest++)
-		patch_instruction(dest, PPC_INST_NOP);
+		raw_patch_instruction(dest, PPC_INST_NOP);
 
 	return 0;
 }
@@ -129,7 +129,7 @@ void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 
 	for (; start < end; start++) {
 		dest = (void *)start + *start;
-		patch_instruction(dest, PPC_INST_LWSYNC);
+		raw_patch_instruction(dest, PPC_INST_LWSYNC);
 	}
 }
 
@@ -147,7 +147,7 @@ static void do_final_fixups(void)
 	length = (__end_interrupts - _stext) / sizeof(int);
 
 	while (length--) {
-		patch_instruction(dest, *src);
+		raw_patch_instruction(dest, *src);
 		src++;
 		dest++;
 	}

commit aa8a5e0062ac940f7659394f4817c948dc8c0667
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Wed Jan 10 03:07:15 2018 +1100

    powerpc/64s: Add support for RFI flush of L1-D cache
    
    On some CPUs we can prevent the Meltdown vulnerability by flushing the
    L1-D cache on exit from kernel to user mode, and from hypervisor to
    guest.
    
    This is known to be the case on at least Power7, Power8 and Power9. At
    this time we do not know the status of the vulnerability on other CPUs
    such as the 970 (Apple G5), pasemi CPUs (AmigaOne X1000) or Freescale
    CPUs. As more information comes to light we can enable this, or other
    mechanisms on those CPUs.
    
    The vulnerability occurs when the load of an architecturally
    inaccessible memory region (eg. userspace load of kernel memory) is
    speculatively executed to the point where its result can influence the
    address of a subsequent speculatively executed load.
    
    In order for that to happen, the first load must hit in the L1,
    because before the load is sent to the L2 the permission check is
    performed. Therefore if no kernel addresses hit in the L1 the
    vulnerability can not occur. We can ensure that is the case by
    flushing the L1 whenever we return to userspace. Similarly for
    hypervisor vs guest.
    
    In order to flush the L1-D cache on exit, we add a section of nops at
    each (h)rfi location that returns to a lower privileged context, and
    patch that with some sequence. Newer firmwares are able to advertise
    to us that there is a special nop instruction that flushes the L1-D.
    If we do not see that advertised, we fall back to doing a displacement
    flush in software.
    
    For guest kernels we support migration between some CPU versions, and
    different CPUs may use different flush instructions. So that we are
    prepared to migrate to a machine with a different flush instruction
    activated, we may have to patch more than one flush instruction at
    boot if the hypervisor tells us to.
    
    In the end this patch is mostly the work of Nicholas Piggin and
    Michael Ellerman. However a cast of thousands contributed to analysis
    of the issue, earlier versions of the patch, back ports testing etc.
    Many thanks to all of them.
    
    Tested-by: Jon Masters <jcm@redhat.com>
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 41cf5ae273cf..a95ea007d654 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -116,6 +116,47 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 	}
 }
 
+#ifdef CONFIG_PPC_BOOK3S_64
+void do_rfi_flush_fixups(enum l1d_flush_type types)
+{
+	unsigned int instrs[3], *dest;
+	long *start, *end;
+	int i;
+
+	start = PTRRELOC(&__start___rfi_flush_fixup),
+	end = PTRRELOC(&__stop___rfi_flush_fixup);
+
+	instrs[0] = 0x60000000; /* nop */
+	instrs[1] = 0x60000000; /* nop */
+	instrs[2] = 0x60000000; /* nop */
+
+	if (types & L1D_FLUSH_FALLBACK)
+		/* b .+16 to fallback flush */
+		instrs[0] = 0x48000010;
+
+	i = 0;
+	if (types & L1D_FLUSH_ORI) {
+		instrs[i++] = 0x63ff0000; /* ori 31,31,0 speculation barrier */
+		instrs[i++] = 0x63de0000; /* ori 30,30,0 L1d flush*/
+	}
+
+	if (types & L1D_FLUSH_MTTRIG)
+		instrs[i++] = 0x7c12dba6; /* mtspr TRIG2,r0 (SPR #882) */
+
+	for (i = 0; start < end; start++, i++) {
+		dest = (void *)start + *start;
+
+		pr_devel("patching dest %lx\n", (unsigned long)dest);
+
+		patch_instruction(dest, instrs[0]);
+		patch_instruction(dest + 1, instrs[1]);
+		patch_instruction(dest + 2, instrs[2]);
+	}
+
+	printk(KERN_DEBUG "rfi-flush: patched %d locations\n", i);
+}
+#endif /* CONFIG_PPC_BOOK3S_64 */
+
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 {
 	long *start, *end;

commit c69a48cdb301a18697bc8c9935baf4f32861cf9e
Author: Daniel Axtens <dja@axtens.net>
Date:   Wed Jul 12 14:36:07 2017 -0700

    powerpc: make feature-fixup tests fortify-safe
    
    Testing the fortified string functions[1] would cause a kernel panic on
    boot in test_feature_fixups() due to a buffer overflow in memcmp.
    
    This boils down to things like this:
    
      extern unsigned int ftr_fixup_test1;
      extern unsigned int ftr_fixup_test1_orig;
    
      check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_orig, size) == 0);
    
    We know that these are asm labels so it is safe to read up to 'size'
    bytes at those addresses.
    
    However, because we have passed the address of a single unsigned int to
    memcmp, the compiler believes the underlying object is in fact a single
    unsigned int.  So if size > sizeof(unsigned int), there will be a panic
    at runtime.
    
    We can fix this by changing the types: instead of calling the asm labels
    unsigned ints, call them unsigned int[]s.  Therefore the size isn't
    incorrectly determined at compile time and we get a regular unsafe
    memcmp and no panic.
    
    [1] http://openwall.com/lists/kernel-hardening/2017/05/09/2
    
    Link: http://lkml.kernel.org/r/1497903987-21002-7-git-send-email-keescook@chromium.org
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Suggested-by: Michael Ellerman <mpe@ellerman.id.au>
    Tested-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Daniel Micay <danielmicay@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index f3917705c686..41cf5ae273cf 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -233,192 +233,192 @@ static long calc_offset(struct fixup_entry *entry, unsigned int *p)
 
 static void test_basic_patching(void)
 {
-	extern unsigned int ftr_fixup_test1;
-	extern unsigned int end_ftr_fixup_test1;
-	extern unsigned int ftr_fixup_test1_orig;
-	extern unsigned int ftr_fixup_test1_expected;
-	int size = &end_ftr_fixup_test1 - &ftr_fixup_test1;
+	extern unsigned int ftr_fixup_test1[];
+	extern unsigned int end_ftr_fixup_test1[];
+	extern unsigned int ftr_fixup_test1_orig[];
+	extern unsigned int ftr_fixup_test1_expected[];
+	int size = end_ftr_fixup_test1 - ftr_fixup_test1;
 
 	fixup.value = fixup.mask = 8;
-	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test1 + 1);
-	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test1 + 2);
+	fixup.start_off = calc_offset(&fixup, ftr_fixup_test1 + 1);
+	fixup.end_off = calc_offset(&fixup, ftr_fixup_test1 + 2);
 	fixup.alt_start_off = fixup.alt_end_off = 0;
 
 	/* Sanity check */
-	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_orig, size) == 0);
+	check(memcmp(ftr_fixup_test1, ftr_fixup_test1_orig, size) == 0);
 
 	/* Check we don't patch if the value matches */
 	patch_feature_section(8, &fixup);
-	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_orig, size) == 0);
+	check(memcmp(ftr_fixup_test1, ftr_fixup_test1_orig, size) == 0);
 
 	/* Check we do patch if the value doesn't match */
 	patch_feature_section(0, &fixup);
-	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_expected, size) == 0);
+	check(memcmp(ftr_fixup_test1, ftr_fixup_test1_expected, size) == 0);
 
 	/* Check we do patch if the mask doesn't match */
-	memcpy(&ftr_fixup_test1, &ftr_fixup_test1_orig, size);
-	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_orig, size) == 0);
+	memcpy(ftr_fixup_test1, ftr_fixup_test1_orig, size);
+	check(memcmp(ftr_fixup_test1, ftr_fixup_test1_orig, size) == 0);
 	patch_feature_section(~8, &fixup);
-	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_expected, size) == 0);
+	check(memcmp(ftr_fixup_test1, ftr_fixup_test1_expected, size) == 0);
 }
 
 static void test_alternative_patching(void)
 {
-	extern unsigned int ftr_fixup_test2;
-	extern unsigned int end_ftr_fixup_test2;
-	extern unsigned int ftr_fixup_test2_orig;
-	extern unsigned int ftr_fixup_test2_alt;
-	extern unsigned int ftr_fixup_test2_expected;
-	int size = &end_ftr_fixup_test2 - &ftr_fixup_test2;
+	extern unsigned int ftr_fixup_test2[];
+	extern unsigned int end_ftr_fixup_test2[];
+	extern unsigned int ftr_fixup_test2_orig[];
+	extern unsigned int ftr_fixup_test2_alt[];
+	extern unsigned int ftr_fixup_test2_expected[];
+	int size = end_ftr_fixup_test2 - ftr_fixup_test2;
 
 	fixup.value = fixup.mask = 0xF;
-	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test2 + 1);
-	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test2 + 2);
-	fixup.alt_start_off = calc_offset(&fixup, &ftr_fixup_test2_alt);
-	fixup.alt_end_off = calc_offset(&fixup, &ftr_fixup_test2_alt + 1);
+	fixup.start_off = calc_offset(&fixup, ftr_fixup_test2 + 1);
+	fixup.end_off = calc_offset(&fixup, ftr_fixup_test2 + 2);
+	fixup.alt_start_off = calc_offset(&fixup, ftr_fixup_test2_alt);
+	fixup.alt_end_off = calc_offset(&fixup, ftr_fixup_test2_alt + 1);
 
 	/* Sanity check */
-	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_orig, size) == 0);
+	check(memcmp(ftr_fixup_test2, ftr_fixup_test2_orig, size) == 0);
 
 	/* Check we don't patch if the value matches */
 	patch_feature_section(0xF, &fixup);
-	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_orig, size) == 0);
+	check(memcmp(ftr_fixup_test2, ftr_fixup_test2_orig, size) == 0);
 
 	/* Check we do patch if the value doesn't match */
 	patch_feature_section(0, &fixup);
-	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_expected, size) == 0);
+	check(memcmp(ftr_fixup_test2, ftr_fixup_test2_expected, size) == 0);
 
 	/* Check we do patch if the mask doesn't match */
-	memcpy(&ftr_fixup_test2, &ftr_fixup_test2_orig, size);
-	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_orig, size) == 0);
+	memcpy(ftr_fixup_test2, ftr_fixup_test2_orig, size);
+	check(memcmp(ftr_fixup_test2, ftr_fixup_test2_orig, size) == 0);
 	patch_feature_section(~0xF, &fixup);
-	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_expected, size) == 0);
+	check(memcmp(ftr_fixup_test2, ftr_fixup_test2_expected, size) == 0);
 }
 
 static void test_alternative_case_too_big(void)
 {
-	extern unsigned int ftr_fixup_test3;
-	extern unsigned int end_ftr_fixup_test3;
-	extern unsigned int ftr_fixup_test3_orig;
-	extern unsigned int ftr_fixup_test3_alt;
-	int size = &end_ftr_fixup_test3 - &ftr_fixup_test3;
+	extern unsigned int ftr_fixup_test3[];
+	extern unsigned int end_ftr_fixup_test3[];
+	extern unsigned int ftr_fixup_test3_orig[];
+	extern unsigned int ftr_fixup_test3_alt[];
+	int size = end_ftr_fixup_test3 - ftr_fixup_test3;
 
 	fixup.value = fixup.mask = 0xC;
-	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test3 + 1);
-	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test3 + 2);
-	fixup.alt_start_off = calc_offset(&fixup, &ftr_fixup_test3_alt);
-	fixup.alt_end_off = calc_offset(&fixup, &ftr_fixup_test3_alt + 2);
+	fixup.start_off = calc_offset(&fixup, ftr_fixup_test3 + 1);
+	fixup.end_off = calc_offset(&fixup, ftr_fixup_test3 + 2);
+	fixup.alt_start_off = calc_offset(&fixup, ftr_fixup_test3_alt);
+	fixup.alt_end_off = calc_offset(&fixup, ftr_fixup_test3_alt + 2);
 
 	/* Sanity check */
-	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+	check(memcmp(ftr_fixup_test3, ftr_fixup_test3_orig, size) == 0);
 
 	/* Expect nothing to be patched, and the error returned to us */
 	check(patch_feature_section(0xF, &fixup) == 1);
-	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+	check(memcmp(ftr_fixup_test3, ftr_fixup_test3_orig, size) == 0);
 	check(patch_feature_section(0, &fixup) == 1);
-	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+	check(memcmp(ftr_fixup_test3, ftr_fixup_test3_orig, size) == 0);
 	check(patch_feature_section(~0xF, &fixup) == 1);
-	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+	check(memcmp(ftr_fixup_test3, ftr_fixup_test3_orig, size) == 0);
 }
 
 static void test_alternative_case_too_small(void)
 {
-	extern unsigned int ftr_fixup_test4;
-	extern unsigned int end_ftr_fixup_test4;
-	extern unsigned int ftr_fixup_test4_orig;
-	extern unsigned int ftr_fixup_test4_alt;
-	extern unsigned int ftr_fixup_test4_expected;
-	int size = &end_ftr_fixup_test4 - &ftr_fixup_test4;
+	extern unsigned int ftr_fixup_test4[];
+	extern unsigned int end_ftr_fixup_test4[];
+	extern unsigned int ftr_fixup_test4_orig[];
+	extern unsigned int ftr_fixup_test4_alt[];
+	extern unsigned int ftr_fixup_test4_expected[];
+	int size = end_ftr_fixup_test4 - ftr_fixup_test4;
 	unsigned long flag;
 
 	/* Check a high-bit flag */
 	flag = 1UL << ((sizeof(unsigned long) - 1) * 8);
 	fixup.value = fixup.mask = flag;
-	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test4 + 1);
-	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test4 + 5);
-	fixup.alt_start_off = calc_offset(&fixup, &ftr_fixup_test4_alt);
-	fixup.alt_end_off = calc_offset(&fixup, &ftr_fixup_test4_alt + 2);
+	fixup.start_off = calc_offset(&fixup, ftr_fixup_test4 + 1);
+	fixup.end_off = calc_offset(&fixup, ftr_fixup_test4 + 5);
+	fixup.alt_start_off = calc_offset(&fixup, ftr_fixup_test4_alt);
+	fixup.alt_end_off = calc_offset(&fixup, ftr_fixup_test4_alt + 2);
 
 	/* Sanity check */
-	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_orig, size) == 0);
+	check(memcmp(ftr_fixup_test4, ftr_fixup_test4_orig, size) == 0);
 
 	/* Check we don't patch if the value matches */
 	patch_feature_section(flag, &fixup);
-	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_orig, size) == 0);
+	check(memcmp(ftr_fixup_test4, ftr_fixup_test4_orig, size) == 0);
 
 	/* Check we do patch if the value doesn't match */
 	patch_feature_section(0, &fixup);
-	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_expected, size) == 0);
+	check(memcmp(ftr_fixup_test4, ftr_fixup_test4_expected, size) == 0);
 
 	/* Check we do patch if the mask doesn't match */
-	memcpy(&ftr_fixup_test4, &ftr_fixup_test4_orig, size);
-	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_orig, size) == 0);
+	memcpy(ftr_fixup_test4, ftr_fixup_test4_orig, size);
+	check(memcmp(ftr_fixup_test4, ftr_fixup_test4_orig, size) == 0);
 	patch_feature_section(~flag, &fixup);
-	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_expected, size) == 0);
+	check(memcmp(ftr_fixup_test4, ftr_fixup_test4_expected, size) == 0);
 }
 
 static void test_alternative_case_with_branch(void)
 {
-	extern unsigned int ftr_fixup_test5;
-	extern unsigned int end_ftr_fixup_test5;
-	extern unsigned int ftr_fixup_test5_expected;
-	int size = &end_ftr_fixup_test5 - &ftr_fixup_test5;
+	extern unsigned int ftr_fixup_test5[];
+	extern unsigned int end_ftr_fixup_test5[];
+	extern unsigned int ftr_fixup_test5_expected[];
+	int size = end_ftr_fixup_test5 - ftr_fixup_test5;
 
-	check(memcmp(&ftr_fixup_test5, &ftr_fixup_test5_expected, size) == 0);
+	check(memcmp(ftr_fixup_test5, ftr_fixup_test5_expected, size) == 0);
 }
 
 static void test_alternative_case_with_external_branch(void)
 {
-	extern unsigned int ftr_fixup_test6;
-	extern unsigned int end_ftr_fixup_test6;
-	extern unsigned int ftr_fixup_test6_expected;
-	int size = &end_ftr_fixup_test6 - &ftr_fixup_test6;
+	extern unsigned int ftr_fixup_test6[];
+	extern unsigned int end_ftr_fixup_test6[];
+	extern unsigned int ftr_fixup_test6_expected[];
+	int size = end_ftr_fixup_test6 - ftr_fixup_test6;
 
-	check(memcmp(&ftr_fixup_test6, &ftr_fixup_test6_expected, size) == 0);
+	check(memcmp(ftr_fixup_test6, ftr_fixup_test6_expected, size) == 0);
 }
 
 static void test_cpu_macros(void)
 {
-	extern u8 ftr_fixup_test_FTR_macros;
-	extern u8 ftr_fixup_test_FTR_macros_expected;
-	unsigned long size = &ftr_fixup_test_FTR_macros_expected -
-			     &ftr_fixup_test_FTR_macros;
+	extern u8 ftr_fixup_test_FTR_macros[];
+	extern u8 ftr_fixup_test_FTR_macros_expected[];
+	unsigned long size = ftr_fixup_test_FTR_macros_expected -
+			     ftr_fixup_test_FTR_macros;
 
 	/* The fixups have already been done for us during boot */
-	check(memcmp(&ftr_fixup_test_FTR_macros,
-		     &ftr_fixup_test_FTR_macros_expected, size) == 0);
+	check(memcmp(ftr_fixup_test_FTR_macros,
+		     ftr_fixup_test_FTR_macros_expected, size) == 0);
 }
 
 static void test_fw_macros(void)
 {
 #ifdef CONFIG_PPC64
-	extern u8 ftr_fixup_test_FW_FTR_macros;
-	extern u8 ftr_fixup_test_FW_FTR_macros_expected;
-	unsigned long size = &ftr_fixup_test_FW_FTR_macros_expected -
-			     &ftr_fixup_test_FW_FTR_macros;
+	extern u8 ftr_fixup_test_FW_FTR_macros[];
+	extern u8 ftr_fixup_test_FW_FTR_macros_expected[];
+	unsigned long size = ftr_fixup_test_FW_FTR_macros_expected -
+			     ftr_fixup_test_FW_FTR_macros;
 
 	/* The fixups have already been done for us during boot */
-	check(memcmp(&ftr_fixup_test_FW_FTR_macros,
-		     &ftr_fixup_test_FW_FTR_macros_expected, size) == 0);
+	check(memcmp(ftr_fixup_test_FW_FTR_macros,
+		     ftr_fixup_test_FW_FTR_macros_expected, size) == 0);
 #endif
 }
 
 static void test_lwsync_macros(void)
 {
-	extern u8 lwsync_fixup_test;
-	extern u8 end_lwsync_fixup_test;
-	extern u8 lwsync_fixup_test_expected_LWSYNC;
-	extern u8 lwsync_fixup_test_expected_SYNC;
-	unsigned long size = &end_lwsync_fixup_test -
-			     &lwsync_fixup_test;
+	extern u8 lwsync_fixup_test[];
+	extern u8 end_lwsync_fixup_test[];
+	extern u8 lwsync_fixup_test_expected_LWSYNC[];
+	extern u8 lwsync_fixup_test_expected_SYNC[];
+	unsigned long size = end_lwsync_fixup_test -
+			     lwsync_fixup_test;
 
 	/* The fixups have already been done for us during boot */
 	if (cur_cpu_spec->cpu_features & CPU_FTR_LWSYNC) {
-		check(memcmp(&lwsync_fixup_test,
-			     &lwsync_fixup_test_expected_LWSYNC, size) == 0);
+		check(memcmp(lwsync_fixup_test,
+			     lwsync_fixup_test_expected_LWSYNC, size) == 0);
 	} else {
-		check(memcmp(&lwsync_fixup_test,
-			     &lwsync_fixup_test_expected_SYNC, size) == 0);
+		check(memcmp(lwsync_fixup_test,
+			     lwsync_fixup_test_expected_SYNC, size) == 0);
 	}
 }
 

commit 589ee62844e042b0b7d19ef57fb4cff77f3ca294
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Prepare to remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Update code that relied on sched.h including various MM types for them.
    
    This will allow us to remove the <linux/mm_types.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 043415f0bdb1..f3917705c686 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -17,6 +17,7 @@
 #include <linux/kernel.h>
 #include <linux/string.h>
 #include <linux/init.h>
+#include <linux/sched/mm.h>
 #include <asm/cputable.h>
 #include <asm/code-patching.h>
 #include <asm/page.h>

commit 97f6e0cc35026a2a09147a6da636d901525e1969
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Wed Aug 10 17:27:34 2016 +1000

    powerpc/32: Fix crash during static key init
    
    We cannot do those initializations from apply_feature_fixups() as
    this function runs in a very restricted environment on 32-bit where
    the kernel isn't running at its linked address and the PTRRELOC()
    macro must be used for any global accesss.
    
    Instead, split them into a separtate steup_feature_keys() function
    which is called in a more suitable spot on ppc32.
    
    Fixes: 309b315b6ec6 ("powerpc: Call jump_label_init() in apply_feature_fixups()")
    Reported-and-tested-by: Christian Kujau <lists@nerdbynature.de>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 74145f02ad41..043415f0bdb1 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -188,7 +188,10 @@ void __init apply_feature_fixups(void)
 			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 #endif
 	do_final_fixups();
+}
 
+void __init setup_feature_keys(void)
+{
 	/*
 	 * Initialise jump label. This causes all the cpu/mmu_has_feature()
 	 * checks to take on their correct polarity based on the current set of

commit 2c0f99516f53911c3f2f81ab3815841e3408f11e
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Aug 2 15:53:01 2016 +1000

    powerpc/32: Fix early access to cpu_spec relocation
    
    Commit 9402c6846131 ("powerpc: Factor do_feature_fixup calls")
    introduced a subtle bug on 32-bit. When reading the cpu spec from the
    global, we not only need to do a pointer relocation on the global
    address but also on the pointer we read from it.
    
    This fixes crashes reported on MPC5200 based machines.
    
    Fixes: 9402c6846131 ("powerpc: Factor do_feature_fixup calls")
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 077fa0ce3382..74145f02ad41 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -161,7 +161,7 @@ static unsigned long __initdata saved_firmware_features;
 
 void __init apply_feature_fixups(void)
 {
-	struct cpu_spec *spec = *PTRRELOC(&cur_cpu_spec);
+	struct cpu_spec *spec = PTRRELOC(*PTRRELOC(&cur_cpu_spec));
 
 	*PTRRELOC(&saved_cpu_features) = spec->cpu_features;
 	*PTRRELOC(&saved_mmu_features) = spec->mmu_features;

commit c12e6f24d4137822d5019c1f78ac65bd27a3447d
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sat Jul 23 14:42:42 2016 +0530

    powerpc: Add option to use jump label for mmu_has_feature()
    
    As we just did for CPU features.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 7a6ccbb74d91..077fa0ce3382 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -196,6 +196,7 @@ void __init apply_feature_fixups(void)
 	 */
 	jump_label_init();
 	cpu_feature_keys_init();
+	mmu_feature_keys_init();
 }
 
 static int __init check_features(void)

commit 4db7327194dba0cb91f274b0f606785a9ee5108d
Author: Kevin Hao <haokexin@gmail.com>
Date:   Sat Jul 23 14:42:41 2016 +0530

    powerpc: Add option to use jump label for cpu_has_feature()
    
    We do binary patching of asm code using CPU features, which is a
    one-time operation, done during early boot. However checks of CPU
    features in C code are currently done at run time, even though the set
    of CPU features can never change after boot.
    
    We can optimise this by using jump labels to implement cpu_has_feature(),
    meaning checks in C code are binary patched into a single nop or branch.
    
    For a C sequence along the lines of:
    
        if (cpu_has_feature(FOO))
             return 2;
    
    The generated code before is roughly:
    
        ld      r9,-27640(r2)
        ld      r9,0(r9)
        lwz     r9,32(r9)
        cmpwi   cr7,r9,0
        bge     cr7, 1f
        li      r3,2
        blr
    1:  ...
    
    After (true):
        nop
        li      r3,2
        blr
    
    After (false):
        b   1f
        li      r3,2
        blr
    1:  ...
    
    mpe: Rename MAX_CPU_FEATURES as we already have a #define with that
    name, and define it simply as a constant, rather than doing tricks with
    sizeof and NULL pointers. Rename the array to cpu_feature_keys. Use the
    kconfig we added to guard it. Add BUILD_BUG_ON() if the feature is not a
    compile time constant. Rewrite the change log.
    
    Signed-off-by: Kevin Hao <haokexin@gmail.com>
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index ae57de78e2aa..7a6ccbb74d91 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -195,6 +195,7 @@ void __init apply_feature_fixups(void)
 	 * CPU/MMU features.
 	 */
 	jump_label_init();
+	cpu_feature_keys_init();
 }
 
 static int __init check_features(void)

commit 309b315b6ec686ce050758cc4e29f6ad1125a83f
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sat Jul 23 14:42:38 2016 +0530

    powerpc: Call jump_label_init() in apply_feature_fixups()
    
    Call jump_label_init() early so that we can use static keys for CPU and
    MMU feature checks.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 59c7caee05b2..ae57de78e2aa 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -13,6 +13,7 @@
  */
 
 #include <linux/types.h>
+#include <linux/jump_label.h>
 #include <linux/kernel.h>
 #include <linux/string.h>
 #include <linux/init.h>
@@ -187,6 +188,13 @@ void __init apply_feature_fixups(void)
 			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 #endif
 	do_final_fixups();
+
+	/*
+	 * Initialise jump label. This causes all the cpu/mmu_has_feature()
+	 * checks to take on their correct polarity based on the current set of
+	 * CPU/MMU features.
+	 */
+	jump_label_init();
 }
 
 static int __init check_features(void)

commit a28e46f109c9637b2539b9995078d5df4f7f6c09
Author: Michael Ellerman <mpe@ellerman.id.au>
Date:   Tue Jul 26 22:29:18 2016 +1000

    powerpc/kernel: Check features don't change after patching
    
    Early in boot we binary patch some sections of code based on the CPU and
    MMU feature bits. But it is a one-time patching, there is no facility
    for repatching the code later if the set of features change.
    
    It is a major bug if the set of features changes after we've done the
    code patching - so add a check for it.
    
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index defb2998b818..59c7caee05b2 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -152,10 +152,19 @@ static void do_final_fixups(void)
 #endif
 }
 
-void apply_feature_fixups(void)
+static unsigned long __initdata saved_cpu_features;
+static unsigned int __initdata saved_mmu_features;
+#ifdef CONFIG_PPC64
+static unsigned long __initdata saved_firmware_features;
+#endif
+
+void __init apply_feature_fixups(void)
 {
 	struct cpu_spec *spec = *PTRRELOC(&cur_cpu_spec);
 
+	*PTRRELOC(&saved_cpu_features) = spec->cpu_features;
+	*PTRRELOC(&saved_mmu_features) = spec->mmu_features;
+
 	/*
 	 * Apply the CPU-specific and firmware specific fixups to kernel text
 	 * (nop out sections not relevant to this CPU or this firmware).
@@ -173,12 +182,28 @@ void apply_feature_fixups(void)
 			 PTRRELOC(&__stop___lwsync_fixup));
 
 #ifdef CONFIG_PPC64
+	saved_firmware_features = powerpc_firmware_features;
 	do_feature_fixups(powerpc_firmware_features,
 			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
 #endif
 	do_final_fixups();
 }
 
+static int __init check_features(void)
+{
+	WARN(saved_cpu_features != cur_cpu_spec->cpu_features,
+	     "CPU features changed after feature patching!\n");
+	WARN(saved_mmu_features != cur_cpu_spec->mmu_features,
+	     "MMU features changed after feature patching!\n");
+#ifdef CONFIG_PPC64
+	WARN(saved_firmware_features != powerpc_firmware_features,
+	     "Firmware features changed after feature patching!\n");
+#endif
+
+	return 0;
+}
+late_initcall(check_features);
+
 #ifdef CONFIG_FTR_FIXUP_SELFTEST
 
 #define check(x)	\

commit 9402c684613163888714df0955fa1f17142b08bf
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Tue Jul 5 15:03:41 2016 +1000

    powerpc: Factor do_feature_fixup calls
    
    32 and 64-bit do a similar set of calls early on, we move it all to
    a single common function to make the boot code more readable.
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 7ce3870d7ddd..defb2998b818 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -20,7 +20,8 @@
 #include <asm/code-patching.h>
 #include <asm/page.h>
 #include <asm/sections.h>
-
+#include <asm/setup.h>
+#include <asm/firmware.h>
 
 struct fixup_entry {
 	unsigned long	mask;
@@ -130,7 +131,7 @@ void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 	}
 }
 
-void do_final_fixups(void)
+static void do_final_fixups(void)
 {
 #if defined(CONFIG_PPC64) && defined(CONFIG_RELOCATABLE)
 	int *src, *dest;
@@ -151,6 +152,33 @@ void do_final_fixups(void)
 #endif
 }
 
+void apply_feature_fixups(void)
+{
+	struct cpu_spec *spec = *PTRRELOC(&cur_cpu_spec);
+
+	/*
+	 * Apply the CPU-specific and firmware specific fixups to kernel text
+	 * (nop out sections not relevant to this CPU or this firmware).
+	 */
+	do_feature_fixups(spec->cpu_features,
+			  PTRRELOC(&__start___ftr_fixup),
+			  PTRRELOC(&__stop___ftr_fixup));
+
+	do_feature_fixups(spec->mmu_features,
+			  PTRRELOC(&__start___mmu_ftr_fixup),
+			  PTRRELOC(&__stop___mmu_ftr_fixup));
+
+	do_lwsync_fixups(spec->cpu_features,
+			 PTRRELOC(&__start___lwsync_fixup),
+			 PTRRELOC(&__stop___lwsync_fixup));
+
+#ifdef CONFIG_PPC64
+	do_feature_fixups(powerpc_firmware_features,
+			  &__start___fw_ftr_fixup, &__stop___fw_ftr_fixup);
+#endif
+	do_final_fixups();
+}
+
 #ifdef CONFIG_FTR_FIXUP_SELFTEST
 
 #define check(x)	\

commit e51df2c170efaeadce4d416e1825b0830de0a795
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 20 08:55:18 2014 +1000

    powerpc: Make a bunch of things static
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 7a8a7487cee8..7ce3870d7ddd 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -164,7 +164,7 @@ static long calc_offset(struct fixup_entry *entry, unsigned int *p)
 	return (unsigned long)p - (unsigned long)entry;
 }
 
-void test_basic_patching(void)
+static void test_basic_patching(void)
 {
 	extern unsigned int ftr_fixup_test1;
 	extern unsigned int end_ftr_fixup_test1;

commit d715e433b7ad19c02fc4becf0d5e9a59f97925de
Author: Anton Blanchard <anton@samba.org>
Date:   Mon Nov 14 12:54:47 2011 +0000

    powerpc: Copy down exception vectors after feature fixups
    
    kdump fails because we try to execute an HV only instruction. Feature
    fixups are being applied after we copy the exception vectors down to 0
    so they miss out on any updates.
    
    We have always had this issue but it only became critical in v3.0
    when we added CFAR support (breaks POWER5) and v3.1 when we added
    POWERNV (breaks everyone).
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: <stable@kernel.org> [v3.0+]
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 0d08d0171392..7a8a7487cee8 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -18,6 +18,8 @@
 #include <linux/init.h>
 #include <asm/cputable.h>
 #include <asm/code-patching.h>
+#include <asm/page.h>
+#include <asm/sections.h>
 
 
 struct fixup_entry {
@@ -128,6 +130,27 @@ void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 	}
 }
 
+void do_final_fixups(void)
+{
+#if defined(CONFIG_PPC64) && defined(CONFIG_RELOCATABLE)
+	int *src, *dest;
+	unsigned long length;
+
+	if (PHYSICAL_START == 0)
+		return;
+
+	src = (int *)(KERNELBASE + PHYSICAL_START);
+	dest = (int *)KERNELBASE;
+	length = (__end_interrupts - _stext) / sizeof(int);
+
+	while (length--) {
+		patch_instruction(dest, *src);
+		src++;
+		dest++;
+	}
+#endif
+}
+
 #ifdef CONFIG_FTR_FIXUP_SELFTEST
 
 #define check(x)	\

commit 3880ecb05bc5ece4c6e392a21ea77518e55b4935
Author: Stephen Rothwell <sfr@canb.auug.org.au>
Date:   Mon Jun 28 21:08:29 2010 +0000

    powerpc: Fix feature-fixup tests for gcc 4.5
    
    The feature-fixup test declare some extern void variables and then take
    their addresses.  Fix this by declaring them as extern u8 instead.
    
    Fixes these warnings (treated as errors):
    
      CC      arch/powerpc/lib/feature-fixups.o
    cc1: warnings being treated as errors
    arch/powerpc/lib/feature-fixups.c: In function 'test_cpu_macros':
    arch/powerpc/lib/feature-fixups.c:293:23: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:294:9: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:297:2: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:297:2: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c: In function 'test_fw_macros':
    arch/powerpc/lib/feature-fixups.c:306:23: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:307:9: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:310:2: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:310:2: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c: In function 'test_lwsync_macros':
    arch/powerpc/lib/feature-fixups.c:321:23: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:322:9: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:326:3: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:326:3: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:329:3: error: taking address of expression of type 'void'
    arch/powerpc/lib/feature-fixups.c:329:3: error: taking address of expression of type 'void'
    
    Signed-off-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index e640175b65ae..0d08d0171392 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -12,6 +12,7 @@
  *  2 of the License, or (at your option) any later version.
  */
 
+#include <linux/types.h>
 #include <linux/kernel.h>
 #include <linux/string.h>
 #include <linux/init.h>
@@ -288,8 +289,8 @@ static void test_alternative_case_with_external_branch(void)
 
 static void test_cpu_macros(void)
 {
-	extern void ftr_fixup_test_FTR_macros;
-	extern void ftr_fixup_test_FTR_macros_expected;
+	extern u8 ftr_fixup_test_FTR_macros;
+	extern u8 ftr_fixup_test_FTR_macros_expected;
 	unsigned long size = &ftr_fixup_test_FTR_macros_expected -
 			     &ftr_fixup_test_FTR_macros;
 
@@ -301,8 +302,8 @@ static void test_cpu_macros(void)
 static void test_fw_macros(void)
 {
 #ifdef CONFIG_PPC64
-	extern void ftr_fixup_test_FW_FTR_macros;
-	extern void ftr_fixup_test_FW_FTR_macros_expected;
+	extern u8 ftr_fixup_test_FW_FTR_macros;
+	extern u8 ftr_fixup_test_FW_FTR_macros_expected;
 	unsigned long size = &ftr_fixup_test_FW_FTR_macros_expected -
 			     &ftr_fixup_test_FW_FTR_macros;
 
@@ -314,10 +315,10 @@ static void test_fw_macros(void)
 
 static void test_lwsync_macros(void)
 {
-	extern void lwsync_fixup_test;
-	extern void end_lwsync_fixup_test;
-	extern void lwsync_fixup_test_expected_LWSYNC;
-	extern void lwsync_fixup_test_expected_SYNC;
+	extern u8 lwsync_fixup_test;
+	extern u8 end_lwsync_fixup_test;
+	extern u8 lwsync_fixup_test_expected_LWSYNC;
+	extern u8 lwsync_fixup_test_expected_SYNC;
 	unsigned long size = &end_lwsync_fixup_test -
 			     &lwsync_fixup_test;
 

commit 3d98ffbffb16f2a1569b83cb78db0b5100e6c937
Author: Benjamin Herrenschmidt <benh@kernel.crashing.org>
Date:   Fri Feb 26 18:29:17 2010 +1100

    powerpc: Fix lwsync feature fixup vs. modules on 64-bit
    
    Anton's commit enabling the use of the lwsync fixup mechanism on 64-bit
    breaks modules. The lwsync fixup section uses .long instead of the
    FTR_ENTRY_OFFSET macro used by other fixups sections, and thus will
    generate 32-bit relocations that our module loader cannot resolve.
    
    This changes it to use the same type as other feature sections.
    
    Note however that we might want to consider using 32-bit for all the
    feature fixup offsets and add support for R_PPC_REL32 to module_64.c
    instead as that would reduce the size of the kernel image. I'll leave
    that as an exercise for the reader for now...
    
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 4dee652dfcad..e640175b65ae 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -112,7 +112,8 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 {
-	int *start, *end, *dest;
+	long *start, *end;
+	unsigned int *dest;
 
 	if (!(value & CPU_FTR_LWSYNC))
 		return ;

commit 53eae2281ad2607fa66a8ad1cb06186c8900da56
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Feb 10 01:07:19 2010 +0000

    powerpc: Fix lwsync patching code on 64bit
    
    do_lwsync_fixups doesn't work on 64bit, we end up writing lwsyncs to the
    wrong addresses:
    
    0:mon> di c0000001000bfacc
    c0000001000bfacc  7c2004ac      lwsync
    
    Since the lwsync section has negative offsets we need to use a signed int
    pointer so we sign extend the value.
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 7e8865bcd683..4dee652dfcad 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -112,7 +112,7 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 
 void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 {
-	unsigned int *start, *end, *dest;
+	int *start, *end, *dest;
 
 	if (!(value & CPU_FTR_LWSYNC))
 		return ;

commit 16c57b3620d77e0bc981da5ef32beae730512684
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Tue Feb 10 20:10:44 2009 +0000

    powerpc: Unify opcode definitions and support
    
    Create a new header that becomes a single location for defining PowerPC
    opcodes used by code that is either generationg instructions
    at runtime (fixups, debug, etc.), emulating instructions, or just
    compiling instructions old assemblers don't know about.
    
    We currently don't handle the floating point emulation or alignment decode
    as both are better handled by the specific decode support they already
    have.
    
    Added support for the new dcbzl, dcbal, msgsnd, tlbilx, & wait instructions
    since older assemblers don't know about them.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 8c5a03be31e0..7e8865bcd683 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -85,7 +85,7 @@ static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
 	}
 
 	for (; dest < end; dest++)
-		patch_instruction(dest, PPC_NOP_INSTR);
+		patch_instruction(dest, PPC_INST_NOP);
 
 	return 0;
 }
@@ -122,7 +122,7 @@ void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 
 	for (; start < end; start++) {
 		dest = (void *)start + *start;
-		patch_instruction(dest, PPC_LWSYNC_INSTR);
+		patch_instruction(dest, PPC_INST_LWSYNC);
 	}
 }
 

commit 1856c020402f602b2bd26bcc9b77250304565b7e
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Thu Jul 17 14:46:00 2008 +1000

    powerpc: Use WARN_ON(1) instead of __WARN()
    
    __WARN() is not defined for all configs, use WARN_ON(1) instead.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Benjamin Herrenschmidt <benh@kernel.crashing.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 4e43702b9813..8c5a03be31e0 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -99,7 +99,7 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 
 	for (; fcur < fend; fcur++) {
 		if (patch_feature_section(value, fcur)) {
-			__WARN();
+			WARN_ON(1);
 			printk("Unable to patch feature section at %p - %p" \
 				" with %p - %p\n",
 				calc_addr(fcur, fcur->start_off),

commit 2d1b2027626d5151fff8ef7c06ca8e7876a1a510
Author: Kumar Gala <galak@kernel.crashing.org>
Date:   Wed Jul 2 01:16:40 2008 +1000

    powerpc: Fixup lwsync at runtime
    
    To allow for a single kernel image on e500 v1/v2/mc we need to fixup lwsync
    at runtime.  On e500v1/v2 lwsync causes an illop so we need to patch up
    the code.  We default to 'sync' since that is always safe and if the cpu
    is capable we will replace 'sync' with 'lwsync'.
    
    We introduce CPU_FTR_LWSYNC as a way to determine at runtime if this is
    needed.  This flag could be moved elsewhere since we dont really use it
    for the normal CPU_FTR purpose.
    
    Finally we only store the relative offset in the fixup section to keep it
    as small as possible rather than using a full fixup_entry.
    
    Signed-off-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 48e1ed89052d..4e43702b9813 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -110,6 +110,22 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 	}
 }
 
+void do_lwsync_fixups(unsigned long value, void *fixup_start, void *fixup_end)
+{
+	unsigned int *start, *end, *dest;
+
+	if (!(value & CPU_FTR_LWSYNC))
+		return ;
+
+	start = fixup_start;
+	end = fixup_end;
+
+	for (; start < end; start++) {
+		dest = (void *)start + *start;
+		patch_instruction(dest, PPC_LWSYNC_INSTR);
+	}
+}
+
 #ifdef CONFIG_FTR_FIXUP_SELFTEST
 
 #define check(x)	\
@@ -295,6 +311,25 @@ static void test_fw_macros(void)
 #endif
 }
 
+static void test_lwsync_macros(void)
+{
+	extern void lwsync_fixup_test;
+	extern void end_lwsync_fixup_test;
+	extern void lwsync_fixup_test_expected_LWSYNC;
+	extern void lwsync_fixup_test_expected_SYNC;
+	unsigned long size = &end_lwsync_fixup_test -
+			     &lwsync_fixup_test;
+
+	/* The fixups have already been done for us during boot */
+	if (cur_cpu_spec->cpu_features & CPU_FTR_LWSYNC) {
+		check(memcmp(&lwsync_fixup_test,
+			     &lwsync_fixup_test_expected_LWSYNC, size) == 0);
+	} else {
+		check(memcmp(&lwsync_fixup_test,
+			     &lwsync_fixup_test_expected_SYNC, size) == 0);
+	}
+}
+
 static int __init test_feature_fixups(void)
 {
 	printk(KERN_DEBUG "Running feature fixup self-tests ...\n");
@@ -307,6 +342,7 @@ static int __init test_feature_fixups(void)
 	test_alternative_case_with_external_branch();
 	test_cpu_macros();
 	test_fw_macros();
+	test_lwsync_macros();
 
 	return 0;
 }

commit 362e7701fd183d990e0863883461edef61392710
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Jun 24 11:33:03 2008 +1000

    powerpc: Add self-tests of the feature fixup code
    
    This commit adds tests of the feature fixup code, they are run during
    boot if CONFIG_FTR_FIXUP_SELFTEST=y. Some of the tests manually invoke
    the patching routines to check their behaviour, and others use the
    macros and so are patched during the normal patching done during boot.
    
    Because we have two sets of macros with different names, we use a macro
    to generate the test of the macros, very niiiice.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 174c1eba9ac8..48e1ed89052d 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -13,6 +13,8 @@
  */
 
 #include <linux/kernel.h>
+#include <linux/string.h>
+#include <linux/init.h>
 #include <asm/cputable.h>
 #include <asm/code-patching.h>
 
@@ -107,3 +109,207 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 		}
 	}
 }
+
+#ifdef CONFIG_FTR_FIXUP_SELFTEST
+
+#define check(x)	\
+	if (!(x)) printk("feature-fixups: test failed at line %d\n", __LINE__);
+
+/* This must be after the text it fixes up, vmlinux.lds.S enforces that atm */
+static struct fixup_entry fixup;
+
+static long calc_offset(struct fixup_entry *entry, unsigned int *p)
+{
+	return (unsigned long)p - (unsigned long)entry;
+}
+
+void test_basic_patching(void)
+{
+	extern unsigned int ftr_fixup_test1;
+	extern unsigned int end_ftr_fixup_test1;
+	extern unsigned int ftr_fixup_test1_orig;
+	extern unsigned int ftr_fixup_test1_expected;
+	int size = &end_ftr_fixup_test1 - &ftr_fixup_test1;
+
+	fixup.value = fixup.mask = 8;
+	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test1 + 1);
+	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test1 + 2);
+	fixup.alt_start_off = fixup.alt_end_off = 0;
+
+	/* Sanity check */
+	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_orig, size) == 0);
+
+	/* Check we don't patch if the value matches */
+	patch_feature_section(8, &fixup);
+	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_orig, size) == 0);
+
+	/* Check we do patch if the value doesn't match */
+	patch_feature_section(0, &fixup);
+	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_expected, size) == 0);
+
+	/* Check we do patch if the mask doesn't match */
+	memcpy(&ftr_fixup_test1, &ftr_fixup_test1_orig, size);
+	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_orig, size) == 0);
+	patch_feature_section(~8, &fixup);
+	check(memcmp(&ftr_fixup_test1, &ftr_fixup_test1_expected, size) == 0);
+}
+
+static void test_alternative_patching(void)
+{
+	extern unsigned int ftr_fixup_test2;
+	extern unsigned int end_ftr_fixup_test2;
+	extern unsigned int ftr_fixup_test2_orig;
+	extern unsigned int ftr_fixup_test2_alt;
+	extern unsigned int ftr_fixup_test2_expected;
+	int size = &end_ftr_fixup_test2 - &ftr_fixup_test2;
+
+	fixup.value = fixup.mask = 0xF;
+	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test2 + 1);
+	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test2 + 2);
+	fixup.alt_start_off = calc_offset(&fixup, &ftr_fixup_test2_alt);
+	fixup.alt_end_off = calc_offset(&fixup, &ftr_fixup_test2_alt + 1);
+
+	/* Sanity check */
+	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_orig, size) == 0);
+
+	/* Check we don't patch if the value matches */
+	patch_feature_section(0xF, &fixup);
+	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_orig, size) == 0);
+
+	/* Check we do patch if the value doesn't match */
+	patch_feature_section(0, &fixup);
+	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_expected, size) == 0);
+
+	/* Check we do patch if the mask doesn't match */
+	memcpy(&ftr_fixup_test2, &ftr_fixup_test2_orig, size);
+	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_orig, size) == 0);
+	patch_feature_section(~0xF, &fixup);
+	check(memcmp(&ftr_fixup_test2, &ftr_fixup_test2_expected, size) == 0);
+}
+
+static void test_alternative_case_too_big(void)
+{
+	extern unsigned int ftr_fixup_test3;
+	extern unsigned int end_ftr_fixup_test3;
+	extern unsigned int ftr_fixup_test3_orig;
+	extern unsigned int ftr_fixup_test3_alt;
+	int size = &end_ftr_fixup_test3 - &ftr_fixup_test3;
+
+	fixup.value = fixup.mask = 0xC;
+	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test3 + 1);
+	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test3 + 2);
+	fixup.alt_start_off = calc_offset(&fixup, &ftr_fixup_test3_alt);
+	fixup.alt_end_off = calc_offset(&fixup, &ftr_fixup_test3_alt + 2);
+
+	/* Sanity check */
+	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+
+	/* Expect nothing to be patched, and the error returned to us */
+	check(patch_feature_section(0xF, &fixup) == 1);
+	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+	check(patch_feature_section(0, &fixup) == 1);
+	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+	check(patch_feature_section(~0xF, &fixup) == 1);
+	check(memcmp(&ftr_fixup_test3, &ftr_fixup_test3_orig, size) == 0);
+}
+
+static void test_alternative_case_too_small(void)
+{
+	extern unsigned int ftr_fixup_test4;
+	extern unsigned int end_ftr_fixup_test4;
+	extern unsigned int ftr_fixup_test4_orig;
+	extern unsigned int ftr_fixup_test4_alt;
+	extern unsigned int ftr_fixup_test4_expected;
+	int size = &end_ftr_fixup_test4 - &ftr_fixup_test4;
+	unsigned long flag;
+
+	/* Check a high-bit flag */
+	flag = 1UL << ((sizeof(unsigned long) - 1) * 8);
+	fixup.value = fixup.mask = flag;
+	fixup.start_off = calc_offset(&fixup, &ftr_fixup_test4 + 1);
+	fixup.end_off = calc_offset(&fixup, &ftr_fixup_test4 + 5);
+	fixup.alt_start_off = calc_offset(&fixup, &ftr_fixup_test4_alt);
+	fixup.alt_end_off = calc_offset(&fixup, &ftr_fixup_test4_alt + 2);
+
+	/* Sanity check */
+	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_orig, size) == 0);
+
+	/* Check we don't patch if the value matches */
+	patch_feature_section(flag, &fixup);
+	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_orig, size) == 0);
+
+	/* Check we do patch if the value doesn't match */
+	patch_feature_section(0, &fixup);
+	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_expected, size) == 0);
+
+	/* Check we do patch if the mask doesn't match */
+	memcpy(&ftr_fixup_test4, &ftr_fixup_test4_orig, size);
+	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_orig, size) == 0);
+	patch_feature_section(~flag, &fixup);
+	check(memcmp(&ftr_fixup_test4, &ftr_fixup_test4_expected, size) == 0);
+}
+
+static void test_alternative_case_with_branch(void)
+{
+	extern unsigned int ftr_fixup_test5;
+	extern unsigned int end_ftr_fixup_test5;
+	extern unsigned int ftr_fixup_test5_expected;
+	int size = &end_ftr_fixup_test5 - &ftr_fixup_test5;
+
+	check(memcmp(&ftr_fixup_test5, &ftr_fixup_test5_expected, size) == 0);
+}
+
+static void test_alternative_case_with_external_branch(void)
+{
+	extern unsigned int ftr_fixup_test6;
+	extern unsigned int end_ftr_fixup_test6;
+	extern unsigned int ftr_fixup_test6_expected;
+	int size = &end_ftr_fixup_test6 - &ftr_fixup_test6;
+
+	check(memcmp(&ftr_fixup_test6, &ftr_fixup_test6_expected, size) == 0);
+}
+
+static void test_cpu_macros(void)
+{
+	extern void ftr_fixup_test_FTR_macros;
+	extern void ftr_fixup_test_FTR_macros_expected;
+	unsigned long size = &ftr_fixup_test_FTR_macros_expected -
+			     &ftr_fixup_test_FTR_macros;
+
+	/* The fixups have already been done for us during boot */
+	check(memcmp(&ftr_fixup_test_FTR_macros,
+		     &ftr_fixup_test_FTR_macros_expected, size) == 0);
+}
+
+static void test_fw_macros(void)
+{
+#ifdef CONFIG_PPC64
+	extern void ftr_fixup_test_FW_FTR_macros;
+	extern void ftr_fixup_test_FW_FTR_macros_expected;
+	unsigned long size = &ftr_fixup_test_FW_FTR_macros_expected -
+			     &ftr_fixup_test_FW_FTR_macros;
+
+	/* The fixups have already been done for us during boot */
+	check(memcmp(&ftr_fixup_test_FW_FTR_macros,
+		     &ftr_fixup_test_FW_FTR_macros_expected, size) == 0);
+#endif
+}
+
+static int __init test_feature_fixups(void)
+{
+	printk(KERN_DEBUG "Running feature fixup self-tests ...\n");
+
+	test_basic_patching();
+	test_alternative_patching();
+	test_alternative_case_too_big();
+	test_alternative_case_too_small();
+	test_alternative_case_with_branch();
+	test_alternative_case_with_external_branch();
+	test_cpu_macros();
+	test_fw_macros();
+
+	return 0;
+}
+late_initcall(test_feature_fixups);
+
+#endif /* CONFIG_FTR_FIXUP_SELFTEST */

commit 9b1a735de64cc975c31a1642ec55e082ddbdfeaf
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Jun 24 11:33:02 2008 +1000

    powerpc: Add logic to patch alternative feature sections
    
    This commit adds the logic to patch alternative sections.  This is fairly
    straightforward, except for branches.  Relative branches that jump from
    inside the else section to outside of it need to be translated as they're
    moved, otherwise they will jump to the wrong location.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index 973d547ef01d..174c1eba9ac8 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -26,24 +26,66 @@ struct fixup_entry {
 	long		alt_end_off;
 };
 
-static void patch_feature_section(unsigned long value, struct fixup_entry *fcur)
+static unsigned int *calc_addr(struct fixup_entry *fcur, long offset)
 {
-	unsigned int *pstart, *pend, *p;
+	/*
+	 * We store the offset to the code as a negative offset from
+	 * the start of the alt_entry, to support the VDSO. This
+	 * routine converts that back into an actual address.
+	 */
+	return (unsigned int *)((unsigned long)fcur + offset);
+}
+
+static int patch_alt_instruction(unsigned int *src, unsigned int *dest,
+				 unsigned int *alt_start, unsigned int *alt_end)
+{
+	unsigned int instr;
+
+	instr = *src;
+
+	if (instr_is_relative_branch(*src)) {
+		unsigned int *target = (unsigned int *)branch_target(src);
+
+		/* Branch within the section doesn't need translating */
+		if (target < alt_start || target >= alt_end) {
+			instr = translate_branch(dest, src);
+			if (!instr)
+				return 1;
+		}
+	}
+
+	patch_instruction(dest, instr);
+
+	return 0;
+}
+
+static int patch_feature_section(unsigned long value, struct fixup_entry *fcur)
+{
+	unsigned int *start, *end, *alt_start, *alt_end, *src, *dest;
+
+	start = calc_addr(fcur, fcur->start_off);
+	end = calc_addr(fcur, fcur->end_off);
+	alt_start = calc_addr(fcur, fcur->alt_start_off);
+	alt_end = calc_addr(fcur, fcur->alt_end_off);
+
+	if ((alt_end - alt_start) > (end - start))
+		return 1;
 
 	if ((value & fcur->mask) == fcur->value)
-		return;
+		return 0;
 
-	pstart = ((unsigned int *)fcur) + (fcur->start_off / 4);
-	pend = ((unsigned int *)fcur) + (fcur->end_off / 4);
+	src = alt_start;
+	dest = start;
 
-	for (p = pstart; p < pend; p++) {
-		*p = PPC_NOP_INSTR;
-		asm volatile ("dcbst 0, %0" : : "r" (p));
+	for (; src < alt_end; src++, dest++) {
+		if (patch_alt_instruction(src, dest, alt_start, alt_end))
+			return 1;
 	}
-	asm volatile ("sync" : : : "memory");
-	for (p = pstart; p < pend; p++)
-		asm volatile ("icbi 0,%0" : : "r" (p));
-	asm volatile ("sync; isync" : : : "memory");
+
+	for (; dest < end; dest++)
+		patch_instruction(dest, PPC_NOP_INSTR);
+
+	return 0;
 }
 
 void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
@@ -53,6 +95,15 @@ void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
 	fcur = fixup_start;
 	fend = fixup_end;
 
-	for (; fcur < fend; fcur++)
-		patch_feature_section(value, fcur);
+	for (; fcur < fend; fcur++) {
+		if (patch_feature_section(value, fcur)) {
+			__WARN();
+			printk("Unable to patch feature section at %p - %p" \
+				" with %p - %p\n",
+				calc_addr(fcur, fcur->start_off),
+				calc_addr(fcur, fcur->end_off),
+				calc_addr(fcur, fcur->alt_start_off),
+				calc_addr(fcur, fcur->alt_end_off));
+		}
+	}
 }

commit fac23fe4be23259a8eaa9bad822f5b14dd07d15c
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Jun 24 11:32:54 2008 +1000

    powerpc: Introduce infrastructure for feature sections with alternatives
    
    The current feature section logic only supports nop'ing out code, this means
    if you want to choose at runtime between instruction sequences, one or both
    cases will have to execute the nop'ed out contents of the other section, eg:
    
    BEGIN_FTR_SECTION
            or      1,1,1
    END_FTR_SECTION_IFSET(FOO)
    BEGIN_FTR_SECTION
            or      2,2,2
    END_FTR_SECTION_IFCLR(FOO)
    
    and the resulting code will be either,
    
            or      1,1,1
            nop
    
    or,
            nop
            or      2,2,2
    
    For small code segments this is fine, but for larger code blocks and in
    performance criticial code segments, it would be nice to avoid the nops.
    This commit starts to implement logic to allow the following:
    
    BEGIN_FTR_SECTION
            or      1,1,1
    FTR_SECTION_ELSE
            or      2,2,2
    ALT_FTR_SECTION_END_IFSET(FOO)
    
    and the resulting code will be:
    
            or      1,1,1
    or,
            or      2,2,2
    
    We achieve this by extending the existing FTR macros. The current feature
    section semantic just becomes a special case, ie. if the else case is empty
    we nop out the default case.
    
    The key limitation is that the size of the else case must be less than or
    equal to the size of the default case. If the else case is smaller the
    remainder of the section is nop'ed.
    
    We let the linker put the else case code in with the rest of the text,
    so that relative branches from the else case are more likley to link,
    this has the disadvantage that we can't free the unused else cases.
    
    This commit introduces the required macro and linker script changes, but
    does not enable the patching of the alternative sections.
    
    We also need to update two hand-made section entries in reg.h and timex.h
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
index f6fd5d2ff10d..973d547ef01d 100644
--- a/arch/powerpc/lib/feature-fixups.c
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -22,6 +22,8 @@ struct fixup_entry {
 	unsigned long	value;
 	long		start_off;
 	long		end_off;
+	long		alt_start_off;
+	long		alt_end_off;
 };
 
 static void patch_feature_section(unsigned long value, struct fixup_entry *fcur)

commit 51c52e86694f19e84600a40f6156889feafd8ae9
Author: Michael Ellerman <michael@ellerman.id.au>
Date:   Tue Jun 24 11:32:36 2008 +1000

    powerpc: Split out do_feature_fixups() from cputable.c
    
    The logic to patch CPU feature sections lives in cputable.c, but these days
    it's used for CPU features as well as firmware features.  Move it into
    it's own file for neatness and as preparation for some additions.
    
    While we're moving the code, we pull the loop body logic into a separate
    routine, and remove a comment which doesn't apply anymore.
    
    Signed-off-by: Michael Ellerman <michael@ellerman.id.au>
    Acked-by: Kumar Gala <galak@kernel.crashing.org>
    Signed-off-by: Paul Mackerras <paulus@samba.org>

diff --git a/arch/powerpc/lib/feature-fixups.c b/arch/powerpc/lib/feature-fixups.c
new file mode 100644
index 000000000000..f6fd5d2ff10d
--- /dev/null
+++ b/arch/powerpc/lib/feature-fixups.c
@@ -0,0 +1,56 @@
+/*
+ *  Copyright (C) 2001 Ben. Herrenschmidt (benh@kernel.crashing.org)
+ *
+ *  Modifications for ppc64:
+ *      Copyright (C) 2003 Dave Engebretsen <engebret@us.ibm.com>
+ *
+ *  Copyright 2008 Michael Ellerman, IBM Corporation.
+ *
+ *  This program is free software; you can redistribute it and/or
+ *  modify it under the terms of the GNU General Public License
+ *  as published by the Free Software Foundation; either version
+ *  2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/kernel.h>
+#include <asm/cputable.h>
+#include <asm/code-patching.h>
+
+
+struct fixup_entry {
+	unsigned long	mask;
+	unsigned long	value;
+	long		start_off;
+	long		end_off;
+};
+
+static void patch_feature_section(unsigned long value, struct fixup_entry *fcur)
+{
+	unsigned int *pstart, *pend, *p;
+
+	if ((value & fcur->mask) == fcur->value)
+		return;
+
+	pstart = ((unsigned int *)fcur) + (fcur->start_off / 4);
+	pend = ((unsigned int *)fcur) + (fcur->end_off / 4);
+
+	for (p = pstart; p < pend; p++) {
+		*p = PPC_NOP_INSTR;
+		asm volatile ("dcbst 0, %0" : : "r" (p));
+	}
+	asm volatile ("sync" : : : "memory");
+	for (p = pstart; p < pend; p++)
+		asm volatile ("icbi 0,%0" : : "r" (p));
+	asm volatile ("sync; isync" : : : "memory");
+}
+
+void do_feature_fixups(unsigned long value, void *fixup_start, void *fixup_end)
+{
+	struct fixup_entry *fcur, *fend;
+
+	fcur = fixup_start;
+	fend = fixup_end;
+
+	for (; fcur < fend; fcur++)
+		patch_feature_section(value, fcur);
+}
