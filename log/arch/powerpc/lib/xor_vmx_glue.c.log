commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/powerpc/lib/xor_vmx_glue.c b/arch/powerpc/lib/xor_vmx_glue.c
index dab2b6bfcf36..80dba916c367 100644
--- a/arch/powerpc/lib/xor_vmx_glue.c
+++ b/arch/powerpc/lib/xor_vmx_glue.c
@@ -1,12 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Altivec XOR operations
  *
  * Copyright 2017 IBM Corp.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <linux/preempt.h>

commit 7cf76a68f1bcf69214da2812e8f615e36e16e60e
Author: Mathieu Malaterre <malat@debian.org>
Date:   Wed Mar 28 20:55:25 2018 +0200

    powerpc/altivec: Add missing prototypes for altivec
    
    Some functions prototypes were missing for the non-altivec code. Add the
    missing prototypes in a new header file, fix warnings treated as errors
    with W=1:
    
      arch/powerpc/lib/xor_vmx_glue.c:18:6: error: no previous prototype for ‘xor_altivec_2’ [-Werror=missing-prototypes]
      arch/powerpc/lib/xor_vmx_glue.c:29:6: error: no previous prototype for ‘xor_altivec_3’ [-Werror=missing-prototypes]
      arch/powerpc/lib/xor_vmx_glue.c:40:6: error: no previous prototype for ‘xor_altivec_4’ [-Werror=missing-prototypes]
      arch/powerpc/lib/xor_vmx_glue.c:52:6: error: no previous prototype for ‘xor_altivec_5’ [-Werror=missing-prototypes]
    
    The prototypes were already present in <asm/xor.h> but this header file is
    meant to be included after <include/linux/raid/xor.h>. Trying to re-use
    <asm/xor.h> directly would lead to warnings such as:
    
      arch/powerpc/include/asm/xor.h:39:15: error: variable ‘xor_block_altivec’ has initializer but incomplete type
    
    Trying to re-use <asm/xor.h> after <include/linux/raid/xor.h> in
    xor_vmx_glue.c would in turn trigger the following warnings:
    
      include/asm-generic/xor.h:688:34: error: ‘xor_block_32regs’ defined but not used [-Werror=unused-variable]
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/xor_vmx_glue.c b/arch/powerpc/lib/xor_vmx_glue.c
index 6521fe5e8cef..dab2b6bfcf36 100644
--- a/arch/powerpc/lib/xor_vmx_glue.c
+++ b/arch/powerpc/lib/xor_vmx_glue.c
@@ -13,6 +13,7 @@
 #include <linux/export.h>
 #include <linux/sched.h>
 #include <asm/switch_to.h>
+#include <asm/xor_altivec.h>
 #include "xor_vmx.h"
 
 void xor_altivec_2(unsigned long bytes, unsigned long *v1_in,

commit f718d426d7e42eec6e5d2932f52a51de23bd3b30
Author: Matt Brown <matthew.brown.dev@gmail.com>
Date:   Wed May 24 09:45:59 2017 +1000

    powerpc/lib/xor_vmx: Ensure no altivec code executes before enable_kernel_altivec()
    
    The xor_vmx.c file is used for the RAID5 xor operations. In these functions
    altivec is enabled to run the operation and then disabled.
    
    The code uses enable_kernel_altivec() around the core of the algorithm, however
    the whole file is built with -maltivec, so the compiler is within its rights to
    generate altivec code anywhere. This has been seen at least once in the wild:
    
      0:mon> di $xor_altivec_2
      c0000000000b97d0  3c4c01d9    addis   r2,r12,473
      c0000000000b97d4  3842db30    addi    r2,r2,-9424
      c0000000000b97d8  7c0802a6    mflr    r0
      c0000000000b97dc  f8010010    std     r0,16(r1)
      c0000000000b97e0  60000000    nop
      c0000000000b97e4  7c0802a6    mflr    r0
      c0000000000b97e8  faa1ffa8    std     r21,-88(r1)
      ...
      c0000000000b981c  f821ff41    stdu    r1,-192(r1)
      c0000000000b9820  7f8101ce    stvx    v28,r1,r0               <-- POP
      c0000000000b9824  38000030    li      r0,48
      c0000000000b9828  7fa101ce    stvx    v29,r1,r0
      ...
      c0000000000b984c  4bf6a06d    bl      c0000000000238b8 # enable_kernel_altivec
    
    This patch splits the non-altivec code into xor_vmx_glue.c which calls the
    altivec functions in xor_vmx.c. By compiling xor_vmx_glue.c without
    -maltivec we can guarantee that altivec instruction will not be executed
    outside of the enable/disable block.
    
    Signed-off-by: Matt Brown <matthew.brown.dev@gmail.com>
    [mpe: Rework change log and include disassembly]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/arch/powerpc/lib/xor_vmx_glue.c b/arch/powerpc/lib/xor_vmx_glue.c
new file mode 100644
index 000000000000..6521fe5e8cef
--- /dev/null
+++ b/arch/powerpc/lib/xor_vmx_glue.c
@@ -0,0 +1,62 @@
+/*
+ * Altivec XOR operations
+ *
+ * Copyright 2017 IBM Corp.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/preempt.h>
+#include <linux/export.h>
+#include <linux/sched.h>
+#include <asm/switch_to.h>
+#include "xor_vmx.h"
+
+void xor_altivec_2(unsigned long bytes, unsigned long *v1_in,
+		   unsigned long *v2_in)
+{
+	preempt_disable();
+	enable_kernel_altivec();
+	__xor_altivec_2(bytes, v1_in, v2_in);
+	disable_kernel_altivec();
+	preempt_enable();
+}
+EXPORT_SYMBOL(xor_altivec_2);
+
+void xor_altivec_3(unsigned long bytes,  unsigned long *v1_in,
+		   unsigned long *v2_in, unsigned long *v3_in)
+{
+	preempt_disable();
+	enable_kernel_altivec();
+	__xor_altivec_3(bytes, v1_in, v2_in, v3_in);
+	disable_kernel_altivec();
+	preempt_enable();
+}
+EXPORT_SYMBOL(xor_altivec_3);
+
+void xor_altivec_4(unsigned long bytes,  unsigned long *v1_in,
+		   unsigned long *v2_in, unsigned long *v3_in,
+		   unsigned long *v4_in)
+{
+	preempt_disable();
+	enable_kernel_altivec();
+	__xor_altivec_4(bytes, v1_in, v2_in, v3_in, v4_in);
+	disable_kernel_altivec();
+	preempt_enable();
+}
+EXPORT_SYMBOL(xor_altivec_4);
+
+void xor_altivec_5(unsigned long bytes,  unsigned long *v1_in,
+		   unsigned long *v2_in, unsigned long *v3_in,
+		   unsigned long *v4_in, unsigned long *v5_in)
+{
+	preempt_disable();
+	enable_kernel_altivec();
+	__xor_altivec_5(bytes, v1_in, v2_in, v3_in, v4_in, v5_in);
+	disable_kernel_altivec();
+	preempt_enable();
+}
+EXPORT_SYMBOL(xor_altivec_5);
