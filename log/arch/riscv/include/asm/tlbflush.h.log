commit 6bd33e1ece528f67646db33bf97406b747dafda0
Author: Christoph Hellwig <hch@lst.de>
Date:   Mon Oct 28 13:10:41 2019 +0100

    riscv: add nommu support
    
    The kernel runs in M-mode without using page tables, and thus can't run
    bare metal without help from additional firmware.
    
    Most of the patch is just stubbing out code not needed without page
    tables, but there is an interesting detail in the signals implementation:
    
     - The normal RISC-V syscall ABI only implements rt_sigreturn as VDSO
       entry point, but the ELF VDSO is not supported for nommu Linux.
       We instead copy the code to call the syscall onto the stack.
    
    In addition to enabling the nommu code a new defconfig for a small
    kernel image that can run in nommu mode on qemu is also provided, to run
    a kernel in qemu you can use the following command line:
    
    qemu-system-riscv64 -smp 2 -m 64 -machine virt -nographic \
            -kernel arch/riscv/boot/loader \
            -drive file=rootfs.ext2,format=raw,id=hd0 \
            -device virtio-blk-device,drive=hd0
    
    Contains contributions from Damien Le Moal <Damien.LeMoal@wdc.com>.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Anup Patel <anup@brainfault.org>
    [paul.walmsley@sifive.com: updated to apply; add CONFIG_MMU guards
     around PCI_IOBASE definition to fix build issues; fixed checkpatch
     issues; move the PCI_IO_* and VMEMMAP address space macros along
     with the others; resolve sparse warning]
    Signed-off-by: Paul Walmsley <paul.walmsley@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index f02188a5b0f4..394cfbccdcd9 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -10,6 +10,7 @@
 #include <linux/mm_types.h>
 #include <asm/smp.h>
 
+#ifdef CONFIG_MMU
 static inline void local_flush_tlb_all(void)
 {
 	__asm__ __volatile__ ("sfence.vma" : : : "memory");
@@ -20,14 +21,19 @@ static inline void local_flush_tlb_page(unsigned long addr)
 {
 	__asm__ __volatile__ ("sfence.vma %0" : : "r" (addr) : "memory");
 }
+#else /* CONFIG_MMU */
+#define local_flush_tlb_all()			do { } while (0)
+#define local_flush_tlb_page(addr)		do { } while (0)
+#endif /* CONFIG_MMU */
 
-#ifdef CONFIG_SMP
+#if defined(CONFIG_SMP) && defined(CONFIG_MMU)
 void flush_tlb_all(void);
 void flush_tlb_mm(struct mm_struct *mm);
 void flush_tlb_page(struct vm_area_struct *vma, unsigned long addr);
 void flush_tlb_range(struct vm_area_struct *vma, unsigned long start,
 		     unsigned long end);
-#else /* CONFIG_SMP */
+#else /* CONFIG_SMP && CONFIG_MMU */
+
 #define flush_tlb_all() local_flush_tlb_all()
 #define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)
 
@@ -38,7 +44,7 @@ static inline void flush_tlb_range(struct vm_area_struct *vma,
 }
 
 #define flush_tlb_mm(mm) flush_tlb_all()
-#endif /* CONFIG_SMP */
+#endif /* !CONFIG_SMP || !CONFIG_MMU */
 
 /* Flush a range of kernel pages */
 static inline void flush_tlb_kernel_range(unsigned long start,

commit 4c8eb19cf9dc5fcc489757acbf93be90baf25848
Author: Paul Walmsley <paul.walmsley@sifive.com>
Date:   Thu Oct 10 15:57:58 2019 -0700

    riscv: tlbflush: remove confusing comment on local_flush_tlb_all()
    
    Remove a confusing comment on our local_flush_tlb_all()
    implementation.  Per an internal discussion with Andrew, while it's
    true that the fence.i is not necessary, it's not the case that an
    sfence.vma implies a fence.i.  We also drop the section about
    "flush[ing] the entire local TLB" to better align with the language in
    section 4.2.1 "Supervisor Memory-Management Fence Instruction" of the
    RISC-V Privileged Specification v20190608.
    
    Fixes: c901e45a999a1 ("RISC-V: `sfence.vma` orderes the instruction cache")
    Reported-by: Alan Kao <alankao@andestech.com>
    Cc: Palmer Dabbelt <palmer@sifive.com>
    Cc: Andrew Waterman <andrew@sifive.com>
    Signed-off-by: Paul Walmsley <paul.walmsley@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 37ae4e367ad2..f02188a5b0f4 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -10,10 +10,6 @@
 #include <linux/mm_types.h>
 #include <asm/smp.h>
 
-/*
- * Flush entire local TLB.  'sfence.vma' implicitly fences with the instruction
- * cache as well, so a 'fence.i' is not necessary.
- */
 static inline void local_flush_tlb_all(void)
 {
 	__asm__ __volatile__ ("sfence.vma" : : : "memory");

commit 95594cb40c6e013e04659f7316fbdebe83913c58
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 21 23:58:37 2019 +0900

    riscv: move the TLB flush logic out of line
    
    The TLB flush logic is going to become more complex.  Start moving
    it out of line.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Atish Patra <atish.patra@wdc.com>
    [paul.walmsley@sifive.com: fixed checkpatch whitespace warnings]
    Signed-off-by: Paul Walmsley <paul.walmsley@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index df31fe2ed09c..37ae4e367ad2 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -25,8 +25,13 @@ static inline void local_flush_tlb_page(unsigned long addr)
 	__asm__ __volatile__ ("sfence.vma %0" : : "r" (addr) : "memory");
 }
 
-#ifndef CONFIG_SMP
-
+#ifdef CONFIG_SMP
+void flush_tlb_all(void);
+void flush_tlb_mm(struct mm_struct *mm);
+void flush_tlb_page(struct vm_area_struct *vma, unsigned long addr);
+void flush_tlb_range(struct vm_area_struct *vma, unsigned long start,
+		     unsigned long end);
+#else /* CONFIG_SMP */
 #define flush_tlb_all() local_flush_tlb_all()
 #define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)
 
@@ -37,34 +42,6 @@ static inline void flush_tlb_range(struct vm_area_struct *vma,
 }
 
 #define flush_tlb_mm(mm) flush_tlb_all()
-
-#else /* CONFIG_SMP */
-
-#include <asm/sbi.h>
-
-static inline void remote_sfence_vma(struct cpumask *cmask, unsigned long start,
-				     unsigned long size)
-{
-	struct cpumask hmask;
-
-	riscv_cpuid_to_hartid_mask(cmask, &hmask);
-	sbi_remote_sfence_vma(hmask.bits, start, size);
-}
-
-#define flush_tlb_all() sbi_remote_sfence_vma(NULL, 0, -1)
-
-#define flush_tlb_range(vma, start, end) \
-	remote_sfence_vma(mm_cpumask((vma)->vm_mm), start, (end) - (start))
-
-static inline void flush_tlb_page(struct vm_area_struct *vma,
-				  unsigned long addr)
-{
-	flush_tlb_range(vma, addr, addr + PAGE_SIZE);
-}
-
-#define flush_tlb_mm(mm)				\
-	remote_sfence_vma(mm_cpumask(mm), 0, -1)
-
 #endif /* CONFIG_SMP */
 
 /* Flush a range of kernel pages */

commit f5bf645d10f2c6cc85294021af70f2b7bcc42d8e
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Aug 21 23:58:35 2019 +0900

    riscv: cleanup riscv_cpuid_to_hartid_mask
    
    Move the initial clearing of the mask from the callers to
    riscv_cpuid_to_hartid_mask, and remove the unused !CONFIG_SMP stub.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Atish Patra <atish.patra@wdc.com>
    Signed-off-by: Paul Walmsley <paul.walmsley@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 4d9bbe8438bf..df31fe2ed09c 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -47,7 +47,6 @@ static inline void remote_sfence_vma(struct cpumask *cmask, unsigned long start,
 {
 	struct cpumask hmask;
 
-	cpumask_clear(&hmask);
 	riscv_cpuid_to_hartid_mask(cmask, &hmask);
 	sbi_remote_sfence_vma(hmask.bits, start, size);
 }

commit eb93685847a9055283d05951c1b205e737f38533
Author: Paul Walmsley <paul.walmsley@sifive.com>
Date:   Wed Aug 7 19:07:34 2019 -0700

    riscv: fix flush_tlb_range() end address for flush_tlb_page()
    
    The RISC-V kernel implementation of flush_tlb_page() when CONFIG_SMP
    is set is wrong.  It passes zero to flush_tlb_range() as the final
    address to flush, but it should be at least 'addr'.
    
    Some other Linux architecture ports use the beginning address to
    flush, plus PAGE_SIZE, as the final address to flush.  This might
    flush slightly more than what's needed, but it seems unlikely that
    being more clever would improve anything.  So let's just take that
    implementation for now.
    
    While here, convert the macro into a static inline function, primarily
    to avoid unintentional multiple evaluations of 'addr'.
    
    This second version of the patch fixes a coding style issue found by
    Christoph Hellwig <hch@lst.de>.
    
    Reported-by: Andreas Schwab <schwab@suse.de>
    Signed-off-by: Paul Walmsley <paul.walmsley@sifive.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 687dd19735a7..4d9bbe8438bf 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -53,10 +53,17 @@ static inline void remote_sfence_vma(struct cpumask *cmask, unsigned long start,
 }
 
 #define flush_tlb_all() sbi_remote_sfence_vma(NULL, 0, -1)
-#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)
+
 #define flush_tlb_range(vma, start, end) \
 	remote_sfence_vma(mm_cpumask((vma)->vm_mm), start, (end) - (start))
-#define flush_tlb_mm(mm) \
+
+static inline void flush_tlb_page(struct vm_area_struct *vma,
+				  unsigned long addr)
+{
+	flush_tlb_range(vma, addr, addr + PAGE_SIZE);
+}
+
+#define flush_tlb_mm(mm)				\
 	remote_sfence_vma(mm_cpumask(mm), 0, -1)
 
 #endif /* CONFIG_SMP */

commit 50acfb2b76e19f73270fef9a32726c7e18d08ec3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed May 29 07:18:00 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 286
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation version 2 this program is distributed
      in the hope that it will be useful but without any warranty without
      even the implied warranty of merchantability or fitness for a
      particular purpose see the gnu general public license for more
      details
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 97 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190529141901.025053186@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 54fee0cadb1e..687dd19735a7 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -1,15 +1,7 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
 /*
  * Copyright (C) 2009 Chen Liqin <liqin.chen@sunplusct.com>
  * Copyright (C) 2012 Regents of the University of California
- *
- *   This program is free software; you can redistribute it and/or
- *   modify it under the terms of the GNU General Public License
- *   as published by the Free Software Foundation, version 2.
- *
- *   This program is distributed in the hope that it will be useful,
- *   but WITHOUT ANY WARRANTY; without even the implied warranty of
- *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- *   GNU General Public License for more details.
  */
 
 #ifndef _ASM_RISCV_TLBFLUSH_H

commit f99fb607fb2bc0d4ce6b9adb764c65e37f40a92b
Author: Atish Patra <atish.patra@wdc.com>
Date:   Tue Oct 2 12:15:05 2018 -0700

    RISC-V: Use Linux logical CPU number instead of hartid
    
    Setup the cpu_logical_map during boot. Moreover, every SBI call
    and PLIC context are based on the physical hartid. Use the logical
    CPU to hartid mapping to pass correct hartid to respective functions.
    
    Signed-off-by: Atish Patra <atish.patra@wdc.com>
    Reviewed-by: Anup Patel <anup@brainfault.org>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 85c2d8bae957..54fee0cadb1e 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -16,6 +16,7 @@
 #define _ASM_RISCV_TLBFLUSH_H
 
 #include <linux/mm_types.h>
+#include <asm/smp.h>
 
 /*
  * Flush entire local TLB.  'sfence.vma' implicitly fences with the instruction
@@ -49,13 +50,22 @@ static inline void flush_tlb_range(struct vm_area_struct *vma,
 
 #include <asm/sbi.h>
 
+static inline void remote_sfence_vma(struct cpumask *cmask, unsigned long start,
+				     unsigned long size)
+{
+	struct cpumask hmask;
+
+	cpumask_clear(&hmask);
+	riscv_cpuid_to_hartid_mask(cmask, &hmask);
+	sbi_remote_sfence_vma(hmask.bits, start, size);
+}
+
 #define flush_tlb_all() sbi_remote_sfence_vma(NULL, 0, -1)
 #define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)
 #define flush_tlb_range(vma, start, end) \
-	sbi_remote_sfence_vma(mm_cpumask((vma)->vm_mm)->bits, \
-			      start, (end) - (start))
+	remote_sfence_vma(mm_cpumask((vma)->vm_mm), start, (end) - (start))
 #define flush_tlb_mm(mm) \
-	sbi_remote_sfence_vma(mm_cpumask(mm)->bits, 0, -1)
+	remote_sfence_vma(mm_cpumask(mm), 0, -1)
 
 #endif /* CONFIG_SMP */
 

commit 2861ae302f6bf7221db2dac5bd4cf0f2e4cab13b
Author: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
Date:   Fri Jun 1 17:21:21 2018 +0200

    riscv: use NULL instead of a plain 0
    
    sbi_remote_sfence_vma() & sbi_remote_fence_i() takes
    a pointer as first argument but some macros call them with
    a plain 0 which, while legal C, is frowned upon in the kernel.
    
    Change this by replacing the 0 by NULL.
    
    Signed-off-by: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 7b209aec355d..85c2d8bae957 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -49,7 +49,7 @@ static inline void flush_tlb_range(struct vm_area_struct *vma,
 
 #include <asm/sbi.h>
 
-#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)
+#define flush_tlb_all() sbi_remote_sfence_vma(NULL, 0, -1)
 #define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)
 #define flush_tlb_range(vma, start, end) \
 	sbi_remote_sfence_vma(mm_cpumask((vma)->vm_mm)->bits, \

commit f1b65f20fb05d1dd94656904848b96cc6df52bc0
Author: Andrew Waterman <andrew@sifive.com>
Date:   Thu Oct 26 01:53:40 2017 -0700

    RISC-V: Limit the scope of TLB shootdowns
    
    RISC-V systems perform TLB shootdows via the SBI, which currently
    performs an IPI to each of the remote harts which then performs a local
    TLB flush.  This process is a bit on the slow side, but we can at least
    speed it up for some common cases by restricting the set of harts to
    shoot down to the actual set of harts that are currently participating
    in the given mm context, as opposed to the entire system.
    
    This should provide a measurable performance increase, but we haven't
    measured it.  Regardless, it seems like obviously the right thing to do
    here.
    
    Signed-off-by: Andrew Waterman <andrew@sifive.com>
    Signed-off-by: Palmer Dabbelt <palmer@dabbelt.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 7b9c24ebdf52..7b209aec355d 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -36,7 +36,14 @@ static inline void local_flush_tlb_page(unsigned long addr)
 
 #define flush_tlb_all() local_flush_tlb_all()
 #define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)
-#define flush_tlb_range(vma, start, end) local_flush_tlb_all()
+
+static inline void flush_tlb_range(struct vm_area_struct *vma,
+		unsigned long start, unsigned long end)
+{
+	local_flush_tlb_all();
+}
+
+#define flush_tlb_mm(mm) flush_tlb_all()
 
 #else /* CONFIG_SMP */
 
@@ -45,16 +52,13 @@ static inline void local_flush_tlb_page(unsigned long addr)
 #define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)
 #define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)
 #define flush_tlb_range(vma, start, end) \
-	sbi_remote_sfence_vma(0, start, (end) - (start))
+	sbi_remote_sfence_vma(mm_cpumask((vma)->vm_mm)->bits, \
+			      start, (end) - (start))
+#define flush_tlb_mm(mm) \
+	sbi_remote_sfence_vma(mm_cpumask(mm)->bits, 0, -1)
 
 #endif /* CONFIG_SMP */
 
-/* Flush the TLB entries of the specified mm context */
-static inline void flush_tlb_mm(struct mm_struct *mm)
-{
-	flush_tlb_all();
-}
-
 /* Flush a range of kernel pages */
 static inline void flush_tlb_kernel_range(unsigned long start,
 	unsigned long end)

commit c163fb38ca34694b0cce99bb5604257bc29bf200
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Jan 4 18:35:02 2018 +0100

    riscv: remove CONFIG_MMU ifdefs
    
    The RISC-V port doesn't suport a nommu mode, so there is no reason
    to provide some code only under a CONFIG_MMU ifdef.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 715b0f10af58..7b9c24ebdf52 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -15,8 +15,6 @@
 #ifndef _ASM_RISCV_TLBFLUSH_H
 #define _ASM_RISCV_TLBFLUSH_H
 
-#ifdef CONFIG_MMU
-
 #include <linux/mm_types.h>
 
 /*
@@ -64,6 +62,4 @@ static inline void flush_tlb_kernel_range(unsigned long start,
 	flush_tlb_all();
 }
 
-#endif /* CONFIG_MMU */
-
 #endif /* _ASM_RISCV_TLBFLUSH_H */

commit 07f8ba7439f9c942d5bd7b63074e7a1528601713
Merge: f8182f613c98 0e710ac6521f
Author: Palmer Dabbelt <palmer@sifive.com>
Date:   Fri Dec 1 13:12:10 2017 -0800

    RISC-V: User-Visible Changes
    
    This merge contains the user-visible, ABI-breaking changes that we want
    to make sure we have in Linux before our first release.   Highlights
    include:
    
    * VDSO entries for clock_get/gettimeofday/getcpu have been added.  These
      are simple syscalls now, but we want to let glibc use them from the
      start so we can make them faster later.
    * A VDSO entry for instruction cache flushing has been added so
      userspace can flush the instruction cache.
    * The VDSO symbol versions for __vdso_cmpxchg{32,64} have been removed,
      as those VDSO entries don't actually exist.
    
    Conflicts:
            arch/riscv/include/asm/tlbflush.h

commit 08f051eda33b51e8ee0f45f05bcfe49d0f0caf6b
Author: Andrew Waterman <andrew@sifive.com>
Date:   Wed Oct 25 14:30:32 2017 -0700

    RISC-V: Flush I$ when making a dirty page executable
    
    The RISC-V ISA allows for instruction caches that are not coherent WRT
    stores, even on a single hart.  As a result, we need to explicitly flush
    the instruction cache whenever marking a dirty page as executable in
    order to preserve the correct system behavior.
    
    Local instruction caches aren't that scary (our implementations actually
    flush the cache, but RISC-V is defined to allow higher-performance
    implementations to exist), but RISC-V defines no way to perform an
    instruction cache shootdown.  When explicitly asked to do so we can
    shoot down remote instruction caches via an IPI, but this is a bit on
    the slow side.
    
    Instead of requiring an IPI to all harts whenever marking a page as
    executable, we simply flush the currently running harts.  In order to
    maintain correct behavior, we additionally mark every other hart as
    needing a deferred instruction cache which will be taken before anything
    runs on it.
    
    Signed-off-by: Andrew Waterman <andrew@sifive.com>
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 5ee4ae370b5e..77edf2826c1f 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -17,6 +17,8 @@
 
 #ifdef CONFIG_MMU
 
+#include <linux/mm_types.h>
+
 /* Flush entire local TLB */
 static inline void local_flush_tlb_all(void)
 {

commit c901e45a999a1935d7adf653e1cf12dfbcd737aa
Author: Palmer Dabbelt <palmer@sifive.com>
Date:   Tue Nov 28 14:06:17 2017 -0800

    RISC-V: `sfence.vma` orderes the instruction cache
    
    This is just a comment change, but it's one that bit me on the mailing
    list.  It turns out that issuing a `sfence.vma` enforces instruction
    cache ordering in addition to TLB ordering.  This isn't explicitly
    called out in the ISA manual, but Andrew will be making that more clear
    in a future revision.
    
    CC: Andrew Waterman <andrew@sifive.com>
    Signed-off-by: Palmer Dabbelt <palmer@sifive.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
index 5ee4ae370b5e..c79fab3d377d 100644
--- a/arch/riscv/include/asm/tlbflush.h
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -17,7 +17,10 @@
 
 #ifdef CONFIG_MMU
 
-/* Flush entire local TLB */
+/*
+ * Flush entire local TLB.  'sfence.vma' implicitly fences with the instruction
+ * cache as well, so a 'fence.i' is not necessary.
+ */
 static inline void local_flush_tlb_all(void)
 {
 	__asm__ __volatile__ ("sfence.vma" : : : "memory");

commit fab957c11efe2f405e08b9f0d080524bc2631428
Author: Palmer Dabbelt <palmer@dabbelt.com>
Date:   Mon Jul 10 18:02:19 2017 -0700

    RISC-V: Atomic and Locking Code
    
    This contains all the code that directly interfaces with the RISC-V
    memory model.  While this code corforms to the current RISC-V ISA
    specifications (user 2.2 and priv 1.10), the memory model is somewhat
    underspecified in those documents.  There is a working group that hopes
    to produce a formal memory model by the end of the year, but my
    understanding is that the basic definitions we're relying on here won't
    change significantly.
    
    Reviewed-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Palmer Dabbelt <palmer@dabbelt.com>

diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h
new file mode 100644
index 000000000000..5ee4ae370b5e
--- /dev/null
+++ b/arch/riscv/include/asm/tlbflush.h
@@ -0,0 +1,64 @@
+/*
+ * Copyright (C) 2009 Chen Liqin <liqin.chen@sunplusct.com>
+ * Copyright (C) 2012 Regents of the University of California
+ *
+ *   This program is free software; you can redistribute it and/or
+ *   modify it under the terms of the GNU General Public License
+ *   as published by the Free Software Foundation, version 2.
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ */
+
+#ifndef _ASM_RISCV_TLBFLUSH_H
+#define _ASM_RISCV_TLBFLUSH_H
+
+#ifdef CONFIG_MMU
+
+/* Flush entire local TLB */
+static inline void local_flush_tlb_all(void)
+{
+	__asm__ __volatile__ ("sfence.vma" : : : "memory");
+}
+
+/* Flush one page from local TLB */
+static inline void local_flush_tlb_page(unsigned long addr)
+{
+	__asm__ __volatile__ ("sfence.vma %0" : : "r" (addr) : "memory");
+}
+
+#ifndef CONFIG_SMP
+
+#define flush_tlb_all() local_flush_tlb_all()
+#define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)
+#define flush_tlb_range(vma, start, end) local_flush_tlb_all()
+
+#else /* CONFIG_SMP */
+
+#include <asm/sbi.h>
+
+#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)
+#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)
+#define flush_tlb_range(vma, start, end) \
+	sbi_remote_sfence_vma(0, start, (end) - (start))
+
+#endif /* CONFIG_SMP */
+
+/* Flush the TLB entries of the specified mm context */
+static inline void flush_tlb_mm(struct mm_struct *mm)
+{
+	flush_tlb_all();
+}
+
+/* Flush a range of kernel pages */
+static inline void flush_tlb_kernel_range(unsigned long start,
+	unsigned long end)
+{
+	flush_tlb_all();
+}
+
+#endif /* CONFIG_MMU */
+
+#endif /* _ASM_RISCV_TLBFLUSH_H */
