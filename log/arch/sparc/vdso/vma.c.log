commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/sparc/vdso/vma.c b/arch/sparc/vdso/vma.c
index 9961b0f81693..cc19e09b0fa1 100644
--- a/arch/sparc/vdso/vma.c
+++ b/arch/sparc/vdso/vma.c
@@ -366,7 +366,7 @@ static int map_vdso(const struct vdso_image *image,
 	unsigned long text_start, addr = 0;
 	int ret = 0;
 
-	down_write(&mm->mmap_sem);
+	mmap_write_lock(mm);
 
 	/*
 	 * First, get an unmapped region: then randomize it, and make sure that
@@ -422,7 +422,7 @@ static int map_vdso(const struct vdso_image *image,
 	if (ret)
 		current->mm->context.vdso = NULL;
 
-	up_write(&mm->mmap_sem);
+	mmap_write_unlock(mm);
 	return ret;
 }
 

commit 003ba957919b1634b437615c86d68459e8450c10
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue May 28 10:10:17 2019 -0700

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 214
    
    Based on 1 normalized pattern(s):
    
      subject to the gpl v 2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 2 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Reviewed-by: Steve Winslow <swinslow@gmail.com>
    Reviewed-by: Alexios Zavras <alexios.zavras@intel.com>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190528171439.372657724@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/sparc/vdso/vma.c b/arch/sparc/vdso/vma.c
index 154fe8adc090..9961b0f81693 100644
--- a/arch/sparc/vdso/vma.c
+++ b/arch/sparc/vdso/vma.c
@@ -1,7 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Set up the VMAs to tell the VM about the vDSO.
  * Copyright 2007 Andi Kleen, SUSE Labs.
- * Subject to the GPL, v.2
  */
 
 /*

commit caf539cd1087f7c36b9c4df271575e9aee49fde5
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Oct 25 10:36:19 2018 -0700

    sparc: Fix VDSO build with older binutils.
    
    Older versions of bintutils do not allow symbol math across different
    segments on sparc:
    
    ====================
    Assembler messages:
    99: Error: operation combines symbols in different segments
    ====================
    
    This is controlled by whether or not DIFF_EXPR_OK is defined in
    gas/config/tc-*.h and for sparc this was not the case until mid-2017.
    
    So we have to patch between %stick and %tick another way.
    
    Do what powerpc does and emit two versions of the relevant functions,
    one using %tick and one using %stick, and patch the symbols in the
    dynamic symbol table.
    
    Fixes: 2f6c9bf31a0b ("sparc: Improve VDSO instruction patching.")
    Reported-by: Meelis Roos <mroos@linux.ee>
    Tested-by: Meelis Roos <mroos@linux.ee>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/sparc/vdso/vma.c b/arch/sparc/vdso/vma.c
index 8874a27d8adc..154fe8adc090 100644
--- a/arch/sparc/vdso/vma.c
+++ b/arch/sparc/vdso/vma.c
@@ -42,24 +42,201 @@ static struct vm_special_mapping vdso_mapping32 = {
 
 struct vvar_data *vvar_data;
 
-struct tick_patch_entry {
-	s32 orig, repl;
+struct vdso_elfinfo32 {
+	Elf32_Ehdr	*hdr;
+	Elf32_Sym	*dynsym;
+	unsigned long	dynsymsize;
+	const char	*dynstr;
+	unsigned long	text;
 };
 
-static void stick_patch(const struct vdso_image *image)
+struct vdso_elfinfo64 {
+	Elf64_Ehdr	*hdr;
+	Elf64_Sym	*dynsym;
+	unsigned long	dynsymsize;
+	const char	*dynstr;
+	unsigned long	text;
+};
+
+struct vdso_elfinfo {
+	union {
+		struct vdso_elfinfo32 elf32;
+		struct vdso_elfinfo64 elf64;
+	} u;
+};
+
+static void *one_section64(struct vdso_elfinfo64 *e, const char *name,
+			   unsigned long *size)
+{
+	const char *snames;
+	Elf64_Shdr *shdrs;
+	unsigned int i;
+
+	shdrs = (void *)e->hdr + e->hdr->e_shoff;
+	snames = (void *)e->hdr + shdrs[e->hdr->e_shstrndx].sh_offset;
+	for (i = 1; i < e->hdr->e_shnum; i++) {
+		if (!strcmp(snames+shdrs[i].sh_name, name)) {
+			if (size)
+				*size = shdrs[i].sh_size;
+			return (void *)e->hdr + shdrs[i].sh_offset;
+		}
+	}
+	return NULL;
+}
+
+static int find_sections64(const struct vdso_image *image, struct vdso_elfinfo *_e)
+{
+	struct vdso_elfinfo64 *e = &_e->u.elf64;
+
+	e->hdr = image->data;
+	e->dynsym = one_section64(e, ".dynsym", &e->dynsymsize);
+	e->dynstr = one_section64(e, ".dynstr", NULL);
+
+	if (!e->dynsym || !e->dynstr) {
+		pr_err("VDSO64: Missing symbol sections.\n");
+		return -ENODEV;
+	}
+	return 0;
+}
+
+static Elf64_Sym *find_sym64(const struct vdso_elfinfo64 *e, const char *name)
+{
+	unsigned int i;
+
+	for (i = 0; i < (e->dynsymsize / sizeof(Elf64_Sym)); i++) {
+		Elf64_Sym *s = &e->dynsym[i];
+		if (s->st_name == 0)
+			continue;
+		if (!strcmp(e->dynstr + s->st_name, name))
+			return s;
+	}
+	return NULL;
+}
+
+static int patchsym64(struct vdso_elfinfo *_e, const char *orig,
+		      const char *new)
+{
+	struct vdso_elfinfo64 *e = &_e->u.elf64;
+	Elf64_Sym *osym = find_sym64(e, orig);
+	Elf64_Sym *nsym = find_sym64(e, new);
+
+	if (!nsym || !osym) {
+		pr_err("VDSO64: Missing symbols.\n");
+		return -ENODEV;
+	}
+	osym->st_value = nsym->st_value;
+	osym->st_size = nsym->st_size;
+	osym->st_info = nsym->st_info;
+	osym->st_other = nsym->st_other;
+	osym->st_shndx = nsym->st_shndx;
+
+	return 0;
+}
+
+static void *one_section32(struct vdso_elfinfo32 *e, const char *name,
+			   unsigned long *size)
+{
+	const char *snames;
+	Elf32_Shdr *shdrs;
+	unsigned int i;
+
+	shdrs = (void *)e->hdr + e->hdr->e_shoff;
+	snames = (void *)e->hdr + shdrs[e->hdr->e_shstrndx].sh_offset;
+	for (i = 1; i < e->hdr->e_shnum; i++) {
+		if (!strcmp(snames+shdrs[i].sh_name, name)) {
+			if (size)
+				*size = shdrs[i].sh_size;
+			return (void *)e->hdr + shdrs[i].sh_offset;
+		}
+	}
+	return NULL;
+}
+
+static int find_sections32(const struct vdso_image *image, struct vdso_elfinfo *_e)
+{
+	struct vdso_elfinfo32 *e = &_e->u.elf32;
+
+	e->hdr = image->data;
+	e->dynsym = one_section32(e, ".dynsym", &e->dynsymsize);
+	e->dynstr = one_section32(e, ".dynstr", NULL);
+
+	if (!e->dynsym || !e->dynstr) {
+		pr_err("VDSO32: Missing symbol sections.\n");
+		return -ENODEV;
+	}
+	return 0;
+}
+
+static Elf32_Sym *find_sym32(const struct vdso_elfinfo32 *e, const char *name)
 {
-	struct tick_patch_entry *p, *p_end;
+	unsigned int i;
+
+	for (i = 0; i < (e->dynsymsize / sizeof(Elf32_Sym)); i++) {
+		Elf32_Sym *s = &e->dynsym[i];
+		if (s->st_name == 0)
+			continue;
+		if (!strcmp(e->dynstr + s->st_name, name))
+			return s;
+	}
+	return NULL;
+}
 
-	p = image->data + image->tick_patch;
-	p_end = (void *)p + image->tick_patch_len;
-	while (p < p_end) {
-		u32 *instr = (void *)&p->orig + p->orig;
-		u32 *repl = (void *)&p->repl + p->repl;
+static int patchsym32(struct vdso_elfinfo *_e, const char *orig,
+		      const char *new)
+{
+	struct vdso_elfinfo32 *e = &_e->u.elf32;
+	Elf32_Sym *osym = find_sym32(e, orig);
+	Elf32_Sym *nsym = find_sym32(e, new);
 
-		*instr = *repl;
-		flushi(instr);
-		p++;
+	if (!nsym || !osym) {
+		pr_err("VDSO32: Missing symbols.\n");
+		return -ENODEV;
 	}
+	osym->st_value = nsym->st_value;
+	osym->st_size = nsym->st_size;
+	osym->st_info = nsym->st_info;
+	osym->st_other = nsym->st_other;
+	osym->st_shndx = nsym->st_shndx;
+
+	return 0;
+}
+
+static int find_sections(const struct vdso_image *image, struct vdso_elfinfo *e,
+			 bool elf64)
+{
+	if (elf64)
+		return find_sections64(image, e);
+	else
+		return find_sections32(image, e);
+}
+
+static int patch_one_symbol(struct vdso_elfinfo *e, const char *orig,
+			    const char *new_target, bool elf64)
+{
+	if (elf64)
+		return patchsym64(e, orig, new_target);
+	else
+		return patchsym32(e, orig, new_target);
+}
+
+static int stick_patch(const struct vdso_image *image, struct vdso_elfinfo *e, bool elf64)
+{
+	int err;
+
+	err = find_sections(image, e, elf64);
+	if (err)
+		return err;
+
+	err = patch_one_symbol(e,
+			       "__vdso_gettimeofday",
+			       "__vdso_gettimeofday_stick", elf64);
+	if (err)
+		return err;
+
+	return patch_one_symbol(e,
+				"__vdso_clock_gettime",
+				"__vdso_clock_gettime_stick", elf64);
+	return 0;
 }
 
 /*
@@ -67,13 +244,19 @@ static void stick_patch(const struct vdso_image *image)
  * kernel image.
  */
 int __init init_vdso_image(const struct vdso_image *image,
-		struct vm_special_mapping *vdso_mapping)
+			   struct vm_special_mapping *vdso_mapping, bool elf64)
 {
-	int i;
+	int cnpages = (image->size) / PAGE_SIZE;
 	struct page *dp, **dpp = NULL;
-	int dnpages = 0;
 	struct page *cp, **cpp = NULL;
-	int cnpages = (image->size) / PAGE_SIZE;
+	struct vdso_elfinfo ei;
+	int i, dnpages = 0;
+
+	if (tlb_type != spitfire) {
+		int err = stick_patch(image, &ei, elf64);
+		if (err)
+			return err;
+	}
 
 	/*
 	 * First, the vdso text.  This is initialied data, an integral number of
@@ -88,9 +271,6 @@ int __init init_vdso_image(const struct vdso_image *image,
 	if (!cpp)
 		goto oom;
 
-	if (tlb_type != spitfire)
-		stick_patch(image);
-
 	for (i = 0; i < cnpages; i++) {
 		cp = alloc_page(GFP_KERNEL);
 		if (!cp)
@@ -153,13 +333,13 @@ static int __init init_vdso(void)
 {
 	int err = 0;
 #ifdef CONFIG_SPARC64
-	err = init_vdso_image(&vdso_image_64_builtin, &vdso_mapping64);
+	err = init_vdso_image(&vdso_image_64_builtin, &vdso_mapping64, true);
 	if (err)
 		return err;
 #endif
 
 #ifdef CONFIG_COMPAT
-	err = init_vdso_image(&vdso_image_32_builtin, &vdso_mapping32);
+	err = init_vdso_image(&vdso_image_32_builtin, &vdso_mapping32, false);
 #endif
 	return err;
 

commit 2f6c9bf31a0b16aeccb42b73f8d0ddf9bea88f3f
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Oct 21 21:44:33 2018 -0700

    sparc: Improve VDSO instruction patching.
    
    The current VDSO patch mechanism has several problems:
    
    1) It assumes how gcc will emit a function, with a register
       window, an initial save instruction and then immediately
       the %tick read when compiling vread_tick().
    
       There is no such guarantees, code generation could change
       at any time, gcc could put a nop between the save and
       the %tick read, etc.
    
       So this is extremely fragile and would fail some day.
    
    2) It disallows us to properly inline vread_tick() into the callers
       and thus get the best possible code sequences.
    
    So fix this to patch properly, with location based annotations.
    
    We have to be careful because we cannot do it the way we do
    patches elsewhere in the kernel.  Those use a sequence like:
    
            1:
            insn
            .section        .whatever_patch, "ax"
            .word           1b
            replacement_insn
            .previous
    
    This is a dynamic shared object, so that .word cannot be resolved at
    build time, and thus cannot be used to execute the patches when the
    kernel initializes the images.
    
    Even trying to use label difference equations doesn't work in the
    above kind of scheme:
    
            1:
            insn
            .section        .whatever_patch, "ax"
            .word           . - 1b
            replacement_insn
            .previous
    
    The assembler complains that it cannot resolve that computation.
    The issue is that this is contained in an executable section.
    
    Borrow the sequence used by x86 alternatives, which is:
    
            1:
            insn
            .pushsection    .whatever_patch, "a"
            .word           . - 1b, . - 1f
            .popsection
            .pushsection    .whatever_patch_replacements, "ax"
            1:
            replacement_insn
            .previous
    
    This works, allows us to inline vread_tick() as much as we like, and
    can be used for arbitrary kinds of VDSO patching in the future.
    
    Also, reverse the condition for patching.  Most systems are %stick
    based, so if we only patch on %tick systems the patching code will
    get little or no testing.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/sparc/vdso/vma.c b/arch/sparc/vdso/vma.c
index 5eaff3c1aa0c..8874a27d8adc 100644
--- a/arch/sparc/vdso/vma.c
+++ b/arch/sparc/vdso/vma.c
@@ -16,6 +16,8 @@
 #include <linux/linkage.h>
 #include <linux/random.h>
 #include <linux/elf.h>
+#include <asm/cacheflush.h>
+#include <asm/spitfire.h>
 #include <asm/vdso.h>
 #include <asm/vvar.h>
 #include <asm/page.h>
@@ -40,7 +42,25 @@ static struct vm_special_mapping vdso_mapping32 = {
 
 struct vvar_data *vvar_data;
 
-#define	SAVE_INSTR_SIZE	4
+struct tick_patch_entry {
+	s32 orig, repl;
+};
+
+static void stick_patch(const struct vdso_image *image)
+{
+	struct tick_patch_entry *p, *p_end;
+
+	p = image->data + image->tick_patch;
+	p_end = (void *)p + image->tick_patch_len;
+	while (p < p_end) {
+		u32 *instr = (void *)&p->orig + p->orig;
+		u32 *repl = (void *)&p->repl + p->repl;
+
+		*instr = *repl;
+		flushi(instr);
+		p++;
+	}
+}
 
 /*
  * Allocate pages for the vdso and vvar, and copy in the vdso text from the
@@ -68,21 +88,8 @@ int __init init_vdso_image(const struct vdso_image *image,
 	if (!cpp)
 		goto oom;
 
-	if (vdso_fix_stick) {
-		/*
-		 * If the system uses %tick instead of %stick, patch the VDSO
-		 * with instruction reading %tick instead of %stick.
-		 */
-		unsigned int j, k = SAVE_INSTR_SIZE;
-		unsigned char *data = image->data;
-
-		for (j = image->sym_vread_tick_patch_start;
-		     j < image->sym_vread_tick_patch_end; j++) {
-
-			data[image->sym_vread_tick + k] = data[j];
-			k++;
-		}
-	}
+	if (tlb_type != spitfire)
+		stick_patch(image);
 
 	for (i = 0; i < cnpages; i++) {
 		cp = alloc_page(GFP_KERNEL);

commit 62d6f3b7b85e9ba79c96bfd36f501f180908e623
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Sat Oct 13 13:26:53 2018 +0300

    sparc: vDSO: Silence an uninitialized variable warning
    
    Smatch complains that "val" would be uninitialized if kstrtoul() fails.
    
    Fixes: 9a08862a5d2e ("vDSO for sparc")
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/sparc/vdso/vma.c b/arch/sparc/vdso/vma.c
index f51595f861b8..5eaff3c1aa0c 100644
--- a/arch/sparc/vdso/vma.c
+++ b/arch/sparc/vdso/vma.c
@@ -262,7 +262,9 @@ static __init int vdso_setup(char *s)
 	unsigned long val;
 
 	err = kstrtoul(s, 10, &val);
+	if (err)
+		return err;
 	vdso_enabled = val;
-	return err;
+	return 0;
 }
 __setup("vdso=", vdso_setup);

commit b3ca676f681409301332a75049e3148b7cb86141
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Fri Jan 19 16:23:21 2018 +0300

    sparc: vDSO: remove an extra tab
    
    This statement is indented one tab too far which is confusing and
    leads to a Smatch warning:
    
        arch/sparc/vdso/vma.c:254 arch_setup_additional_pages()
        warn: curly braces intended?
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Reviewed-by: Nagarathnam Muthusamy <nagarathnam.muthusamy@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/sparc/vdso/vma.c b/arch/sparc/vdso/vma.c
index 0a6f50098e23..f51595f861b8 100644
--- a/arch/sparc/vdso/vma.c
+++ b/arch/sparc/vdso/vma.c
@@ -251,7 +251,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
 	else
 		return map_vdso(&vdso_image_32_builtin, &vdso_mapping32);
 #else
-		return map_vdso(&vdso_image_64_builtin, &vdso_mapping64);
+	return map_vdso(&vdso_image_64_builtin, &vdso_mapping64);
 #endif
 
 }

commit 9a08862a5d2e266ecea1865547463da2745fc687
Author: Nagarathnam Muthusamy <nagarathnam.muthusamy@oracle.com>
Date:   Thu Sep 21 11:05:31 2017 -0400

    vDSO for sparc
    
    Following patch is based on work done by Nick Alcock on 64-bit vDSO for sparc
    in Oracle linux. I have extended it to include support for 32-bit vDSO for sparc
    on 64-bit kernel.
    
    vDSO for sparc is based on the X86 implementation. This patch
    provides vDSO support for both 64-bit and 32-bit programs on 64-bit kernel.
    vDSO will be disabled on 32-bit linux kernel on sparc.
    
    *) vclock_gettime.c contains all the vdso functions. Since data page is mapped
       before the vdso code page, the pointer to data page is got by subracting offset
       from an address in the vdso code page. The return address stored in
       %i7 is used for this purpose.
    *) During compilation, both 32-bit and 64-bit vdso images are compiled and are
       converted into raw bytes by vdso2c program to be ready for mapping into the
       process. 32-bit images are compiled only if CONFIG_COMPAT is enabled. vdso2c
       generates two files vdso-image-64.c and vdso-image-32.c which contains the
       respective vDSO image in C structure.
    *) During vdso initialization, required number of vdso pages are allocated and
       raw bytes are copied into the pages.
    *) During every exec, these pages are mapped into the process through
       arch_setup_additional_pages and the location of mapping is passed on to the
       process through aux vector AT_SYSINFO_EHDR which is used by glibc.
    *) A new update_vsyscall routine for sparc is added to keep the data page in
       vdso updated.
    *) As vDSO cannot contain dynamically relocatable references, a new version of
       cpu_relax is added for the use of vDSO.
    
    This change also requires a putback to glibc to use vDSO. For testing,
    programs planning to try vDSO can be compiled against the generated
    vdso(64/32).so in the source.
    
    Testing:
    
    ========
    [root@localhost ~]# cat vdso_test.c
    int main() {
            struct timespec tv_start, tv_end;
            struct timeval tv_tmp;
            int i;
            int count = 1 * 1000 * 10000;
            long long diff;
    
            clock_gettime(0, &tv_start);
            for (i = 0; i < count; i++)
                  gettimeofday(&tv_tmp, NULL);
            clock_gettime(0, &tv_end);
            diff = (long long)(tv_end.tv_sec -
                    tv_start.tv_sec)*(1*1000*1000*1000);
            diff += (tv_end.tv_nsec - tv_start.tv_nsec);
            printf("Start sec: %d\n", tv_start.tv_sec);
            printf("End sec  : %d\n", tv_end.tv_sec);
            printf("%d cycles in %lld ns = %f ns/cycle\n", count, diff,
                    (double)diff / (double)count);
            return 0;
    }
    
    [root@localhost ~]# cc vdso_test.c -o t32_without_fix -m32 -lrt
    [root@localhost ~]# ./t32_without_fix
    Start sec: 1502396130
    End sec  : 1502396140
    10000000 cycles in 9565148528 ns = 956.514853 ns/cycle
    [root@localhost ~]# cc vdso_test.c -o t32_with_fix -m32 ./vdso32.so.dbg
    [root@localhost ~]# ./t32_with_fix
    Start sec: 1502396168
    End sec  : 1502396169
    10000000 cycles in 798141262 ns = 79.814126 ns/cycle
    [root@localhost ~]# cc vdso_test.c -o t64_without_fix -m64 -lrt
    [root@localhost ~]# ./t64_without_fix
    Start sec: 1502396208
    End sec  : 1502396218
    10000000 cycles in 9846091800 ns = 984.609180 ns/cycle
    [root@localhost ~]# cc vdso_test.c -o t64_with_fix -m64 ./vdso64.so.dbg
    [root@localhost ~]# ./t64_with_fix
    Start sec: 1502396257
    End sec  : 1502396257
    10000000 cycles in 380984048 ns = 38.098405 ns/cycle
    
    V1 to V2 Changes:
    =================
            Added hot patching code to switch the read stick instruction to read
    tick instruction based on the hardware.
    
    V2 to V3 Changes:
    =================
            Merged latest changes from sparc-next and moved the initialization
    of clocksource_tick.archdata.vclock_mode to time_init_early. Disabled
    queued spinlock and rwlock configuration when simulating 32-bit config
    to compile 32-bit VDSO.
    
    V3 to V4 Changes:
    =================
            Hardcoded the page size as 8192 in linker script for both 64-bit and
    32-bit binaries. Removed unused variables in vdso2c.h. Added -mv8plus flag to
    Makefile to prevent the generation of relocation entries for __lshrdi3 in 32-bit
    vdso binary.
    
    Signed-off-by: Nick Alcock <nick.alcock@oracle.com>
    Signed-off-by: Nagarathnam Muthusamy <nagarathnam.muthusamy@oracle.com>
    Reviewed-by: Shannon Nelson <shannon.nelson@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/arch/sparc/vdso/vma.c b/arch/sparc/vdso/vma.c
new file mode 100644
index 000000000000..0a6f50098e23
--- /dev/null
+++ b/arch/sparc/vdso/vma.c
@@ -0,0 +1,268 @@
+/*
+ * Set up the VMAs to tell the VM about the vDSO.
+ * Copyright 2007 Andi Kleen, SUSE Labs.
+ * Subject to the GPL, v.2
+ */
+
+/*
+ * Copyright (c) 2017 Oracle and/or its affiliates. All rights reserved.
+ */
+
+#include <linux/mm.h>
+#include <linux/err.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/init.h>
+#include <linux/linkage.h>
+#include <linux/random.h>
+#include <linux/elf.h>
+#include <asm/vdso.h>
+#include <asm/vvar.h>
+#include <asm/page.h>
+
+unsigned int __read_mostly vdso_enabled = 1;
+
+static struct vm_special_mapping vvar_mapping = {
+	.name = "[vvar]"
+};
+
+#ifdef	CONFIG_SPARC64
+static struct vm_special_mapping vdso_mapping64 = {
+	.name = "[vdso]"
+};
+#endif
+
+#ifdef CONFIG_COMPAT
+static struct vm_special_mapping vdso_mapping32 = {
+	.name = "[vdso]"
+};
+#endif
+
+struct vvar_data *vvar_data;
+
+#define	SAVE_INSTR_SIZE	4
+
+/*
+ * Allocate pages for the vdso and vvar, and copy in the vdso text from the
+ * kernel image.
+ */
+int __init init_vdso_image(const struct vdso_image *image,
+		struct vm_special_mapping *vdso_mapping)
+{
+	int i;
+	struct page *dp, **dpp = NULL;
+	int dnpages = 0;
+	struct page *cp, **cpp = NULL;
+	int cnpages = (image->size) / PAGE_SIZE;
+
+	/*
+	 * First, the vdso text.  This is initialied data, an integral number of
+	 * pages long.
+	 */
+	if (WARN_ON(image->size % PAGE_SIZE != 0))
+		goto oom;
+
+	cpp = kcalloc(cnpages, sizeof(struct page *), GFP_KERNEL);
+	vdso_mapping->pages = cpp;
+
+	if (!cpp)
+		goto oom;
+
+	if (vdso_fix_stick) {
+		/*
+		 * If the system uses %tick instead of %stick, patch the VDSO
+		 * with instruction reading %tick instead of %stick.
+		 */
+		unsigned int j, k = SAVE_INSTR_SIZE;
+		unsigned char *data = image->data;
+
+		for (j = image->sym_vread_tick_patch_start;
+		     j < image->sym_vread_tick_patch_end; j++) {
+
+			data[image->sym_vread_tick + k] = data[j];
+			k++;
+		}
+	}
+
+	for (i = 0; i < cnpages; i++) {
+		cp = alloc_page(GFP_KERNEL);
+		if (!cp)
+			goto oom;
+		cpp[i] = cp;
+		copy_page(page_address(cp), image->data + i * PAGE_SIZE);
+	}
+
+	/*
+	 * Now the vvar page.  This is uninitialized data.
+	 */
+
+	if (vvar_data == NULL) {
+		dnpages = (sizeof(struct vvar_data) / PAGE_SIZE) + 1;
+		if (WARN_ON(dnpages != 1))
+			goto oom;
+		dpp = kcalloc(dnpages, sizeof(struct page *), GFP_KERNEL);
+		vvar_mapping.pages = dpp;
+
+		if (!dpp)
+			goto oom;
+
+		dp = alloc_page(GFP_KERNEL);
+		if (!dp)
+			goto oom;
+
+		dpp[0] = dp;
+		vvar_data = page_address(dp);
+		memset(vvar_data, 0, PAGE_SIZE);
+
+		vvar_data->seq = 0;
+	}
+
+	return 0;
+ oom:
+	if (cpp != NULL) {
+		for (i = 0; i < cnpages; i++) {
+			if (cpp[i] != NULL)
+				__free_page(cpp[i]);
+		}
+		kfree(cpp);
+		vdso_mapping->pages = NULL;
+	}
+
+	if (dpp != NULL) {
+		for (i = 0; i < dnpages; i++) {
+			if (dpp[i] != NULL)
+				__free_page(dpp[i]);
+		}
+		kfree(dpp);
+		vvar_mapping.pages = NULL;
+	}
+
+	pr_warn("Cannot allocate vdso\n");
+	vdso_enabled = 0;
+	return -ENOMEM;
+}
+
+static int __init init_vdso(void)
+{
+	int err = 0;
+#ifdef CONFIG_SPARC64
+	err = init_vdso_image(&vdso_image_64_builtin, &vdso_mapping64);
+	if (err)
+		return err;
+#endif
+
+#ifdef CONFIG_COMPAT
+	err = init_vdso_image(&vdso_image_32_builtin, &vdso_mapping32);
+#endif
+	return err;
+
+}
+subsys_initcall(init_vdso);
+
+struct linux_binprm;
+
+/* Shuffle the vdso up a bit, randomly. */
+static unsigned long vdso_addr(unsigned long start, unsigned int len)
+{
+	unsigned int offset;
+
+	/* This loses some more bits than a modulo, but is cheaper */
+	offset = get_random_int() & (PTRS_PER_PTE - 1);
+	return start + (offset << PAGE_SHIFT);
+}
+
+static int map_vdso(const struct vdso_image *image,
+		struct vm_special_mapping *vdso_mapping)
+{
+	struct mm_struct *mm = current->mm;
+	struct vm_area_struct *vma;
+	unsigned long text_start, addr = 0;
+	int ret = 0;
+
+	down_write(&mm->mmap_sem);
+
+	/*
+	 * First, get an unmapped region: then randomize it, and make sure that
+	 * region is free.
+	 */
+	if (current->flags & PF_RANDOMIZE) {
+		addr = get_unmapped_area(NULL, 0,
+					 image->size - image->sym_vvar_start,
+					 0, 0);
+		if (IS_ERR_VALUE(addr)) {
+			ret = addr;
+			goto up_fail;
+		}
+		addr = vdso_addr(addr, image->size - image->sym_vvar_start);
+	}
+	addr = get_unmapped_area(NULL, addr,
+				 image->size - image->sym_vvar_start, 0, 0);
+	if (IS_ERR_VALUE(addr)) {
+		ret = addr;
+		goto up_fail;
+	}
+
+	text_start = addr - image->sym_vvar_start;
+	current->mm->context.vdso = (void __user *)text_start;
+
+	/*
+	 * MAYWRITE to allow gdb to COW and set breakpoints
+	 */
+	vma = _install_special_mapping(mm,
+				       text_start,
+				       image->size,
+				       VM_READ|VM_EXEC|
+				       VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
+				       vdso_mapping);
+
+	if (IS_ERR(vma)) {
+		ret = PTR_ERR(vma);
+		goto up_fail;
+	}
+
+	vma = _install_special_mapping(mm,
+				       addr,
+				       -image->sym_vvar_start,
+				       VM_READ|VM_MAYREAD,
+				       &vvar_mapping);
+
+	if (IS_ERR(vma)) {
+		ret = PTR_ERR(vma);
+		do_munmap(mm, text_start, image->size, NULL);
+	}
+
+up_fail:
+	if (ret)
+		current->mm->context.vdso = NULL;
+
+	up_write(&mm->mmap_sem);
+	return ret;
+}
+
+int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)
+{
+
+	if (!vdso_enabled)
+		return 0;
+
+#if defined CONFIG_COMPAT
+	if (!(is_32bit_task()))
+		return map_vdso(&vdso_image_64_builtin, &vdso_mapping64);
+	else
+		return map_vdso(&vdso_image_32_builtin, &vdso_mapping32);
+#else
+		return map_vdso(&vdso_image_64_builtin, &vdso_mapping64);
+#endif
+
+}
+
+static __init int vdso_setup(char *s)
+{
+	int err;
+	unsigned long val;
+
+	err = kstrtoul(s, 10, &val);
+	vdso_enabled = val;
+	return err;
+}
+__setup("vdso=", vdso_setup);
