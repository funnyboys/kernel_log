commit 1a4e58cce84ee88129d5d49c064bd2852b481357
Author: Minchan Kim <minchan@kernel.org>
Date:   Wed Sep 25 16:49:15 2019 -0700

    mm: introduce MADV_PAGEOUT
    
    When a process expects no accesses to a certain memory range for a long
    time, it could hint kernel that the pages can be reclaimed instantly but
    data should be preserved for future use.  This could reduce workingset
    eviction so it ends up increasing performance.
    
    This patch introduces the new MADV_PAGEOUT hint to madvise(2) syscall.
    MADV_PAGEOUT can be used by a process to mark a memory range as not
    expected to be used for a long time so that kernel reclaims *any LRU*
    pages instantly.  The hint can help kernel in deciding which pages to
    evict proactively.
    
    A note: It doesn't apply SWAP_CLUSTER_MAX LRU page isolation limit
    intentionally because it's automatically bounded by PMD size.  If PMD
    size(e.g., 256) makes some trouble, we could fix it later by limit it to
    SWAP_CLUSTER_MAX[1].
    
    - man-page material
    
    MADV_PAGEOUT (since Linux x.x)
    
    Do not expect access in the near future so pages in the specified
    regions could be reclaimed instantly regardless of memory pressure.
    Thus, access in the range after successful operation could cause
    major page fault but never lose the up-to-date contents unlike
    MADV_DONTNEED. Pages belonging to a shared mapping are only processed
    if a write access is allowed for the calling process.
    
    MADV_PAGEOUT cannot be applied to locked pages, Huge TLB pages, or
    VM_PFNMAP pages.
    
    [1] https://lore.kernel.org/lkml/20190710194719.GS29695@dhcp22.suse.cz/
    
    [minchan@kernel.org: clear PG_active on MADV_PAGEOUT]
      Link: http://lkml.kernel.org/r/20190802200643.GA181880@google.com
    [akpm@linux-foundation.org: resolve conflicts with hmm.git]
    Link: http://lkml.kernel.org/r/20190726023435.214162-5-minchan@kernel.org
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reported-by: kbuild test robot <lkp@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: James E.J. Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Daniel Colascione <dancol@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Oleksandr Natalenko <oleksandr@redhat.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Sonny Rao <sonnyrao@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tim Murray <timmurray@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index f3258fbf03d0..a18ec7f63888 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -69,6 +69,7 @@
 #define MADV_KEEPONFORK 19		/* Undo MADV_WIPEONFORK */
 
 #define MADV_COLD	20		/* deactivate these pages */
+#define MADV_PAGEOUT	21		/* reclaim these pages */
 
 /* compatibility flags */
 #define MAP_FILE	0

commit 9c276cc65a58faf98be8e56962745ec99ab87636
Author: Minchan Kim <minchan@kernel.org>
Date:   Wed Sep 25 16:49:08 2019 -0700

    mm: introduce MADV_COLD
    
    Patch series "Introduce MADV_COLD and MADV_PAGEOUT", v7.
    
    - Background
    
    The Android terminology used for forking a new process and starting an app
    from scratch is a cold start, while resuming an existing app is a hot
    start.  While we continually try to improve the performance of cold
    starts, hot starts will always be significantly less power hungry as well
    as faster so we are trying to make hot start more likely than cold start.
    
    To increase hot start, Android userspace manages the order that apps
    should be killed in a process called ActivityManagerService.
    ActivityManagerService tracks every Android app or service that the user
    could be interacting with at any time and translates that into a ranked
    list for lmkd(low memory killer daemon).  They are likely to be killed by
    lmkd if the system has to reclaim memory.  In that sense they are similar
    to entries in any other cache.  Those apps are kept alive for
    opportunistic performance improvements but those performance improvements
    will vary based on the memory requirements of individual workloads.
    
    - Problem
    
    Naturally, cached apps were dominant consumers of memory on the system.
    However, they were not significant consumers of swap even though they are
    good candidate for swap.  Under investigation, swapping out only begins
    once the low zone watermark is hit and kswapd wakes up, but the overall
    allocation rate in the system might trip lmkd thresholds and cause a
    cached process to be killed(we measured performance swapping out vs.
    zapping the memory by killing a process.  Unsurprisingly, zapping is 10x
    times faster even though we use zram which is much faster than real
    storage) so kill from lmkd will often satisfy the high zone watermark,
    resulting in very few pages actually being moved to swap.
    
    - Approach
    
    The approach we chose was to use a new interface to allow userspace to
    proactively reclaim entire processes by leveraging platform information.
    This allowed us to bypass the inaccuracy of the kernelâ€™s LRUs for pages
    that are known to be cold from userspace and to avoid races with lmkd by
    reclaiming apps as soon as they entered the cached state.  Additionally,
    it could provide many chances for platform to use much information to
    optimize memory efficiency.
    
    To achieve the goal, the patchset introduce two new options for madvise.
    One is MADV_COLD which will deactivate activated pages and the other is
    MADV_PAGEOUT which will reclaim private pages instantly.  These new
    options complement MADV_DONTNEED and MADV_FREE by adding non-destructive
    ways to gain some free memory space.  MADV_PAGEOUT is similar to
    MADV_DONTNEED in a way that it hints the kernel that memory region is not
    currently needed and should be reclaimed immediately; MADV_COLD is similar
    to MADV_FREE in a way that it hints the kernel that memory region is not
    currently needed and should be reclaimed when memory pressure rises.
    
    This patch (of 5):
    
    When a process expects no accesses to a certain memory range, it could
    give a hint to kernel that the pages can be reclaimed when memory pressure
    happens but data should be preserved for future use.  This could reduce
    workingset eviction so it ends up increasing performance.
    
    This patch introduces the new MADV_COLD hint to madvise(2) syscall.
    MADV_COLD can be used by a process to mark a memory range as not expected
    to be used in the near future.  The hint can help kernel in deciding which
    pages to evict early during memory pressure.
    
    It works for every LRU pages like MADV_[DONTNEED|FREE]. IOW, It moves
    
            active file page -> inactive file LRU
            active anon page -> inacdtive anon LRU
    
    Unlike MADV_FREE, it doesn't move active anonymous pages to inactive file
    LRU's head because MADV_COLD is a little bit different symantic.
    MADV_FREE means it's okay to discard when the memory pressure because the
    content of the page is *garbage* so freeing such pages is almost zero
    overhead since we don't need to swap out and access afterward causes just
    minor fault.  Thus, it would make sense to put those freeable pages in
    inactive file LRU to compete other used-once pages.  It makes sense for
    implmentaion point of view, too because it's not swapbacked memory any
    longer until it would be re-dirtied.  Even, it could give a bonus to make
    them be reclaimed on swapless system.  However, MADV_COLD doesn't mean
    garbage so reclaiming them requires swap-out/in in the end so it's bigger
    cost.  Since we have designed VM LRU aging based on cost-model, anonymous
    cold pages would be better to position inactive anon's LRU list, not file
    LRU.  Furthermore, it would help to avoid unnecessary scanning if system
    doesn't have a swap device.  Let's start simpler way without adding
    complexity at this moment.  However, keep in mind, too that it's a caveat
    that workloads with a lot of pages cache are likely to ignore MADV_COLD on
    anonymous memory because we rarely age anonymous LRU lists.
    
    * man-page material
    
    MADV_COLD (since Linux x.x)
    
    Pages in the specified regions will be treated as less-recently-accessed
    compared to pages in the system with similar access frequencies.  In
    contrast to MADV_FREE, the contents of the region are preserved regardless
    of subsequent writes to pages.
    
    MADV_COLD cannot be applied to locked pages, Huge TLB pages, or VM_PFNMAP
    pages.
    
    [akpm@linux-foundation.org: resolve conflicts with hmm.git]
    Link: http://lkml.kernel.org/r/20190726023435.214162-2-minchan@kernel.org
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Reported-by: kbuild test robot <lkp@intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Johannes Weiner <hannes@cmpxchg.org>
    Cc: James E.J. Bottomley <James.Bottomley@HansenPartnership.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Daniel Colascione <dancol@google.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Joel Fernandes (Google) <joel@joelfernandes.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Oleksandr Natalenko <oleksandr@redhat.com>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Sonny Rao <sonnyrao@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tim Murray <timmurray@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index ac23379b7a87..f3258fbf03d0 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -68,6 +68,8 @@
 #define MADV_WIPEONFORK 18		/* Zero memory on fork, child only */
 #define MADV_KEEPONFORK 19		/* Undo MADV_WIPEONFORK */
 
+#define MADV_COLD	20		/* deactivate these pages */
+
 /* compatibility flags */
 #define MAP_FILE	0
 

commit 746c9398f5ac2b3f5730da4ed09e99ef4bb50b4a
Author: Michael S. Tsirkin <mst@redhat.com>
Date:   Fri Feb 8 01:02:55 2019 -0500

    arch: move common mmap flags to linux/mman.h
    
    Now that we have 3 mmap flags shared by all architectures,
    let's move them into the common header.
    
    This will help discourage future architectures from duplicating code.
    
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index f9d4e6b6d4bd..ac23379b7a87 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -10,9 +10,7 @@
 #define PROT_GROWSDOWN	0x01000000	/* mprotect flag: extend change to start of growsdown vma */
 #define PROT_GROWSUP	0x02000000	/* mprotect flag: extend change to end of growsup vma */
 
-#define MAP_SHARED	0x01		/* Share changes */
-#define MAP_PRIVATE	0x02		/* Changes are private */
-#define MAP_SHARED_VALIDATE 0x03	/* share + validate extension flags */
+/* 0x01 - 0x03 are defined in linux/mman.h */
 #define MAP_TYPE	0x0f		/* Mask for type of mapping (OSF/1 is _wrong_) */
 #define MAP_FIXED	0x100		/* Interpret addr exactly */
 #define MAP_ANONYMOUS	0x10		/* don't use a file */

commit a4ff8e8620d3f4f50ac4b41e8067b7d395056843
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Apr 10 16:35:57 2018 -0700

    mm: introduce MAP_FIXED_NOREPLACE
    
    Patch series "mm: introduce MAP_FIXED_NOREPLACE", v2.
    
    This has started as a follow up discussion [3][4] resulting in the
    runtime failure caused by hardening patch [5] which removes MAP_FIXED
    from the elf loader because MAP_FIXED is inherently dangerous as it
    might silently clobber an existing underlying mapping (e.g.  stack).
    The reason for the failure is that some architectures enforce an
    alignment for the given address hint without MAP_FIXED used (e.g.  for
    shared or file backed mappings).
    
    One way around this would be excluding those archs which do alignment
    tricks from the hardening [6].  The patch is really trivial but it has
    been objected, rightfully so, that this screams for a more generic
    solution.  We basically want a non-destructive MAP_FIXED.
    
    The first patch introduced MAP_FIXED_NOREPLACE which enforces the given
    address but unlike MAP_FIXED it fails with EEXIST if the given range
    conflicts with an existing one.  The flag is introduced as a completely
    new one rather than a MAP_FIXED extension because of the backward
    compatibility.  We really want a never-clobber semantic even on older
    kernels which do not recognize the flag.  Unfortunately mmap sucks
    wrt flags evaluation because we do not EINVAL on unknown flags.  On
    those kernels we would simply use the traditional hint based semantic so
    the caller can still get a different address (which sucks) but at least
    not silently corrupt an existing mapping.  I do not see a good way
    around that.  Except we won't export expose the new semantic to the
    userspace at all.
    
    It seems there are users who would like to have something like that.
    Jemalloc has been mentioned by Michael Ellerman [7]
    
    Florian Weimer has mentioned the following:
    : glibc ld.so currently maps DSOs without hints.  This means that the kernel
    : will map right next to each other, and the offsets between them a completely
    : predictable.  We would like to change that and supply a random address in a
    : window of the address space.  If there is a conflict, we do not want the
    : kernel to pick a non-random address. Instead, we would try again with a
    : random address.
    
    John Hubbard has mentioned CUDA example
    : a) Searches /proc/<pid>/maps for a "suitable" region of available
    : VA space.  "Suitable" generally means it has to have a base address
    : within a certain limited range (a particular device model might
    : have odd limitations, for example), it has to be large enough, and
    : alignment has to be large enough (again, various devices may have
    : constraints that lead us to do this).
    :
    : This is of course subject to races with other threads in the process.
    :
    : Let's say it finds a region starting at va.
    :
    : b) Next it does:
    :     p = mmap(va, ...)
    :
    : *without* setting MAP_FIXED, of course (so va is just a hint), to
    : attempt to safely reserve that region. If p != va, then in most cases,
    : this is a failure (almost certainly due to another thread getting a
    : mapping from that region before we did), and so this layer now has to
    : call munmap(), before returning a "failure: retry" to upper layers.
    :
    :     IMPROVEMENT: --> if instead, we could call this:
    :
    :             p = mmap(va, ... MAP_FIXED_NOREPLACE ...)
    :
    :         , then we could skip the munmap() call upon failure. This
    :         is a small thing, but it is useful here. (Thanks to Piotr
    :         Jaroszynski and Mark Hairgrove for helping me get that detail
    :         exactly right, btw.)
    :
    : c) After that, CUDA suballocates from p, via:
    :
    :      q = mmap(sub_region_start, ... MAP_FIXED ...)
    :
    : Interestingly enough, "freeing" is also done via MAP_FIXED, and
    : setting PROT_NONE to the subregion. Anyway, I just included (c) for
    : general interest.
    
    Atomic address range probing in the multithreaded programs in general
    sounds like an interesting thing to me.
    
    The second patch simply replaces MAP_FIXED use in elf loader by
    MAP_FIXED_NOREPLACE.  I believe other places which rely on MAP_FIXED
    should follow.  Actually real MAP_FIXED usages should be docummented
    properly and they should be more of an exception.
    
    [1] http://lkml.kernel.org/r/20171116101900.13621-1-mhocko@kernel.org
    [2] http://lkml.kernel.org/r/20171129144219.22867-1-mhocko@kernel.org
    [3] http://lkml.kernel.org/r/20171107162217.382cd754@canb.auug.org.au
    [4] http://lkml.kernel.org/r/1510048229.12079.7.camel@abdul.in.ibm.com
    [5] http://lkml.kernel.org/r/20171023082608.6167-1-mhocko@kernel.org
    [6] http://lkml.kernel.org/r/20171113094203.aofz2e7kueitk55y@dhcp22.suse.cz
    [7] http://lkml.kernel.org/r/87efp1w7vy.fsf@concordia.ellerman.id.au
    
    This patch (of 2):
    
    MAP_FIXED is used quite often to enforce mapping at the particular range.
    The main problem of this flag is, however, that it is inherently dangerous
    because it unmaps existing mappings covered by the requested range.  This
    can cause silent memory corruptions.  Some of them even with serious
    security implications.  While the current semantic might be really
    desiderable in many cases there are others which would want to enforce the
    given range but rather see a failure than a silent memory corruption on a
    clashing range.  Please note that there is no guarantee that a given range
    is obeyed by the mmap even when it is free - e.g.  arch specific code is
    allowed to apply an alignment.
    
    Introduce a new MAP_FIXED_NOREPLACE flag for mmap to achieve this
    behavior.  It has the same semantic as MAP_FIXED wrt.  the given address
    request with a single exception that it fails with EEXIST if the requested
    address is already covered by an existing mapping.  We still do rely on
    get_unmaped_area to handle all the arch specific MAP_FIXED treatment and
    check for a conflicting vma after it returns.
    
    The flag is introduced as a completely new one rather than a MAP_FIXED
    extension because of the backward compatibility.  We really want a
    never-clobber semantic even on older kernels which do not recognize the
    flag.  Unfortunately mmap sucks wrt.  flags evaluation because we do not
    EINVAL on unknown flags.  On those kernels we would simply use the
    traditional hint based semantic so the caller can still get a different
    address (which sucks) but at least not silently corrupt an existing
    mapping.  I do not see a good way around that.
    
    [mpe@ellerman.id.au: fix whitespace]
    [fail on clashing range with EEXIST as per Florian Weimer]
    [set MAP_FIXED before round_hint_to_min as per Khalid Aziz]
    Link: http://lkml.kernel.org/r/20171213092550.2774-2-mhocko@kernel.org
    Reviewed-by: Khalid Aziz <khalid.aziz@oracle.com>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Khalid Aziz <khalid.aziz@oracle.com>
    Cc: Russell King - ARM Linux <linux@armlinux.org.uk>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Florian Weimer <fweimer@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Abdul Haleem <abdhalee@linux.vnet.ibm.com>
    Cc: Joel Stanley <joel@jms.id.au>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Jason Evans <jasone@google.com>
    Cc: David Goldblatt <davidtgoldblatt@gmail.com>
    Cc: Edward Tomasz NapieraÅ‚a <trasz@FreeBSD.org>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index 2dbdf59258d9..f9d4e6b6d4bd 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -32,6 +32,7 @@
 #define MAP_NONBLOCK	0x40000		/* do not block on IO */
 #define MAP_STACK	0x80000		/* give out an address that is best suited for process/thread stacks */
 #define MAP_HUGETLB	0x100000	/* create a huge page mapping */
+#define MAP_FIXED_NOREPLACE	0x200000/* MAP_FIXED which doesn't unmap underlying mapping */
 
 #define MS_ASYNC	1		/* sync memory asynchronously */
 #define MS_SYNC		2		/* synchronous memory sync */

commit a3841f94c7ecb3ede0f888d3fcfe8fb6368ddd7a
Merge: adeba81ac2a6 4247f24c2358
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 17 09:51:57 2017 -0800

    Merge tag 'libnvdimm-for-4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull libnvdimm and dax updates from Dan Williams:
     "Save for a few late fixes, all of these commits have shipped in -next
      releases since before the merge window opened, and 0day has given a
      build success notification.
    
      The ext4 touches came from Jan, and the xfs touches have Darrick's
      reviewed-by. An xfstest for the MAP_SYNC feature has been through
      a few round of reviews and is on track to be merged.
    
       - Introduce MAP_SYNC and MAP_SHARED_VALIDATE, a mechanism to enable
         'userspace flush' of persistent memory updates via filesystem-dax
         mappings. It arranges for any filesystem metadata updates that may
         be required to satisfy a write fault to also be flushed ("on disk")
         before the kernel returns to userspace from the fault handler.
         Effectively every write-fault that dirties metadata completes an
         fsync() before returning from the fault handler. The new
         MAP_SHARED_VALIDATE mapping type guarantees that the MAP_SYNC flag
         is validated as supported by the filesystem's ->mmap() file
         operation.
    
       - Add support for the standard ACPI 6.2 label access methods that
         replace the NVDIMM_FAMILY_INTEL (vendor specific) label methods.
         This enables interoperability with environments that only implement
         the standardized methods.
    
       - Add support for the ACPI 6.2 NVDIMM media error injection methods.
    
       - Add support for the NVDIMM_FAMILY_INTEL v1.6 DIMM commands for
         latch last shutdown status, firmware update, SMART error injection,
         and SMART alarm threshold control.
    
       - Cleanup physical address information disclosures to be root-only.
    
       - Fix revalidation of the DIMM "locked label area" status to support
         dynamic unlock of the label area.
    
       - Expand unit test infrastructure to mock the ACPI 6.2 Translate SPA
         (system-physical-address) command and error injection commands.
    
      Acknowledgements that came after the commits were pushed to -next:
    
       - 957ac8c421ad ("dax: fix PMD faults on zero-length files"):
           Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    
       - a39e596baa07 ("xfs: support for synchronous DAX faults") and
         7b565c9f965b ("xfs: Implement xfs_filemap_pfn_mkwrite() using __xfs_filemap_fault()")
            Reviewed-by: Darrick J. Wong <darrick.wong@oracle.com>"
    
    * tag 'libnvdimm-for-4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm: (49 commits)
      acpi, nfit: add 'Enable Latch System Shutdown Status' command support
      dax: fix general protection fault in dax_alloc_inode
      dax: fix PMD faults on zero-length files
      dax: stop requiring a live device for dax_flush()
      brd: remove dax support
      dax: quiet bdev_dax_supported()
      fs, dax: unify IOMAP_F_DIRTY read vs write handling policy in the dax core
      tools/testing/nvdimm: unit test clear-error commands
      acpi, nfit: validate commands against the device type
      tools/testing/nvdimm: stricter bounds checking for error injection commands
      xfs: support for synchronous DAX faults
      xfs: Implement xfs_filemap_pfn_mkwrite() using __xfs_filemap_fault()
      ext4: Support for synchronous DAX faults
      ext4: Simplify error handling in ext4_dax_huge_fault()
      dax: Implement dax_finish_sync_fault()
      dax, iomap: Add support for synchronous faults
      mm: Define MAP_SYNC and VM_SYNC flags
      dax: Allow tuning whether dax_insert_mapping_entry() dirties entry
      dax: Allow dax_iomap_fault() to return pfn
      dax: Fix comment describing dax_iomap_fault()
      ...

commit 1c9725974074a047f6080eecc62c50a8e840d050
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Wed Nov 1 16:36:30 2017 +0100

    mm: introduce MAP_SHARED_VALIDATE, a mechanism to safely define new mmap flags
    
    The mmap(2) syscall suffers from the ABI anti-pattern of not validating
    unknown flags. However, proposals like MAP_SYNC need a mechanism to
    define new behavior that is known to fail on older kernels without the
    support. Define a new MAP_SHARED_VALIDATE flag pattern that is
    guaranteed to fail on all legacy mmap implementations.
    
    It is worth noting that the original proposal was for a standalone
    MAP_VALIDATE flag. However, when that  could not be supported by all
    archs Linus observed:
    
        I see why you *think* you want a bitmap. You think you want
        a bitmap because you want to make MAP_VALIDATE be part of MAP_SYNC
        etc, so that people can do
    
        ret = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED
                        | MAP_SYNC, fd, 0);
    
        and "know" that MAP_SYNC actually takes.
    
        And I'm saying that whole wish is bogus. You're fundamentally
        depending on special semantics, just make it explicit. It's already
        not portable, so don't try to make it so.
    
        Rename that MAP_VALIDATE as MAP_SHARED_VALIDATE, make it have a value
        of 0x3, and make people do
    
        ret = mmap(NULL, size, PROT_READ | PROT_WRITE, MAP_SHARED_VALIDATE
                        | MAP_SYNC, fd, 0);
    
        and then the kernel side is easier too (none of that random garbage
        playing games with looking at the "MAP_VALIDATE bit", but just another
        case statement in that map type thing.
    
        Boom. Done.
    
    Similar to ->fallocate() we also want the ability to validate the
    support for new flags on a per ->mmap() 'struct file_operations'
    instance basis.  Towards that end arrange for flags to be generically
    validated against a mmap_supported_flags exported by 'struct
    file_operations'. By default all existing flags are implicitly
    supported, but new flags require MAP_SHARED_VALIDATE and
    per-instance-opt-in.
    
    Cc: Jan Kara <jack@suse.cz>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Suggested-by: Christoph Hellwig <hch@lst.de>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Jan Kara <jack@suse.cz>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index 3b26cc62dadb..f6d118aaedb9 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -11,6 +11,7 @@
 
 #define MAP_SHARED	0x01		/* Share changes */
 #define MAP_PRIVATE	0x02		/* Changes are private */
+#define MAP_SHARED_VALIDATE 0x03	/* share + validate extension flags */
 #define MAP_TYPE	0x0f		/* Mask for type of mapping (OSF/1 is _wrong_) */
 #define MAP_FIXED	0x100		/* Interpret addr exactly */
 #define MAP_ANONYMOUS	0x10		/* don't use a file */

commit 6f52b16c5b29b89d92c0e7236f4655dc8491ad70
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:08:43 2017 +0100

    License cleanup: add SPDX license identifier to uapi header files with no license
    
    Many user space API headers are missing licensing information, which
    makes it hard for compliance tools to determine the correct license.
    
    By default are files without license information under the default
    license of the kernel, which is GPLV2.  Marking them GPLV2 would exclude
    them from being included in non GPLV2 code, which is obviously not
    intended. The user space API headers fall under the syscall exception
    which is in the kernels COPYING file:
    
       NOTE! This copyright does *not* cover user programs that use kernel
       services by normal system calls - this is merely considered normal use
       of the kernel, and does *not* fall under the heading of "derived work".
    
    otherwise syscall usage would not be possible.
    
    Update the files which contain no license information with an SPDX
    license identifier.  The chosen identifier is 'GPL-2.0 WITH
    Linux-syscall-note' which is the officially assigned identifier for the
    Linux syscall exception.  SPDX license identifiers are a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.  See the previous patch in this series for the
    methodology of how this patch was researched.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index 3b26cc62dadb..6bf730063e3f 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
 #ifndef __ALPHA_MMAN_H__
 #define __ALPHA_MMAN_H__
 

commit d2cd9ede6e193dd7d88b6d27399e96229a551b19
Author: Rik van Riel <riel@redhat.com>
Date:   Wed Sep 6 16:25:15 2017 -0700

    mm,fork: introduce MADV_WIPEONFORK
    
    Introduce MADV_WIPEONFORK semantics, which result in a VMA being empty
    in the child process after fork.  This differs from MADV_DONTFORK in one
    important way.
    
    If a child process accesses memory that was MADV_WIPEONFORK, it will get
    zeroes.  The address ranges are still valid, they are just empty.
    
    If a child process accesses memory that was MADV_DONTFORK, it will get a
    segmentation fault, since those address ranges are no longer valid in
    the child after fork.
    
    Since MADV_DONTFORK also seems to be used to allow very large programs
    to fork in systems with strict memory overcommit restrictions, changing
    the semantics of MADV_DONTFORK might break existing programs.
    
    MADV_WIPEONFORK only works on private, anonymous VMAs.
    
    The use case is libraries that store or cache information, and want to
    know that they need to regenerate it in the child process after fork.
    
    Examples of this would be:
     - systemd/pulseaudio API checks (fail after fork) (replacing a getpid
       check, which is too slow without a PID cache)
     - PKCS#11 API reinitialization check (mandated by specification)
     - glibc's upcoming PRNG (reseed after fork)
     - OpenSSL PRNG (reseed after fork)
    
    The security benefits of a forking server having a re-inialized PRNG in
    every child process are pretty obvious.  However, due to libraries
    having all kinds of internal state, and programs getting compiled with
    many different versions of each library, it is unreasonable to expect
    calling programs to re-initialize everything manually after fork.
    
    A further complication is the proliferation of clone flags, programs
    bypassing glibc's functions to call clone directly, and programs calling
    unshare, causing the glibc pthread_atfork hook to not get called.
    
    It would be better to have the kernel take care of this automatically.
    
    The patch also adds MADV_KEEPONFORK, to undo the effects of a prior
    MADV_WIPEONFORK.
    
    This is similar to the OpenBSD minherit syscall with MAP_INHERIT_ZERO:
    
        https://man.openbsd.org/minherit.2
    
    [akpm@linux-foundation.org: numerically order arch/parisc/include/uapi/asm/mman.h #defines]
    Link: http://lkml.kernel.org/r/20170811212829.29186-3-riel@redhat.com
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Reported-by: Florian Weimer <fweimer@redhat.com>
    Reported-by: Colm MacCÃ¡rtaigh <colm@allcosts.net>
    Reviewed-by: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Drewry <wad@chromium.org>
    Cc: <linux-api@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index 13b52aad3c43..3b26cc62dadb 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -64,6 +64,9 @@
 					   overrides the coredump filter bits */
 #define MADV_DODUMP	17		/* Clear the MADV_NODUMP flag */
 
+#define MADV_WIPEONFORK 18		/* Zero memory on fork, child only */
+#define MADV_KEEPONFORK 19		/* Undo MADV_WIPEONFORK */
+
 /* compatibility flags */
 #define MAP_FILE	0
 

commit aafd4562dfee81a40ba21b5ea3cf5e06664bc7f6
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Wed Sep 6 16:23:29 2017 -0700

    mm: arch: consolidate mmap hugetlb size encodings
    
    A non-default huge page size can be encoded in the flags argument of the
    mmap system call.  The definitions for these encodings are in arch
    specific header files.  However, all architectures use the same values.
    
    Consolidate all the definitions in the primary user header file
    (uapi/linux/mman.h).  Include definitions for all known huge page sizes.
    Use the generic encoding definitions in hugetlb_encode.h as the basis
    for these definitions.
    
    Link: http://lkml.kernel.org/r/1501527386-10736-3-git-send-email-mike.kravetz@oracle.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index 02760f6e6ca4..13b52aad3c43 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -67,17 +67,6 @@
 /* compatibility flags */
 #define MAP_FILE	0
 
-/*
- * When MAP_HUGETLB is set bits [26:31] encode the log2 of the huge page size.
- * This gives us 6 bits, which is enough until someone invents 128 bit address
- * spaces.
- *
- * Assume these are all power of twos.
- * When 0 use the default page size.
- */
-#define MAP_HUGE_SHIFT	26
-#define MAP_HUGE_MASK	0x3f
-
 #define PKEY_DISABLE_ACCESS	0x1
 #define PKEY_DISABLE_WRITE	0x2
 #define PKEY_ACCESS_MASK	(PKEY_DISABLE_ACCESS |\

commit e8c24d3a23a469f1f40d4de24d872ca7023ced0a
Author: Dave Hansen <dave.hansen@linux.intel.com>
Date:   Fri Jul 29 09:30:15 2016 -0700

    x86/pkeys: Allocation/free syscalls
    
    This patch adds two new system calls:
    
            int pkey_alloc(unsigned long flags, unsigned long init_access_rights)
            int pkey_free(int pkey);
    
    These implement an "allocator" for the protection keys
    themselves, which can be thought of as analogous to the allocator
    that the kernel has for file descriptors.  The kernel tracks
    which numbers are in use, and only allows operations on keys that
    are valid.  A key which was not obtained by pkey_alloc() may not,
    for instance, be passed to pkey_mprotect().
    
    These system calls are also very important given the kernel's use
    of pkeys to implement execute-only support.  These help ensure
    that userspace can never assume that it has control of a key
    unless it first asks the kernel.  The kernel does not promise to
    preserve PKRU (right register) contents except for allocated
    pkeys.
    
    The 'init_access_rights' argument to pkey_alloc() specifies the
    rights that will be established for the returned pkey.  For
    instance:
    
            pkey = pkey_alloc(flags, PKEY_DENY_WRITE);
    
    will allocate 'pkey', but also sets the bits in PKRU[1] such that
    writing to 'pkey' is already denied.
    
    The kernel does not prevent pkey_free() from successfully freeing
    in-use pkeys (those still assigned to a memory range by
    pkey_mprotect()).  It would be expensive to implement the checks
    for this, so we instead say, "Just don't do it" since sane
    software will never do it anyway.
    
    Any piece of userspace calling pkey_alloc() needs to be prepared
    for it to fail.  Why?  pkey_alloc() returns the same error code
    (ENOSPC) when there are no pkeys and when pkeys are unsupported.
    They can be unsupported for a whole host of reasons, so apps must
    be prepared for this.  Also, libraries or LD_PRELOADs might steal
    keys before an application gets access to them.
    
    This allocation mechanism could be implemented in userspace.
    Even if we did it in userspace, we would still need additional
    user/kernel interfaces to tell userspace which keys are being
    used by the kernel internally (such as for execute-only
    mappings).  Having the kernel provide this facility completely
    removes the need for these additional interfaces, or having an
    implementation of this in userspace at all.
    
    Note that we have to make changes to all of the architectures
    that do not use mman-common.h because we use the new
    PKEY_DENY_ACCESS/WRITE macros in arch-independent code.
    
    1. PKRU is the Protection Key Rights User register.  It is a
       usermode-accessible register that controls whether writes
       and/or access to each individual pkey is allowed or denied.
    
    Signed-off-by: Dave Hansen <dave.hansen@linux.intel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: linux-arch@vger.kernel.org
    Cc: Dave Hansen <dave@sr71.net>
    Cc: arnd@arndb.de
    Cc: linux-api@vger.kernel.org
    Cc: linux-mm@kvack.org
    Cc: luto@kernel.org
    Cc: akpm@linux-foundation.org
    Cc: torvalds@linux-foundation.org
    Link: http://lkml.kernel.org/r/20160729163015.444FE75F@viggo.jf.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index fec1947b8dbc..02760f6e6ca4 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -78,4 +78,9 @@
 #define MAP_HUGE_SHIFT	26
 #define MAP_HUGE_MASK	0x3f
 
+#define PKEY_DISABLE_ACCESS	0x1
+#define PKEY_DISABLE_WRITE	0x2
+#define PKEY_ACCESS_MASK	(PKEY_DISABLE_ACCESS |\
+				 PKEY_DISABLE_WRITE)
+
 #endif /* __ALPHA_MMAN_H__ */

commit dcd6c87cc59af1b4fe7664b35c6344bbe1c9928f
Author: Guenter Roeck <linux@roeck-us.net>
Date:   Wed Jan 20 14:58:15 2016 -0800

    mm: arch: remove duplicate definitions of MADV_FREE
    
    Commits 21f55b018ba5 ("arch/*/include/uapi/asm/mman.h: : let MADV_FREE
    have same value for all architectures") and ef58978f1eaa ("mm: define
    MADV_FREE for some arches") both defined MADV_FREE, but did not use the
    same values.  This results in build errors such as
    
      ./arch/alpha/include/uapi/asm/mman.h:53:0: error: "MADV_FREE" redefined
      ./arch/alpha/include/uapi/asm/mman.h:50:0: note: this is the location of the previous definition
    
    for the affected architectures.
    
    Fixes: 21f55b018ba5 ("arch/*/include/uapi/asm/mman.h: : let MADV_FREE have same value for all architectures")
    Fixes: ef58978f1eaa ("mm: define MADV_FREE for some arches")
    Signed-off-by: Guenter Roeck <linux@roeck-us.net>
    Cc: Chen Gang <gang.chen.5i5j@gmail.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Acked-by: Helge Deller <deller@gmx.de>  [parisc]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index ab336c06153e..fec1947b8dbc 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -47,7 +47,6 @@
 #define MADV_WILLNEED	3		/* will need these pages */
 #define	MADV_SPACEAVAIL	5		/* ensure resources are available */
 #define MADV_DONTNEED	6		/* don't need these pages */
-#define MADV_FREE	7		/* free pages only if memory pressure */
 
 /* common/generic parameters */
 #define MADV_FREE	8		/* free pages only if memory pressure */

commit 21f55b018ba57897f4d3590ecbe11516bdc540af
Author: Chen Gang <gang.chen.5i5j@gmail.com>
Date:   Fri Jan 15 16:55:02 2016 -0800

    arch/*/include/uapi/asm/mman.h: : let MADV_FREE have same value for all architectures
    
    For uapi, need try to let all macros have same value, and MADV_FREE is
    added into main branch recently, so need redefine MADV_FREE for it.
    
    At present, '8' can be shared with all architectures, so redefine it to
    '8'.
    
    [sudipm.mukherjee@gmail.com: correct uniform value of MADV_FREE]
    Signed-off-by: Chen Gang <gang.chen.5i5j@gmail.com>
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Hugh Dickins <hughd@google.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Roland Dreier <roland@kernel.org>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: <yalin.wang2010@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Daniel Micay <danielmicay@gmail.com>
    Cc: Jason Evans <je@fb.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mika Penttil <mika.penttila@nextfour.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Signed-off-by: Sudip Mukherjee <sudip@vectorindia.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index d828beb5e69b..ab336c06153e 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -50,6 +50,7 @@
 #define MADV_FREE	7		/* free pages only if memory pressure */
 
 /* common/generic parameters */
+#define MADV_FREE	8		/* free pages only if memory pressure */
 #define MADV_REMOVE	9		/* remove these pages & resources */
 #define MADV_DONTFORK	10		/* don't inherit across fork */
 #define MADV_DOFORK	11		/* do inherit across fork */

commit ef58978f1eaab140081ec1808d96ee06e933e760
Author: Minchan Kim <minchan@kernel.org>
Date:   Fri Jan 15 16:54:57 2016 -0800

    mm: define MADV_FREE for some arches
    
    Most architectures use asm-generic, but alpha, mips, parisc, xtensa need
    their own definitions.
    
    This patch defines MADV_FREE for them so it should fix build break for
    their architectures.
    
    Maybe, I should split and feed pieces to arch maintainers but included
    here for mmotm convenience.
    
    [gang.chen.5i5j@gmail.com: let MADV_FREE have same value for all architectures]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Richard Henderson <rth@twiddle.net>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: "James E.J. Bottomley" <jejb@parisc-linux.org>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: <yalin.wang2010@gmail.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chen Gang <gang.chen.5i5j@gmail.com>
    Cc: Daniel Micay <danielmicay@gmail.com>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Evans <je@fb.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Mika Penttil <mika.penttila@nextfour.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Roland Dreier <roland@kernel.org>
    Cc: Russell King <rmk@arm.linux.org.uk>
    Cc: Shaohua Li <shli@kernel.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Chen Gang <gang.chen.5i5j@gmail.com>
    Acked-by: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index f2f949671798..d828beb5e69b 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -47,6 +47,7 @@
 #define MADV_WILLNEED	3		/* will need these pages */
 #define	MADV_SPACEAVAIL	5		/* ensure resources are available */
 #define MADV_DONTNEED	6		/* don't need these pages */
+#define MADV_FREE	7		/* free pages only if memory pressure */
 
 /* common/generic parameters */
 #define MADV_REMOVE	9		/* remove these pages & resources */

commit b0f205c2a3082dd9081f9a94e50658c5fa906ff1
Author: Eric B Munson <emunson@akamai.com>
Date:   Thu Nov 5 18:51:39 2015 -0800

    mm: mlock: add mlock flags to enable VM_LOCKONFAULT usage
    
    The previous patch introduced a flag that specified pages in a VMA should
    be placed on the unevictable LRU, but they should not be made present when
    the area is created.  This patch adds the ability to set this state via
    the new mlock system calls.
    
    We add MLOCK_ONFAULT for mlock2 and MCL_ONFAULT for mlockall.
    MLOCK_ONFAULT will set the VM_LOCKONFAULT modifier for VM_LOCKED.
    MCL_ONFAULT should be used as a modifier to the two other mlockall flags.
    When used with MCL_CURRENT, all current mappings will be marked with
    VM_LOCKED | VM_LOCKONFAULT.  When used with MCL_FUTURE, the mm->def_flags
    will be marked with VM_LOCKED | VM_LOCKONFAULT.  When used with both
    MCL_CURRENT and MCL_FUTURE, all current mappings and mm->def_flags will be
    marked with VM_LOCKED | VM_LOCKONFAULT.
    
    Prior to this patch, mlockall() will unconditionally clear the
    mm->def_flags any time it is called without MCL_FUTURE.  This behavior is
    maintained after adding MCL_ONFAULT.  If a call to mlockall(MCL_FUTURE) is
    followed by mlockall(MCL_CURRENT), the mm->def_flags will be cleared and
    new VMAs will be unlocked.  This remains true with or without MCL_ONFAULT
    in either mlockall() invocation.
    
    munlock() will unconditionally clear both vma flags.  munlockall()
    unconditionally clears for VMA flags on all VMAs and in the mm->def_flags
    field.
    
    Signed-off-by: Eric B Munson <emunson@akamai.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Michael Kerrisk <mtk.manpages@gmail.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Shuah Khan <shuahkh@osg.samsung.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
index 0086b472bc2b..f2f949671798 100644
--- a/arch/alpha/include/uapi/asm/mman.h
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -37,6 +37,9 @@
 
 #define MCL_CURRENT	 8192		/* lock all currently mapped pages */
 #define MCL_FUTURE	16384		/* lock all additions to address space */
+#define MCL_ONFAULT	32768		/* lock all pages that are faulted in */
+
+#define MLOCK_ONFAULT	0x01		/* Lock pages in range after they are faulted in, do not prefault */
 
 #define MADV_NORMAL	0		/* no further special treatment */
 #define MADV_RANDOM	1		/* expect random page references */

commit 96433f6ee49032d7a8bda76de2b05cfde2914354
Author: David Howells <dhowells@redhat.com>
Date:   Mon Dec 17 13:47:09 2012 +0000

    UAPI: (Scripted) Disintegrate arch/alpha/include/asm
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Michael Kerrisk <mtk.manpages@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Dave Jones <davej@redhat.com>
    Acked-by: Michael Cree <mcree@orcon.net.nz>
    Acked-by: Matt Turner <mattst88@gmail.com>

diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h
new file mode 100644
index 000000000000..0086b472bc2b
--- /dev/null
+++ b/arch/alpha/include/uapi/asm/mman.h
@@ -0,0 +1,77 @@
+#ifndef __ALPHA_MMAN_H__
+#define __ALPHA_MMAN_H__
+
+#define PROT_READ	0x1		/* page can be read */
+#define PROT_WRITE	0x2		/* page can be written */
+#define PROT_EXEC	0x4		/* page can be executed */
+#define PROT_SEM	0x8		/* page may be used for atomic ops */
+#define PROT_NONE	0x0		/* page can not be accessed */
+#define PROT_GROWSDOWN	0x01000000	/* mprotect flag: extend change to start of growsdown vma */
+#define PROT_GROWSUP	0x02000000	/* mprotect flag: extend change to end of growsup vma */
+
+#define MAP_SHARED	0x01		/* Share changes */
+#define MAP_PRIVATE	0x02		/* Changes are private */
+#define MAP_TYPE	0x0f		/* Mask for type of mapping (OSF/1 is _wrong_) */
+#define MAP_FIXED	0x100		/* Interpret addr exactly */
+#define MAP_ANONYMOUS	0x10		/* don't use a file */
+
+/* not used by linux, but here to make sure we don't clash with OSF/1 defines */
+#define _MAP_HASSEMAPHORE 0x0200
+#define _MAP_INHERIT	0x0400
+#define _MAP_UNALIGNED	0x0800
+
+/* These are linux-specific */
+#define MAP_GROWSDOWN	0x01000		/* stack-like segment */
+#define MAP_DENYWRITE	0x02000		/* ETXTBSY */
+#define MAP_EXECUTABLE	0x04000		/* mark it as an executable */
+#define MAP_LOCKED	0x08000		/* lock the mapping */
+#define MAP_NORESERVE	0x10000		/* don't check for reservations */
+#define MAP_POPULATE	0x20000		/* populate (prefault) pagetables */
+#define MAP_NONBLOCK	0x40000		/* do not block on IO */
+#define MAP_STACK	0x80000		/* give out an address that is best suited for process/thread stacks */
+#define MAP_HUGETLB	0x100000	/* create a huge page mapping */
+
+#define MS_ASYNC	1		/* sync memory asynchronously */
+#define MS_SYNC		2		/* synchronous memory sync */
+#define MS_INVALIDATE	4		/* invalidate the caches */
+
+#define MCL_CURRENT	 8192		/* lock all currently mapped pages */
+#define MCL_FUTURE	16384		/* lock all additions to address space */
+
+#define MADV_NORMAL	0		/* no further special treatment */
+#define MADV_RANDOM	1		/* expect random page references */
+#define MADV_SEQUENTIAL	2		/* expect sequential page references */
+#define MADV_WILLNEED	3		/* will need these pages */
+#define	MADV_SPACEAVAIL	5		/* ensure resources are available */
+#define MADV_DONTNEED	6		/* don't need these pages */
+
+/* common/generic parameters */
+#define MADV_REMOVE	9		/* remove these pages & resources */
+#define MADV_DONTFORK	10		/* don't inherit across fork */
+#define MADV_DOFORK	11		/* do inherit across fork */
+
+#define MADV_MERGEABLE   12		/* KSM may merge identical pages */
+#define MADV_UNMERGEABLE 13		/* KSM may not merge identical pages */
+
+#define MADV_HUGEPAGE	14		/* Worth backing with hugepages */
+#define MADV_NOHUGEPAGE	15		/* Not worth backing with hugepages */
+
+#define MADV_DONTDUMP   16		/* Explicity exclude from the core dump,
+					   overrides the coredump filter bits */
+#define MADV_DODUMP	17		/* Clear the MADV_NODUMP flag */
+
+/* compatibility flags */
+#define MAP_FILE	0
+
+/*
+ * When MAP_HUGETLB is set bits [26:31] encode the log2 of the huge page size.
+ * This gives us 6 bits, which is enough until someone invents 128 bit address
+ * spaces.
+ *
+ * Assume these are all power of twos.
+ * When 0 use the default page size.
+ */
+#define MAP_HUGE_SHIFT	26
+#define MAP_HUGE_MASK	0x3f
+
+#endif /* __ALPHA_MMAN_H__ */
