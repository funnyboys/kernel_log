commit 55716d26439f5c4008b0bcb7f17d1f7c0d8fbcfc
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Jun 1 10:08:42 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 428
    
    Based on 1 normalized pattern(s):
    
      this file is released under the gplv2
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 68 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Armijn Hemel <armijn@tjaldur.nl>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190531190114.292346262@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 3a2ff5c9192c..20d2b69a13b0 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -1,11 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * mm/percpu-km.c - kernel memory based chunk allocation
  *
  * Copyright (C) 2010		SUSE Linux Products GmbH
  * Copyright (C) 2010		Tejun Heo <tj@kernel.org>
  *
- * This file is released under the GPLv2.
- *
  * Chunks are allocated as a contiguous kernel memory using gfp
  * allocation.  This is to be used on nommu architectures.
  *

commit b239f7daf5530f562000bf55f02cc8028703f507
Author: Dennis Zhou <dennis@kernel.org>
Date:   Wed Feb 13 11:10:30 2019 -0800

    percpu: set PCPU_BITMAP_BLOCK_SIZE to PAGE_SIZE
    
    Previously, block size was flexible based on the constraint that the
    GCD(PCPU_BITMAP_BLOCK_SIZE, PAGE_SIZE) > 1. However, this carried the
    overhead that keeping a floating number of populated free pages required
    scanning over the free regions of a chunk.
    
    Setting the block size to be fixed at PAGE_SIZE lets us know when an
    empty page becomes used as we will break a full contig_hint of a block.
    This means we no longer have to scan the whole chunk upon breaking a
    contig_hint which empty page management piggybacked off. A later patch
    takes advantage of this to optimize the allocation path by only scanning
    forward using the scan_hint introduced later too.
    
    Signed-off-by: Dennis Zhou <dennis@kernel.org>
    Reviewed-by: Peng Fan <peng.fan@nxp.com>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index b68d5df14731..3a2ff5c9192c 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -70,7 +70,7 @@ static struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp)
 	chunk->base_addr = page_address(pages);
 
 	spin_lock_irqsave(&pcpu_lock, flags);
-	pcpu_chunk_populated(chunk, 0, nr_pages, false);
+	pcpu_chunk_populated(chunk, 0, nr_pages);
 	spin_unlock_irqrestore(&pcpu_lock, flags);
 
 	pcpu_stats_chunk_alloc();

commit 1b046b445c0f856c3c1eed38a348bd87cc2dc730
Author: Peng Fan <peng.fan@nxp.com>
Date:   Sun Feb 24 13:13:50 2019 +0000

    percpu: km: no need to consider pcpu_group_offsets[0]
    
    percpu-km is used on UP systems which only has one group,
    so the group offset will be always 0, there is no need
    to subtract pcpu_group_offsets[0] when assigning chunk->base_addr
    
    Signed-off-by: Peng Fan <peng.fan@nxp.com>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Dennis Zhou <dennis@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 0f643dc2dc65..b68d5df14731 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -67,7 +67,7 @@ static struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp)
 		pcpu_set_page_chunk(nth_page(pages, i), chunk);
 
 	chunk->data = pages;
-	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
+	chunk->base_addr = page_address(pages);
 
 	spin_lock_irqsave(&pcpu_lock, flags);
 	pcpu_chunk_populated(chunk, 0, nr_pages, false);

commit 6ab7d47bcbf0144a8cb81536c2cead4cde18acfe
Author: Dennis Zhou <dennis@kernel.org>
Date:   Tue Dec 18 08:42:27 2018 -0800

    percpu: convert spin_lock_irq to spin_lock_irqsave.
    
    From Michael Cree:
      "Bisection lead to commit b38d08f3181c ("percpu: restructure
       locking") as being the cause of lockups at initial boot on
       the kernel built for generic Alpha.
    
       On a suggestion by Tejun Heo that:
    
       So, the only thing I can think of is that it's calling
       spin_unlock_irq() while irq handling isn't set up yet.
       Can you please try the followings?
    
       1. Convert all spin_[un]lock_irq() to
          spin_lock_irqsave/unlock_irqrestore()."
    
    Fixes: b38d08f3181c ("percpu: restructure locking")
    Reported-and-tested-by: Michael Cree <mcree@orcon.net.nz>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Dennis Zhou <dennis@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 38de70ab1a0d..0f643dc2dc65 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -50,6 +50,7 @@ static struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp)
 	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
 	struct pcpu_chunk *chunk;
 	struct page *pages;
+	unsigned long flags;
 	int i;
 
 	chunk = pcpu_alloc_chunk(gfp);
@@ -68,9 +69,9 @@ static struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp)
 	chunk->data = pages;
 	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
 
-	spin_lock_irq(&pcpu_lock);
+	spin_lock_irqsave(&pcpu_lock, flags);
 	pcpu_chunk_populated(chunk, 0, nr_pages, false);
-	spin_unlock_irq(&pcpu_lock);
+	spin_unlock_irqrestore(&pcpu_lock, flags);
 
 	pcpu_stats_chunk_alloc();
 	trace_percpu_create_chunk(chunk->base_addr);

commit 554fef1c39ee148623a496e04569dabb11463406
Author: Dennis Zhou <dennisszhou@gmail.com>
Date:   Fri Feb 16 12:09:58 2018 -0600

    percpu: allow select gfp to be passed to underlying allocators
    
    The prior patch added support for passing gfp flags through to the
    underlying allocators. This patch allows users to pass along gfp flags
    (currently only __GFP_NORETRY and __GFP_NOWARN) to the underlying
    allocators. This should allow users to decide if they are ok with
    failing allocations recovering in a more graceful way.
    
    Additionally, gfp passing was done as additional flags in the previous
    patch. Instead, change this to caller passed semantics. GFP_KERNEL is
    also removed as the default flag. It continues to be used for internally
    caused underlying percpu allocations.
    
    V2:
    Removed gfp_percpu_mask in favor of doing it inline.
    Removed GFP_KERNEL as a default flag for __alloc_percpu_gfp.
    
    Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 0d88d7bd5706..38de70ab1a0d 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -56,7 +56,7 @@ static struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp)
 	if (!chunk)
 		return NULL;
 
-	pages = alloc_pages(gfp | GFP_KERNEL, order_base_2(nr_pages));
+	pages = alloc_pages(gfp, order_base_2(nr_pages));
 	if (!pages) {
 		pcpu_free_chunk(chunk);
 		return NULL;

commit 47504ee04b9241548ae2c28be7d0b01cff3b7aa6
Author: Dennis Zhou <dennisszhou@gmail.com>
Date:   Fri Feb 16 12:07:19 2018 -0600

    percpu: add __GFP_NORETRY semantics to the percpu balancing path
    
    Percpu memory using the vmalloc area based chunk allocator lazily
    populates chunks by first requesting the full virtual address space
    required for the chunk and subsequently adding pages as allocations come
    through. To ensure atomic allocations can succeed, a workqueue item is
    used to maintain a minimum number of empty pages. In certain scenarios,
    such as reported in [1], it is possible that physical memory becomes
    quite scarce which can result in either a rather long time spent trying
    to find free pages or worse, a kernel panic.
    
    This patch adds support for __GFP_NORETRY and __GFP_NOWARN passing them
    through to the underlying allocators. This should prevent any
    unnecessary panics potentially caused by the workqueue item. The passing
    of gfp around is as additional flags rather than a full set of flags.
    The next patch will change these to caller passed semantics.
    
    V2:
    Added const modifier to gfp flags in the balance path.
    Removed an extra whitespace.
    
    [1] https://lkml.org/lkml/2018/2/12/551
    
    Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
    Suggested-by: Daniel Borkmann <daniel@iogearbox.net>
    Reported-by: syzbot+adb03f3f0bb57ce3acda@syzkaller.appspotmail.com
    Acked-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index d2a76642c4ae..0d88d7bd5706 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -34,7 +34,7 @@
 #include <linux/log2.h>
 
 static int pcpu_populate_chunk(struct pcpu_chunk *chunk,
-			       int page_start, int page_end)
+			       int page_start, int page_end, gfp_t gfp)
 {
 	return 0;
 }
@@ -45,18 +45,18 @@ static void pcpu_depopulate_chunk(struct pcpu_chunk *chunk,
 	/* nada */
 }
 
-static struct pcpu_chunk *pcpu_create_chunk(void)
+static struct pcpu_chunk *pcpu_create_chunk(gfp_t gfp)
 {
 	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
 	struct pcpu_chunk *chunk;
 	struct page *pages;
 	int i;
 
-	chunk = pcpu_alloc_chunk();
+	chunk = pcpu_alloc_chunk(gfp);
 	if (!chunk)
 		return NULL;
 
-	pages = alloc_pages(GFP_KERNEL, order_base_2(nr_pages));
+	pages = alloc_pages(gfp | GFP_KERNEL, order_base_2(nr_pages));
 	if (!pages) {
 		pcpu_free_chunk(chunk);
 		return NULL;

commit 40064aeca35c5c14789e2adcf3a1d7e5d4bd65f2
Author: Dennis Zhou (Facebook) <dennisszhou@gmail.com>
Date:   Wed Jul 12 11:27:32 2017 -0700

    percpu: replace area map allocator with bitmap
    
    The percpu memory allocator is experiencing scalability issues when
    allocating and freeing large numbers of counters as in BPF.
    Additionally, there is a corner case where iteration is triggered over
    all chunks if the contig_hint is the right size, but wrong alignment.
    
    This patch replaces the area map allocator with a basic bitmap allocator
    implementation. Each subsequent patch will introduce new features and
    replace full scanning functions with faster non-scanning options when
    possible.
    
    Implementation:
    This patchset removes the area map allocator in favor of a bitmap
    allocator backed by metadata blocks. The primary goal is to provide
    consistency in performance and memory footprint with a focus on small
    allocations (< 64 bytes). The bitmap removes the heavy memmove from the
    freeing critical path and provides a consistent memory footprint. The
    metadata blocks provide a bound on the amount of scanning required by
    maintaining a set of hints.
    
    In an effort to make freeing fast, the metadata is updated on the free
    path if the new free area makes a page free, a block free, or spans
    across blocks. This causes the chunk's contig hint to potentially be
    smaller than what it could allocate by up to the smaller of a page or a
    block. If the chunk's contig hint is contained within a block, a check
    occurs and the hint is kept accurate. Metadata is always kept accurate
    on allocation, so there will not be a situation where a chunk has a
    later contig hint than available.
    
    Evaluation:
    I have primarily done testing against a simple workload of allocation of
    1 million objects (2^20) of varying size. Deallocation was done by in
    order, alternating, and in reverse. These numbers were collected after
    rebasing ontop of a80099a152. I present the worst-case numbers here:
    
      Area Map Allocator:
    
            Object Size | Alloc Time (ms) | Free Time (ms)
            ----------------------------------------------
                  4B    |        310      |     4770
                 16B    |        557      |     1325
                 64B    |        436      |      273
                256B    |        776      |      131
               1024B    |       3280      |      122
    
      Bitmap Allocator:
    
            Object Size | Alloc Time (ms) | Free Time (ms)
            ----------------------------------------------
                  4B    |        490      |       70
                 16B    |        515      |       75
                 64B    |        610      |       80
                256B    |        950      |      100
               1024B    |       3520      |      200
    
    This data demonstrates the inability for the area map allocator to
    handle less than ideal situations. In the best case of reverse
    deallocation, the area map allocator was able to perform within range
    of the bitmap allocator. In the worst case situation, freeing took
    nearly 5 seconds for 1 million 4-byte objects. The bitmap allocator
    dramatically improves the consistency of the free path. The small
    allocations performed nearly identical regardless of the freeing
    pattern.
    
    While it does add to the allocation latency, the allocation scenario
    here is optimal for the area map allocator. The area map allocator runs
    into trouble when it is allocating in chunks where the latter half is
    full. It is difficult to replicate this, so I present a variant where
    the pages are second half filled. Freeing was done sequentially. Below
    are the numbers for this scenario:
    
      Area Map Allocator:
    
            Object Size | Alloc Time (ms) | Free Time (ms)
            ----------------------------------------------
                  4B    |       4118      |     4892
                 16B    |       1651      |     1163
                 64B    |        598      |      285
                256B    |        771      |      158
               1024B    |       3034      |      160
    
      Bitmap Allocator:
    
            Object Size | Alloc Time (ms) | Free Time (ms)
            ----------------------------------------------
                  4B    |        481      |       67
                 16B    |        506      |       69
                 64B    |        636      |       75
                256B    |        892      |       90
               1024B    |       3262      |      147
    
    The data shows a parabolic curve of performance for the area map
    allocator. This is due to the memmove operation being the dominant cost
    with the lower object sizes as more objects are packed in a chunk and at
    higher object sizes, the traversal of the chunk slots is the dominating
    cost. The bitmap allocator suffers this problem as well. The above data
    shows the inability to scale for the allocation path with the area map
    allocator and that the bitmap allocator demonstrates consistent
    performance in general.
    
    The second problem of additional scanning can result in the area map
    allocator completing in 52 minutes when trying to allocate 1 million
    4-byte objects with 8-byte alignment. The same workload takes
    approximately 16 seconds to complete for the bitmap allocator.
    
    V2:
    Fixed a bug in pcpu_alloc_first_chunk end_offset was setting the bitmap
    using bytes instead of bits.
    
    Added a comment to pcpu_cnt_pop_pages to explain bitmap_weight.
    
    Signed-off-by: Dennis Zhou <dennisszhou@gmail.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index eb58aa4c0997..d2a76642c4ae 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -69,7 +69,7 @@ static struct pcpu_chunk *pcpu_create_chunk(void)
 	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
 
 	spin_lock_irq(&pcpu_lock);
-	pcpu_chunk_populated(chunk, 0, nr_pages);
+	pcpu_chunk_populated(chunk, 0, nr_pages, false);
 	spin_unlock_irq(&pcpu_lock);
 
 	pcpu_stats_chunk_alloc();

commit e3efe3db932b55ed34ba32862f568abae32046d0
Author: Dennis Zhou <dennisz@fb.com>
Date:   Thu Jun 29 10:56:26 2017 -0400

    percpu: fix static checker warnings in pcpu_destroy_chunk
    
    From 5021b97f4026334d2c8dfad80797dd1028cddd73 Mon Sep 17 00:00:00 2001
    From: Dennis Zhou <dennisz@fb.com>
    Date: Thu, 29 Jun 2017 07:11:41 -0700
    
    Add NULL check in pcpu_destroy_chunk to correct static checker warnings.
    
    Signed-off-by: Dennis Zhou <dennisz@fb.com>
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 2b79e43c626f..eb58aa4c0997 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -82,10 +82,13 @@ static void pcpu_destroy_chunk(struct pcpu_chunk *chunk)
 {
 	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
 
+	if (!chunk)
+		return;
+
 	pcpu_stats_chunk_dealloc();
 	trace_percpu_destroy_chunk(chunk->base_addr);
 
-	if (chunk && chunk->data)
+	if (chunk->data)
 		__free_pages(chunk->data, order_base_2(nr_pages));
 	pcpu_free_chunk(chunk);
 }

commit df95e795a722892a9e0603ce4b9b62fab9f02967
Author: Dennis Zhou <dennisz@fb.com>
Date:   Mon Jun 19 19:28:32 2017 -0400

    percpu: add tracepoint support for percpu memory
    
    Add support for tracepoints to the following events: chunk allocation,
    chunk free, area allocation, area free, and area allocation failure.
    This should let us replay percpu memory requests and evaluate
    corresponding decisions.
    
    Signed-off-by: Dennis Zhou <dennisz@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 3bbfa0c9d069..2b79e43c626f 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -73,6 +73,7 @@ static struct pcpu_chunk *pcpu_create_chunk(void)
 	spin_unlock_irq(&pcpu_lock);
 
 	pcpu_stats_chunk_alloc();
+	trace_percpu_create_chunk(chunk->base_addr);
 
 	return chunk;
 }
@@ -82,6 +83,7 @@ static void pcpu_destroy_chunk(struct pcpu_chunk *chunk)
 	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
 
 	pcpu_stats_chunk_dealloc();
+	trace_percpu_destroy_chunk(chunk->base_addr);
 
 	if (chunk && chunk->data)
 		__free_pages(chunk->data, order_base_2(nr_pages));

commit 30a5b5367ef9d5c9055414e12ec2f02d9de2e70f
Author: Dennis Zhou <dennisz@fb.com>
Date:   Mon Jun 19 19:28:31 2017 -0400

    percpu: expose statistics about percpu memory via debugfs
    
    There is limited visibility into the use of percpu memory leaving us
    unable to reason about correctness of parameters and overall use of
    percpu memory. These counters and statistics aim to help understand
    basic statistics about percpu memory such as number of allocations over
    the lifetime, allocation sizes, and fragmentation.
    
    New Config: PERCPU_STATS
    
    Signed-off-by: Dennis Zhou <dennisz@fb.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index d66911ff42d9..3bbfa0c9d069 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -72,6 +72,8 @@ static struct pcpu_chunk *pcpu_create_chunk(void)
 	pcpu_chunk_populated(chunk, 0, nr_pages);
 	spin_unlock_irq(&pcpu_lock);
 
+	pcpu_stats_chunk_alloc();
+
 	return chunk;
 }
 
@@ -79,6 +81,8 @@ static void pcpu_destroy_chunk(struct pcpu_chunk *chunk)
 {
 	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
 
+	pcpu_stats_chunk_dealloc();
+
 	if (chunk && chunk->data)
 		__free_pages(chunk->data, order_base_2(nr_pages));
 	pcpu_free_chunk(chunk);

commit 870d4b12ad15d21c5db67b373bdc2f62cfe2ec64
Author: Joe Perches <joe@perches.com>
Date:   Thu Mar 17 14:19:53 2016 -0700

    mm: percpu: use pr_fmt to prefix output
    
    Use the normal mechanism to make the logging output consistently
    "percpu:" instead of a mix of "PERCPU:" and "percpu:"
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 0db94b748986..d66911ff42d9 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -95,7 +95,7 @@ static int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)
 
 	/* all units must be in a single group */
 	if (ai->nr_groups != 1) {
-		pr_crit("percpu: can't handle more than one groups\n");
+		pr_crit("can't handle more than one group\n");
 		return -EINVAL;
 	}
 
@@ -103,7 +103,7 @@ static int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)
 	alloc_pages = roundup_pow_of_two(nr_pages);
 
 	if (alloc_pages > nr_pages)
-		pr_warn("percpu: wasting %zu pages per chunk\n",
+		pr_warn("wasting %zu pages per chunk\n",
 			alloc_pages - nr_pages);
 
 	return 0;

commit 1170532bb49f9468aedabdc1d5a560e2521a2bcc
Author: Joe Perches <joe@perches.com>
Date:   Thu Mar 17 14:19:50 2016 -0700

    mm: convert printk(KERN_<LEVEL> to pr_<level>
    
    Most of the mm subsystem uses pr_<level> so make it consistent.
    
    Miscellanea:
    
     - Realign arguments
     - Add missing newline to format
     - kmemleak-test.c has a "kmemleak: " prefix added to the
       "Kmemleak testing" logging message via pr_fmt
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: Tejun Heo <tj@kernel.org>     [percpu]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 10e3d0b8a86d..0db94b748986 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -95,7 +95,7 @@ static int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)
 
 	/* all units must be in a single group */
 	if (ai->nr_groups != 1) {
-		printk(KERN_CRIT "percpu: can't handle more than one groups\n");
+		pr_crit("percpu: can't handle more than one groups\n");
 		return -EINVAL;
 	}
 
@@ -103,8 +103,8 @@ static int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)
 	alloc_pages = roundup_pow_of_two(nr_pages);
 
 	if (alloc_pages > nr_pages)
-		printk(KERN_WARNING "percpu: wasting %zu pages per chunk\n",
-		       alloc_pages - nr_pages);
+		pr_warn("percpu: wasting %zu pages per chunk\n",
+			alloc_pages - nr_pages);
 
 	return 0;
 }

commit b539b87fed37ffc16c89a6bc3beca2d7aed82e1c
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Sep 2 14:46:05 2014 -0400

    percpu: implmeent pcpu_nr_empty_pop_pages and chunk->nr_populated
    
    pcpu_nr_empty_pop_pages counts the number of empty populated pages
    across all chunks and chunk->nr_populated counts the number of
    populated pages in a chunk.  Both will be used to implement pre/async
    population for atomic allocations.
    
    pcpu_chunk_[de]populated() are added to update chunk->populated,
    chunk->nr_populated and pcpu_nr_empty_pop_pages together.  All
    successful chunk [de]populations should be followed by the
    corresponding pcpu_chunk_[de]populated() calls.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index e662b4947a65..10e3d0b8a86d 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -69,7 +69,7 @@ static struct pcpu_chunk *pcpu_create_chunk(void)
 	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
 
 	spin_lock_irq(&pcpu_lock);
-	bitmap_fill(chunk->populated, nr_pages);
+	pcpu_chunk_populated(chunk, 0, nr_pages);
 	spin_unlock_irq(&pcpu_lock);
 
 	return chunk;

commit b38d08f3181c5025a7ce84646494cc4748492a3b
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Sep 2 14:46:02 2014 -0400

    percpu: restructure locking
    
    At first, the percpu allocator required a sleepable context for both
    alloc and free paths and used pcpu_alloc_mutex to protect everything.
    Later, pcpu_lock was introduced to protect the index data structure so
    that the free path can be invoked from atomic contexts.  The
    conversion only updated what's necessary and left most of the
    allocation path under pcpu_alloc_mutex.
    
    The percpu allocator is planned to add support for atomic allocation
    and this patch restructures locking so that the coverage of
    pcpu_alloc_mutex is further reduced.
    
    * pcpu_alloc() now grab pcpu_alloc_mutex only while creating a new
      chunk and populating the allocated area.  Everything else is now
      protected soley by pcpu_lock.
    
      After this change, multiple instances of pcpu_extend_area_map() may
      race but the function already implements sufficient synchronization
      using pcpu_lock.
    
      This also allows multiple allocators to arrive at new chunk
      creation.  To avoid creating multiple empty chunks back-to-back, a
      new chunk is created iff there is no other empty chunk after
      grabbing pcpu_alloc_mutex.
    
    * pcpu_lock is now held while modifying chunk->populated bitmap.
      After this, all data structures are protected by pcpu_lock.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 67a971b7f745..e662b4947a65 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -68,7 +68,9 @@ static struct pcpu_chunk *pcpu_create_chunk(void)
 	chunk->data = pages;
 	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
 
+	spin_lock_irq(&pcpu_lock);
 	bitmap_fill(chunk->populated, nr_pages);
+	spin_unlock_irq(&pcpu_lock);
 
 	return chunk;
 }

commit a63d4ac4ab6094c051a5a240260d16117a7a2f86
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Sep 2 14:46:02 2014 -0400

    percpu: make percpu-km set chunk->populated bitmap properly
    
    percpu-km instantiates the whole chunk on creation and doesn't make
    use of chunk->populated bitmap and leaves it as zero.  While this
    currently doesn't cause any problem, the inconsistency makes it
    difficult to build further logic on top of chunk->populated.  This
    patch makes percpu-km fill chunk->populated on creation so that the
    bitmap is always consistent.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Christoph Lameter <cl@linux.com>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index a6e34bc1aff4..67a971b7f745 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -67,6 +67,9 @@ static struct pcpu_chunk *pcpu_create_chunk(void)
 
 	chunk->data = pages;
 	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
+
+	bitmap_fill(chunk->populated, nr_pages);
+
 	return chunk;
 }
 

commit a93ace487a339dccf7040be7fee08c3415188e14
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Sep 2 14:46:02 2014 -0400

    percpu: move region iterations out of pcpu_[de]populate_chunk()
    
    Previously, pcpu_[de]populate_chunk() were called with the range which
    may contain multiple target regions in it and
    pcpu_[de]populate_chunk() iterated over the regions.  This has the
    benefit of batching up cache flushes for all the regions; however,
    we're planning to add more bookkeeping logic around [de]population to
    support atomic allocations and this delegation of iterations gets in
    the way.
    
    This patch moves the region iterations out of
    pcpu_[de]populate_chunk() into its callers - pcpu_alloc() and
    pcpu_reclaim() - so that we can later add logic to track more states
    around them.  This change may make cache and tlb flushes more frequent
    but multi-region [de]populations are rare anyway and if this actually
    becomes a problem, it's not difficult to factor out cache flushes as
    separate callbacks which are directly invoked from percpu.c.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 9a9096f08867..a6e34bc1aff4 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -33,12 +33,14 @@
 
 #include <linux/log2.h>
 
-static int pcpu_populate_chunk(struct pcpu_chunk *chunk, int off, int size)
+static int pcpu_populate_chunk(struct pcpu_chunk *chunk,
+			       int page_start, int page_end)
 {
 	return 0;
 }
 
-static void pcpu_depopulate_chunk(struct pcpu_chunk *chunk, int off, int size)
+static void pcpu_depopulate_chunk(struct pcpu_chunk *chunk,
+				  int page_start, int page_end)
 {
 	/* nada */
 }

commit dca496451bddea9aa87b7510dc2eb413d1a19dfd
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Sep 2 14:46:01 2014 -0400

    percpu: move common parts out of pcpu_[de]populate_chunk()
    
    percpu-vm and percpu-km implement separate versions of
    pcpu_[de]populate_chunk() and some part which is or should be common
    are currently in the specific implementations.  Make the following
    changes.
    
    * Allocate area clearing is moved from the pcpu_populate_chunk()
      implementations to pcpu_alloc().  This makes percpu-km's version
      noop.
    
    * Quick exit tests in pcpu_[de]populate_chunk() of percpu-vm are moved
      to their respective callers so that they are applied to percpu-km
      too.  This doesn't make any meaningful difference as both functions
      are noop for percpu-km; however, this is more consistent and will
      help implementing atomic allocation support.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 89633fefc6a2..9a9096f08867 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -35,11 +35,6 @@
 
 static int pcpu_populate_chunk(struct pcpu_chunk *chunk, int off, int size)
 {
-	unsigned int cpu;
-
-	for_each_possible_cpu(cpu)
-		memset((void *)pcpu_chunk_addr(chunk, cpu, 0) + off, 0, size);
-
 	return 0;
 }
 

commit fc1481a956181d0360d3eb129965302489895a1b
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 10 10:49:37 2010 +0200

    percpu: clear memory allocated with the km allocator
    
    Percpu allocator should clear memory before returning it but the km
    allocator forgot to do it.  Fix it.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index 7037bc73bfa4..89633fefc6a2 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -35,7 +35,11 @@
 
 static int pcpu_populate_chunk(struct pcpu_chunk *chunk, int off, int size)
 {
-	/* noop */
+	unsigned int cpu;
+
+	for_each_possible_cpu(cpu)
+		memset((void *)pcpu_chunk_addr(chunk, cpu, 0) + off, 0, size);
+
 	return 0;
 }
 

commit bbddff0545878a8649c091a9dd7c43ce91516734
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Sep 3 18:22:48 2010 +0200

    percpu: use percpu allocator on UP too
    
    On UP, percpu allocations were redirected to kmalloc.  This has the
    following problems.
    
    * For certain amount of allocations (determined by
      PERCPU_DYNAMIC_EARLY_SLOTS and PERCPU_DYNAMIC_EARLY_SIZE), percpu
      allocator can be used before the usual kernel memory allocator is
      brought online.  On SMP, this is used to initialize the kernel
      memory allocator.
    
    * percpu allocator honors alignment upto PAGE_SIZE but kmalloc()
      doesn't.  For example, workqueue makes use of larger alignments for
      cpu_workqueues.
    
    Currently, users of percpu allocators need to handle UP differently,
    which is somewhat fragile and ugly.  Other than small amount of
    memory, there isn't much to lose by enabling percpu allocator on UP.
    It can simply use kernel memory based chunk allocation which was added
    for SMP archs w/o MMUs.
    
    This patch removes mm/percpu_up.c, builds mm/percpu.c on UP too and
    makes UP build use percpu-km.  As percpu addresses and kernel
    addresses are always identity mapped and static percpu variables don't
    need any special treatment, nothing is arch dependent and mm/percpu.c
    implements generic setup_per_cpu_areas() for UP.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
index df680855540a..7037bc73bfa4 100644
--- a/mm/percpu-km.c
+++ b/mm/percpu-km.c
@@ -27,7 +27,7 @@
  *   chunk size is not aligned.  percpu-km code will whine about it.
  */
 
-#ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK
+#if defined(CONFIG_SMP) && defined(CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK)
 #error "contiguous percpu allocation is incompatible with paged first chunk"
 #endif
 

commit b0c9778b1d07ed3aa7e411db201275553527b1b1
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Apr 9 18:57:01 2010 +0900

    percpu: implement kernel memory based chunk allocation
    
    Implement an alternate percpu chunk management based on kernel memeory
    for nommu SMP architectures.  Instead of mapping into vmalloc area,
    chunks are allocated as a contiguous kernel memory using
    alloc_pages().  As such, percpu allocator on nommu will have the
    following restrictions.
    
    * It can't fill chunks on-demand page-by-page.  It has to allocate
      each chunk fully upfront.
    
    * It can't support sparse chunk for NUMA configurations.  SMP w/o mmu
      is crazy enough.  Let's hope no one does NUMA w/o mmu.  :-P
    
    * If chunk size isn't power-of-two multiple of PAGE_SIZE, the
      unaligned amount will be wasted on each chunk.  So, archs which use
      this better align chunk size.
    
    For instructions on how to use this, read the comment on top of
    mm/percpu-km.c.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: David Howells <dhowells@redhat.com>
    Cc: Graff Yang <graff.yang@gmail.com>
    Cc: Sonic Zhang <sonic.adi@gmail.com>

diff --git a/mm/percpu-km.c b/mm/percpu-km.c
new file mode 100644
index 000000000000..df680855540a
--- /dev/null
+++ b/mm/percpu-km.c
@@ -0,0 +1,104 @@
+/*
+ * mm/percpu-km.c - kernel memory based chunk allocation
+ *
+ * Copyright (C) 2010		SUSE Linux Products GmbH
+ * Copyright (C) 2010		Tejun Heo <tj@kernel.org>
+ *
+ * This file is released under the GPLv2.
+ *
+ * Chunks are allocated as a contiguous kernel memory using gfp
+ * allocation.  This is to be used on nommu architectures.
+ *
+ * To use percpu-km,
+ *
+ * - define CONFIG_NEED_PER_CPU_KM from the arch Kconfig.
+ *
+ * - CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK must not be defined.  It's
+ *   not compatible with PER_CPU_KM.  EMBED_FIRST_CHUNK should work
+ *   fine.
+ *
+ * - NUMA is not supported.  When setting up the first chunk,
+ *   @cpu_distance_fn should be NULL or report all CPUs to be nearer
+ *   than or at LOCAL_DISTANCE.
+ *
+ * - It's best if the chunk size is power of two multiple of
+ *   PAGE_SIZE.  Because each chunk is allocated as a contiguous
+ *   kernel memory block using alloc_pages(), memory will be wasted if
+ *   chunk size is not aligned.  percpu-km code will whine about it.
+ */
+
+#ifdef CONFIG_NEED_PER_CPU_PAGE_FIRST_CHUNK
+#error "contiguous percpu allocation is incompatible with paged first chunk"
+#endif
+
+#include <linux/log2.h>
+
+static int pcpu_populate_chunk(struct pcpu_chunk *chunk, int off, int size)
+{
+	/* noop */
+	return 0;
+}
+
+static void pcpu_depopulate_chunk(struct pcpu_chunk *chunk, int off, int size)
+{
+	/* nada */
+}
+
+static struct pcpu_chunk *pcpu_create_chunk(void)
+{
+	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
+	struct pcpu_chunk *chunk;
+	struct page *pages;
+	int i;
+
+	chunk = pcpu_alloc_chunk();
+	if (!chunk)
+		return NULL;
+
+	pages = alloc_pages(GFP_KERNEL, order_base_2(nr_pages));
+	if (!pages) {
+		pcpu_free_chunk(chunk);
+		return NULL;
+	}
+
+	for (i = 0; i < nr_pages; i++)
+		pcpu_set_page_chunk(nth_page(pages, i), chunk);
+
+	chunk->data = pages;
+	chunk->base_addr = page_address(pages) - pcpu_group_offsets[0];
+	return chunk;
+}
+
+static void pcpu_destroy_chunk(struct pcpu_chunk *chunk)
+{
+	const int nr_pages = pcpu_group_sizes[0] >> PAGE_SHIFT;
+
+	if (chunk && chunk->data)
+		__free_pages(chunk->data, order_base_2(nr_pages));
+	pcpu_free_chunk(chunk);
+}
+
+static struct page *pcpu_addr_to_page(void *addr)
+{
+	return virt_to_page(addr);
+}
+
+static int __init pcpu_verify_alloc_info(const struct pcpu_alloc_info *ai)
+{
+	size_t nr_pages, alloc_pages;
+
+	/* all units must be in a single group */
+	if (ai->nr_groups != 1) {
+		printk(KERN_CRIT "percpu: can't handle more than one groups\n");
+		return -EINVAL;
+	}
+
+	nr_pages = (ai->groups[0].nr_units * ai->unit_size) >> PAGE_SHIFT;
+	alloc_pages = roundup_pow_of_two(nr_pages);
+
+	if (alloc_pages > nr_pages)
+		printk(KERN_WARNING "percpu: wasting %zu pages per chunk\n",
+		       alloc_pages - nr_pages);
+
+	return 0;
+}
