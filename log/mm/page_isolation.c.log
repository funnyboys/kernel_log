commit aa218795cb5fd583c94fc838dc76b7379dc4976a
Author: David Hildenbrand <david@redhat.com>
Date:   Thu May 7 16:01:30 2020 +0200

    mm: Allow to offline unmovable PageOffline() pages via MEM_GOING_OFFLINE
    
    virtio-mem wants to allow to offline memory blocks of which some parts
    were unplugged (allocated via alloc_contig_range()), especially, to later
    offline and remove completely unplugged memory blocks. The important part
    is that PageOffline() has to remain set until the section is offline, so
    these pages will never get accessed (e.g., when dumping). The pages should
    not be handed back to the buddy (which would require clearing PageOffline()
    and result in issues if offlining fails and the pages are suddenly in the
    buddy).
    
    Let's allow to do that by allowing to isolate any PageOffline() page
    when offlining. This way, we can reach the memory hotplug notifier
    MEM_GOING_OFFLINE, where the driver can signal that he is fine with
    offlining this page by dropping its reference count. PageOffline() pages
    with a reference count of 0 can then be skipped when offlining the
    pages (like if they were free, however they are not in the buddy).
    
    Anybody who uses PageOffline() pages and does not agree to offline them
    (e.g., Hyper-V balloon, XEN balloon, VMWare balloon for 2MB pages) will not
    decrement the reference count and make offlining fail when trying to
    migrate such an unmovable page. So there should be no observable change.
    Same applies to balloon compaction users (movable PageOffline() pages), the
    pages will simply be migrated.
    
    Note 1: If offlining fails, a driver has to increment the reference
            count again in MEM_CANCEL_OFFLINE.
    
    Note 2: A driver that makes use of this has to be aware that re-onlining
            the memory block has to be handled by hooking into onlining code
            (online_page_callback_t), resetting the page PageOffline() and
            not giving them to the buddy.
    
    Reviewed-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Tested-by: Pankaj Gupta <pankaj.gupta.linux@gmail.com>
    Acked-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Pavel Tatashin <pavel.tatashin@microsoft.com>
    Cc: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Anthony Yznaga <anthony.yznaga@oracle.com>
    Cc: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Link: https://lore.kernel.org/r/20200507140139.17083-7-david@redhat.com
    Signed-off-by: Michael S. Tsirkin <mst@redhat.com>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 2c11a38d6e87..f6d07c5f0d34 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -151,6 +151,7 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  *			a bit mask)
  *			MEMORY_OFFLINE - isolate to offline (!allocate) memory
  *					 e.g., skip over PageHWPoison() pages
+ *					 and PageOffline() pages.
  *			REPORT_FAILURE - report details about the failure to
  *			isolate the range
  *
@@ -259,6 +260,14 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 		else if ((flags & MEMORY_OFFLINE) && PageHWPoison(page))
 			/* A HWPoisoned page cannot be also PageBuddy */
 			pfn++;
+		else if ((flags & MEMORY_OFFLINE) && PageOffline(page) &&
+			 !page_count(page))
+			/*
+			 * The responsible driver agreed to skip PageOffline()
+			 * pages when offlining memory by dropping its
+			 * reference in MEM_GOING_OFFLINE.
+			 */
+			pfn++;
 		else
 			break;
 	}

commit 624f58d8f4639676d2fa1238425ab0148d501c4a
Author: Alexander Duyck <alexander.h.duyck@linux.intel.com>
Date:   Mon Apr 6 20:04:53 2020 -0700

    mm: add function __putback_isolated_page
    
    There are cases where we would benefit from avoiding having to go through
    the allocation and free cycle to return an isolated page.
    
    Examples for this might include page poisoning in which we isolate a page
    and then put it back in the free list without ever having actually
    allocated it.
    
    This will enable us to also avoid notifiers for the future free page
    reporting which will need to avoid retriggering page reporting when
    returning pages that have been reported on.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Acked-by: David Hildenbrand <david@redhat.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Michael S. Tsirkin <mst@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Nitesh Narayan Lal <nitesh@redhat.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Pankaj Gupta <pagupta@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Wei Wang <wei.w.wang@intel.com>
    Cc: Yang Zhang <yang.zhang.wz@gmail.com>
    Cc: wei qi <weiqi4@huawei.com>
    Link: http://lkml.kernel.org/r/20200211224624.29318.89287.stgit@localhost.localdomain
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index a9fd7c740c23..2c11a38d6e87 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -117,13 +117,11 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 		__mod_zone_freepage_state(zone, nr_pages, migratetype);
 	}
 	set_pageblock_migratetype(page, migratetype);
+	if (isolated_page)
+		__putback_isolated_page(page, order, migratetype);
 	zone->nr_isolate_pageblock--;
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
-	if (isolated_page) {
-		post_alloc_hook(page, order, __GFP_MOVABLE);
-		__free_pages(page, order);
-	}
 }
 
 static inline struct page *

commit 3d680bdf60a5bade3e8cbd049927e7f8b1d3fe97
Author: Qian Cai <cai@lca.pw>
Date:   Thu Jan 30 22:15:01 2020 -0800

    mm/page_isolation: fix potential warning from user
    
    It makes sense to call the WARN_ON_ONCE(zone_idx(zone) == ZONE_MOVABLE)
    from start_isolate_page_range(), but should avoid triggering it from
    userspace, i.e, from is_mem_section_removable() because it could crash
    the system by a non-root user if warn_on_panic is set.
    
    While at it, simplify the code a bit by removing an unnecessary jump
    label.
    
    Link: http://lkml.kernel.org/r/20200120163915.1469-1-cai@lca.pw
    Signed-off-by: Qian Cai <cai@lca.pw>
    Suggested-by: Michal Hocko <mhocko@kernel.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index e70586523ca3..a9fd7c740c23 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -54,14 +54,18 @@ static int set_migratetype_isolate(struct page *page, int migratetype, int isol_
 
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
-	if (!ret)
+	if (!ret) {
 		drain_all_pages(zone);
-	else if ((isol_flags & REPORT_FAILURE) && unmovable)
-		/*
-		 * printk() with zone->lock held will guarantee to trigger a
-		 * lockdep splat, so defer it here.
-		 */
-		dump_page(unmovable, "unmovable page");
+	} else {
+		WARN_ON_ONCE(zone_idx(zone) == ZONE_MOVABLE);
+
+		if ((isol_flags & REPORT_FAILURE) && unmovable)
+			/*
+			 * printk() with zone->lock held will likely trigger a
+			 * lockdep splat, so defer it here.
+			 */
+			dump_page(unmovable, "unmovable page");
+	}
 
 	return ret;
 }

commit 4a55c0474a92d5c418bcbbe122368de0910aeac2
Author: Qian Cai <cai@lca.pw>
Date:   Thu Jan 30 22:14:57 2020 -0800

    mm/hotplug: silence a lockdep splat with printk()
    
    It is not that hard to trigger lockdep splats by calling printk from
    under zone->lock.  Most of them are false positives caused by lock
    chains introduced early in the boot process and they do not cause any
    real problems (although most of the early boot lock dependencies could
    happen after boot as well).  There are some console drivers which do
    allocate from the printk context as well and those should be fixed.  In
    any case, false positives are not that trivial to workaround and it is
    far from optimal to lose lockdep functionality for something that is a
    non-issue.
    
    So change has_unmovable_pages() so that it no longer calls dump_page()
    itself - instead it returns a "struct page *" of the unmovable page back
    to the caller so that in the case of a has_unmovable_pages() failure,
    the caller can call dump_page() after releasing zone->lock.  Also, make
    dump_page() is able to report a CMA page as well, so the reason string
    from has_unmovable_pages() can be removed.
    
    Even though has_unmovable_pages doesn't hold any reference to the
    returned page this should be reasonably safe for the purpose of
    reporting the page (dump_page) because it cannot be hotremoved in the
    context of memory unplug.  The state of the page might change but that
    is the case even with the existing code as zone->lock only plays role
    for free pages.
    
    While at it, remove a similar but unnecessary debug-only printk() as
    well.  A sample of one of those lockdep splats is,
    
      WARNING: possible circular locking dependency detected
      ------------------------------------------------------
      test.sh/8653 is trying to acquire lock:
      ffffffff865a4460 (console_owner){-.-.}, at:
      console_unlock+0x207/0x750
    
      but task is already holding lock:
      ffff88883fff3c58 (&(&zone->lock)->rlock){-.-.}, at:
      __offline_isolated_pages+0x179/0x3e0
    
      which lock already depends on the new lock.
    
      the existing dependency chain (in reverse order) is:
    
      -> #3 (&(&zone->lock)->rlock){-.-.}:
             __lock_acquire+0x5b3/0xb40
             lock_acquire+0x126/0x280
             _raw_spin_lock+0x2f/0x40
             rmqueue_bulk.constprop.21+0xb6/0x1160
             get_page_from_freelist+0x898/0x22c0
             __alloc_pages_nodemask+0x2f3/0x1cd0
             alloc_pages_current+0x9c/0x110
             allocate_slab+0x4c6/0x19c0
             new_slab+0x46/0x70
             ___slab_alloc+0x58b/0x960
             __slab_alloc+0x43/0x70
             __kmalloc+0x3ad/0x4b0
             __tty_buffer_request_room+0x100/0x250
             tty_insert_flip_string_fixed_flag+0x67/0x110
             pty_write+0xa2/0xf0
             n_tty_write+0x36b/0x7b0
             tty_write+0x284/0x4c0
             __vfs_write+0x50/0xa0
             vfs_write+0x105/0x290
             redirected_tty_write+0x6a/0xc0
             do_iter_write+0x248/0x2a0
             vfs_writev+0x106/0x1e0
             do_writev+0xd4/0x180
             __x64_sys_writev+0x45/0x50
             do_syscall_64+0xcc/0x76c
             entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
      -> #2 (&(&port->lock)->rlock){-.-.}:
             __lock_acquire+0x5b3/0xb40
             lock_acquire+0x126/0x280
             _raw_spin_lock_irqsave+0x3a/0x50
             tty_port_tty_get+0x20/0x60
             tty_port_default_wakeup+0xf/0x30
             tty_port_tty_wakeup+0x39/0x40
             uart_write_wakeup+0x2a/0x40
             serial8250_tx_chars+0x22e/0x440
             serial8250_handle_irq.part.8+0x14a/0x170
             serial8250_default_handle_irq+0x5c/0x90
             serial8250_interrupt+0xa6/0x130
             __handle_irq_event_percpu+0x78/0x4f0
             handle_irq_event_percpu+0x70/0x100
             handle_irq_event+0x5a/0x8b
             handle_edge_irq+0x117/0x370
             do_IRQ+0x9e/0x1e0
             ret_from_intr+0x0/0x2a
             cpuidle_enter_state+0x156/0x8e0
             cpuidle_enter+0x41/0x70
             call_cpuidle+0x5e/0x90
             do_idle+0x333/0x370
             cpu_startup_entry+0x1d/0x1f
             start_secondary+0x290/0x330
             secondary_startup_64+0xb6/0xc0
    
      -> #1 (&port_lock_key){-.-.}:
             __lock_acquire+0x5b3/0xb40
             lock_acquire+0x126/0x280
             _raw_spin_lock_irqsave+0x3a/0x50
             serial8250_console_write+0x3e4/0x450
             univ8250_console_write+0x4b/0x60
             console_unlock+0x501/0x750
             vprintk_emit+0x10d/0x340
             vprintk_default+0x1f/0x30
             vprintk_func+0x44/0xd4
             printk+0x9f/0xc5
    
      -> #0 (console_owner){-.-.}:
             check_prev_add+0x107/0xea0
             validate_chain+0x8fc/0x1200
             __lock_acquire+0x5b3/0xb40
             lock_acquire+0x126/0x280
             console_unlock+0x269/0x750
             vprintk_emit+0x10d/0x340
             vprintk_default+0x1f/0x30
             vprintk_func+0x44/0xd4
             printk+0x9f/0xc5
             __offline_isolated_pages.cold.52+0x2f/0x30a
             offline_isolated_pages_cb+0x17/0x30
             walk_system_ram_range+0xda/0x160
             __offline_pages+0x79c/0xa10
             offline_pages+0x11/0x20
             memory_subsys_offline+0x7e/0xc0
             device_offline+0xd5/0x110
             state_store+0xc6/0xe0
             dev_attr_store+0x3f/0x60
             sysfs_kf_write+0x89/0xb0
             kernfs_fop_write+0x188/0x240
             __vfs_write+0x50/0xa0
             vfs_write+0x105/0x290
             ksys_write+0xc6/0x160
             __x64_sys_write+0x43/0x50
             do_syscall_64+0xcc/0x76c
             entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
      other info that might help us debug this:
    
      Chain exists of:
        console_owner --> &(&port->lock)->rlock --> &(&zone->lock)->rlock
    
       Possible unsafe locking scenario:
    
             CPU0                    CPU1
             ----                    ----
        lock(&(&zone->lock)->rlock);
                                     lock(&(&port->lock)->rlock);
                                     lock(&(&zone->lock)->rlock);
        lock(console_owner);
    
       *** DEADLOCK ***
    
      9 locks held by test.sh/8653:
       #0: ffff88839ba7d408 (sb_writers#4){.+.+}, at:
      vfs_write+0x25f/0x290
       #1: ffff888277618880 (&of->mutex){+.+.}, at:
      kernfs_fop_write+0x128/0x240
       #2: ffff8898131fc218 (kn->count#115){.+.+}, at:
      kernfs_fop_write+0x138/0x240
       #3: ffffffff86962a80 (device_hotplug_lock){+.+.}, at:
      lock_device_hotplug_sysfs+0x16/0x50
       #4: ffff8884374f4990 (&dev->mutex){....}, at:
      device_offline+0x70/0x110
       #5: ffffffff86515250 (cpu_hotplug_lock.rw_sem){++++}, at:
      __offline_pages+0xbf/0xa10
       #6: ffffffff867405f0 (mem_hotplug_lock.rw_sem){++++}, at:
      percpu_down_write+0x87/0x2f0
       #7: ffff88883fff3c58 (&(&zone->lock)->rlock){-.-.}, at:
      __offline_isolated_pages+0x179/0x3e0
       #8: ffffffff865a4920 (console_lock){+.+.}, at:
      vprintk_emit+0x100/0x340
    
      stack backtrace:
      Hardware name: HPE ProLiant DL560 Gen10/ProLiant DL560 Gen10,
      BIOS U34 05/21/2019
      Call Trace:
       dump_stack+0x86/0xca
       print_circular_bug.cold.31+0x243/0x26e
       check_noncircular+0x29e/0x2e0
       check_prev_add+0x107/0xea0
       validate_chain+0x8fc/0x1200
       __lock_acquire+0x5b3/0xb40
       lock_acquire+0x126/0x280
       console_unlock+0x269/0x750
       vprintk_emit+0x10d/0x340
       vprintk_default+0x1f/0x30
       vprintk_func+0x44/0xd4
       printk+0x9f/0xc5
       __offline_isolated_pages.cold.52+0x2f/0x30a
       offline_isolated_pages_cb+0x17/0x30
       walk_system_ram_range+0xda/0x160
       __offline_pages+0x79c/0xa10
       offline_pages+0x11/0x20
       memory_subsys_offline+0x7e/0xc0
       device_offline+0xd5/0x110
       state_store+0xc6/0xe0
       dev_attr_store+0x3f/0x60
       sysfs_kf_write+0x89/0xb0
       kernfs_fop_write+0x188/0x240
       __vfs_write+0x50/0xa0
       vfs_write+0x105/0x290
       ksys_write+0xc6/0x160
       __x64_sys_write+0x43/0x50
       do_syscall_64+0xcc/0x76c
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    Link: http://lkml.kernel.org/r/20200117181200.20299-1-cai@lca.pw
    Signed-off-by: Qian Cai <cai@lca.pw>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 1f8b9dfecbe8..e70586523ca3 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -17,6 +17,7 @@
 
 static int set_migratetype_isolate(struct page *page, int migratetype, int isol_flags)
 {
+	struct page *unmovable = NULL;
 	struct zone *zone;
 	unsigned long flags;
 	int ret = -EBUSY;
@@ -37,7 +38,8 @@ static int set_migratetype_isolate(struct page *page, int migratetype, int isol_
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, migratetype, isol_flags)) {
+	unmovable = has_unmovable_pages(zone, page, migratetype, isol_flags);
+	if (!unmovable) {
 		unsigned long nr_pages;
 		int mt = get_pageblock_migratetype(page);
 
@@ -54,6 +56,13 @@ static int set_migratetype_isolate(struct page *page, int migratetype, int isol_
 	spin_unlock_irqrestore(&zone->lock, flags);
 	if (!ret)
 		drain_all_pages(zone);
+	else if ((isol_flags & REPORT_FAILURE) && unmovable)
+		/*
+		 * printk() with zone->lock held will guarantee to trigger a
+		 * lockdep splat, so defer it here.
+		 */
+		dump_page(unmovable, "unmovable page");
+
 	return ret;
 }
 

commit fe4c86c916d9151113372369f322e7436167e6f3
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jan 30 22:14:04 2020 -0800

    mm: remove "count" parameter from has_unmovable_pages()
    
    Now that the memory isolate notifier is gone, the parameter is always 0.
    Drop it and cleanup has_unmovable_pages().
    
    Link: http://lkml.kernel.org/r/20191114131911.11783-3-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Wei Yang <richardw.yang@linux.intel.com>
    Cc: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Arun KS <arunks@codeaurora.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 21af88b718aa..1f8b9dfecbe8 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -37,7 +37,7 @@ static int set_migratetype_isolate(struct page *page, int migratetype, int isol_
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, 0, migratetype, isol_flags)) {
+	if (!has_unmovable_pages(zone, page, migratetype, isol_flags)) {
 		unsigned long nr_pages;
 		int mt = get_pageblock_migratetype(page);
 

commit 3f9903b9ca5e981b5862d7b10086d0e8caa20298
Author: David Hildenbrand <david@redhat.com>
Date:   Thu Jan 30 22:14:01 2020 -0800

    mm: remove the memory isolate notifier
    
    Luckily, we have no users left, so we can get rid of it.  Cleanup
    set_migratetype_isolate() a little bit.
    
    Link: http://lkml.kernel.org/r/20191114131911.11783-2-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: "Rafael J. Wysocki" <rafael@kernel.org>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 04ee1663cdbe..21af88b718aa 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -18,9 +18,7 @@
 static int set_migratetype_isolate(struct page *page, int migratetype, int isol_flags)
 {
 	struct zone *zone;
-	unsigned long flags, pfn;
-	struct memory_isolate_notify arg;
-	int notifier_ret;
+	unsigned long flags;
 	int ret = -EBUSY;
 
 	zone = page_zone(page);
@@ -35,41 +33,11 @@ static int set_migratetype_isolate(struct page *page, int migratetype, int isol_
 	if (is_migrate_isolate_page(page))
 		goto out;
 
-	pfn = page_to_pfn(page);
-	arg.start_pfn = pfn;
-	arg.nr_pages = pageblock_nr_pages;
-	arg.pages_found = 0;
-
-	/*
-	 * It may be possible to isolate a pageblock even if the
-	 * migratetype is not MIGRATE_MOVABLE. The memory isolation
-	 * notifier chain is used by balloon drivers to return the
-	 * number of pages in a range that are held by the balloon
-	 * driver to shrink memory. If all the pages are accounted for
-	 * by balloons, are free, or on the LRU, isolation can continue.
-	 * Later, for example, when memory hotplug notifier runs, these
-	 * pages reported as "can be isolated" should be isolated(freed)
-	 * by the balloon driver through the memory notifier chain.
-	 */
-	notifier_ret = memory_isolate_notify(MEM_ISOLATE_COUNT, &arg);
-	notifier_ret = notifier_to_errno(notifier_ret);
-	if (notifier_ret)
-		goto out;
 	/*
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype,
-				 isol_flags))
-		ret = 0;
-
-	/*
-	 * immobile means "not-on-lru" pages. If immobile is larger than
-	 * removable-by-driver pages reported by notifier, we'll fail.
-	 */
-
-out:
-	if (!ret) {
+	if (!has_unmovable_pages(zone, page, 0, migratetype, isol_flags)) {
 		unsigned long nr_pages;
 		int mt = get_pageblock_migratetype(page);
 
@@ -79,8 +47,10 @@ static int set_migratetype_isolate(struct page *page, int migratetype, int isol_
 									NULL);
 
 		__mod_zone_freepage_state(zone, -nr_pages, mt);
+		ret = 0;
 	}
 
+out:
 	spin_unlock_irqrestore(&zone->lock, flags);
 	if (!ret)
 		drain_all_pages(zone);

commit 756d25be457fc5497da0ceee0f3d0c9eb4d8535d
Author: David Hildenbrand <david@redhat.com>
Date:   Sat Nov 30 17:54:07 2019 -0800

    mm/page_isolation.c: convert SKIP_HWPOISON to MEMORY_OFFLINE
    
    We have two types of users of page isolation:
    
     1. Memory offlining:  Offline memory so it can be unplugged. Memory
                           won't be touched.
    
     2. Memory allocation: Allocate memory (e.g., alloc_contig_range()) to
                           become the owner of the memory and make use of
                           it.
    
    For example, in case we want to offline memory, we can ignore (skip
    over) PageHWPoison() pages, as the memory won't get used.  We can allow
    to offline memory.  In contrast, we don't want to allow to allocate such
    memory.
    
    Let's generalize the approach so we can special case other types of
    pages we want to skip over in case we offline memory.  While at it, also
    pass the same flags to test_pages_isolated().
    
    Link: http://lkml.kernel.org/r/20191021172353.3056-3-david@redhat.com
    Signed-off-by: David Hildenbrand <david@redhat.com>
    Suggested-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Oscar Salvador <osalvador@suse.de>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: David Hildenbrand <david@redhat.com>
    Cc: Pingfan Liu <kernelfans@gmail.com>
    Cc: Qian Cai <cai@lca.pw>
    Cc: Pavel Tatashin <pasha.tatashin@soleen.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Alexander Duyck <alexander.h.duyck@linux.intel.com>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Pavel Tatashin <pavel.tatashin@microsoft.com>
    Cc: Wei Yang <richard.weiyang@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 89c19c0feadb..04ee1663cdbe 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -168,7 +168,8 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  * @migratetype:	Migrate type to set in error recovery.
  * @flags:		The following flags are allowed (they can be combined in
  *			a bit mask)
- *			SKIP_HWPOISON - ignore hwpoison pages
+ *			MEMORY_OFFLINE - isolate to offline (!allocate) memory
+ *					 e.g., skip over PageHWPoison() pages
  *			REPORT_FAILURE - report details about the failure to
  *			isolate the range
  *
@@ -257,7 +258,7 @@ void undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
  */
 static unsigned long
 __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
-				  bool skip_hwpoisoned_pages)
+				  int flags)
 {
 	struct page *page;
 
@@ -274,7 +275,7 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 			 * simple way to verify that as VM_BUG_ON(), though.
 			 */
 			pfn += 1 << page_order(page);
-		else if (skip_hwpoisoned_pages && PageHWPoison(page))
+		else if ((flags & MEMORY_OFFLINE) && PageHWPoison(page))
 			/* A HWPoisoned page cannot be also PageBuddy */
 			pfn++;
 		else
@@ -286,7 +287,7 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 
 /* Caller should ensure that requested range is in a single zone */
 int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
-			bool skip_hwpoisoned_pages)
+			int isol_flags)
 {
 	unsigned long pfn, flags;
 	struct page *page;
@@ -308,8 +309,7 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	/* Check all pages are free or marked as ISOLATED */
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
-	pfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn,
-						skip_hwpoisoned_pages);
+	pfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn, isol_flags);
 	spin_unlock_irqrestore(&zone->lock, flags);
 
 	trace_test_pages_isolated(start_pfn, end_pfn, pfn);

commit 1fcf0a561cd09d7fb7f7afa2ddfe05f72f32050e
Author: Pingfan Liu <kernelfans@gmail.com>
Date:   Thu Jul 11 20:54:49 2019 -0700

    mm/page_isolation.c: change the prototype of undo_isolate_page_range()
    
    undo_isolate_page_range() never fails, so no need to return value.
    
    Link: http://lkml.kernel.org/r/1562075604-8979-1-git-send-email-kernelfans@gmail.com
    Signed-off-by: Pingfan Liu <kernelfans@gmail.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Reviewed-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Qian Cai <cai@lca.pw>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index e3638a5bafff..89c19c0feadb 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -230,7 +230,7 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 /*
  * Make isolated pages available again.
  */
-int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
+void undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 			    unsigned migratetype)
 {
 	unsigned long pfn;
@@ -247,7 +247,6 @@ int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 			continue;
 		unset_migratetype_isolate(page, migratetype);
 	}
-	return 0;
 }
 /*
  * Test all pages in the range is free(means isolated) or not.

commit 5e65af19e89ac33dc83e1869c78b33ed7099469b
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Mon May 13 17:17:50 2019 -0700

    mm/page_isolation.c: remove redundant pfn_valid_within() in __first_valid_page()
    
    pfn_valid_within() calls pfn_valid() when CONFIG_HOLES_IN_ZONE making it
    redundant for both definitions (w/wo CONFIG_MEMORY_HOTPLUG) of the helper
    pfn_to_online_page() which either calls pfn_valid() or pfn_valid_within().
    pfn_valid_within() being 1 when !CONFIG_HOLES_IN_ZONE is irrelevant
    either way.  This does not change functionality.
    
    Link: http://lkml.kernel.org/r/1553141595-26907-1-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: Zi Yan <ziy@nvidia.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 019280712e1b..e3638a5bafff 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -151,8 +151,6 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
 	for (i = 0; i < nr_pages; i++) {
 		struct page *page;
 
-		if (!pfn_valid_within(pfn + i))
-			continue;
 		page = pfn_to_online_page(pfn + i);
 		if (!page)
 			continue;

commit f5777bc2d9cf0712554228b1a7927b6f13f5c1f0
Author: Qian Cai <cai@lca.pw>
Date:   Thu Mar 28 20:44:21 2019 -0700

    mm/page_isolation.c: fix a wrong flag in set_migratetype_isolate()
    
    Due to has_unmovable_pages() taking an incorrect irqsave flag instead of
    the isolation flag in set_migratetype_isolate(), there are issues with
    HWPOSION and error reporting where dump_page() is not called when there
    is an unmovable page.
    
    Link: http://lkml.kernel.org/r/20190320204941.53731-1-cai@lca.pw
    Fixes: d381c54760dc ("mm: only report isolation failures when offlining memory")
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Cc: <stable@vger.kernel.org>    [5.0.x]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index bf4159d771c7..019280712e1b 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -59,7 +59,8 @@ static int set_migratetype_isolate(struct page *page, int migratetype, int isol_
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype, flags))
+	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype,
+				 isol_flags))
 		ret = 0;
 
 	/*

commit 9b7ea46a82b31c74a37e6ff1c2a1df7d53e392ab
Author: Qian Cai <cai@lca.pw>
Date:   Thu Mar 28 20:43:34 2019 -0700

    mm/hotplug: fix offline undo_isolate_page_range()
    
    Commit f1dd2cd13c4b ("mm, memory_hotplug: do not associate hotadded
    memory to zones until online") introduced move_pfn_range_to_zone() which
    calls memmap_init_zone() during onlining a memory block.
    memmap_init_zone() will reset pagetype flags and makes migrate type to
    be MOVABLE.
    
    However, in __offline_pages(), it also call undo_isolate_page_range()
    after offline_isolated_pages() to do the same thing.  Due to commit
    2ce13640b3f4 ("mm: __first_valid_page skip over offline pages") changed
    __first_valid_page() to skip offline pages, undo_isolate_page_range()
    here just waste CPU cycles looping around the offlining PFN range while
    doing nothing, because __first_valid_page() will return NULL as
    offline_isolated_pages() has already marked all memory sections within
    the pfn range as offline via offline_mem_sections().
    
    Also, after calling the "useless" undo_isolate_page_range() here, it
    reaches the point of no returning by notifying MEM_OFFLINE.  Those pages
    will be marked as MIGRATE_MOVABLE again once onlining.  The only thing
    left to do is to decrease the number of isolated pageblocks zone counter
    which would make some paths of the page allocation slower that the above
    commit introduced.
    
    Even if alloc_contig_range() can be used to isolate 16GB-hugetlb pages
    on ppc64, an "int" should still be enough to represent the number of
    pageblocks there.  Fix an incorrect comment along the way.
    
    [cai@lca.pw: v4]
      Link: http://lkml.kernel.org/r/20190314150641.59358-1-cai@lca.pw
    Link: http://lkml.kernel.org/r/20190313143133.46200-1-cai@lca.pw
    Fixes: 2ce13640b3f4 ("mm: __first_valid_page skip over offline pages")
    Signed-off-by: Qian Cai <cai@lca.pw>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: <stable@vger.kernel.org>    [4.13+]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index ce323e56b34d..bf4159d771c7 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -160,27 +160,36 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
 	return NULL;
 }
 
-/*
- * start_isolate_page_range() -- make page-allocation-type of range of pages
- * to be MIGRATE_ISOLATE.
- * @start_pfn: The lower PFN of the range to be isolated.
- * @end_pfn: The upper PFN of the range to be isolated.
- * @migratetype: migrate type to set in error recovery.
+/**
+ * start_isolate_page_range() - make page-allocation-type of range of pages to
+ * be MIGRATE_ISOLATE.
+ * @start_pfn:		The lower PFN of the range to be isolated.
+ * @end_pfn:		The upper PFN of the range to be isolated.
+ *			start_pfn/end_pfn must be aligned to pageblock_order.
+ * @migratetype:	Migrate type to set in error recovery.
+ * @flags:		The following flags are allowed (they can be combined in
+ *			a bit mask)
+ *			SKIP_HWPOISON - ignore hwpoison pages
+ *			REPORT_FAILURE - report details about the failure to
+ *			isolate the range
  *
  * Making page-allocation-type to be MIGRATE_ISOLATE means free pages in
  * the range will never be allocated. Any free pages and pages freed in the
- * future will not be allocated again.
- *
- * start_pfn/end_pfn must be aligned to pageblock_order.
- * Return 0 on success and -EBUSY if any part of range cannot be isolated.
+ * future will not be allocated again. If specified range includes migrate types
+ * other than MOVABLE or CMA, this will fail with -EBUSY. For isolating all
+ * pages in the range finally, the caller have to free all pages in the range.
+ * test_page_isolated() can be used for test it.
  *
  * There is no high level synchronization mechanism that prevents two threads
- * from trying to isolate overlapping ranges.  If this happens, one thread
+ * from trying to isolate overlapping ranges. If this happens, one thread
  * will notice pageblocks in the overlapping range already set to isolate.
  * This happens in set_migratetype_isolate, and set_migratetype_isolate
- * returns an error.  We then clean up by restoring the migration type on
- * pageblocks we may have modified and return -EBUSY to caller.  This
+ * returns an error. We then clean up by restoring the migration type on
+ * pageblocks we may have modified and return -EBUSY to caller. This
  * prevents two threads from simultaneously working on overlapping ranges.
+ *
+ * Return: the number of isolated pageblocks on success and -EBUSY if any part
+ * of range cannot be isolated.
  */
 int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 			     unsigned migratetype, int flags)
@@ -188,6 +197,7 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	unsigned long pfn;
 	unsigned long undo_pfn;
 	struct page *page;
+	int nr_isolate_pageblock = 0;
 
 	BUG_ON(!IS_ALIGNED(start_pfn, pageblock_nr_pages));
 	BUG_ON(!IS_ALIGNED(end_pfn, pageblock_nr_pages));
@@ -196,13 +206,15 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	     pfn < end_pfn;
 	     pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
-		if (page &&
-		    set_migratetype_isolate(page, migratetype, flags)) {
-			undo_pfn = pfn;
-			goto undo;
+		if (page) {
+			if (set_migratetype_isolate(page, migratetype, flags)) {
+				undo_pfn = pfn;
+				goto undo;
+			}
+			nr_isolate_pageblock++;
 		}
 	}
-	return 0;
+	return nr_isolate_pageblock;
 undo:
 	for (pfn = start_pfn;
 	     pfn < undo_pfn;

commit d381c54760dcfad23743da40516e7e003d73952a
Author: Michal Hocko <mhocko@suse.com>
Date:   Fri Dec 28 00:33:56 2018 -0800

    mm: only report isolation failures when offlining memory
    
    Heiko has complained that his log is swamped by warnings from
    has_unmovable_pages
    
    [   20.536664] page dumped because: has_unmovable_pages
    [   20.536792] page:000003d081ff4080 count:1 mapcount:0 mapping:000000008ff88600 index:0x0 compound_mapcount: 0
    [   20.536794] flags: 0x3fffe0000010200(slab|head)
    [   20.536795] raw: 03fffe0000010200 0000000000000100 0000000000000200 000000008ff88600
    [   20.536796] raw: 0000000000000000 0020004100000000 ffffffff00000001 0000000000000000
    [   20.536797] page dumped because: has_unmovable_pages
    [   20.536814] page:000003d0823b0000 count:1 mapcount:0 mapping:0000000000000000 index:0x0
    [   20.536815] flags: 0x7fffe0000000000()
    [   20.536817] raw: 07fffe0000000000 0000000000000100 0000000000000200 0000000000000000
    [   20.536818] raw: 0000000000000000 0000000000000000 ffffffff00000001 0000000000000000
    
    which are not triggered by the memory hotplug but rather CMA allocator.
    The original idea behind dumping the page state for all call paths was
    that these messages will be helpful debugging failures.  From the above it
    seems that this is not the case for the CMA path because we are lacking
    much more context.  E.g the second reported page might be a CMA allocated
    page.  It is still interesting to see a slab page in the CMA area but it
    is hard to tell whether this is bug from the above output alone.
    
    Address this issue by dumping the page state only on request.  Both
    start_isolate_page_range and has_unmovable_pages already have an argument
    to ignore hwpoison pages so make this argument more generic and turn it
    into flags and allow callers to combine non-default modes into a mask.
    While we are at it, has_unmovable_pages call from
    is_pageblock_removable_nolock (sysfs removable file) is questionable to
    report the failure so drop it from there as well.
    
    Link: http://lkml.kernel.org/r/20181218092802.31429-1-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Reviewed-by: Oscar Salvador <osalvador@suse.de>
    Cc: Anshuman Khandual <anshuman.khandual@arm.com>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 43e085608846..ce323e56b34d 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -15,8 +15,7 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/page_isolation.h>
 
-static int set_migratetype_isolate(struct page *page, int migratetype,
-				bool skip_hwpoisoned_pages)
+static int set_migratetype_isolate(struct page *page, int migratetype, int isol_flags)
 {
 	struct zone *zone;
 	unsigned long flags, pfn;
@@ -60,8 +59,7 @@ static int set_migratetype_isolate(struct page *page, int migratetype,
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype,
-				 skip_hwpoisoned_pages))
+	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype, flags))
 		ret = 0;
 
 	/*
@@ -185,7 +183,7 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  * prevents two threads from simultaneously working on overlapping ranges.
  */
 int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
-			     unsigned migratetype, bool skip_hwpoisoned_pages)
+			     unsigned migratetype, int flags)
 {
 	unsigned long pfn;
 	unsigned long undo_pfn;
@@ -199,7 +197,7 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	     pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
 		if (page &&
-		    set_migratetype_isolate(page, migratetype, skip_hwpoisoned_pages)) {
+		    set_migratetype_isolate(page, migratetype, flags)) {
 			undo_pfn = pfn;
 			goto undo;
 		}

commit 666feb21a0083e5b29ddd96588553ffa0cc357b6
Author: Michal Hocko <mhocko@suse.com>
Date:   Tue Apr 10 16:30:03 2018 -0700

    mm, migrate: remove reason argument from new_page_t
    
    No allocation callback is using this argument anymore.  new_page_node
    used to use this parameter to convey node_id resp.  migration error up
    to move_pages code (do_move_page_to_node_array).  The error status never
    made it into the final status field and we have a better way to
    communicate node id to the status field now.  All other allocation
    callbacks simply ignored the argument so we can drop it finally.
    
    [mhocko@suse.com: fix migration callback]
      Link: http://lkml.kernel.org/r/20180105085259.GH2801@dhcp22.suse.cz
    [akpm@linux-foundation.org: fix alloc_misplaced_dst_page()]
    [mhocko@kernel.org: fix build]
      Link: http://lkml.kernel.org/r/20180103091134.GB11319@dhcp22.suse.cz
    Link: http://lkml.kernel.org/r/20180103082555.14592-3-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reviewed-by: Zi Yan <zi.yan@cs.rutgers.edu>
    Cc: Andrea Reale <ar@linux.vnet.ibm.com>
    Cc: Anshuman Khandual <khandual@linux.vnet.ibm.com>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Mike Kravetz <mike.kravetz@oracle.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 61dee77bb211..43e085608846 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -309,8 +309,7 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	return pfn < end_pfn ? -EBUSY : 0;
 }
 
-struct page *alloc_migrate_target(struct page *page, unsigned long private,
-				  int **resultp)
+struct page *alloc_migrate_target(struct page *page, unsigned long private)
 {
 	return new_page_nodemask(page, numa_node_id(), &node_states[N_MEMORY]);
 }

commit 2c7452a075d4db2dc8e7a61369ede1afc05cd63e
Author: Mike Kravetz <mike.kravetz@oracle.com>
Date:   Thu Apr 5 16:25:26 2018 -0700

    mm/page_isolation.c: make start_isolate_page_range() fail if already isolated
    
    start_isolate_page_range() is used to set the migrate type of a set of
    pageblocks to MIGRATE_ISOLATE while attempting to start a migration
    operation.  It assumes that only one thread is calling it for the
    specified range.  This routine is used by CMA, memory hotplug and
    gigantic huge pages.  Each of these users synchronize access to the
    range within their subsystem.  However, two subsystems (CMA and gigantic
    huge pages for example) could attempt operations on the same range.  If
    this happens, one thread may 'undo' the work another thread is doing.
    This can result in pageblocks being incorrectly left marked as
    MIGRATE_ISOLATE and therefore not available for page allocation.
    
    What is ideally needed is a way to synchronize access to a set of
    pageblocks that are undergoing isolation and migration.  The only thing
    we know about these pageblocks is that they are all in the same zone.  A
    per-node mutex is too coarse as we want to allow multiple operations on
    different ranges within the same zone concurrently.  Instead, we will
    use the migration type of the pageblocks themselves as a form of
    synchronization.
    
    start_isolate_page_range sets the migration type on a set of page-
    blocks going in order from the one associated with the smallest pfn to
    the largest pfn.  The zone lock is acquired to check and set the
    migration type.  When going through the list of pageblocks check if
    MIGRATE_ISOLATE is already set.  If so, this indicates another thread is
    working on this pageblock.  We know exactly which pageblocks we set, so
    clean up by undo those and return -EBUSY.
    
    This allows start_isolate_page_range to serve as a synchronization
    mechanism and will allow for more general use of callers making use of
    these interfaces.  Update comments in alloc_contig_range to reflect this
    new functionality.
    
    Each CPU holds the associated zone lock to modify or examine the
    migration type of a pageblock.  And, it will only examine/update a
    single pageblock per lock acquire/release cycle.
    
    Link: http://lkml.kernel.org/r/20180309224731.16978-1-mike.kravetz@oracle.com
    Signed-off-by: Mike Kravetz <mike.kravetz@oracle.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 165ed8117bd1..61dee77bb211 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -28,6 +28,14 @@ static int set_migratetype_isolate(struct page *page, int migratetype,
 
 	spin_lock_irqsave(&zone->lock, flags);
 
+	/*
+	 * We assume the caller intended to SET migrate type to isolate.
+	 * If it is already set, then someone else must have raced and
+	 * set it before us.  Return -EBUSY
+	 */
+	if (is_migrate_isolate_page(page))
+		goto out;
+
 	pfn = page_to_pfn(page);
 	arg.start_pfn = pfn;
 	arg.nr_pages = pageblock_nr_pages;
@@ -166,7 +174,15 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  * future will not be allocated again.
  *
  * start_pfn/end_pfn must be aligned to pageblock_order.
- * Returns 0 on success and -EBUSY if any part of range cannot be isolated.
+ * Return 0 on success and -EBUSY if any part of range cannot be isolated.
+ *
+ * There is no high level synchronization mechanism that prevents two threads
+ * from trying to isolate overlapping ranges.  If this happens, one thread
+ * will notice pageblocks in the overlapping range already set to isolate.
+ * This happens in set_migratetype_isolate, and set_migratetype_isolate
+ * returns an error.  We then clean up by restoring the migration type on
+ * pageblocks we may have modified and return -EBUSY to caller.  This
+ * prevents two threads from simultaneously working on overlapping ranges.
  */
 int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 			     unsigned migratetype, bool skip_hwpoisoned_pages)

commit 4da2ce250f986060750fcc5b29112914e31803ba
Author: Michal Hocko <mhocko@suse.com>
Date:   Wed Nov 15 17:33:26 2017 -0800

    mm: distinguish CMA and MOVABLE isolation in has_unmovable_pages()
    
    Joonsoo has noticed that "mm: drop migrate type checks from
    has_unmovable_pages" would break CMA allocator because it relies on
    has_unmovable_pages returning false even for CMA pageblocks which in
    fact don't have to be movable:
    
     alloc_contig_range
       start_isolate_page_range
         set_migratetype_isolate
           has_unmovable_pages
    
    This is a result of the code sharing between CMA and memory hotplug
    while each one has a different idea of what has_unmovable_pages should
    return.  This is unfortunate but fixing it properly would require a lot
    of code duplication.
    
    Fix the issue by introducing the requested migrate type argument and
    special case MIGRATE_CMA case where CMA page blocks are handled
    properly.  This will work for memory hotplug because it requires
    MIGRATE_MOVABLE.
    
    Link: http://lkml.kernel.org/r/20171019122118.y6cndierwl2vnguj@dhcp22.suse.cz
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Reported-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Tested-by: Stefan Wahren <stefan.wahren@i2se.com>
    Tested-by: Ran Wang <ran.wang_1@nxp.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <yasu.isimatu@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 44f213935bf6..165ed8117bd1 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -15,7 +15,7 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/page_isolation.h>
 
-static int set_migratetype_isolate(struct page *page,
+static int set_migratetype_isolate(struct page *page, int migratetype,
 				bool skip_hwpoisoned_pages)
 {
 	struct zone *zone;
@@ -52,7 +52,7 @@ static int set_migratetype_isolate(struct page *page,
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, arg.pages_found,
+	if (!has_unmovable_pages(zone, page, arg.pages_found, migratetype,
 				 skip_hwpoisoned_pages))
 		ret = 0;
 
@@ -64,14 +64,14 @@ static int set_migratetype_isolate(struct page *page,
 out:
 	if (!ret) {
 		unsigned long nr_pages;
-		int migratetype = get_pageblock_migratetype(page);
+		int mt = get_pageblock_migratetype(page);
 
 		set_pageblock_migratetype(page, MIGRATE_ISOLATE);
 		zone->nr_isolate_pageblock++;
 		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE,
 									NULL);
 
-		__mod_zone_freepage_state(zone, -nr_pages, migratetype);
+		__mod_zone_freepage_state(zone, -nr_pages, mt);
 	}
 
 	spin_unlock_irqrestore(&zone->lock, flags);
@@ -183,7 +183,7 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	     pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
 		if (page &&
-		    set_migratetype_isolate(page, skip_hwpoisoned_pages)) {
+		    set_migratetype_isolate(page, migratetype, skip_hwpoisoned_pages)) {
 			undo_pfn = pfn;
 			goto undo;
 		}

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 757410d9f758..44f213935bf6 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * linux/mm/page_isolation.c
  */

commit 8b9132388964df2cfe151a88fd1dd8219dabf23c
Author: Michal Hocko <mhocko@suse.com>
Date:   Mon Jul 10 15:48:47 2017 -0700

    mm: unify new_node_page and alloc_migrate_target
    
    Commit 394e31d2ceb4 ("mem-hotplug: alloc new page from a nearest
    neighbor node when mem-offline") has duplicated a large part of
    alloc_migrate_target with some hotplug specific special casing.
    
    To be more precise it tried to enfore the allocation from a different
    node than the original page.  As a result the two function diverged in
    their shared logic, e.g.  the hugetlb allocation strategy.
    
    Let's unify the two and express different NUMA requirements by the given
    nodemask.  new_node_page will simply exclude the node it doesn't care
    about and alloc_migrate_target will use all the available nodes.
    alloc_migrate_target will then learn to migrate hugetlb pages more
    sanely and use preallocated pool when possible.
    
    Please note that alloc_migrate_target used to call alloc_page resp.
    alloc_pages_current so the memory policy of the current context which is
    quite strange when we consider that it is used in the context of
    alloc_contig_range which just tries to migrate pages which stand in the
    way.
    
    Link: http://lkml.kernel.org/r/20170608074553.22152-4-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: zhong jiang <zhongjiang@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 3606104893e0..757410d9f758 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -8,6 +8,7 @@
 #include <linux/memory.h>
 #include <linux/hugetlb.h>
 #include <linux/page_owner.h>
+#include <linux/migrate.h>
 #include "internal.h"
 
 #define CREATE_TRACE_POINTS
@@ -294,20 +295,5 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 struct page *alloc_migrate_target(struct page *page, unsigned long private,
 				  int **resultp)
 {
-	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE;
-
-	/*
-	 * TODO: allocate a destination hugepage from a nearest neighbor node,
-	 * accordance with memory policy of the user process if possible. For
-	 * now as a simple work-around, we use the next node for destination.
-	 */
-	if (PageHuge(page))
-		return alloc_huge_page_node(page_hstate(compound_head(page)),
-					    next_node_in(page_to_nid(page),
-							 node_online_map));
-
-	if (PageHighMem(page))
-		gfp_mask |= __GFP_HIGHMEM;
-
-	return alloc_page(gfp_mask);
+	return new_page_nodemask(page, numa_node_id(), &node_states[N_MEMORY]);
 }

commit 2ce13640b3f4c8721e6820476cb380fbbfaedc18
Author: Michal Hocko <mhocko@suse.com>
Date:   Thu Jul 6 15:38:04 2017 -0700

    mm: __first_valid_page skip over offline pages
    
    __first_valid_page skips over invalid pfns in the range but it might
    still stumble over offline pages.  At least start_isolate_page_range
    will mark those set_migratetype_isolate.  This doesn't represent any
    immediate AFAICS because alloc_contig_range will fail to isolate those
    pages but it relies on not fully initialized page which will become a
    problem later when we stop associating offline pages to zones.  Use
    pfn_to_online_page to handle this.
    
    This is more a preparatory patch than a fix.
    
    Link: http://lkml.kernel.org/r/20170515085827.16474-10-mhocko@kernel.org
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Daniel Kiper <daniel.kiper@oracle.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Igor Mammedov <imammedo@redhat.com>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Reza Arbab <arbab@linux.vnet.ibm.com>
    Cc: Tobias Regnery <tobias.regnery@gmail.com>
    Cc: Toshi Kani <toshi.kani@hpe.com>
    Cc: Vitaly Kuznetsov <vkuznets@redhat.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 5092e4ef00c8..3606104893e0 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -138,12 +138,18 @@ static inline struct page *
 __first_valid_page(unsigned long pfn, unsigned long nr_pages)
 {
 	int i;
-	for (i = 0; i < nr_pages; i++)
-		if (pfn_valid_within(pfn + i))
-			break;
-	if (unlikely(i == nr_pages))
-		return NULL;
-	return pfn_to_page(pfn + i);
+
+	for (i = 0; i < nr_pages; i++) {
+		struct page *page;
+
+		if (!pfn_valid_within(pfn + i))
+			continue;
+		page = pfn_to_online_page(pfn + i);
+		if (!page)
+			continue;
+		return page;
+	}
+	return NULL;
 }
 
 /*
@@ -184,8 +190,12 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 undo:
 	for (pfn = start_pfn;
 	     pfn < undo_pfn;
-	     pfn += pageblock_nr_pages)
-		unset_migratetype_isolate(pfn_to_page(pfn), migratetype);
+	     pfn += pageblock_nr_pages) {
+		struct page *page = pfn_to_online_page(pfn);
+		if (!page)
+			continue;
+		unset_migratetype_isolate(page, migratetype);
+	}
 
 	return -EBUSY;
 }

commit 02aa0cdd72483c6dd436ed24d1000f86e0038d28
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Mon May 8 15:54:40 2017 -0700

    mm, page_alloc: count movable pages when stealing from pageblock
    
    When stealing pages from pageblock of a different migratetype, we count
    how many free pages were stolen, and change the pageblock's migratetype
    if more than half of the pageblock was free.  This might be too
    conservative, as there might be other pages that are not free, but were
    allocated with the same migratetype as our allocation requested.
    
    While we cannot determine the migratetype of allocated pages precisely
    (at least without the page_owner functionality enabled), we can count
    pages that compaction would try to isolate for migration - those are
    either on LRU or __PageMovable().  The rest can be assumed to be
    MIGRATE_RECLAIMABLE or MIGRATE_UNMOVABLE, which we cannot easily
    distinguish.  This counting can be done as part of free page stealing
    with little additional overhead.
    
    The page stealing code is changed so that it considers free pages plus
    pages of the "good" migratetype for the decision whether to change
    pageblock's migratetype.
    
    The result should be more accurate migratetype of pageblocks wrt the
    actual pages in the pageblocks, when stealing from semi-occupied
    pageblocks.  This should help the efficiency of page grouping by
    mobility.
    
    In testing based on 4.9 kernel with stress-highalloc from mmtests
    configured for order-4 GFP_KERNEL allocations, this patch has reduced
    the number of unmovable allocations falling back to movable pageblocks
    by 47%.  The number of movable allocations falling back to other
    pageblocks are increased by 55%, but these events don't cause permanent
    fragmentation, so the tradeoff should be positive.  Later patches also
    offset the movable fallback increase to some extent.
    
    [akpm@linux-foundation.org: merge fix]
    Link: http://lkml.kernel.org/r/20170307131545.28577-5-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 7927bbb54a4e..5092e4ef00c8 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -66,7 +66,8 @@ static int set_migratetype_isolate(struct page *page,
 
 		set_pageblock_migratetype(page, MIGRATE_ISOLATE);
 		zone->nr_isolate_pageblock++;
-		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE);
+		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE,
+									NULL);
 
 		__mod_zone_freepage_state(zone, -nr_pages, migratetype);
 	}
@@ -120,7 +121,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	 * pageblock scanning for freepage moving.
 	 */
 	if (!isolated_page) {
-		nr_pages = move_freepages_block(zone, page, migratetype);
+		nr_pages = move_freepages_block(zone, page, migratetype, NULL);
 		__mod_zone_freepage_state(zone, nr_pages, migratetype);
 	}
 	set_pageblock_migratetype(page, migratetype);

commit bbf9ce9719b5c64eac7f8483d1da6e8a3d4f98f5
Author: Xishi Qiu <qiuxishi@huawei.com>
Date:   Wed May 3 14:52:55 2017 -0700

    mm: use is_migrate_isolate_page() to simplify the code
    
    Use is_migrate_isolate_page() to simplify the code, no functional
    changes.
    
    Link: http://lkml.kernel.org/r/58B94FB1.8020802@huawei.com
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Acked-by: Michal Hocko <mhocko@suse.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index f4e17a57926a..7927bbb54a4e 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -88,7 +88,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
-	if (get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
+	if (!is_migrate_isolate_page(page))
 		goto out;
 
 	/*
@@ -205,7 +205,7 @@ int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	     pfn < end_pfn;
 	     pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
-		if (!page || get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
+		if (!page || !is_migrate_isolate_page(page))
 			continue;
 		unset_migratetype_isolate(page, migratetype);
 	}
@@ -262,7 +262,7 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	 */
 	for (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
-		if (page && get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
+		if (page && !is_migrate_isolate_page(page))
 			break;
 	}
 	page = __first_valid_page(start_pfn, end_pfn - start_pfn);

commit 13ad59df67f19788f6c22985b1a33e466eceb643
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Wed Feb 22 15:41:51 2017 -0800

    mm, page_alloc: avoid page_to_pfn() when merging buddies
    
    On architectures that allow memory holes, page_is_buddy() has to perform
    page_to_pfn() to check for the memory hole.  After the previous patch,
    we have the pfn already available in __free_one_page(), which is the
    only caller of page_is_buddy(), so move the check there and avoid
    page_to_pfn().
    
    Link: http://lkml.kernel.org/r/20161216120009.20064-2-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index dadb7e74d7d6..f4e17a57926a 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -106,7 +106,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 			buddy_pfn = __find_buddy_pfn(pfn, order);
 			buddy = page + (buddy_pfn - pfn);
 
-			if (pfn_valid_within(page_to_pfn(buddy)) &&
+			if (pfn_valid_within(buddy_pfn) &&
 			    !is_migrate_isolate_page(buddy)) {
 				__isolate_free_page(page, order);
 				isolated_page = true;

commit 76741e776a37973a3e398d504069b3e55c5cc866
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Wed Feb 22 15:41:48 2017 -0800

    mm, page_alloc: don't convert pfn to idx when merging
    
    In __free_one_page() we do the buddy merging arithmetics on "page/buddy
    index", which is just the lower MAX_ORDER bits of pfn.  The operations
    we do that affect the higher bits are bitwise AND and subtraction (in
    that order), where the final result will be the same with the higher
    bits left unmasked, as long as these bits are equal for both buddies -
    which must be true by the definition of a buddy.
    
    We can therefore use pfn's directly instead of "index" and skip the
    zeroing of >MAX_ORDER bits.  This can help a bit by itself, although
    compiler might be smart enough already.  It also helps the next patch to
    avoid page_to_pfn() for memory hole checks.
    
    Link: http://lkml.kernel.org/r/20161216120009.20064-1-vbabka@suse.cz
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index a5594bfcc5ed..dadb7e74d7d6 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -83,7 +83,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	unsigned long flags, nr_pages;
 	bool isolated_page = false;
 	unsigned int order;
-	unsigned long page_idx, buddy_idx;
+	unsigned long pfn, buddy_pfn;
 	struct page *buddy;
 
 	zone = page_zone(page);
@@ -102,9 +102,9 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	if (PageBuddy(page)) {
 		order = page_order(page);
 		if (order >= pageblock_order) {
-			page_idx = page_to_pfn(page) & ((1 << MAX_ORDER) - 1);
-			buddy_idx = __find_buddy_index(page_idx, order);
-			buddy = page + (buddy_idx - page_idx);
+			pfn = page_to_pfn(page);
+			buddy_pfn = __find_buddy_pfn(pfn, order);
+			buddy = page + (buddy_pfn - pfn);
 
 			if (pfn_valid_within(page_to_pfn(buddy)) &&
 			    !is_migrate_isolate_page(buddy)) {

commit ac34dcd263a3afe9a2e4d58a2d93bb66d700ac7c
Author: Yisheng Xie <xieyisheng1@huawei.com>
Date:   Fri Oct 7 17:01:16 2016 -0700

    mm/page_isolation: fix typo: "paes" -> "pages"
    
    Fix typo in comment.
    
    Link: http://lkml.kernel.org/r/1474788764-5774-1-git-send-email-ysxie@foxmail.com
    Signed-off-by: Yisheng Xie <xieyisheng1@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 064b7fb6e0b5..a5594bfcc5ed 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -55,7 +55,7 @@ static int set_migratetype_isolate(struct page *page,
 		ret = 0;
 
 	/*
-	 * immobile means "not-on-lru" paes. If immobile is larger than
+	 * immobile means "not-on-lru" pages. If immobile is larger than
 	 * removable-by-driver pages reported by notifier, we'll fail.
 	 */
 

commit e3a2713c3cfa9c41fac57447533079d8da23319b
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Jul 26 15:24:01 2016 -0700

    mm/page_isolation: clean up confused code
    
    When there is an isolated_page, post_alloc_hook() is called with page
    but __free_pages() is called with isolated_page.  Since they are the
    same so no problem but it's very confusing.  To reduce it, this patch
    changes isolated_page to boolean type and uses page variable
    consistently.
    
    Link: http://lkml.kernel.org/r/1466150259-27727-10-git-send-email-iamjoonsoo.kim@lge.com
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 4639163b78f9..064b7fb6e0b5 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -81,7 +81,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 {
 	struct zone *zone;
 	unsigned long flags, nr_pages;
-	struct page *isolated_page = NULL;
+	bool isolated_page = false;
 	unsigned int order;
 	unsigned long page_idx, buddy_idx;
 	struct page *buddy;
@@ -109,7 +109,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 			if (pfn_valid_within(page_to_pfn(buddy)) &&
 			    !is_migrate_isolate_page(buddy)) {
 				__isolate_free_page(page, order);
-				isolated_page = page;
+				isolated_page = true;
 			}
 		}
 	}
@@ -129,7 +129,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	spin_unlock_irqrestore(&zone->lock, flags);
 	if (isolated_page) {
 		post_alloc_hook(page, order, __GFP_MOVABLE);
-		__free_pages(isolated_page, order);
+		__free_pages(page, order);
 	}
 }
 

commit 46f24fd857b37bb86ddd5d0ac3d194e984dfdf1c
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Jul 26 15:23:58 2016 -0700

    mm/page_alloc: introduce post allocation processing on page allocator
    
    This patch is motivated from Hugh and Vlastimil's concern [1].
    
    There are two ways to get freepage from the allocator.  One is using
    normal memory allocation API and the other is __isolate_free_page()
    which is internally used for compaction and pageblock isolation.  Later
    usage is rather tricky since it doesn't do whole post allocation
    processing done by normal API.
    
    One problematic thing I already know is that poisoned page would not be
    checked if it is allocated by __isolate_free_page().  Perhaps, there
    would be more.
    
    We could add more debug logic for allocated page in the future and this
    separation would cause more problem.  I'd like to fix this situation at
    this time.  Solution is simple.  This patch commonize some logic for
    newly allocated page and uses it on all sites.  This will solve the
    problem.
    
    [1] http://marc.info/?i=alpine.LSU.2.11.1604270029350.7066%40eggly.anvils%3E
    
    [iamjoonsoo.kim@lge.com: mm-page_alloc-introduce-post-allocation-processing-on-page-allocator-v3]
      Link: http://lkml.kernel.org/r/1464230275-25791-7-git-send-email-iamjoonsoo.kim@lge.com
      Link: http://lkml.kernel.org/r/1466150259-27727-9-git-send-email-iamjoonsoo.kim@lge.com
    Link: http://lkml.kernel.org/r/1464230275-25791-7-git-send-email-iamjoonsoo.kim@lge.com
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 927f5ee24c87..4639163b78f9 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -128,9 +128,7 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
 	if (isolated_page) {
-		kernel_map_pages(page, (1 << order), 1);
-		set_page_refcounted(page);
-		set_page_owner(page, order, __GFP_MOVABLE);
+		post_alloc_hook(page, order, __GFP_MOVABLE);
 		__free_pages(isolated_page, order);
 	}
 }

commit 83358ece26b70f20c0ba2e0e00dc84b0ee24fe6d
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Tue Jul 26 15:23:43 2016 -0700

    mm/page_owner: initialize page owner without holding the zone lock
    
    It's not necessary to initialized page_owner with holding the zone lock.
    It would cause more contention on the zone lock although it's not a big
    problem since it is just debug feature.  But, it is better than before
    so do it.  This is also preparation step to use stackdepot in page owner
    feature.  Stackdepot allocates new pages when there is no reserved space
    and holding the zone lock in this case will cause deadlock.
    
    Link: http://lkml.kernel.org/r/1464230275-25791-2-git-send-email-iamjoonsoo.kim@lge.com
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 612122bf6a42..927f5ee24c87 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -7,6 +7,7 @@
 #include <linux/pageblock-flags.h>
 #include <linux/memory.h>
 #include <linux/hugetlb.h>
+#include <linux/page_owner.h>
 #include "internal.h"
 
 #define CREATE_TRACE_POINTS
@@ -108,8 +109,6 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 			if (pfn_valid_within(page_to_pfn(buddy)) &&
 			    !is_migrate_isolate_page(buddy)) {
 				__isolate_free_page(page, order);
-				kernel_map_pages(page, (1 << order), 1);
-				set_page_refcounted(page);
 				isolated_page = page;
 			}
 		}
@@ -128,8 +127,12 @@ static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	zone->nr_isolate_pageblock--;
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
-	if (isolated_page)
+	if (isolated_page) {
+		kernel_map_pages(page, (1 << order), 1);
+		set_page_refcounted(page);
+		set_page_owner(page, order, __GFP_MOVABLE);
 		__free_pages(isolated_page, order);
+	}
 }
 
 static inline struct page *

commit b9eb63191a6ad70127ebdd83650b810cdc1d1117
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu May 19 17:12:06 2016 -0700

    mm/memory_hotplug: add comment to some functions related to memory hotplug
    
    __offline_isolated_pages() and test_pages_isolated() are used by memory
    hotplug.  These functions require that range is in a single zone but
    there is no code to do this because memory hotplug checks it before
    calling these functions.  To avoid confusing future user of these
    functions, this patch adds comments to them.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 67bedd18429c..612122bf6a42 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -246,6 +246,7 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 	return pfn;
 }
 
+/* Caller should ensure that requested range is in a single zone */
 int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 			bool skip_hwpoisoned_pages)
 {

commit 0edaf86cf1a6a97d811fc34765ddbcbc310de564
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Thu May 19 17:10:58 2016 -0700

    include/linux/nodemask.h: create next_node_in() helper
    
    Lots of code does
    
            node = next_node(node, XXX);
            if (node == MAX_NUMNODES)
                    node = first_node(XXX);
    
    so create next_node_in() to do this and use it in various places.
    
    [mhocko@suse.com: use next_node_in() helper]
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Michal Hocko <mhocko@kernel.org>
    Signed-off-by: Michal Hocko <mhocko@suse.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Hui Zhu <zhuhui@xiaomi.com>
    Cc: Wang Xiaoqiang <wangxq10@lzu.edu.cn>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index c4f568206544..67bedd18429c 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -288,13 +288,10 @@ struct page *alloc_migrate_target(struct page *page, unsigned long private,
 	 * accordance with memory policy of the user process if possible. For
 	 * now as a simple work-around, we use the next node for destination.
 	 */
-	if (PageHuge(page)) {
-		int node = next_online_node(page_to_nid(page));
-		if (node == MAX_NUMNODES)
-			node = first_online_node;
+	if (PageHuge(page))
 		return alloc_huge_page_node(page_hstate(compound_head(page)),
-					    node);
-	}
+					    next_node_in(page_to_nid(page),
+							 node_online_map));
 
 	if (PageHighMem(page))
 		gfp_mask |= __GFP_HIGHMEM;

commit ec3b6882509188602dfaadf2814a9cca062f2786
Author: Neil Zhang <neilzhang1123@hotmail.com>
Date:   Fri Apr 1 14:31:37 2016 -0700

    mm/page_isolation.c: fix the function comments
    
    Commit fea85cff11de ("mm/page_isolation.c: return last tested pfn rather
    than failure indicator") changed the meaning of the return value.  Let's
    change the function comments as well.
    
    Signed-off-by: Neil Zhang <neilzhang1123@hotmail.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 31555b689eb9..c4f568206544 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -215,7 +215,7 @@ int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
  * all pages in [start_pfn...end_pfn) must be in the same zone.
  * zone->lock must be held before call this.
  *
- * Returns 1 if all pages in the range are isolated.
+ * Returns the last tested pfn.
  */
 static unsigned long
 __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,

commit 6f25a14a7053b69917e2ebea0d31dd444cd31fd5
Author: Xishi Qiu <qiuxishi@huawei.com>
Date:   Fri Apr 1 14:31:20 2016 -0700

    mm: fix invalid node in alloc_migrate_target()
    
    It is incorrect to use next_node to find a target node, it will return
    MAX_NUMNODES or invalid node.  This will lead to crash in buddy system
    allocation.
    
    Fixes: c8721bbbdd36 ("mm: memory-hotplug: enable memory hotplug to handle hugepage")
    Signed-off-by: Xishi Qiu <qiuxishi@huawei.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Joonsoo Kim <js1304@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: "Laura Abbott" <lauraa@codeaurora.org>
    Cc: Hui Zhu <zhuhui@xiaomi.com>
    Cc: Wang Xiaoqiang <wangxq10@lzu.edu.cn>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 92c4c36501e7..31555b689eb9 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -289,11 +289,11 @@ struct page *alloc_migrate_target(struct page *page, unsigned long private,
 	 * now as a simple work-around, we use the next node for destination.
 	 */
 	if (PageHuge(page)) {
-		nodemask_t src = nodemask_of_node(page_to_nid(page));
-		nodemask_t dst;
-		nodes_complement(dst, src);
+		int node = next_online_node(page_to_nid(page));
+		if (node == MAX_NUMNODES)
+			node = first_online_node;
 		return alloc_huge_page_node(page_hstate(compound_head(page)),
-					    next_node(page_to_nid(page), dst));
+					    node);
 	}
 
 	if (PageHighMem(page))

commit 6f8d2b8a262c6b71e1464d488f41cccf9aafbc07
Author: Wang Xiaoqiang <wangxq10@lzu.edu.cn>
Date:   Fri Jan 15 16:57:13 2016 -0800

    mm/page_isolation: do some cleanup in "undo_isolate_page_range"
    
    Use "IS_ALIGNED" to judge the alignment, rather than directly judging.
    
    Signed-off-by: Wang Xiaoqiang <wang_xiaoq@126.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 5e139fec6c6c..92c4c36501e7 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -196,8 +196,10 @@ int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 {
 	unsigned long pfn;
 	struct page *page;
-	BUG_ON((start_pfn) & (pageblock_nr_pages - 1));
-	BUG_ON((end_pfn) & (pageblock_nr_pages - 1));
+
+	BUG_ON(!IS_ALIGNED(start_pfn, pageblock_nr_pages));
+	BUG_ON(!IS_ALIGNED(end_pfn, pageblock_nr_pages));
+
 	for (pfn = start_pfn;
 	     pfn < end_pfn;
 	     pfn += pageblock_nr_pages) {

commit fec174d6699e2e39d34c7c6ee2fe360e518e68c0
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Thu Jan 14 15:22:13 2016 -0800

    mm/page_isolation: use macro to judge the alignment
    
    Signed-off-by: Wang Xiaoqiang <wangxq10@lzu.edu.cn>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index f484b9300fe3..5e139fec6c6c 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -165,8 +165,8 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	unsigned long undo_pfn;
 	struct page *page;
 
-	BUG_ON((start_pfn) & (pageblock_nr_pages - 1));
-	BUG_ON((end_pfn) & (pageblock_nr_pages - 1));
+	BUG_ON(!IS_ALIGNED(start_pfn, pageblock_nr_pages));
+	BUG_ON(!IS_ALIGNED(end_pfn, pageblock_nr_pages));
 
 	for (pfn = start_pfn;
 	     pfn < end_pfn;

commit 0f0848e5118a4cb2cb92cef0c3af6f647649ff47
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Thu Jan 14 15:18:42 2016 -0800

    mm/page_isolation.c: add new tracepoint, test_pages_isolated
    
    cma allocation should be guranteeded to succeed.  But sometimes it can
    fail in the current implementation.  To track down the problem, we need
    to know which page is problematic and this new tracepoint will report
    it.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 029a171d35dc..f484b9300fe3 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -9,6 +9,9 @@
 #include <linux/hugetlb.h>
 #include "internal.h"
 
+#define CREATE_TRACE_POINTS
+#include <trace/events/page_isolation.h>
+
 static int set_migratetype_isolate(struct page *page,
 				bool skip_hwpoisoned_pages)
 {
@@ -268,6 +271,8 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 						skip_hwpoisoned_pages);
 	spin_unlock_irqrestore(&zone->lock, flags);
 
+	trace_test_pages_isolated(start_pfn, end_pfn, pfn);
+
 	return pfn < end_pfn ? -EBUSY : 0;
 }
 

commit fea85cff11de4377040deff0e85eb2793fb078aa
Author: Joonsoo Kim <js1304@gmail.com>
Date:   Thu Jan 14 15:18:39 2016 -0800

    mm/page_isolation.c: return last tested pfn rather than failure indicator
    
    This is preparation step to report test failed pfn in new tracepoint to
    analyze cma allocation failure problem.  There is no functional change
    in this patch.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 4568fd58f70a..029a171d35dc 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -212,7 +212,7 @@ int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
  *
  * Returns 1 if all pages in the range are isolated.
  */
-static int
+static unsigned long
 __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 				  bool skip_hwpoisoned_pages)
 {
@@ -237,9 +237,8 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 		else
 			break;
 	}
-	if (pfn < end_pfn)
-		return 0;
-	return 1;
+
+	return pfn;
 }
 
 int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
@@ -248,7 +247,6 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	unsigned long pfn, flags;
 	struct page *page;
 	struct zone *zone;
-	int ret;
 
 	/*
 	 * Note: pageblock_nr_pages != MAX_ORDER. Then, chunks of free pages
@@ -266,10 +264,11 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	/* Check all pages are free or marked as ISOLATED */
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
-	ret = __test_page_isolated_in_pageblock(start_pfn, end_pfn,
+	pfn = __test_page_isolated_in_pageblock(start_pfn, end_pfn,
 						skip_hwpoisoned_pages);
 	spin_unlock_irqrestore(&zone->lock, flags);
-	return ret ? 0 : -EBUSY;
+
+	return pfn < end_pfn ? -EBUSY : 0;
 }
 
 struct page *alloc_migrate_target(struct page *page, unsigned long private,

commit c5b4e1b02f2a0c2309ecd58a235a2f5ee4eb0074
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Tue Sep 8 15:02:09 2015 -0700

    mm, page_isolation: make set/unset_migratetype_isolate() file-local
    
    Nowaday, set/unset_migratetype_isolate() is defined and used only in
    mm/page_isolation, so let's limit the scope within the file.
    
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 32fdc1df05e5..4568fd58f70a 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -9,7 +9,8 @@
 #include <linux/hugetlb.h>
 #include "internal.h"
 
-int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
+static int set_migratetype_isolate(struct page *page,
+				bool skip_hwpoisoned_pages)
 {
 	struct zone *zone;
 	unsigned long flags, pfn;
@@ -72,7 +73,7 @@ int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
 	return ret;
 }
 
-void unset_migratetype_isolate(struct page *page, unsigned migratetype)
+static void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 {
 	struct zone *zone;
 	unsigned long flags, nr_pages;

commit aa016d145d4c3b8a7273429528f19d5b423ddbc7
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Tue Sep 8 15:01:22 2015 -0700

    mm, page_isolation: remove bogus tests for isolated pages
    
    The __test_page_isolated_in_pageblock() is used to verify whether all
    pages in pageblock were either successfully isolated, or are hwpoisoned.
    Two of the possible state of pages, that are tested, are however bogus
    and misleading.
    
    Both tests rely on get_freepage_migratetype(page), which however has no
    guarantees about pages on freelists.  Specifically, it doesn't guarantee
    that the migratetype returned by the function actually matches the
    migratetype of the freelist that the page is on.  Such guarantee is not
    its purpose and would have negative impact on allocator performance.
    
    The first test checks whether the freepage_migratetype equals
    MIGRATE_ISOLATE, supposedly to catch races between page isolation and
    allocator activity.  These races should be fixed nowadays with
    51bb1a4093 ("mm/page_alloc: add freepage on isolate pageblock to correct
    buddy list") and related patches.  As explained above, the check
    wouldn't be able to catch them reliably anyway.  For the same reason
    false positives can happen, although they are harmless, as the
    move_freepages() call would just move the page to the same freelist it's
    already on.  So removing the test is not a bug fix, just cleanup.  After
    this patch, we assume that all PageBuddy pages are on the correct
    freelist and that the races were really fixed.  A truly reliable
    verification in the form of e.g.  VM_BUG_ON() would be complicated and
    is arguably not needed.
    
    The second test (page_count(page) == 0 && get_freepage_migratetype(page)
    == MIGRATE_ISOLATE) is probably supposed (the code comes from a big
    memory isolation patch from 2007) to catch pages on MIGRATE_ISOLATE
    pcplists.  However, pcplists don't contain MIGRATE_ISOLATE freepages
    nowadays, those are freed directly to free lists, so the check is
    obsolete.  Remove it as well.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Acked-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Seungho Park <seungho1.park@lge.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 303c908790ef..32fdc1df05e5 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -223,34 +223,16 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
 			continue;
 		}
 		page = pfn_to_page(pfn);
-		if (PageBuddy(page)) {
+		if (PageBuddy(page))
 			/*
-			 * If race between isolatation and allocation happens,
-			 * some free pages could be in MIGRATE_MOVABLE list
-			 * although pageblock's migratation type of the page
-			 * is MIGRATE_ISOLATE. Catch it and move the page into
-			 * MIGRATE_ISOLATE list.
+			 * If the page is on a free list, it has to be on
+			 * the correct MIGRATE_ISOLATE freelist. There is no
+			 * simple way to verify that as VM_BUG_ON(), though.
 			 */
-			if (get_freepage_migratetype(page) != MIGRATE_ISOLATE) {
-				struct page *end_page;
-
-				end_page = page + (1 << page_order(page)) - 1;
-				move_freepages(page_zone(page), page, end_page,
-						MIGRATE_ISOLATE);
-			}
 			pfn += 1 << page_order(page);
-		}
-		else if (page_count(page) == 0 &&
-			get_freepage_migratetype(page) == MIGRATE_ISOLATE)
-			pfn += 1;
-		else if (skip_hwpoisoned_pages && PageHWPoison(page)) {
-			/*
-			 * The HWPoisoned page may be not in buddy
-			 * system, and page_count() is not 0.
-			 */
+		else if (skip_hwpoisoned_pages && PageHWPoison(page))
+			/* A HWPoisoned page cannot be also PageBuddy */
 			pfn++;
-			continue;
-		}
 		else
 			break;
 	}

commit 1ae7013dfa4281644a9a9591105c24d639216b22
Author: Hui Zhu <zhuhui@xiaomi.com>
Date:   Thu May 14 15:17:04 2015 -0700

    CMA: page_isolation: check buddy before accessing it
    
    I had an issue:
    
        Unable to handle kernel NULL pointer dereference at virtual address 0000082a
        pgd = cc970000
        [0000082a] *pgd=00000000
        Internal error: Oops: 5 [#1] PREEMPT SMP ARM
        PC is at get_pageblock_flags_group+0x5c/0xb0
        LR is at unset_migratetype_isolate+0x148/0x1b0
        pc : [<c00cc9a0>]    lr : [<c0109874>]    psr: 80000093
        sp : c7029d00  ip : 00000105  fp : c7029d1c
        r10: 00000001  r9 : 0000000a  r8 : 00000004
        r7 : 60000013  r6 : 000000a4  r5 : c0a357e4  r4 : 00000000
        r3 : 00000826  r2 : 00000002  r1 : 00000000  r0 : 0000003f
        Flags: Nzcv  IRQs off  FIQs on  Mode SVC_32  ISA ARM  Segment user
        Control: 10c5387d  Table: 2cb7006a  DAC: 00000015
        Backtrace:
            get_pageblock_flags_group+0x0/0xb0
            unset_migratetype_isolate+0x0/0x1b0
            undo_isolate_page_range+0x0/0xdc
            __alloc_contig_range+0x0/0x34c
            alloc_contig_range+0x0/0x18
    
    This issue is because when calling unset_migratetype_isolate() to unset
    a part of CMA memory, it try to access the buddy page to get its status:
    
                    if (order >= pageblock_order) {
                            page_idx = page_to_pfn(page) & ((1 << MAX_ORDER) - 1);
                            buddy_idx = __find_buddy_index(page_idx, order);
                            buddy = page + (buddy_idx - page_idx);
    
                            if (!is_migrate_isolate_page(buddy)) {
    
    But the begin addr of this part of CMA memory is very close to a part of
    memory that is reserved at boot time (not in buddy system).  So add a
    check before accessing it.
    
    [akpm@linux-foundation.org: use conventional code layout]
    Signed-off-by: Hui Zhu <zhuhui@xiaomi.com>
    Suggested-by: Laura Abbott <labbott@redhat.com>
    Suggested-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 755a42c76eb4..303c908790ef 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -101,7 +101,8 @@ void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 			buddy_idx = __find_buddy_index(page_idx, order);
 			buddy = page + (buddy_idx - page_idx);
 
-			if (!is_migrate_isolate_page(buddy)) {
+			if (pfn_valid_within(page_to_pfn(buddy)) &&
+			    !is_migrate_isolate_page(buddy)) {
 				__isolate_free_page(page, order);
 				kernel_map_pages(page, (1 << order), 1);
 				set_page_refcounted(page);

commit cfa869438282be84ad4110bba5027ef1fbbe71e4
Author: Laura Abbott <lauraa@codeaurora.org>
Date:   Wed Mar 25 15:55:26 2015 -0700

    mm/page_alloc.c: call kernel_map_pages in unset_migrateype_isolate
    
    Commit 3c605096d315 ("mm/page_alloc: restrict max order of merging on
    isolated pageblock") changed the logic of unset_migratetype_isolate to
    check the buddy allocator and explicitly call __free_pages to merge.
    
    The page that is being freed in this path never had prep_new_page called
    so set_page_refcounted is called explicitly but there is no call to
    kernel_map_pages.  With the default kernel_map_pages this is mostly
    harmless but if kernel_map_pages does any manipulation of the page
    tables (unmapping or setting pages to read only) this may trigger a
    fault:
    
        alloc_contig_range test_pages_isolated(ceb00, ced00) failed
        Unable to handle kernel paging request at virtual address ffffffc0cec00000
        pgd = ffffffc045fc4000
        [ffffffc0cec00000] *pgd=0000000000000000
        Internal error: Oops: 9600004f [#1] PREEMPT SMP
        Modules linked in: exfatfs
        CPU: 1 PID: 23237 Comm: TimedEventQueue Not tainted 3.10.49-gc72ad36-dirty #1
        task: ffffffc03de52100 ti: ffffffc015388000 task.ti: ffffffc015388000
        PC is at memset+0xc8/0x1c0
        LR is at kernel_map_pages+0x1ec/0x244
    
    Fix this by calling kernel_map_pages to ensure the page is set in the
    page table properly
    
    Fixes: 3c605096d315 ("mm/page_alloc: restrict max order of merging on isolated pageblock")
    Signed-off-by: Laura Abbott <lauraa@codeaurora.org>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Acked-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Gioh Kim <gioh.kim@lge.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Vlastimil Babka <vbabka@suse.cz>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 72f5ac381ab3..755a42c76eb4 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -103,6 +103,7 @@ void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 
 			if (!is_migrate_isolate_page(buddy)) {
 				__isolate_free_page(page, order);
+				kernel_map_pages(page, (1 << order), 1);
 				set_page_refcounted(page);
 				isolated_page = page;
 			}

commit ec25af84b23b6862341b5b5b68d24be3f53b8d2c
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Wed Dec 10 15:43:04 2014 -0800

    mm, page_isolation: drain single zone pcplists
    
    When setting MIGRATETYPE_ISOLATE on a pageblock, pcplists are drained to
    have a better chance that all pages will be successfully isolated and
    not left in the per-cpu caches.  Since isolation is always concerned
    with a single zone, we can reduce the pcplists drain to the single zone,
    which is now possible.
    
    The change should make memory isolation faster and not disturbing
    unrelated pcplists anymore.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index f2452e5116b2..72f5ac381ab3 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -68,7 +68,7 @@ int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
 
 	spin_unlock_irqrestore(&zone->lock, flags);
 	if (!ret)
-		drain_all_pages(NULL);
+		drain_all_pages(zone);
 	return ret;
 }
 

commit 93481ff0e5a0c7636359a7ee52248856da5e7859
Author: Vlastimil Babka <vbabka@suse.cz>
Date:   Wed Dec 10 15:43:01 2014 -0800

    mm: introduce single zone pcplists drain
    
    The functions for draining per-cpu pages back to buddy allocators
    currently always operate on all zones.  There are however several cases
    where the drain is only needed in the context of a single zone, and
    spilling other pcplists is a waste of time both due to the extra
    spilling and later refilling.
    
    This patch introduces new zone pointer parameter to drain_all_pages()
    and changes the dummy parameter of drain_local_pages() to be also a zone
    pointer.  When NULL is passed, the functions operate on all zones as
    usual.  Passing a specific zone pointer reduces the work to the single
    zone.
    
    All callers are updated to pass the NULL pointer in this patch.
    Conversion to single zone (where appropriate) is done in further
    patches.
    
    Signed-off-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index c8778f7e208e..f2452e5116b2 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -68,7 +68,7 @@ int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
 
 	spin_unlock_irqrestore(&zone->lock, flags);
 	if (!ret)
-		drain_all_pages();
+		drain_all_pages(NULL);
 	return ret;
 }
 

commit 3c605096d3158216ba9326a16266f6ba128c2c8d
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Nov 13 15:19:21 2014 -0800

    mm/page_alloc: restrict max order of merging on isolated pageblock
    
    Current pageblock isolation logic could isolate each pageblock
    individually.  This causes freepage accounting problem if freepage with
    pageblock order on isolate pageblock is merged with other freepage on
    normal pageblock.  We can prevent merging by restricting max order of
    merging to pageblock order if freepage is on isolate pageblock.
    
    A side-effect of this change is that there could be non-merged buddy
    freepage even if finishing pageblock isolation, because undoing
    pageblock isolation is just to move freepage from isolate buddy list to
    normal buddy list rather than to consider merging.  So, the patch also
    makes undoing pageblock isolation consider freepage merge.  When
    un-isolation, freepage with more than pageblock order and it's buddy are
    checked.  If they are on normal pageblock, instead of just moving, we
    isolate the freepage and free it in order to get merged.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Heesub Shin <heesub.shin@samsung.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Ritesh Harjani <ritesh.list@gmail.com>
    Cc: Gioh Kim <gioh.kim@lge.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 1fa4a4d72ecc..c8778f7e208e 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -76,17 +76,54 @@ void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 {
 	struct zone *zone;
 	unsigned long flags, nr_pages;
+	struct page *isolated_page = NULL;
+	unsigned int order;
+	unsigned long page_idx, buddy_idx;
+	struct page *buddy;
 
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
 	if (get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 		goto out;
-	nr_pages = move_freepages_block(zone, page, migratetype);
-	__mod_zone_freepage_state(zone, nr_pages, migratetype);
+
+	/*
+	 * Because freepage with more than pageblock_order on isolated
+	 * pageblock is restricted to merge due to freepage counting problem,
+	 * it is possible that there is free buddy page.
+	 * move_freepages_block() doesn't care of merge so we need other
+	 * approach in order to merge them. Isolation and free will make
+	 * these pages to be merged.
+	 */
+	if (PageBuddy(page)) {
+		order = page_order(page);
+		if (order >= pageblock_order) {
+			page_idx = page_to_pfn(page) & ((1 << MAX_ORDER) - 1);
+			buddy_idx = __find_buddy_index(page_idx, order);
+			buddy = page + (buddy_idx - page_idx);
+
+			if (!is_migrate_isolate_page(buddy)) {
+				__isolate_free_page(page, order);
+				set_page_refcounted(page);
+				isolated_page = page;
+			}
+		}
+	}
+
+	/*
+	 * If we isolate freepage with more than pageblock_order, there
+	 * should be no freepage in the range, so we could avoid costly
+	 * pageblock scanning for freepage moving.
+	 */
+	if (!isolated_page) {
+		nr_pages = move_freepages_block(zone, page, migratetype);
+		__mod_zone_freepage_state(zone, nr_pages, migratetype);
+	}
 	set_pageblock_migratetype(page, migratetype);
 	zone->nr_isolate_pageblock--;
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
+	if (isolated_page)
+		__free_pages(isolated_page, order);
 }
 
 static inline struct page *

commit ad53f92eb416d81e469fa8ea57153e59455e7175
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Nov 13 15:19:11 2014 -0800

    mm/page_alloc: fix incorrect isolation behavior by rechecking migratetype
    
    Before describing bugs itself, I first explain definition of freepage.
    
     1. pages on buddy list are counted as freepage.
     2. pages on isolate migratetype buddy list are *not* counted as freepage.
     3. pages on cma buddy list are counted as CMA freepage, too.
    
    Now, I describe problems and related patch.
    
    Patch 1: There is race conditions on getting pageblock migratetype that
    it results in misplacement of freepages on buddy list, incorrect
    freepage count and un-availability of freepage.
    
    Patch 2: Freepages on pcp list could have stale cached information to
    determine migratetype of buddy list to go.  This causes misplacement of
    freepages on buddy list and incorrect freepage count.
    
    Patch 4: Merging between freepages on different migratetype of
    pageblocks will cause freepages accouting problem.  This patch fixes it.
    
    Without patchset [3], above problem doesn't happens on my CMA allocation
    test, because CMA reserved pages aren't used at all.  So there is no
    chance for above race.
    
    With patchset [3], I did simple CMA allocation test and get below
    result:
    
     - Virtual machine, 4 cpus, 1024 MB memory, 256 MB CMA reservation
     - run kernel build (make -j16) on background
     - 30 times CMA allocation(8MB * 30 = 240MB) attempts in 5 sec interval
     - Result: more than 5000 freepage count are missed
    
    With patchset [3] and this patchset, I found that no freepage count are
    missed so that I conclude that problems are solved.
    
    On my simple memory offlining test, these problems also occur on that
    environment, too.
    
    This patch (of 4):
    
    There are two paths to reach core free function of buddy allocator,
    __free_one_page(), one is free_one_page()->__free_one_page() and the
    other is free_hot_cold_page()->free_pcppages_bulk()->__free_one_page().
    Each paths has race condition causing serious problems.  At first, this
    patch is focused on first type of freepath.  And then, following patch
    will solve the problem in second type of freepath.
    
    In the first type of freepath, we got migratetype of freeing page
    without holding the zone lock, so it could be racy.  There are two cases
    of this race.
    
     1. pages are added to isolate buddy list after restoring orignal
        migratetype
    
        CPU1                                   CPU2
    
        get migratetype => return MIGRATE_ISOLATE
        call free_one_page() with MIGRATE_ISOLATE
    
                                    grab the zone lock
                                    unisolate pageblock
                                    release the zone lock
    
        grab the zone lock
        call __free_one_page() with MIGRATE_ISOLATE
        freepage go into isolate buddy list,
        although pageblock is already unisolated
    
    This may cause two problems.  One is that we can't use this page anymore
    until next isolation attempt of this pageblock, because freepage is on
    isolate buddy list.  The other is that freepage accouting could be wrong
    due to merging between different buddy list.  Freepages on isolate buddy
    list aren't counted as freepage, but ones on normal buddy list are
    counted as freepage.  If merge happens, buddy freepage on normal buddy
    list is inevitably moved to isolate buddy list without any consideration
    of freepage accouting so it could be incorrect.
    
     2. pages are added to normal buddy list while pageblock is isolated.
        It is similar with above case.
    
    This also may cause two problems.  One is that we can't keep these
    freepages from being allocated.  Although this pageblock is isolated,
    freepage would be added to normal buddy list so that it could be
    allocated without any restriction.  And the other problem is same as
    case 1, that it, incorrect freepage accouting.
    
    This race condition would be prevented by checking migratetype again
    with holding the zone lock.  Because it is somewhat heavy operation and
    it isn't needed in common case, we want to avoid rechecking as much as
    possible.  So this patch introduce new variable, nr_isolate_pageblock in
    struct zone to check if there is isolated pageblock.  With this, we can
    avoid to re-check migratetype in common case and do it only if there is
    isolated pageblock or migratetype is MIGRATE_ISOLATE.  This solve above
    mentioned problems.
    
    Changes from v3:
    Add one more check in free_one_page() that checks whether migratetype is
    MIGRATE_ISOLATE or not. Without this, abovementioned case 1 could happens.
    
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    Acked-by: Minchan Kim <minchan@kernel.org>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Acked-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: "Kirill A. Shutemov" <kirill@shutemov.name>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Zhang Yanfei <zhangyanfei@cn.fujitsu.com>
    Cc: Tang Chen <tangchen@cn.fujitsu.com>
    Cc: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Cc: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Heesub Shin <heesub.shin@samsung.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Cc: Ritesh Harjani <ritesh.list@gmail.com>
    Cc: Gioh Kim <gioh.kim@lge.com>
    Cc: <stable@vger.kernel.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index d1473b2e9481..1fa4a4d72ecc 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -60,6 +60,7 @@ int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
 		int migratetype = get_pageblock_migratetype(page);
 
 		set_pageblock_migratetype(page, MIGRATE_ISOLATE);
+		zone->nr_isolate_pageblock++;
 		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE);
 
 		__mod_zone_freepage_state(zone, -nr_pages, migratetype);
@@ -83,6 +84,7 @@ void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	nr_pages = move_freepages_block(zone, page, migratetype);
 	__mod_zone_freepage_state(zone, nr_pages, migratetype);
 	set_pageblock_migratetype(page, migratetype);
+	zone->nr_isolate_pageblock--;
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
 }

commit c8721bbbdd36382de51cd6b7a56322e0acca2414
Author: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
Date:   Wed Sep 11 14:22:09 2013 -0700

    mm: memory-hotplug: enable memory hotplug to handle hugepage
    
    Until now we can't offline memory blocks which contain hugepages because a
    hugepage is considered as an unmovable page.  But now with this patch
    series, a hugepage has become movable, so by using hugepage migration we
    can offline such memory blocks.
    
    What's different from other users of hugepage migration is that we need to
    decompose all the hugepages inside the target memory block into free buddy
    pages after hugepage migration, because otherwise free hugepages remaining
    in the memory block intervene the memory offlining.  For this reason we
    introduce new functions dissolve_free_huge_page() and
    dissolve_free_huge_pages().
    
    Other than that, what this patch does is straightforwardly to add hugepage
    migration code, that is, adding hugepage code to the functions which scan
    over pfn and collect hugepages to be migrated, and adding a hugepage
    allocation function to alloc_migrate_target().
    
    As for larger hugepages (1GB for x86_64), it's not easy to do hotremove
    over them because it's larger than memory block.  So we now simply leave
    it to fail as it is.
    
    [yongjun_wei@trendmicro.com.cn: remove duplicated include]
    Signed-off-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Acked-by: Andi Kleen <ak@linux.intel.com>
    Cc: Hillf Danton <dhillf@gmail.com>
    Cc: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: "Aneesh Kumar K.V" <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Wei Yongjun <yongjun_wei@trendmicro.com.cn>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 0cee10ffb98d..d1473b2e9481 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -6,6 +6,7 @@
 #include <linux/page-isolation.h>
 #include <linux/pageblock-flags.h>
 #include <linux/memory.h>
+#include <linux/hugetlb.h>
 #include "internal.h"
 
 int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
@@ -252,6 +253,19 @@ struct page *alloc_migrate_target(struct page *page, unsigned long private,
 {
 	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE;
 
+	/*
+	 * TODO: allocate a destination hugepage from a nearest neighbor node,
+	 * accordance with memory policy of the user process if possible. For
+	 * now as a simple work-around, we use the next node for destination.
+	 */
+	if (PageHuge(page)) {
+		nodemask_t src = nodemask_of_node(page_to_nid(page));
+		nodemask_t dst;
+		nodes_complement(dst, src);
+		return alloc_huge_page_node(page_hstate(compound_head(page)),
+					    next_node(page_to_nid(page), dst));
+	}
+
 	if (PageHighMem(page))
 		gfp_mask |= __GFP_HIGHMEM;
 

commit 85dbe706074c8b9091a3bc5f24eff02a044701c3
Author: Tang Chen <tangchen@cn.fujitsu.com>
Date:   Thu Jun 20 18:10:19 2013 +0800

    page_isolation: Fix a comment typo in test_pages_isolated()
    
    pageblock_nr_page should be pageblock_nr_pages, and fist is
    a typo of first.
    
    Signed-off-by: Tang Chen <tangchen@cn.fujitsu.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 383bdbb98b04..0cee10ffb98d 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -226,9 +226,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	int ret;
 
 	/*
-	 * Note: pageblock_nr_page != MAX_ORDER. Then, chunks of free page
-	 * is not aligned to pageblock_nr_pages.
-	 * Then we just check pagetype fist.
+	 * Note: pageblock_nr_pages != MAX_ORDER. Then, chunks of free pages
+	 * are not aligned to pageblock_nr_pages.
+	 * Then we just check migratetype first.
 	 */
 	for (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
@@ -238,7 +238,7 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
 	page = __first_valid_page(start_pfn, end_pfn - start_pfn);
 	if ((pfn < end_pfn) || !page)
 		return -EBUSY;
-	/* Check all pages are free or Marked as ISOLATED */
+	/* Check all pages are free or marked as ISOLATED */
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
 	ret = __test_page_isolated_in_pageblock(start_pfn, end_pfn,

commit a458431e176ddb27e8ef8b98c2a681b217337393
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Fri Jan 4 15:35:08 2013 -0800

    mm: fix zone_watermark_ok_safe() accounting of isolated pages
    
    Commit 702d1a6e0766 ("memory-hotplug: fix kswapd looping forever
    problem") added an isolated pageblocks counter (nr_pageblock_isolate in
    struct zone) and used it to adjust free pages counter in
    zone_watermark_ok_safe() to prevent kswapd looping forever problem.
    
    Then later, commit 2139cbe627b8 ("cma: fix counting of isolated pages")
    fixed accounting of isolated pages in global free pages counter.  It
    made the previous zone_watermark_ok_safe() fix unnecessary and
    potentially harmful (cause now isolated pages may be accounted twice
    making free pages counter incorrect).
    
    This patch removes the special isolated pageblocks counter altogether
    which fixes zone_watermark_ok_safe() free pages check.
    
    Reported-by: Tomasz Stanislawski <t.stanislaws@samsung.com>
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Aaditya Kumar <aaditya.kumar.30@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 9d2264ea4606..383bdbb98b04 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -8,28 +8,6 @@
 #include <linux/memory.h>
 #include "internal.h"
 
-/* called while holding zone->lock */
-static void set_pageblock_isolate(struct page *page)
-{
-	if (get_pageblock_migratetype(page) == MIGRATE_ISOLATE)
-		return;
-
-	set_pageblock_migratetype(page, MIGRATE_ISOLATE);
-	page_zone(page)->nr_pageblock_isolate++;
-}
-
-/* called while holding zone->lock */
-static void restore_pageblock_isolate(struct page *page, int migratetype)
-{
-	struct zone *zone = page_zone(page);
-	if (WARN_ON(get_pageblock_migratetype(page) != MIGRATE_ISOLATE))
-		return;
-
-	BUG_ON(zone->nr_pageblock_isolate <= 0);
-	set_pageblock_migratetype(page, migratetype);
-	zone->nr_pageblock_isolate--;
-}
-
 int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
 {
 	struct zone *zone;
@@ -80,7 +58,7 @@ int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
 		unsigned long nr_pages;
 		int migratetype = get_pageblock_migratetype(page);
 
-		set_pageblock_isolate(page);
+		set_pageblock_migratetype(page, MIGRATE_ISOLATE);
 		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE);
 
 		__mod_zone_freepage_state(zone, -nr_pages, migratetype);
@@ -103,7 +81,7 @@ void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 		goto out;
 	nr_pages = move_freepages_block(zone, page, migratetype);
 	__mod_zone_freepage_state(zone, nr_pages, migratetype);
-	restore_pageblock_isolate(page, migratetype);
+	set_pageblock_migratetype(page, migratetype);
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
 }

commit b023f46813cde6e3b8a8c24f432ff9c1fd8e9a64
Author: Wen Congyang <wency@cn.fujitsu.com>
Date:   Tue Dec 11 16:00:45 2012 -0800

    memory-hotplug: skip HWPoisoned page when offlining pages
    
    hwpoisoned may be set when we offline a page by the sysfs interface
    /sys/devices/system/memory/soft_offline_page or
    /sys/devices/system/memory/hard_offline_page. If we don't clear
    this flag when onlining pages, this page can't be freed, and will
    not in free list. So we can't offline these pages again. So we
    should skip such page when offlining pages.
    
    Signed-off-by: Wen Congyang <wency@cn.fujitsu.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Jiang Liu <liuj97@gmail.com>
    Cc: Len Brown <len.brown@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Minchan Kim <minchan.kim@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index f2f5b4818e94..9d2264ea4606 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -30,7 +30,7 @@ static void restore_pageblock_isolate(struct page *page, int migratetype)
 	zone->nr_pageblock_isolate--;
 }
 
-int set_migratetype_isolate(struct page *page)
+int set_migratetype_isolate(struct page *page, bool skip_hwpoisoned_pages)
 {
 	struct zone *zone;
 	unsigned long flags, pfn;
@@ -66,7 +66,8 @@ int set_migratetype_isolate(struct page *page)
 	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
 	 * We just check MOVABLE pages.
 	 */
-	if (!has_unmovable_pages(zone, page, arg.pages_found))
+	if (!has_unmovable_pages(zone, page, arg.pages_found,
+				 skip_hwpoisoned_pages))
 		ret = 0;
 
 	/*
@@ -134,7 +135,7 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  * Returns 0 on success and -EBUSY if any part of range cannot be isolated.
  */
 int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
-			     unsigned migratetype)
+			     unsigned migratetype, bool skip_hwpoisoned_pages)
 {
 	unsigned long pfn;
 	unsigned long undo_pfn;
@@ -147,7 +148,8 @@ int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
 	     pfn < end_pfn;
 	     pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
-		if (page && set_migratetype_isolate(page)) {
+		if (page &&
+		    set_migratetype_isolate(page, skip_hwpoisoned_pages)) {
 			undo_pfn = pfn;
 			goto undo;
 		}
@@ -190,7 +192,8 @@ int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
  * Returns 1 if all pages in the range are isolated.
  */
 static int
-__test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
+__test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn,
+				  bool skip_hwpoisoned_pages)
 {
 	struct page *page;
 
@@ -220,6 +223,14 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
 		else if (page_count(page) == 0 &&
 			get_freepage_migratetype(page) == MIGRATE_ISOLATE)
 			pfn += 1;
+		else if (skip_hwpoisoned_pages && PageHWPoison(page)) {
+			/*
+			 * The HWPoisoned page may be not in buddy
+			 * system, and page_count() is not 0.
+			 */
+			pfn++;
+			continue;
+		}
 		else
 			break;
 	}
@@ -228,7 +239,8 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
 	return 1;
 }
 
-int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
+int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn,
+			bool skip_hwpoisoned_pages)
 {
 	unsigned long pfn, flags;
 	struct page *page;
@@ -251,7 +263,8 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
 	/* Check all pages are free or Marked as ISOLATED */
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
-	ret = __test_page_isolated_in_pageblock(start_pfn, end_pfn);
+	ret = __test_page_isolated_in_pageblock(start_pfn, end_pfn,
+						skip_hwpoisoned_pages);
 	spin_unlock_irqrestore(&zone->lock, flags);
 	return ret ? 0 : -EBUSY;
 }

commit 723a0644a7255f532575fd43245f9ef976491328
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:32:52 2012 -0700

    mm/page_alloc: refactor out __alloc_contig_migrate_alloc()
    
    __alloc_contig_migrate_alloc() can be used by memory-hotplug so refactor
    it out (move + rename as a common name) into page_isolation.c.
    
    [akpm@linux-foundation.org: checkpatch fixes]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Kamezawa Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Acked-by: Michal Nazarewicz <mina86@mina86.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 5f34a9053ce0..f2f5b4818e94 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -255,3 +255,14 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
 	spin_unlock_irqrestore(&zone->lock, flags);
 	return ret ? 0 : -EBUSY;
 }
+
+struct page *alloc_migrate_target(struct page *page, unsigned long private,
+				  int **resultp)
+{
+	gfp_t gfp_mask = GFP_USER | __GFP_MOVABLE;
+
+	if (PageHighMem(page))
+		gfp_mask |= __GFP_HIGHMEM;
+
+	return alloc_page(gfp_mask);
+}

commit 435b405c06119d93333738172b8060b0ed12af41
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:32:16 2012 -0700

    memory-hotplug: fix pages missed by race rather than failing
    
    If race between allocation and isolation in memory-hotplug offline
    happens, some pages could be in MIGRATE_MOVABLE of free_list although the
    pageblock's migratetype of the page is MIGRATE_ISOLATE.
    
    The race could be detected by get_freepage_migratetype in
    __test_page_isolated_in_pageblock.  If it is detected, now EBUSY gets
    bubbled all the way up and the hotplug operations fails.
    
    But better idea is instead of returning and failing memory-hotremove, move
    the free page to the correct list at the time it is detected.  It could
    enhance memory-hotremove operation success ratio although the race is
    really rare.
    
    Suggested by Mel Gorman.
    
    [akpm@linux-foundation.org: small cleanup]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 6744235d2d0e..5f34a9053ce0 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -201,8 +201,20 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
 		}
 		page = pfn_to_page(pfn);
 		if (PageBuddy(page)) {
-			if (get_freepage_migratetype(page) != MIGRATE_ISOLATE)
-				break;
+			/*
+			 * If race between isolatation and allocation happens,
+			 * some free pages could be in MIGRATE_MOVABLE list
+			 * although pageblock's migratation type of the page
+			 * is MIGRATE_ISOLATE. Catch it and move the page into
+			 * MIGRATE_ISOLATE list.
+			 */
+			if (get_freepage_migratetype(page) != MIGRATE_ISOLATE) {
+				struct page *end_page;
+
+				end_page = page + (1 << page_order(page)) - 1;
+				move_freepages(page_zone(page), page, end_page,
+						MIGRATE_ISOLATE);
+			}
 			pfn += 1 << page_order(page);
 		}
 		else if (page_count(page) == 0 &&

commit 41d575ad4a511b71a4a41c8313004212f5c229b1
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:32:14 2012 -0700

    memory-hotplug: bug fix race between isolation and allocation
    
    Like below, memory-hotplug makes race between page-isolation
    and page-allocation so it can hit BUG_ON in __offline_isolated_pages.
    
            CPU A                                   CPU B
    
    start_isolate_page_range
    set_migratetype_isolate
    spin_lock_irqsave(zone->lock)
    
                                    free_hot_cold_page(Page A)
                                    /* without zone->lock */
                                    migratetype = get_pageblock_migratetype(Page A);
                                    /*
                                     * Page could be moved into MIGRATE_MOVABLE
                                     * of per_cpu_pages
                                     */
                                    list_add_tail(&page->lru, &pcp->lists[migratetype]);
    
    set_pageblock_isolate
    move_freepages_block
    drain_all_pages
    
                                    /* Page A could be in MIGRATE_MOVABLE of free_list. */
    
    check_pages_isolated
    __test_page_isolated_in_pageblock
    /*
     * We can't catch freed page which
     * is free_list[MIGRATE_MOVABLE]
     */
    if (PageBuddy(page A))
            pfn += 1 << page_order(page A);
    
                                    /* So, Page A could be allocated */
    
    __offline_isolated_pages
    /*
     * BUG_ON hit or offline page
     * which is used by someone
     */
    BUG_ON(!PageBuddy(page A));
    
    This patch checks page's migratetype in freelist in
    __test_page_isolated_in_pageblock.  So now
    __test_page_isolated_in_pageblock can check the page caused by above race
    and can fail of memory offlining.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 9c03dca8c2ad..6744235d2d0e 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -200,8 +200,11 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
 			continue;
 		}
 		page = pfn_to_page(pfn);
-		if (PageBuddy(page))
+		if (PageBuddy(page)) {
+			if (get_freepage_migratetype(page) != MIGRATE_ISOLATE)
+				break;
 			pfn += 1 << page_order(page);
+		}
 		else if (page_count(page) == 0 &&
 			get_freepage_migratetype(page) == MIGRATE_ISOLATE)
 			pfn += 1;

commit b12c4ad14ee0232ad47c2bef404b6d42a3578332
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Oct 8 16:32:08 2012 -0700

    mm: page_alloc: use get_freepage_migratetype() instead of page_private()
    
    The page allocator uses set_page_private and page_private for handling
    migratetype when it frees page.  Let's replace them with [set|get]
    _freepage_migratetype to make it more clear.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Reviewed-by: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Cc: Xishi Qiu <qiuxishi@huawei.com>
    Cc: Wen Congyang <wency@cn.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 345643b85bd4..9c03dca8c2ad 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -203,7 +203,7 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
 		if (PageBuddy(page))
 			pfn += 1 << page_order(page);
 		else if (page_count(page) == 0 &&
-				page_private(page) == MIGRATE_ISOLATE)
+			get_freepage_migratetype(page) == MIGRATE_ISOLATE)
 			pfn += 1;
 		else
 			break;

commit d1ce749a0db12202b711d1aba1d29e823034648d
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Mon Oct 8 16:32:02 2012 -0700

    cma: count free CMA pages
    
    Add NR_FREE_CMA_PAGES counter to be later used for checking watermark in
    __zone_watermark_ok().  For simplicity and to avoid #ifdef hell make this
    counter always available (not only when CONFIG_CMA=y).
    
    [akpm@linux-foundation.org: use conventional migratetype naming]
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 3ca1716471bc..345643b85bd4 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -77,11 +77,12 @@ int set_migratetype_isolate(struct page *page)
 out:
 	if (!ret) {
 		unsigned long nr_pages;
+		int migratetype = get_pageblock_migratetype(page);
 
 		set_pageblock_isolate(page);
 		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE);
 
-		__mod_zone_page_state(zone, NR_FREE_PAGES, -nr_pages);
+		__mod_zone_freepage_state(zone, -nr_pages, migratetype);
 	}
 
 	spin_unlock_irqrestore(&zone->lock, flags);
@@ -100,7 +101,7 @@ void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	if (get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 		goto out;
 	nr_pages = move_freepages_block(zone, page, migratetype);
-	__mod_zone_page_state(zone, NR_FREE_PAGES, nr_pages);
+	__mod_zone_freepage_state(zone, nr_pages, migratetype);
 	restore_pageblock_isolate(page, migratetype);
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);

commit 2139cbe627b8910ded55148f87ee10f7485408ed
Author: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
Date:   Mon Oct 8 16:32:00 2012 -0700

    cma: fix counting of isolated pages
    
    Isolated free pages shouldn't be accounted to NR_FREE_PAGES counter.  Fix
    it by properly decreasing/increasing NR_FREE_PAGES counter in
    set_migratetype_isolate()/unset_migratetype_isolate() and removing counter
    adjustment for isolated pages from free_one_page() and split_free_page().
    
    Signed-off-by: Bartlomiej Zolnierkiewicz <b.zolnierkie@samsung.com>
    Signed-off-by: Kyungmin Park <kyungmin.park@samsung.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Michal Nazarewicz <mina86@mina86.com>
    Cc: Minchan Kim <minchan@kernel.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Hugh Dickins <hughd@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 247d1f175739..3ca1716471bc 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -76,8 +76,12 @@ int set_migratetype_isolate(struct page *page)
 
 out:
 	if (!ret) {
+		unsigned long nr_pages;
+
 		set_pageblock_isolate(page);
-		move_freepages_block(zone, page, MIGRATE_ISOLATE);
+		nr_pages = move_freepages_block(zone, page, MIGRATE_ISOLATE);
+
+		__mod_zone_page_state(zone, NR_FREE_PAGES, -nr_pages);
 	}
 
 	spin_unlock_irqrestore(&zone->lock, flags);
@@ -89,12 +93,14 @@ int set_migratetype_isolate(struct page *page)
 void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 {
 	struct zone *zone;
-	unsigned long flags;
+	unsigned long flags, nr_pages;
+
 	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
 	if (get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 		goto out;
-	move_freepages_block(zone, page, migratetype);
+	nr_pages = move_freepages_block(zone, page, migratetype);
+	__mod_zone_page_state(zone, NR_FREE_PAGES, nr_pages);
 	restore_pageblock_isolate(page, migratetype);
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);

commit 702d1a6e0766d45642c934444fd41f658d251305
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:43:56 2012 -0700

    memory-hotplug: fix kswapd looping forever problem
    
    When hotplug offlining happens on zone A, it starts to mark freed page as
    MIGRATE_ISOLATE type in buddy for preventing further allocation.
    (MIGRATE_ISOLATE is very irony type because it's apparently on buddy but
    we can't allocate them).
    
    When the memory shortage happens during hotplug offlining, current task
    starts to reclaim, then wake up kswapd.  Kswapd checks watermark, then go
    sleep because current zone_watermark_ok_safe doesn't consider
    MIGRATE_ISOLATE freed page count.  Current task continue to reclaim in
    direct reclaim path without kswapd's helping.  The problem is that
    zone->all_unreclaimable is set by only kswapd so that current task would
    be looping forever like below.
    
    __alloc_pages_slowpath
    restart:
            wake_all_kswapd
    rebalance:
            __alloc_pages_direct_reclaim
                    do_try_to_free_pages
                            if global_reclaim && !all_unreclaimable
                                    return 1; /* It means we did did_some_progress */
            skip __alloc_pages_may_oom
            should_alloc_retry
                    goto rebalance;
    
    If we apply KOSAKI's patch[1] which doesn't depends on kswapd about
    setting zone->all_unreclaimable, we can solve this problem by killing some
    task in direct reclaim path.  But it doesn't wake up kswapd, still.  It
    could be a problem still if other subsystem needs GFP_ATOMIC request.  So
    kswapd should consider MIGRATE_ISOLATE when it calculate free pages BEFORE
    going sleep.
    
    This patch counts the number of MIGRATE_ISOLATE page block and
    zone_watermark_ok_safe will consider it if the system has such blocks
    (fortunately, it's very rare so no problem in POV overhead and kswapd is
    never hotpath).
    
    Copy/modify from Mel's quote
    "
    Ideal solution would be "allocating" the pageblock.
    It would keep the free space accounting as it is but historically,
    memory hotplug didn't allocate pages because it would be difficult to
    detect if a pageblock was isolated or if part of some balloon.
    Allocating just full pageblocks would work around this, However,
    it would play very badly with CMA.
    "
    
    [1] http://lkml.org/lkml/2012/6/14/74
    
    [akpm@linux-foundation.org: simplify nr_zone_isolate_freepages(), rework zone_watermark_ok_safe() comment, simplify set_pageblock_isolate() and restore_pageblock_isolate()]
    [akpm@linux-foundation.org: fix CONFIG_MEMORY_ISOLATION=n build]
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Suggested-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Tested-by: Aaditya Kumar <aaditya.kumar.30@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index fb482cf438da..247d1f175739 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -8,6 +8,28 @@
 #include <linux/memory.h>
 #include "internal.h"
 
+/* called while holding zone->lock */
+static void set_pageblock_isolate(struct page *page)
+{
+	if (get_pageblock_migratetype(page) == MIGRATE_ISOLATE)
+		return;
+
+	set_pageblock_migratetype(page, MIGRATE_ISOLATE);
+	page_zone(page)->nr_pageblock_isolate++;
+}
+
+/* called while holding zone->lock */
+static void restore_pageblock_isolate(struct page *page, int migratetype)
+{
+	struct zone *zone = page_zone(page);
+	if (WARN_ON(get_pageblock_migratetype(page) != MIGRATE_ISOLATE))
+		return;
+
+	BUG_ON(zone->nr_pageblock_isolate <= 0);
+	set_pageblock_migratetype(page, migratetype);
+	zone->nr_pageblock_isolate--;
+}
+
 int set_migratetype_isolate(struct page *page)
 {
 	struct zone *zone;
@@ -54,7 +76,7 @@ int set_migratetype_isolate(struct page *page)
 
 out:
 	if (!ret) {
-		set_pageblock_migratetype(page, MIGRATE_ISOLATE);
+		set_pageblock_isolate(page);
 		move_freepages_block(zone, page, MIGRATE_ISOLATE);
 	}
 
@@ -72,8 +94,8 @@ void unset_migratetype_isolate(struct page *page, unsigned migratetype)
 	spin_lock_irqsave(&zone->lock, flags);
 	if (get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 		goto out;
-	set_pageblock_migratetype(page, migratetype);
 	move_freepages_block(zone, page, migratetype);
+	restore_pageblock_isolate(page, migratetype);
 out:
 	spin_unlock_irqrestore(&zone->lock, flags);
 }

commit ee6f509c3274014d1f52e7a7a10aee9f85393c5e
Author: Minchan Kim <minchan@kernel.org>
Date:   Tue Jul 31 16:43:50 2012 -0700

    mm: factor out memory isolate functions
    
    mm/page_alloc.c has some memory isolation functions but they are used only
    when we enable CONFIG_{CMA|MEMORY_HOTPLUG|MEMORY_FAILURE}.  So let's make
    it configurable by new CONFIG_MEMORY_ISOLATION so that it can reduce
    binary size and we can check it simple by CONFIG_MEMORY_ISOLATION, not if
    defined CONFIG_{CMA|MEMORY_HOTPLUG|MEMORY_FAILURE}.
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Michal Hocko <mhocko@suse.cz>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index c9f04774f2b8..fb482cf438da 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -5,8 +5,79 @@
 #include <linux/mm.h>
 #include <linux/page-isolation.h>
 #include <linux/pageblock-flags.h>
+#include <linux/memory.h>
 #include "internal.h"
 
+int set_migratetype_isolate(struct page *page)
+{
+	struct zone *zone;
+	unsigned long flags, pfn;
+	struct memory_isolate_notify arg;
+	int notifier_ret;
+	int ret = -EBUSY;
+
+	zone = page_zone(page);
+
+	spin_lock_irqsave(&zone->lock, flags);
+
+	pfn = page_to_pfn(page);
+	arg.start_pfn = pfn;
+	arg.nr_pages = pageblock_nr_pages;
+	arg.pages_found = 0;
+
+	/*
+	 * It may be possible to isolate a pageblock even if the
+	 * migratetype is not MIGRATE_MOVABLE. The memory isolation
+	 * notifier chain is used by balloon drivers to return the
+	 * number of pages in a range that are held by the balloon
+	 * driver to shrink memory. If all the pages are accounted for
+	 * by balloons, are free, or on the LRU, isolation can continue.
+	 * Later, for example, when memory hotplug notifier runs, these
+	 * pages reported as "can be isolated" should be isolated(freed)
+	 * by the balloon driver through the memory notifier chain.
+	 */
+	notifier_ret = memory_isolate_notify(MEM_ISOLATE_COUNT, &arg);
+	notifier_ret = notifier_to_errno(notifier_ret);
+	if (notifier_ret)
+		goto out;
+	/*
+	 * FIXME: Now, memory hotplug doesn't call shrink_slab() by itself.
+	 * We just check MOVABLE pages.
+	 */
+	if (!has_unmovable_pages(zone, page, arg.pages_found))
+		ret = 0;
+
+	/*
+	 * immobile means "not-on-lru" paes. If immobile is larger than
+	 * removable-by-driver pages reported by notifier, we'll fail.
+	 */
+
+out:
+	if (!ret) {
+		set_pageblock_migratetype(page, MIGRATE_ISOLATE);
+		move_freepages_block(zone, page, MIGRATE_ISOLATE);
+	}
+
+	spin_unlock_irqrestore(&zone->lock, flags);
+	if (!ret)
+		drain_all_pages();
+	return ret;
+}
+
+void unset_migratetype_isolate(struct page *page, unsigned migratetype)
+{
+	struct zone *zone;
+	unsigned long flags;
+	zone = page_zone(page);
+	spin_lock_irqsave(&zone->lock, flags);
+	if (get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
+		goto out;
+	set_pageblock_migratetype(page, migratetype);
+	move_freepages_block(zone, page, migratetype);
+out:
+	spin_unlock_irqrestore(&zone->lock, flags);
+}
+
 static inline struct page *
 __first_valid_page(unsigned long pfn, unsigned long nr_pages)
 {

commit 0815f3d81d76dfbf2abcfd93a85ff0a6008fe4c0
Author: Michal Nazarewicz <mina86@mina86.com>
Date:   Tue Apr 3 15:06:15 2012 +0200

    mm: page_isolation: MIGRATE_CMA isolation functions added
    
    This commit changes various functions that change pages and
    pageblocks migrate type between MIGRATE_ISOLATE and
    MIGRATE_MOVABLE in such a way as to allow to work with
    MIGRATE_CMA migrate type.
    
    Signed-off-by: Michal Nazarewicz <mina86@mina86.com>
    Signed-off-by: Marek Szyprowski <m.szyprowski@samsung.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Tested-by: Rob Clark <rob.clark@linaro.org>
    Tested-by: Ohad Ben-Cohen <ohad@wizery.com>
    Tested-by: Benjamin Gaignard <benjamin.gaignard@linaro.org>
    Tested-by: Robert Nelson <robertcnelson@gmail.com>
    Tested-by: Barry Song <Baohua.Song@csr.com>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 4ae42bb40892..c9f04774f2b8 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -24,6 +24,7 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  * to be MIGRATE_ISOLATE.
  * @start_pfn: The lower PFN of the range to be isolated.
  * @end_pfn: The upper PFN of the range to be isolated.
+ * @migratetype: migrate type to set in error recovery.
  *
  * Making page-allocation-type to be MIGRATE_ISOLATE means free pages in
  * the range will never be allocated. Any free pages and pages freed in the
@@ -32,8 +33,8 @@ __first_valid_page(unsigned long pfn, unsigned long nr_pages)
  * start_pfn/end_pfn must be aligned to pageblock_order.
  * Returns 0 on success and -EBUSY if any part of range cannot be isolated.
  */
-int
-start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
+int start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
+			     unsigned migratetype)
 {
 	unsigned long pfn;
 	unsigned long undo_pfn;
@@ -56,7 +57,7 @@ start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
 	for (pfn = start_pfn;
 	     pfn < undo_pfn;
 	     pfn += pageblock_nr_pages)
-		unset_migratetype_isolate(pfn_to_page(pfn));
+		unset_migratetype_isolate(pfn_to_page(pfn), migratetype);
 
 	return -EBUSY;
 }
@@ -64,8 +65,8 @@ start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
 /*
  * Make isolated pages available again.
  */
-int
-undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
+int undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn,
+			    unsigned migratetype)
 {
 	unsigned long pfn;
 	struct page *page;
@@ -77,7 +78,7 @@ undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
 		page = __first_valid_page(pfn, pageblock_nr_pages);
 		if (!page || get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 			continue;
-		unset_migratetype_isolate(page);
+		unset_migratetype_isolate(page, migratetype);
 	}
 	return 0;
 }
@@ -86,7 +87,7 @@ undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
  * all pages in [start_pfn...end_pfn) must be in the same zone.
  * zone->lock must be held before call this.
  *
- * Returns 1 if all pages in the range is isolated.
+ * Returns 1 if all pages in the range are isolated.
  */
 static int
 __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)

commit f6a3607e5f30dc642bead8cd95c48d47b6b4bfbb
Author: Bob Liu <lliubbo@gmail.com>
Date:   Tue Oct 26 14:22:09 2010 -0700

    mm: page_isolation: codeclean fix comment and rm unneeded val init
    
    __test_page_isolated_in_pageblock() returns 1 if all pages in the range
    are isolated, so fix the comment.  Variable `pfn' will be initialised in
    the following loop so remove it.
    
    Signed-off-by: Bob Liu <lliubbo@gmail.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Mel Gorman <mel@csn.ul.ie>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 5e0ffd967452..4ae42bb40892 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -86,7 +86,7 @@ undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
  * all pages in [start_pfn...end_pfn) must be in the same zone.
  * zone->lock must be held before call this.
  *
- * Returns 0 if all pages in the range is isolated.
+ * Returns 1 if all pages in the range is isolated.
  */
 static int
 __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
@@ -119,7 +119,6 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
 	struct zone *zone;
 	int ret;
 
-	pfn = start_pfn;
 	/*
 	 * Note: pageblock_nr_page != MAX_ORDER. Then, chunks of free page
 	 * is not aligned to pageblock_nr_pages.

commit a70dcb969f64e2fa98c24f47854f20bf02ff0092
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Thu Nov 6 12:53:36 2008 -0800

    memory hotplug: fix page_zone() calculation in test_pages_isolated()
    
    My last bugfix here (adding zone->lock) introduced a new problem: Using
    page_zone(pfn_to_page(pfn)) to get the zone after the for() loop is wrong.
     pfn will then be >= end_pfn, which may be in a different zone or not
    present at all.  This may lead to an addressing exception in page_zone()
    or spin_lock_irqsave().
    
    Now I use __first_valid_page() again after the loop to find a valid page
    for page_zone().
    
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Acked-by: Nathan Fontenot <nfont@austin.ibm.com>
    Reviewed-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index b70a7fec1ff6..5e0ffd967452 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -130,10 +130,11 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
 		if (page && get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 			break;
 	}
-	if (pfn < end_pfn)
+	page = __first_valid_page(start_pfn, end_pfn - start_pfn);
+	if ((pfn < end_pfn) || !page)
 		return -EBUSY;
 	/* Check all pages are free or Marked as ISOLATED */
-	zone = page_zone(pfn_to_page(pfn));
+	zone = page_zone(page);
 	spin_lock_irqsave(&zone->lock, flags);
 	ret = __test_page_isolated_in_pageblock(start_pfn, end_pfn);
 	spin_unlock_irqrestore(&zone->lock, flags);

commit 6c1b7f680dd4f550fa6f91f148cc6fa2c4bd0737
Author: Gerald Schaefer <gerald.schaefer@de.ibm.com>
Date:   Thu Oct 2 14:50:16 2008 -0700

    memory hotplug: missing zone->lock in test_pages_isolated()
    
    __test_page_isolated_in_pageblock() in mm/page_isolation.c has a comment
    saying that the caller must hold zone->lock. But the only caller of that
    function, test_pages_isolated(), does not hold zone->lock and the lock is
    also not acquired anywhere before. This patch adds the missing zone->lock
    to test_pages_isolated().
    
    We reproducibly run into BUG_ON(!PageBuddy(page)) in __offline_isolated_pages()
    during memory hotplug stress test, see trace below. This patch fixes that
    problem, it would be good if we could have it in 2.6.27.
    
    kernel BUG at /home/autobuild/BUILD/linux-2.6.26-20080909/mm/page_alloc.c:4561!
    illegal operation: 0001 [#1] PREEMPT SMP
    Modules linked in: dm_multipath sunrpc bonding qeth_l3 dm_mod qeth ccwgroup vmur
    CPU: 1 Not tainted 2.6.26-29.x.20080909-s390default #1
    Process memory_loop_all (pid: 10025, task: 2f444028, ksp: 2b10dd28)
    Krnl PSW : 040c0000 801727ea (__offline_isolated_pages+0x18e/0x1c4)
     R:0 T:1 IO:0 EX:0 Key:0 M:1 W:0 P:0 AS:0 CC:0 PM:0
    Krnl GPRS: 00000000 7e27fc00 00000000 7e27fc00
     00000000 00000400 00014000 7e27fc01
     00606f00 7e27fc00 00013fe0 2b10dd28
     00000005 80172662 801727b2 2b10dd28
    Krnl Code: 801727de: 5810900c l %r1,12(%r9)
     801727e2: a7f4ffb3 brc 15,80172748
     801727e6: a7f40001 brc 15,801727e8
     >801727ea: a7f4ffbc brc 15,80172762
     801727ee: a7f40001 brc 15,801727f0
     801727f2: a7f4ffaf brc 15,80172750
     801727f6: 0707 bcr 0,%r7
     801727f8: 0017 unknown
    Call Trace:
    ([<0000000000172772>] __offline_isolated_pages+0x116/0x1c4)
     [<00000000001953a2>] offline_isolated_pages_cb+0x22/0x34
     [<000000000013164c>] walk_memory_resource+0xcc/0x11c
     [<000000000019520e>] offline_pages+0x36a/0x498
     [<00000000001004d6>] remove_memory+0x36/0x44
     [<000000000028fb06>] memory_block_change_state+0x112/0x150
     [<000000000028ffb8>] store_mem_state+0x90/0xe4
     [<0000000000289c00>] sysdev_store+0x34/0x40
     [<00000000001ee048>] sysfs_write_file+0xd0/0x178
     [<000000000019b1a8>] vfs_write+0x74/0x118
     [<000000000019b9ae>] sys_write+0x46/0x7c
     [<000000000011160e>] sysc_do_restart+0x12/0x16
     [<0000000077f3e8ca>] 0x77f3e8ca
    
    Signed-off-by: Gerald Schaefer <gerald.schaefer@de.ibm.com>
    Acked-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index c69f84fe038d..b70a7fec1ff6 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -114,8 +114,10 @@ __test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
 
 int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
 {
-	unsigned long pfn;
+	unsigned long pfn, flags;
 	struct page *page;
+	struct zone *zone;
+	int ret;
 
 	pfn = start_pfn;
 	/*
@@ -131,7 +133,9 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
 	if (pfn < end_pfn)
 		return -EBUSY;
 	/* Check all pages are free or Marked as ISOLATED */
-	if (__test_page_isolated_in_pageblock(start_pfn, end_pfn))
-		return 0;
-	return -EBUSY;
+	zone = page_zone(pfn_to_page(pfn));
+	spin_lock_irqsave(&zone->lock, flags);
+	ret = __test_page_isolated_in_pageblock(start_pfn, end_pfn);
+	spin_unlock_irqrestore(&zone->lock, flags);
+	return ret ? 0 : -EBUSY;
 }

commit 0ed97ee470c36e05bcaad36c4fb4c501f383ce63
Author: David Woodhouse <David.Woodhouse@intel.com>
Date:   Mon Sep 1 11:10:28 2008 +0100

    Remove '#include <stddef.h>' from mm/page_isolation.c
    
    Signed-off-by: David Woodhouse <David.Woodhouse@intel.com>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 3444b58033c8..c69f84fe038d 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -2,7 +2,6 @@
  * linux/mm/page_isolation.c
  */
 
-#include <stddef.h>
 #include <linux/mm.h>
 #include <linux/page-isolation.h>
 #include <linux/pageblock-flags.h>

commit dbc0e4cefd003834440fe7ac5464616c5235cb94
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Wed Nov 14 16:59:12 2007 -0800

    memory hotremove: unset migrate type "ISOLATE" after removal
    
    We should unset migrate type "ISOLATE" when we successfully removed memory.
     But current code has BUG and cannot works well.
    
    This patch also includes bugfix?  to change get_pageblock_flags to
    get_pageblock_migratetype().
    
    Thanks to Badari Pulavarty for finding this.
    
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Acked-by: Badari Pulavarty <pbadari@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
index 8f92a29695cc..3444b58033c8 100644
--- a/mm/page_isolation.c
+++ b/mm/page_isolation.c
@@ -55,7 +55,7 @@ start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
 	return 0;
 undo:
 	for (pfn = start_pfn;
-	     pfn <= undo_pfn;
+	     pfn < undo_pfn;
 	     pfn += pageblock_nr_pages)
 		unset_migratetype_isolate(pfn_to_page(pfn));
 
@@ -76,7 +76,7 @@ undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
 	     pfn < end_pfn;
 	     pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
-		if (!page || get_pageblock_flags(page) != MIGRATE_ISOLATE)
+		if (!page || get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 			continue;
 		unset_migratetype_isolate(page);
 	}
@@ -126,7 +126,7 @@ int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
 	 */
 	for (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {
 		page = __first_valid_page(pfn, pageblock_nr_pages);
-		if (page && get_pageblock_flags(page) != MIGRATE_ISOLATE)
+		if (page && get_pageblock_migratetype(page) != MIGRATE_ISOLATE)
 			break;
 	}
 	if (pfn < end_pfn)

commit a5d76b54a3f3a40385d7f76069a2feac9f1bad63
Author: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
Date:   Tue Oct 16 01:26:11 2007 -0700

    memory unplug: page isolation
    
    Implement generic chunk-of-pages isolation method by using page grouping ops.
    
    This patch add MIGRATE_ISOLATE to MIGRATE_TYPES. By this
     - MIGRATE_TYPES increases.
     - bitmap for migratetype is enlarged.
    
    pages of MIGRATE_ISOLATE migratetype will not be allocated even if it is free.
    By this, you can isolated *freed* pages from users. How-to-free pages is not
    a purpose of this patch. You may use reclaim and migrate codes to free pages.
    
    If start_isolate_page_range(start,end) is called,
     - migratetype of the range turns to be MIGRATE_ISOLATE  if
       its type is MIGRATE_MOVABLE. (*) this check can be updated if other
       memory reclaiming works make progress.
     - MIGRATE_ISOLATE is not on migratetype fallback list.
     - All free pages and will-be-freed pages are isolated.
    To check all pages in the range are isolated or not,  use test_pages_isolated(),
    To cancel isolation, use undo_isolate_page_range().
    
    Changes V6 -> V7
     - removed unnecessary #ifdef
    
    There are HOLES_IN_ZONE handling codes...I'm glad if we can remove them..
    
    Signed-off-by: Yasunori Goto <y-goto@jp.fujitsu.com>
    Signed-off-by: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/page_isolation.c b/mm/page_isolation.c
new file mode 100644
index 000000000000..8f92a29695cc
--- /dev/null
+++ b/mm/page_isolation.c
@@ -0,0 +1,138 @@
+/*
+ * linux/mm/page_isolation.c
+ */
+
+#include <stddef.h>
+#include <linux/mm.h>
+#include <linux/page-isolation.h>
+#include <linux/pageblock-flags.h>
+#include "internal.h"
+
+static inline struct page *
+__first_valid_page(unsigned long pfn, unsigned long nr_pages)
+{
+	int i;
+	for (i = 0; i < nr_pages; i++)
+		if (pfn_valid_within(pfn + i))
+			break;
+	if (unlikely(i == nr_pages))
+		return NULL;
+	return pfn_to_page(pfn + i);
+}
+
+/*
+ * start_isolate_page_range() -- make page-allocation-type of range of pages
+ * to be MIGRATE_ISOLATE.
+ * @start_pfn: The lower PFN of the range to be isolated.
+ * @end_pfn: The upper PFN of the range to be isolated.
+ *
+ * Making page-allocation-type to be MIGRATE_ISOLATE means free pages in
+ * the range will never be allocated. Any free pages and pages freed in the
+ * future will not be allocated again.
+ *
+ * start_pfn/end_pfn must be aligned to pageblock_order.
+ * Returns 0 on success and -EBUSY if any part of range cannot be isolated.
+ */
+int
+start_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
+{
+	unsigned long pfn;
+	unsigned long undo_pfn;
+	struct page *page;
+
+	BUG_ON((start_pfn) & (pageblock_nr_pages - 1));
+	BUG_ON((end_pfn) & (pageblock_nr_pages - 1));
+
+	for (pfn = start_pfn;
+	     pfn < end_pfn;
+	     pfn += pageblock_nr_pages) {
+		page = __first_valid_page(pfn, pageblock_nr_pages);
+		if (page && set_migratetype_isolate(page)) {
+			undo_pfn = pfn;
+			goto undo;
+		}
+	}
+	return 0;
+undo:
+	for (pfn = start_pfn;
+	     pfn <= undo_pfn;
+	     pfn += pageblock_nr_pages)
+		unset_migratetype_isolate(pfn_to_page(pfn));
+
+	return -EBUSY;
+}
+
+/*
+ * Make isolated pages available again.
+ */
+int
+undo_isolate_page_range(unsigned long start_pfn, unsigned long end_pfn)
+{
+	unsigned long pfn;
+	struct page *page;
+	BUG_ON((start_pfn) & (pageblock_nr_pages - 1));
+	BUG_ON((end_pfn) & (pageblock_nr_pages - 1));
+	for (pfn = start_pfn;
+	     pfn < end_pfn;
+	     pfn += pageblock_nr_pages) {
+		page = __first_valid_page(pfn, pageblock_nr_pages);
+		if (!page || get_pageblock_flags(page) != MIGRATE_ISOLATE)
+			continue;
+		unset_migratetype_isolate(page);
+	}
+	return 0;
+}
+/*
+ * Test all pages in the range is free(means isolated) or not.
+ * all pages in [start_pfn...end_pfn) must be in the same zone.
+ * zone->lock must be held before call this.
+ *
+ * Returns 0 if all pages in the range is isolated.
+ */
+static int
+__test_page_isolated_in_pageblock(unsigned long pfn, unsigned long end_pfn)
+{
+	struct page *page;
+
+	while (pfn < end_pfn) {
+		if (!pfn_valid_within(pfn)) {
+			pfn++;
+			continue;
+		}
+		page = pfn_to_page(pfn);
+		if (PageBuddy(page))
+			pfn += 1 << page_order(page);
+		else if (page_count(page) == 0 &&
+				page_private(page) == MIGRATE_ISOLATE)
+			pfn += 1;
+		else
+			break;
+	}
+	if (pfn < end_pfn)
+		return 0;
+	return 1;
+}
+
+int test_pages_isolated(unsigned long start_pfn, unsigned long end_pfn)
+{
+	unsigned long pfn;
+	struct page *page;
+
+	pfn = start_pfn;
+	/*
+	 * Note: pageblock_nr_page != MAX_ORDER. Then, chunks of free page
+	 * is not aligned to pageblock_nr_pages.
+	 * Then we just check pagetype fist.
+	 */
+	for (pfn = start_pfn; pfn < end_pfn; pfn += pageblock_nr_pages) {
+		page = __first_valid_page(pfn, pageblock_nr_pages);
+		if (page && get_pageblock_flags(page) != MIGRATE_ISOLATE)
+			break;
+	}
+	if (pfn < end_pfn)
+		return -EBUSY;
+	/* Check all pages are free or Marked as ISOLATED */
+	if (__test_page_isolated_in_pageblock(start_pfn, end_pfn))
+		return 0;
+	return -EBUSY;
+}
