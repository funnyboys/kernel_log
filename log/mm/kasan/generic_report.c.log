commit 8cceeff48f23eede76de995df08cf665182ec8fb
Author: Walter Wu <walter-zh.wu@mediatek.com>
Date:   Wed Apr 1 21:09:37 2020 -0700

    kasan: detect negative size in memory operation function
    
    Patch series "fix the missing underflow in memory operation function", v4.
    
    The patchset helps to produce a KASAN report when size is negative in
    memory operation functions.  It is helpful for programmer to solve an
    undefined behavior issue.  Patch 1 based on Dmitry's review and
    suggestion, patch 2 is a test in order to verify the patch 1.
    
    [1]https://bugzilla.kernel.org/show_bug.cgi?id=199341
    [2]https://lore.kernel.org/linux-arm-kernel/20190927034338.15813-1-walter-zh.wu@mediatek.com/
    
    This patch (of 2):
    
    KASAN missed detecting size is a negative number in memset(), memcpy(),
    and memmove(), it will cause out-of-bounds bug.  So needs to be detected
    by KASAN.
    
    If size is a negative number, then it has a reason to be defined as
    out-of-bounds bug type.  Casting negative numbers to size_t would indeed
    turn up as a large size_t and its value will be larger than ULONG_MAX/2,
    so that this can qualify as out-of-bounds.
    
    KASAN report is shown below:
    
     BUG: KASAN: out-of-bounds in kmalloc_memmove_invalid_size+0x70/0xa0
     Read of size 18446744073709551608 at addr ffffff8069660904 by task cat/72
    
     CPU: 2 PID: 72 Comm: cat Not tainted 5.4.0-rc1-next-20191004ajb-00001-gdb8af2f372b2-dirty #1
     Hardware name: linux,dummy-virt (DT)
     Call trace:
      dump_backtrace+0x0/0x288
      show_stack+0x14/0x20
      dump_stack+0x10c/0x164
      print_address_description.isra.9+0x68/0x378
      __kasan_report+0x164/0x1a0
      kasan_report+0xc/0x18
      check_memory_region+0x174/0x1d0
      memmove+0x34/0x88
      kmalloc_memmove_invalid_size+0x70/0xa0
    
    [1] https://bugzilla.kernel.org/show_bug.cgi?id=199341
    
    [cai@lca.pw: fix -Wdeclaration-after-statement warn]
      Link: http://lkml.kernel.org/r/1583509030-27939-1-git-send-email-cai@lca.pw
    [peterz@infradead.org: fix objtool warning]
      Link: http://lkml.kernel.org/r/20200305095436.GV2596@hirez.programming.kicks-ass.net
    Reported-by: kernel test robot <lkp@intel.com>
    Reported-by: Dmitry Vyukov <dvyukov@google.com>
    Suggested-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Walter Wu <walter-zh.wu@mediatek.com>
    Signed-off-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Link: http://lkml.kernel.org/r/20191112065302.7015-1-walter-zh.wu@mediatek.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index 2d97efd4954f..e200acb2d292 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -110,6 +110,17 @@ static const char *get_wild_bug_type(struct kasan_access_info *info)
 
 const char *get_bug_type(struct kasan_access_info *info)
 {
+	/*
+	 * If access_size is a negative number, then it has reason to be
+	 * defined as out-of-bounds bug type.
+	 *
+	 * Casting negative numbers to size_t would indeed turn up as
+	 * a large size_t and its value will be larger than ULONG_MAX/2,
+	 * so that this can qualify as out-of-bounds.
+	 */
+	if (info->access_addr + info->access_size < info->access_addr)
+		return "out-of-bounds";
+
 	if (addr_has_shadow(info->access_addr))
 		return get_shadow_bug_type(info);
 	return get_wild_bug_type(info);

commit 3c5c3cfb9ef4da957e3357a2bd36f76ee34c0862
Author: Daniel Axtens <dja@axtens.net>
Date:   Sat Nov 30 17:54:50 2019 -0800

    kasan: support backing vmalloc space with real shadow memory
    
    Patch series "kasan: support backing vmalloc space with real shadow
    memory", v11.
    
    Currently, vmalloc space is backed by the early shadow page.  This means
    that kasan is incompatible with VMAP_STACK.
    
    This series provides a mechanism to back vmalloc space with real,
    dynamically allocated memory.  I have only wired up x86, because that's
    the only currently supported arch I can work with easily, but it's very
    easy to wire up other architectures, and it appears that there is some
    work-in-progress code to do this on arm64 and s390.
    
    This has been discussed before in the context of VMAP_STACK:
     - https://bugzilla.kernel.org/show_bug.cgi?id=202009
     - https://lkml.org/lkml/2018/7/22/198
     - https://lkml.org/lkml/2019/7/19/822
    
    In terms of implementation details:
    
    Most mappings in vmalloc space are small, requiring less than a full
    page of shadow space.  Allocating a full shadow page per mapping would
    therefore be wasteful.  Furthermore, to ensure that different mappings
    use different shadow pages, mappings would have to be aligned to
    KASAN_SHADOW_SCALE_SIZE * PAGE_SIZE.
    
    Instead, share backing space across multiple mappings.  Allocate a
    backing page when a mapping in vmalloc space uses a particular page of
    the shadow region.  This page can be shared by other vmalloc mappings
    later on.
    
    We hook in to the vmap infrastructure to lazily clean up unused shadow
    memory.
    
    Testing with test_vmalloc.sh on an x86 VM with 2 vCPUs shows that:
    
     - Turning on KASAN, inline instrumentation, without vmalloc, introuduces
       a 4.1x-4.2x slowdown in vmalloc operations.
    
     - Turning this on introduces the following slowdowns over KASAN:
         * ~1.76x slower single-threaded (test_vmalloc.sh performance)
         * ~2.18x slower when both cpus are performing operations
           simultaneously (test_vmalloc.sh sequential_test_order=1)
    
    This is unfortunate but given that this is a debug feature only, not the
    end of the world.  The benchmarks are also a stress-test for the vmalloc
    subsystem: they're not indicative of an overall 2x slowdown!
    
    This patch (of 4):
    
    Hook into vmalloc and vmap, and dynamically allocate real shadow memory
    to back the mappings.
    
    Most mappings in vmalloc space are small, requiring less than a full
    page of shadow space.  Allocating a full shadow page per mapping would
    therefore be wasteful.  Furthermore, to ensure that different mappings
    use different shadow pages, mappings would have to be aligned to
    KASAN_SHADOW_SCALE_SIZE * PAGE_SIZE.
    
    Instead, share backing space across multiple mappings.  Allocate a
    backing page when a mapping in vmalloc space uses a particular page of
    the shadow region.  This page can be shared by other vmalloc mappings
    later on.
    
    We hook in to the vmap infrastructure to lazily clean up unused shadow
    memory.
    
    To avoid the difficulties around swapping mappings around, this code
    expects that the part of the shadow region that covers the vmalloc space
    will not be covered by the early shadow page, but will be left unmapped.
    This will require changes in arch-specific code.
    
    This allows KASAN with VMAP_STACK, and may be helpful for architectures
    that do not have a separate module space (e.g.  powerpc64, which I am
    currently working on).  It also allows relaxing the module alignment
    back to PAGE_SIZE.
    
    Testing with test_vmalloc.sh on an x86 VM with 2 vCPUs shows that:
    
     - Turning on KASAN, inline instrumentation, without vmalloc, introuduces
       a 4.1x-4.2x slowdown in vmalloc operations.
    
     - Turning this on introduces the following slowdowns over KASAN:
         * ~1.76x slower single-threaded (test_vmalloc.sh performance)
         * ~2.18x slower when both cpus are performing operations
           simultaneously (test_vmalloc.sh sequential_test_order=3D1)
    
    This is unfortunate but given that this is a debug feature only, not the
    end of the world.
    
    The full benchmark results are:
    
    Performance
    
                                  No KASAN      KASAN original x baseline  KASAN vmalloc x baseline    x KASAN
    
    fix_size_alloc_test             662004            11404956      17.23       19144610      28.92       1.68
    full_fit_alloc_test             710950            12029752      16.92       13184651      18.55       1.10
    long_busy_list_alloc_test      9431875            43990172       4.66       82970178       8.80       1.89
    random_size_alloc_test         5033626            23061762       4.58       47158834       9.37       2.04
    fix_align_alloc_test           1252514            15276910      12.20       31266116      24.96       2.05
    random_size_align_alloc_te     1648501            14578321       8.84       25560052      15.51       1.75
    align_shift_alloc_test             147                 830       5.65           5692      38.72       6.86
    pcpu_alloc_test                  80732              125520       1.55         140864       1.74       1.12
    Total Cycles              119240774314        763211341128       6.40  1390338696894      11.66       1.82
    
    Sequential, 2 cpus
    
                                  No KASAN      KASAN original x baseline  KASAN vmalloc x baseline    x KASAN
    
    fix_size_alloc_test            1423150            14276550      10.03       27733022      19.49       1.94
    full_fit_alloc_test            1754219            14722640       8.39       15030786       8.57       1.02
    long_busy_list_alloc_test     11451858            52154973       4.55      107016027       9.34       2.05
    random_size_alloc_test         5989020            26735276       4.46       68885923      11.50       2.58
    fix_align_alloc_test           2050976            20166900       9.83       50491675      24.62       2.50
    random_size_align_alloc_te     2858229            17971700       6.29       38730225      13.55       2.16
    align_shift_alloc_test             405                6428      15.87          26253      64.82       4.08
    pcpu_alloc_test                 127183              151464       1.19         216263       1.70       1.43
    Total Cycles               54181269392        308723699764       5.70   650772566394      12.01       2.11
    fix_size_alloc_test            1420404            14289308      10.06       27790035      19.56       1.94
    full_fit_alloc_test            1736145            14806234       8.53       15274301       8.80       1.03
    long_busy_list_alloc_test     11404638            52270785       4.58      107550254       9.43       2.06
    random_size_alloc_test         6017006            26650625       4.43       68696127      11.42       2.58
    fix_align_alloc_test           2045504            20280985       9.91       50414862      24.65       2.49
    random_size_align_alloc_te     2845338            17931018       6.30       38510276      13.53       2.15
    align_shift_alloc_test             472                3760       7.97           9656      20.46       2.57
    pcpu_alloc_test                 118643              132732       1.12         146504       1.23       1.10
    Total Cycles               54040011688        309102805492       5.72   651325675652      12.05       2.11
    
    [dja@axtens.net: fixups]
      Link: http://lkml.kernel.org/r/20191120052719.7201-1-dja@axtens.net
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=3D202009
    Link: http://lkml.kernel.org/r/20191031093909.9228-2-dja@axtens.net
    Signed-off-by: Mark Rutland <mark.rutland@arm.com> [shadow rework]
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Co-developed-by: Mark Rutland <mark.rutland@arm.com>
    Acked-by: Vasily Gorbik <gor@linux.ibm.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Christophe Leroy <christophe.leroy@c-s.fr>
    Cc: Qian Cai <cai@lca.pw>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index 36c645939bc9..2d97efd4954f 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -86,6 +86,9 @@ static const char *get_shadow_bug_type(struct kasan_access_info *info)
 	case KASAN_ALLOCA_RIGHT:
 		bug_type = "alloca-out-of-bounds";
 		break;
+	case KASAN_VMALLOC_INVALID:
+		bug_type = "vmalloc-out-of-bounds";
+		break;
 	}
 
 	return bug_type;

commit 7771bdbbfd3d6f204631b6fd9e1bbc30cd15918e
Author: Andrey Ryabinin <aryabinin@virtuozzo.com>
Date:   Tue Mar 5 15:41:20 2019 -0800

    kasan: remove use after scope bugs detection.
    
    Use after scope bugs detector seems to be almost entirely useless for
    the linux kernel.  It exists over two years, but I've seen only one
    valid bug so far [1].  And the bug was fixed before it has been
    reported.  There were some other use-after-scope reports, but they were
    false-positives due to different reasons like incompatibility with
    structleak plugin.
    
    This feature significantly increases stack usage, especially with GCC <
    9 version, and causes a 32K stack overflow.  It probably adds
    performance penalty too.
    
    Given all that, let's remove use-after-scope detector entirely.
    
    While preparing this patch I've noticed that we mistakenly enable
    use-after-scope detection for clang compiler regardless of
    CONFIG_KASAN_EXTRA setting.  This is also fixed now.
    
    [1] http://lkml.kernel.org/r/<20171129052106.rhgbjhhis53hkgfn@wfg-t540p.sh.intel.com>
    
    Link: http://lkml.kernel.org/r/20190111185842.13978-1-aryabinin@virtuozzo.com
    Signed-off-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Acked-by: Will Deacon <will.deacon@arm.com>             [arm64]
    Cc: Qian Cai <cai@lca.pw>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index 5e12035888f2..36c645939bc9 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -82,9 +82,6 @@ static const char *get_shadow_bug_type(struct kasan_access_info *info)
 	case KASAN_KMALLOC_FREE:
 		bug_type = "use-after-free";
 		break;
-	case KASAN_USE_AFTER_SCOPE:
-		bug_type = "use-after-scope";
-		break;
 	case KASAN_ALLOCA_LEFT:
 	case KASAN_ALLOCA_RIGHT:
 		bug_type = "alloca-out-of-bounds";

commit e886bf9d9abedf8236464bfd21bc5707748b4a02
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:31:14 2018 -0800

    kasan: add SPDX-License-Identifier mark to source files
    
    This patch adds a "SPDX-License-Identifier: GPL-2.0" mark to all source
    files under mm/kasan.
    
    Link: http://lkml.kernel.org/r/bce2d1e618afa5142e81961ab8fa4b4165337380.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index a4604cceae59..5e12035888f2 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * This file contains generic KASAN specific error reporting code.
  *

commit 121e8f81d38cc43834195722d0768340dc130a33
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:42 2018 -0800

    kasan: add bug reporting routines for tag-based mode
    
    This commit adds rountines, that print tag-based KASAN error reports.
    Those are quite similar to generic KASAN, the difference is:
    
    1. The way tag-based KASAN finds the first bad shadow cell (with a
       mismatching tag). Tag-based KASAN compares memory tags from the shadow
       memory to the pointer tag.
    
    2. Tag-based KASAN reports all bugs with the "KASAN: invalid-access"
       header.
    
    Also simplify generic KASAN find_first_bad_addr.
    
    Link: http://lkml.kernel.org/r/aee6897b1bd077732a315fd84c6b4f234dbfdfcb.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
index 5201d1770700..a4604cceae59 100644
--- a/mm/kasan/generic_report.c
+++ b/mm/kasan/generic_report.c
@@ -33,16 +33,13 @@
 #include "kasan.h"
 #include "../slab.h"
 
-static const void *find_first_bad_addr(const void *addr, size_t size)
+void *find_first_bad_addr(void *addr, size_t size)
 {
-	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
-	const void *first_bad_addr = addr;
+	void *p = addr;
 
-	while (!shadow_val && first_bad_addr < addr + size) {
-		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
-		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
-	}
-	return first_bad_addr;
+	while (p < addr + size && !(*(u8 *)kasan_mem_to_shadow(p)))
+		p += KASAN_SHADOW_SCALE_SIZE;
+	return p;
 }
 
 static const char *get_shadow_bug_type(struct kasan_access_info *info)
@@ -50,9 +47,6 @@ static const char *get_shadow_bug_type(struct kasan_access_info *info)
 	const char *bug_type = "unknown-crash";
 	u8 *shadow_addr;
 
-	info->first_bad_addr = find_first_bad_addr(info->access_addr,
-						info->access_size);
-
 	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
 
 	/*

commit 11cd3cd69a256a353dd1a249b48ccd727d945952
Author: Andrey Konovalov <andreyknvl@google.com>
Date:   Fri Dec 28 00:30:38 2018 -0800

    kasan: split out generic_report.c from report.c
    
    Move generic KASAN specific error reporting routines to generic_report.c
    without any functional changes, leaving common error reporting code in
    report.c to be later reused by tag-based KASAN.
    
    Link: http://lkml.kernel.org/r/ba48c32f8e5aefedee78998ccff0413bee9e0f5b.1544099024.git.andreyknvl@google.com
    Signed-off-by: Andrey Konovalov <andreyknvl@google.com>
    Reviewed-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Reviewed-by: Dmitry Vyukov <dvyukov@google.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Mark Rutland <mark.rutland@arm.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/mm/kasan/generic_report.c b/mm/kasan/generic_report.c
new file mode 100644
index 000000000000..5201d1770700
--- /dev/null
+++ b/mm/kasan/generic_report.c
@@ -0,0 +1,158 @@
+/*
+ * This file contains generic KASAN specific error reporting code.
+ *
+ * Copyright (c) 2014 Samsung Electronics Co., Ltd.
+ * Author: Andrey Ryabinin <ryabinin.a.a@gmail.com>
+ *
+ * Some code borrowed from https://github.com/xairy/kasan-prototype by
+ *        Andrey Konovalov <andreyknvl@gmail.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/bitops.h>
+#include <linux/ftrace.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/stackdepot.h>
+#include <linux/stacktrace.h>
+#include <linux/string.h>
+#include <linux/types.h>
+#include <linux/kasan.h>
+#include <linux/module.h>
+
+#include <asm/sections.h>
+
+#include "kasan.h"
+#include "../slab.h"
+
+static const void *find_first_bad_addr(const void *addr, size_t size)
+{
+	u8 shadow_val = *(u8 *)kasan_mem_to_shadow(addr);
+	const void *first_bad_addr = addr;
+
+	while (!shadow_val && first_bad_addr < addr + size) {
+		first_bad_addr += KASAN_SHADOW_SCALE_SIZE;
+		shadow_val = *(u8 *)kasan_mem_to_shadow(first_bad_addr);
+	}
+	return first_bad_addr;
+}
+
+static const char *get_shadow_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+	u8 *shadow_addr;
+
+	info->first_bad_addr = find_first_bad_addr(info->access_addr,
+						info->access_size);
+
+	shadow_addr = (u8 *)kasan_mem_to_shadow(info->first_bad_addr);
+
+	/*
+	 * If shadow byte value is in [0, KASAN_SHADOW_SCALE_SIZE) we can look
+	 * at the next shadow byte to determine the type of the bad access.
+	 */
+	if (*shadow_addr > 0 && *shadow_addr <= KASAN_SHADOW_SCALE_SIZE - 1)
+		shadow_addr++;
+
+	switch (*shadow_addr) {
+	case 0 ... KASAN_SHADOW_SCALE_SIZE - 1:
+		/*
+		 * In theory it's still possible to see these shadow values
+		 * due to a data race in the kernel code.
+		 */
+		bug_type = "out-of-bounds";
+		break;
+	case KASAN_PAGE_REDZONE:
+	case KASAN_KMALLOC_REDZONE:
+		bug_type = "slab-out-of-bounds";
+		break;
+	case KASAN_GLOBAL_REDZONE:
+		bug_type = "global-out-of-bounds";
+		break;
+	case KASAN_STACK_LEFT:
+	case KASAN_STACK_MID:
+	case KASAN_STACK_RIGHT:
+	case KASAN_STACK_PARTIAL:
+		bug_type = "stack-out-of-bounds";
+		break;
+	case KASAN_FREE_PAGE:
+	case KASAN_KMALLOC_FREE:
+		bug_type = "use-after-free";
+		break;
+	case KASAN_USE_AFTER_SCOPE:
+		bug_type = "use-after-scope";
+		break;
+	case KASAN_ALLOCA_LEFT:
+	case KASAN_ALLOCA_RIGHT:
+		bug_type = "alloca-out-of-bounds";
+		break;
+	}
+
+	return bug_type;
+}
+
+static const char *get_wild_bug_type(struct kasan_access_info *info)
+{
+	const char *bug_type = "unknown-crash";
+
+	if ((unsigned long)info->access_addr < PAGE_SIZE)
+		bug_type = "null-ptr-deref";
+	else if ((unsigned long)info->access_addr < TASK_SIZE)
+		bug_type = "user-memory-access";
+	else
+		bug_type = "wild-memory-access";
+
+	return bug_type;
+}
+
+const char *get_bug_type(struct kasan_access_info *info)
+{
+	if (addr_has_shadow(info->access_addr))
+		return get_shadow_bug_type(info);
+	return get_wild_bug_type(info);
+}
+
+#define DEFINE_ASAN_REPORT_LOAD(size)                     \
+void __asan_report_load##size##_noabort(unsigned long addr) \
+{                                                         \
+	kasan_report(addr, size, false, _RET_IP_);	  \
+}                                                         \
+EXPORT_SYMBOL(__asan_report_load##size##_noabort)
+
+#define DEFINE_ASAN_REPORT_STORE(size)                     \
+void __asan_report_store##size##_noabort(unsigned long addr) \
+{                                                          \
+	kasan_report(addr, size, true, _RET_IP_);	   \
+}                                                          \
+EXPORT_SYMBOL(__asan_report_store##size##_noabort)
+
+DEFINE_ASAN_REPORT_LOAD(1);
+DEFINE_ASAN_REPORT_LOAD(2);
+DEFINE_ASAN_REPORT_LOAD(4);
+DEFINE_ASAN_REPORT_LOAD(8);
+DEFINE_ASAN_REPORT_LOAD(16);
+DEFINE_ASAN_REPORT_STORE(1);
+DEFINE_ASAN_REPORT_STORE(2);
+DEFINE_ASAN_REPORT_STORE(4);
+DEFINE_ASAN_REPORT_STORE(8);
+DEFINE_ASAN_REPORT_STORE(16);
+
+void __asan_report_load_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, false, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_load_n_noabort);
+
+void __asan_report_store_n_noabort(unsigned long addr, size_t size)
+{
+	kasan_report(addr, size, true, _RET_IP_);
+}
+EXPORT_SYMBOL(__asan_report_store_n_noabort);
