commit 37d1a04b13a6d2fec91a6813fc034947a27db034
Merge: 37f8173dd849 97a9474aeb78
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Jun 11 20:02:46 2020 +0200

    Rebase locking/kcsan to locking/urgent
    
    Merge the state of the locking kcsan branch before the read/write_once()
    and the atomics modifications got merged.
    
    Squash the fallout of the rebase on top of the read/write once and atomic
    fallback work into the merge. The history of the original branch is
    preserved in tag locking-kcsan-2020-06-02.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit e31cf2f4ca422ac9b14ecc4a1295b8977a20f812
Author: Mike Rapoport <rppt@linux.ibm.com>
Date:   Mon Jun 8 21:32:33 2020 -0700

    mm: don't include asm/pgtable.h if linux/mm.h is already included
    
    Patch series "mm: consolidate definitions of page table accessors", v2.
    
    The low level page table accessors (pXY_index(), pXY_offset()) are
    duplicated across all architectures and sometimes more than once.  For
    instance, we have 31 definition of pgd_offset() for 25 supported
    architectures.
    
    Most of these definitions are actually identical and typically it boils
    down to, e.g.
    
    static inline unsigned long pmd_index(unsigned long address)
    {
            return (address >> PMD_SHIFT) & (PTRS_PER_PMD - 1);
    }
    
    static inline pmd_t *pmd_offset(pud_t *pud, unsigned long address)
    {
            return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(address);
    }
    
    These definitions can be shared among 90% of the arches provided
    XYZ_SHIFT, PTRS_PER_XYZ and xyz_page_vaddr() are defined.
    
    For architectures that really need a custom version there is always
    possibility to override the generic version with the usual ifdefs magic.
    
    These patches introduce include/linux/pgtable.h that replaces
    include/asm-generic/pgtable.h and add the definitions of the page table
    accessors to the new header.
    
    This patch (of 12):
    
    The linux/mm.h header includes <asm/pgtable.h> to allow inlining of the
    functions involving page table manipulations, e.g.  pte_alloc() and
    pmd_alloc().  So, there is no point to explicitly include <asm/pgtable.h>
    in the files that include <linux/mm.h>.
    
    The include statements in such cases are remove with a simple loop:
    
            for f in $(git grep -l "include <linux/mm.h>") ; do
                    sed -i -e '/include <asm\/pgtable.h>/ d' $f
            done
    
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Cain <bcain@codeaurora.org>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Greentime Hu <green.hu@gmail.com>
    Cc: Greg Ungerer <gerg@linux-m68k.org>
    Cc: Guan Xuetao <gxt@pku.edu.cn>
    Cc: Guo Ren <guoren@kernel.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Ley Foon Tan <ley.foon.tan@intel.com>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Max Filippov <jcmvbkbc@gmail.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Rapoport <rppt@kernel.org>
    Cc: Nick Hu <nickhu@andestech.com>
    Cc: Paul Walmsley <paul.walmsley@sifive.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Stafford Horne <shorne@gmail.com>
    Cc: Thomas Bogendoerfer <tsbogend@alpha.franken.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Vincent Chen <deanbo422@gmail.com>
    Cc: Vineet Gupta <vgupta@synopsys.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20200514170327.31389-1-rppt@kernel.org
    Link: http://lkml.kernel.org/r/20200514170327.31389-2-rppt@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/init/init_task.c b/init/init_task.c
index 15303d58d9db..a90ac70cc515 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -13,7 +13,6 @@
 #include <linux/numa.h>
 #include <linux/scs.h>
 
-#include <asm/pgtable.h>
 #include <linux/uaccess.h>
 
 static struct signal_struct init_signals = {

commit 533b220f7be4e461a5222a223d169b42856741ef
Merge: 3ee3723b40d5 082af5ec5080
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 1 15:18:27 2020 -0700

    Merge tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux
    
    Pull arm64 updates from Will Deacon:
     "A sizeable pile of arm64 updates for 5.8.
    
      Summary below, but the big two features are support for Branch Target
      Identification and Clang's Shadow Call stack. The latter is currently
      arm64-only, but the high-level parts are all in core code so it could
      easily be adopted by other architectures pending toolchain support
    
      Branch Target Identification (BTI):
    
       - Support for ARMv8.5-BTI in both user- and kernel-space. This allows
         branch targets to limit the types of branch from which they can be
         called and additionally prevents branching to arbitrary code,
         although kernel support requires a very recent toolchain.
    
       - Function annotation via SYM_FUNC_START() so that assembly functions
         are wrapped with the relevant "landing pad" instructions.
    
       - BPF and vDSO updates to use the new instructions.
    
       - Addition of a new HWCAP and exposure of BTI capability to userspace
         via ID register emulation, along with ELF loader support for the
         BTI feature in .note.gnu.property.
    
       - Non-critical fixes to CFI unwind annotations in the sigreturn
         trampoline.
    
      Shadow Call Stack (SCS):
    
       - Support for Clang's Shadow Call Stack feature, which reserves
         platform register x18 to point at a separate stack for each task
         that holds only return addresses. This protects function return
         control flow from buffer overruns on the main stack.
    
       - Save/restore of x18 across problematic boundaries (user-mode,
         hypervisor, EFI, suspend, etc).
    
       - Core support for SCS, should other architectures want to use it
         too.
    
       - SCS overflow checking on context-switch as part of the existing
         stack limit check if CONFIG_SCHED_STACK_END_CHECK=y.
    
      CPU feature detection:
    
       - Removed numerous "SANITY CHECK" errors when running on a system
         with mismatched AArch32 support at EL1. This is primarily a concern
         for KVM, which disabled support for 32-bit guests on such a system.
    
       - Addition of new ID registers and fields as the architecture has
         been extended.
    
      Perf and PMU drivers:
    
       - Minor fixes and cleanups to system PMU drivers.
    
      Hardware errata:
    
       - Unify KVM workarounds for VHE and nVHE configurations.
    
       - Sort vendor errata entries in Kconfig.
    
      Secure Monitor Call Calling Convention (SMCCC):
    
       - Update to the latest specification from Arm (v1.2).
    
       - Allow PSCI code to query the SMCCC version.
    
      Software Delegated Exception Interface (SDEI):
    
       - Unexport a bunch of unused symbols.
    
       - Minor fixes to handling of firmware data.
    
      Pointer authentication:
    
       - Add support for dumping the kernel PAC mask in vmcoreinfo so that
         the stack can be unwound by tools such as kdump.
    
       - Simplification of key initialisation during CPU bringup.
    
      BPF backend:
    
       - Improve immediate generation for logical and add/sub instructions.
    
      vDSO:
    
       - Minor fixes to the linker flags for consistency with other
         architectures and support for LLVM's unwinder.
    
       - Clean up logic to initialise and map the vDSO into userspace.
    
      ACPI:
    
       - Work around for an ambiguity in the IORT specification relating to
         the "num_ids" field.
    
       - Support _DMA method for all named components rather than only PCIe
         root complexes.
    
       - Minor other IORT-related fixes.
    
      Miscellaneous:
    
       - Initialise debug traps early for KGDB and fix KDB cacheflushing
         deadlock.
    
       - Minor tweaks to early boot state (documentation update, set
         TEXT_OFFSET to 0x0, increase alignment of PE/COFF sections).
    
       - Refactoring and cleanup"
    
    * tag 'arm64-upstream' of git://git.kernel.org/pub/scm/linux/kernel/git/arm64/linux: (148 commits)
      KVM: arm64: Move __load_guest_stage2 to kvm_mmu.h
      KVM: arm64: Check advertised Stage-2 page size capability
      arm64/cpufeature: Add get_arm64_ftr_reg_nowarn()
      ACPI/IORT: Remove the unused __get_pci_rid()
      arm64/cpuinfo: Add ID_MMFR4_EL1 into the cpuinfo_arm64 context
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR1 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64PFR0 register
      arm64/cpufeature: Add remaining feature bits in ID_AA64ISAR0 register
      arm64/cpufeature: Add remaining feature bits in ID_MMFR4 register
      arm64/cpufeature: Add remaining feature bits in ID_PFR0 register
      arm64/cpufeature: Introduce ID_MMFR5 CPU register
      arm64/cpufeature: Introduce ID_DFR1 CPU register
      arm64/cpufeature: Introduce ID_PFR2 CPU register
      arm64/cpufeature: Make doublelock a signed feature in ID_AA64DFR0
      arm64/cpufeature: Drop TraceFilt feature exposure from ID_DFR0 register
      arm64/cpufeature: Add explicit ftr_id_isar0[] for ID_ISAR0 register
      arm64: mm: Add asid_gen_match() helper
      firmware: smccc: Fix missing prototype warning for arm_smccc_version_init
      arm64: vdso: Fix CFI directives in sigreturn trampoline
      arm64: vdso: Don't prefix sigreturn trampoline with a BTI C instruction
      ...

commit d08b9f0ca6605e13dcb48f04e55a30545b3c71eb
Author: Sami Tolvanen <samitolvanen@google.com>
Date:   Mon Apr 27 09:00:07 2020 -0700

    scs: Add support for Clang's Shadow Call Stack (SCS)
    
    This change adds generic support for Clang's Shadow Call Stack,
    which uses a shadow stack to protect return addresses from being
    overwritten by an attacker. Details are available here:
    
      https://clang.llvm.org/docs/ShadowCallStack.html
    
    Note that security guarantees in the kernel differ from the ones
    documented for user space. The kernel must store addresses of
    shadow stacks in memory, which means an attacker capable reading
    and writing arbitrary memory may be able to locate them and hijack
    control flow by modifying the stacks.
    
    Signed-off-by: Sami Tolvanen <samitolvanen@google.com>
    Reviewed-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
    [will: Numerous cosmetic changes]
    Signed-off-by: Will Deacon <will@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index bd403ed3e418..169e34066d35 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -11,6 +11,7 @@
 #include <linux/mm.h>
 #include <linux/audit.h>
 #include <linux/numa.h>
+#include <linux/scs.h>
 
 #include <asm/pgtable.h>
 #include <linux/uaccess.h>
@@ -50,6 +51,13 @@ static struct sighand_struct init_sighand = {
 	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(init_sighand.signalfd_wqh),
 };
 
+#ifdef CONFIG_SHADOW_CALL_STACK
+unsigned long init_shadow_call_stack[SCS_SIZE / sizeof(long)]
+		__init_task_data = {
+	[(SCS_SIZE / sizeof(long)) - 1] = SCS_END_MAGIC
+};
+#endif
+
 /*
  * Set up the first task table, touch at your own risk!. Base=0,
  * limit=0x1fffff (=2MB)

commit 97a9474aeb789183a1d0712e66a4283860279ac9
Merge: 3b02a051d25d 50a19ad4b1ec
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 8 14:58:28 2020 +0200

    Merge branch 'kcsan-for-tip' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into locking/kcsan
    
    Pull KCSAN updates from Paul McKenney.

commit 276c410448dbca357a2bc3539acfe04862e5f172
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Tue Mar 17 16:02:06 2020 -0700

    rcu-tasks: Split ->trc_reader_need_end
    
    This commit splits ->trc_reader_need_end by using the rcu_special union.
    This change permits readers to check to see if a memory barrier is
    required without any added overhead in the common case where no such
    barrier is required.  This commit also adds the read-side checking.
    Later commits will add the machinery to properly set the new
    ->trc_reader_special.b.need_mb field.
    
    This commit also makes rcu_read_unlock_trace_special() tolerate nested
    read-side critical sections within interrupt and NMI handlers.
    
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index e8b3740ee598..825972daec32 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -143,6 +143,7 @@ struct task_struct init_task
 #endif
 #ifdef CONFIG_TASKS_TRACE_RCU
 	.trc_reader_nesting = 0,
+	.trc_reader_special.s = 0,
 	.trc_holdout_list = LIST_HEAD_INIT(init_task.trc_holdout_list),
 #endif
 #ifdef CONFIG_CPUSETS

commit d5f177d35c24429c87db2567d20563fc16f7e8f6
Author: Paul E. McKenney <paulmck@kernel.org>
Date:   Mon Mar 9 19:56:53 2020 -0700

    rcu-tasks: Add an RCU Tasks Trace to simplify protection of tracing hooks
    
    Because RCU does not watch exception early-entry/late-exit, idle-loop,
    or CPU-hotplug execution, protection of tracing and BPF operations is
    needlessly complicated.  This commit therefore adds a variant of
    Tasks RCU that:
    
    o       Has explicit read-side markers to allow finite grace periods in
            the face of in-kernel loops for PREEMPT=n builds.  These markers
            are rcu_read_lock_trace() and rcu_read_unlock_trace().
    
    o       Protects code in the idle loop, exception entry/exit, and
            CPU-hotplug code paths.  In this respect, RCU-tasks trace is
            similar to SRCU, but with lighter-weight readers.
    
    o       Avoids expensive read-side instruction, having overhead similar
            to that of Preemptible RCU.
    
    There are of course downsides:
    
    o       The grace-period code can send IPIs to CPUs, even when those
            CPUs are in the idle loop or in nohz_full userspace.  This is
            mitigated by later commits.
    
    o       It is necessary to scan the full tasklist, much as for Tasks RCU.
    
    o       There is a single callback queue guarded by a single lock,
            again, much as for Tasks RCU.  However, those early use cases
            that request multiple grace periods in quick succession are
            expected to do so from a single task, which makes the single
            lock almost irrelevant.  If needed, multiple callback queues
            can be provided using any number of schemes.
    
    Perhaps most important, this variant of RCU does not affect the vanilla
    flavors, rcu_preempt and rcu_sched.  The fact that RCU Tasks Trace
    readers can operate from idle, offline, and exception entry/exit in no
    way enables rcu_preempt and rcu_sched readers to do so.
    
    The memory ordering was outlined here:
    https://lore.kernel.org/lkml/20200319034030.GX3199@paulmck-ThinkPad-P72/
    
    This effort benefited greatly from off-list discussions of BPF
    requirements with Alexei Starovoitov and Andrii Nakryiko.  At least
    some of the on-list discussions are captured in the Link: tags below.
    In addition, KCSAN was quite helpful in finding some early bugs.
    
    Link: https://lore.kernel.org/lkml/20200219150744.428764577@infradead.org/
    Link: https://lore.kernel.org/lkml/87mu8p797b.fsf@nanos.tec.linutronix.de/
    Link: https://lore.kernel.org/lkml/20200225221305.605144982@linutronix.de/
    Cc: Alexei Starovoitov <alexei.starovoitov@gmail.com>
    Cc: Andrii Nakryiko <andriin@fb.com>
    [ paulmck: Apply feedback from Steve Rostedt and Joel Fernandes. ]
    [ paulmck: Decrement trc_n_readers_need_end upon IPI failure. ]
    [ paulmck: Fix locking issue reported by rcutorture. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index bd403ed3e418..e8b3740ee598 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -141,6 +141,10 @@ struct task_struct init_task
 	.rcu_tasks_holdout_list = LIST_HEAD_INIT(init_task.rcu_tasks_holdout_list),
 	.rcu_tasks_idle_cpu = -1,
 #endif
+#ifdef CONFIG_TASKS_TRACE_RCU
+	.trc_reader_nesting = 0,
+	.trc_holdout_list = LIST_HEAD_INIT(init_task.trc_holdout_list),
+#endif
 #ifdef CONFIG_CPUSETS
 	.mems_allowed_seq = SEQCNT_ZERO(init_task.mems_allowed_seq),
 #endif

commit 757a4cefde76697af2b2c284c8a320912b77e7e6
Author: Marco Elver <elver@google.com>
Date:   Wed Mar 25 17:41:56 2020 +0100

    kcsan: Add support for scoped accesses
    
    This adds support for scoped accesses, where the memory range is checked
    for the duration of the scope. The feature is implemented by inserting
    the relevant access information into a list of scoped accesses for
    the current execution context, which are then checked (until removed)
    on every call (through instrumentation) into the KCSAN runtime.
    
    An alternative, more complex, implementation could set up a watchpoint for
    the scoped access, and keep the watchpoint set up. This, however, would
    require first exposing a handle to the watchpoint, as well as dealing
    with cases such as accesses by the same thread while the watchpoint is
    still set up (and several more cases). It is also doubtful if this would
    provide any benefit, since the majority of delay where the watchpoint
    is set up is likely due to the injected delays by KCSAN.  Therefore,
    the implementation in this patch is simpler and avoids hurting KCSAN's
    main use-case (normal data race detection); it also implicitly increases
    scoped-access race-detection-ability due to increased probability of
    setting up watchpoints by repeatedly calling __kcsan_check_access()
    throughout the scope of the access.
    
    The implementation required adding an additional conditional branch to
    the fast-path. However, the microbenchmark showed a *speedup* of ~5%
    on the fast-path. This appears to be due to subtly improved codegen by
    GCC from moving get_ctx() and associated load of preempt_count earlier.
    
    Suggested-by: Boqun Feng <boqun.feng@gmail.com>
    Suggested-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 096191d177d5..198943851caf 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -168,6 +168,7 @@ struct task_struct init_task
 		.atomic_nest_count	= 0,
 		.in_flat_atomic		= false,
 		.access_mask		= 0,
+		.scoped_accesses	= {LIST_POISON1, NULL},
 	},
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS

commit 3b02a051d25d9600e9d403ad3043aed7de00160e
Merge: f5d2313bd3c5 8f3d9f354286
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Apr 13 09:44:39 2020 +0200

    Merge tag 'v5.7-rc1' into locking/kcsan, to resolve conflicts and refresh
    
    Resolve these conflicts:
    
            arch/x86/Kconfig
            arch/x86/kernel/Makefile
    
    Do a minor "evil merge" to move the KCSAN entry up a bit by a few lines
    in the Kconfig to reduce the probability of future conflicts.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit eea9673250db4e854e9998ef9da6d4584857f0ea
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Mar 25 10:03:36 2020 -0500

    exec: Add exec_update_mutex to replace cred_guard_mutex
    
    The cred_guard_mutex is problematic as it is held over possibly
    indefinite waits for userspace.  The possible indefinite waits for
    userspace that I have identified are: The cred_guard_mutex is held in
    PTRACE_EVENT_EXIT waiting for the tracer.  The cred_guard_mutex is
    held over "put_user(0, tsk->clear_child_tid)" in exit_mm().  The
    cred_guard_mutex is held over "get_user(futex_offset, ...")  in
    exit_robust_list.  The cred_guard_mutex held over copy_strings.
    
    The functions get_user and put_user can trigger a page fault which can
    potentially wait indefinitely in the case of userfaultfd or if
    userspace implements part of the page fault path.
    
    In any of those cases the userspace process that the kernel is waiting
    for might make a different system call that winds up taking the
    cred_guard_mutex and result in deadlock.
    
    Holding a mutex over any of those possibly indefinite waits for
    userspace does not appear necessary.  Add exec_update_mutex that will
    just cover updating the process during exec where the permissions and
    the objects pointed to by the task struct may be out of sync.
    
    The plan is to switch the users of cred_guard_mutex to
    exec_update_mutex one by one.  This lets us move forward while still
    being careful and not introducing any regressions.
    
    Link: https://lore.kernel.org/lkml/20160921152946.GA24210@dhcp22.suse.cz/
    Link: https://lore.kernel.org/lkml/AM6PR03MB5170B06F3A2B75EFB98D071AE4E60@AM6PR03MB5170.eurprd03.prod.outlook.com/
    Link: https://lore.kernel.org/linux-fsdevel/20161102181806.GB1112@redhat.com/
    Link: https://lore.kernel.org/lkml/20160923095031.GA14923@redhat.com/
    Link: https://lore.kernel.org/lkml/20170213141452.GA30203@redhat.com/
    Ref: 45c1a159b85b ("Add PTRACE_O_TRACEVFORKDONE and PTRACE_O_TRACEEXIT facilities.")
    Ref: 456f17cd1a28 ("[PATCH] user-vm-unlock-2.5.31-A2")
    Reviewed-by: Kirill Tkhai <ktkhai@virtuozzo.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Bernd Edlinger <bernd.edlinger@hotmail.de>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/init/init_task.c b/init/init_task.c
index 9e5cbe5eab7b..bd403ed3e418 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -26,6 +26,7 @@ static struct signal_struct init_signals = {
 	.multiprocess	= HLIST_HEAD_INIT,
 	.rlim		= INIT_RLIMITS,
 	.cred_guard_mutex = __MUTEX_INITIALIZER(init_signals.cred_guard_mutex),
+	.exec_update_mutex = __MUTEX_INITIALIZER(init_signals.exec_update_mutex),
 #ifdef CONFIG_POSIX_TIMERS
 	.posix_timers = LIST_HEAD_INIT(init_signals.posix_timers),
 	.cputimer	= {

commit 81af89e15862909881ff010a0adb67148487e88a
Author: Marco Elver <elver@google.com>
Date:   Tue Feb 11 17:04:22 2020 +0100

    kcsan: Add kcsan_set_access_mask() support
    
    When setting up an access mask with kcsan_set_access_mask(), KCSAN will
    only report races if concurrent changes to bits set in access_mask are
    observed. Conveying access_mask via a separate call avoids introducing
    overhead in the common-case fast-path.
    
    Acked-by: John Hubbard <jhubbard@nvidia.com>
    Signed-off-by: Marco Elver <elver@google.com>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 2b4fe98b0f09..096191d177d5 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -167,6 +167,7 @@ struct task_struct init_task
 		.atomic_next		= 0,
 		.atomic_nest_count	= 0,
 		.in_flat_atomic		= false,
+		.access_mask		= 0,
 	},
 #endif
 #ifdef CONFIG_TRACE_IRQFLAGS

commit dfd402a4c4baae42398ce9180ff424d589b8bffc
Author: Marco Elver <elver@google.com>
Date:   Thu Nov 14 19:02:54 2019 +0100

    kcsan: Add Kernel Concurrency Sanitizer infrastructure
    
    Kernel Concurrency Sanitizer (KCSAN) is a dynamic data-race detector for
    kernel space. KCSAN is a sampling watchpoint-based data-race detector.
    See the included Documentation/dev-tools/kcsan.rst for more details.
    
    This patch adds basic infrastructure, but does not yet enable KCSAN for
    any architecture.
    
    Signed-off-by: Marco Elver <elver@google.com>
    Acked-by: Paul E. McKenney <paulmck@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 9e5cbe5eab7b..2b4fe98b0f09 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -161,6 +161,14 @@ struct task_struct init_task
 #ifdef CONFIG_KASAN
 	.kasan_depth	= 1,
 #endif
+#ifdef CONFIG_KCSAN
+	.kcsan_ctx = {
+		.disable_count		= 0,
+		.atomic_next		= 0,
+		.atomic_nest_count	= 0,
+		.in_flat_atomic		= false,
+	},
+#endif
 #ifdef CONFIG_TRACE_IRQFLAGS
 	.softirqs_enabled = 1,
 #endif

commit 7f2444d38f6bbfa12bc15e2533d8f9daa85ca02b
Merge: c5f12fdb8bd8 77b4b5420422
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 12:35:15 2019 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core timer updates from Thomas Gleixner:
     "Timers and timekeeping updates:
    
       - A large overhaul of the posix CPU timer code which is a preparation
         for moving the CPU timer expiry out into task work so it can be
         properly accounted on the task/process.
    
         An update to the bogus permission checks will come later during the
         merge window as feedback was not complete before heading of for
         travel.
    
       - Switch the timerqueue code to use cached rbtrees and get rid of the
         homebrewn caching of the leftmost node.
    
       - Consolidate hrtimer_init() + hrtimer_init_sleeper() calls into a
         single function
    
       - Implement the separation of hrtimers to be forced to expire in hard
         interrupt context even when PREEMPT_RT is enabled and mark the
         affected timers accordingly.
    
       - Implement a mechanism for hrtimers and the timer wheel to protect
         RT against priority inversion and live lock issues when a (hr)timer
         which should be canceled is currently executing the callback.
         Instead of infinitely spinning, the task which tries to cancel the
         timer blocks on a per cpu base expiry lock which is held and
         released by the (hr)timer expiry code.
    
       - Enable the Hyper-V TSC page based sched_clock for Hyper-V guests
         resulting in faster access to timekeeping functions.
    
       - Updates to various clocksource/clockevent drivers and their device
         tree bindings.
    
       - The usual small improvements all over the place"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (101 commits)
      posix-cpu-timers: Fix permission check regression
      posix-cpu-timers: Always clear head pointer on dequeue
      hrtimer: Add a missing bracket and hide `migration_base' on !SMP
      posix-cpu-timers: Make expiry_active check actually work correctly
      posix-timers: Unbreak CONFIG_POSIX_TIMERS=n build
      tick: Mark sched_timer to expire in hard interrupt context
      hrtimer: Add kernel doc annotation for HRTIMER_MODE_HARD
      x86/hyperv: Hide pv_ops access for CONFIG_PARAVIRT=n
      posix-cpu-timers: Utilize timerqueue for storage
      posix-cpu-timers: Move state tracking to struct posix_cputimers
      posix-cpu-timers: Deduplicate rlimit handling
      posix-cpu-timers: Remove pointless comparisons
      posix-cpu-timers: Get rid of 64bit divisions
      posix-cpu-timers: Consolidate timer expiry further
      posix-cpu-timers: Get rid of zero checks
      rlimit: Rewrite non-sensical RLIMIT_CPU comment
      posix-cpu-timers: Respect INFINITY for hard RTTIME limit
      posix-cpu-timers: Switch thread group sampling to array
      posix-cpu-timers: Restructure expiry array
      posix-cpu-timers: Remove cputime_expires
      ...

commit 244d49e30653658d4e7e9b2b8427777cbbc5affe
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Aug 21 21:09:24 2019 +0200

    posix-cpu-timers: Move state tracking to struct posix_cputimers
    
    Put it where it belongs and clean up the ifdeffery in fork completely.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190821192922.743229404@linutronix.de

diff --git a/init/init_task.c b/init/init_task.c
index 7ab773b9b3cd..d49692a0ec51 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -30,8 +30,6 @@ static struct signal_struct init_signals = {
 	.posix_timers = LIST_HEAD_INIT(init_signals.posix_timers),
 	.cputimer	= {
 		.cputime_atomic	= INIT_CPUTIME_ATOMIC,
-		.running	= false,
-		.checking_timer = false,
 	},
 #endif
 	INIT_CPU_TIMERS(init_signals)

commit c1a280b68d4e6b6db4a65aa7865c22d8789ddf09
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 26 23:19:37 2019 +0200

    sched/preempt: Use CONFIG_PREEMPTION where appropriate
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by
    CONFIG_PREEMPT_RT. Both PREEMPT and PREEMPT_RT require the same
    functionality which today depends on CONFIG_PREEMPT.
    
    Switch the preemption code, scheduler and init task over to use
    CONFIG_PREEMPTION.
    
    That's the first step towards RT in that area. The more complex changes are
    coming separately.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20190726212124.117528401@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 7ab773b9b3cd..bfe06c53b14e 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -174,7 +174,7 @@ struct task_struct init_task
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 	.ret_stack	= NULL,
 #endif
-#if defined(CONFIG_TRACING) && defined(CONFIG_PREEMPT)
+#if defined(CONFIG_TRACING) && defined(CONFIG_PREEMPTION)
 	.trace_recursion = 0,
 #endif
 #ifdef CONFIG_LIVEPATCH

commit dad1c12ed831a7a89cc01e5582cd0b81a4be7f19
Merge: 090bc5a2a914 af24bde8df20
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 16:39:53 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - Remove the unused per rq load array and all its infrastructure, by
       Dietmar Eggemann.
    
     - Add utilization clamping support by Patrick Bellasi. This is a
       refinement of the energy aware scheduling framework with support for
       boosting of interactive and capping of background workloads: to make
       sure critical GUI threads get maximum frequency ASAP, and to make
       sure background processing doesn't unnecessarily move to cpufreq
       governor to higher frequencies and less energy efficient CPU modes.
    
     - Add the bare minimum of tracepoints required for LISA EAS regression
       testing, by Qais Yousef - which allows automated testing of various
       power management features, including energy aware scheduling.
    
     - Restructure the former tsk_nr_cpus_allowed() facility that the -rt
       kernel used to modify the scheduler's CPU affinity logic such as
       migrate_disable() - introduce the task->cpus_ptr value instead of
       taking the address of &task->cpus_allowed directly - by Sebastian
       Andrzej Siewior.
    
     - Misc optimizations, fixes, cleanups and small enhancements - see the
       Git log for details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (33 commits)
      sched/uclamp: Add uclamp support to energy_compute()
      sched/uclamp: Add uclamp_util_with()
      sched/cpufreq, sched/uclamp: Add clamps for FAIR and RT tasks
      sched/uclamp: Set default clamps for RT tasks
      sched/uclamp: Reset uclamp values on RESET_ON_FORK
      sched/uclamp: Extend sched_setattr() to support utilization clamping
      sched/core: Allow sched_setattr() to use the current policy
      sched/uclamp: Add system default clamps
      sched/uclamp: Enforce last task's UCLAMP_MAX
      sched/uclamp: Add bucket local max tracking
      sched/uclamp: Add CPU's clamp buckets refcounting
      sched/fair: Rename weighted_cpuload() to cpu_runnable_load()
      sched/debug: Export the newly added tracepoints
      sched/debug: Add sched_overutilized tracepoint
      sched/debug: Add new tracepoint to track PELT at se level
      sched/debug: Add new tracepoints to track PELT at rq level
      sched/debug: Add a new sched_trace_*() helper functions
      sched/autogroup: Make autogroup_path() always available
      sched/wait: Deduplicate code with do-while
      sched/topology: Remove unused 'sd' parameter from arch_scale_cpu_capacity()
      ...

commit f6ec8829ac9d59b637366c13038f15d6f6156fe1
Author: Yuyang Du <duyuyang@gmail.com>
Date:   Mon May 6 16:19:24 2019 +0800

    locking/lockdep: Define INITIAL_CHAIN_KEY for chain keys to start with
    
    Chain keys are computed using Jenkins hash function, which needs an initial
    hash to start with. Dedicate a macro to make this clear and configurable. A
    later patch changes this initial chain key.
    
    Signed-off-by: Yuyang Du <duyuyang@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bvanassche@acm.org
    Cc: frederic@kernel.org
    Cc: ming.lei@redhat.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190506081939.74287-9-duyuyang@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 1b15cb90d64f..afa6ad795355 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -167,7 +167,7 @@ struct task_struct init_task
 #endif
 #ifdef CONFIG_LOCKDEP
 	.lockdep_depth = 0, /* no locks held yet */
-	.curr_chain_key = 0,
+	.curr_chain_key = INITIAL_CHAIN_KEY,
 	.lockdep_recursion = 0,
 #endif
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER

commit e196e479a3b844da6e6e71e0d2a8694040cb4e52
Author: Yuyang Du <duyuyang@gmail.com>
Date:   Mon May 6 16:19:23 2019 +0800

    locking/lockdep: Use lockdep_init_task for task initiation consistently
    
    Despite that there is a lockdep_init_task() which does nothing, lockdep
    initiates tasks by assigning lockdep fields and does so inconsistently. Fix
    this by using lockdep_init_task().
    
    Signed-off-by: Yuyang Du <duyuyang@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bvanassche@acm.org
    Cc: frederic@kernel.org
    Cc: ming.lei@redhat.com
    Cc: will.deacon@arm.com
    Link: https://lkml.kernel.org/r/20190506081939.74287-8-duyuyang@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index c70ef656d0f4..1b15cb90d64f 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -166,6 +166,8 @@ struct task_struct init_task
 	.softirqs_enabled = 1,
 #endif
 #ifdef CONFIG_LOCKDEP
+	.lockdep_depth = 0, /* no locks held yet */
+	.curr_chain_key = 0,
 	.lockdep_recursion = 0,
 #endif
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER

commit 3bd3706251ee8ab67e69d9340ac2abdca217e733
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Apr 23 16:26:36 2019 +0200

    sched/core: Provide a pointer to the valid CPU mask
    
    In commit:
    
      4b53a3412d66 ("sched/core: Remove the tsk_nr_cpus_allowed() wrapper")
    
    the tsk_nr_cpus_allowed() wrapper was removed. There was not
    much difference in !RT but in RT we used this to implement
    migrate_disable(). Within a migrate_disable() section the CPU mask is
    restricted to single CPU while the "normal" CPU mask remains untouched.
    
    As an alternative implementation Ingo suggested to use:
    
            struct task_struct {
                    const cpumask_t         *cpus_ptr;
                    cpumask_t               cpus_mask;
            };
    with
            t->cpus_ptr = &t->cpus_mask;
    
    In -RT we then can switch the cpus_ptr to:
    
            t->cpus_ptr = &cpumask_of(task_cpu(p));
    
    in a migration disabled region. The rules are simple:
    
     - Code that 'uses' ->cpus_allowed would use the pointer.
     - Code that 'modifies' ->cpus_allowed would use the direct mask.
    
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20190423142636.14347-1-bigeasy@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index c70ef656d0f4..3c27c0efa316 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -72,7 +72,8 @@ struct task_struct init_task
 	.static_prio	= MAX_PRIO - 20,
 	.normal_prio	= MAX_PRIO - 20,
 	.policy		= SCHED_NORMAL,
-	.cpus_allowed	= CPU_MASK_ALL,
+	.cpus_ptr	= &init_task.cpus_mask,
+	.cpus_mask	= CPU_MASK_ALL,
 	.nr_cpus_allowed= NR_CPUS,
 	.mm		= NULL,
 	.active_mm	= &init_mm,

commit be37f21a08ce65c7632c7f45e1755a4b07f278a0
Merge: 3ac96c30ccfa 131d34cb0795
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 7 12:20:11 2019 -0800

    Merge tag 'audit-pr-20190305' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit
    
    Pull audit updates from Paul Moore:
     "A lucky 13 audit patches for v5.1.
    
      Despite the rather large diffstat, most of the changes are from two
      bug fix patches that move code from one Kconfig option to another.
    
      Beyond that bit of churn, the remaining changes are largely cleanups
      and bug-fixes as we slowly march towards container auditing. It isn't
      all boring though, we do have a couple of new things: file
      capabilities v3 support, and expanded support for filtering on
      filesystems to solve problems with remote filesystems.
    
      All changes pass the audit-testsuite.  Please merge for v5.1"
    
    * tag 'audit-pr-20190305' of git://git.kernel.org/pub/scm/linux/kernel/git/pcmoore/audit:
      audit: mark expected switch fall-through
      audit: hide auditsc_get_stamp and audit_serial prototypes
      audit: join tty records to their syscall
      audit: remove audit_context when CONFIG_ AUDIT and not AUDITSYSCALL
      audit: remove unused actx param from audit_rule_match
      audit: ignore fcaps on umount
      audit: clean up AUDITSYSCALL prototypes and stubs
      audit: more filter PATH records keyed on filesystem magic
      audit: add support for fcaps v3
      audit: move loginuid and sessionid from CONFIG_AUDITSYSCALL to CONFIG_AUDIT
      audit: add syscall information to CONFIG_CHANGE records
      audit: hand taken context to audit_kill_trees for syscall logging
      audit: give a clue what CONFIG_CHANGE op was involved

commit 8dcd175bc3d50b78413c56d5b17d4bddd77412ef
Merge: afe6fe7036c6 fff04900ea79
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 10:31:36 2019 -0800

    Merge branch 'akpm' (patches from Andrew)
    
    Merge misc updates from Andrew Morton:
    
     - a few misc things
    
     - ocfs2 updates
    
     - most of MM
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (159 commits)
      tools/testing/selftests/proc/proc-self-syscall.c: remove duplicate include
      proc: more robust bulk read test
      proc: test /proc/*/maps, smaps, smaps_rollup, statm
      proc: use seq_puts() everywhere
      proc: read kernel cpu stat pointer once
      proc: remove unused argument in proc_pid_lookup()
      fs/proc/thread_self.c: code cleanup for proc_setup_thread_self()
      fs/proc/self.c: code cleanup for proc_setup_self()
      proc: return exit code 4 for skipped tests
      mm,mremap: bail out earlier in mremap_to under map pressure
      mm/sparse: fix a bad comparison
      mm/memory.c: do_fault: avoid usage of stale vm_area_struct
      writeback: fix inode cgroup switching comment
      mm/huge_memory.c: fix "orig_pud" set but not used
      mm/hotplug: fix an imbalance with DEBUG_PAGEALLOC
      mm/memcontrol.c: fix bad line in comment
      mm/cma.c: cma_declare_contiguous: correct err handling
      mm/page_ext.c: fix an imbalance with kmemleak
      mm/compaction: pass pgdat to too_many_isolated() instead of zone
      mm: remove zone_lru_lock() function, access ->lru_lock directly
      ...

commit 98fa15f34cb379864757670b8e8743b21456a20e
Author: Anshuman Khandual <anshuman.khandual@arm.com>
Date:   Tue Mar 5 15:42:58 2019 -0800

    mm: replace all open encodings for NUMA_NO_NODE
    
    Patch series "Replace all open encodings for NUMA_NO_NODE", v3.
    
    All these places for replacement were found by running the following
    grep patterns on the entire kernel code.  Please let me know if this
    might have missed some instances.  This might also have replaced some
    false positives.  I will appreciate suggestions, inputs and review.
    
    1. git grep "nid == -1"
    2. git grep "node == -1"
    3. git grep "nid = -1"
    4. git grep "node = -1"
    
    This patch (of 2):
    
    At present there are multiple places where invalid node number is
    encoded as -1.  Even though implicitly understood it is always better to
    have macros in there.  Replace these open encodings for an invalid node
    number with the global macro NUMA_NO_NODE.  This helps remove NUMA
    related assumptions like 'invalid node' from various places redirecting
    them to a common definition.
    
    Link: http://lkml.kernel.org/r/1545127933-10711-2-git-send-email-anshuman.khandual@arm.com
    Signed-off-by: Anshuman Khandual <anshuman.khandual@arm.com>
    Reviewed-by: David Hildenbrand <david@redhat.com>
    Acked-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>    [ixgbe]
    Acked-by: Jens Axboe <axboe@kernel.dk>                  [mtip32xx]
    Acked-by: Vinod Koul <vkoul@kernel.org>                 [dmaengine.c]
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>         [powerpc]
    Acked-by: Doug Ledford <dledford@redhat.com>            [drivers/infiniband]
    Cc: Joseph Qi <jiangqi903@gmail.com>
    Cc: Hans Verkuil <hverkuil@xs4all.nl>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/init/init_task.c b/init/init_task.c
index 5aebe3be4d7c..26131e73aa6d 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -10,6 +10,7 @@
 #include <linux/fs.h>
 #include <linux/mm.h>
 #include <linux/audit.h>
+#include <linux/numa.h>
 
 #include <asm/pgtable.h>
 #include <linux/uaccess.h>
@@ -154,7 +155,7 @@ struct task_struct init_task
 	.vtime.state	= VTIME_SYS,
 #endif
 #ifdef CONFIG_NUMA_BALANCING
-	.numa_preferred_nid = -1,
+	.numa_preferred_nid = NUMA_NO_NODE,
 	.numa_group	= NULL,
 	.numa_faults	= NULL,
 #endif

commit f0b89d3958d73cd0785ec381f0ddf8efb6f183d8
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:30 2019 +0200

    sched/core: Convert task_struct.stack_refcount to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable task_struct.stack_refcount is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the task_struct.stack_refcount it might make a difference
    in following places:
    
     - try_get_task_stack(): increment in refcount_inc_not_zero() only
       guarantees control dependency on success vs. fully ordered
       atomic counterpart
     - put_task_stack(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-6-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index aca34c89529f..46dbf546264d 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -61,7 +61,7 @@ struct task_struct init_task
 = {
 #ifdef CONFIG_THREAD_INFO_IN_TASK
 	.thread_info	= INIT_THREAD_INFO(init_task),
-	.stack_refcount	= ATOMIC_INIT(1),
+	.stack_refcount	= REFCOUNT_INIT(1),
 #endif
 	.state		= 0,
 	.stack		= init_stack,

commit ec1d281923cf81cc660343d0cb8ffc837ffb991d
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:29 2019 +0200

    sched/core: Convert task_struct.usage to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable task_struct.usage is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the task_struct.usage it might make a difference
    in following places:
    
     - put_task_struct(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-5-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 9aa3ebc74970..aca34c89529f 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -65,7 +65,7 @@ struct task_struct init_task
 #endif
 	.state		= 0,
 	.stack		= init_stack,
-	.usage		= ATOMIC_INIT(2),
+	.usage		= REFCOUNT_INIT(2),
 	.flags		= PF_KTHREAD,
 	.prio		= MAX_PRIO - 20,
 	.static_prio	= MAX_PRIO - 20,

commit 60d4de3ff7f775509deba94b3db3c1abe55bf7a5
Author: Elena Reshetova <elena.reshetova@intel.com>
Date:   Fri Jan 18 14:27:27 2019 +0200

    sched/core: Convert signal_struct.sigcnt to refcount_t
    
    atomic_t variables are currently used to implement reference
    counters with the following properties:
    
     - counter is initialized to 1 using atomic_set()
     - a resource is freed upon counter reaching zero
     - once counter reaches zero, its further
       increments aren't allowed
     - counter schema uses basic atomic operations
       (set, inc, inc_not_zero, dec_and_test, etc.)
    
    Such atomic variables should be converted to a newly provided
    refcount_t type and API that prevents accidental counter overflows
    and underflows. This is important since overflows and underflows
    can lead to use-after-free situation and be exploitable.
    
    The variable signal_struct.sigcnt is used as pure reference counter.
    Convert it to refcount_t and fix up the operations.
    
    ** Important note for maintainers:
    
    Some functions from refcount_t API defined in lib/refcount.c
    have different memory ordering guarantees than their atomic
    counterparts.
    
    The full comparison can be seen in
    https://lkml.org/lkml/2017/11/15/57 and it is hopefully soon
    in state to be merged to the documentation tree.
    
    Normally the differences should not matter since refcount_t provides
    enough guarantees to satisfy the refcounting use cases, but in
    some rare cases it might matter.
    
    Please double check that you don't have some undocumented
    memory guarantees for this variable usage.
    
    For the signal_struct.sigcnt it might make a difference
    in following places:
    
     - put_signal_struct(): decrement in refcount_dec_and_test() only
       provides RELEASE ordering and control dependency on success
       vs. fully ordered atomic counterpart
    
    Suggested-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: David Windsor <dwindsor@gmail.com>
    Reviewed-by: Hans Liljestrand <ishkamiel@gmail.com>
    Reviewed-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Reviewed-by: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: viro@zeniv.linux.org.uk
    Link: https://lkml.kernel.org/r/1547814450-18902-3-git-send-email-elena.reshetova@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 5aebe3be4d7c..9aa3ebc74970 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -44,7 +44,7 @@ static struct signal_struct init_signals = {
 };
 
 static struct sighand_struct init_sighand = {
-	.count		= ATOMIC_INIT(1),
+	.count		= REFCOUNT_INIT(1),
 	.action		= { { { .sa_handler = SIG_DFL, } }, },
 	.siglock	= __SPIN_LOCK_UNLOCKED(init_sighand.siglock),
 	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(init_sighand.signalfd_wqh),

commit 4b7d248b3a1de483ffe9d05c1debbf32a544164d
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Tue Jan 22 17:06:39 2019 -0500

    audit: move loginuid and sessionid from CONFIG_AUDITSYSCALL to CONFIG_AUDIT
    
    loginuid and sessionid (and audit_log_session_info) should be part of
    CONFIG_AUDIT scope and not CONFIG_AUDITSYSCALL since it is used in
    CONFIG_CHANGE, ANOM_LINK, FEATURE_CHANGE (and INTEGRITY_RULE), none of
    which are otherwise dependent on AUDITSYSCALL.
    
    Please see github issue
    https://github.com/linux-audit/audit-kernel/issues/104
    
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    [PM: tweaked subject line for better grep'ing]
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/init/init_task.c b/init/init_task.c
index 5aebe3be4d7c..39c3109acc1a 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -121,7 +121,7 @@ struct task_struct init_task
 	.thread_pid	= &init_struct_pid,
 	.thread_group	= LIST_HEAD_INIT(init_task.thread_group),
 	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),
-#ifdef CONFIG_AUDITSYSCALL
+#ifdef CONFIG_AUDIT
 	.loginuid	= INVALID_UID,
 	.sessionid	= AUDIT_SID_UNSET,
 #endif

commit c3ad2c3b02e953ead2b8d52a0c9e70312930c3d0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Jul 23 15:20:37 2018 -0500

    signal: Don't restart fork when signals come in.
    
    Wen Yang <wen.yang99@zte.com.cn> and majiang <ma.jiang@zte.com.cn>
    report that a periodic signal received during fork can cause fork to
    continually restart preventing an application from making progress.
    
    The code was being overly pessimistic.  Fork needs to guarantee that a
    signal sent to multiple processes is logically delivered before the
    fork and just to the forking process or logically delivered after the
    fork to both the forking process and it's newly spawned child.  For
    signals like periodic timers that are always delivered to a single
    process fork can safely complete and let them appear to logically
    delivered after the fork().
    
    While examining this issue I also discovered that fork today will miss
    signals delivered to multiple processes during the fork and handled by
    another thread.  Similarly the current code will also miss blocked
    signals that are delivered to multiple process, as those signals will
    not appear pending during fork.
    
    Add a list of each thread that is currently forking, and keep on that
    list a signal set that records all of the signals sent to multiple
    processes.  When fork completes initialize the new processes
    shared_pending signal set with it.  The calculate_sigpending function
    will see those signals and set TIF_SIGPENDING causing the new task to
    take the slow path to userspace to handle those signals.  Making it
    appear as if those signals were received immediately after the fork.
    
    It is not possible to send real time signals to multiple processes and
    exceptions don't go to multiple processes, which means that that are
    no signals sent to multiple processes that require siginfo.  This
    means it is safe to not bother collecting siginfo on signals sent
    during fork.
    
    The sigaction of a child of fork is initially the same as the
    sigaction of the parent process.  So a signal the parent ignores the
    child will also initially ignore.  Therefore it is safe to ignore
    signals sent to multiple processes and ignored by the forking process.
    
    Signals sent to only a single process or only a single thread and delivered
    during fork are treated as if they are received after the fork, and generally
    not dealt with.  They won't cause any problems.
    
    V2: Added removal from the multiprocess list on failure.
    V3: Use -ERESTARTNOINTR directly
    V4: - Don't queue both SIGCONT and SIGSTOP
        - Initialize signal_struct.multiprocess in init_task
        - Move setting of shared_pending to before the new task
          is visible to signals.  This prevents signals from comming
          in before shared_pending.signal is set to delayed.signal
          and being lost.
    V5: - rework list add and delete to account for idle threads
    v6: - Use sigdelsetmask when removing stop signals
    
    Bugzilla: https://bugzilla.kernel.org/show_bug.cgi?id=200447
    Reported-by: Wen Yang <wen.yang99@zte.com.cn> and
    Reported-by: majiang <ma.jiang@zte.com.cn>
    Fixes: 4a2c7a7837da ("[PATCH] make fork() atomic wrt pgrp/session signals")
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/init/init_task.c b/init/init_task.c
index 4f97846256d7..5aebe3be4d7c 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -22,6 +22,7 @@ static struct signal_struct init_signals = {
 		.list = LIST_HEAD_INIT(init_signals.shared_pending.list),
 		.signal =  {{0}}
 	},
+	.multiprocess	= HLIST_HEAD_INIT,
 	.rlim		= INIT_RLIMITS,
 	.cred_guard_mutex = __MUTEX_INITIALIZER(init_signals.cred_guard_mutex),
 #ifdef CONFIG_POSIX_TIMERS

commit 6883f81aac6f44e7df70a6af189b3689ff52cbfb
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Jun 4 04:32:13 2017 -0500

    pid: Implement PIDTYPE_TGID
    
    Everywhere except in the pid array we distinguish between a tasks pid and
    a tasks tgid (thread group id).  Even in the enumeration we want that
    distinction sometimes so we have added __PIDTYPE_TGID.  With leader_pid
    we almost have an implementation of PIDTYPE_TGID in struct signal_struct.
    
    Add PIDTYPE_TGID as a first class member of the pid_type enumeration and
    into the pids array.  Then remove the __PIDTYPE_TGID special case and the
    leader_pid in signal_struct.
    
    The net size increase is just an extra pointer added to struct pid and
    an extra pair of pointers of an hlist_node added to task_struct.
    
    The effect on code maintenance is the removal of a number of special
    cases today and the potential to remove many more special cases as
    PIDTYPE_TGID gets used to it's fullest.  The long term potential
    is allowing zombie thread group leaders to exit, which will remove
    a lot more special cases in the code.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/init/init_task.c b/init/init_task.c
index db12a61259f1..4f97846256d7 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -33,9 +33,9 @@ static struct signal_struct init_signals = {
 	},
 #endif
 	INIT_CPU_TIMERS(init_signals)
-	.leader_pid = &init_struct_pid,
 	.pids = {
 		[PIDTYPE_PID]	= &init_struct_pid,
+		[PIDTYPE_TGID]	= &init_struct_pid,
 		[PIDTYPE_PGID]	= &init_struct_pid,
 		[PIDTYPE_SID]	= &init_struct_pid,
 	},

commit 2c4704756cab7cfa031ada4dab361562f0e357c0
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Sep 26 13:06:43 2017 -0500

    pids: Move the pgrp and session pid pointers from task_struct to signal_struct
    
    To access these fields the code always has to go to group leader so
    going to signal struct is no loss and is actually a fundamental simplification.
    
    This saves a little bit of memory by only allocating the pid pointer array
    once instead of once for every thread, and even better this removes a
    few potential races caused by the fact that group_leader can be changed
    by de_thread, while signal_struct can not.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/init/init_task.c b/init/init_task.c
index 7914ffb8dc73..db12a61259f1 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -34,6 +34,11 @@ static struct signal_struct init_signals = {
 #endif
 	INIT_CPU_TIMERS(init_signals)
 	.leader_pid = &init_struct_pid,
+	.pids = {
+		[PIDTYPE_PID]	= &init_struct_pid,
+		[PIDTYPE_PGID]	= &init_struct_pid,
+		[PIDTYPE_SID]	= &init_struct_pid,
+	},
 	INIT_PREV_CPUTIME(init_signals)
 };
 
@@ -112,11 +117,7 @@ struct task_struct init_task
 	INIT_CPU_TIMERS(init_task)
 	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(init_task.pi_lock),
 	.timer_slack_ns = 50000, /* 50 usec default slack */
-	.pids = {
-		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),
-		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),
-		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),
-	},
+	.thread_pid	= &init_struct_pid,
 	.thread_group	= LIST_HEAD_INIT(init_task.thread_group),
 	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),
 #ifdef CONFIG_AUDITSYSCALL

commit 2896b0f09f2617f953b8038978106cc4cbb4c52b
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri May 5 13:31:23 2017 -0500

    pids: Initialize leader_pid in init_task
    
    This is cheap and no cost so we might as well.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/init/init_task.c b/init/init_task.c
index 74f60baa2799..7914ffb8dc73 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -33,6 +33,7 @@ static struct signal_struct init_signals = {
 	},
 #endif
 	INIT_CPU_TIMERS(init_signals)
+	.leader_pid = &init_struct_pid,
 	INIT_PREV_CPUTIME(init_signals)
 };
 

commit f0b752168d7091f38e7d61a80de2542e8b71d266
Author: Richard Guy Briggs <rgb@redhat.com>
Date:   Sat May 12 21:58:19 2018 -0400

    audit: convert sessionid unset to a macro
    
    Use a macro, "AUDIT_SID_UNSET", to replace each instance of
    initialization and comparison to an audit session ID.
    
    Signed-off-by: Richard Guy Briggs <rgb@redhat.com>
    Signed-off-by: Paul Moore <paul@paul-moore.com>

diff --git a/init/init_task.c b/init/init_task.c
index 3ac6e754cf64..74f60baa2799 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -9,6 +9,7 @@
 #include <linux/init.h>
 #include <linux/fs.h>
 #include <linux/mm.h>
+#include <linux/audit.h>
 
 #include <asm/pgtable.h>
 #include <linux/uaccess.h>
@@ -119,7 +120,7 @@ struct task_struct init_task
 	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),
 #ifdef CONFIG_AUDITSYSCALL
 	.loginuid	= INVALID_UID,
-	.sessionid	= (unsigned int)-1,
+	.sessionid	= AUDIT_SID_UNSET,
 #endif
 #ifdef CONFIG_PERF_EVENTS
 	.perf_event_mutex = __MUTEX_INITIALIZER(init_task.perf_event_mutex),

commit 3678e2fcc293cf2ff1fe961838734a70c185de8a
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand the INIT_SIGNALS and INIT_SIGHAND macros and remove
    
    There doesn't seem to be any need to have the INIT_SIGNALS and INIT_SIGHAND
    macros, so expand them in their single places of use and remove them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/init/init_task.c b/init/init_task.c
index aa4030a939e5..3ac6e754cf64 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -13,8 +13,34 @@
 #include <asm/pgtable.h>
 #include <linux/uaccess.h>
 
-static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
-static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
+static struct signal_struct init_signals = {
+	.nr_threads	= 1,
+	.thread_head	= LIST_HEAD_INIT(init_task.thread_node),
+	.wait_chldexit	= __WAIT_QUEUE_HEAD_INITIALIZER(init_signals.wait_chldexit),
+	.shared_pending	= {
+		.list = LIST_HEAD_INIT(init_signals.shared_pending.list),
+		.signal =  {{0}}
+	},
+	.rlim		= INIT_RLIMITS,
+	.cred_guard_mutex = __MUTEX_INITIALIZER(init_signals.cred_guard_mutex),
+#ifdef CONFIG_POSIX_TIMERS
+	.posix_timers = LIST_HEAD_INIT(init_signals.posix_timers),
+	.cputimer	= {
+		.cputime_atomic	= INIT_CPUTIME_ATOMIC,
+		.running	= false,
+		.checking_timer = false,
+	},
+#endif
+	INIT_CPU_TIMERS(init_signals)
+	INIT_PREV_CPUTIME(init_signals)
+};
+
+static struct sighand_struct init_sighand = {
+	.count		= ATOMIC_INIT(1),
+	.action		= { { { .sa_handler = SIG_DFL, } }, },
+	.siglock	= __SPIN_LOCK_UNLOCKED(init_sighand.siglock),
+	.signalfd_wqh	= __WAIT_QUEUE_HEAD_INITIALIZER(init_sighand.signalfd_wqh),
+};
 
 /*
  * Set up the first task table, touch at your own risk!. Base=0,

commit 4e7e3adbba5224604b34b0d42003ff6dbdc8ddd9
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand various INIT_* macros and remove
    
    Expand various INIT_* macros into the single places they're used in
    init/init_task.c and remove them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/init/init_task.c b/init/init_task.c
index 7b2436f02dad..aa4030a939e5 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -16,7 +16,6 @@
 static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
 static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
 
-
 /*
  * Set up the first task table, touch at your own risk!. Base=0,
  * limit=0x1fffff (=2MB)
@@ -26,20 +25,23 @@ struct task_struct init_task
 	__init_task_data
 #endif
 = {
-	INIT_TASK_TI(init_task)
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	.thread_info	= INIT_THREAD_INFO(init_task),
+	.stack_refcount	= ATOMIC_INIT(1),
+#endif
 	.state		= 0,
 	.stack		= init_stack,
 	.usage		= ATOMIC_INIT(2),
 	.flags		= PF_KTHREAD,
-	.prio		= MAX_PRIO-20,
-	.static_prio	= MAX_PRIO-20,
-	.normal_prio	= MAX_PRIO-20,
+	.prio		= MAX_PRIO - 20,
+	.static_prio	= MAX_PRIO - 20,
+	.normal_prio	= MAX_PRIO - 20,
 	.policy		= SCHED_NORMAL,
 	.cpus_allowed	= CPU_MASK_ALL,
 	.nr_cpus_allowed= NR_CPUS,
 	.mm		= NULL,
 	.active_mm	= &init_mm,
-	.restart_block = {
+	.restart_block	= {
 		.fn = do_no_restart_syscall,
 	},
 	.se		= {
@@ -50,8 +52,12 @@ struct task_struct init_task
 		.time_slice	= RR_TIMESLICE,
 	},
 	.tasks		= LIST_HEAD_INIT(init_task.tasks),
-	INIT_PUSHABLE_TASKS(init_task)
-	INIT_CGROUP_SCHED(init_task)
+#ifdef CONFIG_SMP
+	.pushable_tasks	= PLIST_NODE_INIT(init_task.pushable_tasks, MAX_PRIO),
+#endif
+#ifdef CONFIG_CGROUP_SCHED
+	.sched_task_group = &root_task_group,
+#endif
 	.ptraced	= LIST_HEAD_INIT(init_task.ptraced),
 	.ptrace_entry	= LIST_HEAD_INIT(init_task.ptrace_entry),
 	.real_parent	= &init_task,
@@ -85,24 +91,65 @@ struct task_struct init_task
 	},
 	.thread_group	= LIST_HEAD_INIT(init_task.thread_group),
 	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),
-	INIT_IDS
-	INIT_PERF_EVENTS(init_task)
-	INIT_TRACE_IRQFLAGS
-	INIT_LOCKDEP
-	INIT_FTRACE_GRAPH
-	INIT_TRACE_RECURSION
-	INIT_TASK_RCU_PREEMPT(init_task)
-	INIT_TASK_RCU_TASKS(init_task)
-	INIT_CPUSET_SEQ(init_task)
-	INIT_RT_MUTEXES(init_task)
+#ifdef CONFIG_AUDITSYSCALL
+	.loginuid	= INVALID_UID,
+	.sessionid	= (unsigned int)-1,
+#endif
+#ifdef CONFIG_PERF_EVENTS
+	.perf_event_mutex = __MUTEX_INITIALIZER(init_task.perf_event_mutex),
+	.perf_event_list = LIST_HEAD_INIT(init_task.perf_event_list),
+#endif
+#ifdef CONFIG_PREEMPT_RCU
+	.rcu_read_lock_nesting = 0,
+	.rcu_read_unlock_special.s = 0,
+	.rcu_node_entry = LIST_HEAD_INIT(init_task.rcu_node_entry),
+	.rcu_blocked_node = NULL,
+#endif
+#ifdef CONFIG_TASKS_RCU
+	.rcu_tasks_holdout = false,
+	.rcu_tasks_holdout_list = LIST_HEAD_INIT(init_task.rcu_tasks_holdout_list),
+	.rcu_tasks_idle_cpu = -1,
+#endif
+#ifdef CONFIG_CPUSETS
+	.mems_allowed_seq = SEQCNT_ZERO(init_task.mems_allowed_seq),
+#endif
+#ifdef CONFIG_RT_MUTEXES
+	.pi_waiters	= RB_ROOT_CACHED,
+	.pi_top_task	= NULL,
+#endif
 	INIT_PREV_CPUTIME(init_task)
-	INIT_VTIME(init_task)
-	INIT_NUMA_BALANCING(init_task)
-	INIT_KASAN(init_task)
-	INIT_LIVEPATCH(init_task)
-	INIT_TASK_SECURITY
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
+	.vtime.seqcount	= SEQCNT_ZERO(init_task.vtime_seqcount),
+	.vtime.starttime = 0,
+	.vtime.state	= VTIME_SYS,
+#endif
+#ifdef CONFIG_NUMA_BALANCING
+	.numa_preferred_nid = -1,
+	.numa_group	= NULL,
+	.numa_faults	= NULL,
+#endif
+#ifdef CONFIG_KASAN
+	.kasan_depth	= 1,
+#endif
+#ifdef CONFIG_TRACE_IRQFLAGS
+	.softirqs_enabled = 1,
+#endif
+#ifdef CONFIG_LOCKDEP
+	.lockdep_recursion = 0,
+#endif
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+	.ret_stack	= NULL,
+#endif
+#if defined(CONFIG_TRACING) && defined(CONFIG_PREEMPT)
+	.trace_recursion = 0,
+#endif
+#ifdef CONFIG_LIVEPATCH
+	.patch_state	= KLP_UNDEFINED,
+#endif
+#ifdef CONFIG_SECURITY
+	.security	= NULL,
+#endif
 };
-
 EXPORT_SYMBOL(init_task);
 
 /*

commit d11ed3ab3166a2bfad60681aebf3e13e1c3408a9
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Expand INIT_TASK() in init/init_task.c and remove
    
    It's no longer necessary to have an INIT_TASK() macro, and this can be
    expanded into the one place it is now used and removed.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/init/init_task.c b/init/init_task.c
index 2285aa42cbe1..7b2436f02dad 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -16,12 +16,93 @@
 static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
 static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
 
-/* Initial task structure */
+
+/*
+ * Set up the first task table, touch at your own risk!. Base=0,
+ * limit=0x1fffff (=2MB)
+ */
 struct task_struct init_task
 #ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK
 	__init_task_data
 #endif
-	= INIT_TASK(init_task);
+= {
+	INIT_TASK_TI(init_task)
+	.state		= 0,
+	.stack		= init_stack,
+	.usage		= ATOMIC_INIT(2),
+	.flags		= PF_KTHREAD,
+	.prio		= MAX_PRIO-20,
+	.static_prio	= MAX_PRIO-20,
+	.normal_prio	= MAX_PRIO-20,
+	.policy		= SCHED_NORMAL,
+	.cpus_allowed	= CPU_MASK_ALL,
+	.nr_cpus_allowed= NR_CPUS,
+	.mm		= NULL,
+	.active_mm	= &init_mm,
+	.restart_block = {
+		.fn = do_no_restart_syscall,
+	},
+	.se		= {
+		.group_node 	= LIST_HEAD_INIT(init_task.se.group_node),
+	},
+	.rt		= {
+		.run_list	= LIST_HEAD_INIT(init_task.rt.run_list),
+		.time_slice	= RR_TIMESLICE,
+	},
+	.tasks		= LIST_HEAD_INIT(init_task.tasks),
+	INIT_PUSHABLE_TASKS(init_task)
+	INIT_CGROUP_SCHED(init_task)
+	.ptraced	= LIST_HEAD_INIT(init_task.ptraced),
+	.ptrace_entry	= LIST_HEAD_INIT(init_task.ptrace_entry),
+	.real_parent	= &init_task,
+	.parent		= &init_task,
+	.children	= LIST_HEAD_INIT(init_task.children),
+	.sibling	= LIST_HEAD_INIT(init_task.sibling),
+	.group_leader	= &init_task,
+	RCU_POINTER_INITIALIZER(real_cred, &init_cred),
+	RCU_POINTER_INITIALIZER(cred, &init_cred),
+	.comm		= INIT_TASK_COMM,
+	.thread		= INIT_THREAD,
+	.fs		= &init_fs,
+	.files		= &init_files,
+	.signal		= &init_signals,
+	.sighand	= &init_sighand,
+	.nsproxy	= &init_nsproxy,
+	.pending	= {
+		.list = LIST_HEAD_INIT(init_task.pending.list),
+		.signal = {{0}}
+	},
+	.blocked	= {{0}},
+	.alloc_lock	= __SPIN_LOCK_UNLOCKED(init_task.alloc_lock),
+	.journal_info	= NULL,
+	INIT_CPU_TIMERS(init_task)
+	.pi_lock	= __RAW_SPIN_LOCK_UNLOCKED(init_task.pi_lock),
+	.timer_slack_ns = 50000, /* 50 usec default slack */
+	.pids = {
+		[PIDTYPE_PID]  = INIT_PID_LINK(PIDTYPE_PID),
+		[PIDTYPE_PGID] = INIT_PID_LINK(PIDTYPE_PGID),
+		[PIDTYPE_SID]  = INIT_PID_LINK(PIDTYPE_SID),
+	},
+	.thread_group	= LIST_HEAD_INIT(init_task.thread_group),
+	.thread_node	= LIST_HEAD_INIT(init_signals.thread_head),
+	INIT_IDS
+	INIT_PERF_EVENTS(init_task)
+	INIT_TRACE_IRQFLAGS
+	INIT_LOCKDEP
+	INIT_FTRACE_GRAPH
+	INIT_TRACE_RECURSION
+	INIT_TASK_RCU_PREEMPT(init_task)
+	INIT_TASK_RCU_TASKS(init_task)
+	INIT_CPUSET_SEQ(init_task)
+	INIT_RT_MUTEXES(init_task)
+	INIT_PREV_CPUTIME(init_task)
+	INIT_VTIME(init_task)
+	INIT_NUMA_BALANCING(init_task)
+	INIT_KASAN(init_task)
+	INIT_LIVEPATCH(init_task)
+	INIT_TASK_SECURITY
+};
+
 EXPORT_SYMBOL(init_task);
 
 /*

commit 0500871f21b237b2bea2d9db405eadf78e5aab05
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jan 2 15:12:01 2018 +0000

    Construct init thread stack in the linker script rather than by union
    
    Construct the init thread stack in the linker script rather than doing it
    by means of a union so that ia64's init_task.c can be got rid of.
    
    The following symbols are then made available from INIT_TASK_DATA() linker
    script macro:
    
            init_thread_union
            init_stack
    
    INIT_TASK_DATA() also expands the region to THREAD_SIZE to accommodate the
    size of the init stack.  init_thread_union is given its own section so that
    it can be placed into the stack space in the right order.  I'm assuming
    that the ia64 ordering is correct and that the task_struct is first and the
    thread_info second.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Tony Luck <tony.luck@intel.com>
    Tested-by: Will Deacon <will.deacon@arm.com> (arm64)
    Tested-by: Palmer Dabbelt <palmer@sifive.com>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/init/init_task.c b/init/init_task.c
index 9325fee7dc82..2285aa42cbe1 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -17,15 +17,17 @@ static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
 static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
 
 /* Initial task structure */
-struct task_struct init_task = INIT_TASK(init_task);
+struct task_struct init_task
+#ifdef CONFIG_ARCH_TASK_STRUCT_ON_STACK
+	__init_task_data
+#endif
+	= INIT_TASK(init_task);
 EXPORT_SYMBOL(init_task);
 
 /*
  * Initial thread structure. Alignment of this is handled by a special
  * linker map entry.
  */
-union thread_union init_thread_union __init_task_data = {
 #ifndef CONFIG_THREAD_INFO_IN_TASK
-	INIT_THREAD_INFO(init_task)
+struct thread_info init_thread_info __init_thread_info = INIT_THREAD_INFO(init_task);
 #endif
-};

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/init/init_task.c b/init/init_task.c
index 66787e30a419..9325fee7dc82 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 #include <linux/init_task.h>
 #include <linux/export.h>
 #include <linux/mqueue.h>

commit 9164bb4a18dfa592cd0aca455ea57abf89ca4526
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 01:20:53 2017 +0100

    sched/headers: Prepare to move 'init_task' and 'init_thread_union' from <linux/sched.h> to <linux/sched/task.h>
    
    Update all usage sites first.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 53d4ce942a88..66787e30a419 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -4,6 +4,7 @@
 #include <linux/sched.h>
 #include <linux/sched/sysctl.h>
 #include <linux/sched/rt.h>
+#include <linux/sched/task.h>
 #include <linux/init.h>
 #include <linux/fs.h>
 #include <linux/mm.h>

commit 7c0f6ba682b9c7632072ffbedf8d328c8f3c42ba
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Dec 24 11:46:01 2016 -0800

    Replace <asm/uaccess.h> with <linux/uaccess.h> globally
    
    This was entirely automated, using the script by Al:
    
      PATT='^[[:blank:]]*#[[:blank:]]*include[[:blank:]]*<asm/uaccess.h>'
      sed -i -e "s!$PATT!#include <linux/uaccess.h>!" \
            $(git grep -l "$PATT"|grep -v ^include/linux/uaccess.h)
    
    to do the replacement at the end of the merge window.
    
    Requested-by: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/init/init_task.c b/init/init_task.c
index 11f83be1fa79..53d4ce942a88 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -9,7 +9,7 @@
 #include <linux/mm.h>
 
 #include <asm/pgtable.h>
-#include <asm/uaccess.h>
+#include <linux/uaccess.h>
 
 static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
 static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);

commit c65eacbe290b8141554c71b2c94489e73ade8c8d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:24 2016 -0700

    sched/core: Allow putting thread_info into task_struct
    
    If an arch opts in by setting CONFIG_THREAD_INFO_IN_TASK_STRUCT,
    then thread_info is defined as a single 'u32 flags' and is the first
    entry of task_struct.  thread_info::task is removed (it serves no
    purpose if thread_info is embedded in task_struct), and
    thread_info::cpu gets its own slot in task_struct.
    
    This is heavily based on a patch written by Linus.
    
    Originally-from: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a0898196f0476195ca02713691a5037a14f2aac5.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index ba0a7f362d9e..11f83be1fa79 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -22,5 +22,8 @@ EXPORT_SYMBOL(init_task);
  * Initial thread structure. Alignment of this is handled by a special
  * linker map entry.
  */
-union thread_union init_thread_union __init_task_data =
-	{ INIT_THREAD_INFO(init_task) };
+union thread_union init_thread_union __init_task_data = {
+#ifndef CONFIG_THREAD_INFO_IN_TASK
+	INIT_THREAD_INFO(init_task)
+#endif
+};

commit 8bd75c77b7c6a3954140dd2e20346aef3efe4a35
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:47:07 2013 -0600

    sched/rt: Move rt specific bits into new header file
    
    Move rt scheduler definitions out of include/linux/sched.h into
    new file include/linux/sched/rt.h
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094707.7b9f825f@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index a031ad14c950..ba0a7f362d9e 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -3,6 +3,7 @@
 #include <linux/mqueue.h>
 #include <linux/sched.h>
 #include <linux/sched/sysctl.h>
+#include <linux/sched/rt.h>
 #include <linux/init.h>
 #include <linux/fs.h>
 #include <linux/mm.h>

commit cf4aebc292fac7f34f8345664320e9d4a42ca76c
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:46:59 2013 -0600

    sched: Move sched.h sysctl bits into separate header
    
    Move the sysctl-related bits from include/linux/sched.h into
    a new file: include/linux/sched/sysctl.h. Then update source
    files requiring access to those bits by including the new
    header file.
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094659.06dced96@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/init/init_task.c b/init/init_task.c
index 8b2f3996b035..a031ad14c950 100644
--- a/init/init_task.c
+++ b/init/init_task.c
@@ -2,6 +2,7 @@
 #include <linux/export.h>
 #include <linux/mqueue.h>
 #include <linux/sched.h>
+#include <linux/sched/sysctl.h>
 #include <linux/init.h>
 #include <linux/fs.h>
 #include <linux/mm.h>

commit a4a2eb490e38aaff61eafcb8cde6725ad1be22ab
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu May 3 09:02:48 2012 +0000

    init_task: Create generic init_task instance
    
    All archs define init_task in the same way (except ia64, but there is
    no particular reason why ia64 cannot use the common version). Create a
    generic instance so all archs can be converted over.
    
    The config switch is temporary and will be removed when all archs are
    converted over.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Chen Liqin <liqin.chen@sunplusct.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Chris Zankel <chris@zankel.net>
    Cc: David Howells <dhowells@redhat.com>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Guan Xuetao <gxt@mprc.pku.edu.cn>
    Cc: Haavard Skinnemoen <hskinnemoen@gmail.com>
    Cc: Hirokazu Takata <takata@linux-m32r.org>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jonas Bonn <jonas@southpole.se>
    Cc: Mark Salter <msalter@redhat.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Matt Turner <mattst88@gmail.com>
    Cc: Michal Simek <monstr@monstr.eu>
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Richard Kuo <rkuo@codeaurora.org>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Link: http://lkml.kernel.org/r/20120503085034.092585287@linutronix.de

diff --git a/init/init_task.c b/init/init_task.c
new file mode 100644
index 000000000000..8b2f3996b035
--- /dev/null
+++ b/init/init_task.c
@@ -0,0 +1,24 @@
+#include <linux/init_task.h>
+#include <linux/export.h>
+#include <linux/mqueue.h>
+#include <linux/sched.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+
+#include <asm/pgtable.h>
+#include <asm/uaccess.h>
+
+static struct signal_struct init_signals = INIT_SIGNALS(init_signals);
+static struct sighand_struct init_sighand = INIT_SIGHAND(init_sighand);
+
+/* Initial task structure */
+struct task_struct init_task = INIT_TASK(init_task);
+EXPORT_SYMBOL(init_task);
+
+/*
+ * Initial thread structure. Alignment of this is handled by a special
+ * linker map entry.
+ */
+union thread_union init_thread_union __init_task_data =
+	{ INIT_THREAD_INFO(init_task) };
