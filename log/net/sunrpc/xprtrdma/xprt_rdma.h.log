commit c487eb7d8e41579d87216ce43152acd336f2c4aa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Jun 15 09:21:07 2020 -0400

    xprtrdma: Clean up disconnect
    
    1. Ensure that only rpcrdma_cm_event_handler() modifies
       ep->re_connect_status to avoid racy changes to that field.
    
    2. Ensure that xprt_force_disconnect() is invoked only once as a
       transport is closed or destroyed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 098d05a62ead..43974ef39a50 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -82,6 +82,7 @@ struct rpcrdma_ep {
 	unsigned int		re_max_inline_recv;
 	int			re_async_rc;
 	int			re_connect_status;
+	atomic_t		re_force_disconnect;
 	struct ib_qp_init_attr	re_attr;
 	wait_queue_head_t       re_connect_wait;
 	struct rpc_xprt		*re_xprt;

commit f423f755f41e4944fb4cd1c259cbf2ba3608d647
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Jun 15 09:21:02 2020 -0400

    xprtrdma: Clean up synopsis of rpcrdma_flush_disconnect()
    
    Refactor: Pass struct rpcrdma_xprt instead of an IB layer object.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0a16fdb09b2c..098d05a62ead 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -446,7 +446,7 @@ extern unsigned int xprt_rdma_memreg_strategy;
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */
-void rpcrdma_flush_disconnect(struct ib_cq *cq, struct ib_wc *wc);
+void rpcrdma_flush_disconnect(struct rpcrdma_xprt *r_xprt, struct ib_wc *wc);
 int rpcrdma_xprt_connect(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_xprt_disconnect(struct rpcrdma_xprt *r_xprt);
 

commit e28ce90083f032ca0e8ea03478f5b6a38f5930f7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:01:05 2020 -0500

    xprtrdma: kmalloc rpcrdma_ep separate from rpcrdma_xprt
    
    Change the rpcrdma_xprt_disconnect() function so that it no longer
    waits for the DISCONNECTED event.  This prevents blocking if the
    remote is unresponsive.
    
    In rpcrdma_xprt_disconnect(), the transport's rpcrdma_ep is
    detached. Upon return from rpcrdma_xprt_disconnect(), the transport
    (r_xprt) is ready immediately for a new connection.
    
    The RDMA_CM_DEVICE_REMOVAL and RDMA_CM_DISCONNECTED events are now
    handled almost identically.
    
    However, because the lifetimes of rpcrdma_xprt structures and
    rpcrdma_ep structures are now independent, creating an rpcrdma_ep
    needs to take a module ref count. The ep now owns most of the
    hardware resources for a transport.
    
    Also, a kref is needed to ensure that rpcrdma_ep sticks around
    long enough for the cm_event_handler to finish.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f3c0b826c9ed..0a16fdb09b2c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -68,6 +68,7 @@
  * RDMA Endpoint -- connection endpoint details
  */
 struct rpcrdma_ep {
+	struct kref		re_kref;
 	struct rdma_cm_id 	*re_id;
 	struct ib_pd		*re_pd;
 	unsigned int		re_max_rdma_segs;
@@ -75,7 +76,6 @@ struct rpcrdma_ep {
 	bool			re_implicit_roundup;
 	enum ib_mr_type		re_mrtype;
 	struct completion	re_done;
-	struct completion	re_remove_done;
 	unsigned int		re_send_count;
 	unsigned int		re_send_batch;
 	unsigned int		re_max_inline_send;
@@ -83,7 +83,8 @@ struct rpcrdma_ep {
 	int			re_async_rc;
 	int			re_connect_status;
 	struct ib_qp_init_attr	re_attr;
-	wait_queue_head_t	re_connect_wait;
+	wait_queue_head_t       re_connect_wait;
+	struct rpc_xprt		*re_xprt;
 	struct rpcrdma_connect_private
 				re_cm_private;
 	struct rdma_conn_param	re_remote_cma;
@@ -411,7 +412,7 @@ struct rpcrdma_stats {
  */
 struct rpcrdma_xprt {
 	struct rpc_xprt		rx_xprt;
-	struct rpcrdma_ep	rx_ep;
+	struct rpcrdma_ep	*rx_ep;
 	struct rpcrdma_buffer	rx_buf;
 	struct delayed_work	rx_connect_worker;
 	struct rpc_timeout	rx_timeout;

commit 93aa8e0a9de80e1df2be17158a3469285e572b39
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:54 2020 -0500

    xprtrdma: Merge struct rpcrdma_ia into struct rpcrdma_ep
    
    I eventually want to allocate rpcrdma_ep separately from struct
    rpcrdma_xprt so that on occasion there can be more than one ep per
    xprt.
    
    The new struct rpcrdma_ep will contain all the fields currently in
    rpcrdma_ia and in rpcrdma_ep. This is all the device and CM settings
    for the connection, in addition to per-connection settings
    negotiated with the remote.
    
    Take this opportunity to rename the existing ep fields from rep_* to
    re_* to disambiguate these from struct rpcrdma_rep.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 8a3ac9d7ee81..f3c0b826c9ed 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -65,38 +65,32 @@
 #define RPCRDMA_IDLE_DISC_TO	(5U * 60 * HZ)
 
 /*
- * Interface Adapter -- one per transport instance
+ * RDMA Endpoint -- connection endpoint details
  */
-struct rpcrdma_ia {
-	struct rdma_cm_id 	*ri_id;
-	struct ib_pd		*ri_pd;
-	int			ri_async_rc;
-	unsigned int		ri_max_rdma_segs;
-	unsigned int		ri_max_frwr_depth;
-	bool			ri_implicit_roundup;
-	enum ib_mr_type		ri_mrtype;
-	struct completion	ri_done;
-	struct completion	ri_remove_done;
-};
-
-/*
- * RDMA Endpoint -- one per transport instance
- */
-
 struct rpcrdma_ep {
-	unsigned int		rep_send_count;
-	unsigned int		rep_send_batch;
-	unsigned int		rep_max_inline_send;
-	unsigned int		rep_max_inline_recv;
-	int			rep_connected;
-	struct ib_qp_init_attr	rep_attr;
-	wait_queue_head_t 	rep_connect_wait;
-	struct rpcrdma_connect_private	rep_cm_private;
-	struct rdma_conn_param	rep_remote_cma;
-	unsigned int		rep_max_requests;	/* depends on device */
-	unsigned int		rep_inline_send;	/* negotiated */
-	unsigned int		rep_inline_recv;	/* negotiated */
-	int			rep_receive_count;
+	struct rdma_cm_id 	*re_id;
+	struct ib_pd		*re_pd;
+	unsigned int		re_max_rdma_segs;
+	unsigned int		re_max_fr_depth;
+	bool			re_implicit_roundup;
+	enum ib_mr_type		re_mrtype;
+	struct completion	re_done;
+	struct completion	re_remove_done;
+	unsigned int		re_send_count;
+	unsigned int		re_send_batch;
+	unsigned int		re_max_inline_send;
+	unsigned int		re_max_inline_recv;
+	int			re_async_rc;
+	int			re_connect_status;
+	struct ib_qp_init_attr	re_attr;
+	wait_queue_head_t	re_connect_wait;
+	struct rpcrdma_connect_private
+				re_cm_private;
+	struct rdma_conn_param	re_remote_cma;
+	int			re_receive_count;
+	unsigned int		re_max_requests; /* depends on device */
+	unsigned int		re_inline_send;	/* negotiated */
+	unsigned int		re_inline_recv;	/* negotiated */
 };
 
 /* Pre-allocate extra Work Requests for handling backward receives
@@ -417,7 +411,6 @@ struct rpcrdma_stats {
  */
 struct rpcrdma_xprt {
 	struct rpc_xprt		rx_xprt;
-	struct rpcrdma_ia	rx_ia;
 	struct rpcrdma_ep	rx_ep;
 	struct rpcrdma_buffer	rx_buf;
 	struct delayed_work	rx_connect_worker;
@@ -522,8 +515,7 @@ rpcrdma_data_dir(bool writing)
 /* Memory registration calls xprtrdma/frwr_ops.c
  */
 void frwr_reset(struct rpcrdma_req *req);
-int frwr_query_device(struct rpcrdma_xprt *r_xprt,
-		      const struct ib_device *device);
+int frwr_query_device(struct rpcrdma_ep *ep, const struct ib_device *device);
 int frwr_mr_init(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr *mr);
 void frwr_release_mr(struct rpcrdma_mr *mr);
 struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
@@ -555,7 +547,7 @@ int rpcrdma_prepare_send_sges(struct rpcrdma_xprt *r_xprt,
 			      enum rpcrdma_chunktype rtype);
 void rpcrdma_sendctx_unmap(struct rpcrdma_sendctx *sc);
 int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
-void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
+void rpcrdma_set_max_header_sizes(struct rpcrdma_ep *ep);
 void rpcrdma_reset_cwnd(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);
 void rpcrdma_reply_handler(struct rpcrdma_rep *rep);

commit d6ccebf956338ea015d7d54c4a4c9c17605707cb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:49 2020 -0500

    xprtrdma: Disconnect on flushed completion
    
    Completion errors after a disconnect often occur much sooner than a
    CM_DISCONNECT event. Use this to try to detect connection loss more
    quickly.
    
    Note that other kernel ULPs do take care to disconnect explicitly
    when a WR is flushed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d2a0f125f7a8..8a3ac9d7ee81 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -452,6 +452,7 @@ extern unsigned int xprt_rdma_memreg_strategy;
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */
+void rpcrdma_flush_disconnect(struct ib_cq *cq, struct ib_wc *wc);
 int rpcrdma_xprt_connect(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_xprt_disconnect(struct rpcrdma_xprt *r_xprt);
 

commit 897b7be9bca0caa27cdf7520bdc7689abe989a53
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:44 2020 -0500

    xprtrdma: Remove rpcrdma_ia::ri_flags
    
    Clean up:
    The upper layer serializes calls to xprt_rdma_close, so there is no
    need for an atomic bit operation, saving 8 bytes in rpcrdma_ia.
    
    This enables merging rpcrdma_ia_remove directly into the disconnect
    logic.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 8be1b70b71a2..d2a0f125f7a8 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -75,15 +75,10 @@ struct rpcrdma_ia {
 	unsigned int		ri_max_frwr_depth;
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
-	unsigned long		ri_flags;
 	struct completion	ri_done;
 	struct completion	ri_remove_done;
 };
 
-enum {
-	RPCRDMA_IAF_REMOVING = 0,
-};
-
 /*
  * RDMA Endpoint -- one per transport instance
  */
@@ -454,11 +449,6 @@ extern int xprt_rdma_pad_optimize;
  */
 extern unsigned int xprt_rdma_memreg_strategy;
 
-/*
- * Interface Adapter calls - xprtrdma/verbs.c
- */
-void rpcrdma_ia_remove(struct rpcrdma_ia *ia);
-
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */

commit 81fe0c57f4e136375f3bcda74af413f82a34a1bb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:38 2020 -0500

    xprtrdma: Invoke rpcrdma_ia_open in the connect worker
    
    Move rdma_cm_id creation into rpcrdma_ep_create() so that it is now
    responsible for allocating all per-connection hardware resources.
    
    With this clean-up, all three arms of the switch statement in
    rpcrdma_ep_connect are exactly the same now, thus the switch can be
    removed.
    
    Because device removal behaves a little differently than
    disconnection, there is a little more work to be done before
    rpcrdma_ep_destroy() can release the connection's rdma_cm_id. So
    it is not quite symmetrical with rpcrdma_ep_create() yet.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9ead06b1d8a4..8be1b70b71a2 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -457,9 +457,7 @@ extern unsigned int xprt_rdma_memreg_strategy;
 /*
  * Interface Adapter calls - xprtrdma/verbs.c
  */
-int rpcrdma_ia_open(struct rpcrdma_xprt *xprt);
 void rpcrdma_ia_remove(struct rpcrdma_ia *ia);
-void rpcrdma_ia_close(struct rpcrdma_ia *);
 
 /*
  * Endpoint calls - xprtrdma/verbs.c

commit 9144a803df6ca4185238ca343dbb65d8137c036e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:28 2020 -0500

    xprtrdma: Refactor rpcrdma_ep_connect() and rpcrdma_ep_disconnect()
    
    Clean up: Simplify the synopses of functions in the connect and
    disconnect paths in preparation for combining the rpcrdma_ia and
    struct rpcrdma_ep structures.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 82ec4c25432f..9ead06b1d8a4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -464,8 +464,8 @@ void rpcrdma_ia_close(struct rpcrdma_ia *);
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */
-int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
-void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
+int rpcrdma_xprt_connect(struct rpcrdma_xprt *r_xprt);
+void rpcrdma_xprt_disconnect(struct rpcrdma_xprt *r_xprt);
 
 int rpcrdma_post_sends(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);

commit 97d0de8812a10a66510ff95f8fe6e8d3053fd2ca
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:23 2020 -0500

    xprtrdma: Clean up the post_send path
    
    Clean up: Simplify the synopses of functions in the post_send path
    by combining the struct rpcrdma_ia and struct rpcrdma_ep arguments.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9e3e9a82cb9f..82ec4c25432f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -467,8 +467,7 @@ void rpcrdma_ia_close(struct rpcrdma_ia *);
 int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
-int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
-				struct rpcrdma_req *);
+int rpcrdma_post_sends(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);
 
 /*
@@ -542,7 +541,7 @@ struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 				struct rpcrdma_mr_seg *seg,
 				int nsegs, bool writing, __be32 xid,
 				struct rpcrdma_mr *mr);
-int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
+int frwr_send(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);
 void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 void frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);

commit 253a51622fb03425b611e709e34f1ea70949a61f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:17 2020 -0500

    xprtrdma: Refactor frwr_init_mr()
    
    Clean up: prepare for combining the rpcrdma_ia and rpcrdma_ep
    structures. Take the opportunity to rename the function to be
    consistent with the "subsystem _ object _ verb" naming scheme.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9a536319557e..9e3e9a82cb9f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -536,7 +536,7 @@ rpcrdma_data_dir(bool writing)
 void frwr_reset(struct rpcrdma_req *req);
 int frwr_query_device(struct rpcrdma_xprt *r_xprt,
 		      const struct ib_device *device);
-int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);
+int frwr_mr_init(struct rpcrdma_xprt *r_xprt, struct rpcrdma_mr *mr);
 void frwr_release_mr(struct rpcrdma_mr *mr);
 struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 				struct rpcrdma_mr_seg *seg,

commit 85cd8e2b78eea7374927750ffec60bf047f8f90b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:12 2020 -0500

    xprtrdma: Invoke rpcrdma_ep_create() in the connect worker
    
    Refactor rpcrdma_ep_create(), rpcrdma_ep_disconnect(), and
    rpcrdma_ep_destroy().
    
    rpcrdma_ep_create will be invoked at connect time instead of at
    transport set-up time. It will be responsible for allocating per-
    connection resources. In this patch it allocates the CQs and
    creates a QP. More to come.
    
    rpcrdma_ep_destroy() is the inverse functionality that is
    invoked at disconnect time. It will be responsible for releasing
    the CQs and QP.
    
    These changes should be safe to do because both connect and
    disconnect is guaranteed to be serialized by the transport send
    lock.
    
    This takes us another step closer to resolving the address and route
    only at connect time so that connection failover to another device
    will work correctly.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 37d5080c250b..9a536319557e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -464,8 +464,6 @@ void rpcrdma_ia_close(struct rpcrdma_ia *);
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */
-int rpcrdma_ep_create(struct rpcrdma_xprt *r_xprt);
-void rpcrdma_ep_destroy(struct rpcrdma_xprt *r_xprt);
 int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 

commit b78de1dca00376aaba7a58bb5fe21c1606524abe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:56:53 2020 -0500

    xprtrdma: Allocate and map transport header buffers at connect time
    
    Currently the underlying RDMA device is chosen at transport set-up
    time. But it will soon be at connect time instead.
    
    The maximum size of a transport header is based on device
    capabilities. Thus transport header buffers have to be allocated
    _after_ the underlying device has been chosen (via address and route
    resolution); ie, in the connect worker.
    
    Thus, move the allocation of transport header buffers to the connect
    worker, after the point at which the underlying RDMA device has been
    chosen.
    
    This also means the RDMA device is available to do a DMA mapping of
    these buffers at connect time, instead of in the hot I/O path. Make
    that optimization as well.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0aed1e98f2bf..37d5080c250b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -478,6 +478,7 @@ void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);
  */
 struct rpcrdma_req *rpcrdma_req_create(struct rpcrdma_xprt *r_xprt, size_t size,
 				       gfp_t flags);
+int rpcrdma_req_setup(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 void rpcrdma_req_destroy(struct rpcrdma_req *req);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);

commit 25868e610aed20e06f6ff10a562a04e8aaea5a5e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:56:48 2020 -0500

    xprtrdma: Refactor frwr_is_supported
    
    Refactor: Perform the "is supported" check in rpcrdma_ep_create()
    instead of in rpcrdma_ia_open(). frwr_open() is where most of the
    logic to query device attributes is already located.
    
    The current code displays a redundant error message when the device
    does not support FRWR. As an additional clean-up, this patch removes
    the extra message.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index aac4cf959c3a..0aed1e98f2bf 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -534,9 +534,9 @@ rpcrdma_data_dir(bool writing)
 
 /* Memory registration calls xprtrdma/frwr_ops.c
  */
-bool frwr_is_supported(struct ib_device *device);
 void frwr_reset(struct rpcrdma_req *req);
-int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep);
+int frwr_query_device(struct rpcrdma_xprt *r_xprt,
+		      const struct ib_device *device);
 int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);
 void frwr_release_mr(struct rpcrdma_mr *mr);
 struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,

commit 18d065a5d4f16eeefb690c298671c3f9131121fe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:56:43 2020 -0500

    xprtrdma: Eliminate per-transport "max pages"
    
    To support device hotplug and migrating a connection between devices
    of different capabilities, we have to guarantee that all in-kernel
    devices can support the same max NFS payload size (1 megabyte).
    
    This means that possibly one or two in-tree devices are no longer
    supported for NFS/RDMA because they cannot support 1MB rsize/wsize.
    The only one I confirmed was cxgb3, but it has already been removed
    from the kernel.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0fde694144f5..aac4cf959c3a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -71,7 +71,7 @@ struct rpcrdma_ia {
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
 	int			ri_async_rc;
-	unsigned int		ri_max_segs;
+	unsigned int		ri_max_rdma_segs;
 	unsigned int		ri_max_frwr_depth;
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
@@ -539,7 +539,6 @@ void frwr_reset(struct rpcrdma_req *req);
 int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep);
 int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);
 void frwr_release_mr(struct rpcrdma_mr *mr);
-size_t frwr_maxpages(struct rpcrdma_xprt *r_xprt);
 struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 				struct rpcrdma_mr_seg *seg,
 				int nsegs, bool writing, __be32 xid,

commit 7581d90109cad7d7322fd90cea023c706912f4bd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:56:37 2020 -0500

    xprtrdma: Refactor initialization of ep->rep_max_requests
    
    Clean up: there is no need to keep two copies of the same value.
    Also, in subsequent patches, rpcrdma_ep_create() will be called in
    the connect worker rather than at set-up time.
    
    Minor fix: Initialize the transport's sendctx to the value based on
    the capabilities of the underlying device, not the maximum setting.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 7655a99fd559..0fde694144f5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -98,7 +98,7 @@ struct rpcrdma_ep {
 	wait_queue_head_t 	rep_connect_wait;
 	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
-	unsigned int		rep_max_requests;	/* set by /proc */
+	unsigned int		rep_max_requests;	/* depends on device */
 	unsigned int		rep_inline_send;	/* negotiated */
 	unsigned int		rep_inline_recv;	/* negotiated */
 	int			rep_receive_count;
@@ -372,7 +372,7 @@ struct rpcrdma_buffer {
 
 	struct llist_head	rb_free_reps;
 
-	u32			rb_max_requests;
+	__be32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */
 
 	u32			rb_bc_srv_max_requests;
@@ -582,7 +582,6 @@ static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 
 /* RPC/RDMA module init - xprtrdma/transport.c
  */
-extern unsigned int xprt_rdma_slot_table_entries;
 extern unsigned int xprt_rdma_max_inline_read;
 extern unsigned int xprt_rdma_max_inline_write;
 void xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap);

commit 2e87036814290887a188652a893ab968bad9fad7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:56:27 2020 -0500

    xprtrdma: Eliminate ri_max_send_sges
    
    Clean-up. The max_send_sge value also happens to be stored in
    ep->rep_attr. Let's keep just a single copy.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d796d68609ed..7655a99fd559 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -73,7 +73,6 @@ struct rpcrdma_ia {
 	int			ri_async_rc;
 	unsigned int		ri_max_segs;
 	unsigned int		ri_max_frwr_depth;
-	unsigned int		ri_max_send_sges;
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
 	unsigned long		ri_flags;

commit 671c450b6fe0680ea1cb1cf1526d764fdd5a3d3f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:52:22 2020 -0500

    xprtrdma: Fix oops in Receive handler after device removal
    
    Since v5.4, a device removal occasionally triggered this oops:
    
    Dec  2 17:13:53 manet kernel: BUG: unable to handle page fault for address: 0000000c00000219
    Dec  2 17:13:53 manet kernel: #PF: supervisor read access in kernel mode
    Dec  2 17:13:53 manet kernel: #PF: error_code(0x0000) - not-present page
    Dec  2 17:13:53 manet kernel: PGD 0 P4D 0
    Dec  2 17:13:53 manet kernel: Oops: 0000 [#1] SMP
    Dec  2 17:13:53 manet kernel: CPU: 2 PID: 468 Comm: kworker/2:1H Tainted: G        W         5.4.0-00050-g53717e43af61 #883
    Dec  2 17:13:53 manet kernel: Hardware name: Supermicro SYS-6028R-T/X10DRi, BIOS 1.1a 10/16/2015
    Dec  2 17:13:53 manet kernel: Workqueue: ib-comp-wq ib_cq_poll_work [ib_core]
    Dec  2 17:13:53 manet kernel: RIP: 0010:rpcrdma_wc_receive+0x7c/0xf6 [rpcrdma]
    Dec  2 17:13:53 manet kernel: Code: 6d 8b 43 14 89 c1 89 45 78 48 89 4d 40 8b 43 2c 89 45 14 8b 43 20 89 45 18 48 8b 45 20 8b 53 14 48 8b 30 48 8b 40 10 48 8b 38 <48> 8b 87 18 02 00 00 48 85 c0 75 18 48 8b 05 1e 24 c4 e1 48 85 c0
    Dec  2 17:13:53 manet kernel: RSP: 0018:ffffc900035dfe00 EFLAGS: 00010246
    Dec  2 17:13:53 manet kernel: RAX: ffff888467290000 RBX: ffff88846c638400 RCX: 0000000000000048
    Dec  2 17:13:53 manet kernel: RDX: 0000000000000048 RSI: 00000000f942e000 RDI: 0000000c00000001
    Dec  2 17:13:53 manet kernel: RBP: ffff888467611b00 R08: ffff888464e4a3c4 R09: 0000000000000000
    Dec  2 17:13:53 manet kernel: R10: ffffc900035dfc88 R11: fefefefefefefeff R12: ffff888865af4428
    Dec  2 17:13:53 manet kernel: R13: ffff888466023000 R14: ffff88846c63f000 R15: 0000000000000010
    Dec  2 17:13:53 manet kernel: FS:  0000000000000000(0000) GS:ffff88846fa80000(0000) knlGS:0000000000000000
    Dec  2 17:13:53 manet kernel: CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    Dec  2 17:13:53 manet kernel: CR2: 0000000c00000219 CR3: 0000000002009002 CR4: 00000000001606e0
    Dec  2 17:13:53 manet kernel: Call Trace:
    Dec  2 17:13:53 manet kernel: __ib_process_cq+0x5c/0x14e [ib_core]
    Dec  2 17:13:53 manet kernel: ib_cq_poll_work+0x26/0x70 [ib_core]
    Dec  2 17:13:53 manet kernel: process_one_work+0x19d/0x2cd
    Dec  2 17:13:53 manet kernel: ? cancel_delayed_work_sync+0xf/0xf
    Dec  2 17:13:53 manet kernel: worker_thread+0x1a6/0x25a
    Dec  2 17:13:53 manet kernel: ? cancel_delayed_work_sync+0xf/0xf
    Dec  2 17:13:53 manet kernel: kthread+0xf4/0xf9
    Dec  2 17:13:53 manet kernel: ? kthread_queue_delayed_work+0x74/0x74
    Dec  2 17:13:53 manet kernel: ret_from_fork+0x24/0x30
    
    The proximal cause is that this rpcrdma_rep has a rr_rdmabuf that
    is still pointing to the old ib_device, which has been freed. The
    only way that is possible is if this rpcrdma_rep was not destroyed
    by rpcrdma_ia_remove.
    
    Debugging showed that was indeed the case: this rpcrdma_rep was
    still in use by a completing RPC at the time of the device removal,
    and thus wasn't on the rep free list. So, it was not found by
    rpcrdma_reps_destroy().
    
    The fix is to introduce a list of all rpcrdma_reps so that they all
    can be found when a device is removed. That list is used to perform
    only regbuf DMA unmapping, replacing that call to
    rpcrdma_reps_destroy().
    
    Meanwhile, to prevent corruption of this list, I've moved the
    destruction of temp rpcrdma_rep objects to rpcrdma_post_recvs().
    rpcrdma_xprt_drain() ensures that post_recvs (and thus rep_destroy) is
    not invoked while rpcrdma_reps_unmap is walking rb_all_reps, thus
    protecting the rb_all_reps list.
    
    Fixes: b0b227f071a0 ("xprtrdma: Use an llist to manage free rpcrdma_reps")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5d15140a0266..d796d68609ed 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -203,6 +203,7 @@ struct rpcrdma_rep {
 	struct xdr_stream	rr_stream;
 	struct llist_node	rr_node;
 	struct ib_recv_wr	rr_recv_wr;
+	struct list_head	rr_all;
 };
 
 /* To reduce the rate at which a transport invokes ib_post_recv
@@ -368,6 +369,7 @@ struct rpcrdma_buffer {
 
 	struct list_head	rb_allreqs;
 	struct list_head	rb_all_mrs;
+	struct list_head	rb_all_reps;
 
 	struct llist_head	rb_free_reps;
 

commit 614f3c96d7e5efd1c4dc699524857130a52c6a7f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Oct 17 14:31:53 2019 -0400

    xprtrdma: Pull up sometimes
    
    On some platforms, DMA mapping part of a page is more costly than
    copying bytes. Restore the pull-up code and use that when we
    think it's going to be faster. The heuristic for now is to pull-up
    when the size of the RPC message body fits in the buffer underlying
    the head iovec.
    
    Indeed, not involving the I/O MMU can help the RPC/RDMA transport
    scale better for tiny I/Os across more RDMA devices. This is because
    interaction with the I/O MMU is eliminated, as is handling a Send
    completion, for each of these small I/Os. Without the explicit
    unmapping, the NIC no longer needs to do a costly internal TLB shoot
    down for buffers that are just a handful of bytes.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cdd6a3d43e0f..5d15140a0266 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -554,6 +554,8 @@ void frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 
 enum rpcrdma_chunktype {
 	rpcrdma_noch = 0,
+	rpcrdma_noch_pullup,
+	rpcrdma_noch_mapped,
 	rpcrdma_readch,
 	rpcrdma_areadch,
 	rpcrdma_writech,

commit dc15c3d5f16808f7c171b55da6a82a5c0f279647
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Oct 17 14:31:35 2019 -0400

    xprtrdma: Move the rpcrdma_sendctx::sc_wr field
    
    Clean up: This field is not needed in the Send completion handler,
    so it can be moved to struct rpcrdma_req to reduce the size of
    struct rpcrdma_sendctx, and to reduce the amount of memory that
    is sloshed between the sending process and the Send completion
    process.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0e5b7f373077..cdd6a3d43e0f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -219,7 +219,6 @@ enum {
  */
 struct rpcrdma_req;
 struct rpcrdma_sendctx {
-	struct ib_send_wr	sc_wr;
 	struct ib_cqe		sc_cqe;
 	struct rpcrdma_req	*sc_req;
 	unsigned int		sc_unmap_count;
@@ -314,6 +313,7 @@ struct rpcrdma_req {
 	struct rpcrdma_rep	*rl_reply;
 	struct xdr_stream	rl_stream;
 	struct xdr_buf		rl_hdrbuf;
+	struct ib_send_wr	rl_wr;
 	struct rpcrdma_sendctx	*rl_sendctx;
 	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
 	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */

commit b5cde6aa882dfb40a2b29c1c7371fdc3655c51ce
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Oct 17 14:31:27 2019 -0400

    xprtrdma: Remove rpcrdma_sendctx::sc_device
    
    Micro-optimization: Save eight bytes in a frequently allocated
    structure.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 4897c09d6f61..0e5b7f373077 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -221,7 +221,6 @@ struct rpcrdma_req;
 struct rpcrdma_sendctx {
 	struct ib_send_wr	sc_wr;
 	struct ib_cqe		sc_cqe;
-	struct ib_device	*sc_device;
 	struct rpcrdma_req	*sc_req;
 	unsigned int		sc_unmap_count;
 	struct ib_sge		sc_sges[];

commit f995879ec4aa8b50c3924fda3014b0ab9acad7bd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Oct 17 14:31:18 2019 -0400

    xprtrdma: Remove rpcrdma_sendctx::sc_xprt
    
    Micro-optimization: Save eight bytes in a frequently allocated
    structure.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b8e768d55cb0..4897c09d6f61 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -218,12 +218,10 @@ enum {
 /* struct rpcrdma_sendctx - DMA mapped SGEs to unmap after Send completes
  */
 struct rpcrdma_req;
-struct rpcrdma_xprt;
 struct rpcrdma_sendctx {
 	struct ib_send_wr	sc_wr;
 	struct ib_cqe		sc_cqe;
 	struct ib_device	*sc_device;
-	struct rpcrdma_xprt	*sc_xprt;
 	struct rpcrdma_req	*sc_req;
 	unsigned int		sc_unmap_count;
 	struct ib_sge		sc_sges[];

commit 15d9b015d3d1c997893472cb42d9f225a60a9219
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Oct 17 14:31:09 2019 -0400

    xprtrdma: Ensure ri_id is stable during MR recycling
    
    ia->ri_id is replaced during a reconnect. The connect_worker runs
    with the transport send lock held to prevent ri_id from being
    dereferenced by the send_request path during this process.
    
    Currently, however, there is no guarantee that ia->ri_id is stable
    in the MR recycling worker, which operates in the background and is
    not serialized with the connect_worker in any way.
    
    But now that Local_Inv completions are being done in process
    context, we can handle the recycling operation there instead of
    deferring the recycling work to another process. Because the
    disconnect path drains all work before allowing tear down to
    proceed, it is guaranteed that Local Invalidations complete only
    while the ri_id pointer is stable.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a7ef9653bafd..b8e768d55cb0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -257,7 +257,6 @@ struct rpcrdma_mr {
 	u32			mr_handle;
 	u32			mr_length;
 	u64			mr_offset;
-	struct work_struct	mr_recycle;
 	struct list_head	mr_all;
 };
 
@@ -490,12 +489,6 @@ struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_mr_put(struct rpcrdma_mr *mr);
 void rpcrdma_mrs_refresh(struct rpcrdma_xprt *r_xprt);
 
-static inline void
-rpcrdma_mr_recycle(struct rpcrdma_mr *mr)
-{
-	schedule_work(&mr->mr_recycle);
-}
-
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
 void rpcrdma_buffer_put(struct rpcrdma_buffer *buffers,
 			struct rpcrdma_req *req);

commit 9d2da4ff00f37de17fc25c23e50463b58b9e8fec
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 9 13:07:48 2019 -0400

    xprtrdma: Manage MRs in context of a single connection
    
    MRs are now allocated on demand so we can safely throw them away on
    disconnect. This way an idle transport can disconnect and it won't
    pin hardware MR resources.
    
    Two additional changes:
    
    - Now that all MRs are destroyed on disconnect, there's no need to
      check during header marshaling if a req has MRs to recycle. Each
      req is sent only once per connection, and now rl_registered is
      guaranteed to be empty when rpcrdma_marshal_req is invoked.
    
    - Because MRs are now destroyed in a WQ_MEM_RECLAIM context, they
      also must be allocated in a WQ_MEM_RECLAIM context. This reduces
      the likelihood that device driver memory allocation will trigger
      memory reclaim during NFS writeback.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2f89cfccdb48..a7ef9653bafd 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -488,6 +488,7 @@ struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_xprt *r_xprt);
 
 struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_mr_put(struct rpcrdma_mr *mr);
+void rpcrdma_mrs_refresh(struct rpcrdma_xprt *r_xprt);
 
 static inline void
 rpcrdma_mr_recycle(struct rpcrdma_mr *mr)
@@ -543,7 +544,6 @@ rpcrdma_data_dir(bool writing)
 /* Memory registration calls xprtrdma/frwr_ops.c
  */
 bool frwr_is_supported(struct ib_device *device);
-void frwr_recycle(struct rpcrdma_req *req);
 void frwr_reset(struct rpcrdma_req *req);
 int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep);
 int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);

commit 2ae50ad68cd79224198b525f7bd645c9da98b6ff
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 9 13:07:38 2019 -0400

    xprtrdma: Close window between waking RPC senders and posting Receives
    
    A recent clean up attempted to separate Receive handling and RPC
    Reply processing, in the name of clean layering.
    
    Unfortunately, we can't do this because the Receive Queue has to be
    refilled _after_ the most recent credit update from the responder
    is parsed from the transport header, but _before_ we wake up the
    next RPC sender. That is right in the middle of
    rpcrdma_reply_handler().
    
    Usually this isn't a problem because current responder
    implementations don't vary their credit grant. The one exception is
    when a connection is established: the grant goes from one to a much
    larger number on the first Receive. The requester MUST post enough
    Receives right then so that any outstanding requests can be sent
    without risking RNR and connection loss.
    
    Fixes: 6ceea36890a0 ("xprtrdma: Refactor Receive accounting")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b170cf52c638..2f89cfccdb48 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -474,6 +474,7 @@ void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 				struct rpcrdma_req *);
+void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);
 
 /*
  * Buffer calls - xprtrdma/verbs.c

commit eea63ca7ffa1f3a4a0b02b902ec51eab2d4e9df4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 9 13:07:32 2019 -0400

    xprtrdma: Initialize rb_credits in one place
    
    Clean up/code de-duplication.
    
    Nit: RPC_CWNDSHIFT is incorrect as the initial value for xprt->cwnd.
    This mistake does not appear to have operational consequences, since
    the cwnd value is replaced with a valid value upon the first Receive
    completion.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 65e6b0eb862e..b170cf52c638 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -576,6 +576,7 @@ int rpcrdma_prepare_send_sges(struct rpcrdma_xprt *r_xprt,
 void rpcrdma_sendctx_unmap(struct rpcrdma_sendctx *sc);
 int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
+void rpcrdma_reset_cwnd(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);
 void rpcrdma_reply_handler(struct rpcrdma_rep *rep);
 

commit ee2f412ece32ab685921408ab1242d097557b57c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 26 13:12:46 2019 -0400

    xprtrdma: Recycle MRs after disconnect
    
    The optimization done in "xprtrdma: Simplify rpcrdma_mr_pop" was a
    bit too optimistic. MRs left over after a reconnect still need to
    be recycled, not added back to the free list, since they could be
    in flight or actually fully registered.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index bd1befa83d24..65e6b0eb862e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -542,6 +542,7 @@ rpcrdma_data_dir(bool writing)
 /* Memory registration calls xprtrdma/frwr_ops.c
  */
 bool frwr_is_supported(struct ib_device *device);
+void frwr_recycle(struct rpcrdma_req *req);
 void frwr_reset(struct rpcrdma_req *req);
 int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep);
 int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);

commit b0b227f071a06d0ce5ef803538899299933d2931
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:48:43 2019 -0400

    xprtrdma: Use an llist to manage free rpcrdma_reps
    
    rpcrdma_rep objects are removed from their free list by only a
    single thread: the Receive completion handler. Thus that free list
    can be converted to an llist, where a single-threaded consumer and
    a multi-threaded producer (rpcrdma_buffer_put) can both access the
    llist without the need for any serialization.
    
    This eliminates spin lock contention between the Receive completion
    handler and rpcrdma_buffer_get, and makes the rep consumer wait-
    free.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 200d075bbe31..bd1befa83d24 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -47,6 +47,7 @@
 #include <linux/atomic.h>		/* atomic_t, etc */
 #include <linux/kref.h>			/* struct kref */
 #include <linux/workqueue.h>		/* struct work_struct */
+#include <linux/llist.h>
 
 #include <rdma/rdma_cm.h>		/* RDMA connection api */
 #include <rdma/ib_verbs.h>		/* RDMA verbs api */
@@ -200,7 +201,7 @@ struct rpcrdma_rep {
 	struct rpc_rqst		*rr_rqst;
 	struct xdr_buf		rr_hdrbuf;
 	struct xdr_stream	rr_stream;
-	struct list_head	rr_list;
+	struct llist_node	rr_node;
 	struct ib_recv_wr	rr_recv_wr;
 };
 
@@ -362,7 +363,6 @@ rpcrdma_mr_pop(struct list_head *list)
 struct rpcrdma_buffer {
 	spinlock_t		rb_lock;
 	struct list_head	rb_send_bufs;
-	struct list_head	rb_recv_bufs;
 	struct list_head	rb_mrs;
 
 	unsigned long		rb_sc_head;
@@ -373,6 +373,8 @@ struct rpcrdma_buffer {
 	struct list_head	rb_allreqs;
 	struct list_head	rb_all_mrs;
 
+	struct llist_head	rb_free_reps;
+
 	u32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */
 

commit 4d6b8890ddb16120c5dd25edc8d215cfd8222b63
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:47:57 2019 -0400

    xprtrdma: Remove rpcrdma_buffer::rb_mrlock
    
    Clean up: Now that the free list is used sparingly, get rid of the
    separate spin lock protecting it.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c375b0e434ac..200d075bbe31 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -360,19 +360,18 @@ rpcrdma_mr_pop(struct list_head *list)
  * One of these is associated with a transport instance
  */
 struct rpcrdma_buffer {
-	spinlock_t		rb_mrlock;	/* protect rb_mrs list */
+	spinlock_t		rb_lock;
+	struct list_head	rb_send_bufs;
+	struct list_head	rb_recv_bufs;
 	struct list_head	rb_mrs;
-	struct list_head	rb_all_mrs;
 
 	unsigned long		rb_sc_head;
 	unsigned long		rb_sc_tail;
 	unsigned long		rb_sc_last;
 	struct rpcrdma_sendctx	**rb_sc_ctxs;
 
-	spinlock_t		rb_lock;	/* protect buf lists */
-	struct list_head	rb_send_bufs;
-	struct list_head	rb_recv_bufs;
 	struct list_head	rb_allreqs;
+	struct list_head	rb_all_mrs;
 
 	u32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */

commit 6dc6ec9e04c468d994bff6eb660f3146f94cbfd9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:47:10 2019 -0400

    xprtrdma: Cache free MRs in each rpcrdma_req
    
    Instead of a globally-contended MR free list, cache MRs in each
    rpcrdma_req as they are released. This means acquiring and releasing
    an MR will be lock-free in the common case, even outside the
    transport send lock.
    
    The original idea of per-rpcrdma_req MR free lists was suggested by
    Shirley Ma <shirley.ma@oracle.com> several years ago. I just now
    figured out how to make that idea work with on-demand MR allocation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9573587ca602..c375b0e434ac 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -234,20 +234,20 @@ struct rpcrdma_sendctx {
  * An external memory region is any buffer or page that is registered
  * on the fly (ie, not pre-registered).
  */
-struct rpcrdma_req;
 struct rpcrdma_frwr {
 	struct ib_mr			*fr_mr;
 	struct ib_cqe			fr_cqe;
 	struct completion		fr_linv_done;
-	struct rpcrdma_req		*fr_req;
 	union {
 		struct ib_reg_wr	fr_regwr;
 		struct ib_send_wr	fr_invwr;
 	};
 };
 
+struct rpcrdma_req;
 struct rpcrdma_mr {
 	struct list_head	mr_list;
+	struct rpcrdma_req	*mr_req;
 	struct scatterlist	*mr_sg;
 	int			mr_nents;
 	enum dma_data_direction	mr_dir;
@@ -325,7 +325,8 @@ struct rpcrdma_req {
 	struct list_head	rl_all;
 	struct kref		rl_kref;
 
-	struct list_head	rl_registered;	/* registered segments */
+	struct list_head	rl_free_mrs;
+	struct list_head	rl_registered;
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 };
 

commit 3b39f52a02d4b3322744a0a32d59142e01afa435
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:45:37 2019 -0400

    xprtrdma: Move rpcrdma_mr_get out of frwr_map
    
    Refactor: Retrieve an MR and handle error recovery entirely in
    rpc_rdma.c, as this is not a device-specific function.
    
    Note that since commit 89f90fe1ad8b ("SUNRPC: Allow calls to
    xprt_transmit() to drain the entire transmit queue"), the
    xprt_transmit function handles the cond_resched. The transport no
    longer has to do this itself.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3e0839c2cda2..9573587ca602 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -379,7 +379,7 @@ struct rpcrdma_buffer {
 	u32			rb_bc_srv_max_requests;
 	u32			rb_bc_max_requests;
 
-	struct delayed_work	rb_refresh_worker;
+	struct work_struct	rb_refresh_worker;
 };
 
 /*
@@ -548,7 +548,7 @@ size_t frwr_maxpages(struct rpcrdma_xprt *r_xprt);
 struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 				struct rpcrdma_mr_seg *seg,
 				int nsegs, bool writing, __be32 xid,
-				struct rpcrdma_mr **mr);
+				struct rpcrdma_mr *mr);
 int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
 void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);
 void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);

commit 1ca3f4c054a4e3765bdeb62c849d940b5bc8002d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:44:50 2019 -0400

    xprtrdma: Combine rpcrdma_mr_put and rpcrdma_mr_unmap_and_put
    
    Clean up. There is only one remaining rpcrdma_mr_put call site, and
    it can be directly replaced with unmap_and_put because mr->mr_dir is
    set to DMA_NONE just before the call.
    
    Now all the call sites do a DMA unmap, and we can just rename
    mr_unmap_and_put to mr_put, which nicely matches mr_get.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9663b8ddd733..3e0839c2cda2 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -485,7 +485,6 @@ struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_xprt *r_xprt);
 
 struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_mr_put(struct rpcrdma_mr *mr);
-void rpcrdma_mr_unmap_and_put(struct rpcrdma_mr *mr);
 
 static inline void
 rpcrdma_mr_recycle(struct rpcrdma_mr *mr)

commit 265a38d4611360ae3d5bb612d586a3126507a954
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:44:04 2019 -0400

    xprtrdma: Simplify rpcrdma_mr_pop
    
    Clean up: rpcrdma_mr_pop call sites check if the list is empty
    first. Let's replace the list_empty with less costly logic.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5aaa53b8ae12..9663b8ddd733 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -338,7 +338,7 @@ rpcr_to_rdmar(const struct rpc_rqst *rqst)
 static inline void
 rpcrdma_mr_push(struct rpcrdma_mr *mr, struct list_head *list)
 {
-	list_add_tail(&mr->mr_list, list);
+	list_add(&mr->mr_list, list);
 }
 
 static inline struct rpcrdma_mr *
@@ -346,8 +346,9 @@ rpcrdma_mr_pop(struct list_head *list)
 {
 	struct rpcrdma_mr *mr;
 
-	mr = list_first_entry(list, struct rpcrdma_mr, mr_list);
-	list_del_init(&mr->mr_list);
+	mr = list_first_entry_or_null(list, struct rpcrdma_mr, mr_list);
+	if (mr)
+		list_del_init(&mr->mr_list);
 	return mr;
 }
 

commit eed48a9c161588544999ee8d451392921d846ee8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:42:31 2019 -0400

    xprtrdma: Rename rpcrdma_buffer::rb_all
    
    Clean up: There are other "all" list heads. For code clarity
    distinguish this one as for use only for MRs by renaming it.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index eaf6b907a76e..5aaa53b8ae12 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -360,7 +360,7 @@ rpcrdma_mr_pop(struct list_head *list)
 struct rpcrdma_buffer {
 	spinlock_t		rb_mrlock;	/* protect rb_mrs list */
 	struct list_head	rb_mrs;
-	struct list_head	rb_all;
+	struct list_head	rb_all_mrs;
 
 	unsigned long		rb_sc_head;
 	unsigned long		rb_sc_tail;

commit f3c66a2f56683a20ced4e700a8167d6b357641da
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:40:11 2019 -0400

    xprtrdma: Boost maximum transport header size
    
    Although I haven't seen any performance results that justify it,
    I've received several complaints that NFS/RDMA no longer supports
    a maximum rsize and wsize of 1MB. These days it is somewhat smaller.
    
    To simplify the logic that determines whether a chunk list is
    necessary, the implementation uses a fixed maximum size of the
    transport header. Currently that maximum size is 256 bytes, one
    quarter of the default inline threshold size for RPC/RDMA v1.
    
    Since commit a78868497c2e ("xprtrdma: Reduce max_frwr_depth"), the
    size of chunks is also smaller to take advantage of inline page
    lists in device internal MR data structures.
    
    The combination of these two design choices has reduced the maximum
    NFS rsize and wsize that can be used for most RNIC/HCAs. Increasing
    the maximum transport header size and the maximum number of RDMA
    segments it can contain increases the negotiated maximum rsize/wsize
    on common RNIC/HCAs.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3b2f2041e889..eaf6b907a76e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -155,25 +155,22 @@ static inline void *rdmab_data(const struct rpcrdma_regbuf *rb)
 
 /* To ensure a transport can always make forward progress,
  * the number of RDMA segments allowed in header chunk lists
- * is capped at 8. This prevents less-capable devices and
- * memory registrations from overrunning the Send buffer
- * while building chunk lists.
+ * is capped at 16. This prevents less-capable devices from
+ * overrunning the Send buffer while building chunk lists.
  *
  * Elements of the Read list take up more room than the
- * Write list or Reply chunk. 8 read segments means the Read
- * list (or Write list or Reply chunk) cannot consume more
- * than
+ * Write list or Reply chunk. 16 read segments means the
+ * chunk lists cannot consume more than
  *
- * ((8 + 2) * read segment size) + 1 XDR words, or 244 bytes.
+ * ((16 + 2) * read segment size) + 1 XDR words,
  *
- * And the fixed part of the header is another 24 bytes.
- *
- * The smallest inline threshold is 1024 bytes, ensuring that
- * at least 750 bytes are available for RPC messages.
+ * or about 400 bytes. The fixed part of the header is
+ * another 24 bytes. Thus when the inline threshold is
+ * 1024 bytes, at least 600 bytes are available for RPC
+ * message bodies.
  */
 enum {
-	RPCRDMA_MAX_HDR_SEGS = 8,
-	RPCRDMA_HDRBUF_SIZE = 256,
+	RPCRDMA_MAX_HDR_SEGS = 16,
 };
 
 /*

commit af08a7754a5da7ae704624cbe4ef83e46246c92d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:38:38 2019 -0400

    xprtrdma: Update obsolete comment
    
    Comment was made obsolete by commit 8cec3dba76a4 ("xprtrdma:
    rpcrdma_regbuf alignment").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 92ce09fcea74..3b2f2041e889 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -117,9 +117,6 @@ struct rpcrdma_ep {
 #endif
 
 /* Registered buffer -- registered kmalloc'd memory for RDMA SEND/RECV
- *
- * The below structure appears at the front of a large region of kmalloc'd
- * memory, which always starts on a good alignment boundary.
  */
 
 struct rpcrdma_regbuf {

commit 7402a4fedc2bc448100c2d086406c708451b16dc
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Tue Jul 16 13:51:29 2019 -0400

    SUNRPC: Fix up backchannel slot table accounting
    
    Add a per-transport maximum limit in the socket case, and add
    helpers to allow the NFSv4 code to discover that limit.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 8378f45d2da7..92ce09fcea74 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -605,6 +605,7 @@ void xprt_rdma_cleanup(void);
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 int xprt_rdma_bc_setup(struct rpc_xprt *, unsigned int);
 size_t xprt_rdma_bc_maxpayload(struct rpc_xprt *);
+unsigned int xprt_rdma_bc_max_slots(struct rpc_xprt *);
 int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
 void rpcrdma_bc_receive_call(struct rpcrdma_xprt *, struct rpcrdma_rep *);
 int xprt_rdma_bc_send_reply(struct rpc_rqst *rqst);

commit 675dd90ad0932f2c03912a5252458d792bd7033a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:42 2019 -0400

    xprtrdma: Modernize ops->connect
    
    Adapt and apply changes that were made to the TCP socket connect
    code. See the following commits for details on the purpose of
    these changes:
    
    Commit 7196dbb02ea0 ("SUNRPC: Allow changing of the TCP timeout parameters on the fly")
    Commit 3851f1cdb2b8 ("SUNRPC: Limit the reconnect backoff timer to the max RPC message timeout")
    Commit 02910177aede ("SUNRPC: Fix reconnection timeouts")
    
    Some common transport code is moved to xprt.c to satisfy the code
    duplication police.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 117e32816e4f..8378f45d2da7 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -432,6 +432,7 @@ struct rpcrdma_xprt {
 	struct rpcrdma_ep	rx_ep;
 	struct rpcrdma_buffer	rx_buf;
 	struct delayed_work	rx_connect_worker;
+	struct rpc_timeout	rx_timeout;
 	struct rpcrdma_stats	rx_stats;
 };
 

commit 5828cebad1c8d535f3c194439e394e92a2273fb2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:36 2019 -0400

    xprtrdma: Remove rpcrdma_req::rl_buffer
    
    Clean up.
    
    There is only one remaining function, rpcrdma_buffer_put(), that
    uses this field. Its caller can supply a pointer to the correct
    rpcrdma_buffer, enabling the removal of an 8-byte pointer field
    from a frequently-allocated shared data structure.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5475f0dff22a..117e32816e4f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -320,7 +320,6 @@ struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
 	struct rpc_rqst		rl_slot;
-	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;
 	struct xdr_stream	rl_stream;
 	struct xdr_buf		rl_hdrbuf;
@@ -499,7 +498,8 @@ rpcrdma_mr_recycle(struct rpcrdma_mr *mr)
 }
 
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
-void rpcrdma_buffer_put(struct rpcrdma_req *);
+void rpcrdma_buffer_put(struct rpcrdma_buffer *buffers,
+			struct rpcrdma_req *req);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
 bool rpcrdma_regbuf_realloc(struct rpcrdma_regbuf *rb, size_t size,

commit 0ab115237025f5e379620bbcd56a02697d07b002
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:15 2019 -0400

    xprtrdma: Wake RPCs directly in rpcrdma_wc_send path
    
    Eliminate a context switch in the path that handles RPC wake-ups
    when a Receive completion has to wait for a Send completion.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e465221c9c96..5475f0dff22a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -44,7 +44,8 @@
 
 #include <linux/wait.h> 		/* wait_queue_head_t, etc */
 #include <linux/spinlock.h> 		/* spinlock_t, etc */
-#include <linux/atomic.h>			/* atomic_t, etc */
+#include <linux/atomic.h>		/* atomic_t, etc */
+#include <linux/kref.h>			/* struct kref */
 #include <linux/workqueue.h>		/* struct work_struct */
 
 #include <rdma/rdma_cm.h>		/* RDMA connection api */
@@ -329,17 +330,12 @@ struct rpcrdma_req {
 	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
 
 	struct list_head	rl_all;
-	unsigned long		rl_flags;
+	struct kref		rl_kref;
 
 	struct list_head	rl_registered;	/* registered segments */
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 };
 
-/* rl_flags */
-enum {
-	RPCRDMA_REQ_F_TX_RESOURCES,
-};
-
 static inline struct rpcrdma_req *
 rpcr_to_rdmar(const struct rpc_rqst *rqst)
 {
@@ -584,8 +580,6 @@ int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);
 void rpcrdma_reply_handler(struct rpcrdma_rep *rep);
-void rpcrdma_release_rqst(struct rpcrdma_xprt *r_xprt,
-			  struct rpcrdma_req *req);
 
 static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 {

commit d8099feda4833bab96b1bf312e9e6aad6b771570
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:10 2019 -0400

    xprtrdma: Reduce context switching due to Local Invalidation
    
    Since commit ba69cd122ece ("xprtrdma: Remove support for FMR memory
    registration"), FRWR is the only supported memory registration mode.
    
    We can take advantage of the asynchronous nature of FRWR's LOCAL_INV
    Work Requests to get rid of the completion wait by having the
    LOCAL_INV completion handler take care of DMA unmapping MRs and
    waking the upper layer RPC waiter.
    
    This eliminates two context switches when local invalidation is
    necessary. As a side benefit, we will no longer need the per-xprt
    deferred completion work queue.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a39652884308..e465221c9c96 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -202,10 +202,9 @@ struct rpcrdma_rep {
 	bool			rr_temp;
 	struct rpcrdma_regbuf	*rr_rdmabuf;
 	struct rpcrdma_xprt	*rr_rxprt;
-	struct work_struct	rr_work;
+	struct rpc_rqst		*rr_rqst;
 	struct xdr_buf		rr_hdrbuf;
 	struct xdr_stream	rr_stream;
-	struct rpc_rqst		*rr_rqst;
 	struct list_head	rr_list;
 	struct ib_recv_wr	rr_recv_wr;
 };
@@ -240,10 +239,12 @@ struct rpcrdma_sendctx {
  * An external memory region is any buffer or page that is registered
  * on the fly (ie, not pre-registered).
  */
+struct rpcrdma_req;
 struct rpcrdma_frwr {
 	struct ib_mr			*fr_mr;
 	struct ib_cqe			fr_cqe;
 	struct completion		fr_linv_done;
+	struct rpcrdma_req		*fr_req;
 	union {
 		struct ib_reg_wr	fr_regwr;
 		struct ib_send_wr	fr_invwr;
@@ -388,7 +389,6 @@ struct rpcrdma_buffer {
 	u32			rb_bc_srv_max_requests;
 	u32			rb_bc_max_requests;
 
-	struct workqueue_struct *rb_completion_wq;
 	struct delayed_work	rb_refresh_worker;
 };
 
@@ -561,6 +561,7 @@ struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
 void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);
 void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
+void frwr_unmap_async(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
@@ -585,7 +586,6 @@ void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);
 void rpcrdma_reply_handler(struct rpcrdma_rep *rep);
 void rpcrdma_release_rqst(struct rpcrdma_xprt *r_xprt,
 			  struct rpcrdma_req *req);
-void rpcrdma_deferred_completion(struct work_struct *work);
 
 static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 {

commit 40088f0e9b62d7fa033918b54ef45f8bf7d1ad1c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:04 2019 -0400

    xprtrdma: Add mechanism to place MRs back on the free list
    
    When a marshal operation fails, any MRs that were already set up for
    that request are recycled. Recycling releases MRs and creates new
    ones, which is expensive.
    
    Since commit f2877623082b ("xprtrdma: Chain Send to FastReg WRs")
    was merged, recycling FRWRs is unnecessary. This is because before
    that commit, frwr_map had already posted FAST_REG Work Requests,
    so ownership of the MRs had already been passed to the NIC and thus
    dealing with them had to be delayed until they completed.
    
    Since that commit, however, FAST_REG WRs are posted at the same time
    as the Send WR. This means that if marshaling fails, we are certain
    the MRs are safe to simply unmap and place back on the free list
    because neither the Send nor the FAST_REG WRs have been posted yet.
    The kernel still has ownership of the MRs at this point.
    
    This reduces the total number of MRs that the xprt has to create
    under heavy workloads and makes the marshaling logic less brittle.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a9de116a5c1a..a39652884308 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -549,6 +549,7 @@ rpcrdma_data_dir(bool writing)
 /* Memory registration calls xprtrdma/frwr_ops.c
  */
 bool frwr_is_supported(struct ib_device *device);
+void frwr_reset(struct rpcrdma_req *req);
 int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep);
 int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);
 void frwr_release_mr(struct rpcrdma_mr *mr);

commit 847568942f93e0af77e4bb8a098899f310cb3a88
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:32:59 2019 -0400

    xprtrdma: Remove fr_state
    
    Now that both the Send and Receive completions are handled in
    process context, it is safe to DMA unmap and return MRs to the
    free or recycle lists directly in the completion handlers.
    
    Doing this means rpcrdma_frwr no longer needs to track the state of
    each MR, meaning that a VALID or FLUSHED MR can no longer appear on
    an xprt's MR free list. Thus there is no longer a need to track the
    MR's registration state in rpcrdma_frwr.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3c39aa3c113c..a9de116a5c1a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -240,17 +240,9 @@ struct rpcrdma_sendctx {
  * An external memory region is any buffer or page that is registered
  * on the fly (ie, not pre-registered).
  */
-enum rpcrdma_frwr_state {
-	FRWR_IS_INVALID,	/* ready to be used */
-	FRWR_IS_VALID,		/* in use */
-	FRWR_FLUSHED_FR,	/* flushed FASTREG WR */
-	FRWR_FLUSHED_LI,	/* flushed LOCALINV WR */
-};
-
 struct rpcrdma_frwr {
 	struct ib_mr			*fr_mr;
 	struct ib_cqe			fr_cqe;
-	enum rpcrdma_frwr_state		fr_state;
 	struct completion		fr_linv_done;
 	union {
 		struct ib_reg_wr	fr_regwr;
@@ -567,8 +559,7 @@ struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 				struct rpcrdma_mr **mr);
 int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
 void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);
-void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt,
-		     struct list_head *mrs);
+void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req);
 
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c

commit 5809ea4f7c39bf38e3f85ec185b776da9d81717c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:32:54 2019 -0400

    xprtrdma: Remove the RPCRDMA_REQ_F_PENDING flag
    
    Commit 9590d083c1bb ("xprtrdma: Use xprt_pin_rqst in
    rpcrdma_reply_handler") pins incoming RPC/RDMA replies so they
    can be left in the pending requests queue while they are being
    processed without introducing a race between ->buf_free and the
    transport's reply handler. Therefore RPCRDMA_REQ_F_PENDING is no
    longer necessary.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2c6c5d8c3de1..3c39aa3c113c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -344,7 +344,6 @@ struct rpcrdma_req {
 
 /* rl_flags */
 enum {
-	RPCRDMA_REQ_F_PENDING = 0,
 	RPCRDMA_REQ_F_TX_RESOURCES,
 };
 

commit 05eb06d86685e7d9dac60e6bbb46d7f4c30b056e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:32:48 2019 -0400

    xprtrdma: Fix occasional transport deadlock
    
    Under high I/O workloads, I've noticed that an RPC/RDMA transport
    occasionally deadlocks (IOPS goes to zero, and doesn't recover).
    Diagnosis shows that the sendctx queue is empty, but when sendctxs
    are returned to the queue, the xprt_write_space wake-up never
    occurs. The wake-up logic in rpcrdma_sendctx_put_locked is racy.
    
    I noticed that both EMPTY_SCQ and XPRT_WRITE_SPACE are implemented
    via an atomic bit. Just one of those is sufficient. Removing
    EMPTY_SCQ in favor of the generic bit mechanism makes the deadlock
    un-reproducible.
    
    Without EMPTY_SCQ, rpcrdma_buffer::rb_flags is no longer used and
    is therefore removed.
    
    Unfortunately this patch does not apply cleanly to stable. If
    needed, someone will have to port it and test it.
    
    Fixes: 2fad659209d5 ("xprtrdma: Wait on empty sendctx queue")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d1e0749bcbc4..2c6c5d8c3de1 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -391,7 +391,6 @@ struct rpcrdma_buffer {
 	struct list_head	rb_recv_bufs;
 	struct list_head	rb_allreqs;
 
-	unsigned long		rb_flags;
 	u32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */
 
@@ -402,11 +401,6 @@ struct rpcrdma_buffer {
 	struct delayed_work	rb_refresh_worker;
 };
 
-/* rb_flags */
-enum {
-	RPCRDMA_BUF_F_EMPTY_SCQ = 0,
-};
-
 /*
  * Statistics for RPCRDMA
  */

commit 2cfd11f16f01c0ee8f83bb07027c9d2f43565473
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:41 2019 -0400

    xprtrdma: Remove stale comment
    
    The comment hasn't been accurate for several years.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9e98ee0cd937..d1e0749bcbc4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -239,13 +239,6 @@ struct rpcrdma_sendctx {
  *
  * An external memory region is any buffer or page that is registered
  * on the fly (ie, not pre-registered).
- *
- * Each rpcrdma_buffer has a list of free MWs anchored in rb_mrs. During
- * call_allocate, rpcrdma_buffer_get() assigns one to each segment in
- * an rpcrdma_req. Then rpcrdma_register_external() grabs these to keep
- * track of registration metadata while each RPC is pending.
- * rpcrdma_deregister_external() uses this metadata to unmap and
- * release these resources when an RPC is complete.
  */
 enum rpcrdma_frwr_state {
 	FRWR_IS_INVALID,	/* ready to be used */

commit 86c4ccd9b92ba6541fc4734e82f87139deea0470
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:25 2019 -0400

    xprtrdma: Eliminate struct rpcrdma_create_data_internal
    
    Clean up.
    
    Move the remaining field in rpcrdma_create_data_internal so the
    structure can be removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index edf602afb08a..9e98ee0cd937 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -97,6 +97,7 @@ struct rpcrdma_ep {
 	wait_queue_head_t 	rep_connect_wait;
 	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
+	unsigned int		rep_max_requests;	/* set by /proc */
 	unsigned int		rep_inline_send;	/* negotiated */
 	unsigned int		rep_inline_recv;	/* negotiated */
 	int			rep_receive_count;
@@ -413,16 +414,6 @@ enum {
 	RPCRDMA_BUF_F_EMPTY_SCQ = 0,
 };
 
-/*
- * Internal structure for transport instance creation. This
- * exists primarily for modularity.
- *
- * This data should be set with mount options
- */
-struct rpcrdma_create_data_internal {
-	unsigned int	max_requests;	/* max requests (slots) in flight */
-};
-
 /*
  * Statistics for RPCRDMA
  */
@@ -467,13 +458,11 @@ struct rpcrdma_xprt {
 	struct rpcrdma_ia	rx_ia;
 	struct rpcrdma_ep	rx_ep;
 	struct rpcrdma_buffer	rx_buf;
-	struct rpcrdma_create_data_internal rx_data;
 	struct delayed_work	rx_connect_worker;
 	struct rpcrdma_stats	rx_stats;
 };
 
 #define rpcx_to_rdmax(x) container_of(x, struct rpcrdma_xprt, rx_xprt)
-#define rpcx_to_rdmad(x) (rpcx_to_rdmax(x)->rx_data)
 
 static inline const char *
 rpcrdma_addrstr(const struct rpcrdma_xprt *r_xprt)
@@ -507,9 +496,8 @@ void rpcrdma_ia_close(struct rpcrdma_ia *);
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */
-int rpcrdma_ep_create(struct rpcrdma_ep *, struct rpcrdma_ia *,
-				struct rpcrdma_create_data_internal *);
-void rpcrdma_ep_destroy(struct rpcrdma_ep *, struct rpcrdma_ia *);
+int rpcrdma_ep_create(struct rpcrdma_xprt *r_xprt);
+void rpcrdma_ep_destroy(struct rpcrdma_xprt *r_xprt);
 int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
@@ -583,8 +571,7 @@ rpcrdma_data_dir(bool writing)
 /* Memory registration calls xprtrdma/frwr_ops.c
  */
 bool frwr_is_supported(struct ib_device *device);
-int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep,
-	      struct rpcrdma_create_data_internal *cdata);
+int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep);
 int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);
 void frwr_release_mr(struct rpcrdma_mr *mr);
 size_t frwr_maxpages(struct rpcrdma_xprt *r_xprt);
@@ -630,6 +617,7 @@ static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 
 /* RPC/RDMA module init - xprtrdma/transport.c
  */
+extern unsigned int xprt_rdma_slot_table_entries;
 extern unsigned int xprt_rdma_max_inline_read;
 extern unsigned int xprt_rdma_max_inline_write;
 void xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap);

commit 94087e978e9b645e07cc0fbdcf4140dda02f3d81
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:20 2019 -0400

    xprtrdma: Aggregate the inline settings in struct rpcrdma_ep
    
    Clean up.
    
    The inline settings are actually a characteristic of the endpoint,
    and not related to the device. They are also modified after the
    transport instance is created, so they do not belong in the cdata
    structure either.
    
    Lastly, let's use names that are more natural to RDMA than to NFS:
    inline_write -> inline_send and inline_read -> inline_recv. The
    /proc files retain their names to avoid breaking user space.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d34371d0d0f8..edf602afb08a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -71,8 +71,6 @@ struct rpcrdma_ia {
 	int			ri_async_rc;
 	unsigned int		ri_max_segs;
 	unsigned int		ri_max_frwr_depth;
-	unsigned int		ri_max_inline_write;
-	unsigned int		ri_max_inline_read;
 	unsigned int		ri_max_send_sges;
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
@@ -92,11 +90,15 @@ enum {
 struct rpcrdma_ep {
 	unsigned int		rep_send_count;
 	unsigned int		rep_send_batch;
+	unsigned int		rep_max_inline_send;
+	unsigned int		rep_max_inline_recv;
 	int			rep_connected;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
 	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
+	unsigned int		rep_inline_send;	/* negotiated */
+	unsigned int		rep_inline_recv;	/* negotiated */
 	int			rep_receive_count;
 };
 
@@ -419,8 +421,6 @@ enum {
  */
 struct rpcrdma_create_data_internal {
 	unsigned int	max_requests;	/* max requests (slots) in flight */
-	unsigned int	inline_rsize;	/* max non-rdma read data payload */
-	unsigned int	inline_wsize;	/* max non-rdma write data payload */
 };
 
 /*
@@ -631,6 +631,7 @@ static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 /* RPC/RDMA module init - xprtrdma/transport.c
  */
 extern unsigned int xprt_rdma_max_inline_read;
+extern unsigned int xprt_rdma_max_inline_write;
 void xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap);
 void xprt_rdma_free_addresses(struct rpc_xprt *xprt);
 void xprt_rdma_close(struct rpc_xprt *xprt);

commit fd5951742dbc8c3695151e3f46b2fe2c4dac3559
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:15 2019 -0400

    xprtrdma: Remove rpcrdma_create_data_internal::rsize and wsize
    
    Clean up.
    
    xprt_rdma_max_inline_{read,write} cannot be set to large values
    by virtue of proc_dointvec_minmax. The current maximum is
    RPCRDMA_MAX_INLINE, which is much smaller than RPCRDMA_MAX_SEGS *
    PAGE_SIZE.
    
    The .rsize and .wsize fields are otherwise unused in the current
    code base, and thus can be removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 40912bb34b64..d34371d0d0f8 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -419,8 +419,6 @@ enum {
  */
 struct rpcrdma_create_data_internal {
 	unsigned int	max_requests;	/* max requests (slots) in flight */
-	unsigned int	rsize;		/* mount rsize - max read hdr+data */
-	unsigned int	wsize;		/* mount wsize - max write hdr+data */
 	unsigned int	inline_rsize;	/* max non-rdma read data payload */
 	unsigned int	inline_wsize;	/* max non-rdma write data payload */
 };

commit f19bd0bbd363fb97756ed83f53f48413d3e601aa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:04 2019 -0400

    xprtrdma: Eliminate rpcrdma_ia::ri_device
    
    Clean up.
    
    Since commit 54cbd6b0c6b9 ("xprtrdma: Delay DMA mapping Send and
    Receive buffers"), a pointer to the device is now saved in each
    regbuf when it is DMA mapped.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f8563937c8c6..40912bb34b64 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -66,11 +66,8 @@
  * Interface Adapter -- one per transport instance
  */
 struct rpcrdma_ia {
-	struct ib_device	*ri_device;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
-	struct completion	ri_done;
-	struct completion	ri_remove_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_segs;
 	unsigned int		ri_max_frwr_depth;
@@ -80,6 +77,8 @@ struct rpcrdma_ia {
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
 	unsigned long		ri_flags;
+	struct completion	ri_done;
+	struct completion	ri_remove_done;
 };
 
 enum {
@@ -585,7 +584,7 @@ rpcrdma_data_dir(bool writing)
 
 /* Memory registration calls xprtrdma/frwr_ops.c
  */
-bool frwr_is_supported(struct rpcrdma_ia *);
+bool frwr_is_supported(struct ib_device *device);
 int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep,
 	      struct rpcrdma_create_data_internal *cdata);
 int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);

commit c209e49ceac0ff479f79ac5cd2fbf8be80621203
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:59 2019 -0400

    xprtrdma: More Send completion batching
    
    Instead of using a fixed number, allow the amount of Send completion
    batching to vary based on the client's maximum credit limit.
    
    - A larger default gives a small boost to IOPS throughput
    
    - Reducing it based on max_requests gives a safe result when the
      max credit limit is cranked down (eg. when the device has a small
      max_qp_wr).
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 8afb5fc1de05..f8563937c8c6 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -232,16 +232,6 @@ struct rpcrdma_sendctx {
 	struct ib_sge		sc_sges[];
 };
 
-/* Limit the number of SGEs that can be unmapped during one
- * Send completion. This caps the amount of work a single
- * completion can do before returning to the provider.
- *
- * Setting this to zero disables Send completion batching.
- */
-enum {
-	RPCRDMA_MAX_SEND_BATCH = 7,
-};
-
 /*
  * struct rpcrdma_mr - external memory region metadata
  *

commit dbcc53a52df256880c2ffa4ee45661419435998a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:53 2019 -0400

    xprtrdma: Clean up sendctx functions
    
    Minor clean-ups I've stumbled on since sendctx was merged last year.
    In particular, making Send completion processing more efficient
    appears to have a measurable impact on IOPS throughput.
    
    Note: test_and_clear_bit() returns a value, thus an explicit memory
    barrier is not necessary.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 7621e2d0f107..8afb5fc1de05 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -225,6 +225,7 @@ struct rpcrdma_xprt;
 struct rpcrdma_sendctx {
 	struct ib_send_wr	sc_wr;
 	struct ib_cqe		sc_cqe;
+	struct ib_device	*sc_device;
 	struct rpcrdma_xprt	*sc_xprt;
 	struct rpcrdma_req	*sc_req;
 	unsigned int		sc_unmap_count;
@@ -536,7 +537,7 @@ struct rpcrdma_req *rpcrdma_req_create(struct rpcrdma_xprt *r_xprt, size_t size,
 void rpcrdma_req_destroy(struct rpcrdma_req *req);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
-struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);
+struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_xprt *r_xprt);
 
 struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_mr_put(struct rpcrdma_mr *mr);
@@ -625,7 +626,7 @@ int rpcrdma_prepare_send_sges(struct rpcrdma_xprt *r_xprt,
 			      struct rpcrdma_req *req, u32 hdrlen,
 			      struct xdr_buf *xdr,
 			      enum rpcrdma_chunktype rtype);
-void rpcrdma_unmap_sendctx(struct rpcrdma_sendctx *sc);
+void rpcrdma_sendctx_unmap(struct rpcrdma_sendctx *sc);
 int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);

commit 4ba02e8d0ea5461d0e55e76c91481f1153f63365
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:42 2019 -0400

    xprtrdma: Increase maximum number of backchannel requests
    
    Reflects the change introduced in commit 067c46967160 ("NFSv4.1:
    Bump the default callback session slot count to 16").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b942c9d322a7..7621e2d0f107 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -103,12 +103,14 @@ struct rpcrdma_ep {
 
 /* Pre-allocate extra Work Requests for handling backward receives
  * and sends. This is a fixed value because the Work Queues are
- * allocated when the forward channel is set up.
+ * allocated when the forward channel is set up, long before the
+ * backchannel is provisioned. This value is two times
+ * NFS4_DEF_CB_SLOT_TABLE_SIZE.
  */
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
-#define RPCRDMA_BACKWARD_WRS		(8)
+#define RPCRDMA_BACKWARD_WRS (32)
 #else
-#define RPCRDMA_BACKWARD_WRS		(0)
+#define RPCRDMA_BACKWARD_WRS (0)
 #endif
 
 /* Registered buffer -- registered kmalloc'd memory for RDMA SEND/RECV

commit d2832af38dfd1d3b135b13c6106b2c5de16a6747
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:32 2019 -0400

    xprtrdma: Clean up regbuf helpers
    
    For code legibility, clean up the function names to be consistent
    with the pattern: "rpcrdma" _ object-type _ action
    
    Also rpcrdma_regbuf_alloc and rpcrdma_regbuf_free no longer have any
    callers outside of verbs.c, and can thus be made static.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 751d4761d682..b942c9d322a7 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -550,25 +550,34 @@ struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
 void rpcrdma_buffer_put(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
-struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(size_t, enum dma_data_direction,
-					    gfp_t);
 bool rpcrdma_regbuf_realloc(struct rpcrdma_regbuf *rb, size_t size,
 			    gfp_t flags);
-bool __rpcrdma_dma_map_regbuf(struct rpcrdma_ia *, struct rpcrdma_regbuf *);
-void rpcrdma_free_regbuf(struct rpcrdma_regbuf *);
+bool __rpcrdma_regbuf_dma_map(struct rpcrdma_xprt *r_xprt,
+			      struct rpcrdma_regbuf *rb);
 
-static inline bool
-rpcrdma_regbuf_is_mapped(struct rpcrdma_regbuf *rb)
+/**
+ * rpcrdma_regbuf_is_mapped - check if buffer is DMA mapped
+ *
+ * Returns true if the buffer is now mapped to rb->rg_device.
+ */
+static inline bool rpcrdma_regbuf_is_mapped(struct rpcrdma_regbuf *rb)
 {
 	return rb->rg_device != NULL;
 }
 
-static inline bool
-rpcrdma_dma_map_regbuf(struct rpcrdma_ia *ia, struct rpcrdma_regbuf *rb)
+/**
+ * rpcrdma_regbuf_dma_map - DMA-map a regbuf
+ * @r_xprt: controlling transport instance
+ * @rb: regbuf to be mapped
+ *
+ * Returns true if the buffer is currently DMA mapped.
+ */
+static inline bool rpcrdma_regbuf_dma_map(struct rpcrdma_xprt *r_xprt,
+					  struct rpcrdma_regbuf *rb)
 {
 	if (likely(rpcrdma_regbuf_is_mapped(rb)))
 		return true;
-	return __rpcrdma_dma_map_regbuf(ia, rb);
+	return __rpcrdma_regbuf_dma_map(r_xprt, rb);
 }
 
 /*

commit 0f665ceb71a20520bdce76fb63ad68c21841aa62
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:27 2019 -0400

    xprtrdma: De-duplicate "allocate new, free old regbuf"
    
    Clean up by providing an API to do this common task.
    
    At this point, the difference between rpcrdma_get_sendbuf and
    rpcrdma_get_recvbuf has become tiny. These can be collapsed into a
    single helper.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 03d5ce443bf0..751d4761d682 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -552,6 +552,8 @@ void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
 struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(size_t, enum dma_data_direction,
 					    gfp_t);
+bool rpcrdma_regbuf_realloc(struct rpcrdma_regbuf *rb, size_t size,
+			    gfp_t flags);
 bool __rpcrdma_dma_map_regbuf(struct rpcrdma_ia *, struct rpcrdma_regbuf *);
 void rpcrdma_free_regbuf(struct rpcrdma_regbuf *);
 

commit bb93a1ae2bf4f6eb3cedf05a2ea4a2e6a80712e6
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:21 2019 -0400

    xprtrdma: Allocate req's regbufs at xprt create time
    
    Allocating an rpcrdma_req's regbufs at xprt create time enables
    a pair of micro-optimizations:
    
    First, if these regbufs are always there, we can eliminate two
    conditional branches from the hot xprt_rdma_allocate path.
    
    Second, by allocating a 1KB buffer, it places a lower bound on the
    size of these buffers, without adding yet another conditional
    branch. The lower bound reduces the number of hardway re-
    allocations. In fact, for some workloads it completely eliminates
    hardway allocations.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1af9674572bd..03d5ce443bf0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -529,7 +529,7 @@ int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 /*
  * Buffer calls - xprtrdma/verbs.c
  */
-struct rpcrdma_req *rpcrdma_req_create(struct rpcrdma_xprt *r_xprt,
+struct rpcrdma_req *rpcrdma_req_create(struct rpcrdma_xprt *r_xprt, size_t size,
 				       gfp_t flags);
 void rpcrdma_req_destroy(struct rpcrdma_req *req);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);

commit 8cec3dba76a4d9d7da4a7219663b8c4333f14522
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:16 2019 -0400

    xprtrdma: rpcrdma_regbuf alignment
    
    Allocate the struct rpcrdma_regbuf separately from the I/O buffer
    to better guarantee the alignment of the I/O buffer and eliminate
    the wasted space between the rpcrdma_regbuf metadata and the buffer
    itself.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 539558fc9c62..1af9674572bd 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -121,33 +121,34 @@ struct rpcrdma_regbuf {
 	struct ib_sge		rg_iov;
 	struct ib_device	*rg_device;
 	enum dma_data_direction	rg_direction;
-	__be32			rg_base[0] __attribute__ ((aligned(256)));
+	void			*rg_data;
 };
 
-static inline u64
-rdmab_addr(struct rpcrdma_regbuf *rb)
+static inline u64 rdmab_addr(struct rpcrdma_regbuf *rb)
 {
 	return rb->rg_iov.addr;
 }
 
-static inline u32
-rdmab_length(struct rpcrdma_regbuf *rb)
+static inline u32 rdmab_length(struct rpcrdma_regbuf *rb)
 {
 	return rb->rg_iov.length;
 }
 
-static inline u32
-rdmab_lkey(struct rpcrdma_regbuf *rb)
+static inline u32 rdmab_lkey(struct rpcrdma_regbuf *rb)
 {
 	return rb->rg_iov.lkey;
 }
 
-static inline struct ib_device *
-rdmab_device(struct rpcrdma_regbuf *rb)
+static inline struct ib_device *rdmab_device(struct rpcrdma_regbuf *rb)
 {
 	return rb->rg_device;
 }
 
+static inline void *rdmab_data(const struct rpcrdma_regbuf *rb)
+{
+	return rb->rg_data;
+}
+
 #define RPCRDMA_DEF_GFP		(GFP_NOIO | __GFP_NOWARN)
 
 /* To ensure a transport can always make forward progress,

commit 1769e6a816dff50d960271eb780e0a40b739b256
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:05 2019 -0400

    xprtrdma: Clean up rpcrdma_create_req()
    
    Eventually, I'd like to invoke rpcrdma_create_req() during the
    call_reserve step. Memory allocation there probably needs to use
    GFP_NOIO. Therefore a set of GFP flags needs to be passed in.
    
    As an additional clean up, just return a pointer or NULL, because
    the only error return code here is -ENOMEM.
    
    Lastly, clean up the function names to be consistent with the
    pattern: "rpcrdma" _ object-type _ action
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 10f6593e1a6a..539558fc9c62 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -528,7 +528,8 @@ int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 /*
  * Buffer calls - xprtrdma/verbs.c
  */
-struct rpcrdma_req *rpcrdma_create_req(struct rpcrdma_xprt *);
+struct rpcrdma_req *rpcrdma_req_create(struct rpcrdma_xprt *r_xprt,
+				       gfp_t flags);
 void rpcrdma_req_destroy(struct rpcrdma_req *req);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);

commit e340c2d6ef2a8cdcc11672f8cc839c30ad360e52
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Feb 11 11:23:54 2019 -0500

    xprtrdma: Reduce the doorbell rate (Receive)
    
    Post RECV WRs in batches to reduce the hardware doorbell rate per
    transport. This helps the RPC-over-RDMA client scale better in
    number of transports.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 33db208fa9a8..10f6593e1a6a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -205,6 +205,16 @@ struct rpcrdma_rep {
 	struct ib_recv_wr	rr_recv_wr;
 };
 
+/* To reduce the rate at which a transport invokes ib_post_recv
+ * (and thus the hardware doorbell rate), xprtrdma posts Receive
+ * WRs in batches.
+ *
+ * Setting this to zero disables Receive post batching.
+ */
+enum {
+	RPCRDMA_MAX_RECV_BATCH = 7,
+};
+
 /* struct rpcrdma_sendctx - DMA mapped SGEs to unmap after Send completes
  */
 struct rpcrdma_req;

commit ec482cc1c1180c65da582e17f4e79d204bb338e0
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Feb 11 11:23:44 2019 -0500

    xprtrdma: Fix sparse warnings
    
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:375:63: warning: incorrect type in argument 5 (different base types)
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:375:63:    expected unsigned int [usertype] xid
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:375:63:    got restricted __be32 [usertype] rq_xid
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:432:62: warning: incorrect type in argument 5 (different base types)
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:432:62:    expected unsigned int [usertype] xid
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:432:62:    got restricted __be32 [usertype] rq_xid
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:489:62: warning: incorrect type in argument 5 (different base types)
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:489:62:    expected unsigned int [usertype] xid
    linux/net/sunrpc/xprtrdma/rpc_rdma.c:489:62:    got restricted __be32 [usertype] rq_xid
    
    Fixes: 0a93fbcb16e6 ("xprtrdma: Plant XID in on-the-wire RDMA ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5a18472f2c9c..33db208fa9a8 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -577,7 +577,7 @@ void frwr_release_mr(struct rpcrdma_mr *mr);
 size_t frwr_maxpages(struct rpcrdma_xprt *r_xprt);
 struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 				struct rpcrdma_mr_seg *seg,
-				int nsegs, bool writing, u32 xid,
+				int nsegs, bool writing, __be32 xid,
 				struct rpcrdma_mr **mr);
 int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
 void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);

commit e6b92572808467f35fd159d47c45b650de29e722
Merge: e45428a43676 260f71eff493
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 2 16:35:23 2019 -0800

    Merge tag 'nfs-for-4.21-1' of git://git.linux-nfs.org/projects/anna/linux-nfs
    
    Pull NFS client updates from Anna Schumaker:
     "Stable bugfixes:
       - xprtrdma: Yet another double DMA-unmap # v4.20
    
      Features:
       - Allow some /proc/sys/sunrpc entries without CONFIG_SUNRPC_DEBUG
       - Per-xprt rdma receive workqueues
       - Drop support for FMR memory registration
       - Make port= mount option optional for RDMA mounts
    
      Other bugfixes and cleanups:
       - Remove unused nfs4_xdev_fs_type declaration
       - Fix comments for behavior that has changed
       - Remove generic RPC credentials by switching to 'struct cred'
       - Fix crossing mountpoints with different auth flavors
       - Various xprtrdma fixes from testing and auditing the close code
       - Fixes for disconnect issues when using xprtrdma with krb5
       - Clean up and improve xprtrdma trace points
       - Fix NFS v4.2 async copy reboot recovery"
    
    * tag 'nfs-for-4.21-1' of git://git.linux-nfs.org/projects/anna/linux-nfs: (63 commits)
      sunrpc: convert to DEFINE_SHOW_ATTRIBUTE
      sunrpc: Add xprt after nfs4_test_session_trunk()
      sunrpc: convert unnecessary GFP_ATOMIC to GFP_NOFS
      sunrpc: handle ENOMEM in rpcb_getport_async
      NFS: remove unnecessary test for IS_ERR(cred)
      xprtrdma: Prevent leak of rpcrdma_rep objects
      NFSv4.2 fix async copy reboot recovery
      xprtrdma: Don't leak freed MRs
      xprtrdma: Add documenting comment for rpcrdma_buffer_destroy
      xprtrdma: Replace outdated comment for rpcrdma_ep_post
      xprtrdma: Update comments in frwr_op_send
      SUNRPC: Fix some kernel doc complaints
      SUNRPC: Simplify defining common RPC trace events
      NFS: Fix NFSv4 symbolic trace point output
      xprtrdma: Trace mapping, alloc, and dereg failures
      xprtrdma: Add trace points for calls to transport switch methods
      xprtrdma: Relocate the xprtrdma_mr_map trace points
      xprtrdma: Clean up of xprtrdma chunk trace points
      xprtrdma: Remove unused fields from rpcrdma_ia
      xprtrdma: Cull dprintk() call sites
      ...

commit 9bef848f44b4316fbe12e364eea527bd59fa1ed3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:59:44 2018 -0500

    xprtrdma: Remove unused fields from rpcrdma_ia
    
    Clean up. The last use of these fields was in commit 173b8f49b3af
    ("xprtrdma: Demote "connect" log messages") .
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a1cdc85898c7..90422a66b806 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -80,8 +80,6 @@ struct rpcrdma_ia {
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
 	unsigned long		ri_flags;
-	struct ib_qp_attr	ri_qp_attr;
-	struct ib_qp_init_attr	ri_qp_init_attr;
 };
 
 enum {

commit 92f4433e567a034d87e1e2c9e5402ff5f58b545b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:59:33 2018 -0500

    xprtrdma: Simplify locking that protects the rl_allreqs list
    
    Clean up: There's little chance of contention between the use of
    rb_lock and rb_reqslock, so merge the two. This avoids having to
    take both in some (possibly future) cases.
    
    Transport tear-down is already serialized, thus there is no need for
    locking at all when destroying rpcrdma_reqs.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ff4eab1c3bf1..a1cdc85898c7 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -392,14 +392,13 @@ struct rpcrdma_buffer {
 	spinlock_t		rb_lock;	/* protect buf lists */
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
+	struct list_head	rb_allreqs;
+
 	unsigned long		rb_flags;
 	u32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */
 
 	u32			rb_bc_srv_max_requests;
-	spinlock_t		rb_reqslock;	/* protect rb_allreqs */
-	struct list_head	rb_allreqs;
-
 	u32			rb_bc_max_requests;
 
 	struct workqueue_struct *rb_completion_wq;
@@ -522,7 +521,7 @@ int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
  * Buffer calls - xprtrdma/verbs.c
  */
 struct rpcrdma_req *rpcrdma_create_req(struct rpcrdma_xprt *);
-void rpcrdma_destroy_req(struct rpcrdma_req *);
+void rpcrdma_req_destroy(struct rpcrdma_req *req);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);

commit 0a93fbcb16e6b1f36780f9a20d6427f26cec761d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:59:07 2018 -0500

    xprtrdma: Plant XID in on-the-wire RDMA offset (FRWR)
    
    Place the associated RPC transaction's XID in the upper 32 bits of
    each RDMA segment's rdma_offset field. There are two reasons to do
    this:
    
    - The R_key only has 8 bits that are different from registration to
      registration. The XID adds more uniqueness to each RDMA segment to
      reduce the likelihood of a software bug on the server reading from
      or writing into memory it's not supposed to.
    
    - On-the-wire RDMA Read and Write requests do not otherwise carry
      any identifier that matches them up to an RPC. The XID in the
      upper 32 bits will act as an eye-catcher in network captures.
    
    Suggested-by: Tom Talpey <ttalpey@microsoft.com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c42a0036a0bd..ff4eab1c3bf1 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -580,7 +580,7 @@ void frwr_release_mr(struct rpcrdma_mr *mr);
 size_t frwr_maxpages(struct rpcrdma_xprt *r_xprt);
 struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
 				struct rpcrdma_mr_seg *seg,
-				int nsegs, bool writing,
+				int nsegs, bool writing, u32 xid,
 				struct rpcrdma_mr **mr);
 int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
 void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);

commit 5f62412be3ff738c9575b28c1f4a9b010ac22316
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:59:01 2018 -0500

    xprtrdma: Remove rpcrdma_memreg_ops
    
    Clean up: Now that there is only FRWR, there is no need for a memory
    registration switch. The indirect calls to the memreg operations can
    be replaced with faster direct calls.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 84f7bbecdd86..c42a0036a0bd 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -66,7 +66,6 @@
  * Interface Adapter -- one per transport instance
  */
 struct rpcrdma_ia {
-	const struct rpcrdma_memreg_ops	*ri_ops;
 	struct ib_device	*ri_device;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
@@ -406,7 +405,6 @@ struct rpcrdma_buffer {
 	struct workqueue_struct *rb_completion_wq;
 	struct delayed_work	rb_refresh_worker;
 };
-#define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 
 /* rb_flags */
 enum {
@@ -456,34 +454,6 @@ struct rpcrdma_stats {
 	unsigned long		bcall_count;
 };
 
-/*
- * Per-registration mode operations
- */
-struct rpcrdma_xprt;
-struct rpcrdma_memreg_ops {
-	struct rpcrdma_mr_seg *
-			(*ro_map)(struct rpcrdma_xprt *,
-				  struct rpcrdma_mr_seg *, int, bool,
-				  struct rpcrdma_mr **);
-	int		(*ro_send)(struct rpcrdma_ia *ia,
-				   struct rpcrdma_req *req);
-	void		(*ro_reminv)(struct rpcrdma_rep *rep,
-				     struct list_head *mrs);
-	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
-					 struct list_head *);
-	int		(*ro_open)(struct rpcrdma_ia *,
-				   struct rpcrdma_ep *,
-				   struct rpcrdma_create_data_internal *);
-	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
-	int		(*ro_init_mr)(struct rpcrdma_ia *,
-				      struct rpcrdma_mr *);
-	void		(*ro_release_mr)(struct rpcrdma_mr *mr);
-	const char	*ro_displayname;
-	const int	ro_send_w_inv_ok;
-};
-
-extern const struct rpcrdma_memreg_ops rpcrdma_frwr_memreg_ops;
-
 /*
  * RPCRDMA transport -- encapsulates the structures above for
  * integration with RPC.
@@ -535,7 +505,6 @@ extern unsigned int xprt_rdma_memreg_strategy;
 int rpcrdma_ia_open(struct rpcrdma_xprt *xprt);
 void rpcrdma_ia_remove(struct rpcrdma_ia *ia);
 void rpcrdma_ia_close(struct rpcrdma_ia *);
-bool frwr_is_supported(struct rpcrdma_ia *);
 
 /*
  * Endpoint calls - xprtrdma/verbs.c
@@ -601,6 +570,23 @@ rpcrdma_data_dir(bool writing)
 	return writing ? DMA_FROM_DEVICE : DMA_TO_DEVICE;
 }
 
+/* Memory registration calls xprtrdma/frwr_ops.c
+ */
+bool frwr_is_supported(struct rpcrdma_ia *);
+int frwr_open(struct rpcrdma_ia *ia, struct rpcrdma_ep *ep,
+	      struct rpcrdma_create_data_internal *cdata);
+int frwr_init_mr(struct rpcrdma_ia *ia, struct rpcrdma_mr *mr);
+void frwr_release_mr(struct rpcrdma_mr *mr);
+size_t frwr_maxpages(struct rpcrdma_xprt *r_xprt);
+struct rpcrdma_mr_seg *frwr_map(struct rpcrdma_xprt *r_xprt,
+				struct rpcrdma_mr_seg *seg,
+				int nsegs, bool writing,
+				struct rpcrdma_mr **mr);
+int frwr_send(struct rpcrdma_ia *ia, struct rpcrdma_req *req);
+void frwr_reminv(struct rpcrdma_rep *rep, struct list_head *mrs);
+void frwr_unmap_sync(struct rpcrdma_xprt *r_xprt,
+		     struct list_head *mrs);
+
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */

commit ba69cd122ece618eba47589764c7f9c1f57aed95
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:56 2018 -0500

    xprtrdma: Remove support for FMR memory registration
    
    FMR is not supported on most recent RDMA devices. It is also less
    secure than FRWR because an FMR memory registration can expose
    adjacent bytes to remote reading or writing. As discussed during the
    RDMA BoF at LPC 2018, it is time to remove support for FMR in the
    NFS/RDMA client stack.
    
    Note that NFS/RDMA server-side uses either local memory registration
    or FRWR. FMR is not used.
    
    There are a few Infiniband/RoCE devices in the kernel tree that do
    not appear to support MEM_MGT_EXTENSIONS (FRWR), and therefore will
    not support client-side NFS/RDMA after this patch. These are:
    
     - mthca
     - qib
     - hns (RoCE)
    
    Users of these devices can use NFS/TCP on IPoIB instead.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 99b7f8ea66b0..84f7bbecdd86 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -262,20 +262,12 @@ struct rpcrdma_frwr {
 	};
 };
 
-struct rpcrdma_fmr {
-	struct ib_fmr		*fm_mr;
-	u64			*fm_physaddrs;
-};
-
 struct rpcrdma_mr {
 	struct list_head	mr_list;
 	struct scatterlist	*mr_sg;
 	int			mr_nents;
 	enum dma_data_direction	mr_dir;
-	union {
-		struct rpcrdma_fmr	fmr;
-		struct rpcrdma_frwr	frwr;
-	};
+	struct rpcrdma_frwr	frwr;
 	struct rpcrdma_xprt	*mr_xprt;
 	u32			mr_handle;
 	u32			mr_length;
@@ -490,7 +482,6 @@ struct rpcrdma_memreg_ops {
 	const int	ro_send_w_inv_ok;
 };
 
-extern const struct rpcrdma_memreg_ops rpcrdma_fmr_memreg_ops;
 extern const struct rpcrdma_memreg_ops rpcrdma_frwr_memreg_ops;
 
 /*
@@ -545,7 +536,6 @@ int rpcrdma_ia_open(struct rpcrdma_xprt *xprt);
 void rpcrdma_ia_remove(struct rpcrdma_ia *ia);
 void rpcrdma_ia_close(struct rpcrdma_ia *);
 bool frwr_is_supported(struct rpcrdma_ia *);
-bool fmr_is_supported(struct rpcrdma_ia *);
 
 /*
  * Endpoint calls - xprtrdma/verbs.c

commit 0c0829bcf51aef713806e49b8ea2bac7962f54e2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:40 2018 -0500

    xprtrdma: Don't wake pending tasks until disconnect is done
    
    Transport disconnect processing does a "wake pending tasks" at
    various points.
    
    Suppose an RPC Reply is being processed. The RPC task that Reply
    goes with is waiting on the pending queue. If a disconnect wake-up
    happens before reply processing is done, that reply, even if it is
    good, is thrown away, and the RPC has to be sent again.
    
    This window apparently does not exist for socket transports because
    there is a lock held while a reply is being received which prevents
    the wake-up call until after reply processing is done.
    
    To resolve this, all RPC replies being processed on an RPC-over-RDMA
    transport have to complete before pending tasks are awoken due to a
    transport disconnect.
    
    Callers that already hold the transport write lock may invoke
    ->ops->close directly. Others use a generic helper that schedules
    a close when the write lock can be taken safely.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 7c1b5191a5fe..99b7f8ea66b0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -647,6 +647,7 @@ static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 extern unsigned int xprt_rdma_max_inline_read;
 void xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap);
 void xprt_rdma_free_addresses(struct rpc_xprt *xprt);
+void xprt_rdma_close(struct rpc_xprt *xprt);
 void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq);
 int xprt_rdma_init(void);
 void xprt_rdma_cleanup(void);

commit 3d433ad812baad45fa697f1af45a651147360712
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:35 2018 -0500

    xprtrdma: No qp_event disconnect
    
    After thinking about this more, and auditing other kernel ULP imple-
    mentations, I believe that a DISCONNECT cm_event will occur after a
    fatal QP event. If that's the case, there's no need for an explicit
    disconnect in the QP event handler.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3f198cde41e3..7c1b5191a5fe 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -101,7 +101,6 @@ struct rpcrdma_ep {
 	wait_queue_head_t 	rep_connect_wait;
 	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
-	struct delayed_work	rep_disconnect_worker;
 	int			rep_receive_count;
 };
 

commit 6d2d0ee27c7a12371a0ca51a5db414204901228c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:29 2018 -0500

    xprtrdma: Replace rpcrdma_receive_wq with a per-xprt workqueue
    
    To address a connection-close ordering problem, we need the ability
    to drain the RPC completions running on rpcrdma_receive_wq for just
    one transport. Give each transport its own RPC completion workqueue,
    and drain that workqueue when disconnecting the transport.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 788124cd9258..3f198cde41e3 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -412,6 +412,7 @@ struct rpcrdma_buffer {
 
 	u32			rb_bc_max_requests;
 
+	struct workqueue_struct *rb_completion_wq;
 	struct delayed_work	rb_refresh_worker;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
@@ -547,8 +548,6 @@ void rpcrdma_ia_close(struct rpcrdma_ia *);
 bool frwr_is_supported(struct rpcrdma_ia *);
 bool fmr_is_supported(struct rpcrdma_ia *);
 
-extern struct workqueue_struct *rpcrdma_receive_wq;
-
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */
@@ -603,9 +602,6 @@ rpcrdma_dma_map_regbuf(struct rpcrdma_ia *ia, struct rpcrdma_regbuf *rb)
 	return __rpcrdma_dma_map_regbuf(ia, rb);
 }
 
-int rpcrdma_alloc_wq(void);
-void rpcrdma_destroy_wq(void);
-
 /*
  * Wrappers for chunk registration, shared by read/write chunk code.
  */

commit 6ceea36890a01aa626ce08487eecc5fb43e749b1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:24 2018 -0500

    xprtrdma: Refactor Receive accounting
    
    Clean up: Divide the work cleanly:
    
    - rpcrdma_wc_receive is responsible only for RDMA Receives
    - rpcrdma_reply_handler is responsible only for RPC Replies
    - the posted send and receive counts both belong in rpcrdma_ep
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a13ccb643ce0..788124cd9258 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -102,6 +102,7 @@ struct rpcrdma_ep {
 	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
 	struct delayed_work	rep_disconnect_worker;
+	int			rep_receive_count;
 };
 
 /* Pre-allocate extra Work Requests for handling backward receives
@@ -404,7 +405,6 @@ struct rpcrdma_buffer {
 	unsigned long		rb_flags;
 	u32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */
-	int			rb_posted_receives;
 
 	u32			rb_bc_srv_max_requests;
 	spinlock_t		rb_reqslock;	/* protect rb_allreqs */
@@ -560,7 +560,6 @@ void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 				struct rpcrdma_req *);
-void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);
 
 /*
  * Buffer calls - xprtrdma/verbs.c

commit 4aa5cffefa6f8af8f16490df58b8f0d827911b58
Author: Vasily Averin <vvs@virtuozzo.com>
Date:   Mon Dec 24 14:45:25 2018 +0300

    sunrpc: remove unused bc_up operation from rpc_xprt_ops
    
    Signed-off-by: Vasily Averin <vvs@virtuozzo.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a13ccb643ce0..9218dbebedce 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -661,7 +661,6 @@ void xprt_rdma_cleanup(void);
  */
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 int xprt_rdma_bc_setup(struct rpc_xprt *, unsigned int);
-int xprt_rdma_bc_up(struct svc_serv *, struct net *);
 size_t xprt_rdma_bc_maxpayload(struct rpc_xprt *);
 int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
 void rpcrdma_bc_receive_call(struct rpcrdma_xprt *, struct rpcrdma_rep *);

commit 31e62d25b5b8155b2ff6a7c6d31256475dbbcc7a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:26:08 2018 -0400

    xprtrdma: Simplify RPC wake-ups on connect
    
    Currently, when a connection is established, rpcrdma_conn_upcall
    invokes rpcrdma_conn_func and then
    wake_up_all(&ep->rep_connect_wait). The former wakes waiting RPCs,
    but the connect worker is not done yet, and that leads to races,
    double wakes, and difficulty understanding how this logic is
    supposed to work.
    
    Instead, collect all the "connection established" logic in the
    connect worker (xprt_rdma_connect_worker). A disconnect worker is
    retained to handle provider upcalls safely.
    
    Fixes: 254f91e2fa1f ("xprtrdma: RPC/RDMA must invoke ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index eae21668e692..a13ccb643ce0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -101,7 +101,7 @@ struct rpcrdma_ep {
 	wait_queue_head_t 	rep_connect_wait;
 	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
-	struct delayed_work	rep_connect_worker;
+	struct delayed_work	rep_disconnect_worker;
 };
 
 /* Pre-allocate extra Work Requests for handling backward receives
@@ -556,7 +556,6 @@ int rpcrdma_ep_create(struct rpcrdma_ep *, struct rpcrdma_ia *,
 				struct rpcrdma_create_data_internal *);
 void rpcrdma_ep_destroy(struct rpcrdma_ep *, struct rpcrdma_ia *);
 int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
-void rpcrdma_conn_func(struct rpcrdma_ep *ep);
 void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
@@ -654,7 +653,6 @@ static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 extern unsigned int xprt_rdma_max_inline_read;
 void xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap);
 void xprt_rdma_free_addresses(struct rpc_xprt *xprt);
-void rpcrdma_connect_worker(struct work_struct *work);
 void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq);
 int xprt_rdma_init(void);
 void xprt_rdma_cleanup(void);

commit 61da886bf74e738995d359fa14d77f72d14cfb87
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:25:25 2018 -0400

    xprtrdma: Explicitly resetting MRs is no longer necessary
    
    When a memory operation fails, the MR's driver state might not match
    its hardware state. The only reliable recourse is to dereg the MR.
    This is done in ->ro_recover_mr, which then attempts to allocate a
    fresh MR to replace the released MR.
    
    Since commit e2ac236c0b651 ("xprtrdma: Allocate MRs on demand"),
    xprtrdma dynamically allocates MRs. It can add more MRs whenever
    they are needed.
    
    That makes it possible to simply release an MR when a memory
    operation fails, instead of "recovering" it. It will automatically
    be replaced by the on-demand MR allocator.
    
    This commit is a little larger than I wanted, but it replaces
    ->ro_recover_mr, rb_recovery_lock, rb_recovery_worker, and the
    rb_stale_mrs list with a generic work queue.
    
    Since MRs are no longer orphaned, the mrs_orphaned metric is no
    longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2ca14f7c2d51..eae21668e692 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -280,6 +280,7 @@ struct rpcrdma_mr {
 	u32			mr_handle;
 	u32			mr_length;
 	u64			mr_offset;
+	struct work_struct	mr_recycle;
 	struct list_head	mr_all;
 };
 
@@ -411,9 +412,6 @@ struct rpcrdma_buffer {
 
 	u32			rb_bc_max_requests;
 
-	spinlock_t		rb_recovery_lock; /* protect rb_stale_mrs */
-	struct list_head	rb_stale_mrs;
-	struct delayed_work	rb_recovery_worker;
 	struct delayed_work	rb_refresh_worker;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
@@ -452,7 +450,7 @@ struct rpcrdma_stats {
 	unsigned long		hardway_register_count;
 	unsigned long		failed_marshal_count;
 	unsigned long		bad_reply_count;
-	unsigned long		mrs_recovered;
+	unsigned long		mrs_recycled;
 	unsigned long		mrs_orphaned;
 	unsigned long		mrs_allocated;
 	unsigned long		empty_sendctx_q;
@@ -481,7 +479,6 @@ struct rpcrdma_memreg_ops {
 				     struct list_head *mrs);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
 					 struct list_head *);
-	void		(*ro_recover_mr)(struct rpcrdma_mr *mr);
 	int		(*ro_open)(struct rpcrdma_ia *,
 				   struct rpcrdma_ep *,
 				   struct rpcrdma_create_data_internal *);
@@ -578,7 +575,12 @@ struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);
 struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_mr_put(struct rpcrdma_mr *mr);
 void rpcrdma_mr_unmap_and_put(struct rpcrdma_mr *mr);
-void rpcrdma_mr_defer_recovery(struct rpcrdma_mr *mr);
+
+static inline void
+rpcrdma_mr_recycle(struct rpcrdma_mr *mr)
+{
+	schedule_work(&mr->mr_recycle);
+}
 
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
 void rpcrdma_buffer_put(struct rpcrdma_req *);

commit 0725d4e1b8b08a60838db3a6e65c23ea8824a048
Merge: 89e255678fec 93b7f7ad2018
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 10:09:03 2018 -0700

    Merge tag 'nfs-for-4.18-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      Stable fixes:
    
       - Fix a 1-byte stack overflow in nfs_idmap_read_and_verify_message
    
       - Fix a hang due to incorrect error returns in rpcrdma_convert_iovs()
    
       - Revert an incorrect change to the NFSv4.1 callback channel
    
       - Fix a bug in the NFSv4.1 sequence error handling
    
      Features and optimisations:
    
       - Support for piggybacking a LAYOUTGET operation to the OPEN compound
    
       - RDMA performance enhancements to deal with transport congestion
    
       - Add proper SPDX tags for NetApp-contributed RDMA source
    
       - Do not request delegated file attributes (size+change) from the
         server
    
       - Optimise away a GETATTR in the lookup revalidate code when doing
         NFSv4 OPEN
    
       - Optimise away unnecessary lookups for rename targets
    
       - Misc performance improvements when freeing NFSv4 delegations
    
      Bugfixes and cleanups:
    
       - Try to fail quickly if proto=rdma
    
       - Clean up RDMA receive trace points
    
       - Fix sillyrename to return the delegation when appropriate
    
       - Misc attribute revalidation fixes
    
       - Immediately clear the pNFS layout on a file when the server returns
         ESTALE
    
       - Return NFS4ERR_DELAY when delegation/layout recalls fail due to
         igrab()
    
       - Fix the client behaviour on NFS4ERR_SEQ_FALSE_RETRY"
    
    * tag 'nfs-for-4.18-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (80 commits)
      skip LAYOUTRETURN if layout is invalid
      NFSv4.1: Fix the client behaviour on NFS4ERR_SEQ_FALSE_RETRY
      NFSv4: Fix a typo in nfs41_sequence_process
      NFSv4: Revert commit 5f83d86cf531d ("NFSv4.x: Fix wraparound issues..")
      NFSv4: Return NFS4ERR_DELAY when a layout recall fails due to igrab()
      NFSv4: Return NFS4ERR_DELAY when a delegation recall fails due to igrab()
      NFSv4.0: Remove transport protocol name from non-UCS client ID
      NFSv4.0: Remove cl_ipaddr from non-UCS client ID
      NFSv4: Fix a compiler warning when CONFIG_NFS_V4_1 is undefined
      NFS: Filter cache invalidation when holding a delegation
      NFS: Ignore NFS_INO_REVAL_FORCED in nfs_check_inode_attributes()
      NFS: Improve caching while holding a delegation
      NFS: Fix attribute revalidation
      NFS: fix up nfs_setattr_update_inode
      NFSv4: Ensure the inode is clean when we set a delegation
      NFSv4: Ignore NFS_INO_REVAL_FORCED in nfs4_proc_access
      NFSv4: Don't ask for delegated attributes when adding a hard link
      NFSv4: Don't ask for delegated attributes when revalidating the inode
      NFS: Pass the inode down to the getattr() callback
      NFSv4: Don't request size+change attribute if they are delegated to us
      ...

commit 89e255678fec5a1a9ed59664a62212d19873aedc
Merge: 8efcf34a2639 692ad280bff3
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 09:49:33 2018 -0700

    Merge tag 'nfsd-4.18' of git://linux-nfs.org/~bfields/linux
    
    Pull nfsd updates from Bruce Fields:
     "A relatively quiet cycle for nfsd.
    
      The largest piece is an RDMA update from Chuck Lever with new trace
      points, miscellaneous cleanups, and streamlining of the send and
      receive paths.
    
      Other than that, some miscellaneous bugfixes"
    
    * tag 'nfsd-4.18' of git://linux-nfs.org/~bfields/linux: (26 commits)
      nfsd: fix error handling in nfs4_set_delegation()
      nfsd: fix potential use-after-free in nfsd4_decode_getdeviceinfo
      Fix 16-byte memory leak in gssp_accept_sec_context_upcall
      svcrdma: Fix incorrect return value/type in svc_rdma_post_recvs
      svcrdma: Remove unused svc_rdma_op_ctxt
      svcrdma: Persistently allocate and DMA-map Send buffers
      svcrdma: Simplify svc_rdma_send()
      svcrdma: Remove post_send_wr
      svcrdma: Don't overrun the SGE array in svc_rdma_send_ctxt
      svcrdma: Introduce svc_rdma_send_ctxt
      svcrdma: Clean up Send SGE accounting
      svcrdma: Refactor svc_rdma_dma_map_buf
      svcrdma: Allocate recv_ctxt's on CPU handling Receives
      svcrdma: Persistently allocate and DMA-map Receive buffers
      svcrdma: Preserve Receive buffer until svc_rdma_sendto
      svcrdma: Simplify svc_rdma_recv_ctxt_put
      svcrdma: Remove sc_rq_depth
      svcrdma: Introduce svc_rdma_recv_ctxt
      svcrdma: Trace key RDMA API events
      svcrdma: Trace key RPC/RDMA protocol events
      ...

commit fcda3d5d221bbfc469415b0fa7dc4eb87d90d955
Merge: 3f0b3cf46e05 11d0ac16b02e
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Mon Jun 4 18:57:13 2018 -0400

    Merge tag 'nfs-rdma-for-4.18-1' of git://git.linux-nfs.org/projects/anna/linux-nfs
    
    NFS-over-RDMA client updates for Linux 4.18
    
    Stable patches:
    - xprtrdma: Return -ENOBUFS when no pages are available
    
    New features:
    - Add ->alloc_slot() and ->free_slot() functions
    
    Bugfixes and cleanups:
    - Add missing SPDX tags to some files
    - Try to fail mount quickly if client has no RDMA devices
    - Create transport IDs in the correct network namespace
    - Fix max_send_wr computation
    - Clean up receive tracepoints
    - Refactor receive handling
    - Remove unused functions

commit 2fad659209d5b1dbaa1f58606372571c61fc8cbd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:57 2018 -0400

    xprtrdma: Wait on empty sendctx queue
    
    Currently, when the sendctx queue is exhausted during marshaling, the
    RPC/RDMA transport places the RPC task on the delayq, which forces a
    wait for HZ >> 2 before the marshal and send is retried.
    
    With this change, the transport now places such an RPC task on the
    pending queue, and wakes it just as soon as more sendctxs become
    available. This typically takes less than a millisecond, and the
    write_space waking mechanism is less deadlock-prone.
    
    Moreover, the waiting RPC task is holding the transport's write
    lock, which blocks the transport from sending RPCs. Therefore faster
    recovery from sendctx queue exhaustion is desirable.
    
    Cf. commit 5804891455d5 ("xprtrdma: ->send_request returns -EAGAIN
    when there are no free MRs").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c60687934092..e4a408d7b195 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -400,6 +400,7 @@ struct rpcrdma_buffer {
 	spinlock_t		rb_lock;	/* protect buf lists */
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
+	unsigned long		rb_flags;
 	u32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */
 	int			rb_posted_receives;
@@ -417,6 +418,11 @@ struct rpcrdma_buffer {
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 
+/* rb_flags */
+enum {
+	RPCRDMA_BUF_F_EMPTY_SCQ = 0,
+};
+
 /*
  * Internal structure for transport instance creation. This
  * exists primarily for modularity.

commit b6e717cbf28c8348d34be472f119b0ea82e5e8e7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:05 2018 -0400

    xprtrdma: Prepare RPC/RDMA includes for server-side trace points
    
    Clean up: Move #include <trace/events/rpcrdma.h> into source files,
    similar to how it is done with trace/events/sunrpc.h.
    
    Server-side trace points will be part of the rpcrdma subsystem,
    just like the client-side trace points.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3d3b423fa9c1..3f856c71fa6f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -675,5 +675,3 @@ void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);
 extern struct xprt_class xprt_rdma_bc;
 
 #endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */
-
-#include <trace/events/rpcrdma.h>

commit efd81e90d3f719a2a7fd696c9ed0247e1cae1b7c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:41 2018 -0400

    xprtrdma: Make rpcrdma_sendctx_put_locked() a static function
    
    Clean up: The only call site is in the same file as the function's
    definition.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 44a31246b8f7..c60687934092 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -568,7 +568,6 @@ void rpcrdma_destroy_req(struct rpcrdma_req *);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);
-void rpcrdma_sendctx_put_locked(struct rpcrdma_sendctx *sc);
 
 struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_mr_put(struct rpcrdma_mr *mr);

commit a7986f09986ac1befc85bcab30970312c476dbc7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:25 2018 -0400

    xprtrdma: Remove rpcrdma_ep_{post_recv, post_extra_recv}
    
    Clean up: These functions are no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 4915a6dd58c9..44a31246b8f7 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -558,7 +558,6 @@ void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 				struct rpcrdma_req *);
-int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_rep *);
 void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);
 
 /*
@@ -599,8 +598,6 @@ rpcrdma_dma_map_regbuf(struct rpcrdma_ia *ia, struct rpcrdma_regbuf *rb)
 	return __rpcrdma_dma_map_regbuf(ia, rb);
 }
 
-int rpcrdma_ep_post_extra_recv(struct rpcrdma_xprt *, unsigned int);
-
 int rpcrdma_alloc_wq(void);
 void rpcrdma_destroy_wq(void);
 

commit 7c8d9e7c8863905951d4eaa7a8d277150f3a37f7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:20 2018 -0400

    xprtrdma: Move Receive posting to Receive handler
    
    Receive completion and Reply handling are done by a BOUND
    workqueue, meaning they run on only one CPU.
    
    Posting receives is currently done in the send_request path, which
    on large systems is typically done on a different CPU than the one
    handling Receive completions. This results in movement of
    Receive-related cachelines between the sending and receiving CPUs.
    
    More importantly, it means that currently Receives are posted while
    the transport's write lock is held, which is unnecessary and costly.
    
    Finally, allocation of Receive buffers is performed on-demand in
    the Receive completion handler. This helps guarantee that they are
    allocated on the same NUMA node as the CPU that handles Receive
    completions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1d7bb6e5db18..4915a6dd58c9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -197,6 +197,7 @@ struct rpcrdma_rep {
 	__be32			rr_proc;
 	int			rr_wc_flags;
 	u32			rr_inv_rkey;
+	bool			rr_temp;
 	struct rpcrdma_regbuf	*rr_rdmabuf;
 	struct rpcrdma_xprt	*rr_rxprt;
 	struct work_struct	rr_work;
@@ -397,11 +398,11 @@ struct rpcrdma_buffer {
 	struct rpcrdma_sendctx	**rb_sc_ctxs;
 
 	spinlock_t		rb_lock;	/* protect buf lists */
-	int			rb_send_count, rb_recv_count;
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
 	u32			rb_max_requests;
 	u32			rb_credits;	/* most recent credit grant */
+	int			rb_posted_receives;
 
 	u32			rb_bc_srv_max_requests;
 	spinlock_t		rb_reqslock;	/* protect rb_allreqs */
@@ -558,13 +559,13 @@ void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 				struct rpcrdma_req *);
 int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_rep *);
+void rpcrdma_post_recvs(struct rpcrdma_xprt *r_xprt, bool temp);
 
 /*
  * Buffer calls - xprtrdma/verbs.c
  */
 struct rpcrdma_req *rpcrdma_create_req(struct rpcrdma_xprt *);
 void rpcrdma_destroy_req(struct rpcrdma_req *);
-int rpcrdma_create_rep(struct rpcrdma_xprt *r_xprt);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);
@@ -577,7 +578,6 @@ void rpcrdma_mr_defer_recovery(struct rpcrdma_mr *mr);
 
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
 void rpcrdma_buffer_put(struct rpcrdma_req *);
-void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
 struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(size_t, enum dma_data_direction,

commit edb41e61a54ee75fae31302775e0301fdcb0caaa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:09 2018 -0400

    xprtrdma: Make rpc_rqst part of rpcrdma_req
    
    This simplifies allocation of the generic RPC slot and xprtrdma
    specific per-RPC resources.
    
    It also makes xprtrdma more like the socket-based transports:
    ->buf_alloc and ->buf_free are now responsible only for send and
    receive buffers.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3490a87bc69a..1d7bb6e5db18 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -335,6 +335,7 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
+	struct rpc_rqst		rl_slot;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;
 	struct xdr_stream	rl_stream;
@@ -357,16 +358,10 @@ enum {
 	RPCRDMA_REQ_F_TX_RESOURCES,
 };
 
-static inline void
-rpcrdma_set_xprtdata(struct rpc_rqst *rqst, struct rpcrdma_req *req)
-{
-	rqst->rq_xprtdata = req;
-}
-
 static inline struct rpcrdma_req *
 rpcr_to_rdmar(const struct rpc_rqst *rqst)
 {
-	return rqst->rq_xprtdata;
+	return container_of(rqst, struct rpcrdma_req, rl_slot);
 }
 
 static inline void

commit a2268cfbf599e7f55d4ee68193f08b4f44535fac
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:34:32 2018 -0400

    xprtrdma: Add proper SPDX tags for NetApp-contributed source
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3d3b423fa9c1..3490a87bc69a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause */
 /*
  * Copyright (c) 2014-2017 Oracle.  All rights reserved.
  * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.

commit 054f155721d7af1f343ed52bea246626d8450ca8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 1 11:37:14 2018 -0400

    xprtrdma: Fix list corruption / DMAR errors during MR recovery
    
    The ro_release_mr methods check whether mr->mr_list is empty.
    Therefore, be sure to always use list_del_init when removing an MR
    linked into a list using that field. Otherwise, when recovering from
    transport failures or device removal, list corruption can result, or
    MRs can get mapped or unmapped an odd number of times, resulting in
    IOMMU-related failures.
    
    In general this fix is appropriate back to v4.8. However, code
    changes since then make it impossible to apply this patch directly
    to stable kernels. The fix would have to be applied by hand or
    reworked for kernels earlier than v4.16.
    
    Backport guidance -- there are several cases:
    - When creating an MR, initialize mr_list so that using list_empty
      on an as-yet-unused MR is safe.
    - When an MR is being handled by the remote invalidation path,
      ensure that mr_list is reinitialized when it is removed from
      rl_registered.
    - When an MR is being handled by rpcrdma_destroy_mrs, it is removed
      from mr_all, but it may still be on an rl_registered list. In
      that case, the MR needs to be removed from that list before being
      released.
    - Other cases are covered by using list_del_init in rpcrdma_mr_pop.
    
    Fixes: 9d6b04097882 ('xprtrdma: Place registered MWs on a ... ')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3d3b423fa9c1..cb41b12a3bf8 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -380,7 +380,7 @@ rpcrdma_mr_pop(struct list_head *list)
 	struct rpcrdma_mr *mr;
 
 	mr = list_first_entry(list, struct rpcrdma_mr, mr_list);
-	list_del(&mr->mr_list);
+	list_del_init(&mr->mr_list);
 	return mr;
 }
 

commit f2877623082b720c1424b163cf905fff8eed4126
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:30:59 2018 -0500

    xprtrdma: Chain Send to FastReg WRs
    
    With FRWR, the client transport can perform memory registration and
    post a Send with just a single ib_post_send.
    
    This reduces contention between the send_request path and the Send
    Completion handlers, and reduces the overhead of registering a chunk
    that has multiple segments.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 29ea0b4e98f9..3d3b423fa9c1 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -472,6 +472,8 @@ struct rpcrdma_memreg_ops {
 			(*ro_map)(struct rpcrdma_xprt *,
 				  struct rpcrdma_mr_seg *, int, bool,
 				  struct rpcrdma_mr **);
+	int		(*ro_send)(struct rpcrdma_ia *ia,
+				   struct rpcrdma_req *req);
 	void		(*ro_reminv)(struct rpcrdma_rep *rep,
 				     struct list_head *mrs);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,

commit 8a14793e7aa718d16382e18cadec92e2e531e62a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:30:38 2018 -0500

    xprtrdma: Remove xprt-specific connect cookie
    
    Clean up: The generic rq_connect_cookie is sufficient to detect RPC
    Call retransmission.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 430a6de8300e..29ea0b4e98f9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -334,7 +334,6 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
-	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;
 	struct xdr_stream	rl_stream;

commit 6720a89933739cb8dec748cd253f7c8df2c0ae4d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:30:27 2018 -0500

    xprtrdma: Fix latency regression on NUMA NFS/RDMA clients
    
    With v4.15, on one of my NFS/RDMA clients I measured a nearly
    doubling in the latency of small read and write system calls. There
    was no change in server round trip time. The extra latency appears
    in the whole RPC execution path.
    
    "git bisect" settled on commit ccede7598588 ("xprtrdma: Spread reply
    processing over more CPUs") .
    
    After some experimentation, I found that leaving the WQ bound and
    allowing the scheduler to pick the dispatch CPU seems to eliminate
    the long latencies, and it does not introduce any new regressions.
    
    The fix is implemented by reverting only the part of
    commit ccede7598588 ("xprtrdma: Spread reply processing over more
    CPUs") that dispatches RPC replies specifically on the CPU where the
    matching RPC call was made.
    
    Interestingly, saving the CPU number and later queuing reply
    processing there was effective _only_ for a NFS READ and WRITE
    request. On my NUMA client, in-kernel RPC reply processing for
    asynchronous RPCs was dispatched on the same CPU where the RPC call
    was made, as expected. However synchronous RPCs seem to get their
    reply dispatched on some other CPU than where the call was placed,
    every time.
    
    Fixes: ccede7598588 ("xprtrdma: Spread reply processing over ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Cc: stable@vger.kernel.org # v4.15+
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 69883a960a3f..430a6de8300e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -334,7 +334,6 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
-	int			rl_cpu;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;

commit fc1eb8076fb0eb0641566b24007a40a7d4ae0116
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 20 16:31:37 2017 -0500

    xprtrdma: Add trace points in the client-side backchannel code paths
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a1ca1b638123..69883a960a3f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -365,7 +365,7 @@ rpcrdma_set_xprtdata(struct rpc_rqst *rqst, struct rpcrdma_req *req)
 }
 
 static inline struct rpcrdma_req *
-rpcr_to_rdmar(struct rpc_rqst *rqst)
+rpcr_to_rdmar(const struct rpc_rqst *rqst)
 {
 	return rqst->rq_xprtdata;
 }

commit e48f083e19c9082352a07d0aef6a4ea0dc72dde0
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Jan 20 11:16:34 2018 -0500

    rpcrdma: infrastructure for static trace points in rpcrdma.ko
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 28ae1fb1e2f3..a1ca1b638123 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -675,3 +675,5 @@ void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);
 extern struct xprt_class xprt_rdma_bc;
 
 #endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */
+
+#include <trace/events/rpcrdma.h>

commit ec12e479e30653bf973ca1185bbb09158e9af0b7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:58:04 2017 -0500

    xprtrdma: Introduce rpcrdma_mw_unmap_and_put
    
    Clean up: Code review suggested that a common bit of code can be
    placed into a helper function, and this gives us fewer places to
    stick an "I DMA unmapped something" trace point.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 530ace6ed125..28ae1fb1e2f3 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -576,6 +576,7 @@ void rpcrdma_sendctx_put_locked(struct rpcrdma_sendctx *sc);
 
 struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
 void rpcrdma_mr_put(struct rpcrdma_mr *mr);
+void rpcrdma_mr_unmap_and_put(struct rpcrdma_mr *mr);
 void rpcrdma_mr_defer_recovery(struct rpcrdma_mr *mr);
 
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);

commit 96ceddea3710f61bb5a5f2af25e684b7e1466171
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:55 2017 -0500

    xprtrdma: Remove usage of "mw"
    
    Clean up: struct rpcrdma_mw was named after Memory Windows, but
    xprtrdma no longer supports a Memory Window registration mode.
    Rename rpcrdma_mw and its fields to reduce confusion and make
    the code more sensible to read.
    
    Renaming "mw" was suggested by Tom Talpey, the author of the
    original xprtrdma implementation. It's a good idea, but I haven't
    done this until now because it's a huge diffstat for no benefit
    other than code readability.
    
    However, I'm about to introduce static trace points that expose
    a few of xprtrdma's internal data structures. They should make sense
    in the trace report, and it's reasonable to treat trace points as a
    kernel API contract which might be difficult to change later.
    
    While I'm churning things up, two additional changes:
    - rename variables unhelpfully called "r" to "mr", to improve code
      clarity, and
    - rename the MR-related helper functions using the form
      "rpcrdma_mr_<verb>", to be consistent with other areas of the
      code.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f52269afaa09..530ace6ed125 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -230,12 +230,12 @@ enum {
 };
 
 /*
- * struct rpcrdma_mw - external memory region metadata
+ * struct rpcrdma_mr - external memory region metadata
  *
  * An external memory region is any buffer or page that is registered
  * on the fly (ie, not pre-registered).
  *
- * Each rpcrdma_buffer has a list of free MWs anchored in rb_mws. During
+ * Each rpcrdma_buffer has a list of free MWs anchored in rb_mrs. During
  * call_allocate, rpcrdma_buffer_get() assigns one to each segment in
  * an rpcrdma_req. Then rpcrdma_register_external() grabs these to keep
  * track of registration metadata while each RPC is pending.
@@ -265,20 +265,20 @@ struct rpcrdma_fmr {
 	u64			*fm_physaddrs;
 };
 
-struct rpcrdma_mw {
-	struct list_head	mw_list;
-	struct scatterlist	*mw_sg;
-	int			mw_nents;
-	enum dma_data_direction	mw_dir;
+struct rpcrdma_mr {
+	struct list_head	mr_list;
+	struct scatterlist	*mr_sg;
+	int			mr_nents;
+	enum dma_data_direction	mr_dir;
 	union {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frwr	frwr;
 	};
-	struct rpcrdma_xprt	*mw_xprt;
-	u32			mw_handle;
-	u32			mw_length;
-	u64			mw_offset;
-	struct list_head	mw_all;
+	struct rpcrdma_xprt	*mr_xprt;
+	u32			mr_handle;
+	u32			mr_length;
+	u64			mr_offset;
+	struct list_head	mr_all;
 };
 
 /*
@@ -371,19 +371,19 @@ rpcr_to_rdmar(struct rpc_rqst *rqst)
 }
 
 static inline void
-rpcrdma_push_mw(struct rpcrdma_mw *mw, struct list_head *list)
+rpcrdma_mr_push(struct rpcrdma_mr *mr, struct list_head *list)
 {
-	list_add_tail(&mw->mw_list, list);
+	list_add_tail(&mr->mr_list, list);
 }
 
-static inline struct rpcrdma_mw *
-rpcrdma_pop_mw(struct list_head *list)
+static inline struct rpcrdma_mr *
+rpcrdma_mr_pop(struct list_head *list)
 {
-	struct rpcrdma_mw *mw;
+	struct rpcrdma_mr *mr;
 
-	mw = list_first_entry(list, struct rpcrdma_mw, mw_list);
-	list_del(&mw->mw_list);
-	return mw;
+	mr = list_first_entry(list, struct rpcrdma_mr, mr_list);
+	list_del(&mr->mr_list);
+	return mr;
 }
 
 /*
@@ -393,8 +393,8 @@ rpcrdma_pop_mw(struct list_head *list)
  * One of these is associated with a transport instance
  */
 struct rpcrdma_buffer {
-	spinlock_t		rb_mwlock;	/* protect rb_mws list */
-	struct list_head	rb_mws;
+	spinlock_t		rb_mrlock;	/* protect rb_mrs list */
+	struct list_head	rb_mrs;
 	struct list_head	rb_all;
 
 	unsigned long		rb_sc_head;
@@ -473,19 +473,19 @@ struct rpcrdma_memreg_ops {
 	struct rpcrdma_mr_seg *
 			(*ro_map)(struct rpcrdma_xprt *,
 				  struct rpcrdma_mr_seg *, int, bool,
-				  struct rpcrdma_mw **);
+				  struct rpcrdma_mr **);
 	void		(*ro_reminv)(struct rpcrdma_rep *rep,
-				     struct list_head *mws);
+				     struct list_head *mrs);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
 					 struct list_head *);
-	void		(*ro_recover_mr)(struct rpcrdma_mw *);
+	void		(*ro_recover_mr)(struct rpcrdma_mr *mr);
 	int		(*ro_open)(struct rpcrdma_ia *,
 				   struct rpcrdma_ep *,
 				   struct rpcrdma_create_data_internal *);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	int		(*ro_init_mr)(struct rpcrdma_ia *,
-				      struct rpcrdma_mw *);
-	void		(*ro_release_mr)(struct rpcrdma_mw *);
+				      struct rpcrdma_mr *);
+	void		(*ro_release_mr)(struct rpcrdma_mr *mr);
 	const char	*ro_displayname;
 	const int	ro_send_w_inv_ok;
 };
@@ -574,15 +574,15 @@ void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);
 void rpcrdma_sendctx_put_locked(struct rpcrdma_sendctx *sc);
 
-struct rpcrdma_mw *rpcrdma_get_mw(struct rpcrdma_xprt *);
-void rpcrdma_put_mw(struct rpcrdma_xprt *, struct rpcrdma_mw *);
+struct rpcrdma_mr *rpcrdma_mr_get(struct rpcrdma_xprt *r_xprt);
+void rpcrdma_mr_put(struct rpcrdma_mr *mr);
+void rpcrdma_mr_defer_recovery(struct rpcrdma_mr *mr);
+
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
 void rpcrdma_buffer_put(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
-void rpcrdma_defer_mr_recovery(struct rpcrdma_mw *);
-
 struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(size_t, enum dma_data_direction,
 					    gfp_t);
 bool __rpcrdma_dma_map_regbuf(struct rpcrdma_ia *, struct rpcrdma_regbuf *);

commit ce5b3717828356ce2c61e5a2a830df970fc90fb9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:47 2017 -0500

    xprtrdma: Replace all usage of "frmr" with "frwr"
    
    Clean up: Over time, the industry has adopted the term "frwr"
    instead of "frmr". The term "frwr" is now more widely recognized.
    
    For the past couple of years I've attempted to add new code using
    "frwr" , but there still remains plenty of older code that still
    uses "frmr". Replace all usage of "frmr" to avoid confusion.
    
    While we're churning code, rename variables unhelpfully called "f"
    to "frwr", to improve code clarity.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e084130d3d84..f52269afaa09 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -73,7 +73,7 @@ struct rpcrdma_ia {
 	struct completion	ri_remove_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_segs;
-	unsigned int		ri_max_frmr_depth;
+	unsigned int		ri_max_frwr_depth;
 	unsigned int		ri_max_inline_write;
 	unsigned int		ri_max_inline_read;
 	unsigned int		ri_max_send_sges;
@@ -242,17 +242,17 @@ enum {
  * rpcrdma_deregister_external() uses this metadata to unmap and
  * release these resources when an RPC is complete.
  */
-enum rpcrdma_frmr_state {
-	FRMR_IS_INVALID,	/* ready to be used */
-	FRMR_IS_VALID,		/* in use */
-	FRMR_FLUSHED_FR,	/* flushed FASTREG WR */
-	FRMR_FLUSHED_LI,	/* flushed LOCALINV WR */
+enum rpcrdma_frwr_state {
+	FRWR_IS_INVALID,	/* ready to be used */
+	FRWR_IS_VALID,		/* in use */
+	FRWR_FLUSHED_FR,	/* flushed FASTREG WR */
+	FRWR_FLUSHED_LI,	/* flushed LOCALINV WR */
 };
 
-struct rpcrdma_frmr {
+struct rpcrdma_frwr {
 	struct ib_mr			*fr_mr;
 	struct ib_cqe			fr_cqe;
-	enum rpcrdma_frmr_state		fr_state;
+	enum rpcrdma_frwr_state		fr_state;
 	struct completion		fr_linv_done;
 	union {
 		struct ib_reg_wr	fr_regwr;
@@ -272,7 +272,7 @@ struct rpcrdma_mw {
 	enum dma_data_direction	mw_dir;
 	union {
 		struct rpcrdma_fmr	fmr;
-		struct rpcrdma_frmr	frmr;
+		struct rpcrdma_frwr	frwr;
 	};
 	struct rpcrdma_xprt	*mw_xprt;
 	u32			mw_handle;

commit cf73daf52750fca4b4af0ca812f542891c228066
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:31 2017 -0500

    xprtrdma: Split xprt_rdma_send_request
    
    Clean up. @rqst is set up differently for backchannel Replies. For
    example, rqst->rq_task and task->tk_client are both NULL. So it is
    easier to understand and maintain this code path if it is separated.
    
    Also, we can get rid of the confusing rl_connect_cookie hack in
    rpcrdma_bc_receive_call.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ed7e51304bc2..e084130d3d84 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -666,7 +666,7 @@ int xprt_rdma_bc_up(struct svc_serv *, struct net *);
 size_t xprt_rdma_bc_maxpayload(struct rpc_xprt *);
 int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
 void rpcrdma_bc_receive_call(struct rpcrdma_xprt *, struct rpcrdma_rep *);
-int rpcrdma_bc_marshal_reply(struct rpc_rqst *);
+int xprt_rdma_bc_send_reply(struct rpc_rqst *rqst);
 void xprt_rdma_bc_free_rqst(struct rpc_rqst *);
 void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);
 #endif	/* CONFIG_SUNRPC_BACKCHANNEL */

commit 6c537f2c7cc06da36f6701be4c9413d7b8b47bfb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:23 2017 -0500

    xprtrdma: buf_free not called for CB replies
    
    Since commit 5a6d1db45569 ("SUNRPC: Add a transport-specific private
    field in rpc_rqst"), the rpc_rqst's for RPC-over-RDMA backchannel
    operations leave rq_buffer set to NULL.
    
    xprt_release does not invoke ->op->buf_free when rq_buffer is NULL.
    The RPCRDMA_REQ_F_BACKCHANNEL check in xprt_rdma_free is therefore
    redundant because xprt_rdma_free is not invoked for backchannel
    requests.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 7c09e2ae2542..ed7e51304bc2 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -354,8 +354,7 @@ struct rpcrdma_req {
 
 /* rl_flags */
 enum {
-	RPCRDMA_REQ_F_BACKCHANNEL = 0,
-	RPCRDMA_REQ_F_PENDING,
+	RPCRDMA_REQ_F_PENDING = 0,
 	RPCRDMA_REQ_F_TX_RESOURCES,
 };
 

commit dd229cee4ed2617ccddc0937608728cd87c934c2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:58 2017 -0500

    xprtrdma: Remove another sockaddr_storage field (cdata::addr)
    
    Save more space in struct rpcrdma_xprt by removing the redundant
    "addr" field from struct rpcrdma_create_data_internal. Wherever
    we have rpcrdma_xprt, we also have the rpc_xprt, which has a
    sockaddr_storage field with the same content.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0b28026e1502..7c09e2ae2542 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -430,7 +430,6 @@ struct rpcrdma_buffer {
  * This data should be set with mount options
  */
 struct rpcrdma_create_data_internal {
-	struct sockaddr_storage	addr;	/* RDMA server address */
 	unsigned int	max_requests;	/* max requests (slots) in flight */
 	unsigned int	rsize;		/* mount rsize - max read hdr+data */
 	unsigned int	wsize;		/* mount wsize - max write hdr+data */
@@ -543,7 +542,7 @@ extern unsigned int xprt_rdma_memreg_strategy;
 /*
  * Interface Adapter calls - xprtrdma/verbs.c
  */
-int rpcrdma_ia_open(struct rpcrdma_xprt *xprt, struct sockaddr *addr);
+int rpcrdma_ia_open(struct rpcrdma_xprt *xprt);
 void rpcrdma_ia_remove(struct rpcrdma_ia *ia);
 void rpcrdma_ia_close(struct rpcrdma_ia *);
 bool frwr_is_supported(struct rpcrdma_ia *);

commit d461f1f2fb91b5629019b3b405528bc88c49f863
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:50 2017 -0500

    xprtrdma: Initialize the xprt address string array earlier
    
    This makes the address strings available for debugging messages in
    earlier stages of transport set up.
    
    The first benefit is to get rid of the single-use rep_remote_addr
    field, saving 128+ bytes in struct rpcrdma_ep.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 375df3d3ff59..0b28026e1502 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -100,7 +100,6 @@ struct rpcrdma_ep {
 	wait_queue_head_t 	rep_connect_wait;
 	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
-	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;
 };
 
@@ -519,6 +518,18 @@ struct rpcrdma_xprt {
 #define rpcx_to_rdmax(x) container_of(x, struct rpcrdma_xprt, rx_xprt)
 #define rpcx_to_rdmad(x) (rpcx_to_rdmax(x)->rx_data)
 
+static inline const char *
+rpcrdma_addrstr(const struct rpcrdma_xprt *r_xprt)
+{
+	return r_xprt->rx_xprt.address_strings[RPC_DISPLAY_ADDR];
+}
+
+static inline const char *
+rpcrdma_portstr(const struct rpcrdma_xprt *r_xprt)
+{
+	return r_xprt->rx_xprt.address_strings[RPC_DISPLAY_PORT];
+}
+
 /* Setting this to 0 ensures interoperability with early servers.
  * Setting this to 1 enhances certain unaligned read/write performance.
  * Default is 0, see sysctl entry and rpc_rdma.c rpcrdma_convert_iovs() */

commit 104927042cde1f86e9f3959ba9c8b4dae1616d69
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:42 2017 -0500

    xprtrdma: Remove unused padding variables
    
    Clean up. Remove fields that should have been removed by
    commit b3221d6a53c4 ("xprtrdma: Remove logic that constructs
    RDMA_MSGP type calls").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 80ea3db054d8..375df3d3ff59 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -437,7 +437,6 @@ struct rpcrdma_create_data_internal {
 	unsigned int	wsize;		/* mount wsize - max write hdr+data */
 	unsigned int	inline_rsize;	/* max non-rdma read data payload */
 	unsigned int	inline_wsize;	/* max non-rdma write data payload */
-	unsigned int	padding;	/* non-rdma write header padding */
 };
 
 /*

commit 3f0e3edd6ad209ee18a3ea5c2a6e2046d2a2e783
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:34 2017 -0500

    xprtrdma: Remove ri_reminv_expected
    
    Clean up.
    
    Commit b5f0afbea4f2 ("xprtrdma: Per-connection pad optimization")
    should have removed this.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e787dda3b0f5..80ea3db054d8 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -77,7 +77,6 @@ struct rpcrdma_ia {
 	unsigned int		ri_max_inline_write;
 	unsigned int		ri_max_inline_read;
 	unsigned int		ri_max_send_sges;
-	bool			ri_reminv_expected;
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
 	unsigned long		ri_flags;

commit c34416182f041e4107e531c6083c3df9a8af96f7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:26 2017 -0500

    xprtrdma: Per-mode handling for Remote Invalidation
    
    Refactoring change: Remote Invalidation is particular to the memory
    registration mode that is use. Use a callout instead of a generic
    function to handle Remote Invalidation.
    
    This gets rid of the 8-byte flags field in struct rpcrdma_mw, of
    which only a single bit flag has been allocated.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3b63e61feae2..e787dda3b0f5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -272,7 +272,6 @@ struct rpcrdma_mw {
 	struct scatterlist	*mw_sg;
 	int			mw_nents;
 	enum dma_data_direction	mw_dir;
-	unsigned long		mw_flags;
 	union {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
@@ -284,11 +283,6 @@ struct rpcrdma_mw {
 	struct list_head	mw_all;
 };
 
-/* mw_flags */
-enum {
-	RPCRDMA_MW_F_RI		= 1,
-};
-
 /*
  * struct rpcrdma_req -- structure central to the request/reply sequence.
  *
@@ -485,6 +479,8 @@ struct rpcrdma_memreg_ops {
 			(*ro_map)(struct rpcrdma_xprt *,
 				  struct rpcrdma_mr_seg *, int, bool,
 				  struct rpcrdma_mw **);
+	void		(*ro_reminv)(struct rpcrdma_rep *rep,
+				     struct list_head *mws);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
 					 struct list_head *);
 	void		(*ro_recover_mr)(struct rpcrdma_mw *);

commit d698c4a02ee02053bbebe051322ff427a2dad56a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:09 2017 -0500

    xprtrdma: Fix backchannel allocation of extra rpcrdma_reps
    
    The backchannel code uses rpcrdma_recv_buffer_put to add new reps
    to the free rep list. This also decrements rb_recv_count, which
    spoofs the receive overrun logic in rpcrdma_buffer_get_rep.
    
    Commit 9b06688bc3b9 ("xprtrdma: Fix additional uses of
    spin_lock_irqsave(rb_lock)") replaced the original open-coded
    list_add with a call to rpcrdma_recv_buffer_put(), but then a year
    later, commit 05c974669ece ("xprtrdma: Fix receive buffer
    accounting") added rep accounting to rpcrdma_recv_buffer_put.
    It was an oversight to let the backchannel continue to use this
    function.
    
    The fix this, let's combine the "add to free list" logic with
    rpcrdma_create_rep.
    
    Also, do not allocate RPCRDMA_MAX_BC_REQUESTS rpcrdma_reps in
    rpcrdma_buffer_create and then allocate additional rpcrdma_reps in
    rpcrdma_bc_setup_reps. Allocating the extra reps during backchannel
    set-up is sufficient.
    
    Fixes: 05c974669ece ("xprtrdma: Fix receive buffer accounting")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1342f743f1c4..3b63e61feae2 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -564,8 +564,8 @@ int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_rep *);
  * Buffer calls - xprtrdma/verbs.c
  */
 struct rpcrdma_req *rpcrdma_create_req(struct rpcrdma_xprt *);
-struct rpcrdma_rep *rpcrdma_create_rep(struct rpcrdma_xprt *);
 void rpcrdma_destroy_req(struct rpcrdma_req *);
+int rpcrdma_create_rep(struct rpcrdma_xprt *r_xprt);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);

commit ccede7598588ae344143f82fb763912535648d58
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Dec 4 14:04:04 2017 -0500

    xprtrdma: Spread reply processing over more CPUs
    
    Commit d8f532d20ee4 ("xprtrdma: Invoke rpcrdma_reply_handler
    directly from RECV completion") introduced a performance regression
    for NFS I/O small enough to not need memory registration. In multi-
    threaded benchmarks that generate primarily small I/O requests,
    IOPS throughput is reduced by nearly a third. This patch restores
    the previous level of throughput.
    
    Because workqueues are typically BOUND (in particular ib_comp_wq,
    nfsiod_workqueue, and rpciod_workqueue), NFS/RDMA workloads tend
    to aggregate on the CPU that is handling Receive completions.
    
    The usual approach to addressing this problem is to create a QP
    and CQ for each CPU, and then schedule transactions on the QP
    for the CPU where you want the transaction to complete. The
    transaction then does not require an extra context switch during
    completion to end up on the same CPU where the transaction was
    started.
    
    This approach doesn't work for the Linux NFS/RDMA client because
    currently the Linux NFS client does not support multiple connections
    per client-server pair, and the RDMA core API does not make it
    straightforward for ULPs to determine which CPU is responsible for
    handling Receive completions for a CQ.
    
    So for the moment, record the CPU number in the rpcrdma_req before
    the transport sends each RPC Call. Then during Receive completion,
    queue the RPC completion on that same CPU.
    
    Additionally, move all RPC completion processing to the deferred
    handler so that even RPCs with simple small replies complete on
    the CPU that sent the corresponding RPC Call.
    
    Fixes: d8f532d20ee4 ("xprtrdma: Invoke rpcrdma_reply_handler ...")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 51686d9eac5f..1342f743f1c4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -342,6 +342,7 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
+	int			rl_cpu;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;

commit 62b56a675565a2e40f2cdf50455977448fd87413
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 30 16:22:14 2017 -0400

    xprtrdma: Update copyright notices
    
    Credit work contributed by Oracle engineers since 2014.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 8b9954c41ee4..51686d9eac5f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -1,4 +1,5 @@
 /*
+ * Copyright (c) 2014-2017 Oracle.  All rights reserved.
  * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two

commit 2232df5ece121fd7049ccff95cbb3acfab278d75
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 30 16:21:57 2017 -0400

    rpcrdma: Remove C structure definitions of XDR data items
    
    Clean up: C-structure style XDR encoding and decoding logic has
    been replaced over the past several merge windows on both the
    client and server. These data structures are no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 6e64c8259d34..8b9954c41ee4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -145,12 +145,6 @@ rdmab_lkey(struct rpcrdma_regbuf *rb)
 	return rb->rg_iov.lkey;
 }
 
-static inline struct rpcrdma_msg *
-rdmab_to_msg(struct rpcrdma_regbuf *rb)
-{
-	return (struct rpcrdma_msg *)rb->rg_base;
-}
-
 static inline struct ib_device *
 rdmab_device(struct rpcrdma_regbuf *rb)
 {

commit 6f0afc28257dfa769c210f8f8da0f21d77e7452f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:45 2017 -0400

    xprtrdma: Remove atomic send completion counting
    
    The sendctx circular queue now guarantees that xprtrdma cannot
    overflow the Send Queue, so remove the remaining bits of the
    original Send WQE counting mechanism.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index bccd5d8b9384..6e64c8259d34 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -95,8 +95,6 @@ enum {
 struct rpcrdma_ep {
 	unsigned int		rep_send_count;
 	unsigned int		rep_send_batch;
-	atomic_t		rep_cqcount;
-	int			rep_cqinit;
 	int			rep_connected;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
@@ -106,25 +104,6 @@ struct rpcrdma_ep {
 	struct delayed_work	rep_connect_worker;
 };
 
-static inline void
-rpcrdma_init_cqcount(struct rpcrdma_ep *ep, int count)
-{
-	atomic_set(&ep->rep_cqcount, ep->rep_cqinit - count);
-}
-
-/* To update send queue accounting, provider must take a
- * send completion every now and then.
- */
-static inline void
-rpcrdma_set_signaled(struct rpcrdma_ep *ep, struct ib_send_wr *send_wr)
-{
-	send_wr->send_flags = 0;
-	if (unlikely(atomic_sub_return(1, &ep->rep_cqcount) <= 0)) {
-		rpcrdma_init_cqcount(ep, 0);
-		send_wr->send_flags = IB_SEND_SIGNALED;
-	}
-}
-
 /* Pre-allocate extra Work Requests for handling backward receives
  * and sends. This is a fixed value because the Work Queues are
  * allocated when the forward channel is set up.

commit 01bb35c89d90abe6fd1c0be001f84bbdfa7fa7d1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:36 2017 -0400

    xprtrdma: RPC completion should wait for Send completion
    
    When an RPC Call includes a file data payload, that payload can come
    from pages in the page cache, or a user buffer (for direct I/O).
    
    If the payload can fit inline, xprtrdma includes it in the Send
    using a scatter-gather technique. xprtrdma mustn't allow the RPC
    consumer to re-use the memory where that payload resides before the
    Send completes. Otherwise, the new contents of that memory would be
    exposed by an HCA retransmit of the Send operation.
    
    So, block RPC completion on Send completion, but only in the case
    where a separate file data payload is part of the Send. This
    prevents the reuse of that memory while it is still part of a Send
    operation without an undue cost to other cases.
    
    Waiting is avoided in the common case because typically the Send
    will have completed long before the RPC Reply arrives.
    
    These days, an RPC timeout will trigger a disconnect, which tears
    down the QP. The disconnect flushes all waiting Sends. This bounds
    the amount of time the reply handler has to wait for a Send
    completion.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c260475baa36..bccd5d8b9384 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -236,11 +236,13 @@ struct rpcrdma_rep {
 
 /* struct rpcrdma_sendctx - DMA mapped SGEs to unmap after Send completes
  */
+struct rpcrdma_req;
 struct rpcrdma_xprt;
 struct rpcrdma_sendctx {
 	struct ib_send_wr	sc_wr;
 	struct ib_cqe		sc_cqe;
 	struct rpcrdma_xprt	*sc_xprt;
+	struct rpcrdma_req	*sc_req;
 	unsigned int		sc_unmap_count;
 	struct ib_sge		sc_sges[];
 };
@@ -387,6 +389,7 @@ struct rpcrdma_req {
 enum {
 	RPCRDMA_REQ_F_BACKCHANNEL = 0,
 	RPCRDMA_REQ_F_PENDING,
+	RPCRDMA_REQ_F_TX_RESOURCES,
 };
 
 static inline void
@@ -492,6 +495,7 @@ struct rpcrdma_stats {
 	/* accessed when receiving a reply */
 	unsigned long long	total_rdma_reply;
 	unsigned long long	fixup_copy_count;
+	unsigned long		reply_waits_for_send;
 	unsigned long		local_inv_needed;
 	unsigned long		nomsg_call_count;
 	unsigned long		bcall_count;

commit 0ba6f37012db2f88f881cd818aec6e1886f61abb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:28 2017 -0400

    xprtrdma: Refactor rpcrdma_deferred_completion
    
    Invoke a common routine for releasing hardware resources (for
    example, invalidating MRs). This needs to be done whether an
    RPC Reply has arrived or the RPC was terminated early.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 417532069842..c260475baa36 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -386,6 +386,7 @@ struct rpcrdma_req {
 /* rl_flags */
 enum {
 	RPCRDMA_REQ_F_BACKCHANNEL = 0,
+	RPCRDMA_REQ_F_PENDING,
 };
 
 static inline void
@@ -655,6 +656,8 @@ int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);
 void rpcrdma_reply_handler(struct rpcrdma_rep *rep);
+void rpcrdma_release_rqst(struct rpcrdma_xprt *r_xprt,
+			  struct rpcrdma_req *req);
 void rpcrdma_deferred_completion(struct work_struct *work);
 
 static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)

commit 531cca0c9b17c185377fd081b43ffca953cfecad
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:20 2017 -0400

    xprtrdma: Add a field of bit flags to struct rpcrdma_req
    
    We have one boolean flag in rpcrdma_req today. I'd like to add more
    flags, so convert that boolean to a bit flag.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 537cfabe47d1..417532069842 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -377,12 +377,17 @@ struct rpcrdma_req {
 	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
 
 	struct list_head	rl_all;
-	bool			rl_backchannel;
+	unsigned long		rl_flags;
 
 	struct list_head	rl_registered;	/* registered segments */
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 };
 
+/* rl_flags */
+enum {
+	RPCRDMA_REQ_F_BACKCHANNEL = 0,
+};
+
 static inline void
 rpcrdma_set_xprtdata(struct rpc_rqst *rqst, struct rpcrdma_req *req)
 {

commit ae72950abf99fb250aca972b3451b6e06a096c68
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:12 2017 -0400

    xprtrdma: Add data structure to manage RDMA Send arguments
    
    Problem statement:
    
    Recently Sagi Grimberg <sagi@grimberg.me> observed that kernel RDMA-
    enabled storage initiators don't handle delayed Send completion
    correctly. If Send completion is delayed beyond the end of a ULP
    transaction, the ULP may release resources that are still being used
    by the HCA to complete a long-running Send operation.
    
    This is a common design trait amongst our initiators. Most Send
    operations are faster than the ULP transaction they are part of.
    Waiting for a completion for these is typically unnecessary.
    
    Infrequently, a network partition or some other problem crops up
    where an ordering problem can occur. In NFS parlance, the RPC Reply
    arrives and completes the RPC, but the HCA is still retrying the
    Send WR that conveyed the RPC Call. In this case, the HCA can try
    to use memory that has been invalidated or DMA unmapped, and the
    connection is lost. If that memory has been re-used for something
    else (possibly not related to NFS), and the Send retransmission
    exposes that data on the wire.
    
    Thus we cannot assume that it is safe to release Send-related
    resources just because a ULP reply has arrived.
    
    After some analysis, we have determined that the completion
    housekeeping will not be difficult for xprtrdma:
    
     - Inline Send buffers are registered via the local DMA key, and
       are already left DMA mapped for the lifetime of a transport
       connection, thus no additional handling is necessary for those
     - Gathered Sends involving page cache pages _will_ need to
       DMA unmap those pages after the Send completes. But like
       inline send buffers, they are registered via the local DMA key,
       and thus will not need to be invalidated
    
    In addition, RPC completion will need to wait for Send completion
    in the latter case. However, nearly always, the Send that conveys
    the RPC Call will have completed long before the RPC Reply
    arrives, and thus no additional latency will be accrued.
    
    Design notes:
    
    In this patch, the rpcrdma_sendctx object is introduced, and a
    lock-free circular queue is added to manage a set of them per
    transport.
    
    The RPC client's send path already prevents sending more than one
    RPC Call at the same time. This allows us to treat the consumer
    side of the queue (rpcrdma_sendctx_get_locked) as if there is a
    single consumer thread.
    
    The producer side of the queue (rpcrdma_sendctx_put_locked) is
    invoked only from the Send completion handler, which is a single
    thread of execution (soft IRQ).
    
    The only care that needs to be taken is with the tail index, which
    is shared between the producer and consumer. Only the producer
    updates the tail index. The consumer compares the head with the
    tail to ensure that the a sendctx that is in use is never handed
    out again (or, expressed more conventionally, the queue is empty).
    
    When the sendctx queue empties completely, there are enough Sends
    outstanding that posting more Send operations can result in a Send
    Queue overflow. In this case, the ULP is told to wait and try again.
    This introduces strong Send Queue accounting to xprtrdma.
    
    As a final touch, Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    suggested a mechanism that does not require signaling every Send.
    We signal once every N Sends, and perform SGE unmapping of N Send
    operations during that one completion.
    
    Reported-by: Sagi Grimberg <sagi@grimberg.me>
    Suggested-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0b8ca5e5c706..537cfabe47d1 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -93,6 +93,8 @@ enum {
  */
 
 struct rpcrdma_ep {
+	unsigned int		rep_send_count;
+	unsigned int		rep_send_batch;
 	atomic_t		rep_cqcount;
 	int			rep_cqinit;
 	int			rep_connected;
@@ -232,6 +234,27 @@ struct rpcrdma_rep {
 	struct ib_recv_wr	rr_recv_wr;
 };
 
+/* struct rpcrdma_sendctx - DMA mapped SGEs to unmap after Send completes
+ */
+struct rpcrdma_xprt;
+struct rpcrdma_sendctx {
+	struct ib_send_wr	sc_wr;
+	struct ib_cqe		sc_cqe;
+	struct rpcrdma_xprt	*sc_xprt;
+	unsigned int		sc_unmap_count;
+	struct ib_sge		sc_sges[];
+};
+
+/* Limit the number of SGEs that can be unmapped during one
+ * Send completion. This caps the amount of work a single
+ * completion can do before returning to the provider.
+ *
+ * Setting this to zero disables Send completion batching.
+ */
+enum {
+	RPCRDMA_MAX_SEND_BATCH = 7,
+};
+
 /*
  * struct rpcrdma_mw - external memory region metadata
  *
@@ -343,19 +366,16 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
-	unsigned int		rl_mapped_sges;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;
 	struct xdr_stream	rl_stream;
 	struct xdr_buf		rl_hdrbuf;
-	struct ib_send_wr	rl_send_wr;
-	struct ib_sge		rl_send_sge[RPCRDMA_MAX_SEND_SGES];
+	struct rpcrdma_sendctx	*rl_sendctx;
 	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
 	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */
 	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
 
-	struct ib_cqe		rl_cqe;
 	struct list_head	rl_all;
 	bool			rl_backchannel;
 
@@ -402,6 +422,11 @@ struct rpcrdma_buffer {
 	struct list_head	rb_mws;
 	struct list_head	rb_all;
 
+	unsigned long		rb_sc_head;
+	unsigned long		rb_sc_tail;
+	unsigned long		rb_sc_last;
+	struct rpcrdma_sendctx	**rb_sc_ctxs;
+
 	spinlock_t		rb_lock;	/* protect buf lists */
 	int			rb_send_count, rb_recv_count;
 	struct list_head	rb_send_bufs;
@@ -456,6 +481,7 @@ struct rpcrdma_stats {
 	unsigned long		mrs_recovered;
 	unsigned long		mrs_orphaned;
 	unsigned long		mrs_allocated;
+	unsigned long		empty_sendctx_q;
 
 	/* accessed when receiving a reply */
 	unsigned long long	total_rdma_reply;
@@ -557,6 +583,8 @@ struct rpcrdma_rep *rpcrdma_create_rep(struct rpcrdma_xprt *);
 void rpcrdma_destroy_req(struct rpcrdma_req *);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
+struct rpcrdma_sendctx *rpcrdma_sendctx_get_locked(struct rpcrdma_buffer *buf);
+void rpcrdma_sendctx_put_locked(struct rpcrdma_sendctx *sc);
 
 struct rpcrdma_mw *rpcrdma_get_mw(struct rpcrdma_xprt *);
 void rpcrdma_put_mw(struct rpcrdma_xprt *, struct rpcrdma_mw *);
@@ -617,7 +645,7 @@ int rpcrdma_prepare_send_sges(struct rpcrdma_xprt *r_xprt,
 			      struct rpcrdma_req *req, u32 hdrlen,
 			      struct xdr_buf *xdr,
 			      enum rpcrdma_chunktype rtype);
-void rpcrdma_unmap_sges(struct rpcrdma_ia *, struct rpcrdma_req *);
+void rpcrdma_unmap_sendctx(struct rpcrdma_sendctx *sc);
 int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);

commit 857f9acab9343788fe59f7be3a4710131b705db4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:47:55 2017 -0400

    xprtrdma: Change return value of rpcrdma_prepare_send_sges()
    
    Clean up: Make rpcrdma_prepare_send_sges() return a negative errno
    instead of a bool. Soon callers will want distinct treatments of
    different types of failures.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0e0ae6195a5b..0b8ca5e5c706 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -613,8 +613,10 @@ enum rpcrdma_chunktype {
 	rpcrdma_replych
 };
 
-bool rpcrdma_prepare_send_sges(struct rpcrdma_ia *, struct rpcrdma_req *,
-			       u32, struct xdr_buf *, enum rpcrdma_chunktype);
+int rpcrdma_prepare_send_sges(struct rpcrdma_xprt *r_xprt,
+			      struct rpcrdma_req *req, u32 hdrlen,
+			      struct xdr_buf *xdr,
+			      enum rpcrdma_chunktype rtype);
 void rpcrdma_unmap_sges(struct rpcrdma_ia *, struct rpcrdma_req *);
 int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);

commit be798f9082aa54524b209fac2c8164c81cd28f77
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 16 15:01:39 2017 -0400

    xprtrdma: Decode credits field in rpcrdma_reply_handler
    
    We need to decode and save the incoming rdma_credits field _after_
    we know that the direction of the message is "forward direction
    Reply". Otherwise, the credits value in reverse direction Calls is
    also used to update the forward direction credits.
    
    It is safe to decode the rdma_credits field in rpcrdma_reply_handler
    now that rpcrdma_reply_handler is single-threaded. Receives complete
    in the same order as they were sent on the NFS server.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a85bcd19b37a..0e0ae6195a5b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -407,7 +407,7 @@ struct rpcrdma_buffer {
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
 	u32			rb_max_requests;
-	atomic_t		rb_credits;	/* most recent credit grant */
+	u32			rb_credits;	/* most recent credit grant */
 
 	u32			rb_bc_srv_max_requests;
 	spinlock_t		rb_reqslock;	/* protect rb_allreqs */

commit d8f532d20ee43a0117284798d486bc4f98e3b196
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 16 15:01:30 2017 -0400

    xprtrdma: Invoke rpcrdma_reply_handler directly from RECV completion
    
    I noticed that the soft IRQ thread looked pretty busy under heavy
    I/O workloads. perf suggested one area that was expensive was the
    queue_work() call in rpcrdma_wc_receive. That gave me some ideas.
    
    Instead of scheduling a separate worker to process RPC Replies,
    promote the Receive completion handler to IB_POLL_WORKQUEUE, and
    invoke rpcrdma_reply_handler directly.
    
    Note that the poll workqueue is single-threaded. In order to keep
    memory invalidation from serializing all RPC Replies, handle any
    necessary invalidation tasks in a separate multi-threaded workqueue.
    
    This provides a two-tier scheme, similar to OS I/O interrupt
    handlers: A fast interrupt handler that schedules the slow handler
    and re-enables the interrupt, and a slower handler that is invoked
    for any needed heavy lifting.
    
    Benefits include:
    - One less context switch for RPCs that don't register memory
    - Receive completion handling is moved out of soft IRQ context to
      make room for other users of soft IRQ
    - The same CPU core now DMA syncs and XDR decodes the Receive buffer
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d68a1351d95e..a85bcd19b37a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -533,6 +533,8 @@ void rpcrdma_ia_close(struct rpcrdma_ia *);
 bool frwr_is_supported(struct rpcrdma_ia *);
 bool fmr_is_supported(struct rpcrdma_ia *);
 
+extern struct workqueue_struct *rpcrdma_receive_wq;
+
 /*
  * Endpoint calls - xprtrdma/verbs.c
  */
@@ -617,7 +619,8 @@ void rpcrdma_unmap_sges(struct rpcrdma_ia *, struct rpcrdma_req *);
 int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);
-void rpcrdma_reply_handler(struct work_struct *work);
+void rpcrdma_reply_handler(struct rpcrdma_rep *rep);
+void rpcrdma_deferred_completion(struct work_struct *work);
 
 static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
 {

commit e1352c9610e3235f5e1b159038762d0c01c6ef36
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 16 15:01:22 2017 -0400

    xprtrdma: Refactor rpcrdma_reply_handler some more
    
    Clean up: I'd like to be able to invoke the tail of
    rpcrdma_reply_handler in two different places. Split the tail out
    into its own helper function.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 858b4c52047d..d68a1351d95e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -202,18 +202,17 @@ enum {
 };
 
 /*
- * struct rpcrdma_rep -- this structure encapsulates state required to recv
- * and complete a reply, asychronously. It needs several pieces of
- * state:
- *   o recv buffer (posted to provider)
- *   o ib_sge (also donated to provider)
- *   o status of reply (length, success or not)
- *   o bookkeeping state to get run by reply handler (list, etc)
+ * struct rpcrdma_rep -- this structure encapsulates state required
+ * to receive and complete an RPC Reply, asychronously. It needs
+ * several pieces of state:
  *
- * These are allocated during initialization, per-transport instance.
+ *   o receive buffer and ib_sge (donated to provider)
+ *   o status of receive (success or not, length, inv rkey)
+ *   o bookkeeping state to get run by reply handler (XDR stream)
  *
- * N of these are associated with a transport instance, and stored in
- * struct rpcrdma_buffer. N is the max number of outstanding requests.
+ * These structures are allocated during transport initialization.
+ * N of these are associated with a transport instance, managed by
+ * struct rpcrdma_buffer. N is the max number of outstanding RPCs.
  */
 
 struct rpcrdma_rep {
@@ -228,6 +227,7 @@ struct rpcrdma_rep {
 	struct work_struct	rr_work;
 	struct xdr_buf		rr_hdrbuf;
 	struct xdr_stream	rr_stream;
+	struct rpc_rqst		*rr_rqst;
 	struct list_head	rr_list;
 	struct ib_recv_wr	rr_recv_wr;
 };
@@ -616,6 +616,7 @@ bool rpcrdma_prepare_send_sges(struct rpcrdma_ia *, struct rpcrdma_req *,
 void rpcrdma_unmap_sges(struct rpcrdma_ia *, struct rpcrdma_req *);
 int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
+void rpcrdma_complete_rqst(struct rpcrdma_rep *rep);
 void rpcrdma_reply_handler(struct work_struct *work);
 
 static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)

commit 5381e0ec72eeb9467796ac4181ccb7bbce6d3e81
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 16 15:01:14 2017 -0400

    xprtrdma: Move decoded header fields into rpcrdma_rep
    
    Clean up: Make it easier to pass the decoded XID, vers, credits, and
    proc fields around by moving these variables into struct rpcrdma_rep.
    
    Note: the credits field will be handled in a subsequent patch.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 74e017477e72..858b4c52047d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -218,6 +218,9 @@ enum {
 
 struct rpcrdma_rep {
 	struct ib_cqe		rr_cqe;
+	__be32			rr_xid;
+	__be32			rr_vers;
+	__be32			rr_proc;
 	int			rr_wc_flags;
 	u32			rr_inv_rkey;
 	struct rpcrdma_regbuf	*rr_rdmabuf;

commit 2b4f8923ecaafc0c25ee56bc17ea9256d12b747c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 9 12:03:42 2017 -0400

    xprtrdma: Remove ro_unmap_safe
    
    Clean up: There are no remaining callers of this method.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e26a97d2f922..74e017477e72 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -473,8 +473,6 @@ struct rpcrdma_memreg_ops {
 				  struct rpcrdma_mw **);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
 					 struct list_head *);
-	void		(*ro_unmap_safe)(struct rpcrdma_xprt *,
-					 struct rpcrdma_req *, bool);
 	void		(*ro_recover_mr)(struct rpcrdma_mw *);
 	int		(*ro_open)(struct rpcrdma_ia *,
 				   struct rpcrdma_ep *,

commit 9590d083c1bb1419b7992609d1a0a3e3517d3893
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Aug 23 17:05:58 2017 -0400

    xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler
    
    Adopt the use of xprt_pin_rqst to eliminate contention between
    Call-side users of rb_lock and the use of rb_lock in
    rpcrdma_reply_handler.
    
    This replaces the mechanism introduced in 431af645cf66 ("xprtrdma:
    Fix client lock-up after application signal fires").
    
    Use recv_lock to quickly find the completing rqst, pin it, then
    drop the lock. At that point invalidation and pull-up of the Reply
    XDR can be done. Both are often expensive operations.
    
    Finally, take recv_lock again to signal completion to the RPC
    layer. It also protects adjustment of "cwnd".
    
    This greatly reduces the amount of time a lock is held by the
    reply handler. Comparing lock_stat results shows a marked decrease
    in contention on rb_lock and recv_lock.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    [trond.myklebust@primarydata.com: Remove call to rpcrdma_buffer_put() from
       the "out_norqst:" path in rpcrdma_reply_handler.]
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 45dab2475c99..e26a97d2f922 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -340,7 +340,6 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
-	__be32			rl_xid;
 	unsigned int		rl_mapped_sges;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
@@ -404,7 +403,6 @@ struct rpcrdma_buffer {
 	int			rb_send_count, rb_recv_count;
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
-	struct list_head	rb_pending;
 	u32			rb_max_requests;
 	atomic_t		rb_credits;	/* most recent credit grant */
 
@@ -557,34 +555,6 @@ void rpcrdma_destroy_req(struct rpcrdma_req *);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 
-static inline void
-rpcrdma_insert_req(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
-{
-	spin_lock(&buffers->rb_lock);
-	if (list_empty(&req->rl_list))
-		list_add_tail(&req->rl_list, &buffers->rb_pending);
-	spin_unlock(&buffers->rb_lock);
-}
-
-static inline struct rpcrdma_req *
-rpcrdma_lookup_req_locked(struct rpcrdma_buffer *buffers, __be32 xid)
-{
-	struct rpcrdma_req *pos;
-
-	list_for_each_entry(pos, &buffers->rb_pending, rl_list)
-		if (pos->rl_xid == xid)
-			return pos;
-	return NULL;
-}
-
-static inline void
-rpcrdma_remove_req(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
-{
-	spin_lock(&buffers->rb_lock);
-	list_del(&req->rl_list);
-	spin_unlock(&buffers->rb_lock);
-}
-
 struct rpcrdma_mw *rpcrdma_get_mw(struct rpcrdma_xprt *);
 void rpcrdma_put_mw(struct rpcrdma_xprt *, struct rpcrdma_mw *);
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);

commit 67af6f652f9ccad772c48f7c959ad5aa23bdfb40
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Aug 22 11:19:35 2017 -0400

    xprtrdma: Re-arrange struct rx_stats
    
    To reduce false cacheline sharing, separate counters that are likely
    to be accessed in the Call path from those accessed in the Reply
    path.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 546c26405596..45dab2475c99 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -441,24 +441,27 @@ struct rpcrdma_create_data_internal {
  * Statistics for RPCRDMA
  */
 struct rpcrdma_stats {
+	/* accessed when sending a call */
 	unsigned long		read_chunk_count;
 	unsigned long		write_chunk_count;
 	unsigned long		reply_chunk_count;
-
 	unsigned long long	total_rdma_request;
-	unsigned long long	total_rdma_reply;
 
+	/* rarely accessed error counters */
 	unsigned long long	pullup_copy_count;
-	unsigned long long	fixup_copy_count;
 	unsigned long		hardway_register_count;
 	unsigned long		failed_marshal_count;
 	unsigned long		bad_reply_count;
-	unsigned long		nomsg_call_count;
-	unsigned long		bcall_count;
 	unsigned long		mrs_recovered;
 	unsigned long		mrs_orphaned;
 	unsigned long		mrs_allocated;
+
+	/* accessed when receiving a reply */
+	unsigned long long	total_rdma_reply;
+	unsigned long long	fixup_copy_count;
 	unsigned long		local_inv_needed;
+	unsigned long		nomsg_call_count;
+	unsigned long		bcall_count;
 };
 
 /*

commit 6748b0caf82101f1f01208e48f5c4fd3ce76d562
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 14 15:38:30 2017 -0400

    xprtrdma: Remove imul instructions from chunk list encoders
    
    Re-arrange the pointer arithmetic in the chunk list encoders to
    eliminate several more integer multiplication instructions during
    Transport Header encoding.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2af910662928..546c26405596 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -466,7 +466,8 @@ struct rpcrdma_stats {
  */
 struct rpcrdma_xprt;
 struct rpcrdma_memreg_ops {
-	int		(*ro_map)(struct rpcrdma_xprt *,
+	struct rpcrdma_mr_seg *
+			(*ro_map)(struct rpcrdma_xprt *,
 				  struct rpcrdma_mr_seg *, int, bool,
 				  struct rpcrdma_mw **);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,

commit 7a80f3f0ddf1d7814ac44728f56b6fbba5837703
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Aug 10 12:47:28 2017 -0400

    xprtrdma: Set up an xdr_stream in rpcrdma_marshal_req()
    
    Initialize an xdr_stream at the top of rpcrdma_marshal_req(), and
    use it to encode the fixed transport header fields. This xdr_stream
    will be used to encode the chunk lists in a subsequent patch.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 78958e92a0a1..2af910662928 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -345,6 +345,8 @@ struct rpcrdma_req {
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;
+	struct xdr_stream	rl_stream;
+	struct xdr_buf		rl_hdrbuf;
 	struct ib_send_wr	rl_send_wr;
 	struct ib_sge		rl_send_sge[RPCRDMA_MAX_SEND_SGES];
 	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */

commit 09e60641fc58960c9c63a9b6d1f57b194572dafc
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Aug 10 12:47:12 2017 -0400

    xprtrdma: Clean up rpcrdma_marshal_req() synopsis
    
    Clean up: The caller already has rpcrdma_xprt, so pass that directly
    instead. And provide a documenting comment for this critical
    function.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 52e73eaacebb..78958e92a0a1 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -637,7 +637,7 @@ enum rpcrdma_chunktype {
 bool rpcrdma_prepare_send_sges(struct rpcrdma_ia *, struct rpcrdma_req *,
 			       u32, struct xdr_buf *, enum rpcrdma_chunktype);
 void rpcrdma_unmap_sges(struct rpcrdma_ia *, struct rpcrdma_req *);
-int rpcrdma_marshal_req(struct rpc_rqst *);
+int rpcrdma_marshal_req(struct rpcrdma_xprt *r_xprt, struct rpc_rqst *rqst);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 void rpcrdma_reply_handler(struct work_struct *work);
 

commit c1bcb68e39e4d58dbb73ac4a390c32b16185a91b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Aug 3 14:30:52 2017 -0400

    xprtrdma: Clean up XDR decoding in rpcrdma_update_granted_credits()
    
    Clean up: Replace C-structure based XDR decoding for consistency
    with other areas.
    
    struct rpcrdma_rep is rearranged slightly so that the relevant fields
    are in cache when the Receive completion handler is invoked.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d4a897af9d9b..52e73eaacebb 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -220,13 +220,13 @@ struct rpcrdma_rep {
 	struct ib_cqe		rr_cqe;
 	int			rr_wc_flags;
 	u32			rr_inv_rkey;
+	struct rpcrdma_regbuf	*rr_rdmabuf;
 	struct rpcrdma_xprt	*rr_rxprt;
 	struct work_struct	rr_work;
 	struct xdr_buf		rr_hdrbuf;
 	struct xdr_stream	rr_stream;
 	struct list_head	rr_list;
 	struct ib_recv_wr	rr_recv_wr;
-	struct rpcrdma_regbuf	*rr_rdmabuf;
 };
 
 /*

commit e2a671904149c1c0aa438e3cbe7d0e8ad2cf8721
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Aug 3 14:30:44 2017 -0400

    xprtrdma: Remove rpcrdma_rep::rr_len
    
    This field is no longer used outside the Receive completion handler.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 13556ae11eec..d4a897af9d9b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -218,7 +218,6 @@ enum {
 
 struct rpcrdma_rep {
 	struct ib_cqe		rr_cqe;
-	unsigned int		rr_len;
 	int			rr_wc_flags;
 	u32			rr_inv_rkey;
 	struct rpcrdma_xprt	*rr_rxprt;
@@ -230,8 +229,6 @@ struct rpcrdma_rep {
 	struct rpcrdma_regbuf	*rr_rdmabuf;
 };
 
-#define RPCRDMA_BAD_LEN		(~0U)
-
 /*
  * struct rpcrdma_mw - external memory region metadata
  *

commit 96f8778f70d0f5b988146d757a26dcd5d5b44116
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Aug 3 14:30:03 2017 -0400

    xprtrdma: Add xdr_init_decode to rpcrdma_reply_handler()
    
    Transport header decoding deals with untrusted input data, therefore
    decoding this header needs to be hardened.
    
    Adopt the same infrastructure that is used when XDR decoding NFS
    replies. This is slightly more CPU-intensive than the replaced code,
    but we're not adding new atomics, locking, or context switches. The
    cost is manageable.
    
    Start by initializing an xdr_stream in rpcrdma_reply_handler().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b282d3f8cdd8..13556ae11eec 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -223,6 +223,8 @@ struct rpcrdma_rep {
 	u32			rr_inv_rkey;
 	struct rpcrdma_xprt	*rr_rxprt;
 	struct work_struct	rr_work;
+	struct xdr_buf		rr_hdrbuf;
+	struct xdr_stream	rr_stream;
 	struct list_head	rr_list;
 	struct ib_recv_wr	rr_recv_wr;
 	struct rpcrdma_regbuf	*rr_rdmabuf;
@@ -642,6 +644,12 @@ int rpcrdma_marshal_req(struct rpc_rqst *);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 void rpcrdma_reply_handler(struct work_struct *work);
 
+static inline void rpcrdma_set_xdrlen(struct xdr_buf *xdr, size_t len)
+{
+	xdr->head[0].iov_len = len;
+	xdr->len = len;
+}
+
 /* RPC/RDMA module init - xprtrdma/transport.c
  */
 extern unsigned int xprt_rdma_max_inline_read;

commit 431af645cf662652bc43c7a26f87cb40aedb01d9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 8 11:52:20 2017 -0400

    xprtrdma: Fix client lock-up after application signal fires
    
    After a signal, the RPC client aborts synchronous RPCs running on
    behalf of the signaled application.
    
    The server is still executing those RPCs, and will write the results
    back into the client's memory when it's done. By the time the server
    writes the results, that memory is likely being used for other
    purposes. Therefore xprtrdma has to immediately invalidate all
    memory regions used by those aborted RPCs to prevent the server's
    writes from clobbering that re-used memory.
    
    With FMR memory registration, invalidation takes a relatively long
    time. In fact, the invalidation is often still running when the
    server tries to write the results into the memory regions that are
    being invalidated.
    
    This sets up a race between two processes:
    
    1.  After the signal, xprt_rdma_free calls ro_unmap_safe.
    2.  While ro_unmap_safe is still running, the server replies and
        rpcrdma_reply_handler runs, calling ro_unmap_sync.
    
    Both processes invoke ib_unmap_fmr on the same FMR.
    
    The mlx4 driver allows two ib_unmap_fmr calls on the same FMR at
    the same time, but HCAs generally don't tolerate this. Sometimes
    this can result in a system crash.
    
    If the HCA happens to survive, rpcrdma_reply_handler continues. It
    removes the rpc_rqst from rq_list and releases the transport_lock.
    This enables xprt_rdma_free to run in another process, and the
    rpc_rqst is released while rpcrdma_reply_handler is still waiting
    for the ib_unmap_fmr call to finish.
    
    But further down in rpcrdma_reply_handler, the transport_lock is
    taken again, and "rqst" is dereferenced. If "rqst" has already been
    released, this triggers a general protection fault. Since bottom-
    halves are disabled, the system locks up.
    
    Address both issues by reversing the order of the xprt_lookup_rqst
    call and the ro_unmap_sync call. Introduce a separate lookup
    mechanism for rpcrdma_req's to enable calling ro_unmap_sync before
    xprt_lookup_rqst. Now the handler takes the transport_lock once
    and holds it for the XID lookup and RPC completion.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=305
    Fixes: 68791649a725 ('xprtrdma: Invalidate in the RPC reply ... ')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ad918c840fc7..b282d3f8cdd8 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -341,6 +341,7 @@ enum {
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_list;
+	__be32			rl_xid;
 	unsigned int		rl_mapped_sges;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
@@ -402,6 +403,7 @@ struct rpcrdma_buffer {
 	int			rb_send_count, rb_recv_count;
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
+	struct list_head	rb_pending;
 	u32			rb_max_requests;
 	atomic_t		rb_credits;	/* most recent credit grant */
 
@@ -550,6 +552,34 @@ void rpcrdma_destroy_req(struct rpcrdma_req *);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 
+static inline void
+rpcrdma_insert_req(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
+{
+	spin_lock(&buffers->rb_lock);
+	if (list_empty(&req->rl_list))
+		list_add_tail(&req->rl_list, &buffers->rb_pending);
+	spin_unlock(&buffers->rb_lock);
+}
+
+static inline struct rpcrdma_req *
+rpcrdma_lookup_req_locked(struct rpcrdma_buffer *buffers, __be32 xid)
+{
+	struct rpcrdma_req *pos;
+
+	list_for_each_entry(pos, &buffers->rb_pending, rl_list)
+		if (pos->rl_xid == xid)
+			return pos;
+	return NULL;
+}
+
+static inline void
+rpcrdma_remove_req(struct rpcrdma_buffer *buffers, struct rpcrdma_req *req)
+{
+	spin_lock(&buffers->rb_lock);
+	list_del(&req->rl_list);
+	spin_unlock(&buffers->rb_lock);
+}
+
 struct rpcrdma_mw *rpcrdma_get_mw(struct rpcrdma_xprt *);
 void rpcrdma_put_mw(struct rpcrdma_xprt *, struct rpcrdma_mw *);
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);

commit a80d66c9e0d1ac31fa3427340efa0bf79b338023
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 8 11:52:12 2017 -0400

    xprtrdma: Rename rpcrdma_req::rl_free
    
    Clean up: I'm about to use the rl_free field for purposes other than
    a free list. So use a more generic name.
    
    This is a refactoring change only.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=305
    Fixes: 68791649a725 ('xprtrdma: Invalidate in the RPC reply ... ')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1c23117bf1b0..ad918c840fc7 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -340,7 +340,7 @@ enum {
 
 struct rpcrdma_buffer;
 struct rpcrdma_req {
-	struct list_head	rl_free;
+	struct list_head	rl_list;
 	unsigned int		rl_mapped_sges;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;

commit 451d26e151f0792601d10378a608c52304b6a357
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 8 11:52:04 2017 -0400

    xprtrdma: Pass only the list of registered MRs to ro_unmap_sync
    
    There are rare cases where an rpcrdma_req can be re-used (via
    rpcrdma_buffer_put) while the RPC reply handler is still running.
    This is due to a signal firing at just the wrong instant.
    
    Since commit 9d6b04097882 ("xprtrdma: Place registered MWs on a
    per-req list"), rpcrdma_mws are self-contained; ie., they fully
    describe an MR and scatterlist, and no part of that information is
    stored in struct rpcrdma_req.
    
    As part of closing the above race window, pass only the req's list
    of registered MRs to ro_unmap_sync, rather than the rpcrdma_req
    itself.
    
    Some extra transport header sanity checking is removed. Since the
    client depends on its own recollection of what memory had been
    registered, there doesn't seem to be a way to abuse this change.
    
    And, the check was not terribly effective. If the client had sent
    Read chunks, the "list_empty" test is negative in both of the
    removed cases, which are actually looking for Write or Reply
    chunks.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=305
    Fixes: 68791649a725 ('xprtrdma: Invalidate in the RPC reply ... ')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2e027335fcbc..1c23117bf1b0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -467,7 +467,7 @@ struct rpcrdma_memreg_ops {
 				  struct rpcrdma_mr_seg *, int, bool,
 				  struct rpcrdma_mw **);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
-					 struct rpcrdma_req *);
+					 struct list_head *);
 	void		(*ro_unmap_safe)(struct rpcrdma_xprt *,
 					 struct rpcrdma_req *, bool);
 	void		(*ro_recover_mr)(struct rpcrdma_mw *);

commit 4b196dc6fee9ba838ebabf824e294a429c79b27d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 8 11:51:56 2017 -0400

    xprtrdma: Pre-mark remotely invalidated MRs
    
    There are rare cases where an rpcrdma_req and its matched
    rpcrdma_rep can be re-used, via rpcrdma_buffer_put, while the RPC
    reply handler is still using that req. This is typically due to a
    signal firing at just the wrong instant.
    
    As part of closing this race window, avoid using the wrong
    rpcrdma_rep to detect remotely invalidated MRs. Mark MRs as
    invalidated while we are sure the rep is still OK to use.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=305
    Fixes: 68791649a725 ('xprtrdma: Invalidate in the RPC reply ... ')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1d66acf1a723..2e027335fcbc 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -271,6 +271,7 @@ struct rpcrdma_mw {
 	struct scatterlist	*mw_sg;
 	int			mw_nents;
 	enum dma_data_direction	mw_dir;
+	unsigned long		mw_flags;
 	union {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
@@ -282,6 +283,11 @@ struct rpcrdma_mw {
 	struct list_head	mw_all;
 };
 
+/* mw_flags */
+enum {
+	RPCRDMA_MW_F_RI		= 1,
+};
+
 /*
  * struct rpcrdma_req -- structure central to the request/reply sequence.
  *

commit 2be1fce95e5b017dd7d23ca039d58cbefd0221e6
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Apr 11 13:24:07 2017 -0400

    xprtrdma: Remove rpcrdma_buffer::rb_pool
    
    Since commit 1e465fd4ff47 ("xprtrdma: Replace send and receive
    arrays"), this field is no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1c5de1af195b..1d66acf1a723 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -391,7 +391,6 @@ struct rpcrdma_buffer {
 	spinlock_t		rb_mwlock;	/* protect rb_mws list */
 	struct list_head	rb_mws;
 	struct list_head	rb_all;
-	char			*rb_pool;
 
 	spinlock_t		rb_lock;	/* protect buf lists */
 	int			rb_send_count, rb_recv_count;

commit bebd031866caa404c522e91bb6fd0c69be04c707
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Apr 11 13:23:10 2017 -0400

    xprtrdma: Support unplugging an HCA from under an NFS mount
    
    The device driver for the underlying physical device associated
    with an RPC-over-RDMA transport can be removed while RPC-over-RDMA
    transports are still in use (ie, while NFS filesystems are still
    mounted and active). The IB core performs a connection event upcall
    to request that consumers free all RDMA resources associated with
    a transport.
    
    There may be pending RPCs when this occurs. Care must be taken to
    release associated resources without leaving references that can
    trigger a subsequent crash if a signal or soft timeout occurs. We
    rely on the caller of the transport's ->close method to ensure that
    the previous RPC task has invoked xprt_release but the transport
    remains write-locked.
    
    A DEVICE_REMOVE upcall forces a disconnect then sleeps. When ->close
    is invoked, it destroys the transport's H/W resources, then wakes
    the upcall, which completes and allows the core driver unload to
    continue.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=266
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9d58260533fc..1c5de1af195b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -69,6 +69,7 @@ struct rpcrdma_ia {
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
 	struct completion	ri_done;
+	struct completion	ri_remove_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_segs;
 	unsigned int		ri_max_frmr_depth;
@@ -78,10 +79,15 @@ struct rpcrdma_ia {
 	bool			ri_reminv_expected;
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
+	unsigned long		ri_flags;
 	struct ib_qp_attr	ri_qp_attr;
 	struct ib_qp_init_attr	ri_qp_init_attr;
 };
 
+enum {
+	RPCRDMA_IAF_REMOVING = 0,
+};
+
 /*
  * RDMA Endpoint -- one per transport instance
  */
@@ -511,6 +517,7 @@ extern unsigned int xprt_rdma_memreg_strategy;
  * Interface Adapter calls - xprtrdma/verbs.c
  */
 int rpcrdma_ia_open(struct rpcrdma_xprt *xprt, struct sockaddr *addr);
+void rpcrdma_ia_remove(struct rpcrdma_ia *ia);
 void rpcrdma_ia_close(struct rpcrdma_ia *);
 bool frwr_is_supported(struct rpcrdma_ia *);
 bool fmr_is_supported(struct rpcrdma_ia *);

commit 91a10c52975a8c89e146a4f740e64cd147ba8e8a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Apr 11 13:23:02 2017 -0400

    xprtrdma: Use same device when mapping or syncing DMA buffers
    
    When the underlying device driver is reloaded, ia->ri_device will be
    replaced. All cached copies of that device pointer have to be
    updated as well.
    
    Commit 54cbd6b0c6b9 ("xprtrdma: Delay DMA mapping Send and Receive
    buffers") added the rg_device field to each regbuf. As part of
    handling a device removal, rpcrdma_dma_unmap_regbuf is invoked on
    all regbufs for a transport.
    
    Simply calling rpcrdma_dma_map_regbuf for each Receive buffer after
    the driver has been reloaded should reinitialize rg_device correctly
    for every case except rpcrdma_wc_receive, which still uses
    rpcrdma_rep::rr_device.
    
    Ensure the same device that was used to map a Receive buffer is also
    used to sync it in rpcrdma_wc_receive by using rg_device there
    instead of rr_device.
    
    This is the only use of rr_device, so it can be removed.
    
    The use of regbufs in the send path is also updated, for
    completeness.
    
    Fixes: 54cbd6b0c6b9 ("xprtrdma: Delay DMA mapping Send and ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index af844fc30bd4..9d58260533fc 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -164,6 +164,12 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
 	return (struct rpcrdma_msg *)rb->rg_base;
 }
 
+static inline struct ib_device *
+rdmab_device(struct rpcrdma_regbuf *rb)
+{
+	return rb->rg_device;
+}
+
 #define RPCRDMA_DEF_GFP		(GFP_NOIO | __GFP_NOWARN)
 
 /* To ensure a transport can always make forward progress,
@@ -209,7 +215,6 @@ struct rpcrdma_rep {
 	unsigned int		rr_len;
 	int			rr_wc_flags;
 	u32			rr_inv_rkey;
-	struct ib_device	*rr_device;
 	struct rpcrdma_xprt	*rr_rxprt;
 	struct work_struct	rr_work;
 	struct list_head	rr_list;

commit fff09594edf5e9b8595a2cefdc07e54b70f81729
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Apr 11 13:22:54 2017 -0400

    xprtrdma: Refactor rpcrdma_ia_open()
    
    In order to unload a device driver and reload it, xprtrdma will need
    to close a transport's interface adapter, and then call
    rpcrdma_ia_open again, possibly finding a different interface
    adapter.
    
    Make rpcrdma_ia_open safe to call on the same transport multiple
    times.
    
    This is a refactoring change only.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 171a35116de9..af844fc30bd4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -497,10 +497,15 @@ struct rpcrdma_xprt {
  * Default is 0, see sysctl entry and rpc_rdma.c rpcrdma_convert_iovs() */
 extern int xprt_rdma_pad_optimize;
 
+/* This setting controls the hunt for a supported memory
+ * registration strategy.
+ */
+extern unsigned int xprt_rdma_memreg_strategy;
+
 /*
  * Interface Adapter calls - xprtrdma/verbs.c
  */
-int rpcrdma_ia_open(struct rpcrdma_xprt *, struct sockaddr *, int);
+int rpcrdma_ia_open(struct rpcrdma_xprt *xprt, struct sockaddr *addr);
 void rpcrdma_ia_close(struct rpcrdma_ia *);
 bool frwr_is_supported(struct rpcrdma_ia *);
 bool fmr_is_supported(struct rpcrdma_ia *);

commit 9a5c63e9c4056de8a73555131e6f698ddb0b9e0d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 8 17:00:43 2017 -0500

    xprtrdma: Refactor management of mw_list field
    
    Clean up some duplicate code.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 852dd0a750a5..171a35116de9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -354,6 +354,22 @@ rpcr_to_rdmar(struct rpc_rqst *rqst)
 	return rqst->rq_xprtdata;
 }
 
+static inline void
+rpcrdma_push_mw(struct rpcrdma_mw *mw, struct list_head *list)
+{
+	list_add_tail(&mw->mw_list, list);
+}
+
+static inline struct rpcrdma_mw *
+rpcrdma_pop_mw(struct list_head *list)
+{
+	struct rpcrdma_mw *mw;
+
+	mw = list_first_entry(list, struct rpcrdma_mw, mw_list);
+	list_del(&mw->mw_list);
+	return mw;
+}
+
 /*
  * struct rpcrdma_buffer -- holds list/queue of pre-registered memory for
  * inline requests/replies, and client/server credits.

commit c6f5b47f9fdeef12c0896e5af4bb3416c97d91c4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 8 17:00:18 2017 -0500

    xprtrdma: Shrink send SGEs array
    
    We no longer need to accommodate an xdr_buf whose pages start at an
    offset and cross extra page boundaries. If there are more partial or
    whole pages to send than there are available SGEs, the marshaling
    logic is now smart enough to use a Read chunk instead of failing.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3d7e9c9bad1f..852dd0a750a5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -305,16 +305,19 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 	char		*mr_offset;	/* kva if no page, else offset */
 };
 
-/* Reserve enough Send SGEs to send a maximum size inline request:
+/* The Send SGE array is provisioned to send a maximum size
+ * inline request:
  * - RPC-over-RDMA header
  * - xdr_buf head iovec
- * - RPCRDMA_MAX_INLINE bytes, possibly unaligned, in pages
+ * - RPCRDMA_MAX_INLINE bytes, in pages
  * - xdr_buf tail iovec
+ *
+ * The actual number of array elements consumed by each RPC
+ * depends on the device's max_sge limit.
  */
 enum {
 	RPCRDMA_MIN_SEND_SGES = 3,
-	RPCRDMA_MAX_SEND_PAGES = PAGE_SIZE + RPCRDMA_MAX_INLINE - 1,
-	RPCRDMA_MAX_PAGE_SGES = (RPCRDMA_MAX_SEND_PAGES >> PAGE_SHIFT) + 1,
+	RPCRDMA_MAX_PAGE_SGES = RPCRDMA_MAX_INLINE >> PAGE_SHIFT,
 	RPCRDMA_MAX_SEND_SGES = 1 + 1 + RPCRDMA_MAX_PAGE_SGES + 1,
 };
 

commit 16f906d66cd76fb9895cbc628f447532a7ac1faa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 8 17:00:10 2017 -0500

    xprtrdma: Reduce required number of send SGEs
    
    The MAX_SEND_SGES check introduced in commit 655fec6987be
    ("xprtrdma: Use gathered Send for large inline messages") fails
    for devices that have a small max_sge.
    
    Instead of checking for a large fixed maximum number of SGEs,
    check for a minimum small number. RPC-over-RDMA will switch to
    using a Read chunk if an xdr_buf has more pages than can fit in
    the device's max_sge limit. This is considerably better than
    failing all together to mount the server.
    
    This fix supports devices that have as few as three send SGEs
    available.
    
    Reported-by: Selvin Xavier <selvin.xavier@broadcom.com>
    Reported-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Reported-by: Honggang Li <honli@redhat.com>
    Reported-by: Ram Amrani <Ram.Amrani@cavium.com>
    Fixes: 655fec6987be ("xprtrdma: Use gathered Send for large ...")
    Cc: stable@vger.kernel.org # v4.9+
    Tested-by: Honggang Li <honli@redhat.com>
    Tested-by: Ram Amrani <Ram.Amrani@cavium.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c13715431419..3d7e9c9bad1f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -74,6 +74,7 @@ struct rpcrdma_ia {
 	unsigned int		ri_max_frmr_depth;
 	unsigned int		ri_max_inline_write;
 	unsigned int		ri_max_inline_read;
+	unsigned int		ri_max_send_sges;
 	bool			ri_reminv_expected;
 	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
@@ -311,6 +312,7 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
  * - xdr_buf tail iovec
  */
 enum {
+	RPCRDMA_MIN_SEND_SGES = 3,
 	RPCRDMA_MAX_SEND_PAGES = PAGE_SIZE + RPCRDMA_MAX_INLINE - 1,
 	RPCRDMA_MAX_PAGE_SGES = (RPCRDMA_MAX_SEND_PAGES >> PAGE_SHIFT) + 1,
 	RPCRDMA_MAX_SEND_SGES = 1 + 1 + RPCRDMA_MAX_PAGE_SGES + 1,

commit b5f0afbea4f2ea52c613ac2b06cb6de2ea18cb6d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 8 16:59:54 2017 -0500

    xprtrdma: Per-connection pad optimization
    
    Pad optimization is changed by echoing into
    /proc/sys/sunrpc/rdma_pad_optimize. This is a global setting,
    affecting all RPC-over-RDMA connections to all servers.
    
    The marshaling code picks up that value and uses it for decisions
    about how to construct each RPC-over-RDMA frame. Having it change
    suddenly in mid-operation can result in unexpected failures. And
    some servers a client mounts might need chunk round-up, while
    others don't.
    
    So instead, copy the pad_optimize setting into each connection's
    rpcrdma_ia when the transport is created, and use the copy, which
    can't change during the life of the connection, instead.
    
    This also removes a hack: rpcrdma_convert_iovs was using
    the remote-invalidation-expected flag to predict when it could leave
    out Write chunk padding. This is because the Linux server handles
    implicit XDR padding on Write chunks correctly, and only Linux
    servers can set the connection's remote-invalidation-expected flag.
    
    It's more sensible to use the pad optimization setting instead.
    
    Fixes: 677eb17e94ed ("xprtrdma: Fix XDR tail buffer marshalling")
    Cc: stable@vger.kernel.org # v4.9+
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e35efd4ac1e4..c13715431419 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -75,6 +75,7 @@ struct rpcrdma_ia {
 	unsigned int		ri_max_inline_write;
 	unsigned int		ri_max_inline_read;
 	bool			ri_reminv_expected;
+	bool			ri_implicit_roundup;
 	enum ib_mr_type		ri_mrtype;
 	struct ib_qp_attr	ri_qp_attr;
 	struct ib_qp_init_attr	ri_qp_init_attr;

commit 3a72dc771cc38e4d6e441a86256a3d7788a84c01
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 10:53:37 2016 -0500

    xprtrdma: Relocate connection helper functions
    
    Clean up: Disentangle connection helpers from RPC-over-RDMA reply
    decoding functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b8424fa47d29..e35efd4ac1e4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -490,6 +490,7 @@ int rpcrdma_ep_create(struct rpcrdma_ep *, struct rpcrdma_ia *,
 				struct rpcrdma_create_data_internal *);
 void rpcrdma_ep_destroy(struct rpcrdma_ep *, struct rpcrdma_ia *);
 int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
+void rpcrdma_conn_func(struct rpcrdma_ep *ep);
 void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
@@ -548,13 +549,6 @@ rpcrdma_data_dir(bool writing)
 	return writing ? DMA_FROM_DEVICE : DMA_TO_DEVICE;
 }
 
-/*
- * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c
- */
-void rpcrdma_connect_worker(struct work_struct *);
-void rpcrdma_conn_func(struct rpcrdma_ep *);
-void rpcrdma_reply_handler(struct work_struct *);
-
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
@@ -572,12 +566,14 @@ bool rpcrdma_prepare_send_sges(struct rpcrdma_ia *, struct rpcrdma_req *,
 void rpcrdma_unmap_sges(struct rpcrdma_ia *, struct rpcrdma_req *);
 int rpcrdma_marshal_req(struct rpc_rqst *);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
+void rpcrdma_reply_handler(struct work_struct *work);
 
 /* RPC/RDMA module init - xprtrdma/transport.c
  */
 extern unsigned int xprt_rdma_max_inline_read;
 void xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap);
 void xprt_rdma_free_addresses(struct rpc_xprt *xprt);
+void rpcrdma_connect_worker(struct work_struct *work);
 void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq);
 int xprt_rdma_init(void);
 void xprt_rdma_cleanup(void);

commit 5e9fc6a06bba9e6821ce964067fcf4401496bc29
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 10:52:24 2016 -0500

    xprtrdma: Support for SG_GAP devices
    
    Some devices (such as the Mellanox CX-4) can register, under a
    single R_key, a set of memory regions that are not contiguous. When
    this is done, all the segments in a Reply list, say, can then be
    invalidated in a single LocalInv Work Request (or via Remote
    Invalidation, which can invalidate exactly one R_key when completing
    a Receive).
    
    This means a single FastReg WR is used to register, and one or zero
    LocalInv WRs can invalidate, the memory involved with RDMA transfers
    on behalf of an RPC.
    
    In addition, xprtrdma constructs some Reply chunks from three or
    more segments. By registering them with SG_GAP, only one segment
    is needed for the Reply chunk, allowing the whole chunk to be
    invalidated remotely.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f6ae1b22da47..b8424fa47d29 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -75,6 +75,7 @@ struct rpcrdma_ia {
 	unsigned int		ri_max_inline_write;
 	unsigned int		ri_max_inline_read;
 	bool			ri_reminv_expected;
+	enum ib_mr_type		ri_mrtype;
 	struct ib_qp_attr	ri_qp_attr;
 	struct ib_qp_init_attr	ri_qp_init_attr;
 };

commit 8d38de65644d900199f035277aa5f3da4aa9fc17
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 10:52:16 2016 -0500

    xprtrdma: Make FRWR send queue entry accounting more accurate
    
    Verbs providers may perform house-keeping on the Send Queue during
    each signaled send completion. It is necessary therefore for a verbs
    consumer (like xprtrdma) to occasionally force a signaled send
    completion if it runs unsignaled most of the time.
    
    xprtrdma does not require signaled completions for Send or FastReg
    Work Requests, but does signal some LocalInv Work Requests. To
    ensure that Send Queue house-keeping can run before the Send Queue
    is more than half-consumed, xprtrdma forces a signaled completion
    on occasion by counting the number of Send Queue Entries it
    consumes. It currently does this by counting each ib_post_send as
    one Entry.
    
    Commit c9918ff56dfb ("xprtrdma: Add ro_unmap_sync method for FRWR")
    introduced the ability for frwr_op_unmap_sync to post more than one
    Work Request with a single post_send. Thus the underlying assumption
    of one Send Queue Entry per ib_post_send is no longer true.
    
    Also, FastReg Work Requests are currently never signaled. They
    should be signaled once in a while, just as Send is, to keep the
    accounting of consumed SQEs accurate.
    
    While we're here, convert the CQCOUNT macros to the currently
    preferred kernel coding style, which is inline functions.
    
    Fixes: c9918ff56dfb ("xprtrdma: Add ro_unmap_sync method for FRWR")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 6e1bba358203..f6ae1b22da47 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -95,8 +95,24 @@ struct rpcrdma_ep {
 	struct delayed_work	rep_connect_worker;
 };
 
-#define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
-#define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
+static inline void
+rpcrdma_init_cqcount(struct rpcrdma_ep *ep, int count)
+{
+	atomic_set(&ep->rep_cqcount, ep->rep_cqinit - count);
+}
+
+/* To update send queue accounting, provider must take a
+ * send completion every now and then.
+ */
+static inline void
+rpcrdma_set_signaled(struct rpcrdma_ep *ep, struct ib_send_wr *send_wr)
+{
+	send_wr->send_flags = 0;
+	if (unlikely(atomic_sub_return(1, &ep->rep_cqcount) <= 0)) {
+		rpcrdma_init_cqcount(ep, 0);
+		send_wr->send_flags = IB_SEND_SIGNALED;
+	}
+}
 
 /* Pre-allocate extra Work Requests for handling backward receives
  * and sends. This is a fixed value because the Work Queues are

commit 62bdf94a2049822ef8c6d4b0e83cd9c3a1663ab4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Nov 7 16:16:24 2016 -0500

    xprtrdma: Fix DMAR failure in frwr_op_map() after reconnect
    
    When a LOCALINV WR is flushed, the frmr is marked STALE, then
    frwr_op_unmap_sync DMA-unmaps the frmr's SGL. These STALE frmrs
    are then recovered when frwr_op_map hunts for an INVALID frmr to
    use.
    
    All other cases that need frmr recovery leave that SGL DMA-mapped.
    The FRMR recovery path unconditionally DMA-unmaps the frmr's SGL.
    
    To avoid DMA unmapping the SGL twice for flushed LOCAL_INV WRs,
    alter the recovery logic (rather than the hot frwr_op_unmap_sync
    path) to distinguish among these cases. This solution also takes
    care of the case where multiple LOCAL_INV WRs are issued for the
    same rpcrdma_req, some complete successfully, but some are flushed.
    
    Reported-by: Vasco Steinmetz <linux@kyberraum.net>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Vasco Steinmetz <linux@kyberraum.net>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0d35b761c883..6e1bba358203 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -216,7 +216,8 @@ struct rpcrdma_rep {
 enum rpcrdma_frmr_state {
 	FRMR_IS_INVALID,	/* ready to be used */
 	FRMR_IS_VALID,		/* in use */
-	FRMR_IS_STALE,		/* failed completion */
+	FRMR_FLUSHED_FR,	/* flushed FASTREG WR */
+	FRMR_FLUSHED_LI,	/* flushed LOCALINV WR */
 };
 
 struct rpcrdma_frmr {

commit 496b77a5c5ce8cd36b5fb78b8811f015643a6541
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:57:57 2016 -0400

    xprtrdma: Eliminate rpcrdma_receive_worker()
    
    Clean up: the extra layer of indirection doesn't add value.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b2823d9b79ae..0d35b761c883 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -535,7 +535,7 @@ rpcrdma_data_dir(bool writing)
  */
 void rpcrdma_connect_worker(struct work_struct *);
 void rpcrdma_conn_func(struct rpcrdma_ep *);
-void rpcrdma_reply_handler(struct rpcrdma_rep *);
+void rpcrdma_reply_handler(struct work_struct *);
 
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c

commit 655fec6987be05964e70c2e2efcbb253710e282f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:57:24 2016 -0400

    xprtrdma: Use gathered Send for large inline messages
    
    An RPC Call message that is sent inline but that has a data payload
    (ie, one or more items in rq_snd_buf's page list) must be "pulled
    up:"
    
    - call_allocate has to reserve enough RPC Call buffer space to
    accommodate the data payload
    
    - call_transmit has to memcopy the rq_snd_buf's page list and tail
    into its head iovec before it is sent
    
    As the inline threshold is increased beyond its current 1KB default,
    however, this means data payloads of more than a few KB are copied
    by the host CPU. For example, if the inline threshold is increased
    just to 4KB, then NFS WRITE requests up to 4KB would involve a
    memcpy of the NFS WRITE's payload data into the RPC Call buffer.
    This is an undesirable amount of participation by the host CPU.
    
    The inline threshold may be much larger than 4KB in the future,
    after negotiation with a peer server.
    
    Instead of copying the components of rq_snd_buf into its head iovec,
    construct a gather list of these components, and send them all in
    place. The same approach is already used in the Linux server's
    RPC-over-RDMA reply path.
    
    This mechanism also eliminates the need for rpcrdma_tail_pullup,
    which is used to manage the XDR pad and trailing inline content when
    a Read list is present.
    
    This requires that the pages in rq_snd_buf's page list be DMA-mapped
    during marshaling, and unmapped when a data-bearing RPC is
    completed. This is slightly less efficient for very small I/O
    payloads, but significantly more efficient as data payload size and
    inline threshold increase past a kilobyte.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 64b4e2257478..b2823d9b79ae 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -285,16 +285,27 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 	char		*mr_offset;	/* kva if no page, else offset */
 };
 
-#define RPCRDMA_MAX_IOVS	(2)
+/* Reserve enough Send SGEs to send a maximum size inline request:
+ * - RPC-over-RDMA header
+ * - xdr_buf head iovec
+ * - RPCRDMA_MAX_INLINE bytes, possibly unaligned, in pages
+ * - xdr_buf tail iovec
+ */
+enum {
+	RPCRDMA_MAX_SEND_PAGES = PAGE_SIZE + RPCRDMA_MAX_INLINE - 1,
+	RPCRDMA_MAX_PAGE_SGES = (RPCRDMA_MAX_SEND_PAGES >> PAGE_SHIFT) + 1,
+	RPCRDMA_MAX_SEND_SGES = 1 + 1 + RPCRDMA_MAX_PAGE_SGES + 1,
+};
 
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_free;
+	unsigned int		rl_mapped_sges;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;
 	struct ib_send_wr	rl_send_wr;
-	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
+	struct ib_sge		rl_send_sge[RPCRDMA_MAX_SEND_SGES];
 	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
 	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */
 	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
@@ -529,6 +540,18 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
+
+enum rpcrdma_chunktype {
+	rpcrdma_noch = 0,
+	rpcrdma_readch,
+	rpcrdma_areadch,
+	rpcrdma_writech,
+	rpcrdma_replych
+};
+
+bool rpcrdma_prepare_send_sges(struct rpcrdma_ia *, struct rpcrdma_req *,
+			       u32, struct xdr_buf *, enum rpcrdma_chunktype);
+void rpcrdma_unmap_sges(struct rpcrdma_ia *, struct rpcrdma_req *);
 int rpcrdma_marshal_req(struct rpc_rqst *);
 void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 

commit c8b920bb49939a5c6cf1d2d819300f318ea050d2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:57:16 2016 -0400

    xprtrdma: Basic support for Remote Invalidation
    
    Have frwr's ro_unmap_sync recognize an invalidated rkey that appears
    as part of a Receive completion. Local invalidation can be skipped
    for that rkey.
    
    Use an out-of-band signaling mechanism to indicate to the server
    that the client is prepared to receive RDMA Send With Invalidate.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 89df1680b1eb..64b4e2257478 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -74,6 +74,7 @@ struct rpcrdma_ia {
 	unsigned int		ri_max_frmr_depth;
 	unsigned int		ri_max_inline_write;
 	unsigned int		ri_max_inline_read;
+	bool			ri_reminv_expected;
 	struct ib_qp_attr	ri_qp_attr;
 	struct ib_qp_init_attr	ri_qp_init_attr;
 };
@@ -187,6 +188,8 @@ enum {
 struct rpcrdma_rep {
 	struct ib_cqe		rr_cqe;
 	unsigned int		rr_len;
+	int			rr_wc_flags;
+	u32			rr_inv_rkey;
 	struct ib_device	*rr_device;
 	struct rpcrdma_xprt	*rr_rxprt;
 	struct work_struct	rr_work;
@@ -385,6 +388,7 @@ struct rpcrdma_stats {
 	unsigned long		mrs_recovered;
 	unsigned long		mrs_orphaned;
 	unsigned long		mrs_allocated;
+	unsigned long		local_inv_needed;
 };
 
 /*
@@ -408,6 +412,7 @@ struct rpcrdma_memreg_ops {
 				      struct rpcrdma_mw *);
 	void		(*ro_release_mr)(struct rpcrdma_mw *);
 	const char	*ro_displayname;
+	const int	ro_send_w_inv_ok;
 };
 
 extern const struct rpcrdma_memreg_ops rpcrdma_fmr_memreg_ops;

commit 87cfb9a0c85ce4a0c96a4f3d692a85519b933ade
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:57:07 2016 -0400

    xprtrdma: Client-side support for rpcrdma_connect_private
    
    Send an RDMA-CM private message on connect, and look for one during
    a connection-established event.
    
    Both sides can communicate their various implementation limits.
    Implementations that don't support this sideband protocol ignore it.
    
    Once the client knows the server's inline threshold maxima, it can
    adjust the use of Reply chunks, and eliminate most use of Position
    Zero Read chunks. Moderately-sized I/O can be done using a pure
    inline RDMA Send instead of RDMA operations that require memory
    registration.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9aabca68c49d..89df1680b1eb 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -70,6 +70,7 @@ struct rpcrdma_ia {
 	struct ib_pd		*ri_pd;
 	struct completion	ri_done;
 	int			ri_async_rc;
+	unsigned int		ri_max_segs;
 	unsigned int		ri_max_frmr_depth;
 	unsigned int		ri_max_inline_write;
 	unsigned int		ri_max_inline_read;
@@ -87,6 +88,7 @@ struct rpcrdma_ep {
 	int			rep_connected;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
+	struct rpcrdma_connect_private	rep_cm_private;
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;
@@ -523,9 +525,7 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
 int rpcrdma_marshal_req(struct rpc_rqst *);
-void rpcrdma_set_max_header_sizes(struct rpcrdma_ia *,
-				  struct rpcrdma_create_data_internal *,
-				  unsigned int);
+void rpcrdma_set_max_header_sizes(struct rpcrdma_xprt *);
 
 /* RPC/RDMA module init - xprtrdma/transport.c
  */

commit 6ea8e71150ecdc235fab31f76ed9953d82313923
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:51 2016 -0400

    xprtrdma: Move recv_wr to struct rpcrdma_rep
    
    Clean up: The fields in the recv_wr do not vary. There is no need to
    initialize them before each ib_post_recv(). This removes a large-ish
    data structure from the stack.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3c5a89a4ff4f..9aabca68c49d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -189,6 +189,7 @@ struct rpcrdma_rep {
 	struct rpcrdma_xprt	*rr_rxprt;
 	struct work_struct	rr_work;
 	struct list_head	rr_list;
+	struct ib_recv_wr	rr_recv_wr;
 	struct rpcrdma_regbuf	*rr_rdmabuf;
 };
 

commit 90aab6029606152d3d7ea91b41064580f77d7d19
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:43 2016 -0400

    xprtrdma: Move send_wr to struct rpcrdma_req
    
    Clean up: Most of the fields in each send_wr do not vary. There is
    no need to initialize them before each ib_post_send(). This removes
    a large-ish data structure from the stack.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index decd13417ac2..3c5a89a4ff4f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -284,10 +284,10 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_free;
-	unsigned int		rl_niovs;
 	unsigned int		rl_connect_cookie;
 	struct rpcrdma_buffer	*rl_buffer;
-	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
+	struct rpcrdma_rep	*rl_reply;
+	struct ib_send_wr	rl_send_wr;
 	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
 	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
 	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */

commit b157380af1941a43f3cfa244db1018f717031a42
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:35 2016 -0400

    xprtrdma: Simplify rpcrdma_ep_post_recv()
    
    Clean up.
    
    Since commit fc66448549bb ("xprtrdma: Split the completion queue"),
    rpcrdma_ep_post_recv() no longer uses the "ep" argument.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 4875af772735..decd13417ac2 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -457,8 +457,7 @@ void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 				struct rpcrdma_req *);
-int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_ep *,
-				struct rpcrdma_rep *);
+int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_rep *);
 
 /*
  * Buffer calls - xprtrdma/verbs.c

commit 13650c23f10603154d989cff70b5c8a889e69fc2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:26 2016 -0400

    xprtrdma: Eliminate "ia" argument in rpcrdma_{alloc, free}_regbuf
    
    Clean up. The "ia" argument is no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d37ee24d2534..4875af772735 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -465,7 +465,7 @@ int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_ep *,
  */
 struct rpcrdma_req *rpcrdma_create_req(struct rpcrdma_xprt *);
 struct rpcrdma_rep *rpcrdma_create_rep(struct rpcrdma_xprt *);
-void rpcrdma_destroy_req(struct rpcrdma_ia *, struct rpcrdma_req *);
+void rpcrdma_destroy_req(struct rpcrdma_req *);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 
@@ -478,12 +478,10 @@ void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
 void rpcrdma_defer_mr_recovery(struct rpcrdma_mw *);
 
-struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
-					    size_t, enum dma_data_direction,
+struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(size_t, enum dma_data_direction,
 					    gfp_t);
 bool __rpcrdma_dma_map_regbuf(struct rpcrdma_ia *, struct rpcrdma_regbuf *);
-void rpcrdma_free_regbuf(struct rpcrdma_ia *,
-			 struct rpcrdma_regbuf *);
+void rpcrdma_free_regbuf(struct rpcrdma_regbuf *);
 
 static inline bool
 rpcrdma_regbuf_is_mapped(struct rpcrdma_regbuf *rb)

commit 54cbd6b0c6b9410782da3efe7377d43bb636faaf
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:18 2016 -0400

    xprtrdma: Delay DMA mapping Send and Receive buffers
    
    Currently, each regbuf is allocated and DMA mapped at the same time.
    This is done during transport creation.
    
    When a device driver is unloaded, every DMA-mapped buffer in use by
    a transport has to be unmapped, and then remapped to the new
    device if the driver is loaded again. Remapping will have to be done
    _after_ the connect worker has set up the new device.
    
    But there's an ordering problem:
    
    call_allocate, which invokes xprt_rdma_allocate which calls
    rpcrdma_alloc_regbuf to allocate Send buffers, happens _before_
    the connect worker can run to set up the new device.
    
    Instead, at transport creation, allocate each buffer, but leave it
    unmapped. Once the RPC carries these buffers into ->send_request, by
    which time a transport connection should have been established,
    check to see that the RPC's buffers have been DMA mapped. If not,
    map them there.
    
    When device driver unplug support is added, it will simply unmap all
    the transport's regbufs, but it doesn't have to deallocate the
    underlying memory.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9569b212be54..d37ee24d2534 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -113,6 +113,7 @@ struct rpcrdma_ep {
 
 struct rpcrdma_regbuf {
 	struct ib_sge		rg_iov;
+	struct ib_device	*rg_device;
 	enum dma_data_direction	rg_direction;
 	__be32			rg_base[0] __attribute__ ((aligned(256)));
 };
@@ -480,9 +481,24 @@ void rpcrdma_defer_mr_recovery(struct rpcrdma_mw *);
 struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
 					    size_t, enum dma_data_direction,
 					    gfp_t);
+bool __rpcrdma_dma_map_regbuf(struct rpcrdma_ia *, struct rpcrdma_regbuf *);
 void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 			 struct rpcrdma_regbuf *);
 
+static inline bool
+rpcrdma_regbuf_is_mapped(struct rpcrdma_regbuf *rb)
+{
+	return rb->rg_device != NULL;
+}
+
+static inline bool
+rpcrdma_dma_map_regbuf(struct rpcrdma_ia *ia, struct rpcrdma_regbuf *rb)
+{
+	if (likely(rpcrdma_regbuf_is_mapped(rb)))
+		return true;
+	return __rpcrdma_dma_map_regbuf(ia, rb);
+}
+
 int rpcrdma_ep_post_extra_recv(struct rpcrdma_xprt *, unsigned int);
 
 int rpcrdma_alloc_wq(void);

commit 99ef4db329f1ee2413dad49346e72a6c902474d1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:10 2016 -0400

    xprtrdma: Replace DMA_BIDIRECTIONAL
    
    The use of DMA_BIDIRECTIONAL is discouraged by DMA-API.txt.
    Fortunately, xprtrdma now knows which direction I/O is going as
    soon as it allocates each regbuf.
    
    The RPC Call and Reply buffers are no longer the same regbuf. They
    can each be labeled correctly now. The RPC Reply buffer is never
    part of either a Send or Receive WR, but it can be part of Reply
    chunk, which is mapped and registered via ->ro_map . So it is not
    DMA mapped when it is allocated (DMA_NONE), to avoid a double-
    mapping.
    
    Since Receive buffers are no longer DMA_BIDIRECTIONAL and their
    contents are never modified by the host CPU, DMA-API-HOWTO.txt
    suggests that a DMA sync before posting each buffer should be
    unnecessary. (See my_card_interrupt_handler).
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cc426b165c09..9569b212be54 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -113,6 +113,7 @@ struct rpcrdma_ep {
 
 struct rpcrdma_regbuf {
 	struct ib_sge		rg_iov;
+	enum dma_data_direction	rg_direction;
 	__be32			rg_base[0] __attribute__ ((aligned(256)));
 };
 
@@ -477,7 +478,8 @@ void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 void rpcrdma_defer_mr_recovery(struct rpcrdma_mw *);
 
 struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
-					    size_t, gfp_t);
+					    size_t, enum dma_data_direction,
+					    gfp_t);
 void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 			 struct rpcrdma_regbuf *);
 

commit 08cf2efd5423121985af5962d66e6db14dff4130
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:02 2016 -0400

    xprtrdma: Use smaller buffers for RPC-over-RDMA headers
    
    Commit 949317464bc2 ("xprtrdma: Limit number of RDMA segments in
    RPC-over-RDMA headers") capped the number of chunks that may appear
    in RPC-over-RDMA headers. The maximum header size can be estimated
    and fixed to avoid allocating buffer space that is never used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 444f6370d46c..cc426b165c09 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -160,7 +160,10 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
  * The smallest inline threshold is 1024 bytes, ensuring that
  * at least 750 bytes are available for RPC messages.
  */
-#define RPCRDMA_MAX_HDR_SEGS	(8)
+enum {
+	RPCRDMA_MAX_HDR_SEGS = 8,
+	RPCRDMA_HDRBUF_SIZE = 256,
+};
 
 /*
  * struct rpcrdma_rep -- this structure encapsulates state required to recv

commit 9c40c49f145f8999ecbf81683aeb31d92b61b966
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:53 2016 -0400

    xprtrdma: Initialize separate RPC call and reply buffers
    
    RPC-over-RDMA needs to separate its RPC call and reply buffers.
    
     o When an RPC Call is sent, rq_snd_buf is DMA mapped for an RDMA
       Send operation using DMA_TO_DEVICE
    
     o If the client expects a large RPC reply, it DMA maps rq_rcv_buf
       as part of a Reply chunk using DMA_FROM_DEVICE
    
    The two mappings are for data movement in opposite directions.
    
    DMA-API.txt suggests that if these mappings share a DMA cacheline,
    bad things can happen. This could occur in the final bytes of
    rq_snd_buf and the first bytes of rq_rcv_buf if the two buffers
    happen to share a DMA cacheline.
    
    On x86_64 the cacheline size is typically 8 bytes, and RPC call
    messages are usually much smaller than the send buffer, so this
    hasn't been a noticeable problem. But the DMA cacheline size can be
    larger on other platforms.
    
    Also, often rq_rcv_buf starts most of the way into a page, thus
    an additional RDMA segment is needed to map and register the end of
    that buffer. Try to avoid that scenario to reduce the cost of
    registering and invalidating Reply chunks.
    
    Instead of carrying a single regbuf that covers both rq_snd_buf and
    rq_rcv_buf, each struct rpcrdma_req now carries one regbuf for
    rq_snd_buf and one regbuf for rq_rcv_buf.
    
    Some incidental changes worth noting:
    
    - To clear out some spaghetti, refactor xprt_rdma_allocate.
    - The value stored in rg_size is the same as the value stored in
      the iov.length field, so eliminate rg_size
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 484855eddb85..444f6370d46c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -112,7 +112,6 @@ struct rpcrdma_ep {
  */
 
 struct rpcrdma_regbuf {
-	size_t			rg_size;
 	struct ib_sge		rg_iov;
 	__be32			rg_base[0] __attribute__ ((aligned(256)));
 };
@@ -285,8 +284,9 @@ struct rpcrdma_req {
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
-	struct rpcrdma_regbuf	*rl_rdmabuf;
-	struct rpcrdma_regbuf	*rl_sendbuf;
+	struct rpcrdma_regbuf	*rl_rdmabuf;	/* xprt header */
+	struct rpcrdma_regbuf	*rl_sendbuf;	/* rq_snd_buf */
+	struct rpcrdma_regbuf	*rl_recvbuf;	/* rq_rcv_buf */
 
 	struct ib_cqe		rl_cqe;
 	struct list_head	rl_all;

commit 5a6d1db4556940533f1a5b6521e522f3e46508ed
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:45 2016 -0400

    SUNRPC: Add a transport-specific private field in rpc_rqst
    
    Currently there's a hidden and indirect mechanism for finding the
    rpcrdma_req that goes with an rpc_rqst. It depends on getting from
    the rq_buffer pointer in struct rpc_rqst to the struct
    rpcrdma_regbuf that controls that buffer, and then to the struct
    rpcrdma_req it goes with.
    
    This was done back in the day to avoid the need to add a per-rqst
    pointer or to alter the buf_free API when support for RPC-over-RDMA
    was introduced.
    
    I'm about to change the way regbuf's work to support larger inline
    thresholds. Now is a good time to replace this indirect mechanism
    with something that is more straightforward. I guess this should be
    considered a clean up.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 4838a85bdcf6..484855eddb85 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -113,7 +113,6 @@ struct rpcrdma_ep {
 
 struct rpcrdma_regbuf {
 	size_t			rg_size;
-	struct rpcrdma_req	*rg_owner;
 	struct ib_sge		rg_iov;
 	__be32			rg_base[0] __attribute__ ((aligned(256)));
 };
@@ -297,14 +296,16 @@ struct rpcrdma_req {
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 };
 
+static inline void
+rpcrdma_set_xprtdata(struct rpc_rqst *rqst, struct rpcrdma_req *req)
+{
+	rqst->rq_xprtdata = req;
+}
+
 static inline struct rpcrdma_req *
 rpcr_to_rdmar(struct rpc_rqst *rqst)
 {
-	void *buffer = rqst->rq_buffer;
-	struct rpcrdma_regbuf *rb;
-
-	rb = container_of(buffer, struct rpcrdma_regbuf, rg_base);
-	return rb->rg_owner;
+	return rqst->rq_xprtdata;
 }
 
 /*

commit 3435c74aed2d7b743ccbf34616c523ebee7be943
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:29 2016 -0400

    SUNRPC: Generalize the RPC buffer release API
    
    xprtrdma needs to allocate the Call and Reply buffers separately.
    TBH, the reliance on using a single buffer for the pair of XDR
    buffers is transport implementation-specific.
    
    Instead of passing just the rq_buffer into the buf_free method, pass
    the task structure and let buf_free take care of freeing both
    XDR buffers at once.
    
    There's a micro-optimization here. In the common case, both
    xprt_release and the transport's buf_free method were checking if
    rq_buffer was NULL. Now the check is done only once per RPC.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9df47c857d27..4838a85bdcf6 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -283,7 +283,6 @@ struct rpcrdma_req {
 	struct list_head	rl_free;
 	unsigned int		rl_niovs;
 	unsigned int		rl_connect_cookie;
-	struct rpc_task		*rl_task;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];

commit eb342e9a38a5ad79866fec2df2d3ca4592bc501b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:04 2016 -0400

    xprtrdma: Eliminate INLINE_THRESHOLD macros
    
    Clean up: r_xprt is already available everywhere these macros are
    invoked, so just dereference that directly.
    
    RPCRDMA_INLINE_PAD_VALUE is no longer used, so it can simply be
    removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a71b0f5897d8..9df47c857d27 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -356,15 +356,6 @@ struct rpcrdma_create_data_internal {
 	unsigned int	padding;	/* non-rdma write header padding */
 };
 
-#define RPCRDMA_INLINE_READ_THRESHOLD(rq) \
-	(rpcx_to_rdmad(rq->rq_xprt).inline_rsize)
-
-#define RPCRDMA_INLINE_WRITE_THRESHOLD(rq)\
-	(rpcx_to_rdmad(rq->rq_xprt).inline_wsize)
-
-#define RPCRDMA_INLINE_PAD_VALUE(rq)\
-	rpcx_to_rdmad(rq->rq_xprt).padding
-
 /*
  * Statistics for RPCRDMA
  */

commit 05c974669ecec510a85d8534099bb75404e82c41
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Sep 6 11:22:58 2016 -0400

    xprtrdma: Fix receive buffer accounting
    
    An RPC can terminate before its reply arrives, if a credential
    problem or a soft timeout occurs. After this happens, xprtrdma
    reports it is out of Receive buffers.
    
    A Receive buffer is posted before each RPC is sent, and returned to
    the buffer pool when a reply is received. If no reply is received
    for an RPC, that Receive buffer remains posted. But xprtrdma tries
    to post another when the next RPC is sent.
    
    If this happens a few dozen times, there are no receive buffers left
    to be posted at send time. I don't see a way for a transport
    connection to recover at that point, and it will spit warnings and
    unnecessarily delay RPCs on occasion for its remaining lifetime.
    
    Commit 1e465fd4ff47 ("xprtrdma: Replace send and receive arrays")
    removed a little bit of logic to detect this case and not provide
    a Receive buffer so no more buffers are posted, and then transport
    operation continues correctly. We didn't understand what that logic
    did, and it wasn't commented, so it was removed as part of the
    overhaul to support backchannel requests.
    
    Restore it, but be wary of the need to keep extra Receives posted
    to deal with backchannel requests.
    
    Fixes: 1e465fd4ff47 ("xprtrdma: Replace send and receive arrays")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 670fad57153a..a71b0f5897d8 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -321,6 +321,7 @@ struct rpcrdma_buffer {
 	char			*rb_pool;
 
 	spinlock_t		rb_lock;	/* protect buf lists */
+	int			rb_send_count, rb_recv_count;
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
 	u32			rb_max_requests;

commit 5ab8142839c714ed5ac9a9de1846ab71f87a3ed7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:54:25 2016 -0400

    xprtrdma: Chunk list encoders no longer share one rl_segments array
    
    Currently, all three chunk list encoders each use a portion of the
    one rl_segments array in rpcrdma_req. This is because the MWs for
    each chunk list were preserved in rl_segments so that ro_unmap could
    find and invalidate them after the RPC was complete.
    
    However, now that MWs are placed on a per-req linked list as they
    are registered, there is no longer any information in rpcrdma_mr_seg
    that is shared between ro_map and ro_unmap_{sync,safe}, and thus
    nothing in rl_segments needs to be preserved after
    rpcrdma_marshal_req is complete.
    
    Thus the rl_segments array can be used now just for the needs of
    each rpcrdma_convert_iovs call. Once each chunk list is encoded, the
    next chunk list encoder is free to re-use all of rl_segments.
    
    This means all three chunk lists in one RPC request can now each
    encode a full size data payload with no increase in the size of
    rl_segments.
    
    This is a key requirement for Kerberos support, since both the Call
    and Reply for a single RPC transaction are conveyed via Long
    messages (RDMA Read/Write). Both can be large.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f5d05110de9f..670fad57153a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -171,23 +171,14 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
  *   o recv buffer (posted to provider)
  *   o ib_sge (also donated to provider)
  *   o status of reply (length, success or not)
- *   o bookkeeping state to get run by tasklet (list, etc)
+ *   o bookkeeping state to get run by reply handler (list, etc)
  *
- * These are allocated during initialization, per-transport instance;
- * however, the tasklet execution list itself is global, as it should
- * always be pretty short.
+ * These are allocated during initialization, per-transport instance.
  *
  * N of these are associated with a transport instance, and stored in
  * struct rpcrdma_buffer. N is the max number of outstanding requests.
  */
 
-#define RPCRDMA_MAX_DATA_SEGS	((1 * 1024 * 1024) / PAGE_SIZE)
-
-/* data segments + head/tail for Call + head/tail for Reply */
-#define RPCRDMA_MAX_SEGS 	(RPCRDMA_MAX_DATA_SEGS + 4)
-
-struct rpcrdma_buffer;
-
 struct rpcrdma_rep {
 	struct ib_cqe		rr_cqe;
 	unsigned int		rr_len;
@@ -267,13 +258,18 @@ struct rpcrdma_mw {
  * of iovs for send operations. The reason is that the iovs passed to
  * ib_post_{send,recv} must not be modified until the work request
  * completes.
- *
- * NOTES:
- *   o RPCRDMA_MAX_SEGS is the max number of addressible chunk elements we
- *     marshal. The number needed varies depending on the iov lists that
- *     are passed to us and the memory registration mode we are in.
  */
 
+/* Maximum number of page-sized "segments" per chunk list to be
+ * registered or invalidated. Must handle a Reply chunk:
+ */
+enum {
+	RPCRDMA_MAX_IOV_SEGS	= 3,
+	RPCRDMA_MAX_DATA_SEGS	= ((1 * 1024 * 1024) / PAGE_SIZE) + 1,
+	RPCRDMA_MAX_SEGS	= RPCRDMA_MAX_DATA_SEGS +
+				  RPCRDMA_MAX_IOV_SEGS,
+};
+
 struct rpcrdma_mr_seg {		/* chunk descriptors */
 	u32		mr_len;		/* length of chunk or segment */
 	struct page	*mr_page;	/* owning page, if any */
@@ -282,10 +278,10 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 
 #define RPCRDMA_MAX_IOVS	(2)
 
+struct rpcrdma_buffer;
 struct rpcrdma_req {
 	struct list_head	rl_free;
 	unsigned int		rl_niovs;
-	unsigned int		rl_nchunks;
 	unsigned int		rl_connect_cookie;
 	struct rpc_task		*rl_task;
 	struct rpcrdma_buffer	*rl_buffer;
@@ -293,13 +289,13 @@ struct rpcrdma_req {
 	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
 	struct rpcrdma_regbuf	*rl_rdmabuf;
 	struct rpcrdma_regbuf	*rl_sendbuf;
-	struct list_head	rl_registered;	/* registered segments */
-	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
-	struct rpcrdma_mr_seg	*rl_nextseg;
 
 	struct ib_cqe		rl_cqe;
 	struct list_head	rl_all;
 	bool			rl_backchannel;
+
+	struct list_head	rl_registered;	/* registered segments */
+	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 };
 
 static inline struct rpcrdma_req *

commit 9d6b0409788287b64d8401ffba2ce11a5a86a879
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:54:16 2016 -0400

    xprtrdma: Place registered MWs on a per-req list
    
    Instead of placing registered MWs sparsely into the rl_segments
    array, place these MWs on a per-req list.
    
    ro_unmap_{sync,safe} can then simply pull those MWs off the list
    instead of walking through the array.
    
    This change significantly reduces the size of struct rpcrdma_req
    by removing nsegs and rl_mw from every array element.
    
    As an additional clean-up, chunk co-ordinates are returned in the
    "*mw" output argument so they are no longer needed in every
    array element.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 649d01dda327..f5d05110de9f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -245,6 +245,9 @@ struct rpcrdma_mw {
 		struct rpcrdma_frmr	frmr;
 	};
 	struct rpcrdma_xprt	*mw_xprt;
+	u32			mw_handle;
+	u32			mw_length;
+	u64			mw_offset;
 	struct list_head	mw_all;
 };
 
@@ -272,11 +275,7 @@ struct rpcrdma_mw {
  */
 
 struct rpcrdma_mr_seg {		/* chunk descriptors */
-	struct rpcrdma_mw *rl_mw;	/* registered MR */
-	u64		mr_base;	/* registration result */
-	u32		mr_rkey;	/* registration result */
 	u32		mr_len;		/* length of chunk or segment */
-	int		mr_nsegs;	/* number of segments in chunk or 0 */
 	struct page	*mr_page;	/* owning page, if any */
 	char		*mr_offset;	/* kva if no page, else offset */
 };
@@ -294,6 +293,7 @@ struct rpcrdma_req {
 	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
 	struct rpcrdma_regbuf	*rl_rdmabuf;
 	struct rpcrdma_regbuf	*rl_sendbuf;
+	struct list_head	rl_registered;	/* registered segments */
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 	struct rpcrdma_mr_seg	*rl_nextseg;
 
@@ -397,7 +397,8 @@ struct rpcrdma_stats {
 struct rpcrdma_xprt;
 struct rpcrdma_memreg_ops {
 	int		(*ro_map)(struct rpcrdma_xprt *,
-				  struct rpcrdma_mr_seg *, int, bool);
+				  struct rpcrdma_mr_seg *, int, bool,
+				  struct rpcrdma_mw **);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
 					 struct rpcrdma_req *);
 	void		(*ro_unmap_safe)(struct rpcrdma_xprt *,

commit e2ac236c0b65129f12fef358390f76cc3cacb865
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:54:00 2016 -0400

    xprtrdma: Allocate MRs on demand
    
    Frequent MR list exhaustion can impact I/O throughput, so enough MRs
    are always created during transport set-up to prevent running out.
    This means more MRs are created than most workloads need.
    
    Commit 94f58c58c0b4 ("xprtrdma: Allow Read list and Reply chunk
    simultaneously") introduced support for sending two chunk lists per
    RPC, which consumes more MRs per RPC.
    
    Instead of trying to provision more MRs, introduce a mechanism for
    allocating MRs on demand. A few MRs are allocated during transport
    set-up to kick things off.
    
    This significantly reduces the average number of MRs per transport
    while allowing the MR count to grow for workloads or devices that
    need more MRs.
    
    FRWR with mlx4 allocated almost 400 MRs per transport before this
    patch. Now it starts with 32.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 08d441d65a83..649d01dda327 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -339,6 +339,7 @@ struct rpcrdma_buffer {
 	spinlock_t		rb_recovery_lock; /* protect rb_stale_mrs */
 	struct list_head	rb_stale_mrs;
 	struct delayed_work	rb_recovery_worker;
+	struct delayed_work	rb_refresh_worker;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 
@@ -387,6 +388,7 @@ struct rpcrdma_stats {
 	unsigned long		bcall_count;
 	unsigned long		mrs_recovered;
 	unsigned long		mrs_orphaned;
+	unsigned long		mrs_allocated;
 };
 
 /*
@@ -405,8 +407,9 @@ struct rpcrdma_memreg_ops {
 				   struct rpcrdma_ep *,
 				   struct rpcrdma_create_data_internal *);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
-	int		(*ro_init)(struct rpcrdma_xprt *);
-	void		(*ro_destroy)(struct rpcrdma_buffer *);
+	int		(*ro_init_mr)(struct rpcrdma_ia *,
+				      struct rpcrdma_mw *);
+	void		(*ro_release_mr)(struct rpcrdma_mw *);
 	const char	*ro_displayname;
 };
 

commit b54054ca5590f59469437fc4a78a978edcb01c31
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:53:27 2016 -0400

    xprtrdma: Clean up device capability detection
    
    Clean up: Move device capability detection into memreg-specific
    source files.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f1b6f2fb5355..08d441d65a83 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -446,6 +446,8 @@ extern int xprt_rdma_pad_optimize;
  */
 int rpcrdma_ia_open(struct rpcrdma_xprt *, struct sockaddr *, int);
 void rpcrdma_ia_close(struct rpcrdma_ia *);
+bool frwr_is_supported(struct rpcrdma_ia *);
+bool fmr_is_supported(struct rpcrdma_ia *);
 
 /*
  * Endpoint calls - xprtrdma/verbs.c

commit a473018cfe0ef1e46c0ff9df3fa02afc23c9f1d2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:53:19 2016 -0400

    xprtrdma: Remove rpcrdma_map_one() and friends
    
    Clean up: ALLPHYSICAL is gone and FMR has been converted to use
    scatterlists. There are no more users of these functions.
    
    This patch shrinks the size of struct rpcrdma_req by about 3500
    bytes on x86_64. There is one of these structs for each RPC credit
    (128 credits per transport connection).
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index bcb168e3fe15..f1b6f2fb5355 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -277,9 +277,6 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 	u32		mr_rkey;	/* registration result */
 	u32		mr_len;		/* length of chunk or segment */
 	int		mr_nsegs;	/* number of segments in chunk or 0 */
-	enum dma_data_direction	mr_dir;	/* segment mapping direction */
-	dma_addr_t	mr_dma;		/* segment mapping address */
-	size_t		mr_dmalen;	/* segment mapping length */
 	struct page	*mr_page;	/* owning page, if any */
 	char		*mr_offset;	/* kva if no page, else offset */
 };
@@ -496,45 +493,12 @@ void rpcrdma_destroy_wq(void);
  * Wrappers for chunk registration, shared by read/write chunk code.
  */
 
-void rpcrdma_mapping_error(struct rpcrdma_mr_seg *);
-
 static inline enum dma_data_direction
 rpcrdma_data_dir(bool writing)
 {
 	return writing ? DMA_FROM_DEVICE : DMA_TO_DEVICE;
 }
 
-static inline void
-rpcrdma_map_one(struct ib_device *device, struct rpcrdma_mr_seg *seg,
-		enum dma_data_direction direction)
-{
-	seg->mr_dir = direction;
-	seg->mr_dmalen = seg->mr_len;
-
-	if (seg->mr_page)
-		seg->mr_dma = ib_dma_map_page(device,
-				seg->mr_page, offset_in_page(seg->mr_offset),
-				seg->mr_dmalen, seg->mr_dir);
-	else
-		seg->mr_dma = ib_dma_map_single(device,
-				seg->mr_offset,
-				seg->mr_dmalen, seg->mr_dir);
-
-	if (ib_dma_mapping_error(device, seg->mr_dma))
-		rpcrdma_mapping_error(seg);
-}
-
-static inline void
-rpcrdma_unmap_one(struct ib_device *device, struct rpcrdma_mr_seg *seg)
-{
-	if (seg->mr_page)
-		ib_dma_unmap_page(device,
-				  seg->mr_dma, seg->mr_dmalen, seg->mr_dir);
-	else
-		ib_dma_unmap_single(device,
-				    seg->mr_dma, seg->mr_dmalen, seg->mr_dir);
-}
-
 /*
  * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c
  */

commit 2dc3a69de0d6e7f4dba7dbf8eadd5c3ac34098c7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:53:11 2016 -0400

    xprtrdma: Remove ALLPHYSICAL memory registration mode
    
    No HCA or RNIC in the kernel tree requires the use of ALLPHYSICAL.
    
    ALLPHYSICAL advertises in the clear on the network fabric an R_key
    that is good for all of the client's memory. No known exploit
    exists, but theoretically any user on the server can use that R_key
    on the client's QP to read or update any part of the client's memory.
    
    ALLPHYSICAL exposes the client to server bugs, including:
     o base/bounds errors causing data outside the i/o buffer to be
       accessed
     o RDMA access after reply causing data corruption and/or integrity
       fail
    
    ALLPHYSICAL can't protect application memory regions from server
    update after a local signal or soft timeout has terminated an RPC.
    
    ALLPHYSICAL chunks are no larger than a page. Special cases to
    handle small chunks and long chunk lists have been a source of
    implementation complexity and bugs.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 4e03037d042c..bcb168e3fe15 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -68,7 +68,6 @@ struct rpcrdma_ia {
 	struct ib_device	*ri_device;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
-	struct ib_mr		*ri_dma_mr;
 	struct completion	ri_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_frmr_depth;
@@ -269,8 +268,7 @@ struct rpcrdma_mw {
  * NOTES:
  *   o RPCRDMA_MAX_SEGS is the max number of addressible chunk elements we
  *     marshal. The number needed varies depending on the iov lists that
- *     are passed to us, the memory registration mode we are in, and if
- *     physical addressing is used, the layout.
+ *     are passed to us and the memory registration mode we are in.
  */
 
 struct rpcrdma_mr_seg {		/* chunk descriptors */
@@ -417,7 +415,6 @@ struct rpcrdma_memreg_ops {
 
 extern const struct rpcrdma_memreg_ops rpcrdma_fmr_memreg_ops;
 extern const struct rpcrdma_memreg_ops rpcrdma_frwr_memreg_ops;
-extern const struct rpcrdma_memreg_ops rpcrdma_physical_memreg_ops;
 
 /*
  * RPCRDMA transport -- encapsulates the structures above for

commit 505bbe64dd04b105c1377703252758ac56f92485
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:52:54 2016 -0400

    xprtrdma: Refactor MR recovery work queues
    
    I found that commit ead3f26e359e ("xprtrdma: Add ro_unmap_safe
    memreg method"), which introduces ro_unmap_safe, never wired up the
    FMR recovery worker.
    
    The FMR and FRWR recovery work queues both do the same thing.
    Instead of setting up separate individual work queues for this,
    schedule a delayed worker to deal with them, since recovering MRs is
    not performance-critical.
    
    Fixes: ead3f26e359e ("xprtrdma: Add ro_unmap_safe memreg method")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 04696c046dc5..4e03037d042c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -245,7 +245,6 @@ struct rpcrdma_mw {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
 	};
-	struct work_struct	mw_work;
 	struct rpcrdma_xprt	*mw_xprt;
 	struct list_head	mw_all;
 };
@@ -341,6 +340,10 @@ struct rpcrdma_buffer {
 	struct list_head	rb_allreqs;
 
 	u32			rb_bc_max_requests;
+
+	spinlock_t		rb_recovery_lock; /* protect rb_stale_mrs */
+	struct list_head	rb_stale_mrs;
+	struct delayed_work	rb_recovery_worker;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 
@@ -387,6 +390,8 @@ struct rpcrdma_stats {
 	unsigned long		bad_reply_count;
 	unsigned long		nomsg_call_count;
 	unsigned long		bcall_count;
+	unsigned long		mrs_recovered;
+	unsigned long		mrs_orphaned;
 };
 
 /*
@@ -400,6 +405,7 @@ struct rpcrdma_memreg_ops {
 					 struct rpcrdma_req *);
 	void		(*ro_unmap_safe)(struct rpcrdma_xprt *,
 					 struct rpcrdma_req *, bool);
+	void		(*ro_recover_mr)(struct rpcrdma_mw *);
 	int		(*ro_open)(struct rpcrdma_ia *,
 				   struct rpcrdma_ep *,
 				   struct rpcrdma_create_data_internal *);
@@ -477,6 +483,8 @@ void rpcrdma_buffer_put(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
+void rpcrdma_defer_mr_recovery(struct rpcrdma_mw *);
+
 struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
 					    size_t, gfp_t);
 void rpcrdma_free_regbuf(struct rpcrdma_ia *,
@@ -484,9 +492,6 @@ void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 
 int rpcrdma_ep_post_extra_recv(struct rpcrdma_xprt *, unsigned int);
 
-int frwr_alloc_recovery_wq(void);
-void frwr_destroy_recovery_wq(void);
-
 int rpcrdma_alloc_wq(void);
 void rpcrdma_destroy_wq(void);
 

commit 88975ebed5a82b7f0a16f22c81253fdd1ba15fce
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:52:37 2016 -0400

    xprtrdma: Rename fields in rpcrdma_fmr
    
    Clean up: Use the same naming convention used in other
    RPC/RDMA-related data structures.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c53abd1281b3..04696c046dc5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -232,8 +232,8 @@ struct rpcrdma_frmr {
 };
 
 struct rpcrdma_fmr {
-	struct ib_fmr		*fmr;
-	u64			*physaddrs;
+	struct ib_fmr		*fm_mr;
+	u64			*fm_physaddrs;
 };
 
 struct rpcrdma_mw {

commit 564471d2f2f1ddaf02119b8759813666db93abba
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:52:21 2016 -0400

    xprtrdma: Create common scatterlist fields in rpcrdma_mw
    
    Clean up: FMR is about to replace the rpcrdma_map_one code with
    scatterlists. Move the scatterlist fields out of the FRWR-specific
    union and into the generic part of rpcrdma_mw.
    
    One minor change: -EIO is now returned if FRWR registration fails.
    The RPC is terminated immediately, since the problem is likely due
    to a software bug, thus retrying likely won't help.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 95cdc66225ee..c53abd1281b3 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -221,9 +221,6 @@ enum rpcrdma_frmr_state {
 };
 
 struct rpcrdma_frmr {
-	struct scatterlist		*fr_sg;
-	int				fr_nents;
-	enum dma_data_direction		fr_dir;
 	struct ib_mr			*fr_mr;
 	struct ib_cqe			fr_cqe;
 	enum rpcrdma_frmr_state		fr_state;
@@ -240,13 +237,16 @@ struct rpcrdma_fmr {
 };
 
 struct rpcrdma_mw {
+	struct list_head	mw_list;
+	struct scatterlist	*mw_sg;
+	int			mw_nents;
+	enum dma_data_direction	mw_dir;
 	union {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
 	};
 	struct work_struct	mw_work;
 	struct rpcrdma_xprt	*mw_xprt;
-	struct list_head	mw_list;
 	struct list_head	mw_all;
 };
 

commit 6e14a92c363e1f42c6e1339da8413fcfbe1bdc3a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 4 10:41:48 2016 -0400

    xprtrdma: Remove qplock
    
    Clean up.
    
    After "xprtrdma: Remove ro_unmap() from all registration modes",
    there are no longer any sites that take rpcrdma_ia::qplock for read.
    The one site that takes it for write is always single-threaded. It
    is safe to remove it.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 512bbdcc7d76..95cdc66225ee 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -65,7 +65,6 @@
  */
 struct rpcrdma_ia {
 	const struct rpcrdma_memreg_ops	*ri_ops;
-	rwlock_t		ri_qplock;
 	struct ib_device	*ri_device;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;

commit 0b043b9fb5dabcb6f187136cc685b26a7f8bcdb1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:42:54 2016 -0400

    xprtrdma: Remove ro_unmap() from all registration modes
    
    Clean up: The ro_unmap method is no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 59b647eefc99..512bbdcc7d76 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -399,8 +399,6 @@ struct rpcrdma_memreg_ops {
 				  struct rpcrdma_mr_seg *, int, bool);
 	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
 					 struct rpcrdma_req *);
-	int		(*ro_unmap)(struct rpcrdma_xprt *,
-				    struct rpcrdma_mr_seg *);
 	void		(*ro_unmap_safe)(struct rpcrdma_xprt *,
 					 struct rpcrdma_req *, bool);
 	int		(*ro_open)(struct rpcrdma_ia *,

commit ead3f26e359e12ac8d90baff8ed399b85e82fe5b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:42:46 2016 -0400

    xprtrdma: Add ro_unmap_safe memreg method
    
    There needs to be a safe method of releasing registered memory
    resources when an RPC terminates. Safe can mean a number of things:
    
    + Doesn't have to sleep
    
    + Doesn't rely on having a QP in RTS
    
    ro_unmap_safe will be that safe method. It can be used in cases
    where synchronous memory invalidation can deadlock, or needs to have
    an active QP.
    
    The important case is fencing an RPC's memory regions after it is
    signaled (^C) and before it exits. If this is not done, there is a
    window where the server can write an RPC reply into memory that the
    client has released and re-used for some other purpose.
    
    Note that this is a full solution for FRWR, but FMR and physical
    still have some gaps where a particularly bad server can wreak
    some havoc on the client. These gaps are not made worse by this
    patch and are expected to be exceptionally rare and timing-based.
    They are noted in documenting comments.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 97c90a8f5e01..59b647eefc99 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -295,6 +295,7 @@ struct rpcrdma_req {
 	unsigned int		rl_niovs;
 	unsigned int		rl_nchunks;
 	unsigned int		rl_connect_cookie;
+	struct rpc_task		*rl_task;
 	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
@@ -400,6 +401,8 @@ struct rpcrdma_memreg_ops {
 					 struct rpcrdma_req *);
 	int		(*ro_unmap)(struct rpcrdma_xprt *,
 				    struct rpcrdma_mr_seg *);
+	void		(*ro_unmap_safe)(struct rpcrdma_xprt *,
+					 struct rpcrdma_req *, bool);
 	int		(*ro_open)(struct rpcrdma_ia *,
 				   struct rpcrdma_ep *,
 				   struct rpcrdma_create_data_internal *);

commit 766656b022a629201b6e183c7837160cd0919286
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:42:29 2016 -0400

    xprtrdma: Move fr_xprt and fr_worker to struct rpcrdma_mw
    
    In a subsequent patch, the fr_xprt and fr_worker fields will be
    needed by another memory registration mode. Move them into the
    generic rpcrdma_mw structure that wraps struct rpcrdma_frmr.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b5793cb59d5c..97c90a8f5e01 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -229,8 +229,6 @@ struct rpcrdma_frmr {
 	struct ib_cqe			fr_cqe;
 	enum rpcrdma_frmr_state		fr_state;
 	struct completion		fr_linv_done;
-	struct work_struct		fr_work;
-	struct rpcrdma_xprt		*fr_xprt;
 	union {
 		struct ib_reg_wr	fr_regwr;
 		struct ib_send_wr	fr_invwr;
@@ -247,6 +245,8 @@ struct rpcrdma_mw {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
 	};
+	struct work_struct	mw_work;
+	struct rpcrdma_xprt	*mw_xprt;
 	struct list_head	mw_list;
 	struct list_head	mw_all;
 };

commit a3aa8b2b84a59ddd5f624aae9ee0f8b3333793e8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:42:04 2016 -0400

    xprtrdma: Save I/O direction in struct rpcrdma_frwr
    
    Move the the I/O direction field from rpcrdma_mr_seg into the
    rpcrdma_frmr.
    
    This makes it possible to DMA-unmap the frwr long after an RPC has
    exited and its rpcrdma_mr_seg array has been released and re-used.
    This might occur if an RPC times out while waiting for a new
    connection to be established.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 612e3dec10ad..b5793cb59d5c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -224,6 +224,7 @@ enum rpcrdma_frmr_state {
 struct rpcrdma_frmr {
 	struct scatterlist		*fr_sg;
 	int				fr_nents;
+	enum dma_data_direction		fr_dir;
 	struct ib_mr			*fr_mr;
 	struct ib_cqe			fr_cqe;
 	enum rpcrdma_frmr_state		fr_state;

commit 55fdfce101a0afe7bb9da17b4edbee049ae1c18d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:41:56 2016 -0400

    xprtrdma: Rename rpcrdma_frwr::sg and sg_nents
    
    Clean up: Follow same naming convention as other fields in struct
    rpcrdma_frwr.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 61999b694a15..612e3dec10ad 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -222,8 +222,8 @@ enum rpcrdma_frmr_state {
 };
 
 struct rpcrdma_frmr {
-	struct scatterlist		*sg;
-	int				sg_nents;
+	struct scatterlist		*fr_sg;
+	int				fr_nents;
 	struct ib_mr			*fr_mr;
 	struct ib_cqe			fr_cqe;
 	enum rpcrdma_frmr_state		fr_state;

commit 94f58c58c0b4315542036ce7703adeeaf4764940
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:41:30 2016 -0400

    xprtrdma: Allow Read list and Reply chunk simultaneously
    
    rpcrdma_marshal_req() makes a simplifying assumption: that NFS
    operations with large Call messages have small Reply messages, and
    vice versa. Therefore with RPC-over-RDMA, only one chunk type is
    ever needed for each Call/Reply pair, because one direction needs
    chunks, the other direction will always fit inline.
    
    In fact, this assumption is asserted in the code:
    
      if (rtype != rpcrdma_noch && wtype != rpcrdma_noch) {
            dprintk("RPC:       %s: cannot marshal multiple chunk lists\n",
                    __func__);
            return -EIO;
      }
    
    But RPCGSS_SEC breaks this assumption. Because krb5i and krb5p
    perform data transformation on RPC messages before they are
    transmitted, direct data placement techniques cannot be used, thus
    RPC messages must be sent via a Long call in both directions.
    All such calls are sent with a Position Zero Read chunk, and all
    such replies are handled with a Reply chunk. Thus the client must
    provide every Call/Reply pair with both a Read list and a Reply
    chunk.
    
    Without any special security in effect, NFSv4 WRITEs may now also
    use the Read list and provide a Reply chunk. The marshal_req
    logic was preventing that, meaning an NFSv4 WRITE with a large
    payload that included a GETATTR result larger than the inline
    threshold would fail.
    
    The code that encodes each chunk list is now completely contained in
    its own function. There is some code duplication, but the trade-off
    is that the overall logic should be more clear.
    
    Note that all three chunk lists now share the rl_segments array.
    Some additional per-req accounting is necessary to track this
    usage. For the same reasons that the above simplifying assumption
    has held true for so long, I don't expect more array elements are
    needed at this time.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 4349e03069c7..61999b694a15 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -184,7 +184,9 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
  */
 
 #define RPCRDMA_MAX_DATA_SEGS	((1 * 1024 * 1024) / PAGE_SIZE)
-#define RPCRDMA_MAX_SEGS 	(RPCRDMA_MAX_DATA_SEGS + 2) /* head+tail = 2 */
+
+/* data segments + head/tail for Call + head/tail for Reply */
+#define RPCRDMA_MAX_SEGS 	(RPCRDMA_MAX_DATA_SEGS + 4)
 
 struct rpcrdma_buffer;
 
@@ -298,6 +300,7 @@ struct rpcrdma_req {
 	struct rpcrdma_regbuf	*rl_rdmabuf;
 	struct rpcrdma_regbuf	*rl_sendbuf;
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
+	struct rpcrdma_mr_seg	*rl_nextseg;
 
 	struct ib_cqe		rl_cqe;
 	struct list_head	rl_all;

commit 302d3deb20682a076e1ab551821cacfdc81c5e4f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:41:05 2016 -0400

    xprtrdma: Prevent inline overflow
    
    When deciding whether to send a Call inline, rpcrdma_marshal_req
    doesn't take into account header bytes consumed by chunk lists.
    This results in Call messages on the wire that are sometimes larger
    than the inline threshold.
    
    Likewise, when a Write list or Reply chunk is in play, the server's
    reply has to emit an RDMA Send that includes a larger-than-minimal
    RPC-over-RDMA header.
    
    The actual size of a Call message cannot be estimated until after
    the chunk lists have been registered. Thus the size of each
    RPC-over-RDMA header can be estimated only after chunks are
    registered; but the decision to register chunks is based on the size
    of that header. Chicken, meet egg.
    
    The best a client can do is estimate header size based on the
    largest header that might occur, and then ensure that inline content
    is always smaller than that.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 00287486c62c..4349e03069c7 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -73,6 +73,8 @@ struct rpcrdma_ia {
 	struct completion	ri_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_frmr_depth;
+	unsigned int		ri_max_inline_write;
+	unsigned int		ri_max_inline_read;
 	struct ib_qp_attr	ri_qp_attr;
 	struct ib_qp_init_attr	ri_qp_init_attr;
 };
@@ -538,6 +540,9 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
 int rpcrdma_marshal_req(struct rpc_rqst *);
+void rpcrdma_set_max_header_sizes(struct rpcrdma_ia *,
+				  struct rpcrdma_create_data_internal *,
+				  unsigned int);
 
 /* RPC/RDMA module init - xprtrdma/transport.c
  */

commit 949317464bc2baca0ccc69e35a7b5cd3715633a6
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:40:56 2016 -0400

    xprtrdma: Limit number of RDMA segments in RPC-over-RDMA headers
    
    Send buffer space is shared between the RPC-over-RDMA header and
    an RPC message. A large RPC-over-RDMA header means less space is
    available for the associated RPC message, which then has to be
    moved via an RDMA Read or Write.
    
    As more segments are added to the chunk lists, the header increases
    in size.  Typical modern hardware needs only a few segments to
    convey the maximum payload size, but some devices and registration
    modes may need a lot of segments to convey data payload. Sometimes
    so many are needed that the remaining space in the Send buffer is
    not enough for the RPC message. Sending such a message usually
    fails.
    
    To ensure a transport can always make forward progress, cap the
    number of RDMA segments that are allowed in chunk lists. This
    prevents less-capable devices and memory registrations from
    consuming a large portion of the Send buffer by reducing the
    maximum data payload that can be conveyed with such devices.
    
    For now I choose an arbitrary maximum of 8 RDMA segments. This
    allows a maximum size RPC-over-RDMA header to fit nicely in the
    current 1024 byte inline threshold with over 700 bytes remaining
    for an inline RPC message.
    
    The current maximum data payload of NFS READ or WRITE requests is
    one megabyte. To convey that payload on a client with 4KB pages,
    each chunk segment would need to handle 32 or more data pages. This
    is well within the capabilities of FMR. For physical registration,
    the maximum payload size on platforms with 4KB pages is reduced to
    32KB.
    
    For FRWR, a device's maximum page list depth would need to be at
    least 34 to support the maximum 1MB payload. A device with a smaller
    maximum page list depth means the maximum data payload is reduced
    when using that device.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 7723e5faff4d..00287486c62c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -144,6 +144,26 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
 
 #define RPCRDMA_DEF_GFP		(GFP_NOIO | __GFP_NOWARN)
 
+/* To ensure a transport can always make forward progress,
+ * the number of RDMA segments allowed in header chunk lists
+ * is capped at 8. This prevents less-capable devices and
+ * memory registrations from overrunning the Send buffer
+ * while building chunk lists.
+ *
+ * Elements of the Read list take up more room than the
+ * Write list or Reply chunk. 8 read segments means the Read
+ * list (or Write list or Reply chunk) cannot consume more
+ * than
+ *
+ * ((8 + 2) * read segment size) + 1 XDR words, or 244 bytes.
+ *
+ * And the fixed part of the header is another 24 bytes.
+ *
+ * The smallest inline threshold is 1024 bytes, ensuring that
+ * at least 750 bytes are available for RPC messages.
+ */
+#define RPCRDMA_MAX_HDR_SEGS	(8)
+
 /*
  * struct rpcrdma_rep -- this structure encapsulates state required to recv
  * and complete a reply, asychronously. It needs several pieces of
@@ -456,7 +476,6 @@ struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
 void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 			 struct rpcrdma_regbuf *);
 
-unsigned int rpcrdma_max_segments(struct rpcrdma_xprt *);
 int rpcrdma_ep_post_extra_recv(struct rpcrdma_xprt *, unsigned int);
 
 int frwr_alloc_recovery_wq(void);

commit 6b26cc8c8ead3636a18bfd9489984983f4ddd6f4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:40:40 2016 -0400

    sunrpc: Advertise maximum backchannel payload size
    
    RPC-over-RDMA transports have a limit on how large a backward
    direction (backchannel) RPC message can be. Ensure that the NFSv4.x
    CREATE_SESSION operation advertises this limit to servers.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2ebc743cb96f..7723e5faff4d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -534,6 +534,7 @@ void xprt_rdma_cleanup(void);
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 int xprt_rdma_bc_setup(struct rpc_xprt *, unsigned int);
 int xprt_rdma_bc_up(struct svc_serv *, struct net *);
+size_t xprt_rdma_bc_maxpayload(struct rpc_xprt *);
 int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
 void rpcrdma_bc_receive_call(struct rpcrdma_xprt *, struct rpcrdma_rep *);
 int rpcrdma_bc_marshal_reply(struct rpc_rqst *);

commit 2fa8f88d8892507ecff0126fbc67906740491d31
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Mar 4 11:28:53 2016 -0500

    xprtrdma: Use new CQ API for RPC-over-RDMA client send CQs
    
    Calling ib_poll_cq() to sort through WCs during a completion is a
    common pattern amongst RDMA consumers. Since commit 14d3a3b2498e
    ("IB: add a proper completion queue abstraction"), WC sorting can
    be handled by the IB core.
    
    By converting to this new API, xprtrdma is made a better neighbor to
    other RDMA consumers, as it allows the core to schedule the delivery
    of completions more fairly amongst all active consumers.
    
    Because each ib_cqe carries a pointer to a completion method, the
    core can now post its own operations on a consumer's QP, and handle
    the completions itself, without changes to the consumer.
    
    Send completions were previously handled entirely in the completion
    upcall handler (ie, deferring to a process context is unneeded).
    Thus IB_POLL_SOFTIRQ is a direct replacement for the current
    xprtrdma send code path.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b3c4472a3d6a..2ebc743cb96f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -95,10 +95,6 @@ struct rpcrdma_ep {
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
 #define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
 
-/* Force completion handler to ignore the signal
- */
-#define RPCRDMA_IGNORE_COMPLETION	(0ULL)
-
 /* Pre-allocate extra Work Requests for handling backward receives
  * and sends. This is a fixed value because the Work Queues are
  * allocated when the forward channel is set up.
@@ -205,11 +201,11 @@ struct rpcrdma_frmr {
 	struct scatterlist		*sg;
 	int				sg_nents;
 	struct ib_mr			*fr_mr;
+	struct ib_cqe			fr_cqe;
 	enum rpcrdma_frmr_state		fr_state;
+	struct completion		fr_linv_done;
 	struct work_struct		fr_work;
 	struct rpcrdma_xprt		*fr_xprt;
-	bool				fr_waiter;
-	struct completion		fr_linv_done;;
 	union {
 		struct ib_reg_wr	fr_regwr;
 		struct ib_send_wr	fr_invwr;
@@ -226,7 +222,6 @@ struct rpcrdma_mw {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
 	};
-	void			(*mw_sendcompletion)(struct ib_wc *);
 	struct list_head	mw_list;
 	struct list_head	mw_all;
 };
@@ -282,6 +277,7 @@ struct rpcrdma_req {
 	struct rpcrdma_regbuf	*rl_sendbuf;
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 
+	struct ib_cqe		rl_cqe;
 	struct list_head	rl_all;
 	bool			rl_backchannel;
 };

commit c882a655b78d23eb7037cfd6ebf94af1d7068a58
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Mar 4 11:28:45 2016 -0500

    xprtrdma: Use an anonymous union in struct rpcrdma_mw
    
    Clean up: Make code more readable.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d60feb9aa631..b3c4472a3d6a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -225,7 +225,7 @@ struct rpcrdma_mw {
 	union {
 		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
-	} r;
+	};
 	void			(*mw_sendcompletion)(struct ib_wc *);
 	struct list_head	mw_list;
 	struct list_head	mw_all;

commit 552bf225281f96e7a02e1a1b874966fdb6b997e0
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Mar 4 11:28:36 2016 -0500

    xprtrdma: Use new CQ API for RPC-over-RDMA client receive CQs
    
    Calling ib_poll_cq() to sort through WCs during a completion is a
    common pattern amongst RDMA consumers. Since commit 14d3a3b2498e
    ("IB: add a proper completion queue abstraction"), WC sorting can
    be handled by the IB core.
    
    By converting to this new API, xprtrdma is made a better neighbor to
    other RDMA consumers, as it allows the core to schedule the delivery
    of completions more fairly amongst all active consumers.
    
    Because each ib_cqe carries a pointer to a completion method, the
    core can now post its own operations on a consumer's QP, and handle
    the completions itself, without changes to the consumer.
    
    xprtrdma's reply processing is already handled in a work queue, but
    there is some initial order-dependent processing that is done in the
    soft IRQ context before a work item is scheduled.
    
    IB_POLL_SOFTIRQ is a direct replacement for the current xprtrdma
    receive code path.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 7bf6f43fa4b9..d60feb9aa631 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -171,6 +171,7 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
 struct rpcrdma_buffer;
 
 struct rpcrdma_rep {
+	struct ib_cqe		rr_cqe;
 	unsigned int		rr_len;
 	struct ib_device	*rr_device;
 	struct rpcrdma_xprt	*rr_rxprt;

commit 23826c7aeac7e333bfee6f10a3407a23c58b6147
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Mar 4 11:28:27 2016 -0500

    xprtrdma: Serialize credit accounting again
    
    Commit fe97b47cd623 ("xprtrdma: Use workqueue to process RPC/RDMA
    replies") replaced the reply tasklet with a workqueue that allows
    RPC replies to be processed in parallel. Thus the credit values in
    RPC-over-RDMA replies can be applied in a different order than in
    which the server sent them.
    
    To fix this, revert commit eba8ff660b2d ("xprtrdma: Move credit
    update to RPC reply handler"). Reverting is done by hand to
    accommodate code changes that have occurred since then.
    
    Fixes: fe97b47cd623 ("xprtrdma: Use workqueue to process . . .")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 38fe11b09875..7bf6f43fa4b9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -311,6 +311,7 @@ struct rpcrdma_buffer {
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
 	u32			rb_max_requests;
+	atomic_t		rb_credits;	/* most recent credit grant */
 
 	u32			rb_bc_srv_max_requests;
 	spinlock_t		rb_reqslock;	/* protect rb_allreqs */

commit 048ccca8c1c8f583deec3367d7df521bb1f542ae
Merge: b3e27d5d4a29 34356f64ac0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 23 18:45:06 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "Initial roundup of 4.5 merge window patches
    
       - Remove usage of ib_query_device and instead store attributes in
         ib_device struct
    
       - Move iopoll out of block and into lib, rename to irqpoll, and use
         in several places in the rdma stack as our new completion queue
         polling library mechanism.  Update the other block drivers that
         already used iopoll to use the new mechanism too.
    
       - Replace the per-entry GID table locks with a single GID table lock
    
       - IPoIB multicast cleanup
    
       - Cleanups to the IB MR facility
    
       - Add support for 64bit extended IB counters
    
       - Fix for netlink oops while parsing RDMA nl messages
    
       - RoCEv2 support for the core IB code
    
       - mlx4 RoCEv2 support
    
       - mlx5 RoCEv2 support
    
       - Cross Channel support for mlx5
    
       - Timestamp support for mlx5
    
       - Atomic support for mlx5
    
       - Raw QP support for mlx5
    
       - MAINTAINERS update for mlx4/mlx5
    
       - Misc ocrdma, qib, nes, usNIC, cxgb3, cxgb4, mlx4, mlx5 updates
    
       - Add support for remote invalidate to the iSER driver (pushed
         through the RDMA tree due to dependencies, acknowledged by nab)
    
       - Update to NFSoRDMA (pushed through the RDMA tree due to
         dependencies, acknowledged by Bruce)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (169 commits)
      IB/mlx5: Unify CQ create flags check
      IB/mlx5: Expose Raw Packet QP to user space consumers
      {IB, net}/mlx5: Move the modify QP operation table to mlx5_ib
      IB/mlx5: Support setting Ethernet priority for Raw Packet QPs
      IB/mlx5: Add Raw Packet QP query functionality
      IB/mlx5: Add create and destroy functionality for Raw Packet QP
      IB/mlx5: Refactor mlx5_ib_qp to accommodate other QP types
      IB/mlx5: Allocate a Transport Domain for each ucontext
      net/mlx5_core: Warn on unsupported events of QP/RQ/SQ
      net/mlx5_core: Add RQ and SQ event handling
      net/mlx5_core: Export transport objects
      IB/mlx5: Expose CQE version to user-space
      IB/mlx5: Add CQE version 1 support to user QPs and SRQs
      IB/mlx5: Fix data validation in mlx5_ib_alloc_ucontext
      IB/sa: Fix netlink local service GFP crash
      IB/srpt: Remove redundant wc array
      IB/qib: Improve ipoib UD performance
      IB/mlx4: Advertise RoCE v2 support
      IB/mlx4: Create and use another QP1 for RoCEv2
      IB/mlx4: Enable send of RoCE QP1 packets with IP/UDP headers
      ...

commit 5d252f90a800cee5bc57c76d636ae60464f7a887
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:50:10 2016 -0500

    svcrdma: Add class for RDMA backwards direction transport
    
    To support the server-side of an NFSv4.1 backchannel on RDMA
    connections, add a transport class that enables backward
    direction messages on an existing forward channel connection.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 72276c7907e4..5a38236d0516 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -55,6 +55,11 @@
 #define RDMA_RESOLVE_TIMEOUT	(5000)	/* 5 seconds */
 #define RDMA_CONNECT_RETRY_MAX	(2)	/* retries if no listener backlog */
 
+#define RPCRDMA_BIND_TO		(60U * HZ)
+#define RPCRDMA_INIT_REEST_TO	(5U * HZ)
+#define RPCRDMA_MAX_REEST_TO	(30U * HZ)
+#define RPCRDMA_IDLE_DISC_TO	(5U * 60 * HZ)
+
 /*
  * Interface Adapter -- one per transport instance
  */
@@ -147,6 +152,8 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
 	return (struct rpcrdma_msg *)rb->rg_base;
 }
 
+#define RPCRDMA_DEF_GFP		(GFP_NOIO | __GFP_NOWARN)
+
 /*
  * struct rpcrdma_rep -- this structure encapsulates state required to recv
  * and complete a reply, asychronously. It needs several pieces of
@@ -308,6 +315,8 @@ struct rpcrdma_buffer {
 	u32			rb_bc_srv_max_requests;
 	spinlock_t		rb_reqslock;	/* protect rb_allreqs */
 	struct list_head	rb_allreqs;
+
+	u32			rb_bc_max_requests;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 
@@ -513,6 +522,10 @@ int rpcrdma_marshal_req(struct rpc_rqst *);
 
 /* RPC/RDMA module init - xprtrdma/transport.c
  */
+extern unsigned int xprt_rdma_max_inline_read;
+void xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap);
+void xprt_rdma_free_addresses(struct rpc_xprt *xprt);
+void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq);
 int xprt_rdma_init(void);
 void xprt_rdma_cleanup(void);
 
@@ -528,4 +541,6 @@ void xprt_rdma_bc_free_rqst(struct rpc_rqst *);
 void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);
 #endif	/* CONFIG_SUNRPC_BACKCHANNEL */
 
+extern struct xprt_class xprt_rdma_bc;
+
 #endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */

commit 71810ef3271d1a06f7002c55c7e354d8c3233762
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:49:28 2016 -0500

    svcrdma: Remove unused req_map and ctxt kmem_caches
    
    Clean up.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 419719198caf..72276c7907e4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -528,11 +528,4 @@ void xprt_rdma_bc_free_rqst(struct rpc_rqst *);
 void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);
 #endif	/* CONFIG_SUNRPC_BACKCHANNEL */
 
-/* Temporary NFS request map cache. Created in svc_rdma.c  */
-extern struct kmem_cache *svc_rdma_map_cachep;
-/* WR context cache. Created in svc_rdma.c  */
-extern struct kmem_cache *svc_rdma_ctxt_cachep;
-/* Workqueue created in svc_rdma.c */
-extern struct workqueue_struct *svc_rdma_wq;
-
 #endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */

commit e3e45b1b43988b99007a9908ca0ba738b3fbd0ff
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Fri Dec 18 10:59:48 2015 +0200

    xprtrdma: Avoid calling ib_query_device
    
    Instead, use the cached copy of the attributes present on the device.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ac7f8d4f632a..419719198caf 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -68,7 +68,6 @@ struct rpcrdma_ia {
 	struct completion	ri_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_frmr_depth;
-	struct ib_device_attr	ri_devattr;
 	struct ib_qp_attr	ri_qp_attr;
 	struct ib_qp_init_attr	ri_qp_init_attr;
 };

commit 26ae9d1c5af1b1d669ca1c28fc02bbca3d778d45
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 16 17:23:20 2015 -0500

    xprtrdma: Revert commit e7104a2a9606 ('xprtrdma: Cap req_cqinit').
    
    The root of the problem was that sends (especially unsignalled
    FASTREG and LOCAL_INV Work Requests) were not properly flow-
    controlled, which allowed a send queue overrun.
    
    Now that the RPC/RDMA reply handler waits for invalidation to
    complete, the send queue is properly flow-controlled. Thus this
    limit is no longer necessary.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ddae4909982b..728101ddc44b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -88,12 +88,6 @@ struct rpcrdma_ep {
 	struct delayed_work	rep_connect_worker;
 };
 
-/*
- * Force a signaled SEND Work Request every so often,
- * in case the provider needs to do some housekeeping.
- */
-#define RPCRDMA_MAX_UNSIGNALED_SENDS	(32)
-
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
 #define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
 

commit c9918ff56dfb175ce427140c641280d0b4522dbe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 16 17:22:47 2015 -0500

    xprtrdma: Add ro_unmap_sync method for FRWR
    
    FRWR's ro_unmap is asynchronous. The new ro_unmap_sync posts
    LOCAL_INV Work Requests and waits for them to complete before
    returning.
    
    Note also, DMA unmapping is now done _after_ invalidation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c32cba3f21fb..ddae4909982b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -207,6 +207,8 @@ struct rpcrdma_frmr {
 	enum rpcrdma_frmr_state		fr_state;
 	struct work_struct		fr_work;
 	struct rpcrdma_xprt		*fr_xprt;
+	bool				fr_waiter;
+	struct completion		fr_linv_done;;
 	union {
 		struct ib_reg_wr	fr_regwr;
 		struct ib_send_wr	fr_invwr;

commit 32d0ceecdfd0e941c492390fe5b6237cc1cf9fa6
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 16 17:22:39 2015 -0500

    xprtrdma: Introduce ro_unmap_sync method
    
    In the current xprtrdma implementation, some memreg strategies
    implement ro_unmap synchronously (the MR is knocked down before the
    method returns) and some asynchonously (the MR will be knocked down
    and returned to the pool in the background).
    
    To guarantee the MR is truly invalid before the RPC consumer is
    allowed to resume execution, we need an unmap method that is
    always synchronous, invoked from the RPC/RDMA reply handler.
    
    The new method unmaps all MRs for an RPC. The existing ro_unmap
    method unmaps only one MR at a time.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5c1e0c600faf..c32cba3f21fb 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -368,6 +368,8 @@ struct rpcrdma_xprt;
 struct rpcrdma_memreg_ops {
 	int		(*ro_map)(struct rpcrdma_xprt *,
 				  struct rpcrdma_mr_seg *, int, bool);
+	void		(*ro_unmap_sync)(struct rpcrdma_xprt *,
+					 struct rpcrdma_req *);
 	int		(*ro_unmap)(struct rpcrdma_xprt *,
 				    struct rpcrdma_mr_seg *);
 	int		(*ro_open)(struct rpcrdma_ia *,

commit 3cf4e169be95e1a3a70a063b6bd8103fbd5911f3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 16 17:22:31 2015 -0500

    xprtrdma: Move struct ib_send_wr off the stack
    
    For FRWR FASTREG and LOCAL_INV, move the ib_*_wr structure off
    the stack. This allows frwr_op_map and frwr_op_unmap to chain
    WRs together without limit to register or invalidate a set of MRs
    with a single ib_post_send().
    
    (This will be for chaining LOCAL_INV requests).
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ac7f8d4f632a..5c1e0c600faf 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -207,6 +207,10 @@ struct rpcrdma_frmr {
 	enum rpcrdma_frmr_state		fr_state;
 	struct work_struct		fr_work;
 	struct rpcrdma_xprt		*fr_xprt;
+	union {
+		struct ib_reg_wr	fr_regwr;
+		struct ib_send_wr	fr_invwr;
+	};
 };
 
 struct rpcrdma_fmr {

commit e6604ecb70d4b1dbc0372c6518b51c25c4b135a1
Merge: 9d74288ca792 941c3ff3102c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Nov 9 18:11:22 2015 -0800

    Merge tag 'nfs-for-4.4-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      New features:
       - RDMA client backchannel from Chuck
       - Support for NFSv4.2 file CLONE using the btrfs ioctl
    
      Bugfixes + cleanups:
       - Move socket data receive out of the bottom halves and into a
         workqueue
       - Refactor NFSv4 error handling so synchronous and asynchronous RPC
         handles errors identically.
       - Fix a panic when blocks or object layouts reads return a bad data
         length
       - Fix nfsroot so it can handle a 1024 byte long path.
       - Fix bad usage of page offset in bl_read_pagelist
       - Various NFSv4 callback cleanups+fixes
       - Fix GETATTR bitmap verification
       - Support hexadecimal number for sunrpc debug sysctl files"
    
    * tag 'nfs-for-4.4-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (53 commits)
      Sunrpc: Supports hexadecimal number for sysctl files of sunrpc debug
      nfs: Fix GETATTR bitmap verification
      nfs: Remove unused xdr page offsets in getacl/setacl arguments
      fs/nfs: remove unnecessary new_valid_dev check
      SUNRPC: fix variable type
      NFS: Enable client side NFSv4.1 backchannel to use other transports
      pNFS/flexfiles: Add support for FF_FLAGS_NO_IO_THRU_MDS
      pNFS/flexfiles: When mirrored, retry failed reads by switching mirrors
      SUNRPC: Remove the TCP-only restriction in bc_svc_process()
      svcrdma: Add backward direction service for RPC/RDMA transport
      xprtrdma: Handle incoming backward direction RPC calls
      xprtrdma: Add support for sending backward direction RPC replies
      xprtrdma: Pre-allocate Work Requests for backchannel
      xprtrdma: Pre-allocate backward rpc_rqst and send/receive buffers
      SUNRPC: Abstract backchannel operations
      xprtrdma: Saving IRQs no longer needed for rb_lock
      xprtrdma: Remove reply tasklet
      xprtrdma: Use workqueue to process RPC/RDMA replies
      xprtrdma: Replace send and receive arrays
      xprtrdma: Refactor reply handler error handling
      ...

commit 76566773a1f1c2295ed901b6f1241cfe10d99029
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:28:32 2015 -0400

    NFS: Enable client side NFSv4.1 backchannel to use other transports
    
    Forechannel transports get their own "bc_up" method to create an
    endpoint for the backchannel service.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    [Anna Schumaker: Add forward declaration of struct net to xprt.h]
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index eb87d96e80ca..f8dd17be9f43 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -520,6 +520,7 @@ void xprt_rdma_cleanup(void);
  */
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 int xprt_rdma_bc_setup(struct rpc_xprt *, unsigned int);
+int xprt_rdma_bc_up(struct svc_serv *, struct net *);
 int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
 void rpcrdma_bc_receive_call(struct rpcrdma_xprt *, struct rpcrdma_rep *);
 int rpcrdma_bc_marshal_reply(struct rpc_rqst *);

commit 63cae47005af51c937f4cdcc4835f29075add2ba
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:28:08 2015 -0400

    xprtrdma: Handle incoming backward direction RPC calls
    
    Introduce a code path in the rpcrdma_reply_handler() to catch
    incoming backward direction RPC calls and route them to the ULP's
    backchannel server.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e2d23ea23df9..eb87d96e80ca 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -353,6 +353,7 @@ struct rpcrdma_stats {
 	unsigned long		failed_marshal_count;
 	unsigned long		bad_reply_count;
 	unsigned long		nomsg_call_count;
+	unsigned long		bcall_count;
 };
 
 /*
@@ -520,6 +521,7 @@ void xprt_rdma_cleanup(void);
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 int xprt_rdma_bc_setup(struct rpc_xprt *, unsigned int);
 int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
+void rpcrdma_bc_receive_call(struct rpcrdma_xprt *, struct rpcrdma_rep *);
 int rpcrdma_bc_marshal_reply(struct rpc_rqst *);
 void xprt_rdma_bc_free_rqst(struct rpc_rqst *);
 void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);

commit 83128a60ca74e996c5e0336c4fff0579f4a8c909
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:27:59 2015 -0400

    xprtrdma: Add support for sending backward direction RPC replies
    
    Backward direction RPC replies are sent via the client transport's
    send_request method, the same way forward direction RPC calls are
    sent.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 55d2660df56a..e2d23ea23df9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -520,6 +520,7 @@ void xprt_rdma_cleanup(void);
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 int xprt_rdma_bc_setup(struct rpc_xprt *, unsigned int);
 int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
+int rpcrdma_bc_marshal_reply(struct rpc_rqst *);
 void xprt_rdma_bc_free_rqst(struct rpc_rqst *);
 void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);
 #endif	/* CONFIG_SUNRPC_BACKCHANNEL */

commit 124fa17d3e33060fbb28e995a42c7f5c8b31b345
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:27:51 2015 -0400

    xprtrdma: Pre-allocate Work Requests for backchannel
    
    Pre-allocate extra send and receive Work Requests needed to handle
    backchannel receives and sends.
    
    The transport doesn't know how many extra WRs to pre-allocate until
    the xprt_setup_backchannel() call, but that's long after the WRs are
    allocated during forechannel setup.
    
    So, use a fixed value for now.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1eb86c79f4b9..55d2660df56a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -101,6 +101,16 @@ struct rpcrdma_ep {
  */
 #define RPCRDMA_IGNORE_COMPLETION	(0ULL)
 
+/* Pre-allocate extra Work Requests for handling backward receives
+ * and sends. This is a fixed value because the Work Queues are
+ * allocated when the forward channel is set up.
+ */
+#if defined(CONFIG_SUNRPC_BACKCHANNEL)
+#define RPCRDMA_BACKWARD_WRS		(8)
+#else
+#define RPCRDMA_BACKWARD_WRS		(0)
+#endif
+
 /* Registered buffer -- registered kmalloc'd memory for RDMA SEND/RECV
  *
  * The below structure appears at the front of a large region of kmalloc'd

commit f531a5dbc451afb66e9d6c71a69e8358d1847969
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:27:43 2015 -0400

    xprtrdma: Pre-allocate backward rpc_rqst and send/receive buffers
    
    xprtrdma's backward direction send and receive buffers are the same
    size as the forechannel's inline threshold, and must be pre-
    registered.
    
    The consumer has no control over which receive buffer the adapter
    chooses to catch an incoming backwards-direction call. Any receive
    buffer can be used for either a forward reply or a backward call.
    Thus both types of RPC message must all be the same size.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 6ea1dbe46e88..1eb86c79f4b9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -263,6 +263,9 @@ struct rpcrdma_req {
 	struct rpcrdma_regbuf	*rl_rdmabuf;
 	struct rpcrdma_regbuf	*rl_sendbuf;
 	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
+
+	struct list_head	rl_all;
+	bool			rl_backchannel;
 };
 
 static inline struct rpcrdma_req *
@@ -291,6 +294,10 @@ struct rpcrdma_buffer {
 	struct list_head	rb_send_bufs;
 	struct list_head	rb_recv_bufs;
 	u32			rb_max_requests;
+
+	u32			rb_bc_srv_max_requests;
+	spinlock_t		rb_reqslock;	/* protect rb_allreqs */
+	struct list_head	rb_allreqs;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 
@@ -411,6 +418,9 @@ int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_ep *,
 /*
  * Buffer calls - xprtrdma/verbs.c
  */
+struct rpcrdma_req *rpcrdma_create_req(struct rpcrdma_xprt *);
+struct rpcrdma_rep *rpcrdma_create_rep(struct rpcrdma_xprt *);
+void rpcrdma_destroy_req(struct rpcrdma_ia *, struct rpcrdma_req *);
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 
@@ -427,6 +437,7 @@ void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 			 struct rpcrdma_regbuf *);
 
 unsigned int rpcrdma_max_segments(struct rpcrdma_xprt *);
+int rpcrdma_ep_post_extra_recv(struct rpcrdma_xprt *, unsigned int);
 
 int frwr_alloc_recovery_wq(void);
 void frwr_destroy_recovery_wq(void);
@@ -494,6 +505,15 @@ int rpcrdma_marshal_req(struct rpc_rqst *);
 int xprt_rdma_init(void);
 void xprt_rdma_cleanup(void);
 
+/* Backchannel calls - xprtrdma/backchannel.c
+ */
+#if defined(CONFIG_SUNRPC_BACKCHANNEL)
+int xprt_rdma_bc_setup(struct rpc_xprt *, unsigned int);
+int rpcrdma_bc_post_recv(struct rpcrdma_xprt *, unsigned int);
+void xprt_rdma_bc_free_rqst(struct rpc_rqst *);
+void xprt_rdma_bc_destroy(struct rpc_xprt *, unsigned int);
+#endif	/* CONFIG_SUNRPC_BACKCHANNEL */
+
 /* Temporary NFS request map cache. Created in svc_rdma.c  */
 extern struct kmem_cache *svc_rdma_map_cachep;
 /* WR context cache. Created in svc_rdma.c  */

commit fe97b47cd623ebbaa55a163c336abc47153526d1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:27:10 2015 -0400

    xprtrdma: Use workqueue to process RPC/RDMA replies
    
    The reply tasklet is fast, but it's single threaded. After reply
    traffic saturates a single CPU, there's no more reply processing
    capacity.
    
    Replace the tasklet with a workqueue to spread reply handling across
    all CPUs.  This also moves RPC/RDMA reply handling out of the soft
    IRQ context and into a context that allows sleeps.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e6a358fd1f1d..6ea1dbe46e88 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -164,6 +164,7 @@ struct rpcrdma_rep {
 	unsigned int		rr_len;
 	struct ib_device	*rr_device;
 	struct rpcrdma_xprt	*rr_rxprt;
+	struct work_struct	rr_work;
 	struct list_head	rr_list;
 	struct rpcrdma_regbuf	*rr_rdmabuf;
 };
@@ -430,6 +431,9 @@ unsigned int rpcrdma_max_segments(struct rpcrdma_xprt *);
 int frwr_alloc_recovery_wq(void);
 void frwr_destroy_recovery_wq(void);
 
+int rpcrdma_alloc_wq(void);
+void rpcrdma_destroy_wq(void);
+
 /*
  * Wrappers for chunk registration, shared by read/write chunk code.
  */

commit 1e465fd4ff475cc29c866ee75496c941b3908e69
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:27:02 2015 -0400

    xprtrdma: Replace send and receive arrays
    
    The rb_send_bufs and rb_recv_bufs arrays are used to implement a
    pair of stacks for keeping track of free rpcrdma_req and rpcrdma_rep
    structs. Replace those arrays with free lists.
    
    To allow more than 512 RPCs in-flight at once, each of these arrays
    would be larger than a page (assuming 8-byte addresses and 4KB
    pages). Allowing up to 64K in-flight RPCs (as TCP now does), each
    buffer array would have to be 128 pages. That's an order-6
    allocation. (Not that we're going there.)
    
    A list is easier to expand dynamically. Instead of allocating a
    larger array of pointers and copying the existing pointers to the
    new array, simply append more buffers to each list.
    
    This also makes it simpler to manage receive buffers that might
    catch backwards-direction calls, or to post receive buffers in
    bulk to amortize the overhead of ib_post_recv.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index a13508bbe4c4..e6a358fd1f1d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -252,6 +252,7 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 #define RPCRDMA_MAX_IOVS	(2)
 
 struct rpcrdma_req {
+	struct list_head	rl_free;
 	unsigned int		rl_niovs;
 	unsigned int		rl_nchunks;
 	unsigned int		rl_connect_cookie;
@@ -285,12 +286,10 @@ struct rpcrdma_buffer {
 	struct list_head	rb_all;
 	char			*rb_pool;
 
-	spinlock_t		rb_lock;	/* protect buf arrays */
+	spinlock_t		rb_lock;	/* protect buf lists */
+	struct list_head	rb_send_bufs;
+	struct list_head	rb_recv_bufs;
 	u32			rb_max_requests;
-	int			rb_send_index;
-	int			rb_recv_index;
-	struct rpcrdma_req	**rb_send_bufs;
-	struct rpcrdma_rep	**rb_recv_bufs;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 

commit b0e178a2d8ad4bd6c6bbf5d3f3cf50ca8907581b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:26:54 2015 -0400

    xprtrdma: Refactor reply handler error handling
    
    Clean up: The error cases in rpcrdma_reply_handler() almost never
    execute. Ensure the compiler places them out of the hot path.
    
    No behavior change expected.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 42c8d44a175b..a13508bbe4c4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -168,6 +168,8 @@ struct rpcrdma_rep {
 	struct rpcrdma_regbuf	*rr_rdmabuf;
 };
 
+#define RPCRDMA_BAD_LEN		(~0U)
+
 /*
  * struct rpcrdma_mw - external memory region metadata
  *

commit 4220a07264c0517006a534aed201e29c8d297306
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:26:45 2015 -0400

    xprtrdma: Prevent loss of completion signals
    
    Commit 8301a2c047cc ("xprtrdma: Limit work done by completion
    handler") was supposed to prevent xprtrdma's upcall handlers from
    starving other softIRQ work by letting them return to the provider
    before all CQEs have been polled.
    
    The logic assumes the provider will call the upcall handler again
    immediately if the CQ is re-armed while there are still queued CQEs.
    
    This assumption is invalid. The IBTA spec says that after a CQ is
    armed, the hardware must interrupt only when a new CQE is inserted.
    xprtrdma can't rely on the provider calling again, even though some
    providers do.
    
    Therefore, leaving CQEs on queue makes sense only when there is
    another mechanism that ensures all remaining CQEs are consumed in a
    timely fashion. xprtrdma does not have such a mechanism. If a CQE
    remains queued, the transport can wait forever to send the next RPC.
    
    Finally, move the wcs array back onto the stack to ensure that the
    poll array is always local to the CPU where the completion upcall is
    running.
    
    Fixes: 8301a2c047cc ("xprtrdma: Limit work done by completion ...")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c09414e6f91b..42c8d44a175b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -77,9 +77,6 @@ struct rpcrdma_ia {
  * RDMA Endpoint -- one per transport instance
  */
 
-#define RPCRDMA_WC_BUDGET	(128)
-#define RPCRDMA_POLLSIZE	(16)
-
 struct rpcrdma_ep {
 	atomic_t		rep_cqcount;
 	int			rep_cqinit;
@@ -89,8 +86,6 @@ struct rpcrdma_ep {
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;
-	struct ib_wc		rep_send_wcs[RPCRDMA_POLLSIZE];
-	struct ib_wc		rep_recv_wcs[RPCRDMA_POLLSIZE];
 };
 
 /*

commit 4143f34e01e9cdf1882f98c54d9073e4de8c28fb
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Tue Oct 13 19:11:35 2015 +0300

    xprtrdma: Port to new memory registration API
    
    Instead of maintaining a fastreg page list, keep an sg table
    and convert an array of pages to a sg list. Then call ib_map_mr_sg
    and construct ib_reg_wr.
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Acked-by: Christoph Hellwig <hch@lst.de>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Selvin Xavier <selvin.xavier@avagotech.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c09414e6f91b..c82abf44e39d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -193,7 +193,8 @@ enum rpcrdma_frmr_state {
 };
 
 struct rpcrdma_frmr {
-	struct ib_fast_reg_page_list	*fr_pgl;
+	struct scatterlist		*sg;
+	int				sg_nents;
 	struct ib_mr			*fr_mr;
 	enum rpcrdma_frmr_state		fr_state;
 	struct work_struct		fr_work;

commit bb6c96d72879fe1f674a804eb95b891def4ace61
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 24 10:34:21 2015 +0300

    xprtrdma: Replace global lkey with lkey local to PD
    
    The core API has changed so that devices that do not have a global
    DMA lkey automatically create an mr, per-PD, and make that lkey
    available. The global DMA lkey interface is going away in favor of
    the per-PD DMA lkey.
    
    The per-PD DMA lkey is always available. Convert xprtrdma to use the
    device's per-PD DMA lkey for regbufs, no matter which memory
    registration scheme is in use.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Cc: linux-nfs <linux-nfs@vger.kernel.org>
    Acked-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 02512221b8bc..c09414e6f91b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -65,7 +65,6 @@ struct rpcrdma_ia {
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
 	struct ib_mr		*ri_dma_mr;
-	u32			ri_dma_lkey;
 	struct completion	ri_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_frmr_depth;

commit 4e4adb2f462889b9eac736dd06d60658beb091b6
Merge: 77a78806c7df 5445b1fbd123
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 7 14:02:24 2015 -0700

    Merge tag 'nfs-for-4.3-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      Stable patches:
       - Fix atomicity of pNFS commit list updates
       - Fix NFSv4 handling of open(O_CREAT|O_EXCL|O_RDONLY)
       - nfs_set_pgio_error sometimes misses errors
       - Fix a thinko in xs_connect()
       - Fix borkage in _same_data_server_addrs_locked()
       - Fix a NULL pointer dereference of migration recovery ops for v4.2
         client
       - Don't let the ctime override attribute barriers.
       - Revert "NFSv4: Remove incorrect check in can_open_delegated()"
       - Ensure flexfiles pNFS driver updates the inode after write finishes
       - flexfiles must not pollute the attribute cache with attrbutes from
         the DS
       - Fix a protocol error in layoutreturn
       - Fix a protocol issue with NFSv4.1 CLOSE stateids
    
      Bugfixes + cleanups
       - pNFS blocks bugfixes from Christoph
       - Various cleanups from Anna
       - More fixes for delegation corner cases
       - Don't fsync twice for O_SYNC/IS_SYNC files
       - Fix pNFS and flexfiles layoutstats bugs
       - pnfs/flexfiles: avoid duplicate tracking of mirror data
       - pnfs: Fix layoutget/layoutreturn/return-on-close serialisation
         issues
       - pnfs/flexfiles: error handling retries a layoutget before fallback
         to MDS
    
      Features:
       - Full support for the OPEN NFS4_CREATE_EXCLUSIVE4_1 mode from
         Kinglong
       - More RDMA client transport improvements from Chuck
       - Removal of the deprecated ib_reg_phys_mr() and ib_rereg_phys_mr()
         verbs from the SUNRPC, Lustre and core infiniband tree.
       - Optimise away the close-to-open getattr if there is no cached data"
    
    * tag 'nfs-for-4.3-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (108 commits)
      NFSv4: Respect the server imposed limit on how many changes we may cache
      NFSv4: Express delegation limit in units of pages
      Revert "NFS: Make close(2) asynchronous when closing NFS O_DIRECT files"
      NFS: Optimise away the close-to-open getattr if there is no cached data
      NFSv4.1/flexfiles: Clean up ff_layout_write_done_cb/ff_layout_commit_done_cb
      NFSv4.1/flexfiles: Mark the layout for return in ff_layout_io_track_ds_error()
      nfs: Remove unneeded checking of the return value from scnprintf
      nfs: Fix truncated client owner id without proto type
      NFSv4.1/flexfiles: Mark layout for return if the mirrors are invalid
      NFSv4.1/flexfiles: RW layouts are valid only if all mirrors are valid
      NFSv4.1/flexfiles: Fix incorrect usage of pnfs_generic_mark_devid_invalid()
      NFSv4.1/flexfiles: Fix freeing of mirrors
      NFSv4.1/pNFS: Don't request a minimal read layout beyond the end of file
      NFSv4.1/pnfs: Handle LAYOUTGET return values correctly
      NFSv4.1/pnfs: Don't ask for a read layout for an empty file.
      NFSv4.1: Fix a protocol issue with CLOSE stateids
      NFSv4.1/flexfiles: Don't mark the entire deviceid as bad for file errors
      SUNRPC: Prevent SYN+SYNACK+RST storms
      SUNRPC: xs_reset_transport must mark the connection as disconnected
      NFSv4.1/pnfs: Ensure layoutreturn reserves space for the opaque payload
      ...

commit cc9a903d915c21626b6b2fbf8ed0ff16a7f82210
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Aug 7 16:55:46 2015 -0400

    svcrdma: Change maximum server payload back to RPCSVC_MAXPAYLOAD
    
    Both commit 0380a3f375 ("svcrdma: Add a separate "max data segs"
    macro for svcrdma") and commit 7e5be28827bf ("svcrdma: advertise
    the correct max payload") are incorrect. This commit reverts both
    changes, restoring the server's maximum payload size to 1MB.
    
    Commit 7e5be28827bf based the server's maximum payload on the
    _client's_ RPCRDMA_MAX_DATA_SEGS value. That was wrong.
    
    Commit 0380a3f375 tried to fix this so that the client maximum
    payload size could be raised without affecting the server, but
    managed to confuse matters more on the server side.
    
    More importantly, limiting the advertised maximum payload size was
    meant to be a workaround, not the actual fix. We need to revisit
    
      https://bugzilla.linux-nfs.org/show_bug.cgi?id=270
    
    A Linux client on a platform with 64KB pages can overrun and crash
    an x86_64 NFS/RDMA server when the r/wsize is 1MB. An x86/64 Linux
    client seems to work fine using 1MB reads and writes when the Linux
    server's maximum payload size is restored to 1MB.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=270
    Fixes: 0380a3f375 ("svcrdma: Add a separate "max data segs" macro")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f49dd8b38122..e718d0959af3 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -51,7 +51,6 @@
 #include <linux/sunrpc/clnt.h> 		/* rpc_xprt */
 #include <linux/sunrpc/rpc_rdma.h> 	/* RPC/RDMA protocol */
 #include <linux/sunrpc/xprtrdma.h> 	/* xprt parameters */
-#include <linux/sunrpc/svc.h>		/* RPCSVC_MAXPAYLOAD */
 
 #define RDMA_RESOLVE_TIMEOUT	(5000)	/* 5 seconds */
 #define RDMA_CONNECT_RETRY_MAX	(2)	/* retries if no listener backlog */

commit 860477d1ff176549f2bf438b61e5c1ec6b1d43e5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:04:45 2015 -0400

    xprtrdma: Count RDMA_NOMSG type calls
    
    RDMA_NOMSG type calls are less efficient than RDMA_MSG. Count NOMSG
    calls so administrators can tell if they happen to be used more than
    expected.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 8422c09043b0..d252457ff21a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -340,6 +340,7 @@ struct rpcrdma_stats {
 	unsigned long		hardway_register_count;
 	unsigned long		failed_marshal_count;
 	unsigned long		bad_reply_count;
+	unsigned long		nomsg_call_count;
 };
 
 /*

commit b3221d6a53c44cd572a3a400abdd1e2a24bea587
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:03:39 2015 -0400

    xprtrdma: Remove logic that constructs RDMA_MSGP type calls
    
    RDMA_MSGP type calls insert a zero pad in the middle of the RPC
    message to align the RPC request's data payload to the server's
    alignment preferences. A server can then "page flip" the payload
    into place to avoid a data copy in certain circumstances. However:
    
    1. The client has to have a priori knowledge of the server's
       preferred alignment
    
    2. Requests eligible for RDMA_MSGP are requests that are small
       enough to have been sent inline, and convey a data payload
       at the _end_ of the RPC message
    
    Today 1. is done with a sysctl, and is a global setting that is
    copied during mount. Linux does not support CCP to query the
    server's preferences (RFC 5666, Section 6).
    
    A small-ish NFSv3 WRITE might use RDMA_MSGP, but no NFSv4
    compound fits bullet 2.
    
    Thus the Linux client currently leaves RDMA_MSGP disabled. The
    Linux server handles RDMA_MSGP, but does not use any special
    page flipping, so it confers no benefit.
    
    Clean up the marshaling code by removing the logic that constructs
    RDMA_MSGP type calls. This also reduces the maximum send iovec size
    from four to just two elements.
    
    /proc/sys/sunrpc/rdma_inline_write_padding is a kernel API, and
    thus is left in place.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 82190118b8d9..8422c09043b0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -88,7 +88,6 @@ struct rpcrdma_ep {
 	int			rep_connected;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
-	struct rpcrdma_regbuf	*rep_padbuf;
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;
@@ -255,16 +254,18 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 	char		*mr_offset;	/* kva if no page, else offset */
 };
 
+#define RPCRDMA_MAX_IOVS	(2)
+
 struct rpcrdma_req {
-	unsigned int	rl_niovs;	/* 0, 2 or 4 */
-	unsigned int	rl_nchunks;	/* non-zero if chunks */
-	unsigned int	rl_connect_cookie;	/* retry detection */
-	struct rpcrdma_buffer *rl_buffer; /* home base for this structure */
+	unsigned int		rl_niovs;
+	unsigned int		rl_nchunks;
+	unsigned int		rl_connect_cookie;
+	struct rpcrdma_buffer	*rl_buffer;
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
-	struct ib_sge	rl_send_iov[4];	/* for active requests */
-	struct rpcrdma_regbuf *rl_rdmabuf;
-	struct rpcrdma_regbuf *rl_sendbuf;
-	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];
+	struct ib_sge		rl_send_iov[RPCRDMA_MAX_IOVS];
+	struct rpcrdma_regbuf	*rl_rdmabuf;
+	struct rpcrdma_regbuf	*rl_sendbuf;
+	struct rpcrdma_mr_seg	rl_segments[RPCRDMA_MAX_SEGS];
 };
 
 static inline struct rpcrdma_req *

commit d1ed857e5707e073973cfb1b8df801053a356518
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:03:30 2015 -0400

    xprtrdma: Clean up rpcrdma_ia_open()
    
    Untangle the end of rpcrdma_ia_open() by moving DMA MR set-up, which
    is different for each registration method, to the .ro_open functions.
    
    This is refactoring only. No behavior change is expected.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ce4e79e4cf1d..82190118b8d9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -65,9 +65,8 @@ struct rpcrdma_ia {
 	struct ib_device	*ri_device;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
-	struct ib_mr		*ri_bind_mem;
+	struct ib_mr		*ri_dma_mr;
 	u32			ri_dma_lkey;
-	int			ri_have_dma_lkey;
 	struct completion	ri_done;
 	int			ri_async_rc;
 	unsigned int		ri_max_frmr_depth;

commit e531dcabec8dc2ee141aab01ddf20ca87c52d916
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:03:20 2015 -0400

    xprtrdma: Remove last ib_reg_phys_mr() call site
    
    All HCA providers have an ib_get_dma_mr() verb. Thus
    rpcrdma_ia_open() will either grab the device's local_dma_key if one
    is available, or it will call ib_get_dma_mr(). If ib_get_dma_mr()
    fails, rpcrdma_ia_open() fails and no transport is created.
    
    Therefore execution never reaches the ib_reg_phys_mr() call site in
    rpcrdma_register_internal(), so it can be removed.
    
    The remaining logic in rpcrdma_{de}register_internal() is folded
    into rpcrdma_{alloc,free}_regbuf().
    
    This is clean up only. No behavior change is expected.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-By: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index abee4728f885..ce4e79e4cf1d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -119,7 +119,6 @@ struct rpcrdma_ep {
 struct rpcrdma_regbuf {
 	size_t			rg_size;
 	struct rpcrdma_req	*rg_owner;
-	struct ib_mr		*rg_mr;
 	struct ib_sge		rg_iov;
 	__be32			rg_base[0] __attribute__ ((aligned(256)));
 };

commit 864be126fe5f325ec4b3e28173ca5cad2b8ee28c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:02:50 2015 -0400

    xprtrdma: Raise maximum payload size to one megabyte
    
    The point of larger rsize and wsize is to reduce the per-byte cost
    of memory registration and deregistration. Modern HCAs can typically
    handle a megabyte or more with a single registration operation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-By: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f49dd8b38122..abee4728f885 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -165,8 +165,7 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
  * struct rpcrdma_buffer. N is the max number of outstanding requests.
  */
 
-/* temporary static scatter/gather max */
-#define RPCRDMA_MAX_DATA_SEGS	(64)	/* max scatter/gather */
+#define RPCRDMA_MAX_DATA_SEGS	((1 * 1024 * 1024) / PAGE_SIZE)
 #define RPCRDMA_MAX_SEGS 	(RPCRDMA_MAX_DATA_SEGS + 2) /* head+tail = 2 */
 
 struct rpcrdma_buffer;

commit 8688d9540cc6e17df4cba71615e27f04e0378fe6
Merge: 320cd413faef b4839ebe21fc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 2 11:32:23 2015 -0700

    Merge tag 'nfs-for-4.2-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      Stable patches:
       - Fix a crash in the NFSv4 file locking code.
       - Fix an fsync() regression, where we were failing to retry I/O in
         some circumstances.
       - Fix an infinite loop in NFSv4.0 OPEN stateid recovery
       - Fix a memory leak when an attempted pnfs fails.
       - Fix a memory leak in the backchannel code
       - Large hostnames were not supported correctly in NFSv4.1
       - Fix a pNFS/flexfiles bug that was impeding error reporting on I/O.
       - Fix a couple of credential issues in pNFS/flexfiles
    
      Bugfixes + cleanups:
       - Open flag sanity checks in the NFSv4 atomic open codepath
       - More NFSv4 delegation related bugfixes
       - Various NFSv4.1 backchannel bugfixes and cleanups
       - Fix the NFS swap socket code
       - Various cleanups of the NFSv4 SETCLIENTID and EXCHANGE_ID code
       - Fix a UDP transport deadlock issue
    
      Features:
       - More RDMA client transport improvements
       - NFSv4.2 LAYOUTSTATS functionality for pnfs flexfiles"
    
    * tag 'nfs-for-4.2-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (87 commits)
      nfs: Remove invalid tk_pid from debug message
      nfs: Remove invalid NFS_ATTR_FATTR_V4_REFERRAL checking in nfs4_get_rootfh
      nfs: Drop bad comment in nfs41_walk_client_list()
      nfs: Remove unneeded micro checking of CONFIG_PROC_FS
      nfs: Don't setting FILE_CREATED flags always
      nfs: Use remove_proc_subtree() instead remove_proc_entry()
      nfs: Remove unused argument in nfs_server_set_fsinfo()
      nfs: Fix a memory leak when meeting an unsupported state protect
      nfs: take extra reference to fl->fl_file when running a LOCKU operation
      NFSv4: When returning a delegation, don't reclaim an incompatible open mode.
      NFSv4.2: LAYOUTSTATS is optional to implement
      NFSv4.2: Fix up a decoding error in layoutstats
      pNFS/flexfiles: Fix the reset of struct pgio_header when resending
      pNFS/flexfiles: Turn off layoutcommit for servers that don't need it
      pnfs/flexfiles: protect ktime manipulation with mirror lock
      nfs: provide pnfs_report_layoutstat when NFS42 is disabled
      nfs: verify open flags before allowing open
      nfs: always update creds in mirror, even when we have an already connected ds
      nfs: fix potential credential leak in ff_layout_update_mirror_cred
      pnfs/flexfiles: report layoutstat regularly
      ...

commit acb9da7a57d7905c46d0b69d30fcf944eae261de
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:53:23 2015 -0400

    xprtrdma: Stack relief in fmr_op_map()
    
    fmr_op_map() declares a 64 element array of u64 in automatic
    storage. This is 512 bytes (8 * 64) on the stack.
    
    Instead, when FMR memory registration is in use, pre-allocate a
    physaddr array for each rpcrdma_mw.
    
    This is a pre-requisite for increasing the r/wsize maximum for
    FMR on platforms with 4KB pages.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index df92884400c4..110d685b4ac5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -206,9 +206,14 @@ struct rpcrdma_frmr {
 	struct rpcrdma_xprt		*fr_xprt;
 };
 
+struct rpcrdma_fmr {
+	struct ib_fmr		*fmr;
+	u64			*physaddrs;
+};
+
 struct rpcrdma_mw {
 	union {
-		struct ib_fmr		*fmr;
+		struct rpcrdma_fmr	fmr;
 		struct rpcrdma_frmr	frmr;
 	} r;
 	void			(*mw_sendcompletion)(struct ib_wc *);

commit 58d1dcf5a8ebb0ce8a521286a99efdd636012bf0
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:53:13 2015 -0400

    xprtrdma: Split rb_lock
    
    /proc/lock_stat showed contention between rpcrdma_buffer_get/put
    and the MR allocation functions during I/O intensive workloads.
    
    Now that MRs are no longer allocated in rpcrdma_buffer_get(),
    there's no reason the rb_mws list has to be managed using the
    same lock as the send/receive buffers. Split that lock. The
    new lock does not need to disable interrupts because buffer
    get/put is never called in an interrupt context.
    
    struct rpcrdma_buffer is re-arranged to ensure rb_mwlock and rb_mws
    are always in a different cacheline than rb_lock and the buffer
    pointers.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3ecee38bf1a0..df92884400c4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -282,15 +282,17 @@ rpcr_to_rdmar(struct rpc_rqst *rqst)
  * One of these is associated with a transport instance
  */
 struct rpcrdma_buffer {
-	spinlock_t	rb_lock;	/* protects indexes */
-	u32		rb_max_requests;/* client max requests */
-	struct list_head rb_mws;	/* optional memory windows/fmrs/frmrs */
-	struct list_head rb_all;
-	int		rb_send_index;
+	spinlock_t		rb_mwlock;	/* protect rb_mws list */
+	struct list_head	rb_mws;
+	struct list_head	rb_all;
+	char			*rb_pool;
+
+	spinlock_t		rb_lock;	/* protect buf arrays */
+	u32			rb_max_requests;
+	int			rb_send_index;
+	int			rb_recv_index;
 	struct rpcrdma_req	**rb_send_bufs;
-	int		rb_recv_index;
 	struct rpcrdma_rep	**rb_recv_bufs;
-	char		*rb_pool;
 };
 #define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
 

commit 7e53df111beea8db2543424d07bdee2a630698c3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:53:04 2015 -0400

    xprtrdma: Remove rpcrdma_ia::ri_memreg_strategy
    
    Clean up: This field is no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f19376d35750..3ecee38bf1a0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -70,7 +70,6 @@ struct rpcrdma_ia {
 	int			ri_have_dma_lkey;
 	struct completion	ri_done;
 	int			ri_async_rc;
-	enum rpcrdma_memreg	ri_memreg_strategy;
 	unsigned int		ri_max_frmr_depth;
 	struct ib_device_attr	ri_devattr;
 	struct ib_qp_attr	ri_qp_attr;

commit 3269a94b6206d4fe10dd96cb37e6b0035ee42cd2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:52:54 2015 -0400

    xprtrdma: Remove ->ro_reset
    
    An RPC can exit at any time. When it does so, xprt_rdma_free() is
    called, and it calls ->op_unmap().
    
    If ->ro_reset() is running due to a transport disconnect, the two
    methods can race while processing the same rpcrdma_mw. The results
    are unpredictable.
    
    Because of this, in previous patches I've altered ->ro_map() to
    handle MR reset. ->ro_reset() is no longer needed and can be
    removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c5862a4a5192..f19376d35750 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -352,7 +352,6 @@ struct rpcrdma_memreg_ops {
 				   struct rpcrdma_create_data_internal *);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	int		(*ro_init)(struct rpcrdma_xprt *);
-	void		(*ro_reset)(struct rpcrdma_xprt *);
 	void		(*ro_destroy)(struct rpcrdma_buffer *);
 	const char	*ro_displayname;
 };

commit 951e721ca0d665ece6175b8d8edf93cf7b215bd5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:52:25 2015 -0400

    xprtrdma: Introduce an FRMR recovery workqueue
    
    After a transport disconnect, FRMRs can be left in an undetermined
    state. In particular, the MR's rkey is no good.
    
    Currently, FRMRs are fixed up by the transport connect worker, but
    that can race with ->ro_unmap if an RPC happens to exit while the
    transport connect worker is running.
    
    A better way of dealing with broken FRMRs is to detect them before
    they are re-used by ->ro_map. Such FRMRs are either already invalid
    or are owned by the sending RPC, and thus no race with ->ro_unmap
    is possible.
    
    Introduce a mechanism for handing broken FRMRs to a workqueue to be
    reset in a context that is appropriate for allocating resources
    (ie. an ib_alloc_fast_reg_mr() API call).
    
    This mechanism is not yet used, but will be in subsequent patches.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5b801d561e52..c5862a4a5192 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -203,6 +203,8 @@ struct rpcrdma_frmr {
 	struct ib_fast_reg_page_list	*fr_pgl;
 	struct ib_mr			*fr_mr;
 	enum rpcrdma_frmr_state		fr_state;
+	struct work_struct		fr_work;
+	struct rpcrdma_xprt		*fr_xprt;
 };
 
 struct rpcrdma_mw {
@@ -427,6 +429,9 @@ void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 
 unsigned int rpcrdma_max_segments(struct rpcrdma_xprt *);
 
+int frwr_alloc_recovery_wq(void);
+void frwr_destroy_recovery_wq(void);
+
 /*
  * Wrappers for chunk registration, shared by read/write chunk code.
  */

commit 346aa66b2ab7988ca105f7fee5a968c11712b0d8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:52:06 2015 -0400

    xprtrdma: Introduce helpers for allocating MWs
    
    We eventually want to handle allocating MWs one at a time, as
    needed, instead of grabbing 64 and throwing them at each RPC in the
    pipeline.
    
    Add a helper for grabbing an MW off rb_mws, and a helper for
    returning an MW to rb_mws. These will be used in a subsequent patch.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 300423dea19c..5b801d561e52 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -413,6 +413,8 @@ int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_ep *,
 int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 
+struct rpcrdma_mw *rpcrdma_get_mw(struct rpcrdma_xprt *);
+void rpcrdma_put_mw(struct rpcrdma_xprt *, struct rpcrdma_mw *);
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
 void rpcrdma_buffer_put(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_get(struct rpcrdma_req *);

commit 89e0d11258e9a69d550fd4bfb4609e955bdd84ee
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:51:56 2015 -0400

    xprtrdma: Use ib_device pointer safely
    
    The connect worker can replace ri_id, but prevents ri_id->device
    from changing during the lifetime of a transport instance. The old
    ID is kept around until a new ID is created and the ->device is
    confirmed to be the same.
    
    Cache a copy of ri_id->device in rpcrdma_ia and in rpcrdma_rep.
    The cached copy can be used safely in code that does not serialize
    with the connect worker.
    
    Other code can use it to save an extra address generation (one
    pointer dereference instead of two).
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 230e7fe5e31a..300423dea19c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -62,6 +62,7 @@
 struct rpcrdma_ia {
 	const struct rpcrdma_memreg_ops	*ri_ops;
 	rwlock_t		ri_qplock;
+	struct ib_device	*ri_device;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
 	struct ib_mr		*ri_bind_mem;
@@ -173,6 +174,7 @@ struct rpcrdma_buffer;
 
 struct rpcrdma_rep {
 	unsigned int		rr_len;
+	struct ib_device	*rr_device;
 	struct rpcrdma_xprt	*rr_rxprt;
 	struct list_head	rr_list;
 	struct rpcrdma_regbuf	*rr_rdmabuf;

commit 494ae30d2a47cf439c6a680cc62e09ae0c51d190
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:51:46 2015 -0400

    xprtrdma: Remove rr_func
    
    A posted rpcrdma_rep never has rr_func set to anything but
    rpcrdma_reply_handler.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c3d57c0fd4a5..230e7fe5e31a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -174,7 +174,6 @@ struct rpcrdma_buffer;
 struct rpcrdma_rep {
 	unsigned int		rr_len;
 	struct rpcrdma_xprt	*rr_rxprt;
-	void			(*rr_func)(struct rpcrdma_rep *);
 	struct list_head	rr_list;
 	struct rpcrdma_regbuf	*rr_rdmabuf;
 };

commit fed171b35c7c0777fa0d6aeb3f25cc9b0d5f56ad
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:51:37 2015 -0400

    xprtrdma: Replace rpcrdma_rep::rr_buffer with rr_rxprt
    
    Clean up: Instead of carrying a pointer to the buffer pool and
    the rpc_xprt, carry a pointer to the controlling rpcrdma_xprt.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 78e0b8beaa36..c3d57c0fd4a5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -173,8 +173,7 @@ struct rpcrdma_buffer;
 
 struct rpcrdma_rep {
 	unsigned int		rr_len;
-	struct rpcrdma_buffer	*rr_buffer;
-	struct rpc_xprt		*rr_xprt;
+	struct rpcrdma_xprt	*rr_rxprt;
 	void			(*rr_func)(struct rpcrdma_rep *);
 	struct list_head	rr_list;
 	struct rpcrdma_regbuf	*rr_rdmabuf;

commit ffe1f0df586237a18f9b568597bddabb56e96d5e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 4 11:21:42 2015 -0400

    rpcrdma: Merge svcrdma and xprtrdma modules into one
    
    Bi-directional RPC support means code in svcrdma.ko invokes a bit of
    code in xprtrdma.ko, and vice versa. To avoid loader/linker loops,
    merge the server and client side modules together into a single
    module.
    
    When backchannel capabilities are added, the combined module will
    register all needed transport capabilities so that Upper Layer
    consumers automatically have everything needed to create a
    bi-directional transport connection.
    
    Module aliases are added for backwards compatibility with user
    space, which still may expect svcrdma.ko or xprtrdma.ko to be
    present.
    
    This commit reverts commit 2e8c12e1b765 ("xprtrdma: add separate
    Kconfig options for NFSoRDMA client and server support") and
    provides a single CONFIG option for enabling the new module.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index e60907b0e1f1..58163b88738c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -480,6 +480,11 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
  */
 int rpcrdma_marshal_req(struct rpc_rqst *);
 
+/* RPC/RDMA module init - xprtrdma/transport.c
+ */
+int xprt_rdma_init(void);
+void xprt_rdma_cleanup(void);
+
 /* Temporary NFS request map cache. Created in svc_rdma.c  */
 extern struct kmem_cache *svc_rdma_map_cachep;
 /* WR context cache. Created in svc_rdma.c  */

commit 0380a3f37540ad0582b3c749a74fc127af914689
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 4 11:21:32 2015 -0400

    svcrdma: Add a separate "max data segs macro for svcrdma
    
    The server and client maximum are architecturally independent.
    Allow changing one without affecting the other.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 78e0b8beaa36..e60907b0e1f1 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -487,10 +487,4 @@ extern struct kmem_cache *svc_rdma_ctxt_cachep;
 /* Workqueue created in svc_rdma.c */
 extern struct workqueue_struct *svc_rdma_wq;
 
-#if RPCSVC_MAXPAYLOAD < (RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT)
-#define RPCSVC_MAXPAYLOAD_RDMA RPCSVC_MAXPAYLOAD
-#else
-#define RPCSVC_MAXPAYLOAD_RDMA (RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT)
-#endif
-
 #endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */

commit d654788e98f74f2df8dfc6079fa314938f739486
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:35:44 2015 -0400

    xprtrdma: Make rpcrdma_{un}map_one() into inline functions
    
    These functions are called in a loop for each page transferred via
    RDMA READ or WRITE. Extract loop invariants and inline them to
    reduce CPU overhead.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 54bcbe47075d..78e0b8beaa36 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -424,8 +424,49 @@ void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 			 struct rpcrdma_regbuf *);
 
 unsigned int rpcrdma_max_segments(struct rpcrdma_xprt *);
-void rpcrdma_map_one(struct rpcrdma_ia *, struct rpcrdma_mr_seg *, bool);
-void rpcrdma_unmap_one(struct rpcrdma_ia *, struct rpcrdma_mr_seg *);
+
+/*
+ * Wrappers for chunk registration, shared by read/write chunk code.
+ */
+
+void rpcrdma_mapping_error(struct rpcrdma_mr_seg *);
+
+static inline enum dma_data_direction
+rpcrdma_data_dir(bool writing)
+{
+	return writing ? DMA_FROM_DEVICE : DMA_TO_DEVICE;
+}
+
+static inline void
+rpcrdma_map_one(struct ib_device *device, struct rpcrdma_mr_seg *seg,
+		enum dma_data_direction direction)
+{
+	seg->mr_dir = direction;
+	seg->mr_dmalen = seg->mr_len;
+
+	if (seg->mr_page)
+		seg->mr_dma = ib_dma_map_page(device,
+				seg->mr_page, offset_in_page(seg->mr_offset),
+				seg->mr_dmalen, seg->mr_dir);
+	else
+		seg->mr_dma = ib_dma_map_single(device,
+				seg->mr_offset,
+				seg->mr_dmalen, seg->mr_dir);
+
+	if (ib_dma_mapping_error(device, seg->mr_dma))
+		rpcrdma_mapping_error(seg);
+}
+
+static inline void
+rpcrdma_unmap_one(struct ib_device *device, struct rpcrdma_mr_seg *seg)
+{
+	if (seg->mr_page)
+		ib_dma_unmap_page(device,
+				  seg->mr_dma, seg->mr_dmalen, seg->mr_dir);
+	else
+		ib_dma_unmap_single(device,
+				    seg->mr_dma, seg->mr_dmalen, seg->mr_dir);
+}
 
 /*
  * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c

commit e46ac34c3c34e408435656a5fed605c4c787d081
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:35:35 2015 -0400

    xprtrdma: Handle non-SEND completions via a callout
    
    Allow each memory registration mode to plug in a callout that handles
    the completion of a memory registration operation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9036fb4174d5..54bcbe47075d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -106,6 +106,10 @@ struct rpcrdma_ep {
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
 #define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
 
+/* Force completion handler to ignore the signal
+ */
+#define RPCRDMA_IGNORE_COMPLETION	(0ULL)
+
 /* Registered buffer -- registered kmalloc'd memory for RDMA SEND/RECV
  *
  * The below structure appears at the front of a large region of kmalloc'd
@@ -206,6 +210,7 @@ struct rpcrdma_mw {
 		struct ib_fmr		*fmr;
 		struct rpcrdma_frmr	frmr;
 	} r;
+	void			(*mw_sendcompletion)(struct ib_wc *);
 	struct list_head	mw_list;
 	struct list_head	mw_all;
 };

commit 3968cb58501bf526eed1441f4ef237028aa9cd2d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:35:26 2015 -0400

    xprtrdma: Add "open" memreg op
    
    The open op determines the size of various transport data structures
    based on device capabilities and memory registration mode.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b95e223d3d69..9036fb4174d5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -340,6 +340,9 @@ struct rpcrdma_memreg_ops {
 				  struct rpcrdma_mr_seg *, int, bool);
 	int		(*ro_unmap)(struct rpcrdma_xprt *,
 				    struct rpcrdma_mr_seg *);
+	int		(*ro_open)(struct rpcrdma_ia *,
+				   struct rpcrdma_ep *,
+				   struct rpcrdma_create_data_internal *);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	int		(*ro_init)(struct rpcrdma_xprt *);
 	void		(*ro_reset)(struct rpcrdma_xprt *);

commit 4561f347d49c645fd81d1f47b0fb460e8a6e4587
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:35:17 2015 -0400

    xprtrdma: Add "destroy MRs" memreg op
    
    Memory Region objects associated with a transport instance are
    destroyed before the instance is shutdown and destroyed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 06802394cf89..b95e223d3d69 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -343,6 +343,7 @@ struct rpcrdma_memreg_ops {
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	int		(*ro_init)(struct rpcrdma_xprt *);
 	void		(*ro_reset)(struct rpcrdma_xprt *);
+	void		(*ro_destroy)(struct rpcrdma_buffer *);
 	const char	*ro_displayname;
 };
 

commit 31a701a94751509bb72e13d851f18ddcf22ff722
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:35:07 2015 -0400

    xprtrdma: Add "reset MRs" memreg op
    
    This method is invoked when a transport instance is about to be
    reconnected. Each Memory Region object is reset to its initial
    state.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 90b60feab45a..06802394cf89 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -342,6 +342,7 @@ struct rpcrdma_memreg_ops {
 				    struct rpcrdma_mr_seg *);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	int		(*ro_init)(struct rpcrdma_xprt *);
+	void		(*ro_reset)(struct rpcrdma_xprt *);
 	const char	*ro_displayname;
 };
 

commit 91e70e70e47b3355bb0a8b3b196c93897dcdb440
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:34:58 2015 -0400

    xprtrdma: Add "init MRs" memreg op
    
    This method is used when setting up a new transport instance to
    create a pool of Memory Region objects that will be used to register
    memory during operation.
    
    Memory Regions are not needed for "physical" registration, since
    ->prepare and ->release are no-ops for that mode.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9a727f960df9..90b60feab45a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -341,6 +341,7 @@ struct rpcrdma_memreg_ops {
 	int		(*ro_unmap)(struct rpcrdma_xprt *,
 				    struct rpcrdma_mr_seg *);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
+	int		(*ro_init)(struct rpcrdma_xprt *);
 	const char	*ro_displayname;
 };
 

commit 6814baead86b5d44096ddfbb6f944163578e68c3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:34:48 2015 -0400

    xprtrdma: Add a "deregister_external" op for each memreg mode
    
    There is very little common processing among the different external
    memory deregistration functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 7bf077bf751d..9a727f960df9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -338,6 +338,8 @@ struct rpcrdma_xprt;
 struct rpcrdma_memreg_ops {
 	int		(*ro_map)(struct rpcrdma_xprt *,
 				  struct rpcrdma_mr_seg *, int, bool);
+	int		(*ro_unmap)(struct rpcrdma_xprt *,
+				    struct rpcrdma_mr_seg *);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	const char	*ro_displayname;
 };
@@ -405,9 +407,6 @@ void rpcrdma_buffer_put(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
-int rpcrdma_deregister_external(struct rpcrdma_mr_seg *,
-				struct rpcrdma_xprt *);
-
 struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
 					    size_t, gfp_t);
 void rpcrdma_free_regbuf(struct rpcrdma_ia *,

commit 9c1b4d775f2d7dd5bb806e3de2f3e1244a7cbd16
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:34:39 2015 -0400

    xprtrdma: Add a "register_external" op for each memreg mode
    
    There is very little common processing among the different external
    memory registration functions. Have rpcrdma_create_chunks() call
    the registration method directly. This removes a stack frame and a
    switch statement from the external registration path.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 59e627e96a0b..7bf077bf751d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -336,6 +336,8 @@ struct rpcrdma_stats {
  */
 struct rpcrdma_xprt;
 struct rpcrdma_memreg_ops {
+	int		(*ro_map)(struct rpcrdma_xprt *,
+				  struct rpcrdma_mr_seg *, int, bool);
 	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	const char	*ro_displayname;
 };
@@ -403,8 +405,6 @@ void rpcrdma_buffer_put(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
-int rpcrdma_register_external(struct rpcrdma_mr_seg *,
-				int, int, struct rpcrdma_xprt *);
 int rpcrdma_deregister_external(struct rpcrdma_mr_seg *,
 				struct rpcrdma_xprt *);
 
@@ -414,6 +414,8 @@ void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 			 struct rpcrdma_regbuf *);
 
 unsigned int rpcrdma_max_segments(struct rpcrdma_xprt *);
+void rpcrdma_map_one(struct rpcrdma_ia *, struct rpcrdma_mr_seg *, bool);
+void rpcrdma_unmap_one(struct rpcrdma_ia *, struct rpcrdma_mr_seg *);
 
 /*
  * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c

commit 1c9351ee0e346ec1b3c700a4bc8f881923e1808e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:34:30 2015 -0400

    xprtrdma: Add a "max_payload" op for each memreg mode
    
    The max_payload computation is generalized to ensure that the
    payload maximum is the lesser of RPC_MAX_DATA_SEGS and the number of
    data segments that can be transmitted in an inline buffer.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ef3cf4aeecd6..59e627e96a0b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -334,7 +334,9 @@ struct rpcrdma_stats {
 /*
  * Per-registration mode operations
  */
+struct rpcrdma_xprt;
 struct rpcrdma_memreg_ops {
+	size_t		(*ro_maxpages)(struct rpcrdma_xprt *);
 	const char	*ro_displayname;
 };
 
@@ -411,6 +413,8 @@ struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
 void rpcrdma_free_regbuf(struct rpcrdma_ia *,
 			 struct rpcrdma_regbuf *);
 
+unsigned int rpcrdma_max_segments(struct rpcrdma_xprt *);
+
 /*
  * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c
  */
@@ -422,7 +426,6 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
 int rpcrdma_marshal_req(struct rpc_rqst *);
-size_t rpcrdma_max_payload(struct rpcrdma_xprt *);
 
 /* Temporary NFS request map cache. Created in svc_rdma.c  */
 extern struct kmem_cache *svc_rdma_map_cachep;

commit a0ce85f595c22d28bf03c3fae8545b3077b7be1b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:34:21 2015 -0400

    xprtrdma: Add vector of ops for each memory registration strategy
    
    Instead of employing switch() statements, let's use the typical
    Linux kernel idiom for handling behavioral variation: virtual
    functions.
    
    Start by defining a vector of operations for each supported memory
    registration mode, and by adding a source file for each mode.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c8afd83e8b75..ef3cf4aeecd6 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -60,6 +60,7 @@
  * Interface Adapter -- one per transport instance
  */
 struct rpcrdma_ia {
+	const struct rpcrdma_memreg_ops	*ri_ops;
 	rwlock_t		ri_qplock;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
@@ -330,6 +331,17 @@ struct rpcrdma_stats {
 	unsigned long		bad_reply_count;
 };
 
+/*
+ * Per-registration mode operations
+ */
+struct rpcrdma_memreg_ops {
+	const char	*ro_displayname;
+};
+
+extern const struct rpcrdma_memreg_ops rpcrdma_fmr_memreg_ops;
+extern const struct rpcrdma_memreg_ops rpcrdma_frwr_memreg_ops;
+extern const struct rpcrdma_memreg_ops rpcrdma_physical_memreg_ops;
+
 /*
  * RPCRDMA transport -- encapsulates the structures above for
  * integration with RPC.

commit e23779451ee05e824fedbb68bd17fc5c77e40166
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:33:53 2015 -0400

    xprtrdma: Perform a full marshal on retransmit
    
    Commit 6ab59945f292 ("xprtrdma: Update rkeys after transport
    reconnect" added logic in the ->send_request path to update the
    chunk list when an RPC/RDMA request is retransmitted.
    
    Note that rpc_xdr_encode() resets and re-encodes the entire RPC
    send buffer for each retransmit of an RPC. The RPC send buffer
    is not preserved from the previous transmission of an RPC.
    
    Revert 6ab59945f292, and instead, just force each request to be
    fully marshaled every time through ->send_request. This should
    preserve the fix from 6ab59945f292, while also performing pullup
    during retransmits.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0a16fb6f0885..c8afd83e8b75 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -143,14 +143,6 @@ rdmab_to_msg(struct rpcrdma_regbuf *rb)
 	return (struct rpcrdma_msg *)rb->rg_base;
 }
 
-enum rpcrdma_chunktype {
-	rpcrdma_noch = 0,
-	rpcrdma_readch,
-	rpcrdma_areadch,
-	rpcrdma_writech,
-	rpcrdma_replych
-};
-
 /*
  * struct rpcrdma_rep -- this structure encapsulates state required to recv
  * and complete a reply, asychronously. It needs several pieces of
@@ -258,7 +250,6 @@ struct rpcrdma_req {
 	unsigned int	rl_niovs;	/* 0, 2 or 4 */
 	unsigned int	rl_nchunks;	/* non-zero if chunks */
 	unsigned int	rl_connect_cookie;	/* retry detection */
-	enum rpcrdma_chunktype	rl_rtype, rl_wtype;
 	struct rpcrdma_buffer *rl_buffer; /* home base for this structure */
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct ib_sge	rl_send_iov[4];	/* for active requests */
@@ -418,7 +409,6 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
-ssize_t rpcrdma_marshal_chunks(struct rpc_rqst *, ssize_t);
 int rpcrdma_marshal_req(struct rpc_rqst *);
 size_t rpcrdma_max_payload(struct rpcrdma_xprt *);
 

commit 9b1dcbc8cf4679ecf090b343ac31bda6e55ddabe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Feb 12 10:14:51 2015 -0500

    xprtrdma: Store RDMA credits in unsigned variables
    
    Dan Carpenter's static checker pointed out:
    
       net/sunrpc/xprtrdma/rpc_rdma.c:879 rpcrdma_reply_handler()
       warn: can 'credits' be negative?
    
    "credits" is defined as an int. The credits value comes from the
    server as a 32-bit unsigned integer.
    
    A malicious or broken server can plant a large unsigned integer in
    that field which would result in an underflow in the following
    logic, potentially triggering a deadlock of the mount point by
    blocking the client from issuing more RPC requests.
    
    net/sunrpc/xprtrdma/rpc_rdma.c:
    
      876          credits = be32_to_cpu(headerp->rm_credit);
      877          if (credits == 0)
      878                  credits = 1;    /* don't deadlock */
      879          else if (credits > r_xprt->rx_buf.rb_max_requests)
      880                  credits = r_xprt->rx_buf.rb_max_requests;
      881
      882          cwnd = xprt->cwnd;
      883          xprt->cwnd = credits << RPC_CWNDSHIFT;
      884          if (xprt->cwnd > cwnd)
      885                  xprt_release_rqst_cong(rqst->rq_task);
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Fixes: eba8ff660b2d ("xprtrdma: Move credit update to RPC . . .")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index d1b70397c60f..0a16fb6f0885 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -285,7 +285,7 @@ rpcr_to_rdmar(struct rpc_rqst *rqst)
  */
 struct rpcrdma_buffer {
 	spinlock_t	rb_lock;	/* protects indexes */
-	int		rb_max_requests;/* client max requests */
+	u32		rb_max_requests;/* client max requests */
 	struct list_head rb_mws;	/* optional memory windows/fmrs/frmrs */
 	struct list_head rb_all;
 	int		rb_send_index;

commit b625a61698619c7af652de2701a2fb17c5c5d66e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 4 16:59:32 2015 -0500

    xprtrdma: Address sparse complaint in rpcr_to_rdmar()
    
    With "make ARCH=x86_64 allmodconfig make C=1 CF=-D__CHECK_ENDIAN__":
    
    linux-2.6/net/sunrpc/xprtrdma/xprt_rdma.h:273:30: warning: incorrect
      type in initializer (different base types)
    linux-2.6/net/sunrpc/xprtrdma/xprt_rdma.h:273:30: expected restricted
      __be32 [usertype] *buffer
    linux-2.6/net/sunrpc/xprtrdma/xprt_rdma.h:273:30:    got unsigned int
      [usertype] *rq_buffer
    
    As far as I can tell this is a false positive.
    
    Reported-by: kbuild-all@01.org
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c9d2a02f631b..d1b70397c60f 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -270,9 +270,10 @@ struct rpcrdma_req {
 static inline struct rpcrdma_req *
 rpcr_to_rdmar(struct rpc_rqst *rqst)
 {
-	struct rpcrdma_regbuf *rb = container_of(rqst->rq_buffer,
-						 struct rpcrdma_regbuf,
-						 rg_base[0]);
+	void *buffer = rqst->rq_buffer;
+	struct rpcrdma_regbuf *rb;
+
+	rb = container_of(buffer, struct rpcrdma_regbuf, rg_base);
 	return rb->rg_owner;
 }
 

commit df515ca7b3b47bf6fd489fe6fca0d9ab243e1985
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:41 2015 -0500

    xprtrdma: Clean up after adding regbuf management
    
    rpcrdma_{de}register_internal() are used only in verbs.c now.
    
    MAX_RPCRDMAHDR is no longer used and can be removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5630353ed240..c9d2a02f631b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -171,10 +171,6 @@ enum rpcrdma_chunktype {
 /* temporary static scatter/gather max */
 #define RPCRDMA_MAX_DATA_SEGS	(64)	/* max scatter/gather */
 #define RPCRDMA_MAX_SEGS 	(RPCRDMA_MAX_DATA_SEGS + 2) /* head+tail = 2 */
-#define MAX_RPCRDMAHDR	(\
-	/* max supported RPC/RDMA header */ \
-	sizeof(struct rpcrdma_msg) + (2 * sizeof(u32)) + \
-	(sizeof(struct rpcrdma_read_chunk) * RPCRDMA_MAX_SEGS) + sizeof(u32))
 
 struct rpcrdma_buffer;
 
@@ -401,11 +397,6 @@ void rpcrdma_buffer_put(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
 void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
 
-int rpcrdma_register_internal(struct rpcrdma_ia *, void *, int,
-				struct ib_mr **, struct ib_sge *);
-int rpcrdma_deregister_internal(struct rpcrdma_ia *,
-				struct ib_mr *, struct ib_sge *);
-
 int rpcrdma_register_external(struct rpcrdma_mr_seg *,
 				int, int, struct rpcrdma_xprt *);
 int rpcrdma_deregister_external(struct rpcrdma_mr_seg *,

commit c05fbb5a593571961fdb4ba06a2bff49aed9dcee
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:33 2015 -0500

    xprtrdma: Allocate zero pad separately from rpcrdma_buffer
    
    Use the new rpcrdma_alloc_regbuf() API to shrink the amount of
    contiguous memory needed for a buffer pool by moving the zero
    pad buffer into a regbuf.
    
    This is for consistency with the other uses of internally
    registered memory.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2b69316dfd11..5630353ed240 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -88,8 +88,7 @@ struct rpcrdma_ep {
 	int			rep_connected;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
-	struct ib_sge		rep_pad;	/* holds zeroed pad */
-	struct ib_mr		*rep_pad_mr;	/* holds zeroed pad */
+	struct rpcrdma_regbuf	*rep_padbuf;
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;

commit 6b1184cd4fb086a826f658b02d9d9912dd0dde08
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:25 2015 -0500

    xprtrdma: Allocate RPC/RDMA receive buffer separately from struct rpcrdma_rep
    
    The rr_base field is currently the buffer where RPC replies land.
    
    An RPC/RDMA reply header lands in this buffer. In some cases an RPC
    reply header also lands in this buffer, just after the RPC/RDMA
    header.
    
    The inline threshold is an agreed-on size limit for RDMA SEND
    operations that pass from server and client. The sum of the
    RPC/RDMA reply header size and the RPC reply header size must be
    less than this threshold.
    
    The largest RDMA RECV that the client should have to handle is the
    size of the inline threshold. The receive buffer should thus be the
    size of the inline threshold, and not related to RPCRDMA_MAX_SEGS.
    
    RPC replies received via RDMA WRITE (long replies) are caught in
    rq_rcv_buf, which is the second half of the RPC send buffer. Ie,
    such replies are not involved in any way with rr_base.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 84ad863fe637..2b69316dfd11 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -180,14 +180,12 @@ enum rpcrdma_chunktype {
 struct rpcrdma_buffer;
 
 struct rpcrdma_rep {
-	unsigned int	rr_len;		/* actual received reply length */
-	struct rpcrdma_buffer *rr_buffer; /* home base for this structure */
-	struct rpc_xprt	*rr_xprt;	/* needed for request/reply matching */
-	void (*rr_func)(struct rpcrdma_rep *);/* called by tasklet in softint */
-	struct list_head rr_list;	/* tasklet list */
-	struct ib_sge	rr_iov;		/* for posting */
-	struct ib_mr	*rr_handle;	/* handle for mem in rr_iov */
-	char	rr_base[MAX_RPCRDMAHDR]; /* minimal inline receive buffer */
+	unsigned int		rr_len;
+	struct rpcrdma_buffer	*rr_buffer;
+	struct rpc_xprt		*rr_xprt;
+	void			(*rr_func)(struct rpcrdma_rep *);
+	struct list_head	rr_list;
+	struct rpcrdma_regbuf	*rr_rdmabuf;
 };
 
 /*

commit 85275c874eaeb92fb2a78a1d4ebb1ff4b0f7b732
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:16 2015 -0500

    xprtrdma: Allocate RPC/RDMA send buffer separately from struct rpcrdma_req
    
    The rl_base field is currently the buffer where each RPC/RDMA call
    header is built.
    
    The inline threshold is an agreed-on size limit to for RDMA SEND
    operations that pass between client and server. The sum of the
    RPC/RDMA header size and the RPC header size must be less than or
    equal to this threshold.
    
    Increasing the r/wsize maximum will require MAX_SEGS to grow
    significantly, but the inline threshold size won't change (both
    sides agree on it). The server's inline threshold doesn't change.
    
    Since an RPC/RDMA header can never be larger than the inline
    threshold, make all RPC/RDMA header buffers the size of the
    inline threshold.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index aa82f8d1c5b4..84ad863fe637 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -268,12 +268,10 @@ struct rpcrdma_req {
 	enum rpcrdma_chunktype	rl_rtype, rl_wtype;
 	struct rpcrdma_buffer *rl_buffer; /* home base for this structure */
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
-	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];/* chunk segments */
 	struct ib_sge	rl_send_iov[4];	/* for active requests */
+	struct rpcrdma_regbuf *rl_rdmabuf;
 	struct rpcrdma_regbuf *rl_sendbuf;
-	struct ib_sge	rl_iov;		/* for posting */
-	struct ib_mr	*rl_handle;	/* handle for mem in rl_iov */
-	char		rl_base[MAX_RPCRDMAHDR]; /* start of actual buffer */
+	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];
 };
 
 static inline struct rpcrdma_req *

commit 0ca77dc372110cbed4dbac5e867ffdc60ebccf6a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:08 2015 -0500

    xprtrdma: Allocate RPC send buffer separately from struct rpcrdma_req
    
    Because internal memory registration is an expensive and synchronous
    operation, xprtrdma pre-registers send and receive buffers at mount
    time, and then re-uses them for each RPC.
    
    A "hardway" allocation is a memory allocation and registration that
    replaces a send buffer during the processing of an RPC. Hardway must
    be done if the RPC send buffer is too small to accommodate an RPC's
    call and reply headers.
    
    For xprtrdma, each RPC send buffer is currently part of struct
    rpcrdma_req so that xprt_rdma_free(), which is passed nothing but
    the address of an RPC send buffer, can find its matching struct
    rpcrdma_req and rpcrdma_rep quickly via container_of / offsetof.
    
    That means that hardway currently has to replace a whole rpcrmda_req
    when it replaces an RPC send buffer. This is often a fairly hefty
    chunk of contiguous memory due to the size of the rl_segments array
    and the fact that both the send and receive buffers are part of
    struct rpcrdma_req.
    
    Some obscure re-use of fields in rpcrdma_req is done so that
    xprt_rdma_free() can detect replaced rpcrdma_req structs, and
    restore the original.
    
    This commit breaks apart the RPC send buffer and struct rpcrdma_req
    so that increasing the size of the rl_segments array does not change
    the alignment of each RPC send buffer. (Increasing rl_segments is
    needed to bump up the maximum r/wsize for NFS/RDMA).
    
    This change opens up some interesting possibilities for improving
    the design of xprt_rdma_allocate().
    
    xprt_rdma_allocate() is now the one place where RPC send buffers
    are allocated or re-allocated, and they are now always left in place
    by xprt_rdma_free().
    
    A large re-allocation that includes both the rl_segments array and
    the RPC send buffer is no longer needed. Send buffer re-allocation
    becomes quite rare. Good send buffer alignment is guaranteed no
    matter what the size of the rl_segments array is.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 36c37c60f1fe..aa82f8d1c5b4 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -262,7 +262,6 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 };
 
 struct rpcrdma_req {
-	size_t 		rl_size;	/* actual length of buffer */
 	unsigned int	rl_niovs;	/* 0, 2 or 4 */
 	unsigned int	rl_nchunks;	/* non-zero if chunks */
 	unsigned int	rl_connect_cookie;	/* retry detection */
@@ -271,13 +270,20 @@ struct rpcrdma_req {
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];/* chunk segments */
 	struct ib_sge	rl_send_iov[4];	/* for active requests */
+	struct rpcrdma_regbuf *rl_sendbuf;
 	struct ib_sge	rl_iov;		/* for posting */
 	struct ib_mr	*rl_handle;	/* handle for mem in rl_iov */
 	char		rl_base[MAX_RPCRDMAHDR]; /* start of actual buffer */
-	__u32 		rl_xdr_buf[0];	/* start of returned rpc rq_buffer */
 };
-#define rpcr_to_rdmar(r) \
-	container_of((r)->rq_buffer, struct rpcrdma_req, rl_xdr_buf[0])
+
+static inline struct rpcrdma_req *
+rpcr_to_rdmar(struct rpc_rqst *rqst)
+{
+	struct rpcrdma_regbuf *rb = container_of(rqst->rq_buffer,
+						 struct rpcrdma_regbuf,
+						 rg_base[0]);
+	return rb->rg_owner;
+}
 
 /*
  * struct rpcrdma_buffer -- holds list/queue of pre-registered memory for

commit 9128c3e794a77917a86dd5490ca2c5233a8c6fde
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:00 2015 -0500

    xprtrdma: Add struct rpcrdma_regbuf and helpers
    
    There are several spots that allocate a buffer via kmalloc (usually
    contiguously with another data structure) and then register that
    buffer internally. I'd like to split the buffers out of these data
    structures to allow the data structures to scale.
    
    Start by adding functions that can kmalloc and register a buffer,
    and can manage/preserve the buffer's associated ib_sge and ib_mr
    fields.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5c2fac3f30b6..36c37c60f1fe 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -106,6 +106,44 @@ struct rpcrdma_ep {
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
 #define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
 
+/* Registered buffer -- registered kmalloc'd memory for RDMA SEND/RECV
+ *
+ * The below structure appears at the front of a large region of kmalloc'd
+ * memory, which always starts on a good alignment boundary.
+ */
+
+struct rpcrdma_regbuf {
+	size_t			rg_size;
+	struct rpcrdma_req	*rg_owner;
+	struct ib_mr		*rg_mr;
+	struct ib_sge		rg_iov;
+	__be32			rg_base[0] __attribute__ ((aligned(256)));
+};
+
+static inline u64
+rdmab_addr(struct rpcrdma_regbuf *rb)
+{
+	return rb->rg_iov.addr;
+}
+
+static inline u32
+rdmab_length(struct rpcrdma_regbuf *rb)
+{
+	return rb->rg_iov.length;
+}
+
+static inline u32
+rdmab_lkey(struct rpcrdma_regbuf *rb)
+{
+	return rb->rg_iov.lkey;
+}
+
+static inline struct rpcrdma_msg *
+rdmab_to_msg(struct rpcrdma_regbuf *rb)
+{
+	return (struct rpcrdma_msg *)rb->rg_base;
+}
+
 enum rpcrdma_chunktype {
 	rpcrdma_noch = 0,
 	rpcrdma_readch,
@@ -372,6 +410,11 @@ int rpcrdma_register_external(struct rpcrdma_mr_seg *,
 int rpcrdma_deregister_external(struct rpcrdma_mr_seg *,
 				struct rpcrdma_xprt *);
 
+struct rpcrdma_regbuf *rpcrdma_alloc_regbuf(struct rpcrdma_ia *,
+					    size_t, gfp_t);
+void rpcrdma_free_regbuf(struct rpcrdma_ia *,
+			 struct rpcrdma_regbuf *);
+
 /*
  * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c
  */

commit ac920d04a7f307bfd7633f60abe33fb626f6ec83
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:03:44 2015 -0500

    xprtrdma: Simplify synopsis of rpcrdma_buffer_create()
    
    Clean up: There is one call site for rpcrdma_buffer_create(). All of
    the arguments there are fields of an rpcrdma_xprt.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2b4e7787734d..5c2fac3f30b6 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -354,9 +354,7 @@ int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_ep *,
 /*
  * Buffer calls - xprtrdma/verbs.c
  */
-int rpcrdma_buffer_create(struct rpcrdma_buffer *, struct rpcrdma_ep *,
-				struct rpcrdma_ia *,
-				struct rpcrdma_create_data_internal *);
+int rpcrdma_buffer_create(struct rpcrdma_xprt *);
 void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
 
 struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);

commit ce1ab9ab47973dcff7548abda20e49add2c4ca95
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:03:35 2015 -0500

    xprtrdma: Take struct ib_qp_attr and ib_qp_init_attr off the stack
    
    Reduce stack footprint of the connection upcall handler function.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ec596cebc966..2b4e7787734d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -71,6 +71,8 @@ struct rpcrdma_ia {
 	enum rpcrdma_memreg	ri_memreg_strategy;
 	unsigned int		ri_max_frmr_depth;
 	struct ib_device_attr	ri_devattr;
+	struct ib_qp_attr	ri_qp_attr;
+	struct ib_qp_init_attr	ri_qp_init_attr;
 };
 
 /*

commit 7bc7972cdd1f137552ca979caa11c8acbe119ae8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:03:27 2015 -0500

    xprtrdma: Take struct ib_device_attr off the stack
    
    Device attributes are large, and are used in more than one place.
    Stash a copy in dynamically allocated memory.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 657c370e48b9..ec596cebc966 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -70,6 +70,7 @@ struct rpcrdma_ia {
 	int			ri_async_rc;
 	enum rpcrdma_memreg	ri_memreg_strategy;
 	unsigned int		ri_max_frmr_depth;
+	struct ib_device_attr	ri_devattr;
 };
 
 /*

commit afadc468eb309b7c48ffdc8fa4c72acbb9991613
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:03:11 2015 -0500

    xprtrdma: Remove rpcrdma_ep::rep_func and ::rep_xprt
    
    Clean up: The rep_func field always refers to rpcrdma_conn_func().
    rep_func should have been removed by commit b45ccfd25d50 ("xprtrdma:
    Remove MEMWINDOWS registration modes").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3fcc92b0e3ca..657c370e48b9 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -87,8 +87,6 @@ struct rpcrdma_ep {
 	wait_queue_head_t 	rep_connect_wait;
 	struct ib_sge		rep_pad;	/* holds zeroed pad */
 	struct ib_mr		*rep_pad_mr;	/* holds zeroed pad */
-	void			(*rep_func)(struct rpcrdma_ep *);
-	struct rpc_xprt		*rep_xprt;	/* for rep_func */
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;

commit eba8ff660b2d8b7fcd6669fcab2c025b59f66d26
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:03:02 2015 -0500

    xprtrdma: Move credit update to RPC reply handler
    
    Reduce work in the receive CQ handler, which can be run at hardware
    interrupt level, by moving the RPC/RDMA credit update logic to the
    RPC reply handler.
    
    This has some additional benefits: More header sanity checking is
    done before trusting the incoming credit value, and the receive CQ
    handler no longer touches the RPC/RDMA header (the CPU stalls while
    waiting for the header contents to be brought into the cache).
    
    This further extends work begun by commit e7ce710a8802 ("xprtrdma:
    Avoid deadlock when credit window is reset").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 532d58667b9d..3fcc92b0e3ca 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -248,7 +248,6 @@ struct rpcrdma_req {
  */
 struct rpcrdma_buffer {
 	spinlock_t	rb_lock;	/* protects indexes */
-	atomic_t	rb_credits;	/* most recent server credits */
 	int		rb_max_requests;/* client max requests */
 	struct list_head rb_mws;	/* optional memory windows/fmrs/frmrs */
 	struct list_head rb_all;

commit 3eb358106660195948f4e95822039c5799fc41f8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:02:54 2015 -0500

    xprtrdma: Remove rl_mr field, and the mr_chunk union
    
    Clean up: Since commit 0ac531c18323 ("xprtrdma: Remove REGISTER
    memory registration mode"), the rl_mr pointer is no longer used
    anywhere.
    
    After removal, there's only a single member of the mr_chunk union,
    so mr_chunk can be removed as well, in favor of a single pointer
    field.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 5160a84fdb72..532d58667b9d 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -210,10 +210,7 @@ struct rpcrdma_mw {
  */
 
 struct rpcrdma_mr_seg {		/* chunk descriptors */
-	union {				/* chunk memory handles */
-		struct ib_mr	*rl_mr;		/* if registered directly */
-		struct rpcrdma_mw *rl_mw;	/* if registered from region */
-	} mr_chunk;
+	struct rpcrdma_mw *rl_mw;	/* registered MR */
 	u64		mr_base;	/* registration result */
 	u32		mr_rkey;	/* registration result */
 	u32		mr_len;		/* length of chunk or segment */

commit 5d410ba061c1e4bc0068ce91f2cf349998cde46c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:02:46 2015 -0500

    xprtrdma: Remove rpcrdma_ep::rep_ia
    
    Clean up: This field is not used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9a7aab31bf6e..5160a84fdb72 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -83,7 +83,6 @@ struct rpcrdma_ep {
 	atomic_t		rep_cqcount;
 	int			rep_cqinit;
 	int			rep_connected;
-	struct rpcrdma_ia	*rep_ia;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
 	struct ib_sge		rep_pad;	/* holds zeroed pad */

commit 5abefb861fd4306467813380cf21ce21d4b274ce
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:02:37 2015 -0500

    xprtrdma: Rename "xprt" and "rdma_connect" fields in struct rpcrdma_xprt
    
    Clean up: Use consistent field names in struct rpcrdma_xprt.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index b799041b75bf..9a7aab31bf6e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -318,16 +318,16 @@ struct rpcrdma_stats {
  * during unmount.
  */
 struct rpcrdma_xprt {
-	struct rpc_xprt		xprt;
+	struct rpc_xprt		rx_xprt;
 	struct rpcrdma_ia	rx_ia;
 	struct rpcrdma_ep	rx_ep;
 	struct rpcrdma_buffer	rx_buf;
 	struct rpcrdma_create_data_internal rx_data;
-	struct delayed_work	rdma_connect;
+	struct delayed_work	rx_connect_worker;
 	struct rpcrdma_stats	rx_stats;
 };
 
-#define rpcx_to_rdmax(x) container_of(x, struct rpcrdma_xprt, xprt)
+#define rpcx_to_rdmax(x) container_of(x, struct rpcrdma_xprt, rx_xprt)
 #define rpcx_to_rdmad(x) (rpcx_to_rdmax(x)->rx_data)
 
 /* Setting this to 0 ensures interoperability with early servers.

commit e7104a2a96069975d489c60a30564372c6273a85
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Nov 8 20:14:20 2014 -0500

    xprtrdma: Cap req_cqinit
    
    Recent work made FRMR registration and invalidation completions
    unsignaled. This greatly reduces the adapter interrupt rate.
    
    Every so often, however, a posted send Work Request is allowed to
    signal. Otherwise, the provider's Work Queue will wrap and the
    workload will hang.
    
    The number of Work Requests that are allowed to remain unsignaled is
    determined by the value of req_cqinit. Currently, this is set to the
    size of the send Work Queue divided by two, minus 1.
    
    For FRMR, the send Work Queue is the maximum number of concurrent
    RPCs (currently 32) times the maximum number of Work Requests an
    RPC might use (currently 7, though some adapters may need more).
    
    For mlx4, this is 224 entries. This leaves completion signaling
    disabled for 111 send Work Requests.
    
    Some providers hold back dispatching Work Requests until a CQE is
    generated.  If completions are disabled, then no CQEs are generated
    for quite some time, and that can stall the Work Queue.
    
    I've seen this occur running xfstests generic/113 over NFSv4, where
    eventually, posting a FAST_REG_MR Work Request fails with -ENOMEM
    because the Work Queue has overflowed. The connection is dropped
    and re-established.
    
    Cap the rep_cqinit setting so completions are not left turned off
    for too long.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=269
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index ac7fc9a31342..b799041b75bf 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -97,6 +97,12 @@ struct rpcrdma_ep {
 	struct ib_wc		rep_recv_wcs[RPCRDMA_POLLSIZE];
 };
 
+/*
+ * Force a signaled SEND Work Request every so often,
+ * in case the provider needs to do some housekeeping.
+ */
+#define RPCRDMA_MAX_UNSIGNALED_SENDS	(32)
+
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
 #define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
 

commit 7e5be28827bf5c1989218c4b7bf64fdbc3d679b5
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Tue Sep 23 17:11:22 2014 -0500

    svcrdma: advertise the correct max payload
    
    Svcrdma currently advertises 1MB, which is too large.  The correct value
    is the minimum of RPCSVC_MAXPAYLOAD and the max scatter-gather allowed
    in an NFSRDMA IO chunk * the host page size. This bug is usually benign
    because the Linux X64 NFSRDMA client correctly limits the payload size to
    the correct value (64*4096 = 256KB).  But if the Linux client is PPC64
    with a 64KB page size, then the client will indeed use a payload size
    that will overflow the server.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c419498b8f46..ac7fc9a31342 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -51,6 +51,7 @@
 #include <linux/sunrpc/clnt.h> 		/* rpc_xprt */
 #include <linux/sunrpc/rpc_rdma.h> 	/* RPC/RDMA protocol */
 #include <linux/sunrpc/xprtrdma.h> 	/* xprt parameters */
+#include <linux/sunrpc/svc.h>		/* RPCSVC_MAXPAYLOAD */
 
 #define RDMA_RESOLVE_TIMEOUT	(5000)	/* 5 seconds */
 #define RDMA_CONNECT_RETRY_MAX	(2)	/* retries if no listener backlog */
@@ -392,4 +393,10 @@ extern struct kmem_cache *svc_rdma_ctxt_cachep;
 /* Workqueue created in svc_rdma.c */
 extern struct workqueue_struct *svc_rdma_wq;
 
+#if RPCSVC_MAXPAYLOAD < (RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT)
+#define RPCSVC_MAXPAYLOAD_RDMA RPCSVC_MAXPAYLOAD
+#else
+#define RPCSVC_MAXPAYLOAD_RDMA (RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT)
+#endif
+
 #endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */

commit 282191cb725db9a1aa80269e8369b06e9270a948
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:25:55 2014 -0400

    xprtrdma: Make rpcrdma_ep_disconnect() return void
    
    Clean up: The return code is used only for dprintk's that are
    already redundant.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 1ee6db30abc5..c419498b8f46 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -341,7 +341,7 @@ int rpcrdma_ep_create(struct rpcrdma_ep *, struct rpcrdma_ia *,
 				struct rpcrdma_create_data_internal *);
 void rpcrdma_ep_destroy(struct rpcrdma_ep *, struct rpcrdma_ia *);
 int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
-int rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
+void rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 
 int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
 				struct rpcrdma_req *);

commit 9f9d802a28a107937ecda4ff78de2ab5cedd439d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:24:45 2014 -0400

    xprtrdma: Reset FRMRs when FAST_REG_MR is flushed by a disconnect
    
    FAST_REG_MR Work Requests update a Memory Region's rkey. Rkey's are
    used to block unwanted access to the memory controlled by an MR. The
    rkey is passed to the receiver (the NFS server, in our case), and is
    also used by xprtrdma to invalidate the MR when the RPC is complete.
    
    When a FAST_REG_MR Work Request is flushed after a transport
    disconnect, xprtrdma cannot tell whether the WR actually hit the
    adapter or not. So it is indeterminant at that point whether the
    existing rkey is still valid.
    
    After the transport connection is re-established, the next
    FAST_REG_MR or LOCAL_INV Work Request against that MR can sometimes
    fail because the rkey value does not match what xprtrdma expects.
    
    The only reliable way to recover in this case is to deregister and
    register the MR before it is used again. These operations can be
    done only in a process context, so handle it in the transport
    connect worker.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c1d865287b0e..1ee6db30abc5 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -161,6 +161,7 @@ struct rpcrdma_rep {
 enum rpcrdma_frmr_state {
 	FRMR_IS_INVALID,	/* ready to be used */
 	FRMR_IS_VALID,		/* in use */
+	FRMR_IS_STALE,		/* failed completion */
 };
 
 struct rpcrdma_frmr {

commit 3111d72c7ced444b1034f6e365e0e02444c68aa8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:24:28 2014 -0400

    xprtrdma: Chain together all MWs in same buffer pool
    
    During connection loss recovery, need to visit every MW in a
    buffer pool. Any MW that is in use by an RPC will not be on the
    rb_mws list.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 84c3455a9521..c1d865287b0e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -151,7 +151,7 @@ struct rpcrdma_rep {
  * An external memory region is any buffer or page that is registered
  * on the fly (ie, not pre-registered).
  *
- * Each rpcrdma_buffer has a list of these anchored in rb_mws. During
+ * Each rpcrdma_buffer has a list of free MWs anchored in rb_mws. During
  * call_allocate, rpcrdma_buffer_get() assigns one to each segment in
  * an rpcrdma_req. Then rpcrdma_register_external() grabs these to keep
  * track of registration metadata while each RPC is pending.
@@ -175,6 +175,7 @@ struct rpcrdma_mw {
 		struct rpcrdma_frmr	frmr;
 	} r;
 	struct list_head	mw_list;
+	struct list_head	mw_all;
 };
 
 /*
@@ -246,6 +247,7 @@ struct rpcrdma_buffer {
 	atomic_t	rb_credits;	/* most recent server credits */
 	int		rb_max_requests;/* client max requests */
 	struct list_head rb_mws;	/* optional memory windows/fmrs/frmrs */
+	struct list_head rb_all;
 	int		rb_send_index;
 	struct rpcrdma_req	**rb_send_bufs;
 	int		rb_recv_index;

commit 0dbb4108a6a615589751de2aaf468d3ddbcef24c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:24:09 2014 -0400

    xprtrdma: Unclutter struct rpcrdma_mr_seg
    
    Clean ups:
     - make it obvious that the rl_mw field is a pointer -- allocated
       separately, not as part of struct rpcrdma_mr_seg
     - promote "struct {} frmr;" to a named type
     - promote the state enum to a named type
     - name the MW state field the same way other fields in
       rpcrdma_mw are named
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c270e59cf917..84c3455a9521 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -145,6 +145,38 @@ struct rpcrdma_rep {
 	char	rr_base[MAX_RPCRDMAHDR]; /* minimal inline receive buffer */
 };
 
+/*
+ * struct rpcrdma_mw - external memory region metadata
+ *
+ * An external memory region is any buffer or page that is registered
+ * on the fly (ie, not pre-registered).
+ *
+ * Each rpcrdma_buffer has a list of these anchored in rb_mws. During
+ * call_allocate, rpcrdma_buffer_get() assigns one to each segment in
+ * an rpcrdma_req. Then rpcrdma_register_external() grabs these to keep
+ * track of registration metadata while each RPC is pending.
+ * rpcrdma_deregister_external() uses this metadata to unmap and
+ * release these resources when an RPC is complete.
+ */
+enum rpcrdma_frmr_state {
+	FRMR_IS_INVALID,	/* ready to be used */
+	FRMR_IS_VALID,		/* in use */
+};
+
+struct rpcrdma_frmr {
+	struct ib_fast_reg_page_list	*fr_pgl;
+	struct ib_mr			*fr_mr;
+	enum rpcrdma_frmr_state		fr_state;
+};
+
+struct rpcrdma_mw {
+	union {
+		struct ib_fmr		*fmr;
+		struct rpcrdma_frmr	frmr;
+	} r;
+	struct list_head	mw_list;
+};
+
 /*
  * struct rpcrdma_req -- structure central to the request/reply sequence.
  *
@@ -172,17 +204,7 @@ struct rpcrdma_rep {
 struct rpcrdma_mr_seg {		/* chunk descriptors */
 	union {				/* chunk memory handles */
 		struct ib_mr	*rl_mr;		/* if registered directly */
-		struct rpcrdma_mw {		/* if registered from region */
-			union {
-				struct ib_fmr	*fmr;
-				struct {
-					struct ib_fast_reg_page_list *fr_pgl;
-					struct ib_mr *fr_mr;
-					enum { FRMR_IS_INVALID, FRMR_IS_VALID  } state;
-				} frmr;
-			} r;
-			struct list_head mw_list;
-		} *rl_mw;
+		struct rpcrdma_mw *rl_mw;	/* if registered from region */
 	} mr_chunk;
 	u64		mr_base;	/* registration result */
 	u32		mr_rkey;	/* registration result */

commit 6ab59945f292a5c6cbc4a6c2011f1a732a116af2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:23:43 2014 -0400

    xprtrdma: Update rkeys after transport reconnect
    
    Various reports of:
    
      rpcrdma_qp_async_error_upcall: QP error 3 on device mlx4_0
                    ep ffff8800bfd3e848
    
    Ensure that rkeys in already-marshalled RPC/RDMA headers are
    refreshed after the QP has been replaced by a reconnect.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=249
    Suggested-by: Selvin Xavier <Selvin.Xavier@Emulex.Com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index f3d86b24a4af..c270e59cf917 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -99,6 +99,14 @@ struct rpcrdma_ep {
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
 #define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
 
+enum rpcrdma_chunktype {
+	rpcrdma_noch = 0,
+	rpcrdma_readch,
+	rpcrdma_areadch,
+	rpcrdma_writech,
+	rpcrdma_replych
+};
+
 /*
  * struct rpcrdma_rep -- this structure encapsulates state required to recv
  * and complete a reply, asychronously. It needs several pieces of
@@ -192,6 +200,7 @@ struct rpcrdma_req {
 	unsigned int	rl_niovs;	/* 0, 2 or 4 */
 	unsigned int	rl_nchunks;	/* non-zero if chunks */
 	unsigned int	rl_connect_cookie;	/* retry detection */
+	enum rpcrdma_chunktype	rl_rtype, rl_wtype;
 	struct rpcrdma_buffer *rl_buffer; /* home base for this structure */
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];/* chunk segments */
@@ -347,6 +356,7 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
 /*
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
+ssize_t rpcrdma_marshal_chunks(struct rpc_rqst *, ssize_t);
 int rpcrdma_marshal_req(struct rpc_rqst *);
 size_t rpcrdma_max_payload(struct rpcrdma_xprt *);
 

commit 43e95988178ed70a878a5be6be9ad248342dbf7d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:23:34 2014 -0400

    xprtrdma: Limit data payload size for ALLPHYSICAL
    
    When the client uses physical memory registration, each page in the
    payload gets its own array entry in the RPC/RDMA header's chunk list.
    
    Therefore, don't advertise a maximum payload size that would require
    more array entries than can fit in the RPC buffer where RPC/RDMA
    headers are built.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=248
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 97ca516ec619..f3d86b24a4af 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -348,6 +348,7 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
  * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
  */
 int rpcrdma_marshal_req(struct rpc_rqst *);
+size_t rpcrdma_max_payload(struct rpcrdma_xprt *);
 
 /* Temporary NFS request map cache. Created in svc_rdma.c  */
 extern struct kmem_cache *svc_rdma_map_cachep;

commit 73806c8832b3438ef0439603dab1f3cfc61cb6cd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:23:25 2014 -0400

    xprtrdma: Protect ia->ri_id when unmapping/invalidating MRs
    
    Ensure ia->ri_id remains valid while invoking dma_unmap_page() or
    posting LOCAL_INV during a transport reconnect. Otherwise,
    ia->ri_id->device or ia->ri_id->qp is NULL, which triggers a panic.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=259
    Fixes: ec62f40 'xprtrdma: Ensure ia->ri_id->qp is not NULL when reconnecting'
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 89e7cd479705..97ca516ec619 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -59,6 +59,7 @@
  * Interface Adapter -- one per transport instance
  */
 struct rpcrdma_ia {
+	rwlock_t		ri_qplock;
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
 	struct ib_mr		*ri_bind_mem;

commit e7ce710a8802351bd4118c5d6136c1d850f67cf9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:34:57 2014 -0400

    xprtrdma: Avoid deadlock when credit window is reset
    
    Update the cwnd while processing the server's reply.  Otherwise the
    next task on the xprt_sending queue is still subject to the old
    credit window. Currently, no task is awoken if the old congestion
    window is still exceeded, even if the new window is larger, and a
    deadlock results.
    
    This is an issue during a transport reconnect. Servers don't
    normally shrink the credit window, but the client does reset it to
    1 when reconnecting so the server can safely grow it again.
    
    As a minor optimization, remove the hack of grabbing the initial
    cwnd size (which happens to be RPC_CWNDSCALE) and using that value
    as the congestion scaling factor. The scaling value is invariant,
    and we are better off without the multiplication operation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 0c3b88ea5edb..89e7cd479705 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -212,7 +212,6 @@ struct rpcrdma_req {
 struct rpcrdma_buffer {
 	spinlock_t	rb_lock;	/* protects indexes */
 	atomic_t	rb_credits;	/* most recent server credits */
-	unsigned long	rb_cwndscale;	/* cached framework rpc_cwndscale */
 	int		rb_max_requests;/* client max requests */
 	struct list_head rb_mws;	/* optional memory windows/fmrs/frmrs */
 	int		rb_send_index;

commit 8301a2c047cc25dabd645e5590c1db0ead4c5af4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:33:51 2014 -0400

    xprtrdma: Limit work done by completion handler
    
    Sagi Grimberg <sagig@dev.mellanox.co.il> points out that a steady
    stream of CQ events could starve other work because of the boundless
    loop pooling in rpcrdma_{send,recv}_poll().
    
    Instead of a (potentially infinite) while loop, return after
    collecting a budgeted number of completions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Sagi Grimberg <sagig@dev.mellanox.co.il>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cb4c882b97fe..0c3b88ea5edb 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -74,6 +74,7 @@ struct rpcrdma_ia {
  * RDMA Endpoint -- one per transport instance
  */
 
+#define RPCRDMA_WC_BUDGET	(128)
 #define RPCRDMA_POLLSIZE	(16)
 
 struct rpcrdma_ep {

commit 1c00dd0776543608e13c74a527660cb8cd28a74f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:33:42 2014 -0400

    xprtrmda: Reduce calls to ib_poll_cq() in completion handlers
    
    Change the completion handlers to grab up to 16 items per
    ib_poll_cq() call. No extra ib_poll_cq() is needed if fewer than 16
    items are returned.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 334ab6ee041a..cb4c882b97fe 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -74,6 +74,8 @@ struct rpcrdma_ia {
  * RDMA Endpoint -- one per transport instance
  */
 
+#define RPCRDMA_POLLSIZE	(16)
+
 struct rpcrdma_ep {
 	atomic_t		rep_cqcount;
 	int			rep_cqinit;
@@ -88,6 +90,8 @@ struct rpcrdma_ep {
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
 	struct delayed_work	rep_connect_worker;
+	struct ib_wc		rep_send_wcs[RPCRDMA_POLLSIZE];
+	struct ib_wc		rep_recv_wcs[RPCRDMA_POLLSIZE];
 };
 
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)

commit fc66448549bbb77f2f1a38b270ab2d6b6a22da33
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:33:25 2014 -0400

    xprtrdma: Split the completion queue
    
    The current CQ handler uses the ib_wc.opcode field to distinguish
    between event types. However, the contents of that field are not
    reliable if the completion status is not IB_WC_SUCCESS.
    
    When an error completion occurs on a send event, the CQ handler
    schedules a tasklet with something that is not a struct rpcrdma_rep.
    This is never correct behavior, and sometimes it results in a panic.
    
    To resolve this issue, split the completion queue into a send CQ and
    a receive CQ. The send CQ handler now handles only struct rpcrdma_mw
    wr_id's, and the receive CQ handler now handles only struct
    rpcrdma_rep wr_id's.
    
    Fix suggested by Shirley Ma <shirley.ma@oracle.com>
    
    Reported-by: Rafael Reiter <rafael.reiter@ims.co.at>
    Fixes: 5c635e09cec0feeeb310968e51dad01040244851
    BugLink: https://bugzilla.kernel.org/show_bug.cgi?id=73211
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Klemens Senn <klemens.senn@ims.co.at>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 362a19d16440..334ab6ee041a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -79,7 +79,6 @@ struct rpcrdma_ep {
 	int			rep_cqinit;
 	int			rep_connected;
 	struct rpcrdma_ia	*rep_ia;
-	struct ib_cq		*rep_cq;
 	struct ib_qp_init_attr	rep_attr;
 	wait_queue_head_t 	rep_connect_wait;
 	struct ib_sge		rep_pad;	/* holds zeroed pad */

commit 7f1d54191ed6fa0f79f584fe3ebf6519738e817f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:33:16 2014 -0400

    xprtrdma: Make rpcrdma_ep_destroy() return void
    
    Clean up: rpcrdma_ep_destroy() returns a value that is used
    only to print a debugging message. rpcrdma_ep_destroy() already
    prints debugging messages in all error cases.
    
    Make rpcrdma_ep_destroy() return void instead.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 3f44d6aab881..362a19d16440 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -301,7 +301,7 @@ void rpcrdma_ia_close(struct rpcrdma_ia *);
  */
 int rpcrdma_ep_create(struct rpcrdma_ep *, struct rpcrdma_ia *,
 				struct rpcrdma_create_data_internal *);
-int rpcrdma_ep_destroy(struct rpcrdma_ep *, struct rpcrdma_ia *);
+void rpcrdma_ep_destroy(struct rpcrdma_ep *, struct rpcrdma_ia *);
 int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 int rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
 

commit 13c9ff8f673862b69e795ea99a237b461c557eb3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:33:08 2014 -0400

    xprtrdma: Simplify rpcrdma_deregister_external() synopsis
    
    Clean up: All remaining callers of rpcrdma_deregister_external()
    pass NULL as the last argument, so remove that argument.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index bf08ee0b81e0..3f44d6aab881 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -331,7 +331,7 @@ int rpcrdma_deregister_internal(struct rpcrdma_ia *,
 int rpcrdma_register_external(struct rpcrdma_mr_seg *,
 				int, int, struct rpcrdma_xprt *);
 int rpcrdma_deregister_external(struct rpcrdma_mr_seg *,
-				struct rpcrdma_xprt *, void *);
+				struct rpcrdma_xprt *);
 
 /*
  * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c

commit b45ccfd25d506e83d9ecf93d0ac7edf031d35d2f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:32:34 2014 -0400

    xprtrdma: Remove MEMWINDOWS registration modes
    
    The MEMWINDOWS and MEMWINDOWS_ASYNC memory registration modes were
    intended as stop-gap modes before the introduction of FRMR. They
    are now considered obsolete.
    
    MEMWINDOWS_ASYNC is also considered unsafe because it can leave
    client memory registered and exposed for an indeterminant time after
    each I/O.
    
    At this point, the MEMWINDOWS modes add needless complexity, so
    remove them.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c620d1332933..bf08ee0b81e0 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -127,7 +127,6 @@ struct rpcrdma_rep {
 	struct rpc_xprt	*rr_xprt;	/* needed for request/reply matching */
 	void (*rr_func)(struct rpcrdma_rep *);/* called by tasklet in softint */
 	struct list_head rr_list;	/* tasklet list */
-	wait_queue_head_t rr_unbind;	/* optional unbind wait */
 	struct ib_sge	rr_iov;		/* for posting */
 	struct ib_mr	*rr_handle;	/* handle for mem in rr_iov */
 	char	rr_base[MAX_RPCRDMAHDR]; /* minimal inline receive buffer */
@@ -162,7 +161,6 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 		struct ib_mr	*rl_mr;		/* if registered directly */
 		struct rpcrdma_mw {		/* if registered from region */
 			union {
-				struct ib_mw	*mw;
 				struct ib_fmr	*fmr;
 				struct {
 					struct ib_fast_reg_page_list *fr_pgl;

commit 254f91e2fa1f4cc18fd2eb9d5481888ffe126d5b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:32:17 2014 -0400

    xprtrdma: RPC/RDMA must invoke xprt_wake_pending_tasks() in process context
    
    An IB provider can invoke rpcrdma_conn_func() in an IRQ context,
    thus rpcrdma_conn_func() cannot be allowed to directly invoke
    generic RPC functions like xprt_wake_pending_tasks().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 98340a31f2bc..c620d1332933 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -43,6 +43,7 @@
 #include <linux/wait.h> 		/* wait_queue_head_t, etc */
 #include <linux/spinlock.h> 		/* spinlock_t, etc */
 #include <linux/atomic.h>			/* atomic_t, etc */
+#include <linux/workqueue.h>		/* struct work_struct */
 
 #include <rdma/rdma_cm.h>		/* RDMA connection api */
 #include <rdma/ib_verbs.h>		/* RDMA verbs api */
@@ -87,6 +88,7 @@ struct rpcrdma_ep {
 	struct rpc_xprt		*rep_xprt;	/* for rep_func */
 	struct rdma_conn_param	rep_remote_cma;
 	struct sockaddr_storage	rep_remote_addr;
+	struct delayed_work	rep_connect_worker;
 };
 
 #define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
@@ -336,6 +338,7 @@ int rpcrdma_deregister_external(struct rpcrdma_mr_seg *,
 /*
  * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c
  */
+void rpcrdma_connect_worker(struct work_struct *);
 void rpcrdma_conn_func(struct rpcrdma_ep *);
 void rpcrdma_reply_handler(struct rpcrdma_rep *);
 

commit 0fc6c4e7bb287148eb5e949efd89327929d4841d
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Wed May 28 10:32:00 2014 -0400

    xprtrdma: mind the device's max fast register page list depth
    
    Some rdma devices don't support a fast register page list depth of
    at least RPCRDMA_MAX_DATA_SEGS.  So xprtrdma needs to chunk its fast
    register regions according to the minimum of the device max supported
    depth or RPCRDMA_MAX_DATA_SEGS.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cc1445dc1d1a..98340a31f2bc 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -66,6 +66,7 @@ struct rpcrdma_ia {
 	struct completion	ri_done;
 	int			ri_async_rc;
 	enum rpcrdma_memreg	ri_memreg_strategy;
+	unsigned int		ri_max_frmr_depth;
 };
 
 /*

commit a4f0835c604f80f945ab3e72ffd00547145c4b2b
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Tue Jan 8 09:10:21 2013 -0500

    SUNRPC: Eliminate task->tk_xprt accesses that bypass rcu_dereference()
    
    tk_xprt is just a shortcut for tk_client->cl_xprt, however cl_xprt is
    defined as an __rcu variable. Replace dereferences of tk_xprt with
    non-rcu dereferences where it is safe to do so.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 9a66c95b5837..cc1445dc1d1a 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -235,13 +235,13 @@ struct rpcrdma_create_data_internal {
 };
 
 #define RPCRDMA_INLINE_READ_THRESHOLD(rq) \
-	(rpcx_to_rdmad(rq->rq_task->tk_xprt).inline_rsize)
+	(rpcx_to_rdmad(rq->rq_xprt).inline_rsize)
 
 #define RPCRDMA_INLINE_WRITE_THRESHOLD(rq)\
-	(rpcx_to_rdmad(rq->rq_task->tk_xprt).inline_wsize)
+	(rpcx_to_rdmad(rq->rq_xprt).inline_wsize)
 
 #define RPCRDMA_INLINE_PAD_VALUE(rq)\
-	rpcx_to_rdmad(rq->rq_task->tk_xprt).padding
+	rpcx_to_rdmad(rq->rq_xprt).padding
 
 /*
  * Statistics for RPCRDMA

commit cec56c8ff5e28f58ff13041dca7853738ae577a1
Author: Tom Tucker <tom@ogc.us>
Date:   Wed Feb 15 11:30:00 2012 -0600

    svcrdma: Cleanup sparse warnings in the svcrdma module
    
    The svcrdma transport was un-marshalling requests in-place. This resulted
    in sparse warnings due to __beXX data containing both NBO and HBO data.
    
    The code has been restructured to do byte-swapping as the header is
    parsed instead of when the header is validated immediately after receipt.
    
    Also moved extern declarations for the workqueue and memory pools to the
    private header file.
    
    Signed-off-by: Tom Tucker <tom@ogc.us>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 08c5d5a128fc..9a66c95b5837 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -343,4 +343,11 @@ void rpcrdma_reply_handler(struct rpcrdma_rep *);
  */
 int rpcrdma_marshal_req(struct rpc_rqst *);
 
+/* Temporary NFS request map cache. Created in svc_rdma.c  */
+extern struct kmem_cache *svc_rdma_map_cachep;
+/* WR context cache. Created in svc_rdma.c  */
+extern struct kmem_cache *svc_rdma_ctxt_cachep;
+/* Workqueue created in svc_rdma.c */
+extern struct workqueue_struct *svc_rdma_wq;
+
 #endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */

commit 28890d3598c352ae065b560e0fded3e79c800ba1
Merge: 91d41fdf31f7 ed1e6211a0a1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 27 13:23:02 2011 -0700

    Merge branch 'nfs-for-3.1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    * 'nfs-for-3.1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (44 commits)
      NFSv4: Don't use the delegation->inode in nfs_mark_return_delegation()
      nfs: don't use d_move in nfs_async_rename_done
      RDMA: Increasing RPCRDMA_MAX_DATA_SEGS
      SUNRPC: Replace xprt->resend and xprt->sending with a priority queue
      SUNRPC: Allow caller of rpc_sleep_on() to select priority levels
      SUNRPC: Support dynamic slot allocation for TCP connections
      SUNRPC: Clean up the slot table allocation
      SUNRPC: Initalise the struct xprt upon allocation
      SUNRPC: Ensure that we grab the XPRT_LOCK before calling xprt_alloc_slot
      pnfs: simplify pnfs files module autoloading
      nfs: document nfsv4 sillyrename issues
      NFS: Convert nfs4_set_ds_client to EXPORT_SYMBOL_GPL
      SUNRPC: Convert the backchannel exports to EXPORT_SYMBOL_GPL
      SUNRPC: sunrpc should not explicitly depend on NFS config options
      NFS: Clean up - simplify the switch to read/write-through-MDS
      NFS: Move the pnfs write code into pnfs.c
      NFS: Move the pnfs read code into pnfs.c
      NFS: Allow the nfs_pageio_descriptor to signal that a re-coalesce is needed
      NFS: Use the nfs_pageio_descriptor->pg_bsize in the read/write request
      NFS: Cache rpc_ops in struct nfs_pageio_descriptor
      ...

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cae761a8536c..ddf05288d9f1 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -42,7 +42,7 @@
 
 #include <linux/wait.h> 		/* wait_queue_head_t, etc */
 #include <linux/spinlock.h> 		/* spinlock_t, etc */
-#include <asm/atomic.h>			/* atomic_t, etc */
+#include <linux/atomic.h>			/* atomic_t, etc */
 
 #include <rdma/rdma_cm.h>		/* RDMA connection api */
 #include <rdma/ib_verbs.h>		/* RDMA verbs api */

commit 2773395b34883fe54418de188733a63bb38e0ad6
Author: Steve Dickson <steved@redhat.com>
Date:   Thu Jul 21 13:49:02 2011 -0400

    RDMA: Increasing RPCRDMA_MAX_DATA_SEGS
    
    Our performance team has noticed that increasing
    RPCRDMA_MAX_DATA_SEGS from 8 to 64 significantly
    increases throughput when using the RDMA transport.
    
    Signed-off-by: Steve Dickson <steved@redhat.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index cae761a8536c..5d1cfe58ec20 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -109,7 +109,7 @@ struct rpcrdma_ep {
  */
 
 /* temporary static scatter/gather max */
-#define RPCRDMA_MAX_DATA_SEGS	(8)	/* max scatter/gather */
+#define RPCRDMA_MAX_DATA_SEGS	(64)	/* max scatter/gather */
 #define RPCRDMA_MAX_SEGS 	(RPCRDMA_MAX_DATA_SEGS + 2) /* head+tail = 2 */
 #define MAX_RPCRDMAHDR	(\
 	/* max supported RPC/RDMA header */ \

commit 5c635e09cec0feeeb310968e51dad01040244851
Author: Tom Tucker <tom@ogc.us>
Date:   Wed Feb 9 19:45:34 2011 +0000

    RPCRDMA: Fix FRMR registration/invalidate handling.
    
    When the rpc_memreg_strategy is 5, FRMR are used to map RPC data.
    This mode uses an FRMR to map the RPC data, then invalidates
    (i.e. unregisers) the data in xprt_rdma_free. These FRMR are used
    across connections on the same mount, i.e. if the connection goes
    away on an idle timeout and reconnects later, the FRMR are not
    destroyed and recreated.
    
    This creates a problem for transport errors because the WR that
    invalidate an FRMR may be flushed (i.e. fail) leaving the
    FRMR valid. When the FRMR is later used to map an RPC it will fail,
    tearing down the transport and starting over. Over time, more and
    more of the FRMR pool end up in the wrong state resulting in
    seemingly random disconnects.
    
    This fix keeps track of the FRMR state explicitly by setting it's
    state based on the successful completion of a reg/inv WR. If the FRMR
    is ever used and found to be in the wrong state, an invalidate WR
    is prepended, re-syncing the FRMR state and avoiding the connection loss.
    
    Signed-off-by: Tom Tucker <tom@ogc.us>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index c7a7eba991bc..cae761a8536c 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -164,6 +164,7 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 				struct {
 					struct ib_fast_reg_page_list *fr_pgl;
 					struct ib_mr *fr_mr;
+					enum { FRMR_IS_INVALID, FRMR_IS_VALID  } state;
 				} frmr;
 			} r;
 			struct list_head mw_list;

commit 5675add36e76b9487e7f9e689f854cb8d6afd9b4
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:01:41 2008 -0400

    RPC/RDMA: harden connection logic against missing/late rdma_cm upcalls.
    
    Add defensive timeouts to wait_for_completion() calls in RDMA
    address resolution, and make them interruptible. Fix the timeout
    units to milliseconds (formerly jiffies) and move to private header.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index fde6499a53b2..c7a7eba991bc 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -51,6 +51,9 @@
 #include <linux/sunrpc/rpc_rdma.h> 	/* RPC/RDMA protocol */
 #include <linux/sunrpc/xprtrdma.h> 	/* xprt parameters */
 
+#define RDMA_RESOLVE_TIMEOUT	(5000)	/* 5 seconds */
+#define RDMA_CONNECT_RETRY_MAX	(2)	/* retries if no listener backlog */
+
 /*
  * Interface Adapter -- one per transport instance
  */

commit 9191ca3b381b15b9a88785a8ae2fa4db8e553b0c
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:01:11 2008 -0400

    RPC/RDMA: adhere to protocol for unpadded client trailing write chunks.
    
    The RPC/RDMA protocol allows clients and servers to avoid RDMA
    operations for data which is purely the result of XDR padding.
    On the client, automatically insert the necessary padding for
    such server replies, and optionally don't marshal such chunks.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2db2344d487e..fde6499a53b2 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -280,6 +280,11 @@ struct rpcrdma_xprt {
 #define rpcx_to_rdmax(x) container_of(x, struct rpcrdma_xprt, xprt)
 #define rpcx_to_rdmad(x) (rpcx_to_rdmax(x)->rx_data)
 
+/* Setting this to 0 ensures interoperability with early servers.
+ * Setting this to 1 enhances certain unaligned read/write performance.
+ * Default is 0, see sysctl entry and rpc_rdma.c rpcrdma_convert_iovs() */
+extern int xprt_rdma_pad_optimize;
+
 /*
  * Interface Adapter calls - xprtrdma/verbs.c
  */

commit 575448bd36208f99fe0dd554a43518d798966740
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:00:40 2008 -0400

    RPC/RDMA: suppress retransmit on RPC/RDMA clients.
    
    An RPC/RDMA client cannot retransmit on an unbroken connection,
    doing so violates its flow control with the server.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 05b7898e1f4b..2db2344d487e 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -181,6 +181,7 @@ struct rpcrdma_req {
 	size_t 		rl_size;	/* actual length of buffer */
 	unsigned int	rl_niovs;	/* 0, 2 or 4 */
 	unsigned int	rl_nchunks;	/* non-zero if chunks */
+	unsigned int	rl_connect_cookie;	/* retry detection */
 	struct rpcrdma_buffer *rl_buffer; /* home base for this structure */
 	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
 	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];/* chunk segments */

commit fe9053b30bb48b99f7b45541249f5cfe96bdf7f7
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 14:59:59 2008 -0400

    RPC/RDMA: add data types and new FRMR memory registration enum.
    
    Internal RPC/RDMA structure updates in preparation for FRMR support.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Acked-by: Tom Tucker <tom@opengridcomputing.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
index 2427822f8bd4..05b7898e1f4b 100644
--- a/net/sunrpc/xprtrdma/xprt_rdma.h
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -58,6 +58,8 @@ struct rpcrdma_ia {
 	struct rdma_cm_id 	*ri_id;
 	struct ib_pd		*ri_pd;
 	struct ib_mr		*ri_bind_mem;
+	u32			ri_dma_lkey;
+	int			ri_have_dma_lkey;
 	struct completion	ri_done;
 	int			ri_async_rc;
 	enum rpcrdma_memreg	ri_memreg_strategy;
@@ -156,6 +158,10 @@ struct rpcrdma_mr_seg {		/* chunk descriptors */
 			union {
 				struct ib_mw	*mw;
 				struct ib_fmr	*fmr;
+				struct {
+					struct ib_fast_reg_page_list *fr_pgl;
+					struct ib_mr *fr_mr;
+				} frmr;
 			} r;
 			struct list_head mw_list;
 		} *rl_mw;
@@ -198,7 +204,7 @@ struct rpcrdma_buffer {
 	atomic_t	rb_credits;	/* most recent server credits */
 	unsigned long	rb_cwndscale;	/* cached framework rpc_cwndscale */
 	int		rb_max_requests;/* client max requests */
-	struct list_head rb_mws;	/* optional memory windows/fmrs */
+	struct list_head rb_mws;	/* optional memory windows/fmrs/frmrs */
 	int		rb_send_index;
 	struct rpcrdma_req	**rb_send_bufs;
 	int		rb_recv_index;

commit f58851e6b0f148fb4b2a1c6f70beb2f125863c0f
Author: \"Talpey, Thomas\ <Thomas.Talpey@netapp.com>
Date:   Mon Sep 10 13:50:12 2007 -0400

    RPCRDMA: rpc rdma transport switch
    
    This implements the configuration and building of the core transport
    switch implementation of the rpcrdma transport. Stubs are provided for
    the rpcrdma protocol handling, and the infiniband/iwarp verbs interface.
    These are provided in following patches.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/xprt_rdma.h b/net/sunrpc/xprtrdma/xprt_rdma.h
new file mode 100644
index 000000000000..2427822f8bd4
--- /dev/null
+++ b/net/sunrpc/xprtrdma/xprt_rdma.h
@@ -0,0 +1,330 @@
+/*
+ * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the BSD-type
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *      Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *
+ *      Redistributions in binary form must reproduce the above
+ *      copyright notice, this list of conditions and the following
+ *      disclaimer in the documentation and/or other materials provided
+ *      with the distribution.
+ *
+ *      Neither the name of the Network Appliance, Inc. nor the names of
+ *      its contributors may be used to endorse or promote products
+ *      derived from this software without specific prior written
+ *      permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+#ifndef _LINUX_SUNRPC_XPRT_RDMA_H
+#define _LINUX_SUNRPC_XPRT_RDMA_H
+
+#include <linux/wait.h> 		/* wait_queue_head_t, etc */
+#include <linux/spinlock.h> 		/* spinlock_t, etc */
+#include <asm/atomic.h>			/* atomic_t, etc */
+
+#include <rdma/rdma_cm.h>		/* RDMA connection api */
+#include <rdma/ib_verbs.h>		/* RDMA verbs api */
+
+#include <linux/sunrpc/clnt.h> 		/* rpc_xprt */
+#include <linux/sunrpc/rpc_rdma.h> 	/* RPC/RDMA protocol */
+#include <linux/sunrpc/xprtrdma.h> 	/* xprt parameters */
+
+/*
+ * Interface Adapter -- one per transport instance
+ */
+struct rpcrdma_ia {
+	struct rdma_cm_id 	*ri_id;
+	struct ib_pd		*ri_pd;
+	struct ib_mr		*ri_bind_mem;
+	struct completion	ri_done;
+	int			ri_async_rc;
+	enum rpcrdma_memreg	ri_memreg_strategy;
+};
+
+/*
+ * RDMA Endpoint -- one per transport instance
+ */
+
+struct rpcrdma_ep {
+	atomic_t		rep_cqcount;
+	int			rep_cqinit;
+	int			rep_connected;
+	struct rpcrdma_ia	*rep_ia;
+	struct ib_cq		*rep_cq;
+	struct ib_qp_init_attr	rep_attr;
+	wait_queue_head_t 	rep_connect_wait;
+	struct ib_sge		rep_pad;	/* holds zeroed pad */
+	struct ib_mr		*rep_pad_mr;	/* holds zeroed pad */
+	void			(*rep_func)(struct rpcrdma_ep *);
+	struct rpc_xprt		*rep_xprt;	/* for rep_func */
+	struct rdma_conn_param	rep_remote_cma;
+	struct sockaddr_storage	rep_remote_addr;
+};
+
+#define INIT_CQCOUNT(ep) atomic_set(&(ep)->rep_cqcount, (ep)->rep_cqinit)
+#define DECR_CQCOUNT(ep) atomic_sub_return(1, &(ep)->rep_cqcount)
+
+/*
+ * struct rpcrdma_rep -- this structure encapsulates state required to recv
+ * and complete a reply, asychronously. It needs several pieces of
+ * state:
+ *   o recv buffer (posted to provider)
+ *   o ib_sge (also donated to provider)
+ *   o status of reply (length, success or not)
+ *   o bookkeeping state to get run by tasklet (list, etc)
+ *
+ * These are allocated during initialization, per-transport instance;
+ * however, the tasklet execution list itself is global, as it should
+ * always be pretty short.
+ *
+ * N of these are associated with a transport instance, and stored in
+ * struct rpcrdma_buffer. N is the max number of outstanding requests.
+ */
+
+/* temporary static scatter/gather max */
+#define RPCRDMA_MAX_DATA_SEGS	(8)	/* max scatter/gather */
+#define RPCRDMA_MAX_SEGS 	(RPCRDMA_MAX_DATA_SEGS + 2) /* head+tail = 2 */
+#define MAX_RPCRDMAHDR	(\
+	/* max supported RPC/RDMA header */ \
+	sizeof(struct rpcrdma_msg) + (2 * sizeof(u32)) + \
+	(sizeof(struct rpcrdma_read_chunk) * RPCRDMA_MAX_SEGS) + sizeof(u32))
+
+struct rpcrdma_buffer;
+
+struct rpcrdma_rep {
+	unsigned int	rr_len;		/* actual received reply length */
+	struct rpcrdma_buffer *rr_buffer; /* home base for this structure */
+	struct rpc_xprt	*rr_xprt;	/* needed for request/reply matching */
+	void (*rr_func)(struct rpcrdma_rep *);/* called by tasklet in softint */
+	struct list_head rr_list;	/* tasklet list */
+	wait_queue_head_t rr_unbind;	/* optional unbind wait */
+	struct ib_sge	rr_iov;		/* for posting */
+	struct ib_mr	*rr_handle;	/* handle for mem in rr_iov */
+	char	rr_base[MAX_RPCRDMAHDR]; /* minimal inline receive buffer */
+};
+
+/*
+ * struct rpcrdma_req -- structure central to the request/reply sequence.
+ *
+ * N of these are associated with a transport instance, and stored in
+ * struct rpcrdma_buffer. N is the max number of outstanding requests.
+ *
+ * It includes pre-registered buffer memory for send AND recv.
+ * The recv buffer, however, is not owned by this structure, and
+ * is "donated" to the hardware when a recv is posted. When a
+ * reply is handled, the recv buffer used is given back to the
+ * struct rpcrdma_req associated with the request.
+ *
+ * In addition to the basic memory, this structure includes an array
+ * of iovs for send operations. The reason is that the iovs passed to
+ * ib_post_{send,recv} must not be modified until the work request
+ * completes.
+ *
+ * NOTES:
+ *   o RPCRDMA_MAX_SEGS is the max number of addressible chunk elements we
+ *     marshal. The number needed varies depending on the iov lists that
+ *     are passed to us, the memory registration mode we are in, and if
+ *     physical addressing is used, the layout.
+ */
+
+struct rpcrdma_mr_seg {		/* chunk descriptors */
+	union {				/* chunk memory handles */
+		struct ib_mr	*rl_mr;		/* if registered directly */
+		struct rpcrdma_mw {		/* if registered from region */
+			union {
+				struct ib_mw	*mw;
+				struct ib_fmr	*fmr;
+			} r;
+			struct list_head mw_list;
+		} *rl_mw;
+	} mr_chunk;
+	u64		mr_base;	/* registration result */
+	u32		mr_rkey;	/* registration result */
+	u32		mr_len;		/* length of chunk or segment */
+	int		mr_nsegs;	/* number of segments in chunk or 0 */
+	enum dma_data_direction	mr_dir;	/* segment mapping direction */
+	dma_addr_t	mr_dma;		/* segment mapping address */
+	size_t		mr_dmalen;	/* segment mapping length */
+	struct page	*mr_page;	/* owning page, if any */
+	char		*mr_offset;	/* kva if no page, else offset */
+};
+
+struct rpcrdma_req {
+	size_t 		rl_size;	/* actual length of buffer */
+	unsigned int	rl_niovs;	/* 0, 2 or 4 */
+	unsigned int	rl_nchunks;	/* non-zero if chunks */
+	struct rpcrdma_buffer *rl_buffer; /* home base for this structure */
+	struct rpcrdma_rep	*rl_reply;/* holder for reply buffer */
+	struct rpcrdma_mr_seg rl_segments[RPCRDMA_MAX_SEGS];/* chunk segments */
+	struct ib_sge	rl_send_iov[4];	/* for active requests */
+	struct ib_sge	rl_iov;		/* for posting */
+	struct ib_mr	*rl_handle;	/* handle for mem in rl_iov */
+	char		rl_base[MAX_RPCRDMAHDR]; /* start of actual buffer */
+	__u32 		rl_xdr_buf[0];	/* start of returned rpc rq_buffer */
+};
+#define rpcr_to_rdmar(r) \
+	container_of((r)->rq_buffer, struct rpcrdma_req, rl_xdr_buf[0])
+
+/*
+ * struct rpcrdma_buffer -- holds list/queue of pre-registered memory for
+ * inline requests/replies, and client/server credits.
+ *
+ * One of these is associated with a transport instance
+ */
+struct rpcrdma_buffer {
+	spinlock_t	rb_lock;	/* protects indexes */
+	atomic_t	rb_credits;	/* most recent server credits */
+	unsigned long	rb_cwndscale;	/* cached framework rpc_cwndscale */
+	int		rb_max_requests;/* client max requests */
+	struct list_head rb_mws;	/* optional memory windows/fmrs */
+	int		rb_send_index;
+	struct rpcrdma_req	**rb_send_bufs;
+	int		rb_recv_index;
+	struct rpcrdma_rep	**rb_recv_bufs;
+	char		*rb_pool;
+};
+#define rdmab_to_ia(b) (&container_of((b), struct rpcrdma_xprt, rx_buf)->rx_ia)
+
+/*
+ * Internal structure for transport instance creation. This
+ * exists primarily for modularity.
+ *
+ * This data should be set with mount options
+ */
+struct rpcrdma_create_data_internal {
+	struct sockaddr_storage	addr;	/* RDMA server address */
+	unsigned int	max_requests;	/* max requests (slots) in flight */
+	unsigned int	rsize;		/* mount rsize - max read hdr+data */
+	unsigned int	wsize;		/* mount wsize - max write hdr+data */
+	unsigned int	inline_rsize;	/* max non-rdma read data payload */
+	unsigned int	inline_wsize;	/* max non-rdma write data payload */
+	unsigned int	padding;	/* non-rdma write header padding */
+};
+
+#define RPCRDMA_INLINE_READ_THRESHOLD(rq) \
+	(rpcx_to_rdmad(rq->rq_task->tk_xprt).inline_rsize)
+
+#define RPCRDMA_INLINE_WRITE_THRESHOLD(rq)\
+	(rpcx_to_rdmad(rq->rq_task->tk_xprt).inline_wsize)
+
+#define RPCRDMA_INLINE_PAD_VALUE(rq)\
+	rpcx_to_rdmad(rq->rq_task->tk_xprt).padding
+
+/*
+ * Statistics for RPCRDMA
+ */
+struct rpcrdma_stats {
+	unsigned long		read_chunk_count;
+	unsigned long		write_chunk_count;
+	unsigned long		reply_chunk_count;
+
+	unsigned long long	total_rdma_request;
+	unsigned long long	total_rdma_reply;
+
+	unsigned long long	pullup_copy_count;
+	unsigned long long	fixup_copy_count;
+	unsigned long		hardway_register_count;
+	unsigned long		failed_marshal_count;
+	unsigned long		bad_reply_count;
+};
+
+/*
+ * RPCRDMA transport -- encapsulates the structures above for
+ * integration with RPC.
+ *
+ * The contained structures are embedded, not pointers,
+ * for convenience. This structure need not be visible externally.
+ *
+ * It is allocated and initialized during mount, and released
+ * during unmount.
+ */
+struct rpcrdma_xprt {
+	struct rpc_xprt		xprt;
+	struct rpcrdma_ia	rx_ia;
+	struct rpcrdma_ep	rx_ep;
+	struct rpcrdma_buffer	rx_buf;
+	struct rpcrdma_create_data_internal rx_data;
+	struct delayed_work	rdma_connect;
+	struct rpcrdma_stats	rx_stats;
+};
+
+#define rpcx_to_rdmax(x) container_of(x, struct rpcrdma_xprt, xprt)
+#define rpcx_to_rdmad(x) (rpcx_to_rdmax(x)->rx_data)
+
+/*
+ * Interface Adapter calls - xprtrdma/verbs.c
+ */
+int rpcrdma_ia_open(struct rpcrdma_xprt *, struct sockaddr *, int);
+void rpcrdma_ia_close(struct rpcrdma_ia *);
+
+/*
+ * Endpoint calls - xprtrdma/verbs.c
+ */
+int rpcrdma_ep_create(struct rpcrdma_ep *, struct rpcrdma_ia *,
+				struct rpcrdma_create_data_internal *);
+int rpcrdma_ep_destroy(struct rpcrdma_ep *, struct rpcrdma_ia *);
+int rpcrdma_ep_connect(struct rpcrdma_ep *, struct rpcrdma_ia *);
+int rpcrdma_ep_disconnect(struct rpcrdma_ep *, struct rpcrdma_ia *);
+
+int rpcrdma_ep_post(struct rpcrdma_ia *, struct rpcrdma_ep *,
+				struct rpcrdma_req *);
+int rpcrdma_ep_post_recv(struct rpcrdma_ia *, struct rpcrdma_ep *,
+				struct rpcrdma_rep *);
+
+/*
+ * Buffer calls - xprtrdma/verbs.c
+ */
+int rpcrdma_buffer_create(struct rpcrdma_buffer *, struct rpcrdma_ep *,
+				struct rpcrdma_ia *,
+				struct rpcrdma_create_data_internal *);
+void rpcrdma_buffer_destroy(struct rpcrdma_buffer *);
+
+struct rpcrdma_req *rpcrdma_buffer_get(struct rpcrdma_buffer *);
+void rpcrdma_buffer_put(struct rpcrdma_req *);
+void rpcrdma_recv_buffer_get(struct rpcrdma_req *);
+void rpcrdma_recv_buffer_put(struct rpcrdma_rep *);
+
+int rpcrdma_register_internal(struct rpcrdma_ia *, void *, int,
+				struct ib_mr **, struct ib_sge *);
+int rpcrdma_deregister_internal(struct rpcrdma_ia *,
+				struct ib_mr *, struct ib_sge *);
+
+int rpcrdma_register_external(struct rpcrdma_mr_seg *,
+				int, int, struct rpcrdma_xprt *);
+int rpcrdma_deregister_external(struct rpcrdma_mr_seg *,
+				struct rpcrdma_xprt *, void *);
+
+/*
+ * RPC/RDMA connection management calls - xprtrdma/rpc_rdma.c
+ */
+void rpcrdma_conn_func(struct rpcrdma_ep *);
+void rpcrdma_reply_handler(struct rpcrdma_rep *);
+
+/*
+ * RPC/RDMA protocol calls - xprtrdma/rpc_rdma.c
+ */
+int rpcrdma_marshal_req(struct rpc_rqst *);
+
+#endif				/* _LINUX_SUNRPC_XPRT_RDMA_H */
