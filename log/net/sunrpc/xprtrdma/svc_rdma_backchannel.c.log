commit f5046b8f43d180d6f4c9129a750b98f9a503de2d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Mar 20 15:11:56 2020 -0400

    svcrdma: Remove backchannel dprintk call sites
    
    Clean up.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index b174f3b109a5..1ee73f7cf931 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -10,10 +10,6 @@
 #include "xprt_rdma.h"
 #include <trace/events/rpcrdma.h>
 
-#define RPCDBG_FACILITY	RPCDBG_SVCXPRT
-
-#undef SVCRDMA_BACKCHANNEL_DEBUG
-
 /**
  * svc_rdma_handle_bc_reply - Process incoming backchannel Reply
  * @rqstp: resources for handling the Reply
@@ -31,33 +27,17 @@ void svc_rdma_handle_bc_reply(struct svc_rqst *rqstp,
 	__be32 *rdma_resp = rctxt->rc_recv_buf;
 	struct rpc_rqst *req;
 	u32 credits;
-	size_t len;
-	__be32 xid;
-	__be32 *p;
-
-	p = (__be32 *)src->iov_base;
-	len = src->iov_len;
-	xid = *rdma_resp;
-
-#ifdef SVCRDMA_BACKCHANNEL_DEBUG
-	pr_info("%s: xid=%08x, length=%zu\n",
-		__func__, be32_to_cpu(xid), len);
-	pr_info("%s: RPC/RDMA: %*ph\n",
-		__func__, (int)RPCRDMA_HDRLEN_MIN, rdma_resp);
-	pr_info("%s:      RPC: %*ph\n",
-		__func__, (int)len, p);
-#endif
 
 	spin_lock(&xprt->queue_lock);
-	req = xprt_lookup_rqst(xprt, xid);
+	req = xprt_lookup_rqst(xprt, *rdma_resp);
 	if (!req)
 		goto out_unlock;
 
 	dst = &req->rq_private_buf.head[0];
 	memcpy(&req->rq_private_buf, &req->rq_rcv_buf, sizeof(struct xdr_buf));
-	if (dst->iov_len < len)
+	if (dst->iov_len < src->iov_len)
 		goto out_unlock;
-	memcpy(dst->iov_base, p, len);
+	memcpy(dst->iov_base, src->iov_base, src->iov_len);
 	xprt_pin_rqst(req);
 	spin_unlock(&xprt->queue_lock);
 
@@ -66,7 +46,6 @@ void svc_rdma_handle_bc_reply(struct svc_rqst *rqstp,
 		credits = 1;	/* don't deadlock */
 	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)
 		credits = r_xprt->rx_buf.rb_bc_max_requests;
-
 	spin_lock(&xprt->transport_lock);
 	xprt->cwnd = credits << RPC_CWNDSHIFT;
 	spin_unlock(&xprt->transport_lock);
@@ -174,10 +153,6 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	*p++ = xdr_zero;
 	*p   = xdr_zero;
 
-#ifdef SVCRDMA_BACKCHANNEL_DEBUG
-	pr_info("%s: %*ph\n", __func__, 64, rqst->rq_buffer);
-#endif
-
 	rqst->rq_xtime = ktime_get();
 	rc = svc_rdma_bc_sendto(rdma, rqst, ctxt);
 	if (rc)
@@ -188,7 +163,6 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	svc_rdma_send_ctxt_put(rdma, ctxt);
 
 drop_connection:
-	dprintk("svcrdma: failed to send bc call\n");
 	return -ENOTCONN;
 }
 
@@ -207,9 +181,6 @@ static int xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
 		container_of(sxprt, struct svcxprt_rdma, sc_xprt);
 	int ret;
 
-	dprintk("svcrdma: sending bc call with xid: %08x\n",
-		be32_to_cpu(rqst->rq_xid));
-
 	if (test_bit(XPT_DEAD, &sxprt->xpt_flags))
 		return -ENOTCONN;
 
@@ -222,8 +193,6 @@ static int xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
 static void
 xprt_rdma_bc_close(struct rpc_xprt *xprt)
 {
-	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
-
 	xprt_disconnect_done(xprt);
 	xprt->cwnd = RPC_CWNDSHIFT;
 }
@@ -231,8 +200,6 @@ xprt_rdma_bc_close(struct rpc_xprt *xprt)
 static void
 xprt_rdma_bc_put(struct rpc_xprt *xprt)
 {
-	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
-
 	xprt_rdma_free_addresses(xprt);
 	xprt_free(xprt);
 }
@@ -267,19 +234,14 @@ xprt_setup_rdma_bc(struct xprt_create *args)
 	struct rpc_xprt *xprt;
 	struct rpcrdma_xprt *new_xprt;
 
-	if (args->addrlen > sizeof(xprt->addr)) {
-		dprintk("RPC:       %s: address too large\n", __func__);
+	if (args->addrlen > sizeof(xprt->addr))
 		return ERR_PTR(-EBADF);
-	}
 
 	xprt = xprt_alloc(args->net, sizeof(*new_xprt),
 			  RPCRDMA_MAX_BC_REQUESTS,
 			  RPCRDMA_MAX_BC_REQUESTS);
-	if (!xprt) {
-		dprintk("RPC:       %s: couldn't allocate rpc_xprt\n",
-			__func__);
+	if (!xprt)
 		return ERR_PTR(-ENOMEM);
-	}
 
 	xprt->timeout = &xprt_rdma_bc_timeout;
 	xprt_set_bound(xprt);

commit ea740bd5f58e2912e74f401fd01a9d6aa985ca05
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Mar 20 17:32:41 2020 -0400

    svcrdma: Fix backchannel return code
    
    Way back when I was writing the RPC/RDMA server-side backchannel
    code, I misread the TCP backchannel reply handler logic. When
    svc_tcp_recvfrom() successfully receives a backchannel reply, it
    does not return -EAGAIN. It sets XPT_DATA and returns zero.
    
    Update svc_rdma_recvfrom() to return zero. Here, XPT_DATA doesn't
    need to be set again: it is set whenever a new message is received,
    behind a spin lock in a single threaded context.
    
    Also, if handling the cb reply is not successful, the message is
    simply dropped. There's no special message framing to deal with as
    there is in the TCP case.
    
    Now that the handle_bc_reply() return value is ignored, I've removed
    the dprintk call sites in the error exit of handle_bc_reply() in
    favor of trace points in other areas that already report the error
    cases.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index d9aab4504f2c..b174f3b109a5 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -15,26 +15,25 @@
 #undef SVCRDMA_BACKCHANNEL_DEBUG
 
 /**
- * svc_rdma_handle_bc_reply - Process incoming backchannel reply
- * @xprt: controlling backchannel transport
- * @rdma_resp: pointer to incoming transport header
- * @rcvbuf: XDR buffer into which to decode the reply
+ * svc_rdma_handle_bc_reply - Process incoming backchannel Reply
+ * @rqstp: resources for handling the Reply
+ * @rctxt: Received message
  *
- * Returns:
- *	%0 if @rcvbuf is filled in, xprt_complete_rqst called,
- *	%-EAGAIN if server should call ->recvfrom again.
  */
-int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
-			     struct xdr_buf *rcvbuf)
+void svc_rdma_handle_bc_reply(struct svc_rqst *rqstp,
+			      struct svc_rdma_recv_ctxt *rctxt)
 {
+	struct svc_xprt *sxprt = rqstp->rq_xprt;
+	struct rpc_xprt *xprt = sxprt->xpt_bc_xprt;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct xdr_buf *rcvbuf = &rqstp->rq_arg;
 	struct kvec *dst, *src = &rcvbuf->head[0];
+	__be32 *rdma_resp = rctxt->rc_recv_buf;
 	struct rpc_rqst *req;
 	u32 credits;
 	size_t len;
 	__be32 xid;
 	__be32 *p;
-	int ret;
 
 	p = (__be32 *)src->iov_base;
 	len = src->iov_len;
@@ -49,14 +48,10 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 		__func__, (int)len, p);
 #endif
 
-	ret = -EAGAIN;
-	if (src->iov_len < 24)
-		goto out_shortreply;
-
 	spin_lock(&xprt->queue_lock);
 	req = xprt_lookup_rqst(xprt, xid);
 	if (!req)
-		goto out_notfound;
+		goto out_unlock;
 
 	dst = &req->rq_private_buf.head[0];
 	memcpy(&req->rq_private_buf, &req->rq_rcv_buf, sizeof(struct xdr_buf));
@@ -77,25 +72,12 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	spin_unlock(&xprt->transport_lock);
 
 	spin_lock(&xprt->queue_lock);
-	ret = 0;
 	xprt_complete_rqst(req->rq_task, rcvbuf->len);
 	xprt_unpin_rqst(req);
 	rcvbuf->len = 0;
 
 out_unlock:
 	spin_unlock(&xprt->queue_lock);
-out:
-	return ret;
-
-out_shortreply:
-	dprintk("svcrdma: short bc reply: xprt=%p, len=%zu\n",
-		xprt, src->iov_len);
-	goto out;
-
-out_notfound:
-	dprintk("svcrdma: unrecognized bc reply: xprt=%p, xid=%08x\n",
-		xprt, be32_to_cpu(xid));
-	goto out_unlock;
 }
 
 /* Send a backwards direction RPC call.

commit ca4faf543a33373bed3650812d5f0cd0bd295b1a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat May 2 10:37:44 2020 -0400

    SUNRPC: Move xpt_mutex into socket xpo_sendto methods
    
    It appears that the RPC/RDMA transport does not need serialization
    of calls to its xpo_sendto method. Move the mutex into the socket
    methods that still need that serialization.
    
    Tail latencies are unambiguously better with this patch applied.
    fio randrw 8KB 70/30 on NFSv3, smaller numbers are better:
    
        clat percentiles (usec):
    
    With xpt_mutex:
    r    | 99.99th=[ 8848]
    w    | 99.99th=[ 9634]
    
    Without xpt_mutex:
    r    | 99.99th=[ 8586]
    w    | 99.99th=[ 8979]
    
    Serializing the construction of RPC/RDMA transport headers is not
    really necessary at this point, because the Linux NFS server
    implementation never changes its credit grant on a connection. If
    that should change, then svc_rdma_sendto will need to serialize
    access to the transport's credit grant fields.
    
    Reported-by: kbuild test robot <lkp@intel.com>
    [ cel: fix uninitialized variable warning ]
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index af7eb8d202ae..d9aab4504f2c 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -210,34 +210,31 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	return -ENOTCONN;
 }
 
-/* Send an RPC call on the passive end of a transport
- * connection.
+/**
+ * xprt_rdma_bc_send_request - Send a reverse-direction Call
+ * @rqst: rpc_rqst containing Call message to be sent
+ *
+ * Return values:
+ *   %0 if the message was sent successfully
+ *   %ENOTCONN if the message was not sent
  */
-static int
-xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
+static int xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
 {
 	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
-	struct svcxprt_rdma *rdma;
+	struct svcxprt_rdma *rdma =
+		container_of(sxprt, struct svcxprt_rdma, sc_xprt);
 	int ret;
 
 	dprintk("svcrdma: sending bc call with xid: %08x\n",
 		be32_to_cpu(rqst->rq_xid));
 
-	mutex_lock(&sxprt->xpt_mutex);
-
-	ret = -ENOTCONN;
-	rdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);
-	if (!test_bit(XPT_DEAD, &sxprt->xpt_flags)) {
-		ret = rpcrdma_bc_send_request(rdma, rqst);
-		if (ret == -ENOTCONN)
-			svc_close_xprt(sxprt);
-	}
+	if (test_bit(XPT_DEAD, &sxprt->xpt_flags))
+		return -ENOTCONN;
 
-	mutex_unlock(&sxprt->xpt_mutex);
-
-	if (ret < 0)
-		return ret;
-	return 0;
+	ret = rpcrdma_bc_send_request(rdma, rqst);
+	if (ret == -ENOTCONN)
+		svc_close_xprt(sxprt);
+	return ret;
 }
 
 static void

commit 6221f1d9b63fed6260273e59a2b89ab30537a811
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Apr 17 12:40:31 2020 -0400

    SUNRPC: Fix backchannel RPC soft lockups
    
    Currently, after the forward channel connection goes away,
    backchannel operations are causing soft lockups on the server
    because call_transmit_status's SOFTCONN logic ignores ENOTCONN.
    Such backchannel Calls are aggressively retried until the client
    reconnects.
    
    Backchannel Calls should use RPC_TASK_NOCONNECT rather than
    RPC_TASK_SOFTCONN. If there is no forward connection, the server is
    not capable of establishing a connection back to the client, thus
    that backchannel request should fail before the server attempts to
    send it. Commit 58255a4e3ce5 ("NFSD: NFSv4 callback client should
    use RPC_TASK_SOFTCONN") was merged several years before
    RPC_TASK_NOCONNECT was available.
    
    Because setup_callback_client() explicitly sets NOPING, the NFSv4.0
    callback connection depends on the first callback RPC to initiate
    a connection to the client. Thus NFSv4.0 needs to continue to use
    RPC_TASK_SOFTCONN.
    
    Suggested-by: Trond Myklebust <trondmy@hammerspace.com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Cc: <stable@vger.kernel.org> # v4.20+

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index d510a3a15d4b..af7eb8d202ae 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -244,6 +244,8 @@ static void
 xprt_rdma_bc_close(struct rpc_xprt *xprt)
 {
 	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
+
+	xprt_disconnect_done(xprt);
 	xprt->cwnd = RPC_CWNDSHIFT;
 }
 

commit 1a33d8a284b1e85e03b8c7b1ea8fb985fccd1d71
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 24 16:53:59 2020 -0400

    svcrdma: Fix leak of transport addresses
    
    Kernel memory leak detected:
    
    unreferenced object 0xffff888849cdf480 (size 8):
      comm "kworker/u8:3", pid 2086, jiffies 4297898756 (age 4269.856s)
      hex dump (first 8 bytes):
        30 00 cd 49 88 88 ff ff                          0..I....
      backtrace:
        [<00000000acfc370b>] __kmalloc_track_caller+0x137/0x183
        [<00000000a2724354>] kstrdup+0x2b/0x43
        [<0000000082964f84>] xprt_rdma_format_addresses+0x114/0x17d [rpcrdma]
        [<00000000dfa6ed00>] xprt_setup_rdma_bc+0xc0/0x10c [rpcrdma]
        [<0000000073051a83>] xprt_create_transport+0x3f/0x1a0 [sunrpc]
        [<0000000053531a8e>] rpc_create+0x118/0x1cd [sunrpc]
        [<000000003a51b5f8>] setup_callback_client+0x1a5/0x27d [nfsd]
        [<000000001bd410af>] nfsd4_process_cb_update.isra.7+0x16c/0x1ac [nfsd]
        [<000000007f4bbd56>] nfsd4_run_cb_work+0x4c/0xbd [nfsd]
        [<0000000055c5586b>] process_one_work+0x1b2/0x2fe
        [<00000000b1e3e8ef>] worker_thread+0x1a6/0x25a
        [<000000005205fb78>] kthread+0xf6/0xfb
        [<000000006d2dc057>] ret_from_fork+0x3a/0x50
    
    Introduce a call to xprt_rdma_free_addresses() similar to the way
    that the TCP backchannel releases a transport's peer address
    strings.
    
    Fixes: 5d252f90a800 ("svcrdma: Add class for RDMA backwards direction transport")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 46b59e91d34a..d510a3a15d4b 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -252,6 +252,7 @@ xprt_rdma_bc_put(struct rpc_xprt *xprt)
 {
 	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
 
+	xprt_rdma_free_addresses(xprt);
 	xprt_free(xprt);
 }
 

commit aee4b74a3f273b54d136132fedf575ec464f4134
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 3 11:08:05 2020 -0500

    svcrdma: Fix double sync of transport header buffer
    
    Performance optimization: Avoid syncing the transport buffer twice
    when Reply buffer pull-up is necessary.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 9830748c58d2..46b59e91d34a 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -191,7 +191,6 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	*p++ = xdr_zero;
 	*p++ = xdr_zero;
 	*p   = xdr_zero;
-	svc_rdma_sync_reply_hdr(rdma, ctxt, ctxt->sc_hdrbuf.len);
 
 #ifdef SVCRDMA_BACKCHANNEL_DEBUG
 	pr_info("%s: %*ph\n", __func__, 64, rqst->rq_buffer);

commit 6fd5034db45c9c0ca57c98f3d5b9a0ce5869eab3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 15:02:20 2020 -0500

    svcrdma: Refactor chunk list encoders
    
    Same idea as the receive-side changes I did a while back: use
    xdr_stream helpers rather than open-coding the XDR chunk list
    encoders. This builds the Reply transport header from beginning to
    end without backtracking.
    
    As additional clean-ups, fill in documenting comments for the XDR
    encoders and sprinkle some trace points in the new encoding
    functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index ce1a7a706f36..9830748c58d2 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -181,7 +181,9 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	if (!ctxt)
 		goto drop_connection;
 
-	p = ctxt->sc_xprt_buf;
+	p = xdr_reserve_space(&ctxt->sc_stream, RPCRDMA_HDRLEN_MIN);
+	if (!p)
+		goto put_ctxt;
 	*p++ = rqst->rq_xid;
 	*p++ = rpcrdma_version;
 	*p++ = cpu_to_be32(r_xprt->rx_buf.rb_bc_max_requests);
@@ -189,7 +191,7 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	*p++ = xdr_zero;
 	*p++ = xdr_zero;
 	*p   = xdr_zero;
-	svc_rdma_sync_reply_hdr(rdma, ctxt, RPCRDMA_HDRLEN_MIN);
+	svc_rdma_sync_reply_hdr(rdma, ctxt, ctxt->sc_hdrbuf.len);
 
 #ifdef SVCRDMA_BACKCHANNEL_DEBUG
 	pr_info("%s: %*ph\n", __func__, 64, rqst->rq_buffer);
@@ -197,12 +199,13 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 
 	rqst->rq_xtime = ktime_get();
 	rc = svc_rdma_bc_sendto(rdma, rqst, ctxt);
-	if (rc) {
-		svc_rdma_send_ctxt_put(rdma, ctxt);
-		goto drop_connection;
-	}
+	if (rc)
+		goto put_ctxt;
 	return 0;
 
+put_ctxt:
+	svc_rdma_send_ctxt_put(rdma, ctxt);
+
 drop_connection:
 	dprintk("svcrdma: failed to send bc call\n");
 	return -ENOTCONN;

commit 4554755ed81bb690d709168550aba5b46447f069
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 2 15:02:19 2020 -0500

    svcrdma: Update synopsis of svc_rdma_map_reply_msg()
    
    Preparing for subsequent patches, no behavior change expected.
    
    Pass the RPC Call's svc_rdma_recv_ctxt deeper into the sendto()
    path. This enables passing more information about Requester-
    provided Write and Reply chunks into those lower-level functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 908e78bb87c6..ce1a7a706f36 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -117,7 +117,7 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 {
 	int ret;
 
-	ret = svc_rdma_map_reply_msg(rdma, ctxt, &rqst->rq_snd_buf, NULL);
+	ret = svc_rdma_map_reply_msg(rdma, ctxt, NULL, &rqst->rq_snd_buf);
 	if (ret < 0)
 		return -EIO;
 

commit 8729aaba74626c4ebce3abf1b9e96bb62d2958ca
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Nov 20 16:25:46 2019 -0500

    SUNRPC: Fix backchannel latency metrics
    
    I noticed that for callback requests, the reported backlog latency
    is always zero, and the rtt value is crazy big. The problem was that
    rqst->rq_xtime is never set for backchannel requests.
    
    Fixes: 78215759e20d ("SUNRPC: Make RTT measurement more ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index d1fcc41d5eb5..908e78bb87c6 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -195,6 +195,7 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	pr_info("%s: %*ph\n", __func__, 64, rqst->rq_buffer);
 #endif
 
+	rqst->rq_xtime = ktime_get();
 	rc = svc_rdma_bc_sendto(rdma, rqst, ctxt);
 	if (rc) {
 		svc_rdma_send_ctxt_put(rdma, ctxt);

commit b5e924191f87239e555f3ef3b8d8e697bb95e7dc
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Thu May 2 11:21:08 2019 -0400

    SUNRPC: Remove the bh-safe lock requirement on xprt->transport_lock
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index bed57d8b5c19..d1fcc41d5eb5 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -72,9 +72,9 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)
 		credits = r_xprt->rx_buf.rb_bc_max_requests;
 
-	spin_lock_bh(&xprt->transport_lock);
+	spin_lock(&xprt->transport_lock);
 	xprt->cwnd = credits << RPC_CWNDSHIFT;
-	spin_unlock_bh(&xprt->transport_lock);
+	spin_unlock(&xprt->transport_lock);
 
 	spin_lock(&xprt->queue_lock);
 	ret = 0;

commit 8ba6a92d0182091e0c2fa15c1a5b5458bac25fc3
Author: Trond Myklebust <trondmy@gmail.com>
Date:   Sun Apr 7 13:58:46 2019 -0400

    SUNRPC: Refactor xprt_request_wait_receive()
    
    Convert the transport callback to actually put the request to sleep
    instead of just setting a timeout. This is in preparation for
    rpc_sleep_on_timeout().
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 907464c2a9f0..bed57d8b5c19 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -261,7 +261,7 @@ static const struct rpc_xprt_ops xprt_rdma_bc_procs = {
 	.buf_alloc		= xprt_rdma_bc_allocate,
 	.buf_free		= xprt_rdma_bc_free,
 	.send_request		= xprt_rdma_bc_send_request,
-	.set_retrans_timeout	= xprt_set_retrans_timeout_def,
+	.wait_for_reply_request	= xprt_wait_for_reply_request_def,
 	.close			= xprt_rdma_bc_close,
 	.destroy		= xprt_rdma_bc_put,
 	.print_stats		= xprt_rdma_print_stats

commit 067fb11b12af1448f7bbcacca41e470cb775e9fa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Feb 11 11:24:37 2019 -0500

    SUNRPC: Remove rpc_xprt::tsh_size
    
    tsh_size was added to accommodate transports that send a pre-amble
    before each RPC message. However, this assumes the pre-amble is
    fixed in size, which isn't true for some transports. That makes
    tsh_size not very generic.
    
    Also I'd like to make the estimation of RPC send and receive
    buffer sizes more precise. tsh_size doesn't currently appear to be
    accounted for at all by call_allocate.
    
    Therefore let's just remove the tsh_size concept, and make the only
    transports that have a non-zero tsh_size employ a direct approach.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index b908f2ca08fd..907464c2a9f0 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -304,7 +304,6 @@ xprt_setup_rdma_bc(struct xprt_create *args)
 	xprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;
 
 	xprt->prot = XPRT_TRANSPORT_BC_RDMA;
-	xprt->tsh_size = 0;
 	xprt->ops = &xprt_rdma_bc_procs;
 
 	memcpy(&xprt->addr, args->dstaddr, args->addrlen);

commit 0c0829bcf51aef713806e49b8ea2bac7962f54e2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:40 2018 -0500

    xprtrdma: Don't wake pending tasks until disconnect is done
    
    Transport disconnect processing does a "wake pending tasks" at
    various points.
    
    Suppose an RPC Reply is being processed. The RPC task that Reply
    goes with is waiting on the pending queue. If a disconnect wake-up
    happens before reply processing is done, that reply, even if it is
    good, is thrown away, and the RPC has to be sent again.
    
    This window apparently does not exist for socket transports because
    there is a lock held while a reply is being received which prevents
    the wake-up call until after reply processing is done.
    
    To resolve this, all RPC replies being processed on an RPC-over-RDMA
    transport have to complete before pending tasks are awoken due to a
    transport disconnect.
    
    Callers that already hold the transport write lock may invoke
    ->ops->close directly. Others use a generic helper that schedules
    a close when the write lock can be taken safely.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index f3c147d70286..b908f2ca08fd 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -200,11 +200,10 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 		svc_rdma_send_ctxt_put(rdma, ctxt);
 		goto drop_connection;
 	}
-	return rc;
+	return 0;
 
 drop_connection:
 	dprintk("svcrdma: failed to send bc call\n");
-	xprt_disconnect_done(xprt);
 	return -ENOTCONN;
 }
 
@@ -225,8 +224,11 @@ xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
 
 	ret = -ENOTCONN;
 	rdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);
-	if (!test_bit(XPT_DEAD, &sxprt->xpt_flags))
+	if (!test_bit(XPT_DEAD, &sxprt->xpt_flags)) {
 		ret = rpcrdma_bc_send_request(rdma, rqst);
+		if (ret == -ENOTCONN)
+			svc_close_xprt(sxprt);
+	}
 
 	mutex_unlock(&sxprt->xpt_mutex);
 

commit 310c7585e8300ddc46211df0757c11e4299ec482
Merge: 9b190ecca11c 93f38b6fae0e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 30 13:03:29 2018 -0700

    Merge tag 'nfsd-4.20' of git://linux-nfs.org/~bfields/linux
    
    Pull nfsd updates from Bruce Fields:
     "Olga added support for the NFSv4.2 asynchronous copy protocol. We
      already supported COPY, by copying a limited amount of data and then
      returning a short result, letting the client resend. The asynchronous
      protocol should offer better performance at the expense of some
      complexity.
    
      The other highlight is Trond's work to convert the duplicate reply
      cache to a red-black tree, and to move it and some other server caches
      to RCU. (Previously these have meant taking global spinlocks on every
      RPC)
    
      Otherwise, some RDMA work and miscellaneous bugfixes"
    
    * tag 'nfsd-4.20' of git://linux-nfs.org/~bfields/linux: (30 commits)
      lockd: fix access beyond unterminated strings in prints
      nfsd: Fix an Oops in free_session()
      nfsd: correctly decrement odstate refcount in error path
      svcrdma: Increase the default connection credit limit
      svcrdma: Remove try_module_get from backchannel
      svcrdma: Remove ->release_rqst call in bc reply handler
      svcrdma: Reduce max_send_sges
      nfsd: fix fall-through annotations
      knfsd: Improve lookup performance in the duplicate reply cache using an rbtree
      knfsd: Further simplify the cache lookup
      knfsd: Simplify NFS duplicate replay cache
      knfsd: Remove dead code from nfsd_cache_lookup
      SUNRPC: Simplify TCP receive code
      SUNRPC: Replace the cache_detail->hash_lock with a regular spinlock
      SUNRPC: Remove non-RCU protected lookup
      NFS: Fix up a typo in nfs_dns_ent_put
      NFS: Lockless DNS lookups
      knfsd: Lockless lookup of NFSv4 identities.
      SUNRPC: Lockless server RPCSEC_GSS context lookup
      knfsd: Allow lockless lookups of the exports
      ...

commit 07880fa4968b07db9df0f126bb7fbde245ed12ff
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:16:06 2018 -0400

    svcrdma: Remove try_module_get from backchannel
    
    Since commit ffe1f0df5862 ("rpcrdma: Merge svcrdma and xprtrdma
    modules into one"), the forward and backchannel components are part
    of the same kernel module. A separate try_module_get() call in the
    backchannel code is no longer necessary.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 642f0310cc16..7e188bc793a2 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -5,8 +5,6 @@
  * Support for backward direction RPCs on RPC/RDMA (server-side).
  */
 
-#include <linux/module.h>
-
 #include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
@@ -255,7 +253,6 @@ xprt_rdma_bc_put(struct rpc_xprt *xprt)
 	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
 
 	xprt_free(xprt);
-	module_put(THIS_MODULE);
 }
 
 static const struct rpc_xprt_ops xprt_rdma_bc_procs = {
@@ -327,20 +324,9 @@ xprt_setup_rdma_bc(struct xprt_create *args)
 	args->bc_xprt->xpt_bc_xprt = xprt;
 	xprt->bc_xprt = args->bc_xprt;
 
-	if (!try_module_get(THIS_MODULE))
-		goto out_fail;
-
 	/* Final put for backchannel xprt is in __svc_rdma_free */
 	xprt_get(xprt);
 	return xprt;
-
-out_fail:
-	xprt_rdma_free_addresses(xprt);
-	args->bc_xprt->xpt_bc_xprt = NULL;
-	args->bc_xprt->xpt_bc_xps = NULL;
-	xprt_put(xprt);
-	xprt_free(xprt);
-	return ERR_PTR(-EINVAL);
 }
 
 struct xprt_class xprt_rdma_bc = {

commit 596f2a1950437095fee3062b75935c3bfdda0289
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:16:01 2018 -0400

    svcrdma: Remove ->release_rqst call in bc reply handler
    
    Similar to a change made in the client's forward channel reply
    handler: The xprt_release_rqst_cong() call is not necessary.
    
    Also, release xprt->recv_lock when taking xprt->transport_lock
    to avoid disabling and enabling BH's while holding another
    spin lock.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index a68180090554..642f0310cc16 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -32,7 +32,6 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	struct kvec *dst, *src = &rcvbuf->head[0];
 	struct rpc_rqst *req;
-	unsigned long cwnd;
 	u32 credits;
 	size_t len;
 	__be32 xid;
@@ -66,6 +65,8 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	if (dst->iov_len < len)
 		goto out_unlock;
 	memcpy(dst->iov_base, p, len);
+	xprt_pin_rqst(req);
+	spin_unlock(&xprt->recv_lock);
 
 	credits = be32_to_cpup(rdma_resp + 2);
 	if (credits == 0)
@@ -74,15 +75,13 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 		credits = r_xprt->rx_buf.rb_bc_max_requests;
 
 	spin_lock_bh(&xprt->transport_lock);
-	cwnd = xprt->cwnd;
 	xprt->cwnd = credits << RPC_CWNDSHIFT;
-	if (xprt->cwnd > cwnd)
-		xprt_release_rqst_cong(req->rq_task);
 	spin_unlock_bh(&xprt->transport_lock);
 
-
+	spin_lock(&xprt->recv_lock);
 	ret = 0;
 	xprt_complete_rqst(req->rq_task, rcvbuf->len);
+	xprt_unpin_rqst(req);
 	rcvbuf->len = 0;
 
 out_unlock:

commit 93bdcf9fdb89646e9446fab25aec76a754969619
Merge: 826799e66e86 470443e0b379
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Thu Oct 18 17:29:00 2018 -0400

    Merge tag 'nfs-rdma-for-4.20-1' of git://git.linux-nfs.org/projects/anna/linux-nfs
    
    NFS RDMA client updates for Linux 4.20
    
    Stable bugfixes:
    - Reset credit grant properly after a disconnect
    
    Other bugfixes and cleanups:
    - xprt_release_rqst_cong is called outside of transport_lock
    - Create more MRs at a time and toss out old ones during recovery
    - Various improvements to the RDMA connection and disconnection code:
      - Improve naming of trace events, functions, and variables
      - Add documenting comments
      - Fix metrics and stats reporting
    - Fix a tracepoint sparse warning
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

commit ef739b2175dde9c05594f768cb78149f1ce2ac36
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:25:14 2018 -0400

    xprtrdma: Reset credit grant properly after a disconnect
    
    On a fresh connection, an RPC/RDMA client is supposed to send only
    one RPC Call until it gets a credit grant in the first RPC Reply
    from the server [RFC 8166, Section 3.3.3].
    
    There is a bug in the Linux client's credit accounting mechanism
    introduced by commit e7ce710a8802 ("xprtrdma: Avoid deadlock when
    credit window is reset"). On connect, it simply dumps all pending
    RPC Calls onto the new connection.
    
    Servers have been tolerant of this bad behavior. Currently no server
    implementation ever changes its credit grant over reconnects, and
    servers always repost enough Receives before connections are fully
    established.
    
    To correct this issue, ensure that the client resets both the credit
    grant _and_ the congestion window when handling a reconnect.
    
    Fixes: e7ce710a8802 ("xprtrdma: Avoid deadlock when credit ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Cc: stable@kernel.org
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index a68180090554..b9827665ff35 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -248,6 +248,7 @@ static void
 xprt_rdma_bc_close(struct rpc_xprt *xprt)
 {
 	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
+	xprt->cwnd = RPC_CWNDSHIFT;
 }
 
 static void

commit adfa71446dd0943ba376eff3e05c7c89582f8038
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Mon Sep 3 23:58:59 2018 -0400

    SUNRPC: Cleanup: remove the unused 'task' argument from the request_send()
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 35a8c3aab302..992312504cfd 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -215,7 +215,7 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
  * connection.
  */
 static int
-xprt_rdma_bc_send_request(struct rpc_rqst *rqst, struct rpc_task *task)
+xprt_rdma_bc_send_request(struct rpc_rqst *rqst)
 {
 	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
 	struct svcxprt_rdma *rdma;

commit c544577daddb618c7dd5fa7fb98d6a41782f020e
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Mon Sep 3 23:39:27 2018 -0400

    SUNRPC: Clean up transport write space handling
    
    Treat socket write space handling in the same way we now treat transport
    congestion: by denying the XPRT_LOCK until the transport signals that it
    has free buffer space.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index d1618c70edb4..35a8c3aab302 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -224,12 +224,7 @@ xprt_rdma_bc_send_request(struct rpc_rqst *rqst, struct rpc_task *task)
 	dprintk("svcrdma: sending bc call with xid: %08x\n",
 		be32_to_cpu(rqst->rq_xid));
 
-	if (!mutex_trylock(&sxprt->xpt_mutex)) {
-		rpc_sleep_on(&sxprt->xpt_bc_pending, task, NULL);
-		if (!mutex_trylock(&sxprt->xpt_mutex))
-			return -EAGAIN;
-		rpc_wake_up_queued_task(&sxprt->xpt_bc_pending, task);
-	}
+	mutex_lock(&sxprt->xpt_mutex);
 
 	ret = -ENOTCONN;
 	rdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);

commit 50f484e298218b7271fad8a23bd44c82fb3110e1
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Thu Aug 30 13:27:29 2018 -0400

    SUNRPC: Treat the task and request as separate in the xprt_ops->send_request()
    
    When we shift to using the transmit queue, then the task that holds the
    write lock will not necessarily be the same as the one being transmitted.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 09b12b7568fe..d1618c70edb4 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -215,9 +215,8 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
  * connection.
  */
 static int
-xprt_rdma_bc_send_request(struct rpc_task *task)
+xprt_rdma_bc_send_request(struct rpc_rqst *rqst, struct rpc_task *task)
 {
-	struct rpc_rqst *rqst = task->tk_rqstp;
 	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
 	struct svcxprt_rdma *rdma;
 	int ret;

commit 75c84151a9dc7a755c607e6761d8f14a1690dbf0
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Fri Aug 31 10:21:00 2018 -0400

    SUNRPC: Rename xprt->recv_lock to xprt->queue_lock
    
    We will use the same lock to protect both the transmit and receive queues.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index a68180090554..09b12b7568fe 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -56,7 +56,7 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	if (src->iov_len < 24)
 		goto out_shortreply;
 
-	spin_lock(&xprt->recv_lock);
+	spin_lock(&xprt->queue_lock);
 	req = xprt_lookup_rqst(xprt, xid);
 	if (!req)
 		goto out_notfound;
@@ -86,7 +86,7 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	rcvbuf->len = 0;
 
 out_unlock:
-	spin_unlock(&xprt->recv_lock);
+	spin_unlock(&xprt->queue_lock);
 out:
 	return ret;
 

commit 0725d4e1b8b08a60838db3a6e65c23ea8824a048
Merge: 89e255678fec 93b7f7ad2018
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 10:09:03 2018 -0700

    Merge tag 'nfs-for-4.18-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      Stable fixes:
    
       - Fix a 1-byte stack overflow in nfs_idmap_read_and_verify_message
    
       - Fix a hang due to incorrect error returns in rpcrdma_convert_iovs()
    
       - Revert an incorrect change to the NFSv4.1 callback channel
    
       - Fix a bug in the NFSv4.1 sequence error handling
    
      Features and optimisations:
    
       - Support for piggybacking a LAYOUTGET operation to the OPEN compound
    
       - RDMA performance enhancements to deal with transport congestion
    
       - Add proper SPDX tags for NetApp-contributed RDMA source
    
       - Do not request delegated file attributes (size+change) from the
         server
    
       - Optimise away a GETATTR in the lookup revalidate code when doing
         NFSv4 OPEN
    
       - Optimise away unnecessary lookups for rename targets
    
       - Misc performance improvements when freeing NFSv4 delegations
    
      Bugfixes and cleanups:
    
       - Try to fail quickly if proto=rdma
    
       - Clean up RDMA receive trace points
    
       - Fix sillyrename to return the delegation when appropriate
    
       - Misc attribute revalidation fixes
    
       - Immediately clear the pNFS layout on a file when the server returns
         ESTALE
    
       - Return NFS4ERR_DELAY when delegation/layout recalls fail due to
         igrab()
    
       - Fix the client behaviour on NFS4ERR_SEQ_FALSE_RETRY"
    
    * tag 'nfs-for-4.18-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (80 commits)
      skip LAYOUTRETURN if layout is invalid
      NFSv4.1: Fix the client behaviour on NFS4ERR_SEQ_FALSE_RETRY
      NFSv4: Fix a typo in nfs41_sequence_process
      NFSv4: Revert commit 5f83d86cf531d ("NFSv4.x: Fix wraparound issues..")
      NFSv4: Return NFS4ERR_DELAY when a layout recall fails due to igrab()
      NFSv4: Return NFS4ERR_DELAY when a delegation recall fails due to igrab()
      NFSv4.0: Remove transport protocol name from non-UCS client ID
      NFSv4.0: Remove cl_ipaddr from non-UCS client ID
      NFSv4: Fix a compiler warning when CONFIG_NFS_V4_1 is undefined
      NFS: Filter cache invalidation when holding a delegation
      NFS: Ignore NFS_INO_REVAL_FORCED in nfs_check_inode_attributes()
      NFS: Improve caching while holding a delegation
      NFS: Fix attribute revalidation
      NFS: fix up nfs_setattr_update_inode
      NFSv4: Ensure the inode is clean when we set a delegation
      NFSv4: Ignore NFS_INO_REVAL_FORCED in nfs4_proc_access
      NFSv4: Don't ask for delegated attributes when adding a hard link
      NFSv4: Don't ask for delegated attributes when revalidating the inode
      NFS: Pass the inode down to the getattr() callback
      NFSv4: Don't request size+change attribute if they are delegated to us
      ...

commit 99722fe4d5a634707ced8d8f42b883b87a86b3c5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:25 2018 -0400

    svcrdma: Persistently allocate and DMA-map Send buffers
    
    While sending each RPC Reply, svc_rdma_sendto allocates and DMA-
    maps a separate buffer where the RPC/RDMA transport header is
    constructed. The buffer is unmapped and released in the Send
    completion handler. This is significant per-RPC overhead,
    especially for small RPCs.
    
    Instead, allocate and DMA-map a buffer, and cache it in each
    svc_rdma_send_ctxt. This buffer and its mapping can be re-used
    for each RPC, saving the cost of memory allocation and DMA
    mapping.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 40f5e4afbcc8..343e7add672c 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -115,43 +115,21 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
  * the adapter has a small maximum SQ depth.
  */
 static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
-			      struct rpc_rqst *rqst)
+			      struct rpc_rqst *rqst,
+			      struct svc_rdma_send_ctxt *ctxt)
 {
-	struct svc_rdma_send_ctxt *ctxt;
 	int ret;
 
-	ctxt = svc_rdma_send_ctxt_get(rdma);
-	if (!ctxt) {
-		ret = -ENOMEM;
-		goto out_err;
-	}
-
-	/* rpcrdma_bc_send_request builds the transport header and
-	 * the backchannel RPC message in the same buffer. Thus only
-	 * one SGE is needed to send both.
-	 */
-	ret = svc_rdma_map_reply_hdr(rdma, ctxt, rqst->rq_buffer,
-				     rqst->rq_snd_buf.len);
+	ret = svc_rdma_map_reply_msg(rdma, ctxt, &rqst->rq_snd_buf, NULL);
 	if (ret < 0)
-		goto out_err;
+		return -EIO;
 
 	/* Bump page refcnt so Send completion doesn't release
 	 * the rq_buffer before all retransmits are complete.
 	 */
 	get_page(virt_to_page(rqst->rq_buffer));
 	ctxt->sc_send_wr.opcode = IB_WR_SEND;
-	ret = svc_rdma_send(rdma, &ctxt->sc_send_wr);
-	if (ret)
-		goto out_unmap;
-
-out_err:
-	dprintk("svcrdma: %s returns %d\n", __func__, ret);
-	return ret;
-
-out_unmap:
-	svc_rdma_send_ctxt_put(rdma, ctxt);
-	ret = -EIO;
-	goto out_err;
+	return svc_rdma_send(rdma, &ctxt->sc_send_wr);
 }
 
 /* Server-side transport endpoint wants a whole page for its send
@@ -198,13 +176,15 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 {
 	struct rpc_xprt *xprt = rqst->rq_xprt;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct svc_rdma_send_ctxt *ctxt;
 	__be32 *p;
 	int rc;
 
-	/* Space in the send buffer for an RPC/RDMA header is reserved
-	 * via xprt->tsh_size.
-	 */
-	p = rqst->rq_buffer;
+	ctxt = svc_rdma_send_ctxt_get(rdma);
+	if (!ctxt)
+		goto drop_connection;
+
+	p = ctxt->sc_xprt_buf;
 	*p++ = rqst->rq_xid;
 	*p++ = rpcrdma_version;
 	*p++ = cpu_to_be32(r_xprt->rx_buf.rb_bc_max_requests);
@@ -212,14 +192,17 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 	*p++ = xdr_zero;
 	*p++ = xdr_zero;
 	*p   = xdr_zero;
+	svc_rdma_sync_reply_hdr(rdma, ctxt, RPCRDMA_HDRLEN_MIN);
 
 #ifdef SVCRDMA_BACKCHANNEL_DEBUG
 	pr_info("%s: %*ph\n", __func__, 64, rqst->rq_buffer);
 #endif
 
-	rc = svc_rdma_bc_sendto(rdma, rqst);
-	if (rc)
+	rc = svc_rdma_bc_sendto(rdma, rqst, ctxt);
+	if (rc) {
+		svc_rdma_send_ctxt_put(rdma, ctxt);
 		goto drop_connection;
+	}
 	return rc;
 
 drop_connection:
@@ -327,7 +310,7 @@ xprt_setup_rdma_bc(struct xprt_create *args)
 	xprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;
 
 	xprt->prot = XPRT_TRANSPORT_BC_RDMA;
-	xprt->tsh_size = RPCRDMA_HDRLEN_MIN / sizeof(__be32);
+	xprt->tsh_size = 0;
 	xprt->ops = &xprt_rdma_bc_procs;
 
 	memcpy(&xprt->addr, args->dstaddr, args->addrlen);

commit 986b78894b268f605e9ea055b99959bdce0e5945
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:15 2018 -0400

    svcrdma: Remove post_send_wr
    
    Clean up: Now that the send_wr is part of the svc_rdma_send_ctxt,
    svc_rdma_post_send_wr is nearly empty.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 95e33511cc6f..40f5e4afbcc8 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -139,7 +139,8 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	 * the rq_buffer before all retransmits are complete.
 	 */
 	get_page(virt_to_page(rqst->rq_buffer));
-	ret = svc_rdma_post_send_wr(rdma, ctxt, 0);
+	ctxt->sc_send_wr.opcode = IB_WR_SEND;
+	ret = svc_rdma_send(rdma, &ctxt->sc_send_wr);
 	if (ret)
 		goto out_unmap;
 

commit 4201c7464753827803366b40e82eb050c04ebdef
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:28:04 2018 -0400

    svcrdma: Introduce svc_rdma_send_ctxt
    
    svc_rdma_op_ctxt's are pre-allocated and maintained on a per-xprt
    free list. This eliminates the overhead of calling kmalloc / kfree,
    both of which grab a globally shared lock that disables interrupts.
    Introduce a replacement to svc_rdma_op_ctxt's that is built
    especially for the svcrdma Send path.
    
    Subsequent patches will take advantage of this new structure by
    allocating real resources which are then cached in these objects.
    The allocations are freed when the transport is torn down.
    
    I've renamed the structure so that static type checking can be used
    to ensure that uses of op_ctxt and send_ctxt are not confused. As an
    additional clean up, structure fields are renamed to conform with
    kernel coding conventions.
    
    Additional clean ups:
    - Handle svc_rdma_send_ctxt_get allocation failure at each call
      site, rather than pre-allocating and hoping we guessed correctly
    - All send_ctxt_put call-sites request page freeing, so remove
      the @free_pages argument
    - All send_ctxt_put call-sites unmap SGEs, so fold that into
      svc_rdma_send_ctxt_put
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 0b9ba9f50a76..95e33511cc6f 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -1,6 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 /*
- * Copyright (c) 2015 Oracle.  All rights reserved.
+ * Copyright (c) 2015-2018 Oracle.  All rights reserved.
  *
  * Support for backward direction RPCs on RPC/RDMA (server-side).
  */
@@ -117,10 +117,14 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 			      struct rpc_rqst *rqst)
 {
-	struct svc_rdma_op_ctxt *ctxt;
+	struct svc_rdma_send_ctxt *ctxt;
 	int ret;
 
-	ctxt = svc_rdma_get_context(rdma);
+	ctxt = svc_rdma_send_ctxt_get(rdma);
+	if (!ctxt) {
+		ret = -ENOMEM;
+		goto out_err;
+	}
 
 	/* rpcrdma_bc_send_request builds the transport header and
 	 * the backchannel RPC message in the same buffer. Thus only
@@ -144,8 +148,7 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	return ret;
 
 out_unmap:
-	svc_rdma_unmap_dma(ctxt);
-	svc_rdma_put_context(ctxt, 1);
+	svc_rdma_send_ctxt_put(rdma, ctxt);
 	ret = -EIO;
 	goto out_err;
 }

commit 232627905f12a05df75853c62451ce0886803cee
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:59 2018 -0400

    svcrdma: Clean up Send SGE accounting
    
    Clean up: Since there's already a svc_rdma_op_ctxt being passed
    around with the running count of mapped SGEs, drop unneeded
    parameters to svc_rdma_post_send_wr().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index d50152163190..0b9ba9f50a76 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -135,7 +135,7 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	 * the rq_buffer before all retransmits are complete.
 	 */
 	get_page(virt_to_page(rqst->rq_buffer));
-	ret = svc_rdma_post_send_wr(rdma, ctxt, 1, 0);
+	ret = svc_rdma_post_send_wr(rdma, ctxt, 0);
 	if (ret)
 		goto out_unmap;
 

commit bd2abef33394dc16d63580c38c01420db991f0f2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:16 2018 -0400

    svcrdma: Trace key RDMA API events
    
    This includes:
      * Posting on the Send and Receive queues
      * Send, Receive, Read, and Write completion
      * Connect upcalls
      * QP errors
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index a73632ca9048..d50152163190 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -6,8 +6,11 @@
  */
 
 #include <linux/module.h>
+
 #include <linux/sunrpc/svc_rdma.h>
+
 #include "xprt_rdma.h"
+#include <trace/events/rpcrdma.h>
 
 #define RPCDBG_FACILITY	RPCDBG_SVCXPRT
 

commit a9cde23ab7cdf5e4e93432dffd0e734267f2b745
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:34:59 2018 -0400

    SUNRPC: Add a ->free_slot transport callout
    
    Refactor: xprtrdma needs to have better control over when RPCs are
    awoken from the backlog queue, so replace xprt_free_slot with a
    transport op callout.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index a73632ca9048..1035516d54e2 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -273,6 +273,7 @@ static const struct rpc_xprt_ops xprt_rdma_bc_procs = {
 	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong,
 	.alloc_slot		= xprt_alloc_slot,
+	.free_slot		= xprt_free_slot,
 	.release_request	= xprt_release_rqst_cong,
 	.buf_alloc		= xprt_rdma_bc_allocate,
 	.buf_free		= xprt_rdma_bc_free,

commit 482725027ff32bc857f5527fb17feda5361265fe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 3 15:42:18 2018 -0500

    svcrdma: Post Receives in the Receive completion handler
    
    This change improves Receive efficiency by posting Receives only
    on the same CPU that handles Receive completion. Improved latency
    and throughput has been noted with this change.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index af7893501e40..a73632ca9048 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -95,7 +95,6 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 out_notfound:
 	dprintk("svcrdma: unrecognized bc reply: xprt=%p, xid=%08x\n",
 		xprt, be32_to_cpu(xid));
-
 	goto out_unlock;
 }
 
@@ -129,10 +128,6 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	if (ret < 0)
 		goto out_err;
 
-	ret = svc_rdma_repost_recv(rdma, GFP_NOIO);
-	if (ret)
-		goto out_err;
-
 	/* Bump page refcnt so Send completion doesn't release
 	 * the rq_buffer before all retransmits are complete.
 	 */

commit 4dd3c2e5a4225e3df85afc6033e62ce8b09f0ed2
Merge: 07c455ee222f 22700f3c6df5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 18 11:22:04 2017 -0800

    Merge tag 'nfsd-4.15' of git://linux-nfs.org/~bfields/linux
    
    Pull nfsd updates from Bruce Fields:
     "Lots of good bugfixes, including:
    
       -  fix a number of races in the NFSv4+ state code
    
       -  fix some shutdown crashes in multiple-network-namespace cases
    
       -  relax our 4.1 session limits; if you've an artificially low limit
          to the number of 4.1 clients that can mount simultaneously, try
          upgrading"
    
    * tag 'nfsd-4.15' of git://linux-nfs.org/~bfields/linux: (22 commits)
      SUNRPC: Improve ordering of transport processing
      nfsd: deal with revoked delegations appropriately
      svcrdma: Enqueue after setting XPT_CLOSE in completion handlers
      nfsd: use nfs->ns.inum as net ID
      rpc: remove some BUG()s
      svcrdma: Preserve CB send buffer across retransmits
      nfds: avoid gettimeofday for nfssvc_boot time
      fs, nfsd: convert nfs4_file.fi_ref from atomic_t to refcount_t
      fs, nfsd: convert nfs4_cntl_odstate.co_odcount from atomic_t to refcount_t
      fs, nfsd: convert nfs4_stid.sc_count from atomic_t to refcount_t
      lockd: double unregister of inetaddr notifiers
      nfsd4: catch some false session retries
      nfsd4: fix cached replies to solo SEQUENCE compounds
      sunrcp: make function _svc_create_xprt static
      SUNRPC: Fix tracepoint storage issues with svc_recv and svc_rqst_status
      nfsd: use ARRAY_SIZE
      nfsd: give out fewer session slots as limit approaches
      nfsd: increase DRC cache limit
      nfsd: remove unnecessary nofilehandle checks
      nfs_common: convert int to bool
      ...

commit 0bad47cada5defba13e98827d22d06f13258dfb3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 16 12:14:33 2017 -0400

    svcrdma: Preserve CB send buffer across retransmits
    
    During each NFSv4 callback Call, an RDMA Send completion frees the
    page that contains the RPC Call message. If the upper layer
    determines that a retransmit is necessary, this is too soon.
    
    One possible symptom: after a GARBAGE_ARGS response an NFSv4.1
    callback request, the following BUG fires on the NFS server:
    
    kernel: BUG: Bad page state in process kworker/0:2H  pfn:7d3ce2
    kernel: page:ffffea001f4f3880 count:-2 mapcount:0 mapping:          (null) index:0x0
    kernel: flags: 0x2fffff80000000()
    kernel: raw: 002fffff80000000 0000000000000000 0000000000000000 fffffffeffffffff
    kernel: raw: dead000000000100 dead000000000200 0000000000000000 0000000000000000
    kernel: page dumped because: nonzero _refcount
    kernel: Modules linked in: cts rpcsec_gss_krb5 ocfs2_dlmfs ocfs2_stack_o2cb ocfs2_dlm
    ocfs2_nodemanager ocfs2_stackglue rpcrdm a ib_ipoib rdma_ucm ib_ucm ib_uverbs ib_umad
    rdma_cm ib_cm iw_cm x86_pkg_temp_thermal intel_powerclamp coretemp kvm_intel
    kvm irqbypass crct10dif_pc lmul crc32_pclmul ghash_clmulni_intel pcbc iTCO_wdt
    iTCO_vendor_support aesni_intel crypto_simd glue_helper cryptd pcspkr lpc_ich i2c_i801
    mei_me mf d_core mei raid0 sg wmi ioatdma ipmi_si ipmi_devintf ipmi_msghandler shpchp
    acpi_power_meter acpi_pad nfsd nfs_acl lockd auth_rpcgss grace sunrpc ip_tables xfs
    libcrc32c mlx4_en mlx4_ib mlx5_ib ib_core sd_mod sr_mod cdrom ast drm_kms_helper
    syscopyarea sysfillrect sysimgblt fb_sys_fops ttm ahci crc32c_intel libahci drm
    mlx5_core igb libata mlx4_core dca i2c_algo_bit i2c_core nvme
    kernel: ptp nvme_core pps_core dm_mirror dm_region_hash dm_log dm_mod dax
    kernel: CPU: 0 PID: 11495 Comm: kworker/0:2H Not tainted 4.14.0-rc3-00001-g577ce48 #811
    kernel: Hardware name: Supermicro Super Server/X10SRL-F, BIOS 1.0c 09/09/2015
    kernel: Workqueue: ib-comp-wq ib_cq_poll_work [ib_core]
    kernel: Call Trace:
    kernel: dump_stack+0x62/0x80
    kernel: bad_page+0xfe/0x11a
    kernel: free_pages_check_bad+0x76/0x78
    kernel: free_pcppages_bulk+0x364/0x441
    kernel: ? ttwu_do_activate.isra.61+0x71/0x78
    kernel: free_hot_cold_page+0x1c5/0x202
    kernel: __put_page+0x2c/0x36
    kernel: svc_rdma_put_context+0xd9/0xe4 [rpcrdma]
    kernel: svc_rdma_wc_send+0x50/0x98 [rpcrdma]
    
    This issue exists all the way back to v4.5, but refactoring and code
    re-organization prevents this simple patch from applying to kernels
    older than v4.12. The fix is the same, however, if someone needs to
    backport it.
    
    Reported-by: Ben Coddington <bcodding@redhat.com>
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=314
    Fixes: 5d252f90a800 ('svcrdma: Add class for RDMA backwards ... ')
    Cc: stable@vger.kernel.org # v4.12
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index ec37ad83b068..1854db227742 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -132,6 +132,10 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	if (ret)
 		goto out_err;
 
+	/* Bump page refcnt so Send completion doesn't release
+	 * the rq_buffer before all retransmits are complete.
+	 */
+	get_page(virt_to_page(rqst->rq_buffer));
 	ret = svc_rdma_post_send_wr(rdma, ctxt, 1, 0);
 	if (ret)
 		goto out_unmap;
@@ -164,7 +168,6 @@ xprt_rdma_bc_allocate(struct rpc_task *task)
 		return -EINVAL;
 	}
 
-	/* svc_rdma_sendto releases this page */
 	page = alloc_page(RPCRDMA_DEF_GFP);
 	if (!page)
 		return -ENOMEM;
@@ -183,6 +186,7 @@ xprt_rdma_bc_free(struct rpc_task *task)
 {
 	struct rpc_rqst *rqst = task->tk_rqstp;
 
+	put_page(virt_to_page(rqst->rq_buffer));
 	kfree(rqst->rq_rbuffer);
 }
 

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index ec37ad83b068..992594b7cc6b 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (c) 2015 Oracle.  All rights reserved.
  *

commit f9773b22a27a4234f436c9570afd62d905e00a13
Merge: 7af7a5963c40 67af6f652f9c
Author: Trond Myklebust <trond.myklebust@primarydata.com>
Date:   Tue Sep 5 15:16:04 2017 -0400

    Merge tag 'nfs-rdma-for-4.14-1' of git://git.linux-nfs.org/projects/anna/linux-nfs into linux-next
    
    NFS-over-RDMA client updates for Linux 4.14
    
    Bugfixes and cleanups:
    - Constify rpc_xprt_ops
    - Harden RPC call encoding and decoding
    - Clean up rpc call decoding to use xdr_streams
    - Remove unused variables from various structures
    - Refactor code to remove imul instructions
    - Rearrange rx_stats structure for better cacheline sharing

commit ce7c252a8c741aba7c38f817b86e34361f561e42
Author: Trond Myklebust <trond.myklebust@primarydata.com>
Date:   Wed Aug 16 15:30:35 2017 -0400

    SUNRPC: Add a separate spinlock to protect the RPC request receive list
    
    This further reduces contention with the transport_lock, and allows us
    to convert to using a non-bh-safe spinlock, since the list is now never
    accessed from a bh context.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index c676ed0efb5a..0d574cda242d 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -52,7 +52,7 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	if (src->iov_len < 24)
 		goto out_shortreply;
 
-	spin_lock_bh(&xprt->transport_lock);
+	spin_lock(&xprt->recv_lock);
 	req = xprt_lookup_rqst(xprt, xid);
 	if (!req)
 		goto out_notfound;
@@ -69,17 +69,20 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)
 		credits = r_xprt->rx_buf.rb_bc_max_requests;
 
+	spin_lock_bh(&xprt->transport_lock);
 	cwnd = xprt->cwnd;
 	xprt->cwnd = credits << RPC_CWNDSHIFT;
 	if (xprt->cwnd > cwnd)
 		xprt_release_rqst_cong(req->rq_task);
+	spin_unlock_bh(&xprt->transport_lock);
+
 
 	ret = 0;
 	xprt_complete_rqst(req->rq_task, rcvbuf->len);
 	rcvbuf->len = 0;
 
 out_unlock:
-	spin_unlock_bh(&xprt->transport_lock);
+	spin_unlock(&xprt->recv_lock);
 out:
 	return ret;
 

commit d31ae2548142b7cd12404929ef3a13ae27c9d961
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Aug 1 12:00:39 2017 -0400

    sunrpc: Const-ify all instances of struct rpc_xprt_ops
    
    After transport instance creation, these function pointers never
    change. Mark them as constant to prevent their use as an attack
    vector for code injections.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index c676ed0efb5a..ca41e28d2b36 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -266,7 +266,7 @@ xprt_rdma_bc_put(struct rpc_xprt *xprt)
 	module_put(THIS_MODULE);
 }
 
-static struct rpc_xprt_ops xprt_rdma_bc_procs = {
+static const struct rpc_xprt_ops xprt_rdma_bc_procs = {
 	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong,
 	.alloc_slot		= xprt_alloc_slot,

commit f5821c76b2c9c2fb98b276c0bf6a101bfe9050a3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:06:49 2017 -0400

    svcrdma: Clean up RPC-over-RDMA backchannel reply processing
    
    Replace C structure-based XDR decoding with pointer arithmetic.
    Pointer arithmetic is considered more portable.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index bf185b79c98f..c676ed0efb5a 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -12,7 +12,17 @@
 
 #undef SVCRDMA_BACKCHANNEL_DEBUG
 
-int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,
+/**
+ * svc_rdma_handle_bc_reply - Process incoming backchannel reply
+ * @xprt: controlling backchannel transport
+ * @rdma_resp: pointer to incoming transport header
+ * @rcvbuf: XDR buffer into which to decode the reply
+ *
+ * Returns:
+ *	%0 if @rcvbuf is filled in, xprt_complete_rqst called,
+ *	%-EAGAIN if server should call ->recvfrom again.
+ */
+int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, __be32 *rdma_resp,
 			     struct xdr_buf *rcvbuf)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
@@ -27,13 +37,13 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,
 
 	p = (__be32 *)src->iov_base;
 	len = src->iov_len;
-	xid = rmsgp->rm_xid;
+	xid = *rdma_resp;
 
 #ifdef SVCRDMA_BACKCHANNEL_DEBUG
 	pr_info("%s: xid=%08x, length=%zu\n",
 		__func__, be32_to_cpu(xid), len);
 	pr_info("%s: RPC/RDMA: %*ph\n",
-		__func__, (int)RPCRDMA_HDRLEN_MIN, rmsgp);
+		__func__, (int)RPCRDMA_HDRLEN_MIN, rdma_resp);
 	pr_info("%s:      RPC: %*ph\n",
 		__func__, (int)len, p);
 #endif
@@ -53,7 +63,7 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,
 		goto out_unlock;
 	memcpy(dst->iov_base, p, len);
 
-	credits = be32_to_cpu(rmsgp->rm_credit);
+	credits = be32_to_cpup(rdma_resp + 2);
 	if (credits == 0)
 		credits = 1;	/* don't deadlock */
 	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)

commit 9a6a180b7867ceceeeab88a6f011bac23174b939
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:06:25 2017 -0400

    svcrdma: Use rdma_rw API in RPC reply path
    
    The current svcrdma sendto code path posts one RDMA Write WR at a
    time. Each of these Writes typically carries a small number of pages
    (for instance, up to 30 pages for mlx4 devices). That means a 1MB
    NFS READ reply requires 9 ib_post_send() calls for the Write WRs,
    and one for the Send WR carrying the actual RPC Reply message.
    
    Instead, use the new rdma_rw API. The details of Write WR chain
    construction and memory registration are taken care of in the RDMA
    core. svcrdma can focus on the details of the RPC-over-RDMA
    protocol. This gives three main benefits:
    
    1. All Write WRs for one RDMA segment are posted in a single chain.
    As few as one ib_post_send() for each Write chunk.
    
    2. The Write path can now use FRWR to register the Write buffers.
    If the device's maximum page list depth is large, this means a
    single Write WR is needed for each RPC's Write chunk data.
    
    3. The new code introduces support for RPCs that carry both a Write
    list and a Reply chunk. This combination can be used for an NFSv4
    READ where the data payload is large, and thus is removed from the
    Payload Stream, but the Payload Stream is still larger than the
    inline threshold.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 0305b33d482f..bf185b79c98f 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -90,9 +90,9 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,
  * Caller holds the connection's mutex and has already marshaled
  * the RPC/RDMA request.
  *
- * This is similar to svc_rdma_reply, but takes an rpc_rqst
- * instead, does not support chunks, and avoids blocking memory
- * allocation.
+ * This is similar to svc_rdma_send_reply_msg, but takes a struct
+ * rpc_rqst instead, does not support chunks, and avoids blocking
+ * memory allocation.
  *
  * XXX: There is still an opportunity to block in svc_rdma_send()
  * if there are no SQ entries to post the Send. This may occur if

commit 6e6092ca305ad785c605d7e313727aad96c228a5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:05:44 2017 -0400

    svcrdma: Add svc_rdma_map_reply_hdr()
    
    Introduce a helper to DMA-map a reply's transport header before
    sending it. This will in part replace the map vector cache.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index f12f39c189c3..0305b33d482f 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -101,50 +101,36 @@ int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,
 static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 			      struct rpc_rqst *rqst)
 {
-	struct xdr_buf *sndbuf = &rqst->rq_snd_buf;
 	struct svc_rdma_op_ctxt *ctxt;
-	struct svc_rdma_req_map *vec;
 	int ret;
 
-	vec = svc_rdma_get_req_map(rdma);
-	ret = svc_rdma_map_xdr(rdma, sndbuf, vec, false);
-	if (ret)
+	ctxt = svc_rdma_get_context(rdma);
+
+	/* rpcrdma_bc_send_request builds the transport header and
+	 * the backchannel RPC message in the same buffer. Thus only
+	 * one SGE is needed to send both.
+	 */
+	ret = svc_rdma_map_reply_hdr(rdma, ctxt, rqst->rq_buffer,
+				     rqst->rq_snd_buf.len);
+	if (ret < 0)
 		goto out_err;
 
 	ret = svc_rdma_repost_recv(rdma, GFP_NOIO);
 	if (ret)
 		goto out_err;
 
-	ctxt = svc_rdma_get_context(rdma);
-	ctxt->pages[0] = virt_to_page(rqst->rq_buffer);
-	ctxt->count = 1;
-
-	ctxt->direction = DMA_TO_DEVICE;
-	ctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;
-	ctxt->sge[0].length = sndbuf->len;
-	ctxt->sge[0].addr =
-	    ib_dma_map_page(rdma->sc_cm_id->device, ctxt->pages[0], 0,
-			    sndbuf->len, DMA_TO_DEVICE);
-	if (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr)) {
-		ret = -EIO;
-		goto out_unmap;
-	}
-	svc_rdma_count_mappings(rdma, ctxt);
-
 	ret = svc_rdma_post_send_wr(rdma, ctxt, 1, 0);
-	if (ret) {
-		ret = -EIO;
+	if (ret)
 		goto out_unmap;
-	}
 
 out_err:
-	svc_rdma_put_req_map(rdma, vec);
 	dprintk("svcrdma: %s returns %d\n", __func__, ret);
 	return ret;
 
 out_unmap:
 	svc_rdma_unmap_dma(ctxt);
 	svc_rdma_put_context(ctxt, 1);
+	ret = -EIO;
 	goto out_err;
 }
 

commit 17f5f7f506aaca985b95df7ef7fc2ff49c36a8e9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Apr 9 13:05:36 2017 -0400

    svcrdma: Move send_wr to svc_rdma_op_ctxt
    
    Clean up: Move the ib_send_wr off the stack, and move common code
    to post a Send Work Request into a helper.
    
    This is a refactoring change only.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index ff1df40f0d26..f12f39c189c3 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -104,7 +104,6 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	struct xdr_buf *sndbuf = &rqst->rq_snd_buf;
 	struct svc_rdma_op_ctxt *ctxt;
 	struct svc_rdma_req_map *vec;
-	struct ib_send_wr send_wr;
 	int ret;
 
 	vec = svc_rdma_get_req_map(rdma);
@@ -132,15 +131,7 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	}
 	svc_rdma_count_mappings(rdma, ctxt);
 
-	memset(&send_wr, 0, sizeof(send_wr));
-	ctxt->cqe.done = svc_rdma_wc_send;
-	send_wr.wr_cqe = &ctxt->cqe;
-	send_wr.sg_list = ctxt->sge;
-	send_wr.num_sge = 1;
-	send_wr.opcode = IB_WR_SEND;
-	send_wr.send_flags = IB_SEND_SIGNALED;
-
-	ret = svc_rdma_send(rdma, &send_wr);
+	ret = svc_rdma_post_send_wr(rdma, ctxt, 1, 0);
 	if (ret) {
 		ret = -EIO;
 		goto out_unmap;

commit 8313064c2e75542201e557e2b496668811c2484a
Merge: b2deee2dc06d ff7d11797e72
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 28 15:39:09 2017 -0800

    Merge tag 'nfsd-4.11' of git://linux-nfs.org/~bfields/linux
    
    Pull nfsd updates from Bruce Fields:
     "The nfsd update this round is mainly a lot of miscellaneous cleanups
      and bugfixes.
    
      A couple changes could theoretically break working setups on upgrade.
      I don't expect complaints in practice, but they seem worth calling out
      just in case:
    
       - NFS security labels are now off by default; a new security_label
         export flag reenables it per export. But, having them on by default
         is a disaster, as it generally only makes sense if all your clients
         and servers have similar enough selinux policies. Thanks to Jason
         Tibbitts for pointing this out.
    
       - NFSv4/UDP support is off. It was never really supported, and the
         spec explicitly forbids it. We only ever left it on out of
         laziness; thanks to Jeff Layton for finally fixing that"
    
    * tag 'nfsd-4.11' of git://linux-nfs.org/~bfields/linux: (34 commits)
      nfsd: Fix display of the version string
      nfsd: fix configuration of supported minor versions
      sunrpc: don't register UDP port with rpcbind when version needs congestion control
      nfs/nfsd/sunrpc: enforce transport requirements for NFSv4
      sunrpc: flag transports as having congestion control
      sunrpc: turn bitfield flags in svc_version into bools
      nfsd: remove superfluous KERN_INFO
      nfsd: special case truncates some more
      nfsd: minor nfsd_setattr cleanup
      NFSD: Reserve adequate space for LOCKT operation
      NFSD: Get response size before operation for all RPCs
      nfsd/callback: Drop a useless data copy when comparing sessionid
      nfsd/callback: skip the callback tag
      nfsd/callback: Cleanup callback cred on shutdown
      nfsd/idmap: return nfserr_inval for 0-length names
      SUNRPC/Cache: Always treat the invalid cache as unexpired
      SUNRPC: Drop all entries from cache_detail when cache_purge()
      svcrdma: Poll CQs in "workqueue" mode
      svcrdma: Combine list fields in struct svc_rdma_op_ctxt
      svcrdma: Remove unused sc_dto_q field
      ...

commit f39b2dde4886450b965ccd0a6defe8bc33fcc938
Author: Russell King <rmk+kernel@armlinux.org.uk>
Date:   Tue Feb 7 15:02:54 2017 -0800

    net: sunrpc: fix build errors when linux/phy*.h is removed from net/dsa.h
    
    Removing linux/phy.h from net/dsa.h reveals a build error in the sunrpc
    code:
    
    net/sunrpc/xprtrdma/svc_rdma_backchannel.c: In function 'xprt_rdma_bc_put':
    net/sunrpc/xprtrdma/svc_rdma_backchannel.c:277:2: error: implicit declaration of function 'module_put' [-Werror=implicit-function-declaration]
    net/sunrpc/xprtrdma/svc_rdma_backchannel.c: In function 'xprt_setup_rdma_bc':
    net/sunrpc/xprtrdma/svc_rdma_backchannel.c:348:7: error: implicit declaration of function 'try_module_get' [-Werror=implicit-function-declaration]
    
    Fix this by adding linux/module.h to svc_rdma_backchannel.c
    
    Signed-off-by: Russell King <rmk+kernel@armlinux.org.uk>
    Acked-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 288e35c2d8f4..cb1e48e54eb1 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -4,6 +4,7 @@
  * Support for backward direction RPCs on RPC/RDMA (server-side).
  */
 
+#include <linux/module.h>
 #include <linux/sunrpc/svc_rdma.h>
 #include "xprt_rdma.h"
 

commit c2ccf64a6c4bf3bcbf4e6e6b94a51500991022a1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Feb 7 11:58:40 2017 -0500

    svcrdma: Clean up backchannel send header encoding
    
    Replace C structure-based XDR decoding with pointer arithmetic.
    Pointer arithmetic is considered more portable.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 288e35c2d8f4..b1bc7a796782 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -200,19 +200,20 @@ rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
 {
 	struct rpc_xprt *xprt = rqst->rq_xprt;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
-	struct rpcrdma_msg *headerp = (struct rpcrdma_msg *)rqst->rq_buffer;
+	__be32 *p;
 	int rc;
 
 	/* Space in the send buffer for an RPC/RDMA header is reserved
 	 * via xprt->tsh_size.
 	 */
-	headerp->rm_xid = rqst->rq_xid;
-	headerp->rm_vers = rpcrdma_version;
-	headerp->rm_credit = cpu_to_be32(r_xprt->rx_buf.rb_bc_max_requests);
-	headerp->rm_type = rdma_msg;
-	headerp->rm_body.rm_chunks[0] = xdr_zero;
-	headerp->rm_body.rm_chunks[1] = xdr_zero;
-	headerp->rm_body.rm_chunks[2] = xdr_zero;
+	p = rqst->rq_buffer;
+	*p++ = rqst->rq_xid;
+	*p++ = rpcrdma_version;
+	*p++ = cpu_to_be32(r_xprt->rx_buf.rb_bc_max_requests);
+	*p++ = rdma_msg;
+	*p++ = xdr_zero;
+	*p++ = xdr_zero;
+	*p   = xdr_zero;
 
 #ifdef SVCRDMA_BACKCHANNEL_DEBUG
 	pr_info("%s: %*ph\n", __func__, 64, rqst->rq_buffer);

commit 9a16a34d2146e1bcfad6c6ae6d214d645b424265
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 11:05:15 2016 -0500

    svcrdma: Remove unused variables in xprt_rdma_bc_allocate()
    
    Clean up.
    
    /linux-2.6/net/sunrpc/xprtrdma/svc_rdma_backchannel.c: In function
     xprt_rdma_bc_allocate:
    linux-2.6/net/sunrpc/xprtrdma/svc_rdma_backchannel.c:169:23: warning:
     variable rdma set but not used [-Wunused-but-set-variable]
      struct svcxprt_rdma *rdma;
                           ^
    
    Fixes: 5d252f90a800 ("svcrdma: Add class for RDMA backwards ...")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 6035c5a380a6..288e35c2d8f4 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -164,13 +164,9 @@ static int
 xprt_rdma_bc_allocate(struct rpc_task *task)
 {
 	struct rpc_rqst *rqst = task->tk_rqstp;
-	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
 	size_t size = rqst->rq_callsize;
-	struct svcxprt_rdma *rdma;
 	struct page *page;
 
-	rdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);
-
 	if (size > PAGE_SIZE) {
 		WARN_ONCE(1, "svcrdma: large bc buffer request (size %zu)\n",
 			  size);

commit 1b9f700b8cfc31089e2dfa5d0905c52fd4529b50
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 11:04:26 2016 -0500

    svcrdma: Clear xpt_bc_xps in xprt_setup_rdma_bc() error exit arm
    
    Logic copied from xs_setup_bc_tcp().
    
    Fixes: 39a9beab5acb ('rpc: share one xps between all backchannels')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 20027f8de129..6035c5a380a6 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -359,6 +359,7 @@ xprt_setup_rdma_bc(struct xprt_create *args)
 out_fail:
 	xprt_rdma_free_addresses(xprt);
 	args->bc_xprt->xpt_bc_xprt = NULL;
+	args->bc_xprt->xpt_bc_xps = NULL;
 	xprt_put(xprt);
 	xprt_free(xprt);
 	return ERR_PTR(-EINVAL);

commit 8d42629be0cb6c58a5c77cd02e47974c0e724e63
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 28 22:22:33 2016 -0400

    svcrdma: backchannel cannot share a page for send and rcv buffers
    
    The underlying transport releases the page pointed to by rq_buffer
    during xprt_rdma_bc_send_request. When the backchannel reply arrives,
    rq_rbuffer then points to freed memory.
    
    Fixes: 68778945e46f ('SUNRPC: Separate buffer pointers for RPC ...')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Cc: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index fc4535ead7c2..20027f8de129 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -177,19 +177,26 @@ xprt_rdma_bc_allocate(struct rpc_task *task)
 		return -EINVAL;
 	}
 
+	/* svc_rdma_sendto releases this page */
 	page = alloc_page(RPCRDMA_DEF_GFP);
 	if (!page)
 		return -ENOMEM;
-
 	rqst->rq_buffer = page_address(page);
-	rqst->rq_rbuffer = (char *)rqst->rq_buffer + rqst->rq_callsize;
+
+	rqst->rq_rbuffer = kmalloc(rqst->rq_rcvsize, RPCRDMA_DEF_GFP);
+	if (!rqst->rq_rbuffer) {
+		put_page(page);
+		return -ENOMEM;
+	}
 	return 0;
 }
 
 static void
 xprt_rdma_bc_free(struct rpc_task *task)
 {
-	/* No-op: ctxt and page have already been freed. */
+	struct rpc_rqst *rqst = task->tk_rqstp;
+
+	kfree(rqst->rq_rbuffer);
 }
 
 static int

commit 18e601d6adae5042f82d105ccd3d4498050f2ebf
Author: Jeff Layton <jlayton@redhat.com>
Date:   Mon Oct 24 20:33:23 2016 -0400

    sunrpc: fix some missing rq_rbuffer assignments
    
    We've been seeing some crashes in testing that look like this:
    
    BUG: unable to handle kernel NULL pointer dereference at           (null)
    IP: [<ffffffff8135ce99>] memcpy_orig+0x29/0x110
    PGD 212ca2067 PUD 212ca3067 PMD 0
    Oops: 0002 [#1] SMP
    Modules linked in: rpcsec_gss_krb5 nfsv4 dns_resolver nfs fscache ppdev parport_pc i2c_piix4 sg parport i2c_core virtio_balloon pcspkr acpi_cpufreq nfsd auth_rpcgss nfs_acl lockd grace sunrpc ip_tables xfs libcrc32c sd_mod ata_generic pata_acpi virtio_scsi 8139too ata_piix libata 8139cp mii virtio_pci floppy virtio_ring serio_raw virtio
    CPU: 1 PID: 1540 Comm: nfsd Not tainted 4.9.0-rc1 #39
    Hardware name: Red Hat KVM, BIOS 0.5.1 01/01/2007
    task: ffff88020d7ed200 task.stack: ffff880211838000
    RIP: 0010:[<ffffffff8135ce99>]  [<ffffffff8135ce99>] memcpy_orig+0x29/0x110
    RSP: 0018:ffff88021183bdd0  EFLAGS: 00010206
    RAX: 0000000000000000 RBX: ffff88020d7fa000 RCX: 000000f400000000
    RDX: 0000000000000014 RSI: ffff880212927020 RDI: 0000000000000000
    RBP: ffff88021183be30 R08: 01000000ef896996 R09: 0000000000000000
    R10: 0000000000000000 R11: 0000000000000000 R12: ffff880211704ca8
    R13: ffff88021473f000 R14: 00000000ef896996 R15: ffff880211704800
    FS:  0000000000000000(0000) GS:ffff88021fc80000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000000 CR3: 0000000212ca1000 CR4: 00000000000006e0
    Stack:
     ffffffffa01ea087 ffffffff63400001 ffff880215145e00 ffff880211bacd00
     ffff88021473f2b8 0000000000000004 00000000d0679d67 ffff880211bacd00
     ffff88020d7fa000 ffff88021473f000 0000000000000000 ffff88020d7faa30
    Call Trace:
     [<ffffffffa01ea087>] ? svc_tcp_recvfrom+0x5a7/0x790 [sunrpc]
     [<ffffffffa01f84d8>] svc_recv+0xad8/0xbd0 [sunrpc]
     [<ffffffffa0262d5e>] nfsd+0xde/0x160 [nfsd]
     [<ffffffffa0262c80>] ? nfsd_destroy+0x60/0x60 [nfsd]
     [<ffffffff810a9418>] kthread+0xd8/0xf0
     [<ffffffff816dbdbf>] ret_from_fork+0x1f/0x40
     [<ffffffff810a9340>] ? kthread_park+0x60/0x60
    Code: 00 00 48 89 f8 48 83 fa 20 72 7e 40 38 fe 7c 35 48 83 ea 20 48 83 ea 20 4c 8b 06 4c 8b 4e 08 4c 8b 56 10 4c 8b 5e 18 48 8d 76 20 <4c> 89 07 4c 89 4f 08 4c 89 57 10 4c 89 5f 18 48 8d 7f 20 73 d4
    RIP  [<ffffffff8135ce99>] memcpy_orig+0x29/0x110
     RSP <ffff88021183bdd0>
    CR2: 0000000000000000
    
    Both Bruce and Eryu ran a bisect here and found that the problematic
    patch was 68778945e46 (SUNRPC: Separate buffer pointers for RPC Call and
    Reply messages).
    
    That patch changed rpc_xdr_encode to use a new rq_rbuffer pointer to
    set up the receive buffer, but didn't change all of the necessary
    codepaths to set it properly. In particular the backchannel setup was
    missing.
    
    We need to set rq_rbuffer whenever rq_buffer is set. Ensure that it is.
    
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Chuck Lever <chuck.lever@oracle.com>
    Reported-by: Eryu Guan <guaneryu@gmail.com>
    Tested-by: Eryu Guan <guaneryu@gmail.com>
    Fixes: 68778945e46 "SUNRPC: Separate buffer pointers..."
    Reported-by: J. Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 2d8545c34095..fc4535ead7c2 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -182,6 +182,7 @@ xprt_rdma_bc_allocate(struct rpc_task *task)
 		return -ENOMEM;
 
 	rqst->rq_buffer = page_address(page);
+	rqst->rq_rbuffer = (char *)rqst->rq_buffer + rqst->rq_callsize;
 	return 0;
 }
 

commit c4a86165d1ec70c8e592fa7b7cc7070971533021
Merge: 2778556474b1 3f807e5ae559
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Oct 13 21:28:20 2016 -0700

    Merge tag 'nfs-for-4.9-1' of git://git.linux-nfs.org/projects/anna/linux-nfs
    
    Pull NFS client updates from Anna Schumaker:
     "Highlights include:
    
      Stable bugfixes:
       - sunrpc: fix writ espace race causing stalls
       - NFS: Fix inode corruption in nfs_prime_dcache()
       - NFSv4: Don't report revoked delegations as valid in nfs_have_delegation()
       - NFSv4: nfs4_copy_delegation_stateid() must fail if the delegation is invalid
       - NFSv4: Open state recovery must account for file permission changes
       - NFSv4.2: Fix a reference leak in nfs42_proc_layoutstats_generic
    
      Features:
       - Add support for tracking multiple layout types with an ordered list
       - Add support for using multiple backchannel threads on the client
       - Add support for pNFS file layout session trunking
       - Delay xprtrdma use of DMA API (for device driver removal)
       - Add support for xprtrdma remote invalidation
       - Add support for larger xprtrdma inline thresholds
       - Use a scatter/gather list for sending xprtrdma RPC calls
       - Add support for the CB_NOTIFY_LOCK callback
       - Improve hashing sunrpc auth_creds by using both uid and gid
    
      Bugfixes:
       - Fix xprtrdma use of DMA API
       - Validate filenames before adding to the dcache
       - Fix corruption of xdr->nwords in xdr_copy_to_scratch
       - Fix setting buffer length in xdr_set_next_buffer()
       - Don't deadlock the state manager on the SEQUENCE status flags
       - Various delegation and stateid related fixes
       - Retry operations if an interrupted slot receives EREMOTEIO
       - Make nfs boot time y2038 safe"
    
    * tag 'nfs-for-4.9-1' of git://git.linux-nfs.org/projects/anna/linux-nfs: (100 commits)
      NFSv4.2: Fix a reference leak in nfs42_proc_layoutstats_generic
      fs: nfs: Make nfs boot time y2038 safe
      sunrpc: replace generic auth_cred hash with auth-specific function
      sunrpc: add RPCSEC_GSS hash_cred() function
      sunrpc: add auth_unix hash_cred() function
      sunrpc: add generic_auth hash_cred() function
      sunrpc: add hash_cred() function to rpc_authops struct
      Retry operation on EREMOTEIO on an interrupted slot
      pNFS: Fix atime updates on pNFS clients
      sunrpc: queue work on system_power_efficient_wq
      NFSv4.1: Even if the stateid is OK, we may need to recover the open modes
      NFSv4: If recovery failed for a specific open stateid, then don't retry
      NFSv4: Fix retry issues with nfs41_test/free_stateid
      NFSv4: Open state recovery must account for file permission changes
      NFSv4: Mark the lock and open stateids as invalid after freeing them
      NFSv4: Don't test open_stateid unless it is set
      NFSv4: nfs4_do_handle_exception() handle revoke/expiry of a single stateid
      NFS: Always call nfs_inode_find_state_and_recover() when revoking a delegation
      NFSv4: Fix a race when updating an open_stateid
      NFSv4: Fix a race in nfs_inode_reclaim_delegation()
      ...

commit cace564f8b6260e806f5e28d7f192fd0e0c603ed
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Sep 13 10:52:50 2016 -0400

    svcrdma: Tail iovec leaves an orphaned DMA mapping
    
    The ctxt's count field is overloaded to mean the number of pages in
    the ctxt->page array and the number of SGEs in the ctxt->sge array.
    Typically these two numbers are the same.
    
    However, when an inline RPC reply is constructed from an xdr_buf
    with a tail iovec, the head and tail often occupy the same page,
    but each are DMA mapped independently. In that case, ->count equals
    the number of pages, but it does not equal the number of SGEs.
    There's one more SGE, for the tail iovec. Hence there is one more
    DMA mapping than there are pages in the ctxt->page array.
    
    This isn't a real problem until the server's iommu is enabled. Then
    each RPC reply that has content in that iovec orphans a DMA mapping
    that consists of real resources.
    
    krb5i and krb5p always populate that tail iovec. After a couple
    million sent krb5i/p RPC replies, the NFS server starts behaving
    erratically. Reboot is needed to clear the problem.
    
    Fixes: 9d11b51ce7c1 ("svcrdma: Fix send_reply() scatter/gather set-up")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index a2a7519b0f23..cd0c5581498c 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -129,7 +129,7 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 		ret = -EIO;
 		goto out_unmap;
 	}
-	atomic_inc(&rdma->sc_dma_used);
+	svc_rdma_count_mappings(rdma, ctxt);
 
 	memset(&send_wr, 0, sizeof(send_wr));
 	ctxt->cqe.done = svc_rdma_wc_send;

commit 3435c74aed2d7b743ccbf34616c523ebee7be943
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:29 2016 -0400

    SUNRPC: Generalize the RPC buffer release API
    
    xprtrdma needs to allocate the Call and Reply buffers separately.
    TBH, the reliance on using a single buffer for the pair of XDR
    buffers is transport implementation-specific.
    
    Instead of passing just the rq_buffer into the buf_free method, pass
    the task structure and let buf_free take care of freeing both
    XDR buffers at once.
    
    There's a micro-optimization here. In the common case, both
    xprt_release and the transport's buf_free method were checking if
    rq_buffer was NULL. Now the check is done only once per RPC.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 124688ba67e5..fa893507d9eb 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -186,7 +186,7 @@ xprt_rdma_bc_allocate(struct rpc_task *task)
 }
 
 static void
-xprt_rdma_bc_free(void *buffer)
+xprt_rdma_bc_free(struct rpc_task *task)
 {
 	/* No-op: ctxt and page have already been freed. */
 }

commit 5fe6eaa1f9a00b9a5927e3b791ecad2f3eaab130
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:20 2016 -0400

    SUNRPC: Generalize the RPC buffer allocation API
    
    xprtrdma needs to allocate the Call and Reply buffers separately.
    TBH, the reliance on using a single buffer for the pair of XDR
    buffers is transport implementation-specific.
    
    Transports that want to allocate separate Call and Reply buffers
    will ignore the "size" argument anyway.  Don't bother passing it.
    
    The buf_alloc method can't return two pointers. Instead, make the
    method's return value an error code, and set the rq_buffer pointer
    in the method itself.
    
    This gives call_allocate an opportunity to terminate an RPC instead
    of looping forever when a permanent problem occurs. If a request is
    just bogus, or the transport is in a state where it can't allocate
    resources for any request, there needs to be a way to kill the RPC
    right there and not loop.
    
    This immediately fixes a rare problem in the backchannel send path,
    which loops if the server happens to send a CB request whose
    call+reply size is larger than a page (which it shouldn't do yet).
    
    One more issue: looks like xprt_inject_disconnect was incorrectly
    placed in the failure path in call_allocate. It needs to be in the
    success path, as it is for other call-sites.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index a2a7519b0f23..124688ba67e5 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -159,29 +159,30 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 /* Server-side transport endpoint wants a whole page for its send
  * buffer. The client RPC code constructs the RPC header in this
  * buffer before it invokes ->send_request.
- *
- * Returns NULL if there was a temporary allocation failure.
  */
-static void *
-xprt_rdma_bc_allocate(struct rpc_task *task, size_t size)
+static int
+xprt_rdma_bc_allocate(struct rpc_task *task)
 {
 	struct rpc_rqst *rqst = task->tk_rqstp;
 	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
+	size_t size = rqst->rq_callsize;
 	struct svcxprt_rdma *rdma;
 	struct page *page;
 
 	rdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);
 
-	/* Prevent an infinite loop: try to make this case work */
-	if (size > PAGE_SIZE)
+	if (size > PAGE_SIZE) {
 		WARN_ONCE(1, "svcrdma: large bc buffer request (size %zu)\n",
 			  size);
+		return -EINVAL;
+	}
 
 	page = alloc_page(RPCRDMA_DEF_GFP);
 	if (!page)
-		return NULL;
+		return -ENOMEM;
 
-	return page_address(page);
+	rqst->rq_buffer = page_address(page);
+	return 0;
 }
 
 static void

commit be99bb11400ce02552c35a6d3bf054de393ce30e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:07:22 2016 -0500

    svcrdma: Use new CQ API for RPC-over-RDMA server send CQs
    
    Calling ib_poll_cq() to sort through WCs during a completion is a
    common pattern amongst RDMA consumers. Since commit 14d3a3b2498e
    ("IB: add a proper completion queue abstraction"), WC sorting can
    be handled by the IB core.
    
    By converting to this new API, svcrdma is made a better neighbor to
    other RDMA consumers, as it allows the core to schedule the delivery
    of completions more fairly amongst all active consumers.
    
    This new API also aims each completion at a function that is
    specific to the WR's opcode. Thus the ctxt->wr_op field and the
    switch in process_context is replaced by a set of methods that
    handle each completion type.
    
    Because each ib_cqe carries a pointer to a completion method, the
    core can now post operations on a consumer's QP, and handle the
    completions itself.
    
    The server's rdma_stat_sq_poll and rdma_stat_sq_prod metrics are no
    longer updated.
    
    As a clean up, the cq_event_handler, the dto_tasklet, and all
    associated locking is removed, as they are no longer referenced or
    used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 254be8661981..a2a7519b0f23 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -119,7 +119,6 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	ctxt->pages[0] = virt_to_page(rqst->rq_buffer);
 	ctxt->count = 1;
 
-	ctxt->wr_op = IB_WR_SEND;
 	ctxt->direction = DMA_TO_DEVICE;
 	ctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;
 	ctxt->sge[0].length = sndbuf->len;
@@ -133,7 +132,8 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	atomic_inc(&rdma->sc_dma_used);
 
 	memset(&send_wr, 0, sizeof(send_wr));
-	send_wr.wr_id = (unsigned long)ctxt;
+	ctxt->cqe.done = svc_rdma_wc_send;
+	send_wr.wr_cqe = &ctxt->cqe;
 	send_wr.sg_list = ctxt->sge;
 	send_wr.num_sge = 1;
 	send_wr.opcode = IB_WR_SEND;

commit bf36387ad394ad4fc93ad85fdd4a95dfa583556a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:06:20 2016 -0500

    svcrdma: svc_rdma_post_recv() should close connection on error
    
    Clean up: Most svc_rdma_post_recv() call sites close the transport
    connection when a receive cannot be posted. Wrap that in a common
    helper.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Tested-by: Devesh Sharma <devesh.sharma@broadcom.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index de3919624fac..254be8661981 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -111,16 +111,9 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	if (ret)
 		goto out_err;
 
-	/* Post a recv buffer to handle the reply for this request. */
-	ret = svc_rdma_post_recv(rdma, GFP_NOIO);
-	if (ret) {
-		pr_err("svcrdma: Failed to post bc receive buffer, err=%d.\n",
-		       ret);
-		pr_err("svcrdma: closing transport %p.\n", rdma);
-		set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
-		ret = -ENOTCONN;
+	ret = svc_rdma_repost_recv(rdma, GFP_NOIO);
+	if (ret)
 		goto out_err;
-	}
 
 	ctxt = svc_rdma_get_context(rdma);
 	ctxt->pages[0] = virt_to_page(rqst->rq_buffer);

commit f6763c29ab86c3ee27760a06e07bbeab47635b61
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Mar 1 13:05:54 2016 -0500

    svcrdma: Do not send Write chunk XDR pad with inline content
    
    The NFS server's XDR encoders adds an XDR pad for content in the
    xdr_buf page list at the beginning of the xdr_buf's tail buffer.
    
    On RDMA transports, Write chunks are sent separately and without an
    XDR pad.
    
    If a Write chunk is being sent, strip off the pad in the tail buffer
    so that inline content following the Write chunk remains XDR-aligned
    when it is sent to the client.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=294
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index 65a7c232a345..de3919624fac 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -107,7 +107,7 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 	int ret;
 
 	vec = svc_rdma_get_req_map(rdma);
-	ret = svc_rdma_map_xdr(rdma, sndbuf, vec);
+	ret = svc_rdma_map_xdr(rdma, sndbuf, vec, false);
 	if (ret)
 		goto out_err;
 

commit 5fe1043da84887369d32459514f2c7d98ff37936
Author: Christoph Hellwig <hch@infradead.org>
Date:   Thu Jan 7 23:53:41 2016 -0800

    svc_rdma: use local_dma_lkey
    
    We now alwasy have a per-PD local_dma_lkey available.  Make use of that
    fact in svc_rdma and stop registering our own MR.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Reviewed-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Acked-by: J. Bruce Fields <bfields@redhat.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
index deff06a82914..65a7c232a345 100644
--- a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -128,7 +128,7 @@ static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
 
 	ctxt->wr_op = IB_WR_SEND;
 	ctxt->direction = DMA_TO_DEVICE;
-	ctxt->sge[0].lkey = rdma->sc_dma_lkey;
+	ctxt->sge[0].lkey = rdma->sc_pd->local_dma_lkey;
 	ctxt->sge[0].length = sndbuf->len;
 	ctxt->sge[0].addr =
 	    ib_dma_map_page(rdma->sc_cm_id->device, ctxt->pages[0], 0,

commit 5d252f90a800cee5bc57c76d636ae60464f7a887
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:50:10 2016 -0500

    svcrdma: Add class for RDMA backwards direction transport
    
    To support the server-side of an NFSv4.1 backchannel on RDMA
    connections, add a transport class that enables backward
    direction messages on an existing forward channel connection.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/svc_rdma_backchannel.c b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
new file mode 100644
index 000000000000..deff06a82914
--- /dev/null
+++ b/net/sunrpc/xprtrdma/svc_rdma_backchannel.c
@@ -0,0 +1,371 @@
+/*
+ * Copyright (c) 2015 Oracle.  All rights reserved.
+ *
+ * Support for backward direction RPCs on RPC/RDMA (server-side).
+ */
+
+#include <linux/sunrpc/svc_rdma.h>
+#include "xprt_rdma.h"
+
+#define RPCDBG_FACILITY	RPCDBG_SVCXPRT
+
+#undef SVCRDMA_BACKCHANNEL_DEBUG
+
+int svc_rdma_handle_bc_reply(struct rpc_xprt *xprt, struct rpcrdma_msg *rmsgp,
+			     struct xdr_buf *rcvbuf)
+{
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct kvec *dst, *src = &rcvbuf->head[0];
+	struct rpc_rqst *req;
+	unsigned long cwnd;
+	u32 credits;
+	size_t len;
+	__be32 xid;
+	__be32 *p;
+	int ret;
+
+	p = (__be32 *)src->iov_base;
+	len = src->iov_len;
+	xid = rmsgp->rm_xid;
+
+#ifdef SVCRDMA_BACKCHANNEL_DEBUG
+	pr_info("%s: xid=%08x, length=%zu\n",
+		__func__, be32_to_cpu(xid), len);
+	pr_info("%s: RPC/RDMA: %*ph\n",
+		__func__, (int)RPCRDMA_HDRLEN_MIN, rmsgp);
+	pr_info("%s:      RPC: %*ph\n",
+		__func__, (int)len, p);
+#endif
+
+	ret = -EAGAIN;
+	if (src->iov_len < 24)
+		goto out_shortreply;
+
+	spin_lock_bh(&xprt->transport_lock);
+	req = xprt_lookup_rqst(xprt, xid);
+	if (!req)
+		goto out_notfound;
+
+	dst = &req->rq_private_buf.head[0];
+	memcpy(&req->rq_private_buf, &req->rq_rcv_buf, sizeof(struct xdr_buf));
+	if (dst->iov_len < len)
+		goto out_unlock;
+	memcpy(dst->iov_base, p, len);
+
+	credits = be32_to_cpu(rmsgp->rm_credit);
+	if (credits == 0)
+		credits = 1;	/* don't deadlock */
+	else if (credits > r_xprt->rx_buf.rb_bc_max_requests)
+		credits = r_xprt->rx_buf.rb_bc_max_requests;
+
+	cwnd = xprt->cwnd;
+	xprt->cwnd = credits << RPC_CWNDSHIFT;
+	if (xprt->cwnd > cwnd)
+		xprt_release_rqst_cong(req->rq_task);
+
+	ret = 0;
+	xprt_complete_rqst(req->rq_task, rcvbuf->len);
+	rcvbuf->len = 0;
+
+out_unlock:
+	spin_unlock_bh(&xprt->transport_lock);
+out:
+	return ret;
+
+out_shortreply:
+	dprintk("svcrdma: short bc reply: xprt=%p, len=%zu\n",
+		xprt, src->iov_len);
+	goto out;
+
+out_notfound:
+	dprintk("svcrdma: unrecognized bc reply: xprt=%p, xid=%08x\n",
+		xprt, be32_to_cpu(xid));
+
+	goto out_unlock;
+}
+
+/* Send a backwards direction RPC call.
+ *
+ * Caller holds the connection's mutex and has already marshaled
+ * the RPC/RDMA request.
+ *
+ * This is similar to svc_rdma_reply, but takes an rpc_rqst
+ * instead, does not support chunks, and avoids blocking memory
+ * allocation.
+ *
+ * XXX: There is still an opportunity to block in svc_rdma_send()
+ * if there are no SQ entries to post the Send. This may occur if
+ * the adapter has a small maximum SQ depth.
+ */
+static int svc_rdma_bc_sendto(struct svcxprt_rdma *rdma,
+			      struct rpc_rqst *rqst)
+{
+	struct xdr_buf *sndbuf = &rqst->rq_snd_buf;
+	struct svc_rdma_op_ctxt *ctxt;
+	struct svc_rdma_req_map *vec;
+	struct ib_send_wr send_wr;
+	int ret;
+
+	vec = svc_rdma_get_req_map(rdma);
+	ret = svc_rdma_map_xdr(rdma, sndbuf, vec);
+	if (ret)
+		goto out_err;
+
+	/* Post a recv buffer to handle the reply for this request. */
+	ret = svc_rdma_post_recv(rdma, GFP_NOIO);
+	if (ret) {
+		pr_err("svcrdma: Failed to post bc receive buffer, err=%d.\n",
+		       ret);
+		pr_err("svcrdma: closing transport %p.\n", rdma);
+		set_bit(XPT_CLOSE, &rdma->sc_xprt.xpt_flags);
+		ret = -ENOTCONN;
+		goto out_err;
+	}
+
+	ctxt = svc_rdma_get_context(rdma);
+	ctxt->pages[0] = virt_to_page(rqst->rq_buffer);
+	ctxt->count = 1;
+
+	ctxt->wr_op = IB_WR_SEND;
+	ctxt->direction = DMA_TO_DEVICE;
+	ctxt->sge[0].lkey = rdma->sc_dma_lkey;
+	ctxt->sge[0].length = sndbuf->len;
+	ctxt->sge[0].addr =
+	    ib_dma_map_page(rdma->sc_cm_id->device, ctxt->pages[0], 0,
+			    sndbuf->len, DMA_TO_DEVICE);
+	if (ib_dma_mapping_error(rdma->sc_cm_id->device, ctxt->sge[0].addr)) {
+		ret = -EIO;
+		goto out_unmap;
+	}
+	atomic_inc(&rdma->sc_dma_used);
+
+	memset(&send_wr, 0, sizeof(send_wr));
+	send_wr.wr_id = (unsigned long)ctxt;
+	send_wr.sg_list = ctxt->sge;
+	send_wr.num_sge = 1;
+	send_wr.opcode = IB_WR_SEND;
+	send_wr.send_flags = IB_SEND_SIGNALED;
+
+	ret = svc_rdma_send(rdma, &send_wr);
+	if (ret) {
+		ret = -EIO;
+		goto out_unmap;
+	}
+
+out_err:
+	svc_rdma_put_req_map(rdma, vec);
+	dprintk("svcrdma: %s returns %d\n", __func__, ret);
+	return ret;
+
+out_unmap:
+	svc_rdma_unmap_dma(ctxt);
+	svc_rdma_put_context(ctxt, 1);
+	goto out_err;
+}
+
+/* Server-side transport endpoint wants a whole page for its send
+ * buffer. The client RPC code constructs the RPC header in this
+ * buffer before it invokes ->send_request.
+ *
+ * Returns NULL if there was a temporary allocation failure.
+ */
+static void *
+xprt_rdma_bc_allocate(struct rpc_task *task, size_t size)
+{
+	struct rpc_rqst *rqst = task->tk_rqstp;
+	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
+	struct svcxprt_rdma *rdma;
+	struct page *page;
+
+	rdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);
+
+	/* Prevent an infinite loop: try to make this case work */
+	if (size > PAGE_SIZE)
+		WARN_ONCE(1, "svcrdma: large bc buffer request (size %zu)\n",
+			  size);
+
+	page = alloc_page(RPCRDMA_DEF_GFP);
+	if (!page)
+		return NULL;
+
+	return page_address(page);
+}
+
+static void
+xprt_rdma_bc_free(void *buffer)
+{
+	/* No-op: ctxt and page have already been freed. */
+}
+
+static int
+rpcrdma_bc_send_request(struct svcxprt_rdma *rdma, struct rpc_rqst *rqst)
+{
+	struct rpc_xprt *xprt = rqst->rq_xprt;
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct rpcrdma_msg *headerp = (struct rpcrdma_msg *)rqst->rq_buffer;
+	int rc;
+
+	/* Space in the send buffer for an RPC/RDMA header is reserved
+	 * via xprt->tsh_size.
+	 */
+	headerp->rm_xid = rqst->rq_xid;
+	headerp->rm_vers = rpcrdma_version;
+	headerp->rm_credit = cpu_to_be32(r_xprt->rx_buf.rb_bc_max_requests);
+	headerp->rm_type = rdma_msg;
+	headerp->rm_body.rm_chunks[0] = xdr_zero;
+	headerp->rm_body.rm_chunks[1] = xdr_zero;
+	headerp->rm_body.rm_chunks[2] = xdr_zero;
+
+#ifdef SVCRDMA_BACKCHANNEL_DEBUG
+	pr_info("%s: %*ph\n", __func__, 64, rqst->rq_buffer);
+#endif
+
+	rc = svc_rdma_bc_sendto(rdma, rqst);
+	if (rc)
+		goto drop_connection;
+	return rc;
+
+drop_connection:
+	dprintk("svcrdma: failed to send bc call\n");
+	xprt_disconnect_done(xprt);
+	return -ENOTCONN;
+}
+
+/* Send an RPC call on the passive end of a transport
+ * connection.
+ */
+static int
+xprt_rdma_bc_send_request(struct rpc_task *task)
+{
+	struct rpc_rqst *rqst = task->tk_rqstp;
+	struct svc_xprt *sxprt = rqst->rq_xprt->bc_xprt;
+	struct svcxprt_rdma *rdma;
+	int ret;
+
+	dprintk("svcrdma: sending bc call with xid: %08x\n",
+		be32_to_cpu(rqst->rq_xid));
+
+	if (!mutex_trylock(&sxprt->xpt_mutex)) {
+		rpc_sleep_on(&sxprt->xpt_bc_pending, task, NULL);
+		if (!mutex_trylock(&sxprt->xpt_mutex))
+			return -EAGAIN;
+		rpc_wake_up_queued_task(&sxprt->xpt_bc_pending, task);
+	}
+
+	ret = -ENOTCONN;
+	rdma = container_of(sxprt, struct svcxprt_rdma, sc_xprt);
+	if (!test_bit(XPT_DEAD, &sxprt->xpt_flags))
+		ret = rpcrdma_bc_send_request(rdma, rqst);
+
+	mutex_unlock(&sxprt->xpt_mutex);
+
+	if (ret < 0)
+		return ret;
+	return 0;
+}
+
+static void
+xprt_rdma_bc_close(struct rpc_xprt *xprt)
+{
+	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
+}
+
+static void
+xprt_rdma_bc_put(struct rpc_xprt *xprt)
+{
+	dprintk("svcrdma: %s: xprt %p\n", __func__, xprt);
+
+	xprt_free(xprt);
+	module_put(THIS_MODULE);
+}
+
+static struct rpc_xprt_ops xprt_rdma_bc_procs = {
+	.reserve_xprt		= xprt_reserve_xprt_cong,
+	.release_xprt		= xprt_release_xprt_cong,
+	.alloc_slot		= xprt_alloc_slot,
+	.release_request	= xprt_release_rqst_cong,
+	.buf_alloc		= xprt_rdma_bc_allocate,
+	.buf_free		= xprt_rdma_bc_free,
+	.send_request		= xprt_rdma_bc_send_request,
+	.set_retrans_timeout	= xprt_set_retrans_timeout_def,
+	.close			= xprt_rdma_bc_close,
+	.destroy		= xprt_rdma_bc_put,
+	.print_stats		= xprt_rdma_print_stats
+};
+
+static const struct rpc_timeout xprt_rdma_bc_timeout = {
+	.to_initval = 60 * HZ,
+	.to_maxval = 60 * HZ,
+};
+
+/* It shouldn't matter if the number of backchannel session slots
+ * doesn't match the number of RPC/RDMA credits. That just means
+ * one or the other will have extra slots that aren't used.
+ */
+static struct rpc_xprt *
+xprt_setup_rdma_bc(struct xprt_create *args)
+{
+	struct rpc_xprt *xprt;
+	struct rpcrdma_xprt *new_xprt;
+
+	if (args->addrlen > sizeof(xprt->addr)) {
+		dprintk("RPC:       %s: address too large\n", __func__);
+		return ERR_PTR(-EBADF);
+	}
+
+	xprt = xprt_alloc(args->net, sizeof(*new_xprt),
+			  RPCRDMA_MAX_BC_REQUESTS,
+			  RPCRDMA_MAX_BC_REQUESTS);
+	if (!xprt) {
+		dprintk("RPC:       %s: couldn't allocate rpc_xprt\n",
+			__func__);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	xprt->timeout = &xprt_rdma_bc_timeout;
+	xprt_set_bound(xprt);
+	xprt_set_connected(xprt);
+	xprt->bind_timeout = RPCRDMA_BIND_TO;
+	xprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;
+	xprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;
+
+	xprt->prot = XPRT_TRANSPORT_BC_RDMA;
+	xprt->tsh_size = RPCRDMA_HDRLEN_MIN / sizeof(__be32);
+	xprt->ops = &xprt_rdma_bc_procs;
+
+	memcpy(&xprt->addr, args->dstaddr, args->addrlen);
+	xprt->addrlen = args->addrlen;
+	xprt_rdma_format_addresses(xprt, (struct sockaddr *)&xprt->addr);
+	xprt->resvport = 0;
+
+	xprt->max_payload = xprt_rdma_max_inline_read;
+
+	new_xprt = rpcx_to_rdmax(xprt);
+	new_xprt->rx_buf.rb_bc_max_requests = xprt->max_reqs;
+
+	xprt_get(xprt);
+	args->bc_xprt->xpt_bc_xprt = xprt;
+	xprt->bc_xprt = args->bc_xprt;
+
+	if (!try_module_get(THIS_MODULE))
+		goto out_fail;
+
+	/* Final put for backchannel xprt is in __svc_rdma_free */
+	xprt_get(xprt);
+	return xprt;
+
+out_fail:
+	xprt_rdma_free_addresses(xprt);
+	args->bc_xprt->xpt_bc_xprt = NULL;
+	xprt_put(xprt);
+	xprt_free(xprt);
+	return ERR_PTR(-EINVAL);
+}
+
+struct xprt_class xprt_rdma_bc = {
+	.list			= LIST_HEAD_INIT(xprt_rdma_bc.list),
+	.name			= "rdma backchannel",
+	.owner			= THIS_MODULE,
+	.ident			= XPRT_TRANSPORT_BC_RDMA,
+	.setup			= xprt_setup_rdma_bc,
+};
