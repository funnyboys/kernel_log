commit 4cf44be6f1e86da302085bf3e1dc2c86f3cdaaaa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Jun 27 12:35:09 2020 -0400

    xprtrdma: Fix recursion into rpcrdma_xprt_disconnect()
    
    Both Dan and I have observed two processes invoking
    rpcrdma_xprt_disconnect() concurrently. In my case:
    
    1. The connect worker invokes rpcrdma_xprt_disconnect(), which
       drains the QP and waits for the final completion
    2. This causes the newly posted Receive to flush and invoke
       xprt_force_disconnect()
    3. xprt_force_disconnect() sets CLOSE_WAIT and wakes up the RPC task
       that is holding the transport lock
    4. The RPC task invokes xprt_connect(), which calls ->ops->close
    5. xprt_rdma_close() invokes rpcrdma_xprt_disconnect(), which tries
       to destroy the QP.
    
    Deadlock.
    
    To prevent xprt_force_disconnect() from waking anything, handle the
    clean up after a failed connection attempt in the xprt's sndtask.
    
    The retry loop is removed from rpcrdma_xprt_connect() to ensure
    that the newly allocated ep and id are properly released before
    a REJECTED connection attempt can be retried.
    
    Reported-by: Dan Aloni <dan@kernelim.com>
    Fixes: e28ce90083f0 ("xprtrdma: kmalloc rpcrdma_ep separate from rpcrdma_xprt")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 14165b673b20..053c8ab1265a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -249,6 +249,11 @@ xprt_rdma_connect_worker(struct work_struct *work)
 					   xprt->stat.connect_start;
 		xprt_set_connected(xprt);
 		rc = -EAGAIN;
+	} else {
+		/* Force a call to xprt_rdma_close to clean up */
+		spin_lock(&xprt->transport_lock);
+		set_bit(XPRT_CLOSE_WAIT, &xprt->state);
+		spin_unlock(&xprt->transport_lock);
 	}
 	xprt_wake_pending_tasks(xprt, rc);
 }

commit 2d97f4637666704953dfbb10322c2b73bb53d5e7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Jun 15 09:20:57 2020 -0400

    xprtrdma: Use re_connect_status safely in rpcrdma_xprt_connect()
    
    Clean up: Sometimes creating a fresh rpcrdma_ep can fail. That's why
    xprt_rdma_connect() always checks if the r_xprt->rx_ep pointer is
    valid before dereferencing it. Instead, xprt_rdma_connect() can
    simply check rpcrdma_xprt_connect()'s return value.
    
    Also, there's no need to set re_connect_status to zero just after
    the rpcrdma_ep is created, since it is allocated with kzalloc.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 0c4af7f5e241..14165b673b20 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -242,7 +242,7 @@ xprt_rdma_connect_worker(struct work_struct *work)
 
 	rc = rpcrdma_xprt_connect(r_xprt);
 	xprt_clear_connecting(xprt);
-	if (r_xprt->rx_ep && r_xprt->rx_ep->re_connect_status > 0) {
+	if (!rc) {
 		xprt->connect_cookie++;
 		xprt->stat.connect_count++;
 		xprt->stat.connect_time += (long)jiffies -

commit 5bffb00621b121c9f4726f1e755ad4246cf4b800
Author: Zou Wei <zou_wei@huawei.com>
Date:   Thu Apr 23 15:10:02 2020 +0800

    xprtrdma: Make xprt_rdma_slot_table_entries static
    
    Fix the following sparse warning:
    
    net/sunrpc/xprtrdma/transport.c:71:14: warning: symbol 'xprt_rdma_slot_table_entries'
    was not declared. Should it be static?
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: Zou Wei <zou_wei@huawei.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 048c2fd85728..0c4af7f5e241 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -68,7 +68,7 @@
  * tunables
  */
 
-unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
+static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
 unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 unsigned int xprt_rdma_memreg_strategy		= RPCRDMA_FRWR;

commit 911813d7a15221362d1655d246f3502c0f898c14
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 12 17:13:34 2020 -0400

    SUNRPC: Trace transport lifetime events
    
    Refactor: Hoist create/destroy/disconnect tracepoints out of
    xprtrdma and into the generic RPC client. Some benefits include:
    
    - Enable tracing of xprt lifetime events for the socket transport
      types
    
    - Expose the different types of disconnect to help run down
      issues with lingering connections
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 659da37020a4..048c2fd85728 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -281,8 +281,6 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
-	trace_xprtrdma_op_destroy(r_xprt);
-
 	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
 	rpcrdma_xprt_disconnect(r_xprt);
@@ -365,10 +363,6 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	xprt->max_payload = RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT;
 
-	dprintk("RPC:       %s: %s:%s\n", __func__,
-		xprt->address_strings[RPC_DISPLAY_ADDR],
-		xprt->address_strings[RPC_DISPLAY_PORT]);
-	trace_xprtrdma_create(new_xprt);
 	return xprt;
 }
 
@@ -385,8 +379,6 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
-	trace_xprtrdma_op_close(r_xprt);
-
 	rpcrdma_xprt_disconnect(r_xprt);
 
 	xprt->reestablish_timeout = 0;

commit e28ce90083f032ca0e8ea03478f5b6a38f5930f7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:01:05 2020 -0500

    xprtrdma: kmalloc rpcrdma_ep separate from rpcrdma_xprt
    
    Change the rpcrdma_xprt_disconnect() function so that it no longer
    waits for the DISCONNECTED event.  This prevents blocking if the
    remote is unresponsive.
    
    In rpcrdma_xprt_disconnect(), the transport's rpcrdma_ep is
    detached. Upon return from rpcrdma_xprt_disconnect(), the transport
    (r_xprt) is ready immediately for a new connection.
    
    The RDMA_CM_DEVICE_REMOVAL and RDMA_CM_DISCONNECTED events are now
    handled almost identically.
    
    However, because the lifetimes of rpcrdma_xprt structures and
    rpcrdma_ep structures are now independent, creating an rpcrdma_ep
    needs to take a module ref count. The ep now owns most of the
    hardware resources for a transport.
    
    Also, a kref is needed to ensure that rpcrdma_ep sticks around
    long enough for the cm_event_handler to finish.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 4352fd6e9817..659da37020a4 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -238,12 +238,12 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	struct rpcrdma_xprt *r_xprt = container_of(work, struct rpcrdma_xprt,
 						   rx_connect_worker.work);
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
-	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
 	int rc;
 
 	rc = rpcrdma_xprt_connect(r_xprt);
 	xprt_clear_connecting(xprt);
-	if (ep->re_connect_status > 0) {
+	if (r_xprt->rx_ep && r_xprt->rx_ep->re_connect_status > 0) {
+		xprt->connect_cookie++;
 		xprt->stat.connect_count++;
 		xprt->stat.connect_time += (long)jiffies -
 					   xprt->stat.connect_start;
@@ -266,7 +266,7 @@ xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
 	trace_xprtrdma_op_inject_dsc(r_xprt);
-	rdma_disconnect(r_xprt->rx_ep.re_id);
+	rdma_disconnect(r_xprt->rx_ep->re_id);
 }
 
 /**
@@ -316,10 +316,15 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (args->addrlen > sizeof(xprt->addr))
 		return ERR_PTR(-EBADF);
 
+	if (!try_module_get(THIS_MODULE))
+		return ERR_PTR(-EIO);
+
 	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt), 0,
 			  xprt_rdma_slot_table_entries);
-	if (!xprt)
+	if (!xprt) {
+		module_put(THIS_MODULE);
 		return ERR_PTR(-ENOMEM);
+	}
 
 	xprt->timeout = &xprt_rdma_default_timeout;
 	xprt->connect_timeout = xprt->timeout->to_initval;
@@ -348,11 +353,12 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	new_xprt = rpcx_to_rdmax(xprt);
 	rc = rpcrdma_buffer_create(new_xprt);
-	if (rc)
-		goto out2;
-
-	if (!try_module_get(THIS_MODULE))
-		goto out4;
+	if (rc) {
+		xprt_rdma_free_addresses(xprt);
+		xprt_free(xprt);
+		module_put(THIS_MODULE);
+		return ERR_PTR(rc);
+	}
 
 	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
 			  xprt_rdma_connect_worker);
@@ -364,15 +370,6 @@ xprt_setup_rdma(struct xprt_create *args)
 		xprt->address_strings[RPC_DISPLAY_PORT]);
 	trace_xprtrdma_create(new_xprt);
 	return xprt;
-
-out4:
-	rpcrdma_buffer_destroy(&new_xprt->rx_buf);
-	rc = -ENODEV;
-out2:
-	trace_xprtrdma_op_destroy(new_xprt);
-	xprt_rdma_free_addresses(xprt);
-	xprt_free(xprt);
-	return ERR_PTR(rc);
 }
 
 /**
@@ -491,11 +488,11 @@ static void
 xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
-	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
+	struct rpcrdma_ep *ep = r_xprt->rx_ep;
 	unsigned long delay;
 
 	delay = 0;
-	if (ep->re_connect_status != 0) {
+	if (ep && ep->re_connect_status != 0) {
 		delay = xprt_reconnect_delay(xprt);
 		xprt_reconnect_backoff(xprt, RPCRDMA_INIT_REEST_TO);
 	}

commit 93aa8e0a9de80e1df2be17158a3469285e572b39
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:54 2020 -0500

    xprtrdma: Merge struct rpcrdma_ia into struct rpcrdma_ep
    
    I eventually want to allocate rpcrdma_ep separately from struct
    rpcrdma_xprt so that on occasion there can be more than one ep per
    xprt.
    
    The new struct rpcrdma_ep will contain all the fields currently in
    rpcrdma_ia and in rpcrdma_ep. This is all the device and CM settings
    for the connection, in addition to per-connection settings
    negotiated with the remote.
    
    Take this opportunity to rename the existing ep fields from rep_* to
    re_* to disambiguate these from struct rpcrdma_rep.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d7b7dab0aeb6..4352fd6e9817 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -238,11 +238,12 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	struct rpcrdma_xprt *r_xprt = container_of(work, struct rpcrdma_xprt,
 						   rx_connect_worker.work);
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
+	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
 	int rc;
 
 	rc = rpcrdma_xprt_connect(r_xprt);
 	xprt_clear_connecting(xprt);
-	if (r_xprt->rx_ep.rep_connected > 0) {
+	if (ep->re_connect_status > 0) {
 		xprt->stat.connect_count++;
 		xprt->stat.connect_time += (long)jiffies -
 					   xprt->stat.connect_start;
@@ -265,7 +266,7 @@ xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
 	trace_xprtrdma_op_inject_dsc(r_xprt);
-	rdma_disconnect(r_xprt->rx_ia.ri_id);
+	rdma_disconnect(r_xprt->rx_ep.re_id);
 }
 
 /**
@@ -355,6 +356,7 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
 			  xprt_rdma_connect_worker);
+
 	xprt->max_payload = RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT;
 
 	dprintk("RPC:       %s: %s:%s\n", __func__,
@@ -489,10 +491,11 @@ static void
 xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
 	unsigned long delay;
 
 	delay = 0;
-	if (r_xprt->rx_ep.rep_connected != 0) {
+	if (ep->re_connect_status != 0) {
 		delay = xprt_reconnect_delay(xprt);
 		xprt_reconnect_backoff(xprt, RPCRDMA_INIT_REEST_TO);
 	}

commit 897b7be9bca0caa27cdf7520bdc7689abe989a53
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:44 2020 -0500

    xprtrdma: Remove rpcrdma_ia::ri_flags
    
    Clean up:
    The upper layer serializes calls to xprt_rdma_close, so there is no
    need for an atomic bit operation, saving 8 bytes in rpcrdma_ia.
    
    This enables merging rpcrdma_ia_remove directly into the disconnect
    logic.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 745dfd149637..d7b7dab0aeb6 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -385,26 +385,11 @@ xprt_setup_rdma(struct xprt_create *args)
 void xprt_rdma_close(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
-	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
-	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
-
-	might_sleep();
 
 	trace_xprtrdma_op_close(r_xprt);
 
-	/* Prevent marshaling and sending of new requests */
-	xprt_clear_connected(xprt);
-
-	if (test_and_clear_bit(RPCRDMA_IAF_REMOVING, &ia->ri_flags)) {
-		rpcrdma_ia_remove(ia);
-		goto out;
-	}
-
-	if (ep->rep_connected == -ENODEV)
-		return;
 	rpcrdma_xprt_disconnect(r_xprt);
 
-out:
 	xprt->reestablish_timeout = 0;
 	++xprt->connect_cookie;
 	xprt_disconnect_done(xprt);

commit 81fe0c57f4e136375f3bcda74af413f82a34a1bb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:38 2020 -0500

    xprtrdma: Invoke rpcrdma_ia_open in the connect worker
    
    Move rdma_cm_id creation into rpcrdma_ep_create() so that it is now
    responsible for allocating all per-connection hardware resources.
    
    With this clean-up, all three arms of the switch statement in
    rpcrdma_ep_connect are exactly the same now, thus the switch can be
    removed.
    
    Because device removal behaves a little differently than
    disconnection, there is a little more work to be done before
    rpcrdma_ep_destroy() can release the connection's rdma_cm_id. So
    it is not quite symmetrical with rpcrdma_ep_create() yet.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6349e6c98b57..745dfd149637 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -286,7 +286,6 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	rpcrdma_xprt_disconnect(r_xprt);
 	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
-	rpcrdma_ia_close(&r_xprt->rx_ia);
 
 	xprt_rdma_free_addresses(xprt);
 	xprt_free(xprt);
@@ -347,10 +346,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	xprt_rdma_format_addresses(xprt, sap);
 
 	new_xprt = rpcx_to_rdmax(xprt);
-	rc = rpcrdma_ia_open(new_xprt);
-	if (rc)
-		goto out1;
-
 	rc = rpcrdma_buffer_create(new_xprt);
 	if (rc)
 		goto out2;
@@ -372,8 +367,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	rpcrdma_buffer_destroy(&new_xprt->rx_buf);
 	rc = -ENODEV;
 out2:
-	rpcrdma_ia_close(&new_xprt->rx_ia);
-out1:
 	trace_xprtrdma_op_destroy(new_xprt);
 	xprt_rdma_free_addresses(xprt);
 	xprt_free(xprt);

commit 9144a803df6ca4185238ca343dbb65d8137c036e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:28 2020 -0500

    xprtrdma: Refactor rpcrdma_ep_connect() and rpcrdma_ep_disconnect()
    
    Clean up: Simplify the synopses of functions in the connect and
    disconnect paths in preparation for combining the rpcrdma_ia and
    struct rpcrdma_ep structures.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8934c24a5701..6349e6c98b57 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -240,7 +240,7 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
 	int rc;
 
-	rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	rc = rpcrdma_xprt_connect(r_xprt);
 	xprt_clear_connecting(xprt);
 	if (r_xprt->rx_ep.rep_connected > 0) {
 		xprt->stat.connect_count++;
@@ -284,7 +284,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
-	rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	rpcrdma_xprt_disconnect(r_xprt);
 	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
 	rpcrdma_ia_close(&r_xprt->rx_ia);
 
@@ -409,7 +409,7 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 
 	if (ep->rep_connected == -ENODEV)
 		return;
-	rpcrdma_ep_disconnect(ep, ia);
+	rpcrdma_xprt_disconnect(r_xprt);
 
 out:
 	xprt->reestablish_timeout = 0;

commit 97d0de8812a10a66510ff95f8fe6e8d3053fd2ca
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:23 2020 -0500

    xprtrdma: Clean up the post_send path
    
    Clean up: Simplify the synopses of functions in the post_send path
    by combining the struct rpcrdma_ia and struct rpcrdma_ep arguments.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d915524a8e68..8934c24a5701 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -688,7 +688,7 @@ xprt_rdma_send_request(struct rpc_rqst *rqst)
 		goto drop_connection;
 	rqst->rq_xtime = ktime_get();
 
-	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))
+	if (rpcrdma_post_sends(r_xprt, req))
 		goto drop_connection;
 
 	rqst->rq_xmit_bytes_sent += rqst->rq_snd_buf.len;

commit 85cd8e2b78eea7374927750ffec60bf047f8f90b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Feb 21 17:00:12 2020 -0500

    xprtrdma: Invoke rpcrdma_ep_create() in the connect worker
    
    Refactor rpcrdma_ep_create(), rpcrdma_ep_disconnect(), and
    rpcrdma_ep_destroy().
    
    rpcrdma_ep_create will be invoked at connect time instead of at
    transport set-up time. It will be responsible for allocating per-
    connection resources. In this patch it allocates the CQs and
    creates a QP. More to come.
    
    rpcrdma_ep_destroy() is the inverse functionality that is
    invoked at disconnect time. It will be responsible for releasing
    the CQs and QP.
    
    These changes should be safe to do because both connect and
    disconnect is guaranteed to be serialized by the transport send
    lock.
    
    This takes us another step closer to resolving the address and route
    only at connect time so that connection failover to another device
    will work correctly.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 3cfeba68ee9a..d915524a8e68 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -284,7 +284,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
-	rpcrdma_ep_destroy(r_xprt);
+	rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
 	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
 	rpcrdma_ia_close(&r_xprt->rx_ia);
 
@@ -351,13 +351,9 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (rc)
 		goto out1;
 
-	rc = rpcrdma_ep_create(new_xprt);
-	if (rc)
-		goto out2;
-
 	rc = rpcrdma_buffer_create(new_xprt);
 	if (rc)
-		goto out3;
+		goto out2;
 
 	if (!try_module_get(THIS_MODULE))
 		goto out4;
@@ -375,8 +371,6 @@ xprt_setup_rdma(struct xprt_create *args)
 out4:
 	rpcrdma_buffer_destroy(&new_xprt->rx_buf);
 	rc = -ENODEV;
-out3:
-	rpcrdma_ep_destroy(new_xprt);
 out2:
 	rpcrdma_ia_close(&new_xprt->rx_ia);
 out1:

commit 18d065a5d4f16eeefb690c298671c3f9131121fe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:56:43 2020 -0500

    xprtrdma: Eliminate per-transport "max pages"
    
    To support device hotplug and migrating a connection between devices
    of different capabilities, we have to guarantee that all in-kernel
    devices can support the same max NFS payload size (1 megabyte).
    
    This means that possibly one or two in-tree devices are no longer
    supported for NFS/RDMA because they cannot support 1MB rsize/wsize.
    The only one I confirmed was cxgb3, but it has already been removed
    from the kernel.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index f868a75057ad..3cfeba68ee9a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -359,19 +359,13 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (rc)
 		goto out3;
 
-	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
-			  xprt_rdma_connect_worker);
-
-	xprt->max_payload = frwr_maxpages(new_xprt);
-	if (xprt->max_payload == 0)
-		goto out4;
-	xprt->max_payload <<= PAGE_SHIFT;
-	dprintk("RPC:       %s: transport data payload maximum: %zu bytes\n",
-		__func__, xprt->max_payload);
-
 	if (!try_module_get(THIS_MODULE))
 		goto out4;
 
+	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
+			  xprt_rdma_connect_worker);
+	xprt->max_payload = RPCRDMA_MAX_DATA_SEGS << PAGE_SHIFT;
+
 	dprintk("RPC:       %s: %s:%s\n", __func__,
 		xprt->address_strings[RPC_DISPLAY_ADDR],
 		xprt->address_strings[RPC_DISPLAY_PORT]);

commit 7581d90109cad7d7322fd90cea023c706912f4bd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Jan 3 11:56:37 2020 -0500

    xprtrdma: Refactor initialization of ep->rep_max_requests
    
    Clean up: there is no need to keep two copies of the same value.
    Also, in subsequent patches, rpcrdma_ep_create() will be called in
    the connect worker rather than at set-up time.
    
    Minor fix: Initialize the transport's sendctx to the value based on
    the capabilities of the underlying device, not the maximum setting.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7395eb2cfdeb..f868a75057ad 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -316,7 +316,8 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (args->addrlen > sizeof(xprt->addr))
 		return ERR_PTR(-EBADF);
 
-	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt), 0, 0);
+	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt), 0,
+			  xprt_rdma_slot_table_entries);
 	if (!xprt)
 		return ERR_PTR(-ENOMEM);
 

commit a52c23b8b207d676d6cdf531af482a79fa622b9d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 23 10:02:14 2019 -0400

    xprtrdma: Replace dprintk in xprt_rdma_set_port
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ce263e6f779c..7395eb2cfdeb 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -441,12 +441,6 @@ xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
 	struct sockaddr *sap = (struct sockaddr *)&xprt->addr;
 	char buf[8];
 
-	dprintk("RPC:       %s: setting port for xprt %p (%s:%s) to %u\n",
-		__func__, xprt,
-		xprt->address_strings[RPC_DISPLAY_ADDR],
-		xprt->address_strings[RPC_DISPLAY_PORT],
-		port);
-
 	rpc_set_port(sap, port);
 
 	kfree(xprt->address_strings[RPC_DISPLAY_PORT]);
@@ -456,6 +450,9 @@ xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
 	kfree(xprt->address_strings[RPC_DISPLAY_HEX_PORT]);
 	snprintf(buf, sizeof(buf), "%4hx", port);
 	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = kstrdup(buf, GFP_KERNEL);
+
+	trace_xprtrdma_op_setport(container_of(xprt, struct rpcrdma_xprt,
+					       rx_xprt));
 }
 
 /**

commit 7b020f17bbd34c219419b634d9efb9e93a3af4c2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 23 10:01:58 2019 -0400

    xprtrdma: Report the computed connect delay
    
    For debugging, the op_connect trace point should report the computed
    connect delay. We can then ensure that the delay is computed at the
    proper times, for example.
    
    As a further clean-up, remove a few low-value "heartbeat" trace
    points in the connect path.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 361e59146807..ce263e6f779c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -527,13 +527,12 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	unsigned long delay;
 
-	trace_xprtrdma_op_connect(r_xprt);
-
 	delay = 0;
 	if (r_xprt->rx_ep.rep_connected != 0) {
 		delay = xprt_reconnect_delay(xprt);
 		xprt_reconnect_backoff(xprt, RPCRDMA_INIT_REEST_TO);
 	}
+	trace_xprtrdma_op_connect(r_xprt, delay);
 	queue_delayed_work(xprtiod_workqueue, &r_xprt->rx_connect_worker,
 			   delay);
 }

commit 6cb28687fd1db3f94b35c2a7b37bf468f945244a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 23 10:01:52 2019 -0400

    xprtrdma: Wake tasks after connect worker fails
    
    Pending tasks are currently never awoken when the connect worker
    fails. The reason is that XPRT_CONNECTED is always clear after a
    failure return of rpcrdma_ep_connect, thus the
    xprt_test_and_clear_connected() check in xprt_rdma_connect_worker()
    always fails.
    
    - xprt_rdma_close always clears XPRT_CONNECTED.
    
    - rpcrdma_ep_connect always clears XPRT_CONNECTED.
    
    After reviewing the TCP connect worker, it appears that there's no
    need for extra test_and_set paranoia in xprt_rdma_connect_worker.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 0711308277eb..361e59146807 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -243,16 +243,13 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
 	xprt_clear_connecting(xprt);
 	if (r_xprt->rx_ep.rep_connected > 0) {
-		if (!xprt_test_and_set_connected(xprt)) {
-			xprt->stat.connect_count++;
-			xprt->stat.connect_time += (long)jiffies -
-						   xprt->stat.connect_start;
-			xprt_wake_pending_tasks(xprt, -EAGAIN);
-		}
-	} else {
-		if (xprt_test_and_clear_connected(xprt))
-			xprt_wake_pending_tasks(xprt, rc);
+		xprt->stat.connect_count++;
+		xprt->stat.connect_time += (long)jiffies -
+					   xprt->stat.connect_start;
+		xprt_set_connected(xprt);
+		rc = -EAGAIN;
 	}
+	xprt_wake_pending_tasks(xprt, rc);
 }
 
 /**

commit eea63ca7ffa1f3a4a0b02b902ec51eab2d4e9df4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 9 13:07:32 2019 -0400

    xprtrdma: Initialize rb_credits in one place
    
    Clean up/code de-duplication.
    
    Nit: RPC_CWNDSHIFT is incorrect as the initial value for xprt->cwnd.
    This mistake does not appear to have operational consequences, since
    the cwnd value is replaced with a valid value upon the first Receive
    completion.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c67d465dc062..0711308277eb 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -425,15 +425,6 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 		return;
 	rpcrdma_ep_disconnect(ep, ia);
 
-	/* Prepare @xprt for the next connection by reinitializing
-	 * its credit grant to one (see RFC 8166, Section 3.3.3).
-	 */
-	spin_lock(&xprt->transport_lock);
-	r_xprt->rx_buf.rb_credits = 1;
-	xprt->cong = 0;
-	xprt->cwnd = RPC_CWNDSHIFT;
-	spin_unlock(&xprt->transport_lock);
-
 out:
 	xprt->reestablish_timeout = 0;
 	++xprt->connect_cookie;

commit a31b2f939219dd9bffdf01a45bd91f209f8cc369
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Oct 9 13:07:27 2019 -0400

    xprtrdma: Connection becomes unstable after a reconnect
    
    This is because xprt_request_get_cong() is allowing more than one
    RPC Call to be transmitted before the first Receive on the new
    connection. The first Receive fills the Receive Queue based on the
    server's credit grant. Before that Receive, there is only a single
    Receive WR posted because the client doesn't know the server's
    credit grant.
    
    Solution is to clear rq_cong on all outstanding rpc_rqsts when the
    the cwnd is reset. This is because an RPC/RDMA credit is good for
    one connection instance only.
    
    Fixes: 75891f502f5f ("SUNRPC: Support for congestion control ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 160558b4135e..c67d465dc062 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -428,8 +428,11 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 	/* Prepare @xprt for the next connection by reinitializing
 	 * its credit grant to one (see RFC 8166, Section 3.3.3).
 	 */
+	spin_lock(&xprt->transport_lock);
 	r_xprt->rx_buf.rb_credits = 1;
+	xprt->cong = 0;
 	xprt->cwnd = RPC_CWNDSHIFT;
+	spin_unlock(&xprt->transport_lock);
 
 out:
 	xprt->reestablish_timeout = 0;

commit f9e1afe0fa729337309fa44921da998d2e6e6198
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 26 13:12:51 2019 -0400

    xprtrdma: Clear xprt->reestablish_timeout on close
    
    Ensure that the re-establishment delay does not grow exponentially
    on each good reconnect. This probably should have been part of
    commit 675dd90ad093 ("xprtrdma: Modernize ops->connect").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 993b96ff6760..160558b4135e 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -423,8 +423,6 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 
 	if (ep->rep_connected == -ENODEV)
 		return;
-	if (ep->rep_connected > 0)
-		xprt->reestablish_timeout = 0;
 	rpcrdma_ep_disconnect(ep, ia);
 
 	/* Prepare @xprt for the next connection by reinitializing
@@ -434,6 +432,7 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 	xprt->cwnd = RPC_CWNDSHIFT;
 
 out:
+	xprt->reestablish_timeout = 0;
 	++xprt->connect_cookie;
 	xprt_disconnect_done(xprt);
 }

commit 2a7f77c7be1b5adb69712e440e97e0c6fa7ebecb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:49:30 2019 -0400

    xprtrdma: Clean up xprt_rdma_set_connect_timeout()
    
    Clean up: The function name should match the documenting comment.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index f4763e8a6761..993b96ff6760 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -494,9 +494,9 @@ xprt_rdma_timer(struct rpc_xprt *xprt, struct rpc_task *task)
  * @reconnect_timeout: reconnect timeout after server disconnects
  *
  */
-static void xprt_rdma_tcp_set_connect_timeout(struct rpc_xprt *xprt,
-					      unsigned long connect_timeout,
-					      unsigned long reconnect_timeout)
+static void xprt_rdma_set_connect_timeout(struct rpc_xprt *xprt,
+					  unsigned long connect_timeout,
+					  unsigned long reconnect_timeout)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
@@ -805,7 +805,7 @@ static const struct rpc_xprt_ops xprt_rdma_procs = {
 	.send_request		= xprt_rdma_send_request,
 	.close			= xprt_rdma_close,
 	.destroy		= xprt_rdma_destroy,
-	.set_connect_timeout	= xprt_rdma_tcp_set_connect_timeout,
+	.set_connect_timeout	= xprt_rdma_set_connect_timeout,
 	.print_stats		= xprt_rdma_print_stats,
 	.enable_swap		= xprt_rdma_enable_swap,
 	.disable_swap		= xprt_rdma_disable_swap,

commit 395790566eec37706dedeb94779045adc3a7581e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 19 18:43:17 2019 -0400

    xprtrdma: Toggle XPRT_CONGESTED in xprtrdma's slot methods
    
    Commit 48be539dd44a ("xprtrdma: Introduce ->alloc_slot call-out for
    xprtrdma") added a separate alloc_slot and free_slot to the RPC/RDMA
    transport. Later, commit 75891f502f5f ("SUNRPC: Support for
    congestion control when queuing is enabled") modified the generic
    alloc/free_slot methods, but neglected the methods in xprtrdma.
    
    Found via code review.
    
    Fixes: 75891f502f5f ("SUNRPC: Support for congestion control ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 2ec349ed4770..f4763e8a6761 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -571,6 +571,7 @@ xprt_rdma_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)
 	return;
 
 out_sleep:
+	set_bit(XPRT_CONGESTED, &xprt->state);
 	rpc_sleep_on(&xprt->backlog, task, NULL);
 	task->tk_status = -EAGAIN;
 }
@@ -589,7 +590,8 @@ xprt_rdma_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *rqst)
 
 	memset(rqst, 0, sizeof(*rqst));
 	rpcrdma_buffer_put(&r_xprt->rx_buf, rpcr_to_rdmar(rqst));
-	rpc_wake_up_next(&xprt->backlog);
+	if (unlikely(!rpc_wake_up_next(&xprt->backlog)))
+		clear_bit(XPRT_CONGESTED, &xprt->state);
 }
 
 static bool rpcrdma_check_regbuf(struct rpcrdma_xprt *r_xprt,

commit 249be8511b269495bc95cb8bdfdd5840b2ba73c0
Merge: 3bfe1fc46794 eec4844fae7c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jul 19 09:45:58 2019 -0700

    Merge branch 'akpm' (patches from Andrew)
    
    Merge yet more updates from Andrew Morton:
     "The rest of MM and a kernel-wide procfs cleanup.
    
      Summary of the more significant patches:
    
       - Patch series "mm/memory_hotplug: Factor out memory block
         devicehandling", v3. David Hildenbrand.
    
         Some spring-cleaning of the memory hotplug code, notably in
         drivers/base/memory.c
    
       - "mm: thp: fix false negative of shmem vma's THP eligibility". Yang
         Shi.
    
         Fix /proc/pid/smaps output for THP pages used in shmem.
    
       - "resource: fix locking in find_next_iomem_res()" + 1. Nadav Amit.
    
         Bugfix and speedup for kernel/resource.c
    
       - Patch series "mm: Further memory block device cleanups", David
         Hildenbrand.
    
         More spring-cleaning of the memory hotplug code.
    
       - Patch series "mm: Sub-section memory hotplug support". Dan
         Williams.
    
         Generalise the memory hotplug code so that pmem can use it more
         completely. Then remove the hacks from the libnvdimm code which
         were there to work around the memory-hotplug code's constraints.
    
       - "proc/sysctl: add shared variables for range check", Matteo Croce.
    
         We have about 250 instances of
    
              int zero;
              ...
                      .extra1 = &zero,
    
         in the tree. This is a tree-wide sweep to make all those private
         "zero"s and "one"s use global variables.
    
         Alas, it isn't practical to make those two global integers const"
    
    * emailed patches from Andrew Morton <akpm@linux-foundation.org>: (38 commits)
      proc/sysctl: add shared variables for range check
      mm: migrate: remove unused mode argument
      mm/sparsemem: cleanup 'section number' data types
      libnvdimm/pfn: stop padding pmem namespaces to section alignment
      libnvdimm/pfn: fix fsdax-mode namespace info-block zero-fields
      mm/devm_memremap_pages: enable sub-section remap
      mm: document ZONE_DEVICE memory-model implications
      mm/sparsemem: support sub-section hotplug
      mm/sparsemem: prepare for sub-section ranges
      mm: kill is_dev_zone() helper
      mm/hotplug: kill is_dev_zone() usage in __remove_pages()
      mm/sparsemem: convert kmalloc_section_memmap() to populate_section_memmap()
      mm/hotplug: prepare shrink_{zone, pgdat}_span for sub-section removal
      mm/sparsemem: add helpers track active portions of a section at boot
      mm/sparsemem: introduce a SECTION_IS_EARLY flag
      mm/sparsemem: introduce struct mem_section_usage
      drivers/base/memory.c: get rid of find_memory_block_hinted()
      mm/memory_hotplug: move and simplify walk_memory_blocks()
      mm/memory_hotplug: rename walk_memory_range() and pass start+size instead of pfns
      mm: make register_mem_sect_under_node() static
      ...

commit eec4844fae7c033a0c1fc1eb3b8517aeb8b6cc49
Author: Matteo Croce <mcroce@redhat.com>
Date:   Thu Jul 18 15:58:50 2019 -0700

    proc/sysctl: add shared variables for range check
    
    In the sysctl code the proc_dointvec_minmax() function is often used to
    validate the user supplied value between an allowed range.  This
    function uses the extra1 and extra2 members from struct ctl_table as
    minimum and maximum allowed value.
    
    On sysctl handler declaration, in every source file there are some
    readonly variables containing just an integer which address is assigned
    to the extra1 and extra2 members, so the sysctl range is enforced.
    
    The special values 0, 1 and INT_MAX are very often used as range
    boundary, leading duplication of variables like zero=0, one=1,
    int_max=INT_MAX in different source files:
    
        $ git grep -E '\.extra[12].*&(zero|one|int_max)' |wc -l
        248
    
    Add a const int array containing the most commonly used values, some
    macros to refer more easily to the correct array member, and use them
    instead of creating a local one for every object file.
    
    This is the bloat-o-meter output comparing the old and new binary
    compiled with the default Fedora config:
    
        # scripts/bloat-o-meter -d vmlinux.o.old vmlinux.o
        add/remove: 2/2 grow/shrink: 0/2 up/down: 24/-188 (-164)
        Data                                         old     new   delta
        sysctl_vals                                    -      12     +12
        __kstrtab_sysctl_vals                          -      12     +12
        max                                           14      10      -4
        int_max                                       16       -     -16
        one                                           68       -     -68
        zero                                         128      28    -100
        Total: Before=20583249, After=20583085, chg -0.00%
    
    [mcroce@redhat.com: tipc: remove two unused variables]
      Link: http://lkml.kernel.org/r/20190530091952.4108-1-mcroce@redhat.com
    [akpm@linux-foundation.org: fix net/ipv6/sysctl_net_ipv6.c]
    [arnd@arndb.de: proc/sysctl: make firmware loader table conditional]
      Link: http://lkml.kernel.org/r/20190617130014.1713870-1-arnd@arndb.de
    [akpm@linux-foundation.org: fix fs/eventpoll.c]
    Link: http://lkml.kernel.org/r/20190430180111.10688-1-mcroce@redhat.com
    Signed-off-by: Matteo Croce <mcroce@redhat.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Acked-by: Kees Cook <keescook@chromium.org>
    Reviewed-by: Aaron Tomlin <atomlin@redhat.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 1f73a6a7e43c..ffb1684c4573 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -80,7 +80,6 @@ static unsigned int min_slot_table_size = RPCRDMA_MIN_SLOT_TABLE;
 static unsigned int max_slot_table_size = RPCRDMA_MAX_SLOT_TABLE;
 static unsigned int min_inline_size = RPCRDMA_MIN_INLINE;
 static unsigned int max_inline_size = RPCRDMA_MAX_INLINE;
-static unsigned int zero;
 static unsigned int max_padding = PAGE_SIZE;
 static unsigned int min_memreg = RPCRDMA_BOUNCEBUFFERS;
 static unsigned int max_memreg = RPCRDMA_LAST - 1;
@@ -122,7 +121,7 @@ static struct ctl_table xr_tunables_table[] = {
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec_minmax,
-		.extra1		= &zero,
+		.extra1		= SYSCTL_ZERO,
 		.extra2		= &max_padding,
 	},
 	{

commit 7402a4fedc2bc448100c2d086406c708451b16dc
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Tue Jul 16 13:51:29 2019 -0400

    SUNRPC: Fix up backchannel slot table accounting
    
    Add a per-transport maximum limit in the socket case, and add
    helpers to allow the NFSv4 code to discover that limit.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 4993aa49ecbe..52abddac19e5 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -812,6 +812,7 @@ static const struct rpc_xprt_ops xprt_rdma_procs = {
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 	.bc_setup		= xprt_rdma_bc_setup,
 	.bc_maxpayload		= xprt_rdma_bc_maxpayload,
+	.bc_num_slots		= xprt_rdma_bc_max_slots,
 	.bc_free_rqst		= xprt_rdma_bc_free_rqst,
 	.bc_destroy		= xprt_rdma_bc_destroy,
 #endif

commit 675dd90ad0932f2c03912a5252458d792bd7033a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:42 2019 -0400

    xprtrdma: Modernize ops->connect
    
    Adapt and apply changes that were made to the TCP socket connect
    code. See the following commits for details on the purpose of
    these changes:
    
    Commit 7196dbb02ea0 ("SUNRPC: Allow changing of the TCP timeout parameters on the fly")
    Commit 3851f1cdb2b8 ("SUNRPC: Limit the reconnect backoff timer to the max RPC message timeout")
    Commit 02910177aede ("SUNRPC: Fix reconnection timeouts")
    
    Some common transport code is moved to xprt.c to satisfy the code
    duplication police.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 3688e0782587..4993aa49ecbe 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -298,6 +298,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 	module_put(THIS_MODULE);
 }
 
+/* 60 second timeout, no retries */
 static const struct rpc_timeout xprt_rdma_default_timeout = {
 	.to_initval = 60 * HZ,
 	.to_maxval = 60 * HZ,
@@ -323,8 +324,9 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (!xprt)
 		return ERR_PTR(-ENOMEM);
 
-	/* 60 second timeout, no retries */
 	xprt->timeout = &xprt_rdma_default_timeout;
+	xprt->connect_timeout = xprt->timeout->to_initval;
+	xprt->max_reconnect_timeout = xprt->timeout->to_maxval;
 	xprt->bind_timeout = RPCRDMA_BIND_TO;
 	xprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;
 	xprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;
@@ -487,31 +489,64 @@ xprt_rdma_timer(struct rpc_xprt *xprt, struct rpc_task *task)
 }
 
 /**
- * xprt_rdma_connect - try to establish a transport connection
+ * xprt_rdma_set_connect_timeout - set timeouts for establishing a connection
+ * @xprt: controlling transport instance
+ * @connect_timeout: reconnect timeout after client disconnects
+ * @reconnect_timeout: reconnect timeout after server disconnects
+ *
+ */
+static void xprt_rdma_tcp_set_connect_timeout(struct rpc_xprt *xprt,
+					      unsigned long connect_timeout,
+					      unsigned long reconnect_timeout)
+{
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+
+	trace_xprtrdma_op_set_cto(r_xprt, connect_timeout, reconnect_timeout);
+
+	spin_lock(&xprt->transport_lock);
+
+	if (connect_timeout < xprt->connect_timeout) {
+		struct rpc_timeout to;
+		unsigned long initval;
+
+		to = *xprt->timeout;
+		initval = connect_timeout;
+		if (initval < RPCRDMA_INIT_REEST_TO << 1)
+			initval = RPCRDMA_INIT_REEST_TO << 1;
+		to.to_initval = initval;
+		to.to_maxval = initval;
+		r_xprt->rx_timeout = to;
+		xprt->timeout = &r_xprt->rx_timeout;
+		xprt->connect_timeout = connect_timeout;
+	}
+
+	if (reconnect_timeout < xprt->max_reconnect_timeout)
+		xprt->max_reconnect_timeout = reconnect_timeout;
+
+	spin_unlock(&xprt->transport_lock);
+}
+
+/**
+ * xprt_rdma_connect - schedule an attempt to reconnect
  * @xprt: transport state
- * @task: RPC scheduler context
+ * @task: RPC scheduler context (unused)
  *
  */
 static void
 xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	unsigned long delay;
 
 	trace_xprtrdma_op_connect(r_xprt);
+
+	delay = 0;
 	if (r_xprt->rx_ep.rep_connected != 0) {
-		/* Reconnect */
-		schedule_delayed_work(&r_xprt->rx_connect_worker,
-				      xprt->reestablish_timeout);
-		xprt->reestablish_timeout <<= 1;
-		if (xprt->reestablish_timeout > RPCRDMA_MAX_REEST_TO)
-			xprt->reestablish_timeout = RPCRDMA_MAX_REEST_TO;
-		else if (xprt->reestablish_timeout < RPCRDMA_INIT_REEST_TO)
-			xprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;
-	} else {
-		schedule_delayed_work(&r_xprt->rx_connect_worker, 0);
-		if (!RPC_IS_ASYNC(task))
-			flush_delayed_work(&r_xprt->rx_connect_worker);
+		delay = xprt_reconnect_delay(xprt);
+		xprt_reconnect_backoff(xprt, RPCRDMA_INIT_REEST_TO);
 	}
+	queue_delayed_work(xprtiod_workqueue, &r_xprt->rx_connect_worker,
+			   delay);
 }
 
 /**
@@ -769,6 +804,7 @@ static const struct rpc_xprt_ops xprt_rdma_procs = {
 	.send_request		= xprt_rdma_send_request,
 	.close			= xprt_rdma_close,
 	.destroy		= xprt_rdma_destroy,
+	.set_connect_timeout	= xprt_rdma_tcp_set_connect_timeout,
 	.print_stats		= xprt_rdma_print_stats,
 	.enable_swap		= xprt_rdma_enable_swap,
 	.disable_swap		= xprt_rdma_disable_swap,

commit 5828cebad1c8d535f3c194439e394e92a2273fb2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:36 2019 -0400

    xprtrdma: Remove rpcrdma_req::rl_buffer
    
    Clean up.
    
    There is only one remaining function, rpcrdma_buffer_put(), that
    uses this field. Its caller can supply a pointer to the correct
    rpcrdma_buffer, enabling the removal of an 8-byte pointer field
    from a frequently-allocated shared data structure.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9575f1d8db07..3688e0782587 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -550,8 +550,11 @@ xprt_rdma_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)
 static void
 xprt_rdma_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *rqst)
 {
+	struct rpcrdma_xprt *r_xprt =
+		container_of(xprt, struct rpcrdma_xprt, rx_xprt);
+
 	memset(rqst, 0, sizeof(*rqst));
-	rpcrdma_buffer_put(rpcr_to_rdmar(rqst));
+	rpcrdma_buffer_put(&r_xprt->rx_buf, rpcr_to_rdmar(rqst));
 	rpc_wake_up_next(&xprt->backlog);
 }
 

commit 0ab115237025f5e379620bbcd56a02697d07b002
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:33:15 2019 -0400

    xprtrdma: Wake RPCs directly in rpcrdma_wc_send path
    
    Eliminate a context switch in the path that handles RPC wake-ups
    when a Receive completion has to wait for a Send completion.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index f84375ddbb4d..9575f1d8db07 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -618,8 +618,16 @@ xprt_rdma_free(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 
-	rpcrdma_release_rqst(r_xprt, req);
 	trace_xprtrdma_op_free(task, req);
+
+	if (!list_empty(&req->rl_registered))
+		frwr_unmap_sync(r_xprt, req);
+
+	/* XXX: If the RPC is completing because of a signal and
+	 * not because a reply was received, we ought to ensure
+	 * that the Send completion has fired, so that memory
+	 * involved with the Send is not still visible to the NIC.
+	 */
 }
 
 /**

commit 5809ea4f7c39bf38e3f85ec185b776da9d81717c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 19 10:32:54 2019 -0400

    xprtrdma: Remove the RPCRDMA_REQ_F_PENDING flag
    
    Commit 9590d083c1bb ("xprtrdma: Use xprt_pin_rqst in
    rpcrdma_reply_handler") pins incoming RPC/RDMA replies so they
    can be left in the pending requests queue while they are being
    processed without introducing a race between ->buf_free and the
    transport's reply handler. Therefore RPCRDMA_REQ_F_PENDING is no
    longer necessary.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 1f73a6a7e43c..f84375ddbb4d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -618,8 +618,7 @@ xprt_rdma_free(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 
-	if (test_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags))
-		rpcrdma_release_rqst(r_xprt, req);
+	rpcrdma_release_rqst(r_xprt, req);
 	trace_xprtrdma_op_free(task, req);
 }
 
@@ -667,7 +666,6 @@ xprt_rdma_send_request(struct rpc_rqst *rqst)
 		goto drop_connection;
 	rqst->rq_xtime = ktime_get();
 
-	__set_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags);
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))
 		goto drop_connection;
 

commit 86c4ccd9b92ba6541fc4734e82f87139deea0470
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:25 2019 -0400

    xprtrdma: Eliminate struct rpcrdma_create_data_internal
    
    Clean up.
    
    Move the remaining field in rpcrdma_create_data_internal so the
    structure can be removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index b37a3e0f6728..1f73a6a7e43c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -68,7 +68,7 @@
  * tunables
  */
 
-static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
+unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
 unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 unsigned int xprt_rdma_memreg_strategy		= RPCRDMA_FRWR;
@@ -288,7 +288,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
-	rpcrdma_ep_destroy(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	rpcrdma_ep_destroy(r_xprt);
 	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
 	rpcrdma_ia_close(&r_xprt->rx_ia);
 
@@ -311,10 +311,8 @@ static const struct rpc_timeout xprt_rdma_default_timeout = {
 static struct rpc_xprt *
 xprt_setup_rdma(struct xprt_create *args)
 {
-	struct rpcrdma_create_data_internal cdata;
 	struct rpc_xprt *xprt;
 	struct rpcrdma_xprt *new_xprt;
-	struct rpcrdma_ep *new_ep;
 	struct sockaddr *sap;
 	int rc;
 
@@ -349,29 +347,12 @@ xprt_setup_rdma(struct xprt_create *args)
 		xprt_set_bound(xprt);
 	xprt_rdma_format_addresses(xprt, sap);
 
-	cdata.max_requests = xprt_rdma_slot_table_entries;
-
-	/*
-	 * Create new transport instance, which includes initialized
-	 *  o ia
-	 *  o endpoint
-	 *  o buffers
-	 */
-
 	new_xprt = rpcx_to_rdmax(xprt);
-
 	rc = rpcrdma_ia_open(new_xprt);
 	if (rc)
 		goto out1;
 
-	/*
-	 * initialize and create ep
-	 */
-	new_xprt->rx_data = cdata;
-	new_ep = &new_xprt->rx_ep;
-
-	rc = rpcrdma_ep_create(&new_xprt->rx_ep,
-				&new_xprt->rx_ia, &new_xprt->rx_data);
+	rc = rpcrdma_ep_create(new_xprt);
 	if (rc)
 		goto out2;
 
@@ -402,7 +383,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	rpcrdma_buffer_destroy(&new_xprt->rx_buf);
 	rc = -ENODEV;
 out3:
-	rpcrdma_ep_destroy(new_ep, &new_xprt->rx_ia);
+	rpcrdma_ep_destroy(new_xprt);
 out2:
 	rpcrdma_ia_close(&new_xprt->rx_ia);
 out1:

commit 94087e978e9b645e07cc0fbdcf4140dda02f3d81
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:20 2019 -0400

    xprtrdma: Aggregate the inline settings in struct rpcrdma_ep
    
    Clean up.
    
    The inline settings are actually a characteristic of the endpoint,
    and not related to the device. They are also modified after the
    transport instance is created, so they do not belong in the cdata
    structure either.
    
    Lastly, let's use names that are more natural to RDMA than to NFS:
    inline_write -> inline_send and inline_read -> inline_recv. The
    /proc files retain their names to avoid breaking user space.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6b7c84166d13..b37a3e0f6728 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -70,7 +70,7 @@
 
 static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
 unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
-static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
+unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 unsigned int xprt_rdma_memreg_strategy		= RPCRDMA_FRWR;
 int xprt_rdma_pad_optimize;
 
@@ -350,8 +350,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	xprt_rdma_format_addresses(xprt, sap);
 
 	cdata.max_requests = xprt_rdma_slot_table_entries;
-	cdata.inline_wsize = xprt_rdma_max_inline_write;
-	cdata.inline_rsize = xprt_rdma_max_inline_read;
 
 	/*
 	 * Create new transport instance, which includes initialized

commit fd5951742dbc8c3695151e3f46b2fe2c4dac3559
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:40:15 2019 -0400

    xprtrdma: Remove rpcrdma_create_data_internal::rsize and wsize
    
    Clean up.
    
    xprt_rdma_max_inline_{read,write} cannot be set to large values
    by virtue of proc_dointvec_minmax. The current maximum is
    RPCRDMA_MAX_INLINE, which is much smaller than RPCRDMA_MAX_SEGS *
    PAGE_SIZE.
    
    The .rsize and .wsize fields are otherwise unused in the current
    code base, and thus can be removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8cf4fa36ed66..6b7c84166d13 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -350,17 +350,8 @@ xprt_setup_rdma(struct xprt_create *args)
 	xprt_rdma_format_addresses(xprt, sap);
 
 	cdata.max_requests = xprt_rdma_slot_table_entries;
-
-	cdata.rsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA write max */
-	cdata.wsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA read max */
-
 	cdata.inline_wsize = xprt_rdma_max_inline_write;
-	if (cdata.inline_wsize > cdata.wsize)
-		cdata.inline_wsize = cdata.wsize;
-
 	cdata.inline_rsize = xprt_rdma_max_inline_read;
-	if (cdata.inline_rsize > cdata.rsize)
-		cdata.inline_rsize = cdata.rsize;
 
 	/*
 	 * Create new transport instance, which includes initialized

commit 0f665ceb71a20520bdce76fb63ad68c21841aa62
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:27 2019 -0400

    xprtrdma: De-duplicate "allocate new, free old regbuf"
    
    Clean up by providing an API to do this common task.
    
    At this point, the difference between rpcrdma_get_sendbuf and
    rpcrdma_get_recvbuf has become tiny. These can be collapsed into a
    single helper.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index a5da43f3b035..8cf4fa36ed66 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -585,52 +585,15 @@ xprt_rdma_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *rqst)
 	rpc_wake_up_next(&xprt->backlog);
 }
 
-static bool
-rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
-		    size_t size, gfp_t flags)
+static bool rpcrdma_check_regbuf(struct rpcrdma_xprt *r_xprt,
+				 struct rpcrdma_regbuf *rb, size_t size,
+				 gfp_t flags)
 {
-	struct rpcrdma_regbuf *rb;
-
-	if (likely(rdmab_length(req->rl_sendbuf) >= size))
-		return true;
-
-	rb = rpcrdma_alloc_regbuf(size, DMA_TO_DEVICE, flags);
-	if (!rb)
-		return false;
-
-	rpcrdma_free_regbuf(req->rl_sendbuf);
-	r_xprt->rx_stats.hardway_register_count += size;
-	req->rl_sendbuf = rb;
-	return true;
-}
-
-/* The rq_rcv_buf is used only if a Reply chunk is necessary.
- * The decision to use a Reply chunk is made later in
- * rpcrdma_marshal_req. This buffer is registered at that time.
- *
- * Otherwise, the associated RPC Reply arrives in a separate
- * Receive buffer, arbitrarily chosen by the HCA. The buffer
- * allocated here for the RPC Reply is not utilized in that
- * case. See rpcrdma_inline_fixup.
- *
- * A regbuf is used here to remember the buffer size.
- */
-static bool
-rpcrdma_get_recvbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
-		    size_t size, gfp_t flags)
-{
-	struct rpcrdma_regbuf *rb;
-
-	if (likely(rdmab_length(req->rl_recvbuf) >= size))
-		return true;
-
-	rb = rpcrdma_alloc_regbuf(size, DMA_NONE, flags);
-	if (!rb)
-		return false;
-
-	rpcrdma_free_regbuf(req->rl_recvbuf);
-	r_xprt->rx_stats.hardway_register_count += size;
-	req->rl_recvbuf = rb;
+	if (unlikely(rdmab_length(rb) < size)) {
+		if (!rpcrdma_regbuf_realloc(rb, size, flags))
+			return false;
+		r_xprt->rx_stats.hardway_register_count += size;
+	}
 	return true;
 }
 
@@ -655,9 +618,11 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (RPC_IS_SWAPPER(task))
 		flags = __GFP_MEMALLOC | GFP_NOWAIT | __GFP_NOWARN;
 
-	if (!rpcrdma_get_sendbuf(r_xprt, req, rqst->rq_callsize, flags))
+	if (!rpcrdma_check_regbuf(r_xprt, req->rl_sendbuf, rqst->rq_callsize,
+				  flags))
 		goto out_fail;
-	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))
+	if (!rpcrdma_check_regbuf(r_xprt, req->rl_recvbuf, rqst->rq_rcvsize,
+				  flags))
 		goto out_fail;
 
 	rqst->rq_buffer = rdmab_data(req->rl_sendbuf);

commit bb93a1ae2bf4f6eb3cedf05a2ea4a2e6a80712e6
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:21 2019 -0400

    xprtrdma: Allocate req's regbufs at xprt create time
    
    Allocating an rpcrdma_req's regbufs at xprt create time enables
    a pair of micro-optimizations:
    
    First, if these regbufs are always there, we can eliminate two
    conditional branches from the hot xprt_rdma_allocate path.
    
    Second, by allocating a 1KB buffer, it places a lower bound on the
    size of these buffers, without adding yet another conditional
    branch. The lower bound reduces the number of hardway re-
    allocations. In fact, for some workloads it completely eliminates
    hardway allocations.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ced9812940f7..a5da43f3b035 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -591,7 +591,7 @@ rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 {
 	struct rpcrdma_regbuf *rb;
 
-	if (req->rl_sendbuf && rdmab_length(req->rl_sendbuf) >= size)
+	if (likely(rdmab_length(req->rl_sendbuf) >= size))
 		return true;
 
 	rb = rpcrdma_alloc_regbuf(size, DMA_TO_DEVICE, flags);
@@ -621,7 +621,7 @@ rpcrdma_get_recvbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 {
 	struct rpcrdma_regbuf *rb;
 
-	if (req->rl_recvbuf && rdmab_length(req->rl_recvbuf) >= size)
+	if (likely(rdmab_length(req->rl_recvbuf) >= size))
 		return true;
 
 	rb = rpcrdma_alloc_regbuf(size, DMA_NONE, flags);

commit 8cec3dba76a4d9d7da4a7219663b8c4333f14522
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Apr 24 09:39:16 2019 -0400

    xprtrdma: rpcrdma_regbuf alignment
    
    Allocate the struct rpcrdma_regbuf separately from the I/O buffer
    to better guarantee the alignment of the I/O buffer and eliminate
    the wasted space between the rpcrdma_regbuf metadata and the buffer
    itself.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7e73abe01cfe..ced9812940f7 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -595,7 +595,7 @@ rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		return true;
 
 	rb = rpcrdma_alloc_regbuf(size, DMA_TO_DEVICE, flags);
-	if (IS_ERR(rb))
+	if (!rb)
 		return false;
 
 	rpcrdma_free_regbuf(req->rl_sendbuf);
@@ -625,7 +625,7 @@ rpcrdma_get_recvbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		return true;
 
 	rb = rpcrdma_alloc_regbuf(size, DMA_NONE, flags);
-	if (IS_ERR(rb))
+	if (!rb)
 		return false;
 
 	rpcrdma_free_regbuf(req->rl_recvbuf);
@@ -660,8 +660,8 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))
 		goto out_fail;
 
-	rqst->rq_buffer = req->rl_sendbuf->rg_base;
-	rqst->rq_rbuffer = req->rl_recvbuf->rg_base;
+	rqst->rq_buffer = rdmab_data(req->rl_sendbuf);
+	rqst->rq_rbuffer = rdmab_data(req->rl_recvbuf);
 	trace_xprtrdma_op_allocate(task, req);
 	return 0;
 

commit 8ba6a92d0182091e0c2fa15c1a5b5458bac25fc3
Author: Trond Myklebust <trondmy@gmail.com>
Date:   Sun Apr 7 13:58:46 2019 -0400

    SUNRPC: Refactor xprt_request_wait_receive()
    
    Convert the transport callback to actually put the request to sleep
    instead of just setting a timeout. This is in preparation for
    rpc_sleep_on_timeout().
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 5d261353bd90..7e73abe01cfe 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -815,7 +815,7 @@ static const struct rpc_xprt_ops xprt_rdma_procs = {
 	.alloc_slot		= xprt_rdma_alloc_slot,
 	.free_slot		= xprt_rdma_free_slot,
 	.release_request	= xprt_release_rqst_cong,       /* ditto */
-	.set_retrans_timeout	= xprt_set_retrans_timeout_def, /* ditto */
+	.wait_for_reply_request	= xprt_wait_for_reply_request_def, /* ditto */
 	.timer			= xprt_rdma_timer,
 	.rpcbind		= rpcb_getport_async,	/* sunrpc/rpcb_clnt.c */
 	.set_port		= xprt_rdma_set_port,

commit 06b5fc3ad94eebf25d5abc07f84e16b8b33dcf8c
Merge: 5085607d2091 2c94b8eca1a2
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Mon Feb 25 08:39:26 2019 -0500

    Merge tag 'nfs-rdma-for-5.1-1' of git://git.linux-nfs.org/projects/anna/linux-nfs
    
    NFSoRDMA client updates for 5.1
    
    New features:
    - Convert rpc auth layer to use xdr_streams
    - Config option to disable insecure enctypes
    - Reduce size of RPC receive buffers
    
    Bugfixes and cleanups:
    - Fix sparse warnings
    - Check inline size before providing a write chunk
    - Reduce the receive doorbell rate
    - Various tracepoint improvements
    
    [Trond: Fix up merge conflicts]
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

commit b9779a54bb224cd2a993c9bf8a1910a7494c062e
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Wed Jan 2 15:54:42 2019 -0500

    SUNRPC: Ensure rq_bytes_sent is reset before request transmission
    
    When we resend a request, ensure that the 'rq_bytes_sent' is reset
    to zero.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index fbc171ebfe91..3c490172d383 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -738,7 +738,6 @@ xprt_rdma_send_request(struct rpc_rqst *rqst)
 		goto drop_connection;
 
 	rqst->rq_xmit_bytes_sent += rqst->rq_snd_buf.len;
-	rqst->rq_bytes_sent = 0;
 
 	/* An RPC with no reply will throw off credit accounting,
 	 * so drop the connection to reset the credit grant.

commit 067fb11b12af1448f7bbcacca41e470cb775e9fa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Feb 11 11:24:37 2019 -0500

    SUNRPC: Remove rpc_xprt::tsh_size
    
    tsh_size was added to accommodate transports that send a pre-amble
    before each RPC message. However, this assumes the pre-amble is
    fixed in size, which isn't true for some transports. That makes
    tsh_size not very generic.
    
    Also I'd like to make the estimation of RPC send and receive
    buffer sizes more precise. tsh_size doesn't currently appear to be
    accounted for at all by call_allocate.
    
    Therefore let's just remove the tsh_size concept, and make the only
    transports that have a non-zero tsh_size employ a direct approach.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index fbc171ebfe91..e7274dc10120 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -332,7 +332,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	xprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;
 
 	xprt->resvport = 0;		/* privileged port not needed */
-	xprt->tsh_size = 0;		/* RPC-RDMA handles framing */
 	xprt->ops = &xprt_rdma_procs;
 
 	/*

commit e6b92572808467f35fd159d47c45b650de29e722
Merge: e45428a43676 260f71eff493
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 2 16:35:23 2019 -0800

    Merge tag 'nfs-for-4.21-1' of git://git.linux-nfs.org/projects/anna/linux-nfs
    
    Pull NFS client updates from Anna Schumaker:
     "Stable bugfixes:
       - xprtrdma: Yet another double DMA-unmap # v4.20
    
      Features:
       - Allow some /proc/sys/sunrpc entries without CONFIG_SUNRPC_DEBUG
       - Per-xprt rdma receive workqueues
       - Drop support for FMR memory registration
       - Make port= mount option optional for RDMA mounts
    
      Other bugfixes and cleanups:
       - Remove unused nfs4_xdev_fs_type declaration
       - Fix comments for behavior that has changed
       - Remove generic RPC credentials by switching to 'struct cred'
       - Fix crossing mountpoints with different auth flavors
       - Various xprtrdma fixes from testing and auditing the close code
       - Fixes for disconnect issues when using xprtrdma with krb5
       - Clean up and improve xprtrdma trace points
       - Fix NFS v4.2 async copy reboot recovery"
    
    * tag 'nfs-for-4.21-1' of git://git.linux-nfs.org/projects/anna/linux-nfs: (63 commits)
      sunrpc: convert to DEFINE_SHOW_ATTRIBUTE
      sunrpc: Add xprt after nfs4_test_session_trunk()
      sunrpc: convert unnecessary GFP_ATOMIC to GFP_NOFS
      sunrpc: handle ENOMEM in rpcb_getport_async
      NFS: remove unnecessary test for IS_ERR(cred)
      xprtrdma: Prevent leak of rpcrdma_rep objects
      NFSv4.2 fix async copy reboot recovery
      xprtrdma: Don't leak freed MRs
      xprtrdma: Add documenting comment for rpcrdma_buffer_destroy
      xprtrdma: Replace outdated comment for rpcrdma_ep_post
      xprtrdma: Update comments in frwr_op_send
      SUNRPC: Fix some kernel doc complaints
      SUNRPC: Simplify defining common RPC trace events
      NFS: Fix NFSv4 symbolic trace point output
      xprtrdma: Trace mapping, alloc, and dereg failures
      xprtrdma: Add trace points for calls to transport switch methods
      xprtrdma: Relocate the xprtrdma_mr_map trace points
      xprtrdma: Clean up of xprtrdma chunk trace points
      xprtrdma: Remove unused fields from rpcrdma_ia
      xprtrdma: Cull dprintk() call sites
      ...

commit 395069fc37e7a76280b176c0327d1ead6ca29838
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 11:00:00 2018 -0500

    xprtrdma: Add trace points for calls to transport switch methods
    
    Name them "trace_xprtrdma_op_*" so they can be easily enabled as a
    group. No trace point is added where the generic layer already has
    observability.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6a57033a5846..d94da3c57593 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -268,7 +268,7 @@ xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
-	trace_xprtrdma_inject_dsc(r_xprt);
+	trace_xprtrdma_op_inject_dsc(r_xprt);
 	rdma_disconnect(r_xprt->rx_ia.ri_id);
 }
 
@@ -284,7 +284,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
-	trace_xprtrdma_destroy(r_xprt);
+	trace_xprtrdma_op_destroy(r_xprt);
 
 	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
@@ -418,7 +418,7 @@ xprt_setup_rdma(struct xprt_create *args)
 out2:
 	rpcrdma_ia_close(&new_xprt->rx_ia);
 out1:
-	trace_xprtrdma_destroy(new_xprt);
+	trace_xprtrdma_op_destroy(new_xprt);
 	xprt_rdma_free_addresses(xprt);
 	xprt_free(xprt);
 	return ERR_PTR(rc);
@@ -428,7 +428,8 @@ xprt_setup_rdma(struct xprt_create *args)
  * xprt_rdma_close - close a transport connection
  * @xprt: transport context
  *
- * Called during transport shutdown, reconnect, or device removal.
+ * Called during autoclose or device removal.
+ *
  * Caller holds @xprt's send lock to prevent activity on this
  * transport while the connection is torn down.
  */
@@ -440,6 +441,8 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 
 	might_sleep();
 
+	trace_xprtrdma_op_close(r_xprt);
+
 	/* Prevent marshaling and sending of new requests */
 	xprt_clear_connected(xprt);
 
@@ -525,6 +528,7 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
+	trace_xprtrdma_op_connect(r_xprt);
 	if (r_xprt->rx_ep.rep_connected != 0) {
 		/* Reconnect */
 		schedule_delayed_work(&r_xprt->rx_connect_worker,
@@ -659,11 +663,11 @@ xprt_rdma_allocate(struct rpc_task *task)
 
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
 	rqst->rq_rbuffer = req->rl_recvbuf->rg_base;
-	trace_xprtrdma_allocate(task, req);
+	trace_xprtrdma_op_allocate(task, req);
 	return 0;
 
 out_fail:
-	trace_xprtrdma_allocate(task, NULL);
+	trace_xprtrdma_op_allocate(task, NULL);
 	return -ENOMEM;
 }
 
@@ -682,7 +686,7 @@ xprt_rdma_free(struct rpc_task *task)
 
 	if (test_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags))
 		rpcrdma_release_rqst(r_xprt, req);
-	trace_xprtrdma_rpc_done(task, req);
+	trace_xprtrdma_op_free(task, req);
 }
 
 /**

commit ddbb347f0c68a66cb20b78af4bc318b10d30e425
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:59:39 2018 -0500

    xprtrdma: Cull dprintk() call sites
    
    Clean up: Remove dprintk() call sites that report rare or impossible
    errors. Leave a few that display high-value low noise status
    information.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 89e11f95c747..6a57033a5846 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -318,17 +318,12 @@ xprt_setup_rdma(struct xprt_create *args)
 	struct sockaddr *sap;
 	int rc;
 
-	if (args->addrlen > sizeof(xprt->addr)) {
-		dprintk("RPC:       %s: address too large\n", __func__);
+	if (args->addrlen > sizeof(xprt->addr))
 		return ERR_PTR(-EBADF);
-	}
 
 	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt), 0, 0);
-	if (xprt == NULL) {
-		dprintk("RPC:       %s: couldn't allocate rpcrdma_xprt\n",
-			__func__);
+	if (!xprt)
 		return ERR_PTR(-ENOMEM);
-	}
 
 	/* 60 second timeout, no retries */
 	xprt->timeout = &xprt_rdma_default_timeout;
@@ -445,8 +440,6 @@ void xprt_rdma_close(struct rpc_xprt *xprt)
 
 	might_sleep();
 
-	dprintk("RPC:       %s: closing xprt %p\n", __func__, xprt);
-
 	/* Prevent marshaling and sending of new requests */
 	xprt_clear_connected(xprt);
 
@@ -853,24 +846,15 @@ static struct xprt_class xprt_rdma = {
 
 void xprt_rdma_cleanup(void)
 {
-	int rc;
-
-	dprintk("RPCRDMA Module Removed, deregister RPC RDMA transport\n");
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 	if (sunrpc_table_header) {
 		unregister_sysctl_table(sunrpc_table_header);
 		sunrpc_table_header = NULL;
 	}
 #endif
-	rc = xprt_unregister_transport(&xprt_rdma);
-	if (rc)
-		dprintk("RPC:       %s: xprt_unregister returned %i\n",
-			__func__, rc);
 
-	rc = xprt_unregister_transport(&xprt_rdma_bc);
-	if (rc)
-		dprintk("RPC:       %s: xprt_unregister(bc) returned %i\n",
-			__func__, rc);
+	xprt_unregister_transport(&xprt_rdma);
+	xprt_unregister_transport(&xprt_rdma_bc);
 }
 
 int xprt_rdma_init(void)
@@ -887,15 +871,6 @@ int xprt_rdma_init(void)
 		return rc;
 	}
 
-	dprintk("RPCRDMA Module Init, register RPC RDMA transport\n");
-
-	dprintk("Defaults:\n");
-	dprintk("\tSlots %d\n"
-		"\tMaxInlineRead %d\n\tMaxInlineWrite %d\n",
-		xprt_rdma_slot_table_entries,
-		xprt_rdma_max_inline_read, xprt_rdma_max_inline_write);
-	dprintk("\tPadding 0\n\tMemreg %d\n", xprt_rdma_memreg_strategy);
-
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 	if (!sunrpc_table_header)
 		sunrpc_table_header = register_sysctl_table(sunrpc_table);

commit 5f62412be3ff738c9575b28c1f4a9b010ac22316
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:59:01 2018 -0500

    xprtrdma: Remove rpcrdma_memreg_ops
    
    Clean up: Now that there is only FRWR, there is no need for a memory
    registration switch. The indirect calls to the memreg operations can
    be replaced with faster direct calls.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d3f26c570067..89e11f95c747 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -399,7 +399,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
 			  xprt_rdma_connect_worker);
 
-	xprt->max_payload = new_xprt->rx_ia.ri_ops->ro_maxpages(new_xprt);
+	xprt->max_payload = frwr_maxpages(new_xprt);
 	if (xprt->max_payload == 0)
 		goto out4;
 	xprt->max_payload <<= PAGE_SHIFT;

commit 6946f82380a83acb7023fab9d7033e1f016cb818
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:45 2018 -0500

    xprtrdma: Fix ri_max_segs and the result of ro_maxpages
    
    With certain combinations of krb5i/p, MR size, and r/wsize, I/O can
    fail with EMSGSIZE. This is because the calculated value of
    ri_max_segs (the max number of MRs per RPC) exceeded
    RPCRDMA_MAX_HDR_SEGS, which caused Read or Write list encoding to
    walk off the end of the transport header.
    
    Once that was addressed, the ro_maxpages result has to be corrected
    to account for the number of MRs needed for Reply chunks, which is
    2 MRs smaller than a normal Read or Write chunk.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 134aae2ee779..d3f26c570067 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -703,8 +703,10 @@ xprt_rdma_free(struct rpc_task *task)
  *	%-ENOTCONN if the caller should reconnect and call again
  *	%-EAGAIN if the caller should call again
  *	%-ENOBUFS if the caller should call again after a delay
- *	%-EIO if a permanent error occurred and the request was not
- *		sent. Do not try to send this message again.
+ *	%-EMSGSIZE if encoding ran out of buffer space. The request
+ *		was not sent. Do not try to send this message again.
+ *	%-EIO if an I/O error occurred. The request was not sent.
+ *		Do not try to send this message again.
  */
 static int
 xprt_rdma_send_request(struct rpc_rqst *rqst)

commit 0c0829bcf51aef713806e49b8ea2bac7962f54e2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:40 2018 -0500

    xprtrdma: Don't wake pending tasks until disconnect is done
    
    Transport disconnect processing does a "wake pending tasks" at
    various points.
    
    Suppose an RPC Reply is being processed. The RPC task that Reply
    goes with is waiting on the pending queue. If a disconnect wake-up
    happens before reply processing is done, that reply, even if it is
    good, is thrown away, and the RPC has to be sent again.
    
    This window apparently does not exist for socket transports because
    there is a lock held while a reply is being received which prevents
    the wake-up call until after reply processing is done.
    
    To resolve this, all RPC replies being processed on an RPC-over-RDMA
    transport have to complete before pending tasks are awoken due to a
    transport disconnect.
    
    Callers that already hold the transport write lock may invoke
    ->ops->close directly. Others use a generic helper that schedules
    a close when the write lock can be taken safely.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 91c476a8f51c..134aae2ee779 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -437,8 +437,7 @@ xprt_setup_rdma(struct xprt_create *args)
  * Caller holds @xprt's send lock to prevent activity on this
  * transport while the connection is torn down.
  */
-static void
-xprt_rdma_close(struct rpc_xprt *xprt)
+void xprt_rdma_close(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
@@ -453,13 +452,13 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 
 	if (test_and_clear_bit(RPCRDMA_IAF_REMOVING, &ia->ri_flags)) {
 		rpcrdma_ia_remove(ia);
-		return;
+		goto out;
 	}
+
 	if (ep->rep_connected == -ENODEV)
 		return;
 	if (ep->rep_connected > 0)
 		xprt->reestablish_timeout = 0;
-	xprt_disconnect_done(xprt);
 	rpcrdma_ep_disconnect(ep, ia);
 
 	/* Prepare @xprt for the next connection by reinitializing
@@ -467,6 +466,10 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 	 */
 	r_xprt->rx_buf.rb_credits = 1;
 	xprt->cwnd = RPC_CWNDSHIFT;
+
+out:
+	++xprt->connect_cookie;
+	xprt_disconnect_done(xprt);
 }
 
 /**
@@ -717,7 +720,7 @@ xprt_rdma_send_request(struct rpc_rqst *rqst)
 #endif	/* CONFIG_SUNRPC_BACKCHANNEL */
 
 	if (!xprt_connected(xprt))
-		goto drop_connection;
+		return -ENOTCONN;
 
 	if (!xprt_request_get_cong(xprt, rqst))
 		return -EBADSLT;
@@ -749,8 +752,8 @@ xprt_rdma_send_request(struct rpc_rqst *rqst)
 	if (rc != -ENOTCONN)
 		return rc;
 drop_connection:
-	xprt_disconnect_done(xprt);
-	return -ENOTCONN;	/* implies disconnect */
+	xprt_rdma_close(xprt);
+	return -ENOTCONN;
 }
 
 void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)

commit 6d2d0ee27c7a12371a0ca51a5db414204901228c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 19 10:58:29 2018 -0500

    xprtrdma: Replace rpcrdma_receive_wq with a per-xprt workqueue
    
    To address a connection-close ordering problem, we need the ability
    to drain the RPC completions running on rpcrdma_receive_wq for just
    one transport. Give each transport its own RPC completion workqueue,
    and drain that workqueue when disconnecting the transport.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ae2a83828953..91c476a8f51c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -444,10 +444,14 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
 	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 
+	might_sleep();
+
 	dprintk("RPC:       %s: closing xprt %p\n", __func__, xprt);
 
+	/* Prevent marshaling and sending of new requests */
+	xprt_clear_connected(xprt);
+
 	if (test_and_clear_bit(RPCRDMA_IAF_REMOVING, &ia->ri_flags)) {
-		xprt_clear_connected(xprt);
 		rpcrdma_ia_remove(ia);
 		return;
 	}
@@ -858,8 +862,6 @@ void xprt_rdma_cleanup(void)
 		dprintk("RPC:       %s: xprt_unregister returned %i\n",
 			__func__, rc);
 
-	rpcrdma_destroy_wq();
-
 	rc = xprt_unregister_transport(&xprt_rdma_bc);
 	if (rc)
 		dprintk("RPC:       %s: xprt_unregister(bc) returned %i\n",
@@ -870,20 +872,13 @@ int xprt_rdma_init(void)
 {
 	int rc;
 
-	rc = rpcrdma_alloc_wq();
-	if (rc)
-		return rc;
-
 	rc = xprt_register_transport(&xprt_rdma);
-	if (rc) {
-		rpcrdma_destroy_wq();
+	if (rc)
 		return rc;
-	}
 
 	rc = xprt_register_transport(&xprt_rdma_bc);
 	if (rc) {
 		xprt_unregister_transport(&xprt_rdma);
-		rpcrdma_destroy_wq();
 		return rc;
 	}
 

commit 4aa5cffefa6f8af8f16490df58b8f0d827911b58
Author: Vasily Averin <vvs@virtuozzo.com>
Date:   Mon Dec 24 14:45:25 2018 +0300

    sunrpc: remove unused bc_up operation from rpc_xprt_ops
    
    Signed-off-by: Vasily Averin <vvs@virtuozzo.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ae2a83828953..9141068693fa 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -827,7 +827,6 @@ static const struct rpc_xprt_ops xprt_rdma_procs = {
 	.inject_disconnect	= xprt_rdma_inject_disconnect,
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 	.bc_setup		= xprt_rdma_bc_setup,
-	.bc_up			= xprt_rdma_bc_up,
 	.bc_maxpayload		= xprt_rdma_bc_maxpayload,
 	.bc_free_rqst		= xprt_rdma_bc_free_rqst,
 	.bc_destroy		= xprt_rdma_bc_destroy,

commit 93bdcf9fdb89646e9446fab25aec76a754969619
Merge: 826799e66e86 470443e0b379
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Thu Oct 18 17:29:00 2018 -0400

    Merge tag 'nfs-rdma-for-4.20-1' of git://git.linux-nfs.org/projects/anna/linux-nfs
    
    NFS RDMA client updates for Linux 4.20
    
    Stable bugfixes:
    - Reset credit grant properly after a disconnect
    
    Other bugfixes and cleanups:
    - xprt_release_rqst_cong is called outside of transport_lock
    - Create more MRs at a time and toss out old ones during recovery
    - Various improvements to the RDMA connection and disconnection code:
      - Improve naming of trace events, functions, and variables
      - Add documenting comments
      - Fix metrics and stats reporting
    - Fix a tracepoint sparse warning
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

commit ad0911802cf6be48ddf5911ca1837d071b26e92d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:26:45 2018 -0400

    xprtrdma: Clean up xprt_rdma_disconnect_inject
    
    Clean up: Use the appropriate C macro instead of open-coding
    container_of() .
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 39b7991862d0..0cfa7bf41118 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -266,8 +266,7 @@ xprt_rdma_connect_worker(struct work_struct *work)
 static void
 xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
 {
-	struct rpcrdma_xprt *r_xprt = container_of(xprt, struct rpcrdma_xprt,
-						   rx_xprt);
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
 	trace_xprtrdma_inject_dsc(r_xprt);
 	rdma_disconnect(r_xprt->rx_ia.ri_id);

commit f26c32fa5c6ab616e12d4ff0748c85927b092601
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:26:40 2018 -0400

    xprtrdma: Add documenting comments
    
    Clean up: fill in or update documenting comments for transport
    switch entry points.
    
    For xprt_rdma_allocate:
    
    The first paragraph is no longer true since commit 5a6d1db45569
    ("SUNRPC: Add a transport-specific private field in rpc_rqst").
    
    The second paragraph is no longer true since commit 54cbd6b0c6b9
    ("xprtrdma: Delay DMA mapping Send and Receive buffers").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d7c4255e9d5d..39b7991862d0 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -256,6 +256,13 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	}
 }
 
+/**
+ * xprt_rdma_inject_disconnect - inject a connection fault
+ * @xprt: transport context
+ *
+ * If @xprt is connected, disconnect it to simulate spurious connection
+ * loss.
+ */
 static void
 xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
 {
@@ -266,16 +273,12 @@ xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
 	rdma_disconnect(r_xprt->rx_ia.ri_id);
 }
 
-/*
- * xprt_rdma_destroy
+/**
+ * xprt_rdma_destroy - Full tear down of transport
+ * @xprt: doomed transport context
  *
- * Destroy the xprt.
- * Free all memory associated with the object, including its own.
- * NOTE: none of the *destroy methods free memory for their top-level
- * objects, even though they may have allocated it (they do free
- * private memory). It's up to the caller to handle it. In this
- * case (RDMA transport), all structure memory is inlined with the
- * struct rpcrdma_xprt.
+ * Caller guarantees there will be no more calls to us with
+ * this @xprt.
  */
 static void
 xprt_rdma_destroy(struct rpc_xprt *xprt)
@@ -428,11 +431,12 @@ xprt_setup_rdma(struct xprt_create *args)
 }
 
 /**
- * xprt_rdma_close - Close down RDMA connection
- * @xprt: generic transport to be closed
+ * xprt_rdma_close - close a transport connection
+ * @xprt: transport context
  *
- * Called during transport shutdown reconnect, or device
- * removal. Caller holds the transport's write lock.
+ * Called during transport shutdown, reconnect, or device removal.
+ * Caller holds @xprt's send lock to prevent activity on this
+ * transport while the connection is torn down.
  */
 static void
 xprt_rdma_close(struct rpc_xprt *xprt)
@@ -511,6 +515,12 @@ xprt_rdma_timer(struct rpc_xprt *xprt, struct rpc_task *task)
 	xprt_force_disconnect(xprt);
 }
 
+/**
+ * xprt_rdma_connect - try to establish a transport connection
+ * @xprt: transport state
+ * @task: RPC scheduler context
+ *
+ */
 static void
 xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 {
@@ -630,13 +640,6 @@ rpcrdma_get_recvbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
  *        0:	Success; rq_buffer points to RPC buffer to use
  *   ENOMEM:	Out of memory, call again later
  *      EIO:	A permanent error occurred, do not retry
- *
- * The RDMA allocate/free functions need the task structure as a place
- * to hide the struct rpcrdma_req, which is necessary for the actual
- * send/recv sequence.
- *
- * xprt_rdma_allocate provides buffers that are already mapped for
- * DMA, and a local DMA lkey is provided for each.
  */
 static int
 xprt_rdma_allocate(struct rpc_task *task)

commit 31e62d25b5b8155b2ff6a7c6d31256475dbbcc7a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:26:08 2018 -0400

    xprtrdma: Simplify RPC wake-ups on connect
    
    Currently, when a connection is established, rpcrdma_conn_upcall
    invokes rpcrdma_conn_func and then
    wake_up_all(&ep->rep_connect_wait). The former wakes waiting RPCs,
    but the connect worker is not done yet, and that leads to races,
    double wakes, and difficulty understanding how this logic is
    supposed to work.
    
    Instead, collect all the "connection established" logic in the
    connect worker (xprt_rdma_connect_worker). A disconnect worker is
    retained to handle provider upcalls safely.
    
    Fixes: 254f91e2fa1f ("xprtrdma: RPC/RDMA must invoke ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 289d13cad638..d7c4255e9d5d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -225,51 +225,35 @@ xprt_rdma_free_addresses(struct rpc_xprt *xprt)
 		}
 }
 
-void
-rpcrdma_conn_func(struct rpcrdma_ep *ep)
-{
-	schedule_delayed_work(&ep->rep_connect_worker, 0);
-}
-
-void
-rpcrdma_connect_worker(struct work_struct *work)
+/**
+ * xprt_rdma_connect_worker - establish connection in the background
+ * @work: worker thread context
+ *
+ * Requester holds the xprt's send lock to prevent activity on this
+ * transport while a fresh connection is being established. RPC tasks
+ * sleep on the xprt's pending queue waiting for connect to complete.
+ */
+static void
+xprt_rdma_connect_worker(struct work_struct *work)
 {
-	struct rpcrdma_ep *ep =
-		container_of(work, struct rpcrdma_ep, rep_connect_worker.work);
-	struct rpcrdma_xprt *r_xprt =
-		container_of(ep, struct rpcrdma_xprt, rx_ep);
+	struct rpcrdma_xprt *r_xprt = container_of(work, struct rpcrdma_xprt,
+						   rx_connect_worker.work);
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
+	int rc;
 
-	spin_lock_bh(&xprt->transport_lock);
-	if (ep->rep_connected > 0) {
+	rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	xprt_clear_connecting(xprt);
+	if (r_xprt->rx_ep.rep_connected > 0) {
 		if (!xprt_test_and_set_connected(xprt)) {
 			xprt->stat.connect_count++;
 			xprt->stat.connect_time += (long)jiffies -
 						   xprt->stat.connect_start;
-			xprt_wake_pending_tasks(xprt, 0);
+			xprt_wake_pending_tasks(xprt, -EAGAIN);
 		}
 	} else {
 		if (xprt_test_and_clear_connected(xprt))
-			xprt_wake_pending_tasks(xprt, -ENOTCONN);
+			xprt_wake_pending_tasks(xprt, rc);
 	}
-	spin_unlock_bh(&xprt->transport_lock);
-}
-
-static void
-xprt_rdma_connect_worker(struct work_struct *work)
-{
-	struct rpcrdma_xprt *r_xprt = container_of(work, struct rpcrdma_xprt,
-						   rx_connect_worker.work);
-	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
-	int rc = 0;
-
-	xprt_clear_connected(xprt);
-
-	rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
-	if (rc)
-		xprt_wake_pending_tasks(xprt, rc);
-
-	xprt_clear_connecting(xprt);
 }
 
 static void
@@ -302,8 +286,6 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
-	xprt_clear_connected(xprt);
-
 	rpcrdma_ep_destroy(&r_xprt->rx_ep, &r_xprt->rx_ia);
 	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
 	rpcrdma_ia_close(&r_xprt->rx_ia);

commit 8440a886112b46a8b402679dca9d8b5662a0d73e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:25:41 2018 -0400

    sunrpc: Report connect_time in seconds
    
    The way connection-oriented transports report connect_time is wrong:
    it's supposed to be in seconds, not in jiffies.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 087acfce142a..289d13cad638 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -776,7 +776,7 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   0,	/* need a local port? */
 		   xprt->stat.bind_count,
 		   xprt->stat.connect_count,
-		   xprt->stat.connect_time,
+		   xprt->stat.connect_time / HZ,
 		   idle_time,
 		   xprt->stat.sends,
 		   xprt->stat.recvs,

commit 3968a8a5310404c2f0b9e4d9f28cab13a12bc4fd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:25:36 2018 -0400

    sunrpc: Fix connect metrics
    
    For TCP, the logic in xprt_connect_status is currently never invoked
    to record a successful connection. Commit 2a4919919a97 ("SUNRPC:
    Return EAGAIN instead of ENOTCONN when waking up xprt->pending")
    changed the way TCP xprt's are awoken after a connect succeeds.
    
    Instead, change connection-oriented transports to bump connect_count
    and compute connect_time the moment that XPRT_CONNECTED is set.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 3ae73e6a5c93..087acfce142a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -242,8 +242,12 @@ rpcrdma_connect_worker(struct work_struct *work)
 
 	spin_lock_bh(&xprt->transport_lock);
 	if (ep->rep_connected > 0) {
-		if (!xprt_test_and_set_connected(xprt))
+		if (!xprt_test_and_set_connected(xprt)) {
+			xprt->stat.connect_count++;
+			xprt->stat.connect_time += (long)jiffies -
+						   xprt->stat.connect_start;
 			xprt_wake_pending_tasks(xprt, 0);
+		}
 	} else {
 		if (xprt_test_and_clear_connected(xprt))
 			xprt_wake_pending_tasks(xprt, -ENOTCONN);

commit 61da886bf74e738995d359fa14d77f72d14cfb87
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:25:25 2018 -0400

    xprtrdma: Explicitly resetting MRs is no longer necessary
    
    When a memory operation fails, the MR's driver state might not match
    its hardware state. The only reliable recourse is to dereg the MR.
    This is done in ->ro_recover_mr, which then attempts to allocate a
    fresh MR to replace the released MR.
    
    Since commit e2ac236c0b651 ("xprtrdma: Allocate MRs on demand"),
    xprtrdma dynamically allocates MRs. It can add more MRs whenever
    they are needed.
    
    That makes it possible to simply release an MR when a memory
    operation fails, instead of "recovering" it. It will automatically
    be replaced by the on-demand MR allocator.
    
    This commit is a little larger than I wanted, but it replaces
    ->ro_recover_mr, rb_recovery_lock, rb_recovery_worker, and the
    rb_stale_mrs list with a generic work queue.
    
    Since MRs are no longer orphaned, the mrs_orphaned metric is no
    longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 98cbc7b060ba..3ae73e6a5c93 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -792,7 +792,7 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   r_xprt->rx_stats.bad_reply_count,
 		   r_xprt->rx_stats.nomsg_call_count);
 	seq_printf(seq, "%lu %lu %lu %lu %lu %lu\n",
-		   r_xprt->rx_stats.mrs_recovered,
+		   r_xprt->rx_stats.mrs_recycled,
 		   r_xprt->rx_stats.mrs_orphaned,
 		   r_xprt->rx_stats.mrs_allocated,
 		   r_xprt->rx_stats.local_inv_needed,

commit ef739b2175dde9c05594f768cb78149f1ce2ac36
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 1 14:25:14 2018 -0400

    xprtrdma: Reset credit grant properly after a disconnect
    
    On a fresh connection, an RPC/RDMA client is supposed to send only
    one RPC Call until it gets a credit grant in the first RPC Reply
    from the server [RFC 8166, Section 3.3.3].
    
    There is a bug in the Linux client's credit accounting mechanism
    introduced by commit e7ce710a8802 ("xprtrdma: Avoid deadlock when
    credit window is reset"). On connect, it simply dumps all pending
    RPC Calls onto the new connection.
    
    Servers have been tolerant of this bad behavior. Currently no server
    implementation ever changes its credit grant over reconnects, and
    servers always repost enough Receives before connections are fully
    established.
    
    To correct this issue, ensure that the client resets both the credit
    grant _and_ the congestion window when handling a reconnect.
    
    Fixes: e7ce710a8802 ("xprtrdma: Avoid deadlock when credit ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Cc: stable@kernel.org
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 143ce2579ba9..98cbc7b060ba 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -468,6 +468,12 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 		xprt->reestablish_timeout = 0;
 	xprt_disconnect_done(xprt);
 	rpcrdma_ep_disconnect(ep, ia);
+
+	/* Prepare @xprt for the next connection by reinitializing
+	 * its credit grant to one (see RFC 8166, Section 3.3.3).
+	 */
+	r_xprt->rx_buf.rb_credits = 1;
+	xprt->cwnd = RPC_CWNDSHIFT;
 }
 
 /**

commit adfa71446dd0943ba376eff3e05c7c89582f8038
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Mon Sep 3 23:58:59 2018 -0400

    SUNRPC: Cleanup: remove the unused 'task' argument from the request_send()
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9ff322e53f37..a5a6a4a353f2 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -693,7 +693,7 @@ xprt_rdma_free(struct rpc_task *task)
 
 /**
  * xprt_rdma_send_request - marshal and send an RPC request
- * @task: RPC task with an RPC message in rq_snd_buf
+ * @rqst: RPC message in rq_snd_buf
  *
  * Caller holds the transport's write lock.
  *
@@ -706,7 +706,7 @@ xprt_rdma_free(struct rpc_task *task)
  *		sent. Do not try to send this message again.
  */
 static int
-xprt_rdma_send_request(struct rpc_rqst *rqst, struct rpc_task *task)
+xprt_rdma_send_request(struct rpc_rqst *rqst)
 {
 	struct rpc_xprt *xprt = rqst->rq_xprt;
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);

commit 75891f502f5fc70f52a01af5b924384ed4866907
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Mon Sep 3 17:37:36 2018 -0400

    SUNRPC: Support for congestion control when queuing is enabled
    
    Both RDMA and UDP transports require the request to get a "congestion control"
    credit before they can be transmitted. Right now, this is done when
    the request locks the socket. We'd like it to happen when a request attempts
    to be transmitted for the first time.
    In order to support retransmission of requests that already hold such
    credits, we also want to ensure that they get queued first, so that we
    don't deadlock with requests that have yet to obtain a credit.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index fa684bf4d090..9ff322e53f37 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -721,6 +721,9 @@ xprt_rdma_send_request(struct rpc_rqst *rqst, struct rpc_task *task)
 	if (!xprt_connected(xprt))
 		goto drop_connection;
 
+	if (!xprt_request_get_cong(xprt, rqst))
+		return -EBADSLT;
+
 	rc = rpcrdma_marshal_req(r_xprt, rqst);
 	if (rc < 0)
 		goto failed_marshal;

commit 50f484e298218b7271fad8a23bd44c82fb3110e1
Author: Trond Myklebust <trond.myklebust@hammerspace.com>
Date:   Thu Aug 30 13:27:29 2018 -0400

    SUNRPC: Treat the task and request as separate in the xprt_ops->send_request()
    
    When we shift to using the transmit queue, then the task that holds the
    write lock will not necessarily be the same as the one being transmitted.
    
    Signed-off-by: Trond Myklebust <trond.myklebust@hammerspace.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 143ce2579ba9..fa684bf4d090 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -706,9 +706,8 @@ xprt_rdma_free(struct rpc_task *task)
  *		sent. Do not try to send this message again.
  */
 static int
-xprt_rdma_send_request(struct rpc_task *task)
+xprt_rdma_send_request(struct rpc_rqst *rqst, struct rpc_task *task)
 {
-	struct rpc_rqst *rqst = task->tk_rqstp;
 	struct rpc_xprt *xprt = rqst->rq_xprt;
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
@@ -741,7 +740,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	/* An RPC with no reply will throw off credit accounting,
 	 * so drop the connection to reset the credit grant.
 	 */
-	if (!rpc_reply_expected(task))
+	if (!rpc_reply_expected(rqst->rq_task))
 		goto drop_connection;
 	return 0;
 

commit 0725d4e1b8b08a60838db3a6e65c23ea8824a048
Merge: 89e255678fec 93b7f7ad2018
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jun 12 10:09:03 2018 -0700

    Merge tag 'nfs-for-4.18-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      Stable fixes:
    
       - Fix a 1-byte stack overflow in nfs_idmap_read_and_verify_message
    
       - Fix a hang due to incorrect error returns in rpcrdma_convert_iovs()
    
       - Revert an incorrect change to the NFSv4.1 callback channel
    
       - Fix a bug in the NFSv4.1 sequence error handling
    
      Features and optimisations:
    
       - Support for piggybacking a LAYOUTGET operation to the OPEN compound
    
       - RDMA performance enhancements to deal with transport congestion
    
       - Add proper SPDX tags for NetApp-contributed RDMA source
    
       - Do not request delegated file attributes (size+change) from the
         server
    
       - Optimise away a GETATTR in the lookup revalidate code when doing
         NFSv4 OPEN
    
       - Optimise away unnecessary lookups for rename targets
    
       - Misc performance improvements when freeing NFSv4 delegations
    
      Bugfixes and cleanups:
    
       - Try to fail quickly if proto=rdma
    
       - Clean up RDMA receive trace points
    
       - Fix sillyrename to return the delegation when appropriate
    
       - Misc attribute revalidation fixes
    
       - Immediately clear the pNFS layout on a file when the server returns
         ESTALE
    
       - Return NFS4ERR_DELAY when delegation/layout recalls fail due to
         igrab()
    
       - Fix the client behaviour on NFS4ERR_SEQ_FALSE_RETRY"
    
    * tag 'nfs-for-4.18-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (80 commits)
      skip LAYOUTRETURN if layout is invalid
      NFSv4.1: Fix the client behaviour on NFS4ERR_SEQ_FALSE_RETRY
      NFSv4: Fix a typo in nfs41_sequence_process
      NFSv4: Revert commit 5f83d86cf531d ("NFSv4.x: Fix wraparound issues..")
      NFSv4: Return NFS4ERR_DELAY when a layout recall fails due to igrab()
      NFSv4: Return NFS4ERR_DELAY when a delegation recall fails due to igrab()
      NFSv4.0: Remove transport protocol name from non-UCS client ID
      NFSv4.0: Remove cl_ipaddr from non-UCS client ID
      NFSv4: Fix a compiler warning when CONFIG_NFS_V4_1 is undefined
      NFS: Filter cache invalidation when holding a delegation
      NFS: Ignore NFS_INO_REVAL_FORCED in nfs_check_inode_attributes()
      NFS: Improve caching while holding a delegation
      NFS: Fix attribute revalidation
      NFS: fix up nfs_setattr_update_inode
      NFSv4: Ensure the inode is clean when we set a delegation
      NFSv4: Ignore NFS_INO_REVAL_FORCED in nfs4_proc_access
      NFSv4: Don't ask for delegated attributes when adding a hard link
      NFSv4: Don't ask for delegated attributes when revalidating the inode
      NFS: Pass the inode down to the getattr() callback
      NFSv4: Don't request size+change attribute if they are delegated to us
      ...

commit bd2abef33394dc16d63580c38c01420db991f0f2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:16 2018 -0400

    svcrdma: Trace key RDMA API events
    
    This includes:
      * Posting on the Send and Receive queues
      * Send, Receive, Read, and Write completion
      * Connect upcalls
      * QP errors
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 3d1b27748aba..caca977e3755 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -51,7 +51,10 @@
 #include <linux/module.h>
 #include <linux/slab.h>
 #include <linux/seq_file.h>
+#include <linux/smp.h>
+
 #include <linux/sunrpc/addr.h>
+#include <linux/sunrpc/svc_rdma.h>
 
 #include "xprt_rdma.h"
 #include <trace/events/rpcrdma.h>

commit b6e717cbf28c8348d34be472f119b0ea82e5e8e7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 7 15:27:05 2018 -0400

    xprtrdma: Prepare RPC/RDMA includes for server-side trace points
    
    Clean up: Move #include <trace/events/rpcrdma.h> into source files,
    similar to how it is done with trace/events/sunrpc.h.
    
    Server-side trace points will be part of the rpcrdma subsystem,
    just like the client-side trace points.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index cc1aad325496..3d1b27748aba 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -54,6 +54,7 @@
 #include <linux/sunrpc/addr.h>
 
 #include "xprt_rdma.h"
+#include <trace/events/rpcrdma.h>
 
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 # define RPCDBG_FACILITY	RPCDBG_TRANS

commit 7c8d9e7c8863905951d4eaa7a8d277150f3a37f7
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:20 2018 -0400

    xprtrdma: Move Receive posting to Receive handler
    
    Receive completion and Reply handling are done by a BOUND
    workqueue, meaning they run on only one CPU.
    
    Posting receives is currently done in the send_request path, which
    on large systems is typically done on a different CPU than the one
    handling Receive completions. This results in movement of
    Receive-related cachelines between the sending and receiving CPUs.
    
    More importantly, it means that currently Receives are posted while
    the transport's write lock is held, which is unnecessary and costly.
    
    Finally, allocation of Receive buffers is performed on-demand in
    the Receive completion handler. This helps guarantee that they are
    allocated on the same NUMA node as the CPU that handles Receive
    completions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 79885aa39c24..0c775f05123c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -722,9 +722,6 @@ xprt_rdma_send_request(struct rpc_task *task)
 	if (rc < 0)
 		goto failed_marshal;
 
-	if (req->rl_reply == NULL) 		/* e.g. reconnection */
-		rpcrdma_recv_buffer_get(req);
-
 	/* Must suppress retransmit to maintain credits */
 	if (rqst->rq_connect_cookie == xprt->connect_cookie)
 		goto drop_connection;

commit edb41e61a54ee75fae31302775e0301fdcb0caaa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:09 2018 -0400

    xprtrdma: Make rpc_rqst part of rpcrdma_req
    
    This simplifies allocation of the generic RPC slot and xprtrdma
    specific per-RPC resources.
    
    It also makes xprtrdma more like the socket-based transports:
    ->buf_alloc and ->buf_free are now responsible only for send and
    receive buffers.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8f9338e98c4f..79885aa39c24 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -331,9 +331,7 @@ xprt_setup_rdma(struct xprt_create *args)
 		return ERR_PTR(-EBADF);
 	}
 
-	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt),
-			xprt_rdma_slot_table_entries,
-			xprt_rdma_slot_table_entries);
+	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt), 0, 0);
 	if (xprt == NULL) {
 		dprintk("RPC:       %s: couldn't allocate rpcrdma_xprt\n",
 			__func__);
@@ -365,7 +363,7 @@ xprt_setup_rdma(struct xprt_create *args)
 		xprt_set_bound(xprt);
 	xprt_rdma_format_addresses(xprt, sap);
 
-	cdata.max_requests = xprt->max_reqs;
+	cdata.max_requests = xprt_rdma_slot_table_entries;
 
 	cdata.rsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA write max */
 	cdata.wsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA read max */
@@ -550,22 +548,18 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 static void
 xprt_rdma_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)
 {
-	struct rpc_rqst *rqst;
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct rpcrdma_req *req;
 
-	spin_lock(&xprt->reserve_lock);
-	if (list_empty(&xprt->free))
+	req = rpcrdma_buffer_get(&r_xprt->rx_buf);
+	if (!req)
 		goto out_sleep;
-	rqst = list_first_entry(&xprt->free, struct rpc_rqst, rq_list);
-	list_del(&rqst->rq_list);
-	spin_unlock(&xprt->reserve_lock);
-
-	task->tk_rqstp = rqst;
+	task->tk_rqstp = &req->rl_slot;
 	task->tk_status = 0;
 	return;
 
 out_sleep:
 	rpc_sleep_on(&xprt->backlog, task, NULL);
-	spin_unlock(&xprt->reserve_lock);
 	task->tk_status = -EAGAIN;
 }
 
@@ -579,11 +573,8 @@ static void
 xprt_rdma_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *rqst)
 {
 	memset(rqst, 0, sizeof(*rqst));
-
-	spin_lock(&xprt->reserve_lock);
-	list_add(&rqst->rq_list, &xprt->free);
+	rpcrdma_buffer_put(rpcr_to_rdmar(rqst));
 	rpc_wake_up_next(&xprt->backlog);
-	spin_unlock(&xprt->reserve_lock);
 }
 
 static bool
@@ -656,13 +647,9 @@ xprt_rdma_allocate(struct rpc_task *task)
 {
 	struct rpc_rqst *rqst = task->tk_rqstp;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
-	struct rpcrdma_req *req;
+	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 	gfp_t flags;
 
-	req = rpcrdma_buffer_get(&r_xprt->rx_buf);
-	if (req == NULL)
-		goto out_get;
-
 	flags = RPCRDMA_DEF_GFP;
 	if (RPC_IS_SWAPPER(task))
 		flags = __GFP_MEMALLOC | GFP_NOWAIT | __GFP_NOWARN;
@@ -672,15 +659,12 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))
 		goto out_fail;
 
-	rpcrdma_set_xprtdata(rqst, req);
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
 	rqst->rq_rbuffer = req->rl_recvbuf->rg_base;
 	trace_xprtrdma_allocate(task, req);
 	return 0;
 
 out_fail:
-	rpcrdma_buffer_put(req);
-out_get:
 	trace_xprtrdma_allocate(task, NULL);
 	return -ENOMEM;
 }
@@ -701,7 +685,6 @@ xprt_rdma_free(struct rpc_task *task)
 	if (test_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags))
 		rpcrdma_release_rqst(r_xprt, req);
 	trace_xprtrdma_rpc_done(task, req);
-	rpcrdma_buffer_put(req);
 }
 
 /**

commit 48be539dd44a3a010a6a330d09610d60ad42758a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:35:04 2018 -0400

    xprtrdma: Introduce ->alloc_slot call-out for xprtrdma
    
    rpcrdma_buffer_get acquires an rpcrdma_req and rep for each RPC.
    Currently this is done in the call_allocate action, and sometimes it
    can fail if there are many outstanding RPCs.
    
    When call_allocate fails, the RPC task is put on the delayq. It is
    awoken a few milliseconds later, but there's no guarantee it will
    get a buffer at that time. The RPC task can be repeatedly put back
    to sleep or even starved.
    
    The call_allocate action should rarely fail. The delayq mechanism is
    not meant to deal with transport congestion.
    
    In the current sunrpc stack, there is a friendlier way to deal with
    this situation. These objects are actually tantamount to an RPC
    slot (rpc_rqst) and there is a separate FSM action, distinct from
    call_allocate, for allocating slot resources. This is the
    call_reserve action.
    
    When allocation fails during this action, the RPC is placed on the
    transport's backlog queue. The backlog mechanism provides a stronger
    guarantee that when the RPC is awoken, a buffer will be available
    for it; and backlogged RPCs are awoken one-at-a-time.
    
    To make slot resource allocation occur in the call_reserve action,
    create special ->alloc_slot and ->free_slot call-outs for xprtrdma.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index cf5e866ee969..8f9338e98c4f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -538,6 +538,54 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 	}
 }
 
+/**
+ * xprt_rdma_alloc_slot - allocate an rpc_rqst
+ * @xprt: controlling RPC transport
+ * @task: RPC task requesting a fresh rpc_rqst
+ *
+ * tk_status values:
+ *	%0 if task->tk_rqstp points to a fresh rpc_rqst
+ *	%-EAGAIN if no rpc_rqst is available; queued on backlog
+ */
+static void
+xprt_rdma_alloc_slot(struct rpc_xprt *xprt, struct rpc_task *task)
+{
+	struct rpc_rqst *rqst;
+
+	spin_lock(&xprt->reserve_lock);
+	if (list_empty(&xprt->free))
+		goto out_sleep;
+	rqst = list_first_entry(&xprt->free, struct rpc_rqst, rq_list);
+	list_del(&rqst->rq_list);
+	spin_unlock(&xprt->reserve_lock);
+
+	task->tk_rqstp = rqst;
+	task->tk_status = 0;
+	return;
+
+out_sleep:
+	rpc_sleep_on(&xprt->backlog, task, NULL);
+	spin_unlock(&xprt->reserve_lock);
+	task->tk_status = -EAGAIN;
+}
+
+/**
+ * xprt_rdma_free_slot - release an rpc_rqst
+ * @xprt: controlling RPC transport
+ * @rqst: rpc_rqst to release
+ *
+ */
+static void
+xprt_rdma_free_slot(struct rpc_xprt *xprt, struct rpc_rqst *rqst)
+{
+	memset(rqst, 0, sizeof(*rqst));
+
+	spin_lock(&xprt->reserve_lock);
+	list_add(&rqst->rq_list, &xprt->free);
+	rpc_wake_up_next(&xprt->backlog);
+	spin_unlock(&xprt->reserve_lock);
+}
+
 static bool
 rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		    size_t size, gfp_t flags)
@@ -780,8 +828,8 @@ xprt_rdma_disable_swap(struct rpc_xprt *xprt)
 static const struct rpc_xprt_ops xprt_rdma_procs = {
 	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong, /* sunrpc/xprt.c */
-	.alloc_slot		= xprt_alloc_slot,
-	.free_slot		= xprt_free_slot,
+	.alloc_slot		= xprt_rdma_alloc_slot,
+	.free_slot		= xprt_rdma_free_slot,
 	.release_request	= xprt_release_rqst_cong,       /* ditto */
 	.set_retrans_timeout	= xprt_set_retrans_timeout_def, /* ditto */
 	.timer			= xprt_rdma_timer,

commit a9cde23ab7cdf5e4e93432dffd0e734267f2b745
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:34:59 2018 -0400

    SUNRPC: Add a ->free_slot transport callout
    
    Refactor: xprtrdma needs to have better control over when RPCs are
    awoken from the backlog queue, so replace xprt_free_slot with a
    transport op callout.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 4717578b4b81..cf5e866ee969 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -781,6 +781,7 @@ static const struct rpc_xprt_ops xprt_rdma_procs = {
 	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong, /* sunrpc/xprt.c */
 	.alloc_slot		= xprt_alloc_slot,
+	.free_slot		= xprt_free_slot,
 	.release_request	= xprt_release_rqst_cong,       /* ditto */
 	.set_retrans_timeout	= xprt_set_retrans_timeout_def, /* ditto */
 	.timer			= xprt_rdma_timer,

commit a2268cfbf599e7f55d4ee68193f08b4f44535fac
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri May 4 15:34:32 2018 -0400

    xprtrdma: Add proper SPDX tags for NetApp-contributed source
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index cc1aad325496..4717578b4b81 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0 OR BSD-3-Clause
 /*
  * Copyright (c) 2014-2017 Oracle.  All rights reserved.
  * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.

commit 78215759e20d859b8f1de7d0aebd08878fbc4eed
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 5 15:13:07 2018 -0500

    SUNRPC: Make RTT measurement more precise (Send)
    
    Some RPC transports have more overhead in their send_request
    callouts than others. For example, for RPC-over-RDMA:
    
    - Marshaling an RPC often has to DMA map the RPC arguments
    
    - Registration methods perform memory registration as part of
      marshaling
    
    To capture just server and network latencies more precisely: when
    sending a Call, capture the rq_xtime timestamp _after_ the transport
    header has been marshaled.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 67e438612c18..cc1aad325496 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -696,6 +696,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	/* Must suppress retransmit to maintain credits */
 	if (rqst->rq_connect_cookie == xprt->connect_cookie)
 		goto drop_connection;
+	rqst->rq_xtime = ktime_get();
 
 	__set_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags);
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))

commit 2dd4a012d9e73c423a8c48d7e0f2e427caecce3d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:31:05 2018 -0500

    xprtrdma: Move creation of rl_rdmabuf to rpcrdma_create_req
    
    Refactor: Both rpcrdma_create_req call sites have to allocate the
    buffer where the transport header is built, so just move that
    allocation into rpcrdma_create_req.
    
    This buffer is a fixed size. There's no needed information available
    in call_allocate that is not also available when the transport is
    created.
    
    The original purpose for allocating these buffers on demand was to
    reduce the possibility that an allocation failure during transport
    creation will hork the mount operation during low memory scenarios.
    Some relief for this rare possibility is coming up in the next few
    patches.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7e39faa90c41..67e438612c18 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -537,29 +537,6 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 	}
 }
 
-/* Allocate a fixed-size buffer in which to construct and send the
- * RPC-over-RDMA header for this request.
- */
-static bool
-rpcrdma_get_rdmabuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
-		    gfp_t flags)
-{
-	size_t size = RPCRDMA_HDRBUF_SIZE;
-	struct rpcrdma_regbuf *rb;
-
-	if (req->rl_rdmabuf)
-		return true;
-
-	rb = rpcrdma_alloc_regbuf(size, DMA_TO_DEVICE, flags);
-	if (IS_ERR(rb))
-		return false;
-
-	r_xprt->rx_stats.hardway_register_count += size;
-	req->rl_rdmabuf = rb;
-	xdr_buf_init(&req->rl_hdrbuf, rb->rg_base, rdmab_length(rb));
-	return true;
-}
-
 static bool
 rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		    size_t size, gfp_t flags)
@@ -641,8 +618,6 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (RPC_IS_SWAPPER(task))
 		flags = __GFP_MEMALLOC | GFP_NOWAIT | __GFP_NOWARN;
 
-	if (!rpcrdma_get_rdmabuf(r_xprt, req, flags))
-		goto out_fail;
 	if (!rpcrdma_get_sendbuf(r_xprt, req, rqst->rq_callsize, flags))
 		goto out_fail;
 	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))

commit fb14ae8853e4f0347950f98e604fa2f4f3b3abe1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:30:54 2018 -0500

    xprtrdma: "Support" call-only RPCs
    
    RPC-over-RDMA version 1 credit accounting relies on there being a
    response message for every RPC Call. This means that RPC procedures
    that have no reply will disrupt credit accounting, just in the same
    way as a retransmit would (since it is sent because no reply has
    arrived). Deal with the "no reply" case the same way.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 08196896953d..7e39faa90c41 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -728,6 +728,12 @@ xprt_rdma_send_request(struct rpc_task *task)
 
 	rqst->rq_xmit_bytes_sent += rqst->rq_snd_buf.len;
 	rqst->rq_bytes_sent = 0;
+
+	/* An RPC with no reply will throw off credit accounting,
+	 * so drop the connection to reset the credit grant.
+	 */
+	if (!rpc_reply_expected(task))
+		goto drop_connection;
 	return 0;
 
 failed_marshal:

commit 9e679d5e7660eb3e75255a4f583d44789ad1b743
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:30:44 2018 -0500

    xprtrdma: ->send_request returns -EAGAIN when there are no free MRs
    
    Currently, when the MR free list is exhausted during marshaling, the
    RPC/RDMA transport places the RPC task on the delayq, which forces a
    wait for HZ >> 2 before the marshal and send is retried.
    
    With this change, the transport now places such an RPC task on the
    pending queue, and wakes it just as soon as more MRs have been
    created. Creating more MRs typically takes less than a millisecond,
    and this waking mechanism is less deadlock-prone.
    
    Moreover, the waiting RPC task is holding the transport's write
    lock, which blocks the transport from sending RPCs. Therefore faster
    recovery from MR exhaustion is desirable.
    
    This is the same mechanism that the TCP transport utilizes when
    handling write buffer space exhaustion.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 47b4604e820a..08196896953d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -689,7 +689,8 @@ xprt_rdma_free(struct rpc_task *task)
  * Returns:
  *	%0 if the RPC message has been sent
  *	%-ENOTCONN if the caller should reconnect and call again
- *	%-ENOBUFS if the caller should call again later
+ *	%-EAGAIN if the caller should call again
+ *	%-ENOBUFS if the caller should call again after a delay
  *	%-EIO if a permanent error occurred and the request was not
  *		sent. Do not try to send this message again.
  */

commit 8a14793e7aa718d16382e18cadec92e2e531e62a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:30:38 2018 -0500

    xprtrdma: Remove xprt-specific connect cookie
    
    Clean up: The generic rq_connect_cookie is sufficient to detect RPC
    Call retransmission.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index f86021e3b853..47b4604e820a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -236,8 +236,6 @@ rpcrdma_connect_worker(struct work_struct *work)
 	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
 
 	spin_lock_bh(&xprt->transport_lock);
-	if (++xprt->connect_cookie == 0)	/* maintain a reserved value */
-		++xprt->connect_cookie;
 	if (ep->rep_connected > 0) {
 		if (!xprt_test_and_set_connected(xprt))
 			xprt_wake_pending_tasks(xprt, 0);
@@ -650,7 +648,6 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))
 		goto out_fail;
 
-	req->rl_connect_cookie = 0;	/* our reserved value */
 	rpcrdma_set_xprtdata(rqst, req);
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
 	rqst->rq_rbuffer = req->rl_recvbuf->rg_base;
@@ -721,9 +718,8 @@ xprt_rdma_send_request(struct rpc_task *task)
 		rpcrdma_recv_buffer_get(req);
 
 	/* Must suppress retransmit to maintain credits */
-	if (req->rl_connect_cookie == xprt->connect_cookie)
+	if (rqst->rq_connect_cookie == xprt->connect_cookie)
 		goto drop_connection;
-	req->rl_connect_cookie = xprt->connect_cookie;
 
 	__set_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags);
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))

commit 6720a89933739cb8dec748cd253f7c8df2c0ae4d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 28 15:30:27 2018 -0500

    xprtrdma: Fix latency regression on NUMA NFS/RDMA clients
    
    With v4.15, on one of my NFS/RDMA clients I measured a nearly
    doubling in the latency of small read and write system calls. There
    was no change in server round trip time. The extra latency appears
    in the whole RPC execution path.
    
    "git bisect" settled on commit ccede7598588 ("xprtrdma: Spread reply
    processing over more CPUs") .
    
    After some experimentation, I found that leaving the WQ bound and
    allowing the scheduler to pick the dispatch CPU seems to eliminate
    the long latencies, and it does not introduce any new regressions.
    
    The fix is implemented by reverting only the part of
    commit ccede7598588 ("xprtrdma: Spread reply processing over more
    CPUs") that dispatches RPC replies specifically on the CPU where the
    matching RPC call was made.
    
    Interestingly, saving the CPU number and later queuing reply
    processing there was effective _only_ for a NFS READ and WRITE
    request. On my NUMA client, in-kernel RPC reply processing for
    asynchronous RPCs was dispatched on the same CPU where the RPC call
    was made, as expected. However synchronous RPCs seem to get their
    reply dispatched on some other CPU than where the call was placed,
    every time.
    
    Fixes: ccede7598588 ("xprtrdma: Spread reply processing over ... ")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Cc: stable@vger.kernel.org # v4.15+
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 4b1ecfe979cf..f86021e3b853 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -52,7 +52,6 @@
 #include <linux/slab.h>
 #include <linux/seq_file.h>
 #include <linux/sunrpc/addr.h>
-#include <linux/smp.h>
 
 #include "xprt_rdma.h"
 
@@ -651,7 +650,6 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))
 		goto out_fail;
 
-	req->rl_cpu = smp_processor_id();
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	rpcrdma_set_xprtdata(rqst, req);
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;

commit 82476d9f955be0dfefadad22a87be6151cd70777
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 3 15:38:25 2018 -0500

    SUNRPC: Trace xprt_timer events
    
    Track RPC timeouts: report the XID and the server address to match
    the content of network capture.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index b90179af88bf..4b1ecfe979cf 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -516,8 +516,6 @@ xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
 static void
 xprt_rdma_timer(struct rpc_xprt *xprt, struct rpc_task *task)
 {
-	dprintk("RPC: %5u %s: xprt = %p\n", task->tk_pid, __func__, xprt);
-
 	xprt_force_disconnect(xprt);
 }
 

commit ae7246762530af00109c3fb8a30031da054c0aa0
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 20 16:31:53 2017 -0500

    xprtrdma: Instrument allocation/release of rpcrdma_req/rep objects
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 25d1160dc085..b90179af88bf 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -640,7 +640,7 @@ xprt_rdma_allocate(struct rpc_task *task)
 
 	req = rpcrdma_buffer_get(&r_xprt->rx_buf);
 	if (req == NULL)
-		return -ENOMEM;
+		goto out_get;
 
 	flags = RPCRDMA_DEF_GFP;
 	if (RPC_IS_SWAPPER(task))
@@ -653,19 +653,18 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))
 		goto out_fail;
 
-	dprintk("RPC: %5u %s: send size = %zd, recv size = %zd, req = %p\n",
-		task->tk_pid, __func__, rqst->rq_callsize,
-		rqst->rq_rcvsize, req);
-
 	req->rl_cpu = smp_processor_id();
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	rpcrdma_set_xprtdata(rqst, req);
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
 	rqst->rq_rbuffer = req->rl_recvbuf->rg_base;
+	trace_xprtrdma_allocate(task, req);
 	return 0;
 
 out_fail:
 	rpcrdma_buffer_put(req);
+out_get:
+	trace_xprtrdma_allocate(task, NULL);
 	return -ENOMEM;
 }
 
@@ -682,10 +681,9 @@ xprt_rdma_free(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 
-	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
-
 	if (test_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags))
 		rpcrdma_release_rqst(r_xprt, req);
+	trace_xprtrdma_rpc_done(task, req);
 	rpcrdma_buffer_put(req);
 }
 

commit b4744e00a39e6213d84a83a86e6d304886316f5f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 20 16:31:29 2017 -0500

    xprtrdma: Add trace points for connect events
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ddf0d87812ef..25d1160dc085 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -259,13 +259,10 @@ xprt_rdma_connect_worker(struct work_struct *work)
 
 	xprt_clear_connected(xprt);
 
-	dprintk("RPC:       %s: %sconnect\n", __func__,
-			r_xprt->rx_ep.rep_connected != 0 ? "re" : "");
 	rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
 	if (rc)
 		xprt_wake_pending_tasks(xprt, rc);
 
-	dprintk("RPC:       %s: exit\n", __func__);
 	xprt_clear_connecting(xprt);
 }
 
@@ -275,7 +272,7 @@ xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
 	struct rpcrdma_xprt *r_xprt = container_of(xprt, struct rpcrdma_xprt,
 						   rx_xprt);
 
-	pr_info("rpcrdma: injecting transport disconnect on xprt=%p\n", xprt);
+	trace_xprtrdma_inject_dsc(r_xprt);
 	rdma_disconnect(r_xprt->rx_ia.ri_id);
 }
 
@@ -295,7 +292,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
-	dprintk("RPC:       %s: called\n", __func__);
+	trace_xprtrdma_destroy(r_xprt);
 
 	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
@@ -306,11 +303,8 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 	rpcrdma_ia_close(&r_xprt->rx_ia);
 
 	xprt_rdma_free_addresses(xprt);
-
 	xprt_free(xprt);
 
-	dprintk("RPC:       %s: returning\n", __func__);
-
 	module_put(THIS_MODULE);
 }
 
@@ -430,6 +424,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	dprintk("RPC:       %s: %s:%s\n", __func__,
 		xprt->address_strings[RPC_DISPLAY_ADDR],
 		xprt->address_strings[RPC_DISPLAY_PORT]);
+	trace_xprtrdma_create(new_xprt);
 	return xprt;
 
 out4:
@@ -440,6 +435,7 @@ xprt_setup_rdma(struct xprt_create *args)
 out2:
 	rpcrdma_ia_close(&new_xprt->rx_ia);
 out1:
+	trace_xprtrdma_destroy(new_xprt);
 	xprt_rdma_free_addresses(xprt);
 	xprt_free(xprt);
 	return ERR_PTR(rc);

commit ce5b3717828356ce2c61e5a2a830df970fc90fb9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:47 2017 -0500

    xprtrdma: Replace all usage of "frmr" with "frwr"
    
    Clean up: Over time, the industry has adopted the term "frwr"
    instead of "frmr". The term "frwr" is now more widely recognized.
    
    For the past couple of years I've attempted to add new code using
    "frwr" , but there still remains plenty of older code that still
    uses "frmr". Replace all usage of "frmr" to avoid confusion.
    
    While we're churning code, rename variables unhelpfully called "f"
    to "frwr", to improve code clarity.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index be8c4e62d3f2..ddf0d87812ef 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -67,7 +67,7 @@
 static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
 unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
-unsigned int xprt_rdma_memreg_strategy		= RPCRDMA_FRMR;
+unsigned int xprt_rdma_memreg_strategy		= RPCRDMA_FRWR;
 int xprt_rdma_pad_optimize;
 
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)

commit cf73daf52750fca4b4af0ca812f542891c228066
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:31 2017 -0500

    xprtrdma: Split xprt_rdma_send_request
    
    Clean up. @rqst is set up differently for backchannel Replies. For
    example, rqst->rq_task and task->tk_client are both NULL. So it is
    easier to understand and maintain this code path if it is separated.
    
    Also, we can get rid of the confusing rl_connect_cookie hack in
    rpcrdma_bc_receive_call.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d0cd6d411b64..be8c4e62d3f2 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -699,22 +699,12 @@ xprt_rdma_free(struct rpc_task *task)
  *
  * Caller holds the transport's write lock.
  *
- * Return values:
- *        0:	The request has been sent
- * ENOTCONN:	Caller needs to invoke connect logic then call again
- *  ENOBUFS:	Call again later to send the request
- *      EIO:	A permanent error occurred. The request was not sent,
- *		and don't try it again
- *
- * send_request invokes the meat of RPC RDMA. It must do the following:
- *
- *  1.  Marshal the RPC request into an RPC RDMA request, which means
- *	putting a header in front of data, and creating IOVs for RDMA
- *	from those in the request.
- *  2.  In marshaling, detect opportunities for RDMA, and use them.
- *  3.  Post a recv message to set up asynch completion, then send
- *	the request (rpcrdma_ep_post).
- *  4.  No partial sends are possible in the RPC-RDMA protocol (as in UDP).
+ * Returns:
+ *	%0 if the RPC message has been sent
+ *	%-ENOTCONN if the caller should reconnect and call again
+ *	%-ENOBUFS if the caller should call again later
+ *	%-EIO if a permanent error occurred and the request was not
+ *		sent. Do not try to send this message again.
  */
 static int
 xprt_rdma_send_request(struct rpc_task *task)
@@ -725,6 +715,11 @@ xprt_rdma_send_request(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	int rc = 0;
 
+#if defined(CONFIG_SUNRPC_BACKCHANNEL)
+	if (unlikely(!rqst->rq_buffer))
+		return xprt_rdma_bc_send_reply(rqst);
+#endif	/* CONFIG_SUNRPC_BACKCHANNEL */
+
 	if (!xprt_connected(xprt))
 		goto drop_connection;
 

commit 6c537f2c7cc06da36f6701be4c9413d7b8b47bfb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:23 2017 -0500

    xprtrdma: buf_free not called for CB replies
    
    Since commit 5a6d1db45569 ("SUNRPC: Add a transport-specific private
    field in rpc_rqst"), the rpc_rqst's for RPC-over-RDMA backchannel
    operations leave rq_buffer set to NULL.
    
    xprt_release does not invoke ->op->buf_free when rq_buffer is NULL.
    The RPCRDMA_REQ_F_BACKCHANNEL check in xprt_rdma_free is therefore
    redundant because xprt_rdma_free is not invoked for backchannel
    requests.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d77dee5a2748..d0cd6d411b64 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -686,9 +686,6 @@ xprt_rdma_free(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 
-	if (test_bit(RPCRDMA_REQ_F_BACKCHANNEL, &req->rl_flags))
-		return;
-
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
 	if (test_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags))

commit a2b6470b1c51dee7be4faf4f6b64803a6fcf637f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:14 2017 -0500

    xprtrdma: Move unmap-safe logic to rpcrdma_marshal_req
    
    Clean up. This logic is related to marshaling the request, and I'd
    like to keep everything that touches req->rl_registered close
    together, for CPU cache efficiency.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index cebcd027d085..d77dee5a2748 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -731,11 +731,6 @@ xprt_rdma_send_request(struct rpc_task *task)
 	if (!xprt_connected(xprt))
 		goto drop_connection;
 
-	/* On retransmit, remove any previously registered chunks */
-	if (unlikely(!list_empty(&req->rl_registered)))
-		r_xprt->rx_ia.ri_ops->ro_unmap_sync(r_xprt,
-						    &req->rl_registered);
-
 	rc = rpcrdma_marshal_req(r_xprt, rqst);
 	if (rc < 0)
 		goto failed_marshal;

commit 20035edf3c349638a679cbc8e861d81fbc104b53
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:57:06 2017 -0500

    xprtrdma: Support IPv6 in xprt_rdma_set_port
    
    Clean up a harmless oversight. xprtrdma's ->set_port method has
    never properly supported IPv6.
    
    This issue has never been a problem because NFS/RDMA mounts have
    always required "port=20049", thus so far, rpcbind is not invoked
    for these mounts.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8ba0aa8c566f..cebcd027d085 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -474,14 +474,34 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 	rpcrdma_ep_disconnect(ep, ia);
 }
 
+/**
+ * xprt_rdma_set_port - update server port with rpcbind result
+ * @xprt: controlling RPC transport
+ * @port: new port value
+ *
+ * Transport connect status is unchanged.
+ */
 static void
 xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
 {
-	struct sockaddr_in *sap;
+	struct sockaddr *sap = (struct sockaddr *)&xprt->addr;
+	char buf[8];
+
+	dprintk("RPC:       %s: setting port for xprt %p (%s:%s) to %u\n",
+		__func__, xprt,
+		xprt->address_strings[RPC_DISPLAY_ADDR],
+		xprt->address_strings[RPC_DISPLAY_PORT],
+		port);
+
+	rpc_set_port(sap, port);
 
-	sap = (struct sockaddr_in *)&xprt->addr;
-	sap->sin_port = htons(port);
-	dprintk("RPC:       %s: %u\n", __func__, port);
+	kfree(xprt->address_strings[RPC_DISPLAY_PORT]);
+	snprintf(buf, sizeof(buf), "%u", port);
+	xprt->address_strings[RPC_DISPLAY_PORT] = kstrdup(buf, GFP_KERNEL);
+
+	kfree(xprt->address_strings[RPC_DISPLAY_HEX_PORT]);
+	snprintf(buf, sizeof(buf), "%4hx", port);
+	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = kstrdup(buf, GFP_KERNEL);
 }
 
 /**

commit dd229cee4ed2617ccddc0937608728cd87c934c2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:58 2017 -0500

    xprtrdma: Remove another sockaddr_storage field (cdata::addr)
    
    Save more space in struct rpcrdma_xprt by removing the redundant
    "addr" field from struct rpcrdma_create_data_internal. Wherever
    we have rpcrdma_xprt, we also have the rpc_xprt, which has a
    sockaddr_storage field with the same content.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index f6d171e15d7d..8ba0aa8c566f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -361,9 +361,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	/*
 	 * Set up RDMA-specific connect data.
 	 */
-
-	sap = (struct sockaddr *)&cdata.addr;
-	memcpy(sap, args->dstaddr, args->addrlen);
+	sap = args->dstaddr;
 
 	/* Ensure xprt->addr holds valid server TCP (not RDMA)
 	 * address, for any side protocols which peek at it */
@@ -397,7 +395,7 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	new_xprt = rpcx_to_rdmax(xprt);
 
-	rc = rpcrdma_ia_open(new_xprt, sap);
+	rc = rpcrdma_ia_open(new_xprt);
 	if (rc)
 		goto out1;
 
@@ -483,8 +481,6 @@ xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
 
 	sap = (struct sockaddr_in *)&xprt->addr;
 	sap->sin_port = htons(port);
-	sap = (struct sockaddr_in *)&rpcx_to_rdmad(xprt).addr;
-	sap->sin_port = htons(port);
 	dprintk("RPC:       %s: %u\n", __func__, port);
 }
 

commit d461f1f2fb91b5629019b3b405528bc88c49f863
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:50 2017 -0500

    xprtrdma: Initialize the xprt address string array earlier
    
    This makes the address strings available for debugging messages in
    earlier stages of transport set up.
    
    The first benefit is to get rid of the single-use rep_remote_addr
    field, saving 128+ bytes in struct rpcrdma_ep.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index dc9000d087a9..f6d171e15d7d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -373,6 +373,7 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	if (rpc_get_port(sap))
 		xprt_set_bound(xprt);
+	xprt_rdma_format_addresses(xprt, sap);
 
 	cdata.max_requests = xprt->max_reqs;
 
@@ -405,7 +406,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	 */
 	new_xprt->rx_data = cdata;
 	new_ep = &new_xprt->rx_ep;
-	new_ep->rep_remote_addr = cdata.addr;
 
 	rc = rpcrdma_ep_create(&new_xprt->rx_ep,
 				&new_xprt->rx_ia, &new_xprt->rx_data);
@@ -419,7 +419,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
 			  xprt_rdma_connect_worker);
 
-	xprt_rdma_format_addresses(xprt, sap);
 	xprt->max_payload = new_xprt->rx_ia.ri_ops->ro_maxpages(new_xprt);
 	if (xprt->max_payload == 0)
 		goto out4;
@@ -437,13 +436,13 @@ xprt_setup_rdma(struct xprt_create *args)
 
 out4:
 	rpcrdma_buffer_destroy(&new_xprt->rx_buf);
-	xprt_rdma_free_addresses(xprt);
 	rc = -ENODEV;
 out3:
 	rpcrdma_ep_destroy(new_ep, &new_xprt->rx_ia);
 out2:
 	rpcrdma_ia_close(&new_xprt->rx_ia);
 out1:
+	xprt_rdma_free_addresses(xprt);
 	xprt_free(xprt);
 	return ERR_PTR(rc);
 }

commit 104927042cde1f86e9f3959ba9c8b4dae1616d69
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:42 2017 -0500

    xprtrdma: Remove unused padding variables
    
    Clean up. Remove fields that should have been removed by
    commit b3221d6a53c4 ("xprtrdma: Remove logic that constructs
    RDMA_MSGP type calls").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7db063f8dd8a..dc9000d087a9 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -67,7 +67,6 @@
 static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
 unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
-static unsigned int xprt_rdma_inline_write_padding;
 unsigned int xprt_rdma_memreg_strategy		= RPCRDMA_FRMR;
 int xprt_rdma_pad_optimize;
 
@@ -81,6 +80,7 @@ static unsigned int zero;
 static unsigned int max_padding = PAGE_SIZE;
 static unsigned int min_memreg = RPCRDMA_BOUNCEBUFFERS;
 static unsigned int max_memreg = RPCRDMA_LAST - 1;
+static unsigned int dummy;
 
 static struct ctl_table_header *sunrpc_table_header;
 
@@ -114,7 +114,7 @@ static struct ctl_table xr_tunables_table[] = {
 	},
 	{
 		.procname	= "rdma_inline_write_padding",
-		.data		= &xprt_rdma_inline_write_padding,
+		.data		= &dummy,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec_minmax,
@@ -387,8 +387,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (cdata.inline_rsize > cdata.rsize)
 		cdata.inline_rsize = cdata.rsize;
 
-	cdata.padding = xprt_rdma_inline_write_padding;
-
 	/*
 	 * Create new transport instance, which includes initialized
 	 *  o ia
@@ -895,8 +893,7 @@ int xprt_rdma_init(void)
 		"\tMaxInlineRead %d\n\tMaxInlineWrite %d\n",
 		xprt_rdma_slot_table_entries,
 		xprt_rdma_max_inline_read, xprt_rdma_max_inline_write);
-	dprintk("\tPadding %d\n\tMemreg %d\n",
-		xprt_rdma_inline_write_padding, xprt_rdma_memreg_strategy);
+	dprintk("\tPadding 0\n\tMemreg %d\n", xprt_rdma_memreg_strategy);
 
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 	if (!sunrpc_table_header)

commit 42b9f5c58aa8c59c91ead0254f0c193e3438b020
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:18 2017 -0500

    xprtrdma: Eliminate unnecessary lock cycle in xprt_rdma_send_request
    
    The rpcrdma_req is not shared yet, and its associated Send hasn't
    been posted, thus RMW should be safe. There's no need for the
    expense of a lock cycle here.
    
    Fixes: 0ba6f37012db ("xprtrdma: Refactor rpcrdma_deferred_completion")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7f9b62822807..7db063f8dd8a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -735,7 +735,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 		goto drop_connection;
 	req->rl_connect_cookie = xprt->connect_cookie;
 
-	set_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags);
+	__set_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags);
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))
 		goto drop_connection;
 

commit 03ac1a76ce5e5a6052a421e1d6a5c97778e88a8c
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Dec 14 20:56:01 2017 -0500

    xprtrdma: Fix buffer leak after transport set up failure
    
    This leak has been around forever, and is exceptionally rare.
    
    EINVAL causes mount to fail with "an incorrect mount option was
    specified" although it's not likely that one of the mount
    options is incorrect. Instead, return ENODEV in this case, as this
    appears to be an issue with system or device configuration rather
    than a specific mount option.
    
    Some obsolete comments are also removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6ee1ad8978f3..7f9b62822807 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -414,20 +414,10 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (rc)
 		goto out2;
 
-	/*
-	 * Allocate pre-registered send and receive buffers for headers and
-	 * any inline data. Also specify any padding which will be provided
-	 * from a preregistered zero buffer.
-	 */
 	rc = rpcrdma_buffer_create(new_xprt);
 	if (rc)
 		goto out3;
 
-	/*
-	 * Register a callback for connection events. This is necessary because
-	 * connection loss notification is async. We also catch connection loss
-	 * when reaping receives.
-	 */
 	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
 			  xprt_rdma_connect_worker);
 
@@ -448,8 +438,9 @@ xprt_setup_rdma(struct xprt_create *args)
 	return xprt;
 
 out4:
+	rpcrdma_buffer_destroy(&new_xprt->rx_buf);
 	xprt_rdma_free_addresses(xprt);
-	rc = -EINVAL;
+	rc = -ENODEV;
 out3:
 	rpcrdma_ep_destroy(new_ep, &new_xprt->rx_ia);
 out2:

commit ccede7598588ae344143f82fb763912535648d58
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Dec 4 14:04:04 2017 -0500

    xprtrdma: Spread reply processing over more CPUs
    
    Commit d8f532d20ee4 ("xprtrdma: Invoke rpcrdma_reply_handler
    directly from RECV completion") introduced a performance regression
    for NFS I/O small enough to not need memory registration. In multi-
    threaded benchmarks that generate primarily small I/O requests,
    IOPS throughput is reduced by nearly a third. This patch restores
    the previous level of throughput.
    
    Because workqueues are typically BOUND (in particular ib_comp_wq,
    nfsiod_workqueue, and rpciod_workqueue), NFS/RDMA workloads tend
    to aggregate on the CPU that is handling Receive completions.
    
    The usual approach to addressing this problem is to create a QP
    and CQ for each CPU, and then schedule transactions on the QP
    for the CPU where you want the transaction to complete. The
    transaction then does not require an extra context switch during
    completion to end up on the same CPU where the transaction was
    started.
    
    This approach doesn't work for the Linux NFS/RDMA client because
    currently the Linux NFS client does not support multiple connections
    per client-server pair, and the RDMA core API does not make it
    straightforward for ULPs to determine which CPU is responsible for
    handling Receive completions for a CQ.
    
    So for the moment, record the CPU number in the rpcrdma_req before
    the transport sends each RPC Call. Then during Receive completion,
    queue the RPC completion on that same CPU.
    
    Additionally, move all RPC completion processing to the deferred
    handler so that even RPCs with simple small replies complete on
    the CPU that sent the corresponding RPC Call.
    
    Fixes: d8f532d20ee4 ("xprtrdma: Invoke rpcrdma_reply_handler ...")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 646c24494ea7..6ee1ad8978f3 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -52,6 +52,7 @@
 #include <linux/slab.h>
 #include <linux/seq_file.h>
 #include <linux/sunrpc/addr.h>
+#include <linux/smp.h>
 
 #include "xprt_rdma.h"
 
@@ -656,6 +657,7 @@ xprt_rdma_allocate(struct rpc_task *task)
 		task->tk_pid, __func__, rqst->rq_callsize,
 		rqst->rq_rcvsize, req);
 
+	req->rl_cpu = smp_processor_id();
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	rpcrdma_set_xprtdata(rqst, req);
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;

commit 62b56a675565a2e40f2cdf50455977448fd87413
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 30 16:22:14 2017 -0400

    xprtrdma: Update copyright notices
    
    Credit work contributed by Oracle engineers since 2014.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9fdd11e4758c..646c24494ea7 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -1,4 +1,5 @@
 /*
+ * Copyright (c) 2014-2017 Oracle.  All rights reserved.
  * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.
  *
  * This software is available to you under a choice of one of two

commit 01bb35c89d90abe6fd1c0be001f84bbdfa7fa7d1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:36 2017 -0400

    xprtrdma: RPC completion should wait for Send completion
    
    When an RPC Call includes a file data payload, that payload can come
    from pages in the page cache, or a user buffer (for direct I/O).
    
    If the payload can fit inline, xprtrdma includes it in the Send
    using a scatter-gather technique. xprtrdma mustn't allow the RPC
    consumer to re-use the memory where that payload resides before the
    Send completes. Otherwise, the new contents of that memory would be
    exposed by an HCA retransmit of the Send operation.
    
    So, block RPC completion on Send completion, but only in the case
    where a separate file data payload is part of the Send. This
    prevents the reuse of that memory while it is still part of a Send
    operation without an undue cost to other cases.
    
    Waiting is avoided in the common case because typically the Send
    will have completed long before the RPC Reply arrives.
    
    These days, an RPC timeout will trigger a disconnect, which tears
    down the QP. The disconnect flushes all waiting Sends. This bounds
    the amount of time the reply handler has to wait for a Send
    completion.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 35aefe201848..9fdd11e4758c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -789,12 +789,13 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   r_xprt->rx_stats.failed_marshal_count,
 		   r_xprt->rx_stats.bad_reply_count,
 		   r_xprt->rx_stats.nomsg_call_count);
-	seq_printf(seq, "%lu %lu %lu %lu %lu\n",
+	seq_printf(seq, "%lu %lu %lu %lu %lu %lu\n",
 		   r_xprt->rx_stats.mrs_recovered,
 		   r_xprt->rx_stats.mrs_orphaned,
 		   r_xprt->rx_stats.mrs_allocated,
 		   r_xprt->rx_stats.local_inv_needed,
-		   r_xprt->rx_stats.empty_sendctx_q);
+		   r_xprt->rx_stats.empty_sendctx_q,
+		   r_xprt->rx_stats.reply_waits_for_send);
 }
 
 static int

commit 0ba6f37012db2f88f881cd818aec6e1886f61abb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:28 2017 -0400

    xprtrdma: Refactor rpcrdma_deferred_completion
    
    Invoke a common routine for releasing hardware resources (for
    example, invalidating MRs). This needs to be done whether an
    RPC Reply has arrived or the RPC was terminated early.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index acdb2e9c72c8..35aefe201848 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -678,15 +678,14 @@ xprt_rdma_free(struct rpc_task *task)
 	struct rpc_rqst *rqst = task->tk_rqstp;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
-	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 
 	if (test_bit(RPCRDMA_REQ_F_BACKCHANNEL, &req->rl_flags))
 		return;
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	if (!list_empty(&req->rl_registered))
-		ia->ri_ops->ro_unmap_sync(r_xprt, &req->rl_registered);
+	if (test_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags))
+		rpcrdma_release_rqst(r_xprt, req);
 	rpcrdma_buffer_put(req);
 }
 
@@ -742,6 +741,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 		goto drop_connection;
 	req->rl_connect_cookie = xprt->connect_cookie;
 
+	set_bit(RPCRDMA_REQ_F_PENDING, &req->rl_flags);
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))
 		goto drop_connection;
 

commit 531cca0c9b17c185377fd081b43ffca953cfecad
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:20 2017 -0400

    xprtrdma: Add a field of bit flags to struct rpcrdma_req
    
    We have one boolean flag in rpcrdma_req today. I'd like to add more
    flags, so convert that boolean to a bit flag.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7be6e2519197..acdb2e9c72c8 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -680,7 +680,7 @@ xprt_rdma_free(struct rpc_task *task)
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 
-	if (req->rl_backchannel)
+	if (test_bit(RPCRDMA_REQ_F_BACKCHANNEL, &req->rl_flags))
 		return;
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);

commit ae72950abf99fb250aca972b3451b6e06a096c68
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Fri Oct 20 10:48:12 2017 -0400

    xprtrdma: Add data structure to manage RDMA Send arguments
    
    Problem statement:
    
    Recently Sagi Grimberg <sagi@grimberg.me> observed that kernel RDMA-
    enabled storage initiators don't handle delayed Send completion
    correctly. If Send completion is delayed beyond the end of a ULP
    transaction, the ULP may release resources that are still being used
    by the HCA to complete a long-running Send operation.
    
    This is a common design trait amongst our initiators. Most Send
    operations are faster than the ULP transaction they are part of.
    Waiting for a completion for these is typically unnecessary.
    
    Infrequently, a network partition or some other problem crops up
    where an ordering problem can occur. In NFS parlance, the RPC Reply
    arrives and completes the RPC, but the HCA is still retrying the
    Send WR that conveyed the RPC Call. In this case, the HCA can try
    to use memory that has been invalidated or DMA unmapped, and the
    connection is lost. If that memory has been re-used for something
    else (possibly not related to NFS), and the Send retransmission
    exposes that data on the wire.
    
    Thus we cannot assume that it is safe to release Send-related
    resources just because a ULP reply has arrived.
    
    After some analysis, we have determined that the completion
    housekeeping will not be difficult for xprtrdma:
    
     - Inline Send buffers are registered via the local DMA key, and
       are already left DMA mapped for the lifetime of a transport
       connection, thus no additional handling is necessary for those
     - Gathered Sends involving page cache pages _will_ need to
       DMA unmap those pages after the Send completes. But like
       inline send buffers, they are registered via the local DMA key,
       and thus will not need to be invalidated
    
    In addition, RPC completion will need to wait for Send completion
    in the latter case. However, nearly always, the Send that conveys
    the RPC Call will have completed long before the RPC Reply
    arrives, and thus no additional latency will be accrued.
    
    Design notes:
    
    In this patch, the rpcrdma_sendctx object is introduced, and a
    lock-free circular queue is added to manage a set of them per
    transport.
    
    The RPC client's send path already prevents sending more than one
    RPC Call at the same time. This allows us to treat the consumer
    side of the queue (rpcrdma_sendctx_get_locked) as if there is a
    single consumer thread.
    
    The producer side of the queue (rpcrdma_sendctx_put_locked) is
    invoked only from the Send completion handler, which is a single
    thread of execution (soft IRQ).
    
    The only care that needs to be taken is with the tail index, which
    is shared between the producer and consumer. Only the producer
    updates the tail index. The consumer compares the head with the
    tail to ensure that the a sendctx that is in use is never handed
    out again (or, expressed more conventionally, the queue is empty).
    
    When the sendctx queue empties completely, there are enough Sends
    outstanding that posting more Send operations can result in a Send
    Queue overflow. In this case, the ULP is told to wait and try again.
    This introduces strong Send Queue accounting to xprtrdma.
    
    As a final touch, Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    suggested a mechanism that does not require signaling every Send.
    We signal once every N Sends, and perform SGE unmapping of N Send
    operations during that one completion.
    
    Reported-by: Sagi Grimberg <sagi@grimberg.me>
    Suggested-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index eb46d2479b09..7be6e2519197 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -687,7 +687,6 @@ xprt_rdma_free(struct rpc_task *task)
 
 	if (!list_empty(&req->rl_registered))
 		ia->ri_ops->ro_unmap_sync(r_xprt, &req->rl_registered);
-	rpcrdma_unmap_sges(ia, req);
 	rpcrdma_buffer_put(req);
 }
 
@@ -790,11 +789,12 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   r_xprt->rx_stats.failed_marshal_count,
 		   r_xprt->rx_stats.bad_reply_count,
 		   r_xprt->rx_stats.nomsg_call_count);
-	seq_printf(seq, "%lu %lu %lu %lu\n",
+	seq_printf(seq, "%lu %lu %lu %lu %lu\n",
 		   r_xprt->rx_stats.mrs_recovered,
 		   r_xprt->rx_stats.mrs_orphaned,
 		   r_xprt->rx_stats.mrs_allocated,
-		   r_xprt->rx_stats.local_inv_needed);
+		   r_xprt->rx_stats.local_inv_needed,
+		   r_xprt->rx_stats.empty_sendctx_q);
 }
 
 static int

commit 4ce6c04c2acef91a10d65a3bcb622654bb01d930
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 9 12:03:34 2017 -0400

    xprtrdma: Use ro_unmap_sync in xprt_rdma_send_request
    
    The "safe" version of ro_unmap is used here to avoid waiting
    unnecessarily. However:
    
     - It is safe to wait. After all, we have to wait anyway when using
       FMR to register memory.
    
     - This case is rare: it occurs only after a reconnect.
    
    By switching this call site to ro_unmap_sync, the final use of
    ro_unmap_safe is removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8cf5ccfe180d..eb46d2479b09 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -728,7 +728,8 @@ xprt_rdma_send_request(struct rpc_task *task)
 
 	/* On retransmit, remove any previously registered chunks */
 	if (unlikely(!list_empty(&req->rl_registered)))
-		r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req, false);
+		r_xprt->rx_ia.ri_ops->ro_unmap_sync(r_xprt,
+						    &req->rl_registered);
 
 	rc = rpcrdma_marshal_req(r_xprt, rqst);
 	if (rc < 0)

commit 8f66b1a529047a972cb9602a919c53a95f3d7a2b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Oct 9 12:03:26 2017 -0400

    xprtrdma: Don't defer fencing an async RPC's chunks
    
    In current kernels, waiting in xprt_release appears to be safe to
    do. I had erroneously believed that for ASYNC RPCs, waiting of any
    kind in xprt_release->xprt_rdma_free would result in deadlock. I've
    done injection testing and consulted with Trond to confirm that
    waiting in the RPC release path is safe.
    
    For the very few times where RPC resources haven't yet been released
    earlier by the reply handler, it is safe to wait synchronously in
    xprt_rdma_free for invalidation rather than defering it to MR
    recovery.
    
    Note: When the QP is error state, posting a LocalInvalidate should
    flush and mark the MR as bad. There is no way the remote HCA can
    access that MR via a QP in error state, so it is effectively already
    inaccessible and thus safe for the Upper Layer to access. The next
    time the MR is used it should be recognized and cleaned up properly
    by frwr_op_map.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c84e2b644e13..8cf5ccfe180d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -686,7 +686,7 @@ xprt_rdma_free(struct rpc_task *task)
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
 	if (!list_empty(&req->rl_registered))
-		ia->ri_ops->ro_unmap_safe(r_xprt, req, !RPC_IS_ASYNC(task));
+		ia->ri_ops->ro_unmap_sync(r_xprt, &req->rl_registered);
 	rpcrdma_unmap_sges(ia, req);
 	rpcrdma_buffer_put(req);
 }

commit 9590d083c1bb1419b7992609d1a0a3e3517d3893
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Aug 23 17:05:58 2017 -0400

    xprtrdma: Use xprt_pin_rqst in rpcrdma_reply_handler
    
    Adopt the use of xprt_pin_rqst to eliminate contention between
    Call-side users of rb_lock and the use of rb_lock in
    rpcrdma_reply_handler.
    
    This replaces the mechanism introduced in 431af645cf66 ("xprtrdma:
    Fix client lock-up after application signal fires").
    
    Use recv_lock to quickly find the completing rqst, pin it, then
    drop the lock. At that point invalidation and pull-up of the Reply
    XDR can be done. Both are often expensive operations.
    
    Finally, take recv_lock again to signal completion to the RPC
    layer. It also protects adjustment of "cwnd".
    
    This greatly reduces the amount of time a lock is held by the
    reply handler. Comparing lock_stat results shows a marked decrease
    in contention on rb_lock and recv_lock.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    [trond.myklebust@primarydata.com: Remove call to rpcrdma_buffer_put() from
       the "out_norqst:" path in rpcrdma_reply_handler.]
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index b680591f6763..c84e2b644e13 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -685,7 +685,6 @@ xprt_rdma_free(struct rpc_task *task)
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	rpcrdma_remove_req(&r_xprt->rx_buf, req);
 	if (!list_empty(&req->rl_registered))
 		ia->ri_ops->ro_unmap_safe(r_xprt, req, !RPC_IS_ASYNC(task));
 	rpcrdma_unmap_sges(ia, req);

commit 7a80f3f0ddf1d7814ac44728f56b6fbba5837703
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Aug 10 12:47:28 2017 -0400

    xprtrdma: Set up an xdr_stream in rpcrdma_marshal_req()
    
    Initialize an xdr_stream at the top of rpcrdma_marshal_req(), and
    use it to encode the fixed transport header fields. This xdr_stream
    will be used to encode the chunk lists in a subsequent patch.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index a43b8280349f..b680591f6763 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -559,6 +559,7 @@ rpcrdma_get_rdmabuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 
 	r_xprt->rx_stats.hardway_register_count += size;
 	req->rl_rdmabuf = rb;
+	xdr_buf_init(&req->rl_hdrbuf, rb->rg_base, rdmab_length(rb));
 	return true;
 }
 

commit 09e60641fc58960c9c63a9b6d1f57b194572dafc
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Aug 10 12:47:12 2017 -0400

    xprtrdma: Clean up rpcrdma_marshal_req() synopsis
    
    Clean up: The caller already has rpcrdma_xprt, so pass that directly
    instead. And provide a documenting comment for this critical
    function.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 42752e4cc996..a43b8280349f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -730,7 +730,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	if (unlikely(!list_empty(&req->rl_registered)))
 		r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req, false);
 
-	rc = rpcrdma_marshal_req(rqst);
+	rc = rpcrdma_marshal_req(r_xprt, rqst);
 	if (rc < 0)
 		goto failed_marshal;
 

commit d31ae2548142b7cd12404929ef3a13ae27c9d961
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Aug 1 12:00:39 2017 -0400

    sunrpc: Const-ify all instances of struct rpc_xprt_ops
    
    After transport instance creation, these function pointers never
    change. Mark them as constant to prevent their use as an attack
    vector for code injections.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d1c458e5ec4d..42752e4cc996 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -149,7 +149,7 @@ static struct ctl_table sunrpc_table[] = {
 
 #endif
 
-static struct rpc_xprt_ops xprt_rdma_procs;	/*forward reference */
+static const struct rpc_xprt_ops xprt_rdma_procs;
 
 static void
 xprt_rdma_format_addresses4(struct rpc_xprt *xprt, struct sockaddr *sap)
@@ -811,7 +811,7 @@ xprt_rdma_disable_swap(struct rpc_xprt *xprt)
  * Plumbing for rpc transport switch and kernel module
  */
 
-static struct rpc_xprt_ops xprt_rdma_procs = {
+static const struct rpc_xprt_ops xprt_rdma_procs = {
 	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong, /* sunrpc/xprt.c */
 	.alloc_slot		= xprt_alloc_slot,

commit 431af645cf662652bc43c7a26f87cb40aedb01d9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 8 11:52:20 2017 -0400

    xprtrdma: Fix client lock-up after application signal fires
    
    After a signal, the RPC client aborts synchronous RPCs running on
    behalf of the signaled application.
    
    The server is still executing those RPCs, and will write the results
    back into the client's memory when it's done. By the time the server
    writes the results, that memory is likely being used for other
    purposes. Therefore xprtrdma has to immediately invalidate all
    memory regions used by those aborted RPCs to prevent the server's
    writes from clobbering that re-used memory.
    
    With FMR memory registration, invalidation takes a relatively long
    time. In fact, the invalidation is often still running when the
    server tries to write the results into the memory regions that are
    being invalidated.
    
    This sets up a race between two processes:
    
    1.  After the signal, xprt_rdma_free calls ro_unmap_safe.
    2.  While ro_unmap_safe is still running, the server replies and
        rpcrdma_reply_handler runs, calling ro_unmap_sync.
    
    Both processes invoke ib_unmap_fmr on the same FMR.
    
    The mlx4 driver allows two ib_unmap_fmr calls on the same FMR at
    the same time, but HCAs generally don't tolerate this. Sometimes
    this can result in a system crash.
    
    If the HCA happens to survive, rpcrdma_reply_handler continues. It
    removes the rpc_rqst from rq_list and releases the transport_lock.
    This enables xprt_rdma_free to run in another process, and the
    rpc_rqst is released while rpcrdma_reply_handler is still waiting
    for the ib_unmap_fmr call to finish.
    
    But further down in rpcrdma_reply_handler, the transport_lock is
    taken again, and "rqst" is dereferenced. If "rqst" has already been
    released, this triggers a general protection fault. Since bottom-
    halves are disabled, the system locks up.
    
    Address both issues by reversing the order of the xprt_lookup_rqst
    call and the ro_unmap_sync call. Introduce a separate lookup
    mechanism for rpcrdma_req's to enable calling ro_unmap_sync before
    xprt_lookup_rqst. Now the handler takes the transport_lock once
    and holds it for the XID lookup and RPC completion.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=305
    Fixes: 68791649a725 ('xprtrdma: Invalidate in the RPC reply ... ')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 62ecbccd9748..d1c458e5ec4d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -684,7 +684,8 @@ xprt_rdma_free(struct rpc_task *task)
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	if (unlikely(!list_empty(&req->rl_registered)))
+	rpcrdma_remove_req(&r_xprt->rx_buf, req);
+	if (!list_empty(&req->rl_registered))
 		ia->ri_ops->ro_unmap_safe(r_xprt, req, !RPC_IS_ASYNC(task));
 	rpcrdma_unmap_sges(ia, req);
 	rpcrdma_buffer_put(req);

commit bebd031866caa404c522e91bb6fd0c69be04c707
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Apr 11 13:23:10 2017 -0400

    xprtrdma: Support unplugging an HCA from under an NFS mount
    
    The device driver for the underlying physical device associated
    with an RPC-over-RDMA transport can be removed while RPC-over-RDMA
    transports are still in use (ie, while NFS filesystems are still
    mounted and active). The IB core performs a connection event upcall
    to request that consumers free all RDMA resources associated with
    a transport.
    
    There may be pending RPCs when this occurs. Care must be taken to
    release associated resources without leaving references that can
    trigger a subsequent crash if a signal or soft timeout occurs. We
    rely on the caller of the transport's ->close method to ensure that
    the previous RPC task has invoked xprt_release but the transport
    remains write-locked.
    
    A DEVICE_REMOVE upcall forces a disconnect then sleeps. When ->close
    is invoked, it destroys the transport's H/W resources, then wakes
    the upcall, which completes and allows the core driver unload to
    continue.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=266
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 83e219d7aba4..62ecbccd9748 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -457,19 +457,33 @@ xprt_setup_rdma(struct xprt_create *args)
 	return ERR_PTR(rc);
 }
 
-/*
- * Close a connection, during shutdown or timeout/reconnect
+/**
+ * xprt_rdma_close - Close down RDMA connection
+ * @xprt: generic transport to be closed
+ *
+ * Called during transport shutdown reconnect, or device
+ * removal. Caller holds the transport's write lock.
  */
 static void
 xprt_rdma_close(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct rpcrdma_ep *ep = &r_xprt->rx_ep;
+	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
+
+	dprintk("RPC:       %s: closing xprt %p\n", __func__, xprt);
 
-	dprintk("RPC:       %s: closing\n", __func__);
-	if (r_xprt->rx_ep.rep_connected > 0)
+	if (test_and_clear_bit(RPCRDMA_IAF_REMOVING, &ia->ri_flags)) {
+		xprt_clear_connected(xprt);
+		rpcrdma_ia_remove(ia);
+		return;
+	}
+	if (ep->rep_connected == -ENODEV)
+		return;
+	if (ep->rep_connected > 0)
 		xprt->reestablish_timeout = 0;
 	xprt_disconnect_done(xprt);
-	rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	rpcrdma_ep_disconnect(ep, ia);
 }
 
 static void
@@ -680,6 +694,8 @@ xprt_rdma_free(struct rpc_task *task)
  * xprt_rdma_send_request - marshal and send an RPC request
  * @task: RPC task with an RPC message in rq_snd_buf
  *
+ * Caller holds the transport's write lock.
+ *
  * Return values:
  *        0:	The request has been sent
  * ENOTCONN:	Caller needs to invoke connect logic then call again
@@ -706,6 +722,9 @@ xprt_rdma_send_request(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	int rc = 0;
 
+	if (!xprt_connected(xprt))
+		goto drop_connection;
+
 	/* On retransmit, remove any previously registered chunks */
 	if (unlikely(!list_empty(&req->rl_registered)))
 		r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req, false);

commit fff09594edf5e9b8595a2cefdc07e54b70f81729
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Apr 11 13:22:54 2017 -0400

    xprtrdma: Refactor rpcrdma_ia_open()
    
    In order to unload a device driver and reload it, xprtrdma will need
    to close a transport's interface adapter, and then call
    rpcrdma_ia_open again, possibly finding a different interface
    adapter.
    
    Make rpcrdma_ia_open safe to call on the same transport multiple
    times.
    
    This is a refactoring change only.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index acf5d81f4d6e..83e219d7aba4 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -66,8 +66,8 @@ static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
 unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_inline_write_padding;
-static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
-		int xprt_rdma_pad_optimize = 0;
+unsigned int xprt_rdma_memreg_strategy		= RPCRDMA_FRMR;
+int xprt_rdma_pad_optimize;
 
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 
@@ -396,7 +396,7 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	new_xprt = rpcx_to_rdmax(xprt);
 
-	rc = rpcrdma_ia_open(new_xprt, sap, xprt_rdma_memreg_strategy);
+	rc = rpcrdma_ia_open(new_xprt, sap);
 	if (rc)
 		goto out1;
 

commit 33849792cbcdae2b04819cfb09fe3dca0a84a11e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Apr 11 13:22:46 2017 -0400

    xprtrdma: Detect unreachable NFS/RDMA servers more reliably
    
    Current NFS clients rely on connection loss to determine when to
    retransmit. In particular, for protocols like NFSv4, clients no
    longer rely on RPC timeouts to drive retransmission: NFSv4 servers
    are required to terminate a connection when they need a client to
    retransmit pending RPCs.
    
    When a server is no longer reachable, either because it has crashed
    or because the network path has broken, the server cannot actively
    terminate a connection. Thus NFS clients depend on transport-level
    keepalive to determine when a connection must be replaced and
    pending RPCs retransmitted.
    
    However, RDMA RC connections do not have a native keepalive
    mechanism. If an NFS/RDMA server crashes after a client has sent
    RPCs successfully (an RC ACK has been received for all OTW RDMA
    requests), there is no way for the client to know the connection is
    moribund.
    
    In addition, new RDMA requests are subject to the RPC-over-RDMA
    credit limit. If the client has consumed all granted credits with
    NFS traffic, it is not allowed to send another RDMA request until
    the server replies. Thus it has no way to send a true keepalive when
    the workload has already consumed all credits with pending RPCs.
    
    To address this, forcibly disconnect a transport when an RPC times
    out. This prevents moribund connections from stopping the
    detection of failover or other configuration changes on the server.
    
    Note that even if the connection is still good, retransmitting
    any RPC will trigger a disconnect thanks to this logic in
    xprt_rdma_send_request:
    
            /* Must suppress retransmit to maintain credits */
            if (req->rl_connect_cookie == xprt->connect_cookie)
                    goto drop_connection;
            req->rl_connect_cookie = xprt->connect_cookie;
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c717f5410776..acf5d81f4d6e 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -484,6 +484,27 @@ xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
 	dprintk("RPC:       %s: %u\n", __func__, port);
 }
 
+/**
+ * xprt_rdma_timer - invoked when an RPC times out
+ * @xprt: controlling RPC transport
+ * @task: RPC task that timed out
+ *
+ * Invoked when the transport is still connected, but an RPC
+ * retransmit timeout occurs.
+ *
+ * Since RDMA connections don't have a keep-alive, forcibly
+ * disconnect and retry to connect. This drives full
+ * detection of the network path, and retransmissions of
+ * all pending RPCs.
+ */
+static void
+xprt_rdma_timer(struct rpc_xprt *xprt, struct rpc_task *task)
+{
+	dprintk("RPC: %5u %s: xprt = %p\n", task->tk_pid, __func__, xprt);
+
+	xprt_force_disconnect(xprt);
+}
+
 static void
 xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 {
@@ -776,6 +797,7 @@ static struct rpc_xprt_ops xprt_rdma_procs = {
 	.alloc_slot		= xprt_alloc_slot,
 	.release_request	= xprt_release_rqst_cong,       /* ditto */
 	.set_retrans_timeout	= xprt_set_retrans_timeout_def, /* ditto */
+	.timer			= xprt_rdma_timer,
 	.rpcbind		= rpcb_getport_async,	/* sunrpc/rpcb_clnt.c */
 	.set_port		= xprt_rdma_set_port,
 	.connect		= xprt_rdma_connect,

commit 18c0fb31a034023e5cb2d3c9c1320d5d47d91afe
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 8 17:00:27 2017 -0500

    xprtrdma: Properly recover FRWRs with in-flight FASTREG WRs
    
    Sriharsha (sriharsha.basavapatna@broadcom.com) reports an occasional
    double DMA unmap of an FRWR MR when a connection is lost. I see one
    way this can happen.
    
    When a request requires more than one segment or chunk,
    rpcrdma_marshal_req loops, invoking ->frwr_op_map for each segment
    (MR) in each chunk. Each call posts a FASTREG Work Request to
    register one MR.
    
    Now suppose that the transport connection is lost part-way through
    marshaling this request. As part of recovering and resetting that
    req, rpcrdma_marshal_req invokes ->frwr_op_unmap_safe, which hands
    all the req's registered FRWRs to the MR recovery thread.
    
    But note: FRWR registration is asynchronous. So it's possible that
    some of these "already registered" FRWRs are fully registered, and
    some are still waiting for their FASTREG WR to complete.
    
    When the connection is lost, the "already registered" frmrs are
    marked FRMR_IS_VALID, and the "still waiting" WRs flush. Then
    frwr_wc_fastreg marks these frmrs FRMR_FLUSHED_FR.
    
    But thanks to ->frwr_op_unmap_safe, the MR recovery thread is doing
    an unreg / alloc_mr, a DMA unmap, and marking each of these frwrs
    FRMR_IS_INVALID, at the same time frwr_wc_fastreg might be running.
    
    - If the recovery thread runs last, then the frmr is marked
    FRMR_IS_INVALID, and life continues.
    
    - If frwr_wc_fastreg runs last, the frmr is marked FRMR_FLUSHED_FR,
    but the recovery thread has already DMA unmapped that MR. When
    ->frwr_op_map later re-uses this frmr, it sees it is not marked
    FRMR_IS_INVALID, and tries to recover it before using it, resulting
    in a second DMA unmap of the same MR.
    
    The fix is to guarantee in-flight FASTREG WRs have flushed before MR
    recovery runs on those FRWRs. Thus we depend on ro_unmap_safe
    (called from xprt_rdma_send_request on retransmit, or from
    xprt_rdma_free) to clean up old registrations as needed.
    
    Reported-by: Sriharsha Basavapatna <sriharsha.basavapatna@broadcom.com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Sriharsha Basavapatna <sriharsha.basavapatna@broadcom.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 699058169cfc..c717f5410776 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -709,10 +709,6 @@ xprt_rdma_send_request(struct rpc_task *task)
 	return 0;
 
 failed_marshal:
-	dprintk("RPC:       %s: rpcrdma_marshal_req failed, status %i\n",
-		__func__, rc);
-	if (rc == -EIO)
-		r_xprt->rx_stats.failed_marshal_count++;
 	if (rc != -ENOTCONN)
 		return rc;
 drop_connection:

commit c95a3c6b88658bcb8f77f85f31a0b9d9036e8016
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Feb 8 17:00:02 2017 -0500

    xprtrdma: Disable pad optimization by default
    
    Commit d5440e27d3e5 ("xprtrdma: Enable pad optimization") made the
    Linux client omit XDR round-up padding in normal Read and Write
    chunks so that the client doesn't have to register and invalidate
    3-byte memory regions that contain no real data.
    
    Unfortunately, my cheery 2014 assessment that this optimization "is
    supported now by both Linux and Solaris servers" was premature.
    We've found bugs in Solaris in this area since commit d5440e27d3e5
    ("xprtrdma: Enable pad optimization") was merged (SYMLINK is the
    main offender).
    
    So for maximum interoperability, I'm disabling this optimization
    again. If a CM private message is exchanged when connecting, the
    client recognizes that the server is Linux, and enables the
    optimization for that connection.
    
    Until now the Solaris server bugs did not impact common operations,
    and were thus largely benign. Soon, less capable devices on Linux
    NFS/RDMA clients will make use of Read chunks more often, and these
    Solaris bugs will prevent interoperation in more cases.
    
    Fixes: 677eb17e94ed ("xprtrdma: Fix XDR tail buffer marshalling")
    Cc: stable@vger.kernel.org # v4.9+
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 534c178d2a7e..699058169cfc 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -67,7 +67,7 @@ unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_inline_write_padding;
 static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
-		int xprt_rdma_pad_optimize = 1;
+		int xprt_rdma_pad_optimize = 0;
 
 #if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 

commit 3a72dc771cc38e4d6e441a86256a3d7788a84c01
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 10:53:37 2016 -0500

    xprtrdma: Relocate connection helper functions
    
    Clean up: Disentangle connection helpers from RPC-over-RDMA reply
    decoding functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 545d3fc8de5a..534c178d2a7e 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -219,6 +219,34 @@ xprt_rdma_free_addresses(struct rpc_xprt *xprt)
 		}
 }
 
+void
+rpcrdma_conn_func(struct rpcrdma_ep *ep)
+{
+	schedule_delayed_work(&ep->rep_connect_worker, 0);
+}
+
+void
+rpcrdma_connect_worker(struct work_struct *work)
+{
+	struct rpcrdma_ep *ep =
+		container_of(work, struct rpcrdma_ep, rep_connect_worker.work);
+	struct rpcrdma_xprt *r_xprt =
+		container_of(ep, struct rpcrdma_xprt, rx_ep);
+	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
+
+	spin_lock_bh(&xprt->transport_lock);
+	if (++xprt->connect_cookie == 0)	/* maintain a reserved value */
+		++xprt->connect_cookie;
+	if (ep->rep_connected > 0) {
+		if (!xprt_test_and_set_connected(xprt))
+			xprt_wake_pending_tasks(xprt, 0);
+	} else {
+		if (xprt_test_and_clear_connected(xprt))
+			xprt_wake_pending_tasks(xprt, -ENOTCONN);
+	}
+	spin_unlock_bh(&xprt->transport_lock);
+}
+
 static void
 xprt_rdma_connect_worker(struct work_struct *work)
 {

commit 48016dce46ad504a378849490bfb99c98be5cfaa
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Nov 29 10:52:48 2016 -0500

    xprtrdma: Avoid calls to ro_unmap_safe()
    
    Micro-optimization: Most of the time, calls to ro_unmap_safe are
    expensive no-ops. Call only when there is work to do.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ed5e285fd2ea..545d3fc8de5a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -621,7 +621,8 @@ xprt_rdma_free(struct rpc_task *task)
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	ia->ri_ops->ro_unmap_safe(r_xprt, req, !RPC_IS_ASYNC(task));
+	if (unlikely(!list_empty(&req->rl_registered)))
+		ia->ri_ops->ro_unmap_safe(r_xprt, req, !RPC_IS_ASYNC(task));
 	rpcrdma_unmap_sges(ia, req);
 	rpcrdma_buffer_put(req);
 }
@@ -657,7 +658,8 @@ xprt_rdma_send_request(struct rpc_task *task)
 	int rc = 0;
 
 	/* On retransmit, remove any previously registered chunks */
-	r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req, false);
+	if (unlikely(!list_empty(&req->rl_registered)))
+		r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req, false);
 
 	rc = rpcrdma_marshal_req(rqst);
 	if (rc < 0)

commit 44829d02d2d7a7064842ecf36239ea24df1cdf58
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:57:32 2016 -0400

    xprtrdma: Support larger inline thresholds
    
    The Version One default inline threshold is still 1KB. But allow
    testing with thresholds up to 64KB.
    
    This maximum is somewhat arbitrary. There's no fundamental
    architectural limit I'm aware of, but it's good to keep the size of
    Receive buffers reasonable. Now that Send can use a s/g list, a
    Send buffer is only as large as each RPC requires. Receive buffers
    are always the size of the inline threshold, however.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6a358ab6ce27..ed5e285fd2ea 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -97,7 +97,7 @@ static struct ctl_table xr_tunables_table[] = {
 		.data		= &xprt_rdma_max_inline_read,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
+		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= &min_inline_size,
 		.extra2		= &max_inline_size,
 	},
@@ -106,7 +106,7 @@ static struct ctl_table xr_tunables_table[] = {
 		.data		= &xprt_rdma_max_inline_write,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= proc_dointvec,
+		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= &min_inline_size,
 		.extra2		= &max_inline_size,
 	},

commit 655fec6987be05964e70c2e2efcbb253710e282f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:57:24 2016 -0400

    xprtrdma: Use gathered Send for large inline messages
    
    An RPC Call message that is sent inline but that has a data payload
    (ie, one or more items in rq_snd_buf's page list) must be "pulled
    up:"
    
    - call_allocate has to reserve enough RPC Call buffer space to
    accommodate the data payload
    
    - call_transmit has to memcopy the rq_snd_buf's page list and tail
    into its head iovec before it is sent
    
    As the inline threshold is increased beyond its current 1KB default,
    however, this means data payloads of more than a few KB are copied
    by the host CPU. For example, if the inline threshold is increased
    just to 4KB, then NFS WRITE requests up to 4KB would involve a
    memcpy of the NFS WRITE's payload data into the RPC Call buffer.
    This is an undesirable amount of participation by the host CPU.
    
    The inline threshold may be much larger than 4KB in the future,
    after negotiation with a peer server.
    
    Instead of copying the components of rq_snd_buf into its head iovec,
    construct a gather list of these components, and send them all in
    place. The same approach is already used in the Linux server's
    RPC-over-RDMA reply path.
    
    This mechanism also eliminates the need for rpcrdma_tail_pullup,
    which is used to manage the XDR pad and trailing inline content when
    a Read list is present.
    
    This requires that the pages in rq_snd_buf's page list be DMA-mapped
    during marshaling, and unmapped when a data-bearing RPC is
    completed. This is slightly less efficient for very small I/O
    payloads, but significantly more efficient as data payload size and
    inline threshold increase past a kilobyte.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7e11d7191208..6a358ab6ce27 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -499,30 +499,21 @@ rpcrdma_get_rdmabuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 	return true;
 }
 
-/* RPC/RDMA marshaling may choose to send payload bearing ops inline,
- * if the resulting Call message is smaller than the inline threshold.
- * The value of the "rq_callsize" argument accounts for RPC header
- * requirements, but not for the data payload in these cases.
- *
- * See rpcrdma_inline_pullup.
- */
 static bool
 rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		    size_t size, gfp_t flags)
 {
 	struct rpcrdma_regbuf *rb;
-	size_t min_size;
 
 	if (req->rl_sendbuf && rdmab_length(req->rl_sendbuf) >= size)
 		return true;
 
-	min_size = max_t(size_t, size, r_xprt->rx_data.inline_wsize);
-	rb = rpcrdma_alloc_regbuf(min_size, DMA_TO_DEVICE, flags);
+	rb = rpcrdma_alloc_regbuf(size, DMA_TO_DEVICE, flags);
 	if (IS_ERR(rb))
 		return false;
 
 	rpcrdma_free_regbuf(req->rl_sendbuf);
-	r_xprt->rx_stats.hardway_register_count += min_size;
+	r_xprt->rx_stats.hardway_register_count += size;
 	req->rl_sendbuf = rb;
 	return true;
 }
@@ -623,14 +614,15 @@ xprt_rdma_free(struct rpc_task *task)
 	struct rpc_rqst *rqst = task->tk_rqstp;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
+	struct rpcrdma_ia *ia = &r_xprt->rx_ia;
 
 	if (req->rl_backchannel)
 		return;
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req,
-					    !RPC_IS_ASYNC(task));
+	ia->ri_ops->ro_unmap_safe(r_xprt, req, !RPC_IS_ASYNC(task));
+	rpcrdma_unmap_sges(ia, req);
 	rpcrdma_buffer_put(req);
 }
 

commit c8b920bb49939a5c6cf1d2d819300f318ea050d2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:57:16 2016 -0400

    xprtrdma: Basic support for Remote Invalidation
    
    Have frwr's ro_unmap_sync recognize an invalidated rkey that appears
    as part of a Receive completion. Local invalidation can be skipped
    for that rkey.
    
    Use an out-of-band signaling mechanism to indicate to the server
    that the client is prepared to receive RDMA Send With Invalidate.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 5adaa1d3d1e7..7e11d7191208 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -730,10 +730,11 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   r_xprt->rx_stats.failed_marshal_count,
 		   r_xprt->rx_stats.bad_reply_count,
 		   r_xprt->rx_stats.nomsg_call_count);
-	seq_printf(seq, "%lu %lu %lu\n",
+	seq_printf(seq, "%lu %lu %lu %lu\n",
 		   r_xprt->rx_stats.mrs_recovered,
 		   r_xprt->rx_stats.mrs_orphaned,
-		   r_xprt->rx_stats.mrs_allocated);
+		   r_xprt->rx_stats.mrs_allocated,
+		   r_xprt->rx_stats.local_inv_needed);
 }
 
 static int

commit 13650c23f10603154d989cff70b5c8a889e69fc2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:26 2016 -0400

    xprtrdma: Eliminate "ia" argument in rpcrdma_{alloc, free}_regbuf
    
    Clean up. The "ia" argument is no longer used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 34246916434b..5adaa1d3d1e7 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -490,7 +490,7 @@ rpcrdma_get_rdmabuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 	if (req->rl_rdmabuf)
 		return true;
 
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, DMA_TO_DEVICE, flags);
+	rb = rpcrdma_alloc_regbuf(size, DMA_TO_DEVICE, flags);
 	if (IS_ERR(rb))
 		return false;
 
@@ -517,12 +517,11 @@ rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		return true;
 
 	min_size = max_t(size_t, size, r_xprt->rx_data.inline_wsize);
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, min_size,
-				  DMA_TO_DEVICE, flags);
+	rb = rpcrdma_alloc_regbuf(min_size, DMA_TO_DEVICE, flags);
 	if (IS_ERR(rb))
 		return false;
 
-	rpcrdma_free_regbuf(&r_xprt->rx_ia, req->rl_sendbuf);
+	rpcrdma_free_regbuf(req->rl_sendbuf);
 	r_xprt->rx_stats.hardway_register_count += min_size;
 	req->rl_sendbuf = rb;
 	return true;
@@ -548,11 +547,11 @@ rpcrdma_get_recvbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 	if (req->rl_recvbuf && rdmab_length(req->rl_recvbuf) >= size)
 		return true;
 
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, DMA_NONE, flags);
+	rb = rpcrdma_alloc_regbuf(size, DMA_NONE, flags);
 	if (IS_ERR(rb))
 		return false;
 
-	rpcrdma_free_regbuf(&r_xprt->rx_ia, req->rl_recvbuf);
+	rpcrdma_free_regbuf(req->rl_recvbuf);
 	r_xprt->rx_stats.hardway_register_count += size;
 	req->rl_recvbuf = rb;
 	return true;

commit 99ef4db329f1ee2413dad49346e72a6c902474d1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:10 2016 -0400

    xprtrdma: Replace DMA_BIDIRECTIONAL
    
    The use of DMA_BIDIRECTIONAL is discouraged by DMA-API.txt.
    Fortunately, xprtrdma now knows which direction I/O is going as
    soon as it allocates each regbuf.
    
    The RPC Call and Reply buffers are no longer the same regbuf. They
    can each be labeled correctly now. The RPC Reply buffer is never
    part of either a Send or Receive WR, but it can be part of Reply
    chunk, which is mapped and registered via ->ro_map . So it is not
    DMA mapped when it is allocated (DMA_NONE), to avoid a double-
    mapping.
    
    Since Receive buffers are no longer DMA_BIDIRECTIONAL and their
    contents are never modified by the host CPU, DMA-API-HOWTO.txt
    suggests that a DMA sync before posting each buffer should be
    unnecessary. (See my_card_interrupt_handler).
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 94dbfd3e89a7..34246916434b 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -490,7 +490,7 @@ rpcrdma_get_rdmabuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 	if (req->rl_rdmabuf)
 		return true;
 
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, flags);
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, DMA_TO_DEVICE, flags);
 	if (IS_ERR(rb))
 		return false;
 
@@ -517,7 +517,8 @@ rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		return true;
 
 	min_size = max_t(size_t, size, r_xprt->rx_data.inline_wsize);
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, min_size, flags);
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, min_size,
+				  DMA_TO_DEVICE, flags);
 	if (IS_ERR(rb))
 		return false;
 
@@ -547,7 +548,7 @@ rpcrdma_get_recvbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 	if (req->rl_recvbuf && rdmab_length(req->rl_recvbuf) >= size)
 		return true;
 
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, flags);
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, DMA_NONE, flags);
 	if (IS_ERR(rb))
 		return false;
 

commit 08cf2efd5423121985af5962d66e6db14dff4130
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:56:02 2016 -0400

    xprtrdma: Use smaller buffers for RPC-over-RDMA headers
    
    Commit 949317464bc2 ("xprtrdma: Limit number of RDMA segments in
    RPC-over-RDMA headers") capped the number of chunks that may appear
    in RPC-over-RDMA headers. The maximum header size can be estimated
    and fixed to avoid allocating buffer space that is never used.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ecdc3ad7dbb6..94dbfd3e89a7 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -484,7 +484,7 @@ static bool
 rpcrdma_get_rdmabuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
 		    gfp_t flags)
 {
-	size_t size = r_xprt->rx_data.inline_wsize;
+	size_t size = RPCRDMA_HDRBUF_SIZE;
 	struct rpcrdma_regbuf *rb;
 
 	if (req->rl_rdmabuf)

commit 9c40c49f145f8999ecbf81683aeb31d92b61b966
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:53 2016 -0400

    xprtrdma: Initialize separate RPC call and reply buffers
    
    RPC-over-RDMA needs to separate its RPC call and reply buffers.
    
     o When an RPC Call is sent, rq_snd_buf is DMA mapped for an RDMA
       Send operation using DMA_TO_DEVICE
    
     o If the client expects a large RPC reply, it DMA maps rq_rcv_buf
       as part of a Reply chunk using DMA_FROM_DEVICE
    
    The two mappings are for data movement in opposite directions.
    
    DMA-API.txt suggests that if these mappings share a DMA cacheline,
    bad things can happen. This could occur in the final bytes of
    rq_snd_buf and the first bytes of rq_rcv_buf if the two buffers
    happen to share a DMA cacheline.
    
    On x86_64 the cacheline size is typically 8 bytes, and RPC call
    messages are usually much smaller than the send buffer, so this
    hasn't been a noticeable problem. But the DMA cacheline size can be
    larger on other platforms.
    
    Also, often rq_rcv_buf starts most of the way into a page, thus
    an additional RDMA segment is needed to map and register the end of
    that buffer. Try to avoid that scenario to reduce the cost of
    registering and invalidating Reply chunks.
    
    Instead of carrying a single regbuf that covers both rq_snd_buf and
    rq_rcv_buf, each struct rpcrdma_req now carries one regbuf for
    rq_snd_buf and one regbuf for rq_rcv_buf.
    
    Some incidental changes worth noting:
    
    - To clear out some spaghetti, refactor xprt_rdma_allocate.
    - The value stored in rg_size is the same as the value stored in
      the iov.length field, so eliminate rg_size
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d83bffa92dfc..ecdc3ad7dbb6 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -477,6 +477,86 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 	}
 }
 
+/* Allocate a fixed-size buffer in which to construct and send the
+ * RPC-over-RDMA header for this request.
+ */
+static bool
+rpcrdma_get_rdmabuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
+		    gfp_t flags)
+{
+	size_t size = r_xprt->rx_data.inline_wsize;
+	struct rpcrdma_regbuf *rb;
+
+	if (req->rl_rdmabuf)
+		return true;
+
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, flags);
+	if (IS_ERR(rb))
+		return false;
+
+	r_xprt->rx_stats.hardway_register_count += size;
+	req->rl_rdmabuf = rb;
+	return true;
+}
+
+/* RPC/RDMA marshaling may choose to send payload bearing ops inline,
+ * if the resulting Call message is smaller than the inline threshold.
+ * The value of the "rq_callsize" argument accounts for RPC header
+ * requirements, but not for the data payload in these cases.
+ *
+ * See rpcrdma_inline_pullup.
+ */
+static bool
+rpcrdma_get_sendbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
+		    size_t size, gfp_t flags)
+{
+	struct rpcrdma_regbuf *rb;
+	size_t min_size;
+
+	if (req->rl_sendbuf && rdmab_length(req->rl_sendbuf) >= size)
+		return true;
+
+	min_size = max_t(size_t, size, r_xprt->rx_data.inline_wsize);
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, min_size, flags);
+	if (IS_ERR(rb))
+		return false;
+
+	rpcrdma_free_regbuf(&r_xprt->rx_ia, req->rl_sendbuf);
+	r_xprt->rx_stats.hardway_register_count += min_size;
+	req->rl_sendbuf = rb;
+	return true;
+}
+
+/* The rq_rcv_buf is used only if a Reply chunk is necessary.
+ * The decision to use a Reply chunk is made later in
+ * rpcrdma_marshal_req. This buffer is registered at that time.
+ *
+ * Otherwise, the associated RPC Reply arrives in a separate
+ * Receive buffer, arbitrarily chosen by the HCA. The buffer
+ * allocated here for the RPC Reply is not utilized in that
+ * case. See rpcrdma_inline_fixup.
+ *
+ * A regbuf is used here to remember the buffer size.
+ */
+static bool
+rpcrdma_get_recvbuf(struct rpcrdma_xprt *r_xprt, struct rpcrdma_req *req,
+		    size_t size, gfp_t flags)
+{
+	struct rpcrdma_regbuf *rb;
+
+	if (req->rl_recvbuf && rdmab_length(req->rl_recvbuf) >= size)
+		return true;
+
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, flags);
+	if (IS_ERR(rb))
+		return false;
+
+	rpcrdma_free_regbuf(&r_xprt->rx_ia, req->rl_recvbuf);
+	r_xprt->rx_stats.hardway_register_count += size;
+	req->rl_recvbuf = rb;
+	return true;
+}
+
 /**
  * xprt_rdma_allocate - allocate transport resources for an RPC
  * @task: RPC task
@@ -487,22 +567,18 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
  *      EIO:	A permanent error occurred, do not retry
  *
  * The RDMA allocate/free functions need the task structure as a place
- * to hide the struct rpcrdma_req, which is necessary for the actual send/recv
- * sequence.
+ * to hide the struct rpcrdma_req, which is necessary for the actual
+ * send/recv sequence.
  *
- * The RPC layer allocates both send and receive buffers in the same call
- * (rq_send_buf and rq_rcv_buf are both part of a single contiguous buffer).
- * We may register rq_rcv_buf when using reply chunks.
+ * xprt_rdma_allocate provides buffers that are already mapped for
+ * DMA, and a local DMA lkey is provided for each.
  */
 static int
 xprt_rdma_allocate(struct rpc_task *task)
 {
 	struct rpc_rqst *rqst = task->tk_rqstp;
-	size_t size = rqst->rq_callsize + rqst->rq_rcvsize;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
-	struct rpcrdma_regbuf *rb;
 	struct rpcrdma_req *req;
-	size_t min_size;
 	gfp_t flags;
 
 	req = rpcrdma_buffer_get(&r_xprt->rx_buf);
@@ -513,59 +589,23 @@ xprt_rdma_allocate(struct rpc_task *task)
 	if (RPC_IS_SWAPPER(task))
 		flags = __GFP_MEMALLOC | GFP_NOWAIT | __GFP_NOWARN;
 
-	if (req->rl_rdmabuf == NULL)
-		goto out_rdmabuf;
-	if (req->rl_sendbuf == NULL)
-		goto out_sendbuf;
-	if (size > req->rl_sendbuf->rg_size)
-		goto out_sendbuf;
+	if (!rpcrdma_get_rdmabuf(r_xprt, req, flags))
+		goto out_fail;
+	if (!rpcrdma_get_sendbuf(r_xprt, req, rqst->rq_callsize, flags))
+		goto out_fail;
+	if (!rpcrdma_get_recvbuf(r_xprt, req, rqst->rq_rcvsize, flags))
+		goto out_fail;
+
+	dprintk("RPC: %5u %s: send size = %zd, recv size = %zd, req = %p\n",
+		task->tk_pid, __func__, rqst->rq_callsize,
+		rqst->rq_rcvsize, req);
 
-out:
-	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	rpcrdma_set_xprtdata(rqst, req);
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
-	rqst->rq_rbuffer = (char *)rqst->rq_buffer + rqst->rq_rcvsize;
+	rqst->rq_rbuffer = req->rl_recvbuf->rg_base;
 	return 0;
 
-out_rdmabuf:
-	min_size = r_xprt->rx_data.inline_wsize;
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, min_size, flags);
-	if (IS_ERR(rb))
-		goto out_fail;
-	req->rl_rdmabuf = rb;
-
-out_sendbuf:
-	/* XDR encoding and RPC/RDMA marshaling of this request has not
-	 * yet occurred. Thus a lower bound is needed to prevent buffer
-	 * overrun during marshaling.
-	 *
-	 * RPC/RDMA marshaling may choose to send payload bearing ops
-	 * inline, if the result is smaller than the inline threshold.
-	 * The value of the "size" argument accounts for header
-	 * requirements but not for the payload in these cases.
-	 *
-	 * Likewise, allocate enough space to receive a reply up to the
-	 * size of the inline threshold.
-	 *
-	 * It's unlikely that both the send header and the received
-	 * reply will be large, but slush is provided here to allow
-	 * flexibility when marshaling.
-	 */
-	min_size = r_xprt->rx_data.inline_rsize;
-	min_size += r_xprt->rx_data.inline_wsize;
-	if (size < min_size)
-		size = min_size;
-
-	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, flags);
-	if (IS_ERR(rb))
-		goto out_fail;
-
-	r_xprt->rx_stats.hardway_register_count += size;
-	rpcrdma_free_regbuf(&r_xprt->rx_ia, req->rl_sendbuf);
-	req->rl_sendbuf = rb;
-	goto out;
-
 out_fail:
 	rpcrdma_buffer_put(req);
 	return -ENOMEM;

commit 5a6d1db4556940533f1a5b6521e522f3e46508ed
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:45 2016 -0400

    SUNRPC: Add a transport-specific private field in rpc_rqst
    
    Currently there's a hidden and indirect mechanism for finding the
    rpcrdma_req that goes with an rpc_rqst. It depends on getting from
    the rq_buffer pointer in struct rpc_rqst to the struct
    rpcrdma_regbuf that controls that buffer, and then to the struct
    rpcrdma_req it goes with.
    
    This was done back in the day to avoid the need to add a per-rqst
    pointer or to alter the buf_free API when support for RPC-over-RDMA
    was introduced.
    
    I'm about to change the way regbuf's work to support larger inline
    thresholds. Now is a good time to replace this indirect mechanism
    with something that is more straightforward. I guess this should be
    considered a clean up.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 136caf3dd299..d83bffa92dfc 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -523,6 +523,7 @@ xprt_rdma_allocate(struct rpc_task *task)
 out:
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 	req->rl_connect_cookie = 0;	/* our reserved value */
+	rpcrdma_set_xprtdata(rqst, req);
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
 	rqst->rq_rbuffer = (char *)rqst->rq_buffer + rqst->rq_rcvsize;
 	return 0;
@@ -559,7 +560,6 @@ xprt_rdma_allocate(struct rpc_task *task)
 	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, flags);
 	if (IS_ERR(rb))
 		goto out_fail;
-	rb->rg_owner = req;
 
 	r_xprt->rx_stats.hardway_register_count += size;
 	rpcrdma_free_regbuf(&r_xprt->rx_ia, req->rl_sendbuf);

commit 68778945e46f143ed7974b427a8065f69a4ce944
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:37 2016 -0400

    SUNRPC: Separate buffer pointers for RPC Call and Reply messages
    
    For xprtrdma, the RPC Call and Reply buffers are involved in real
    I/O operations.
    
    To start with, the DMA direction of the I/O for a Call is opposite
    that of a Reply.
    
    In the current arrangement, the Reply buffer address is on a
    four-byte alignment just past the call buffer. Would be friendlier
    on some platforms if that was at a DMA cache alignment instead.
    
    Because the current arrangement allocates a single memory region
    which contains both buffers, the RPC Reply buffer often contains a
    page boundary in it when the Call buffer is large enough (which is
    frequent).
    
    It would be a little nicer for setting up DMA operations (and
    possible registration of the Reply buffer) if the two buffers were
    separated, well-aligned, and contained as few page boundaries as
    possible.
    
    Now, I could just pad out the single memory region used for the pair
    of buffers. But frequently that would mean a lot of unused space to
    ensure the Reply buffer did not have a page boundary.
    
    Add a separate pointer to rpc_rqst that points right to the RPC
    Reply buffer. This makes no difference to xprtsock, but it will help
    xprtrdma in subsequent patches.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ebf14ba437c6..136caf3dd299 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -524,6 +524,7 @@ xprt_rdma_allocate(struct rpc_task *task)
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
+	rqst->rq_rbuffer = (char *)rqst->rq_buffer + rqst->rq_rcvsize;
 	return 0;
 
 out_rdmabuf:

commit 3435c74aed2d7b743ccbf34616c523ebee7be943
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:29 2016 -0400

    SUNRPC: Generalize the RPC buffer release API
    
    xprtrdma needs to allocate the Call and Reply buffers separately.
    TBH, the reliance on using a single buffer for the pair of XDR
    buffers is transport implementation-specific.
    
    Instead of passing just the rq_buffer into the buf_free method, pass
    the task structure and let buf_free take care of freeing both
    XDR buffers at once.
    
    There's a micro-optimization here. In the common case, both
    xprt_release and the transport's buf_free method were checking if
    rq_buffer was NULL. Now the check is done only once per RPC.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index daa7d4d43fd8..ebf14ba437c6 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -523,7 +523,6 @@ xprt_rdma_allocate(struct rpc_task *task)
 out:
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 	req->rl_connect_cookie = 0;	/* our reserved value */
-	req->rl_task = task;
 	rqst->rq_buffer = req->rl_sendbuf->rg_base;
 	return 0;
 
@@ -571,31 +570,26 @@ xprt_rdma_allocate(struct rpc_task *task)
 	return -ENOMEM;
 }
 
-/*
- * This function returns all RDMA resources to the pool.
+/**
+ * xprt_rdma_free - release resources allocated by xprt_rdma_allocate
+ * @task: RPC task
+ *
+ * Caller guarantees rqst->rq_buffer is non-NULL.
  */
 static void
-xprt_rdma_free(void *buffer)
+xprt_rdma_free(struct rpc_task *task)
 {
-	struct rpcrdma_req *req;
-	struct rpcrdma_xprt *r_xprt;
-	struct rpcrdma_regbuf *rb;
-
-	if (buffer == NULL)
-		return;
+	struct rpc_rqst *rqst = task->tk_rqstp;
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
+	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 
-	rb = container_of(buffer, struct rpcrdma_regbuf, rg_base[0]);
-	req = rb->rg_owner;
 	if (req->rl_backchannel)
 		return;
 
-	r_xprt = container_of(req->rl_buffer, struct rpcrdma_xprt, rx_buf);
-
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
 	r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req,
-					    !RPC_IS_ASYNC(req->rl_task));
-
+					    !RPC_IS_ASYNC(task));
 	rpcrdma_buffer_put(req);
 }
 

commit 5fe6eaa1f9a00b9a5927e3b791ecad2f3eaab130
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:20 2016 -0400

    SUNRPC: Generalize the RPC buffer allocation API
    
    xprtrdma needs to allocate the Call and Reply buffers separately.
    TBH, the reliance on using a single buffer for the pair of XDR
    buffers is transport implementation-specific.
    
    Transports that want to allocate separate Call and Reply buffers
    will ignore the "size" argument anyway.  Don't bother passing it.
    
    The buf_alloc method can't return two pointers. Instead, make the
    method's return value an error code, and set the rq_buffer pointer
    in the method itself.
    
    This gives call_allocate an opportunity to terminate an RPC instead
    of looping forever when a permanent problem occurs. If a request is
    just bogus, or the transport is in a state where it can't allocate
    resources for any request, there needs to be a way to kill the RPC
    right there and not loop.
    
    This immediately fixes a rare problem in the backchannel send path,
    which loops if the server happens to send a CB request whose
    call+reply size is larger than a page (which it shouldn't do yet).
    
    One more issue: looks like xprt_inject_disconnect was incorrectly
    placed in the failure path in call_allocate. It needs to be in the
    success path, as it is for other call-sites.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index be95eced0726..daa7d4d43fd8 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -477,7 +477,15 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 	}
 }
 
-/*
+/**
+ * xprt_rdma_allocate - allocate transport resources for an RPC
+ * @task: RPC task
+ *
+ * Return values:
+ *        0:	Success; rq_buffer points to RPC buffer to use
+ *   ENOMEM:	Out of memory, call again later
+ *      EIO:	A permanent error occurred, do not retry
+ *
  * The RDMA allocate/free functions need the task structure as a place
  * to hide the struct rpcrdma_req, which is necessary for the actual send/recv
  * sequence.
@@ -486,11 +494,12 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
  * (rq_send_buf and rq_rcv_buf are both part of a single contiguous buffer).
  * We may register rq_rcv_buf when using reply chunks.
  */
-static void *
-xprt_rdma_allocate(struct rpc_task *task, size_t size)
+static int
+xprt_rdma_allocate(struct rpc_task *task)
 {
-	struct rpc_xprt *xprt = task->tk_rqstp->rq_xprt;
-	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct rpc_rqst *rqst = task->tk_rqstp;
+	size_t size = rqst->rq_callsize + rqst->rq_rcvsize;
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(rqst->rq_xprt);
 	struct rpcrdma_regbuf *rb;
 	struct rpcrdma_req *req;
 	size_t min_size;
@@ -498,7 +507,7 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 
 	req = rpcrdma_buffer_get(&r_xprt->rx_buf);
 	if (req == NULL)
-		return NULL;
+		return -ENOMEM;
 
 	flags = RPCRDMA_DEF_GFP;
 	if (RPC_IS_SWAPPER(task))
@@ -515,7 +524,8 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	req->rl_task = task;
-	return req->rl_sendbuf->rg_base;
+	rqst->rq_buffer = req->rl_sendbuf->rg_base;
+	return 0;
 
 out_rdmabuf:
 	min_size = r_xprt->rx_data.inline_wsize;
@@ -558,7 +568,7 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 
 out_fail:
 	rpcrdma_buffer_put(req);
-	return NULL;
+	return -ENOMEM;
 }
 
 /*

commit eb342e9a38a5ad79866fec2df2d3ca4592bc501b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Sep 15 10:55:04 2016 -0400

    xprtrdma: Eliminate INLINE_THRESHOLD macros
    
    Clean up: r_xprt is already available everywhere these macros are
    invoked, so just dereference that directly.
    
    RPCRDMA_INLINE_PAD_VALUE is no longer used, so it can simply be
    removed.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 81f0e879f019..be95eced0726 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -518,7 +518,7 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	return req->rl_sendbuf->rg_base;
 
 out_rdmabuf:
-	min_size = RPCRDMA_INLINE_WRITE_THRESHOLD(task->tk_rqstp);
+	min_size = r_xprt->rx_data.inline_wsize;
 	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, min_size, flags);
 	if (IS_ERR(rb))
 		goto out_fail;
@@ -541,8 +541,8 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	 * reply will be large, but slush is provided here to allow
 	 * flexibility when marshaling.
 	 */
-	min_size = RPCRDMA_INLINE_READ_THRESHOLD(task->tk_rqstp);
-	min_size += RPCRDMA_INLINE_WRITE_THRESHOLD(task->tk_rqstp);
+	min_size = r_xprt->rx_data.inline_rsize;
+	min_size += r_xprt->rx_data.inline_wsize;
 	if (size < min_size)
 		size = min_size;
 

commit 9d6b0409788287b64d8401ffba2ce11a5a86a879
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:54:16 2016 -0400

    xprtrdma: Place registered MWs on a per-req list
    
    Instead of placing registered MWs sparsely into the rl_segments
    array, place these MWs on a per-req list.
    
    ro_unmap_{sync,safe} can then simply pull those MWs off the list
    instead of walking through the array.
    
    This change significantly reduces the size of struct rpcrdma_req
    by removing nsegs and rl_mw from every array element.
    
    As an additional clean-up, chunk co-ordinates are returned in the
    "*mw" output argument so they are no longer needed in every
    array element.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index b1dd42a93484..81f0e879f019 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -619,6 +619,9 @@ xprt_rdma_send_request(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	int rc = 0;
 
+	/* On retransmit, remove any previously registered chunks */
+	r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req, false);
+
 	rc = rpcrdma_marshal_req(rqst);
 	if (rc < 0)
 		goto failed_marshal;

commit e2ac236c0b65129f12fef358390f76cc3cacb865
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:54:00 2016 -0400

    xprtrdma: Allocate MRs on demand
    
    Frequent MR list exhaustion can impact I/O throughput, so enough MRs
    are always created during transport set-up to prevent running out.
    This means more MRs are created than most workloads need.
    
    Commit 94f58c58c0b4 ("xprtrdma: Allow Read list and Reply chunk
    simultaneously") introduced support for sending two chunk lists per
    RPC, which consumes more MRs per RPC.
    
    Instead of trying to provision more MRs, introduce a mechanism for
    allocating MRs on demand. A few MRs are allocated during transport
    set-up to kick things off.
    
    This significantly reduces the average number of MRs per transport
    while allowing the MR count to grow for workloads or devices that
    need more MRs.
    
    FRWR with mlx4 allocated almost 400 MRs per transport before this
    patch. Now it starts with 32.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index be4dd2c7c680..b1dd42a93484 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -682,9 +682,10 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   r_xprt->rx_stats.failed_marshal_count,
 		   r_xprt->rx_stats.bad_reply_count,
 		   r_xprt->rx_stats.nomsg_call_count);
-	seq_printf(seq, "%lu %lu\n",
+	seq_printf(seq, "%lu %lu %lu\n",
 		   r_xprt->rx_stats.mrs_recovered,
-		   r_xprt->rx_stats.mrs_orphaned);
+		   r_xprt->rx_stats.mrs_orphaned,
+		   r_xprt->rx_stats.mrs_allocated);
 }
 
 static int

commit 7a89f9c626e337ba6528d8a2829b228c933877fb
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:53:43 2016 -0400

    xprtrdma: Honor ->send_request API contract
    
    Commit c93c62231cf5 ("xprtrdma: Disconnect on registration failure")
    added a disconnect for some RPC marshaling failures. This is needed
    only in a handful of cases, but it was triggering for simple stuff
    like temporary resource shortages. Try to straighten this out.
    
    Fix up the lower layers so they don't return -ENOMEM or other error
    codes that the RPC client's FSM doesn't explicitly recognize.
    
    Also fix up the places in the send_request path that do want a
    disconnect. For example, when ib_post_send or ib_post_recv fail,
    this is a sign that there is a send or receive queue resource
    miscalculation. That should be rare, and is a sign of a software
    bug. But xprtrdma can recover: disconnect to reset the transport and
    start over.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 4c8e7f11b906..be4dd2c7c680 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -558,7 +558,6 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 
 out_fail:
 	rpcrdma_buffer_put(req);
-	r_xprt->rx_stats.failed_marshal_count++;
 	return NULL;
 }
 
@@ -590,8 +589,19 @@ xprt_rdma_free(void *buffer)
 	rpcrdma_buffer_put(req);
 }
 
-/*
+/**
+ * xprt_rdma_send_request - marshal and send an RPC request
+ * @task: RPC task with an RPC message in rq_snd_buf
+ *
+ * Return values:
+ *        0:	The request has been sent
+ * ENOTCONN:	Caller needs to invoke connect logic then call again
+ *  ENOBUFS:	Call again later to send the request
+ *      EIO:	A permanent error occurred. The request was not sent,
+ *		and don't try it again
+ *
  * send_request invokes the meat of RPC RDMA. It must do the following:
+ *
  *  1.  Marshal the RPC request into an RPC RDMA request, which means
  *	putting a header in front of data, and creating IOVs for RDMA
  *	from those in the request.
@@ -600,7 +610,6 @@ xprt_rdma_free(void *buffer)
  *	the request (rpcrdma_ep_post).
  *  4.  No partial sends are possible in the RPC-RDMA protocol (as in UDP).
  */
-
 static int
 xprt_rdma_send_request(struct rpc_task *task)
 {
@@ -630,11 +639,12 @@ xprt_rdma_send_request(struct rpc_task *task)
 	return 0;
 
 failed_marshal:
-	r_xprt->rx_stats.failed_marshal_count++;
 	dprintk("RPC:       %s: rpcrdma_marshal_req failed, status %i\n",
 		__func__, rc);
 	if (rc == -EIO)
-		return -EIO;
+		r_xprt->rx_stats.failed_marshal_count++;
+	if (rc != -ENOTCONN)
+		return rc;
 drop_connection:
 	xprt_disconnect_done(xprt);
 	return -ENOTCONN;	/* implies disconnect */

commit 505bbe64dd04b105c1377703252758ac56f92485
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jun 29 13:52:54 2016 -0400

    xprtrdma: Refactor MR recovery work queues
    
    I found that commit ead3f26e359e ("xprtrdma: Add ro_unmap_safe
    memreg method"), which introduces ro_unmap_safe, never wired up the
    FMR recovery worker.
    
    The FMR and FRWR recovery work queues both do the same thing.
    Instead of setting up separate individual work queues for this,
    schedule a delayed worker to deal with them, since recovering MRs is
    not performance-critical.
    
    Fixes: ead3f26e359e ("xprtrdma: Add ro_unmap_safe memreg method")
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 99d2e5b72726..4c8e7f11b906 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -660,7 +660,7 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   xprt->stat.bad_xids,
 		   xprt->stat.req_u,
 		   xprt->stat.bklog_u);
-	seq_printf(seq, "%lu %lu %lu %llu %llu %llu %llu %lu %lu %lu %lu\n",
+	seq_printf(seq, "%lu %lu %lu %llu %llu %llu %llu %lu %lu %lu %lu ",
 		   r_xprt->rx_stats.read_chunk_count,
 		   r_xprt->rx_stats.write_chunk_count,
 		   r_xprt->rx_stats.reply_chunk_count,
@@ -672,6 +672,9 @@ void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   r_xprt->rx_stats.failed_marshal_count,
 		   r_xprt->rx_stats.bad_reply_count,
 		   r_xprt->rx_stats.nomsg_call_count);
+	seq_printf(seq, "%lu %lu\n",
+		   r_xprt->rx_stats.mrs_recovered,
+		   r_xprt->rx_stats.mrs_orphaned);
 }
 
 static int
@@ -741,7 +744,6 @@ void xprt_rdma_cleanup(void)
 			__func__, rc);
 
 	rpcrdma_destroy_wq();
-	frwr_destroy_recovery_wq();
 
 	rc = xprt_unregister_transport(&xprt_rdma_bc);
 	if (rc)
@@ -753,20 +755,13 @@ int xprt_rdma_init(void)
 {
 	int rc;
 
-	rc = frwr_alloc_recovery_wq();
-	if (rc)
-		return rc;
-
 	rc = rpcrdma_alloc_wq();
-	if (rc) {
-		frwr_destroy_recovery_wq();
+	if (rc)
 		return rc;
-	}
 
 	rc = xprt_register_transport(&xprt_rdma);
 	if (rc) {
 		rpcrdma_destroy_wq();
-		frwr_destroy_recovery_wq();
 		return rc;
 	}
 
@@ -774,7 +769,6 @@ int xprt_rdma_init(void)
 	if (rc) {
 		xprt_unregister_transport(&xprt_rdma);
 		rpcrdma_destroy_wq();
-		frwr_destroy_recovery_wq();
 		return rc;
 	}
 

commit ead3f26e359e12ac8d90baff8ed399b85e82fe5b
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:42:46 2016 -0400

    xprtrdma: Add ro_unmap_safe memreg method
    
    There needs to be a safe method of releasing registered memory
    resources when an RPC terminates. Safe can mean a number of things:
    
    + Doesn't have to sleep
    
    + Doesn't rely on having a QP in RTS
    
    ro_unmap_safe will be that safe method. It can be used in cases
    where synchronous memory invalidation can deadlock, or needs to have
    an active QP.
    
    The important case is fencing an RPC's memory regions after it is
    signaled (^C) and before it exits. If this is not done, there is a
    window where the server can write an RPC reply into memory that the
    client has released and re-used for some other purpose.
    
    Note that this is a full solution for FRWR, but FMR and physical
    still have some gaps where a particularly bad server can wreak
    some havoc on the client. These gaps are not made worse by this
    patch and are expected to be exceptionally rare and timing-based.
    They are noted in documenting comments.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 16595ff91994..99d2e5b72726 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -514,6 +514,7 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 out:
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 	req->rl_connect_cookie = 0;	/* our reserved value */
+	req->rl_task = task;
 	return req->rl_sendbuf->rg_base;
 
 out_rdmabuf:
@@ -570,7 +571,6 @@ xprt_rdma_free(void *buffer)
 	struct rpcrdma_req *req;
 	struct rpcrdma_xprt *r_xprt;
 	struct rpcrdma_regbuf *rb;
-	int i;
 
 	if (buffer == NULL)
 		return;
@@ -584,11 +584,8 @@ xprt_rdma_free(void *buffer)
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	for (i = 0; req->rl_nchunks;) {
-		--req->rl_nchunks;
-		i += r_xprt->rx_ia.ri_ops->ro_unmap(r_xprt,
-						    &req->rl_segments[i]);
-	}
+	r_xprt->rx_ia.ri_ops->ro_unmap_safe(r_xprt, req,
+					    !RPC_IS_ASYNC(req->rl_task));
 
 	rpcrdma_buffer_put(req);
 }

commit 29c554227aeec48cde5c22f911e51763f096e125
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:40:48 2016 -0400

    xprtrdma: Bound the inline threshold values
    
    Currently the sysctls that allow setting the inline threshold allow
    any value to be set.
    
    Small values only make the transport run slower. The default 1KB
    setting is as low as is reasonable. And the logic that decides how
    to divide a Send buffer between RPC-over-RDMA header and RPC message
    assumes (but does not check) that the lower bound is not crazy (say,
    57 bytes).
    
    Send and receive buffers share a page with some control information.
    Values larger than about 3KB can't be supported, currently.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9954342924df..16595ff91994 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -73,6 +73,8 @@ static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
 
 static unsigned int min_slot_table_size = RPCRDMA_MIN_SLOT_TABLE;
 static unsigned int max_slot_table_size = RPCRDMA_MAX_SLOT_TABLE;
+static unsigned int min_inline_size = RPCRDMA_MIN_INLINE;
+static unsigned int max_inline_size = RPCRDMA_MAX_INLINE;
 static unsigned int zero;
 static unsigned int max_padding = PAGE_SIZE;
 static unsigned int min_memreg = RPCRDMA_BOUNCEBUFFERS;
@@ -96,6 +98,8 @@ static struct ctl_table xr_tunables_table[] = {
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
+		.extra1		= &min_inline_size,
+		.extra2		= &max_inline_size,
 	},
 	{
 		.procname	= "rdma_max_inline_write",
@@ -103,6 +107,8 @@ static struct ctl_table xr_tunables_table[] = {
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= proc_dointvec,
+		.extra1		= &min_inline_size,
+		.extra2		= &max_inline_size,
 	},
 	{
 		.procname	= "rdma_inline_write_padding",

commit 6b26cc8c8ead3636a18bfd9489984983f4ddd6f4
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 2 14:40:40 2016 -0400

    sunrpc: Advertise maximum backchannel payload size
    
    RPC-over-RDMA transports have a limit on how large a backward
    direction (backchannel) RPC message can be. Ensure that the NFSv4.x
    CREATE_SESSION operation advertises this limit to servers.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index b1b009f10ea3..9954342924df 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -707,6 +707,7 @@ static struct rpc_xprt_ops xprt_rdma_procs = {
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 	.bc_setup		= xprt_rdma_bc_setup,
 	.bc_up			= xprt_rdma_bc_up,
+	.bc_maxpayload		= xprt_rdma_bc_maxpayload,
 	.bc_free_rqst		= xprt_rdma_bc_free_rqst,
 	.bc_destroy		= xprt_rdma_bc_destroy,
 #endif

commit 048ccca8c1c8f583deec3367d7df521bb1f542ae
Merge: b3e27d5d4a29 34356f64ac0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 23 18:45:06 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "Initial roundup of 4.5 merge window patches
    
       - Remove usage of ib_query_device and instead store attributes in
         ib_device struct
    
       - Move iopoll out of block and into lib, rename to irqpoll, and use
         in several places in the rdma stack as our new completion queue
         polling library mechanism.  Update the other block drivers that
         already used iopoll to use the new mechanism too.
    
       - Replace the per-entry GID table locks with a single GID table lock
    
       - IPoIB multicast cleanup
    
       - Cleanups to the IB MR facility
    
       - Add support for 64bit extended IB counters
    
       - Fix for netlink oops while parsing RDMA nl messages
    
       - RoCEv2 support for the core IB code
    
       - mlx4 RoCEv2 support
    
       - mlx5 RoCEv2 support
    
       - Cross Channel support for mlx5
    
       - Timestamp support for mlx5
    
       - Atomic support for mlx5
    
       - Raw QP support for mlx5
    
       - MAINTAINERS update for mlx4/mlx5
    
       - Misc ocrdma, qib, nes, usNIC, cxgb3, cxgb4, mlx4, mlx5 updates
    
       - Add support for remote invalidate to the iSER driver (pushed
         through the RDMA tree due to dependencies, acknowledged by nab)
    
       - Update to NFSoRDMA (pushed through the RDMA tree due to
         dependencies, acknowledged by Bruce)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (169 commits)
      IB/mlx5: Unify CQ create flags check
      IB/mlx5: Expose Raw Packet QP to user space consumers
      {IB, net}/mlx5: Move the modify QP operation table to mlx5_ib
      IB/mlx5: Support setting Ethernet priority for Raw Packet QPs
      IB/mlx5: Add Raw Packet QP query functionality
      IB/mlx5: Add create and destroy functionality for Raw Packet QP
      IB/mlx5: Refactor mlx5_ib_qp to accommodate other QP types
      IB/mlx5: Allocate a Transport Domain for each ucontext
      net/mlx5_core: Warn on unsupported events of QP/RQ/SQ
      net/mlx5_core: Add RQ and SQ event handling
      net/mlx5_core: Export transport objects
      IB/mlx5: Expose CQE version to user-space
      IB/mlx5: Add CQE version 1 support to user QPs and SRQs
      IB/mlx5: Fix data validation in mlx5_ib_alloc_ucontext
      IB/sa: Fix netlink local service GFP crash
      IB/srpt: Remove redundant wc array
      IB/qib: Improve ipoib UD performance
      IB/mlx4: Advertise RoCE v2 support
      IB/mlx4: Create and use another QP1 for RoCEv2
      IB/mlx4: Enable send of RoCE QP1 packets with IP/UDP headers
      ...

commit 5d252f90a800cee5bc57c76d636ae60464f7a887
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jan 7 14:50:10 2016 -0500

    svcrdma: Add class for RDMA backwards direction transport
    
    To support the server-side of an NFSv4.1 backchannel on RDMA
    connections, add a transport class that enables backward
    direction messages on an existing forward channel connection.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Bruce Fields <bfields@fieldses.org>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8c545f7d7525..5c7d235672fa 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -63,7 +63,7 @@
  */
 
 static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
-static unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
+unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_inline_write_padding;
 static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
@@ -143,12 +143,7 @@ static struct ctl_table sunrpc_table[] = {
 
 #endif
 
-#define RPCRDMA_BIND_TO		(60U * HZ)
-#define RPCRDMA_INIT_REEST_TO	(5U * HZ)
-#define RPCRDMA_MAX_REEST_TO	(30U * HZ)
-#define RPCRDMA_IDLE_DISC_TO	(5U * 60 * HZ)
-
-static struct rpc_xprt_ops xprt_rdma_procs;	/* forward reference */
+static struct rpc_xprt_ops xprt_rdma_procs;	/*forward reference */
 
 static void
 xprt_rdma_format_addresses4(struct rpc_xprt *xprt, struct sockaddr *sap)
@@ -174,7 +169,7 @@ xprt_rdma_format_addresses6(struct rpc_xprt *xprt, struct sockaddr *sap)
 	xprt->address_strings[RPC_DISPLAY_NETID] = RPCBIND_NETID_RDMA6;
 }
 
-static void
+void
 xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap)
 {
 	char buf[128];
@@ -203,7 +198,7 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap)
 	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
 }
 
-static void
+void
 xprt_rdma_free_addresses(struct rpc_xprt *xprt)
 {
 	unsigned int i;
@@ -499,7 +494,7 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	if (req == NULL)
 		return NULL;
 
-	flags = GFP_NOIO | __GFP_NOWARN;
+	flags = RPCRDMA_DEF_GFP;
 	if (RPC_IS_SWAPPER(task))
 		flags = __GFP_MEMALLOC | GFP_NOWAIT | __GFP_NOWARN;
 
@@ -639,7 +634,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	return -ENOTCONN;	/* implies disconnect */
 }
 
-static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
+void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	long idle_time = 0;
@@ -740,6 +735,11 @@ void xprt_rdma_cleanup(void)
 
 	rpcrdma_destroy_wq();
 	frwr_destroy_recovery_wq();
+
+	rc = xprt_unregister_transport(&xprt_rdma_bc);
+	if (rc)
+		dprintk("RPC:       %s: xprt_unregister(bc) returned %i\n",
+			__func__, rc);
 }
 
 int xprt_rdma_init(void)
@@ -763,6 +763,14 @@ int xprt_rdma_init(void)
 		return rc;
 	}
 
+	rc = xprt_register_transport(&xprt_rdma_bc);
+	if (rc) {
+		xprt_unregister_transport(&xprt_rdma);
+		rpcrdma_destroy_wq();
+		frwr_destroy_recovery_wq();
+		return rc;
+	}
+
 	dprintk("RPCRDMA Module Init, register RPC RDMA transport\n");
 
 	dprintk("Defaults:\n");

commit ffc4d9b1596c34caa98962722e930e97912c8a9f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Dec 16 17:22:14 2015 -0500

    xprtrdma: xprt_rdma_free() must not release backchannel reqs
    
    Preserve any rpcrdma_req that is attached to rpc_rqst's allocated
    for the backchannel. Otherwise, after all the pre-allocated
    backchannel req's are consumed, incoming backward calls start
    writing on freed memory.
    
    Somehow this hunk got lost.
    
    Fixes: f531a5dbc451 ('xprtrdma: Pre-allocate backward rpc_rqst')
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8c545f7d7525..740bddcf3488 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -576,6 +576,9 @@ xprt_rdma_free(void *buffer)
 
 	rb = container_of(buffer, struct rpcrdma_regbuf, rg_base[0]);
 	req = rb->rg_owner;
+	if (req->rl_backchannel)
+		return;
+
 	r_xprt = container_of(req->rl_buffer, struct rpcrdma_xprt, rx_buf);
 
 	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);

commit 76566773a1f1c2295ed901b6f1241cfe10d99029
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:28:32 2015 -0400

    NFS: Enable client side NFSv4.1 backchannel to use other transports
    
    Forechannel transports get their own "bc_up" method to create an
    endpoint for the backchannel service.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    [Anna Schumaker: Add forward declaration of struct net to xprt.h]
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 845278e63be0..8c545f7d7525 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -708,6 +708,7 @@ static struct rpc_xprt_ops xprt_rdma_procs = {
 	.inject_disconnect	= xprt_rdma_inject_disconnect,
 #if defined(CONFIG_SUNRPC_BACKCHANNEL)
 	.bc_setup		= xprt_rdma_bc_setup,
+	.bc_up			= xprt_rdma_bc_up,
 	.bc_free_rqst		= xprt_rdma_bc_free_rqst,
 	.bc_destroy		= xprt_rdma_bc_destroy,
 #endif

commit f531a5dbc451afb66e9d6c71a69e8358d1847969
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:27:43 2015 -0400

    xprtrdma: Pre-allocate backward rpc_rqst and send/receive buffers
    
    xprtrdma's backward direction send and receive buffers are the same
    size as the forechannel's inline threshold, and must be pre-
    registered.
    
    The consumer has no control over which receive buffer the adapter
    chooses to catch an incoming backwards-direction call. Any receive
    buffer can be used for either a forward reply or a backward call.
    Thus both types of RPC message must all be the same size.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 897a2f3a9620..845278e63be0 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -705,7 +705,12 @@ static struct rpc_xprt_ops xprt_rdma_procs = {
 	.print_stats		= xprt_rdma_print_stats,
 	.enable_swap		= xprt_rdma_enable_swap,
 	.disable_swap		= xprt_rdma_disable_swap,
-	.inject_disconnect	= xprt_rdma_inject_disconnect
+	.inject_disconnect	= xprt_rdma_inject_disconnect,
+#if defined(CONFIG_SUNRPC_BACKCHANNEL)
+	.bc_setup		= xprt_rdma_bc_setup,
+	.bc_free_rqst		= xprt_rdma_bc_free_rqst,
+	.bc_destroy		= xprt_rdma_bc_destroy,
+#endif
 };
 
 static struct xprt_class xprt_rdma = {

commit fe97b47cd623ebbaa55a163c336abc47153526d1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:27:10 2015 -0400

    xprtrdma: Use workqueue to process RPC/RDMA replies
    
    The reply tasklet is fast, but it's single threaded. After reply
    traffic saturates a single CPU, there's no more reply processing
    capacity.
    
    Replace the tasklet with a workqueue to spread reply handling across
    all CPUs.  This also moves RPC/RDMA reply handling out of the soft
    IRQ context and into a context that allows sleeps.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index e9e5ed7a57fe..897a2f3a9620 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -732,6 +732,7 @@ void xprt_rdma_cleanup(void)
 		dprintk("RPC:       %s: xprt_unregister returned %i\n",
 			__func__, rc);
 
+	rpcrdma_destroy_wq();
 	frwr_destroy_recovery_wq();
 }
 
@@ -743,8 +744,15 @@ int xprt_rdma_init(void)
 	if (rc)
 		return rc;
 
+	rc = rpcrdma_alloc_wq();
+	if (rc) {
+		frwr_destroy_recovery_wq();
+		return rc;
+	}
+
 	rc = xprt_register_transport(&xprt_rdma);
 	if (rc) {
+		rpcrdma_destroy_wq();
 		frwr_destroy_recovery_wq();
 		return rc;
 	}

commit a045178887ebafa9514d6b4cb840ac13a26c8365
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Oct 24 17:26:29 2015 -0400

    xprtrdma: Enable swap-on-NFS/RDMA
    
    After adding a swapfile on an NFS/RDMA mount and removing the
    normal swap partition, I was able to push the NFS client well
    into swap without any issue.
    
    I forgot to swapoff the NFS file before rebooting. This pinned
    the NFS mount and the IB core and provider, causing shutdown to
    hang. I think this is expected and safe behavior. Probably
    shutdown scripts should "swapoff -a" before unmounting any
    filesystems.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 41e452bc580c..e9e5ed7a57fe 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -676,7 +676,7 @@ static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 static int
 xprt_rdma_enable_swap(struct rpc_xprt *xprt)
 {
-	return -EINVAL;
+	return 0;
 }
 
 static void

commit 72c021738252dde5849d575a650239d6404930ee
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Mon Sep 21 12:24:23 2015 -0500

    xprtrdma: disconnect and flush cqs before freeing buffers
    
    Otherwise a FRMR completion can cause a touch-after-free crash.
    
    In xprt_rdma_destroy(), call rpcrdma_buffer_destroy() only after calling
    rpcrdma_ep_destroy().
    
    In rpcrdma_ep_destroy(), disconnect the cm_id first which should flush the
    qp, then drain the cqs, then destroy the qp, and finally destroy the cqs.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 64443eb754ad..41e452bc580c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -270,8 +270,8 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	xprt_clear_connected(xprt);
 
-	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
 	rpcrdma_ep_destroy(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
 	rpcrdma_ia_close(&r_xprt->rx_ia);
 
 	xprt_rdma_free_addresses(xprt);

commit 860477d1ff176549f2bf438b61e5c1ec6b1d43e5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:04:45 2015 -0400

    xprtrdma: Count RDMA_NOMSG type calls
    
    RDMA_NOMSG type calls are less efficient than RDMA_MSG. Count NOMSG
    calls so administrators can tell if they happen to be used more than
    expected.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 0985b2b5de9d..64443eb754ad 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -659,7 +659,7 @@ static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   xprt->stat.bad_xids,
 		   xprt->stat.req_u,
 		   xprt->stat.bklog_u);
-	seq_printf(seq, "%lu %lu %lu %llu %llu %llu %llu %lu %lu %lu\n",
+	seq_printf(seq, "%lu %lu %lu %llu %llu %llu %llu %lu %lu %lu %lu\n",
 		   r_xprt->rx_stats.read_chunk_count,
 		   r_xprt->rx_stats.write_chunk_count,
 		   r_xprt->rx_stats.reply_chunk_count,
@@ -669,7 +669,8 @@ static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 		   r_xprt->rx_stats.fixup_copy_count,
 		   r_xprt->rx_stats.hardway_register_count,
 		   r_xprt->rx_stats.failed_marshal_count,
-		   r_xprt->rx_stats.bad_reply_count);
+		   r_xprt->rx_stats.bad_reply_count,
+		   r_xprt->rx_stats.nomsg_call_count);
 }
 
 static int

commit 763f7e4e4b9033ac6a0f13aa37ba43636fc3c0af
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:04:36 2015 -0400

    xprtrdma: Clean up xprt_rdma_print_stats()
    
    checkpatch.pl complained about the seq_printf() format string split
    across lines and the use of %Lu.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d737300ebc42..0985b2b5de9d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -647,31 +647,29 @@ static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 	if (xprt_connected(xprt))
 		idle_time = (long)(jiffies - xprt->last_used) / HZ;
 
-	seq_printf(seq,
-	  "\txprt:\trdma %u %lu %lu %lu %ld %lu %lu %lu %Lu %Lu "
-	  "%lu %lu %lu %Lu %Lu %Lu %Lu %lu %lu %lu\n",
-
-	   0,	/* need a local port? */
-	   xprt->stat.bind_count,
-	   xprt->stat.connect_count,
-	   xprt->stat.connect_time,
-	   idle_time,
-	   xprt->stat.sends,
-	   xprt->stat.recvs,
-	   xprt->stat.bad_xids,
-	   xprt->stat.req_u,
-	   xprt->stat.bklog_u,
-
-	   r_xprt->rx_stats.read_chunk_count,
-	   r_xprt->rx_stats.write_chunk_count,
-	   r_xprt->rx_stats.reply_chunk_count,
-	   r_xprt->rx_stats.total_rdma_request,
-	   r_xprt->rx_stats.total_rdma_reply,
-	   r_xprt->rx_stats.pullup_copy_count,
-	   r_xprt->rx_stats.fixup_copy_count,
-	   r_xprt->rx_stats.hardway_register_count,
-	   r_xprt->rx_stats.failed_marshal_count,
-	   r_xprt->rx_stats.bad_reply_count);
+	seq_puts(seq, "\txprt:\trdma ");
+	seq_printf(seq, "%u %lu %lu %lu %ld %lu %lu %lu %llu %llu ",
+		   0,	/* need a local port? */
+		   xprt->stat.bind_count,
+		   xprt->stat.connect_count,
+		   xprt->stat.connect_time,
+		   idle_time,
+		   xprt->stat.sends,
+		   xprt->stat.recvs,
+		   xprt->stat.bad_xids,
+		   xprt->stat.req_u,
+		   xprt->stat.bklog_u);
+	seq_printf(seq, "%lu %lu %lu %llu %llu %llu %llu %lu %lu %lu\n",
+		   r_xprt->rx_stats.read_chunk_count,
+		   r_xprt->rx_stats.write_chunk_count,
+		   r_xprt->rx_stats.reply_chunk_count,
+		   r_xprt->rx_stats.total_rdma_request,
+		   r_xprt->rx_stats.total_rdma_reply,
+		   r_xprt->rx_stats.pullup_copy_count,
+		   r_xprt->rx_stats.fixup_copy_count,
+		   r_xprt->rx_stats.hardway_register_count,
+		   r_xprt->rx_stats.failed_marshal_count,
+		   r_xprt->rx_stats.bad_reply_count);
 }
 
 static int

commit 5231eb9773c61a2a17590eabcadf4aecf44100bd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Aug 3 13:02:41 2015 -0400

    xprtrdma: Make xprt_setup_rdma() agnostic to family of server address
    
    In particular, recognize when an IPv6 connection is bound.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 680f888a9ddd..d737300ebc42 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -175,10 +175,8 @@ xprt_rdma_format_addresses6(struct rpc_xprt *xprt, struct sockaddr *sap)
 }
 
 static void
-xprt_rdma_format_addresses(struct rpc_xprt *xprt)
+xprt_rdma_format_addresses(struct rpc_xprt *xprt, struct sockaddr *sap)
 {
-	struct sockaddr *sap = (struct sockaddr *)
-					&rpcx_to_rdmad(xprt).addr;
 	char buf[128];
 
 	switch (sap->sa_family) {
@@ -302,7 +300,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	struct rpc_xprt *xprt;
 	struct rpcrdma_xprt *new_xprt;
 	struct rpcrdma_ep *new_ep;
-	struct sockaddr_in *sin;
+	struct sockaddr *sap;
 	int rc;
 
 	if (args->addrlen > sizeof(xprt->addr)) {
@@ -333,26 +331,20 @@ xprt_setup_rdma(struct xprt_create *args)
 	 * Set up RDMA-specific connect data.
 	 */
 
-	/* Put server RDMA address in local cdata */
-	memcpy(&cdata.addr, args->dstaddr, args->addrlen);
+	sap = (struct sockaddr *)&cdata.addr;
+	memcpy(sap, args->dstaddr, args->addrlen);
 
 	/* Ensure xprt->addr holds valid server TCP (not RDMA)
 	 * address, for any side protocols which peek at it */
 	xprt->prot = IPPROTO_TCP;
 	xprt->addrlen = args->addrlen;
-	memcpy(&xprt->addr, &cdata.addr, xprt->addrlen);
+	memcpy(&xprt->addr, sap, xprt->addrlen);
 
-	sin = (struct sockaddr_in *)&cdata.addr;
-	if (ntohs(sin->sin_port) != 0)
+	if (rpc_get_port(sap))
 		xprt_set_bound(xprt);
 
-	dprintk("RPC:       %s: %pI4:%u\n",
-		__func__, &sin->sin_addr.s_addr, ntohs(sin->sin_port));
-
-	/* Set max requests */
 	cdata.max_requests = xprt->max_reqs;
 
-	/* Set some length limits */
 	cdata.rsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA write max */
 	cdata.wsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA read max */
 
@@ -375,8 +367,7 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	new_xprt = rpcx_to_rdmax(xprt);
 
-	rc = rpcrdma_ia_open(new_xprt, (struct sockaddr *) &cdata.addr,
-				xprt_rdma_memreg_strategy);
+	rc = rpcrdma_ia_open(new_xprt, sap, xprt_rdma_memreg_strategy);
 	if (rc)
 		goto out1;
 
@@ -409,7 +400,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
 			  xprt_rdma_connect_worker);
 
-	xprt_rdma_format_addresses(xprt);
+	xprt_rdma_format_addresses(xprt, sap);
 	xprt->max_payload = new_xprt->rx_ia.ri_ops->ro_maxpages(new_xprt);
 	if (xprt->max_payload == 0)
 		goto out4;
@@ -420,6 +411,9 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (!try_module_get(THIS_MODULE))
 		goto out4;
 
+	dprintk("RPC:       %s: %s:%s\n", __func__,
+		xprt->address_strings[RPC_DISPLAY_ADDR],
+		xprt->address_strings[RPC_DISPLAY_PORT]);
 	return xprt;
 
 out4:

commit 8688d9540cc6e17df4cba71615e27f04e0378fe6
Merge: 320cd413faef b4839ebe21fc
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 2 11:32:23 2015 -0700

    Merge tag 'nfs-for-4.2-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs
    
    Pull NFS client updates from Trond Myklebust:
     "Highlights include:
    
      Stable patches:
       - Fix a crash in the NFSv4 file locking code.
       - Fix an fsync() regression, where we were failing to retry I/O in
         some circumstances.
       - Fix an infinite loop in NFSv4.0 OPEN stateid recovery
       - Fix a memory leak when an attempted pnfs fails.
       - Fix a memory leak in the backchannel code
       - Large hostnames were not supported correctly in NFSv4.1
       - Fix a pNFS/flexfiles bug that was impeding error reporting on I/O.
       - Fix a couple of credential issues in pNFS/flexfiles
    
      Bugfixes + cleanups:
       - Open flag sanity checks in the NFSv4 atomic open codepath
       - More NFSv4 delegation related bugfixes
       - Various NFSv4.1 backchannel bugfixes and cleanups
       - Fix the NFS swap socket code
       - Various cleanups of the NFSv4 SETCLIENTID and EXCHANGE_ID code
       - Fix a UDP transport deadlock issue
    
      Features:
       - More RDMA client transport improvements
       - NFSv4.2 LAYOUTSTATS functionality for pnfs flexfiles"
    
    * tag 'nfs-for-4.2-1' of git://git.linux-nfs.org/projects/trondmy/linux-nfs: (87 commits)
      nfs: Remove invalid tk_pid from debug message
      nfs: Remove invalid NFS_ATTR_FATTR_V4_REFERRAL checking in nfs4_get_rootfh
      nfs: Drop bad comment in nfs41_walk_client_list()
      nfs: Remove unneeded micro checking of CONFIG_PROC_FS
      nfs: Don't setting FILE_CREATED flags always
      nfs: Use remove_proc_subtree() instead remove_proc_entry()
      nfs: Remove unused argument in nfs_server_set_fsinfo()
      nfs: Fix a memory leak when meeting an unsupported state protect
      nfs: take extra reference to fl->fl_file when running a LOCKU operation
      NFSv4: When returning a delegation, don't reclaim an incompatible open mode.
      NFSv4.2: LAYOUTSTATS is optional to implement
      NFSv4.2: Fix up a decoding error in layoutstats
      pNFS/flexfiles: Fix the reset of struct pgio_header when resending
      pNFS/flexfiles: Turn off layoutcommit for servers that don't need it
      pnfs/flexfiles: protect ktime manipulation with mirror lock
      nfs: provide pnfs_report_layoutstat when NFS42 is disabled
      nfs: verify open flags before allowing open
      nfs: always update creds in mirror, even when we have an already connected ds
      nfs: fix potential credential leak in ff_layout_update_mirror_cred
      pnfs/flexfiles: report layoutstat regularly
      ...

commit 3438995bc43939ed1502bb2639ff7007169574ea
Merge: 5ba12443a132 40c6ed0c8a7f
Author: Trond Myklebust <trond.myklebust@primarydata.com>
Date:   Tue Jun 16 11:36:47 2015 -0400

    Merge tag 'nfs-rdma-for-4.2' of git://git.linux-nfs.org/projects/anna/nfs-rdma
    
    NFS: NFSoRDMA Client Changes
    
    These patches continue to build up for improving the rsize and wsize that the
    NFS client uses when talking over RDMA.  In addition, these patches also add
    in scalability enhancements and other bugfixes.
    
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>
    
    * tag 'nfs-rdma-for-4.2' of git://git.linux-nfs.org/projects/anna/nfs-rdma: (142 commits)
      xprtrdma: Reduce per-transport MR allocation
      xprtrdma: Stack relief in fmr_op_map()
      xprtrdma: Split rb_lock
      xprtrdma: Remove rpcrdma_ia::ri_memreg_strategy
      xprtrdma: Remove ->ro_reset
      xprtrdma: Remove unused LOCAL_INV recovery logic
      xprtrdma: Acquire MRs in rpcrdma_register_external()
      xprtrdma: Introduce an FRMR recovery workqueue
      xprtrdma: Acquire FMRs in rpcrdma_fmr_register_external()
      xprtrdma: Introduce helpers for allocating MWs
      xprtrdma: Use ib_device pointer safely
      xprtrdma: Remove rr_func
      xprtrdma: Replace rpcrdma_rep::rr_buffer with rr_rxprt
      xprtrdma: Warn when there are orphaned IB objects
      ...

commit 951e721ca0d665ece6175b8d8edf93cf7b215bd5
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:52:25 2015 -0400

    xprtrdma: Introduce an FRMR recovery workqueue
    
    After a transport disconnect, FRMRs can be left in an undetermined
    state. In particular, the MR's rkey is no good.
    
    Currently, FRMRs are fixed up by the transport connect worker, but
    that can race with ->ro_unmap if an RPC happens to exit while the
    transport connect worker is running.
    
    A better way of dealing with broken FRMRs is to detect them before
    they are re-used by ->ro_map. Such FRMRs are either already invalid
    or are owned by the sending RPC, and thus no race with ->ro_unmap
    is possible.
    
    Introduce a mechanism for handing broken FRMRs to a workqueue to be
    reset in a context that is appropriate for allocating resources
    (ie. an ib_alloc_fast_reg_mr() API call).
    
    This mechanism is not yet used, but will be in subsequent patches.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 91b34348ace0..ed861ff3586d 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -720,17 +720,24 @@ static void __exit xprt_rdma_cleanup(void)
 	if (rc)
 		dprintk("RPC:       %s: xprt_unregister returned %i\n",
 			__func__, rc);
+
+	frwr_destroy_recovery_wq();
 }
 
 static int __init xprt_rdma_init(void)
 {
 	int rc;
 
-	rc = xprt_register_transport(&xprt_rdma);
-
+	rc = frwr_alloc_recovery_wq();
 	if (rc)
 		return rc;
 
+	rc = xprt_register_transport(&xprt_rdma);
+	if (rc) {
+		frwr_destroy_recovery_wq();
+		return rc;
+	}
+
 	dprintk("RPCRDMA Module Init, register RPC RDMA transport\n");
 
 	dprintk("Defaults:\n");

commit 494ae30d2a47cf439c6a680cc62e09ae0c51d190
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:51:46 2015 -0400

    xprtrdma: Remove rr_func
    
    A posted rpcrdma_rep never has rr_func set to anything but
    rpcrdma_reply_handler.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ce95eb3595d0..91b34348ace0 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -617,9 +617,6 @@ xprt_rdma_send_request(struct rpc_task *task)
 
 	if (req->rl_reply == NULL) 		/* e.g. reconnection */
 		rpcrdma_recv_buffer_get(req);
-	/* rpcrdma_recv_buffer_get may have set rl_reply, so check again */
-	if (req->rl_reply)
-		req->rl_reply->rr_func = rpcrdma_reply_handler;
 
 	/* Must suppress retransmit to maintain credits */
 	if (req->rl_connect_cookie == xprt->connect_cookie)

commit fed171b35c7c0777fa0d6aeb3f25cc9b0d5f56ad
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue May 26 11:51:37 2015 -0400

    xprtrdma: Replace rpcrdma_rep::rr_buffer with rr_rxprt
    
    Clean up: Instead of carrying a pointer to the buffer pool and
    the rpc_xprt, carry a pointer to the controlling rpcrdma_xprt.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-By: Devesh Sharma <devesh.sharma@avagotech.com>
    Reviewed-by: Doug Ledford <dledford@redhat.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 54f23b1be986..ce95eb3595d0 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -617,12 +617,9 @@ xprt_rdma_send_request(struct rpc_task *task)
 
 	if (req->rl_reply == NULL) 		/* e.g. reconnection */
 		rpcrdma_recv_buffer_get(req);
-
-	if (req->rl_reply) {
+	/* rpcrdma_recv_buffer_get may have set rl_reply, so check again */
+	if (req->rl_reply)
 		req->rl_reply->rr_func = rpcrdma_reply_handler;
-		/* this need only be done once, but... */
-		req->rl_reply->rr_xprt = xprt;
-	}
 
 	/* Must suppress retransmit to maintain credits */
 	if (req->rl_connect_cookie == xprt->connect_cookie)

commit 4a06825839889cc1756d0dd8a52d6b1071ee0263
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon May 11 14:02:25 2015 -0400

    SUNRPC: Transport fault injection
    
    It has been exceptionally useful to exercise the logic that handles
    local immediate errors and RDMA connection loss.  To enable
    developers to test this regularly and repeatably, add logic to
    simulate connection loss every so often.
    
    Fault injection is disabled by default. It is enabled with
    
      $ sudo echo xxx > /sys/kernel/debug/sunrpc/inject_fault/disconnect
    
    where "xxx" is a large positive number of transport method calls
    before a disconnect. A value of several thousand is usually a good
    number that allows reasonable forward progress while still causing a
    lot of connection drops.
    
    These hooks are disabled when SUNRPC_DEBUG is turned off.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ebf6fe759f0e..b8aac23b1b0c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -246,6 +246,16 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	xprt_clear_connecting(xprt);
 }
 
+static void
+xprt_rdma_inject_disconnect(struct rpc_xprt *xprt)
+{
+	struct rpcrdma_xprt *r_xprt = container_of(xprt, struct rpcrdma_xprt,
+						   rx_xprt);
+
+	pr_info("rpcrdma: injecting transport disconnect on xprt=%p\n", xprt);
+	rdma_disconnect(r_xprt->rx_ia.ri_id);
+}
+
 /*
  * xprt_rdma_destroy
  *
@@ -714,6 +724,7 @@ static struct rpc_xprt_ops xprt_rdma_procs = {
 	.print_stats		= xprt_rdma_print_stats,
 	.enable_swap		= xprt_rdma_enable_swap,
 	.disable_swap		= xprt_rdma_disable_swap,
+	.inject_disconnect	= xprt_rdma_inject_disconnect
 };
 
 static struct xprt_class xprt_rdma = {

commit d67fa4d85a2143b46052b2e9ccc6749a4c97b2de
Author: Jeff Layton <jlayton@poochiereds.net>
Date:   Wed Jun 3 16:14:29 2015 -0400

    sunrpc: turn swapper_enable/disable functions into rpc_xprt_ops
    
    RDMA xprts don't have a sock_xprt, but an rdma_xprt, so the
    xs_swapper_enable/disable functions will likely oops when fed an RDMA
    xprt. Turn these functions into rpc_xprt_ops so that that doesn't
    occur. For now the RDMA versions are no-ops that just return -EINVAL
    on an attempt to swapon.
    
    Cc: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Jeff Layton <jeff.layton@primarydata.com>
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 54f23b1be986..ebf6fe759f0e 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -682,6 +682,17 @@ static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 	   r_xprt->rx_stats.bad_reply_count);
 }
 
+static int
+xprt_rdma_enable_swap(struct rpc_xprt *xprt)
+{
+	return -EINVAL;
+}
+
+static void
+xprt_rdma_disable_swap(struct rpc_xprt *xprt)
+{
+}
+
 /*
  * Plumbing for rpc transport switch and kernel module
  */
@@ -700,7 +711,9 @@ static struct rpc_xprt_ops xprt_rdma_procs = {
 	.send_request		= xprt_rdma_send_request,
 	.close			= xprt_rdma_close,
 	.destroy		= xprt_rdma_destroy,
-	.print_stats		= xprt_rdma_print_stats
+	.print_stats		= xprt_rdma_print_stats,
+	.enable_swap		= xprt_rdma_enable_swap,
+	.disable_swap		= xprt_rdma_disable_swap,
 };
 
 static struct xprt_class xprt_rdma = {

commit ffe1f0df586237a18f9b568597bddabb56e96d5e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Thu Jun 4 11:21:42 2015 -0400

    rpcrdma: Merge svcrdma and xprtrdma modules into one
    
    Bi-directional RPC support means code in svcrdma.ko invokes a bit of
    code in xprtrdma.ko, and vice versa. To avoid loader/linker loops,
    merge the server and client side modules together into a single
    module.
    
    When backchannel capabilities are added, the combined module will
    register all needed transport capabilities so that Upper Layer
    consumers automatically have everything needed to create a
    bi-directional transport connection.
    
    Module aliases are added for backwards compatibility with user
    space, which still may expect svcrdma.ko or xprtrdma.ko to be
    present.
    
    This commit reverts commit 2e8c12e1b765 ("xprtrdma: add separate
    Kconfig options for NFSoRDMA client and server support") and
    provides a single CONFIG option for enabling the new module.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 54f23b1be986..436da2caec95 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -48,7 +48,6 @@
  */
 
 #include <linux/module.h>
-#include <linux/init.h>
 #include <linux/slab.h>
 #include <linux/seq_file.h>
 #include <linux/sunrpc/addr.h>
@@ -59,11 +58,6 @@
 # define RPCDBG_FACILITY	RPCDBG_TRANS
 #endif
 
-MODULE_LICENSE("Dual BSD/GPL");
-
-MODULE_DESCRIPTION("RPC/RDMA Transport for Linux kernel NFS");
-MODULE_AUTHOR("Network Appliance, Inc.");
-
 /*
  * tunables
  */
@@ -711,7 +705,7 @@ static struct xprt_class xprt_rdma = {
 	.setup			= xprt_setup_rdma,
 };
 
-static void __exit xprt_rdma_cleanup(void)
+void xprt_rdma_cleanup(void)
 {
 	int rc;
 
@@ -728,7 +722,7 @@ static void __exit xprt_rdma_cleanup(void)
 			__func__, rc);
 }
 
-static int __init xprt_rdma_init(void)
+int xprt_rdma_init(void)
 {
 	int rc;
 
@@ -753,6 +747,3 @@ static int __init xprt_rdma_init(void)
 #endif
 	return 0;
 }
-
-module_init(xprt_rdma_init);
-module_exit(xprt_rdma_cleanup);

commit 6814baead86b5d44096ddfbb6f944163578e68c3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:34:48 2015 -0400

    xprtrdma: Add a "deregister_external" op for each memreg mode
    
    There is very little common processing among the different external
    memory deregistration functions.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index da71a24641e3..54f23b1be986 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -584,8 +584,8 @@ xprt_rdma_free(void *buffer)
 
 	for (i = 0; req->rl_nchunks;) {
 		--req->rl_nchunks;
-		i += rpcrdma_deregister_external(
-			&req->rl_segments[i], r_xprt);
+		i += r_xprt->rx_ia.ri_ops->ro_unmap(r_xprt,
+						    &req->rl_segments[i]);
 	}
 
 	rpcrdma_buffer_put(req);

commit 1c9351ee0e346ec1b3c700a4bc8f881923e1808e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:34:30 2015 -0400

    xprtrdma: Add a "max_payload" op for each memreg mode
    
    The max_payload computation is generalized to ensure that the
    payload maximum is the lesser of RPC_MAX_DATA_SEGS and the number of
    data segments that can be transmitted in an inline buffer.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 97f656292feb..da71a24641e3 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -406,7 +406,10 @@ xprt_setup_rdma(struct xprt_create *args)
 			  xprt_rdma_connect_worker);
 
 	xprt_rdma_format_addresses(xprt);
-	xprt->max_payload = rpcrdma_max_payload(new_xprt);
+	xprt->max_payload = new_xprt->rx_ia.ri_ops->ro_maxpages(new_xprt);
+	if (xprt->max_payload == 0)
+		goto out4;
+	xprt->max_payload <<= PAGE_SHIFT;
 	dprintk("RPC:       %s: transport data payload maximum: %zu bytes\n",
 		__func__, xprt->max_payload);
 

commit e23779451ee05e824fedbb68bd17fc5c77e40166
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:33:53 2015 -0400

    xprtrdma: Perform a full marshal on retransmit
    
    Commit 6ab59945f292 ("xprtrdma: Update rkeys after transport
    reconnect" added logic in the ->send_request path to update the
    chunk list when an RPC/RDMA request is retransmitted.
    
    Note that rpc_xdr_encode() resets and re-encodes the entire RPC
    send buffer for each retransmit of an RPC. The RPC send buffer
    is not preserved from the previous transmission of an RPC.
    
    Revert 6ab59945f292, and instead, just force each request to be
    fully marshaled every time through ->send_request. This should
    preserve the fix from 6ab59945f292, while also performing pullup
    during retransmits.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Acked-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9be7f97205ba..97f656292feb 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -608,10 +608,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	int rc = 0;
 
-	if (req->rl_niovs == 0)
-		rc = rpcrdma_marshal_req(rqst);
-	else if (r_xprt->rx_ia.ri_memreg_strategy != RPCRDMA_ALLPHYSICAL)
-		rc = rpcrdma_marshal_chunks(rqst, 0);
+	rc = rpcrdma_marshal_req(rqst);
 	if (rc < 0)
 		goto failed_marshal;
 

commit 0dd39cae26f3990789b4a558f9abafe59adc6fc1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Mar 30 14:33:43 2015 -0400

    xprtrdma: Display IPv6 addresses and port numbers correctly
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Devesh Sharma <Devesh.Sharma@Emulex.Com>
    Tested-by: Meghana Cheripady <Meghana.Cheripady@Emulex.Com>
    Tested-by: Veeresh U. Kokatnur <veereshuk@chelsio.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 2e192baa59f3..9be7f97205ba 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -156,13 +156,48 @@ static struct ctl_table sunrpc_table[] = {
 
 static struct rpc_xprt_ops xprt_rdma_procs;	/* forward reference */
 
+static void
+xprt_rdma_format_addresses4(struct rpc_xprt *xprt, struct sockaddr *sap)
+{
+	struct sockaddr_in *sin = (struct sockaddr_in *)sap;
+	char buf[20];
+
+	snprintf(buf, sizeof(buf), "%08x", ntohl(sin->sin_addr.s_addr));
+	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = kstrdup(buf, GFP_KERNEL);
+
+	xprt->address_strings[RPC_DISPLAY_NETID] = RPCBIND_NETID_RDMA;
+}
+
+static void
+xprt_rdma_format_addresses6(struct rpc_xprt *xprt, struct sockaddr *sap)
+{
+	struct sockaddr_in6 *sin6 = (struct sockaddr_in6 *)sap;
+	char buf[40];
+
+	snprintf(buf, sizeof(buf), "%pi6", &sin6->sin6_addr);
+	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = kstrdup(buf, GFP_KERNEL);
+
+	xprt->address_strings[RPC_DISPLAY_NETID] = RPCBIND_NETID_RDMA6;
+}
+
 static void
 xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 {
 	struct sockaddr *sap = (struct sockaddr *)
 					&rpcx_to_rdmad(xprt).addr;
-	struct sockaddr_in *sin = (struct sockaddr_in *)sap;
-	char buf[64];
+	char buf[128];
+
+	switch (sap->sa_family) {
+	case AF_INET:
+		xprt_rdma_format_addresses4(xprt, sap);
+		break;
+	case AF_INET6:
+		xprt_rdma_format_addresses6(xprt, sap);
+		break;
+	default:
+		pr_err("rpcrdma: Unrecognized address family\n");
+		return;
+	}
 
 	(void)rpc_ntop(sap, buf, sizeof(buf));
 	xprt->address_strings[RPC_DISPLAY_ADDR] = kstrdup(buf, GFP_KERNEL);
@@ -170,16 +205,10 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 	snprintf(buf, sizeof(buf), "%u", rpc_get_port(sap));
 	xprt->address_strings[RPC_DISPLAY_PORT] = kstrdup(buf, GFP_KERNEL);
 
-	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
-
-	snprintf(buf, sizeof(buf), "%08x", ntohl(sin->sin_addr.s_addr));
-	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = kstrdup(buf, GFP_KERNEL);
-
 	snprintf(buf, sizeof(buf), "%4hx", rpc_get_port(sap));
 	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = kstrdup(buf, GFP_KERNEL);
 
-	/* netid */
-	xprt->address_strings[RPC_DISPLAY_NETID] = "rdma";
+	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
 }
 
 static void

commit a0a1d50cd1e80652142af5cddcde500d06c71bdd
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Jan 26 17:11:47 2015 -0500

    xprtrdma: Update the GFP flags used in xprt_rdma_allocate()
    
    Reflect the more conservative approach used in the socket transport's
    version of this transport method. An RPC buffer allocation should
    avoid forcing not just FS activity, but any I/O.
    
    In particular, two recent changes missed updating xprtrdma:
    
     - Commit c6c8fe79a83e ("net, sunrpc: suppress allocation warning ...")
     - Commit a564b8f03986 ("nfs: enable swap on NFS")
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 2c2fabe99d84..2e192baa59f3 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -463,13 +463,16 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	struct rpcrdma_regbuf *rb;
 	struct rpcrdma_req *req;
 	size_t min_size;
-	gfp_t flags = task->tk_flags & RPC_TASK_SWAPPER ?
-						GFP_ATOMIC : GFP_NOFS;
+	gfp_t flags;
 
 	req = rpcrdma_buffer_get(&r_xprt->rx_buf);
 	if (req == NULL)
 		return NULL;
 
+	flags = GFP_NOIO | __GFP_NOWARN;
+	if (RPC_IS_SWAPPER(task))
+		flags = __GFP_MEMALLOC | GFP_NOWAIT | __GFP_NOWARN;
+
 	if (req->rl_rdmabuf == NULL)
 		goto out_rdmabuf;
 	if (req->rl_sendbuf == NULL)

commit 85275c874eaeb92fb2a78a1d4ebb1ff4b0f7b732
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:16 2015 -0500

    xprtrdma: Allocate RPC/RDMA send buffer separately from struct rpcrdma_req
    
    The rl_base field is currently the buffer where each RPC/RDMA call
    header is built.
    
    The inline threshold is an agreed-on size limit to for RDMA SEND
    operations that pass between client and server. The sum of the
    RPC/RDMA header size and the RPC header size must be less than or
    equal to this threshold.
    
    Increasing the r/wsize maximum will require MAX_SEGS to grow
    significantly, but the inline threshold size won't change (both
    sides agree on it). The server's inline threshold doesn't change.
    
    Since an RPC/RDMA header can never be larger than the inline
    threshold, make all RPC/RDMA header buffers the size of the
    inline threshold.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index a9d566227e7e..2c2fabe99d84 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -470,6 +470,8 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	if (req == NULL)
 		return NULL;
 
+	if (req->rl_rdmabuf == NULL)
+		goto out_rdmabuf;
 	if (req->rl_sendbuf == NULL)
 		goto out_sendbuf;
 	if (size > req->rl_sendbuf->rg_size)
@@ -480,6 +482,13 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	return req->rl_sendbuf->rg_base;
 
+out_rdmabuf:
+	min_size = RPCRDMA_INLINE_WRITE_THRESHOLD(task->tk_rqstp);
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, min_size, flags);
+	if (IS_ERR(rb))
+		goto out_fail;
+	req->rl_rdmabuf = rb;
+
 out_sendbuf:
 	/* XDR encoding and RPC/RDMA marshaling of this request has not
 	 * yet occurred. Thus a lower bound is needed to prevent buffer

commit 0ca77dc372110cbed4dbac5e867ffdc60ebccf6a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:04:08 2015 -0500

    xprtrdma: Allocate RPC send buffer separately from struct rpcrdma_req
    
    Because internal memory registration is an expensive and synchronous
    operation, xprtrdma pre-registers send and receive buffers at mount
    time, and then re-uses them for each RPC.
    
    A "hardway" allocation is a memory allocation and registration that
    replaces a send buffer during the processing of an RPC. Hardway must
    be done if the RPC send buffer is too small to accommodate an RPC's
    call and reply headers.
    
    For xprtrdma, each RPC send buffer is currently part of struct
    rpcrdma_req so that xprt_rdma_free(), which is passed nothing but
    the address of an RPC send buffer, can find its matching struct
    rpcrdma_req and rpcrdma_rep quickly via container_of / offsetof.
    
    That means that hardway currently has to replace a whole rpcrmda_req
    when it replaces an RPC send buffer. This is often a fairly hefty
    chunk of contiguous memory due to the size of the rl_segments array
    and the fact that both the send and receive buffers are part of
    struct rpcrdma_req.
    
    Some obscure re-use of fields in rpcrdma_req is done so that
    xprt_rdma_free() can detect replaced rpcrdma_req structs, and
    restore the original.
    
    This commit breaks apart the RPC send buffer and struct rpcrdma_req
    so that increasing the size of the rl_segments array does not change
    the alignment of each RPC send buffer. (Increasing rl_segments is
    needed to bump up the maximum r/wsize for NFS/RDMA).
    
    This change opens up some interesting possibilities for improving
    the design of xprt_rdma_allocate().
    
    xprt_rdma_allocate() is now the one place where RPC send buffers
    are allocated or re-allocated, and they are now always left in place
    by xprt_rdma_free().
    
    A large re-allocation that includes both the rl_segments array and
    the RPC send buffer is no longer needed. Send buffer re-allocation
    becomes quite rare. Good send buffer alignment is guaranteed no
    matter what the size of the rl_segments array is.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 808b3c52427a..a9d566227e7e 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -449,77 +449,72 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 /*
  * The RDMA allocate/free functions need the task structure as a place
  * to hide the struct rpcrdma_req, which is necessary for the actual send/recv
- * sequence. For this reason, the recv buffers are attached to send
- * buffers for portions of the RPC. Note that the RPC layer allocates
- * both send and receive buffers in the same call. We may register
- * the receive buffer portion when using reply chunks.
+ * sequence.
+ *
+ * The RPC layer allocates both send and receive buffers in the same call
+ * (rq_send_buf and rq_rcv_buf are both part of a single contiguous buffer).
+ * We may register rq_rcv_buf when using reply chunks.
  */
 static void *
 xprt_rdma_allocate(struct rpc_task *task, size_t size)
 {
 	struct rpc_xprt *xprt = task->tk_rqstp->rq_xprt;
-	struct rpcrdma_req *req, *nreq;
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	struct rpcrdma_regbuf *rb;
+	struct rpcrdma_req *req;
+	size_t min_size;
+	gfp_t flags = task->tk_flags & RPC_TASK_SWAPPER ?
+						GFP_ATOMIC : GFP_NOFS;
 
-	req = rpcrdma_buffer_get(&rpcx_to_rdmax(xprt)->rx_buf);
+	req = rpcrdma_buffer_get(&r_xprt->rx_buf);
 	if (req == NULL)
 		return NULL;
 
-	if (size > req->rl_size) {
-		dprintk("RPC:       %s: size %zd too large for buffer[%zd]: "
-			"prog %d vers %d proc %d\n",
-			__func__, size, req->rl_size,
-			task->tk_client->cl_prog, task->tk_client->cl_vers,
-			task->tk_msg.rpc_proc->p_proc);
-		/*
-		 * Outgoing length shortage. Our inline write max must have
-		 * been configured to perform direct i/o.
-		 *
-		 * This is therefore a large metadata operation, and the
-		 * allocate call was made on the maximum possible message,
-		 * e.g. containing long filename(s) or symlink data. In
-		 * fact, while these metadata operations *might* carry
-		 * large outgoing payloads, they rarely *do*. However, we
-		 * have to commit to the request here, so reallocate and
-		 * register it now. The data path will never require this
-		 * reallocation.
-		 *
-		 * If the allocation or registration fails, the RPC framework
-		 * will (doggedly) retry.
-		 */
-		if (task->tk_flags & RPC_TASK_SWAPPER)
-			nreq = kmalloc(sizeof *req + size, GFP_ATOMIC);
-		else
-			nreq = kmalloc(sizeof *req + size, GFP_NOFS);
-		if (nreq == NULL)
-			goto outfail;
-
-		if (rpcrdma_register_internal(&rpcx_to_rdmax(xprt)->rx_ia,
-				nreq->rl_base, size + sizeof(struct rpcrdma_req)
-				- offsetof(struct rpcrdma_req, rl_base),
-				&nreq->rl_handle, &nreq->rl_iov)) {
-			kfree(nreq);
-			goto outfail;
-		}
-		rpcx_to_rdmax(xprt)->rx_stats.hardway_register_count += size;
-		nreq->rl_size = size;
-		nreq->rl_niovs = 0;
-		nreq->rl_nchunks = 0;
-		nreq->rl_buffer = (struct rpcrdma_buffer *)req;
-		nreq->rl_reply = req->rl_reply;
-		memcpy(nreq->rl_segments,
-			req->rl_segments, sizeof nreq->rl_segments);
-		/* flag the swap with an unused field */
-		nreq->rl_iov.length = 0;
-		req->rl_reply = NULL;
-		req = nreq;
-	}
+	if (req->rl_sendbuf == NULL)
+		goto out_sendbuf;
+	if (size > req->rl_sendbuf->rg_size)
+		goto out_sendbuf;
+
+out:
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 	req->rl_connect_cookie = 0;	/* our reserved value */
-	return req->rl_xdr_buf;
-
-outfail:
+	return req->rl_sendbuf->rg_base;
+
+out_sendbuf:
+	/* XDR encoding and RPC/RDMA marshaling of this request has not
+	 * yet occurred. Thus a lower bound is needed to prevent buffer
+	 * overrun during marshaling.
+	 *
+	 * RPC/RDMA marshaling may choose to send payload bearing ops
+	 * inline, if the result is smaller than the inline threshold.
+	 * The value of the "size" argument accounts for header
+	 * requirements but not for the payload in these cases.
+	 *
+	 * Likewise, allocate enough space to receive a reply up to the
+	 * size of the inline threshold.
+	 *
+	 * It's unlikely that both the send header and the received
+	 * reply will be large, but slush is provided here to allow
+	 * flexibility when marshaling.
+	 */
+	min_size = RPCRDMA_INLINE_READ_THRESHOLD(task->tk_rqstp);
+	min_size += RPCRDMA_INLINE_WRITE_THRESHOLD(task->tk_rqstp);
+	if (size < min_size)
+		size = min_size;
+
+	rb = rpcrdma_alloc_regbuf(&r_xprt->rx_ia, size, flags);
+	if (IS_ERR(rb))
+		goto out_fail;
+	rb->rg_owner = req;
+
+	r_xprt->rx_stats.hardway_register_count += size;
+	rpcrdma_free_regbuf(&r_xprt->rx_ia, req->rl_sendbuf);
+	req->rl_sendbuf = rb;
+	goto out;
+
+out_fail:
 	rpcrdma_buffer_put(req);
-	rpcx_to_rdmax(xprt)->rx_stats.failed_marshal_count++;
+	r_xprt->rx_stats.failed_marshal_count++;
 	return NULL;
 }
 
@@ -531,47 +526,24 @@ xprt_rdma_free(void *buffer)
 {
 	struct rpcrdma_req *req;
 	struct rpcrdma_xprt *r_xprt;
-	struct rpcrdma_rep *rep;
+	struct rpcrdma_regbuf *rb;
 	int i;
 
 	if (buffer == NULL)
 		return;
 
-	req = container_of(buffer, struct rpcrdma_req, rl_xdr_buf[0]);
-	if (req->rl_iov.length == 0) {	/* see allocate above */
-		r_xprt = container_of(((struct rpcrdma_req *) req->rl_buffer)->rl_buffer,
-				      struct rpcrdma_xprt, rx_buf);
-	} else
-		r_xprt = container_of(req->rl_buffer, struct rpcrdma_xprt, rx_buf);
-	rep = req->rl_reply;
+	rb = container_of(buffer, struct rpcrdma_regbuf, rg_base[0]);
+	req = rb->rg_owner;
+	r_xprt = container_of(req->rl_buffer, struct rpcrdma_xprt, rx_buf);
 
-	dprintk("RPC:       %s: called on 0x%p%s\n",
-		__func__, rep, (rep && rep->rr_func) ? " (with waiter)" : "");
+	dprintk("RPC:       %s: called on 0x%p\n", __func__, req->rl_reply);
 
-	/*
-	 * Finish the deregistration.  The process is considered
-	 * complete when the rr_func vector becomes NULL - this
-	 * was put in place during rpcrdma_reply_handler() - the wait
-	 * call below will not block if the dereg is "done". If
-	 * interrupted, our framework will clean up.
-	 */
 	for (i = 0; req->rl_nchunks;) {
 		--req->rl_nchunks;
 		i += rpcrdma_deregister_external(
 			&req->rl_segments[i], r_xprt);
 	}
 
-	if (req->rl_iov.length == 0) {	/* see allocate above */
-		struct rpcrdma_req *oreq = (struct rpcrdma_req *)req->rl_buffer;
-		oreq->rl_reply = req->rl_reply;
-		(void) rpcrdma_deregister_internal(&r_xprt->rx_ia,
-						   req->rl_handle,
-						   &req->rl_iov);
-		kfree(req);
-		req = oreq;
-	}
-
-	/* Put back request+reply buffers */
 	rpcrdma_buffer_put(req);
 }
 

commit ac920d04a7f307bfd7633f60abe33fb626f6ec83
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:03:44 2015 -0500

    xprtrdma: Simplify synopsis of rpcrdma_buffer_create()
    
    Clean up: There is one call site for rpcrdma_buffer_create(). All of
    the arguments there are fields of an rpcrdma_xprt.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index a487bde71b4a..808b3c52427a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -364,8 +364,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	 * any inline data. Also specify any padding which will be provided
 	 * from a preregistered zero buffer.
 	 */
-	rc = rpcrdma_buffer_create(&new_xprt->rx_buf, new_ep, &new_xprt->rx_ia,
-				&new_xprt->rx_data);
+	rc = rpcrdma_buffer_create(new_xprt);
 	if (rc)
 		goto out3;
 

commit afadc468eb309b7c48ffdc8fa4c72acbb9991613
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:03:11 2015 -0500

    xprtrdma: Remove rpcrdma_ep::rep_func and ::rep_xprt
    
    Clean up: The rep_func field always refers to rpcrdma_conn_func().
    rep_func should have been removed by commit b45ccfd25d50 ("xprtrdma:
    Remove MEMWINDOWS registration modes").
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ee5751326339..a487bde71b4a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -376,8 +376,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	 */
 	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
 			  xprt_rdma_connect_worker);
-	new_ep->rep_func = rpcrdma_conn_func;
-	new_ep->rep_xprt = xprt;
 
 	xprt_rdma_format_addresses(xprt);
 	xprt->max_payload = rpcrdma_max_payload(new_xprt);

commit 5abefb861fd4306467813380cf21ce21d4b274ce
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Jan 21 11:02:37 2015 -0500

    xprtrdma: Rename "xprt" and "rdma_connect" fields in struct rpcrdma_xprt
    
    Clean up: Use consistent field names in struct rpcrdma_xprt.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Reviewed-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index bbd6155d3e34..ee5751326339 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -200,9 +200,9 @@ xprt_rdma_free_addresses(struct rpc_xprt *xprt)
 static void
 xprt_rdma_connect_worker(struct work_struct *work)
 {
-	struct rpcrdma_xprt *r_xprt =
-		container_of(work, struct rpcrdma_xprt, rdma_connect.work);
-	struct rpc_xprt *xprt = &r_xprt->xprt;
+	struct rpcrdma_xprt *r_xprt = container_of(work, struct rpcrdma_xprt,
+						   rx_connect_worker.work);
+	struct rpc_xprt *xprt = &r_xprt->rx_xprt;
 	int rc = 0;
 
 	xprt_clear_connected(xprt);
@@ -235,7 +235,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	dprintk("RPC:       %s: called\n", __func__);
 
-	cancel_delayed_work_sync(&r_xprt->rdma_connect);
+	cancel_delayed_work_sync(&r_xprt->rx_connect_worker);
 
 	xprt_clear_connected(xprt);
 
@@ -374,7 +374,8 @@ xprt_setup_rdma(struct xprt_create *args)
 	 * connection loss notification is async. We also catch connection loss
 	 * when reaping receives.
 	 */
-	INIT_DELAYED_WORK(&new_xprt->rdma_connect, xprt_rdma_connect_worker);
+	INIT_DELAYED_WORK(&new_xprt->rx_connect_worker,
+			  xprt_rdma_connect_worker);
 	new_ep->rep_func = rpcrdma_conn_func;
 	new_ep->rep_xprt = xprt;
 
@@ -434,17 +435,17 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 
 	if (r_xprt->rx_ep.rep_connected != 0) {
 		/* Reconnect */
-		schedule_delayed_work(&r_xprt->rdma_connect,
-			xprt->reestablish_timeout);
+		schedule_delayed_work(&r_xprt->rx_connect_worker,
+				      xprt->reestablish_timeout);
 		xprt->reestablish_timeout <<= 1;
 		if (xprt->reestablish_timeout > RPCRDMA_MAX_REEST_TO)
 			xprt->reestablish_timeout = RPCRDMA_MAX_REEST_TO;
 		else if (xprt->reestablish_timeout < RPCRDMA_INIT_REEST_TO)
 			xprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;
 	} else {
-		schedule_delayed_work(&r_xprt->rdma_connect, 0);
+		schedule_delayed_work(&r_xprt->rx_connect_worker, 0);
 		if (!RPC_IS_ASYNC(task))
-			flush_delayed_work(&r_xprt->rdma_connect);
+			flush_delayed_work(&r_xprt->rx_connect_worker);
 	}
 }
 

commit ea5264138d240825a7e3f11d5945d568b74efb91
Merge: 1702562db4bf 7ff11de1bae0
Author: Trond Myklebust <trond.myklebust@primarydata.com>
Date:   Wed Nov 26 17:37:13 2014 -0500

    Merge tag 'nfs-rdma-for-3.19' of git://git.linux-nfs.org/projects/anna/nfs-rdma into linux-next
    
    Pull NFS client RDMA changes for 3.19 from Anna Schumaker:
     "NFS: Client side changes for RDMA
    
      These patches various bugfixes and cleanups for using NFS over RDMA, including
      better error handling and performance improvements by using pad optimization.
    
      Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>"
    
    * tag 'nfs-rdma-for-3.19' of git://git.linux-nfs.org/projects/anna/nfs-rdma:
      xprtrdma: Display async errors
      xprtrdma: Enable pad optimization
      xprtrdma: Re-write rpcrdma_flush_cqs()
      xprtrdma: Refactor tasklet scheduling
      xprtrdma: unmap all FMRs during transport disconnect
      xprtrdma: Cap req_cqinit
      xprtrdma: Return an errno from rpcrdma_register_external()

commit d5440e27d3e572272db957d6b9661a902b7c339f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Nov 8 20:14:53 2014 -0500

    xprtrdma: Enable pad optimization
    
    The Linux NFS/RDMA server used to reject NFSv3 WRITE requests when
    pad optimization was enabled. That bug was fixed by commit
    e560e3b510d2 ("svcrdma: Add zero padding if the client doesn't send
    it").
    
    We can now enable pad optimization on the client, which helps
    performance and is supported now by both Linux and Solaris servers.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index cfe9a810e6bc..8ed25760764b 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -73,7 +73,7 @@ static unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_inline_write_padding;
 static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
-                int xprt_rdma_pad_optimize = 0;
+		int xprt_rdma_pad_optimize = 1;
 
 #ifdef RPC_DEBUG
 

commit 467c9674bccc073684ee34f4bd205cf1b135d76e
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sat Nov 8 20:14:29 2014 -0500

    xprtrdma: unmap all FMRs during transport disconnect
    
    When using RPCRDMA_MTHCAFMR memory registration, after a few
    transport disconnect / reconnect cycles, ib_map_phys_fmr() starts to
    return EINVAL because the provider has exhausted its map pool.
    
    Make sure that all FMRs are unmapped during transport disconnect,
    and that ->send_request remarshals them during an RPC retransmit.
    This resets the transport's MRs to ensure that none are leaked
    during a disconnect.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6a4615dd0261..cfe9a810e6bc 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -599,7 +599,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 
 	if (req->rl_niovs == 0)
 		rc = rpcrdma_marshal_req(rqst);
-	else if (r_xprt->rx_ia.ri_memreg_strategy == RPCRDMA_FRMR)
+	else if (r_xprt->rx_ia.ri_memreg_strategy != RPCRDMA_ALLPHYSICAL)
 		rc = rpcrdma_marshal_chunks(rqst, 0);
 	if (rc < 0)
 		goto failed_marshal;

commit f895b252d4edf66b2895fb5a7b17a638665f3e1f
Author: Jeff Layton <jlayton@primarydata.com>
Date:   Mon Nov 17 16:58:04 2014 -0500

    sunrpc: eliminate RPC_DEBUG
    
    It's always set to whatever CONFIG_SUNRPC_DEBUG is, so just use that.
    
    Signed-off-by: Jeff Layton <jlayton@primarydata.com>
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6a4615dd0261..ef58ebadb3ae 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -55,7 +55,7 @@
 
 #include "xprt_rdma.h"
 
-#ifdef RPC_DEBUG
+#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 # define RPCDBG_FACILITY	RPCDBG_TRANS
 #endif
 
@@ -75,7 +75,7 @@ static unsigned int xprt_rdma_inline_write_padding;
 static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
                 int xprt_rdma_pad_optimize = 0;
 
-#ifdef RPC_DEBUG
+#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 
 static unsigned int min_slot_table_size = RPCRDMA_MIN_SLOT_TABLE;
 static unsigned int max_slot_table_size = RPCRDMA_MAX_SLOT_TABLE;
@@ -705,7 +705,7 @@ static void __exit xprt_rdma_cleanup(void)
 	int rc;
 
 	dprintk("RPCRDMA Module Removed, deregister RPC RDMA transport\n");
-#ifdef RPC_DEBUG
+#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 	if (sunrpc_table_header) {
 		unregister_sysctl_table(sunrpc_table_header);
 		sunrpc_table_header = NULL;
@@ -736,7 +736,7 @@ static int __init xprt_rdma_init(void)
 	dprintk("\tPadding %d\n\tMemreg %d\n",
 		xprt_rdma_inline_write_padding, xprt_rdma_memreg_strategy);
 
-#ifdef RPC_DEBUG
+#if IS_ENABLED(CONFIG_SUNRPC_DEBUG)
 	if (!sunrpc_table_header)
 		sunrpc_table_header = register_sysctl_table(sunrpc_table);
 #endif

commit 1aff52562939485e503936e17934be077ffaea53
Author: NeilBrown <neilb@suse.de>
Date:   Wed Sep 24 11:28:32 2014 +1000

    NFS/SUNRPC: Remove other deadlock-avoidance mechanisms in nfs_release_page()
    
    Now that nfs_release_page() doesn't block indefinitely, other deadlock
    avoidance mechanisms aren't needed.
     - it doesn't hurt for kswapd to block occasionally.  If it doesn't
       want to block it would clear __GFP_WAIT.  The current_is_kswapd()
       was only added to avoid deadlocks and we have a new approach for
       that.
     - memory allocation in the SUNRPC layer can very rarely try to
       ->releasepage() a page it is trying to handle.  The deadlock
       is removed as nfs_release_page() doesn't block indefinitely.
    
    So we don't need to set PF_FSTRANS for sunrpc network operations any
    more.
    
    Signed-off-by: NeilBrown <neilb@suse.de>
    Acked-by: Jeff Layton <jlayton@primarydata.com>
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 2faac4940563..6a4615dd0261 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -205,7 +205,6 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	struct rpc_xprt *xprt = &r_xprt->xprt;
 	int rc = 0;
 
-	current->flags |= PF_FSTRANS;
 	xprt_clear_connected(xprt);
 
 	dprintk("RPC:       %s: %sconnect\n", __func__,
@@ -216,7 +215,6 @@ xprt_rdma_connect_worker(struct work_struct *work)
 
 	dprintk("RPC:       %s: exit\n", __func__);
 	xprt_clear_connecting(xprt);
-	current->flags &= ~PF_FSTRANS;
 }
 
 /*

commit 282191cb725db9a1aa80269e8369b06e9270a948
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:25:55 2014 -0400

    xprtrdma: Make rpcrdma_ep_disconnect() return void
    
    Clean up: The return code is used only for dprintk's that are
    already redundant.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index f6d280b31dc9..2faac4940563 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -414,7 +414,7 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 	if (r_xprt->rx_ep.rep_connected > 0)
 		xprt->reestablish_timeout = 0;
 	xprt_disconnect_done(xprt);
-	(void) rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
 }
 
 static void

commit 6ab59945f292a5c6cbc4a6c2011f1a732a116af2
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:23:43 2014 -0400

    xprtrdma: Update rkeys after transport reconnect
    
    Various reports of:
    
      rpcrdma_qp_async_error_upcall: QP error 3 on device mlx4_0
                    ep ffff8800bfd3e848
    
    Ensure that rkeys in already-marshalled RPC/RDMA headers are
    refreshed after the QP has been replaced by a reconnect.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=249
    Suggested-by: Selvin Xavier <Selvin.Xavier@Emulex.Com>
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 418510202919..f6d280b31dc9 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -597,13 +597,14 @@ xprt_rdma_send_request(struct rpc_task *task)
 	struct rpc_xprt *xprt = rqst->rq_xprt;
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
-	int rc;
+	int rc = 0;
 
-	if (req->rl_niovs == 0) {
+	if (req->rl_niovs == 0)
 		rc = rpcrdma_marshal_req(rqst);
-		if (rc < 0)
-			goto failed_marshal;
-	}
+	else if (r_xprt->rx_ia.ri_memreg_strategy == RPCRDMA_FRMR)
+		rc = rpcrdma_marshal_chunks(rqst, 0);
+	if (rc < 0)
+		goto failed_marshal;
 
 	if (req->rl_reply == NULL) 		/* e.g. reconnection */
 		rpcrdma_recv_buffer_get(req);

commit 43e95988178ed70a878a5be6be9ad248342dbf7d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Tue Jul 29 17:23:34 2014 -0400

    xprtrdma: Limit data payload size for ALLPHYSICAL
    
    When the client uses physical memory registration, each page in the
    payload gets its own array entry in the RPC/RDMA header's chunk list.
    
    Therefore, don't advertise a maximum payload size that would require
    more array entries than can fit in the RPC buffer where RPC/RDMA
    headers are built.
    
    BugLink: https://bugzilla.linux-nfs.org/show_bug.cgi?id=248
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Tested-by: Shirley Ma <shirley.ma@oracle.com>
    Tested-by: Devesh Sharma <devesh.sharma@emulex.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 66f91f0d071a..418510202919 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -296,7 +296,6 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	xprt->resvport = 0;		/* privileged port not needed */
 	xprt->tsh_size = 0;		/* RPC-RDMA handles framing */
-	xprt->max_payload = RPCRDMA_MAX_DATA_SEGS * PAGE_SIZE;
 	xprt->ops = &xprt_rdma_procs;
 
 	/*
@@ -382,6 +381,9 @@ xprt_setup_rdma(struct xprt_create *args)
 	new_ep->rep_xprt = xprt;
 
 	xprt_rdma_format_addresses(xprt);
+	xprt->max_payload = rpcrdma_max_payload(new_xprt);
+	dprintk("RPC:       %s: transport data payload maximum: %zu bytes\n",
+		__func__, xprt->max_payload);
 
 	if (!try_module_get(THIS_MODULE))
 		goto out4;

commit c93c62231cf55df4a26bd08937efeea97e6fc5e8
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:35:14 2014 -0400

    xprtrdma: Disconnect on registration failure
    
    If rpcrdma_register_external() fails during request marshaling, the
    current RPC request is killed. Instead, this RPC should be retried
    after reconnecting the transport instance.
    
    The most likely reason for registration failure with FRMR is a
    failed post_send, which would be due to a remote transport
    disconnect or memory exhaustion. These issues can be recovered
    by a retry.
    
    Problems encountered in the marshaling logic itself will not be
    corrected by trying again, so these should still kill a request.
    
    Now that we've added a clean exit for marshaling errors, take the
    opportunity to defang some BUG_ON's.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 93fe7753ff94..66f91f0d071a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -595,13 +595,12 @@ xprt_rdma_send_request(struct rpc_task *task)
 	struct rpc_xprt *xprt = rqst->rq_xprt;
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	int rc;
 
-	/* marshal the send itself */
-	if (req->rl_niovs == 0 && rpcrdma_marshal_req(rqst) != 0) {
-		r_xprt->rx_stats.failed_marshal_count++;
-		dprintk("RPC:       %s: rpcrdma_marshal_req failed\n",
-			__func__);
-		return -EIO;
+	if (req->rl_niovs == 0) {
+		rc = rpcrdma_marshal_req(rqst);
+		if (rc < 0)
+			goto failed_marshal;
 	}
 
 	if (req->rl_reply == NULL) 		/* e.g. reconnection */
@@ -625,6 +624,12 @@ xprt_rdma_send_request(struct rpc_task *task)
 	rqst->rq_bytes_sent = 0;
 	return 0;
 
+failed_marshal:
+	r_xprt->rx_stats.failed_marshal_count++;
+	dprintk("RPC:       %s: rpcrdma_marshal_req failed, status %i\n",
+		__func__, rc);
+	if (rc == -EIO)
+		return -EIO;
 drop_connection:
 	xprt_disconnect_done(xprt);
 	return -ENOTCONN;	/* implies disconnect */

commit c977dea22708688eae31774f70126c97aa4dfe83
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:35:06 2014 -0400

    xprtrdma: Remove BUG_ON() call sites
    
    If an error occurs in the marshaling logic, fail the RPC request
    being processed, but leave the client running.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 187894b4c33f..93fe7753ff94 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -463,7 +463,8 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	struct rpcrdma_req *req, *nreq;
 
 	req = rpcrdma_buffer_get(&rpcx_to_rdmax(xprt)->rx_buf);
-	BUG_ON(NULL == req);
+	if (req == NULL)
+		return NULL;
 
 	if (size > req->rl_size) {
 		dprintk("RPC:       %s: size %zd too large for buffer[%zd]: "

commit e7ce710a8802351bd4118c5d6136c1d850f67cf9
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:34:57 2014 -0400

    xprtrdma: Avoid deadlock when credit window is reset
    
    Update the cwnd while processing the server's reply.  Otherwise the
    next task on the xprt_sending queue is still subject to the old
    credit window. Currently, no task is awoken if the old congestion
    window is still exceeded, even if the new window is larger, and a
    deadlock results.
    
    This is an issue during a transport reconnect. Servers don't
    normally shrink the credit window, but the client does reset it to
    1 when reconnecting so the server can safely grow it again.
    
    As a minor optimization, remove the hack of grabbing the initial
    cwnd size (which happens to be RPC_CWNDSCALE) and using that value
    as the congestion scaling factor. The scaling value is invariant,
    and we are better off without the multiplication operation.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6b84d7d59e18..187894b4c33f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -448,23 +448,6 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 	}
 }
 
-static int
-xprt_rdma_reserve_xprt(struct rpc_xprt *xprt, struct rpc_task *task)
-{
-	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
-	int credits = atomic_read(&r_xprt->rx_buf.rb_credits);
-
-	/* == RPC_CWNDSCALE @ init, but *after* setup */
-	if (r_xprt->rx_buf.rb_cwndscale == 0UL) {
-		r_xprt->rx_buf.rb_cwndscale = xprt->cwnd;
-		dprintk("RPC:       %s: cwndscale %lu\n", __func__,
-			r_xprt->rx_buf.rb_cwndscale);
-		BUG_ON(r_xprt->rx_buf.rb_cwndscale <= 0);
-	}
-	xprt->cwnd = credits * r_xprt->rx_buf.rb_cwndscale;
-	return xprt_reserve_xprt_cong(xprt, task);
-}
-
 /*
  * The RDMA allocate/free functions need the task structure as a place
  * to hide the struct rpcrdma_req, which is necessary for the actual send/recv
@@ -686,7 +669,7 @@ static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
  */
 
 static struct rpc_xprt_ops xprt_rdma_procs = {
-	.reserve_xprt		= xprt_rdma_reserve_xprt,
+	.reserve_xprt		= xprt_reserve_xprt_cong,
 	.release_xprt		= xprt_release_xprt_cong, /* sunrpc/xprt.c */
 	.alloc_slot		= xprt_alloc_slot,
 	.release_request	= xprt_release_rqst_cong,       /* ditto */

commit bfaee096deaa680195df5491eb650f81051c145d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:34:32 2014 -0400

    xprtrdma: Use macros for reconnection timeout constants
    
    Clean up: Ensure the same max and min constant values are used
    everywhere when setting reconnect timeouts.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d18b2a364762..6b84d7d59e18 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -149,6 +149,11 @@ static struct ctl_table sunrpc_table[] = {
 
 #endif
 
+#define RPCRDMA_BIND_TO		(60U * HZ)
+#define RPCRDMA_INIT_REEST_TO	(5U * HZ)
+#define RPCRDMA_MAX_REEST_TO	(30U * HZ)
+#define RPCRDMA_IDLE_DISC_TO	(5U * 60 * HZ)
+
 static struct rpc_xprt_ops xprt_rdma_procs;	/* forward reference */
 
 static void
@@ -285,9 +290,9 @@ xprt_setup_rdma(struct xprt_create *args)
 
 	/* 60 second timeout, no retries */
 	xprt->timeout = &xprt_rdma_default_timeout;
-	xprt->bind_timeout = (60U * HZ);
-	xprt->reestablish_timeout = (5U * HZ);
-	xprt->idle_timeout = (5U * 60 * HZ);
+	xprt->bind_timeout = RPCRDMA_BIND_TO;
+	xprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;
+	xprt->idle_timeout = RPCRDMA_IDLE_DISC_TO;
 
 	xprt->resvport = 0;		/* privileged port not needed */
 	xprt->tsh_size = 0;		/* RPC-RDMA handles framing */
@@ -432,10 +437,10 @@ xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 		schedule_delayed_work(&r_xprt->rdma_connect,
 			xprt->reestablish_timeout);
 		xprt->reestablish_timeout <<= 1;
-		if (xprt->reestablish_timeout > (30 * HZ))
-			xprt->reestablish_timeout = (30 * HZ);
-		else if (xprt->reestablish_timeout < (5 * HZ))
-			xprt->reestablish_timeout = (5 * HZ);
+		if (xprt->reestablish_timeout > RPCRDMA_MAX_REEST_TO)
+			xprt->reestablish_timeout = RPCRDMA_MAX_REEST_TO;
+		else if (xprt->reestablish_timeout < RPCRDMA_INIT_REEST_TO)
+			xprt->reestablish_timeout = RPCRDMA_INIT_REEST_TO;
 	} else {
 		schedule_delayed_work(&r_xprt->rdma_connect, 0);
 		if (!RPC_IS_ASYNC(task))

commit 7f1d54191ed6fa0f79f584fe3ebf6519738e817f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:33:16 2014 -0400

    xprtrdma: Make rpcrdma_ep_destroy() return void
    
    Clean up: rpcrdma_ep_destroy() returns a value that is used
    only to print a debugging message. rpcrdma_ep_destroy() already
    prints debugging messages in all error cases.
    
    Make rpcrdma_ep_destroy() return void instead.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 430cabba5a81..d18b2a364762 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -229,7 +229,6 @@ static void
 xprt_rdma_destroy(struct rpc_xprt *xprt)
 {
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
-	int rc;
 
 	dprintk("RPC:       %s: called\n", __func__);
 
@@ -238,10 +237,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 	xprt_clear_connected(xprt);
 
 	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
-	rc = rpcrdma_ep_destroy(&r_xprt->rx_ep, &r_xprt->rx_ia);
-	if (rc)
-		dprintk("RPC:       %s: rpcrdma_ep_destroy returned %i\n",
-			__func__, rc);
+	rpcrdma_ep_destroy(&r_xprt->rx_ep, &r_xprt->rx_ia);
 	rpcrdma_ia_close(&r_xprt->rx_ia);
 
 	xprt_rdma_free_addresses(xprt);
@@ -391,7 +387,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	xprt_rdma_free_addresses(xprt);
 	rc = -EINVAL;
 out3:
-	(void) rpcrdma_ep_destroy(new_ep, &new_xprt->rx_ia);
+	rpcrdma_ep_destroy(new_ep, &new_xprt->rx_ia);
 out2:
 	rpcrdma_ia_close(&new_xprt->rx_ia);
 out1:

commit 13c9ff8f673862b69e795ea99a237b461c557eb3
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:33:08 2014 -0400

    xprtrdma: Simplify rpcrdma_deregister_external() synopsis
    
    Clean up: All remaining callers of rpcrdma_deregister_external()
    pass NULL as the last argument, so remove that argument.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c23b0c13f77f..430cabba5a81 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -575,7 +575,7 @@ xprt_rdma_free(void *buffer)
 	for (i = 0; req->rl_nchunks;) {
 		--req->rl_nchunks;
 		i += rpcrdma_deregister_external(
-			&req->rl_segments[i], r_xprt, NULL);
+			&req->rl_segments[i], r_xprt);
 	}
 
 	if (req->rl_iov.length == 0) {	/* see allocate above */

commit b45ccfd25d506e83d9ecf93d0ac7edf031d35d2f
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:32:34 2014 -0400

    xprtrdma: Remove MEMWINDOWS registration modes
    
    The MEMWINDOWS and MEMWINDOWS_ASYNC memory registration modes were
    intended as stop-gap modes before the introduction of FRMR. They
    are now considered obsolete.
    
    MEMWINDOWS_ASYNC is also considered unsafe because it can leave
    client memory registered and exposed for an indeterminant time after
    each I/O.
    
    At this point, the MEMWINDOWS modes add needless complexity, so
    remove them.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 8c5035a2792e..c23b0c13f77f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -566,9 +566,7 @@ xprt_rdma_free(void *buffer)
 		__func__, rep, (rep && rep->rr_func) ? " (with waiter)" : "");
 
 	/*
-	 * Finish the deregistration. When using mw bind, this was
-	 * begun in rpcrdma_reply_handler(). In all other modes, we
-	 * do it here, in thread context. The process is considered
+	 * Finish the deregistration.  The process is considered
 	 * complete when the rr_func vector becomes NULL - this
 	 * was put in place during rpcrdma_reply_handler() - the wait
 	 * call below will not block if the dereg is "done". If
@@ -580,11 +578,6 @@ xprt_rdma_free(void *buffer)
 			&req->rl_segments[i], r_xprt, NULL);
 	}
 
-	if (rep && wait_event_interruptible(rep->rr_unbind, !rep->rr_func)) {
-		rep->rr_func = NULL;	/* abandon the callback */
-		req->rl_reply = NULL;
-	}
-
 	if (req->rl_iov.length == 0) {	/* see allocate above */
 		struct rpcrdma_req *oreq = (struct rpcrdma_req *)req->rl_buffer;
 		oreq->rl_reply = req->rl_reply;

commit 03ff8821eb5ed168792667cfc3ddff903e97af99
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed May 28 10:32:26 2014 -0400

    xprtrdma: Remove BOUNCEBUFFERS memory registration mode
    
    Clean up: This memory registration mode is slow and was never
    meant for use in production environments. Remove it to reduce
    implementation complexity.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Anna Schumaker <Anna.Schumaker@Netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 1eb9c468d0c9..8c5035a2792e 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -503,18 +503,6 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 		 * If the allocation or registration fails, the RPC framework
 		 * will (doggedly) retry.
 		 */
-		if (rpcx_to_rdmax(xprt)->rx_ia.ri_memreg_strategy ==
-				RPCRDMA_BOUNCEBUFFERS) {
-			/* forced to "pure inline" */
-			dprintk("RPC:       %s: too much data (%zd) for inline "
-					"(r/w max %d/%d)\n", __func__, size,
-					rpcx_to_rdmad(xprt).inline_rsize,
-					rpcx_to_rdmad(xprt).inline_wsize);
-			size = req->rl_size;
-			rpc_exit(task, -EIO);		/* fail the operation */
-			rpcx_to_rdmax(xprt)->rx_stats.failed_marshal_count++;
-			goto out;
-		}
 		if (task->tk_flags & RPC_TASK_SWAPPER)
 			nreq = kmalloc(sizeof *req + size, GFP_ATOMIC);
 		else
@@ -543,7 +531,6 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 		req = nreq;
 	}
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
-out:
 	req->rl_connect_cookie = 0;	/* our reserved value */
 	return req->rl_xdr_buf;
 

commit 3a0799a94c0384a3b275a73267aaa10517b1bf7d
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Wed Mar 12 12:51:39 2014 -0400

    SUNRPC: remove KERN_INFO from dprintk() call sites
    
    The use of KERN_INFO causes garbage characters to appear when
    debugging is enabled.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <trond.myklebust@primarydata.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 285dc0884115..1eb9c468d0c9 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -733,7 +733,7 @@ static void __exit xprt_rdma_cleanup(void)
 {
 	int rc;
 
-	dprintk(KERN_INFO "RPCRDMA Module Removed, deregister RPC RDMA transport\n");
+	dprintk("RPCRDMA Module Removed, deregister RPC RDMA transport\n");
 #ifdef RPC_DEBUG
 	if (sunrpc_table_header) {
 		unregister_sysctl_table(sunrpc_table_header);
@@ -755,14 +755,14 @@ static int __init xprt_rdma_init(void)
 	if (rc)
 		return rc;
 
-	dprintk(KERN_INFO "RPCRDMA Module Init, register RPC RDMA transport\n");
+	dprintk("RPCRDMA Module Init, register RPC RDMA transport\n");
 
-	dprintk(KERN_INFO "Defaults:\n");
-	dprintk(KERN_INFO "\tSlots %d\n"
+	dprintk("Defaults:\n");
+	dprintk("\tSlots %d\n"
 		"\tMaxInlineRead %d\n\tMaxInlineWrite %d\n",
 		xprt_rdma_slot_table_entries,
 		xprt_rdma_max_inline_read, xprt_rdma_max_inline_write);
-	dprintk(KERN_INFO "\tPadding %d\n\tMemreg %d\n",
+	dprintk("\tPadding %d\n\tMemreg %d\n",
 		xprt_rdma_inline_write_padding, xprt_rdma_memreg_strategy);
 
 #ifdef RPC_DEBUG

commit fe2c6338fd2c6f383c4d4164262f35c8f3708e1f
Author: Joe Perches <joe@perches.com>
Date:   Tue Jun 11 23:04:25 2013 -0700

    net: Convert uses of typedef ctl_table to struct ctl_table
    
    Reduce the uses of this unnecessary typedef.
    
    Done via perl script:
    
    $ git grep --name-only -w ctl_table net | \
      xargs perl -p -i -e '\
            sub trim { my ($local) = @_; $local =~ s/(^\s+|\s+$)//g; return $local; } \
            s/\b(?<!struct\s)ctl_table\b(\s*\*\s*|\s+\w+)/"struct ctl_table " . trim($1)/ge'
    
    Reflow the modified lines that now exceed 80 columns.
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 794312f22b9b..285dc0884115 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -86,7 +86,7 @@ static unsigned int max_memreg = RPCRDMA_LAST - 1;
 
 static struct ctl_table_header *sunrpc_table_header;
 
-static ctl_table xr_tunables_table[] = {
+static struct ctl_table xr_tunables_table[] = {
 	{
 		.procname	= "rdma_slot_table_entries",
 		.data		= &xprt_rdma_slot_table_entries,
@@ -138,7 +138,7 @@ static ctl_table xr_tunables_table[] = {
 	{ },
 };
 
-static ctl_table sunrpc_table[] = {
+static struct ctl_table sunrpc_table[] = {
 	{
 		.procname	= "sunrpc",
 		.mode		= 0555,

commit b6669737d3db7df79fad07180837c23dbe581db5
Merge: 1cf0209c431f dc107402ae06
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 28 18:02:55 2013 -0800

    Merge branch 'for-3.9' of git://linux-nfs.org/~bfields/linux
    
    Pull nfsd changes from J Bruce Fields:
     "Miscellaneous bugfixes, plus:
    
       - An overhaul of the DRC cache by Jeff Layton.  The main effect is
         just to make it larger.  This decreases the chances of intermittent
         errors especially in the UDP case.  But we'll need to watch for any
         reports of performance regressions.
    
       - Containerized nfsd: with some limitations, we now support
         per-container nfs-service, thanks to extensive work from Stanislav
         Kinsbursky over the last year."
    
    Some notes about conflicts, since there were *two* non-data semantic
    conflicts here:
    
     - idr_remove_all() had been added by a memory leak fix, but has since
       become deprecated since idr_destroy() does it for us now.
    
     - xs_local_connect() had been added by this branch to make AF_LOCAL
       connections be synchronous, but in the meantime Trond had changed the
       calling convention in order to avoid a RCU dereference.
    
    There were a couple of more obvious actual source-level conflicts due to
    the hlist traversal changes and one just due to code changes next to
    each other, but those were trivial.
    
    * 'for-3.9' of git://linux-nfs.org/~bfields/linux: (49 commits)
      SUNRPC: make AF_LOCAL connect synchronous
      nfsd: fix compiler warning about ambiguous types in nfsd_cache_csum
      svcrpc: fix rpc server shutdown races
      svcrpc: make svc_age_temp_xprts enqueue under sv_lock
      lockd: nlmclnt_reclaim(): avoid stack overflow
      nfsd: enable NFSv4 state in containers
      nfsd: disable usermode helper client tracker in container
      nfsd: use proper net while reading "exports" file
      nfsd: containerize NFSd filesystem
      nfsd: fix comments on nfsd_cache_lookup
      SUNRPC: move cache_detail->cache_request callback call to cache_read()
      SUNRPC: remove "cache_request" argument in sunrpc_cache_pipe_upcall() function
      SUNRPC: rework cache upcall logic
      SUNRPC: introduce cache_detail->cache_request callback
      NFS: simplify and clean cache library
      NFS: use SUNRPC cache creation and destruction helper for DNS cache
      nfsd4: free_stid can be static
      nfsd: keep a checksum of the first 256 bytes of request
      sunrpc: trim off trailing checksum before returning decrypted or integrity authenticated buffer
      sunrpc: fix comment in struct xdr_buf definition
      ...

commit 5976687a2b3d1969f02aba16b80ad3ed79be6ad3
Author: Jeff Layton <jlayton@redhat.com>
Date:   Mon Feb 4 12:50:00 2013 -0500

    sunrpc: move address copy/cmp/convert routines and prototypes from clnt.h to addr.h
    
    These routines are used by server and client code, so having them in a
    separate header would be best.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Acked-by: Trond Myklebust <Trond.Myklebust@netapp.com>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c9aa7a35f3bf..66591dcd7e6b 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -51,6 +51,7 @@
 #include <linux/init.h>
 #include <linux/slab.h>
 #include <linux/seq_file.h>
+#include <linux/sunrpc/addr.h>
 
 #include "xprt_rdma.h"
 

commit 1b092092bf0e2e8b7af1c2a03f615b4e60b05d47
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Tue Jan 8 09:26:49 2013 -0500

    SUNRPC: Pass a pointer to struct rpc_xprt to the connect callback
    
    Avoid another RCU dereference by passing the pointer to struct rpc_xprt
    from the caller.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index b357c528d29c..d0074289708e 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -426,9 +426,8 @@ xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
 }
 
 static void
-xprt_rdma_connect(struct rpc_task *task)
+xprt_rdma_connect(struct rpc_xprt *xprt, struct rpc_task *task)
 {
-	struct rpc_xprt *xprt = (struct rpc_xprt *)task->tk_xprt;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
 	if (r_xprt->rx_ep.rep_connected != 0) {

commit a4f0835c604f80f945ab3e72ffd00547145c4b2b
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Tue Jan 8 09:10:21 2013 -0500

    SUNRPC: Eliminate task->tk_xprt accesses that bypass rcu_dereference()
    
    tk_xprt is just a shortcut for tk_client->cl_xprt, however cl_xprt is
    defined as an __rcu variable. Replace dereferences of tk_xprt with
    non-rcu dereferences where it is safe to do so.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c9aa7a35f3bf..b357c528d29c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -475,7 +475,7 @@ xprt_rdma_reserve_xprt(struct rpc_xprt *xprt, struct rpc_task *task)
 static void *
 xprt_rdma_allocate(struct rpc_task *task, size_t size)
 {
-	struct rpc_xprt *xprt = task->tk_xprt;
+	struct rpc_xprt *xprt = task->tk_rqstp->rq_xprt;
 	struct rpcrdma_req *req, *nreq;
 
 	req = rpcrdma_buffer_get(&rpcx_to_rdmax(xprt)->rx_buf);
@@ -627,7 +627,7 @@ static int
 xprt_rdma_send_request(struct rpc_task *task)
 {
 	struct rpc_rqst *rqst = task->tk_rqstp;
-	struct rpc_xprt *xprt = task->tk_xprt;
+	struct rpc_xprt *xprt = rqst->rq_xprt;
 	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 

commit d19751e7b9bd8a01d00372325439589886674f79
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Tue Sep 11 17:21:25 2012 -0400

    SUNRPC: Get rid of the redundant xprt->shutdown bit field
    
    It is only set after everyone has dereferenced the transport,
    and serves no useful purpose: setting it is racy, so all the
    socket code, etc still needs to be able to cope with the cases
    where they miss reading it.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 5d9202dc7cb1..c9aa7a35f3bf 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -199,21 +199,15 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	struct rpc_xprt *xprt = &r_xprt->xprt;
 	int rc = 0;
 
-	if (!xprt->shutdown) {
-		current->flags |= PF_FSTRANS;
-		xprt_clear_connected(xprt);
-
-		dprintk("RPC:       %s: %sconnect\n", __func__,
-				r_xprt->rx_ep.rep_connected != 0 ? "re" : "");
-		rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
-		if (rc)
-			goto out;
-	}
-	goto out_clear;
+	current->flags |= PF_FSTRANS;
+	xprt_clear_connected(xprt);
+
+	dprintk("RPC:       %s: %sconnect\n", __func__,
+			r_xprt->rx_ep.rep_connected != 0 ? "re" : "");
+	rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	if (rc)
+		xprt_wake_pending_tasks(xprt, rc);
 
-out:
-	xprt_wake_pending_tasks(xprt, rc);
-out_clear:
 	dprintk("RPC:       %s: exit\n", __func__);
 	xprt_clear_connecting(xprt);
 	current->flags &= ~PF_FSTRANS;

commit f39c1bfb5a03e2d255451bff05be0d7255298fa4
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Fri Sep 7 11:08:50 2012 -0400

    SUNRPC: Fix a UDP transport regression
    
    Commit 43cedbf0e8dfb9c5610eb7985d5f21263e313802 (SUNRPC: Ensure that
    we grab the XPRT_LOCK before calling xprt_alloc_slot) is causing
    hangs in the case of NFS over UDP mounts.
    
    Since neither the UDP or the RDMA transport mechanism use dynamic slot
    allocation, we can skip grabbing the socket lock for those transports.
    Add a new rpc_xprt_op to allow switching between the TCP and UDP/RDMA
    case.
    
    Note that the NFSv4.1 back channel assigns the slot directly
    through rpc_run_bc_task, so we can ignore that case.
    
    Reported-by: Dick Streefland <dick.streefland@altium.nl>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: stable@vger.kernel.org [>= 3.1]

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 06cdbff79e4a..5d9202dc7cb1 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -713,6 +713,7 @@ static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
 static struct rpc_xprt_ops xprt_rdma_procs = {
 	.reserve_xprt		= xprt_rdma_reserve_xprt,
 	.release_xprt		= xprt_release_xprt_cong, /* sunrpc/xprt.c */
+	.alloc_slot		= xprt_alloc_slot,
 	.release_request	= xprt_release_rqst_cong,       /* ditto */
 	.set_retrans_timeout	= xprt_set_retrans_timeout_def, /* ditto */
 	.rpcbind		= rpcb_getport_async,	/* sunrpc/rpcb_clnt.c */

commit 5cf02d09b50b1ee1c2d536c9cf64af5a7d433f56
Author: Jeff Layton <jlayton@redhat.com>
Date:   Mon Jul 23 13:58:51 2012 -0400

    nfs: skip commit in releasepage if we're freeing memory for fs-related reasons
    
    We've had some reports of a deadlock where rpciod ends up with a stack
    trace like this:
    
        PID: 2507   TASK: ffff88103691ab40  CPU: 14  COMMAND: "rpciod/14"
         #0 [ffff8810343bf2f0] schedule at ffffffff814dabd9
         #1 [ffff8810343bf3b8] nfs_wait_bit_killable at ffffffffa038fc04 [nfs]
         #2 [ffff8810343bf3c8] __wait_on_bit at ffffffff814dbc2f
         #3 [ffff8810343bf418] out_of_line_wait_on_bit at ffffffff814dbcd8
         #4 [ffff8810343bf488] nfs_commit_inode at ffffffffa039e0c1 [nfs]
         #5 [ffff8810343bf4f8] nfs_release_page at ffffffffa038bef6 [nfs]
         #6 [ffff8810343bf528] try_to_release_page at ffffffff8110c670
         #7 [ffff8810343bf538] shrink_page_list.clone.0 at ffffffff81126271
         #8 [ffff8810343bf668] shrink_inactive_list at ffffffff81126638
         #9 [ffff8810343bf818] shrink_zone at ffffffff8112788f
        #10 [ffff8810343bf8c8] do_try_to_free_pages at ffffffff81127b1e
        #11 [ffff8810343bf958] try_to_free_pages at ffffffff8112812f
        #12 [ffff8810343bfa08] __alloc_pages_nodemask at ffffffff8111fdad
        #13 [ffff8810343bfb28] kmem_getpages at ffffffff81159942
        #14 [ffff8810343bfb58] fallback_alloc at ffffffff8115a55a
        #15 [ffff8810343bfbd8] ____cache_alloc_node at ffffffff8115a2d9
        #16 [ffff8810343bfc38] kmem_cache_alloc at ffffffff8115b09b
        #17 [ffff8810343bfc78] sk_prot_alloc at ffffffff81411808
        #18 [ffff8810343bfcb8] sk_alloc at ffffffff8141197c
        #19 [ffff8810343bfce8] inet_create at ffffffff81483ba6
        #20 [ffff8810343bfd38] __sock_create at ffffffff8140b4a7
        #21 [ffff8810343bfd98] xs_create_sock at ffffffffa01f649b [sunrpc]
        #22 [ffff8810343bfdd8] xs_tcp_setup_socket at ffffffffa01f6965 [sunrpc]
        #23 [ffff8810343bfe38] worker_thread at ffffffff810887d0
        #24 [ffff8810343bfee8] kthread at ffffffff8108dd96
        #25 [ffff8810343bff48] kernel_thread at ffffffff8100c1ca
    
    rpciod is trying to allocate memory for a new socket to talk to the
    server. The VM ends up calling ->releasepage to get more memory, and it
    tries to do a blocking commit. That commit can't succeed however without
    a connected socket, so we deadlock.
    
    Fix this by setting PF_FSTRANS on the workqueue task prior to doing the
    socket allocation, and having nfs_release_page check for that flag when
    deciding whether to do a commit call. Also, set PF_FSTRANS
    unconditionally in rpc_async_schedule since that function can also do
    allocations sometimes.
    
    Signed-off-by: Jeff Layton <jlayton@redhat.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>
    Cc: stable@vger.kernel.org

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index b446e100286f..06cdbff79e4a 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -200,6 +200,7 @@ xprt_rdma_connect_worker(struct work_struct *work)
 	int rc = 0;
 
 	if (!xprt->shutdown) {
+		current->flags |= PF_FSTRANS;
 		xprt_clear_connected(xprt);
 
 		dprintk("RPC:       %s: %sconnect\n", __func__,
@@ -212,10 +213,10 @@ xprt_rdma_connect_worker(struct work_struct *work)
 
 out:
 	xprt_wake_pending_tasks(xprt, rc);
-
 out_clear:
 	dprintk("RPC:       %s: exit\n", __func__);
 	xprt_clear_connecting(xprt);
+	current->flags &= ~PF_FSTRANS;
 }
 
 /*

commit d9ba131d8f58c0d2ff5029e7002ab43f913b36f9
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Sun Jul 17 18:11:30 2011 -0400

    SUNRPC: Support dynamic slot allocation for TCP connections
    
    Allow the number of available slots to grow with the TCP window size.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 674a49224450..b446e100286f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -283,6 +283,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	}
 
 	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt),
+			xprt_rdma_slot_table_entries,
 			xprt_rdma_slot_table_entries);
 	if (xprt == NULL) {
 		dprintk("RPC:       %s: couldn't allocate rpcrdma_xprt\n",

commit 43cedbf0e8dfb9c5610eb7985d5f21263e313802
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Sun Jul 17 16:01:03 2011 -0400

    SUNRPC: Ensure that we grab the XPRT_LOCK before calling xprt_alloc_slot
    
    This throttles the allocation of new slots when the socket is busy
    reconnecting and/or is out of buffer space.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 0867070bb5ca..674a49224450 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -452,9 +452,8 @@ xprt_rdma_connect(struct rpc_task *task)
 }
 
 static int
-xprt_rdma_reserve_xprt(struct rpc_task *task)
+xprt_rdma_reserve_xprt(struct rpc_xprt *xprt, struct rpc_task *task)
 {
-	struct rpc_xprt *xprt = task->tk_xprt;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 	int credits = atomic_read(&r_xprt->rx_buf.rb_credits);
 
@@ -466,7 +465,7 @@ xprt_rdma_reserve_xprt(struct rpc_task *task)
 		BUG_ON(r_xprt->rx_buf.rb_cwndscale <= 0);
 	}
 	xprt->cwnd = credits * r_xprt->rx_buf.rb_cwndscale;
-	return xprt_reserve_xprt_cong(task);
+	return xprt_reserve_xprt_cong(xprt, task);
 }
 
 /*

commit a25e758c5fa1137e1bbc440194e55f7c59177145
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Oct 15 17:49:27 2010 +0200

    sunrpc/xprtrdma: clean up workqueue usage
    
    * Create and use svc_rdma_wq instead of using the system workqueue and
      flush_scheduled_work().  This workqueue is necessary to serve as
      flushing domain for rdma->sc_work which is used to destroy itself
      and thus can't be flushed explicitly.
    
    * Replace cancel_delayed_work() + flush_scheduled_work() with
      cancel_delayed_work_sync().
    
    * Implement synchronous connect in xprt_rdma_connect() using
      flush_delayed_work() on the rdma_connect work instead of using
      flush_scheduled_work().
    
    This is to prepare for the deprecation and removal of
    flush_scheduled_work().
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 2da32b40bfcf..0867070bb5ca 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -237,8 +237,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	dprintk("RPC:       %s: called\n", __func__);
 
-	cancel_delayed_work(&r_xprt->rdma_connect);
-	flush_scheduled_work();
+	cancel_delayed_work_sync(&r_xprt->rdma_connect);
 
 	xprt_clear_connected(xprt);
 
@@ -448,7 +447,7 @@ xprt_rdma_connect(struct rpc_task *task)
 	} else {
 		schedule_delayed_work(&r_xprt->rdma_connect, 0);
 		if (!RPC_IS_ASYNC(task))
-			flush_scheduled_work();
+			flush_delayed_work(&r_xprt->rdma_connect);
 	}
 }
 

commit 37aa2133731d9231eb834f700119f0d3f1ed2664
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Wed Sep 29 16:05:43 2010 +0400

    sunrpc: Tag rpc_xprt with net
    
    The net is known from the xprt_create and this tagging will also
    give un the context in the conntection workers where real sockets
    are created.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 0f7a1b9d05ad..2da32b40bfcf 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -283,7 +283,7 @@ xprt_setup_rdma(struct xprt_create *args)
 		return ERR_PTR(-EBADF);
 	}
 
-	xprt = xprt_alloc(sizeof(struct rpcrdma_xprt),
+	xprt = xprt_alloc(args->net, sizeof(struct rpcrdma_xprt),
 			xprt_rdma_slot_table_entries);
 	if (xprt == NULL) {
 		dprintk("RPC:       %s: couldn't allocate rpcrdma_xprt\n",

commit e204e621b4160c802315bc2d0fa335337c0d62e8
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Wed Sep 29 16:03:13 2010 +0400

    sunrpc: Factor out rpc_xprt freeing
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9d77bf25829f..0f7a1b9d05ad 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -251,9 +251,7 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 
 	xprt_rdma_free_addresses(xprt);
 
-	kfree(xprt->slot);
-	xprt->slot = NULL;
-	kfree(xprt);
+	xprt_free(xprt);
 
 	dprintk("RPC:       %s: returning\n", __func__);
 
@@ -401,8 +399,7 @@ xprt_setup_rdma(struct xprt_create *args)
 out2:
 	rpcrdma_ia_close(&new_xprt->rx_ia);
 out1:
-	kfree(xprt->slot);
-	kfree(xprt);
+	xprt_free(xprt);
 	return ERR_PTR(rc);
 }
 

commit bd1722d4316e42a12fe6337ebe34d7e1e2c088b2
Author: Pavel Emelyanov <xemul@parallels.com>
Date:   Wed Sep 29 16:02:43 2010 +0400

    sunrpc: Factor out rpc_xprt allocation
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: J. Bruce Fields <bfields@redhat.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index a85e866a77f7..9d77bf25829f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -285,23 +285,14 @@ xprt_setup_rdma(struct xprt_create *args)
 		return ERR_PTR(-EBADF);
 	}
 
-	xprt = kzalloc(sizeof(struct rpcrdma_xprt), GFP_KERNEL);
+	xprt = xprt_alloc(sizeof(struct rpcrdma_xprt),
+			xprt_rdma_slot_table_entries);
 	if (xprt == NULL) {
 		dprintk("RPC:       %s: couldn't allocate rpcrdma_xprt\n",
 			__func__);
 		return ERR_PTR(-ENOMEM);
 	}
 
-	xprt->max_reqs = xprt_rdma_slot_table_entries;
-	xprt->slot = kcalloc(xprt->max_reqs,
-				sizeof(struct rpc_rqst), GFP_KERNEL);
-	if (xprt->slot == NULL) {
-		dprintk("RPC:       %s: couldn't allocate %d slots\n",
-			__func__, xprt->max_reqs);
-		kfree(xprt);
-		return ERR_PTR(-ENOMEM);
-	}
-
 	/* 60 second timeout, no retries */
 	xprt->timeout = &xprt_rdma_default_timeout;
 	xprt->bind_timeout = (60U * HZ);

commit d60dbb20a74c2cfa142be0a34dac3c6547ea086c
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Thu May 13 12:51:49 2010 -0400

    SUNRPC: Move the task->tk_bytes_sent and tk_rtt to struct rpc_rqst
    
    It seems strange to maintain stats for bytes_sent in one structure, and
    bytes received in another. Try to assemble all the RPC request-related
    stats in struct rpc_rqst
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 3f3b38c5642f..a85e866a77f7 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -674,7 +674,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))
 		goto drop_connection;
 
-	task->tk_bytes_sent += rqst->rq_snd_buf.len;
+	rqst->rq_xmit_bytes_sent += rqst->rq_snd_buf.len;
 	rqst->rq_bytes_sent = 0;
 	return 0;
 

commit a8ce4a8f37fef0a09a1e920c2e09f67a80426c7e
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Fri Apr 16 16:42:12 2010 -0400

    SUNRPC: Fail over more quickly on connect errors
    
    We should not allow soft tasks to wait for longer than the major timeout
    period when waiting for a reconnect to occur.
    
    Remove the field xprt->connect_timeout since it has been obsoleted by
    xprt->reestablish_timeout.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 0607b9aaae91..3f3b38c5642f 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -305,7 +305,6 @@ xprt_setup_rdma(struct xprt_create *args)
 	/* 60 second timeout, no retries */
 	xprt->timeout = &xprt_rdma_default_timeout;
 	xprt->bind_timeout = (60U * HZ);
-	xprt->connect_timeout = (60U * HZ);
 	xprt->reestablish_timeout = (5U * HZ);
 	xprt->idle_timeout = (5U * 60 * HZ);
 

commit 0b9e79431377df452348e78262dd5a3dc359eeef
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Fri Apr 16 16:41:57 2010 -0400

    SUNRPC: Move the test for XPRT_CONNECTING into xprt_connect()
    
    This fixes a bug with setting xprt->stat.connect_start.
    
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 187257b1d880..0607b9aaae91 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -449,21 +449,19 @@ xprt_rdma_connect(struct rpc_task *task)
 	struct rpc_xprt *xprt = (struct rpc_xprt *)task->tk_xprt;
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
-	if (!xprt_test_and_set_connecting(xprt)) {
-		if (r_xprt->rx_ep.rep_connected != 0) {
-			/* Reconnect */
-			schedule_delayed_work(&r_xprt->rdma_connect,
-				xprt->reestablish_timeout);
-			xprt->reestablish_timeout <<= 1;
-			if (xprt->reestablish_timeout > (30 * HZ))
-				xprt->reestablish_timeout = (30 * HZ);
-			else if (xprt->reestablish_timeout < (5 * HZ))
-				xprt->reestablish_timeout = (5 * HZ);
-		} else {
-			schedule_delayed_work(&r_xprt->rdma_connect, 0);
-			if (!RPC_IS_ASYNC(task))
-				flush_scheduled_work();
-		}
+	if (r_xprt->rx_ep.rep_connected != 0) {
+		/* Reconnect */
+		schedule_delayed_work(&r_xprt->rdma_connect,
+			xprt->reestablish_timeout);
+		xprt->reestablish_timeout <<= 1;
+		if (xprt->reestablish_timeout > (30 * HZ))
+			xprt->reestablish_timeout = (30 * HZ);
+		else if (xprt->reestablish_timeout < (5 * HZ))
+			xprt->reestablish_timeout = (5 * HZ);
+	} else {
+		schedule_delayed_work(&r_xprt->rdma_connect, 0);
+		if (!RPC_IS_ASYNC(task))
+			flush_scheduled_work();
 	}
 }
 

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index f96c2fe6137b..187257b1d880 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -49,6 +49,7 @@
 
 #include <linux/module.h>
 #include <linux/init.h>
+#include <linux/slab.h>
 #include <linux/seq_file.h>
 
 #include "xprt_rdma.h"

commit 81160e66cca3d3a16b7d88e0e2dccfc5c76f36f9
Author: Joe Perches <joe@perches.com>
Date:   Mon Mar 8 12:15:59 2010 -0800

    net/sunrpc: Convert (void)snprintf to snprintf
    
    (Applies on top of "Remove uses of NIPQUAD, use %pI4")
    
    Casts to void of snprintf are most uncommon in kernel source.
    9 use casts, 1301 do not.
    
    Remove the remaining uses in net/sunrpc/
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 83d339fd66d5..f96c2fe6137b 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -160,15 +160,15 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 	(void)rpc_ntop(sap, buf, sizeof(buf));
 	xprt->address_strings[RPC_DISPLAY_ADDR] = kstrdup(buf, GFP_KERNEL);
 
-	(void)snprintf(buf, sizeof(buf), "%u", rpc_get_port(sap));
+	snprintf(buf, sizeof(buf), "%u", rpc_get_port(sap));
 	xprt->address_strings[RPC_DISPLAY_PORT] = kstrdup(buf, GFP_KERNEL);
 
 	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
 
-	(void)snprintf(buf, sizeof(buf), "%08x", ntohl(sin->sin_addr.s_addr));
+	snprintf(buf, sizeof(buf), "%08x", ntohl(sin->sin_addr.s_addr));
 	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = kstrdup(buf, GFP_KERNEL);
 
-	(void)snprintf(buf, sizeof(buf), "%4hx", rpc_get_port(sap));
+	snprintf(buf, sizeof(buf), "%4hx", rpc_get_port(sap));
 	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = kstrdup(buf, GFP_KERNEL);
 
 	/* netid */

commit fc0b579168cbe737c83c6b9bbfe265d3ae6baca6
Author: Joe Perches <joe@perches.com>
Date:   Mon Mar 8 12:15:28 2010 -0800

    net/sunrpc: Remove uses of NIPQUAD, use %pI4
    
    Originally submitted Jan 1, 2010
    http://patchwork.kernel.org/patch/71221/
    
    Convert NIPQUAD to the %pI4 format extension where possible
    Convert %02x%02x%02x%02x/NIPQUAD to %08x/ntohl
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 7018eef1dcdd..83d339fd66d5 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -165,8 +165,7 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 
 	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
 
-	(void)snprintf(buf, sizeof(buf), "%02x%02x%02x%02x",
-				NIPQUAD(sin->sin_addr.s_addr));
+	(void)snprintf(buf, sizeof(buf), "%08x", ntohl(sin->sin_addr.s_addr));
 	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = kstrdup(buf, GFP_KERNEL);
 
 	(void)snprintf(buf, sizeof(buf), "%4hx", rpc_get_port(sap));

commit 6d4561110a3e9fa742aeec6717248a491dfb1878
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Mon Nov 16 03:11:48 2009 -0800

    sysctl: Drop & in front of every proc_handler.
    
    For consistency drop & in front of every proc_handler.  Explicity
    taking the address is unnecessary and it prevents optimizations
    like stubbing the proc_handlers to NULL.
    
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Joe Perches <joe@perches.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 476816062243..7018eef1dcdd 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -90,7 +90,7 @@ static ctl_table xr_tunables_table[] = {
 		.data		= &xprt_rdma_slot_table_entries,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= &proc_dointvec_minmax,
+		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= &min_slot_table_size,
 		.extra2		= &max_slot_table_size
 	},
@@ -99,21 +99,21 @@ static ctl_table xr_tunables_table[] = {
 		.data		= &xprt_rdma_max_inline_read,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= &proc_dointvec,
+		.proc_handler	= proc_dointvec,
 	},
 	{
 		.procname	= "rdma_max_inline_write",
 		.data		= &xprt_rdma_max_inline_write,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= &proc_dointvec,
+		.proc_handler	= proc_dointvec,
 	},
 	{
 		.procname	= "rdma_inline_write_padding",
 		.data		= &xprt_rdma_inline_write_padding,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= &proc_dointvec_minmax,
+		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= &zero,
 		.extra2		= &max_padding,
 	},
@@ -122,7 +122,7 @@ static ctl_table xr_tunables_table[] = {
 		.data		= &xprt_rdma_memreg_strategy,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= &proc_dointvec_minmax,
+		.proc_handler	= proc_dointvec_minmax,
 		.extra1		= &min_memreg,
 		.extra2		= &max_memreg,
 	},
@@ -131,7 +131,7 @@ static ctl_table xr_tunables_table[] = {
 		.data		= &xprt_rdma_pad_optimize,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
-		.proc_handler	= &proc_dointvec,
+		.proc_handler	= proc_dointvec,
 	},
 	{ },
 };

commit f8572d8f2a2ba75408b97dc24ef47c83671795d7
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Nov 5 13:32:03 2009 -0800

    sysctl net: Remove unused binary sysctl code
    
    Now that sys_sysctl is a compatiblity wrapper around /proc/sys
    all sysctl strategy routines, and all ctl_name and strategy
    entries in the sysctl tables are unused, and can be
    revmoed.
    
    In addition neigh_sysctl_register has been modified to no longer
    take a strategy argument and it's callers have been modified not
    to pass one.
    
    Cc: "David Miller" <davem@davemloft.net>
    Cc: Hideaki YOSHIFUJI <yoshfuji@linux-ipv6.org>
    Cc: netdev@vger.kernel.org
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9a63f669ece4..476816062243 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -86,79 +86,63 @@ static struct ctl_table_header *sunrpc_table_header;
 
 static ctl_table xr_tunables_table[] = {
 	{
-		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_slot_table_entries",
 		.data		= &xprt_rdma_slot_table_entries,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
-		.strategy	= &sysctl_intvec,
 		.extra1		= &min_slot_table_size,
 		.extra2		= &max_slot_table_size
 	},
 	{
-		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_max_inline_read",
 		.data		= &xprt_rdma_max_inline_read,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec,
-		.strategy	= &sysctl_intvec,
 	},
 	{
-		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_max_inline_write",
 		.data		= &xprt_rdma_max_inline_write,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec,
-		.strategy	= &sysctl_intvec,
 	},
 	{
-		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_inline_write_padding",
 		.data		= &xprt_rdma_inline_write_padding,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
-		.strategy	= &sysctl_intvec,
 		.extra1		= &zero,
 		.extra2		= &max_padding,
 	},
 	{
-		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_memreg_strategy",
 		.data		= &xprt_rdma_memreg_strategy,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec_minmax,
-		.strategy	= &sysctl_intvec,
 		.extra1		= &min_memreg,
 		.extra2		= &max_memreg,
 	},
 	{
-		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_pad_optimize",
 		.data		= &xprt_rdma_pad_optimize,
 		.maxlen		= sizeof(unsigned int),
 		.mode		= 0644,
 		.proc_handler	= &proc_dointvec,
 	},
-	{
-		.ctl_name = 0,
-	},
+	{ },
 };
 
 static ctl_table sunrpc_table[] = {
 	{
-		.ctl_name	= CTL_SUNRPC,
 		.procname	= "sunrpc",
 		.mode		= 0555,
 		.child		= xr_tunables_table
 	},
-	{
-		.ctl_name = 0,
-	},
+	{ },
 };
 
 #endif

commit c740eff84bcfd63c0497ef880e80171931cb8222
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Aug 9 15:09:46 2009 -0400

    SUNRPC: Kill RPC_DISPLAY_ALL
    
    At some point, I recall that rpc_pipe_fs used RPC_DISPLAY_ALL.
    Currently there are no uses of RPC_DISPLAY_ALL outside the transport
    modules themselves, so we can safely get rid of it.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 5f9b8676b6bd..9a63f669ece4 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -181,11 +181,6 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 
 	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
 
-	(void)snprintf(buf, sizeof(buf), "addr=%s port=%s proto=rdma",
-			xprt->address_strings[RPC_DISPLAY_ADDR],
-			xprt->address_strings[RPC_DISPLAY_PORT]);
-	xprt->address_strings[RPC_DISPLAY_ALL] = kstrdup(buf, GFP_KERNEL);
-
 	(void)snprintf(buf, sizeof(buf), "%02x%02x%02x%02x",
 				NIPQUAD(sin->sin_addr.s_addr));
 	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = kstrdup(buf, GFP_KERNEL);

commit c877b849d302d1275452af80b7221a2555dc02e1
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Aug 9 15:09:36 2009 -0400

    SUNRPC: Use rpc_ntop() for constructing transport address strings
    
    Clean up:  In addition to using the new generic rpc_ntop() and
    rpc_get_port() functions, have the RPC client compute the presentation
    address buffer sizes dynamically using kstrdup().
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 537c210a8b92..5f9b8676b6bd 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -168,39 +168,30 @@ static struct rpc_xprt_ops xprt_rdma_procs;	/* forward reference */
 static void
 xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 {
-	struct sockaddr_in *addr = (struct sockaddr_in *)
+	struct sockaddr *sap = (struct sockaddr *)
 					&rpcx_to_rdmad(xprt).addr;
-	char *buf;
+	struct sockaddr_in *sin = (struct sockaddr_in *)sap;
+	char buf[64];
 
-	buf = kzalloc(20, GFP_KERNEL);
-	if (buf)
-		snprintf(buf, 20, "%pI4", &addr->sin_addr.s_addr);
-	xprt->address_strings[RPC_DISPLAY_ADDR] = buf;
+	(void)rpc_ntop(sap, buf, sizeof(buf));
+	xprt->address_strings[RPC_DISPLAY_ADDR] = kstrdup(buf, GFP_KERNEL);
 
-	buf = kzalloc(8, GFP_KERNEL);
-	if (buf)
-		snprintf(buf, 8, "%u", ntohs(addr->sin_port));
-	xprt->address_strings[RPC_DISPLAY_PORT] = buf;
+	(void)snprintf(buf, sizeof(buf), "%u", rpc_get_port(sap));
+	xprt->address_strings[RPC_DISPLAY_PORT] = kstrdup(buf, GFP_KERNEL);
 
 	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
 
-	buf = kzalloc(48, GFP_KERNEL);
-	if (buf)
-		snprintf(buf, 48, "addr=%pI4 port=%u proto=%s",
-			&addr->sin_addr.s_addr,
-			ntohs(addr->sin_port), "rdma");
-	xprt->address_strings[RPC_DISPLAY_ALL] = buf;
-
-	buf = kzalloc(10, GFP_KERNEL);
-	if (buf)
-		snprintf(buf, 10, "%02x%02x%02x%02x",
-			NIPQUAD(addr->sin_addr.s_addr));
-	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = buf;
-
-	buf = kzalloc(8, GFP_KERNEL);
-	if (buf)
-		snprintf(buf, 8, "%4hx", ntohs(addr->sin_port));
-	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = buf;
+	(void)snprintf(buf, sizeof(buf), "addr=%s port=%s proto=rdma",
+			xprt->address_strings[RPC_DISPLAY_ADDR],
+			xprt->address_strings[RPC_DISPLAY_PORT]);
+	xprt->address_strings[RPC_DISPLAY_ALL] = kstrdup(buf, GFP_KERNEL);
+
+	(void)snprintf(buf, sizeof(buf), "%02x%02x%02x%02x",
+				NIPQUAD(sin->sin_addr.s_addr));
+	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = kstrdup(buf, GFP_KERNEL);
+
+	(void)snprintf(buf, sizeof(buf), "%4hx", rpc_get_port(sap));
+	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = kstrdup(buf, GFP_KERNEL);
 
 	/* netid */
 	xprt->address_strings[RPC_DISPLAY_NETID] = "rdma";

commit ba809130bc260fce04141aca01ef9e068d32af2a
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Sun Aug 9 15:09:35 2009 -0400

    SUNRPC: Remove duplicate universal address generation
    
    RPC universal address generation is currently done in several places:
    rpcb_clnt.c, nfs4proc.c xprtsock.c, and xprtrdma.c.  Remove the
    redundant cases that convert a socket address to a universal
    address.  The nfs4proc.c case takes a pre-formatted presentation
    address string, not a socket address, so we'll leave that one.
    
    Because the new uaddr constructor uses the recently introduced
    rpc_ntop(), it now supports proper "::" shorthanding for IPv6
    addresses.  This allows the kernel to register properly formed
    universal addresses with the local rpcbind service, in _all_ cases.
    
    The kernel can now also send properly formed universal addresses in
    RPCB_GETADDR requests, and support link-local properly when
    encoding and decoding IPv6 addresses.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 1dd6123070e9..537c210a8b92 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -202,14 +202,6 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 		snprintf(buf, 8, "%4hx", ntohs(addr->sin_port));
 	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = buf;
 
-	buf = kzalloc(30, GFP_KERNEL);
-	if (buf)
-		snprintf(buf, 30, "%pI4.%u.%u",
-			&addr->sin_addr.s_addr,
-			ntohs(addr->sin_port) >> 8,
-			ntohs(addr->sin_port) & 0xff);
-	xprt->address_strings[RPC_DISPLAY_UNIVERSAL_ADDR] = buf;
-
 	/* netid */
 	xprt->address_strings[RPC_DISPLAY_NETID] = "rdma";
 }

commit 21454aaad30651ba0dcc16fe5271bc12ee21f132
Author: Harvey Harrison <harvey.harrison@gmail.com>
Date:   Fri Oct 31 00:54:56 2008 -0700

    net: replace NIPQUAD() in net/*/
    
    Using NIPQUAD() with NIPQUAD_FMT, %d.%d.%d.%d or %u.%u.%u.%u
    can be replaced with %pI4
    
    Signed-off-by: Harvey Harrison <harvey.harrison@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 9839c3d94145..1dd6123070e9 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -174,7 +174,7 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 
 	buf = kzalloc(20, GFP_KERNEL);
 	if (buf)
-		snprintf(buf, 20, NIPQUAD_FMT, NIPQUAD(addr->sin_addr.s_addr));
+		snprintf(buf, 20, "%pI4", &addr->sin_addr.s_addr);
 	xprt->address_strings[RPC_DISPLAY_ADDR] = buf;
 
 	buf = kzalloc(8, GFP_KERNEL);
@@ -186,8 +186,8 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 
 	buf = kzalloc(48, GFP_KERNEL);
 	if (buf)
-		snprintf(buf, 48, "addr="NIPQUAD_FMT" port=%u proto=%s",
-			NIPQUAD(addr->sin_addr.s_addr),
+		snprintf(buf, 48, "addr=%pI4 port=%u proto=%s",
+			&addr->sin_addr.s_addr,
 			ntohs(addr->sin_port), "rdma");
 	xprt->address_strings[RPC_DISPLAY_ALL] = buf;
 
@@ -204,8 +204,8 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 
 	buf = kzalloc(30, GFP_KERNEL);
 	if (buf)
-		snprintf(buf, 30, NIPQUAD_FMT".%u.%u",
-			NIPQUAD(addr->sin_addr.s_addr),
+		snprintf(buf, 30, "%pI4.%u.%u",
+			&addr->sin_addr.s_addr,
 			ntohs(addr->sin_port) >> 8,
 			ntohs(addr->sin_port) & 0xff);
 	xprt->address_strings[RPC_DISPLAY_UNIVERSAL_ADDR] = buf;
@@ -369,8 +369,8 @@ xprt_setup_rdma(struct xprt_create *args)
 	if (ntohs(sin->sin_port) != 0)
 		xprt_set_bound(xprt);
 
-	dprintk("RPC:       %s: %u.%u.%u.%u:%u\n", __func__,
-			NIPQUAD(sin->sin_addr.s_addr), ntohs(sin->sin_port));
+	dprintk("RPC:       %s: %pI4:%u\n",
+		__func__, &sin->sin_addr.s_addr, ntohs(sin->sin_port));
 
 	/* Set max requests */
 	cdata.max_requests = xprt->max_reqs;

commit 08ca0dce1eafa419059ac4cad9ed522af7052526
Author: Tom Talpey <talpey@netapp.com>
Date:   Fri Oct 10 11:32:34 2008 -0400

    RPC/RDMA: correct the reconnect timer backoff
    
    The RPC/RDMA code had a constant 5-second reconnect backoff, and
    always performed it, even when re-establishing a connection to a
    server after the RPC layer closed it due to being idle. Make it
    an geometric backoff (up to 30 seconds), and don't delay idle
    reconnect.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c2da680273c5..9839c3d94145 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -463,6 +463,8 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
 	dprintk("RPC:       %s: closing\n", __func__);
+	if (r_xprt->rx_ep.rep_connected > 0)
+		xprt->reestablish_timeout = 0;
 	xprt_disconnect_done(xprt);
 	(void) rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
 }
@@ -490,6 +492,11 @@ xprt_rdma_connect(struct rpc_task *task)
 			/* Reconnect */
 			schedule_delayed_work(&r_xprt->rdma_connect,
 				xprt->reestablish_timeout);
+			xprt->reestablish_timeout <<= 1;
+			if (xprt->reestablish_timeout > (30 * HZ))
+				xprt->reestablish_timeout = (30 * HZ);
+			else if (xprt->reestablish_timeout < (5 * HZ))
+				xprt->reestablish_timeout = (5 * HZ);
 		} else {
 			schedule_delayed_work(&r_xprt->rdma_connect, 0);
 			if (!RPC_IS_ASYNC(task))

commit b3cd8d45a764e6edb06e7bd386faf99a879569b8
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:02:02 2008 -0400

    RPC/RDMA: optionally emit useful transport info upon connect/disconnect.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index c7d2380bb5e3..c2da680273c5 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -784,7 +784,7 @@ static void __exit xprt_rdma_cleanup(void)
 {
 	int rc;
 
-	dprintk("RPCRDMA Module Removed, deregister RPC RDMA transport\n");
+	dprintk(KERN_INFO "RPCRDMA Module Removed, deregister RPC RDMA transport\n");
 #ifdef RPC_DEBUG
 	if (sunrpc_table_header) {
 		unregister_sysctl_table(sunrpc_table_header);

commit 9191ca3b381b15b9a88785a8ae2fa4db8e553b0c
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:01:11 2008 -0400

    RPC/RDMA: adhere to protocol for unpadded client trailing write chunks.
    
    The RPC/RDMA protocol allows clients and servers to avoid RDMA
    operations for data which is purely the result of XDR padding.
    On the client, automatically insert the necessary padding for
    such server replies, and optionally don't marshal such chunks.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index ec6d1e7a1941..c7d2380bb5e3 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -71,6 +71,7 @@ static unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_inline_write_padding;
 static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
+                int xprt_rdma_pad_optimize = 0;
 
 #ifdef RPC_DEBUG
 
@@ -135,6 +136,14 @@ static ctl_table xr_tunables_table[] = {
 		.extra1		= &min_memreg,
 		.extra2		= &max_memreg,
 	},
+	{
+		.ctl_name       = CTL_UNNUMBERED,
+		.procname	= "rdma_pad_optimize",
+		.data		= &xprt_rdma_pad_optimize,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+	},
 	{
 		.ctl_name = 0,
 	},

commit ad0e9e01da4ece70ff524b49c77c5e850d5dd53e
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:00:50 2008 -0400

    RPC/RDMA: maintain the RPC task bytes-sent statistic.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 0aefc6485385..ec6d1e7a1941 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -699,6 +699,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))
 		goto drop_connection;
 
+	task->tk_bytes_sent += rqst->rq_snd_buf.len;
 	rqst->rq_bytes_sent = 0;
 	return 0;
 

commit 575448bd36208f99fe0dd554a43518d798966740
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:00:40 2008 -0400

    RPC/RDMA: suppress retransmit on RPC/RDMA clients.
    
    An RPC/RDMA client cannot retransmit on an unbroken connection,
    doing so violates its flow control with the server.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 89970b0a4cc9..0aefc6485385 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -587,6 +587,7 @@ xprt_rdma_allocate(struct rpc_task *task, size_t size)
 	}
 	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
 out:
+	req->rl_connect_cookie = 0;	/* our reserved value */
 	return req->rl_xdr_buf;
 
 outfail:
@@ -690,13 +691,20 @@ xprt_rdma_send_request(struct rpc_task *task)
 		req->rl_reply->rr_xprt = xprt;
 	}
 
-	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req)) {
-		xprt_disconnect_done(xprt);
-		return -ENOTCONN;	/* implies disconnect */
-	}
+	/* Must suppress retransmit to maintain credits */
+	if (req->rl_connect_cookie == xprt->connect_cookie)
+		goto drop_connection;
+	req->rl_connect_cookie = xprt->connect_cookie;
+
+	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req))
+		goto drop_connection;
 
 	rqst->rq_bytes_sent = 0;
 	return 0;
+
+drop_connection:
+	xprt_disconnect_done(xprt);
+	return -ENOTCONN;	/* implies disconnect */
 }
 
 static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)

commit 3197d309f5fb042499b2c4c8f2fcb67372df5201
Author: Tom Talpey <talpey@netapp.com>
Date:   Thu Oct 9 15:00:20 2008 -0400

    RPC/RDMA: support FRMR client memory registration.
    
    Configure, detect and use "fastreg" support from IB/iWARP verbs
    layer to perform RPC/RDMA memory registration.
    
    Make FRMR the default memreg mode (will fall back if not supported
    by the selected RDMA adapter).
    
    This allows full and optimal operation over the cxgb3 adapter, and others.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Acked-by: Tom Tucker <tom@opengridcomputing.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index a564c1a39ec5..89970b0a4cc9 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -70,11 +70,7 @@ static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
 static unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
 static unsigned int xprt_rdma_inline_write_padding;
-#if !RPCRDMA_PERSISTENT_REGISTRATION
-static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_REGISTER; /* FMR? */
-#else
-static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_ALLPHYSICAL;
-#endif
+static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_FRMR;
 
 #ifdef RPC_DEBUG
 

commit ee1a2c564f67407947e89f1dac75ac0af0ba88c7
Author: Tom Talpey <tmt@netapp.com>
Date:   Wed Feb 27 15:04:26 2008 -0500

    SUNRPC: Fix a nfs4 over rdma transport oops
    
    Prevent an RPC oops when freeing a dynamically allocated RDMA
    buffer, used in certain special-case large metadata operations.
    
    Signed-off-by: Tom Talpey <tmt@netapp.com>
    Signed-off-by: James Lentini <jlentini@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 02c522c17de5..a564c1a39ec5 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -614,7 +614,11 @@ xprt_rdma_free(void *buffer)
 		return;
 
 	req = container_of(buffer, struct rpcrdma_req, rl_xdr_buf[0]);
-	r_xprt = container_of(req->rl_buffer, struct rpcrdma_xprt, rx_buf);
+	if (req->rl_iov.length == 0) {	/* see allocate above */
+		r_xprt = container_of(((struct rpcrdma_req *) req->rl_buffer)->rl_buffer,
+				      struct rpcrdma_xprt, rx_buf);
+	} else
+		r_xprt = container_of(req->rl_buffer, struct rpcrdma_xprt, rx_buf);
 	rep = req->rl_reply;
 
 	dprintk("RPC:       %s: called on 0x%p%s\n",

commit 33e01dc7f578813cda074ceaeaf68b0f3ffcc393
Author: Chuck Lever <chuck.lever@oracle.com>
Date:   Mon Jan 14 12:32:20 2008 -0500

    SUNRPC: Clean up functions that free address_strings array
    
    Clean up: document the rule (kfree) and the exceptions
    (RPC_DISPLAY_PROTO and RPC_DISPLAY_NETID) when freeing the objects in
    a transport's address_strings array.
    
    Signed-off-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index d1389afc8342..02c522c17de5 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -212,12 +212,16 @@ xprt_rdma_format_addresses(struct rpc_xprt *xprt)
 static void
 xprt_rdma_free_addresses(struct rpc_xprt *xprt)
 {
-	kfree(xprt->address_strings[RPC_DISPLAY_ADDR]);
-	kfree(xprt->address_strings[RPC_DISPLAY_PORT]);
-	kfree(xprt->address_strings[RPC_DISPLAY_ALL]);
-	kfree(xprt->address_strings[RPC_DISPLAY_HEX_ADDR]);
-	kfree(xprt->address_strings[RPC_DISPLAY_HEX_PORT]);
-	kfree(xprt->address_strings[RPC_DISPLAY_UNIVERSAL_ADDR]);
+	unsigned int i;
+
+	for (i = 0; i < RPC_DISPLAY_MAX; i++)
+		switch (i) {
+		case RPC_DISPLAY_PROTO:
+		case RPC_DISPLAY_NETID:
+			continue;
+		default:
+			kfree(xprt->address_strings[i]);
+		}
 }
 
 static void

commit ba7392bb37cb12781890f45d7ddee1618e33a036
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Thu Dec 20 16:03:55 2007 -0500

    SUNRPC: Add support for per-client timeout values
    
    In order to be able to support setting the timeo and retrans parameters on
    a per-mountpoint basis, we move the rpc_timeout structure into the
    rpc_clnt.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 39f10016c86b..d1389afc8342 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -332,7 +332,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	}
 
 	/* 60 second timeout, no retries */
-	memcpy(&xprt->timeout, &xprt_rdma_default_timeout, sizeof(xprt->timeout));
+	xprt->timeout = &xprt_rdma_default_timeout;
 	xprt->bind_timeout = (60U * HZ);
 	xprt->connect_timeout = (60U * HZ);
 	xprt->reestablish_timeout = (5U * HZ);

commit 2881ae74e68ecfe3b32a90936e5d93a9ba598c3a
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Thu Dec 20 16:03:54 2007 -0500

    SUNRPC: Clean up the transport timeout initialisation
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 73033d824eed..39f10016c86b 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -289,6 +289,11 @@ xprt_rdma_destroy(struct rpc_xprt *xprt)
 	module_put(THIS_MODULE);
 }
 
+static const struct rpc_timeout xprt_rdma_default_timeout = {
+	.to_initval = 60 * HZ,
+	.to_maxval = 60 * HZ,
+};
+
 /**
  * xprt_setup_rdma - Set up transport to use RDMA
  *
@@ -327,7 +332,7 @@ xprt_setup_rdma(struct xprt_create *args)
 	}
 
 	/* 60 second timeout, no retries */
-	xprt_set_timeout(&xprt->timeout, 0, 60UL * HZ);
+	memcpy(&xprt->timeout, &xprt_rdma_default_timeout, sizeof(xprt->timeout));
 	xprt->bind_timeout = (60U * HZ);
 	xprt->connect_timeout = (60U * HZ);
 	xprt->reestablish_timeout = (5U * HZ);

commit 62da3b24880bccd4ffc32cf8d9a7e23fab475bdd
Author: Trond Myklebust <Trond.Myklebust@netapp.com>
Date:   Tue Nov 6 18:44:20 2007 -0500

    SUNRPC: Rename xprt_disconnect()
    
    xprt_disconnect() should really only be called when the transport shutdown
    is completed, and it is time to wake up any pending tasks. Rename it to
    xprt_disconnect_done() in order to reflect the semantical change.
    
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 6f2112dd9f78..73033d824eed 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -449,7 +449,7 @@ xprt_rdma_close(struct rpc_xprt *xprt)
 	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
 
 	dprintk("RPC:       %s: closing\n", __func__);
-	xprt_disconnect(xprt);
+	xprt_disconnect_done(xprt);
 	(void) rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
 }
 
@@ -682,7 +682,7 @@ xprt_rdma_send_request(struct rpc_task *task)
 	}
 
 	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req)) {
-		xprt_disconnect(xprt);
+		xprt_disconnect_done(xprt);
 		return -ENOTCONN;	/* implies disconnect */
 	}
 

commit cfcb43ff7ce28f347a39e1a7519e01850b588718
Author: James Lentini <jlentini@netapp.com>
Date:   Mon Nov 26 12:42:44 2007 -0500

    SUNRPC: remove NFS/RDMA client's binary sysctls
    
    Support for binary sysctls is being deprecated in 2.6.24. Since there
    are no applications using the NFS/RDMA client's binary sysctls, it
    makes sense to remove them. The patch below does this while leaving
    the /proc/sys interface unchanged.
    
    Please consider this for 2.6.24.
    
    Signed-off-by: James Lentini <jlentini@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index 1afeb3eb8e4c..6f2112dd9f78 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -89,7 +89,7 @@ static struct ctl_table_header *sunrpc_table_header;
 
 static ctl_table xr_tunables_table[] = {
 	{
-		.ctl_name	= CTL_SLOTTABLE_RDMA,
+		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_slot_table_entries",
 		.data		= &xprt_rdma_slot_table_entries,
 		.maxlen		= sizeof(unsigned int),
@@ -100,7 +100,7 @@ static ctl_table xr_tunables_table[] = {
 		.extra2		= &max_slot_table_size
 	},
 	{
-		.ctl_name	= CTL_RDMA_MAXINLINEREAD,
+		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_max_inline_read",
 		.data		= &xprt_rdma_max_inline_read,
 		.maxlen		= sizeof(unsigned int),
@@ -109,7 +109,7 @@ static ctl_table xr_tunables_table[] = {
 		.strategy	= &sysctl_intvec,
 	},
 	{
-		.ctl_name	= CTL_RDMA_MAXINLINEWRITE,
+		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_max_inline_write",
 		.data		= &xprt_rdma_max_inline_write,
 		.maxlen		= sizeof(unsigned int),
@@ -118,7 +118,7 @@ static ctl_table xr_tunables_table[] = {
 		.strategy	= &sysctl_intvec,
 	},
 	{
-		.ctl_name	= CTL_RDMA_WRITEPADDING,
+		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_inline_write_padding",
 		.data		= &xprt_rdma_inline_write_padding,
 		.maxlen		= sizeof(unsigned int),
@@ -129,7 +129,7 @@ static ctl_table xr_tunables_table[] = {
 		.extra2		= &max_padding,
 	},
 	{
-		.ctl_name	= CTL_RDMA_MEMREG,
+		.ctl_name       = CTL_UNNUMBERED,
 		.procname	= "rdma_memreg_strategy",
 		.data		= &xprt_rdma_memreg_strategy,
 		.maxlen		= sizeof(unsigned int),

commit d5cd97872dca9b79c31224ca014bcea7ca01f5f1
Author: Adrian Bunk <bunk@kernel.org>
Date:   Wed Nov 14 17:00:00 2007 -0800

    sunrpc/xprtrdma/transport.c: fix use-after-free
    
    Fix an obvious use-after-free spotted by the Coverity checker.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Cc: Trond Myklebust <trond.myklebust@fys.uio.no>
    Cc: "J. Bruce Fields" <bfields@fieldses.org>
    Cc: Neil Brown <neilb@suse.de>
    Cc: "David S. Miller" <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
index dc55cc974c90..1afeb3eb8e4c 100644
--- a/net/sunrpc/xprtrdma/transport.c
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -320,9 +320,9 @@ xprt_setup_rdma(struct xprt_create *args)
 	xprt->slot = kcalloc(xprt->max_reqs,
 				sizeof(struct rpc_rqst), GFP_KERNEL);
 	if (xprt->slot == NULL) {
-		kfree(xprt);
 		dprintk("RPC:       %s: couldn't allocate %d slots\n",
 			__func__, xprt->max_reqs);
+		kfree(xprt);
 		return ERR_PTR(-ENOMEM);
 	}
 

commit f58851e6b0f148fb4b2a1c6f70beb2f125863c0f
Author: \"Talpey, Thomas\ <Thomas.Talpey@netapp.com>
Date:   Mon Sep 10 13:50:12 2007 -0400

    RPCRDMA: rpc rdma transport switch
    
    This implements the configuration and building of the core transport
    switch implementation of the rpcrdma transport. Stubs are provided for
    the rpcrdma protocol handling, and the infiniband/iwarp verbs interface.
    These are provided in following patches.
    
    Signed-off-by: Tom Talpey <talpey@netapp.com>
    Signed-off-by: Trond Myklebust <Trond.Myklebust@netapp.com>

diff --git a/net/sunrpc/xprtrdma/transport.c b/net/sunrpc/xprtrdma/transport.c
new file mode 100644
index 000000000000..dc55cc974c90
--- /dev/null
+++ b/net/sunrpc/xprtrdma/transport.c
@@ -0,0 +1,800 @@
+/*
+ * Copyright (c) 2003-2007 Network Appliance, Inc. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the BSD-type
+ * license below:
+ *
+ * Redistribution and use in source and binary forms, with or without
+ * modification, are permitted provided that the following conditions
+ * are met:
+ *
+ *      Redistributions of source code must retain the above copyright
+ *      notice, this list of conditions and the following disclaimer.
+ *
+ *      Redistributions in binary form must reproduce the above
+ *      copyright notice, this list of conditions and the following
+ *      disclaimer in the documentation and/or other materials provided
+ *      with the distribution.
+ *
+ *      Neither the name of the Network Appliance, Inc. nor the names of
+ *      its contributors may be used to endorse or promote products
+ *      derived from this software without specific prior written
+ *      permission.
+ *
+ * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
+ * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
+ * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
+ * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
+ * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
+ * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
+ * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
+ * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
+ * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
+ * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
+ * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
+ */
+
+/*
+ * transport.c
+ *
+ * This file contains the top-level implementation of an RPC RDMA
+ * transport.
+ *
+ * Naming convention: functions beginning with xprt_ are part of the
+ * transport switch. All others are RPC RDMA internal.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/seq_file.h>
+
+#include "xprt_rdma.h"
+
+#ifdef RPC_DEBUG
+# define RPCDBG_FACILITY	RPCDBG_TRANS
+#endif
+
+MODULE_LICENSE("Dual BSD/GPL");
+
+MODULE_DESCRIPTION("RPC/RDMA Transport for Linux kernel NFS");
+MODULE_AUTHOR("Network Appliance, Inc.");
+
+/*
+ * tunables
+ */
+
+static unsigned int xprt_rdma_slot_table_entries = RPCRDMA_DEF_SLOT_TABLE;
+static unsigned int xprt_rdma_max_inline_read = RPCRDMA_DEF_INLINE;
+static unsigned int xprt_rdma_max_inline_write = RPCRDMA_DEF_INLINE;
+static unsigned int xprt_rdma_inline_write_padding;
+#if !RPCRDMA_PERSISTENT_REGISTRATION
+static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_REGISTER; /* FMR? */
+#else
+static unsigned int xprt_rdma_memreg_strategy = RPCRDMA_ALLPHYSICAL;
+#endif
+
+#ifdef RPC_DEBUG
+
+static unsigned int min_slot_table_size = RPCRDMA_MIN_SLOT_TABLE;
+static unsigned int max_slot_table_size = RPCRDMA_MAX_SLOT_TABLE;
+static unsigned int zero;
+static unsigned int max_padding = PAGE_SIZE;
+static unsigned int min_memreg = RPCRDMA_BOUNCEBUFFERS;
+static unsigned int max_memreg = RPCRDMA_LAST - 1;
+
+static struct ctl_table_header *sunrpc_table_header;
+
+static ctl_table xr_tunables_table[] = {
+	{
+		.ctl_name	= CTL_SLOTTABLE_RDMA,
+		.procname	= "rdma_slot_table_entries",
+		.data		= &xprt_rdma_slot_table_entries,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.strategy	= &sysctl_intvec,
+		.extra1		= &min_slot_table_size,
+		.extra2		= &max_slot_table_size
+	},
+	{
+		.ctl_name	= CTL_RDMA_MAXINLINEREAD,
+		.procname	= "rdma_max_inline_read",
+		.data		= &xprt_rdma_max_inline_read,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+		.strategy	= &sysctl_intvec,
+	},
+	{
+		.ctl_name	= CTL_RDMA_MAXINLINEWRITE,
+		.procname	= "rdma_max_inline_write",
+		.data		= &xprt_rdma_max_inline_write,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec,
+		.strategy	= &sysctl_intvec,
+	},
+	{
+		.ctl_name	= CTL_RDMA_WRITEPADDING,
+		.procname	= "rdma_inline_write_padding",
+		.data		= &xprt_rdma_inline_write_padding,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.strategy	= &sysctl_intvec,
+		.extra1		= &zero,
+		.extra2		= &max_padding,
+	},
+	{
+		.ctl_name	= CTL_RDMA_MEMREG,
+		.procname	= "rdma_memreg_strategy",
+		.data		= &xprt_rdma_memreg_strategy,
+		.maxlen		= sizeof(unsigned int),
+		.mode		= 0644,
+		.proc_handler	= &proc_dointvec_minmax,
+		.strategy	= &sysctl_intvec,
+		.extra1		= &min_memreg,
+		.extra2		= &max_memreg,
+	},
+	{
+		.ctl_name = 0,
+	},
+};
+
+static ctl_table sunrpc_table[] = {
+	{
+		.ctl_name	= CTL_SUNRPC,
+		.procname	= "sunrpc",
+		.mode		= 0555,
+		.child		= xr_tunables_table
+	},
+	{
+		.ctl_name = 0,
+	},
+};
+
+#endif
+
+static struct rpc_xprt_ops xprt_rdma_procs;	/* forward reference */
+
+static void
+xprt_rdma_format_addresses(struct rpc_xprt *xprt)
+{
+	struct sockaddr_in *addr = (struct sockaddr_in *)
+					&rpcx_to_rdmad(xprt).addr;
+	char *buf;
+
+	buf = kzalloc(20, GFP_KERNEL);
+	if (buf)
+		snprintf(buf, 20, NIPQUAD_FMT, NIPQUAD(addr->sin_addr.s_addr));
+	xprt->address_strings[RPC_DISPLAY_ADDR] = buf;
+
+	buf = kzalloc(8, GFP_KERNEL);
+	if (buf)
+		snprintf(buf, 8, "%u", ntohs(addr->sin_port));
+	xprt->address_strings[RPC_DISPLAY_PORT] = buf;
+
+	xprt->address_strings[RPC_DISPLAY_PROTO] = "rdma";
+
+	buf = kzalloc(48, GFP_KERNEL);
+	if (buf)
+		snprintf(buf, 48, "addr="NIPQUAD_FMT" port=%u proto=%s",
+			NIPQUAD(addr->sin_addr.s_addr),
+			ntohs(addr->sin_port), "rdma");
+	xprt->address_strings[RPC_DISPLAY_ALL] = buf;
+
+	buf = kzalloc(10, GFP_KERNEL);
+	if (buf)
+		snprintf(buf, 10, "%02x%02x%02x%02x",
+			NIPQUAD(addr->sin_addr.s_addr));
+	xprt->address_strings[RPC_DISPLAY_HEX_ADDR] = buf;
+
+	buf = kzalloc(8, GFP_KERNEL);
+	if (buf)
+		snprintf(buf, 8, "%4hx", ntohs(addr->sin_port));
+	xprt->address_strings[RPC_DISPLAY_HEX_PORT] = buf;
+
+	buf = kzalloc(30, GFP_KERNEL);
+	if (buf)
+		snprintf(buf, 30, NIPQUAD_FMT".%u.%u",
+			NIPQUAD(addr->sin_addr.s_addr),
+			ntohs(addr->sin_port) >> 8,
+			ntohs(addr->sin_port) & 0xff);
+	xprt->address_strings[RPC_DISPLAY_UNIVERSAL_ADDR] = buf;
+
+	/* netid */
+	xprt->address_strings[RPC_DISPLAY_NETID] = "rdma";
+}
+
+static void
+xprt_rdma_free_addresses(struct rpc_xprt *xprt)
+{
+	kfree(xprt->address_strings[RPC_DISPLAY_ADDR]);
+	kfree(xprt->address_strings[RPC_DISPLAY_PORT]);
+	kfree(xprt->address_strings[RPC_DISPLAY_ALL]);
+	kfree(xprt->address_strings[RPC_DISPLAY_HEX_ADDR]);
+	kfree(xprt->address_strings[RPC_DISPLAY_HEX_PORT]);
+	kfree(xprt->address_strings[RPC_DISPLAY_UNIVERSAL_ADDR]);
+}
+
+static void
+xprt_rdma_connect_worker(struct work_struct *work)
+{
+	struct rpcrdma_xprt *r_xprt =
+		container_of(work, struct rpcrdma_xprt, rdma_connect.work);
+	struct rpc_xprt *xprt = &r_xprt->xprt;
+	int rc = 0;
+
+	if (!xprt->shutdown) {
+		xprt_clear_connected(xprt);
+
+		dprintk("RPC:       %s: %sconnect\n", __func__,
+				r_xprt->rx_ep.rep_connected != 0 ? "re" : "");
+		rc = rpcrdma_ep_connect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+		if (rc)
+			goto out;
+	}
+	goto out_clear;
+
+out:
+	xprt_wake_pending_tasks(xprt, rc);
+
+out_clear:
+	dprintk("RPC:       %s: exit\n", __func__);
+	xprt_clear_connecting(xprt);
+}
+
+/*
+ * xprt_rdma_destroy
+ *
+ * Destroy the xprt.
+ * Free all memory associated with the object, including its own.
+ * NOTE: none of the *destroy methods free memory for their top-level
+ * objects, even though they may have allocated it (they do free
+ * private memory). It's up to the caller to handle it. In this
+ * case (RDMA transport), all structure memory is inlined with the
+ * struct rpcrdma_xprt.
+ */
+static void
+xprt_rdma_destroy(struct rpc_xprt *xprt)
+{
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	int rc;
+
+	dprintk("RPC:       %s: called\n", __func__);
+
+	cancel_delayed_work(&r_xprt->rdma_connect);
+	flush_scheduled_work();
+
+	xprt_clear_connected(xprt);
+
+	rpcrdma_buffer_destroy(&r_xprt->rx_buf);
+	rc = rpcrdma_ep_destroy(&r_xprt->rx_ep, &r_xprt->rx_ia);
+	if (rc)
+		dprintk("RPC:       %s: rpcrdma_ep_destroy returned %i\n",
+			__func__, rc);
+	rpcrdma_ia_close(&r_xprt->rx_ia);
+
+	xprt_rdma_free_addresses(xprt);
+
+	kfree(xprt->slot);
+	xprt->slot = NULL;
+	kfree(xprt);
+
+	dprintk("RPC:       %s: returning\n", __func__);
+
+	module_put(THIS_MODULE);
+}
+
+/**
+ * xprt_setup_rdma - Set up transport to use RDMA
+ *
+ * @args: rpc transport arguments
+ */
+static struct rpc_xprt *
+xprt_setup_rdma(struct xprt_create *args)
+{
+	struct rpcrdma_create_data_internal cdata;
+	struct rpc_xprt *xprt;
+	struct rpcrdma_xprt *new_xprt;
+	struct rpcrdma_ep *new_ep;
+	struct sockaddr_in *sin;
+	int rc;
+
+	if (args->addrlen > sizeof(xprt->addr)) {
+		dprintk("RPC:       %s: address too large\n", __func__);
+		return ERR_PTR(-EBADF);
+	}
+
+	xprt = kzalloc(sizeof(struct rpcrdma_xprt), GFP_KERNEL);
+	if (xprt == NULL) {
+		dprintk("RPC:       %s: couldn't allocate rpcrdma_xprt\n",
+			__func__);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	xprt->max_reqs = xprt_rdma_slot_table_entries;
+	xprt->slot = kcalloc(xprt->max_reqs,
+				sizeof(struct rpc_rqst), GFP_KERNEL);
+	if (xprt->slot == NULL) {
+		kfree(xprt);
+		dprintk("RPC:       %s: couldn't allocate %d slots\n",
+			__func__, xprt->max_reqs);
+		return ERR_PTR(-ENOMEM);
+	}
+
+	/* 60 second timeout, no retries */
+	xprt_set_timeout(&xprt->timeout, 0, 60UL * HZ);
+	xprt->bind_timeout = (60U * HZ);
+	xprt->connect_timeout = (60U * HZ);
+	xprt->reestablish_timeout = (5U * HZ);
+	xprt->idle_timeout = (5U * 60 * HZ);
+
+	xprt->resvport = 0;		/* privileged port not needed */
+	xprt->tsh_size = 0;		/* RPC-RDMA handles framing */
+	xprt->max_payload = RPCRDMA_MAX_DATA_SEGS * PAGE_SIZE;
+	xprt->ops = &xprt_rdma_procs;
+
+	/*
+	 * Set up RDMA-specific connect data.
+	 */
+
+	/* Put server RDMA address in local cdata */
+	memcpy(&cdata.addr, args->dstaddr, args->addrlen);
+
+	/* Ensure xprt->addr holds valid server TCP (not RDMA)
+	 * address, for any side protocols which peek at it */
+	xprt->prot = IPPROTO_TCP;
+	xprt->addrlen = args->addrlen;
+	memcpy(&xprt->addr, &cdata.addr, xprt->addrlen);
+
+	sin = (struct sockaddr_in *)&cdata.addr;
+	if (ntohs(sin->sin_port) != 0)
+		xprt_set_bound(xprt);
+
+	dprintk("RPC:       %s: %u.%u.%u.%u:%u\n", __func__,
+			NIPQUAD(sin->sin_addr.s_addr), ntohs(sin->sin_port));
+
+	/* Set max requests */
+	cdata.max_requests = xprt->max_reqs;
+
+	/* Set some length limits */
+	cdata.rsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA write max */
+	cdata.wsize = RPCRDMA_MAX_SEGS * PAGE_SIZE; /* RDMA read max */
+
+	cdata.inline_wsize = xprt_rdma_max_inline_write;
+	if (cdata.inline_wsize > cdata.wsize)
+		cdata.inline_wsize = cdata.wsize;
+
+	cdata.inline_rsize = xprt_rdma_max_inline_read;
+	if (cdata.inline_rsize > cdata.rsize)
+		cdata.inline_rsize = cdata.rsize;
+
+	cdata.padding = xprt_rdma_inline_write_padding;
+
+	/*
+	 * Create new transport instance, which includes initialized
+	 *  o ia
+	 *  o endpoint
+	 *  o buffers
+	 */
+
+	new_xprt = rpcx_to_rdmax(xprt);
+
+	rc = rpcrdma_ia_open(new_xprt, (struct sockaddr *) &cdata.addr,
+				xprt_rdma_memreg_strategy);
+	if (rc)
+		goto out1;
+
+	/*
+	 * initialize and create ep
+	 */
+	new_xprt->rx_data = cdata;
+	new_ep = &new_xprt->rx_ep;
+	new_ep->rep_remote_addr = cdata.addr;
+
+	rc = rpcrdma_ep_create(&new_xprt->rx_ep,
+				&new_xprt->rx_ia, &new_xprt->rx_data);
+	if (rc)
+		goto out2;
+
+	/*
+	 * Allocate pre-registered send and receive buffers for headers and
+	 * any inline data. Also specify any padding which will be provided
+	 * from a preregistered zero buffer.
+	 */
+	rc = rpcrdma_buffer_create(&new_xprt->rx_buf, new_ep, &new_xprt->rx_ia,
+				&new_xprt->rx_data);
+	if (rc)
+		goto out3;
+
+	/*
+	 * Register a callback for connection events. This is necessary because
+	 * connection loss notification is async. We also catch connection loss
+	 * when reaping receives.
+	 */
+	INIT_DELAYED_WORK(&new_xprt->rdma_connect, xprt_rdma_connect_worker);
+	new_ep->rep_func = rpcrdma_conn_func;
+	new_ep->rep_xprt = xprt;
+
+	xprt_rdma_format_addresses(xprt);
+
+	if (!try_module_get(THIS_MODULE))
+		goto out4;
+
+	return xprt;
+
+out4:
+	xprt_rdma_free_addresses(xprt);
+	rc = -EINVAL;
+out3:
+	(void) rpcrdma_ep_destroy(new_ep, &new_xprt->rx_ia);
+out2:
+	rpcrdma_ia_close(&new_xprt->rx_ia);
+out1:
+	kfree(xprt->slot);
+	kfree(xprt);
+	return ERR_PTR(rc);
+}
+
+/*
+ * Close a connection, during shutdown or timeout/reconnect
+ */
+static void
+xprt_rdma_close(struct rpc_xprt *xprt)
+{
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+
+	dprintk("RPC:       %s: closing\n", __func__);
+	xprt_disconnect(xprt);
+	(void) rpcrdma_ep_disconnect(&r_xprt->rx_ep, &r_xprt->rx_ia);
+}
+
+static void
+xprt_rdma_set_port(struct rpc_xprt *xprt, u16 port)
+{
+	struct sockaddr_in *sap;
+
+	sap = (struct sockaddr_in *)&xprt->addr;
+	sap->sin_port = htons(port);
+	sap = (struct sockaddr_in *)&rpcx_to_rdmad(xprt).addr;
+	sap->sin_port = htons(port);
+	dprintk("RPC:       %s: %u\n", __func__, port);
+}
+
+static void
+xprt_rdma_connect(struct rpc_task *task)
+{
+	struct rpc_xprt *xprt = (struct rpc_xprt *)task->tk_xprt;
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+
+	if (!xprt_test_and_set_connecting(xprt)) {
+		if (r_xprt->rx_ep.rep_connected != 0) {
+			/* Reconnect */
+			schedule_delayed_work(&r_xprt->rdma_connect,
+				xprt->reestablish_timeout);
+		} else {
+			schedule_delayed_work(&r_xprt->rdma_connect, 0);
+			if (!RPC_IS_ASYNC(task))
+				flush_scheduled_work();
+		}
+	}
+}
+
+static int
+xprt_rdma_reserve_xprt(struct rpc_task *task)
+{
+	struct rpc_xprt *xprt = task->tk_xprt;
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	int credits = atomic_read(&r_xprt->rx_buf.rb_credits);
+
+	/* == RPC_CWNDSCALE @ init, but *after* setup */
+	if (r_xprt->rx_buf.rb_cwndscale == 0UL) {
+		r_xprt->rx_buf.rb_cwndscale = xprt->cwnd;
+		dprintk("RPC:       %s: cwndscale %lu\n", __func__,
+			r_xprt->rx_buf.rb_cwndscale);
+		BUG_ON(r_xprt->rx_buf.rb_cwndscale <= 0);
+	}
+	xprt->cwnd = credits * r_xprt->rx_buf.rb_cwndscale;
+	return xprt_reserve_xprt_cong(task);
+}
+
+/*
+ * The RDMA allocate/free functions need the task structure as a place
+ * to hide the struct rpcrdma_req, which is necessary for the actual send/recv
+ * sequence. For this reason, the recv buffers are attached to send
+ * buffers for portions of the RPC. Note that the RPC layer allocates
+ * both send and receive buffers in the same call. We may register
+ * the receive buffer portion when using reply chunks.
+ */
+static void *
+xprt_rdma_allocate(struct rpc_task *task, size_t size)
+{
+	struct rpc_xprt *xprt = task->tk_xprt;
+	struct rpcrdma_req *req, *nreq;
+
+	req = rpcrdma_buffer_get(&rpcx_to_rdmax(xprt)->rx_buf);
+	BUG_ON(NULL == req);
+
+	if (size > req->rl_size) {
+		dprintk("RPC:       %s: size %zd too large for buffer[%zd]: "
+			"prog %d vers %d proc %d\n",
+			__func__, size, req->rl_size,
+			task->tk_client->cl_prog, task->tk_client->cl_vers,
+			task->tk_msg.rpc_proc->p_proc);
+		/*
+		 * Outgoing length shortage. Our inline write max must have
+		 * been configured to perform direct i/o.
+		 *
+		 * This is therefore a large metadata operation, and the
+		 * allocate call was made on the maximum possible message,
+		 * e.g. containing long filename(s) or symlink data. In
+		 * fact, while these metadata operations *might* carry
+		 * large outgoing payloads, they rarely *do*. However, we
+		 * have to commit to the request here, so reallocate and
+		 * register it now. The data path will never require this
+		 * reallocation.
+		 *
+		 * If the allocation or registration fails, the RPC framework
+		 * will (doggedly) retry.
+		 */
+		if (rpcx_to_rdmax(xprt)->rx_ia.ri_memreg_strategy ==
+				RPCRDMA_BOUNCEBUFFERS) {
+			/* forced to "pure inline" */
+			dprintk("RPC:       %s: too much data (%zd) for inline "
+					"(r/w max %d/%d)\n", __func__, size,
+					rpcx_to_rdmad(xprt).inline_rsize,
+					rpcx_to_rdmad(xprt).inline_wsize);
+			size = req->rl_size;
+			rpc_exit(task, -EIO);		/* fail the operation */
+			rpcx_to_rdmax(xprt)->rx_stats.failed_marshal_count++;
+			goto out;
+		}
+		if (task->tk_flags & RPC_TASK_SWAPPER)
+			nreq = kmalloc(sizeof *req + size, GFP_ATOMIC);
+		else
+			nreq = kmalloc(sizeof *req + size, GFP_NOFS);
+		if (nreq == NULL)
+			goto outfail;
+
+		if (rpcrdma_register_internal(&rpcx_to_rdmax(xprt)->rx_ia,
+				nreq->rl_base, size + sizeof(struct rpcrdma_req)
+				- offsetof(struct rpcrdma_req, rl_base),
+				&nreq->rl_handle, &nreq->rl_iov)) {
+			kfree(nreq);
+			goto outfail;
+		}
+		rpcx_to_rdmax(xprt)->rx_stats.hardway_register_count += size;
+		nreq->rl_size = size;
+		nreq->rl_niovs = 0;
+		nreq->rl_nchunks = 0;
+		nreq->rl_buffer = (struct rpcrdma_buffer *)req;
+		nreq->rl_reply = req->rl_reply;
+		memcpy(nreq->rl_segments,
+			req->rl_segments, sizeof nreq->rl_segments);
+		/* flag the swap with an unused field */
+		nreq->rl_iov.length = 0;
+		req->rl_reply = NULL;
+		req = nreq;
+	}
+	dprintk("RPC:       %s: size %zd, request 0x%p\n", __func__, size, req);
+out:
+	return req->rl_xdr_buf;
+
+outfail:
+	rpcrdma_buffer_put(req);
+	rpcx_to_rdmax(xprt)->rx_stats.failed_marshal_count++;
+	return NULL;
+}
+
+/*
+ * This function returns all RDMA resources to the pool.
+ */
+static void
+xprt_rdma_free(void *buffer)
+{
+	struct rpcrdma_req *req;
+	struct rpcrdma_xprt *r_xprt;
+	struct rpcrdma_rep *rep;
+	int i;
+
+	if (buffer == NULL)
+		return;
+
+	req = container_of(buffer, struct rpcrdma_req, rl_xdr_buf[0]);
+	r_xprt = container_of(req->rl_buffer, struct rpcrdma_xprt, rx_buf);
+	rep = req->rl_reply;
+
+	dprintk("RPC:       %s: called on 0x%p%s\n",
+		__func__, rep, (rep && rep->rr_func) ? " (with waiter)" : "");
+
+	/*
+	 * Finish the deregistration. When using mw bind, this was
+	 * begun in rpcrdma_reply_handler(). In all other modes, we
+	 * do it here, in thread context. The process is considered
+	 * complete when the rr_func vector becomes NULL - this
+	 * was put in place during rpcrdma_reply_handler() - the wait
+	 * call below will not block if the dereg is "done". If
+	 * interrupted, our framework will clean up.
+	 */
+	for (i = 0; req->rl_nchunks;) {
+		--req->rl_nchunks;
+		i += rpcrdma_deregister_external(
+			&req->rl_segments[i], r_xprt, NULL);
+	}
+
+	if (rep && wait_event_interruptible(rep->rr_unbind, !rep->rr_func)) {
+		rep->rr_func = NULL;	/* abandon the callback */
+		req->rl_reply = NULL;
+	}
+
+	if (req->rl_iov.length == 0) {	/* see allocate above */
+		struct rpcrdma_req *oreq = (struct rpcrdma_req *)req->rl_buffer;
+		oreq->rl_reply = req->rl_reply;
+		(void) rpcrdma_deregister_internal(&r_xprt->rx_ia,
+						   req->rl_handle,
+						   &req->rl_iov);
+		kfree(req);
+		req = oreq;
+	}
+
+	/* Put back request+reply buffers */
+	rpcrdma_buffer_put(req);
+}
+
+/*
+ * send_request invokes the meat of RPC RDMA. It must do the following:
+ *  1.  Marshal the RPC request into an RPC RDMA request, which means
+ *	putting a header in front of data, and creating IOVs for RDMA
+ *	from those in the request.
+ *  2.  In marshaling, detect opportunities for RDMA, and use them.
+ *  3.  Post a recv message to set up asynch completion, then send
+ *	the request (rpcrdma_ep_post).
+ *  4.  No partial sends are possible in the RPC-RDMA protocol (as in UDP).
+ */
+
+static int
+xprt_rdma_send_request(struct rpc_task *task)
+{
+	struct rpc_rqst *rqst = task->tk_rqstp;
+	struct rpc_xprt *xprt = task->tk_xprt;
+	struct rpcrdma_req *req = rpcr_to_rdmar(rqst);
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+
+	/* marshal the send itself */
+	if (req->rl_niovs == 0 && rpcrdma_marshal_req(rqst) != 0) {
+		r_xprt->rx_stats.failed_marshal_count++;
+		dprintk("RPC:       %s: rpcrdma_marshal_req failed\n",
+			__func__);
+		return -EIO;
+	}
+
+	if (req->rl_reply == NULL) 		/* e.g. reconnection */
+		rpcrdma_recv_buffer_get(req);
+
+	if (req->rl_reply) {
+		req->rl_reply->rr_func = rpcrdma_reply_handler;
+		/* this need only be done once, but... */
+		req->rl_reply->rr_xprt = xprt;
+	}
+
+	if (rpcrdma_ep_post(&r_xprt->rx_ia, &r_xprt->rx_ep, req)) {
+		xprt_disconnect(xprt);
+		return -ENOTCONN;	/* implies disconnect */
+	}
+
+	rqst->rq_bytes_sent = 0;
+	return 0;
+}
+
+static void xprt_rdma_print_stats(struct rpc_xprt *xprt, struct seq_file *seq)
+{
+	struct rpcrdma_xprt *r_xprt = rpcx_to_rdmax(xprt);
+	long idle_time = 0;
+
+	if (xprt_connected(xprt))
+		idle_time = (long)(jiffies - xprt->last_used) / HZ;
+
+	seq_printf(seq,
+	  "\txprt:\trdma %u %lu %lu %lu %ld %lu %lu %lu %Lu %Lu "
+	  "%lu %lu %lu %Lu %Lu %Lu %Lu %lu %lu %lu\n",
+
+	   0,	/* need a local port? */
+	   xprt->stat.bind_count,
+	   xprt->stat.connect_count,
+	   xprt->stat.connect_time,
+	   idle_time,
+	   xprt->stat.sends,
+	   xprt->stat.recvs,
+	   xprt->stat.bad_xids,
+	   xprt->stat.req_u,
+	   xprt->stat.bklog_u,
+
+	   r_xprt->rx_stats.read_chunk_count,
+	   r_xprt->rx_stats.write_chunk_count,
+	   r_xprt->rx_stats.reply_chunk_count,
+	   r_xprt->rx_stats.total_rdma_request,
+	   r_xprt->rx_stats.total_rdma_reply,
+	   r_xprt->rx_stats.pullup_copy_count,
+	   r_xprt->rx_stats.fixup_copy_count,
+	   r_xprt->rx_stats.hardway_register_count,
+	   r_xprt->rx_stats.failed_marshal_count,
+	   r_xprt->rx_stats.bad_reply_count);
+}
+
+/*
+ * Plumbing for rpc transport switch and kernel module
+ */
+
+static struct rpc_xprt_ops xprt_rdma_procs = {
+	.reserve_xprt		= xprt_rdma_reserve_xprt,
+	.release_xprt		= xprt_release_xprt_cong, /* sunrpc/xprt.c */
+	.release_request	= xprt_release_rqst_cong,       /* ditto */
+	.set_retrans_timeout	= xprt_set_retrans_timeout_def, /* ditto */
+	.rpcbind		= rpcb_getport_async,	/* sunrpc/rpcb_clnt.c */
+	.set_port		= xprt_rdma_set_port,
+	.connect		= xprt_rdma_connect,
+	.buf_alloc		= xprt_rdma_allocate,
+	.buf_free		= xprt_rdma_free,
+	.send_request		= xprt_rdma_send_request,
+	.close			= xprt_rdma_close,
+	.destroy		= xprt_rdma_destroy,
+	.print_stats		= xprt_rdma_print_stats
+};
+
+static struct xprt_class xprt_rdma = {
+	.list			= LIST_HEAD_INIT(xprt_rdma.list),
+	.name			= "rdma",
+	.owner			= THIS_MODULE,
+	.ident			= XPRT_TRANSPORT_RDMA,
+	.setup			= xprt_setup_rdma,
+};
+
+static void __exit xprt_rdma_cleanup(void)
+{
+	int rc;
+
+	dprintk("RPCRDMA Module Removed, deregister RPC RDMA transport\n");
+#ifdef RPC_DEBUG
+	if (sunrpc_table_header) {
+		unregister_sysctl_table(sunrpc_table_header);
+		sunrpc_table_header = NULL;
+	}
+#endif
+	rc = xprt_unregister_transport(&xprt_rdma);
+	if (rc)
+		dprintk("RPC:       %s: xprt_unregister returned %i\n",
+			__func__, rc);
+}
+
+static int __init xprt_rdma_init(void)
+{
+	int rc;
+
+	rc = xprt_register_transport(&xprt_rdma);
+
+	if (rc)
+		return rc;
+
+	dprintk(KERN_INFO "RPCRDMA Module Init, register RPC RDMA transport\n");
+
+	dprintk(KERN_INFO "Defaults:\n");
+	dprintk(KERN_INFO "\tSlots %d\n"
+		"\tMaxInlineRead %d\n\tMaxInlineWrite %d\n",
+		xprt_rdma_slot_table_entries,
+		xprt_rdma_max_inline_read, xprt_rdma_max_inline_write);
+	dprintk(KERN_INFO "\tPadding %d\n\tMemreg %d\n",
+		xprt_rdma_inline_write_padding, xprt_rdma_memreg_strategy);
+
+#ifdef RPC_DEBUG
+	if (!sunrpc_table_header)
+		sunrpc_table_header = register_sysctl_table(sunrpc_table);
+#endif
+	return 0;
+}
+
+module_init(xprt_rdma_init);
+module_exit(xprt_rdma_cleanup);
