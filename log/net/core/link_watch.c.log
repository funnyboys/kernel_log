commit eec517cdb4810b3843eb7707971de3164088bff1
Author: Andrew Lunn <andrew@lunn.ch>
Date:   Mon Apr 20 00:11:50 2020 +0200

    net: Add IF_OPER_TESTING
    
    RFC 2863 defines the operational state testing. Add support for this
    state, both as a IF_LINK_MODE_ and __LINK_STATE_.
    
    Signed-off-by: Andrew Lunn <andrew@lunn.ch>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index f153e0601838..75431ca9300f 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -34,6 +34,9 @@ static DEFINE_SPINLOCK(lweventlist_lock);
 
 static unsigned char default_operstate(const struct net_device *dev)
 {
+	if (netif_testing(dev))
+		return IF_OPER_TESTING;
+
 	if (!netif_carrier_ok(dev))
 		return (dev->ifindex != dev_get_iflink(dev) ?
 			IF_OPER_LOWERLAYERDOWN : IF_OPER_DOWN);
@@ -55,11 +58,15 @@ static void rfc2863_policy(struct net_device *dev)
 	write_lock_bh(&dev_base_lock);
 
 	switch(dev->link_mode) {
+	case IF_LINK_MODE_TESTING:
+		if (operstate == IF_OPER_UP)
+			operstate = IF_OPER_TESTING;
+		break;
+
 	case IF_LINK_MODE_DORMANT:
 		if (operstate == IF_OPER_UP)
 			operstate = IF_OPER_DORMANT;
 		break;
-
 	case IF_LINK_MODE_DEFAULT:
 	default:
 		break;
@@ -74,7 +81,8 @@ static void rfc2863_policy(struct net_device *dev)
 void linkwatch_init_dev(struct net_device *dev)
 {
 	/* Handle pre-registration link state changes */
-	if (!netif_carrier_ok(dev) || netif_dormant(dev))
+	if (!netif_carrier_ok(dev) || netif_dormant(dev) ||
+	    netif_testing(dev))
 		rfc2863_policy(dev);
 }
 

commit 27ba4059e06b3bbd38a7d944fd5a78cdf47534f4
Author: Yunsheng Lin <linyunsheng@huawei.com>
Date:   Fri Jun 28 09:13:19 2019 +0800

    net: link_watch: prevent starvation when processing linkwatch wq
    
    When user has configured a large number of virtual netdev, such
    as 4K vlans, the carrier on/off operation of the real netdev
    will also cause it's virtual netdev's link state to be processed
    in linkwatch. Currently, the processing is done in a work queue,
    which may cause rtnl locking starvation problem and worker
    starvation problem for other work queue, such as irqfd_inject wq.
    
    This patch releases the cpu when link watch worker has processed
    a fixed number of netdev' link watch event, and schedule the
    work queue again when there is still link watch event remaining.
    
    Signed-off-by: Yunsheng Lin <linyunsheng@huawei.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 04fdc9535772..f153e0601838 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -163,9 +163,16 @@ static void linkwatch_do_dev(struct net_device *dev)
 
 static void __linkwatch_run_queue(int urgent_only)
 {
+#define MAX_DO_DEV_PER_LOOP	100
+
+	int do_dev = MAX_DO_DEV_PER_LOOP;
 	struct net_device *dev;
 	LIST_HEAD(wrk);
 
+	/* Give urgent case more budget */
+	if (urgent_only)
+		do_dev += MAX_DO_DEV_PER_LOOP;
+
 	/*
 	 * Limit the number of linkwatch events to one
 	 * per second so that a runaway driver does not
@@ -184,7 +191,7 @@ static void __linkwatch_run_queue(int urgent_only)
 	spin_lock_irq(&lweventlist_lock);
 	list_splice_init(&lweventlist, &wrk);
 
-	while (!list_empty(&wrk)) {
+	while (!list_empty(&wrk) && do_dev > 0) {
 
 		dev = list_first_entry(&wrk, struct net_device, link_watch_list);
 		list_del_init(&dev->link_watch_list);
@@ -195,9 +202,13 @@ static void __linkwatch_run_queue(int urgent_only)
 		}
 		spin_unlock_irq(&lweventlist_lock);
 		linkwatch_do_dev(dev);
+		do_dev--;
 		spin_lock_irq(&lweventlist_lock);
 	}
 
+	/* Add the remaining work back to lweventlist */
+	list_splice_init(&wrk, &lweventlist);
+
 	if (!list_empty(&lweventlist))
 		linkwatch_schedule_work(0);
 	spin_unlock_irq(&lweventlist_lock);

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 7f51efb2b3ab..04fdc9535772 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -1,14 +1,9 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Linux network device link state notification
  *
  * Author:
  *     Stefan Rompf <sux@loplof.de>
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
- *
  */
 
 #include <linux/module.h>

commit 124eee3f6955f7aa19b9e6ff5c9b6d37cb3d1e2c
Author: Heiner Kallweit <hkallweit1@gmail.com>
Date:   Tue Sep 18 21:55:36 2018 +0200

    net: linkwatch: add check for netdevice being present to linkwatch_do_dev
    
    When bringing down the netdevice (incl. detaching it) and calling
    netif_carrier_off directly or indirectly the latter triggers an
    asynchronous linkwatch event.
    This linkwatch event eventually may fail to access chip registers in
    the ndo_get_stats/ndo_get_stats64 callback because the device isn't
    accessible any longer, see call trace in [0].
    
    To prevent this scenario don't check for IFF_UP only, but also make
    sure that the netdevice is present.
    
    [0] https://lists.openwall.net/netdev/2018/03/15/62
    
    Signed-off-by: Heiner Kallweit <hkallweit1@gmail.com>
    Tested-by: Geert Uytterhoeven <geert+renesas@glider.be>
    Reviewed-by: Florian Fainelli <f.fainelli@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index e38e641e98d5..7f51efb2b3ab 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -155,7 +155,7 @@ static void linkwatch_do_dev(struct net_device *dev)
 	clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
 
 	rfc2863_policy(dev);
-	if (dev->flags & IFF_UP) {
+	if (dev->flags & IFF_UP && netif_device_present(dev)) {
 		if (netif_carrier_ok(dev))
 			dev_activate(dev);
 		else

commit b76f4189df5c54a892ae54ac23908cc54ae7134f
Author: Roopa Prabhu <roopa@cumulusnetworks.com>
Date:   Mon Jan 22 08:07:19 2018 -0800

    net: link_watch: mark bonding link events urgent
    
    It takes 1sec for bond link down notification to hit user-space
    when all slaves of the bond go down. 1sec is too long for
    protocol daemons in user-space relying on bond notification
    to recover (eg: multichassis lag implementations in user-space).
    Since the link event code already marks team device port link events
     as urgent, this patch moves the code to cover all lag ports and master.
    
    Signed-off-by: Roopa Prabhu <roopa@cumulusnetworks.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Reviewed-by: Nikolay Aleksandrov <nikolay@cumulusnetworks.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 982861607f88..e38e641e98d5 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -92,7 +92,7 @@ static bool linkwatch_urgent_event(struct net_device *dev)
 	if (dev->ifindex != dev_get_iflink(dev))
 		return true;
 
-	if (dev->priv_flags & IFF_TEAM_PORT)
+	if (netif_is_lag_port(dev) || netif_is_lag_master(dev))
 		return true;
 
 	return netif_carrier_ok(dev) &&	qdisc_tx_changing(dev);

commit a54acb3a6f853e8394c4cb7b6a4d93c88f13eefd
Author: Nicolas Dichtel <nicolas.dichtel@6wind.com>
Date:   Thu Apr 2 17:07:00 2015 +0200

    dev: introduce dev_get_iflink()
    
    The goal of this patch is to prepare the removal of the iflink field. It
    introduces a new ndo function, which will be implemented by virtual interfaces.
    
    There is no functional change into this patch. All readers of iflink field
    now call dev_get_iflink().
    
    Signed-off-by: Nicolas Dichtel <nicolas.dichtel@6wind.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 49a9e3e06c08..982861607f88 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -40,7 +40,7 @@ static DEFINE_SPINLOCK(lweventlist_lock);
 static unsigned char default_operstate(const struct net_device *dev)
 {
 	if (!netif_carrier_ok(dev))
-		return (dev->ifindex != dev->iflink ?
+		return (dev->ifindex != dev_get_iflink(dev) ?
 			IF_OPER_LOWERLAYERDOWN : IF_OPER_DOWN);
 
 	if (netif_dormant(dev))
@@ -89,7 +89,7 @@ static bool linkwatch_urgent_event(struct net_device *dev)
 	if (!netif_running(dev))
 		return false;
 
-	if (dev->ifindex != dev->iflink)
+	if (dev->ifindex != dev_get_iflink(dev))
 		return true;
 
 	if (dev->priv_flags & IFF_TEAM_PORT)

commit e56f735913c8d3c417c65c7e7fcdd65011f8d96f
Author: Fabian Frederick <fabf@skynet.be>
Date:   Mon Nov 17 22:08:22 2014 +0100

    net/core: include linux/types.h instead of asm/types.h
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index bd0767e6b2b3..49a9e3e06c08 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -21,7 +21,7 @@
 #include <linux/spinlock.h>
 #include <linux/workqueue.h>
 #include <linux/bitops.h>
-#include <asm/types.h>
+#include <linux/types.h>
 
 
 enum lw_bits {

commit 4e857c58efeb99393cba5a5d0d8ec7117183137c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Mar 17 18:06:10 2014 +0100

    arch: Mass conversion of smp_mb__*()
    
    Mostly scripted conversion of the smp_mb__* barriers.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/n/tip-55dhyhocezdw1dg7u19hmh1u@git.kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 9c3a839322ba..bd0767e6b2b3 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -147,7 +147,7 @@ static void linkwatch_do_dev(struct net_device *dev)
 	 * Make sure the above read is complete since it can be
 	 * rewritten as soon as we clear the bit below.
 	 */
-	smp_mb__before_clear_bit();
+	smp_mb__before_atomic();
 
 	/* We are about to handle this device,
 	 * so new events can be accepted

commit 194f4a6df2a92c3d0bc65a85facfbc2433b25d06
Author: Flavio Leitner <fbl@redhat.com>
Date:   Tue Jun 11 23:09:29 2013 +0200

    net: make all team port device link events urgent
    
    Since team functionality relies heavily on userspace daemon, we need to
    deliver event to userspace via Netlink as quick as possible. So make all
    team port device link events urgent.
    
    Signed-off-by: Flavio Leitner <fbl@redhat.com>
    Signed-off-by: Jiri Pirko <jiri@resnulli.us>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 8f82a5cc3851..9c3a839322ba 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -92,6 +92,9 @@ static bool linkwatch_urgent_event(struct net_device *dev)
 	if (dev->ifindex != dev->iflink)
 		return true;
 
+	if (dev->priv_flags & IFF_TEAM_PORT)
+		return true;
+
 	return netif_carrier_ok(dev) &&	qdisc_tx_changing(dev);
 }
 

commit aecdc33e111b2c447b622e287c6003726daa1426
Merge: a20acf99f75e a3a6cab5ea10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 13:38:27 2012 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking changes from David Miller:
    
     1) GRE now works over ipv6, from Dmitry Kozlov.
    
     2) Make SCTP more network namespace aware, from Eric Biederman.
    
     3) TEAM driver now works with non-ethernet devices, from Jiri Pirko.
    
     4) Make openvswitch network namespace aware, from Pravin B Shelar.
    
     5) IPV6 NAT implementation, from Patrick McHardy.
    
     6) Server side support for TCP Fast Open, from Jerry Chu and others.
    
     7) Packet BPF filter supports MOD and XOR, from Eric Dumazet and Daniel
        Borkmann.
    
     8) Increate the loopback default MTU to 64K, from Eric Dumazet.
    
     9) Use a per-task rather than per-socket page fragment allocator for
        outgoing networking traffic.  This benefits processes that have very
        many mostly idle sockets, which is quite common.
    
        From Eric Dumazet.
    
    10) Use up to 32K for page fragment allocations, with fallbacks to
        smaller sizes when higher order page allocations fail.  Benefits are
        a) less segments for driver to process b) less calls to page
        allocator c) less waste of space.
    
        From Eric Dumazet.
    
    11) Allow GRO to be used on GRE tunnels, from Eric Dumazet.
    
    12) VXLAN device driver, one way to handle VLAN issues such as the
        limitation of 4096 VLAN IDs yet still have some level of isolation.
        From Stephen Hemminger.
    
    13) As usual there is a large boatload of driver changes, with the scale
        perhaps tilted towards the wireless side this time around.
    
    Fix up various fairly trivial conflicts, mostly caused by the user
    namespace changes.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1012 commits)
      hyperv: Add buffer for extended info after the RNDIS response message.
      hyperv: Report actual status in receive completion packet
      hyperv: Remove extra allocated space for recv_pkt_list elements
      hyperv: Fix page buffer handling in rndis_filter_send_request()
      hyperv: Fix the missing return value in rndis_filter_set_packet_filter()
      hyperv: Fix the max_xfer_size in RNDIS initialization
      vxlan: put UDP socket in correct namespace
      vxlan: Depend on CONFIG_INET
      sfc: Fix the reported priorities of different filter types
      sfc: Remove EFX_FILTER_FLAG_RX_OVERRIDE_IP
      sfc: Fix loopback self-test with separate_tx_channels=1
      sfc: Fix MCDI structure field lookup
      sfc: Add parentheses around use of bitfield macro arguments
      sfc: Fix null function pointer in efx_sriov_channel_type
      vxlan: virtual extensible lan
      igmp: export symbol ip_mc_leave_group
      netlink: add attributes to fdb interface
      tg3: unconditionally select HWMON support when tg3 is enabled.
      Revert "net: ti cpsw ethernet: allow reading phy interface mode from DT"
      gre: fix sparse warning
      ...

commit 8f4cccbbd92f2ad0ddbbc498ef7cee2a1c3defe9
Author: Ben Hutchings <bhutchings@solarflare.com>
Date:   Mon Aug 20 22:16:51 2012 +0100

    net: Set device operstate at registration time
    
    The operstate of a device is initially IF_OPER_UNKNOWN and is updated
    asynchronously by linkwatch after each change of carrier state
    reported by the driver.  The default carrier state of a net device is
    on, and this will never be changed on drivers that do not support
    carrier detection, thus the operstate remains IF_OPER_UNKNOWN.
    
    For devices that do support carrier detection, the driver must set the
    carrier state to off initially, then poll the hardware state when the
    device is opened.  However, we must not activate linkwatch for a
    unregistered device, and commit b473001 ('net: Do not fire linkwatch
    events until the device is registered.') ensured that we don't.  But
    this means that the operstate for many devices that support carrier
    detection remains IF_OPER_UNKNOWN when it should be IF_OPER_DOWN.
    
    The same issue exists with the dormant state.
    
    The proper initialisation sequence, avoiding a race with opening of
    the device, is:
    
            rtnl_lock();
            rc = register_netdevice(dev);
            if (rc)
                    goto out_unlock;
            netif_carrier_off(dev); /* or netif_dormant_on(dev) */
            rtnl_unlock();
    
    but it seems silly that this should have to be repeated in so many
    drivers.  Further, the operstate seen immediately after opening the
    device may still be IF_OPER_UNKNOWN due to the asynchronous nature of
    linkwatch.
    
    Commit 22604c8 ('net: Fix for initial link state in 2.6.28') attempted
    to fix this by setting the operstate synchronously, but it was
    reverted as it could lead to deadlock.
    
    This initialises the operstate synchronously at registration time
    only.
    
    Signed-off-by: Ben Hutchings <bhutchings@solarflare.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index c3519c6d1b16..a01922219a23 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -76,6 +76,14 @@ static void rfc2863_policy(struct net_device *dev)
 }
 
 
+void linkwatch_init_dev(struct net_device *dev)
+{
+	/* Handle pre-registration link state changes */
+	if (!netif_carrier_ok(dev) || netif_dormant(dev))
+		rfc2863_policy(dev);
+}
+
+
 static bool linkwatch_urgent_event(struct net_device *dev)
 {
 	if (!netif_running(dev))

commit e7c2f967445dd2041f0f8e3179cca22bb8bb7f79
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Aug 21 13:18:24 2012 -0700

    workqueue: use mod_delayed_work() instead of __cancel + queue
    
    Now that mod_delayed_work() is safe to call from IRQ handlers,
    __cancel_delayed_work() followed by queue_delayed_work() can be
    replaced with mod_delayed_work().
    
    Most conversions are straight-forward except for the following.
    
    * net/core/link_watch.c: linkwatch_schedule_work() was doing a quite
      elaborate dancing around its delayed_work.  Collapse it such that
      linkwatch_work is queued for immediate execution if LW_URGENT and
      existing timer is kept otherwise.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Tomi Valkeinen <tomi.valkeinen@ti.com>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index c3519c6d1b16..8e397a69005a 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -120,22 +120,13 @@ static void linkwatch_schedule_work(int urgent)
 		delay = 0;
 
 	/*
-	 * This is true if we've scheduled it immeditately or if we don't
-	 * need an immediate execution and it's already pending.
+	 * If urgent, schedule immediate execution; otherwise, don't
+	 * override the existing timer.
 	 */
-	if (schedule_delayed_work(&linkwatch_work, delay) == !delay)
-		return;
-
-	/* Don't bother if there is nothing urgent. */
-	if (!test_bit(LW_URGENT, &linkwatch_flags))
-		return;
-
-	/* It's already running which is good enough. */
-	if (!__cancel_delayed_work(&linkwatch_work))
-		return;
-
-	/* Otherwise we reschedule it again for immediate execution. */
-	schedule_delayed_work(&linkwatch_work, 0);
+	if (test_bit(LW_URGENT, &linkwatch_flags))
+		mod_delayed_work(system_wq, &linkwatch_work, 0);
+	else
+		schedule_delayed_work(&linkwatch_work, delay);
 }
 
 

commit c37e0c993055d8c4fd82202331d06e6ef9bfec4b
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Aug 30 23:31:58 2011 +0000

    net: linkwatch: allow vlans to get carrier changes faster
    
    There is a time-lag of IFF_RUNNING flag consistency between vlan and
    real devices when the real devices are in problem such as link or cable
    broken.
    
    This leads to a degradation of Availability such as a delay of failover
    in HA systems using vlan since the detection of the problem at real
    device is delayed.
    
    We can avoid the linkwatch delay (~1 sec) for devices linked to another
    ones, since delay is already done for the realdev.
    
    Based on a previous patch from Mitsuo Hayasaka
    
    Reported-by: Mitsuo Hayasaka <mitsuo.hayasaka.hu@hitachi.com>
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Herbert Xu <herbert@gondor.apana.org.au>
    Cc: Patrick McHardy <kaber@trash.net>
    Cc: "Michał Mirosław" <mirq-linux@rere.qmqm.pl>
    Cc: Tom Herbert <therbert@google.com>
    Cc: Stephen Hemminger <shemminger@vyatta.com>
    Cc: Jesse Gross <jesse@nicira.com>
    Tested-by: Mitsuo Hayasaka <mitsuo.hayasaka.hu@hitachi.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 357bd4ee4baa..c3519c6d1b16 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -78,8 +78,13 @@ static void rfc2863_policy(struct net_device *dev)
 
 static bool linkwatch_urgent_event(struct net_device *dev)
 {
-	return netif_running(dev) && netif_carrier_ok(dev) &&
-		qdisc_tx_changing(dev);
+	if (!netif_running(dev))
+		return false;
+
+	if (dev->ifindex != dev->iflink)
+		return true;
+
+	return netif_carrier_ok(dev) &&	qdisc_tx_changing(dev);
 }
 
 

commit 1821f7cd65ad9ea56580b830ac79bf4c4fef59cb
Author: stephen hemminger <shemminger@vyatta.com>
Date:   Fri Jul 22 12:53:56 2011 +0000

    net: allow netif_carrier to be called safely from IRQ
    
    As reported by Ben Greer and Froncois Romieu. The code path in
    the netif_carrier code leads it to try and disable
    a late workqueue to reenable it immediately
    netif_carrier_on
    -> linkwatch_fire_event
       -> linkwatch_schedule_work
          -> cancel_delayed_work
             -> del_timer_sync
    
    If __cancel_delayed_work is used instead then there is no
    problem of waiting for running linkwatch_event.
    
    There is a race between linkwatch_event running re-scheduling
    but it is harmless to schedule an extra scan of the linkwatch queue.
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index a7b342131869..357bd4ee4baa 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -126,7 +126,7 @@ static void linkwatch_schedule_work(int urgent)
 		return;
 
 	/* It's already running which is good enough. */
-	if (!cancel_delayed_work(&linkwatch_work))
+	if (!__cancel_delayed_work(&linkwatch_work))
 		return;
 
 	/* Otherwise we reschedule it again for immediate execution. */

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 01a1101b5936..a7b342131869 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -129,7 +129,7 @@ static void linkwatch_schedule_work(int urgent)
 	if (!cancel_delayed_work(&linkwatch_work))
 		return;
 
-	/* Otherwise we reschedule it again for immediate exection. */
+	/* Otherwise we reschedule it again for immediate execution. */
 	schedule_delayed_work(&linkwatch_work, 0);
 }
 

commit 9e34a5b51684bc90ac827ec4ba339f3892632eac
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Fri Jul 9 21:22:04 2010 +0000

    net/core: EXPORT_SYMBOL cleanups
    
    CodingStyle cleanups
    
    EXPORT_SYMBOL should immediately follow the symbol declaration.
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index bdbce2f5875b..01a1101b5936 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -243,5 +243,4 @@ void linkwatch_fire_event(struct net_device *dev)
 
 	linkwatch_schedule_work(urgent);
 }
-
 EXPORT_SYMBOL(linkwatch_fire_event);

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 5910b555a54a..bdbce2f5875b 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -19,7 +19,6 @@
 #include <linux/rtnetlink.h>
 #include <linux/jiffies.h>
 #include <linux/spinlock.h>
-#include <linux/slab.h>
 #include <linux/workqueue.h>
 #include <linux/bitops.h>
 #include <asm/types.h>

commit e014debecd3ee3832e6476b3a9c948edfcfd1250
Author: Eric Dumazet <eric.dumazet@gmail.com>
Date:   Tue Nov 17 05:59:21 2009 +0000

    linkwatch: linkwatch_forget_dev() to speedup device dismantle
    
    Herbert Xu a écrit :
    > On Tue, Nov 17, 2009 at 04:26:04AM -0800, David Miller wrote:
    >> Really, the link watch stuff is just due for a redesign.  I don't
    >> think a simple hack is going to cut it this time, sorry Eric :-)
    >
    > I have no objections against any redesigns, but since the only
    > caller of linkwatch_forget_dev runs in process context with the
    > RTNL, it could also legally emit those events.
    
    Thanks guys, here an updated version then, before linkwatch surgery ?
    
    In this version, I force the event to be sent synchronously.
    
    [PATCH net-next-2.6] linkwatch: linkwatch_forget_dev() to speedup device dismantle
    
    time ip link del eth3.103 ; time ip link del eth3.104 ; time ip link del eth3.105
    
    real    0m0.266s
    user    0m0.000s
    sys     0m0.001s
    
    real    0m0.770s
    user    0m0.000s
    sys     0m0.000s
    
    real    0m1.022s
    user    0m0.000s
    sys     0m0.000s
    
    One problem of current schem in vlan dismantle phase is the
    holding of device done by following chain :
    
    vlan_dev_stop() ->
            netif_carrier_off(dev) ->
                    linkwatch_fire_event(dev) ->
                            dev_hold() ...
    
    And __linkwatch_run_queue() runs up to one second later...
    
    A generic fix to this problem is to add a linkwatch_forget_dev() method
    to unlink the device from the list of watched devices.
    
    dev->link_watch_next becomes dev->link_watch_list (and use a bit more memory),
    to be able to unlink device in O(1).
    
    After patch :
    time ip link del eth3.103 ; time ip link del eth3.104 ; time ip link del eth3.105
    
    real    0m0.024s
    user    0m0.000s
    sys     0m0.000s
    
    real    0m0.032s
    user    0m0.000s
    sys     0m0.001s
    
    real    0m0.033s
    user    0m0.000s
    sys     0m0.000s
    
    Signed-off-by: Eric Dumazet <eric.dumazet@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index bf8f7af699d7..5910b555a54a 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -35,7 +35,7 @@ static unsigned long linkwatch_nextevent;
 static void linkwatch_event(struct work_struct *dummy);
 static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event);
 
-static struct net_device *lweventlist;
+static LIST_HEAD(lweventlist);
 static DEFINE_SPINLOCK(lweventlist_lock);
 
 static unsigned char default_operstate(const struct net_device *dev)
@@ -89,8 +89,10 @@ static void linkwatch_add_event(struct net_device *dev)
 	unsigned long flags;
 
 	spin_lock_irqsave(&lweventlist_lock, flags);
-	dev->link_watch_next = lweventlist;
-	lweventlist = dev;
+	if (list_empty(&dev->link_watch_list)) {
+		list_add_tail(&dev->link_watch_list, &lweventlist);
+		dev_hold(dev);
+	}
 	spin_unlock_irqrestore(&lweventlist_lock, flags);
 }
 
@@ -133,9 +135,35 @@ static void linkwatch_schedule_work(int urgent)
 }
 
 
+static void linkwatch_do_dev(struct net_device *dev)
+{
+	/*
+	 * Make sure the above read is complete since it can be
+	 * rewritten as soon as we clear the bit below.
+	 */
+	smp_mb__before_clear_bit();
+
+	/* We are about to handle this device,
+	 * so new events can be accepted
+	 */
+	clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
+
+	rfc2863_policy(dev);
+	if (dev->flags & IFF_UP) {
+		if (netif_carrier_ok(dev))
+			dev_activate(dev);
+		else
+			dev_deactivate(dev);
+
+		netdev_state_change(dev);
+	}
+	dev_put(dev);
+}
+
 static void __linkwatch_run_queue(int urgent_only)
 {
-	struct net_device *next;
+	struct net_device *dev;
+	LIST_HEAD(wrk);
 
 	/*
 	 * Limit the number of linkwatch events to one
@@ -153,46 +181,40 @@ static void __linkwatch_run_queue(int urgent_only)
 	clear_bit(LW_URGENT, &linkwatch_flags);
 
 	spin_lock_irq(&lweventlist_lock);
-	next = lweventlist;
-	lweventlist = NULL;
-	spin_unlock_irq(&lweventlist_lock);
+	list_splice_init(&lweventlist, &wrk);
 
-	while (next) {
-		struct net_device *dev = next;
+	while (!list_empty(&wrk)) {
 
-		next = dev->link_watch_next;
+		dev = list_first_entry(&wrk, struct net_device, link_watch_list);
+		list_del_init(&dev->link_watch_list);
 
 		if (urgent_only && !linkwatch_urgent_event(dev)) {
-			linkwatch_add_event(dev);
+			list_add_tail(&dev->link_watch_list, &lweventlist);
 			continue;
 		}
-
-		/*
-		 * Make sure the above read is complete since it can be
-		 * rewritten as soon as we clear the bit below.
-		 */
-		smp_mb__before_clear_bit();
-
-		/* We are about to handle this device,
-		 * so new events can be accepted
-		 */
-		clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
-
-		rfc2863_policy(dev);
-		if (dev->flags & IFF_UP) {
-			if (netif_carrier_ok(dev))
-				dev_activate(dev);
-			else
-				dev_deactivate(dev);
-
-			netdev_state_change(dev);
-		}
-
-		dev_put(dev);
+		spin_unlock_irq(&lweventlist_lock);
+		linkwatch_do_dev(dev);
+		spin_lock_irq(&lweventlist_lock);
 	}
 
-	if (lweventlist)
+	if (!list_empty(&lweventlist))
 		linkwatch_schedule_work(0);
+	spin_unlock_irq(&lweventlist_lock);
+}
+
+void linkwatch_forget_dev(struct net_device *dev)
+{
+	unsigned long flags;
+	int clean = 0;
+
+	spin_lock_irqsave(&lweventlist_lock, flags);
+	if (!list_empty(&dev->link_watch_list)) {
+		list_del_init(&dev->link_watch_list);
+		clean = 1;
+	}
+	spin_unlock_irqrestore(&lweventlist_lock, flags);
+	if (clean)
+		linkwatch_do_dev(dev);
 }
 
 
@@ -216,8 +238,6 @@ void linkwatch_fire_event(struct net_device *dev)
 	bool urgent = linkwatch_urgent_event(dev);
 
 	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
-		dev_hold(dev);
-
 		linkwatch_add_event(dev);
 	} else if (!urgent)
 		return;

commit 6fa9864b53f0680e432a2c431c2cf2055daa3a88
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 8 23:01:06 2008 -0700

    net: Clean up explicit ->tx_queue references in link watch.
    
    First, we add a qdisc_tx_changing() helper which returns true if the
    qdisc attachment is in transition.
    
    Second, we remove an assertion warning which is of limited value and
    is hard to express precisely in a multiqueue environment.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 50218218445b..bf8f7af699d7 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -77,12 +77,10 @@ static void rfc2863_policy(struct net_device *dev)
 }
 
 
-static int linkwatch_urgent_event(struct net_device *dev)
+static bool linkwatch_urgent_event(struct net_device *dev)
 {
-	struct netdev_queue *txq = &dev->tx_queue;
-
 	return netif_running(dev) && netif_carrier_ok(dev) &&
-	       txq->qdisc != txq->qdisc_sleeping;
+		qdisc_tx_changing(dev);
 }
 
 
@@ -182,12 +180,9 @@ static void __linkwatch_run_queue(int urgent_only)
 
 		rfc2863_policy(dev);
 		if (dev->flags & IFF_UP) {
-			if (netif_carrier_ok(dev)) {
-				struct netdev_queue *txq = &dev->tx_queue;
-
-				WARN_ON(txq->qdisc_sleeping == &noop_qdisc);
+			if (netif_carrier_ok(dev))
 				dev_activate(dev);
-			} else
+			else
 				dev_deactivate(dev);
 
 			netdev_state_change(dev);
@@ -218,7 +213,7 @@ static void linkwatch_event(struct work_struct *dummy)
 
 void linkwatch_fire_event(struct net_device *dev)
 {
-	int urgent = linkwatch_urgent_event(dev);
+	bool urgent = linkwatch_urgent_event(dev);
 
 	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
 		dev_hold(dev);

commit b0e1e6462df3c5944010b3328a546d8fe5d932cd
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Jul 8 17:42:10 2008 -0700

    netdev: Move rest of qdisc state into struct netdev_queue
    
    Now qdisc, qdisc_sleeping, and qdisc_list also live there.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index a5e372b9ec4d..50218218445b 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -79,8 +79,10 @@ static void rfc2863_policy(struct net_device *dev)
 
 static int linkwatch_urgent_event(struct net_device *dev)
 {
+	struct netdev_queue *txq = &dev->tx_queue;
+
 	return netif_running(dev) && netif_carrier_ok(dev) &&
-	       dev->qdisc != dev->qdisc_sleeping;
+	       txq->qdisc != txq->qdisc_sleeping;
 }
 
 
@@ -181,7 +183,9 @@ static void __linkwatch_run_queue(int urgent_only)
 		rfc2863_policy(dev);
 		if (dev->flags & IFF_UP) {
 			if (netif_carrier_ok(dev)) {
-				WARN_ON(dev->qdisc_sleeping == &noop_qdisc);
+				struct netdev_queue *txq = &dev->tx_queue;
+
+				WARN_ON(txq->qdisc_sleeping == &noop_qdisc);
 				dev_activate(dev);
 			} else
 				dev_deactivate(dev);

commit d9568ba91b1fdd1ea4fdbf9fcc76b867cca6c1d5
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Wed May 9 00:17:30 2007 -0700

    [NET] link_watch: Always schedule urgent events
    
    Urgent events may be delayed if we already have a non-urgent event
    queued for that device.  This patch changes this by making sure that
    an urgent event is always looked at immediately.
    
    I've replaced the LW_RUNNING flag by LW_URGENT since whether work
    is scheduled is already kept track by the work queue system.
    
    The only complication is that we have to provide some exclusion for
    the setting linkwatch_nextevent which is available in the actual
    work function.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 4674ae574128..a5e372b9ec4d 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -26,7 +26,7 @@
 
 
 enum lw_bits {
-	LW_RUNNING = 0,
+	LW_URGENT = 0,
 };
 
 static unsigned long linkwatch_flags;
@@ -95,18 +95,41 @@ static void linkwatch_add_event(struct net_device *dev)
 }
 
 
-static void linkwatch_schedule_work(unsigned long delay)
+static void linkwatch_schedule_work(int urgent)
 {
-	if (test_and_set_bit(LW_RUNNING, &linkwatch_flags))
+	unsigned long delay = linkwatch_nextevent - jiffies;
+
+	if (test_bit(LW_URGENT, &linkwatch_flags))
 		return;
 
-	/* If we wrap around we'll delay it by at most HZ. */
-	if (delay > HZ) {
-		linkwatch_nextevent = jiffies;
+	/* Minimise down-time: drop delay for up event. */
+	if (urgent) {
+		if (test_and_set_bit(LW_URGENT, &linkwatch_flags))
+			return;
 		delay = 0;
 	}
 
-	schedule_delayed_work(&linkwatch_work, delay);
+	/* If we wrap around we'll delay it by at most HZ. */
+	if (delay > HZ)
+		delay = 0;
+
+	/*
+	 * This is true if we've scheduled it immeditately or if we don't
+	 * need an immediate execution and it's already pending.
+	 */
+	if (schedule_delayed_work(&linkwatch_work, delay) == !delay)
+		return;
+
+	/* Don't bother if there is nothing urgent. */
+	if (!test_bit(LW_URGENT, &linkwatch_flags))
+		return;
+
+	/* It's already running which is good enough. */
+	if (!cancel_delayed_work(&linkwatch_work))
+		return;
+
+	/* Otherwise we reschedule it again for immediate exection. */
+	schedule_delayed_work(&linkwatch_work, 0);
 }
 
 
@@ -123,7 +146,11 @@ static void __linkwatch_run_queue(int urgent_only)
 	 */
 	if (!urgent_only)
 		linkwatch_nextevent = jiffies + HZ;
-	clear_bit(LW_RUNNING, &linkwatch_flags);
+	/* Limit wrap-around effect on delay. */
+	else if (time_after(linkwatch_nextevent, jiffies + HZ))
+		linkwatch_nextevent = jiffies;
+
+	clear_bit(LW_URGENT, &linkwatch_flags);
 
 	spin_lock_irq(&lweventlist_lock);
 	next = lweventlist;
@@ -166,7 +193,7 @@ static void __linkwatch_run_queue(int urgent_only)
 	}
 
 	if (lweventlist)
-		linkwatch_schedule_work(linkwatch_nextevent - jiffies);
+		linkwatch_schedule_work(0);
 }
 
 
@@ -187,21 +214,16 @@ static void linkwatch_event(struct work_struct *dummy)
 
 void linkwatch_fire_event(struct net_device *dev)
 {
-	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
-		unsigned long delay;
+	int urgent = linkwatch_urgent_event(dev);
 
+	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
 		dev_hold(dev);
 
 		linkwatch_add_event(dev);
+	} else if (!urgent)
+		return;
 
-		delay = linkwatch_nextevent - jiffies;
-
-		/* Minimise down-time: drop delay for up event. */
-		if (linkwatch_urgent_event(dev))
-			delay = 0;
-
-		linkwatch_schedule_work(delay);
-	}
+	linkwatch_schedule_work(urgent);
 }
 
 EXPORT_SYMBOL(linkwatch_fire_event);

commit db0ccffed91e234cad99a35f07d5a322f410baa2
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue May 8 23:22:43 2007 -0700

    [NET] link_watch: Eliminate potential delay on wrap-around
    
    When the jiffies wrap around or when the system boots up for the first
    time, down events can be delayed indefinitely since we no longer
    update linkwatch_nextevent when only urgent events are processed.
    
    This patch fixes this by setting linkwatch_nextevent when a
    wrap-around occurs.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index b5f45799c2f3..4674ae574128 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -101,8 +101,10 @@ static void linkwatch_schedule_work(unsigned long delay)
 		return;
 
 	/* If we wrap around we'll delay it by at most HZ. */
-	if (delay > HZ)
+	if (delay > HZ) {
+		linkwatch_nextevent = jiffies;
 		delay = 0;
+	}
 
 	schedule_delayed_work(&linkwatch_work, delay);
 }

commit 294cc44b7e48a6e7732499eebcf409b231460d8e
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue May 8 18:36:28 2007 -0700

    [NET]: Remove link_watch delay for up even when we're down
    
    Currently all link carrier events are delayed by up to a second
    before they're processed to prevent link storms.  This causes
    unnecessary packet loss during that interval.
    
    In fact, we can achieve the same effect in preventing storms by
    only delaying down events and unnecssary up events.  The latter
    is defined as up events when we're already up.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 71a35da275d4..b5f45799c2f3 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -77,11 +77,52 @@ static void rfc2863_policy(struct net_device *dev)
 }
 
 
-/* Must be called with the rtnl semaphore held */
-void linkwatch_run_queue(void)
+static int linkwatch_urgent_event(struct net_device *dev)
+{
+	return netif_running(dev) && netif_carrier_ok(dev) &&
+	       dev->qdisc != dev->qdisc_sleeping;
+}
+
+
+static void linkwatch_add_event(struct net_device *dev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&lweventlist_lock, flags);
+	dev->link_watch_next = lweventlist;
+	lweventlist = dev;
+	spin_unlock_irqrestore(&lweventlist_lock, flags);
+}
+
+
+static void linkwatch_schedule_work(unsigned long delay)
+{
+	if (test_and_set_bit(LW_RUNNING, &linkwatch_flags))
+		return;
+
+	/* If we wrap around we'll delay it by at most HZ. */
+	if (delay > HZ)
+		delay = 0;
+
+	schedule_delayed_work(&linkwatch_work, delay);
+}
+
+
+static void __linkwatch_run_queue(int urgent_only)
 {
 	struct net_device *next;
 
+	/*
+	 * Limit the number of linkwatch events to one
+	 * per second so that a runaway driver does not
+	 * cause a storm of messages on the netlink
+	 * socket.  This limit does not apply to up events
+	 * while the device qdisc is down.
+	 */
+	if (!urgent_only)
+		linkwatch_nextevent = jiffies + HZ;
+	clear_bit(LW_RUNNING, &linkwatch_flags);
+
 	spin_lock_irq(&lweventlist_lock);
 	next = lweventlist;
 	lweventlist = NULL;
@@ -92,6 +133,11 @@ void linkwatch_run_queue(void)
 
 		next = dev->link_watch_next;
 
+		if (urgent_only && !linkwatch_urgent_event(dev)) {
+			linkwatch_add_event(dev);
+			continue;
+		}
+
 		/*
 		 * Make sure the above read is complete since it can be
 		 * rewritten as soon as we clear the bit below.
@@ -116,21 +162,23 @@ void linkwatch_run_queue(void)
 
 		dev_put(dev);
 	}
+
+	if (lweventlist)
+		linkwatch_schedule_work(linkwatch_nextevent - jiffies);
 }
 
 
-static void linkwatch_event(struct work_struct *dummy)
+/* Must be called with the rtnl semaphore held */
+void linkwatch_run_queue(void)
 {
-	/* Limit the number of linkwatch events to one
-	 * per second so that a runaway driver does not
-	 * cause a storm of messages on the netlink
-	 * socket
-	 */
-	linkwatch_nextevent = jiffies + HZ;
-	clear_bit(LW_RUNNING, &linkwatch_flags);
+	__linkwatch_run_queue(0);
+}
+
 
+static void linkwatch_event(struct work_struct *dummy)
+{
 	rtnl_lock();
-	linkwatch_run_queue();
+	__linkwatch_run_queue(time_after(linkwatch_nextevent, jiffies));
 	rtnl_unlock();
 }
 
@@ -138,23 +186,19 @@ static void linkwatch_event(struct work_struct *dummy)
 void linkwatch_fire_event(struct net_device *dev)
 {
 	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
-		unsigned long flags;
+		unsigned long delay;
 
 		dev_hold(dev);
 
-		spin_lock_irqsave(&lweventlist_lock, flags);
-		dev->link_watch_next = lweventlist;
-		lweventlist = dev;
-		spin_unlock_irqrestore(&lweventlist_lock, flags);
+		linkwatch_add_event(dev);
 
-		if (!test_and_set_bit(LW_RUNNING, &linkwatch_flags)) {
-			unsigned long delay = linkwatch_nextevent - jiffies;
+		delay = linkwatch_nextevent - jiffies;
 
-			/* If we wrap around we'll delay it by at most HZ. */
-			if (delay > HZ)
-				delay = 0;
-			schedule_delayed_work(&linkwatch_work, delay);
-		}
+		/* Minimise down-time: drop delay for up event. */
+		if (linkwatch_urgent_event(dev))
+			delay = 0;
+
+		linkwatch_schedule_work(delay);
 	}
 }
 

commit 572a103ded0ad880f75ce83e99f0512fbb80b5b0
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue May 8 18:34:17 2007 -0700

    [NET] link_watch: Move link watch list into net_device
    
    These days the link watch mechanism is an integral part of the
    network subsystem as it manages the carrier status.  So it now
    makes sense to allocate some memory for it in net_device rather
    than allocating it on demand.
    
    In fact, this is necessary because we can't tolerate a memory
    allocation failure since that means we'd have to potentially
    throw a link up event away.
    
    It also simplifies the code greatly.
    
    In doing so I discovered a subtle race condition in the use
    of singleevent.  This race condition still exists (and is
    somewhat magnified) without singleevent but it's now plugged
    thanks to an smp_mb__before_clear_bit.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index e3c26a9ccad6..71a35da275d4 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -19,7 +19,6 @@
 #include <linux/rtnetlink.h>
 #include <linux/jiffies.h>
 #include <linux/spinlock.h>
-#include <linux/list.h>
 #include <linux/slab.h>
 #include <linux/workqueue.h>
 #include <linux/bitops.h>
@@ -28,7 +27,6 @@
 
 enum lw_bits {
 	LW_RUNNING = 0,
-	LW_SE_USED
 };
 
 static unsigned long linkwatch_flags;
@@ -37,17 +35,9 @@ static unsigned long linkwatch_nextevent;
 static void linkwatch_event(struct work_struct *dummy);
 static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event);
 
-static LIST_HEAD(lweventlist);
+static struct net_device *lweventlist;
 static DEFINE_SPINLOCK(lweventlist_lock);
 
-struct lw_event {
-	struct list_head list;
-	struct net_device *dev;
-};
-
-/* Avoid kmalloc() for most systems */
-static struct lw_event singleevent;
-
 static unsigned char default_operstate(const struct net_device *dev)
 {
 	if (!netif_carrier_ok(dev))
@@ -90,21 +80,23 @@ static void rfc2863_policy(struct net_device *dev)
 /* Must be called with the rtnl semaphore held */
 void linkwatch_run_queue(void)
 {
-	struct list_head head, *n, *next;
+	struct net_device *next;
 
 	spin_lock_irq(&lweventlist_lock);
-	list_replace_init(&lweventlist, &head);
+	next = lweventlist;
+	lweventlist = NULL;
 	spin_unlock_irq(&lweventlist_lock);
 
-	list_for_each_safe(n, next, &head) {
-		struct lw_event *event = list_entry(n, struct lw_event, list);
-		struct net_device *dev = event->dev;
+	while (next) {
+		struct net_device *dev = next;
 
-		if (event == &singleevent) {
-			clear_bit(LW_SE_USED, &linkwatch_flags);
-		} else {
-			kfree(event);
-		}
+		next = dev->link_watch_next;
+
+		/*
+		 * Make sure the above read is complete since it can be
+		 * rewritten as soon as we clear the bit below.
+		 */
+		smp_mb__before_clear_bit();
 
 		/* We are about to handle this device,
 		 * so new events can be accepted
@@ -147,24 +139,12 @@ void linkwatch_fire_event(struct net_device *dev)
 {
 	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
 		unsigned long flags;
-		struct lw_event *event;
-
-		if (test_and_set_bit(LW_SE_USED, &linkwatch_flags)) {
-			event = kmalloc(sizeof(struct lw_event), GFP_ATOMIC);
-
-			if (unlikely(event == NULL)) {
-				clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
-				return;
-			}
-		} else {
-			event = &singleevent;
-		}
 
 		dev_hold(dev);
-		event->dev = dev;
 
 		spin_lock_irqsave(&lweventlist_lock, flags);
-		list_add_tail(&event->list, &lweventlist);
+		dev->link_watch_next = lweventlist;
+		lweventlist = dev;
 		spin_unlock_irqrestore(&lweventlist_lock, flags);
 
 		if (!test_and_set_bit(LW_RUNNING, &linkwatch_flags)) {

commit 3ff50b7997fe06cd5d276b229967bb52d6b3b6c1
Author: Stephen Hemminger <shemminger@linux-foundation.org>
Date:   Fri Apr 20 17:09:22 2007 -0700

    [NET]: cleanup extra semicolons
    
    Spring cleaning time...
    
    There seems to be a lot of places in the network code that have
    extra bogus semicolons after conditionals.  Most commonly is a
    bogus semicolon after: switch() { }
    
    Signed-off-by: Stephen Hemminger <shemminger@linux-foundation.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 8b45c9d3b249..e3c26a9ccad6 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -79,7 +79,7 @@ static void rfc2863_policy(struct net_device *dev)
 	case IF_LINK_MODE_DEFAULT:
 	default:
 		break;
-	};
+	}
 
 	dev->operstate = operstate;
 

commit 4ec93edb14fe5fdee9fae6335f2cbba204627eac
Author: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
Date:   Fri Feb 9 23:24:36 2007 +0900

    [NET] CORE: Fix whitespace errors.
    
    Signed-off-by: YOSHIFUJI Hideaki <yoshfuji@linux-ipv6.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 549a2ce951b0..8b45c9d3b249 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -124,7 +124,7 @@ void linkwatch_run_queue(void)
 
 		dev_put(dev);
 	}
-}       
+}
 
 
 static void linkwatch_event(struct work_struct *dummy)
@@ -133,7 +133,7 @@ static void linkwatch_event(struct work_struct *dummy)
 	 * per second so that a runaway driver does not
 	 * cause a storm of messages on the netlink
 	 * socket
-	 */	
+	 */
 	linkwatch_nextevent = jiffies + HZ;
 	clear_bit(LW_RUNNING, &linkwatch_flags);
 

commit 65f27f38446e1976cc98fd3004b110fedcddd189
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:55:48 2006 +0000

    WorkStruct: Pass the work_struct pointer instead of context data
    
    Pass the work_struct pointer to the work function rather than context data.
    The work function can use container_of() to work out the data.
    
    For the cases where the container of the work_struct may go away the moment the
    pending bit is cleared, it is made possible to defer the release of the
    structure by deferring the clearing of the pending bit.
    
    To make this work, an extra flag is introduced into the management side of the
    work_struct.  This governs auto-release of the structure upon execution.
    
    Ordinarily, the work queue executor would release the work_struct for further
    scheduling or deallocation by clearing the pending bit prior to jumping to the
    work function.  This means that, unless the driver makes some guarantee itself
    that the work_struct won't go away, the work function may not access anything
    else in the work_struct or its container lest they be deallocated..  This is a
    problem if the auxiliary data is taken away (as done by the last patch).
    
    However, if the pending bit is *not* cleared before jumping to the work
    function, then the work function *may* access the work_struct and its container
    with no problems.  But then the work function must itself release the
    work_struct by calling work_release().
    
    In most cases, automatic release is fine, so this is the default.  Special
    initiators exist for the non-auto-release case (ending in _NAR).
    
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index f2ed09e25dfd..549a2ce951b0 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -34,8 +34,8 @@ enum lw_bits {
 static unsigned long linkwatch_flags;
 static unsigned long linkwatch_nextevent;
 
-static void linkwatch_event(void *dummy);
-static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event, NULL);
+static void linkwatch_event(struct work_struct *dummy);
+static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event);
 
 static LIST_HEAD(lweventlist);
 static DEFINE_SPINLOCK(lweventlist_lock);
@@ -127,7 +127,7 @@ void linkwatch_run_queue(void)
 }       
 
 
-static void linkwatch_event(void *dummy)
+static void linkwatch_event(struct work_struct *dummy)
 {
 	/* Limit the number of linkwatch events to one
 	 * per second so that a runaway driver does not

commit 52bad64d95bd89e08c49ec5a071fa6dcbe5a1a9c
Author: David Howells <dhowells@redhat.com>
Date:   Wed Nov 22 14:54:01 2006 +0000

    WorkStruct: Separate delayable and non-delayable events.
    
    Separate delayable work items from non-delayable work items be splitting them
    into a separate structure (delayed_work), which incorporates a work_struct and
    the timer_list removed from work_struct.
    
    The work_struct struct is huge, and this limits it's usefulness.  On a 64-bit
    architecture it's nearly 100 bytes in size.  This reduces that by half for the
    non-delayable type of event.
    
    Signed-Off-By: David Howells <dhowells@redhat.com>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 4b36114744c5..f2ed09e25dfd 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -35,7 +35,7 @@ static unsigned long linkwatch_flags;
 static unsigned long linkwatch_nextevent;
 
 static void linkwatch_event(void *dummy);
-static DECLARE_WORK(linkwatch_work, linkwatch_event, NULL);
+static DECLARE_DELAYED_WORK(linkwatch_work, linkwatch_event, NULL);
 
 static LIST_HEAD(lweventlist);
 static DEFINE_SPINLOCK(lweventlist_lock);
@@ -171,10 +171,9 @@ void linkwatch_fire_event(struct net_device *dev)
 			unsigned long delay = linkwatch_nextevent - jiffies;
 
 			/* If we wrap around we'll delay it by at most HZ. */
-			if (!delay || delay > HZ)
-				schedule_work(&linkwatch_work);
-			else
-				schedule_delayed_work(&linkwatch_work, delay);
+			if (delay > HZ)
+				delay = 0;
+			schedule_delayed_work(&linkwatch_work, delay);
 		}
 	}
 }

commit 6ab3d5624e172c553004ecc862bfeac16d9d68b7
Author: Jörn Engel <joern@wohnheim.fh-wedel.de>
Date:   Fri Jun 30 19:25:36 2006 +0200

    Remove obsolete #include <linux/config.h>
    
    Signed-off-by: Jörn Engel <joern@wohnheim.fh-wedel.de>
    Signed-off-by: Adrian Bunk <bunk@stusta.de>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 0f37266411b5..4b36114744c5 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -11,7 +11,6 @@
  *
  */
 
-#include <linux/config.h>
 #include <linux/module.h>
 #include <linux/netdevice.h>
 #include <linux/if.h>

commit 626ab0e69d376fa07599af669af8ba92d58e87c1
Author: Oleg Nesterov <oleg@tv-sign.ru>
Date:   Fri Jun 23 02:05:55 2006 -0700

    [PATCH] list: use list_replace_init() instead of list_splice_init()
    
    list_splice_init(list, head) does unneeded job if it is known that
    list_empty(head) == 1.  We can use list_replace_init() instead.
    
    Signed-off-by: Oleg Nesterov <oleg@tv-sign.ru>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 646937cc2d84..0f37266411b5 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -91,11 +91,10 @@ static void rfc2863_policy(struct net_device *dev)
 /* Must be called with the rtnl semaphore held */
 void linkwatch_run_queue(void)
 {
-	LIST_HEAD(head);
-	struct list_head *n, *next;
+	struct list_head head, *n, *next;
 
 	spin_lock_irq(&lweventlist_lock);
-	list_splice_init(&lweventlist, &head);
+	list_replace_init(&lweventlist, &head);
 	spin_unlock_irq(&lweventlist_lock);
 
 	list_for_each_safe(n, next, &head) {

commit 8c1056839e808aad728db86d739ffec71d2d1db8
Author: Herbert Xu <herbert@gondor.apana.org.au>
Date:   Tue May 9 15:27:54 2006 -0700

    [NET] linkwatch: Handle jiffies wrap-around
    
    The test used in the linkwatch does not handle wrap-arounds correctly.
    Since the intention of the code is to eliminate bursts of messages we
    can afford to delay things up to a second.  Using that fact we can
    easily handle wrap-arounds by making sure that we don't delay things
    by more than one second.
    
    This is based on diagnosis and a patch by Stefan Rompf.
    
    Signed-off-by: Herbert Xu <herbert@gondor.apana.org.au>
    Acked-by: Stefan Rompf <stefan@loplof.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 341de44c7ed1..646937cc2d84 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -170,13 +170,13 @@ void linkwatch_fire_event(struct net_device *dev)
 		spin_unlock_irqrestore(&lweventlist_lock, flags);
 
 		if (!test_and_set_bit(LW_RUNNING, &linkwatch_flags)) {
-			unsigned long thisevent = jiffies;
+			unsigned long delay = linkwatch_nextevent - jiffies;
 
-			if (thisevent >= linkwatch_nextevent) {
+			/* If we wrap around we'll delay it by at most HZ. */
+			if (!delay || delay > HZ)
 				schedule_work(&linkwatch_work);
-			} else {
-				schedule_delayed_work(&linkwatch_work, linkwatch_nextevent - thisevent);
-			}
+			else
+				schedule_delayed_work(&linkwatch_work, delay);
 		}
 	}
 }

commit 6756ae4b4e97aba48c042b4aa6b77a18f507d2cb
Author: Stephen Hemminger <shemminger@osdl.org>
Date:   Mon Mar 20 22:23:58 2006 -0800

    [NET]: Convert RTNL to mutex.
    
    This patch turns the RTNL from a semaphore to a new 2.6.16 mutex and
    gets rid of some of the leftover legacy.
    
    Signed-off-by: Stephen Hemminger <shemminger@osdl.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index e82a451d330b..341de44c7ed1 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -139,9 +139,9 @@ static void linkwatch_event(void *dummy)
 	linkwatch_nextevent = jiffies + HZ;
 	clear_bit(LW_RUNNING, &linkwatch_flags);
 
-	rtnl_shlock();
+	rtnl_lock();
 	linkwatch_run_queue();
-	rtnl_shunlock();
+	rtnl_unlock();
 }
 
 

commit b00055aacdb172c05067612278ba27265fcd05ce
Author: Stefan Rompf <stefan@loplof.de>
Date:   Mon Mar 20 17:09:11 2006 -0800

    [NET] core: add RFC2863 operstate
    
    this patch adds a dormant flag to network devices, RFC2863 operstate derived
    from these flags and possibility for userspace interaction. It allows drivers
    to signal that a device is unusable for user traffic without disabling
    queueing (and therefore the possibility for protocol establishment traffic to
    flow) and a userspace supplicant (WPA, 802.1X) to mark a device unusable
    without changes to the driver.
    
    It is the result of our long discussion. However I must admit that it
    represents what Jamal and I agreed on with compromises towards Krzysztof, but
    Thomas and Krzysztof still disagree with some parts. Anyway I think it should
    be applied.
    
    Signed-off-by: Stefan Rompf <stefan@loplof.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index d43d1201275c..e82a451d330b 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -49,6 +49,45 @@ struct lw_event {
 /* Avoid kmalloc() for most systems */
 static struct lw_event singleevent;
 
+static unsigned char default_operstate(const struct net_device *dev)
+{
+	if (!netif_carrier_ok(dev))
+		return (dev->ifindex != dev->iflink ?
+			IF_OPER_LOWERLAYERDOWN : IF_OPER_DOWN);
+
+	if (netif_dormant(dev))
+		return IF_OPER_DORMANT;
+
+	return IF_OPER_UP;
+}
+
+
+static void rfc2863_policy(struct net_device *dev)
+{
+	unsigned char operstate = default_operstate(dev);
+
+	if (operstate == dev->operstate)
+		return;
+
+	write_lock_bh(&dev_base_lock);
+
+	switch(dev->link_mode) {
+	case IF_LINK_MODE_DORMANT:
+		if (operstate == IF_OPER_UP)
+			operstate = IF_OPER_DORMANT;
+		break;
+
+	case IF_LINK_MODE_DEFAULT:
+	default:
+		break;
+	};
+
+	dev->operstate = operstate;
+
+	write_unlock_bh(&dev_base_lock);
+}
+
+
 /* Must be called with the rtnl semaphore held */
 void linkwatch_run_queue(void)
 {
@@ -74,6 +113,7 @@ void linkwatch_run_queue(void)
 		 */
 		clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
 
+		rfc2863_policy(dev);
 		if (dev->flags & IFF_UP) {
 			if (netif_carrier_ok(dev)) {
 				WARN_ON(dev->qdisc_sleeping == &noop_qdisc);

commit cacaddf57ed4d5ca994e9a7e2bd5558061f5d89d
Author: Tommy S. Christensen <tommy.christensen@tpack.net>
Date:   Tue May 3 16:18:52 2005 -0700

    [NET]: Disable queueing when carrier is lost.
    
    Some network drivers call netif_stop_queue() when detecting loss of
    carrier. This leads to packets being queued up at the qdisc level for
    an unbound period of time. In order to prevent this effect, the core
    networking stack will now cease to queue packets for any device, that
    is operationally down (i.e. the queue is flushed and disabled).
    
    Signed-off-by: Tommy S. Christensen <tommy.christensen@tpack.net>
    Acked-by: Herbert Xu <herbert@gondor.apana.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
index 4859b7446c6f..d43d1201275c 100644
--- a/net/core/link_watch.c
+++ b/net/core/link_watch.c
@@ -16,6 +16,7 @@
 #include <linux/netdevice.h>
 #include <linux/if.h>
 #include <net/sock.h>
+#include <net/pkt_sched.h>
 #include <linux/rtnetlink.h>
 #include <linux/jiffies.h>
 #include <linux/spinlock.h>
@@ -74,6 +75,12 @@ void linkwatch_run_queue(void)
 		clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
 
 		if (dev->flags & IFF_UP) {
+			if (netif_carrier_ok(dev)) {
+				WARN_ON(dev->qdisc_sleeping == &noop_qdisc);
+				dev_activate(dev);
+			} else
+				dev_deactivate(dev);
+
 			netdev_state_change(dev);
 		}
 

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/net/core/link_watch.c b/net/core/link_watch.c
new file mode 100644
index 000000000000..4859b7446c6f
--- /dev/null
+++ b/net/core/link_watch.c
@@ -0,0 +1,137 @@
+/*
+ * Linux network device link state notification
+ *
+ * Author:
+ *     Stefan Rompf <sux@loplof.de>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ *
+ */
+
+#include <linux/config.h>
+#include <linux/module.h>
+#include <linux/netdevice.h>
+#include <linux/if.h>
+#include <net/sock.h>
+#include <linux/rtnetlink.h>
+#include <linux/jiffies.h>
+#include <linux/spinlock.h>
+#include <linux/list.h>
+#include <linux/slab.h>
+#include <linux/workqueue.h>
+#include <linux/bitops.h>
+#include <asm/types.h>
+
+
+enum lw_bits {
+	LW_RUNNING = 0,
+	LW_SE_USED
+};
+
+static unsigned long linkwatch_flags;
+static unsigned long linkwatch_nextevent;
+
+static void linkwatch_event(void *dummy);
+static DECLARE_WORK(linkwatch_work, linkwatch_event, NULL);
+
+static LIST_HEAD(lweventlist);
+static DEFINE_SPINLOCK(lweventlist_lock);
+
+struct lw_event {
+	struct list_head list;
+	struct net_device *dev;
+};
+
+/* Avoid kmalloc() for most systems */
+static struct lw_event singleevent;
+
+/* Must be called with the rtnl semaphore held */
+void linkwatch_run_queue(void)
+{
+	LIST_HEAD(head);
+	struct list_head *n, *next;
+
+	spin_lock_irq(&lweventlist_lock);
+	list_splice_init(&lweventlist, &head);
+	spin_unlock_irq(&lweventlist_lock);
+
+	list_for_each_safe(n, next, &head) {
+		struct lw_event *event = list_entry(n, struct lw_event, list);
+		struct net_device *dev = event->dev;
+
+		if (event == &singleevent) {
+			clear_bit(LW_SE_USED, &linkwatch_flags);
+		} else {
+			kfree(event);
+		}
+
+		/* We are about to handle this device,
+		 * so new events can be accepted
+		 */
+		clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
+
+		if (dev->flags & IFF_UP) {
+			netdev_state_change(dev);
+		}
+
+		dev_put(dev);
+	}
+}       
+
+
+static void linkwatch_event(void *dummy)
+{
+	/* Limit the number of linkwatch events to one
+	 * per second so that a runaway driver does not
+	 * cause a storm of messages on the netlink
+	 * socket
+	 */	
+	linkwatch_nextevent = jiffies + HZ;
+	clear_bit(LW_RUNNING, &linkwatch_flags);
+
+	rtnl_shlock();
+	linkwatch_run_queue();
+	rtnl_shunlock();
+}
+
+
+void linkwatch_fire_event(struct net_device *dev)
+{
+	if (!test_and_set_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state)) {
+		unsigned long flags;
+		struct lw_event *event;
+
+		if (test_and_set_bit(LW_SE_USED, &linkwatch_flags)) {
+			event = kmalloc(sizeof(struct lw_event), GFP_ATOMIC);
+
+			if (unlikely(event == NULL)) {
+				clear_bit(__LINK_STATE_LINKWATCH_PENDING, &dev->state);
+				return;
+			}
+		} else {
+			event = &singleevent;
+		}
+
+		dev_hold(dev);
+		event->dev = dev;
+
+		spin_lock_irqsave(&lweventlist_lock, flags);
+		list_add_tail(&event->list, &lweventlist);
+		spin_unlock_irqrestore(&lweventlist_lock, flags);
+
+		if (!test_and_set_bit(LW_RUNNING, &linkwatch_flags)) {
+			unsigned long thisevent = jiffies;
+
+			if (thisevent >= linkwatch_nextevent) {
+				schedule_work(&linkwatch_work);
+			} else {
+				schedule_delayed_work(&linkwatch_work, linkwatch_nextevent - thisevent);
+			}
+		}
+	}
+}
+
+EXPORT_SYMBOL(linkwatch_fire_event);
