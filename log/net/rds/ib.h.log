commit 33cf601da7a42b1a78d04746aea47b85ccb5c49b
Author: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
Date:   Mon Jun 15 00:40:25 2020 -0700

    net/rds: NULL pointer de-reference in rds_ib_add_one()
    
    The parent field of a struct device may be NULL.  The macro
    ibdev_to_node() should check for that.
    
    Signed-off-by: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 5ae069d39eab..8dfff43cf07f 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -264,7 +264,13 @@ struct rds_ib_device {
 	int			*vector_load;
 };
 
-#define ibdev_to_node(ibdev) dev_to_node((ibdev)->dev.parent)
+static inline int ibdev_to_node(struct ib_device *ibdev)
+{
+	struct device *parent;
+
+	parent = ibdev->dev.parent;
+	return parent ? dev_to_node(parent) : NUMA_NO_NODE;
+}
 #define rdsibdev_to_node(rdsibdev) ibdev_to_node(rdsibdev->dev)
 
 /* bits for i_ack_flags */

commit 07549ee21ce5247143ffb069bf838025d86b908c
Author: Max Gurtovoy <maxg@mellanox.com>
Date:   Thu May 28 16:45:45 2020 -0300

    RDMA/rds: Remove FMR support for memory registration
    
    Use FRWR method for memory registration by default and remove the ancient
    and unsafe FMR method.
    
    Link: https://lore.kernel.org/r/3-v3-f58e6669d5d3+2cf-fmr_removal_jgg@mellanox.com
    Signed-off-by: Max Gurtovoy <maxg@mellanox.com>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 0296f1f7acda..5ae069d39eab 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -247,13 +247,11 @@ struct rds_ib_device {
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
 	struct dma_pool		*rid_hdrs_pool; /* RDS headers DMA pool */
-	u8			use_fastreg:1;
 	u8			odp_capable:1;
 
 	unsigned int		max_mrs;
 	struct rds_ib_mr_pool	*mr_1m_pool;
 	struct rds_ib_mr_pool   *mr_8k_pool;
-	unsigned int		fmr_max_remaps;
 	unsigned int		max_8k_mrs;
 	unsigned int		max_1m_mrs;
 	int			max_sge;

commit 2eafa1746f17872483d1033b0116ec71435ea19d
Author: Hans Westgaard Ry <hans.westgaard.ry@oracle.com>
Date:   Wed Jan 15 14:43:39 2020 +0200

    net/rds: Handle ODP mr registration/unregistration
    
    On-Demand-Paging MRs are registered using ib_reg_user_mr and
    unregistered with ib_dereg_mr.
    
    Signed-off-by: Hans Westgaard Ry <hans.westgaard.ry@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 6e6f24753998..0296f1f7acda 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -247,7 +247,8 @@ struct rds_ib_device {
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
 	struct dma_pool		*rid_hdrs_pool; /* RDS headers DMA pool */
-	bool                    use_fastreg;
+	u8			use_fastreg:1;
+	u8			odp_capable:1;
 
 	unsigned int		max_mrs;
 	struct rds_ib_mr_pool	*mr_1m_pool;

commit 9b17f5884be4484e4d9090a9dccf17e763e0589b
Author: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
Date:   Wed Oct 2 21:11:08 2019 -0700

    net/rds: Use DMA memory pool allocation for rds_header
    
    Currently, RDS calls ib_dma_alloc_coherent() to allocate a large piece
    of contiguous DMA coherent memory to store struct rds_header for
    sending/receiving packets.  The memory allocated is then partitioned
    into struct rds_header.  This is not necessary and can be costly at
    times when memory is fragmented.  Instead, RDS should use the DMA
    memory pool interface to handle this.  The DMA addresses of the pre-
    allocated headers are stored in an array.  At send/receive ring
    initialization and refill time, this arrary is de-referenced to get
    the DMA addresses.  This array is not accessed at send/receive packet
    processing.
    
    Suggested-by: HÃ¥kon Bugge <haakon.bugge@oracle.com>
    Signed-off-by: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index f2b558e8b5ea..6e6f24753998 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -165,8 +165,8 @@ struct rds_ib_connection {
 	/* tx */
 	struct rds_ib_work_ring	i_send_ring;
 	struct rm_data_op	*i_data_op;
-	struct rds_header	*i_send_hdrs;
-	dma_addr_t		i_send_hdrs_dma;
+	struct rds_header	**i_send_hdrs;
+	dma_addr_t		*i_send_hdrs_dma;
 	struct rds_ib_send_work *i_sends;
 	atomic_t		i_signaled_sends;
 
@@ -175,8 +175,8 @@ struct rds_ib_connection {
 	struct rds_ib_work_ring	i_recv_ring;
 	struct rds_ib_incoming	*i_ibinc;
 	u32			i_recv_data_rem;
-	struct rds_header	*i_recv_hdrs;
-	dma_addr_t		i_recv_hdrs_dma;
+	struct rds_header	**i_recv_hdrs;
+	dma_addr_t		*i_recv_hdrs_dma;
 	struct rds_ib_recv_work *i_recvs;
 	u64			i_ack_recv;	/* last ACK received */
 	struct rds_ib_refill_cache i_cache_incs;
@@ -246,6 +246,7 @@ struct rds_ib_device {
 	struct list_head	conn_list;
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
+	struct dma_pool		*rid_hdrs_pool; /* RDS headers DMA pool */
 	bool                    use_fastreg;
 
 	unsigned int		max_mrs;
@@ -381,7 +382,11 @@ int rds_ib_cm_handle_connect(struct rdma_cm_id *cm_id,
 int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id, bool isv6);
 void rds_ib_cm_connect_complete(struct rds_connection *conn,
 				struct rdma_cm_event *event);
-
+struct rds_header **rds_dma_hdrs_alloc(struct ib_device *ibdev,
+				       struct dma_pool *pool,
+				       dma_addr_t **dma_addrs, u32 num_hdrs);
+void rds_dma_hdrs_free(struct dma_pool *pool, struct rds_header **hdrs,
+		       dma_addr_t *dma_addrs, u32 num_hdrs);
 
 #define rds_ib_conn_error(conn, fmt...) \
 	__rds_ib_conn_error(conn, KERN_WARNING "RDS/IB: " fmt)

commit e0e6d062822529dbe9be21939359b0d1e065bb0f
Author: Zhu Yanjun <yanjun.zhu@oracle.com>
Date:   Fri Aug 23 21:04:16 2019 -0400

    net: rds: add service level support in rds-info
    
    >From IB specific 7.6.5 SERVICE LEVEL, Service Level (SL)
    is used to identify different flows within an IBA subnet.
    It is carried in the local route header of the packet.
    
    Before this commit, run "rds-info -I". The outputs are as
    below:
    "
    RDS IB Connections:
     LocalAddr  RemoteAddr Tos SL  LocalDev               RemoteDev
    192.2.95.3  192.2.95.1  2   0  fe80::21:28:1a:39  fe80::21:28:10:b9
    192.2.95.3  192.2.95.1  1   0  fe80::21:28:1a:39  fe80::21:28:10:b9
    192.2.95.3  192.2.95.1  0   0  fe80::21:28:1a:39  fe80::21:28:10:b9
    "
    After this commit, the output is as below:
    "
    RDS IB Connections:
     LocalAddr  RemoteAddr Tos SL  LocalDev               RemoteDev
    192.2.95.3  192.2.95.1  2   2  fe80::21:28:1a:39  fe80::21:28:10:b9
    192.2.95.3  192.2.95.1  1   1  fe80::21:28:1a:39  fe80::21:28:10:b9
    192.2.95.3  192.2.95.1  0   0  fe80::21:28:1a:39  fe80::21:28:10:b9
    "
    
    The commit fe3475af3bdf ("net: rds: add per rds connection cache
    statistics") adds cache_allocs in struct rds_info_rdma_connection
    as below:
    struct rds_info_rdma_connection {
    ...
            __u32           rdma_mr_max;
            __u32           rdma_mr_size;
            __u8            tos;
            __u32           cache_allocs;
     };
    The peer struct in rds-tools of struct rds_info_rdma_connection is as
    below:
    struct rds_info_rdma_connection {
    ...
            uint32_t        rdma_mr_max;
            uint32_t        rdma_mr_size;
            uint8_t         tos;
            uint8_t         sl;
            uint32_t        cache_allocs;
    };
    The difference between userspace and kernel is the member variable sl.
    In the kernel struct, the member variable sl is missing. This will
    introduce risks. So it is necessary to use this commit to avoid this risk.
    
    Fixes: fe3475af3bdf ("net: rds: add per rds connection cache statistics")
    CC: Joe Jin <joe.jin@oracle.com>
    CC: JUNXIAO_BI <junxiao.bi@oracle.com>
    Suggested-by: Gerd Rausch <gerd.rausch@oracle.com>
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 303c6ee8bdb7..f2b558e8b5ea 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -220,6 +220,7 @@ struct rds_ib_connection {
 	/* Send/Recv vectors */
 	int			i_scq_vector;
 	int			i_rcq_vector;
+	u8			i_sl;
 };
 
 /* This assumes that atomic_t is at least 32 bits */

commit 3a2886cca703fde5ee21baea9fedf8b1389c59d7
Author: Gerd Rausch <gerd.rausch@oracle.com>
Date:   Tue Jul 16 15:29:17 2019 -0700

    net/rds: Keep track of and wait for FRWR segments in use upon shutdown
    
    Since "rds_ib_free_frmr" and "rds_ib_free_frmr_list" simply put
    the FRMR memory segments on the "drop_list" or "free_list",
    and it is the job of "rds_ib_flush_mr_pool" to reap those entries
    by ultimately issuing a "IB_WR_LOCAL_INV" work-request,
    we need to trigger and then wait for all those memory segments
    attached to a particular connection to be fully released before
    we can move on to release the QP, CQ, etc.
    
    So we make "rds_ib_conn_path_shutdown" wait for one more
    atomic_t called "i_fastreg_inuse_count" that keeps track of how
    many FRWR memory segments are out there marked "FRMR_IS_INUSE"
    (and also wake_up rds_ib_ring_empty_wait, as they go away).
    
    Signed-off-by: Gerd Rausch <gerd.rausch@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 66c03c7665b2..303c6ee8bdb7 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -156,6 +156,7 @@ struct rds_ib_connection {
 
 	/* To control the number of wrs from fastreg */
 	atomic_t		i_fastreg_wrs;
+	atomic_t		i_fastreg_inuse_count;
 
 	/* interrupt handling */
 	struct tasklet_struct	i_send_tasklet;

commit a55207884708bf1d5b8c87a3c504502de77a5416
Author: Gerd Rausch <gerd.rausch@oracle.com>
Date:   Fri Jun 28 17:31:19 2019 -0700

    Revert "RDS: IB: split the mr registration and invalidation path"
    
    This reverts commit 56012459310a1dbcc55c2dbf5500a9f7571402cb.
    
    RDS kept spinning inside function "rds_ib_post_reg_frmr", waiting for
    "i_fastreg_wrs" to become incremented:
             while (atomic_dec_return(&ibmr->ic->i_fastreg_wrs) <= 0) {
                     atomic_inc(&ibmr->ic->i_fastreg_wrs);
                     cpu_relax();
             }
    
    Looking at the original commit:
    
    commit 56012459310a ("RDS: IB: split the mr registration and
    invalidation path")
    
    In there, the "rds_ib_mr_cqe_handler" was changed in the following
    way:
    
     void rds_ib_mr_cqe_handler(struct
     rds_ib_connection *ic,
     struct ib_wc *wc)
            if (frmr->fr_inv) {
                      frmr->fr_state = FRMR_IS_FREE;
                      frmr->fr_inv = false;
                    atomic_inc(&ic->i_fastreg_wrs);
            } else {
                    atomic_inc(&ic->i_fastunreg_wrs);
            }
    
    It looks like it's got it exactly backwards:
    
    Function "rds_ib_post_reg_frmr" keeps track of the outstanding
    requests via "i_fastreg_wrs".
    
    Function "rds_ib_post_inv" keeps track of the outstanding requests
    via "i_fastunreg_wrs" (post original commit). It also sets:
             frmr->fr_inv = true;
    
    However the completion handler "rds_ib_mr_cqe_handler" adjusts
    "i_fastreg_wrs" when "fr_inv" had been true, and adjusts
    "i_fastunreg_wrs" otherwise.
    
    The original commit was done in the name of performance:
    to remove the performance bottleneck
    
    No performance benefit could be observed with a fixed-up version
    of the original commit measured between two Oracle X7 servers,
    both equipped with Mellanox Connect-X5 HCAs.
    
    The prudent course of action is to revert this commit.
    
    Signed-off-by: Gerd Rausch <gerd.rausch@oracle.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 67a715b076ca..66c03c7665b2 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -15,8 +15,7 @@
 
 #define RDS_IB_DEFAULT_RECV_WR		1024
 #define RDS_IB_DEFAULT_SEND_WR		256
-#define RDS_IB_DEFAULT_FR_WR		256
-#define RDS_IB_DEFAULT_FR_INV_WR	256
+#define RDS_IB_DEFAULT_FR_WR		512
 
 #define RDS_IB_DEFAULT_RETRY_COUNT	1
 
@@ -157,7 +156,6 @@ struct rds_ib_connection {
 
 	/* To control the number of wrs from fastreg */
 	atomic_t		i_fastreg_wrs;
-	atomic_t		i_fastunreg_wrs;
 
 	/* interrupt handling */
 	struct tasklet_struct	i_send_tasklet;

commit a50243b1ddcdd766d0d17fbfeeb1a22e62fdc461
Merge: 2901752c14b8 fca22e7e595f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 9 15:53:03 2019 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma
    
    Pull rdma updates from Jason Gunthorpe:
     "This has been a slightly more active cycle than normal with ongoing
      core changes and quite a lot of collected driver updates.
    
       - Various driver fixes for bnxt_re, cxgb4, hns, mlx5, pvrdma, rxe
    
       - A new data transfer mode for HFI1 giving higher performance
    
       - Significant functional and bug fix update to the mlx5
         On-Demand-Paging MR feature
    
       - A chip hang reset recovery system for hns
    
       - Change mm->pinned_vm to an atomic64
    
       - Update bnxt_re to support a new 57500 chip
    
       - A sane netlink 'rdma link add' method for creating rxe devices and
         fixing the various unregistration race conditions in rxe's
         unregister flow
    
       - Allow lookup up objects by an ID over netlink
    
       - Various reworking of the core to driver interface:
           - drivers should not assume umem SGLs are in PAGE_SIZE chunks
           - ucontext is accessed via udata not other means
           - start to make the core code responsible for object memory
             allocation
           - drivers should convert struct device to struct ib_device via a
             helper
           - drivers have more tools to avoid use after unregister problems"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/rdma/rdma: (280 commits)
      net/mlx5: ODP support for XRC transport is not enabled by default in FW
      IB/hfi1: Close race condition on user context disable and close
      RDMA/umem: Revert broken 'off by one' fix
      RDMA/umem: minor bug fix in error handling path
      RDMA/hns: Use GFP_ATOMIC in hns_roce_v2_modify_qp
      cxgb4: kfree mhp after the debug print
      IB/rdmavt: Fix concurrency panics in QP post_send and modify to error
      IB/rdmavt: Fix loopback send with invalidate ordering
      IB/iser: Fix dma_nents type definition
      IB/mlx5: Set correct write permissions for implicit ODP MR
      bnxt_re: Clean cq for kernel consumers only
      RDMA/uverbs: Don't do double free of allocated PD
      RDMA: Handle ucontext allocations by IB/core
      RDMA/core: Fix a WARN() message
      bnxt_re: fix the regression due to changes in alloc_pbl
      IB/mlx4: Increase the timeout for CM cache
      IB/core: Abort page fault handler silently during owning process exit
      IB/mlx5: Validate correct PD before prefetch MR
      IB/mlx5: Protect against prefetch of invalid MR
      RDMA/uverbs: Store PR pointer before it is overwritten
      ...

commit fd261ce6a30e01ad67c416e2c67e263024b3a6f9
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Sat Oct 13 22:13:23 2018 +0800

    rds: rdma: update rdma transport for tos
    
    For RDMA transports, RDS TOS is an extension of IB QoS(Annex A13)
    to provide clients the ability to segregate traffic flows for
    different type of data. RDMA CM abstract it for ULPs using
    rdma_set_service_type(). Internally, each traffic flow is
    represented by a connection with all of its independent resources
    like that of a normal connection, and is differentiated by
    service type. In other words, there can be multiple qp connections
    between an IP pair and each supports a unique service type.
    
    The feature has been added from RDSv4.1 onwards and supports
    rolling upgrades. RDMA connection metadata also carries the tos
    information to set up SL on end to end context. The original
    code was developed by Bang Nguyen in downstream kernel back in
    2.6.32 kernel days and it has evolved over period of time.
    
    Reviewed-by: Sowmini Varadhan <sowmini.varadhan@oracle.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    [yanjun.zhu@oracle.com: Adapted original patch with ipv6 changes]
    Signed-off-by: Zhu Yanjun <yanjun.zhu@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 71ff356ee702..752f92235a38 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -67,7 +67,9 @@ struct rds_ib_conn_priv_cmn {
 	u8			ricpc_protocol_major;
 	u8			ricpc_protocol_minor;
 	__be16			ricpc_protocol_minor_mask;	/* bitmask */
-	__be32			ricpc_reserved1;
+	u8			ricpc_dp_toss;
+	u8			ripc_reserved1;
+	__be16			ripc_reserved2;
 	__be64			ricpc_ack_seq;
 	__be32			ricpc_credit;	/* non-zero enables flow ctl */
 };

commit a163afc88556e099271a7b423295bc5176fcecce
Author: Bart Van Assche <bvanassche@acm.org>
Date:   Thu Jan 31 08:30:34 2019 -0800

    IB/core: Remove ib_sg_dma_address() and ib_sg_dma_len()
    
    Keeping single line wrapper functions is not useful. Hence remove the
    ib_sg_dma_address() and ib_sg_dma_len() functions. This patch does not
    change any functionality.
    
    Signed-off-by: Bart Van Assche <bvanassche@acm.org>
    Signed-off-by: Jason Gunthorpe <jgg@mellanox.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 71ff356ee702..1fd1cac85da2 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -331,10 +331,8 @@ static inline void rds_ib_dma_sync_sg_for_cpu(struct ib_device *dev,
 	unsigned int i;
 
 	for_each_sg(sglist, sg, sg_dma_len, i) {
-		ib_dma_sync_single_for_cpu(dev,
-				ib_sg_dma_address(dev, sg),
-				ib_sg_dma_len(dev, sg),
-				direction);
+		ib_dma_sync_single_for_cpu(dev, sg_dma_address(sg),
+					   sg_dma_len(sg), direction);
 	}
 }
 #define ib_dma_sync_sg_for_cpu	rds_ib_dma_sync_sg_for_cpu
@@ -348,10 +346,8 @@ static inline void rds_ib_dma_sync_sg_for_device(struct ib_device *dev,
 	unsigned int i;
 
 	for_each_sg(sglist, sg, sg_dma_len, i) {
-		ib_dma_sync_single_for_device(dev,
-				ib_sg_dma_address(dev, sg),
-				ib_sg_dma_len(dev, sg),
-				direction);
+		ib_dma_sync_single_for_device(dev, sg_dma_address(sg),
+					      sg_dma_len(sg), direction);
 	}
 }
 #define ib_dma_sync_sg_for_device	rds_ib_dma_sync_sg_for_device

commit 16fdf8ba98391650ce4bc4f3f71629d8a413bc21
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Sep 23 12:25:15 2018 -0700

    rds: Fix build regression.
    
    Use DECLARE_* not DEFINE_*
    
    Fixes: 8360ed6745df ("RDS: IB: Use DEFINE_PER_CPU_SHARED_ALIGNED for rds_ib_stats")
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index fd483760c910..71ff356ee702 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -443,7 +443,7 @@ int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,
 int rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op);
 
 /* ib_stats.c */
-DEFINE_PER_CPU_SHARED_ALIGNED(struct rds_ib_statistics, rds_ib_stats);
+DECLARE_PER_CPU_SHARED_ALIGNED(struct rds_ib_statistics, rds_ib_stats);
 #define rds_ib_stats_inc(member) rds_stats_inc_which(rds_ib_stats, member)
 #define rds_ib_stats_add(member, count) \
 		rds_stats_add_which(rds_ib_stats, member, count)

commit 8360ed6745df6de2d8ddff8e2116789258fe5890
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Fri Sep 21 11:04:51 2018 -0700

    RDS: IB: Use DEFINE_PER_CPU_SHARED_ALIGNED for rds_ib_stats
    
    Clang warns when two declarations' section attributes don't match.
    
    net/rds/ib_stats.c:40:1: warning: section does not match previous
    declaration [-Wsection]
    DEFINE_PER_CPU_SHARED_ALIGNED(struct rds_ib_statistics, rds_ib_stats);
    ^
    ./include/linux/percpu-defs.h:142:2: note: expanded from macro
    'DEFINE_PER_CPU_SHARED_ALIGNED'
            DEFINE_PER_CPU_SECTION(type, name,
    PER_CPU_SHARED_ALIGNED_SECTION) \
            ^
    ./include/linux/percpu-defs.h:93:9: note: expanded from macro
    'DEFINE_PER_CPU_SECTION'
            extern __PCPU_ATTRS(sec) __typeof__(type) name;
    \
                   ^
    ./include/linux/percpu-defs.h:49:26: note: expanded from macro
    '__PCPU_ATTRS'
            __percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))
    \
                                    ^
    net/rds/ib.h:446:1: note: previous attribute is here
    DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);
    ^
    ./include/linux/percpu-defs.h:111:2: note: expanded from macro
    'DECLARE_PER_CPU'
            DECLARE_PER_CPU_SECTION(type, name, "")
            ^
    ./include/linux/percpu-defs.h:87:9: note: expanded from macro
    'DECLARE_PER_CPU_SECTION'
            extern __PCPU_ATTRS(sec) __typeof__(type) name
                   ^
    ./include/linux/percpu-defs.h:49:26: note: expanded from macro
    '__PCPU_ATTRS'
            __percpu __attribute__((section(PER_CPU_BASE_SECTION sec)))
    \
                                    ^
    1 warning generated.
    
    The initial definition was added in commit ec16227e1414 ("RDS/IB:
    Infiniband transport") and the cache aligned definition was added in
    commit e6babe4cc4ce ("RDS/IB: Stats and sysctls") right after. The
    definition probably should have been updated in net/rds/ib.h, which is
    what this patch does.
    
    Link: https://github.com/ClangBuiltLinux/linux/issues/114
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 73427ff439f9..fd483760c910 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -443,7 +443,7 @@ int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,
 int rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op);
 
 /* ib_stats.c */
-DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);
+DEFINE_PER_CPU_SHARED_ALIGNED(struct rds_ib_statistics, rds_ib_stats);
 #define rds_ib_stats_inc(member) rds_stats_inc_which(rds_ib_stats, member)
 #define rds_ib_stats_add(member, count) \
 		rds_stats_add_which(rds_ib_stats, member, count)

commit f394ad28feffbeebab77c8bf9a203bd49b957c9a
Author: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
Date:   Mon Jul 30 22:48:41 2018 -0700

    rds: rds_ib_recv_alloc_cache() should call alloc_percpu_gfp() instead
    
    Currently, rds_ib_conn_alloc() calls rds_ib_recv_alloc_caches()
    without passing along the gfp_t flag.  But rds_ib_recv_alloc_caches()
    and rds_ib_recv_alloc_cache() should take a gfp_t parameter so that
    rds_ib_recv_alloc_cache() can call alloc_percpu_gfp() using the
    correct flag instead of calling alloc_percpu().
    
    Signed-off-by: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index beb95b893f78..73427ff439f9 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -400,7 +400,7 @@ void rds_ib_mr_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc);
 int rds_ib_recv_init(void);
 void rds_ib_recv_exit(void);
 int rds_ib_recv_path(struct rds_conn_path *conn);
-int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic);
+int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic, gfp_t gfp);
 void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_refill(struct rds_connection *conn, int prefill, gfp_t gfp);
 void rds_ib_inc_free(struct rds_incoming *inc);

commit eee2fa6ab3225192d6d894c54a6fb02ac9efdff6
Author: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
Date:   Mon Jul 23 20:51:21 2018 -0700

    rds: Changing IP address internal representation to struct in6_addr
    
    This patch changes the internal representation of an IP address to use
    struct in6_addr.  IPv4 address is stored as an IPv4 mapped address.
    All the functions which take an IP address as argument are also
    changed to use struct in6_addr.  But RDS socket layer is not modified
    such that it still does not accept IPv6 address from an application.
    And RDS layer does not accept nor initiate IPv6 connections.
    
    v2: Fixed sparse warnings.
    
    Signed-off-by: Ka-Cheong Poon <ka-cheong.poon@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index a6f4d7d68e95..beb95b893f78 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -57,16 +57,44 @@ struct rds_ib_refill_cache {
 	struct list_head	 *ready;
 };
 
+/* This is the common structure for the IB private data exchange in setting up
+ * an RDS connection.  The exchange is different for IPv4 and IPv6 connections.
+ * The reason is that the address size is different and the addresses
+ * exchanged are in the beginning of the structure.  Hence it is not possible
+ * for interoperability if same structure is used.
+ */
+struct rds_ib_conn_priv_cmn {
+	u8			ricpc_protocol_major;
+	u8			ricpc_protocol_minor;
+	__be16			ricpc_protocol_minor_mask;	/* bitmask */
+	__be32			ricpc_reserved1;
+	__be64			ricpc_ack_seq;
+	__be32			ricpc_credit;	/* non-zero enables flow ctl */
+};
+
 struct rds_ib_connect_private {
 	/* Add new fields at the end, and don't permute existing fields. */
-	__be32			dp_saddr;
-	__be32			dp_daddr;
-	u8			dp_protocol_major;
-	u8			dp_protocol_minor;
-	__be16			dp_protocol_minor_mask; /* bitmask */
-	__be32			dp_reserved1;
-	__be64			dp_ack_seq;
-	__be32			dp_credit;		/* non-zero enables flow ctl */
+	__be32				dp_saddr;
+	__be32				dp_daddr;
+	struct rds_ib_conn_priv_cmn	dp_cmn;
+};
+
+struct rds6_ib_connect_private {
+	/* Add new fields at the end, and don't permute existing fields. */
+	struct in6_addr			dp_saddr;
+	struct in6_addr			dp_daddr;
+	struct rds_ib_conn_priv_cmn	dp_cmn;
+};
+
+#define dp_protocol_major	dp_cmn.ricpc_protocol_major
+#define dp_protocol_minor	dp_cmn.ricpc_protocol_minor
+#define dp_protocol_minor_mask	dp_cmn.ricpc_protocol_minor_mask
+#define dp_ack_seq		dp_cmn.ricpc_ack_seq
+#define dp_credit		dp_cmn.ricpc_credit
+
+union rds_ib_conn_priv {
+	struct rds_ib_connect_private	ricp_v4;
+	struct rds6_ib_connect_private	ricp_v6;
 };
 
 struct rds_ib_send_work {
@@ -351,8 +379,8 @@ void rds_ib_listen_stop(void);
 __printf(2, 3)
 void __rds_ib_conn_error(struct rds_connection *conn, const char *, ...);
 int rds_ib_cm_handle_connect(struct rdma_cm_id *cm_id,
-			     struct rdma_cm_event *event);
-int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id);
+			     struct rdma_cm_event *event, bool isv6);
+int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id, bool isv6);
 void rds_ib_cm_connect_complete(struct rds_connection *conn,
 				struct rdma_cm_event *event);
 
@@ -361,7 +389,8 @@ void rds_ib_cm_connect_complete(struct rds_connection *conn,
 	__rds_ib_conn_error(conn, KERN_WARNING "RDS/IB: " fmt)
 
 /* ib_rdma.c */
-int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr);
+int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev,
+			 struct in6_addr *ipaddr);
 void rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_remove_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_destroy_nodev_conns(void);

commit 2a171788ba7bb61995e98e8163204fc7880f63b2
Merge: bf5345882bd1 d4c2e9fca5b7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Nov 4 09:26:51 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Files removed in 'net-next' had their license header updated
    in 'net'.  We take the remove from 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index bf4822407567..86a8578d95b8 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 #ifndef _RDS_IB_H
 #define _RDS_IB_H
 

commit 9dff9936f0ccfffba5106ee4217c71c7bcf95143
Author: Avinash Repaka <avinash.repaka@oracle.com>
Date:   Wed Oct 4 12:10:43 2017 -0700

    RDS: IB: Limit the scope of has_fr/has_fmr variables
    
    This patch fixes the scope of has_fr and has_fmr variables as they are
    needed only in rds_ib_add_one().
    
    Signed-off-by: Avinash Repaka <avinash.repaka@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index bf4822407567..6ea6a27891b0 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -215,8 +215,6 @@ struct rds_ib_device {
 	struct list_head	conn_list;
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
-	bool                    has_fmr;
-	bool                    has_fr;
 	bool                    use_fastreg;
 
 	unsigned int		max_mrs;

commit 50d61ff789d728a9b6624582ce28441dc0bf5b42
Author: Reshetova, Elena <elena.reshetova@intel.com>
Date:   Tue Jul 4 15:53:15 2017 +0300

    net, rds: convert rds_ib_device.refcount from atomic_t to refcount_t
    
    refcount_t type and corresponding API should be
    used instead of atomic_t when the variable is used as
    a reference counter. This allows to avoid accidental
    refcounter overflows that might lead to use-after-free
    situations.
    
    Signed-off-by: Elena Reshetova <elena.reshetova@intel.com>
    Signed-off-by: Hans Liljestrand <ishkamiel@gmail.com>
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David Windsor <dwindsor@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index ec550626e221..bf4822407567 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -230,7 +230,7 @@ struct rds_ib_device {
 	unsigned int		max_initiator_depth;
 	unsigned int		max_responder_resources;
 	spinlock_t		spinlock;	/* protect the above */
-	atomic_t		refcount;
+	refcount_t		refcount;
 	struct work_struct	free_work;
 	int			*vector_load;
 };

commit ac1820fb286b552b6885d40ab34f1e59b815f1f1
Merge: edccb5942965 0bbb3b7496ea
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Feb 25 13:45:43 2017 -0800

    Merge tag 'for-next-dma_ops' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma DMA mapping updates from Doug Ledford:
     "Drop IB DMA mapping code and use core DMA code instead.
    
      Bart Van Assche noted that the ib DMA mapping code was significantly
      similar enough to the core DMA mapping code that with a few changes it
      was possible to remove the IB DMA mapping code entirely and switch the
      RDMA stack to use the core DMA mapping code.
    
      This resulted in a nice set of cleanups, but touched the entire tree
      and has been kept separate for that reason."
    
    * tag 'for-next-dma_ops' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (37 commits)
      IB/rxe, IB/rdmavt: Use dma_virt_ops instead of duplicating it
      IB/core: Remove ib_device.dma_device
      nvme-rdma: Switch from dma_device to dev.parent
      RDS: net: Switch from dma_device to dev.parent
      IB/srpt: Modify a debug statement
      IB/srp: Switch from dma_device to dev.parent
      IB/iser: Switch from dma_device to dev.parent
      IB/IPoIB: Switch from dma_device to dev.parent
      IB/rxe: Switch from dma_device to dev.parent
      IB/vmw_pvrdma: Switch from dma_device to dev.parent
      IB/usnic: Switch from dma_device to dev.parent
      IB/qib: Switch from dma_device to dev.parent
      IB/qedr: Switch from dma_device to dev.parent
      IB/ocrdma: Switch from dma_device to dev.parent
      IB/nes: Remove a superfluous assignment statement
      IB/mthca: Switch from dma_device to dev.parent
      IB/mlx5: Switch from dma_device to dev.parent
      IB/mlx4: Switch from dma_device to dev.parent
      IB/i40iw: Remove a superfluous assignment statement
      IB/hns: Switch from dma_device to dev.parent
      ...

commit 5f68dcaff5524007ebaeafb45e9e9477f4da2a73
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:34 2017 -0800

    RDS: net: Switch from dma_device to dev.parent
    
    Prepare for removal of ib_device.dma_device.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Cc: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index d21ca88ab628..4a520826da23 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -223,7 +223,7 @@ struct rds_ib_device {
 	struct work_struct	free_work;
 };
 
-#define ibdev_to_node(ibdev) dev_to_node(ibdev->dma_device)
+#define ibdev_to_node(ibdev) dev_to_node((ibdev)->dev.parent)
 #define rdsibdev_to_node(rdsibdev) ibdev_to_node(rdsibdev->dev)
 
 /* bits for i_ack_flags */

commit d43dbacfc06349309a99e50c575d86bc36ca4178
Author: Bart Van Assche <bart.vanassche@sandisk.com>
Date:   Fri Jan 20 13:04:10 2017 -0800

    IB/core: Change the type of an ib_dma_alloc_coherent() argument
    
    Change the type of the dma_handle argument from u64 * to dma_addr_t *.
    This patch does not change any functionality.
    
    Signed-off-by: Bart Van Assche <bart.vanassche@sandisk.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 45ac8e8e58f4..d21ca88ab628 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -134,7 +134,7 @@ struct rds_ib_connection {
 	struct rds_ib_work_ring	i_send_ring;
 	struct rm_data_op	*i_data_op;
 	struct rds_header	*i_send_hdrs;
-	u64			i_send_hdrs_dma;
+	dma_addr_t		i_send_hdrs_dma;
 	struct rds_ib_send_work *i_sends;
 	atomic_t		i_signaled_sends;
 
@@ -144,7 +144,7 @@ struct rds_ib_connection {
 	struct rds_ib_incoming	*i_ibinc;
 	u32			i_recv_data_rem;
 	struct rds_header	*i_recv_hdrs;
-	u64			i_recv_hdrs_dma;
+	dma_addr_t		i_recv_hdrs_dma;
 	struct rds_ib_recv_work *i_recvs;
 	u64			i_ack_recv;	/* last ACK received */
 	struct rds_ib_refill_cache i_cache_incs;
@@ -161,7 +161,7 @@ struct rds_ib_connection {
 	struct rds_header	*i_ack;
 	struct ib_send_wr	i_ack_wr;
 	struct ib_sge		i_ack_sge;
-	u64			i_ack_dma;
+	dma_addr_t		i_ack_dma;
 	unsigned long		i_ack_queued;
 
 	/* Flow control related information

commit cf657269d311d575eb196c7045579b3443631b8b
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Thu Sep 29 11:07:11 2016 -0700

    RDS: IB: fix panic due to handlers running post teardown
    
    Shutdown code reaping loop takes care of emptying the
    CQ's before they being destroyed. And once tasklets are
    killed, the hanlders are not expected to run.
    
    But because of core tasklet code issues, tasklet handler could
    still run even after tasklet_kill,
    RDS IB shutdown code already reaps the CQs before freeing
    cq/qp resources so as such the handlers have nothing left
    to do post shutdown.
    
    On other hand any handler running after teardown and trying
    to access already freed qp/cq resources causes issues
    Patch fixes this race by  makes sure that handlers returns
    without any action post teardown.
    
    Reviewed-by: Wengang <wen.gang.wang@oracle.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 1fe9f79fead5..540458928f3c 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -185,6 +185,7 @@ struct rds_ib_connection {
 
 	/* Endpoint role in connection */
 	bool			i_active_side;
+	atomic_t		i_cq_quiesce;
 
 	/* Send/Recv vectors */
 	int			i_scq_vector;

commit be2f76eacc278c272f26d46e4168efe5a55f5383
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Mon Jul 4 16:16:36 2016 -0700

    RDS: IB: Add vector spreading for cqs
    
    Based on available device vectors, allocate cqs accordingly to
    get better spread of completion vectors which helps performace
    great deal..
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c62e5513d306..1fe9f79fead5 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -185,6 +185,10 @@ struct rds_ib_connection {
 
 	/* Endpoint role in connection */
 	bool			i_active_side;
+
+	/* Send/Recv vectors */
+	int			i_scq_vector;
+	int			i_rcq_vector;
 };
 
 /* This assumes that atomic_t is at least 32 bits */
@@ -227,6 +231,7 @@ struct rds_ib_device {
 	spinlock_t		spinlock;	/* protect the above */
 	atomic_t		refcount;
 	struct work_struct	free_work;
+	int			*vector_load;
 };
 
 #define ibdev_to_node(ibdev) dev_to_node(ibdev->dma_device)

commit 09b2b8f52895addd9bf28dc5ac98ff5cc750cf9a
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Sat Jul 9 17:14:02 2016 -0700

    RDS: IB: add few useful cache stasts
    
    Tracks the ib receive cache total, incoming and frag allocations.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 5f02b4d8f10c..c62e5513d306 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -151,6 +151,7 @@ struct rds_ib_connection {
 	u64			i_ack_recv;	/* last ACK received */
 	struct rds_ib_refill_cache i_cache_incs;
 	struct rds_ib_refill_cache i_cache_frags;
+	atomic_t		i_cache_allocs;
 
 	/* sending acks */
 	unsigned long		i_ack_flags;
@@ -254,6 +255,8 @@ struct rds_ib_statistics {
 	uint64_t	s_ib_rx_refill_from_cq;
 	uint64_t	s_ib_rx_refill_from_thread;
 	uint64_t	s_ib_rx_alloc_limit;
+	uint64_t	s_ib_rx_total_frags;
+	uint64_t	s_ib_rx_total_incs;
 	uint64_t	s_ib_rx_credit_updates;
 	uint64_t	s_ib_ack_sent;
 	uint64_t	s_ib_ack_send_failure;
@@ -276,6 +279,8 @@ struct rds_ib_statistics {
 	uint64_t	s_ib_rdma_mr_1m_reused;
 	uint64_t	s_ib_atomic_cswp;
 	uint64_t	s_ib_atomic_fadd;
+	uint64_t	s_ib_recv_added_to_cache;
+	uint64_t	s_ib_recv_removed_from_cache;
 };
 
 extern struct workqueue_struct *rds_ib_wq;
@@ -406,6 +411,8 @@ int rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op);
 /* ib_stats.c */
 DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);
 #define rds_ib_stats_inc(member) rds_stats_inc_which(rds_ib_stats, member)
+#define rds_ib_stats_add(member, count) \
+		rds_stats_add_which(rds_ib_stats, member, count)
 unsigned int rds_ib_stats_info_copy(struct rds_info_iterator *iter,
 				    unsigned int avail);
 

commit 581d53c91cbf7b31415a9ed5e9a8b89d6af609b3
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Sat Jul 9 18:31:38 2016 -0700

    RDS: IB: track and log active side endpoint in connection
    
    Useful to know the active and passive end points in a
    RDS IB connection.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index f14c26d22b27..5f02b4d8f10c 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -181,6 +181,9 @@ struct rds_ib_connection {
 
 	/* Batched completions */
 	unsigned int		i_unsignaled_wrs;
+
+	/* Endpoint role in connection */
+	bool			i_active_side;
 };
 
 /* This assumes that atomic_t is at least 32 bits */

commit 56012459310a1dbcc55c2dbf5500a9f7571402cb
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Tue Mar 8 09:19:01 2016 -0800

    RDS: IB: split the mr registration and invalidation path
    
    MR invalidation in RDS is done in background thread and not in
    data path like registration. So break the dependency between them
    which helps to remove the performance bottleneck.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index f4e81214e70a..f14c26d22b27 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -14,7 +14,8 @@
 
 #define RDS_IB_DEFAULT_RECV_WR		1024
 #define RDS_IB_DEFAULT_SEND_WR		256
-#define RDS_IB_DEFAULT_FR_WR		512
+#define RDS_IB_DEFAULT_FR_WR		256
+#define RDS_IB_DEFAULT_FR_INV_WR	256
 
 #define RDS_IB_DEFAULT_RETRY_COUNT	1
 
@@ -125,6 +126,7 @@ struct rds_ib_connection {
 
 	/* To control the number of wrs from fastreg */
 	atomic_t		i_fastreg_wrs;
+	atomic_t		i_fastunreg_wrs;
 
 	/* interrupt handling */
 	struct tasklet_struct	i_send_tasklet;

commit fab8688d7185a1fe01ee9e0930fc59c0f161ee93
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Mon Jul 4 15:31:21 2016 -0700

    RDS: IB: make the transport retry count smallest
    
    Transport retry is not much useful since it indicate packet loss
    in fabric so its better to failover fast rather than longer retry.
    
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 45ac8e8e58f4..f4e81214e70a 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -16,7 +16,7 @@
 #define RDS_IB_DEFAULT_SEND_WR		256
 #define RDS_IB_DEFAULT_FR_WR		512
 
-#define RDS_IB_DEFAULT_RETRY_COUNT	2
+#define RDS_IB_DEFAULT_RETRY_COUNT	1
 
 #define RDS_IB_SUPPORTED_PROTOCOLS	0x00000003	/* minor versions supported */
 

commit 6cdaf03f8c23fcbfe7d4fa6876e114193bb6b632
Author: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
Date:   Fri Aug 5 22:11:12 2016 +0200

    RDS: add __printf format attribute to error reporting functions
    
    This is helpful to detect at compile-time errors related to format
    strings.
    
    Signed-off-by: Nicolas Iooss <nicolas.iooss_linux@m4x.org>
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 046f7508c06b..45ac8e8e58f4 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -333,6 +333,7 @@ void rds_ib_conn_path_shutdown(struct rds_conn_path *cp);
 void rds_ib_state_change(struct sock *sk);
 int rds_ib_listen_init(void);
 void rds_ib_listen_stop(void);
+__printf(2, 3)
 void __rds_ib_conn_error(struct rds_connection *conn, const char *, ...);
 int rds_ib_cm_handle_connect(struct rdma_cm_id *cm_id,
 			     struct rdma_cm_event *event);

commit b04e8554f7637999af8f54cca4dcfcf49f2ae7c8
Author: Sowmini Varadhan <sowmini.varadhan@oracle.com>
Date:   Thu Jun 30 16:11:16 2016 -0700

    RDS: TCP: Hooks to set up a single connection path
    
    This patch adds ->conn_path_connect callbacks in the rds_transport
    that are used to set up a single connection path.
    
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: Sowmini Varadhan <sowmini.varadhan@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 579de7e6369c..046f7508c06b 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -328,7 +328,7 @@ extern struct list_head ib_nodev_conns;
 /* ib_cm.c */
 int rds_ib_conn_alloc(struct rds_connection *conn, gfp_t gfp);
 void rds_ib_conn_free(void *arg);
-int rds_ib_conn_connect(struct rds_connection *conn);
+int rds_ib_conn_path_connect(struct rds_conn_path *cp);
 void rds_ib_conn_path_shutdown(struct rds_conn_path *cp);
 void rds_ib_state_change(struct sock *sk);
 int rds_ib_listen_init(void);

commit 2da43c4a1b517d02e71d9611a2242273e7d399ba
Author: Sowmini Varadhan <sowmini.varadhan@oracle.com>
Date:   Thu Jun 30 16:11:15 2016 -0700

    RDS: TCP: make receive path use the rds_conn_path
    
    The ->sk_user_data contains a pointer to the rds_conn_path
    for the socket. Use this consistently in the rds_tcp_data_ready
    callbacks to get the rds_conn_path for rds_recv_incoming.
    
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: Sowmini Varadhan <sowmini.varadhan@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 2051f4bd7a66..579de7e6369c 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -354,7 +354,7 @@ void rds_ib_mr_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc);
 /* ib_recv.c */
 int rds_ib_recv_init(void);
 void rds_ib_recv_exit(void);
-int rds_ib_recv(struct rds_connection *conn);
+int rds_ib_recv_path(struct rds_conn_path *conn);
 int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_refill(struct rds_connection *conn, int prefill, gfp_t gfp);

commit 226f7a7d97e37220a442f52eb85ebff2cd6fc0d2
Author: Sowmini Varadhan <sowmini.varadhan@oracle.com>
Date:   Thu Jun 30 16:11:10 2016 -0700

    RDS: Rework path specific indirections
    
    Refactor code to avoid separate indirections for single-path
    and multipath transports. All transports (both single and mp-capable)
    will get a pointer to the rds_conn_path, and can trivially derive
    the rds_connection from the ->cp_conn.
    
    Acked-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: Sowmini Varadhan <sowmini.varadhan@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 627fb79aee65..2051f4bd7a66 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -329,7 +329,7 @@ extern struct list_head ib_nodev_conns;
 int rds_ib_conn_alloc(struct rds_connection *conn, gfp_t gfp);
 void rds_ib_conn_free(void *arg);
 int rds_ib_conn_connect(struct rds_connection *conn);
-void rds_ib_conn_shutdown(struct rds_connection *conn);
+void rds_ib_conn_path_shutdown(struct rds_conn_path *cp);
 void rds_ib_state_change(struct sock *sk);
 int rds_ib_listen_init(void);
 void rds_ib_listen_stop(void);
@@ -384,7 +384,7 @@ u32 rds_ib_ring_completed(struct rds_ib_work_ring *ring, u32 wr_id, u32 oldest);
 extern wait_queue_head_t rds_ib_ring_empty_wait;
 
 /* ib_send.c */
-void rds_ib_xmit_complete(struct rds_connection *conn);
+void rds_ib_xmit_path_complete(struct rds_conn_path *cp);
 int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 		unsigned int hdr_off, unsigned int sg, unsigned int off);
 void rds_ib_send_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc);

commit 1659185fb4d0025835eb2058a141f0746c5cab00
Author: Avinash Repaka <avinash.repaka@oracle.com>
Date:   Tue Mar 1 15:20:54 2016 -0800

    RDS: IB: Support Fastreg MR (FRMR) memory registration mode
    
    Fastreg MR(FRMR) is another method with which one can
    register memory to HCA. Some of the newer HCAs supports only fastreg
    mr mode, so we need to add support for it to have RDS functional
    on them.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Avinash Repaka <avinash.repaka@oracle.com>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index eeb0d6c85cb3..627fb79aee65 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -349,6 +349,7 @@ int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr);
 void rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_remove_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_destroy_nodev_conns(void);
+void rds_ib_mr_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc);
 
 /* ib_recv.c */
 int rds_ib_recv_init(void);

commit ad6832f950d35df8c70b577993a24b31b34d88e4
Author: santosh.shilimkar@oracle.com <santosh.shilimkar@oracle.com>
Date:   Tue Mar 1 15:20:53 2016 -0800

    RDS: IB: allocate extra space on queues for FRMR support
    
    Fastreg MR(FRMR) memory registration and invalidation makes use
    of work request and completion queues for its operation. Patch
    allocates extra queue space towards these operation(s).
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c5eddc2086ff..eeb0d6c85cb3 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -14,6 +14,7 @@
 
 #define RDS_IB_DEFAULT_RECV_WR		1024
 #define RDS_IB_DEFAULT_SEND_WR		256
+#define RDS_IB_DEFAULT_FR_WR		512
 
 #define RDS_IB_DEFAULT_RETRY_COUNT	2
 
@@ -122,6 +123,9 @@ struct rds_ib_connection {
 	struct ib_wc		i_send_wc[RDS_IB_WC_MAX];
 	struct ib_wc		i_recv_wc[RDS_IB_WC_MAX];
 
+	/* To control the number of wrs from fastreg */
+	atomic_t		i_fastreg_wrs;
+
 	/* interrupt handling */
 	struct tasklet_struct	i_send_tasklet;
 	struct tasklet_struct	i_recv_tasklet;

commit 2cb2912d65633e751d3f8397377174501412aa47
Author: santosh.shilimkar@oracle.com <santosh.shilimkar@oracle.com>
Date:   Tue Mar 1 15:20:52 2016 -0800

    RDS: IB: add Fastreg MR (FRMR) detection support
    
    Discovere Fast Memmory Registration support using IB device
    IB_DEVICE_MEM_MGT_EXTENSIONS. Certain HCA might support just FRMR
    or FMR or both FMR and FRWR. In case both mr type are supported,
    default FMR is used.
    
    Default MR is still kept as FMR against what everyone else
    is following. Default will be changed to FRMR once the
    RDS performance with FRMR is comparable with FMR. The
    work is in progress for the same.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 62fe7d5330ad..c5eddc2086ff 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -200,6 +200,10 @@ struct rds_ib_device {
 	struct list_head	conn_list;
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
+	bool                    has_fmr;
+	bool                    has_fr;
+	bool                    use_fastreg;
+
 	unsigned int		max_mrs;
 	struct rds_ib_mr_pool	*mr_1m_pool;
 	struct rds_ib_mr_pool   *mr_8k_pool;

commit db42753adb638b63572583162bb08ea193947309
Author: santosh.shilimkar@oracle.com <santosh.shilimkar@oracle.com>
Date:   Tue Mar 1 15:20:51 2016 -0800

    RDS: IB: add mr reused stats
    
    Add MR reuse statistics to RDS IB transport.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c88cb229db16..62fe7d5330ad 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -259,6 +259,8 @@ struct rds_ib_statistics {
 	uint64_t	s_ib_rdma_mr_1m_pool_flush;
 	uint64_t	s_ib_rdma_mr_1m_pool_wait;
 	uint64_t	s_ib_rdma_mr_1m_pool_depleted;
+	uint64_t	s_ib_rdma_mr_8k_reused;
+	uint64_t	s_ib_rdma_mr_1m_reused;
 	uint64_t	s_ib_atomic_cswp;
 	uint64_t	s_ib_atomic_fadd;
 };

commit f6df683f32f09f8c0c440bdd3741a1e7dc7927fe
Author: santosh.shilimkar@oracle.com <santosh.shilimkar@oracle.com>
Date:   Tue Mar 1 15:20:46 2016 -0800

    RDS: IB: Re-organise ibmr code
    
    No functional changes. This is in preperation towards adding
    fastreg memory resgitration support.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 09cd8e3a90aa..c88cb229db16 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -9,12 +9,6 @@
 #include "rds.h"
 #include "rdma_transport.h"
 
-#define RDS_FMR_1M_POOL_SIZE		(8192 / 2)
-#define RDS_FMR_1M_MSG_SIZE		256
-#define RDS_FMR_8K_MSG_SIZE		2
-#define RDS_MR_8K_SCALE			(256 / (RDS_FMR_8K_MSG_SIZE + 1))
-#define RDS_FMR_8K_POOL_SIZE		(RDS_MR_8K_SCALE * (8192 / 2))
-
 #define RDS_IB_MAX_SGE			8
 #define RDS_IB_RECV_SGE 		2
 
@@ -206,12 +200,12 @@ struct rds_ib_device {
 	struct list_head	conn_list;
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
-	unsigned int		max_fmrs;
+	unsigned int		max_mrs;
 	struct rds_ib_mr_pool	*mr_1m_pool;
 	struct rds_ib_mr_pool   *mr_8k_pool;
 	unsigned int		fmr_max_remaps;
-	unsigned int		max_8k_fmrs;
-	unsigned int		max_1m_fmrs;
+	unsigned int		max_8k_mrs;
+	unsigned int		max_1m_mrs;
 	int			max_sge;
 	unsigned int		max_wrs;
 	unsigned int		max_initiator_depth;
@@ -316,8 +310,6 @@ struct rds_ib_device *rds_ib_get_client_data(struct ib_device *device);
 void rds_ib_dev_put(struct rds_ib_device *rds_ibdev);
 extern struct ib_client rds_ib_client;
 
-extern unsigned int rds_ib_fmr_1m_pool_size;
-extern unsigned int rds_ib_fmr_8k_pool_size;
 extern unsigned int rds_ib_retry_count;
 
 extern spinlock_t ib_nodev_conns_lock;
@@ -347,17 +339,6 @@ int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr);
 void rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_remove_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_destroy_nodev_conns(void);
-struct rds_ib_mr_pool *rds_ib_create_mr_pool(struct rds_ib_device *rds_dev,
-					     int npages);
-void rds_ib_get_mr_info(struct rds_ib_device *rds_ibdev, struct rds_info_rdma_connection *iinfo);
-void rds_ib_destroy_mr_pool(struct rds_ib_mr_pool *);
-void *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,
-		    struct rds_sock *rs, u32 *key_ret);
-void rds_ib_sync_mr(void *trans_private, int dir);
-void rds_ib_free_mr(void *trans_private, int invalidate);
-void rds_ib_flush_mrs(void);
-int rds_ib_fmr_init(void);
-void rds_ib_fmr_exit(void);
 
 /* ib_recv.c */
 int rds_ib_recv_init(void);

commit dcfd041c8710320d59fce322fd901bddaf912ae8
Author: santosh.shilimkar@oracle.com <santosh.shilimkar@oracle.com>
Date:   Tue Mar 1 15:20:45 2016 -0800

    RDS: IB: Remove the RDS_IB_SEND_OP dependency
    
    This helps to combine asynchronous fastreg MR completion handler
    with send completion handler.
    
    No functional change.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index b3fdebb57460..09cd8e3a90aa 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -28,7 +28,6 @@
 #define RDS_IB_RECYCLE_BATCH_COUNT	32
 
 #define RDS_IB_WC_MAX			32
-#define RDS_IB_SEND_OP			BIT_ULL(63)
 
 extern struct rw_semaphore rds_ib_devices_lock;
 extern struct list_head rds_ib_devices;

commit ab9f2faf8f40604551336e5b0a18e0910a57b92c
Merge: 75021d28594d db7489e07669
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 7 13:33:07 2015 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "This is my initial round of 4.4 merge window patches.  There are a few
      other things I wish to get in for 4.4 that aren't in this pull, as
      this represents what has gone through merge/build/run testing and not
      what is the last few items for which testing is not yet complete.
    
       - "Checksum offload support in user space" enablement
       - Misc cxgb4 fixes, add T6 support
       - Misc usnic fixes
       - 32 bit build warning fixes
       - Misc ocrdma fixes
       - Multicast loopback prevention extension
       - Extend the GID cache to store and return attributes of GIDs
       - Misc iSER updates
       - iSER clustering update
       - Network NameSpace support for rdma CM
       - Work Request cleanup series
       - New Memory Registration API"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (76 commits)
      IB/core, cma: Make __attribute_const__ declarations sparse-friendly
      IB/core: Remove old fast registration API
      IB/ipath: Remove fast registration from the code
      IB/hfi1: Remove fast registration from the code
      RDMA/nes: Remove old FRWR API
      IB/qib: Remove old FRWR API
      iw_cxgb4: Remove old FRWR API
      RDMA/cxgb3: Remove old FRWR API
      RDMA/ocrdma: Remove old FRWR API
      IB/mlx4: Remove old FRWR API support
      IB/mlx5: Remove old FRWR API support
      IB/srp: Dont allocate a page vector when using fast_reg
      IB/srp: Remove srp_finish_mapping
      IB/srp: Convert to new registration API
      IB/srp: Split srp_map_sg
      RDS/IW: Convert to new memory registration API
      svcrdma: Port to new memory registration API
      xprtrdma: Port to new memory registration API
      iser-target: Port to new memory registration API
      IB/iser: Port to new fast registration API
      ...

commit e622f2f4ad2142d2a613a57fb85f8cf737935ef5
Author: Christoph Hellwig <hch@lst.de>
Date:   Thu Oct 8 09:16:33 2015 +0100

    IB: split struct ib_send_wr
    
    This patch split up struct ib_send_wr so that all non-trivial verbs
    use their own structure which embedds struct ib_send_wr.  This dramaticly
    shrinks the size of a WR for most common operations:
    
    sizeof(struct ib_send_wr) (old):        96
    
    sizeof(struct ib_send_wr):              48
    sizeof(struct ib_rdma_wr):              64
    sizeof(struct ib_atomic_wr):            96
    sizeof(struct ib_ud_wr):                88
    sizeof(struct ib_fast_reg_wr):          88
    sizeof(struct ib_bind_mw_wr):           96
    sizeof(struct ib_sig_handover_wr):      80
    
    And with Sagi's pending MR rework the fast registration WR will also be
    down to a reasonable size:
    
    sizeof(struct ib_fastreg_wr):           64
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Reviewed-by: Bart Van Assche <bart.vanassche@sandisk.com> [srp, srpt]
    Reviewed-by: Chuck Lever <chuck.lever@oracle.com> [sunrpc]
    Tested-by: Haggai Eran <haggaie@mellanox.com>
    Tested-by: Sagi Grimberg <sagig@mellanox.com>
    Tested-by: Steve Wise <swise@opengridcomputing.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index aae60fda77f6..301c48385166 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -69,7 +69,11 @@ struct rds_ib_connect_private {
 
 struct rds_ib_send_work {
 	void			*s_op;
-	struct ib_send_wr	s_wr;
+	union {
+		struct ib_send_wr	s_wr;
+		struct ib_rdma_wr	s_rdma_wr;
+		struct ib_atomic_wr	s_atomic_wr;
+	};
 	struct ib_sge		s_sge[RDS_IB_MAX_SGE];
 	unsigned long		s_queued;
 };

commit 06766513232d1619ac84e87b1d839d3fcc23a540
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Thu Sep 10 21:20:57 2015 -0700

    RDS: IB: split mr pool to improve 8K messages performance
    
    8K message sizes are pretty important usecase for RDS current
    workloads so we make provison to have 8K mrs available from the pool.
    Based on number of SG's in the RDS message, we pick a pool to use.
    
    Also to make sure that we don't under utlise mrs when say 8k messages
    are dominating which could lead to 8k pull being exhausted, we fall-back
    to 1m pool till 8k pool recovers for use.
    
    This helps to at least push ~55 kB/s bidirectional data which
    is a nice improvement.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 3a8cd31d4048..f17d09567890 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -9,8 +9,11 @@
 #include "rds.h"
 #include "rdma_transport.h"
 
-#define RDS_FMR_SIZE			256
-#define RDS_FMR_POOL_SIZE		8192
+#define RDS_FMR_1M_POOL_SIZE		(8192 / 2)
+#define RDS_FMR_1M_MSG_SIZE		256
+#define RDS_FMR_8K_MSG_SIZE		2
+#define RDS_MR_8K_SCALE			(256 / (RDS_FMR_8K_MSG_SIZE + 1))
+#define RDS_FMR_8K_POOL_SIZE		(RDS_MR_8K_SCALE * (8192 / 2))
 
 #define RDS_IB_MAX_SGE			8
 #define RDS_IB_RECV_SGE 		2
@@ -189,15 +192,23 @@ struct rds_ib_ipaddr {
 	struct rcu_head		rcu;
 };
 
+enum {
+	RDS_IB_MR_8K_POOL,
+	RDS_IB_MR_1M_POOL,
+};
+
 struct rds_ib_device {
 	struct list_head	list;
 	struct list_head	ipaddr_list;
 	struct list_head	conn_list;
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
-	struct rds_ib_mr_pool	*mr_pool;
-	unsigned int		fmr_max_remaps;
 	unsigned int		max_fmrs;
+	struct rds_ib_mr_pool	*mr_1m_pool;
+	struct rds_ib_mr_pool   *mr_8k_pool;
+	unsigned int		fmr_max_remaps;
+	unsigned int		max_8k_fmrs;
+	unsigned int		max_1m_fmrs;
 	int			max_sge;
 	unsigned int		max_wrs;
 	unsigned int		max_initiator_depth;
@@ -239,12 +250,18 @@ struct rds_ib_statistics {
 	uint64_t	s_ib_ack_send_delayed;
 	uint64_t	s_ib_ack_send_piggybacked;
 	uint64_t	s_ib_ack_received;
-	uint64_t	s_ib_rdma_mr_alloc;
-	uint64_t	s_ib_rdma_mr_free;
-	uint64_t	s_ib_rdma_mr_used;
-	uint64_t	s_ib_rdma_mr_pool_flush;
-	uint64_t	s_ib_rdma_mr_pool_wait;
-	uint64_t	s_ib_rdma_mr_pool_depleted;
+	uint64_t	s_ib_rdma_mr_8k_alloc;
+	uint64_t	s_ib_rdma_mr_8k_free;
+	uint64_t	s_ib_rdma_mr_8k_used;
+	uint64_t	s_ib_rdma_mr_8k_pool_flush;
+	uint64_t	s_ib_rdma_mr_8k_pool_wait;
+	uint64_t	s_ib_rdma_mr_8k_pool_depleted;
+	uint64_t	s_ib_rdma_mr_1m_alloc;
+	uint64_t	s_ib_rdma_mr_1m_free;
+	uint64_t	s_ib_rdma_mr_1m_used;
+	uint64_t	s_ib_rdma_mr_1m_pool_flush;
+	uint64_t	s_ib_rdma_mr_1m_pool_wait;
+	uint64_t	s_ib_rdma_mr_1m_pool_depleted;
 	uint64_t	s_ib_atomic_cswp;
 	uint64_t	s_ib_atomic_fadd;
 };
@@ -296,7 +313,8 @@ struct rds_ib_device *rds_ib_get_client_data(struct ib_device *device);
 void rds_ib_dev_put(struct rds_ib_device *rds_ibdev);
 extern struct ib_client rds_ib_client;
 
-extern unsigned int fmr_message_size;
+extern unsigned int rds_ib_fmr_1m_pool_size;
+extern unsigned int rds_ib_fmr_8k_pool_size;
 extern unsigned int rds_ib_retry_count;
 
 extern spinlock_t ib_nodev_conns_lock;
@@ -326,7 +344,8 @@ int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr);
 void rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_remove_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_destroy_nodev_conns(void);
-struct rds_ib_mr_pool *rds_ib_create_mr_pool(struct rds_ib_device *);
+struct rds_ib_mr_pool *rds_ib_create_mr_pool(struct rds_ib_device *rds_dev,
+					     int npages);
 void rds_ib_get_mr_info(struct rds_ib_device *rds_ibdev, struct rds_info_rdma_connection *iinfo);
 void rds_ib_destroy_mr_pool(struct rds_ib_mr_pool *);
 void *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,

commit 0c28c04500cf956c82d542c199f5bddabd590af3
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Sun Sep 6 02:18:51 2015 -0400

    RDS: IB: split send completion handling and do batch ack
    
    Similar to what we did with receive CQ completion handling, we split
    the transmit completion handler so that it lets us implement batched
    work completion handling.
    
    We re-use the cq_poll routine and makes use of RDS_IB_SEND_OP to
    identify the send vs receive completion event handler invocation.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 727759b30579..3a8cd31d4048 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -25,6 +25,7 @@
 #define RDS_IB_RECYCLE_BATCH_COUNT	32
 
 #define RDS_IB_WC_MAX			32
+#define RDS_IB_SEND_OP			BIT_ULL(63)
 
 extern struct rw_semaphore rds_ib_devices_lock;
 extern struct list_head rds_ib_devices;
@@ -118,9 +119,11 @@ struct rds_ib_connection {
 	struct ib_pd		*i_pd;
 	struct ib_cq		*i_send_cq;
 	struct ib_cq		*i_recv_cq;
+	struct ib_wc		i_send_wc[RDS_IB_WC_MAX];
 	struct ib_wc		i_recv_wc[RDS_IB_WC_MAX];
 
 	/* interrupt handling */
+	struct tasklet_struct	i_send_tasklet;
 	struct tasklet_struct	i_recv_tasklet;
 
 	/* tx */
@@ -217,7 +220,6 @@ struct rds_ib_device {
 struct rds_ib_statistics {
 	uint64_t	s_ib_connect_raced;
 	uint64_t	s_ib_listen_closed_stale;
-	uint64_t	s_ib_tx_cq_call;
 	uint64_t	s_ib_evt_handler_call;
 	uint64_t	s_ib_tasklet_call;
 	uint64_t	s_ib_tx_cq_event;
@@ -371,7 +373,7 @@ extern wait_queue_head_t rds_ib_ring_empty_wait;
 void rds_ib_xmit_complete(struct rds_connection *conn);
 int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 		unsigned int hdr_off, unsigned int sg, unsigned int off);
-void rds_ib_send_cq_comp_handler(struct ib_cq *cq, void *context);
+void rds_ib_send_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc);
 void rds_ib_send_init_ring(struct rds_ib_connection *ic);
 void rds_ib_send_clear_ring(struct rds_ib_connection *ic);
 int rds_ib_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op);

commit f4f943c958a2869b0601092857c1cf0e485d3ce8
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Sun Sep 6 02:18:51 2015 -0400

    RDS: IB: ack more receive completions to improve performance
    
    For better performance, we split the receive completion IRQ handler. That
    lets us acknowledge several WCE events in one call. We also limit the WC
    to max 32 to avoid latency. Acknowledging several completions in one call
    instead of several calls each time will provide better performance since
    less mutual exclusion locks are being performed.
    
    In next patch, send completion is also split which re-uses the poll_cq()
    and hence the code is moved to ib_cm.c
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index f1fd5ffec4e1..727759b30579 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -24,6 +24,8 @@
 
 #define RDS_IB_RECYCLE_BATCH_COUNT	32
 
+#define RDS_IB_WC_MAX			32
+
 extern struct rw_semaphore rds_ib_devices_lock;
 extern struct list_head rds_ib_devices;
 
@@ -89,6 +91,20 @@ struct rds_ib_work_ring {
 	atomic_t	w_free_ctr;
 };
 
+/* Rings are posted with all the allocations they'll need to queue the
+ * incoming message to the receiving socket so this can't fail.
+ * All fragments start with a header, so we can make sure we're not receiving
+ * garbage, and we can tell a small 8 byte fragment from an ACK frame.
+ */
+struct rds_ib_ack_state {
+	u64		ack_next;
+	u64		ack_recv;
+	unsigned int	ack_required:1;
+	unsigned int	ack_next_valid:1;
+	unsigned int	ack_recv_valid:1;
+};
+
+
 struct rds_ib_device;
 
 struct rds_ib_connection {
@@ -102,6 +118,10 @@ struct rds_ib_connection {
 	struct ib_pd		*i_pd;
 	struct ib_cq		*i_send_cq;
 	struct ib_cq		*i_recv_cq;
+	struct ib_wc		i_recv_wc[RDS_IB_WC_MAX];
+
+	/* interrupt handling */
+	struct tasklet_struct	i_recv_tasklet;
 
 	/* tx */
 	struct rds_ib_work_ring	i_send_ring;
@@ -112,7 +132,6 @@ struct rds_ib_connection {
 	atomic_t		i_signaled_sends;
 
 	/* rx */
-	struct tasklet_struct	i_recv_tasklet;
 	struct mutex		i_recv_mutex;
 	struct rds_ib_work_ring	i_recv_ring;
 	struct rds_ib_incoming	*i_ibinc;
@@ -199,13 +218,14 @@ struct rds_ib_statistics {
 	uint64_t	s_ib_connect_raced;
 	uint64_t	s_ib_listen_closed_stale;
 	uint64_t	s_ib_tx_cq_call;
+	uint64_t	s_ib_evt_handler_call;
+	uint64_t	s_ib_tasklet_call;
 	uint64_t	s_ib_tx_cq_event;
 	uint64_t	s_ib_tx_ring_full;
 	uint64_t	s_ib_tx_throttle;
 	uint64_t	s_ib_tx_sg_mapping_failure;
 	uint64_t	s_ib_tx_stalled;
 	uint64_t	s_ib_tx_credit_updates;
-	uint64_t	s_ib_rx_cq_call;
 	uint64_t	s_ib_rx_cq_event;
 	uint64_t	s_ib_rx_ring_empty;
 	uint64_t	s_ib_rx_refill_from_cq;
@@ -324,7 +344,8 @@ void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_refill(struct rds_connection *conn, int prefill, gfp_t gfp);
 void rds_ib_inc_free(struct rds_incoming *inc);
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to);
-void rds_ib_recv_cq_comp_handler(struct ib_cq *cq, void *context);
+void rds_ib_recv_cqe_handler(struct rds_ib_connection *ic, struct ib_wc *wc,
+			     struct rds_ib_ack_state *state);
 void rds_ib_recv_tasklet_fn(unsigned long data);
 void rds_ib_recv_init_ring(struct rds_ib_connection *ic);
 void rds_ib_recv_clear_ring(struct rds_ib_connection *ic);
@@ -332,6 +353,7 @@ void rds_ib_recv_init_ack(struct rds_ib_connection *ic);
 void rds_ib_attempt_ack(struct rds_ib_connection *ic);
 void rds_ib_ack_send_complete(struct rds_ib_connection *ic);
 u64 rds_ib_piggyb_ack(struct rds_ib_connection *ic);
+void rds_ib_set_ack(struct rds_ib_connection *ic, u64 seq, int ack_required);
 
 /* ib_ring.c */
 void rds_ib_ring_init(struct rds_ib_work_ring *ring, u32 nr);

commit 59fe4606748f7016e9e02bbd26da185f8620661a
Author: Santosh Shilimkar <santosh.shilimkar@oracle.com>
Date:   Fri Feb 3 11:09:23 2012 -0500

    RDS: use kfree_rcu in rds_ib_remove_ipaddr
    
    synchronize_rcu() slowing down un-necessarily the socket shutdown
    path. It is used just kfree() the ip addresses in rds_ib_remove_ipaddr()
    which is perfect usecase for kfree_rcu();
    
    So lets use that to gain some speedup.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index aae60fda77f6..f1fd5ffec4e1 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -164,6 +164,7 @@ struct rds_ib_connection {
 struct rds_ib_ipaddr {
 	struct list_head	list;
 	__be32			ipaddr;
+	struct rcu_head		rcu;
 };
 
 struct rds_ib_device {

commit 26d2177e977c912863ac04f6c1a967e793ca3a56
Merge: a794b4f32921 d1178cbcdcf9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 9 08:33:31 2015 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull inifiniband/rdma updates from Doug Ledford:
     "This is a fairly sizeable set of changes.  I've put them through a
      decent amount of testing prior to sending the pull request due to
      that.
    
      There are still a few fixups that I know are coming, but I wanted to
      go ahead and get the big, sizable chunk into your hands sooner rather
      than waiting for those last few fixups.
    
      Of note is the fact that this creates what is intended to be a
      temporary area in the drivers/staging tree specifically for some
      cleanups and additions that are coming for the RDMA stack.  We
      deprecated two drivers (ipath and amso1100) and are waiting to hear
      back if we can deprecate another one (ehca).  We also put Intel's new
      hfi1 driver into this area because it needs to be refactored and a
      transfer library created out of the factored out code, and then it and
      the qib driver and the soft-roce driver should all be modified to use
      that library.
    
      I expect drivers/staging/rdma to be around for three or four kernel
      releases and then to go away as all of the work is completed and final
      deletions of deprecated drivers are done.
    
      Summary of changes for 4.3:
    
       - Create drivers/staging/rdma
       - Move amso1100 driver to staging/rdma and schedule for deletion
       - Move ipath driver to staging/rdma and schedule for deletion
       - Add hfi1 driver to staging/rdma and set TODO for move to regular
         tree
       - Initial support for namespaces to be used on RDMA devices
       - Add RoCE GID table handling to the RDMA core caching code
       - Infrastructure to support handling of devices with differing read
         and write scatter gather capabilities
       - Various iSER updates
       - Kill off unsafe usage of global mr registrations
       - Update SRP driver
       - Misc  mlx4 driver updates
       - Support for the mr_alloc verb
       - Support for a netlink interface between kernel and user space cache
         daemon to speed path record queries and route resolution
       - Ininitial support for safe hot removal of verbs devices"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (136 commits)
      IB/ipoib: Suppress warning for send only join failures
      IB/ipoib: Clean up send-only multicast joins
      IB/srp: Fix possible protection fault
      IB/core: Move SM class defines from ib_mad.h to ib_smi.h
      IB/core: Remove unnecessary defines from ib_mad.h
      IB/hfi1: Add PSM2 user space header to header_install
      IB/hfi1: Add CSRs for CONFIG_SDMA_VERBOSITY
      mlx5: Fix incorrect wc pkey_index assignment for GSI messages
      IB/mlx5: avoid destroying a NULL mr in reg_user_mr error flow
      IB/uverbs: reject invalid or unknown opcodes
      IB/cxgb4: Fix if statement in pick_local_ip6adddrs
      IB/sa: Fix rdma netlink message flags
      IB/ucma: HW Device hot-removal support
      IB/mlx4_ib: Disassociate support
      IB/uverbs: Enable device removal when there are active user space applications
      IB/uverbs: Explicitly pass ib_dev to uverbs commands
      IB/uverbs: Fix race between ib_uverbs_open and remove_one
      IB/uverbs: Fix reference counting usage of event files
      IB/core: Make ib_dealloc_pd return void
      IB/srp: Create an insecure all physical rkey only if needed
      ...

commit e5580242aa8fab292579a1661463f7479275f7ff
Author: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
Date:   Thu Jul 30 17:22:26 2015 -0600

    rds/ib: Remove ib_get_dma_mr calls
    
    The pd now has a local_dma_lkey member which completely replaces
    ib_get_dma_mr, use it instead.
    
    Signed-off-by: Jason Gunthorpe <jgunthorpe@obsidianresearch.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 86d88ec5d556..36f7d808ffaa 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -100,7 +100,6 @@ struct rds_ib_connection {
 	/* alphabet soup, IBTA style */
 	struct rdma_cm_id	*i_cm_id;
 	struct ib_pd		*i_pd;
-	struct ib_mr		*i_mr;
 	struct ib_cq		*i_send_cq;
 	struct ib_cq		*i_recv_cq;
 
@@ -173,7 +172,6 @@ struct rds_ib_device {
 	struct list_head	conn_list;
 	struct ib_device	*dev;
 	struct ib_pd		*pd;
-	struct ib_mr		*mr;
 	struct rds_ib_mr_pool	*mr_pool;
 	unsigned int		fmr_max_remaps;
 	unsigned int		max_fmrs;

commit ad1d7dc0d79d3dd2c5d2931b13edbd4fe33e5fac
Author: santosh.shilimkar@oracle.com <santosh.shilimkar@oracle.com>
Date:   Tue Aug 25 12:02:01 2015 -0700

    RDS: push FMR pool flush work to its own worker
    
    RDS FMR flush operation and also it races with connect/reconect
    which happes a lot with RDS. FMR flush being on common rds_wq aggrevates
    the problem. Lets push RDS FMR pool flush work to its own worker.
    
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 6422c52682e5..9fc95e38659a 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -313,6 +313,8 @@ void *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,
 void rds_ib_sync_mr(void *trans_private, int dir);
 void rds_ib_free_mr(void *trans_private, int invalidate);
 void rds_ib_flush_mrs(void);
+int rds_ib_fmr_init(void);
+void rds_ib_fmr_exit(void);
 
 /* ib_recv.c */
 int rds_ib_recv_init(void);

commit 73ce4317bf983282593aff710b112a7e705620c3
Author: santosh.shilimkar@oracle.com <santosh.shilimkar@oracle.com>
Date:   Sat Aug 22 15:45:26 2015 -0700

    RDS: make sure we post recv buffers
    
    If we get an ENOMEM during rds_ib_recv_refill, we might never come
    back and refill again later. Patch makes sure to kick krdsd into
    helping out.
    
    To achieve this we add RDS_RECV_REFILL flag and update in the refill
    path based on that so that at least some therad will keep posting
    receive buffers.
    
    Since krdsd and softirq both might race for refill, we decide to
    schedule on work queue based on ring_low instead of ring_empty.
    
    Reviewed-by: Ajaykumar Hotchandani <ajaykumar.hotchandani@oracle.com>
    Signed-off-by: Santosh Shilimkar <ssantosh@kernel.org>
    Signed-off-by: Santosh Shilimkar <santosh.shilimkar@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 86d88ec5d556..6422c52682e5 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -320,7 +320,7 @@ void rds_ib_recv_exit(void);
 int rds_ib_recv(struct rds_connection *conn);
 int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
-void rds_ib_recv_refill(struct rds_connection *conn, int prefill);
+void rds_ib_recv_refill(struct rds_connection *conn, int prefill, gfp_t gfp);
 void rds_ib_inc_free(struct rds_incoming *inc);
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to);
 void rds_ib_recv_cq_comp_handler(struct ib_cq *cq, void *context);

commit e0456717e483bb8a9431b80a5bdc99a928b9b003
Merge: 98ec21a01896 1ea2d020ba47
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 16:49:49 2015 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Add TX fast path in mac80211, from Johannes Berg.
    
     2) Add TSO/GRO support to ibmveth, from Thomas Falcon
    
     3) Move away from cached routes in ipv6, just like ipv4, from Martin
        KaFai Lau.
    
     4) Lots of new rhashtable tests, from Thomas Graf.
    
     5) Run ingress qdisc lockless, from Alexei Starovoitov.
    
     6) Allow servers to fetch TCP packet headers for SYN packets of new
        connections, for fingerprinting.  From Eric Dumazet.
    
     7) Add mode parameter to pktgen, for testing receive.  From Alexei
        Starovoitov.
    
     8) Cache access optimizations via simplifications of build_skb(), from
        Alexander Duyck.
    
     9) Move page frag allocator under mm/, also from Alexander.
    
    10) Add xmit_more support to hv_netvsc, from KY Srinivasan.
    
    11) Add a counter guard in case we try to perform endless reclassify
        loops in the packet scheduler.
    
    12) Extern flow dissector to be programmable and use it in new "Flower"
        classifier.  From Jiri Pirko.
    
    13) AF_PACKET fanout rollover fixes, performance improvements, and new
        statistics.  From Willem de Bruijn.
    
    14) Add netdev driver for GENEVE tunnels, from John W Linville.
    
    15) Add ingress netfilter hooks and filtering, from Pablo Neira Ayuso.
    
    16) Fix handling of epoll edge triggers in TCP, from Eric Dumazet.
    
    17) Add an ECN retry fallback for the initial TCP handshake, from Daniel
        Borkmann.
    
    18) Add tail call support to BPF, from Alexei Starovoitov.
    
    19) Add several pktgen helper scripts, from Jesper Dangaard Brouer.
    
    20) Add zerocopy support to AF_UNIX, from Hannes Frederic Sowa.
    
    21) Favor even port numbers for allocation to connect() requests, and
        odd port numbers for bind(0), in an effort to help avoid
        ip_local_port_range exhaustion.  From Eric Dumazet.
    
    22) Add Cavium ThunderX driver, from Sunil Goutham.
    
    23) Allow bpf programs to access skb_iif and dev->ifindex SKB metadata,
        from Alexei Starovoitov.
    
    24) Add support for T6 chips in cxgb4vf driver, from Hariprasad Shenai.
    
    25) Double TCP Small Queues default to 256K to accomodate situations
        like the XEN driver and wireless aggregation.  From Wei Liu.
    
    26) Add more entropy inputs to flow dissector, from Tom Herbert.
    
    27) Add CDG congestion control algorithm to TCP, from Kenneth Klette
        Jonassen.
    
    28) Convert ipset over to RCU locking, from Jozsef Kadlecsik.
    
    29) Track and act upon link status of ipv4 route nexthops, from Andy
        Gospodarek.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1670 commits)
      bridge: vlan: flush the dynamically learned entries on port vlan delete
      bridge: multicast: add a comment to br_port_state_selection about blocking state
      net: inet_diag: export IPV6_V6ONLY sockopt
      stmmac: troubleshoot unexpected bits in des0 & des1
      net: ipv4 sysctl option to ignore routes when nexthop link is down
      net: track link-status of ipv4 nexthops
      net: switchdev: ignore unsupported bridge flags
      net: Cavium: Fix MAC address setting in shutdown state
      drivers: net: xgene: fix for ACPI support without ACPI
      ip: report the original address of ICMP messages
      net/mlx5e: Prefetch skb data on RX
      net/mlx5e: Pop cq outside mlx5e_get_cqe
      net/mlx5e: Remove mlx5e_cq.sqrq back-pointer
      net/mlx5e: Remove extra spaces
      net/mlx5e: Avoid TX CQE generation if more xmit packets expected
      net/mlx5e: Avoid redundant dev_kfree_skb() upon NOP completion
      net/mlx5e: Remove re-assignment of wq type in mlx5e_enable_rq()
      net/mlx5e: Use skb_shinfo(skb)->gso_segs rather than counting them
      net/mlx5e: Static mapping of netdev priv resources to/from netdev TX queues
      net/mlx4_en: Use HW counters for rx/tx bytes/packets in PF device
      ...

commit d2a9ec6472650c49ac3f285637b9c04b53f11ae6
Author: Fabian Frederick <fabf@skynet.be>
Date:   Tue Jun 16 20:44:07 2015 +0200

    net: rds: use for_each_sg() for scatterlist parsing
    
    This patch also renames sg to sglist and aligns function parameters.
    See Documentation/DMA-API.txt - Part Id for scatterlist details
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c36d713229e0..2de28982260c 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -235,28 +235,34 @@ extern struct workqueue_struct *rds_ib_wq;
  * doesn't define it.
  */
 static inline void rds_ib_dma_sync_sg_for_cpu(struct ib_device *dev,
-		struct scatterlist *sg, unsigned int sg_dma_len, int direction)
+					      struct scatterlist *sglist,
+					      unsigned int sg_dma_len,
+					      int direction)
 {
+	struct scatterlist *sg;
 	unsigned int i;
 
-	for (i = 0; i < sg_dma_len; ++i) {
+	for_each_sg(sglist, sg, sg_dma_len, i) {
 		ib_dma_sync_single_for_cpu(dev,
-				ib_sg_dma_address(dev, &sg[i]),
-				ib_sg_dma_len(dev, &sg[i]),
+				ib_sg_dma_address(dev, sg),
+				ib_sg_dma_len(dev, sg),
 				direction);
 	}
 }
 #define ib_dma_sync_sg_for_cpu	rds_ib_dma_sync_sg_for_cpu
 
 static inline void rds_ib_dma_sync_sg_for_device(struct ib_device *dev,
-		struct scatterlist *sg, unsigned int sg_dma_len, int direction)
+						 struct scatterlist *sglist,
+						 unsigned int sg_dma_len,
+						 int direction)
 {
+	struct scatterlist *sg;
 	unsigned int i;
 
-	for (i = 0; i < sg_dma_len; ++i) {
+	for_each_sg(sglist, sg, sg_dma_len, i) {
 		ib_dma_sync_single_for_device(dev,
-				ib_sg_dma_address(dev, &sg[i]),
-				ib_sg_dma_len(dev, &sg[i]),
+				ib_sg_dma_address(dev, sg),
+				ib_sg_dma_len(dev, sg),
 				direction);
 	}
 }

commit 3c88f3dcff7bfd5059b704bdede2bfe972c0cec9
Author: Sagi Grimberg <sagig@mellanox.com>
Date:   Mon May 18 13:40:33 2015 +0300

    RDS: Switch to generic logging helpers
    
    Signed-off-by: Sagi Grimberg <sagig@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c36d713229e0..333611d9e07d 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -339,7 +339,6 @@ u32 rds_ib_ring_completed(struct rds_ib_work_ring *ring, u32 wr_id, u32 oldest);
 extern wait_queue_head_t rds_ib_ring_empty_wait;
 
 /* ib_send.c */
-char *rds_ib_wc_status_str(enum ib_wc_status status);
 void rds_ib_xmit_complete(struct rds_connection *conn);
 int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 		unsigned int hdr_off, unsigned int sg, unsigned int off);

commit c310e72c89926e06138e4881f21e4c8da3e7ef18
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Nov 20 09:21:14 2014 -0500

    rds: switch ->inc_copy_to_user() to passing iov_iter
    
    instances get considerably simpler from that...
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 7280ab8810c2..c36d713229e0 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -316,8 +316,7 @@ int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_refill(struct rds_connection *conn, int prefill);
 void rds_ib_inc_free(struct rds_incoming *inc);
-int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
-			     size_t size);
+int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iov_iter *to);
 void rds_ib_recv_cq_comp_handler(struct ib_cq *cq, void *context);
 void rds_ib_recv_tasklet_fn(unsigned long data);
 void rds_ib_recv_init_ring(struct rds_ib_connection *ic);

commit ae4b46e9d7128d2d76e6857fe0b9fc240e8ac695
Author: Shan Wei <davidshan@tencent.com>
Date:   Mon Nov 12 15:52:01 2012 +0000

    net: rds: use this_cpu_* per-cpu helper
    
    Signed-off-by: Shan Wei <davidshan@tencent.com>
    Reviewed-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 8d2b3d5a7c21..7280ab8810c2 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -50,7 +50,7 @@ struct rds_ib_cache_head {
 };
 
 struct rds_ib_refill_cache {
-	struct rds_ib_cache_head *percpu;
+	struct rds_ib_cache_head __percpu *percpu;
 	struct list_head	 *xfer;
 	struct list_head	 *ready;
 };

commit a0c6ffbcfe600606b2d913dded4dc6b37b3bbbfd
Author: Thadeu Lima de Souza Cascardo <cascardo@linux.vnet.ibm.com>
Date:   Mon May 28 08:52:05 2012 +0000

    rds_rdma: don't assume infiniband device is PCI
    
    RDS code assumes that the struct ib_device dma_device member, which is a
    pointer, points to a struct device embedded in a struct pci_dev.
    
    This is not the case for ehca, for example, which is a OF driver, and
    makes dma_device point to a struct device embedded in a struct
    platform_device.
    
    This will make the system crash when rds_rdma is loaded in a system
    with ehca, since it will try to access the bus member of a non-existent
    struct pci_dev.
    
    The only reason rds_rdma uses the struct pci_dev is to get the NUMA node
    the device is attached to. Using dev_to_node for that is much better,
    since it won't assume which bus the infiniband is attached to.
    
    Signed-off-by: Thadeu Lima de Souza Cascardo <cascardo@linux.vnet.ibm.com>
    Cc: dledford@redhat.com
    Cc: Jes.Sorensen@redhat.com
    Cc: Venkat Venkatsubra <venkat.x.venkatsubra@oracle.com>
    Acked-by: Venkat Venkatsubra <venkat.x.venkatsubra@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index edfaaaf164eb..8d2b3d5a7c21 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -186,8 +186,7 @@ struct rds_ib_device {
 	struct work_struct	free_work;
 };
 
-#define pcidev_to_node(pcidev) pcibus_to_node(pcidev->bus)
-#define ibdev_to_node(ibdev) pcidev_to_node(to_pci_dev(ibdev->dma_device))
+#define ibdev_to_node(ibdev) dev_to_node(ibdev->dma_device)
 #define rdsibdev_to_node(rdsibdev) ibdev_to_node(rdsibdev->dev)
 
 /* bits for i_ack_flags */

commit a6b7a407865aab9f849dd99a71072b7cd1175116
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Mon Jun 6 10:43:46 2011 +0000

    net: remove interrupt.h inclusion from netdevice.h
    
    * remove interrupt.g inclusion from netdevice.h -- not needed
    * fixup fallout, add interrupt.h and hardirq.h back where needed.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 4297d92788dc..edfaaaf164eb 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -3,6 +3,7 @@
 
 #include <rdma/ib_verbs.h>
 #include <rdma/rdma_cm.h>
+#include <linux/interrupt.h>
 #include <linux/pci.h>
 #include <linux/slab.h>
 #include "rds.h"

commit c534a107e8fe446202b0fab102abc015c56c0317
Author: Tejun Heo <tj@kernel.org>
Date:   Tue Feb 1 11:42:43 2011 +0100

    rds/ib: use system_wq instead of rds_ib_fmr_wq
    
    With cmwq, there's no reason to use dedicated rds_ib_fmr_wq - it's not
    in the memory reclaim path and the maximum number of concurrent work
    items is bound by the number of devices.  Drop it and use system_wq
    instead.  This rds_ib_fmr_init/exit() noops.  Both removed.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Cc: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index e34ad032b66d..4297d92788dc 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -307,8 +307,6 @@ void *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,
 void rds_ib_sync_mr(void *trans_private, int dir);
 void rds_ib_free_mr(void *trans_private, int invalidate);
 void rds_ib_flush_mrs(void);
-int rds_ib_fmr_init(void);
-void rds_ib_fmr_exit(void);
 
 /* ib_recv.c */
 int rds_ib_recv_init(void);

commit ff51bf841587c75b58d25ed77263158619784dd3
Author: stephen hemminger <shemminger@vyatta.com>
Date:   Tue Oct 19 08:08:33 2010 +0000

    rds: make local functions/variables static
    
    The RDS protocol has lots of functions that should be
    declared static. rds_message_get/add_version_extension is
    removed since it defined but never used.
    
    Signed-off-by: Stephen Hemminger <shemminger@vyatta.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 7ad3d57e06a5..e34ad032b66d 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -265,13 +265,10 @@ static inline void rds_ib_dma_sync_sg_for_device(struct ib_device *dev,
 
 /* ib.c */
 extern struct rds_transport rds_ib_transport;
-extern void rds_ib_add_one(struct ib_device *device);
-extern void rds_ib_remove_one(struct ib_device *device);
 struct rds_ib_device *rds_ib_get_client_data(struct ib_device *device);
 void rds_ib_dev_put(struct rds_ib_device *rds_ibdev);
 extern struct ib_client rds_ib_client;
 
-extern unsigned int fmr_pool_size;
 extern unsigned int fmr_message_size;
 extern unsigned int rds_ib_retry_count;
 
@@ -374,6 +371,5 @@ extern unsigned long rds_ib_sysctl_max_unsig_wrs;
 extern unsigned long rds_ib_sysctl_max_unsig_bytes;
 extern unsigned long rds_ib_sysctl_max_recv_allocation;
 extern unsigned int rds_ib_sysctl_flow_control;
-extern ctl_table rds_ib_sysctl_table[];
 
 #endif

commit 59f740a6aeb2cde2f79fe0df38262d4c1ef35cd8
Author: Zach Brown <zach.brown@oracle.com>
Date:   Tue Aug 3 13:52:47 2010 -0700

    RDS/IB: print string constants in more places
    
    This prints the constant identifier for work completion status and rdma
    cm event types, like we already do for IB event types.
    
    A core string array helper is added that each string type uses.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 2189fd47a738..7ad3d57e06a5 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -345,6 +345,7 @@ u32 rds_ib_ring_completed(struct rds_ib_work_ring *ring, u32 wr_id, u32 oldest);
 extern wait_queue_head_t rds_ib_ring_empty_wait;
 
 /* ib_send.c */
+char *rds_ib_wc_status_str(enum ib_wc_status status);
 void rds_ib_xmit_complete(struct rds_connection *conn);
 int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 		unsigned int hdr_off, unsigned int sg, unsigned int off);

commit ea819867b788728aca60717e4fdacb3df771f670
Author: Zach Brown <zach.brown@oracle.com>
Date:   Thu Jul 15 12:34:33 2010 -0700

    RDS/IB: protect the list of IB devices
    
    The RDS IB device list wasn't protected by any locking.  Traversal in
    both the get_mr and FMR flushing paths could race with additon and
    removal.
    
    List manipulation is done with RCU primatives and is protected by the
    write side of a rwsem.  The list traversal in the get_mr fast path is
    protected by a rcu read critical section.  The FMR list traversal is
    more problematic because it can block while traversing the list.  We
    protect this with the read side of the rwsem.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index a13ced504145..2189fd47a738 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -23,6 +23,7 @@
 
 #define RDS_IB_RECYCLE_BATCH_COUNT	32
 
+extern struct rw_semaphore rds_ib_devices_lock;
 extern struct list_head rds_ib_devices;
 
 /*

commit f046011cd73c372267befd10242988eb744649fe
Author: Zach Brown <zach.brown@oracle.com>
Date:   Wed Jul 14 13:55:35 2010 -0700

    RDS/IB: track signaled sends
    
    We're seeing bugs today where IB connection shutdown clears the send
    ring while the tasklet is processing completed sends.  Implementation
    details cause this to dereference a null pointer.  Shutdown needs to
    wait for send completion to stop before tearing down the connection.  We
    can't simply wait for the ring to empty because it may contain
    unsignaled sends that will never be processed.
    
    This patch tracks the number of signaled sends that we've posted and
    waits for them to complete.  It also makes sure that the tasklet has
    finished executing.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index acda2dbc6576..a13ced504145 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -108,6 +108,7 @@ struct rds_ib_connection {
 	struct rds_header	*i_send_hdrs;
 	u64			i_send_hdrs_dma;
 	struct rds_ib_send_work *i_sends;
+	atomic_t		i_signaled_sends;
 
 	/* rx */
 	struct tasklet_struct	i_recv_tasklet;

commit ef87b7ea39a91906218a262686bcb8bad8b6b46e
Author: Zach Brown <zach.brown@oracle.com>
Date:   Fri Jul 9 12:26:20 2010 -0700

    RDS: remove __init and __exit annotation
    
    The trivial amount of memory saved isn't worth the cost of dealing with section
    mismatches.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index fd4ea69d2443..acda2dbc6576 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -282,7 +282,7 @@ void rds_ib_conn_free(void *arg);
 int rds_ib_conn_connect(struct rds_connection *conn);
 void rds_ib_conn_shutdown(struct rds_connection *conn);
 void rds_ib_state_change(struct sock *sk);
-int __init rds_ib_listen_init(void);
+int rds_ib_listen_init(void);
 void rds_ib_listen_stop(void);
 void __rds_ib_conn_error(struct rds_connection *conn, const char *, ...);
 int rds_ib_cm_handle_connect(struct rdma_cm_id *cm_id,
@@ -308,11 +308,11 @@ void *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,
 void rds_ib_sync_mr(void *trans_private, int dir);
 void rds_ib_free_mr(void *trans_private, int invalidate);
 void rds_ib_flush_mrs(void);
-int __init rds_ib_fmr_init(void);
-void __exit rds_ib_fmr_exit(void);
+int rds_ib_fmr_init(void);
+void rds_ib_fmr_exit(void);
 
 /* ib_recv.c */
-int __init rds_ib_recv_init(void);
+int rds_ib_recv_init(void);
 void rds_ib_recv_exit(void);
 int rds_ib_recv(struct rds_connection *conn);
 int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic);
@@ -363,7 +363,7 @@ unsigned int rds_ib_stats_info_copy(struct rds_info_iterator *iter,
 				    unsigned int avail);
 
 /* ib_sysctl.c */
-int __init rds_ib_sysctl_init(void);
+int rds_ib_sysctl_init(void);
 void rds_ib_sysctl_exit(void);
 extern unsigned long rds_ib_sysctl_max_send_wr;
 extern unsigned long rds_ib_sysctl_max_recv_wr;

commit 515e079dab19cf774d1eec6e5f4ed65509e31ef1
Author: Zach Brown <zach.brown@oracle.com>
Date:   Tue Jul 6 15:09:56 2010 -0700

    RDS/IB: create a work queue for FMR flushing
    
    This patch moves the FMR flushing work in to its own mult-threaded work queue.
    This is to maintain performance in preparation for returning the main krdsd
    work queue back to a single threaded work queue to avoid deep-rooted
    concurrency bugs.
    
    This is also good because it further separates FMRs, which might be removed
    some day, from the rest of the code base.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index e9f9ddf440ca..fd4ea69d2443 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -308,6 +308,8 @@ void *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,
 void rds_ib_sync_mr(void *trans_private, int dir);
 void rds_ib_free_mr(void *trans_private, int invalidate);
 void rds_ib_flush_mrs(void);
+int __init rds_ib_fmr_init(void);
+void __exit rds_ib_fmr_exit(void);
 
 /* ib_recv.c */
 int __init rds_ib_recv_init(void);

commit 8aeb1ba6630ffd44001ae9833842794df0107676
Author: Zach Brown <zach.brown@oracle.com>
Date:   Fri Jun 25 14:58:16 2010 -0700

    RDS/IB: destroy connections on rmmod
    
    IB connections were not being destroyed during rmmod.
    
    First, recently IB device removal callback was changed to disconnect
    connections that used the removing device rather than destroying them.  So
    connections with devices during rmmod were not being destroyed.
    
    Second, rds_ib_destroy_nodev_conns() was being called before connections are
    disassociated with devices.  It would almost never find connections in the
    nodev list.
    
    We first get rid of rds_ib_destroy_conns(), which is no longer called, and
    refactor the existing caller into the main body of the function and get rid of
    the list and lock wrappers.
    
    Then we call rds_ib_destroy_nodev_conns() *after* ib_unregister_client() has
    removed the IB device from all the conns and put the conns on the nodev list.
    
    The result is that IB connections are destroyed by rmmod.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 6265ada08959..e9f9ddf440ca 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -299,15 +299,7 @@ void rds_ib_cm_connect_complete(struct rds_connection *conn,
 int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr);
 void rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
 void rds_ib_remove_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
-void __rds_ib_destroy_conns(struct list_head *list, spinlock_t *list_lock);
-static inline void rds_ib_destroy_nodev_conns(void)
-{
-	__rds_ib_destroy_conns(&ib_nodev_conns, &ib_nodev_conns_lock);
-}
-static inline void rds_ib_destroy_conns(struct rds_ib_device *rds_ibdev)
-{
-	__rds_ib_destroy_conns(&rds_ibdev->conn_list, &rds_ibdev->spinlock);
-}
+void rds_ib_destroy_nodev_conns(void);
 struct rds_ib_mr_pool *rds_ib_create_mr_pool(struct rds_ib_device *);
 void rds_ib_get_mr_info(struct rds_ib_device *rds_ibdev, struct rds_info_rdma_connection *iinfo);
 void rds_ib_destroy_mr_pool(struct rds_ib_mr_pool *);

commit b6fb0df12db6c8b6bbcc7b5c9459b3bbf5f0cee6
Author: Andy Grover <andy.grover@oracle.com>
Date:   Wed Jun 23 18:06:30 2010 -0700

    RDS/IB: Make ib_recv_refill return void
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index d2dd23d5c1a0..6265ada08959 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -323,7 +323,7 @@ void rds_ib_recv_exit(void);
 int rds_ib_recv(struct rds_connection *conn);
 int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic);
 void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
-int rds_ib_recv_refill(struct rds_connection *conn, int prefill);
+void rds_ib_recv_refill(struct rds_connection *conn, int prefill);
 void rds_ib_inc_free(struct rds_incoming *inc);
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
 			     size_t size);

commit eabb732279f1a41ac9d066aeb56973ae505c4cbc
Author: Chris Mason <chris.mason@oracle.com>
Date:   Fri Jun 11 11:18:57 2010 -0700

    rds: more FMRs are faster
    
    When we add more FMRs, we flush them less often and so we go faster.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 2efd9d11e7d4..d2dd23d5c1a0 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -9,7 +9,7 @@
 #include "rdma_transport.h"
 
 #define RDS_FMR_SIZE			256
-#define RDS_FMR_POOL_SIZE		4096
+#define RDS_FMR_POOL_SIZE		8192
 
 #define RDS_IB_MAX_SGE			8
 #define RDS_IB_RECV_SGE 		2

commit 33244125871734ebc0d8d147680a0d7e99385e0b
Author: Chris Mason <chris.mason@oracle.com>
Date:   Wed May 26 22:05:37 2010 -0700

    RDS/IB: Add caching of frags and incs
    
    This patch is based heavily on an initial patch by Chris Mason.
    Instead of freeing slab memory and pages, it keeps them, and
    funnels them back to be reused.
    
    The lock minimization strategy uses xchg and cmpxchg atomic ops
    for manipulation of pointers to list heads. We anchor the lists with a
    pointer to a list_head struct instead of a static list_head struct.
    We just have to carefully use the existing primitives with
    the difference between a pointer and a static head struct.
    
    For example, 'list_empty()' means that our anchor pointer points to a list with
    a single item instead of meaning that our static head element doesn't point to
    any list items.
    
    Original patch by Chris, with significant mods and fixes by Andy and Zach.
    
    Signed-off-by: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 9bb7a7412a44..2efd9d11e7d4 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -21,6 +21,8 @@
 
 #define RDS_IB_SUPPORTED_PROTOCOLS	0x00000003	/* minor versions supported */
 
+#define RDS_IB_RECYCLE_BATCH_COUNT	32
+
 extern struct list_head rds_ib_devices;
 
 /*
@@ -30,14 +32,27 @@ extern struct list_head rds_ib_devices;
  */
 struct rds_page_frag {
 	struct list_head	f_item;
+	struct list_head	f_cache_entry;
 	struct scatterlist	f_sg;
 };
 
 struct rds_ib_incoming {
 	struct list_head	ii_frags;
+	struct list_head	ii_cache_entry;
 	struct rds_incoming	ii_inc;
 };
 
+struct rds_ib_cache_head {
+	struct list_head *first;
+	unsigned long count;
+};
+
+struct rds_ib_refill_cache {
+	struct rds_ib_cache_head *percpu;
+	struct list_head	 *xfer;
+	struct list_head	 *ready;
+};
+
 struct rds_ib_connect_private {
 	/* Add new fields at the end, and don't permute existing fields. */
 	__be32			dp_saddr;
@@ -104,6 +119,8 @@ struct rds_ib_connection {
 	u64			i_recv_hdrs_dma;
 	struct rds_ib_recv_work *i_recvs;
 	u64			i_ack_recv;	/* last ACK received */
+	struct rds_ib_refill_cache i_cache_incs;
+	struct rds_ib_refill_cache i_cache_frags;
 
 	/* sending acks */
 	unsigned long		i_ack_flags;
@@ -304,6 +321,8 @@ void rds_ib_flush_mrs(void);
 int __init rds_ib_recv_init(void);
 void rds_ib_recv_exit(void);
 int rds_ib_recv(struct rds_connection *conn);
+int rds_ib_recv_alloc_caches(struct rds_ib_connection *ic);
+void rds_ib_recv_free_caches(struct rds_ib_connection *ic);
 int rds_ib_recv_refill(struct rds_connection *conn, int prefill);
 void rds_ib_inc_free(struct rds_incoming *inc);
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,

commit 0b088e003ccf316a76c51be5dec2d70b93be3be8
Author: Andy Grover <andy.grover@oracle.com>
Date:   Mon May 24 20:12:41 2010 -0700

    RDS: Use page_remainder_alloc() for recv bufs
    
    Instead of splitting up a page into RDS_FRAG_SIZE chunks
    ourselves, ask rds_page_remainder_alloc() to do it. While it
    is possible PAGE_SIZE > FRAG_SIZE, on x86en it isn't, so having
    duplicate "carve up a page into buffers" code seems excessive.
    
    The other modification this spawns is the use of a single
    struct scatterlist in rds_page_frag instead of a bare page ptr.
    This causes verbosity to increase in some places, and decrease
    in others.
    
    Finally, I decided to unify the lifetimes and alloc/free of
    rds_page_frag and its page. This is a nice simplification in itself,
    but will be extra-nice once we come to adding cmason's recycling
    patch.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 282ec69fe282..9bb7a7412a44 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -28,13 +28,9 @@ extern struct list_head rds_ib_devices;
  * try and minimize the amount of memory tied up both the device and
  * socket receive queues.
  */
-/* page offset of the final full frag that fits in the page */
-#define RDS_PAGE_LAST_OFF (((PAGE_SIZE  / RDS_FRAG_SIZE) - 1) * RDS_FRAG_SIZE)
 struct rds_page_frag {
 	struct list_head	f_item;
-	struct page		*f_page;
-	unsigned long		f_offset;
-	dma_addr_t 		f_mapped;
+	struct scatterlist	f_sg;
 };
 
 struct rds_ib_incoming {
@@ -107,7 +103,6 @@ struct rds_ib_connection {
 	struct rds_header	*i_recv_hdrs;
 	u64			i_recv_hdrs_dma;
 	struct rds_ib_recv_work *i_recvs;
-	struct rds_page_frag	i_frag;
 	u64			i_ack_recv;	/* last ACK received */
 
 	/* sending acks */

commit 3e0249f9c05cb77b66f7f09644ca9ca208d991a9
Author: Zach Brown <zach.brown@oracle.com>
Date:   Tue May 18 15:48:51 2010 -0700

    RDS/IB: add refcount tracking to struct rds_ib_device
    
    The RDS IB client .remove callback used to free the rds_ibdev for the given
    device unconditionally.  This could race other users of the struct.  This patch
    adds refcounting so that we only free the rds_ibdev once all of its users are
    done.
    
    Many rds_ibdev users are tied to connections.  We give the connection a
    reference and change these users to reference the device in the connection
    instead of looking it up in the IB client data.  The only user of the IB client
    data remaining is the first lookup of the device as connections are built up.
    
    Incrementing the reference count of a device found in the IB client data could
    race with final freeing so we use an RCU grace period to make sure that freeing
    won't happen until those lookups are done.
    
    MRs need the rds_ibdev to get at the pool that they're freed in to.  They exist
    outside a connection and many MRs can reference different devices from one
    socket, so it was natural to have each MR hold a reference.  MR refs can be
    dropped from interrupt handlers and final device teardown can block so we push
    it off to a work struct.  Pool teardown had to be fixed to cancel its pending
    work instead of deadlocking waiting for all queued work, including itself, to
    finish.
    
    MRs get their reference from the global device list, which gets a reference.
    It is left unprotected by locks and remains racy.  A simple global lock would
    be a significant bottleneck.  More scalable (complicated) locking should be
    done carefully in a later patch.
    
    Signed-off-by: Zach Brown <zach.brown@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 4bc3e2fba25a..282ec69fe282 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -167,6 +167,8 @@ struct rds_ib_device {
 	unsigned int		max_initiator_depth;
 	unsigned int		max_responder_resources;
 	spinlock_t		spinlock;	/* protect the above */
+	atomic_t		refcount;
+	struct work_struct	free_work;
 };
 
 #define pcidev_to_node(pcidev) pcibus_to_node(pcidev->bus)
@@ -251,6 +253,8 @@ static inline void rds_ib_dma_sync_sg_for_device(struct ib_device *dev,
 extern struct rds_transport rds_ib_transport;
 extern void rds_ib_add_one(struct ib_device *device);
 extern void rds_ib_remove_one(struct ib_device *device);
+struct rds_ib_device *rds_ib_get_client_data(struct ib_device *device);
+void rds_ib_dev_put(struct rds_ib_device *rds_ibdev);
 extern struct ib_client rds_ib_client;
 
 extern unsigned int fmr_pool_size;

commit e4c52c98e04937ea87b0979a81354d0040d284f9
Author: Andy Grover <andy.grover@oracle.com>
Date:   Fri Apr 23 10:49:53 2010 -0700

    RDS/IB: add _to_node() macros for numa and use {k,v}malloc_node()
    
    Allocate send/recv rings in memory that is node-local to the HCA.
    This significantly helps performance.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c506604325d5..4bc3e2fba25a 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -3,6 +3,8 @@
 
 #include <rdma/ib_verbs.h>
 #include <rdma/rdma_cm.h>
+#include <linux/pci.h>
+#include <linux/slab.h>
 #include "rds.h"
 #include "rdma_transport.h"
 
@@ -167,6 +169,10 @@ struct rds_ib_device {
 	spinlock_t		spinlock;	/* protect the above */
 };
 
+#define pcidev_to_node(pcidev) pcibus_to_node(pcidev->bus)
+#define ibdev_to_node(ibdev) pcidev_to_node(to_pci_dev(ibdev->dma_device))
+#define rdsibdev_to_node(rdsibdev) ibdev_to_node(rdsibdev->dev)
+
 /* bits for i_ack_flags */
 #define IB_ACK_IN_FLIGHT	0
 #define IB_ACK_REQUESTED	1

commit 51e2cba8b5936c13b40f0fa11aa4e84683dbc751
Author: Andy Grover <andy.grover@oracle.com>
Date:   Mon Mar 29 17:47:30 2010 -0700

    RDS: Move atomic stats from general to ib-specific area
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 4c2ee2401979..c506604325d5 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -202,6 +202,8 @@ struct rds_ib_statistics {
 	uint64_t	s_ib_rdma_mr_pool_flush;
 	uint64_t	s_ib_rdma_mr_pool_wait;
 	uint64_t	s_ib_rdma_mr_pool_depleted;
+	uint64_t	s_ib_atomic_cswp;
+	uint64_t	s_ib_atomic_fadd;
 };
 
 extern struct workqueue_struct *rds_ib_wq;

commit f17a1a55fb672d7f64be7f2e940ef5669e5efa0a
Author: Andy Grover <andy.grover@oracle.com>
Date:   Thu Mar 18 17:19:52 2010 -0700

    RDS: Refill recv ring directly from tasklet
    
    Performance is better if we use allocations that don't block
    to refill the receive ring. Since the whole reason we were
    kicking out to the worker thread was so we could do blocking
    allocs, we no longer need to do this.
    
    Remove gfp params from rds_ib_recv_refill(); we always use
    GFP_NOWAIT.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 202140a84f0c..4c2ee2401979 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -297,8 +297,7 @@ void rds_ib_flush_mrs(void);
 int __init rds_ib_recv_init(void);
 void rds_ib_recv_exit(void);
 int rds_ib_recv(struct rds_connection *conn);
-int rds_ib_recv_refill(struct rds_connection *conn, gfp_t kptr_gfp,
-		       gfp_t page_gfp, int prefill);
+int rds_ib_recv_refill(struct rds_connection *conn, int prefill);
 void rds_ib_inc_free(struct rds_incoming *inc);
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
 			     size_t size);

commit ff3d7d36134ef7138803734fdbf91cc986ea7976
Author: Andy Grover <andy.grover@oracle.com>
Date:   Mon Mar 1 14:03:09 2010 -0800

    RDS: Perform unmapping ops in stages
    
    Previously, RDS would wait until the final send WR had completed
    and then handle cleanup. With silent ops, we do not know
    if an atomic, rdma, or data op will be last. This patch
    handles any of these cases by keeping a pointer to the last
    op in the message in m_last_op.
    
    When the TX completion event fires, rds dispatches to per-op-type
    cleanup functions, and then does whole-message cleanup, if the
    last op equalled m_last_op.
    
    This patch also moves towards having op-specific functions take
    the op struct, instead of the overall rm struct.
    
    rds_ib_connection has a pointer to keep track of a a partially-
    completed data send operation. This patch changes it from an
    rds_message pointer to the narrower rm_data_op pointer, and
    modifies places that use this pointer as needed.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index d64b5087eefe..202140a84f0c 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -53,8 +53,7 @@ struct rds_ib_connect_private {
 };
 
 struct rds_ib_send_work {
-	struct rds_message	*s_rm;
-	struct rm_rdma_op	*s_op;
+	void			*s_op;
 	struct ib_send_wr	s_wr;
 	struct ib_sge		s_sge[RDS_IB_MAX_SGE];
 	unsigned long		s_queued;
@@ -92,7 +91,7 @@ struct rds_ib_connection {
 
 	/* tx */
 	struct rds_ib_work_ring	i_send_ring;
-	struct rds_message	*i_rm;
+	struct rm_data_op	*i_data_op;
 	struct rds_header	*i_send_hdrs;
 	u64			i_send_hdrs_dma;
 	struct rds_ib_send_work *i_sends;
@@ -336,7 +335,7 @@ void rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits);
 void rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted);
 int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,
 			     u32 *adv_credits, int need_posted, int max_posted);
-int rds_ib_xmit_atomic(struct rds_connection *conn, struct rds_message *rm);
+int rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op);
 
 /* ib_stats.c */
 DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);

commit f8b3aaf2ba8ca9e27b47f8bfdff07c8b968f2c05
Author: Andy Grover <andy.grover@oracle.com>
Date:   Mon Mar 1 14:11:53 2010 -0800

    RDS: Remove struct rds_rdma_op
    
    A big changeset, but it's all pretty dumb.
    
    struct rds_rdma_op was already embedded in struct rm_rdma_op.
    Remove rds_rdma_op and put its members in rm_rdma_op. Rename
    members with "op_" prefix instead of "r_", for consistency.
    
    Of course this breaks a lot, so fixup the code accordingly.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 96769b86a536..d64b5087eefe 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -54,7 +54,7 @@ struct rds_ib_connect_private {
 
 struct rds_ib_send_work {
 	struct rds_message	*s_rm;
-	struct rds_rdma_op	*s_op;
+	struct rm_rdma_op	*s_op;
 	struct ib_send_wr	s_wr;
 	struct ib_sge		s_sge[RDS_IB_MAX_SGE];
 	unsigned long		s_queued;
@@ -331,7 +331,7 @@ int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
 void rds_ib_send_cq_comp_handler(struct ib_cq *cq, void *context);
 void rds_ib_send_init_ring(struct rds_ib_connection *ic);
 void rds_ib_send_clear_ring(struct rds_ib_connection *ic);
-int rds_ib_xmit_rdma(struct rds_connection *conn, struct rds_rdma_op *op);
+int rds_ib_xmit_rdma(struct rds_connection *conn, struct rm_rdma_op *op);
 void rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits);
 void rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted);
 int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,

commit 241eef3e2f51fe4ad50abacd7f79c4e2d468197e
Author: Andy Grover <andy.grover@oracle.com>
Date:   Tue Jan 19 21:25:26 2010 -0800

    RDS: Implement silent atomics
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 148818174a04..96769b86a536 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -336,7 +336,7 @@ void rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits);
 void rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted);
 int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,
 			     u32 *adv_credits, int need_posted, int max_posted);
-int rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op);
+int rds_ib_xmit_atomic(struct rds_connection *conn, struct rds_message *rm);
 
 /* ib_stats.c */
 DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);

commit 1d34f175712b59ad292ecbbaa8fc05402a1fd8ed
Author: Andy Grover <andy.grover@oracle.com>
Date:   Thu Jan 14 15:08:33 2010 -0800

    RDS: Remove unsignaled_bytes sysctl
    
    Removed unsignaled_bytes sysctl and code to signal
    based on it. I believe unsignaled_wrs is more than
    sufficient for our purposes.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index dbf8d6e31493..148818174a04 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -138,7 +138,6 @@ struct rds_ib_connection {
 
 	/* Batched completions */
 	unsigned int		i_unsignaled_wrs;
-	long			i_unsignaled_bytes;
 };
 
 /* This assumes that atomic_t is at least 32 bits */

commit 919ced4ce7d6ac62dd5be62d8993fe22a527d53a
Author: Andy Grover <andy.grover@oracle.com>
Date:   Wed Jan 13 16:32:24 2010 -0800

    RDS/IB: Remove ib_[header/data]_sge() functions
    
    These functions were to cope with differently ordered
    sg entries depending on RDS 3.0 or 3.1+. Now that
    we've dropped 3.0 compatibility we no longer need them.
    
    Also, modify usage sites for these to refer to sge[0] or [1]
    directly. Reorder code to initialize header sgs first.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 426035ac54a1..dbf8d6e31493 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -356,28 +356,4 @@ extern unsigned long rds_ib_sysctl_max_recv_allocation;
 extern unsigned int rds_ib_sysctl_flow_control;
 extern ctl_table rds_ib_sysctl_table[];
 
-/*
- * Helper functions for getting/setting the header and data SGEs in
- * RDS packets (not RDMA)
- *
- * From version 3.1 onwards, header is in front of data in the sge.
- */
-static inline struct ib_sge *
-rds_ib_header_sge(struct rds_ib_connection *ic, struct ib_sge *sge)
-{
-	if (ic->conn->c_version > RDS_PROTOCOL_3_0)
-		return &sge[0];
-	else
-		return &sge[1];
-}
-
-static inline struct ib_sge *
-rds_ib_data_sge(struct rds_ib_connection *ic, struct ib_sge *sge)
-{
-	if (ic->conn->c_version > RDS_PROTOCOL_3_0)
-		return &sge[1];
-	else
-		return &sge[0];
-}
-
 #endif

commit 809fa148a29467954280fe8b7f97c92403f6293c
Author: Andy Grover <andy.grover@oracle.com>
Date:   Tue Jan 12 14:41:46 2010 -0800

    RDS: inc_purge() transport function unused - remove it
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index a303f13111c2..426035ac54a1 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -301,7 +301,6 @@ void rds_ib_recv_exit(void);
 int rds_ib_recv(struct rds_connection *conn);
 int rds_ib_recv_refill(struct rds_connection *conn, gfp_t kptr_gfp,
 		       gfp_t page_gfp, int prefill);
-void rds_ib_inc_purge(struct rds_incoming *inc);
 void rds_ib_inc_free(struct rds_incoming *inc);
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
 			     size_t size);

commit 40589e74f7ba855f3a887c9d4abe9d100c5b039c
Author: Andy Grover <andy.grover@oracle.com>
Date:   Tue Jan 12 10:50:48 2010 -0800

    RDS: Base init_depth and responder_resources on hw values
    
    Instead of using a constant for initiator_depth and
    responder_resources, read the per-QP values when the
    device is enumerated, and then use these values when creating
    the connection.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index d2fd0aa4fde7..a303f13111c2 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -164,6 +164,8 @@ struct rds_ib_device {
 	unsigned int		max_fmrs;
 	int			max_sge;
 	unsigned int		max_wrs;
+	unsigned int		max_initiator_depth;
+	unsigned int		max_responder_resources;
 	spinlock_t		spinlock;	/* protect the above */
 };
 

commit 15133f6e67d8d646d0744336b4daa3135452cb0d
Author: Andy Grover <andy.grover@oracle.com>
Date:   Tue Jan 12 14:33:38 2010 -0800

    RDS: Implement atomic operations
    
    Implement a CMSG-based interface to do FADD and CSWP ops.
    
    Alter send routines to handle atomic ops.
    
    Add atomic counters to stats.
    
    Add xmit_atomic() to struct rds_transport
    
    Inline rds_ib_send_unmap_rdma into unmap_rm
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 64df4e79b29f..d2fd0aa4fde7 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -336,6 +336,7 @@ void rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits);
 void rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted);
 int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,
 			     u32 *adv_credits, int need_posted, int max_posted);
+int rds_ib_xmit_atomic(struct rds_connection *conn, struct rm_atomic_op *op);
 
 /* ib_stats.c */
 DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);

commit d521b63b27e3a397e0ef7ca86b6e813861083c83
Author: Andy Grover <andy.grover@oracle.com>
Date:   Fri Oct 30 08:51:57 2009 +0000

    RDS/IB+IW: Move recv processing to a tasklet
    
    Move receive processing from event handler to a tasklet.
    This should help prevent hangcheck timer from going off
    when RDS is under heavy load.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 1378b854cac0..64df4e79b29f 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -98,6 +98,7 @@ struct rds_ib_connection {
 	struct rds_ib_send_work *i_sends;
 
 	/* rx */
+	struct tasklet_struct	i_recv_tasklet;
 	struct mutex		i_recv_mutex;
 	struct rds_ib_work_ring	i_recv_ring;
 	struct rds_ib_incoming	*i_ibinc;
@@ -303,6 +304,7 @@ void rds_ib_inc_free(struct rds_incoming *inc);
 int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
 			     size_t size);
 void rds_ib_recv_cq_comp_handler(struct ib_cq *cq, void *context);
+void rds_ib_recv_tasklet_fn(unsigned long data);
 void rds_ib_recv_init_ring(struct rds_ib_connection *ic);
 void rds_ib_recv_clear_ring(struct rds_ib_connection *ic);
 void rds_ib_recv_init_ack(struct rds_ib_connection *ic);

commit a870d62726721785c34fa73d852bd35e5d1b295b
Author: Andy Grover <andy.grover@oracle.com>
Date:   Fri Jul 17 13:13:33 2009 +0000

    RDS/IB: Always use PAGE_SIZE for FMR page size
    
    While FMRs allow significant flexibility in what size of pages they can use,
    we really just want FMR pages to match CPU page size. Roland says we can
    count on this always being supported, so this simplifies things.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c0de7af6cf60..1378b854cac0 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -159,9 +159,6 @@ struct rds_ib_device {
 	struct ib_pd		*pd;
 	struct ib_mr		*mr;
 	struct rds_ib_mr_pool	*mr_pool;
-	int			fmr_page_shift;
-	int			fmr_page_size;
-	u64			fmr_page_mask;
 	unsigned int		fmr_max_remaps;
 	unsigned int		max_fmrs;
 	int			max_sge;

commit 02a6a2592e41d27644d647f3bce23598649961bc
Author: Andy Grover <andy.grover@oracle.com>
Date:   Fri Jul 17 13:13:24 2009 +0000

    RDS/IB: Handle connections using RDS 3.0 wire protocol
    
    The big differences between RDS 3.0 and 3.1 are protocol-level
    flow control, and with 3.1 the header is in front of the data. The header
    always ends up in the header buffer, and the data goes in the data page.
    
    In 3.0 our "header" is a trailer, and will end up either in the data
    page, the header buffer, or split across the two. Since 3.1 is backwards-
    compatible with 3.0, we need to continue to support these cases. This
    patch does that -- if using RDS 3.0 wire protocol, it will copy the header
    from wherever it ended up into the header buffer.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 420afb95ca1a..c0de7af6cf60 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -358,17 +358,25 @@ extern ctl_table rds_ib_sysctl_table[];
 /*
  * Helper functions for getting/setting the header and data SGEs in
  * RDS packets (not RDMA)
+ *
+ * From version 3.1 onwards, header is in front of data in the sge.
  */
 static inline struct ib_sge *
 rds_ib_header_sge(struct rds_ib_connection *ic, struct ib_sge *sge)
 {
-	return &sge[0];
+	if (ic->conn->c_version > RDS_PROTOCOL_3_0)
+		return &sge[0];
+	else
+		return &sge[1];
 }
 
 static inline struct ib_sge *
 rds_ib_data_sge(struct rds_ib_connection *ic, struct ib_sge *sge)
 {
-	return &sge[1];
+	if (ic->conn->c_version > RDS_PROTOCOL_3_0)
+		return &sge[1];
+	else
+		return &sge[0];
 }
 
 #endif

commit 3ba23ade464cca7c4a7ba5628c613339d3f2e161
Author: Andy Grover <andy.grover@oracle.com>
Date:   Fri Jul 17 13:13:22 2009 +0000

    RDS: Set retry_count to 2 and make modifiable via modparam
    
    This will be default cause IB connections to failover faster,
    but allow a longer retry count to be used if desired.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 455ae73047fe..420afb95ca1a 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -15,6 +15,8 @@
 #define RDS_IB_DEFAULT_RECV_WR		1024
 #define RDS_IB_DEFAULT_SEND_WR		256
 
+#define RDS_IB_DEFAULT_RETRY_COUNT	2
+
 #define RDS_IB_SUPPORTED_PROTOCOLS	0x00000003	/* minor versions supported */
 
 extern struct list_head rds_ib_devices;
@@ -247,6 +249,7 @@ extern struct ib_client rds_ib_client;
 
 extern unsigned int fmr_pool_size;
 extern unsigned int fmr_message_size;
+extern unsigned int rds_ib_retry_count;
 
 extern spinlock_t ib_nodev_conns_lock;
 extern struct list_head ib_nodev_conns;

commit 7b70d0336da777c00395cc7a503497c2cdabd1a8
Author: Steve Wise <swise@opengridcomputing.com>
Date:   Thu Apr 9 14:09:39 2009 +0000

    RDS/IW+IB: Allow max credit advertise window.
    
    Fix hack that restricts the credit advertisement to 127.
    
    Signed-off-by: Steve Wise <swise@opengridcomputing.com>
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 069206cae733..455ae73047fe 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -333,7 +333,7 @@ int rds_ib_xmit_rdma(struct rds_connection *conn, struct rds_rdma_op *op);
 void rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits);
 void rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted);
 int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,
-			     u32 *adv_credits, int need_posted);
+			     u32 *adv_credits, int need_posted, int max_posted);
 
 /* ib_stats.c */
 DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);

commit 8cbd9606a6367c221a7bbcc47f3ab1a8c31b6437
Author: Andy Grover <andy.grover@oracle.com>
Date:   Wed Apr 1 08:20:20 2009 +0000

    RDS: Use spinlock to protect 64b value update on 32b archs
    
    We have a 64bit value that needs to be set atomically.
    This is easy and quick on all 64bit archs, and can also be done
    on x86/32 with set_64bit() (uses cmpxchg8b). However other
    32b archs don't have this.
    
    I actually changed this to the current state in preparation for
    mainline because the old way (using a spinlock on 32b) resulted in
    unsightly #ifdefs in the code. But obviously, being correct takes
    precedence.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index c08ffffb3164..069206cae733 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -108,7 +108,12 @@ struct rds_ib_connection {
 
 	/* sending acks */
 	unsigned long		i_ack_flags;
+#ifdef KERNEL_HAS_ATOMIC64
+	atomic64_t		i_ack_next;	/* next ACK to send */
+#else
+	spinlock_t		i_ack_lock;	/* protect i_ack_next */
 	u64			i_ack_next;	/* next ACK to send */
+#endif
 	struct rds_header	*i_ack;
 	struct ib_send_wr	i_ack_wr;
 	struct ib_sge		i_ack_sge;
@@ -363,13 +368,4 @@ rds_ib_data_sge(struct rds_ib_connection *ic, struct ib_sge *sge)
 	return &sge[1];
 }
 
-static inline void rds_ib_set_64bit(u64 *ptr, u64 val)
-{
-#if BITS_PER_LONG == 64
-	*ptr = val;
-#else
-	set_64bit(ptr, val);
-#endif
-}
-
 #endif

commit 745cbccac3fe8cead529a1b3358e1e86a1505bfa
Author: Andy Grover <andy.grover@oracle.com>
Date:   Wed Apr 1 08:20:19 2009 +0000

    RDS: Rewrite connection cleanup, fixing oops on rmmod
    
    This fixes a bug where a connection was unexpectedly
    not on *any* list while being destroyed. It also
    cleans up some code duplication and regularizes some
    function names.
    
    * Grab appropriate lock in conn_free() and explain in comment
    * Ensure via locking that a conn is never not on either
      a dev's list or the nodev list
    * Add rds_xx_remove_conn() to match rds_xx_add_conn()
    * Make rds_xx_add_conn() return void
    * Rename remove_{,nodev_}conns() to
      destroy_{,nodev_}conns() and unify their implementation
      in a helper function
    * Document lock ordering as nodev conn_lock before
      dev_conn_lock
    
    Reported-by: Yosef Etigin <yosefe@voltaire.com>
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
index 8be563a1363a..c08ffffb3164 100644
--- a/net/rds/ib.h
+++ b/net/rds/ib.h
@@ -267,9 +267,17 @@ void rds_ib_cm_connect_complete(struct rds_connection *conn,
 
 /* ib_rdma.c */
 int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr);
-int rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
-void rds_ib_remove_nodev_conns(void);
-void rds_ib_remove_conns(struct rds_ib_device *rds_ibdev);
+void rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
+void rds_ib_remove_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
+void __rds_ib_destroy_conns(struct list_head *list, spinlock_t *list_lock);
+static inline void rds_ib_destroy_nodev_conns(void)
+{
+	__rds_ib_destroy_conns(&ib_nodev_conns, &ib_nodev_conns_lock);
+}
+static inline void rds_ib_destroy_conns(struct rds_ib_device *rds_ibdev)
+{
+	__rds_ib_destroy_conns(&rds_ibdev->conn_list, &rds_ibdev->spinlock);
+}
 struct rds_ib_mr_pool *rds_ib_create_mr_pool(struct rds_ib_device *);
 void rds_ib_get_mr_info(struct rds_ib_device *rds_ibdev, struct rds_info_rdma_connection *iinfo);
 void rds_ib_destroy_mr_pool(struct rds_ib_mr_pool *);

commit ec16227e14141e4fd7ae76354c09dadfe2449d9e
Author: Andy Grover <andy.grover@oracle.com>
Date:   Tue Feb 24 15:30:30 2009 +0000

    RDS/IB: Infiniband transport
    
    Registers as an RDS transport and an IB client, and uses IB CM
    API to allocate ids, queue pairs, and the rest of that fun stuff.
    
    Signed-off-by: Andy Grover <andy.grover@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rds/ib.h b/net/rds/ib.h
new file mode 100644
index 000000000000..8be563a1363a
--- /dev/null
+++ b/net/rds/ib.h
@@ -0,0 +1,367 @@
+#ifndef _RDS_IB_H
+#define _RDS_IB_H
+
+#include <rdma/ib_verbs.h>
+#include <rdma/rdma_cm.h>
+#include "rds.h"
+#include "rdma_transport.h"
+
+#define RDS_FMR_SIZE			256
+#define RDS_FMR_POOL_SIZE		4096
+
+#define RDS_IB_MAX_SGE			8
+#define RDS_IB_RECV_SGE 		2
+
+#define RDS_IB_DEFAULT_RECV_WR		1024
+#define RDS_IB_DEFAULT_SEND_WR		256
+
+#define RDS_IB_SUPPORTED_PROTOCOLS	0x00000003	/* minor versions supported */
+
+extern struct list_head rds_ib_devices;
+
+/*
+ * IB posts RDS_FRAG_SIZE fragments of pages to the receive queues to
+ * try and minimize the amount of memory tied up both the device and
+ * socket receive queues.
+ */
+/* page offset of the final full frag that fits in the page */
+#define RDS_PAGE_LAST_OFF (((PAGE_SIZE  / RDS_FRAG_SIZE) - 1) * RDS_FRAG_SIZE)
+struct rds_page_frag {
+	struct list_head	f_item;
+	struct page		*f_page;
+	unsigned long		f_offset;
+	dma_addr_t 		f_mapped;
+};
+
+struct rds_ib_incoming {
+	struct list_head	ii_frags;
+	struct rds_incoming	ii_inc;
+};
+
+struct rds_ib_connect_private {
+	/* Add new fields at the end, and don't permute existing fields. */
+	__be32			dp_saddr;
+	__be32			dp_daddr;
+	u8			dp_protocol_major;
+	u8			dp_protocol_minor;
+	__be16			dp_protocol_minor_mask; /* bitmask */
+	__be32			dp_reserved1;
+	__be64			dp_ack_seq;
+	__be32			dp_credit;		/* non-zero enables flow ctl */
+};
+
+struct rds_ib_send_work {
+	struct rds_message	*s_rm;
+	struct rds_rdma_op	*s_op;
+	struct ib_send_wr	s_wr;
+	struct ib_sge		s_sge[RDS_IB_MAX_SGE];
+	unsigned long		s_queued;
+};
+
+struct rds_ib_recv_work {
+	struct rds_ib_incoming 	*r_ibinc;
+	struct rds_page_frag	*r_frag;
+	struct ib_recv_wr	r_wr;
+	struct ib_sge		r_sge[2];
+};
+
+struct rds_ib_work_ring {
+	u32		w_nr;
+	u32		w_alloc_ptr;
+	u32		w_alloc_ctr;
+	u32		w_free_ptr;
+	atomic_t	w_free_ctr;
+};
+
+struct rds_ib_device;
+
+struct rds_ib_connection {
+
+	struct list_head	ib_node;
+	struct rds_ib_device	*rds_ibdev;
+	struct rds_connection	*conn;
+
+	/* alphabet soup, IBTA style */
+	struct rdma_cm_id	*i_cm_id;
+	struct ib_pd		*i_pd;
+	struct ib_mr		*i_mr;
+	struct ib_cq		*i_send_cq;
+	struct ib_cq		*i_recv_cq;
+
+	/* tx */
+	struct rds_ib_work_ring	i_send_ring;
+	struct rds_message	*i_rm;
+	struct rds_header	*i_send_hdrs;
+	u64			i_send_hdrs_dma;
+	struct rds_ib_send_work *i_sends;
+
+	/* rx */
+	struct mutex		i_recv_mutex;
+	struct rds_ib_work_ring	i_recv_ring;
+	struct rds_ib_incoming	*i_ibinc;
+	u32			i_recv_data_rem;
+	struct rds_header	*i_recv_hdrs;
+	u64			i_recv_hdrs_dma;
+	struct rds_ib_recv_work *i_recvs;
+	struct rds_page_frag	i_frag;
+	u64			i_ack_recv;	/* last ACK received */
+
+	/* sending acks */
+	unsigned long		i_ack_flags;
+	u64			i_ack_next;	/* next ACK to send */
+	struct rds_header	*i_ack;
+	struct ib_send_wr	i_ack_wr;
+	struct ib_sge		i_ack_sge;
+	u64			i_ack_dma;
+	unsigned long		i_ack_queued;
+
+	/* Flow control related information
+	 *
+	 * Our algorithm uses a pair variables that we need to access
+	 * atomically - one for the send credits, and one posted
+	 * recv credits we need to transfer to remote.
+	 * Rather than protect them using a slow spinlock, we put both into
+	 * a single atomic_t and update it using cmpxchg
+	 */
+	atomic_t		i_credits;
+
+	/* Protocol version specific information */
+	unsigned int		i_flowctl:1;	/* enable/disable flow ctl */
+
+	/* Batched completions */
+	unsigned int		i_unsignaled_wrs;
+	long			i_unsignaled_bytes;
+};
+
+/* This assumes that atomic_t is at least 32 bits */
+#define IB_GET_SEND_CREDITS(v)	((v) & 0xffff)
+#define IB_GET_POST_CREDITS(v)	((v) >> 16)
+#define IB_SET_SEND_CREDITS(v)	((v) & 0xffff)
+#define IB_SET_POST_CREDITS(v)	((v) << 16)
+
+struct rds_ib_ipaddr {
+	struct list_head	list;
+	__be32			ipaddr;
+};
+
+struct rds_ib_device {
+	struct list_head	list;
+	struct list_head	ipaddr_list;
+	struct list_head	conn_list;
+	struct ib_device	*dev;
+	struct ib_pd		*pd;
+	struct ib_mr		*mr;
+	struct rds_ib_mr_pool	*mr_pool;
+	int			fmr_page_shift;
+	int			fmr_page_size;
+	u64			fmr_page_mask;
+	unsigned int		fmr_max_remaps;
+	unsigned int		max_fmrs;
+	int			max_sge;
+	unsigned int		max_wrs;
+	spinlock_t		spinlock;	/* protect the above */
+};
+
+/* bits for i_ack_flags */
+#define IB_ACK_IN_FLIGHT	0
+#define IB_ACK_REQUESTED	1
+
+/* Magic WR_ID for ACKs */
+#define RDS_IB_ACK_WR_ID	(~(u64) 0)
+
+struct rds_ib_statistics {
+	uint64_t	s_ib_connect_raced;
+	uint64_t	s_ib_listen_closed_stale;
+	uint64_t	s_ib_tx_cq_call;
+	uint64_t	s_ib_tx_cq_event;
+	uint64_t	s_ib_tx_ring_full;
+	uint64_t	s_ib_tx_throttle;
+	uint64_t	s_ib_tx_sg_mapping_failure;
+	uint64_t	s_ib_tx_stalled;
+	uint64_t	s_ib_tx_credit_updates;
+	uint64_t	s_ib_rx_cq_call;
+	uint64_t	s_ib_rx_cq_event;
+	uint64_t	s_ib_rx_ring_empty;
+	uint64_t	s_ib_rx_refill_from_cq;
+	uint64_t	s_ib_rx_refill_from_thread;
+	uint64_t	s_ib_rx_alloc_limit;
+	uint64_t	s_ib_rx_credit_updates;
+	uint64_t	s_ib_ack_sent;
+	uint64_t	s_ib_ack_send_failure;
+	uint64_t	s_ib_ack_send_delayed;
+	uint64_t	s_ib_ack_send_piggybacked;
+	uint64_t	s_ib_ack_received;
+	uint64_t	s_ib_rdma_mr_alloc;
+	uint64_t	s_ib_rdma_mr_free;
+	uint64_t	s_ib_rdma_mr_used;
+	uint64_t	s_ib_rdma_mr_pool_flush;
+	uint64_t	s_ib_rdma_mr_pool_wait;
+	uint64_t	s_ib_rdma_mr_pool_depleted;
+};
+
+extern struct workqueue_struct *rds_ib_wq;
+
+/*
+ * Fake ib_dma_sync_sg_for_{cpu,device} as long as ib_verbs.h
+ * doesn't define it.
+ */
+static inline void rds_ib_dma_sync_sg_for_cpu(struct ib_device *dev,
+		struct scatterlist *sg, unsigned int sg_dma_len, int direction)
+{
+	unsigned int i;
+
+	for (i = 0; i < sg_dma_len; ++i) {
+		ib_dma_sync_single_for_cpu(dev,
+				ib_sg_dma_address(dev, &sg[i]),
+				ib_sg_dma_len(dev, &sg[i]),
+				direction);
+	}
+}
+#define ib_dma_sync_sg_for_cpu	rds_ib_dma_sync_sg_for_cpu
+
+static inline void rds_ib_dma_sync_sg_for_device(struct ib_device *dev,
+		struct scatterlist *sg, unsigned int sg_dma_len, int direction)
+{
+	unsigned int i;
+
+	for (i = 0; i < sg_dma_len; ++i) {
+		ib_dma_sync_single_for_device(dev,
+				ib_sg_dma_address(dev, &sg[i]),
+				ib_sg_dma_len(dev, &sg[i]),
+				direction);
+	}
+}
+#define ib_dma_sync_sg_for_device	rds_ib_dma_sync_sg_for_device
+
+
+/* ib.c */
+extern struct rds_transport rds_ib_transport;
+extern void rds_ib_add_one(struct ib_device *device);
+extern void rds_ib_remove_one(struct ib_device *device);
+extern struct ib_client rds_ib_client;
+
+extern unsigned int fmr_pool_size;
+extern unsigned int fmr_message_size;
+
+extern spinlock_t ib_nodev_conns_lock;
+extern struct list_head ib_nodev_conns;
+
+/* ib_cm.c */
+int rds_ib_conn_alloc(struct rds_connection *conn, gfp_t gfp);
+void rds_ib_conn_free(void *arg);
+int rds_ib_conn_connect(struct rds_connection *conn);
+void rds_ib_conn_shutdown(struct rds_connection *conn);
+void rds_ib_state_change(struct sock *sk);
+int __init rds_ib_listen_init(void);
+void rds_ib_listen_stop(void);
+void __rds_ib_conn_error(struct rds_connection *conn, const char *, ...);
+int rds_ib_cm_handle_connect(struct rdma_cm_id *cm_id,
+			     struct rdma_cm_event *event);
+int rds_ib_cm_initiate_connect(struct rdma_cm_id *cm_id);
+void rds_ib_cm_connect_complete(struct rds_connection *conn,
+				struct rdma_cm_event *event);
+
+
+#define rds_ib_conn_error(conn, fmt...) \
+	__rds_ib_conn_error(conn, KERN_WARNING "RDS/IB: " fmt)
+
+/* ib_rdma.c */
+int rds_ib_update_ipaddr(struct rds_ib_device *rds_ibdev, __be32 ipaddr);
+int rds_ib_add_conn(struct rds_ib_device *rds_ibdev, struct rds_connection *conn);
+void rds_ib_remove_nodev_conns(void);
+void rds_ib_remove_conns(struct rds_ib_device *rds_ibdev);
+struct rds_ib_mr_pool *rds_ib_create_mr_pool(struct rds_ib_device *);
+void rds_ib_get_mr_info(struct rds_ib_device *rds_ibdev, struct rds_info_rdma_connection *iinfo);
+void rds_ib_destroy_mr_pool(struct rds_ib_mr_pool *);
+void *rds_ib_get_mr(struct scatterlist *sg, unsigned long nents,
+		    struct rds_sock *rs, u32 *key_ret);
+void rds_ib_sync_mr(void *trans_private, int dir);
+void rds_ib_free_mr(void *trans_private, int invalidate);
+void rds_ib_flush_mrs(void);
+
+/* ib_recv.c */
+int __init rds_ib_recv_init(void);
+void rds_ib_recv_exit(void);
+int rds_ib_recv(struct rds_connection *conn);
+int rds_ib_recv_refill(struct rds_connection *conn, gfp_t kptr_gfp,
+		       gfp_t page_gfp, int prefill);
+void rds_ib_inc_purge(struct rds_incoming *inc);
+void rds_ib_inc_free(struct rds_incoming *inc);
+int rds_ib_inc_copy_to_user(struct rds_incoming *inc, struct iovec *iov,
+			     size_t size);
+void rds_ib_recv_cq_comp_handler(struct ib_cq *cq, void *context);
+void rds_ib_recv_init_ring(struct rds_ib_connection *ic);
+void rds_ib_recv_clear_ring(struct rds_ib_connection *ic);
+void rds_ib_recv_init_ack(struct rds_ib_connection *ic);
+void rds_ib_attempt_ack(struct rds_ib_connection *ic);
+void rds_ib_ack_send_complete(struct rds_ib_connection *ic);
+u64 rds_ib_piggyb_ack(struct rds_ib_connection *ic);
+
+/* ib_ring.c */
+void rds_ib_ring_init(struct rds_ib_work_ring *ring, u32 nr);
+void rds_ib_ring_resize(struct rds_ib_work_ring *ring, u32 nr);
+u32 rds_ib_ring_alloc(struct rds_ib_work_ring *ring, u32 val, u32 *pos);
+void rds_ib_ring_free(struct rds_ib_work_ring *ring, u32 val);
+void rds_ib_ring_unalloc(struct rds_ib_work_ring *ring, u32 val);
+int rds_ib_ring_empty(struct rds_ib_work_ring *ring);
+int rds_ib_ring_low(struct rds_ib_work_ring *ring);
+u32 rds_ib_ring_oldest(struct rds_ib_work_ring *ring);
+u32 rds_ib_ring_completed(struct rds_ib_work_ring *ring, u32 wr_id, u32 oldest);
+extern wait_queue_head_t rds_ib_ring_empty_wait;
+
+/* ib_send.c */
+void rds_ib_xmit_complete(struct rds_connection *conn);
+int rds_ib_xmit(struct rds_connection *conn, struct rds_message *rm,
+		unsigned int hdr_off, unsigned int sg, unsigned int off);
+void rds_ib_send_cq_comp_handler(struct ib_cq *cq, void *context);
+void rds_ib_send_init_ring(struct rds_ib_connection *ic);
+void rds_ib_send_clear_ring(struct rds_ib_connection *ic);
+int rds_ib_xmit_rdma(struct rds_connection *conn, struct rds_rdma_op *op);
+void rds_ib_send_add_credits(struct rds_connection *conn, unsigned int credits);
+void rds_ib_advertise_credits(struct rds_connection *conn, unsigned int posted);
+int rds_ib_send_grab_credits(struct rds_ib_connection *ic, u32 wanted,
+			     u32 *adv_credits, int need_posted);
+
+/* ib_stats.c */
+DECLARE_PER_CPU(struct rds_ib_statistics, rds_ib_stats);
+#define rds_ib_stats_inc(member) rds_stats_inc_which(rds_ib_stats, member)
+unsigned int rds_ib_stats_info_copy(struct rds_info_iterator *iter,
+				    unsigned int avail);
+
+/* ib_sysctl.c */
+int __init rds_ib_sysctl_init(void);
+void rds_ib_sysctl_exit(void);
+extern unsigned long rds_ib_sysctl_max_send_wr;
+extern unsigned long rds_ib_sysctl_max_recv_wr;
+extern unsigned long rds_ib_sysctl_max_unsig_wrs;
+extern unsigned long rds_ib_sysctl_max_unsig_bytes;
+extern unsigned long rds_ib_sysctl_max_recv_allocation;
+extern unsigned int rds_ib_sysctl_flow_control;
+extern ctl_table rds_ib_sysctl_table[];
+
+/*
+ * Helper functions for getting/setting the header and data SGEs in
+ * RDS packets (not RDMA)
+ */
+static inline struct ib_sge *
+rds_ib_header_sge(struct rds_ib_connection *ic, struct ib_sge *sge)
+{
+	return &sge[0];
+}
+
+static inline struct ib_sge *
+rds_ib_data_sge(struct rds_ib_connection *ic, struct ib_sge *sge)
+{
+	return &sge[1];
+}
+
+static inline void rds_ib_set_64bit(u64 *ptr, u64 val)
+{
+#if BITS_PER_LONG == 64
+	*ptr = val;
+#else
+	set_64bit(ptr, val);
+#endif
+}
+
+#endif
