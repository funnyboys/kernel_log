commit 0041cd5a50442db6e456b145892a0eaf2dff061f
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jun 19 23:38:16 2020 +0100

    rxrpc: Fix notification call on completion of discarded calls
    
    When preallocated service calls are being discarded, they're passed to
    ->discard_new_call() to have the caller clean up any attached higher-layer
    preallocated pieces before being marked completed.  However, the act of
    marking them completed now invokes the call's notification function - which
    causes a problem because that function might assume that the previously
    freed pieces of memory are still there.
    
    Fix this by setting a dummy notification function on the socket after
    calling ->discard_new_call().
    
    This results in the following kasan message when the kafs module is
    removed.
    
    ==================================================================
    BUG: KASAN: use-after-free in afs_wake_up_async_call+0x6aa/0x770 fs/afs/rxrpc.c:707
    Write of size 1 at addr ffff8880946c39e4 by task kworker/u4:1/21
    
    CPU: 0 PID: 21 Comm: kworker/u4:1 Not tainted 5.8.0-rc1-syzkaller #0
    Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
    Workqueue: netns cleanup_net
    Call Trace:
     __dump_stack lib/dump_stack.c:77 [inline]
     dump_stack+0x18f/0x20d lib/dump_stack.c:118
     print_address_description.constprop.0.cold+0xd3/0x413 mm/kasan/report.c:383
     __kasan_report mm/kasan/report.c:513 [inline]
     kasan_report.cold+0x1f/0x37 mm/kasan/report.c:530
     afs_wake_up_async_call+0x6aa/0x770 fs/afs/rxrpc.c:707
     rxrpc_notify_socket+0x1db/0x5d0 net/rxrpc/recvmsg.c:40
     __rxrpc_set_call_completion.part.0+0x172/0x410 net/rxrpc/recvmsg.c:76
     __rxrpc_call_completed net/rxrpc/recvmsg.c:112 [inline]
     rxrpc_call_completed+0xca/0xf0 net/rxrpc/recvmsg.c:111
     rxrpc_discard_prealloc+0x781/0xab0 net/rxrpc/call_accept.c:233
     rxrpc_listen+0x147/0x360 net/rxrpc/af_rxrpc.c:245
     afs_close_socket+0x95/0x320 fs/afs/rxrpc.c:110
     afs_net_exit+0x1bc/0x310 fs/afs/main.c:155
     ops_exit_list.isra.0+0xa8/0x150 net/core/net_namespace.c:186
     cleanup_net+0x511/0xa50 net/core/net_namespace.c:603
     process_one_work+0x965/0x1690 kernel/workqueue.c:2269
     worker_thread+0x96/0xe10 kernel/workqueue.c:2415
     kthread+0x3b5/0x4a0 kernel/kthread.c:291
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:293
    
    Allocated by task 6820:
     save_stack+0x1b/0x40 mm/kasan/common.c:48
     set_track mm/kasan/common.c:56 [inline]
     __kasan_kmalloc mm/kasan/common.c:494 [inline]
     __kasan_kmalloc.constprop.0+0xbf/0xd0 mm/kasan/common.c:467
     kmem_cache_alloc_trace+0x153/0x7d0 mm/slab.c:3551
     kmalloc include/linux/slab.h:555 [inline]
     kzalloc include/linux/slab.h:669 [inline]
     afs_alloc_call+0x55/0x630 fs/afs/rxrpc.c:141
     afs_charge_preallocation+0xe9/0x2d0 fs/afs/rxrpc.c:757
     afs_open_socket+0x292/0x360 fs/afs/rxrpc.c:92
     afs_net_init+0xa6c/0xe30 fs/afs/main.c:125
     ops_init+0xaf/0x420 net/core/net_namespace.c:151
     setup_net+0x2de/0x860 net/core/net_namespace.c:341
     copy_net_ns+0x293/0x590 net/core/net_namespace.c:482
     create_new_namespaces+0x3fb/0xb30 kernel/nsproxy.c:110
     unshare_nsproxy_namespaces+0xbd/0x1f0 kernel/nsproxy.c:231
     ksys_unshare+0x43d/0x8e0 kernel/fork.c:2983
     __do_sys_unshare kernel/fork.c:3051 [inline]
     __se_sys_unshare kernel/fork.c:3049 [inline]
     __x64_sys_unshare+0x2d/0x40 kernel/fork.c:3049
     do_syscall_64+0x60/0xe0 arch/x86/entry/common.c:359
     entry_SYSCALL_64_after_hwframe+0x44/0xa9
    
    Freed by task 21:
     save_stack+0x1b/0x40 mm/kasan/common.c:48
     set_track mm/kasan/common.c:56 [inline]
     kasan_set_free_info mm/kasan/common.c:316 [inline]
     __kasan_slab_free+0xf7/0x140 mm/kasan/common.c:455
     __cache_free mm/slab.c:3426 [inline]
     kfree+0x109/0x2b0 mm/slab.c:3757
     afs_put_call+0x585/0xa40 fs/afs/rxrpc.c:190
     rxrpc_discard_prealloc+0x764/0xab0 net/rxrpc/call_accept.c:230
     rxrpc_listen+0x147/0x360 net/rxrpc/af_rxrpc.c:245
     afs_close_socket+0x95/0x320 fs/afs/rxrpc.c:110
     afs_net_exit+0x1bc/0x310 fs/afs/main.c:155
     ops_exit_list.isra.0+0xa8/0x150 net/core/net_namespace.c:186
     cleanup_net+0x511/0xa50 net/core/net_namespace.c:603
     process_one_work+0x965/0x1690 kernel/workqueue.c:2269
     worker_thread+0x96/0xe10 kernel/workqueue.c:2415
     kthread+0x3b5/0x4a0 kernel/kthread.c:291
     ret_from_fork+0x1f/0x30 arch/x86/entry/entry_64.S:293
    
    The buggy address belongs to the object at ffff8880946c3800
     which belongs to the cache kmalloc-1k of size 1024
    The buggy address is located 484 bytes inside of
     1024-byte region [ffff8880946c3800, ffff8880946c3c00)
    The buggy address belongs to the page:
    page:ffffea000251b0c0 refcount:1 mapcount:0 mapping:0000000000000000 index:0x0
    flags: 0xfffe0000000200(slab)
    raw: 00fffe0000000200 ffffea0002546508 ffffea00024fa248 ffff8880aa000c40
    raw: 0000000000000000 ffff8880946c3000 0000000100000002 0000000000000000
    page dumped because: kasan: bad access detected
    
    Memory state around the buggy address:
     ffff8880946c3880: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
     ffff8880946c3900: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
    >ffff8880946c3980: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
                                                           ^
     ffff8880946c3a00: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
     ffff8880946c3a80: fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb fb
    ==================================================================
    
    Reported-by: syzbot+d3eccef36ddbd02713e9@syzkaller.appspotmail.com
    Fixes: 5ac0d62226a0 ("rxrpc: Fix missing notification")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index b7611cc159e5..032ed76c0166 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -22,6 +22,11 @@
 #include <net/ip.h>
 #include "ar-internal.h"
 
+static void rxrpc_dummy_notify(struct sock *sk, struct rxrpc_call *call,
+			       unsigned long user_call_ID)
+{
+}
+
 /*
  * Preallocate a single service call, connection and peer and, if possible,
  * give them a user ID and attach the user's side of the ID to them.
@@ -228,6 +233,8 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 		if (rx->discard_new_call) {
 			_debug("discard %lx", call->user_call_ID);
 			rx->discard_new_call(call, call->user_call_ID);
+			if (call->notify_rx)
+				call->notify_rx = rxrpc_dummy_notify;
 			rxrpc_put_call(call, rxrpc_call_put_kernel);
 		}
 		rxrpc_call_completed(call);

commit c410bf01933e5e09d142c66c3df9ad470a7eec13
Author: David Howells <dhowells@redhat.com>
Date:   Mon May 11 14:54:34 2020 +0100

    rxrpc: Fix the excessive initial retransmission timeout
    
    rxrpc currently uses a fixed 4s retransmission timeout until the RTT is
    sufficiently sampled.  This can cause problems with some fileservers with
    calls to the cache manager in the afs filesystem being dropped from the
    fileserver because a packet goes missing and the retransmission timeout is
    greater than the call expiry timeout.
    
    Fix this by:
    
     (1) Copying the RTT/RTO calculation code from Linux's TCP implementation
         and altering it to fit rxrpc.
    
     (2) Altering the various users of the RTT to make use of the new SRTT
         value.
    
     (3) Replacing the use of rxrpc_resend_timeout to use the calculated RTO
         value instead (which is needed in jiffies), along with a backoff.
    
    Notes:
    
     (1) rxrpc provides RTT samples by matching the serial numbers on outgoing
         DATA packets that have the RXRPC_REQUEST_ACK set and PING ACK packets
         against the reference serial number in incoming REQUESTED ACK and
         PING-RESPONSE ACK packets.
    
     (2) Each packet that is transmitted on an rxrpc connection gets a new
         per-connection serial number, even for retransmissions, so an ACK can
         be cross-referenced to a specific trigger packet.  This allows RTT
         information to be drawn from retransmitted DATA packets also.
    
     (3) rxrpc maintains the RTT/RTO state on the rxrpc_peer record rather than
         on an rxrpc_call because many RPC calls won't live long enough to
         generate more than one sample.
    
     (4) The calculated SRTT value is in units of 8ths of a microsecond rather
         than nanoseconds.
    
    The (S)RTT and RTO values are displayed in /proc/net/rxrpc/peers.
    
    Fixes: 17926a79320a ([AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both"")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 70e44abf106c..b7611cc159e5 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -248,7 +248,7 @@ static void rxrpc_send_ping(struct rxrpc_call *call, struct sk_buff *skb)
 	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
 	ktime_t now = skb->tstamp;
 
-	if (call->peer->rtt_usage < 3 ||
+	if (call->peer->rtt_count < 3 ||
 	    ktime_before(ktime_add_ms(call->peer->rtt_last_req, 1000), now))
 		rxrpc_propose_ACK(call, RXRPC_ACK_PING, sp->hdr.serial,
 				  true, true,

commit 063c60d39180cec7c9317f5acfc3071f8fecd705
Author: David Howells <dhowells@redhat.com>
Date:   Fri Dec 20 16:17:16 2019 +0000

    rxrpc: Fix missing security check on incoming calls
    
    Fix rxrpc_new_incoming_call() to check that we have a suitable service key
    available for the combination of service ID and security class of a new
    incoming call - and to reject calls for which we don't.
    
    This causes an assertion like the following to appear:
    
            rxrpc: Assertion failed - 6(0x6) == 12(0xc) is false
            kernel BUG at net/rxrpc/call_object.c:456!
    
    Where call->state is RXRPC_CALL_SERVER_SECURING (6) rather than
    RXRPC_CALL_COMPLETE (12).
    
    Fixes: 248f219cb8bc ("rxrpc: Rewrite the data and ack handling code")
    Reported-by: Marc Dionne <marc.dionne@auristor.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 44fa22b020ef..70e44abf106c 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -263,6 +263,8 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 						    struct rxrpc_local *local,
 						    struct rxrpc_peer *peer,
 						    struct rxrpc_connection *conn,
+						    const struct rxrpc_security *sec,
+						    struct key *key,
 						    struct sk_buff *skb)
 {
 	struct rxrpc_backlog *b = rx->backlog;
@@ -310,7 +312,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 		conn->params.local = rxrpc_get_local(local);
 		conn->params.peer = peer;
 		rxrpc_see_connection(conn);
-		rxrpc_new_incoming_connection(rx, conn, skb);
+		rxrpc_new_incoming_connection(rx, conn, sec, key, skb);
 	} else {
 		rxrpc_get_connection(conn);
 	}
@@ -349,9 +351,11 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 					   struct sk_buff *skb)
 {
 	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
+	const struct rxrpc_security *sec = NULL;
 	struct rxrpc_connection *conn;
 	struct rxrpc_peer *peer = NULL;
-	struct rxrpc_call *call;
+	struct rxrpc_call *call = NULL;
+	struct key *key = NULL;
 
 	_enter("");
 
@@ -372,7 +376,11 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	 */
 	conn = rxrpc_find_connection_rcu(local, skb, &peer);
 
-	call = rxrpc_alloc_incoming_call(rx, local, peer, conn, skb);
+	if (!conn && !rxrpc_look_up_server_security(local, rx, &sec, &key, skb))
+		goto no_call;
+
+	call = rxrpc_alloc_incoming_call(rx, local, peer, conn, sec, key, skb);
+	key_put(key);
 	if (!call) {
 		skb->mark = RXRPC_SKB_MARK_REJECT_BUSY;
 		goto no_call;

commit 13b7955a0252e15265386b229b814152f109b234
Author: David Howells <dhowells@redhat.com>
Date:   Fri Dec 20 16:20:56 2019 +0000

    rxrpc: Don't take call->user_mutex in rxrpc_new_incoming_call()
    
    Standard kernel mutexes cannot be used in any way from interrupt or softirq
    context, so the user_mutex which manages access to a call cannot be a mutex
    since on a new call the mutex must start off locked and be unlocked within
    the softirq handler to prevent userspace interfering with a call we're
    setting up.
    
    Commit a0855d24fc22d49cdc25664fb224caee16998683 ("locking/mutex: Complain
    upon mutex API misuse in IRQ contexts") causes big warnings to be splashed
    in dmesg for each a new call that comes in from the server.  Whilst it
    *seems* like it should be okay, since the accept path uses trylock, there
    are issues with PI boosting and marking the wrong task as the owner.
    
    Fix this by not taking the mutex in the softirq path at all.  It's not
    obvious that there should be any need for it as the state is set before the
    first notification is generated for the new call.
    
    There's also no particular reason why the link-assessing ping should be
    triggered inside the mutex.  It's not actually transmitted there anyway,
    but rather it has to be deferred to a workqueue.
    
    Further, I don't think that there's any particular reason that the socket
    notification needs to be done from within rx->incoming_lock, so the amount
    of time that lock is held can be shortened too and the ping prepared before
    the new call notification is sent.
    
    Fixes: 540b1c48c37a ("rxrpc: Fix deadlock between call creation and sendmsg/recvmsg")
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    cc: Ingo Molnar <mingo@redhat.com>
    cc: Will Deacon <will@kernel.org>
    cc: Davidlohr Bueso <dave@stgolabs.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 3685b1732f65..44fa22b020ef 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -381,18 +381,6 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	trace_rxrpc_receive(call, rxrpc_receive_incoming,
 			    sp->hdr.serial, sp->hdr.seq);
 
-	/* Lock the call to prevent rxrpc_kernel_send/recv_data() and
-	 * sendmsg()/recvmsg() inconveniently stealing the mutex once the
-	 * notification is generated.
-	 *
-	 * The BUG should never happen because the kernel should be well
-	 * behaved enough not to access the call before the first notification
-	 * event and userspace is prevented from doing so until the state is
-	 * appropriate.
-	 */
-	if (!mutex_trylock(&call->user_mutex))
-		BUG();
-
 	/* Make the call live. */
 	rxrpc_incoming_call(rx, call, skb);
 	conn = call->conn;
@@ -433,6 +421,9 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 		BUG();
 	}
 	spin_unlock(&conn->state_lock);
+	spin_unlock(&rx->incoming_lock);
+
+	rxrpc_send_ping(call, skb);
 
 	if (call->state == RXRPC_CALL_SERVER_ACCEPTING)
 		rxrpc_notify_socket(call);
@@ -444,11 +435,6 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	 */
 	rxrpc_put_call(call, rxrpc_call_put);
 
-	spin_unlock(&rx->incoming_lock);
-
-	rxrpc_send_ping(call, skb);
-	mutex_unlock(&call->user_mutex);
-
 	_leave(" = %p{%d}", call, call->debug_id);
 	return call;
 

commit f33121cbe91973a08e68e4bde8c3f7e6e4e351c1
Author: David Howells <dhowells@redhat.com>
Date:   Wed Dec 18 16:38:49 2019 +0000

    rxrpc: Unlock new call in rxrpc_new_incoming_call() rather than the caller
    
    Move the unlock and the ping transmission for a new incoming call into
    rxrpc_new_incoming_call() rather than doing it in the caller.  This makes
    it clearer to see what's going on.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    cc: Ingo Molnar <mingo@redhat.com>
    cc: Will Deacon <will@kernel.org>
    cc: Davidlohr Bueso <dave@stgolabs.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 135bf5cd8dd5..3685b1732f65 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -239,6 +239,22 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 	kfree(b);
 }
 
+/*
+ * Ping the other end to fill our RTT cache and to retrieve the rwind
+ * and MTU parameters.
+ */
+static void rxrpc_send_ping(struct rxrpc_call *call, struct sk_buff *skb)
+{
+	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
+	ktime_t now = skb->tstamp;
+
+	if (call->peer->rtt_usage < 3 ||
+	    ktime_before(ktime_add_ms(call->peer->rtt_last_req, 1000), now))
+		rxrpc_propose_ACK(call, RXRPC_ACK_PING, sp->hdr.serial,
+				  true, true,
+				  rxrpc_propose_ack_ping_for_params);
+}
+
 /*
  * Allocate a new incoming call from the prealloc pool, along with a connection
  * and a peer as necessary.
@@ -346,9 +362,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 				  sp->hdr.seq, RX_INVALID_OPERATION, ESHUTDOWN);
 		skb->mark = RXRPC_SKB_MARK_REJECT_ABORT;
 		skb->priority = RX_INVALID_OPERATION;
-		_leave(" = NULL [close]");
-		call = NULL;
-		goto out;
+		goto no_call;
 	}
 
 	/* The peer, connection and call may all have sprung into existence due
@@ -361,9 +375,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	call = rxrpc_alloc_incoming_call(rx, local, peer, conn, skb);
 	if (!call) {
 		skb->mark = RXRPC_SKB_MARK_REJECT_BUSY;
-		_leave(" = NULL [busy]");
-		call = NULL;
-		goto out;
+		goto no_call;
 	}
 
 	trace_rxrpc_receive(call, rxrpc_receive_incoming,
@@ -432,10 +444,18 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	 */
 	rxrpc_put_call(call, rxrpc_call_put);
 
-	_leave(" = %p{%d}", call, call->debug_id);
-out:
 	spin_unlock(&rx->incoming_lock);
+
+	rxrpc_send_ping(call, skb);
+	mutex_unlock(&call->user_mutex);
+
+	_leave(" = %p{%d}", call, call->debug_id);
 	return call;
+
+no_call:
+	spin_unlock(&rx->incoming_lock);
+	_leave(" = NULL [%u]", skb->mark);
+	return NULL;
 }
 
 /*

commit 91fcfbe8852edb929ff8702534525031a15d0aa6
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 7 10:58:29 2019 +0100

    rxrpc: Fix call crypto state cleanup
    
    Fix the cleanup of the crypto state on a call after the call has been
    disconnected.  As the call has been disconnected, its connection ref has
    been discarded and so we can't go through that to get to the security ops
    table.
    
    Fix this by caching the security ops pointer in the rxrpc_call struct and
    using that when freeing the call security state.  Also use this in other
    places we're dealing with call-specific security.
    
    The symptoms look like:
    
        BUG: KASAN: use-after-free in rxrpc_release_call+0xb2d/0xb60
        net/rxrpc/call_object.c:481
        Read of size 8 at addr ffff888062ffeb50 by task syz-executor.5/4764
    
    Fixes: 1db88c534371 ("rxrpc: Fix -Wframe-larger-than= warnings from on-stack crypto")
    Reported-by: syzbot+eed305768ece6682bb7f@syzkaller.appspotmail.com
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 1f778102ed8d..135bf5cd8dd5 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -307,6 +307,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 
 	rxrpc_see_call(call);
 	call->conn = conn;
+	call->security = conn->security;
 	call->peer = rxrpc_get_peer(conn->params.peer);
 	call->cong_cwnd = call->peer->cong_cwnd;
 	return call;

commit 48c9e0ec7cbbb7370448f859ccc8e3b7eb69e755
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 7 10:58:29 2019 +0100

    rxrpc: Fix trace-after-put looking at the put call record
    
    rxrpc_put_call() calls trace_rxrpc_call() after it has done the decrement
    of the refcount - which looks at the debug_id in the call record.  But
    unless the refcount was reduced to zero, we no longer have the right to
    look in the record and, indeed, it may be deleted by some other thread.
    
    Fix this by getting the debug_id out before decrementing the refcount and
    then passing that into the tracepoint.
    
    Fixes: e34d4234b0b7 ("rxrpc: Trace rxrpc_call usage")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index c1b1b7dd2924..1f778102ed8d 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -97,7 +97,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 	call->flags |= (1 << RXRPC_CALL_IS_SERVICE);
 	call->state = RXRPC_CALL_SERVER_PREALLOC;
 
-	trace_rxrpc_call(call, rxrpc_call_new_service,
+	trace_rxrpc_call(call->debug_id, rxrpc_call_new_service,
 			 atomic_read(&call->usage),
 			 here, (const void *)user_call_ID);
 

commit 4c1295dccc0afe0905b6ca4c62ade7f2406f2cfb
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 7 10:58:29 2019 +0100

    rxrpc: Fix trace-after-put looking at the put connection record
    
    rxrpc_put_*conn() calls trace_rxrpc_conn() after they have done the
    decrement of the refcount - which looks at the debug_id in the connection
    record.  But unless the refcount was reduced to zero, we no longer have the
    right to look in the record and, indeed, it may be deleted by some other
    thread.
    
    Fix this by getting the debug_id out before decrementing the refcount and
    then passing that into the tracepoint.
    
    Fixes: 363deeab6d0f ("rxrpc: Add connection tracepoint and client conn state tracepoint")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 00c095d74145..c1b1b7dd2924 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -84,7 +84,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 		smp_store_release(&b->conn_backlog_head,
 				  (head + 1) & (size - 1));
 
-		trace_rxrpc_conn(conn, rxrpc_conn_new_service,
+		trace_rxrpc_conn(conn->debug_id, rxrpc_conn_new_service,
 				 atomic_read(&conn->usage), here);
 	}
 

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 44860505246d..00c095d74145 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -1,12 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /* incoming call handling
  *
  * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
  * Written by David Howells (dhowells@redhat.com)
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

commit 2e2d6f0342be7f73a34526077fa96f42f0e8c661
Merge: 9333f2079203 48995423143a
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 19 11:03:06 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    net/sched/cls_api.c has overlapping changes to a call to
    nlmsg_parse(), one (from 'net') added rtm_tca_policy instead of NULL
    to the 5th argument, and another (from 'net-next') added cb->extack
    instead of NULL to the 6th argument.
    
    net/ipv4/ipmr_base.c is a case of a bug fix in 'net' being done to
    code which moved (to mr_table_dump)) in 'net-next'.  Thanks to David
    Ahern for the heads up.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit d7b4c24f45d2efe51b8f213da4593fefd49240ba
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 11 22:32:31 2018 +0100

    rxrpc: Fix an uninitialised variable
    
    Fix an uninitialised variable introduced by the last patch.  This can cause
    a crash when a new call comes in to a local service, such as when an AFS
    fileserver calls back to the local cache manager.
    
    Fixes: c1e15b4944c9 ("rxrpc: Fix the packet reception routine")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 652e314de38e..8079aacaecac 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -337,7 +337,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 {
 	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
 	struct rxrpc_connection *conn;
-	struct rxrpc_peer *peer;
+	struct rxrpc_peer *peer = NULL;
 	struct rxrpc_call *call;
 
 	_enter("");

commit d864991b220b7c62e81d21209e1fd978fd67352c
Merge: a688c53a0277 bab5c80b2110
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Oct 12 21:38:46 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts were easy to resolve using immediate context mostly,
    except the cls_u32.c one where I simply too the entire HEAD
    chunk.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c1e15b4944c9fa7fbbb74f7a5920a1e31b4b965a
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 8 15:46:25 2018 +0100

    rxrpc: Fix the packet reception routine
    
    The rxrpc_input_packet() function and its call tree was built around the
    assumption that data_ready() handler called from UDP to inform a kernel
    service that there is data to be had was non-reentrant.  This means that
    certain locking could be dispensed with.
    
    This, however, turns out not to be the case with a multi-queue network card
    that can deliver packets to multiple cpus simultaneously.  Each of those
    cpus can be in the rxrpc_input_packet() function at the same time.
    
    Fix by adding or changing some structure members:
    
     (1) Add peer->rtt_input_lock to serialise access to the RTT buffer.
    
     (2) Make conn->service_id into a 32-bit variable so that it can be
         cmpxchg'd on all arches.
    
     (3) Add call->input_lock to serialise access to the Rx/Tx state.  Note
         that although the Rx and Tx states are (almost) entirely separate,
         there's no point completing the separation and having separate locks
         since it's a bi-phasal RPC protocol rather than a bi-direction
         streaming protocol.  Data transmission and data reception do not take
         place simultaneously on any particular call.
    
    and making the following functional changes:
    
     (1) In rxrpc_input_data(), hold call->input_lock around the core to
         prevent simultaneous producing of packets into the Rx ring and
         updating of tracking state for a particular call.
    
     (2) In rxrpc_input_ping_response(), only read call->ping_serial once, and
         check it before checking RXRPC_CALL_PINGING as that's a cheaper test.
         The bit test and bit clear can then be combined.  No further locking
         is needed here.
    
     (3) In rxrpc_input_ack(), take call->input_lock after we've parsed much of
         the ACK packet.  The superseded ACK check is then done both before and
         after the lock is taken.
    
         The handing of ackinfo data is split, parsing before the lock is taken
         and processing with it held.  This is keyed on rxMTU being non-zero.
    
         Congestion management is also done within the locked section.
    
     (4) In rxrpc_input_ackall(), take call->input_lock around the Tx window
         rotation.  The ACKALL packet carries no information and is only really
         useful after all packets have been transmitted since it's imprecise.
    
     (5) In rxrpc_input_implicit_end_call(), we use rx->incoming_lock to
         prevent calls being simultaneously implicitly ended on two cpus and
         also to prevent any races with incoming call setup.
    
     (6) In rxrpc_input_packet(), use cmpxchg() to effect the service upgrade
         on a connection.  It is only permitted to happen once for a
         connection.
    
     (7) In rxrpc_new_incoming_call(), we have to recheck the routing inside
         rx->incoming_lock to see if someone else set up the call, connection
         or peer whilst we were getting there.  We can't trust the values from
         the earlier routing check unless we pin refs on them - which we want
         to avoid.
    
         Further, we need to allow for an incoming call to have its state
         changed on another CPU between us making it live and us adjusting it
         because the conn is now in the RXRPC_CONN_SERVICE state.
    
     (8) In rxrpc_peer_add_rtt(), take peer->rtt_input_lock around the access
         to the RTT buffer.  Don't need to lock around setting peer->rtt.
    
    For reference, the inventory of state-accessing or state-altering functions
    used by the packet input procedure is:
    
    > rxrpc_input_packet()
      * PACKET CHECKING
    
      * ROUTING
        > rxrpc_post_packet_to_local()
        > rxrpc_find_connection_rcu() - uses RCU
          > rxrpc_lookup_peer_rcu() - uses RCU
          > rxrpc_find_service_conn_rcu() - uses RCU
          > idr_find() - uses RCU
    
      * CONNECTION-LEVEL PROCESSING
        - Service upgrade
          - Can only happen once per conn
          ! Changed to use cmpxchg
        > rxrpc_post_packet_to_conn()
        - Setting conn->hi_serial
          - Probably safe not using locks
          - Maybe use cmpxchg
    
      * CALL-LEVEL PROCESSING
        > Old-call checking
          > rxrpc_input_implicit_end_call()
            > rxrpc_call_completed()
            > rxrpc_queue_call()
            ! Need to take rx->incoming_lock
            > __rxrpc_disconnect_call()
            > rxrpc_notify_socket()
        > rxrpc_new_incoming_call()
          - Uses rx->incoming_lock for the entire process
            - Might be able to drop this earlier in favour of the call lock
          > rxrpc_incoming_call()
            ! Conflicts with rxrpc_input_implicit_end_call()
        > rxrpc_send_ping()
          - Don't need locks to check rtt state
          > rxrpc_propose_ACK
    
      * PACKET DISTRIBUTION
        > rxrpc_input_call_packet()
          > rxrpc_input_data()
            * QUEUE DATA PACKET ON CALL
            > rxrpc_reduce_call_timer()
              - Uses timer_reduce()
            ! Needs call->input_lock()
            > rxrpc_receiving_reply()
              ! Needs locking around ack state
              > rxrpc_rotate_tx_window()
              > rxrpc_end_tx_phase()
            > rxrpc_proto_abort()
            > rxrpc_input_dup_data()
            - Fills the Rx buffer
            - rxrpc_propose_ACK()
            - rxrpc_notify_socket()
    
          > rxrpc_input_ack()
            * APPLY ACK PACKET TO CALL AND DISCARD PACKET
            > rxrpc_input_ping_response()
              - Probably doesn't need any extra locking
              ! Need READ_ONCE() on call->ping_serial
              > rxrpc_input_check_for_lost_ack()
                - Takes call->lock to consult Tx buffer
              > rxrpc_peer_add_rtt()
                ! Needs to take a lock (peer->rtt_input_lock)
                ! Could perhaps manage with cmpxchg() and xadd() instead
            > rxrpc_input_requested_ack
              - Consults Tx buffer
                ! Probably needs a lock
              > rxrpc_peer_add_rtt()
            > rxrpc_propose_ack()
            > rxrpc_input_ackinfo()
              - Changes call->tx_winsize
                ! Use cmpxchg to handle change
                ! Should perhaps track serial number
              - Uses peer->lock to record MTU specification changes
            > rxrpc_proto_abort()
            ! Need to take call->input_lock
            > rxrpc_rotate_tx_window()
            > rxrpc_end_tx_phase()
            > rxrpc_input_soft_acks()
            - Consults the Tx buffer
            > rxrpc_congestion_management()
              - Modifies the Tx annotations
              ! Needs call->input_lock()
              > rxrpc_queue_call()
    
          > rxrpc_input_abort()
            * APPLY ABORT PACKET TO CALL AND DISCARD PACKET
            > rxrpc_set_call_completion()
            > rxrpc_notify_socket()
    
          > rxrpc_input_ackall()
            * APPLY ACKALL PACKET TO CALL AND DISCARD PACKET
            ! Need to take call->input_lock
            > rxrpc_rotate_tx_window()
            > rxrpc_end_tx_phase()
    
        > rxrpc_reject_packet()
    
    There are some functions used by the above that queue the packet, after
    which the procedure is terminated:
    
     - rxrpc_post_packet_to_local()
       - local->event_queue is an sk_buff_head
       - local->processor is a work_struct
     - rxrpc_post_packet_to_conn()
       - conn->rx_queue is an sk_buff_head
       - conn->processor is a work_struct
     - rxrpc_reject_packet()
       - local->reject_queue is an sk_buff_head
       - local->processor is a work_struct
    
    And some that offload processing to process context:
    
     - rxrpc_notify_socket()
       - Uses RCU lock
       - Uses call->notify_lock to call call->notify_rx
       - Uses call->recvmsg_lock to queue recvmsg side
     - rxrpc_queue_call()
       - call->processor is a work_struct
     - rxrpc_propose_ACK()
       - Uses call->lock to wrap __rxrpc_propose_ACK()
    
    And a bunch that complete a call, all of which use call->state_lock to
    protect the call state:
    
     - rxrpc_call_completed()
     - rxrpc_set_call_completion()
     - rxrpc_abort_call()
     - rxrpc_proto_abort()
       - Also uses rxrpc_queue_call()
    
    Fixes: 17926a79320a ("[AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 1c4ebc0cb25b..652e314de38e 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -333,11 +333,11 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
  */
 struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 					   struct rxrpc_sock *rx,
-					   struct rxrpc_peer *peer,
-					   struct rxrpc_connection *conn,
 					   struct sk_buff *skb)
 {
 	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
+	struct rxrpc_connection *conn;
+	struct rxrpc_peer *peer;
 	struct rxrpc_call *call;
 
 	_enter("");
@@ -354,6 +354,13 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 		goto out;
 	}
 
+	/* The peer, connection and call may all have sprung into existence due
+	 * to a duplicate packet being handled on another CPU in parallel, so
+	 * we have to recheck the routing.  However, we're now holding
+	 * rx->incoming_lock, so the values should remain stable.
+	 */
+	conn = rxrpc_find_connection_rcu(local, skb, &peer);
+
 	call = rxrpc_alloc_incoming_call(rx, local, peer, conn, skb);
 	if (!call) {
 		skb->mark = RXRPC_SKB_MARK_REJECT_BUSY;
@@ -396,10 +403,12 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	case RXRPC_CONN_SERVICE:
 		write_lock(&call->state_lock);
-		if (rx->discard_new_call)
-			call->state = RXRPC_CALL_SERVER_RECV_REQUEST;
-		else
-			call->state = RXRPC_CALL_SERVER_ACCEPTING;
+		if (call->state < RXRPC_CALL_COMPLETE) {
+			if (rx->discard_new_call)
+				call->state = RXRPC_CALL_SERVER_RECV_REQUEST;
+			else
+				call->state = RXRPC_CALL_SERVER_ACCEPTING;
+		}
 		write_unlock(&call->state_lock);
 		break;
 

commit 647530924f47c93db472ee3cf43b7ef1425581b6
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 8 15:46:17 2018 +0100

    rxrpc: Fix connection-level abort handling
    
    Fix connection-level abort handling to cache the abort and error codes
    properly so that a new incoming call can be properly aborted if it races
    with the parent connection being aborted by another CPU.
    
    The abort_code and error parameters can then be dropped from
    rxrpc_abort_calls().
    
    Fixes: f5c17aaeb2ae ("rxrpc: Calls should only have one terminal state")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index f55f67894465..1c4ebc0cb25b 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -405,11 +405,11 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	case RXRPC_CONN_REMOTELY_ABORTED:
 		rxrpc_set_call_completion(call, RXRPC_CALL_REMOTELY_ABORTED,
-					  conn->remote_abort, -ECONNABORTED);
+					  conn->abort_code, conn->error);
 		break;
 	case RXRPC_CONN_LOCALLY_ABORTED:
 		rxrpc_abort_call("CON", call, sp->hdr.seq,
-				 conn->local_abort, -ECONNABORTED);
+				 conn->abort_code, conn->error);
 		break;
 	default:
 		BUG();

commit 5e33a23ba4b56c109b732d57a0a76558a37d9ec5
Author: David Howells <dhowells@redhat.com>
Date:   Fri Oct 5 14:05:34 2018 +0100

    rxrpc: Fix some missed refs to init_net
    
    Fix some refs to init_net that should've been changed to the appropriate
    network namespace.
    
    Fixes: 2baec2c3f854 ("rxrpc: Support network namespacing")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Paolo Abeni <pabeni@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 9c7f26d06a52..f55f67894465 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -287,7 +287,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 					  (peer_tail + 1) &
 					  (RXRPC_BACKLOG_MAX - 1));
 
-			rxrpc_new_incoming_peer(local, peer);
+			rxrpc_new_incoming_peer(rx, local, peer);
 		}
 
 		/* Now allocate and set up the connection */

commit 5a790b7375414cffb0f7e8ab0f175d2e02a0af0e
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 4 09:32:28 2018 +0100

    rxrpc: Drop the local endpoint arg from rxrpc_extract_addr_from_skb()
    
    rxrpc_extract_addr_from_skb() doesn't use the argument that points to the
    local endpoint, so remove the argument.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 9c7f26d06a52..8354cadbb839 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -280,7 +280,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 			peer = NULL;
 		if (!peer) {
 			peer = b->peer_backlog[peer_tail];
-			if (rxrpc_extract_addr_from_skb(local, &peer->srx, skb) < 0)
+			if (rxrpc_extract_addr_from_skb(&peer->srx, skb) < 0)
 				return NULL;
 			b->peer_backlog[peer_tail] = NULL;
 			smp_store_release(&b->peer_backlog_tail,

commit 0099dc589bfa7caf6f2608c4cbc1181cfee22b0c
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 27 15:13:09 2018 +0100

    rxrpc: Make service call handling more robust
    
    Make the following changes to improve the robustness of the code that sets
    up a new service call:
    
     (1) Cache the rxrpc_sock struct obtained in rxrpc_data_ready() to do a
         service ID check and pass that along to rxrpc_new_incoming_call().
         This means that I can remove the check from rxrpc_new_incoming_call()
         without the need to worry about the socket attached to the local
         endpoint getting replaced - which would invalidate the check.
    
     (2) Cache the rxrpc_peer struct, thereby allowing the peer search to be
         done once.  The peer is passed to rxrpc_new_incoming_call(), thereby
         saving the need to repeat the search.
    
         This also reduces the possibility of rxrpc_publish_service_conn()
         BUG()'ing due to the detection of a duplicate connection, despite the
         initial search done by rxrpc_find_connection_rcu() having turned up
         nothing.
    
         This BUG() shouldn't ever get hit since rxrpc_data_ready() *should* be
         non-reentrant and the result of the initial search should still hold
         true, but it has proven possible to hit.
    
         I *think* this may be due to __rxrpc_lookup_peer_rcu() cutting short
         the iteration over the hash table if it finds a matching peer with a
         zero usage count, but I don't know for sure since it's only ever been
         hit once that I know of.
    
         Another possibility is that a bug in rxrpc_data_ready() that checked
         the wrong byte in the header for the RXRPC_CLIENT_INITIATED flag
         might've let through a packet that caused a spurious and invalid call
         to be set up.  That is addressed in another patch.
    
     (3) Fix __rxrpc_lookup_peer_rcu() to skip peer records that have a zero
         usage count rather than stopping and returning not found, just in case
         there's another peer record behind it in the bucket.
    
     (4) Don't search the peer records in rxrpc_alloc_incoming_call(), but
         rather either use the peer cached in (2) or, if one wasn't found,
         preemptively install a new one.
    
    Fixes: 8496af50eb38 ("rxrpc: Use RCU to access a peer's service connection tree")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index e88f131c1d7f..9c7f26d06a52 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -249,11 +249,11 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
  */
 static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 						    struct rxrpc_local *local,
+						    struct rxrpc_peer *peer,
 						    struct rxrpc_connection *conn,
 						    struct sk_buff *skb)
 {
 	struct rxrpc_backlog *b = rx->backlog;
-	struct rxrpc_peer *peer, *xpeer;
 	struct rxrpc_call *call;
 	unsigned short call_head, conn_head, peer_head;
 	unsigned short call_tail, conn_tail, peer_tail;
@@ -276,21 +276,18 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 		return NULL;
 
 	if (!conn) {
-		/* No connection.  We're going to need a peer to start off
-		 * with.  If one doesn't yet exist, use a spare from the
-		 * preallocation set.  We dump the address into the spare in
-		 * anticipation - and to save on stack space.
-		 */
-		xpeer = b->peer_backlog[peer_tail];
-		if (rxrpc_extract_addr_from_skb(local, &xpeer->srx, skb) < 0)
-			return NULL;
-
-		peer = rxrpc_lookup_incoming_peer(local, xpeer);
-		if (peer == xpeer) {
+		if (peer && !rxrpc_get_peer_maybe(peer))
+			peer = NULL;
+		if (!peer) {
+			peer = b->peer_backlog[peer_tail];
+			if (rxrpc_extract_addr_from_skb(local, &peer->srx, skb) < 0)
+				return NULL;
 			b->peer_backlog[peer_tail] = NULL;
 			smp_store_release(&b->peer_backlog_tail,
 					  (peer_tail + 1) &
 					  (RXRPC_BACKLOG_MAX - 1));
+
+			rxrpc_new_incoming_peer(local, peer);
 		}
 
 		/* Now allocate and set up the connection */
@@ -335,30 +332,16 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
  * The call is returned with the user access mutex held.
  */
 struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
+					   struct rxrpc_sock *rx,
+					   struct rxrpc_peer *peer,
 					   struct rxrpc_connection *conn,
 					   struct sk_buff *skb)
 {
 	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
-	struct rxrpc_sock *rx;
 	struct rxrpc_call *call;
-	u16 service_id = sp->hdr.serviceId;
 
 	_enter("");
 
-	/* Get the socket providing the service */
-	rx = rcu_dereference(local->service);
-	if (rx && (service_id == rx->srx.srx_service ||
-		   service_id == rx->second_service))
-		goto found_service;
-
-	trace_rxrpc_abort(0, "INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,
-			  RX_INVALID_OPERATION, EOPNOTSUPP);
-	skb->mark = RXRPC_SKB_MARK_REJECT_ABORT;
-	skb->priority = RX_INVALID_OPERATION;
-	_leave(" = NULL [service]");
-	return NULL;
-
-found_service:
 	spin_lock(&rx->incoming_lock);
 	if (rx->sk.sk_state == RXRPC_SERVER_LISTEN_DISABLED ||
 	    rx->sk.sk_state == RXRPC_CLOSE) {
@@ -371,7 +354,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 		goto out;
 	}
 
-	call = rxrpc_alloc_incoming_call(rx, local, conn, skb);
+	call = rxrpc_alloc_incoming_call(rx, local, peer, conn, skb);
 	if (!call) {
 		skb->mark = RXRPC_SKB_MARK_REJECT_BUSY;
 		_leave(" = NULL [busy]");

commit ece64fec164f523bfbe874abdef2a0e6ff376251
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 27 15:13:08 2018 +0100

    rxrpc: Emit BUSY packets when supposed to rather than ABORTs
    
    In the input path, a received sk_buff can be marked for rejection by
    setting RXRPC_SKB_MARK_* in skb->mark and, if needed, some auxiliary data
    (such as an abort code) in skb->priority.  The rejection is handled by
    queueing the sk_buff up for dealing with in process context.  The output
    code reads the mark and priority and, theoretically, generates an
    appropriate response packet.
    
    However, if RXRPC_SKB_MARK_BUSY is set, this isn't noticed and an ABORT
    message with a random abort code is generated (since skb->priority wasn't
    set to anything).
    
    Fix this by outputting the appropriate sort of packet.
    
    Also, whilst we're at it, most of the marks are no longer used, so remove
    them and rename the remaining two to something more obvious.
    
    Fixes: 248f219cb8bc ("rxrpc: Rewrite the data and ack handling code")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 9d1e298b784c..e88f131c1d7f 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -353,7 +353,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	trace_rxrpc_abort(0, "INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,
 			  RX_INVALID_OPERATION, EOPNOTSUPP);
-	skb->mark = RXRPC_SKB_MARK_LOCAL_ABORT;
+	skb->mark = RXRPC_SKB_MARK_REJECT_ABORT;
 	skb->priority = RX_INVALID_OPERATION;
 	_leave(" = NULL [service]");
 	return NULL;
@@ -364,7 +364,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	    rx->sk.sk_state == RXRPC_CLOSE) {
 		trace_rxrpc_abort(0, "CLS", sp->hdr.cid, sp->hdr.callNumber,
 				  sp->hdr.seq, RX_INVALID_OPERATION, ESHUTDOWN);
-		skb->mark = RXRPC_SKB_MARK_LOCAL_ABORT;
+		skb->mark = RXRPC_SKB_MARK_REJECT_ABORT;
 		skb->priority = RX_INVALID_OPERATION;
 		_leave(" = NULL [close]");
 		call = NULL;
@@ -373,7 +373,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	call = rxrpc_alloc_incoming_call(rx, local, conn, skb);
 	if (!call) {
-		skb->mark = RXRPC_SKB_MARK_BUSY;
+		skb->mark = RXRPC_SKB_MARK_REJECT_BUSY;
 		_leave(" = NULL [busy]");
 		call = NULL;
 		goto out;

commit c01f6c9b3207e52fc9973a066a856ddf7a0538d8
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Wed Aug 1 13:27:23 2018 +0100

    rxrpc: Fix user call ID check in rxrpc_service_prealloc_one
    
    There just check the user call ID isn't already in use, hence should
    compare user_call_ID with xcall->user_call_ID, which is current
    node's user_call_ID.
    
    Fixes: 540b1c48c37a ("rxrpc: Fix deadlock between call creation and sendmsg/recvmsg")
    Suggested-by: David Howells <dhowells@redhat.com>
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index a9a9be5519b9..9d1e298b784c 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -116,9 +116,9 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 		while (*pp) {
 			parent = *pp;
 			xcall = rb_entry(parent, struct rxrpc_call, sock_node);
-			if (user_call_ID < call->user_call_ID)
+			if (user_call_ID < xcall->user_call_ID)
 				pp = &(*pp)->rb_left;
-			else if (user_call_ID > call->user_call_ID)
+			else if (user_call_ID > xcall->user_call_ID)
 				pp = &(*pp)->rb_right;
 			else
 				goto id_in_use;

commit 5bb053bef82523a8fd78d650bca81c9f114fa276
Merge: bb2407a72197 159f02977b2f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 3 14:04:18 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Support offloading wireless authentication to userspace via
        NL80211_CMD_EXTERNAL_AUTH, from Srinivas Dasari.
    
     2) A lot of work on network namespace setup/teardown from Kirill Tkhai.
        Setup and cleanup of namespaces now all run asynchronously and thus
        performance is significantly increased.
    
     3) Add rx/tx timestamping support to mv88e6xxx driver, from Brandon
        Streiff.
    
     4) Support zerocopy on RDS sockets, from Sowmini Varadhan.
    
     5) Use denser instruction encoding in x86 eBPF JIT, from Daniel
        Borkmann.
    
     6) Support hw offload of vlan filtering in mvpp2 dreiver, from Maxime
        Chevallier.
    
     7) Support grafting of child qdiscs in mlxsw driver, from Nogah
        Frankel.
    
     8) Add packet forwarding tests to selftests, from Ido Schimmel.
    
     9) Deal with sub-optimal GSO packets better in BBR congestion control,
        from Eric Dumazet.
    
    10) Support 5-tuple hashing in ipv6 multipath routing, from David Ahern.
    
    11) Add path MTU tests to selftests, from Stefano Brivio.
    
    12) Various bits of IPSEC offloading support for mlx5, from Aviad
        Yehezkel, Yossi Kuperman, and Saeed Mahameed.
    
    13) Support RSS spreading on ntuple filters in SFC driver, from Edward
        Cree.
    
    14) Lots of sockmap work from John Fastabend. Applications can use eBPF
        to filter sendmsg and sendpage operations.
    
    15) In-kernel receive TLS support, from Dave Watson.
    
    16) Add XDP support to ixgbevf, this is significant because it should
        allow optimized XDP usage in various cloud environments. From Tony
        Nguyen.
    
    17) Add new Intel E800 series "ice" ethernet driver, from Anirudh
        Venkataramanan et al.
    
    18) IP fragmentation match offload support in nfp driver, from Pieter
        Jansen van Vuuren.
    
    19) Support XDP redirect in i40e driver, from Björn Töpel.
    
    20) Add BPF_RAW_TRACEPOINT program type for accessing the arguments of
        tracepoints in their raw form, from Alexei Starovoitov.
    
    21) Lots of striding RQ improvements to mlx5 driver with many
        performance improvements, from Tariq Toukan.
    
    22) Use rhashtable for inet frag reassembly, from Eric Dumazet.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1678 commits)
      net: mvneta: improve suspend/resume
      net: mvneta: split rxq/txq init and txq deinit into SW and HW parts
      ipv6: frags: fix /proc/sys/net/ipv6/ip6frag_low_thresh
      net: bgmac: Fix endian access in bgmac_dma_tx_ring_free()
      net: bgmac: Correctly annotate register space
      route: check sysctl_fib_multipath_use_neigh earlier than hash
      fix typo in command value in drivers/net/phy/mdio-bitbang.
      sky2: Increase D3 delay to sky2 stops working after suspend
      net/mlx5e: Set EQE based as default TX interrupt moderation mode
      ibmvnic: Disable irqs before exiting reset from closed state
      net: sched: do not emit messages while holding spinlock
      vlan: also check phy_driver ts_info for vlan's real device
      Bluetooth: Mark expected switch fall-throughs
      Bluetooth: Set HCI_QUIRK_SIMULTANEOUS_DISCOVERY for BTUSB_QCA_ROME
      Bluetooth: btrsi: remove unused including <linux/version.h>
      Bluetooth: hci_bcm: Remove DMI quirk for the MINIX Z83-4
      sh_eth: kill useless check in __sh_eth_get_regs()
      sh_eth: add sh_eth_cpu_data::no_xdfar flag
      ipv6: factorize sk_wmem_alloc updates done by __ip6_append_data()
      ipv4: factorize sk_wmem_alloc updates done by __ip_append_data()
      ...

commit 31f5f9a1691ebef2113c8bdb3edcb8859f30f702
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 30 21:05:33 2018 +0100

    rxrpc: Fix apparent leak of rxrpc_local objects
    
    rxrpc_local objects cannot be disposed of until all the connections that
    point to them have been RCU'd as a connection object holds refcount on the
    local endpoint it is communicating through.  Currently, this can cause an
    assertion failure to occur when a network namespace is destroyed as there's
    no check that the RCU destructors for the connections have been run before
    we start trying to destroy local endpoints.
    
    The kernel reports:
    
            rxrpc: AF_RXRPC: Leaked local 0000000036a41bc1 {5}
            ------------[ cut here ]------------
            kernel BUG at ../net/rxrpc/local_object.c:439!
    
    Fix this by keeping a count of the live connections and waiting for it to
    go to zero at the end of rxrpc_destroy_all_connections().
    
    Fixes: dee46364ce6f ("rxrpc: Add RCU destruction for connections and calls")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 5a9b1d916124..f67017dcb25e 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -219,6 +219,8 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 		list_del(&conn->proc_link);
 		write_unlock(&rxnet->conn_lock);
 		kfree(conn);
+		if (atomic_dec_and_test(&rxnet->nr_conns))
+			wake_up_atomic_t(&rxnet->nr_conns);
 		tail = (tail + 1) & (size - 1);
 	}
 

commit 09d2bf595db4b4075ea721acd61e180d6bb18f88
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 30 21:05:28 2018 +0100

    rxrpc: Add a tracepoint to track rxrpc_local refcounting
    
    Add a tracepoint to track reference counting on the rxrpc_local struct.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 493545033e42..5a9b1d916124 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -296,8 +296,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 		b->conn_backlog[conn_tail] = NULL;
 		smp_store_release(&b->conn_backlog_tail,
 				  (conn_tail + 1) & (RXRPC_BACKLOG_MAX - 1));
-		rxrpc_get_local(local);
-		conn->params.local = local;
+		conn->params.local = rxrpc_get_local(local);
 		conn->params.peer = peer;
 		rxrpc_see_connection(conn);
 		rxrpc_new_incoming_connection(rx, conn, skb);

commit d3be4d244330f7ef53242d8dc1b7f77d105e767f
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 30 21:05:23 2018 +0100

    rxrpc: Fix potential call vs socket/net destruction race
    
    rxrpc_call structs don't pin sockets or network namespaces, but may attempt
    to access both after their refcount reaches 0 so that they can detach
    themselves from the network namespace.  However, there's no guarantee that
    the socket still exists at this point (so sock_net(&call->socket->sk) may
    be invalid) and the namespace may have gone away if the call isn't pinning
    a peer.
    
    Fix this by (a) carrying a net pointer in the rxrpc_call struct and (b)
    waiting for all calls to be destroyed when the network namespace goes away.
    
    This was detected by checker:
    
    net/rxrpc/call_object.c:634:57: warning: incorrect type in argument 1 (different address spaces)
    net/rxrpc/call_object.c:634:57:    expected struct sock const *sk
    net/rxrpc/call_object.c:634:57:    got struct sock [noderef] <asn:4>*<noident>
    
    Fixes: 2baec2c3f854 ("rxrpc: Support network namespacing")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 4ce24c000653..493545033e42 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -138,6 +138,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 
 	write_unlock(&rx->call_lock);
 
+	rxnet = call->rxnet;
 	write_lock(&rxnet->call_lock);
 	list_add_tail(&call->link, &rxnet->calls);
 	write_unlock(&rxnet->call_lock);

commit 88f2a8257c9aa7df957b1a79a104f348d60d8027
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 30 21:05:17 2018 +0100

    rxrpc: Fix checker warnings and errors
    
    Fix various issues detected by checker.
    
    Errors:
    
     (*) rxrpc_discard_prealloc() should be using rcu_assign_pointer to set
         call->socket.
    
    Warnings:
    
     (*) rxrpc_service_connection_reaper() should be passing NULL rather than 0 to
         trace_rxrpc_conn() as the where argument.
    
     (*) rxrpc_disconnect_client_call() should get its net pointer via the
         call->conn rather than call->sock to avoid a warning about accessing
         an RCU pointer without protection.
    
     (*) Proc seq start/stop functions need annotation as they pass locks
         between the functions.
    
    False positives:
    
     (*) Checker doesn't correctly handle of seq-retry lock context balance in
         rxrpc_find_service_conn_rcu().
    
     (*) Checker thinks execution may proceed past the BUG() in
         rxrpc_publish_service_conn().
    
     (*) Variable length array warnings from SKCIPHER_REQUEST_ON_STACK() in
         rxkad.c.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 92ebd1d7e0bb..4ce24c000653 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -225,7 +225,7 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 	tail = b->call_backlog_tail;
 	while (CIRC_CNT(head, tail, size) > 0) {
 		struct rxrpc_call *call = b->call_backlog[tail];
-		call->socket = rx;
+		rcu_assign_pointer(call->socket, rx);
 		if (rx->discard_new_call) {
 			_debug("discard %lx", call->user_call_ID);
 			rx->discard_new_call(call, call->user_call_ID);
@@ -456,6 +456,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 				     unsigned long user_call_ID,
 				     rxrpc_notify_rx_t notify_rx)
 	__releases(&rx->sk.sk_lock.slock)
+	__acquires(call->user_mutex)
 {
 	struct rxrpc_call *call;
 	struct rb_node *parent, **pp;

commit a25e21f0bcd25673b91b97b9805db33350feec0f
Author: David Howells <dhowells@redhat.com>
Date:   Tue Mar 27 23:03:00 2018 +0100

    rxrpc, afs: Use debug_ids rather than pointers in traces
    
    In rxrpc and afs, use the debug_ids that are monotonically allocated to
    various objects as they're allocated rather than pointers as kernel
    pointers are now hashed making them less useful.  Further, the debug ids
    aren't reused anywhere nearly as quickly.
    
    In addition, allow kernel services that use rxrpc, such as afs, to take
    numbers from the rxrpc counter, assign them to their own call struct and
    pass them in to rxrpc for both client and service calls so that the trace
    lines for each will have the same ID tag.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 3028298ca561..92ebd1d7e0bb 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -34,7 +34,8 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 				      struct rxrpc_backlog *b,
 				      rxrpc_notify_rx_t notify_rx,
 				      rxrpc_user_attach_call_t user_attach_call,
-				      unsigned long user_call_ID, gfp_t gfp)
+				      unsigned long user_call_ID, gfp_t gfp,
+				      unsigned int debug_id)
 {
 	const void *here = __builtin_return_address(0);
 	struct rxrpc_call *call;
@@ -94,7 +95,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 	/* Now it gets complicated, because calls get registered with the
 	 * socket here, particularly if a user ID is preassigned by the user.
 	 */
-	call = rxrpc_alloc_call(rx, gfp);
+	call = rxrpc_alloc_call(rx, gfp, debug_id);
 	if (!call)
 		return -ENOMEM;
 	call->flags |= (1 << RXRPC_CALL_IS_SERVICE);
@@ -174,7 +175,8 @@ int rxrpc_service_prealloc(struct rxrpc_sock *rx, gfp_t gfp)
 	if (rx->discard_new_call)
 		return 0;
 
-	while (rxrpc_service_prealloc_one(rx, b, NULL, NULL, 0, gfp) == 0)
+	while (rxrpc_service_prealloc_one(rx, b, NULL, NULL, 0, gfp,
+					  atomic_inc_return(&rxrpc_debug_id)) == 0)
 		;
 
 	return 0;
@@ -347,7 +349,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 		   service_id == rx->second_service))
 		goto found_service;
 
-	trace_rxrpc_abort("INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,
+	trace_rxrpc_abort(0, "INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,
 			  RX_INVALID_OPERATION, EOPNOTSUPP);
 	skb->mark = RXRPC_SKB_MARK_LOCAL_ABORT;
 	skb->priority = RX_INVALID_OPERATION;
@@ -358,7 +360,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	spin_lock(&rx->incoming_lock);
 	if (rx->sk.sk_state == RXRPC_SERVER_LISTEN_DISABLED ||
 	    rx->sk.sk_state == RXRPC_CLOSE) {
-		trace_rxrpc_abort("CLS", sp->hdr.cid, sp->hdr.callNumber,
+		trace_rxrpc_abort(0, "CLS", sp->hdr.cid, sp->hdr.callNumber,
 				  sp->hdr.seq, RX_INVALID_OPERATION, ESHUTDOWN);
 		skb->mark = RXRPC_SKB_MARK_LOCAL_ABORT;
 		skb->priority = RX_INVALID_OPERATION;
@@ -635,6 +637,7 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
  * @user_attach_call: Func to attach call to user_call_ID
  * @user_call_ID: The tag to attach to the preallocated call
  * @gfp: The allocation conditions.
+ * @debug_id: The tracing debug ID.
  *
  * Charge up the socket with preallocated calls, each with a user ID.  A
  * function should be provided to effect the attachment from the user's side.
@@ -645,7 +648,8 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 int rxrpc_kernel_charge_accept(struct socket *sock,
 			       rxrpc_notify_rx_t notify_rx,
 			       rxrpc_user_attach_call_t user_attach_call,
-			       unsigned long user_call_ID, gfp_t gfp)
+			       unsigned long user_call_ID, gfp_t gfp,
+			       unsigned int debug_id)
 {
 	struct rxrpc_sock *rx = rxrpc_sk(sock->sk);
 	struct rxrpc_backlog *b = rx->backlog;
@@ -655,6 +659,6 @@ int rxrpc_kernel_charge_accept(struct socket *sock,
 
 	return rxrpc_service_prealloc_one(rx, b, notify_rx,
 					  user_attach_call, user_call_ID,
-					  gfp);
+					  gfp, debug_id);
 }
 EXPORT_SYMBOL(rxrpc_kernel_charge_accept);

commit 9faaff593404a9c4e5abc6839a641635d7b9d0cd
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 24 10:18:40 2017 +0000

    rxrpc: Provide a different lockdep key for call->user_mutex for kernel calls
    
    Provide a different lockdep key for rxrpc_call::user_mutex when the call is
    made on a kernel socket, such as by the AFS filesystem.
    
    The problem is that lockdep registers a false positive between userspace
    calling the sendmsg syscall on a user socket where call->user_mutex is held
    whilst userspace memory is accessed whereas the AFS filesystem may perform
    operations with mmap_sem held by the caller.
    
    In such a case, the following warning is produced.
    
    ======================================================
    WARNING: possible circular locking dependency detected
    4.14.0-fscache+ #243 Tainted: G            E
    ------------------------------------------------------
    modpost/16701 is trying to acquire lock:
     (&vnode->io_lock){+.+.}, at: [<ffffffffa000fc40>] afs_begin_vnode_operation+0x33/0x77 [kafs]
    
    but task is already holding lock:
     (&mm->mmap_sem){++++}, at: [<ffffffff8104376a>] __do_page_fault+0x1ef/0x486
    
    which lock already depends on the new lock.
    
    the existing dependency chain (in reverse order) is:
    
    -> #3 (&mm->mmap_sem){++++}:
           __might_fault+0x61/0x89
           _copy_from_iter_full+0x40/0x1fa
           rxrpc_send_data+0x8dc/0xff3
           rxrpc_do_sendmsg+0x62f/0x6a1
           rxrpc_sendmsg+0x166/0x1b7
           sock_sendmsg+0x2d/0x39
           ___sys_sendmsg+0x1ad/0x22b
           __sys_sendmsg+0x41/0x62
           do_syscall_64+0x89/0x1be
           return_from_SYSCALL_64+0x0/0x75
    
    -> #2 (&call->user_mutex){+.+.}:
           __mutex_lock+0x86/0x7d2
           rxrpc_new_client_call+0x378/0x80e
           rxrpc_kernel_begin_call+0xf3/0x154
           afs_make_call+0x195/0x454 [kafs]
           afs_vl_get_capabilities+0x193/0x198 [kafs]
           afs_vl_lookup_vldb+0x5f/0x151 [kafs]
           afs_create_volume+0x2e/0x2f4 [kafs]
           afs_mount+0x56a/0x8d7 [kafs]
           mount_fs+0x6a/0x109
           vfs_kern_mount+0x67/0x135
           do_mount+0x90b/0xb57
           SyS_mount+0x72/0x98
           do_syscall_64+0x89/0x1be
           return_from_SYSCALL_64+0x0/0x75
    
    -> #1 (k-sk_lock-AF_RXRPC){+.+.}:
           lock_sock_nested+0x74/0x8a
           rxrpc_kernel_begin_call+0x8a/0x154
           afs_make_call+0x195/0x454 [kafs]
           afs_fs_get_capabilities+0x17a/0x17f [kafs]
           afs_probe_fileserver+0xf7/0x2f0 [kafs]
           afs_select_fileserver+0x83f/0x903 [kafs]
           afs_fetch_status+0x89/0x11d [kafs]
           afs_iget+0x16f/0x4f8 [kafs]
           afs_mount+0x6c6/0x8d7 [kafs]
           mount_fs+0x6a/0x109
           vfs_kern_mount+0x67/0x135
           do_mount+0x90b/0xb57
           SyS_mount+0x72/0x98
           do_syscall_64+0x89/0x1be
           return_from_SYSCALL_64+0x0/0x75
    
    -> #0 (&vnode->io_lock){+.+.}:
           lock_acquire+0x174/0x19f
           __mutex_lock+0x86/0x7d2
           afs_begin_vnode_operation+0x33/0x77 [kafs]
           afs_fetch_data+0x80/0x12a [kafs]
           afs_readpages+0x314/0x405 [kafs]
           __do_page_cache_readahead+0x203/0x2ba
           filemap_fault+0x179/0x54d
           __do_fault+0x17/0x60
           __handle_mm_fault+0x6d7/0x95c
           handle_mm_fault+0x24e/0x2a3
           __do_page_fault+0x301/0x486
           do_page_fault+0x236/0x259
           page_fault+0x22/0x30
           __clear_user+0x3d/0x60
           padzero+0x1c/0x2b
           load_elf_binary+0x785/0xdc7
           search_binary_handler+0x81/0x1ff
           do_execveat_common.isra.14+0x600/0x888
           do_execve+0x1f/0x21
           SyS_execve+0x28/0x2f
           do_syscall_64+0x89/0x1be
           return_from_SYSCALL_64+0x0/0x75
    
    other info that might help us debug this:
    
    Chain exists of:
      &vnode->io_lock --> &call->user_mutex --> &mm->mmap_sem
    
     Possible unsafe locking scenario:
    
           CPU0                    CPU1
           ----                    ----
      lock(&mm->mmap_sem);
                                   lock(&call->user_mutex);
                                   lock(&mm->mmap_sem);
      lock(&vnode->io_lock);
    
     *** DEADLOCK ***
    
    1 lock held by modpost/16701:
     #0:  (&mm->mmap_sem){++++}, at: [<ffffffff8104376a>] __do_page_fault+0x1ef/0x486
    
    stack backtrace:
    CPU: 0 PID: 16701 Comm: modpost Tainted: G            E   4.14.0-fscache+ #243
    Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
    Call Trace:
     dump_stack+0x67/0x8e
     print_circular_bug+0x341/0x34f
     check_prev_add+0x11f/0x5d4
     ? add_lock_to_list.isra.12+0x8b/0x8b
     ? add_lock_to_list.isra.12+0x8b/0x8b
     ? __lock_acquire+0xf77/0x10b4
     __lock_acquire+0xf77/0x10b4
     lock_acquire+0x174/0x19f
     ? afs_begin_vnode_operation+0x33/0x77 [kafs]
     __mutex_lock+0x86/0x7d2
     ? afs_begin_vnode_operation+0x33/0x77 [kafs]
     ? afs_begin_vnode_operation+0x33/0x77 [kafs]
     ? afs_begin_vnode_operation+0x33/0x77 [kafs]
     afs_begin_vnode_operation+0x33/0x77 [kafs]
     afs_fetch_data+0x80/0x12a [kafs]
     afs_readpages+0x314/0x405 [kafs]
     __do_page_cache_readahead+0x203/0x2ba
     ? filemap_fault+0x179/0x54d
     filemap_fault+0x179/0x54d
     __do_fault+0x17/0x60
     __handle_mm_fault+0x6d7/0x95c
     handle_mm_fault+0x24e/0x2a3
     __do_page_fault+0x301/0x486
     do_page_fault+0x236/0x259
     page_fault+0x22/0x30
    RIP: 0010:__clear_user+0x3d/0x60
    RSP: 0018:ffff880071e93da0 EFLAGS: 00010202
    RAX: 0000000000000000 RBX: 000000000000011c RCX: 000000000000011c
    RDX: 0000000000000000 RSI: 0000000000000008 RDI: 000000000060f720
    RBP: 000000000060f720 R08: 0000000000000001 R09: 0000000000000000
    R10: 0000000000000001 R11: ffff8800b5459b68 R12: ffff8800ce150e00
    R13: 000000000060f720 R14: 00000000006127a8 R15: 0000000000000000
     padzero+0x1c/0x2b
     load_elf_binary+0x785/0xdc7
     search_binary_handler+0x81/0x1ff
     do_execveat_common.isra.14+0x600/0x888
     do_execve+0x1f/0x21
     SyS_execve+0x28/0x2f
     do_syscall_64+0x89/0x1be
     entry_SYSCALL64_slow_path+0x25/0x25
    RIP: 0033:0x7fdb6009ee07
    RSP: 002b:00007fff566d9728 EFLAGS: 00000246 ORIG_RAX: 000000000000003b
    RAX: ffffffffffffffda RBX: 000055ba57280900 RCX: 00007fdb6009ee07
    RDX: 000055ba5727f270 RSI: 000055ba5727cac0 RDI: 000055ba57280900
    RBP: 000055ba57280900 R08: 00007fff566d9700 R09: 0000000000000000
    R10: 000055ba5727cac0 R11: 0000000000000246 R12: 0000000000000000
    R13: 000055ba5727cac0 R14: 000055ba5727f270 R15: 0000000000000000
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index cbd1701e813a..3028298ca561 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -94,7 +94,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 	/* Now it gets complicated, because calls get registered with the
 	 * socket here, particularly if a user ID is preassigned by the user.
 	 */
-	call = rxrpc_alloc_call(gfp);
+	call = rxrpc_alloc_call(rx, gfp);
 	if (!call)
 		return -ENOMEM;
 	call->flags |= (1 << RXRPC_CALL_IS_SERVICE);

commit 7b674e390e513ea443c5a0f27ec95e93fb8a94fa
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 29 10:18:37 2017 +0100

    rxrpc: Fix IPv6 support
    
    Fix IPv6 support in AF_RXRPC in the following ways:
    
     (1) When extracting the address from a received IPv4 packet, if the local
         transport socket is open for IPv6 then fill out the sockaddr_rxrpc
         struct for an IPv4-mapped-to-IPv6 AF_INET6 transport address instead
         of an AF_INET one.
    
     (2) When sending CHALLENGE or RESPONSE packets, the transport length needs
         to be set from the sockaddr_rxrpc::transport_len field rather than
         sizeof() on the IPv4 transport address.
    
     (3) When processing an IPv4 ICMP packet received by an IPv6 socket, set up
         the address correctly before searching for the affected peer.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index ec3383f97d4c..cbd1701e813a 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -277,7 +277,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 		 * anticipation - and to save on stack space.
 		 */
 		xpeer = b->peer_backlog[peer_tail];
-		if (rxrpc_extract_addr_from_skb(&xpeer->srx, skb) < 0)
+		if (rxrpc_extract_addr_from_skb(local, &xpeer->srx, skb) < 0)
 			return NULL;
 
 		peer = rxrpc_lookup_incoming_peer(local, xpeer);

commit 9a19bad70cf16b0cdf3576efda7deb490e7aa529
Author: David Howells <dhowells@redhat.com>
Date:   Fri Aug 18 00:19:42 2017 +0100

    rxrpc: Fix oops when discarding a preallocated service call
    
    rxrpc_service_prealloc_one() doesn't set the socket pointer on any new call
    it preallocates, but does add it to the rxrpc net namespace call list.
    This, however, causes rxrpc_put_call() to oops when the call is discarded
    when the socket is closed.  rxrpc_put_call() needs the socket to be able to
    reach the namespace so that it can use a lock held therein.
    
    Fix this by setting a call's socket pointer immediately before discarding
    it.
    
    This can be triggered by unloading the kafs module, resulting in an oops
    like the following:
    
    BUG: unable to handle kernel NULL pointer dereference at 0000000000000030
    IP: rxrpc_put_call+0x1e2/0x32d
    PGD 0
    P4D 0
    Oops: 0000 [#1] SMP
    Modules linked in: kafs(E-)
    CPU: 3 PID: 3037 Comm: rmmod Tainted: G            E   4.12.0-fscache+ #213
    Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
    task: ffff8803fc92e2c0 task.stack: ffff8803fef74000
    RIP: 0010:rxrpc_put_call+0x1e2/0x32d
    RSP: 0018:ffff8803fef77e08 EFLAGS: 00010282
    RAX: 0000000000000000 RBX: ffff8803fab99ac0 RCX: 000000000000000f
    RDX: ffffffff81c50a40 RSI: 000000000000000c RDI: ffff8803fc92ea88
    RBP: ffff8803fef77e30 R08: ffff8803fc87b941 R09: ffffffff82946d20
    R10: ffff8803fef77d10 R11: 00000000000076fc R12: 0000000000000005
    R13: ffff8803fab99c20 R14: 0000000000000001 R15: ffffffff816c6aee
    FS:  00007f915a059700(0000) GS:ffff88041fb80000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 0000000000000030 CR3: 00000003fef39000 CR4: 00000000001406e0
    Call Trace:
     rxrpc_discard_prealloc+0x325/0x341
     rxrpc_listen+0xf9/0x146
     kernel_listen+0xb/0xd
     afs_close_socket+0x3e/0x173 [kafs]
     afs_exit+0x1f/0x57 [kafs]
     SyS_delete_module+0x10f/0x19a
     do_syscall_64+0x8a/0x149
     entry_SYSCALL64_slow_path+0x25/0x25
    
    Fixes: 2baec2c3f854 ("rxrpc: Support network namespacing")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index dd30d74824b0..ec3383f97d4c 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -223,6 +223,7 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 	tail = b->call_backlog_tail;
 	while (CIRC_CNT(head, tail, size) > 0) {
 		struct rxrpc_call *call = b->call_backlog[tail];
+		call->socket = rx;
 		if (rx->discard_new_call) {
 			_debug("discard %lx", call->user_call_ID);
 			rx->discard_new_call(call, call->user_call_ID);

commit f7aec129a356ad049edddcb7e77b04a474fcf41f
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jun 14 17:56:50 2017 +0100

    rxrpc: Cache the congestion window setting
    
    Cache the congestion window setting that was determined during a call's
    transmission phase when it finishes so that it can be used by the next call
    to the same peer, thereby shortcutting the slow-start algorithm.
    
    The value is stored in the rxrpc_peer struct and is accessed without
    locking.  Each call takes the value that happens to be there when it starts
    and just overwrites the value when it finishes.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 0d4d84e8c074..dd30d74824b0 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -310,6 +310,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 	rxrpc_see_call(call);
 	call->conn = conn;
 	call->peer = rxrpc_get_peer(conn->params.peer);
+	call->cong_cwnd = call->peer->cong_cwnd;
 	return call;
 }
 

commit 4722974d90e06d0164ca1b73a6b34cec6bdb64ad
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 5 14:30:49 2017 +0100

    rxrpc: Implement service upgrade
    
    Implement AuriStor's service upgrade facility.  There are three problems
    that this is meant to deal with:
    
     (1) Various of the standard AFS RPC calls have IPv4 addresses in their
         requests and/or replies - but there's no room for including IPv6
         addresses.
    
     (2) Definition of IPv6-specific RPC operations in the standard operation
         sets has not yet been achieved.
    
     (3) One could envision the creation a new service on the same port that as
         the original service.  The new service could implement improved
         operations - and the client could try this first, falling back to the
         original service if it's not there.
    
         Unfortunately, certain servers ignore packets addressed to a service
         they don't implement and don't respond in any way - not even with an
         ABORT.  This means that the client must then wait for the call timeout
         to occur.
    
    What service upgrade does is to see if the connection is marked as being
    'upgradeable' and if so, change the service ID in the server and thus the
    request and reply formats.  Note that the upgrade isn't mandatory - a
    server that supports only the original call set will ignore the upgrade
    request.
    
    In the protocol, the procedure is then as follows:
    
     (1) To request an upgrade, the first DATA packet in a new connection must
         have the userStatus set to 1 (this is normally 0).  The userStatus
         value is normally ignored by the server.
    
     (2) If the server doesn't support upgrading, the reply packets will
         contain the same service ID as for the first request packet.
    
     (3) If the server does support upgrading, all future reply packets on that
         connection will contain the new service ID and the new service ID will
         be applied to *all* further calls on that connection as well.
    
     (4) The RPC op used to probe the upgrade must take the same request data
         as the shadow call in the upgrade set (but may return a different
         reply).  GetCapability RPC ops were added to all standard sets for
         just this purpose.  Ops where the request formats differ cannot be
         used for probing.
    
     (5) The client must wait for completion of the probe before sending any
         further RPC ops to the same destination.  It should then use the
         service ID that recvmsg() reported back in all future calls.
    
     (6) The shadow service must have call definitions for all the operation
         IDs defined by the original service.
    
    
    To support service upgrading, a server should:
    
     (1) Call bind() twice on its AF_RXRPC socket before calling listen().
         Each bind() should supply a different service ID, but the transport
         addresses must be the same.  This allows the server to receive
         requests with either service ID.
    
     (2) Enable automatic upgrading by calling setsockopt(), specifying
         RXRPC_UPGRADEABLE_SERVICE and passing in a two-member array of
         unsigned shorts as the argument:
    
            unsigned short optval[2];
    
         This specifies a pair of service IDs.  They must be different and must
         match the service IDs bound to the socket.  Member 0 is the service ID
         to upgrade from and member 1 is the service ID to upgrade to.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 544df53ccf79..0d4d84e8c074 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -296,7 +296,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 		conn->params.local = local;
 		conn->params.peer = peer;
 		rxrpc_see_connection(conn);
-		rxrpc_new_incoming_connection(conn, skb);
+		rxrpc_new_incoming_connection(rx, conn, skb);
 	} else {
 		rxrpc_get_connection(conn);
 	}

commit 28036f44851e2515aa91b547b45cefddcac52ff6
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 5 14:30:49 2017 +0100

    rxrpc: Permit multiple service binding
    
    Permit bind() to be called on an AF_RXRPC socket more than once (currently
    maximum twice) to bind multiple listening services to it.  There are some
    restrictions:
    
     (1) All bind() calls involved must have a non-zero service ID.
    
     (2) The service IDs must all be different.
    
     (3) The rest of the address (notably the transport part) must be the same
         in all (a single UDP socket is shared).
    
     (4) This must be done before listen() or sendmsg() is called.
    
    This allows someone to connect to the service socket with different service
    IDs and lays the foundation for service upgrading.
    
    The service ID used by an incoming call can be extracted from the msg_name
    returned by recvmsg().
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index a8515b0d4717..544df53ccf79 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -341,7 +341,8 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	/* Get the socket providing the service */
 	rx = rcu_dereference(local->service);
-	if (rx && service_id == rx->srx.srx_service)
+	if (rx && (service_id == rx->srx.srx_service ||
+		   service_id == rx->second_service))
 		goto found_service;
 
 	trace_rxrpc_abort("INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,

commit 2baec2c3f854d1f79c7bb28386484e144e864a14
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 24 17:02:32 2017 +0100

    rxrpc: Support network namespacing
    
    Support network namespacing in AF_RXRPC with the following changes:
    
     (1) All the local endpoint, peer and call lists, locks, counters, etc. are
         moved into the per-namespace record.
    
     (2) All the connection tracking is moved into the per-namespace record
         with the exception of the client connection ID tree, which is kept
         global so that connection IDs are kept unique per-machine.
    
     (3) Each namespace gets its own epoch.  This allows each network namespace
         to pretend to be a separate client machine.
    
     (4) The /proc/net/rxrpc_xxx files are now called /proc/net/rxrpc/xxx and
         the contents reflect the namespace.
    
    fs/afs/ should be okay with this patch as it explicitly requires the current
    net namespace to be init_net to permit a mount to proceed at the moment.  It
    will, however, need updating so that cells, IP addresses and DNS records are
    per-namespace also.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 1752fcf8e8f1..a8515b0d4717 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -38,6 +38,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 {
 	const void *here = __builtin_return_address(0);
 	struct rxrpc_call *call;
+	struct rxrpc_net *rxnet = rxrpc_net(sock_net(&rx->sk));
 	int max, tmp;
 	unsigned int size = RXRPC_BACKLOG_MAX;
 	unsigned int head, tail, call_head, call_tail;
@@ -79,7 +80,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 	if (CIRC_CNT(head, tail, size) < max) {
 		struct rxrpc_connection *conn;
 
-		conn = rxrpc_prealloc_service_connection(gfp);
+		conn = rxrpc_prealloc_service_connection(rxnet, gfp);
 		if (!conn)
 			return -ENOMEM;
 		b->conn_backlog[head] = conn;
@@ -136,9 +137,9 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 
 	write_unlock(&rx->call_lock);
 
-	write_lock(&rxrpc_call_lock);
-	list_add_tail(&call->link, &rxrpc_calls);
-	write_unlock(&rxrpc_call_lock);
+	write_lock(&rxnet->call_lock);
+	list_add_tail(&call->link, &rxnet->calls);
+	write_unlock(&rxnet->call_lock);
 
 	b->call_backlog[call_head] = call;
 	smp_store_release(&b->call_backlog_head, (call_head + 1) & (size - 1));
@@ -185,6 +186,7 @@ int rxrpc_service_prealloc(struct rxrpc_sock *rx, gfp_t gfp)
 void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 {
 	struct rxrpc_backlog *b = rx->backlog;
+	struct rxrpc_net *rxnet = rxrpc_net(sock_net(&rx->sk));
 	unsigned int size = RXRPC_BACKLOG_MAX, head, tail;
 
 	if (!b)
@@ -209,10 +211,10 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 	tail = b->conn_backlog_tail;
 	while (CIRC_CNT(head, tail, size) > 0) {
 		struct rxrpc_connection *conn = b->conn_backlog[tail];
-		write_lock(&rxrpc_connection_lock);
+		write_lock(&rxnet->conn_lock);
 		list_del(&conn->link);
 		list_del(&conn->proc_link);
-		write_unlock(&rxrpc_connection_lock);
+		write_unlock(&rxnet->conn_lock);
 		kfree(conn);
 		tail = (tail + 1) & (size - 1);
 	}

commit 3a92789af0d625caff1e0bf5701aec8edf0d057d
Author: David Howells <dhowells@redhat.com>
Date:   Thu Apr 6 10:11:56 2017 +0100

    rxrpc: Use negative error codes in rxrpc_call struct
    
    Use negative error codes in struct rxrpc_call::error because that's what
    the kernel normally deals with and to make the code consistent.  We only
    turn them positive when transcribing into a cmsg for userspace recvmsg.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 0ed181f53f32..1752fcf8e8f1 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -413,11 +413,11 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	case RXRPC_CONN_REMOTELY_ABORTED:
 		rxrpc_set_call_completion(call, RXRPC_CALL_REMOTELY_ABORTED,
-					  conn->remote_abort, ECONNABORTED);
+					  conn->remote_abort, -ECONNABORTED);
 		break;
 	case RXRPC_CONN_LOCALLY_ABORTED:
 		rxrpc_abort_call("CON", call, sp->hdr.seq,
-				 conn->local_abort, ECONNABORTED);
+				 conn->local_abort, -ECONNABORTED);
 		break;
 	default:
 		BUG();
@@ -600,7 +600,7 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 	write_lock_bh(&call->state_lock);
 	switch (call->state) {
 	case RXRPC_CALL_SERVER_ACCEPTING:
-		__rxrpc_abort_call("REJ", call, 1, RX_USER_ABORT, ECONNABORTED);
+		__rxrpc_abort_call("REJ", call, 1, RX_USER_ABORT, -ECONNABORTED);
 		abort = true;
 		/* fall through */
 	case RXRPC_CALL_COMPLETE:

commit 540b1c48c37ac0ad66212004db21e1ff7e2d78be
Author: David Howells <dhowells@redhat.com>
Date:   Mon Feb 27 15:43:06 2017 +0000

    rxrpc: Fix deadlock between call creation and sendmsg/recvmsg
    
    All the routines by which rxrpc is accessed from the outside are serialised
    by means of the socket lock (sendmsg, recvmsg, bind,
    rxrpc_kernel_begin_call(), ...) and this presents a problem:
    
     (1) If a number of calls on the same socket are in the process of
         connection to the same peer, a maximum of four concurrent live calls
         are permitted before further calls need to wait for a slot.
    
     (2) If a call is waiting for a slot, it is deep inside sendmsg() or
         rxrpc_kernel_begin_call() and the entry function is holding the socket
         lock.
    
     (3) sendmsg() and recvmsg() or the in-kernel equivalents are prevented
         from servicing the other calls as they need to take the socket lock to
         do so.
    
     (4) The socket is stuck until a call is aborted and makes its slot
         available to the waiter.
    
    Fix this by:
    
     (1) Provide each call with a mutex ('user_mutex') that arbitrates access
         by the users of rxrpc separately for each specific call.
    
     (2) Make rxrpc_sendmsg() and rxrpc_recvmsg() unlock the socket as soon as
         they've got a call and taken its mutex.
    
         Note that I'm returning EWOULDBLOCK from recvmsg() if MSG_DONTWAIT is
         set but someone else has the lock.  Should I instead only return
         EWOULDBLOCK if there's nothing currently to be done on a socket, and
         sleep in this particular instance because there is something to be
         done, but we appear to be blocked by the interrupt handler doing its
         ping?
    
     (3) Make rxrpc_new_client_call() unlock the socket after allocating a new
         call, locking its user mutex and adding it to the socket's call tree.
         The call is returned locked so that sendmsg() can add data to it
         immediately.
    
         From the moment the call is in the socket tree, it is subject to
         access by sendmsg() and recvmsg() - even if it isn't connected yet.
    
     (4) Lock new service calls in the UDP data_ready handler (in
         rxrpc_new_incoming_call()) because they may already be in the socket's
         tree and the data_ready handler makes them live immediately if a user
         ID has already been preassigned.
    
         Note that the new call is locked before any notifications are sent
         that it is live, so doing mutex_trylock() *ought* to always succeed.
         Userspace is prevented from doing sendmsg() on calls that are in a
         too-early state in rxrpc_do_sendmsg().
    
     (5) Make rxrpc_new_incoming_call() return the call with the user mutex
         held so that a ping can be scheduled immediately under it.
    
         Note that it might be worth moving the ping call into
         rxrpc_new_incoming_call() and then we can drop the mutex there.
    
     (6) Make rxrpc_accept_call() take the lock on the call it is accepting and
         release the socket after adding the call to the socket's tree.  This
         is slightly tricky as we've dequeued the call by that point and have
         to requeue it.
    
         Note that requeuing emits a trace event.
    
     (7) Make rxrpc_kernel_send_data() and rxrpc_kernel_recv_data() take the
         new mutex immediately and don't bother with the socket mutex at all.
    
    This patch has the nice bonus that calls on the same socket are now to some
    extent parallelisable.
    
    Note that we might want to move rxrpc_service_prealloc() calls out from the
    socket lock and give it its own lock, so that we don't hang progress in
    other calls because we're waiting for the allocator.
    
    We probably also want to avoid calling rxrpc_notify_socket() from within
    the socket lock (rxrpc_accept_call()).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Marc Dionne <marc.c.dionne@auristor.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 7c4c64ab8da2..0ed181f53f32 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -323,6 +323,8 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
  *
  * If we want to report an error, we mark the skb with the packet type and
  * abort code and return NULL.
+ *
+ * The call is returned with the user access mutex held.
  */
 struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 					   struct rxrpc_connection *conn,
@@ -371,6 +373,18 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	trace_rxrpc_receive(call, rxrpc_receive_incoming,
 			    sp->hdr.serial, sp->hdr.seq);
 
+	/* Lock the call to prevent rxrpc_kernel_send/recv_data() and
+	 * sendmsg()/recvmsg() inconveniently stealing the mutex once the
+	 * notification is generated.
+	 *
+	 * The BUG should never happen because the kernel should be well
+	 * behaved enough not to access the call before the first notification
+	 * event and userspace is prevented from doing so until the state is
+	 * appropriate.
+	 */
+	if (!mutex_trylock(&call->user_mutex))
+		BUG();
+
 	/* Make the call live. */
 	rxrpc_incoming_call(rx, call, skb);
 	conn = call->conn;
@@ -429,10 +443,12 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 /*
  * handle acceptance of a call by userspace
  * - assign the user call ID to the call at the front of the queue
+ * - called with the socket locked.
  */
 struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 				     unsigned long user_call_ID,
 				     rxrpc_notify_rx_t notify_rx)
+	__releases(&rx->sk.sk_lock.slock)
 {
 	struct rxrpc_call *call;
 	struct rb_node *parent, **pp;
@@ -446,6 +462,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 
 	if (list_empty(&rx->to_be_accepted)) {
 		write_unlock(&rx->call_lock);
+		release_sock(&rx->sk);
 		kleave(" = -ENODATA [empty]");
 		return ERR_PTR(-ENODATA);
 	}
@@ -470,10 +487,39 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	 */
 	call = list_entry(rx->to_be_accepted.next,
 			  struct rxrpc_call, accept_link);
+	write_unlock(&rx->call_lock);
+
+	/* We need to gain the mutex from the interrupt handler without
+	 * upsetting lockdep, so we have to release it there and take it here.
+	 * We are, however, still holding the socket lock, so other accepts
+	 * must wait for us and no one can add the user ID behind our backs.
+	 */
+	if (mutex_lock_interruptible(&call->user_mutex) < 0) {
+		release_sock(&rx->sk);
+		kleave(" = -ERESTARTSYS");
+		return ERR_PTR(-ERESTARTSYS);
+	}
+
+	write_lock(&rx->call_lock);
 	list_del_init(&call->accept_link);
 	sk_acceptq_removed(&rx->sk);
 	rxrpc_see_call(call);
 
+	/* Find the user ID insertion point. */
+	pp = &rx->calls.rb_node;
+	parent = NULL;
+	while (*pp) {
+		parent = *pp;
+		call = rb_entry(parent, struct rxrpc_call, sock_node);
+
+		if (user_call_ID < call->user_call_ID)
+			pp = &(*pp)->rb_left;
+		else if (user_call_ID > call->user_call_ID)
+			pp = &(*pp)->rb_right;
+		else
+			BUG();
+	}
+
 	write_lock_bh(&call->state_lock);
 	switch (call->state) {
 	case RXRPC_CALL_SERVER_ACCEPTING:
@@ -499,6 +545,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	write_unlock(&rx->call_lock);
 	rxrpc_notify_socket(call);
 	rxrpc_service_prealloc(rx, GFP_KERNEL);
+	release_sock(&rx->sk);
 	_leave(" = %p{%d}", call, call->debug_id);
 	return call;
 
@@ -515,6 +562,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	write_unlock(&rx->call_lock);
 out:
 	rxrpc_service_prealloc(rx, GFP_KERNEL);
+	release_sock(&rx->sk);
 	_leave(" = %d", ret);
 	return ERR_PTR(ret);
 }

commit 210f035316f545e6f507e7d61e191495ba983e27
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jan 5 10:38:36 2017 +0000

    rxrpc: Allow listen(sock, 0) to be used to disable listening
    
    Allow listen() with a backlog of 0 to be used to disable listening on an
    AF_RXRPC socket.  This also releases any preallocation, thereby making it
    easier for a kernel service to account for all allocated call structures
    when shutting down the service.
    
    The socket cannot thereafter have listening reenabled, but must rather be
    closed and reopened.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 832d854c2d5c..7c4c64ab8da2 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -349,7 +349,8 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 found_service:
 	spin_lock(&rx->incoming_lock);
-	if (rx->sk.sk_state == RXRPC_CLOSE) {
+	if (rx->sk.sk_state == RXRPC_SERVER_LISTEN_DISABLED ||
+	    rx->sk.sk_state == RXRPC_CLOSE) {
 		trace_rxrpc_abort("CLS", sp->hdr.cid, sp->hdr.callNumber,
 				  sp->hdr.seq, RX_INVALID_OPERATION, ESHUTDOWN);
 		skb->mark = RXRPC_SKB_MARK_LOCAL_ABORT;

commit 26cb02aa6d3efeb543805ed9ad599dae24f7c6d4
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 6 08:11:49 2016 +0100

    rxrpc: Fix warning by splitting rxrpc_send_call_packet()
    
    Split rxrpc_send_data_packet() to separate ACK generation (which is more
    complicated) from ABORT generation.  This simplifies the code a bit and
    fixes the following warning:
    
    In file included from ../net/rxrpc/output.c:20:0:
    net/rxrpc/output.c: In function 'rxrpc_send_call_packet':
    net/rxrpc/ar-internal.h:1187:27: error: 'top' may be used uninitialized in this function [-Werror=maybe-uninitialized]
    net/rxrpc/output.c:103:24: note: 'top' was declared here
    net/rxrpc/output.c:225:25: error: 'hard_ack' may be used uninitialized in this function [-Werror=maybe-uninitialized]
    
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 22cd8a18c481..832d854c2d5c 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -565,7 +565,7 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 	write_unlock_bh(&call->state_lock);
 	write_unlock(&rx->call_lock);
 	if (abort) {
-		rxrpc_send_call_packet(call, RXRPC_PACKET_TYPE_ABORT);
+		rxrpc_send_abort_packet(call);
 		rxrpc_release_call(rx, call);
 		rxrpc_put_call(call, rxrpc_call_put);
 	}

commit 7212a57e8eaa2572481398532d7be0c2685362b9
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 6 08:11:49 2016 +0100

    rxrpc: Fix oops on incoming call to serviceless endpoint
    
    If an call comes in to a local endpoint that isn't listening for any
    incoming calls at the moment, an oops will happen.  We need to check that
    the local endpoint's service pointer isn't NULL before we dereference it.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 3cac231d8405..22cd8a18c481 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -337,7 +337,7 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	/* Get the socket providing the service */
 	rx = rcu_dereference(local->service);
-	if (service_id == rx->srx.srx_service)
+	if (rx && service_id == rx->srx.srx_service)
 		goto found_service;
 
 	trace_rxrpc_abort("INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,

commit 1e9e5c9521d3667664a6e3c97075f71afec23720
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 29 22:37:15 2016 +0100

    rxrpc: Reduce the rxrpc_local::services list to a pointer
    
    Reduce the rxrpc_local::services list to just a pointer as we don't permit
    multiple service endpoints to bind to a single transport endpoints (this is
    excluded by rxrpc_lookup_local()).
    
    The reason we don't allow this is that if you send a request to an AFS
    filesystem service, it will try to talk back to your cache manager on the
    port you sent from (this is how file change notifications are handled).  To
    prevent someone from stealing your CM callbacks, we don't let AF_RXRPC
    sockets share a UDP socket if at least one of them has a service bound.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index a8d39d7cf42c..3cac231d8405 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -331,14 +331,14 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
 	struct rxrpc_sock *rx;
 	struct rxrpc_call *call;
+	u16 service_id = sp->hdr.serviceId;
 
 	_enter("");
 
 	/* Get the socket providing the service */
-	hlist_for_each_entry_rcu_bh(rx, &local->services, listen_link) {
-		if (rx->srx.srx_service == sp->hdr.serviceId)
-			goto found_service;
-	}
+	rx = rcu_dereference(local->service);
+	if (service_id == rx->srx.srx_service)
+		goto found_service;
 
 	trace_rxrpc_abort("INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,
 			  RX_INVALID_OPERATION, EOPNOTSUPP);

commit 58dc63c998ea3c5a27e2bf9251eddbf0977056a6
Author: David Howells <dhowells@redhat.com>
Date:   Sat Sep 17 10:49:13 2016 +0100

    rxrpc: Add a tracepoint to follow packets in the Rx buffer
    
    Add a tracepoint to follow the life of packets that get added to a call's
    receive buffer.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 3e474508ba75..a8d39d7cf42c 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -367,6 +367,9 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 		goto out;
 	}
 
+	trace_rxrpc_receive(call, rxrpc_receive_incoming,
+			    sp->hdr.serial, sp->hdr.seq);
+
 	/* Make the call live. */
 	rxrpc_incoming_call(rx, call, skb);
 	conn = call->conn;

commit 363deeab6d0f308d33d011323661ae9cf5f9f8d6
Author: David Howells <dhowells@redhat.com>
Date:   Sat Sep 17 10:49:14 2016 +0100

    rxrpc: Add connection tracepoint and client conn state tracepoint
    
    Add a pair of tracepoints, one to track rxrpc_connection struct ref
    counting and the other to track the client connection cache state.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 323b8da50163..3e474508ba75 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -85,6 +85,9 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 		b->conn_backlog[head] = conn;
 		smp_store_release(&b->conn_backlog_head,
 				  (head + 1) & (size - 1));
+
+		trace_rxrpc_conn(conn, rxrpc_conn_new_service,
+				 atomic_read(&conn->usage), here);
 	}
 
 	/* Now it gets complicated, because calls get registered with the
@@ -290,6 +293,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 		rxrpc_get_local(local);
 		conn->params.local = local;
 		conn->params.peer = peer;
+		rxrpc_see_connection(conn);
 		rxrpc_new_incoming_connection(conn, skb);
 	} else {
 		rxrpc_get_connection(conn);

commit e6f3afb3fc058e17b407b6f7cac08058b19e641c
Author: David Howells <dhowells@redhat.com>
Date:   Sat Sep 17 10:49:11 2016 +0100

    rxrpc: Record calls that need to be accepted
    
    Record calls that need to be accepted using sk_acceptq_added() otherwise
    the backlog counter goes negative because sk_acceptq_removed() is called.
    This causes the preallocator to malfunction.
    
    Calls that are preaccepted by AFS within the kernel aren't affected by
    this.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 26c293ef98eb..323b8da50163 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -369,6 +369,8 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 
 	if (rx->notify_new_call)
 		rx->notify_new_call(&rx->sk, call, call->user_call_ID);
+	else
+		sk_acceptq_added(&rx->sk);
 
 	spin_lock(&conn->state_lock);
 	switch (conn->state) {

commit 3432a757b1f889f8c0d33cd9fcabdae172ed812b
Author: David Howells <dhowells@redhat.com>
Date:   Tue Sep 13 09:05:14 2016 +0100

    rxrpc: Fix prealloc refcounting
    
    The preallocated call buffer holds a ref on the calls within that buffer.
    The ref was being released in the wrong place - it worked okay for incoming
    calls to the AFS cache manager service, but doesn't work right for incoming
    calls to a userspace service.
    
    Instead of releasing an extra ref service calls in rxrpc_release_call(),
    the ref needs to be released during the acceptance/rejectance process.  To
    this end:
    
     (1) The prealloc ref is now normally released during
         rxrpc_new_incoming_call().
    
     (2) For preallocated kernel API calls, the kernel API's ref needs to be
         released when the call is discarded on socket close.
    
     (3) We shouldn't take a second ref in rxrpc_accept_call().
    
     (4) rxrpc_recvmsg_new_call() needs to get a ref of its own when it adds
         the call to the to_be_accepted socket queue.
    
    In doing (4) above, we would prefer not to put the call's refcount down to
    0 as that entails doing cleanup in softirq context, but it's unlikely as
    there are several refs held elsewhere, at least one of which must be put by
    someone in process context calling rxrpc_release_call().  However, it's not
    a problem if we do have to do that.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 5fd9d2c89b7f..26c293ef98eb 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -221,6 +221,7 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 		if (rx->discard_new_call) {
 			_debug("discard %lx", call->user_call_ID);
 			rx->discard_new_call(call, call->user_call_ID);
+			rxrpc_put_call(call, rxrpc_call_put_kernel);
 		}
 		rxrpc_call_completed(call);
 		rxrpc_release_call(rx, call);
@@ -402,6 +403,13 @@ struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
 	if (call->state == RXRPC_CALL_SERVER_ACCEPTING)
 		rxrpc_notify_socket(call);
 
+	/* We have to discard the prealloc queue's ref here and rely on a
+	 * combination of the RCU read lock and refs held either by the socket
+	 * (recvmsg queue, to-be-accepted queue or user ID tree) or the kernel
+	 * service to prevent the call from being deallocated too early.
+	 */
+	rxrpc_put_call(call, rxrpc_call_put);
+
 	_leave(" = %p{%d}", call, call->debug_id);
 out:
 	spin_unlock(&rx->incoming_lock);
@@ -469,7 +477,6 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	}
 
 	/* formalise the acceptance */
-	rxrpc_get_call(call, rxrpc_call_got);
 	call->notify_rx = notify_rx;
 	call->user_call_ID = user_call_ID;
 	rxrpc_get_call(call, rxrpc_call_got_userid);

commit cbd00891de9bb4756bac6f6edfa945d5a6468977
Author: David Howells <dhowells@redhat.com>
Date:   Tue Sep 13 09:12:34 2016 +0100

    rxrpc: Adjust the call ref tracepoint to show kernel API refs
    
    Adjust the call ref tracepoint to show references held on a call by the
    kernel API separately as much as possible and add an additional trace to at
    the allocation point from the preallocation buffer for an incoming call.
    
    Note that this doesn't show the allocation of a client call for the kernel
    separately at the moment.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 06e328f6b0f0..5fd9d2c89b7f 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -121,7 +121,7 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 
 		call->user_call_ID = user_call_ID;
 		call->notify_rx = notify_rx;
-		rxrpc_get_call(call, rxrpc_call_got);
+		rxrpc_get_call(call, rxrpc_call_got_kernel);
 		user_attach_call(call, user_call_ID);
 		rxrpc_get_call(call, rxrpc_call_got_userid);
 		rb_link_node(&call->sock_node, parent, pp);
@@ -300,6 +300,7 @@ static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
 	smp_store_release(&b->call_backlog_tail,
 			  (call_tail + 1) & (RXRPC_BACKLOG_MAX - 1));
 
+	rxrpc_see_call(call);
 	call->conn = conn;
 	call->peer = rxrpc_get_peer(conn->params.peer);
 	return call;

commit b25de3605339c94a6c27d42efe8f7748ea206a8b
Author: David Howells <dhowells@redhat.com>
Date:   Tue Sep 13 22:36:22 2016 +0100

    rxrpc: Add missing unlock in rxrpc_call_accept()
    
    Add a missing unlock in rxrpc_call_accept() in the path taken if there's no
    call to wake up.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index b8acec0d596e..06e328f6b0f0 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -425,9 +425,11 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 
 	write_lock(&rx->call_lock);
 
-	ret = -ENODATA;
-	if (list_empty(&rx->to_be_accepted))
-		goto out;
+	if (list_empty(&rx->to_be_accepted)) {
+		write_unlock(&rx->call_lock);
+		kleave(" = -ENODATA [empty]");
+		return ERR_PTR(-ENODATA);
+	}
 
 	/* check the user ID isn't already in use */
 	pp = &rx->calls.rb_node;

commit 248f219cb8bcbfbd7f132752d44afa2df7c241d1
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 8 11:10:12 2016 +0100

    rxrpc: Rewrite the data and ack handling code
    
    Rewrite the data and ack handling code such that:
    
     (1) Parsing of received ACK and ABORT packets and the distribution and the
         filing of DATA packets happens entirely within the data_ready context
         called from the UDP socket.  This allows us to process and discard ACK
         and ABORT packets much more quickly (they're no longer stashed on a
         queue for a background thread to process).
    
     (2) We avoid calling skb_clone(), pskb_pull() and pskb_trim().  We instead
         keep track of the offset and length of the content of each packet in
         the sk_buff metadata.  This means we don't do any allocation in the
         receive path.
    
     (3) Jumbo DATA packet parsing is now done in data_ready context.  Rather
         than cloning the packet once for each subpacket and pulling/trimming
         it, we file the packet multiple times with an annotation for each
         indicating which subpacket is there.  From that we can directly
         calculate the offset and length.
    
     (4) A call's receive queue can be accessed without taking locks (memory
         barriers do have to be used, though).
    
     (5) Incoming calls are set up from preallocated resources and immediately
         made live.  They can than have packets queued upon them and ACKs
         generated.  If insufficient resources exist, DATA packet #1 is given a
         BUSY reply and other DATA packets are discarded).
    
     (6) sk_buffs no longer take a ref on their parent call.
    
    To make this work, the following changes are made:
    
     (1) Each call's receive buffer is now a circular buffer of sk_buff
         pointers (rxtx_buffer) rather than a number of sk_buff_heads spread
         between the call and the socket.  This permits each sk_buff to be in
         the buffer multiple times.  The receive buffer is reused for the
         transmit buffer.
    
     (2) A circular buffer of annotations (rxtx_annotations) is kept parallel
         to the data buffer.  Transmission phase annotations indicate whether a
         buffered packet has been ACK'd or not and whether it needs
         retransmission.
    
         Receive phase annotations indicate whether a slot holds a whole packet
         or a jumbo subpacket and, if the latter, which subpacket.  They also
         note whether the packet has been decrypted in place.
    
     (3) DATA packet window tracking is much simplified.  Each phase has just
         two numbers representing the window (rx_hard_ack/rx_top and
         tx_hard_ack/tx_top).
    
         The hard_ack number is the sequence number before base of the window,
         representing the last packet the other side says it has consumed.
         hard_ack starts from 0 and the first packet is sequence number 1.
    
         The top number is the sequence number of the highest-numbered packet
         residing in the buffer.  Packets between hard_ack+1 and top are
         soft-ACK'd to indicate they've been received, but not yet consumed.
    
         Four macros, before(), before_eq(), after() and after_eq() are added
         to compare sequence numbers within the window.  This allows for the
         top of the window to wrap when the hard-ack sequence number gets close
         to the limit.
    
         Two flags, RXRPC_CALL_RX_LAST and RXRPC_CALL_TX_LAST, are added also
         to indicate when rx_top and tx_top point at the packets with the
         LAST_PACKET bit set, indicating the end of the phase.
    
     (4) Calls are queued on the socket 'receive queue' rather than packets.
         This means that we don't need have to invent dummy packets to queue to
         indicate abnormal/terminal states and we don't have to keep metadata
         packets (such as ABORTs) around
    
     (5) The offset and length of a (sub)packet's content are now passed to
         the verify_packet security op.  This is currently expected to decrypt
         the packet in place and validate it.
    
         However, there's now nowhere to store the revised offset and length of
         the actual data within the decrypted blob (there may be a header and
         padding to skip) because an sk_buff may represent multiple packets, so
         a locate_data security op is added to retrieve these details from the
         sk_buff content when needed.
    
     (6) recvmsg() now has to handle jumbo subpackets, where each subpacket is
         individually secured and needs to be individually decrypted.  The code
         to do this is broken out into rxrpc_recvmsg_data() and shared with the
         kernel API.  It now iterates over the call's receive buffer rather
         than walking the socket receive queue.
    
    Additional changes:
    
     (1) The timers are condensed to a single timer that is set for the soonest
         of three timeouts (delayed ACK generation, DATA retransmission and
         call lifespan).
    
     (2) Transmission of ACK and ABORT packets is effected immediately from
         process-context socket ops/kernel API calls that cause them instead of
         them being punted off to a background work item.  The data_ready
         handler still has to defer to the background, though.
    
     (3) A shutdown op is added to the AF_RXRPC socket so that the AFS
         filesystem can shut down the socket and flush its own work items
         before closing the socket to deal with any in-progress service calls.
    
    Future additional changes that will need to be considered:
    
     (1) Make sure that a call doesn't hog the front of the queue by receiving
         data from the network as fast as userspace is consuming it to the
         exclusion of other calls.
    
     (2) Transmit delayed ACKs from within recvmsg() when we've consumed
         sufficiently more packets to avoid the background work item needing to
         run.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index cc7194e05a15..b8acec0d596e 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -129,6 +129,8 @@ static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
 		set_bit(RXRPC_CALL_HAS_USERID, &call->flags);
 	}
 
+	list_add(&call->sock_link, &rx->sock_calls);
+
 	write_unlock(&rx->call_lock);
 
 	write_lock(&rxrpc_call_lock);
@@ -186,6 +188,12 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 		return;
 	rx->backlog = NULL;
 
+	/* Make sure that there aren't any incoming calls in progress before we
+	 * clear the preallocation buffers.
+	 */
+	spin_lock_bh(&rx->incoming_lock);
+	spin_unlock_bh(&rx->incoming_lock);
+
 	head = b->peer_backlog_head;
 	tail = b->peer_backlog_tail;
 	while (CIRC_CNT(head, tail, size) > 0) {
@@ -224,251 +232,179 @@ void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
 }
 
 /*
- * generate a connection-level abort
+ * Allocate a new incoming call from the prealloc pool, along with a connection
+ * and a peer as necessary.
  */
-static int rxrpc_busy(struct rxrpc_local *local, struct sockaddr_rxrpc *srx,
-		      struct rxrpc_wire_header *whdr)
+static struct rxrpc_call *rxrpc_alloc_incoming_call(struct rxrpc_sock *rx,
+						    struct rxrpc_local *local,
+						    struct rxrpc_connection *conn,
+						    struct sk_buff *skb)
 {
-	struct msghdr msg;
-	struct kvec iov[1];
-	size_t len;
-	int ret;
-
-	_enter("%d,,", local->debug_id);
-
-	whdr->type	= RXRPC_PACKET_TYPE_BUSY;
-	whdr->serial	= htonl(1);
-
-	msg.msg_name	= &srx->transport.sin;
-	msg.msg_namelen	= sizeof(srx->transport.sin);
-	msg.msg_control	= NULL;
-	msg.msg_controllen = 0;
-	msg.msg_flags	= 0;
-
-	iov[0].iov_base	= whdr;
-	iov[0].iov_len	= sizeof(*whdr);
-
-	len = iov[0].iov_len;
-
-	_proto("Tx BUSY %%1");
+	struct rxrpc_backlog *b = rx->backlog;
+	struct rxrpc_peer *peer, *xpeer;
+	struct rxrpc_call *call;
+	unsigned short call_head, conn_head, peer_head;
+	unsigned short call_tail, conn_tail, peer_tail;
+	unsigned short call_count, conn_count;
+
+	/* #calls >= #conns >= #peers must hold true. */
+	call_head = smp_load_acquire(&b->call_backlog_head);
+	call_tail = b->call_backlog_tail;
+	call_count = CIRC_CNT(call_head, call_tail, RXRPC_BACKLOG_MAX);
+	conn_head = smp_load_acquire(&b->conn_backlog_head);
+	conn_tail = b->conn_backlog_tail;
+	conn_count = CIRC_CNT(conn_head, conn_tail, RXRPC_BACKLOG_MAX);
+	ASSERTCMP(conn_count, >=, call_count);
+	peer_head = smp_load_acquire(&b->peer_backlog_head);
+	peer_tail = b->peer_backlog_tail;
+	ASSERTCMP(CIRC_CNT(peer_head, peer_tail, RXRPC_BACKLOG_MAX), >=,
+		  conn_count);
+
+	if (call_count == 0)
+		return NULL;
+
+	if (!conn) {
+		/* No connection.  We're going to need a peer to start off
+		 * with.  If one doesn't yet exist, use a spare from the
+		 * preallocation set.  We dump the address into the spare in
+		 * anticipation - and to save on stack space.
+		 */
+		xpeer = b->peer_backlog[peer_tail];
+		if (rxrpc_extract_addr_from_skb(&xpeer->srx, skb) < 0)
+			return NULL;
+
+		peer = rxrpc_lookup_incoming_peer(local, xpeer);
+		if (peer == xpeer) {
+			b->peer_backlog[peer_tail] = NULL;
+			smp_store_release(&b->peer_backlog_tail,
+					  (peer_tail + 1) &
+					  (RXRPC_BACKLOG_MAX - 1));
+		}
 
-	ret = kernel_sendmsg(local->socket, &msg, iov, 1, len);
-	if (ret < 0) {
-		_leave(" = -EAGAIN [sendmsg failed: %d]", ret);
-		return -EAGAIN;
+		/* Now allocate and set up the connection */
+		conn = b->conn_backlog[conn_tail];
+		b->conn_backlog[conn_tail] = NULL;
+		smp_store_release(&b->conn_backlog_tail,
+				  (conn_tail + 1) & (RXRPC_BACKLOG_MAX - 1));
+		rxrpc_get_local(local);
+		conn->params.local = local;
+		conn->params.peer = peer;
+		rxrpc_new_incoming_connection(conn, skb);
+	} else {
+		rxrpc_get_connection(conn);
 	}
 
-	_leave(" = 0");
-	return 0;
+	/* And now we can allocate and set up a new call */
+	call = b->call_backlog[call_tail];
+	b->call_backlog[call_tail] = NULL;
+	smp_store_release(&b->call_backlog_tail,
+			  (call_tail + 1) & (RXRPC_BACKLOG_MAX - 1));
+
+	call->conn = conn;
+	call->peer = rxrpc_get_peer(conn->params.peer);
+	return call;
 }
 
 /*
- * accept an incoming call that needs peer, transport and/or connection setting
- * up
+ * Set up a new incoming call.  Called in BH context with the RCU read lock
+ * held.
+ *
+ * If this is for a kernel service, when we allocate the call, it will have
+ * three refs on it: (1) the kernel service, (2) the user_call_ID tree, (3) the
+ * retainer ref obtained from the backlog buffer.  Prealloc calls for userspace
+ * services only have the ref from the backlog buffer.  We want to pass this
+ * ref to non-BH context to dispose of.
+ *
+ * If we want to report an error, we mark the skb with the packet type and
+ * abort code and return NULL.
  */
-static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
-				      struct rxrpc_sock *rx,
-				      struct sk_buff *skb,
-				      struct sockaddr_rxrpc *srx)
+struct rxrpc_call *rxrpc_new_incoming_call(struct rxrpc_local *local,
+					   struct rxrpc_connection *conn,
+					   struct sk_buff *skb)
 {
-	struct rxrpc_connection *conn;
-	struct rxrpc_skb_priv *sp, *nsp;
+	struct rxrpc_skb_priv *sp = rxrpc_skb(skb);
+	struct rxrpc_sock *rx;
 	struct rxrpc_call *call;
-	struct sk_buff *notification;
-	int ret;
 
 	_enter("");
 
-	sp = rxrpc_skb(skb);
-
-	/* get a notification message to send to the server app */
-	notification = alloc_skb(0, GFP_NOFS);
-	if (!notification) {
-		_debug("no memory");
-		ret = -ENOMEM;
-		goto error_nofree;
-	}
-	rxrpc_new_skb(notification);
-	notification->mark = RXRPC_SKB_MARK_NEW_CALL;
-
-	conn = rxrpc_incoming_connection(local, srx, skb);
-	if (IS_ERR(conn)) {
-		_debug("no conn");
-		ret = PTR_ERR(conn);
-		goto error;
-	}
-
-	call = rxrpc_incoming_call(rx, conn, skb);
-	rxrpc_put_connection(conn);
-	if (IS_ERR(call)) {
-		_debug("no call");
-		ret = PTR_ERR(call);
-		goto error;
+	/* Get the socket providing the service */
+	hlist_for_each_entry_rcu_bh(rx, &local->services, listen_link) {
+		if (rx->srx.srx_service == sp->hdr.serviceId)
+			goto found_service;
 	}
 
-	/* attach the call to the socket */
-	read_lock_bh(&local->services_lock);
-	if (rx->sk.sk_state == RXRPC_CLOSE)
-		goto invalid_service;
-
-	write_lock(&rx->call_lock);
-	if (!test_and_set_bit(RXRPC_CALL_INIT_ACCEPT, &call->flags)) {
-		rxrpc_get_call(call, rxrpc_call_got);
-
-		spin_lock(&call->conn->state_lock);
-		if (sp->hdr.securityIndex > 0 &&
-		    call->conn->state == RXRPC_CONN_SERVICE_UNSECURED) {
-			_debug("await conn sec");
-			list_add_tail(&call->accept_link, &rx->secureq);
-			call->conn->state = RXRPC_CONN_SERVICE_CHALLENGING;
-			set_bit(RXRPC_CONN_EV_CHALLENGE, &call->conn->events);
-			rxrpc_queue_conn(call->conn);
-		} else {
-			_debug("conn ready");
-			call->state = RXRPC_CALL_SERVER_ACCEPTING;
-			list_add_tail(&call->accept_link, &rx->acceptq);
-			rxrpc_get_call_for_skb(call, notification);
-			nsp = rxrpc_skb(notification);
-			nsp->call = call;
-
-			ASSERTCMP(atomic_read(&call->usage), >=, 3);
-
-			_debug("notify");
-			spin_lock(&call->lock);
-			ret = rxrpc_queue_rcv_skb(call, notification, true,
-						  false);
-			spin_unlock(&call->lock);
-			notification = NULL;
-			BUG_ON(ret < 0);
-		}
-		spin_unlock(&call->conn->state_lock);
+	trace_rxrpc_abort("INV", sp->hdr.cid, sp->hdr.callNumber, sp->hdr.seq,
+			  RX_INVALID_OPERATION, EOPNOTSUPP);
+	skb->mark = RXRPC_SKB_MARK_LOCAL_ABORT;
+	skb->priority = RX_INVALID_OPERATION;
+	_leave(" = NULL [service]");
+	return NULL;
 
-		_debug("queued");
+found_service:
+	spin_lock(&rx->incoming_lock);
+	if (rx->sk.sk_state == RXRPC_CLOSE) {
+		trace_rxrpc_abort("CLS", sp->hdr.cid, sp->hdr.callNumber,
+				  sp->hdr.seq, RX_INVALID_OPERATION, ESHUTDOWN);
+		skb->mark = RXRPC_SKB_MARK_LOCAL_ABORT;
+		skb->priority = RX_INVALID_OPERATION;
+		_leave(" = NULL [close]");
+		call = NULL;
+		goto out;
 	}
-	write_unlock(&rx->call_lock);
 
-	_debug("process");
-	rxrpc_fast_process_packet(call, skb);
-
-	_debug("done");
-	read_unlock_bh(&local->services_lock);
-	rxrpc_free_skb(notification);
-	rxrpc_put_call(call, rxrpc_call_put);
-	_leave(" = 0");
-	return 0;
-
-invalid_service:
-	_debug("invalid");
-	read_unlock_bh(&local->services_lock);
-
-	rxrpc_release_call(rx, call);
-	rxrpc_put_call(call, rxrpc_call_put);
-	ret = -ECONNREFUSED;
-error:
-	rxrpc_free_skb(notification);
-error_nofree:
-	_leave(" = %d", ret);
-	return ret;
-}
+	call = rxrpc_alloc_incoming_call(rx, local, conn, skb);
+	if (!call) {
+		skb->mark = RXRPC_SKB_MARK_BUSY;
+		_leave(" = NULL [busy]");
+		call = NULL;
+		goto out;
+	}
 
-/*
- * accept incoming calls that need peer, transport and/or connection setting up
- * - the packets we get are all incoming client DATA packets that have seq == 1
- */
-void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
-{
-	struct rxrpc_skb_priv *sp;
-	struct sockaddr_rxrpc srx;
-	struct rxrpc_sock *rx;
-	struct rxrpc_wire_header whdr;
-	struct sk_buff *skb;
-	int ret;
+	/* Make the call live. */
+	rxrpc_incoming_call(rx, call, skb);
+	conn = call->conn;
 
-	_enter("%d", local->debug_id);
+	if (rx->notify_new_call)
+		rx->notify_new_call(&rx->sk, call, call->user_call_ID);
 
-	skb = skb_dequeue(&local->accept_queue);
-	if (!skb) {
-		_leave("\n");
-		return;
-	}
+	spin_lock(&conn->state_lock);
+	switch (conn->state) {
+	case RXRPC_CONN_SERVICE_UNSECURED:
+		conn->state = RXRPC_CONN_SERVICE_CHALLENGING;
+		set_bit(RXRPC_CONN_EV_CHALLENGE, &call->conn->events);
+		rxrpc_queue_conn(call->conn);
+		break;
 
-	_net("incoming call skb %p", skb);
-
-	rxrpc_see_skb(skb);
-	sp = rxrpc_skb(skb);
-
-	/* Set up a response packet header in case we need it */
-	whdr.epoch	= htonl(sp->hdr.epoch);
-	whdr.cid	= htonl(sp->hdr.cid);
-	whdr.callNumber	= htonl(sp->hdr.callNumber);
-	whdr.seq	= htonl(sp->hdr.seq);
-	whdr.serial	= 0;
-	whdr.flags	= 0;
-	whdr.type	= 0;
-	whdr.userStatus	= 0;
-	whdr.securityIndex = sp->hdr.securityIndex;
-	whdr._rsvd	= 0;
-	whdr.serviceId	= htons(sp->hdr.serviceId);
-
-	if (rxrpc_extract_addr_from_skb(&srx, skb) < 0)
-		goto drop;
-
-	/* get the socket providing the service */
-	read_lock_bh(&local->services_lock);
-	hlist_for_each_entry(rx, &local->services, listen_link) {
-		if (rx->srx.srx_service == sp->hdr.serviceId &&
-		    rx->sk.sk_state != RXRPC_CLOSE)
-			goto found_service;
-	}
-	read_unlock_bh(&local->services_lock);
-	goto invalid_service;
+	case RXRPC_CONN_SERVICE:
+		write_lock(&call->state_lock);
+		if (rx->discard_new_call)
+			call->state = RXRPC_CALL_SERVER_RECV_REQUEST;
+		else
+			call->state = RXRPC_CALL_SERVER_ACCEPTING;
+		write_unlock(&call->state_lock);
+		break;
 
-found_service:
-	_debug("found service %hd", rx->srx.srx_service);
-	if (sk_acceptq_is_full(&rx->sk))
-		goto backlog_full;
-	sk_acceptq_added(&rx->sk);
-	read_unlock_bh(&local->services_lock);
-
-	ret = rxrpc_accept_incoming_call(local, rx, skb, &srx);
-	if (ret < 0)
-		sk_acceptq_removed(&rx->sk);
-	switch (ret) {
-	case -ECONNRESET: /* old calls are ignored */
-	case -ECONNABORTED: /* aborted calls are reaborted or ignored */
-	case 0:
-		return;
-	case -ECONNREFUSED:
-		goto invalid_service;
-	case -EBUSY:
-		goto busy;
-	case -EKEYREJECTED:
-		goto security_mismatch;
+	case RXRPC_CONN_REMOTELY_ABORTED:
+		rxrpc_set_call_completion(call, RXRPC_CALL_REMOTELY_ABORTED,
+					  conn->remote_abort, ECONNABORTED);
+		break;
+	case RXRPC_CONN_LOCALLY_ABORTED:
+		rxrpc_abort_call("CON", call, sp->hdr.seq,
+				 conn->local_abort, ECONNABORTED);
+		break;
 	default:
 		BUG();
 	}
+	spin_unlock(&conn->state_lock);
 
-backlog_full:
-	read_unlock_bh(&local->services_lock);
-busy:
-	rxrpc_busy(local, &srx, &whdr);
-	rxrpc_free_skb(skb);
-	return;
-
-drop:
-	rxrpc_free_skb(skb);
-	return;
+	if (call->state == RXRPC_CALL_SERVER_ACCEPTING)
+		rxrpc_notify_socket(call);
 
-invalid_service:
-	skb->priority = RX_INVALID_OPERATION;
-	rxrpc_reject_packet(local, skb);
-	return;
-
-	/* can't change connection security type mid-flow */
-security_mismatch:
-	skb->priority = RX_PROTOCOL_ERROR;
-	rxrpc_reject_packet(local, skb);
-	return;
+	_leave(" = %p{%d}", call, call->debug_id);
+out:
+	spin_unlock(&rx->incoming_lock);
+	return call;
 }
 
 /*
@@ -490,11 +426,10 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	write_lock(&rx->call_lock);
 
 	ret = -ENODATA;
-	if (list_empty(&rx->acceptq))
+	if (list_empty(&rx->to_be_accepted))
 		goto out;
 
 	/* check the user ID isn't already in use */
-	ret = -EBADSLT;
 	pp = &rx->calls.rb_node;
 	parent = NULL;
 	while (*pp) {
@@ -506,11 +441,14 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 		else if (user_call_ID > call->user_call_ID)
 			pp = &(*pp)->rb_right;
 		else
-			goto out;
+			goto id_in_use;
 	}
 
-	/* dequeue the first call and check it's still valid */
-	call = list_entry(rx->acceptq.next, struct rxrpc_call, accept_link);
+	/* Dequeue the first call and check it's still valid.  We gain
+	 * responsibility for the queue's reference.
+	 */
+	call = list_entry(rx->to_be_accepted.next,
+			  struct rxrpc_call, accept_link);
 	list_del_init(&call->accept_link);
 	sk_acceptq_removed(&rx->sk);
 	rxrpc_see_call(call);
@@ -528,31 +466,35 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	}
 
 	/* formalise the acceptance */
-	rxrpc_get_call(call, rxrpc_call_got_userid);
+	rxrpc_get_call(call, rxrpc_call_got);
 	call->notify_rx = notify_rx;
 	call->user_call_ID = user_call_ID;
+	rxrpc_get_call(call, rxrpc_call_got_userid);
 	rb_link_node(&call->sock_node, parent, pp);
 	rb_insert_color(&call->sock_node, &rx->calls);
 	if (test_and_set_bit(RXRPC_CALL_HAS_USERID, &call->flags))
 		BUG();
-	if (test_and_set_bit(RXRPC_CALL_EV_ACCEPTED, &call->events))
-		BUG();
 
 	write_unlock_bh(&call->state_lock);
 	write_unlock(&rx->call_lock);
-	rxrpc_queue_call(call);
+	rxrpc_notify_socket(call);
+	rxrpc_service_prealloc(rx, GFP_KERNEL);
 	_leave(" = %p{%d}", call, call->debug_id);
 	return call;
 
 out_release:
+	_debug("release %p", call);
 	write_unlock_bh(&call->state_lock);
 	write_unlock(&rx->call_lock);
-	_debug("release %p", call);
 	rxrpc_release_call(rx, call);
-	_leave(" = %d", ret);
-	return ERR_PTR(ret);
-out:
+	rxrpc_put_call(call, rxrpc_call_put);
+	goto out;
+
+id_in_use:
+	ret = -EBADSLT;
 	write_unlock(&rx->call_lock);
+out:
+	rxrpc_service_prealloc(rx, GFP_KERNEL);
 	_leave(" = %d", ret);
 	return ERR_PTR(ret);
 }
@@ -564,6 +506,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 int rxrpc_reject_call(struct rxrpc_sock *rx)
 {
 	struct rxrpc_call *call;
+	bool abort = false;
 	int ret;
 
 	_enter("");
@@ -572,15 +515,16 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 
 	write_lock(&rx->call_lock);
 
-	ret = -ENODATA;
-	if (list_empty(&rx->acceptq)) {
+	if (list_empty(&rx->to_be_accepted)) {
 		write_unlock(&rx->call_lock);
-		_leave(" = -ENODATA");
 		return -ENODATA;
 	}
 
-	/* dequeue the first call and check it's still valid */
-	call = list_entry(rx->acceptq.next, struct rxrpc_call, accept_link);
+	/* Dequeue the first call and check it's still valid.  We gain
+	 * responsibility for the queue's reference.
+	 */
+	call = list_entry(rx->to_be_accepted.next,
+			  struct rxrpc_call, accept_link);
 	list_del_init(&call->accept_link);
 	sk_acceptq_removed(&rx->sk);
 	rxrpc_see_call(call);
@@ -588,66 +532,28 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 	write_lock_bh(&call->state_lock);
 	switch (call->state) {
 	case RXRPC_CALL_SERVER_ACCEPTING:
-		__rxrpc_set_call_completion(call, RXRPC_CALL_SERVER_BUSY,
-					    0, ECONNABORTED);
-		if (test_and_set_bit(RXRPC_CALL_EV_REJECT_BUSY, &call->events))
-			rxrpc_queue_call(call);
-		ret = 0;
-		break;
+		__rxrpc_abort_call("REJ", call, 1, RX_USER_ABORT, ECONNABORTED);
+		abort = true;
+		/* fall through */
 	case RXRPC_CALL_COMPLETE:
 		ret = call->error;
-		break;
+		goto out_discard;
 	default:
 		BUG();
 	}
 
+out_discard:
 	write_unlock_bh(&call->state_lock);
 	write_unlock(&rx->call_lock);
-	rxrpc_release_call(rx, call);
-	_leave(" = %d", ret);
-	return ret;
-}
-
-/**
- * rxrpc_kernel_accept_call - Allow a kernel service to accept an incoming call
- * @sock: The socket on which the impending call is waiting
- * @user_call_ID: The tag to attach to the call
- * @notify_rx: Where to send notifications instead of socket queue
- *
- * Allow a kernel service to accept an incoming call, assuming the incoming
- * call is still valid.  The caller should immediately trigger their own
- * notification as there must be data waiting.
- */
-struct rxrpc_call *rxrpc_kernel_accept_call(struct socket *sock,
-					    unsigned long user_call_ID,
-					    rxrpc_notify_rx_t notify_rx)
-{
-	struct rxrpc_call *call;
-
-	_enter(",%lx", user_call_ID);
-	call = rxrpc_accept_call(rxrpc_sk(sock->sk), user_call_ID, notify_rx);
-	_leave(" = %p", call);
-	return call;
-}
-EXPORT_SYMBOL(rxrpc_kernel_accept_call);
-
-/**
- * rxrpc_kernel_reject_call - Allow a kernel service to reject an incoming call
- * @sock: The socket on which the impending call is waiting
- *
- * Allow a kernel service to reject an incoming call with a BUSY message,
- * assuming the incoming call is still valid.
- */
-int rxrpc_kernel_reject_call(struct socket *sock)
-{
-	int ret;
-
-	_enter("");
-	ret = rxrpc_reject_call(rxrpc_sk(sock->sk));
+	if (abort) {
+		rxrpc_send_call_packet(call, RXRPC_PACKET_TYPE_ABORT);
+		rxrpc_release_call(rx, call);
+		rxrpc_put_call(call, rxrpc_call_put);
+	}
+	rxrpc_service_prealloc(rx, GFP_KERNEL);
 	_leave(" = %d", ret);
 	return ret;
 }
-EXPORT_SYMBOL(rxrpc_kernel_reject_call);
 
 /*
  * rxrpc_kernel_charge_accept - Charge up socket with preallocated calls

commit 00e907127e6f86d0f9b122d9b4347a8aa09a8b61
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 8 11:10:12 2016 +0100

    rxrpc: Preallocate peers, conns and calls for incoming service requests
    
    Make it possible for the data_ready handler called from the UDP transport
    socket to completely instantiate an rxrpc_call structure and make it
    immediately live by preallocating all the memory it might need.  The idea
    is to cut out the background thread usage as much as possible.
    
    [Note that the preallocated structs are not actually used in this patch -
     that will be done in a future patch.]
    
    If insufficient resources are available in the preallocation buffers, it
    will be possible to discard the DATA packet in the data_ready handler or
    schedule a BUSY packet without the need to schedule an attempt at
    allocation in a background thread.
    
    To this end:
    
     (1) Preallocate rxrpc_peer, rxrpc_connection and rxrpc_call structs to a
         maximum number each of the listen backlog size.  The backlog size is
         limited to a maxmimum of 32.  Only this many of each can be in the
         preallocation buffer.
    
     (2) For userspace sockets, the preallocation is charged initially by
         listen() and will be recharged by accepting or rejecting pending
         new incoming calls.
    
     (3) For kernel services {,re,dis}charging of the preallocation buffers is
         handled manually.  Two notifier callbacks have to be provided before
         kernel_listen() is invoked:
    
         (a) An indication that a new call has been instantiated.  This can be
             used to trigger background recharging.
    
         (b) An indication that a call is being discarded.  This is used when
             the socket is being released.
    
         A function, rxrpc_kernel_charge_accept() is called by the kernel
         service to preallocate a single call.  It should be passed the user ID
         to be used for that call and a callback to associate the rxrpc call
         with the kernel service's side of the ID.
    
     (4) Discard the preallocation when the socket is closed.
    
     (5) Temporarily bump the refcount on the call allocated in
         rxrpc_incoming_call() so that rxrpc_release_call() can ditch the
         preallocation ref on service calls unconditionally.  This will no
         longer be necessary once the preallocation is used.
    
    Note that this does not yet control the number of active service calls on a
    client - that will come in a later patch.
    
    A future development would be to provide a setsockopt() call that allows a
    userspace server to manually charge the preallocation buffer.  This would
    allow user call IDs to be provided in advance and the awkward manual accept
    stage to be bypassed.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 4c71efcf82ed..cc7194e05a15 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -20,11 +20,209 @@
 #include <linux/in6.h>
 #include <linux/icmp.h>
 #include <linux/gfp.h>
+#include <linux/circ_buf.h>
 #include <net/sock.h>
 #include <net/af_rxrpc.h>
 #include <net/ip.h>
 #include "ar-internal.h"
 
+/*
+ * Preallocate a single service call, connection and peer and, if possible,
+ * give them a user ID and attach the user's side of the ID to them.
+ */
+static int rxrpc_service_prealloc_one(struct rxrpc_sock *rx,
+				      struct rxrpc_backlog *b,
+				      rxrpc_notify_rx_t notify_rx,
+				      rxrpc_user_attach_call_t user_attach_call,
+				      unsigned long user_call_ID, gfp_t gfp)
+{
+	const void *here = __builtin_return_address(0);
+	struct rxrpc_call *call;
+	int max, tmp;
+	unsigned int size = RXRPC_BACKLOG_MAX;
+	unsigned int head, tail, call_head, call_tail;
+
+	max = rx->sk.sk_max_ack_backlog;
+	tmp = rx->sk.sk_ack_backlog;
+	if (tmp >= max) {
+		_leave(" = -ENOBUFS [full %u]", max);
+		return -ENOBUFS;
+	}
+	max -= tmp;
+
+	/* We don't need more conns and peers than we have calls, but on the
+	 * other hand, we shouldn't ever use more peers than conns or conns
+	 * than calls.
+	 */
+	call_head = b->call_backlog_head;
+	call_tail = READ_ONCE(b->call_backlog_tail);
+	tmp = CIRC_CNT(call_head, call_tail, size);
+	if (tmp >= max) {
+		_leave(" = -ENOBUFS [enough %u]", tmp);
+		return -ENOBUFS;
+	}
+	max = tmp + 1;
+
+	head = b->peer_backlog_head;
+	tail = READ_ONCE(b->peer_backlog_tail);
+	if (CIRC_CNT(head, tail, size) < max) {
+		struct rxrpc_peer *peer = rxrpc_alloc_peer(rx->local, gfp);
+		if (!peer)
+			return -ENOMEM;
+		b->peer_backlog[head] = peer;
+		smp_store_release(&b->peer_backlog_head,
+				  (head + 1) & (size - 1));
+	}
+
+	head = b->conn_backlog_head;
+	tail = READ_ONCE(b->conn_backlog_tail);
+	if (CIRC_CNT(head, tail, size) < max) {
+		struct rxrpc_connection *conn;
+
+		conn = rxrpc_prealloc_service_connection(gfp);
+		if (!conn)
+			return -ENOMEM;
+		b->conn_backlog[head] = conn;
+		smp_store_release(&b->conn_backlog_head,
+				  (head + 1) & (size - 1));
+	}
+
+	/* Now it gets complicated, because calls get registered with the
+	 * socket here, particularly if a user ID is preassigned by the user.
+	 */
+	call = rxrpc_alloc_call(gfp);
+	if (!call)
+		return -ENOMEM;
+	call->flags |= (1 << RXRPC_CALL_IS_SERVICE);
+	call->state = RXRPC_CALL_SERVER_PREALLOC;
+
+	trace_rxrpc_call(call, rxrpc_call_new_service,
+			 atomic_read(&call->usage),
+			 here, (const void *)user_call_ID);
+
+	write_lock(&rx->call_lock);
+	if (user_attach_call) {
+		struct rxrpc_call *xcall;
+		struct rb_node *parent, **pp;
+
+		/* Check the user ID isn't already in use */
+		pp = &rx->calls.rb_node;
+		parent = NULL;
+		while (*pp) {
+			parent = *pp;
+			xcall = rb_entry(parent, struct rxrpc_call, sock_node);
+			if (user_call_ID < call->user_call_ID)
+				pp = &(*pp)->rb_left;
+			else if (user_call_ID > call->user_call_ID)
+				pp = &(*pp)->rb_right;
+			else
+				goto id_in_use;
+		}
+
+		call->user_call_ID = user_call_ID;
+		call->notify_rx = notify_rx;
+		rxrpc_get_call(call, rxrpc_call_got);
+		user_attach_call(call, user_call_ID);
+		rxrpc_get_call(call, rxrpc_call_got_userid);
+		rb_link_node(&call->sock_node, parent, pp);
+		rb_insert_color(&call->sock_node, &rx->calls);
+		set_bit(RXRPC_CALL_HAS_USERID, &call->flags);
+	}
+
+	write_unlock(&rx->call_lock);
+
+	write_lock(&rxrpc_call_lock);
+	list_add_tail(&call->link, &rxrpc_calls);
+	write_unlock(&rxrpc_call_lock);
+
+	b->call_backlog[call_head] = call;
+	smp_store_release(&b->call_backlog_head, (call_head + 1) & (size - 1));
+	_leave(" = 0 [%d -> %lx]", call->debug_id, user_call_ID);
+	return 0;
+
+id_in_use:
+	write_unlock(&rx->call_lock);
+	rxrpc_cleanup_call(call);
+	_leave(" = -EBADSLT");
+	return -EBADSLT;
+}
+
+/*
+ * Preallocate sufficient service connections, calls and peers to cover the
+ * entire backlog of a socket.  When a new call comes in, if we don't have
+ * sufficient of each available, the call gets rejected as busy or ignored.
+ *
+ * The backlog is replenished when a connection is accepted or rejected.
+ */
+int rxrpc_service_prealloc(struct rxrpc_sock *rx, gfp_t gfp)
+{
+	struct rxrpc_backlog *b = rx->backlog;
+
+	if (!b) {
+		b = kzalloc(sizeof(struct rxrpc_backlog), gfp);
+		if (!b)
+			return -ENOMEM;
+		rx->backlog = b;
+	}
+
+	if (rx->discard_new_call)
+		return 0;
+
+	while (rxrpc_service_prealloc_one(rx, b, NULL, NULL, 0, gfp) == 0)
+		;
+
+	return 0;
+}
+
+/*
+ * Discard the preallocation on a service.
+ */
+void rxrpc_discard_prealloc(struct rxrpc_sock *rx)
+{
+	struct rxrpc_backlog *b = rx->backlog;
+	unsigned int size = RXRPC_BACKLOG_MAX, head, tail;
+
+	if (!b)
+		return;
+	rx->backlog = NULL;
+
+	head = b->peer_backlog_head;
+	tail = b->peer_backlog_tail;
+	while (CIRC_CNT(head, tail, size) > 0) {
+		struct rxrpc_peer *peer = b->peer_backlog[tail];
+		kfree(peer);
+		tail = (tail + 1) & (size - 1);
+	}
+
+	head = b->conn_backlog_head;
+	tail = b->conn_backlog_tail;
+	while (CIRC_CNT(head, tail, size) > 0) {
+		struct rxrpc_connection *conn = b->conn_backlog[tail];
+		write_lock(&rxrpc_connection_lock);
+		list_del(&conn->link);
+		list_del(&conn->proc_link);
+		write_unlock(&rxrpc_connection_lock);
+		kfree(conn);
+		tail = (tail + 1) & (size - 1);
+	}
+
+	head = b->call_backlog_head;
+	tail = b->call_backlog_tail;
+	while (CIRC_CNT(head, tail, size) > 0) {
+		struct rxrpc_call *call = b->call_backlog[tail];
+		if (rx->discard_new_call) {
+			_debug("discard %lx", call->user_call_ID);
+			rx->discard_new_call(call, call->user_call_ID);
+		}
+		rxrpc_call_completed(call);
+		rxrpc_release_call(rx, call);
+		rxrpc_put_call(call, rxrpc_call_put);
+		tail = (tail + 1) & (size - 1);
+	}
+
+	kfree(b);
+}
+
 /*
  * generate a connection-level abort
  */
@@ -450,3 +648,34 @@ int rxrpc_kernel_reject_call(struct socket *sock)
 	return ret;
 }
 EXPORT_SYMBOL(rxrpc_kernel_reject_call);
+
+/*
+ * rxrpc_kernel_charge_accept - Charge up socket with preallocated calls
+ * @sock: The socket on which to preallocate
+ * @notify_rx: Event notification function for the call
+ * @user_attach_call: Func to attach call to user_call_ID
+ * @user_call_ID: The tag to attach to the preallocated call
+ * @gfp: The allocation conditions.
+ *
+ * Charge up the socket with preallocated calls, each with a user ID.  A
+ * function should be provided to effect the attachment from the user's side.
+ * The user is given a ref to hold on the call.
+ *
+ * Note that the call may be come connected before this function returns.
+ */
+int rxrpc_kernel_charge_accept(struct socket *sock,
+			       rxrpc_notify_rx_t notify_rx,
+			       rxrpc_user_attach_call_t user_attach_call,
+			       unsigned long user_call_ID, gfp_t gfp)
+{
+	struct rxrpc_sock *rx = rxrpc_sk(sock->sk);
+	struct rxrpc_backlog *b = rx->backlog;
+
+	if (sock->sk->sk_state == RXRPC_CLOSE)
+		return -ESHUTDOWN;
+
+	return rxrpc_service_prealloc_one(rx, b, notify_rx,
+					  user_attach_call, user_call_ID,
+					  gfp);
+}
+EXPORT_SYMBOL(rxrpc_kernel_charge_accept);

commit de8d6c7401ae8f25db3788804c86887ad7347bee
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 8 11:10:11 2016 +0100

    rxrpc: Convert rxrpc_local::services to an hlist
    
    Convert the rxrpc_local::services list to an hlist so that it can be
    accessed under RCU conditions more readily.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 879a964de80c..4c71efcf82ed 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -217,7 +217,7 @@ void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
 
 	/* get the socket providing the service */
 	read_lock_bh(&local->services_lock);
-	list_for_each_entry(rx, &local->services, listen_link) {
+	hlist_for_each_entry(rx, &local->services, listen_link) {
 		if (rx->srx.srx_service == sp->hdr.serviceId &&
 		    rx->sk.sk_state != RXRPC_CLOSE)
 			goto found_service;

commit 8d94aa381dab19f3c0f524f5d255248b0ae50125
Author: David Howells <dhowells@redhat.com>
Date:   Wed Sep 7 09:19:31 2016 +0100

    rxrpc: Calls shouldn't hold socket refs
    
    rxrpc calls shouldn't hold refs on the sock struct.  This was done so that
    the socket wouldn't go away whilst the call was in progress, such that the
    call could reach the socket's queues.
    
    However, we can mark the socket as requiring an RCU release and rely on the
    RCU read lock.
    
    To make this work, we do:
    
     (1) rxrpc_release_call() removes the call's call user ID.  This is now
         only called from socket operations and not from the call processor:
    
            rxrpc_accept_call() / rxrpc_kernel_accept_call()
            rxrpc_reject_call() / rxrpc_kernel_reject_call()
            rxrpc_kernel_end_call()
            rxrpc_release_calls_on_socket()
            rxrpc_recvmsg()
    
         Though it is also called in the cleanup path of
         rxrpc_accept_incoming_call() before we assign a user ID.
    
     (2) Pass the socket pointer into rxrpc_release_call() rather than getting
         it from the call so that we can get rid of uninitialised calls.
    
     (3) Fix call processor queueing to pass a ref to the work queue and to
         release that ref at the end of the processor function (or to pass it
         back to the work queue if we have to requeue).
    
     (4) Skip out of the call processor function asap if the call is complete
         and don't requeue it if the call is complete.
    
     (5) Clean up the call immediately that the refcount reaches 0 rather than
         trying to defer it.  Actual deallocation is deferred to RCU, however.
    
     (6) Don't hold socket refs for allocated calls.
    
     (7) Use the RCU read lock when queueing a message on a socket and treat
         the call's socket pointer according to RCU rules and check it for
         NULL.
    
         We also need to use the RCU read lock when viewing a call through
         procfs.
    
     (8) Transmit the final ACK/ABORT to a client call in rxrpc_release_call()
         if this hasn't been done yet so that we can then disconnect the call.
         Once the call is disconnected, it won't have any access to the
         connection struct and the UDP socket for the call work processor to be
         able to send the ACK.  Terminal retransmission will be handled by the
         connection processor.
    
     (9) Release all calls immediately on the closing of a socket rather than
         trying to defer this.  Incomplete calls will be aborted.
    
    The call refcount model is much simplified.  Refs are held on the call by:
    
     (1) A socket's user ID tree.
    
     (2) A socket's incoming call secureq and acceptq.
    
     (3) A kernel service that has a call in progress.
    
     (4) A queued call work processor.  We have to take care to put any call
         that we failed to queue.
    
     (5) sk_buffs on a socket's receive queue.  A future patch will get rid of
         this.
    
    Whilst we're at it, we can do:
    
     (1) Get rid of the RXRPC_CALL_EV_RELEASE event.  Release is now done
         entirely from the socket routines and never from the call's processor.
    
     (2) Get rid of the RXRPC_CALL_DEAD state.  Calls now end in the
         RXRPC_CALL_COMPLETE state.
    
     (3) Get rid of the rxrpc_call::destroyer work item.  Calls are now torn
         down when their refcount reaches 0 and then handed over to RCU for
         final cleanup.
    
     (4) Get rid of the rxrpc_call::deadspan timer.  Calls are cleaned up
         immediately they're finished with and don't hang around.
         Post-completion retransmission is handled by the connection processor
         once the call is disconnected.
    
     (5) Get rid of the dead call expiry setting as there's no longer a timer
         to set.
    
     (6) rxrpc_destroy_all_calls() can just check that the call list is empty.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 487ae7aa86db..879a964de80c 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -163,13 +163,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 	_debug("invalid");
 	read_unlock_bh(&local->services_lock);
 
-	read_lock_bh(&call->state_lock);
-	if (!test_bit(RXRPC_CALL_RELEASED, &call->flags) &&
-	    !test_and_set_bit(RXRPC_CALL_EV_RELEASE, &call->events)) {
-		rxrpc_get_call(call, rxrpc_call_got);
-		rxrpc_queue_call(call);
-	}
-	read_unlock_bh(&call->state_lock);
+	rxrpc_release_call(rx, call);
 	rxrpc_put_call(call, rxrpc_call_put);
 	ret = -ECONNREFUSED;
 error:
@@ -236,13 +230,11 @@ void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
 	if (sk_acceptq_is_full(&rx->sk))
 		goto backlog_full;
 	sk_acceptq_added(&rx->sk);
-	sock_hold(&rx->sk);
 	read_unlock_bh(&local->services_lock);
 
 	ret = rxrpc_accept_incoming_call(local, rx, skb, &srx);
 	if (ret < 0)
 		sk_acceptq_removed(&rx->sk);
-	sock_put(&rx->sk);
 	switch (ret) {
 	case -ECONNRESET: /* old calls are ignored */
 	case -ECONNABORTED: /* aborted calls are reaborted or ignored */
@@ -333,9 +325,6 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	case RXRPC_CALL_COMPLETE:
 		ret = call->error;
 		goto out_release;
-	case RXRPC_CALL_DEAD:
-		ret = -ETIME;
-		goto out_discard;
 	default:
 		BUG();
 	}
@@ -350,24 +339,20 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 		BUG();
 	if (test_and_set_bit(RXRPC_CALL_EV_ACCEPTED, &call->events))
 		BUG();
-	rxrpc_queue_call(call);
 
 	write_unlock_bh(&call->state_lock);
 	write_unlock(&rx->call_lock);
+	rxrpc_queue_call(call);
 	_leave(" = %p{%d}", call, call->debug_id);
 	return call;
 
-	/* if the call is already dying or dead, then we leave the socket's ref
-	 * on it to be released by rxrpc_dead_call_expired() as induced by
-	 * rxrpc_release_call() */
 out_release:
-	_debug("release %p", call);
-	if (!test_bit(RXRPC_CALL_RELEASED, &call->flags) &&
-	    !test_and_set_bit(RXRPC_CALL_EV_RELEASE, &call->events))
-		rxrpc_queue_call(call);
-out_discard:
 	write_unlock_bh(&call->state_lock);
-	_debug("discard %p", call);
+	write_unlock(&rx->call_lock);
+	_debug("release %p", call);
+	rxrpc_release_call(rx, call);
+	_leave(" = %d", ret);
+	return ERR_PTR(ret);
 out:
 	write_unlock(&rx->call_lock);
 	_leave(" = %d", ret);
@@ -390,8 +375,11 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 	write_lock(&rx->call_lock);
 
 	ret = -ENODATA;
-	if (list_empty(&rx->acceptq))
-		goto out;
+	if (list_empty(&rx->acceptq)) {
+		write_unlock(&rx->call_lock);
+		_leave(" = -ENODATA");
+		return -ENODATA;
+	}
 
 	/* dequeue the first call and check it's still valid */
 	call = list_entry(rx->acceptq.next, struct rxrpc_call, accept_link);
@@ -407,30 +395,17 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 		if (test_and_set_bit(RXRPC_CALL_EV_REJECT_BUSY, &call->events))
 			rxrpc_queue_call(call);
 		ret = 0;
-		goto out_release;
+		break;
 	case RXRPC_CALL_COMPLETE:
 		ret = call->error;
-		goto out_release;
-	case RXRPC_CALL_DEAD:
-		ret = -ETIME;
-		goto out_discard;
+		break;
 	default:
 		BUG();
 	}
 
-	/* if the call is already dying or dead, then we leave the socket's ref
-	 * on it to be released by rxrpc_dead_call_expired() as induced by
-	 * rxrpc_release_call() */
-out_release:
-	_debug("release %p", call);
-	if (!test_bit(RXRPC_CALL_RELEASED, &call->flags) &&
-	    !test_and_set_bit(RXRPC_CALL_EV_RELEASE, &call->events))
-		rxrpc_queue_call(call);
-out_discard:
 	write_unlock_bh(&call->state_lock);
-	_debug("discard %p", call);
-out:
 	write_unlock(&rx->call_lock);
+	rxrpc_release_call(rx, call);
 	_leave(" = %d", ret);
 	return ret;
 }

commit fff72429c2e83bdbe32dc7f1ad6398dfe50750c6
Author: David Howells <dhowells@redhat.com>
Date:   Wed Sep 7 14:34:21 2016 +0100

    rxrpc: Improve the call tracking tracepoint
    
    Improve the call tracking tracepoint by showing more differentiation
    between some of the put and get events, including:
    
      (1) Getting and putting refs for the socket call user ID tree.
    
      (2) Getting and putting refs for queueing and failing to queue the call
          processor work item.
    
    Note that these aren't necessarily used in this patch, but will be taken
    advantage of in future patches.
    
    An enum is added for the event subtype numbers rather than coding them
    directly as decimal numbers and a table of 3-letter strings is provided
    rather than a sequence of ?: operators.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 68a439e30df1..487ae7aa86db 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -115,7 +115,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 
 	write_lock(&rx->call_lock);
 	if (!test_and_set_bit(RXRPC_CALL_INIT_ACCEPT, &call->flags)) {
-		rxrpc_get_call(call);
+		rxrpc_get_call(call, rxrpc_call_got);
 
 		spin_lock(&call->conn->state_lock);
 		if (sp->hdr.securityIndex > 0 &&
@@ -155,7 +155,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 	_debug("done");
 	read_unlock_bh(&local->services_lock);
 	rxrpc_free_skb(notification);
-	rxrpc_put_call(call);
+	rxrpc_put_call(call, rxrpc_call_put);
 	_leave(" = 0");
 	return 0;
 
@@ -166,11 +166,11 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 	read_lock_bh(&call->state_lock);
 	if (!test_bit(RXRPC_CALL_RELEASED, &call->flags) &&
 	    !test_and_set_bit(RXRPC_CALL_EV_RELEASE, &call->events)) {
-		rxrpc_get_call(call);
+		rxrpc_get_call(call, rxrpc_call_got);
 		rxrpc_queue_call(call);
 	}
 	read_unlock_bh(&call->state_lock);
-	rxrpc_put_call(call);
+	rxrpc_put_call(call, rxrpc_call_put);
 	ret = -ECONNREFUSED;
 error:
 	rxrpc_free_skb(notification);
@@ -341,6 +341,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	}
 
 	/* formalise the acceptance */
+	rxrpc_get_call(call, rxrpc_call_got_userid);
 	call->notify_rx = notify_rx;
 	call->user_call_ID = user_call_ID;
 	rb_link_node(&call->sock_node, parent, pp);
@@ -351,7 +352,6 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 		BUG();
 	rxrpc_queue_call(call);
 
-	rxrpc_get_call(call);
 	write_unlock_bh(&call->state_lock);
 	write_unlock(&rx->call_lock);
 	_leave(" = %p{%d}", call, call->debug_id);

commit d001648ec7cf8b21ae9eec8b9ba4a18295adfb14
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 30 20:42:14 2016 +0100

    rxrpc: Don't expose skbs to in-kernel users [ver #2]
    
    Don't expose skbs to in-kernel users, such as the AFS filesystem, but
    instead provide a notification hook the indicates that a call needs
    attention and another that indicates that there's a new call to be
    collected.
    
    This makes the following possibilities more achievable:
    
     (1) Call refcounting can be made simpler if skbs don't hold refs to calls.
    
     (2) skbs referring to non-data events will be able to be freed much sooner
         rather than being queued for AFS to pick up as rxrpc_kernel_recv_data
         will be able to consult the call state.
    
     (3) We can shortcut the receive phase when a call is remotely aborted
         because we don't have to go through all the packets to get to the one
         cancelling the operation.
    
     (4) It makes it easier to do encryption/decryption directly between AFS's
         buffers and sk_buffs.
    
     (5) Encryption/decryption can more easily be done in the AFS's thread
         contexts - usually that of the userspace process that issued a syscall
         - rather than in one of rxrpc's background threads on a workqueue.
    
     (6) AFS will be able to wait synchronously on a call inside AF_RXRPC.
    
    To make this work, the following interface function has been added:
    
         int rxrpc_kernel_recv_data(
                    struct socket *sock, struct rxrpc_call *call,
                    void *buffer, size_t bufsize, size_t *_offset,
                    bool want_more, u32 *_abort_code);
    
    This is the recvmsg equivalent.  It allows the caller to find out about the
    state of a specific call and to transfer received data into a buffer
    piecemeal.
    
    afs_extract_data() and rxrpc_kernel_recv_data() now do all the extraction
    logic between them.  They don't wait synchronously yet because the socket
    lock needs to be dealt with.
    
    Five interface functions have been removed:
    
            rxrpc_kernel_is_data_last()
            rxrpc_kernel_get_abort_code()
            rxrpc_kernel_get_error_number()
            rxrpc_kernel_free_skb()
            rxrpc_kernel_data_consumed()
    
    As a temporary hack, sk_buffs going to an in-kernel call are queued on the
    rxrpc_call struct (->knlrecv_queue) rather than being handed over to the
    in-kernel user.  To process the queue internally, a temporary function,
    temp_deliver_data() has been added.  This will be replaced with common code
    between the rxrpc_recvmsg() path and the kernel_rxrpc_recv_data() path in a
    future patch.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 03af88fe798b..68a439e30df1 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -286,7 +286,8 @@ void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
  * - assign the user call ID to the call at the front of the queue
  */
 struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
-				     unsigned long user_call_ID)
+				     unsigned long user_call_ID,
+				     rxrpc_notify_rx_t notify_rx)
 {
 	struct rxrpc_call *call;
 	struct rb_node *parent, **pp;
@@ -340,6 +341,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	}
 
 	/* formalise the acceptance */
+	call->notify_rx = notify_rx;
 	call->user_call_ID = user_call_ID;
 	rb_link_node(&call->sock_node, parent, pp);
 	rb_insert_color(&call->sock_node, &rx->calls);
@@ -437,17 +439,20 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
  * rxrpc_kernel_accept_call - Allow a kernel service to accept an incoming call
  * @sock: The socket on which the impending call is waiting
  * @user_call_ID: The tag to attach to the call
+ * @notify_rx: Where to send notifications instead of socket queue
  *
  * Allow a kernel service to accept an incoming call, assuming the incoming
- * call is still valid.
+ * call is still valid.  The caller should immediately trigger their own
+ * notification as there must be data waiting.
  */
 struct rxrpc_call *rxrpc_kernel_accept_call(struct socket *sock,
-					    unsigned long user_call_ID)
+					    unsigned long user_call_ID,
+					    rxrpc_notify_rx_t notify_rx)
 {
 	struct rxrpc_call *call;
 
 	_enter(",%lx", user_call_ID);
-	call = rxrpc_accept_call(rxrpc_sk(sock->sk), user_call_ID);
+	call = rxrpc_accept_call(rxrpc_sk(sock->sk), user_call_ID, notify_rx);
 	_leave(" = %p", call);
 	return call;
 }

commit e34d4234b0b77a8a8b6dd7cf29aff468c288d9e4
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 30 09:49:29 2016 +0100

    rxrpc: Trace rxrpc_call usage
    
    Add a trace event for debuging rxrpc_call struct usage.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index ef9ef0d6c917..03af88fe798b 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -129,8 +129,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 			_debug("conn ready");
 			call->state = RXRPC_CALL_SERVER_ACCEPTING;
 			list_add_tail(&call->accept_link, &rx->acceptq);
-			rxrpc_get_call(call);
-			atomic_inc(&call->skb_count);
+			rxrpc_get_call_for_skb(call, notification);
 			nsp = rxrpc_skb(notification);
 			nsp->call = call;
 
@@ -323,6 +322,7 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	call = list_entry(rx->acceptq.next, struct rxrpc_call, accept_link);
 	list_del_init(&call->accept_link);
 	sk_acceptq_removed(&rx->sk);
+	rxrpc_see_call(call);
 
 	write_lock_bh(&call->state_lock);
 	switch (call->state) {
@@ -395,6 +395,7 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 	call = list_entry(rx->acceptq.next, struct rxrpc_call, accept_link);
 	list_del_init(&call->accept_link);
 	sk_acceptq_removed(&rx->sk);
+	rxrpc_see_call(call);
 
 	write_lock_bh(&call->state_lock);
 	switch (call->state) {

commit f5c17aaeb2aee9b6c30d082bbe652a7e5589adff
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 30 09:49:28 2016 +0100

    rxrpc: Calls should only have one terminal state
    
    Condense the terminal states of a call state machine to a single state,
    plus a separate completion type value.  The value is then set, along with
    error and abort code values, only when the call is transitioned to the
    completion state.
    
    Helpers are provided to simplify this.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 669ac79d3b44..ef9ef0d6c917 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -329,12 +329,8 @@ struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
 	case RXRPC_CALL_SERVER_ACCEPTING:
 		call->state = RXRPC_CALL_SERVER_RECV_REQUEST;
 		break;
-	case RXRPC_CALL_REMOTELY_ABORTED:
-	case RXRPC_CALL_LOCALLY_ABORTED:
-		ret = -ECONNABORTED;
-		goto out_release;
-	case RXRPC_CALL_NETWORK_ERROR:
-		ret = call->conn->error;
+	case RXRPC_CALL_COMPLETE:
+		ret = call->error;
 		goto out_release;
 	case RXRPC_CALL_DEAD:
 		ret = -ETIME;
@@ -403,17 +399,14 @@ int rxrpc_reject_call(struct rxrpc_sock *rx)
 	write_lock_bh(&call->state_lock);
 	switch (call->state) {
 	case RXRPC_CALL_SERVER_ACCEPTING:
-		call->state = RXRPC_CALL_SERVER_BUSY;
+		__rxrpc_set_call_completion(call, RXRPC_CALL_SERVER_BUSY,
+					    0, ECONNABORTED);
 		if (test_and_set_bit(RXRPC_CALL_EV_REJECT_BUSY, &call->events))
 			rxrpc_queue_call(call);
 		ret = 0;
 		goto out_release;
-	case RXRPC_CALL_REMOTELY_ABORTED:
-	case RXRPC_CALL_LOCALLY_ABORTED:
-		ret = -ECONNABORTED;
-		goto out_release;
-	case RXRPC_CALL_NETWORK_ERROR:
-		ret = call->conn->error;
+	case RXRPC_CALL_COMPLETE:
+		ret = call->error;
 		goto out_release;
 	case RXRPC_CALL_DEAD:
 		ret = -ETIME;

commit df844fd46b98c2efde8f4ac2d50d59bc90c4c679
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 23 15:27:24 2016 +0100

    rxrpc: Use a tracepoint for skb accounting debugging
    
    Use a tracepoint to log various skb accounting points to help in debugging
    refcounting errors.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 9bae21e66d65..669ac79d3b44 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -203,6 +203,7 @@ void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
 
 	_net("incoming call skb %p", skb);
 
+	rxrpc_see_skb(skb);
 	sp = rxrpc_skb(skb);
 
 	/* Set up a response packet header in case we need it */

commit 372ee16386bbf6dc5eeb0387e1ede963debba82a
Author: David Howells <dhowells@redhat.com>
Date:   Wed Aug 3 14:11:40 2016 +0100

    rxrpc: Fix races between skb free, ACK generation and replying
    
    Inside the kafs filesystem it is possible to occasionally have a call
    processed and terminated before we've had a chance to check whether we need
    to clean up the rx queue for that call because afs_send_simple_reply() ends
    the call when it is done, but this is done in a workqueue item that might
    happen to run to completion before afs_deliver_to_call() completes.
    
    Further, it is possible for rxrpc_kernel_send_data() to be called to send a
    reply before the last request-phase data skb is released.  The rxrpc skb
    destructor is where the ACK processing is done and the call state is
    advanced upon release of the last skb.  ACK generation is also deferred to
    a work item because it's possible that the skb destructor is not called in
    a context where kernel_sendmsg() can be invoked.
    
    To this end, the following changes are made:
    
     (1) kernel_rxrpc_data_consumed() is added.  This should be called whenever
         an skb is emptied so as to crank the ACK and call states.  This does
         not release the skb, however.  kernel_rxrpc_free_skb() must now be
         called to achieve that.  These together replace
         rxrpc_kernel_data_delivered().
    
     (2) kernel_rxrpc_data_consumed() is wrapped by afs_data_consumed().
    
         This makes afs_deliver_to_call() easier to work as the skb can simply
         be discarded unconditionally here without trying to work out what the
         return value of the ->deliver() function means.
    
         The ->deliver() functions can, via afs_data_complete(),
         afs_transfer_reply() and afs_extract_data() mark that an skb has been
         consumed (thereby cranking the state) without the need to
         conditionally free the skb to make sure the state is correct on an
         incoming call for when the call processor tries to send the reply.
    
     (3) rxrpc_recvmsg() now has to call kernel_rxrpc_data_consumed() when it
         has finished with a packet and MSG_PEEK isn't set.
    
     (4) rxrpc_packet_destructor() no longer calls rxrpc_hard_ACK_data().
    
         Because of this, we no longer need to clear the destructor and put the
         call before we free the skb in cases where we don't want the ACK/call
         state to be cranked.
    
     (5) The ->deliver() call-type callbacks are made to return -EAGAIN rather
         than 0 if they expect more data (afs_extract_data() returns -EAGAIN to
         the delivery function already), and the caller is now responsible for
         producing an abort if that was the last packet.
    
     (6) There are many bits of unmarshalling code where:
    
                    ret = afs_extract_data(call, skb, last, ...);
                    switch (ret) {
                    case 0:         break;
                    case -EAGAIN:   return 0;
                    default:        return ret;
                    }
    
         is to be found.  As -EAGAIN can now be passed back to the caller, we
         now just return if ret < 0:
    
                    ret = afs_extract_data(call, skb, last, ...);
                    if (ret < 0)
                            return ret;
    
     (7) Checks for trailing data and empty final data packets has been
         consolidated as afs_data_complete().  So:
    
                    if (skb->len > 0)
                            return -EBADMSG;
                    if (!last)
                            return 0;
    
         becomes:
    
                    ret = afs_data_complete(call, skb, last);
                    if (ret < 0)
                            return ret;
    
     (8) afs_transfer_reply() now checks the amount of data it has against the
         amount of data desired and the amount of data in the skb and returns
         an error to induce an abort if we don't get exactly what we want.
    
    Without these changes, the following oops can occasionally be observed,
    particularly if some printks are inserted into the delivery path:
    
    general protection fault: 0000 [#1] SMP
    Modules linked in: kafs(E) af_rxrpc(E) [last unloaded: af_rxrpc]
    CPU: 0 PID: 1305 Comm: kworker/u8:3 Tainted: G            E   4.7.0-fsdevel+ #1303
    Hardware name: ASUS All Series/H97-PLUS, BIOS 2306 10/09/2014
    Workqueue: kafsd afs_async_workfn [kafs]
    task: ffff88040be041c0 ti: ffff88040c070000 task.ti: ffff88040c070000
    RIP: 0010:[<ffffffff8108fd3c>]  [<ffffffff8108fd3c>] __lock_acquire+0xcf/0x15a1
    RSP: 0018:ffff88040c073bc0  EFLAGS: 00010002
    RAX: 6b6b6b6b6b6b6b6b RBX: 0000000000000000 RCX: ffff88040d29a710
    RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff88040d29a710
    RBP: ffff88040c073c70 R08: 0000000000000001 R09: 0000000000000001
    R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000000
    R13: 0000000000000000 R14: ffff88040be041c0 R15: ffffffff814c928f
    FS:  0000000000000000(0000) GS:ffff88041fa00000(0000) knlGS:0000000000000000
    CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    CR2: 00007fa4595f4750 CR3: 0000000001c14000 CR4: 00000000001406f0
    Stack:
     0000000000000006 000000000be04930 0000000000000000 ffff880400000000
     ffff880400000000 ffffffff8108f847 ffff88040be041c0 ffffffff81050446
     ffff8803fc08a920 ffff8803fc08a958 ffff88040be041c0 ffff88040c073c38
    Call Trace:
     [<ffffffff8108f847>] ? mark_held_locks+0x5e/0x74
     [<ffffffff81050446>] ? __local_bh_enable_ip+0x9b/0xa1
     [<ffffffff8108f9ca>] ? trace_hardirqs_on_caller+0x16d/0x189
     [<ffffffff810915f4>] lock_acquire+0x122/0x1b6
     [<ffffffff810915f4>] ? lock_acquire+0x122/0x1b6
     [<ffffffff814c928f>] ? skb_dequeue+0x18/0x61
     [<ffffffff81609dbf>] _raw_spin_lock_irqsave+0x35/0x49
     [<ffffffff814c928f>] ? skb_dequeue+0x18/0x61
     [<ffffffff814c928f>] skb_dequeue+0x18/0x61
     [<ffffffffa009aa92>] afs_deliver_to_call+0x344/0x39d [kafs]
     [<ffffffffa009ab37>] afs_process_async_call+0x4c/0xd5 [kafs]
     [<ffffffffa0099e9c>] afs_async_workfn+0xe/0x10 [kafs]
     [<ffffffff81063a3a>] process_one_work+0x29d/0x57c
     [<ffffffff81064ac2>] worker_thread+0x24a/0x385
     [<ffffffff81064878>] ? rescuer_thread+0x2d0/0x2d0
     [<ffffffff810696f5>] kthread+0xf3/0xfb
     [<ffffffff8160a6ff>] ret_from_fork+0x1f/0x40
     [<ffffffff81069602>] ? kthread_create_on_node+0x1cf/0x1cf
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 0b2832141bd0..9bae21e66d65 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -130,6 +130,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 			call->state = RXRPC_CALL_SERVER_ACCEPTING;
 			list_add_tail(&call->accept_link, &rx->acceptq);
 			rxrpc_get_call(call);
+			atomic_inc(&call->skb_count);
 			nsp = rxrpc_skb(notification);
 			nsp->call = call;
 

commit d991b4a32f65076efaf78739c4a46406ca8c7e79
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jun 29 14:40:39 2016 +0100

    rxrpc: Move peer lookup from call-accept to new-incoming-conn
    
    Move the lookup of a peer from a call that's being accepted into the
    function that creates a new incoming connection.  This will allow us to
    avoid incrementing the peer's usage count in some cases in future.
    
    Note that I haven't bother to integrate rxrpc_get_addr_from_skb() with
    rxrpc_extract_addr_from_skb() as I'm going to delete the former in the very
    near future.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 5367dbe9b96f..0b2832141bd0 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -75,7 +75,6 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 {
 	struct rxrpc_connection *conn;
 	struct rxrpc_skb_priv *sp, *nsp;
-	struct rxrpc_peer *peer;
 	struct rxrpc_call *call;
 	struct sk_buff *notification;
 	int ret;
@@ -94,15 +93,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 	rxrpc_new_skb(notification);
 	notification->mark = RXRPC_SKB_MARK_NEW_CALL;
 
-	peer = rxrpc_lookup_peer(local, srx, GFP_NOIO);
-	if (!peer) {
-		_debug("no peer");
-		ret = -EBUSY;
-		goto error;
-	}
-
-	conn = rxrpc_incoming_connection(local, peer, skb);
-	rxrpc_put_peer(peer);
+	conn = rxrpc_incoming_connection(local, srx, skb);
 	if (IS_ERR(conn)) {
 		_debug("no conn");
 		ret = PTR_ERR(conn);
@@ -226,20 +217,8 @@ void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
 	whdr._rsvd	= 0;
 	whdr.serviceId	= htons(sp->hdr.serviceId);
 
-	/* determine the remote address */
-	memset(&srx, 0, sizeof(srx));
-	srx.srx_family = AF_RXRPC;
-	srx.transport.family = local->srx.transport.family;
-	srx.transport_type = local->srx.transport_type;
-	switch (srx.transport.family) {
-	case AF_INET:
-		srx.transport_len = sizeof(struct sockaddr_in);
-		srx.transport.sin.sin_port = udp_hdr(skb)->source;
-		srx.transport.sin.sin_addr.s_addr = ip_hdr(skb)->saddr;
-		break;
-	default:
-		goto busy;
-	}
+	if (rxrpc_extract_addr_from_skb(&srx, skb) < 0)
+		goto drop;
 
 	/* get the socket providing the service */
 	read_lock_bh(&local->services_lock);
@@ -285,6 +264,10 @@ void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
 	rxrpc_free_skb(skb);
 	return;
 
+drop:
+	rxrpc_free_skb(skb);
+	return;
+
 invalid_service:
 	skb->priority = RX_INVALID_OPERATION;
 	rxrpc_reject_packet(local, skb);

commit 2c4579e4b1d5a6219522c6e970500b2fd43fe1f8
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 27 10:32:03 2016 +0100

    rxrpc: Move usage count getting into rxrpc_queue_conn()
    
    Rather than calling rxrpc_get_connection() manually before calling
    rxrpc_queue_conn(), do it inside the queue wrapper.
    
    This allows us to do some important fixes:
    
     (1) If the usage count is 0, do nothing.  This prevents connections from
         being reanimated once they're dead.
    
     (2) If rxrpc_queue_work() fails because the work item is already queued,
         retract the usage count increment which would otherwise be lost.
    
     (3) Don't take a ref on the connection in the work function.  By passing
         the ref through the work item, this is unnecessary.  Doing it in the
         work function is too late anyway.  Previously, connection-directed
         packets held a ref on the connection, but that's not really the best
         idea.
    
    And another useful changes:
    
     (*) Don't need to take a refcount on the connection in the data_ready
         handler unless we invoke the connection's work item.  We're using RCU
         there so that's otherwise redundant.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 1c0860df150e..5367dbe9b96f 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -132,7 +132,6 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 			_debug("await conn sec");
 			list_add_tail(&call->accept_link, &rx->secureq);
 			call->conn->state = RXRPC_CONN_SERVICE_CHALLENGING;
-			rxrpc_get_connection(call->conn);
 			set_bit(RXRPC_CONN_EV_CHALLENGE, &call->conn->events);
 			rxrpc_queue_conn(call->conn);
 		} else {

commit bba304db34ec3ca0d13e7f48e5a4e9896536cacc
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 27 10:32:02 2016 +0100

    rxrpc: Turn connection #defines into enums and put outside struct def
    
    Turn the connection event and state #define lists into enums and move
    outside of the struct definition.
    
    Whilst we're at it, change _SERVER to _SERVICE in those identifiers and add
    EV_ into the event name to distinguish them from flags and states.
    
    Also add a symbol indicating the number of states and use that in the state
    text array.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 202e053a3c6d..1c0860df150e 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -128,12 +128,12 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 
 		spin_lock(&call->conn->state_lock);
 		if (sp->hdr.securityIndex > 0 &&
-		    call->conn->state == RXRPC_CONN_SERVER_UNSECURED) {
+		    call->conn->state == RXRPC_CONN_SERVICE_UNSECURED) {
 			_debug("await conn sec");
 			list_add_tail(&call->accept_link, &rx->secureq);
-			call->conn->state = RXRPC_CONN_SERVER_CHALLENGING;
+			call->conn->state = RXRPC_CONN_SERVICE_CHALLENGING;
 			rxrpc_get_connection(call->conn);
-			set_bit(RXRPC_CONN_CHALLENGE, &call->conn->events);
+			set_bit(RXRPC_CONN_EV_CHALLENGE, &call->conn->events);
 			rxrpc_queue_conn(call->conn);
 		} else {
 			_debug("conn ready");

commit aa390bbe2113dd0de99cf35c39d7701d4412b744
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jun 17 10:06:56 2016 +0100

    rxrpc: Kill off the rxrpc_transport struct
    
    The rxrpc_transport struct is now redundant, given that the rxrpc_peer
    struct is now per peer port rather than per peer host, so get rid of it.
    
    Service connection lists are transferred to the rxrpc_peer struct, as is
    the conn_lock.  Previous patches moved the client connection handling out
    of the rxrpc_transport struct and discarded the connection bundling code.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 833ad0622b61..202e053a3c6d 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -74,7 +74,6 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 				      struct sockaddr_rxrpc *srx)
 {
 	struct rxrpc_connection *conn;
-	struct rxrpc_transport *trans;
 	struct rxrpc_skb_priv *sp, *nsp;
 	struct rxrpc_peer *peer;
 	struct rxrpc_call *call;
@@ -102,16 +101,8 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 		goto error;
 	}
 
-	trans = rxrpc_get_transport(local, peer, GFP_NOIO);
+	conn = rxrpc_incoming_connection(local, peer, skb);
 	rxrpc_put_peer(peer);
-	if (IS_ERR(trans)) {
-		_debug("no trans");
-		ret = -EBUSY;
-		goto error;
-	}
-
-	conn = rxrpc_incoming_connection(trans, skb);
-	rxrpc_put_transport(trans);
 	if (IS_ERR(conn)) {
 		_debug("no conn");
 		ret = PTR_ERR(conn);

commit 5627cc8b961e4b07d5d649d9bd01ac929dcc1a95
Author: David Howells <dhowells@redhat.com>
Date:   Mon Apr 4 14:00:38 2016 +0100

    rxrpc: Provide more refcount helper functions
    
    Provide refcount helper functions for connections so that the code doesn't
    touch local or connection usage counts directly.
    
    Also make it such that local and peer put functions can take a NULL
    pointer.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 5a70dc4e28c6..833ad0622b61 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -141,7 +141,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 			_debug("await conn sec");
 			list_add_tail(&call->accept_link, &rx->secureq);
 			call->conn->state = RXRPC_CONN_SERVER_CHALLENGING;
-			atomic_inc(&call->conn->usage);
+			rxrpc_get_connection(call->conn);
 			set_bit(RXRPC_CONN_CHALLENGE, &call->conn->events);
 			rxrpc_queue_conn(call->conn);
 		} else {

commit 42886ffe77f142c36ecf585d60fff2edd06b5be8
Author: David Howells <dhowells@redhat.com>
Date:   Thu Jun 16 13:31:07 2016 +0100

    rxrpc: Pass sk_buff * rather than rxrpc_host_header * to functions
    
    Pass a pointer to struct sk_buff rather than struct rxrpc_host_header to
    functions so that they can in the future get at transport protocol parameters
    rather than just RxRPC parameters.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 553b67c144e5..5a70dc4e28c6 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -110,7 +110,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 		goto error;
 	}
 
-	conn = rxrpc_incoming_connection(trans, &sp->hdr);
+	conn = rxrpc_incoming_connection(trans, skb);
 	rxrpc_put_transport(trans);
 	if (IS_ERR(conn)) {
 		_debug("no conn");
@@ -118,7 +118,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 		goto error;
 	}
 
-	call = rxrpc_incoming_call(rx, conn, &sp->hdr);
+	call = rxrpc_incoming_call(rx, conn, skb);
 	rxrpc_put_connection(conn);
 	if (IS_ERR(call)) {
 		_debug("no call");

commit 0e4699e4a37be4cfa07f0340cef6a3fa6a46f5f8
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Sat Jun 18 11:44:03 2016 +0300

    rxrpc: checking for IS_ERR() instead of NULL
    
    rxrpc_lookup_peer_rcu() and rxrpc_lookup_peer() return NULL on error, never
    error pointers, so IS_ERR() can't be used.
    
    Fix three callers of those functions.
    
    Fixes: be6e6707f6ee ('rxrpc: Rework peer object handling to use hash table and RCU')
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index 50136c76ebd1..553b67c144e5 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -96,7 +96,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 	notification->mark = RXRPC_SKB_MARK_NEW_CALL;
 
 	peer = rxrpc_lookup_peer(local, srx, GFP_NOIO);
-	if (IS_ERR(peer)) {
+	if (!peer) {
 		_debug("no peer");
 		ret = -EBUSY;
 		goto error;

commit 4f95dd78a77edc42454de55bb32332be293fb461
Author: David Howells <dhowells@redhat.com>
Date:   Mon Apr 4 14:00:35 2016 +0100

    rxrpc: Rework local endpoint management
    
    Rework the local RxRPC endpoint management.
    
    Local endpoint objects are maintained in a flat list as before.  This
    should be okay as there shouldn't be more than one per open AF_RXRPC socket
    (there can be fewer as local endpoints can be shared if their local service
    ID is 0 and they share the same local transport parameters).
    
    Changes:
    
     (1) Local endpoints may now only be shared if they have local service ID 0
         (ie. they're not being used for listening).
    
         This prevents a scenario where process A is listening of the Cache
         Manager port and process B contacts a fileserver - which may then
         attempt to send CM requests back to B.  But if A and B are sharing a
         local endpoint, A will get the CM requests meant for B.
    
     (2) We use a mutex to handle lookups and don't provide RCU-only lookups
         since we only expect to access the list when opening a socket or
         destroying an endpoint.
    
         The local endpoint object is pointed to by the transport socket's
         sk_user_data for the life of the transport socket - allowing us to
         refer to it directly from the sk_data_ready and sk_error_report
         callbacks.
    
     (3) atomic_inc_not_zero() now exists and can be used to only share a local
         endpoint if the last reference hasn't yet gone.
    
     (4) We can remove rxrpc_local_lock - a spinlock that had to be taken with
         BH processing disabled given that we assume sk_user_data won't change
         under us.
    
     (5) The transport socket is shut down before we clear the sk_user_data
         pointer so that we can be sure that the transport socket's callbacks
         won't be invoked once the RCU destruction is scheduled.
    
     (6) Local endpoints have a work item that handles both destruction and
         event processing.  The means that destruction doesn't then need to
         wait for event processing.  The event queues can then be cleared after
         the transport socket is shut down.
    
     (7) Local endpoints are no longer available for resurrection beyond the
         life of the sockets that had them open.  As soon as their last ref
         goes, they are scheduled for destruction and may not have their usage
         count moved from 0.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index e5723f4dce89..50136c76ebd1 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -202,10 +202,8 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
  * accept incoming calls that need peer, transport and/or connection setting up
  * - the packets we get are all incoming client DATA packets that have seq == 1
  */
-void rxrpc_accept_incoming_calls(struct work_struct *work)
+void rxrpc_accept_incoming_calls(struct rxrpc_local *local)
 {
-	struct rxrpc_local *local =
-		container_of(work, struct rxrpc_local, acceptor);
 	struct rxrpc_skb_priv *sp;
 	struct sockaddr_rxrpc srx;
 	struct rxrpc_sock *rx;
@@ -215,21 +213,8 @@ void rxrpc_accept_incoming_calls(struct work_struct *work)
 
 	_enter("%d", local->debug_id);
 
-	read_lock_bh(&rxrpc_local_lock);
-	if (atomic_read(&local->usage) > 0)
-		rxrpc_get_local(local);
-	else
-		local = NULL;
-	read_unlock_bh(&rxrpc_local_lock);
-	if (!local) {
-		_leave(" [local dead]");
-		return;
-	}
-
-process_next_packet:
 	skb = skb_dequeue(&local->accept_queue);
 	if (!skb) {
-		rxrpc_put_local(local);
 		_leave("\n");
 		return;
 	}
@@ -292,7 +277,7 @@ void rxrpc_accept_incoming_calls(struct work_struct *work)
 	case -ECONNRESET: /* old calls are ignored */
 	case -ECONNABORTED: /* aborted calls are reaborted or ignored */
 	case 0:
-		goto process_next_packet;
+		return;
 	case -ECONNREFUSED:
 		goto invalid_service;
 	case -EBUSY:
@@ -308,18 +293,18 @@ void rxrpc_accept_incoming_calls(struct work_struct *work)
 busy:
 	rxrpc_busy(local, &srx, &whdr);
 	rxrpc_free_skb(skb);
-	goto process_next_packet;
+	return;
 
 invalid_service:
 	skb->priority = RX_INVALID_OPERATION;
 	rxrpc_reject_packet(local, skb);
-	goto process_next_packet;
+	return;
 
 	/* can't change connection security type mid-flow */
 security_mismatch:
 	skb->priority = RX_PROTOCOL_ERROR;
 	rxrpc_reject_packet(local, skb);
-	goto process_next_packet;
+	return;
 }
 
 /*

commit be6e6707f6eec2048d9be608bc0ceecde5bd4cef
Author: David Howells <dhowells@redhat.com>
Date:   Mon Apr 4 14:00:32 2016 +0100

    rxrpc: Rework peer object handling to use hash table and RCU
    
    Rework peer object handling to use a hash table instead of a flat list and
    to use RCU.  Peer objects are no longer destroyed by passing them to a
    workqueue to process, but rather are just passed to the RCU garbage
    collector as kfree'able objects.
    
    The hash function uses the local endpoint plus all the components of the
    remote address, except for the RxRPC service ID.  Peers thus represent a
    UDP port on the remote machine as contacted by a UDP port on this machine.
    
    The RCU read lock is used to handle non-creating lookups so that they can
    be called from bottom half context in the sk_error_report handler without
    having to lock the hash table against modification.
    rxrpc_lookup_peer_rcu() *does* take a reference on the peer object as in
    the future, this will be passed to a work item for error distribution in
    the error_report path and this function will cease being used in the
    data_ready path.
    
    Creating lookups are done under spinlock rather than mutex as they might be
    set up due to an external stimulus if the local endpoint is a server.
    
    Captured network error messages (ICMP) are handled with respect to this
    struct and MTU size and RTT are cached here.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
index eea5f4a5d8b1..e5723f4dce89 100644
--- a/net/rxrpc/call_accept.c
+++ b/net/rxrpc/call_accept.c
@@ -95,7 +95,7 @@ static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
 	rxrpc_new_skb(notification);
 	notification->mark = RXRPC_SKB_MARK_NEW_CALL;
 
-	peer = rxrpc_get_peer(srx, GFP_NOIO);
+	peer = rxrpc_lookup_peer(local, srx, GFP_NOIO);
 	if (IS_ERR(peer)) {
 		_debug("no peer");
 		ret = -EBUSY;

commit 8c3e34a4ff85142ca5dba3f18cbc2061899e2612
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 13 12:16:05 2016 +0100

    rxrpc: Rename files matching ar-*.c to git rid of the "ar-" prefix
    
    Rename files matching net/rxrpc/ar-*.c to get rid of the "ar-" prefix.
    This will aid splitting those files by making easier to come up with new
    names.
    
    Note that the not all files are simply renamed from ar-X.c to X.c.  The
    following exceptions are made:
    
     (*) ar-call.c -> call_object.c
         ar-ack.c -> call_event.c
    
         call_object.c is going to contain the core of the call object
         handling.  Call event handling is all going to be in call_event.c.
    
     (*) ar-accept.c -> call_accept.c
    
         Incoming call handling is going to be here.
    
     (*) ar-connection.c -> conn_object.c
         ar-connevent.c -> conn_event.c
    
         The former file is going to have the basic connection object handling,
         but there will likely be some differentiation between client
         connections and service connections in additional files later.  The
         latter file will have all the connection-level event handling.
    
     (*) ar-local.c -> local_object.c
    
         This will have the local endpoint object handling code.  The local
         endpoint event handling code will later be split out into
         local_event.c.
    
     (*) ar-peer.c -> peer_object.c
    
         This will have the peer endpoint object handling code.  Peer event
         handling code will be placed in peer_event.c (for the moment, there is
         none).
    
     (*) ar-error.c -> peer_event.c
    
         This will become the peer event handling code, though for the moment
         it's actually driven from the local endpoint's perspective.
    
    Note that I haven't renamed ar-transport.c to transport_object.c as the
    intention is to delete it when the rxrpc_transport struct is excised.
    
    The only file that actually has its contents changed is net/rxrpc/Makefile.
    
    net/rxrpc/ar-internal.h will need its section marker comments updating, but
    I'll do that in a separate patch to make it easier for git to follow the
    history across the rename.  I may also want to rename ar-internal.h at some
    point - but that would mean updating all the #includes and I'd rather do
    that in a separate step.
    
    Signed-off-by: David Howells <dhowells@redhat.com.

diff --git a/net/rxrpc/call_accept.c b/net/rxrpc/call_accept.c
new file mode 100644
index 000000000000..eea5f4a5d8b1
--- /dev/null
+++ b/net/rxrpc/call_accept.c
@@ -0,0 +1,518 @@
+/* incoming call handling
+ *
+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
+ * Written by David Howells (dhowells@redhat.com)
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/net.h>
+#include <linux/skbuff.h>
+#include <linux/errqueue.h>
+#include <linux/udp.h>
+#include <linux/in.h>
+#include <linux/in6.h>
+#include <linux/icmp.h>
+#include <linux/gfp.h>
+#include <net/sock.h>
+#include <net/af_rxrpc.h>
+#include <net/ip.h>
+#include "ar-internal.h"
+
+/*
+ * generate a connection-level abort
+ */
+static int rxrpc_busy(struct rxrpc_local *local, struct sockaddr_rxrpc *srx,
+		      struct rxrpc_wire_header *whdr)
+{
+	struct msghdr msg;
+	struct kvec iov[1];
+	size_t len;
+	int ret;
+
+	_enter("%d,,", local->debug_id);
+
+	whdr->type	= RXRPC_PACKET_TYPE_BUSY;
+	whdr->serial	= htonl(1);
+
+	msg.msg_name	= &srx->transport.sin;
+	msg.msg_namelen	= sizeof(srx->transport.sin);
+	msg.msg_control	= NULL;
+	msg.msg_controllen = 0;
+	msg.msg_flags	= 0;
+
+	iov[0].iov_base	= whdr;
+	iov[0].iov_len	= sizeof(*whdr);
+
+	len = iov[0].iov_len;
+
+	_proto("Tx BUSY %%1");
+
+	ret = kernel_sendmsg(local->socket, &msg, iov, 1, len);
+	if (ret < 0) {
+		_leave(" = -EAGAIN [sendmsg failed: %d]", ret);
+		return -EAGAIN;
+	}
+
+	_leave(" = 0");
+	return 0;
+}
+
+/*
+ * accept an incoming call that needs peer, transport and/or connection setting
+ * up
+ */
+static int rxrpc_accept_incoming_call(struct rxrpc_local *local,
+				      struct rxrpc_sock *rx,
+				      struct sk_buff *skb,
+				      struct sockaddr_rxrpc *srx)
+{
+	struct rxrpc_connection *conn;
+	struct rxrpc_transport *trans;
+	struct rxrpc_skb_priv *sp, *nsp;
+	struct rxrpc_peer *peer;
+	struct rxrpc_call *call;
+	struct sk_buff *notification;
+	int ret;
+
+	_enter("");
+
+	sp = rxrpc_skb(skb);
+
+	/* get a notification message to send to the server app */
+	notification = alloc_skb(0, GFP_NOFS);
+	if (!notification) {
+		_debug("no memory");
+		ret = -ENOMEM;
+		goto error_nofree;
+	}
+	rxrpc_new_skb(notification);
+	notification->mark = RXRPC_SKB_MARK_NEW_CALL;
+
+	peer = rxrpc_get_peer(srx, GFP_NOIO);
+	if (IS_ERR(peer)) {
+		_debug("no peer");
+		ret = -EBUSY;
+		goto error;
+	}
+
+	trans = rxrpc_get_transport(local, peer, GFP_NOIO);
+	rxrpc_put_peer(peer);
+	if (IS_ERR(trans)) {
+		_debug("no trans");
+		ret = -EBUSY;
+		goto error;
+	}
+
+	conn = rxrpc_incoming_connection(trans, &sp->hdr);
+	rxrpc_put_transport(trans);
+	if (IS_ERR(conn)) {
+		_debug("no conn");
+		ret = PTR_ERR(conn);
+		goto error;
+	}
+
+	call = rxrpc_incoming_call(rx, conn, &sp->hdr);
+	rxrpc_put_connection(conn);
+	if (IS_ERR(call)) {
+		_debug("no call");
+		ret = PTR_ERR(call);
+		goto error;
+	}
+
+	/* attach the call to the socket */
+	read_lock_bh(&local->services_lock);
+	if (rx->sk.sk_state == RXRPC_CLOSE)
+		goto invalid_service;
+
+	write_lock(&rx->call_lock);
+	if (!test_and_set_bit(RXRPC_CALL_INIT_ACCEPT, &call->flags)) {
+		rxrpc_get_call(call);
+
+		spin_lock(&call->conn->state_lock);
+		if (sp->hdr.securityIndex > 0 &&
+		    call->conn->state == RXRPC_CONN_SERVER_UNSECURED) {
+			_debug("await conn sec");
+			list_add_tail(&call->accept_link, &rx->secureq);
+			call->conn->state = RXRPC_CONN_SERVER_CHALLENGING;
+			atomic_inc(&call->conn->usage);
+			set_bit(RXRPC_CONN_CHALLENGE, &call->conn->events);
+			rxrpc_queue_conn(call->conn);
+		} else {
+			_debug("conn ready");
+			call->state = RXRPC_CALL_SERVER_ACCEPTING;
+			list_add_tail(&call->accept_link, &rx->acceptq);
+			rxrpc_get_call(call);
+			nsp = rxrpc_skb(notification);
+			nsp->call = call;
+
+			ASSERTCMP(atomic_read(&call->usage), >=, 3);
+
+			_debug("notify");
+			spin_lock(&call->lock);
+			ret = rxrpc_queue_rcv_skb(call, notification, true,
+						  false);
+			spin_unlock(&call->lock);
+			notification = NULL;
+			BUG_ON(ret < 0);
+		}
+		spin_unlock(&call->conn->state_lock);
+
+		_debug("queued");
+	}
+	write_unlock(&rx->call_lock);
+
+	_debug("process");
+	rxrpc_fast_process_packet(call, skb);
+
+	_debug("done");
+	read_unlock_bh(&local->services_lock);
+	rxrpc_free_skb(notification);
+	rxrpc_put_call(call);
+	_leave(" = 0");
+	return 0;
+
+invalid_service:
+	_debug("invalid");
+	read_unlock_bh(&local->services_lock);
+
+	read_lock_bh(&call->state_lock);
+	if (!test_bit(RXRPC_CALL_RELEASED, &call->flags) &&
+	    !test_and_set_bit(RXRPC_CALL_EV_RELEASE, &call->events)) {
+		rxrpc_get_call(call);
+		rxrpc_queue_call(call);
+	}
+	read_unlock_bh(&call->state_lock);
+	rxrpc_put_call(call);
+	ret = -ECONNREFUSED;
+error:
+	rxrpc_free_skb(notification);
+error_nofree:
+	_leave(" = %d", ret);
+	return ret;
+}
+
+/*
+ * accept incoming calls that need peer, transport and/or connection setting up
+ * - the packets we get are all incoming client DATA packets that have seq == 1
+ */
+void rxrpc_accept_incoming_calls(struct work_struct *work)
+{
+	struct rxrpc_local *local =
+		container_of(work, struct rxrpc_local, acceptor);
+	struct rxrpc_skb_priv *sp;
+	struct sockaddr_rxrpc srx;
+	struct rxrpc_sock *rx;
+	struct rxrpc_wire_header whdr;
+	struct sk_buff *skb;
+	int ret;
+
+	_enter("%d", local->debug_id);
+
+	read_lock_bh(&rxrpc_local_lock);
+	if (atomic_read(&local->usage) > 0)
+		rxrpc_get_local(local);
+	else
+		local = NULL;
+	read_unlock_bh(&rxrpc_local_lock);
+	if (!local) {
+		_leave(" [local dead]");
+		return;
+	}
+
+process_next_packet:
+	skb = skb_dequeue(&local->accept_queue);
+	if (!skb) {
+		rxrpc_put_local(local);
+		_leave("\n");
+		return;
+	}
+
+	_net("incoming call skb %p", skb);
+
+	sp = rxrpc_skb(skb);
+
+	/* Set up a response packet header in case we need it */
+	whdr.epoch	= htonl(sp->hdr.epoch);
+	whdr.cid	= htonl(sp->hdr.cid);
+	whdr.callNumber	= htonl(sp->hdr.callNumber);
+	whdr.seq	= htonl(sp->hdr.seq);
+	whdr.serial	= 0;
+	whdr.flags	= 0;
+	whdr.type	= 0;
+	whdr.userStatus	= 0;
+	whdr.securityIndex = sp->hdr.securityIndex;
+	whdr._rsvd	= 0;
+	whdr.serviceId	= htons(sp->hdr.serviceId);
+
+	/* determine the remote address */
+	memset(&srx, 0, sizeof(srx));
+	srx.srx_family = AF_RXRPC;
+	srx.transport.family = local->srx.transport.family;
+	srx.transport_type = local->srx.transport_type;
+	switch (srx.transport.family) {
+	case AF_INET:
+		srx.transport_len = sizeof(struct sockaddr_in);
+		srx.transport.sin.sin_port = udp_hdr(skb)->source;
+		srx.transport.sin.sin_addr.s_addr = ip_hdr(skb)->saddr;
+		break;
+	default:
+		goto busy;
+	}
+
+	/* get the socket providing the service */
+	read_lock_bh(&local->services_lock);
+	list_for_each_entry(rx, &local->services, listen_link) {
+		if (rx->srx.srx_service == sp->hdr.serviceId &&
+		    rx->sk.sk_state != RXRPC_CLOSE)
+			goto found_service;
+	}
+	read_unlock_bh(&local->services_lock);
+	goto invalid_service;
+
+found_service:
+	_debug("found service %hd", rx->srx.srx_service);
+	if (sk_acceptq_is_full(&rx->sk))
+		goto backlog_full;
+	sk_acceptq_added(&rx->sk);
+	sock_hold(&rx->sk);
+	read_unlock_bh(&local->services_lock);
+
+	ret = rxrpc_accept_incoming_call(local, rx, skb, &srx);
+	if (ret < 0)
+		sk_acceptq_removed(&rx->sk);
+	sock_put(&rx->sk);
+	switch (ret) {
+	case -ECONNRESET: /* old calls are ignored */
+	case -ECONNABORTED: /* aborted calls are reaborted or ignored */
+	case 0:
+		goto process_next_packet;
+	case -ECONNREFUSED:
+		goto invalid_service;
+	case -EBUSY:
+		goto busy;
+	case -EKEYREJECTED:
+		goto security_mismatch;
+	default:
+		BUG();
+	}
+
+backlog_full:
+	read_unlock_bh(&local->services_lock);
+busy:
+	rxrpc_busy(local, &srx, &whdr);
+	rxrpc_free_skb(skb);
+	goto process_next_packet;
+
+invalid_service:
+	skb->priority = RX_INVALID_OPERATION;
+	rxrpc_reject_packet(local, skb);
+	goto process_next_packet;
+
+	/* can't change connection security type mid-flow */
+security_mismatch:
+	skb->priority = RX_PROTOCOL_ERROR;
+	rxrpc_reject_packet(local, skb);
+	goto process_next_packet;
+}
+
+/*
+ * handle acceptance of a call by userspace
+ * - assign the user call ID to the call at the front of the queue
+ */
+struct rxrpc_call *rxrpc_accept_call(struct rxrpc_sock *rx,
+				     unsigned long user_call_ID)
+{
+	struct rxrpc_call *call;
+	struct rb_node *parent, **pp;
+	int ret;
+
+	_enter(",%lx", user_call_ID);
+
+	ASSERT(!irqs_disabled());
+
+	write_lock(&rx->call_lock);
+
+	ret = -ENODATA;
+	if (list_empty(&rx->acceptq))
+		goto out;
+
+	/* check the user ID isn't already in use */
+	ret = -EBADSLT;
+	pp = &rx->calls.rb_node;
+	parent = NULL;
+	while (*pp) {
+		parent = *pp;
+		call = rb_entry(parent, struct rxrpc_call, sock_node);
+
+		if (user_call_ID < call->user_call_ID)
+			pp = &(*pp)->rb_left;
+		else if (user_call_ID > call->user_call_ID)
+			pp = &(*pp)->rb_right;
+		else
+			goto out;
+	}
+
+	/* dequeue the first call and check it's still valid */
+	call = list_entry(rx->acceptq.next, struct rxrpc_call, accept_link);
+	list_del_init(&call->accept_link);
+	sk_acceptq_removed(&rx->sk);
+
+	write_lock_bh(&call->state_lock);
+	switch (call->state) {
+	case RXRPC_CALL_SERVER_ACCEPTING:
+		call->state = RXRPC_CALL_SERVER_RECV_REQUEST;
+		break;
+	case RXRPC_CALL_REMOTELY_ABORTED:
+	case RXRPC_CALL_LOCALLY_ABORTED:
+		ret = -ECONNABORTED;
+		goto out_release;
+	case RXRPC_CALL_NETWORK_ERROR:
+		ret = call->conn->error;
+		goto out_release;
+	case RXRPC_CALL_DEAD:
+		ret = -ETIME;
+		goto out_discard;
+	default:
+		BUG();
+	}
+
+	/* formalise the acceptance */
+	call->user_call_ID = user_call_ID;
+	rb_link_node(&call->sock_node, parent, pp);
+	rb_insert_color(&call->sock_node, &rx->calls);
+	if (test_and_set_bit(RXRPC_CALL_HAS_USERID, &call->flags))
+		BUG();
+	if (test_and_set_bit(RXRPC_CALL_EV_ACCEPTED, &call->events))
+		BUG();
+	rxrpc_queue_call(call);
+
+	rxrpc_get_call(call);
+	write_unlock_bh(&call->state_lock);
+	write_unlock(&rx->call_lock);
+	_leave(" = %p{%d}", call, call->debug_id);
+	return call;
+
+	/* if the call is already dying or dead, then we leave the socket's ref
+	 * on it to be released by rxrpc_dead_call_expired() as induced by
+	 * rxrpc_release_call() */
+out_release:
+	_debug("release %p", call);
+	if (!test_bit(RXRPC_CALL_RELEASED, &call->flags) &&
+	    !test_and_set_bit(RXRPC_CALL_EV_RELEASE, &call->events))
+		rxrpc_queue_call(call);
+out_discard:
+	write_unlock_bh(&call->state_lock);
+	_debug("discard %p", call);
+out:
+	write_unlock(&rx->call_lock);
+	_leave(" = %d", ret);
+	return ERR_PTR(ret);
+}
+
+/*
+ * Handle rejection of a call by userspace
+ * - reject the call at the front of the queue
+ */
+int rxrpc_reject_call(struct rxrpc_sock *rx)
+{
+	struct rxrpc_call *call;
+	int ret;
+
+	_enter("");
+
+	ASSERT(!irqs_disabled());
+
+	write_lock(&rx->call_lock);
+
+	ret = -ENODATA;
+	if (list_empty(&rx->acceptq))
+		goto out;
+
+	/* dequeue the first call and check it's still valid */
+	call = list_entry(rx->acceptq.next, struct rxrpc_call, accept_link);
+	list_del_init(&call->accept_link);
+	sk_acceptq_removed(&rx->sk);
+
+	write_lock_bh(&call->state_lock);
+	switch (call->state) {
+	case RXRPC_CALL_SERVER_ACCEPTING:
+		call->state = RXRPC_CALL_SERVER_BUSY;
+		if (test_and_set_bit(RXRPC_CALL_EV_REJECT_BUSY, &call->events))
+			rxrpc_queue_call(call);
+		ret = 0;
+		goto out_release;
+	case RXRPC_CALL_REMOTELY_ABORTED:
+	case RXRPC_CALL_LOCALLY_ABORTED:
+		ret = -ECONNABORTED;
+		goto out_release;
+	case RXRPC_CALL_NETWORK_ERROR:
+		ret = call->conn->error;
+		goto out_release;
+	case RXRPC_CALL_DEAD:
+		ret = -ETIME;
+		goto out_discard;
+	default:
+		BUG();
+	}
+
+	/* if the call is already dying or dead, then we leave the socket's ref
+	 * on it to be released by rxrpc_dead_call_expired() as induced by
+	 * rxrpc_release_call() */
+out_release:
+	_debug("release %p", call);
+	if (!test_bit(RXRPC_CALL_RELEASED, &call->flags) &&
+	    !test_and_set_bit(RXRPC_CALL_EV_RELEASE, &call->events))
+		rxrpc_queue_call(call);
+out_discard:
+	write_unlock_bh(&call->state_lock);
+	_debug("discard %p", call);
+out:
+	write_unlock(&rx->call_lock);
+	_leave(" = %d", ret);
+	return ret;
+}
+
+/**
+ * rxrpc_kernel_accept_call - Allow a kernel service to accept an incoming call
+ * @sock: The socket on which the impending call is waiting
+ * @user_call_ID: The tag to attach to the call
+ *
+ * Allow a kernel service to accept an incoming call, assuming the incoming
+ * call is still valid.
+ */
+struct rxrpc_call *rxrpc_kernel_accept_call(struct socket *sock,
+					    unsigned long user_call_ID)
+{
+	struct rxrpc_call *call;
+
+	_enter(",%lx", user_call_ID);
+	call = rxrpc_accept_call(rxrpc_sk(sock->sk), user_call_ID);
+	_leave(" = %p", call);
+	return call;
+}
+EXPORT_SYMBOL(rxrpc_kernel_accept_call);
+
+/**
+ * rxrpc_kernel_reject_call - Allow a kernel service to reject an incoming call
+ * @sock: The socket on which the impending call is waiting
+ *
+ * Allow a kernel service to reject an incoming call with a BUSY message,
+ * assuming the incoming call is still valid.
+ */
+int rxrpc_kernel_reject_call(struct socket *sock)
+{
+	int ret;
+
+	_enter("");
+	ret = rxrpc_reject_call(rxrpc_sk(sock->sk));
+	_leave(" = %d", ret);
+	return ret;
+}
+EXPORT_SYMBOL(rxrpc_kernel_reject_call);
