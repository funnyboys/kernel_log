commit c410bf01933e5e09d142c66c3df9ad470a7eec13
Author: David Howells <dhowells@redhat.com>
Date:   Mon May 11 14:54:34 2020 +0100

    rxrpc: Fix the excessive initial retransmission timeout
    
    rxrpc currently uses a fixed 4s retransmission timeout until the RTT is
    sufficiently sampled.  This can cause problems with some fileservers with
    calls to the cache manager in the afs filesystem being dropped from the
    fileserver because a packet goes missing and the retransmission timeout is
    greater than the call expiry timeout.
    
    Fix this by:
    
     (1) Copying the RTT/RTO calculation code from Linux's TCP implementation
         and altering it to fit rxrpc.
    
     (2) Altering the various users of the RTT to make use of the new SRTT
         value.
    
     (3) Replacing the use of rxrpc_resend_timeout to use the calculated RTO
         value instead (which is needed in jiffies), along with a backoff.
    
    Notes:
    
     (1) rxrpc provides RTT samples by matching the serial numbers on outgoing
         DATA packets that have the RXRPC_REQUEST_ACK set and PING ACK packets
         against the reference serial number in incoming REQUESTED ACK and
         PING-RESPONSE ACK packets.
    
     (2) Each packet that is transmitted on an rxrpc connection gets a new
         per-connection serial number, even for retransmissions, so an ACK can
         be cross-referenced to a specific trigger packet.  This allows RTT
         information to be drawn from retransmitted DATA packets also.
    
     (3) rxrpc maintains the RTT/RTO state on the rxrpc_peer record rather than
         on an rxrpc_call because many RPC calls won't live long enough to
         generate more than one sample.
    
     (4) The calculated SRTT value is in units of 8ths of a microsecond rather
         than nanoseconds.
    
    The (S)RTT and RTO values are displayed in /proc/net/rxrpc/peers.
    
    Fixes: 17926a79320a ([AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both"")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 452163eadb98..ca29976bb193 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -225,6 +225,8 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 		spin_lock_init(&peer->rtt_input_lock);
 		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
 
+		rxrpc_peer_init_rtt(peer);
+
 		if (RXRPC_TX_SMSS > 2190)
 			peer->cong_cwnd = 2;
 		else if (RXRPC_TX_SMSS > 1095)
@@ -497,14 +499,14 @@ void rxrpc_kernel_get_peer(struct socket *sock, struct rxrpc_call *call,
 EXPORT_SYMBOL(rxrpc_kernel_get_peer);
 
 /**
- * rxrpc_kernel_get_rtt - Get a call's peer RTT
+ * rxrpc_kernel_get_srtt - Get a call's peer smoothed RTT
  * @sock: The socket on which the call is in progress.
  * @call: The call to query
  *
- * Get the call's peer RTT.
+ * Get the call's peer smoothed RTT.
  */
-u64 rxrpc_kernel_get_rtt(struct socket *sock, struct rxrpc_call *call)
+u32 rxrpc_kernel_get_srtt(struct socket *sock, struct rxrpc_call *call)
 {
-	return call->peer->rtt;
+	return call->peer->srtt_us >> 3;
 }
-EXPORT_SYMBOL(rxrpc_kernel_get_rtt);
+EXPORT_SYMBOL(rxrpc_kernel_get_srtt);

commit 2f184393e0c2d409c62262f57f2a57efdf9370b8
Merge: ebcd670d05d5 531e93d11470
Author: David S. Miller <davem@davemloft.net>
Date:   Sat Oct 19 22:51:25 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Several cases of overlapping changes which were for the most
    part trivially resolvable.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 9ebeddef58c41bd700419cdcece24cf64ce32276
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 7 10:58:29 2019 +0100

    rxrpc: rxrpc_peer needs to hold a ref on the rxrpc_local record
    
    The rxrpc_peer record needs to hold a reference on the rxrpc_local record
    it points as the peer is used as a base to access information in the
    rxrpc_local record.
    
    This can cause problems in __rxrpc_put_peer(), where we need the network
    namespace pointer, and in rxrpc_send_keepalive(), where we need to access
    the UDP socket, leading to symptoms like:
    
        BUG: KASAN: use-after-free in __rxrpc_put_peer net/rxrpc/peer_object.c:411
        [inline]
        BUG: KASAN: use-after-free in rxrpc_put_peer+0x685/0x6a0
        net/rxrpc/peer_object.c:435
        Read of size 8 at addr ffff888097ec0058 by task syz-executor823/24216
    
    Fix this by taking a ref on the local record for the peer record.
    
    Fixes: ace45bec6d77 ("rxrpc: Fix firewall route keepalive")
    Fixes: 2baec2c3f854 ("rxrpc: Support network namespacing")
    Reported-by: syzbot+b9be979c55f2bea8ed30@syzkaller.appspotmail.com
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index b700b7ecaa3d..64830d8c1fdb 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -216,7 +216,7 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 	peer = kzalloc(sizeof(struct rxrpc_peer), gfp);
 	if (peer) {
 		atomic_set(&peer->usage, 1);
-		peer->local = local;
+		peer->local = rxrpc_get_local(local);
 		INIT_HLIST_HEAD(&peer->error_targets);
 		peer->service_conns = RB_ROOT;
 		seqlock_init(&peer->service_conn_lock);
@@ -307,7 +307,6 @@ void rxrpc_new_incoming_peer(struct rxrpc_sock *rx, struct rxrpc_local *local,
 	unsigned long hash_key;
 
 	hash_key = rxrpc_peer_hash_key(local, &peer->srx);
-	peer->local = local;
 	rxrpc_init_peer(rx, peer, hash_key);
 
 	spin_lock(&rxnet->peer_hash_lock);
@@ -417,6 +416,7 @@ static void __rxrpc_put_peer(struct rxrpc_peer *peer)
 	list_del_init(&peer->keepalive_link);
 	spin_unlock_bh(&rxnet->peer_hash_lock);
 
+	rxrpc_put_local(peer->local);
 	kfree_rcu(peer, rcu);
 }
 
@@ -453,6 +453,7 @@ void rxrpc_put_peer_locked(struct rxrpc_peer *peer)
 	if (n == 0) {
 		hash_del_rcu(&peer->hash_link);
 		list_del_init(&peer->keepalive_link);
+		rxrpc_put_local(peer->local);
 		kfree_rcu(peer, rcu);
 	}
 }

commit 55f6c98e3674ce16038a1949c3f9ca5a9a99f289
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 7 10:58:29 2019 +0100

    rxrpc: Fix trace-after-put looking at the put peer record
    
    rxrpc_put_peer() calls trace_rxrpc_peer() after it has done the decrement
    of the refcount - which looks at the debug_id in the peer record.  But
    unless the refcount was reduced to zero, we no longer have the right to
    look in the record and, indeed, it may be deleted by some other thread.
    
    Fix this by getting the debug_id out before decrementing the refcount and
    then passing that into the tracepoint.
    
    This can cause the following symptoms:
    
        BUG: KASAN: use-after-free in __rxrpc_put_peer net/rxrpc/peer_object.c:411
        [inline]
        BUG: KASAN: use-after-free in rxrpc_put_peer+0x685/0x6a0
        net/rxrpc/peer_object.c:435
        Read of size 8 at addr ffff888097ec0058 by task syz-executor823/24216
    
    Fixes: 1159d4b496f5 ("rxrpc: Add a tracepoint to track rxrpc_peer refcounting")
    Reported-by: syzbot+b9be979c55f2bea8ed30@syzkaller.appspotmail.com
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 9c3ac96f71cb..b700b7ecaa3d 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -382,7 +382,7 @@ struct rxrpc_peer *rxrpc_get_peer(struct rxrpc_peer *peer)
 	int n;
 
 	n = atomic_inc_return(&peer->usage);
-	trace_rxrpc_peer(peer, rxrpc_peer_got, n, here);
+	trace_rxrpc_peer(peer->debug_id, rxrpc_peer_got, n, here);
 	return peer;
 }
 
@@ -396,7 +396,7 @@ struct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer)
 	if (peer) {
 		int n = atomic_fetch_add_unless(&peer->usage, 1, 0);
 		if (n > 0)
-			trace_rxrpc_peer(peer, rxrpc_peer_got, n + 1, here);
+			trace_rxrpc_peer(peer->debug_id, rxrpc_peer_got, n + 1, here);
 		else
 			peer = NULL;
 	}
@@ -426,11 +426,13 @@ static void __rxrpc_put_peer(struct rxrpc_peer *peer)
 void rxrpc_put_peer(struct rxrpc_peer *peer)
 {
 	const void *here = __builtin_return_address(0);
+	unsigned int debug_id;
 	int n;
 
 	if (peer) {
+		debug_id = peer->debug_id;
 		n = atomic_dec_return(&peer->usage);
-		trace_rxrpc_peer(peer, rxrpc_peer_put, n, here);
+		trace_rxrpc_peer(debug_id, rxrpc_peer_put, n, here);
 		if (n == 0)
 			__rxrpc_put_peer(peer);
 	}
@@ -443,10 +445,11 @@ void rxrpc_put_peer(struct rxrpc_peer *peer)
 void rxrpc_put_peer_locked(struct rxrpc_peer *peer)
 {
 	const void *here = __builtin_return_address(0);
+	unsigned int debug_id = peer->debug_id;
 	int n;
 
 	n = atomic_dec_return(&peer->usage);
-	trace_rxrpc_peer(peer, rxrpc_peer_put, n, here);
+	trace_rxrpc_peer(debug_id, rxrpc_peer_put, n, here);
 	if (n == 0) {
 		hash_del_rcu(&peer->hash_link);
 		list_del_init(&peer->keepalive_link);

commit 033b2c7f0f26d236f5e87888aca3d5ecb6a64cb7
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 3 17:45:57 2019 +0100

    rxrpc: Add missing "new peer" trace
    
    There was supposed to be a trace indicating that a new peer had been
    created.  Add it.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 9c3ac96f71cb..bf4dd6cf79a0 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -209,6 +209,7 @@ static void rxrpc_assess_MTU_size(struct rxrpc_sock *rx,
  */
 struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 {
+	const void *here = __builtin_return_address(0);
 	struct rxrpc_peer *peer;
 
 	_enter("");
@@ -230,6 +231,7 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 			peer->cong_cwnd = 3;
 		else
 			peer->cong_cwnd = 4;
+		trace_rxrpc_peer(peer, rxrpc_peer_new, 1, here);
 	}
 
 	_leave(" = %p", peer);

commit 60034d3d146b11922ab1db613bce062dddc0327a
Author: David Howells <dhowells@redhat.com>
Date:   Tue Jul 30 14:42:50 2019 +0100

    rxrpc: Fix potential deadlock
    
    There is a potential deadlock in rxrpc_peer_keepalive_dispatch() whereby
    rxrpc_put_peer() is called with the peer_hash_lock held, but if it reduces
    the peer's refcount to 0, rxrpc_put_peer() calls __rxrpc_put_peer() - which
    the tries to take the already held lock.
    
    Fix this by providing a version of rxrpc_put_peer() that can be called in
    situations where the lock is already held.
    
    The bug may produce the following lockdep report:
    
    ============================================
    WARNING: possible recursive locking detected
    5.2.0-next-20190718 #41 Not tainted
    --------------------------------------------
    kworker/0:3/21678 is trying to acquire lock:
    00000000aa5eecdf (&(&rxnet->peer_hash_lock)->rlock){+.-.}, at: spin_lock_bh
    /./include/linux/spinlock.h:343 [inline]
    00000000aa5eecdf (&(&rxnet->peer_hash_lock)->rlock){+.-.}, at:
    __rxrpc_put_peer /net/rxrpc/peer_object.c:415 [inline]
    00000000aa5eecdf (&(&rxnet->peer_hash_lock)->rlock){+.-.}, at:
    rxrpc_put_peer+0x2d3/0x6a0 /net/rxrpc/peer_object.c:435
    
    but task is already holding lock:
    00000000aa5eecdf (&(&rxnet->peer_hash_lock)->rlock){+.-.}, at: spin_lock_bh
    /./include/linux/spinlock.h:343 [inline]
    00000000aa5eecdf (&(&rxnet->peer_hash_lock)->rlock){+.-.}, at:
    rxrpc_peer_keepalive_dispatch /net/rxrpc/peer_event.c:378 [inline]
    00000000aa5eecdf (&(&rxnet->peer_hash_lock)->rlock){+.-.}, at:
    rxrpc_peer_keepalive_worker+0x6b3/0xd02 /net/rxrpc/peer_event.c:430
    
    Fixes: 330bdcfadcee ("rxrpc: Fix the keepalive generator [ver #2]")
    Reported-by: syzbot+72af434e4b3417318f84@syzkaller.appspotmail.com
    Signed-off-by: David Howells <dhowells@redhat.com>
    Reviewed-by: Marc Dionne <marc.dionne@auristor.com>
    Reviewed-by: Jeffrey Altman <jaltman@auristor.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 9d3ce81cf8ae..9c3ac96f71cb 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -436,6 +436,24 @@ void rxrpc_put_peer(struct rxrpc_peer *peer)
 	}
 }
 
+/*
+ * Drop a ref on a peer record where the caller already holds the
+ * peer_hash_lock.
+ */
+void rxrpc_put_peer_locked(struct rxrpc_peer *peer)
+{
+	const void *here = __builtin_return_address(0);
+	int n;
+
+	n = atomic_dec_return(&peer->usage);
+	trace_rxrpc_peer(peer, rxrpc_peer_put, n, here);
+	if (n == 0) {
+		hash_del_rcu(&peer->hash_link);
+		list_del_init(&peer->keepalive_link);
+		kfree_rcu(peer, rcu);
+	}
+}
+
 /*
  * Make sure all peer records have been discarded.
  */

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 5691b7d266ca..9d3ce81cf8ae 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -1,12 +1,8 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /* RxRPC remote transport endpoint record management
  *
  * Copyright (C) 2007, 2016 Red Hat, Inc. All Rights Reserved.
  * Written by David Howells (dhowells@redhat.com)
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

commit c1e15b4944c9fa7fbbb74f7a5920a1e31b4b965a
Author: David Howells <dhowells@redhat.com>
Date:   Mon Oct 8 15:46:25 2018 +0100

    rxrpc: Fix the packet reception routine
    
    The rxrpc_input_packet() function and its call tree was built around the
    assumption that data_ready() handler called from UDP to inform a kernel
    service that there is data to be had was non-reentrant.  This means that
    certain locking could be dispensed with.
    
    This, however, turns out not to be the case with a multi-queue network card
    that can deliver packets to multiple cpus simultaneously.  Each of those
    cpus can be in the rxrpc_input_packet() function at the same time.
    
    Fix by adding or changing some structure members:
    
     (1) Add peer->rtt_input_lock to serialise access to the RTT buffer.
    
     (2) Make conn->service_id into a 32-bit variable so that it can be
         cmpxchg'd on all arches.
    
     (3) Add call->input_lock to serialise access to the Rx/Tx state.  Note
         that although the Rx and Tx states are (almost) entirely separate,
         there's no point completing the separation and having separate locks
         since it's a bi-phasal RPC protocol rather than a bi-direction
         streaming protocol.  Data transmission and data reception do not take
         place simultaneously on any particular call.
    
    and making the following functional changes:
    
     (1) In rxrpc_input_data(), hold call->input_lock around the core to
         prevent simultaneous producing of packets into the Rx ring and
         updating of tracking state for a particular call.
    
     (2) In rxrpc_input_ping_response(), only read call->ping_serial once, and
         check it before checking RXRPC_CALL_PINGING as that's a cheaper test.
         The bit test and bit clear can then be combined.  No further locking
         is needed here.
    
     (3) In rxrpc_input_ack(), take call->input_lock after we've parsed much of
         the ACK packet.  The superseded ACK check is then done both before and
         after the lock is taken.
    
         The handing of ackinfo data is split, parsing before the lock is taken
         and processing with it held.  This is keyed on rxMTU being non-zero.
    
         Congestion management is also done within the locked section.
    
     (4) In rxrpc_input_ackall(), take call->input_lock around the Tx window
         rotation.  The ACKALL packet carries no information and is only really
         useful after all packets have been transmitted since it's imprecise.
    
     (5) In rxrpc_input_implicit_end_call(), we use rx->incoming_lock to
         prevent calls being simultaneously implicitly ended on two cpus and
         also to prevent any races with incoming call setup.
    
     (6) In rxrpc_input_packet(), use cmpxchg() to effect the service upgrade
         on a connection.  It is only permitted to happen once for a
         connection.
    
     (7) In rxrpc_new_incoming_call(), we have to recheck the routing inside
         rx->incoming_lock to see if someone else set up the call, connection
         or peer whilst we were getting there.  We can't trust the values from
         the earlier routing check unless we pin refs on them - which we want
         to avoid.
    
         Further, we need to allow for an incoming call to have its state
         changed on another CPU between us making it live and us adjusting it
         because the conn is now in the RXRPC_CONN_SERVICE state.
    
     (8) In rxrpc_peer_add_rtt(), take peer->rtt_input_lock around the access
         to the RTT buffer.  Don't need to lock around setting peer->rtt.
    
    For reference, the inventory of state-accessing or state-altering functions
    used by the packet input procedure is:
    
    > rxrpc_input_packet()
      * PACKET CHECKING
    
      * ROUTING
        > rxrpc_post_packet_to_local()
        > rxrpc_find_connection_rcu() - uses RCU
          > rxrpc_lookup_peer_rcu() - uses RCU
          > rxrpc_find_service_conn_rcu() - uses RCU
          > idr_find() - uses RCU
    
      * CONNECTION-LEVEL PROCESSING
        - Service upgrade
          - Can only happen once per conn
          ! Changed to use cmpxchg
        > rxrpc_post_packet_to_conn()
        - Setting conn->hi_serial
          - Probably safe not using locks
          - Maybe use cmpxchg
    
      * CALL-LEVEL PROCESSING
        > Old-call checking
          > rxrpc_input_implicit_end_call()
            > rxrpc_call_completed()
            > rxrpc_queue_call()
            ! Need to take rx->incoming_lock
            > __rxrpc_disconnect_call()
            > rxrpc_notify_socket()
        > rxrpc_new_incoming_call()
          - Uses rx->incoming_lock for the entire process
            - Might be able to drop this earlier in favour of the call lock
          > rxrpc_incoming_call()
            ! Conflicts with rxrpc_input_implicit_end_call()
        > rxrpc_send_ping()
          - Don't need locks to check rtt state
          > rxrpc_propose_ACK
    
      * PACKET DISTRIBUTION
        > rxrpc_input_call_packet()
          > rxrpc_input_data()
            * QUEUE DATA PACKET ON CALL
            > rxrpc_reduce_call_timer()
              - Uses timer_reduce()
            ! Needs call->input_lock()
            > rxrpc_receiving_reply()
              ! Needs locking around ack state
              > rxrpc_rotate_tx_window()
              > rxrpc_end_tx_phase()
            > rxrpc_proto_abort()
            > rxrpc_input_dup_data()
            - Fills the Rx buffer
            - rxrpc_propose_ACK()
            - rxrpc_notify_socket()
    
          > rxrpc_input_ack()
            * APPLY ACK PACKET TO CALL AND DISCARD PACKET
            > rxrpc_input_ping_response()
              - Probably doesn't need any extra locking
              ! Need READ_ONCE() on call->ping_serial
              > rxrpc_input_check_for_lost_ack()
                - Takes call->lock to consult Tx buffer
              > rxrpc_peer_add_rtt()
                ! Needs to take a lock (peer->rtt_input_lock)
                ! Could perhaps manage with cmpxchg() and xadd() instead
            > rxrpc_input_requested_ack
              - Consults Tx buffer
                ! Probably needs a lock
              > rxrpc_peer_add_rtt()
            > rxrpc_propose_ack()
            > rxrpc_input_ackinfo()
              - Changes call->tx_winsize
                ! Use cmpxchg to handle change
                ! Should perhaps track serial number
              - Uses peer->lock to record MTU specification changes
            > rxrpc_proto_abort()
            ! Need to take call->input_lock
            > rxrpc_rotate_tx_window()
            > rxrpc_end_tx_phase()
            > rxrpc_input_soft_acks()
            - Consults the Tx buffer
            > rxrpc_congestion_management()
              - Modifies the Tx annotations
              ! Needs call->input_lock()
              > rxrpc_queue_call()
    
          > rxrpc_input_abort()
            * APPLY ABORT PACKET TO CALL AND DISCARD PACKET
            > rxrpc_set_call_completion()
            > rxrpc_notify_socket()
    
          > rxrpc_input_ackall()
            * APPLY ACKALL PACKET TO CALL AND DISCARD PACKET
            ! Need to take call->input_lock
            > rxrpc_rotate_tx_window()
            > rxrpc_end_tx_phase()
    
        > rxrpc_reject_packet()
    
    There are some functions used by the above that queue the packet, after
    which the procedure is terminated:
    
     - rxrpc_post_packet_to_local()
       - local->event_queue is an sk_buff_head
       - local->processor is a work_struct
     - rxrpc_post_packet_to_conn()
       - conn->rx_queue is an sk_buff_head
       - conn->processor is a work_struct
     - rxrpc_reject_packet()
       - local->reject_queue is an sk_buff_head
       - local->processor is a work_struct
    
    And some that offload processing to process context:
    
     - rxrpc_notify_socket()
       - Uses RCU lock
       - Uses call->notify_lock to call call->notify_rx
       - Uses call->recvmsg_lock to queue recvmsg side
     - rxrpc_queue_call()
       - call->processor is a work_struct
     - rxrpc_propose_ACK()
       - Uses call->lock to wrap __rxrpc_propose_ACK()
    
    And a bunch that complete a call, all of which use call->state_lock to
    protect the call state:
    
     - rxrpc_call_completed()
     - rxrpc_set_call_completion()
     - rxrpc_abort_call()
     - rxrpc_proto_abort()
       - Also uses rxrpc_queue_call()
    
    Fixes: 17926a79320a ("[AF_RXRPC]: Provide secure RxRPC sockets for use by userspace and kernel both")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 2d39eaf19620..5691b7d266ca 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -225,6 +225,7 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 		peer->service_conns = RB_ROOT;
 		seqlock_init(&peer->service_conn_lock);
 		spin_lock_init(&peer->lock);
+		spin_lock_init(&peer->rtt_input_lock);
 		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
 
 		if (RXRPC_TX_SMSS > 2190)

commit 5e33a23ba4b56c109b732d57a0a76558a37d9ec5
Author: David Howells <dhowells@redhat.com>
Date:   Fri Oct 5 14:05:34 2018 +0100

    rxrpc: Fix some missed refs to init_net
    
    Fix some refs to init_net that should've been changed to the appropriate
    network namespace.
    
    Fixes: 2baec2c3f854 ("rxrpc: Support network namespacing")
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: Paolo Abeni <pabeni@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 01a9febfa367..2d39eaf19620 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -153,8 +153,10 @@ struct rxrpc_peer *rxrpc_lookup_peer_rcu(struct rxrpc_local *local,
  * assess the MTU size for the network interface through which this peer is
  * reached
  */
-static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
+static void rxrpc_assess_MTU_size(struct rxrpc_sock *rx,
+				  struct rxrpc_peer *peer)
 {
+	struct net *net = sock_net(&rx->sk);
 	struct dst_entry *dst;
 	struct rtable *rt;
 	struct flowi fl;
@@ -169,7 +171,7 @@ static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 	switch (peer->srx.transport.family) {
 	case AF_INET:
 		rt = ip_route_output_ports(
-			&init_net, fl4, NULL,
+			net, fl4, NULL,
 			peer->srx.transport.sin.sin_addr.s_addr, 0,
 			htons(7000), htons(7001), IPPROTO_UDP, 0, 0);
 		if (IS_ERR(rt)) {
@@ -188,7 +190,7 @@ static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 		       sizeof(struct in6_addr));
 		fl6->fl6_dport = htons(7001);
 		fl6->fl6_sport = htons(7000);
-		dst = ip6_route_output(&init_net, NULL, fl6);
+		dst = ip6_route_output(net, NULL, fl6);
 		if (dst->error) {
 			_leave(" [route err %d]", dst->error);
 			return;
@@ -240,10 +242,11 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 /*
  * Initialise peer record.
  */
-static void rxrpc_init_peer(struct rxrpc_peer *peer, unsigned long hash_key)
+static void rxrpc_init_peer(struct rxrpc_sock *rx, struct rxrpc_peer *peer,
+			    unsigned long hash_key)
 {
 	peer->hash_key = hash_key;
-	rxrpc_assess_MTU_size(peer);
+	rxrpc_assess_MTU_size(rx, peer);
 	peer->mtu = peer->if_mtu;
 	peer->rtt_last_req = ktime_get_real();
 
@@ -275,7 +278,8 @@ static void rxrpc_init_peer(struct rxrpc_peer *peer, unsigned long hash_key)
 /*
  * Set up a new peer.
  */
-static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local,
+static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_sock *rx,
+					    struct rxrpc_local *local,
 					    struct sockaddr_rxrpc *srx,
 					    unsigned long hash_key,
 					    gfp_t gfp)
@@ -287,7 +291,7 @@ static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local,
 	peer = rxrpc_alloc_peer(local, gfp);
 	if (peer) {
 		memcpy(&peer->srx, srx, sizeof(*srx));
-		rxrpc_init_peer(peer, hash_key);
+		rxrpc_init_peer(rx, peer, hash_key);
 	}
 
 	_leave(" = %p", peer);
@@ -299,14 +303,15 @@ static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local,
  * since we've already done a search in the list from the non-reentrant context
  * (the data_ready handler) that is the only place we can add new peers.
  */
-void rxrpc_new_incoming_peer(struct rxrpc_local *local, struct rxrpc_peer *peer)
+void rxrpc_new_incoming_peer(struct rxrpc_sock *rx, struct rxrpc_local *local,
+			     struct rxrpc_peer *peer)
 {
 	struct rxrpc_net *rxnet = local->rxnet;
 	unsigned long hash_key;
 
 	hash_key = rxrpc_peer_hash_key(local, &peer->srx);
 	peer->local = local;
-	rxrpc_init_peer(peer, hash_key);
+	rxrpc_init_peer(rx, peer, hash_key);
 
 	spin_lock(&rxnet->peer_hash_lock);
 	hash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);
@@ -317,7 +322,8 @@ void rxrpc_new_incoming_peer(struct rxrpc_local *local, struct rxrpc_peer *peer)
 /*
  * obtain a remote transport endpoint for the specified address
  */
-struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
+struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_sock *rx,
+				     struct rxrpc_local *local,
 				     struct sockaddr_rxrpc *srx, gfp_t gfp)
 {
 	struct rxrpc_peer *peer, *candidate;
@@ -337,7 +343,7 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 		/* The peer is not yet present in hash - create a candidate
 		 * for a new record and then redo the search.
 		 */
-		candidate = rxrpc_create_peer(local, srx, hash_key, gfp);
+		candidate = rxrpc_create_peer(rx, local, srx, hash_key, gfp);
 		if (!candidate) {
 			_leave(" = NULL [nomem]");
 			return NULL;

commit f334430316e7fd37c4821ebec627e27714bb5d76
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 27 15:13:09 2018 +0100

    rxrpc: Fix error distribution
    
    Fix error distribution by immediately delivering the errors to all the
    affected calls rather than deferring them to a worker thread.  The problem
    with the latter is that retries and things can happen in the meantime when we
    want to stop that sooner.
    
    To this end:
    
     (1) Stop the error distributor from removing calls from the error_targets
         list so that peer->lock isn't needed to synchronise against other adds
         and removals.
    
     (2) Require the peer's error_targets list to be accessed with RCU, thereby
         avoiding the need to take peer->lock over distribution.
    
     (3) Don't attempt to affect a call's state if it is already marked complete.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 70083e8fb6e5..01a9febfa367 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -220,8 +220,6 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 		atomic_set(&peer->usage, 1);
 		peer->local = local;
 		INIT_HLIST_HEAD(&peer->error_targets);
-		INIT_WORK(&peer->error_distributor,
-			  &rxrpc_peer_error_distributor);
 		peer->service_conns = RB_ROOT;
 		seqlock_init(&peer->service_conn_lock);
 		spin_lock_init(&peer->lock);
@@ -402,21 +400,6 @@ struct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer)
 	return peer;
 }
 
-/*
- * Queue a peer record.  This passes the caller's ref to the workqueue.
- */
-void __rxrpc_queue_peer_error(struct rxrpc_peer *peer)
-{
-	const void *here = __builtin_return_address(0);
-	int n;
-
-	n = atomic_read(&peer->usage);
-	if (rxrpc_queue_work(&peer->error_distributor))
-		trace_rxrpc_peer(peer, rxrpc_peer_queued_error, n, here);
-	else
-		rxrpc_put_peer(peer);
-}
-
 /*
  * Discard a peer record.
  */

commit 0099dc589bfa7caf6f2608c4cbc1181cfee22b0c
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 27 15:13:09 2018 +0100

    rxrpc: Make service call handling more robust
    
    Make the following changes to improve the robustness of the code that sets
    up a new service call:
    
     (1) Cache the rxrpc_sock struct obtained in rxrpc_data_ready() to do a
         service ID check and pass that along to rxrpc_new_incoming_call().
         This means that I can remove the check from rxrpc_new_incoming_call()
         without the need to worry about the socket attached to the local
         endpoint getting replaced - which would invalidate the check.
    
     (2) Cache the rxrpc_peer struct, thereby allowing the peer search to be
         done once.  The peer is passed to rxrpc_new_incoming_call(), thereby
         saving the need to repeat the search.
    
         This also reduces the possibility of rxrpc_publish_service_conn()
         BUG()'ing due to the detection of a duplicate connection, despite the
         initial search done by rxrpc_find_connection_rcu() having turned up
         nothing.
    
         This BUG() shouldn't ever get hit since rxrpc_data_ready() *should* be
         non-reentrant and the result of the initial search should still hold
         true, but it has proven possible to hit.
    
         I *think* this may be due to __rxrpc_lookup_peer_rcu() cutting short
         the iteration over the hash table if it finds a matching peer with a
         zero usage count, but I don't know for sure since it's only ever been
         hit once that I know of.
    
         Another possibility is that a bug in rxrpc_data_ready() that checked
         the wrong byte in the header for the RXRPC_CLIENT_INITIATED flag
         might've let through a packet that caused a spurious and invalid call
         to be set up.  That is addressed in another patch.
    
     (3) Fix __rxrpc_lookup_peer_rcu() to skip peer records that have a zero
         usage count rather than stopping and returning not found, just in case
         there's another peer record behind it in the bucket.
    
     (4) Don't search the peer records in rxrpc_alloc_incoming_call(), but
         rather either use the peer cached in (2) or, if one wasn't found,
         preemptively install a new one.
    
    Fixes: 8496af50eb38 ("rxrpc: Use RCU to access a peer's service connection tree")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 1dc7648e3eff..70083e8fb6e5 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -124,11 +124,9 @@ static struct rxrpc_peer *__rxrpc_lookup_peer_rcu(
 	struct rxrpc_net *rxnet = local->rxnet;
 
 	hash_for_each_possible_rcu(rxnet->peer_hash, peer, hash_link, hash_key) {
-		if (rxrpc_peer_cmp_key(peer, local, srx, hash_key) == 0) {
-			if (atomic_read(&peer->usage) == 0)
-				return NULL;
+		if (rxrpc_peer_cmp_key(peer, local, srx, hash_key) == 0 &&
+		    atomic_read(&peer->usage) > 0)
 			return peer;
-		}
 	}
 
 	return NULL;
@@ -299,34 +297,23 @@ static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local,
 }
 
 /*
- * Set up a new incoming peer.  The address is prestored in the preallocated
- * peer.
+ * Set up a new incoming peer.  There shouldn't be any other matching peers
+ * since we've already done a search in the list from the non-reentrant context
+ * (the data_ready handler) that is the only place we can add new peers.
  */
-struct rxrpc_peer *rxrpc_lookup_incoming_peer(struct rxrpc_local *local,
-					      struct rxrpc_peer *prealloc)
+void rxrpc_new_incoming_peer(struct rxrpc_local *local, struct rxrpc_peer *peer)
 {
-	struct rxrpc_peer *peer;
 	struct rxrpc_net *rxnet = local->rxnet;
 	unsigned long hash_key;
 
-	hash_key = rxrpc_peer_hash_key(local, &prealloc->srx);
-	prealloc->local = local;
-	rxrpc_init_peer(prealloc, hash_key);
+	hash_key = rxrpc_peer_hash_key(local, &peer->srx);
+	peer->local = local;
+	rxrpc_init_peer(peer, hash_key);
 
 	spin_lock(&rxnet->peer_hash_lock);
-
-	/* Need to check that we aren't racing with someone else */
-	peer = __rxrpc_lookup_peer_rcu(local, &prealloc->srx, hash_key);
-	if (peer && !rxrpc_get_peer_maybe(peer))
-		peer = NULL;
-	if (!peer) {
-		peer = prealloc;
-		hash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);
-		list_add_tail(&peer->keepalive_link, &rxnet->peer_keepalive_new);
-	}
-
+	hash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);
+	list_add_tail(&peer->keepalive_link, &rxnet->peer_keepalive_new);
 	spin_unlock(&rxnet->peer_hash_lock);
-	return peer;
 }
 
 /*

commit de5d1b39ea0b38a9f4dfb08966042b7b91e2df30
Merge: 1c594774283a fd2efaa4eb53
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 12:23:39 2018 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking/atomics update from Thomas Gleixner:
     "The locking, atomics and memory model brains delivered:
    
       - A larger update to the atomics code which reworks the ordering
         barriers, consolidates the atomic primitives, provides the new
         atomic64_fetch_add_unless() primitive and cleans up the include
         hell.
    
       - Simplify cmpxchg() instrumentation and add instrumentation for
         xchg() and cmpxchg_double().
    
       - Updates to the memory model and documentation"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (48 commits)
      locking/atomics: Rework ordering barriers
      locking/atomics: Instrument cmpxchg_double*()
      locking/atomics: Instrument xchg()
      locking/atomics: Simplify cmpxchg() instrumentation
      locking/atomics/x86: Reduce arch_cmpxchg64*() instrumentation
      tools/memory-model: Rename litmus tests to comply to norm7
      tools/memory-model/Documentation: Fix typo, smb->smp
      sched/Documentation: Update wake_up() & co. memory-barrier guarantees
      locking/spinlock, sched/core: Clarify requirements for smp_mb__after_spinlock()
      sched/core: Use smp_mb() in wake_woken_function()
      tools/memory-model: Add informal LKMM documentation to MAINTAINERS
      locking/atomics/Documentation: Describe atomic_set() as a write operation
      tools/memory-model: Make scripts executable
      tools/memory-model: Remove ACCESS_ONCE() from model
      tools/memory-model: Remove ACCESS_ONCE() from recipes
      locking/memory-barriers.txt/kokr: Update Korean translation to fix broken DMA vs. MMIO ordering example
      MAINTAINERS: Add Daniel Lustig as an LKMM reviewer
      tools/memory-model: Fix ISA2+pooncelock+pooncelock+pombonce name
      tools/memory-model: Add litmus test for full multicopy atomicity
      locking/refcount: Always allow checked forms
      ...

commit 330bdcfadceea5e9a1526d731711e163f9a90975
Author: David Howells <dhowells@redhat.com>
Date:   Wed Aug 8 11:30:02 2018 +0100

    rxrpc: Fix the keepalive generator [ver #2]
    
    AF_RXRPC has a keepalive message generator that generates a message for a
    peer ~20s after the last transmission to that peer to keep firewall ports
    open.  The implementation is incorrect in the following ways:
    
     (1) It mixes up ktime_t and time64_t types.
    
     (2) It uses ktime_get_real(), the output of which may jump forward or
         backward due to adjustments to the time of day.
    
     (3) If the current time jumps forward too much or jumps backwards, the
         generator function will crank the base of the time ring round one slot
         at a time (ie. a 1s period) until it catches up, spewing out VERSION
         packets as it goes.
    
    Fix the problem by:
    
     (1) Only using time64_t.  There's no need for sub-second resolution.
    
     (2) Use ktime_get_seconds() rather than ktime_get_real() so that time
         isn't perceived to go backwards.
    
     (3) Simplifying rxrpc_peer_keepalive_worker() by splitting it into two
         parts:
    
         (a) The "worker" function that manages the buckets and the timer.
    
         (b) The "dispatch" function that takes the pending peers and
             potentially transmits a keepalive packet before putting them back
             in the ring into the slot appropriate to the revised last-Tx time.
    
     (4) Taking everything that's pending out of the ring and splicing it into
         a temporary collector list for processing.
    
         In the case that there's been a significant jump forward, the ring
         gets entirely emptied and then the time base can be warped forward
         before the peers are processed.
    
         The warping can't happen if the ring isn't empty because the slot a
         peer is in is keepalive-time dependent, relative to the base time.
    
     (5) Limit the number of iterations of the bucket array when scanning it.
    
     (6) Set the timer to skip any empty slots as there's no point waking up if
         there's nothing to do yet.
    
    This can be triggered by an incoming call from a server after a reboot with
    AF_RXRPC and AFS built into the kernel causing a peer record to be set up
    before userspace is started.  The system clock is then adjusted by
    userspace, thereby potentially causing the keepalive generator to have a
    meltdown - which leads to a message like:
    
            watchdog: BUG: soft lockup - CPU#0 stuck for 23s! [kworker/0:1:23]
            ...
            Workqueue: krxrpcd rxrpc_peer_keepalive_worker
            EIP: lock_acquire+0x69/0x80
            ...
            Call Trace:
             ? rxrpc_peer_keepalive_worker+0x5e/0x350
             ? _raw_spin_lock_bh+0x29/0x60
             ? rxrpc_peer_keepalive_worker+0x5e/0x350
             ? rxrpc_peer_keepalive_worker+0x5e/0x350
             ? __lock_acquire+0x3d3/0x870
             ? process_one_work+0x110/0x340
             ? process_one_work+0x166/0x340
             ? process_one_work+0x110/0x340
             ? worker_thread+0x39/0x3c0
             ? kthread+0xdb/0x110
             ? cancel_delayed_work+0x90/0x90
             ? kthread_stop+0x70/0x70
             ? ret_from_fork+0x19/0x24
    
    Fixes: ace45bec6d77 ("rxrpc: Fix firewall route keepalive")
    Reported-by: kernel test robot <lkp@intel.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 1b7e8107b3ae..24ec7cdcf332 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -322,7 +322,7 @@ struct rxrpc_peer *rxrpc_lookup_incoming_peer(struct rxrpc_local *local,
 	if (!peer) {
 		peer = prealloc;
 		hash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);
-		hlist_add_head(&peer->keepalive_link, &rxnet->peer_keepalive_new);
+		list_add_tail(&peer->keepalive_link, &rxnet->peer_keepalive_new);
 	}
 
 	spin_unlock(&rxnet->peer_hash_lock);
@@ -367,8 +367,8 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 		if (!peer) {
 			hash_add_rcu(rxnet->peer_hash,
 				     &candidate->hash_link, hash_key);
-			hlist_add_head(&candidate->keepalive_link,
-				       &rxnet->peer_keepalive_new);
+			list_add_tail(&candidate->keepalive_link,
+				      &rxnet->peer_keepalive_new);
 		}
 
 		spin_unlock_bh(&rxnet->peer_hash_lock);
@@ -441,7 +441,7 @@ static void __rxrpc_put_peer(struct rxrpc_peer *peer)
 
 	spin_lock_bh(&rxnet->peer_hash_lock);
 	hash_del_rcu(&peer->hash_link);
-	hlist_del_init(&peer->keepalive_link);
+	list_del_init(&peer->keepalive_link);
 	spin_unlock_bh(&rxnet->peer_hash_lock);
 
 	kfree_rcu(peer, rcu);

commit bfc18e389c7a09fbbbed6bf4032396685b14246e
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Thu Jun 21 13:13:04 2018 +0100

    atomics/treewide: Rename __atomic_add_unless() => atomic_fetch_add_unless()
    
    While __atomic_add_unless() was originally intended as a building-block
    for atomic_add_unless(), it's now used in a number of places around the
    kernel. It's the only common atomic operation named __atomic*(), rather
    than atomic_*(), and for consistency it would be better named
    atomic_fetch_add_unless().
    
    This lack of consistency is slightly confusing, and gets in the way of
    scripting atomics. Given that, let's clean things up and promote it to
    an official part of the atomics API, in the form of
    atomic_fetch_add_unless().
    
    This patch converts definitions and invocations over to the new name,
    including the instrumented version, using the following script:
    
      ----
      git grep -w __atomic_add_unless | while read line; do
      sed -i '{s/\<__atomic_add_unless\>/atomic_fetch_add_unless/}' "${line%%:*}";
      done
      git grep -w __arch_atomic_add_unless | while read line; do
      sed -i '{s/\<__arch_atomic_add_unless\>/arch_atomic_fetch_add_unless/}' "${line%%:*}";
      done
      ----
    
    Note that we do not have atomic{64,_long}_fetch_add_unless(), which will
    be introduced by later patches.
    
    There should be no functional change as a result of this patch.
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Acked-by: Geert Uytterhoeven <geert@linux-m68k.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Palmer Dabbelt <palmer@sifive.com>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/lkml/20180621121321.4761-2-mark.rutland@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 1b7e8107b3ae..1cf3b408017a 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -406,7 +406,7 @@ struct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer)
 	const void *here = __builtin_return_address(0);
 
 	if (peer) {
-		int n = __atomic_add_unless(&peer->usage, 1, 0);
+		int n = atomic_fetch_add_unless(&peer->usage, 1, 0);
 		if (n > 0)
 			trace_rxrpc_peer(peer, rxrpc_peer_got, n + 1, here);
 		else

commit 17226f1240381812c3a4927dc9da2814fb71c8ac
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 30 21:05:44 2018 +0100

    rxrpc: Fix leak of rxrpc_peer objects
    
    When a new client call is requested, an rxrpc_conn_parameters struct object
    is passed in with a bunch of parameters set, such as the local endpoint to
    use.  A pointer to the target peer record is also placed in there by
    rxrpc_get_client_conn() - and this is removed if and only if a new
    connection object is allocated.  Thus it leaks if a new connection object
    isn't allocated.
    
    Fix this by putting any peer object attached to the rxrpc_conn_parameters
    object in the function that allocated it.
    
    Fixes: 19ffa01c9c45 ("rxrpc: Use structs to hold connection params and protocol info")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index a4a750aea1e5..1b7e8107b3ae 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -463,6 +463,27 @@ void rxrpc_put_peer(struct rxrpc_peer *peer)
 	}
 }
 
+/*
+ * Make sure all peer records have been discarded.
+ */
+void rxrpc_destroy_all_peers(struct rxrpc_net *rxnet)
+{
+	struct rxrpc_peer *peer;
+	int i;
+
+	for (i = 0; i < HASH_SIZE(rxnet->peer_hash); i++) {
+		if (hlist_empty(&rxnet->peer_hash[i]))
+			continue;
+
+		hlist_for_each_entry(peer, &rxnet->peer_hash[i], hash_link) {
+			pr_err("Leaked peer %u {%u} %pISp\n",
+			       peer->debug_id,
+			       atomic_read(&peer->usage),
+			       &peer->srx.transport);
+		}
+	}
+}
+
 /**
  * rxrpc_kernel_get_peer - Get the peer address of a call
  * @sock: The socket on which the call is in progress.

commit 1159d4b496f57d5b8ee27c8b90b9d01c332e2e11
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 30 21:05:38 2018 +0100

    rxrpc: Add a tracepoint to track rxrpc_peer refcounting
    
    Add a tracepoint to track reference counting on the rxrpc_peer struct.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 94a6dbfcf129..a4a750aea1e5 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -386,9 +386,54 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 }
 
 /*
- * Discard a ref on a remote peer record.
+ * Get a ref on a peer record.
  */
-void __rxrpc_put_peer(struct rxrpc_peer *peer)
+struct rxrpc_peer *rxrpc_get_peer(struct rxrpc_peer *peer)
+{
+	const void *here = __builtin_return_address(0);
+	int n;
+
+	n = atomic_inc_return(&peer->usage);
+	trace_rxrpc_peer(peer, rxrpc_peer_got, n, here);
+	return peer;
+}
+
+/*
+ * Get a ref on a peer record unless its usage has already reached 0.
+ */
+struct rxrpc_peer *rxrpc_get_peer_maybe(struct rxrpc_peer *peer)
+{
+	const void *here = __builtin_return_address(0);
+
+	if (peer) {
+		int n = __atomic_add_unless(&peer->usage, 1, 0);
+		if (n > 0)
+			trace_rxrpc_peer(peer, rxrpc_peer_got, n + 1, here);
+		else
+			peer = NULL;
+	}
+	return peer;
+}
+
+/*
+ * Queue a peer record.  This passes the caller's ref to the workqueue.
+ */
+void __rxrpc_queue_peer_error(struct rxrpc_peer *peer)
+{
+	const void *here = __builtin_return_address(0);
+	int n;
+
+	n = atomic_read(&peer->usage);
+	if (rxrpc_queue_work(&peer->error_distributor))
+		trace_rxrpc_peer(peer, rxrpc_peer_queued_error, n, here);
+	else
+		rxrpc_put_peer(peer);
+}
+
+/*
+ * Discard a peer record.
+ */
+static void __rxrpc_put_peer(struct rxrpc_peer *peer)
 {
 	struct rxrpc_net *rxnet = peer->local->rxnet;
 
@@ -402,6 +447,22 @@ void __rxrpc_put_peer(struct rxrpc_peer *peer)
 	kfree_rcu(peer, rcu);
 }
 
+/*
+ * Drop a ref on a peer record.
+ */
+void rxrpc_put_peer(struct rxrpc_peer *peer)
+{
+	const void *here = __builtin_return_address(0);
+	int n;
+
+	if (peer) {
+		n = atomic_dec_return(&peer->usage);
+		trace_rxrpc_peer(peer, rxrpc_peer_put, n, here);
+		if (n == 0)
+			__rxrpc_put_peer(peer);
+	}
+}
+
 /**
  * rxrpc_kernel_get_peer - Get the peer address of a call
  * @sock: The socket on which the call is in progress.

commit ace45bec6d77bc061c3c3d8ad99e298ea9800c2b
Author: David Howells <dhowells@redhat.com>
Date:   Fri Mar 30 21:04:43 2018 +0100

    rxrpc: Fix firewall route keepalive
    
    Fix the firewall route keepalive part of AF_RXRPC which is currently
    function incorrectly by replying to VERSION REPLY packets from the server
    with VERSION REQUEST packets.
    
    Instead, send VERSION REPLY packets to the peers of service connections to
    act as keep-alives 20s after the latest packet was transmitted to that
    peer.
    
    Also, just discard VERSION REPLY packets rather than replying to them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index d02a99f37f5f..94a6dbfcf129 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -322,6 +322,7 @@ struct rxrpc_peer *rxrpc_lookup_incoming_peer(struct rxrpc_local *local,
 	if (!peer) {
 		peer = prealloc;
 		hash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);
+		hlist_add_head(&peer->keepalive_link, &rxnet->peer_keepalive_new);
 	}
 
 	spin_unlock(&rxnet->peer_hash_lock);
@@ -363,9 +364,12 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 		peer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);
 		if (peer && !rxrpc_get_peer_maybe(peer))
 			peer = NULL;
-		if (!peer)
+		if (!peer) {
 			hash_add_rcu(rxnet->peer_hash,
 				     &candidate->hash_link, hash_key);
+			hlist_add_head(&candidate->keepalive_link,
+				       &rxnet->peer_keepalive_new);
+		}
 
 		spin_unlock_bh(&rxnet->peer_hash_lock);
 
@@ -392,6 +396,7 @@ void __rxrpc_put_peer(struct rxrpc_peer *peer)
 
 	spin_lock_bh(&rxnet->peer_hash_lock);
 	hash_del_rcu(&peer->hash_link);
+	hlist_del_init(&peer->keepalive_link);
 	spin_unlock_bh(&rxnet->peer_hash_lock);
 
 	kfree_rcu(peer, rcu);

commit f4d15fb6f99af9b99f688bd87579137be44f85ee
Author: David Howells <dhowells@redhat.com>
Date:   Wed Oct 18 11:07:31 2017 +0100

    rxrpc: Provide functions for allowing cleaner handling of signals
    
    Provide a couple of functions to allow cleaner handling of signals in a
    kernel service.  They are:
    
     (1) rxrpc_kernel_get_rtt()
    
         This allows the kernel service to find out the RTT time for a call, so
         as to better judge how large a timeout to employ.
    
         Note, though, that whilst this returns a value in nanoseconds, the
         timeouts can only actually be in jiffies.
    
     (2) rxrpc_kernel_check_life()
    
         This returns a number that is updated when ACKs are received from the
         peer (notably including PING RESPONSE ACKs which we can elicit by
         sending PING ACKs to see if the call still exists on the server).
    
         The caller should compare the numbers of two calls to see if the call
         is still alive.
    
    These can be used to provide an extending timeout rather than returning
    immediately in the case that a signal occurs that would otherwise abort an
    RPC operation.  The timeout would be extended if the server is still
    responsive and the call is still apparently alive on the server.
    
    For most operations this isn't that necessary - but for FS.StoreData it is:
    OpenAFS writes the data to storage as it comes in without making a backup,
    so if we immediately abort it when partially complete on a CTRL+C, say, we
    have no idea of the state of the file after the abort.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 5787f97f5330..d02a99f37f5f 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -411,3 +411,16 @@ void rxrpc_kernel_get_peer(struct socket *sock, struct rxrpc_call *call,
 	*_srx = call->peer->srx;
 }
 EXPORT_SYMBOL(rxrpc_kernel_get_peer);
+
+/**
+ * rxrpc_kernel_get_rtt - Get a call's peer RTT
+ * @sock: The socket on which the call is in progress.
+ * @call: The call to query
+ *
+ * Get the call's peer RTT.
+ */
+u64 rxrpc_kernel_get_rtt(struct socket *sock, struct rxrpc_call *call)
+{
+	return call->peer->rtt;
+}
+EXPORT_SYMBOL(rxrpc_kernel_get_rtt);

commit f7aec129a356ad049edddcb7e77b04a474fcf41f
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jun 14 17:56:50 2017 +0100

    rxrpc: Cache the congestion window setting
    
    Cache the congestion window setting that was determined during a call's
    transmission phase when it finishes so that it can be used by the next call
    to the same peer, thereby shortcutting the slow-start algorithm.
    
    The value is stored in the rxrpc_peer struct and is accessed without
    locking.  Each call takes the value that happens to be there when it starts
    and just overwrites the value when it finishes.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index cfed3b27adf0..5787f97f5330 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -228,6 +228,13 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 		seqlock_init(&peer->service_conn_lock);
 		spin_lock_init(&peer->lock);
 		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
+
+		if (RXRPC_TX_SMSS > 2190)
+			peer->cong_cwnd = 2;
+		else if (RXRPC_TX_SMSS > 1095)
+			peer->cong_cwnd = 3;
+		else
+			peer->cong_cwnd = 4;
 	}
 
 	_leave(" = %p", peer);

commit 2baec2c3f854d1f79c7bb28386484e144e864a14
Author: David Howells <dhowells@redhat.com>
Date:   Wed May 24 17:02:32 2017 +0100

    rxrpc: Support network namespacing
    
    Support network namespacing in AF_RXRPC with the following changes:
    
     (1) All the local endpoint, peer and call lists, locks, counters, etc. are
         moved into the per-namespace record.
    
     (2) All the connection tracking is moved into the per-namespace record
         with the exception of the client connection ID tree, which is kept
         global so that connection IDs are kept unique per-machine.
    
     (3) Each namespace gets its own epoch.  This allows each network namespace
         to pretend to be a separate client machine.
    
     (4) The /proc/net/rxrpc_xxx files are now called /proc/net/rxrpc/xxx and
         the contents reflect the namespace.
    
    fs/afs/ should be okay with this patch as it explicitly requires the current
    net namespace to be init_net to permit a mount to proceed at the moment.  It
    will, however, need updating so that cells, IP addresses and DNS records are
    per-namespace also.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 862eea6b266c..cfed3b27adf0 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -26,9 +26,6 @@
 #include <net/ip6_route.h>
 #include "ar-internal.h"
 
-static DEFINE_HASHTABLE(rxrpc_peer_hash, 10);
-static DEFINE_SPINLOCK(rxrpc_peer_hash_lock);
-
 /*
  * Hash a peer key.
  */
@@ -124,8 +121,9 @@ static struct rxrpc_peer *__rxrpc_lookup_peer_rcu(
 	unsigned long hash_key)
 {
 	struct rxrpc_peer *peer;
+	struct rxrpc_net *rxnet = local->rxnet;
 
-	hash_for_each_possible_rcu(rxrpc_peer_hash, peer, hash_link, hash_key) {
+	hash_for_each_possible_rcu(rxnet->peer_hash, peer, hash_link, hash_key) {
 		if (rxrpc_peer_cmp_key(peer, local, srx, hash_key) == 0) {
 			if (atomic_read(&peer->usage) == 0)
 				return NULL;
@@ -301,13 +299,14 @@ struct rxrpc_peer *rxrpc_lookup_incoming_peer(struct rxrpc_local *local,
 					      struct rxrpc_peer *prealloc)
 {
 	struct rxrpc_peer *peer;
+	struct rxrpc_net *rxnet = local->rxnet;
 	unsigned long hash_key;
 
 	hash_key = rxrpc_peer_hash_key(local, &prealloc->srx);
 	prealloc->local = local;
 	rxrpc_init_peer(prealloc, hash_key);
 
-	spin_lock(&rxrpc_peer_hash_lock);
+	spin_lock(&rxnet->peer_hash_lock);
 
 	/* Need to check that we aren't racing with someone else */
 	peer = __rxrpc_lookup_peer_rcu(local, &prealloc->srx, hash_key);
@@ -315,10 +314,10 @@ struct rxrpc_peer *rxrpc_lookup_incoming_peer(struct rxrpc_local *local,
 		peer = NULL;
 	if (!peer) {
 		peer = prealloc;
-		hash_add_rcu(rxrpc_peer_hash, &peer->hash_link, hash_key);
+		hash_add_rcu(rxnet->peer_hash, &peer->hash_link, hash_key);
 	}
 
-	spin_unlock(&rxrpc_peer_hash_lock);
+	spin_unlock(&rxnet->peer_hash_lock);
 	return peer;
 }
 
@@ -329,6 +328,7 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 				     struct sockaddr_rxrpc *srx, gfp_t gfp)
 {
 	struct rxrpc_peer *peer, *candidate;
+	struct rxrpc_net *rxnet = local->rxnet;
 	unsigned long hash_key = rxrpc_peer_hash_key(local, srx);
 
 	_enter("{%pISp}", &srx->transport);
@@ -350,17 +350,17 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 			return NULL;
 		}
 
-		spin_lock_bh(&rxrpc_peer_hash_lock);
+		spin_lock_bh(&rxnet->peer_hash_lock);
 
 		/* Need to check that we aren't racing with someone else */
 		peer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);
 		if (peer && !rxrpc_get_peer_maybe(peer))
 			peer = NULL;
 		if (!peer)
-			hash_add_rcu(rxrpc_peer_hash,
+			hash_add_rcu(rxnet->peer_hash,
 				     &candidate->hash_link, hash_key);
 
-		spin_unlock_bh(&rxrpc_peer_hash_lock);
+		spin_unlock_bh(&rxnet->peer_hash_lock);
 
 		if (peer)
 			kfree(candidate);
@@ -379,11 +379,13 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
  */
 void __rxrpc_put_peer(struct rxrpc_peer *peer)
 {
+	struct rxrpc_net *rxnet = peer->local->rxnet;
+
 	ASSERT(hlist_empty(&peer->error_targets));
 
-	spin_lock_bh(&rxrpc_peer_hash_lock);
+	spin_lock_bh(&rxnet->peer_hash_lock);
 	hash_del_rcu(&peer->hash_link);
-	spin_unlock_bh(&rxrpc_peer_hash_lock);
+	spin_unlock_bh(&rxnet->peer_hash_lock);
 
 	kfree_rcu(peer, rcu);
 }

commit 07096f612fdf2bb5578cd1fecb2884bdbb1cde42
Author: David Howells <dhowells@redhat.com>
Date:   Thu Oct 13 08:43:17 2016 +0100

    rxrpc: Fix checking of error from ip6_route_output()
    
    ip6_route_output() doesn't return a negative error when it fails, rather
    the ->error field of the returned dst_entry struct needs to be checked.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Fixes: 75b54cb57ca3 ("rxrpc: Add IPv6 support")
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 941b724d523b..862eea6b266c 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -193,8 +193,8 @@ static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 		fl6->fl6_dport = htons(7001);
 		fl6->fl6_sport = htons(7000);
 		dst = ip6_route_output(&init_net, NULL, fl6);
-		if (IS_ERR(dst)) {
-			_leave(" [route err %ld]", PTR_ERR(dst));
+		if (dst->error) {
+			_leave(" [route err %d]", dst->error);
 			return;
 		}
 		break;

commit 0d4b103c008ac9f6f438d2618c155f6e868e5a67
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 22 00:29:31 2016 +0100

    rxrpc: Reduce the number of ACK-Requests sent
    
    Reduce the number of ACK-Requests we set on DATA packets that we're sending
    to reduce network traffic.  We set the flag on odd-numbered DATA packets to
    start off the RTT cache until we have at least three entries in it and then
    probe once per second thereafter to keep it topped up.
    
    This could be made tunable in future.
    
    Note that from this point, the RXRPC_REQUEST_ACK flag is set on DATA
    packets as we transmit them and not stored statically in the sk_buff.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index f3e5766910fd..941b724d523b 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -244,6 +244,7 @@ static void rxrpc_init_peer(struct rxrpc_peer *peer, unsigned long hash_key)
 	peer->hash_key = hash_key;
 	rxrpc_assess_MTU_size(peer);
 	peer->mtu = peer->if_mtu;
+	peer->rtt_last_req = ktime_get_real();
 
 	switch (peer->srx.transport.family) {
 	case AF_INET:

commit d19127473a575c629c70974cee0bb8acb6374f08
Author: David Howells <dhowells@redhat.com>
Date:   Sat Sep 17 07:26:01 2016 +0100

    rxrpc: Make IPv6 support conditional on CONFIG_IPV6
    
    Add CONFIG_AF_RXRPC_IPV6 and make the IPv6 support code conditional on it.
    This is then made conditional on CONFIG_IPV6.
    
    Without this, the following can be seen:
    
       net/built-in.o: In function `rxrpc_init_peer':
    >> peer_object.c:(.text+0x18c3c8): undefined reference to `ip6_route_output_flags'
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index dfc07b41a472..f3e5766910fd 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -52,11 +52,13 @@ static unsigned long rxrpc_peer_hash_key(struct rxrpc_local *local,
 		size = sizeof(srx->transport.sin.sin_addr);
 		p = (u16 *)&srx->transport.sin.sin_addr;
 		break;
+#ifdef CONFIG_AF_RXRPC_IPV6
 	case AF_INET6:
 		hash_key += (u16 __force)srx->transport.sin.sin_port;
 		size = sizeof(srx->transport.sin6.sin6_addr);
 		p = (u16 *)&srx->transport.sin6.sin6_addr;
 		break;
+#endif
 	default:
 		WARN(1, "AF_RXRPC: Unsupported transport address family\n");
 		return 0;
@@ -100,12 +102,14 @@ static long rxrpc_peer_cmp_key(const struct rxrpc_peer *peer,
 			memcmp(&peer->srx.transport.sin.sin_addr,
 			       &srx->transport.sin.sin_addr,
 			       sizeof(struct in_addr));
+#ifdef CONFIG_AF_RXRPC_IPV6
 	case AF_INET6:
 		return ((u16 __force)peer->srx.transport.sin6.sin6_port -
 			(u16 __force)srx->transport.sin6.sin6_port) ?:
 			memcmp(&peer->srx.transport.sin6.sin6_addr,
 			       &srx->transport.sin6.sin6_addr,
 			       sizeof(struct in6_addr));
+#endif
 	default:
 		BUG();
 	}
@@ -159,7 +163,9 @@ static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 	struct rtable *rt;
 	struct flowi fl;
 	struct flowi4 *fl4 = &fl.u.ip4;
+#ifdef CONFIG_AF_RXRPC_IPV6
 	struct flowi6 *fl6 = &fl.u.ip6;
+#endif
 
 	peer->if_mtu = 1500;
 
@@ -177,6 +183,7 @@ static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 		dst = &rt->dst;
 		break;
 
+#ifdef CONFIG_AF_RXRPC_IPV6
 	case AF_INET6:
 		fl6->flowi6_iif = LOOPBACK_IFINDEX;
 		fl6->flowi6_scope = RT_SCOPE_UNIVERSE;
@@ -191,6 +198,7 @@ static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 			return;
 		}
 		break;
+#endif
 
 	default:
 		BUG();
@@ -241,9 +249,11 @@ static void rxrpc_init_peer(struct rxrpc_peer *peer, unsigned long hash_key)
 	case AF_INET:
 		peer->hdrsize = sizeof(struct iphdr);
 		break;
+#ifdef CONFIG_AF_RXRPC_IPV6
 	case AF_INET6:
 		peer->hdrsize = sizeof(struct ipv6hdr);
 		break;
+#endif
 	default:
 		BUG();
 	}

commit 75b54cb57ca34cbe7a87c6ac757c55360a624590
Author: David Howells <dhowells@redhat.com>
Date:   Tue Sep 13 08:49:05 2016 +0100

    rxrpc: Add IPv6 support
    
    Add IPv6 support to AF_RXRPC.  With this, AF_RXRPC sockets can be created:
    
            service = socket(AF_RXRPC, SOCK_DGRAM, PF_INET6);
    
    instead of:
    
            service = socket(AF_RXRPC, SOCK_DGRAM, PF_INET);
    
    The AFS filesystem doesn't support IPv6 at the moment, though, since that
    requires upgrades to some of the RPC calls.
    
    Note that a good portion of this patch is replacing "%pI4:%u" in print
    statements with "%pISpc" which is able to handle both protocols and print
    the port.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 3e6cd174b53d..dfc07b41a472 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -16,12 +16,14 @@
 #include <linux/skbuff.h>
 #include <linux/udp.h>
 #include <linux/in.h>
+#include <linux/in6.h>
 #include <linux/slab.h>
 #include <linux/hashtable.h>
 #include <net/sock.h>
 #include <net/af_rxrpc.h>
 #include <net/ip.h>
 #include <net/route.h>
+#include <net/ip6_route.h>
 #include "ar-internal.h"
 
 static DEFINE_HASHTABLE(rxrpc_peer_hash, 10);
@@ -50,6 +52,11 @@ static unsigned long rxrpc_peer_hash_key(struct rxrpc_local *local,
 		size = sizeof(srx->transport.sin.sin_addr);
 		p = (u16 *)&srx->transport.sin.sin_addr;
 		break;
+	case AF_INET6:
+		hash_key += (u16 __force)srx->transport.sin.sin_port;
+		size = sizeof(srx->transport.sin6.sin6_addr);
+		p = (u16 *)&srx->transport.sin6.sin6_addr;
+		break;
 	default:
 		WARN(1, "AF_RXRPC: Unsupported transport address family\n");
 		return 0;
@@ -93,6 +100,12 @@ static long rxrpc_peer_cmp_key(const struct rxrpc_peer *peer,
 			memcmp(&peer->srx.transport.sin.sin_addr,
 			       &srx->transport.sin.sin_addr,
 			       sizeof(struct in_addr));
+	case AF_INET6:
+		return ((u16 __force)peer->srx.transport.sin6.sin6_port -
+			(u16 __force)srx->transport.sin6.sin6_port) ?:
+			memcmp(&peer->srx.transport.sin6.sin6_addr,
+			       &srx->transport.sin6.sin6_addr,
+			       sizeof(struct in6_addr));
 	default:
 		BUG();
 	}
@@ -130,17 +143,7 @@ struct rxrpc_peer *rxrpc_lookup_peer_rcu(struct rxrpc_local *local,
 
 	peer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);
 	if (peer) {
-		switch (srx->transport.family) {
-		case AF_INET:
-			_net("PEER %d {%d,%u,%pI4+%hu}",
-			     peer->debug_id,
-			     peer->srx.transport_type,
-			     peer->srx.transport.family,
-			     &peer->srx.transport.sin.sin_addr,
-			     ntohs(peer->srx.transport.sin.sin_port));
-			break;
-		}
-
+		_net("PEER %d {%pISp}", peer->debug_id, &peer->srx.transport);
 		_leave(" = %p {u=%d}", peer, atomic_read(&peer->usage));
 	}
 	return peer;
@@ -152,22 +155,49 @@ struct rxrpc_peer *rxrpc_lookup_peer_rcu(struct rxrpc_local *local,
  */
 static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 {
+	struct dst_entry *dst;
 	struct rtable *rt;
-	struct flowi4 fl4;
+	struct flowi fl;
+	struct flowi4 *fl4 = &fl.u.ip4;
+	struct flowi6 *fl6 = &fl.u.ip6;
 
 	peer->if_mtu = 1500;
 
-	rt = ip_route_output_ports(&init_net, &fl4, NULL,
-				   peer->srx.transport.sin.sin_addr.s_addr, 0,
-				   htons(7000), htons(7001),
-				   IPPROTO_UDP, 0, 0);
-	if (IS_ERR(rt)) {
-		_leave(" [route err %ld]", PTR_ERR(rt));
-		return;
+	memset(&fl, 0, sizeof(fl));
+	switch (peer->srx.transport.family) {
+	case AF_INET:
+		rt = ip_route_output_ports(
+			&init_net, fl4, NULL,
+			peer->srx.transport.sin.sin_addr.s_addr, 0,
+			htons(7000), htons(7001), IPPROTO_UDP, 0, 0);
+		if (IS_ERR(rt)) {
+			_leave(" [route err %ld]", PTR_ERR(rt));
+			return;
+		}
+		dst = &rt->dst;
+		break;
+
+	case AF_INET6:
+		fl6->flowi6_iif = LOOPBACK_IFINDEX;
+		fl6->flowi6_scope = RT_SCOPE_UNIVERSE;
+		fl6->flowi6_proto = IPPROTO_UDP;
+		memcpy(&fl6->daddr, &peer->srx.transport.sin6.sin6_addr,
+		       sizeof(struct in6_addr));
+		fl6->fl6_dport = htons(7001);
+		fl6->fl6_sport = htons(7000);
+		dst = ip6_route_output(&init_net, NULL, fl6);
+		if (IS_ERR(dst)) {
+			_leave(" [route err %ld]", PTR_ERR(dst));
+			return;
+		}
+		break;
+
+	default:
+		BUG();
 	}
 
-	peer->if_mtu = dst_mtu(&rt->dst);
-	dst_release(&rt->dst);
+	peer->if_mtu = dst_mtu(dst);
+	dst_release(dst);
 
 	_leave(" [if_mtu %u]", peer->if_mtu);
 }
@@ -207,17 +237,22 @@ static void rxrpc_init_peer(struct rxrpc_peer *peer, unsigned long hash_key)
 	rxrpc_assess_MTU_size(peer);
 	peer->mtu = peer->if_mtu;
 
-	if (peer->srx.transport.family == AF_INET) {
+	switch (peer->srx.transport.family) {
+	case AF_INET:
 		peer->hdrsize = sizeof(struct iphdr);
-		switch (peer->srx.transport_type) {
-		case SOCK_DGRAM:
-			peer->hdrsize += sizeof(struct udphdr);
-			break;
-		default:
-			BUG();
-			break;
-		}
-	} else {
+		break;
+	case AF_INET6:
+		peer->hdrsize = sizeof(struct ipv6hdr);
+		break;
+	default:
+		BUG();
+	}
+
+	switch (peer->srx.transport_type) {
+	case SOCK_DGRAM:
+		peer->hdrsize += sizeof(struct udphdr);
+		break;
+	default:
 		BUG();
 	}
 
@@ -285,11 +320,7 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 	struct rxrpc_peer *peer, *candidate;
 	unsigned long hash_key = rxrpc_peer_hash_key(local, srx);
 
-	_enter("{%d,%d,%pI4+%hu}",
-	       srx->transport_type,
-	       srx->transport_len,
-	       &srx->transport.sin.sin_addr,
-	       ntohs(srx->transport.sin.sin_port));
+	_enter("{%pISp}", &srx->transport);
 
 	/* search the peer list first */
 	rcu_read_lock();
@@ -326,11 +357,7 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 			peer = candidate;
 	}
 
-	_net("PEER %d {%d,%pI4+%hu}",
-	     peer->debug_id,
-	     peer->srx.transport_type,
-	     &peer->srx.transport.sin.sin_addr,
-	     ntohs(peer->srx.transport.sin.sin_port));
+	_net("PEER %d {%pISp}", peer->debug_id, &peer->srx.transport);
 
 	_leave(" = %p {u=%d}", peer, atomic_read(&peer->usage));
 	return peer;

commit 08a39685a771b4b1108889ea5e4e0a71b51782ba
Author: David Howells <dhowells@redhat.com>
Date:   Tue Sep 13 22:36:21 2016 +0100

    rxrpc: Make sure we initialise the peer hash key
    
    Peer records created for incoming connections weren't getting their hash
    key set.  This meant that incoming calls wouldn't see more than one DATA
    packet - which is not a problem for AFS CM calls with small request data
    blobs.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 2efe29a4c232..3e6cd174b53d 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -203,6 +203,7 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
  */
 static void rxrpc_init_peer(struct rxrpc_peer *peer, unsigned long hash_key)
 {
+	peer->hash_key = hash_key;
 	rxrpc_assess_MTU_size(peer);
 	peer->mtu = peer->if_mtu;
 
@@ -238,7 +239,6 @@ static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local,
 
 	peer = rxrpc_alloc_peer(local, gfp);
 	if (peer) {
-		peer->hash_key = hash_key;
 		memcpy(&peer->srx, srx, sizeof(*srx));
 		rxrpc_init_peer(peer, hash_key);
 	}

commit 248f219cb8bcbfbd7f132752d44afa2df7c241d1
Author: David Howells <dhowells@redhat.com>
Date:   Thu Sep 8 11:10:12 2016 +0100

    rxrpc: Rewrite the data and ack handling code
    
    Rewrite the data and ack handling code such that:
    
     (1) Parsing of received ACK and ABORT packets and the distribution and the
         filing of DATA packets happens entirely within the data_ready context
         called from the UDP socket.  This allows us to process and discard ACK
         and ABORT packets much more quickly (they're no longer stashed on a
         queue for a background thread to process).
    
     (2) We avoid calling skb_clone(), pskb_pull() and pskb_trim().  We instead
         keep track of the offset and length of the content of each packet in
         the sk_buff metadata.  This means we don't do any allocation in the
         receive path.
    
     (3) Jumbo DATA packet parsing is now done in data_ready context.  Rather
         than cloning the packet once for each subpacket and pulling/trimming
         it, we file the packet multiple times with an annotation for each
         indicating which subpacket is there.  From that we can directly
         calculate the offset and length.
    
     (4) A call's receive queue can be accessed without taking locks (memory
         barriers do have to be used, though).
    
     (5) Incoming calls are set up from preallocated resources and immediately
         made live.  They can than have packets queued upon them and ACKs
         generated.  If insufficient resources exist, DATA packet #1 is given a
         BUSY reply and other DATA packets are discarded).
    
     (6) sk_buffs no longer take a ref on their parent call.
    
    To make this work, the following changes are made:
    
     (1) Each call's receive buffer is now a circular buffer of sk_buff
         pointers (rxtx_buffer) rather than a number of sk_buff_heads spread
         between the call and the socket.  This permits each sk_buff to be in
         the buffer multiple times.  The receive buffer is reused for the
         transmit buffer.
    
     (2) A circular buffer of annotations (rxtx_annotations) is kept parallel
         to the data buffer.  Transmission phase annotations indicate whether a
         buffered packet has been ACK'd or not and whether it needs
         retransmission.
    
         Receive phase annotations indicate whether a slot holds a whole packet
         or a jumbo subpacket and, if the latter, which subpacket.  They also
         note whether the packet has been decrypted in place.
    
     (3) DATA packet window tracking is much simplified.  Each phase has just
         two numbers representing the window (rx_hard_ack/rx_top and
         tx_hard_ack/tx_top).
    
         The hard_ack number is the sequence number before base of the window,
         representing the last packet the other side says it has consumed.
         hard_ack starts from 0 and the first packet is sequence number 1.
    
         The top number is the sequence number of the highest-numbered packet
         residing in the buffer.  Packets between hard_ack+1 and top are
         soft-ACK'd to indicate they've been received, but not yet consumed.
    
         Four macros, before(), before_eq(), after() and after_eq() are added
         to compare sequence numbers within the window.  This allows for the
         top of the window to wrap when the hard-ack sequence number gets close
         to the limit.
    
         Two flags, RXRPC_CALL_RX_LAST and RXRPC_CALL_TX_LAST, are added also
         to indicate when rx_top and tx_top point at the packets with the
         LAST_PACKET bit set, indicating the end of the phase.
    
     (4) Calls are queued on the socket 'receive queue' rather than packets.
         This means that we don't need have to invent dummy packets to queue to
         indicate abnormal/terminal states and we don't have to keep metadata
         packets (such as ABORTs) around
    
     (5) The offset and length of a (sub)packet's content are now passed to
         the verify_packet security op.  This is currently expected to decrypt
         the packet in place and validate it.
    
         However, there's now nowhere to store the revised offset and length of
         the actual data within the decrypted blob (there may be a header and
         padding to skip) because an sk_buff may represent multiple packets, so
         a locate_data security op is added to retrieve these details from the
         sk_buff content when needed.
    
     (6) recvmsg() now has to handle jumbo subpackets, where each subpacket is
         individually secured and needs to be individually decrypted.  The code
         to do this is broken out into rxrpc_recvmsg_data() and shared with the
         kernel API.  It now iterates over the call's receive buffer rather
         than walking the socket receive queue.
    
    Additional changes:
    
     (1) The timers are condensed to a single timer that is set for the soonest
         of three timeouts (delayed ACK generation, DATA retransmission and
         call lifespan).
    
     (2) Transmission of ACK and ABORT packets is effected immediately from
         process-context socket ops/kernel API calls that cause them instead of
         them being punted off to a background work item.  The data_ready
         handler still has to defer to the background, though.
    
     (3) A shutdown op is added to the AF_RXRPC socket so that the AFS
         filesystem can shut down the socket and flush its own work items
         before closing the socket to deal with any in-progress service calls.
    
    Future additional changes that will need to be considered:
    
     (1) Make sure that a call doesn't hog the front of the queue by receiving
         data from the network as fast as userspace is consuming it to the
         exclusion of other calls.
    
     (2) Transmit delayed ACKs from within recvmsg() when we've consumed
         sufficiently more packets to avoid the background work item needing to
         run.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index aebc73ac16dc..2efe29a4c232 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -198,6 +198,32 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 	return peer;
 }
 
+/*
+ * Initialise peer record.
+ */
+static void rxrpc_init_peer(struct rxrpc_peer *peer, unsigned long hash_key)
+{
+	rxrpc_assess_MTU_size(peer);
+	peer->mtu = peer->if_mtu;
+
+	if (peer->srx.transport.family == AF_INET) {
+		peer->hdrsize = sizeof(struct iphdr);
+		switch (peer->srx.transport_type) {
+		case SOCK_DGRAM:
+			peer->hdrsize += sizeof(struct udphdr);
+			break;
+		default:
+			BUG();
+			break;
+		}
+	} else {
+		BUG();
+	}
+
+	peer->hdrsize += sizeof(struct rxrpc_wire_header);
+	peer->maxdata = peer->mtu - peer->hdrsize;
+}
+
 /*
  * Set up a new peer.
  */
@@ -214,29 +240,39 @@ static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local,
 	if (peer) {
 		peer->hash_key = hash_key;
 		memcpy(&peer->srx, srx, sizeof(*srx));
+		rxrpc_init_peer(peer, hash_key);
+	}
 
-		rxrpc_assess_MTU_size(peer);
-		peer->mtu = peer->if_mtu;
-
-		if (srx->transport.family == AF_INET) {
-			peer->hdrsize = sizeof(struct iphdr);
-			switch (srx->transport_type) {
-			case SOCK_DGRAM:
-				peer->hdrsize += sizeof(struct udphdr);
-				break;
-			default:
-				BUG();
-				break;
-			}
-		} else {
-			BUG();
-		}
+	_leave(" = %p", peer);
+	return peer;
+}
 
-		peer->hdrsize += sizeof(struct rxrpc_wire_header);
-		peer->maxdata = peer->mtu - peer->hdrsize;
+/*
+ * Set up a new incoming peer.  The address is prestored in the preallocated
+ * peer.
+ */
+struct rxrpc_peer *rxrpc_lookup_incoming_peer(struct rxrpc_local *local,
+					      struct rxrpc_peer *prealloc)
+{
+	struct rxrpc_peer *peer;
+	unsigned long hash_key;
+
+	hash_key = rxrpc_peer_hash_key(local, &prealloc->srx);
+	prealloc->local = local;
+	rxrpc_init_peer(prealloc, hash_key);
+
+	spin_lock(&rxrpc_peer_hash_lock);
+
+	/* Need to check that we aren't racing with someone else */
+	peer = __rxrpc_lookup_peer_rcu(local, &prealloc->srx, hash_key);
+	if (peer && !rxrpc_get_peer_maybe(peer))
+		peer = NULL;
+	if (!peer) {
+		peer = prealloc;
+		hash_add_rcu(rxrpc_peer_hash, &peer->hash_link, hash_key);
 	}
 
-	_leave(" = %p", peer);
+	spin_unlock(&rxrpc_peer_hash_lock);
 	return peer;
 }
 
@@ -272,7 +308,7 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 			return NULL;
 		}
 
-		spin_lock(&rxrpc_peer_hash_lock);
+		spin_lock_bh(&rxrpc_peer_hash_lock);
 
 		/* Need to check that we aren't racing with someone else */
 		peer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);
@@ -282,7 +318,7 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
 			hash_add_rcu(rxrpc_peer_hash,
 				     &candidate->hash_link, hash_key);
 
-		spin_unlock(&rxrpc_peer_hash_lock);
+		spin_unlock_bh(&rxrpc_peer_hash_lock);
 
 		if (peer)
 			kfree(candidate);
@@ -307,9 +343,9 @@ void __rxrpc_put_peer(struct rxrpc_peer *peer)
 {
 	ASSERT(hlist_empty(&peer->error_targets));
 
-	spin_lock(&rxrpc_peer_hash_lock);
+	spin_lock_bh(&rxrpc_peer_hash_lock);
 	hash_del_rcu(&peer->hash_link);
-	spin_unlock(&rxrpc_peer_hash_lock);
+	spin_unlock_bh(&rxrpc_peer_hash_lock);
 
 	kfree_rcu(peer, rcu);
 }

commit 8324f0bcfbfc645cf248e4b93ab58341b7d3b135
Author: David Howells <dhowells@redhat.com>
Date:   Tue Aug 30 09:49:29 2016 +0100

    rxrpc: Provide a way for AFS to ask for the peer address of a call
    
    Provide a function so that kernel users, such as AFS, can ask for the peer
    address of a call:
    
       void rxrpc_kernel_get_peer(struct rxrpc_call *call,
                                  struct sockaddr_rxrpc *_srx);
    
    In the future the kernel service won't get sk_buffs to look inside.
    Further, this allows us to hide any canonicalisation inside AF_RXRPC for
    when IPv6 support is added.
    
    Also propagate this through to afs_find_server() and issue a warning if we
    can't handle the address family yet.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 538e9831c699..aebc73ac16dc 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -313,3 +313,18 @@ void __rxrpc_put_peer(struct rxrpc_peer *peer)
 
 	kfree_rcu(peer, rcu);
 }
+
+/**
+ * rxrpc_kernel_get_peer - Get the peer address of a call
+ * @sock: The socket on which the call is in progress.
+ * @call: The call to query
+ * @_srx: Where to place the result
+ *
+ * Get the address of the remote peer in a call.
+ */
+void rxrpc_kernel_get_peer(struct socket *sock, struct rxrpc_call *call,
+			   struct sockaddr_rxrpc *_srx)
+{
+	*_srx = call->peer->srx;
+}
+EXPORT_SYMBOL(rxrpc_kernel_get_peer);

commit 8496af50eb385c1cadff9ad396fd5359e96b6c27
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jul 1 07:51:50 2016 +0100

    rxrpc: Use RCU to access a peer's service connection tree
    
    Move to using RCU access to a peer's service connection tree when routing
    an incoming packet.  This is done using a seqlock to trigger retrying of
    the tree walk if a change happened.
    
    Further, we no longer get a ref on the connection looked up in the
    data_ready handler unless we queue the connection's work item - and then
    only if the refcount > 0.
    
    
    Note that I'm avoiding the use of a hash table for service connections
    because each service connection is addressed by a 62-bit number
    (constructed from epoch and connection ID >> 2) that would allow the client
    to engage in bucket stuffing, given knowledge of the hash algorithm.
    Peers, however, are hashed as the network address is less controllable by
    the client.  The total number of peers will also be limited in a future
    commit.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 01d4930a11f7..538e9831c699 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -189,7 +189,7 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 		INIT_WORK(&peer->error_distributor,
 			  &rxrpc_peer_error_distributor);
 		peer->service_conns = RB_ROOT;
-		rwlock_init(&peer->conn_lock);
+		seqlock_init(&peer->service_conn_lock);
 		spin_lock_init(&peer->lock);
 		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
 	}

commit aa390bbe2113dd0de99cf35c39d7701d4412b744
Author: David Howells <dhowells@redhat.com>
Date:   Fri Jun 17 10:06:56 2016 +0100

    rxrpc: Kill off the rxrpc_transport struct
    
    The rxrpc_transport struct is now redundant, given that the rxrpc_peer
    struct is now per peer port rather than per peer host, so get rid of it.
    
    Service connection lists are transferred to the rxrpc_peer struct, as is
    the conn_lock.  Previous patches moved the client connection handling out
    of the rxrpc_transport struct and discarded the connection bundling code.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 6baad708f3b1..01d4930a11f7 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -188,6 +188,8 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 		INIT_HLIST_HEAD(&peer->error_targets);
 		INIT_WORK(&peer->error_distributor,
 			  &rxrpc_peer_error_distributor);
+		peer->service_conns = RB_ROOT;
+		rwlock_init(&peer->conn_lock);
 		spin_lock_init(&peer->lock);
 		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
 	}

commit 2f9f9f5210887b1019fbd0327ffdf7c3aff271fd
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jun 17 11:55:22 2016 +0200

    rxrpc: fix uninitialized variable use
    
    Hashing the peer key was introduced for AF_INET, but gcc
    warns about the rxrpc_peer_hash_key function returning uninitialized
    data for any other value of srx->transport.family:
    
    net/rxrpc/peer_object.c: In function 'rxrpc_peer_hash_key':
    net/rxrpc/peer_object.c:57:15: error: 'p' may be used uninitialized in this function [-Werror=maybe-uninitialized]
    
    Assuming that nothing else can be set here, this changes the
    function to just return zero in case of an unknown address
    family.
    
    Fixes: be6e6707f6ee ("rxrpc: Rework peer object handling to use hash table and RCU")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index faf222c21698..6baad708f3b1 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -50,6 +50,9 @@ static unsigned long rxrpc_peer_hash_key(struct rxrpc_local *local,
 		size = sizeof(srx->transport.sin.sin_addr);
 		p = (u16 *)&srx->transport.sin.sin_addr;
 		break;
+	default:
+		WARN(1, "AF_RXRPC: Unsupported transport address family\n");
+		return 0;
 	}
 
 	/* Step through the peer address in 16-bit portions for speed */

commit f66d7490196055cb9fb058f8936d19111a6231b9
Author: David Howells <dhowells@redhat.com>
Date:   Mon Apr 4 14:00:34 2016 +0100

    rxrpc: Use the peer record to distribute network errors
    
    Use the peer record to distribute network errors rather than the transport
    object (which I want to get rid of).  An error from a particular peer
    terminates all calls on that peer.
    
    For future consideration:
    
     (1) For ICMP-induced errors it might be worth trying to extract the RxRPC
         header from the offending packet, if one is returned attached to the
         ICMP packet, to better direct the error.
    
         This may be overkill, though, since an ICMP packet would be expected
         to be relating to the destination port, machine or network.  RxRPC
         ABORT and BUSY packets give notice at RxRPC level.
    
     (2) To also abort connection-level communications (such as CHALLENGE
         packets) where indicted by an error - but that requires some revamping
         of the connection event handling first.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 7fc50dc7d333..faf222c21698 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -182,7 +182,9 @@ struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 	if (peer) {
 		atomic_set(&peer->usage, 1);
 		peer->local = local;
-		INIT_LIST_HEAD(&peer->error_targets);
+		INIT_HLIST_HEAD(&peer->error_targets);
+		INIT_WORK(&peer->error_distributor,
+			  &rxrpc_peer_error_distributor);
 		spin_lock_init(&peer->lock);
 		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
 	}
@@ -298,7 +300,7 @@ struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
  */
 void __rxrpc_put_peer(struct rxrpc_peer *peer)
 {
-	ASSERT(list_empty(&peer->error_targets));
+	ASSERT(hlist_empty(&peer->error_targets));
 
 	spin_lock(&rxrpc_peer_hash_lock);
 	hash_del_rcu(&peer->hash_link);

commit be6e6707f6eec2048d9be608bc0ceecde5bd4cef
Author: David Howells <dhowells@redhat.com>
Date:   Mon Apr 4 14:00:32 2016 +0100

    rxrpc: Rework peer object handling to use hash table and RCU
    
    Rework peer object handling to use a hash table instead of a flat list and
    to use RCU.  Peer objects are no longer destroyed by passing them to a
    workqueue to process, but rather are just passed to the RCU garbage
    collector as kfree'able objects.
    
    The hash function uses the local endpoint plus all the components of the
    remote address, except for the RxRPC service ID.  Peers thus represent a
    UDP port on the remote machine as contacted by a UDP port on this machine.
    
    The RCU read lock is used to handle non-creating lookups so that they can
    be called from bottom half context in the sk_error_report handler without
    having to lock the hash table against modification.
    rxrpc_lookup_peer_rcu() *does* take a reference on the peer object as in
    the future, this will be passed to a work item for error distribution in
    the error_report path and this function will cease being used in the
    data_ready path.
    
    Creating lookups are done under spinlock rather than mutex as they might be
    set up due to an external stimulus if the local endpoint is a server.
    
    Captured network error messages (ICMP) are handled with respect to this
    struct and MTU size and RTT are cached here.
    
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
index 0b54cda3d8e5..7fc50dc7d333 100644
--- a/net/rxrpc/peer_object.c
+++ b/net/rxrpc/peer_object.c
@@ -1,6 +1,6 @@
-/* RxRPC remote transport endpoint management
+/* RxRPC remote transport endpoint record management
  *
- * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
+ * Copyright (C) 2007, 2016 Red Hat, Inc. All Rights Reserved.
  * Written by David Howells (dhowells@redhat.com)
  *
  * This program is free software; you can redistribute it and/or
@@ -16,20 +16,132 @@
 #include <linux/skbuff.h>
 #include <linux/udp.h>
 #include <linux/in.h>
-#include <linux/in6.h>
-#include <linux/icmp.h>
 #include <linux/slab.h>
+#include <linux/hashtable.h>
 #include <net/sock.h>
 #include <net/af_rxrpc.h>
 #include <net/ip.h>
 #include <net/route.h>
 #include "ar-internal.h"
 
-static LIST_HEAD(rxrpc_peers);
-static DEFINE_RWLOCK(rxrpc_peer_lock);
-static DECLARE_WAIT_QUEUE_HEAD(rxrpc_peer_wq);
+static DEFINE_HASHTABLE(rxrpc_peer_hash, 10);
+static DEFINE_SPINLOCK(rxrpc_peer_hash_lock);
 
-static void rxrpc_destroy_peer(struct work_struct *work);
+/*
+ * Hash a peer key.
+ */
+static unsigned long rxrpc_peer_hash_key(struct rxrpc_local *local,
+					 const struct sockaddr_rxrpc *srx)
+{
+	const u16 *p;
+	unsigned int i, size;
+	unsigned long hash_key;
+
+	_enter("");
+
+	hash_key = (unsigned long)local / __alignof__(*local);
+	hash_key += srx->transport_type;
+	hash_key += srx->transport_len;
+	hash_key += srx->transport.family;
+
+	switch (srx->transport.family) {
+	case AF_INET:
+		hash_key += (u16 __force)srx->transport.sin.sin_port;
+		size = sizeof(srx->transport.sin.sin_addr);
+		p = (u16 *)&srx->transport.sin.sin_addr;
+		break;
+	}
+
+	/* Step through the peer address in 16-bit portions for speed */
+	for (i = 0; i < size; i += sizeof(*p), p++)
+		hash_key += *p;
+
+	_leave(" 0x%lx", hash_key);
+	return hash_key;
+}
+
+/*
+ * Compare a peer to a key.  Return -ve, 0 or +ve to indicate less than, same
+ * or greater than.
+ *
+ * Unfortunately, the primitives in linux/hashtable.h don't allow for sorted
+ * buckets and mid-bucket insertion, so we don't make full use of this
+ * information at this point.
+ */
+static long rxrpc_peer_cmp_key(const struct rxrpc_peer *peer,
+			       struct rxrpc_local *local,
+			       const struct sockaddr_rxrpc *srx,
+			       unsigned long hash_key)
+{
+	long diff;
+
+	diff = ((peer->hash_key - hash_key) ?:
+		((unsigned long)peer->local - (unsigned long)local) ?:
+		(peer->srx.transport_type - srx->transport_type) ?:
+		(peer->srx.transport_len - srx->transport_len) ?:
+		(peer->srx.transport.family - srx->transport.family));
+	if (diff != 0)
+		return diff;
+
+	switch (srx->transport.family) {
+	case AF_INET:
+		return ((u16 __force)peer->srx.transport.sin.sin_port -
+			(u16 __force)srx->transport.sin.sin_port) ?:
+			memcmp(&peer->srx.transport.sin.sin_addr,
+			       &srx->transport.sin.sin_addr,
+			       sizeof(struct in_addr));
+	default:
+		BUG();
+	}
+}
+
+/*
+ * Look up a remote transport endpoint for the specified address using RCU.
+ */
+static struct rxrpc_peer *__rxrpc_lookup_peer_rcu(
+	struct rxrpc_local *local,
+	const struct sockaddr_rxrpc *srx,
+	unsigned long hash_key)
+{
+	struct rxrpc_peer *peer;
+
+	hash_for_each_possible_rcu(rxrpc_peer_hash, peer, hash_link, hash_key) {
+		if (rxrpc_peer_cmp_key(peer, local, srx, hash_key) == 0) {
+			if (atomic_read(&peer->usage) == 0)
+				return NULL;
+			return peer;
+		}
+	}
+
+	return NULL;
+}
+
+/*
+ * Look up a remote transport endpoint for the specified address using RCU.
+ */
+struct rxrpc_peer *rxrpc_lookup_peer_rcu(struct rxrpc_local *local,
+					 const struct sockaddr_rxrpc *srx)
+{
+	struct rxrpc_peer *peer;
+	unsigned long hash_key = rxrpc_peer_hash_key(local, srx);
+
+	peer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);
+	if (peer) {
+		switch (srx->transport.family) {
+		case AF_INET:
+			_net("PEER %d {%d,%u,%pI4+%hu}",
+			     peer->debug_id,
+			     peer->srx.transport_type,
+			     peer->srx.transport.family,
+			     &peer->srx.transport.sin.sin_addr,
+			     ntohs(peer->srx.transport.sin.sin_port));
+			break;
+		}
+
+		_leave(" = %p {u=%d}", peer, atomic_read(&peer->usage));
+	}
+	return peer;
+}
 
 /*
  * assess the MTU size for the network interface through which this peer is
@@ -58,10 +170,9 @@ static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
 }
 
 /*
- * allocate a new peer
+ * Allocate a peer.
  */
-static struct rxrpc_peer *rxrpc_alloc_peer(struct sockaddr_rxrpc *srx,
-					   gfp_t gfp)
+struct rxrpc_peer *rxrpc_alloc_peer(struct rxrpc_local *local, gfp_t gfp)
 {
 	struct rxrpc_peer *peer;
 
@@ -69,12 +180,32 @@ static struct rxrpc_peer *rxrpc_alloc_peer(struct sockaddr_rxrpc *srx,
 
 	peer = kzalloc(sizeof(struct rxrpc_peer), gfp);
 	if (peer) {
-		INIT_WORK(&peer->destroyer, &rxrpc_destroy_peer);
-		INIT_LIST_HEAD(&peer->link);
+		atomic_set(&peer->usage, 1);
+		peer->local = local;
 		INIT_LIST_HEAD(&peer->error_targets);
 		spin_lock_init(&peer->lock);
-		atomic_set(&peer->usage, 1);
 		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
+	}
+
+	_leave(" = %p", peer);
+	return peer;
+}
+
+/*
+ * Set up a new peer.
+ */
+static struct rxrpc_peer *rxrpc_create_peer(struct rxrpc_local *local,
+					    struct sockaddr_rxrpc *srx,
+					    unsigned long hash_key,
+					    gfp_t gfp)
+{
+	struct rxrpc_peer *peer;
+
+	_enter("");
+
+	peer = rxrpc_alloc_peer(local, gfp);
+	if (peer) {
+		peer->hash_key = hash_key;
 		memcpy(&peer->srx, srx, sizeof(*srx));
 
 		rxrpc_assess_MTU_size(peer);
@@ -105,11 +236,11 @@ static struct rxrpc_peer *rxrpc_alloc_peer(struct sockaddr_rxrpc *srx,
 /*
  * obtain a remote transport endpoint for the specified address
  */
-struct rxrpc_peer *rxrpc_get_peer(struct sockaddr_rxrpc *srx, gfp_t gfp)
+struct rxrpc_peer *rxrpc_lookup_peer(struct rxrpc_local *local,
+				     struct sockaddr_rxrpc *srx, gfp_t gfp)
 {
 	struct rxrpc_peer *peer, *candidate;
-	const char *new = "old";
-	int usage;
+	unsigned long hash_key = rxrpc_peer_hash_key(local, srx);
 
 	_enter("{%d,%d,%pI4+%hu}",
 	       srx->transport_type,
@@ -118,188 +249,60 @@ struct rxrpc_peer *rxrpc_get_peer(struct sockaddr_rxrpc *srx, gfp_t gfp)
 	       ntohs(srx->transport.sin.sin_port));
 
 	/* search the peer list first */
-	read_lock_bh(&rxrpc_peer_lock);
-	list_for_each_entry(peer, &rxrpc_peers, link) {
-		_debug("check PEER %d { u=%d t=%d l=%d }",
-		       peer->debug_id,
-		       atomic_read(&peer->usage),
-		       peer->srx.transport_type,
-		       peer->srx.transport_len);
-
-		if (atomic_read(&peer->usage) > 0 &&
-		    peer->srx.transport_type == srx->transport_type &&
-		    peer->srx.transport_len == srx->transport_len &&
-		    memcmp(&peer->srx.transport,
-			   &srx->transport,
-			   srx->transport_len) == 0)
-			goto found_extant_peer;
-	}
-	read_unlock_bh(&rxrpc_peer_lock);
-
-	/* not yet present - create a candidate for a new record and then
-	 * redo the search */
-	candidate = rxrpc_alloc_peer(srx, gfp);
-	if (!candidate) {
-		_leave(" = -ENOMEM");
-		return ERR_PTR(-ENOMEM);
-	}
+	rcu_read_lock();
+	peer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);
+	if (peer && !rxrpc_get_peer_maybe(peer))
+		peer = NULL;
+	rcu_read_unlock();
+
+	if (!peer) {
+		/* The peer is not yet present in hash - create a candidate
+		 * for a new record and then redo the search.
+		 */
+		candidate = rxrpc_create_peer(local, srx, hash_key, gfp);
+		if (!candidate) {
+			_leave(" = NULL [nomem]");
+			return NULL;
+		}
 
-	write_lock_bh(&rxrpc_peer_lock);
+		spin_lock(&rxrpc_peer_hash_lock);
 
-	list_for_each_entry(peer, &rxrpc_peers, link) {
-		if (atomic_read(&peer->usage) > 0 &&
-		    peer->srx.transport_type == srx->transport_type &&
-		    peer->srx.transport_len == srx->transport_len &&
-		    memcmp(&peer->srx.transport,
-			   &srx->transport,
-			   srx->transport_len) == 0)
-			goto found_extant_second;
-	}
+		/* Need to check that we aren't racing with someone else */
+		peer = __rxrpc_lookup_peer_rcu(local, srx, hash_key);
+		if (peer && !rxrpc_get_peer_maybe(peer))
+			peer = NULL;
+		if (!peer)
+			hash_add_rcu(rxrpc_peer_hash,
+				     &candidate->hash_link, hash_key);
 
-	/* we can now add the new candidate to the list */
-	peer = candidate;
-	candidate = NULL;
-	usage = atomic_read(&peer->usage);
+		spin_unlock(&rxrpc_peer_hash_lock);
 
-	list_add_tail(&peer->link, &rxrpc_peers);
-	write_unlock_bh(&rxrpc_peer_lock);
-	new = "new";
+		if (peer)
+			kfree(candidate);
+		else
+			peer = candidate;
+	}
 
-success:
-	_net("PEER %s %d {%d,%u,%pI4+%hu}",
-	     new,
+	_net("PEER %d {%d,%pI4+%hu}",
 	     peer->debug_id,
 	     peer->srx.transport_type,
-	     peer->srx.transport.family,
 	     &peer->srx.transport.sin.sin_addr,
 	     ntohs(peer->srx.transport.sin.sin_port));
 
-	_leave(" = %p {u=%d}", peer, usage);
-	return peer;
-
-	/* we found the peer in the list immediately */
-found_extant_peer:
-	usage = atomic_inc_return(&peer->usage);
-	read_unlock_bh(&rxrpc_peer_lock);
-	goto success;
-
-	/* we found the peer on the second time through the list */
-found_extant_second:
-	usage = atomic_inc_return(&peer->usage);
-	write_unlock_bh(&rxrpc_peer_lock);
-	kfree(candidate);
-	goto success;
-}
-
-/*
- * find the peer associated with a packet
- */
-struct rxrpc_peer *rxrpc_find_peer(struct rxrpc_local *local,
-				   __be32 addr, __be16 port)
-{
-	struct rxrpc_peer *peer;
-
-	_enter("");
-
-	/* search the peer list */
-	read_lock_bh(&rxrpc_peer_lock);
-
-	if (local->srx.transport.family == AF_INET &&
-	    local->srx.transport_type == SOCK_DGRAM
-	    ) {
-		list_for_each_entry(peer, &rxrpc_peers, link) {
-			if (atomic_read(&peer->usage) > 0 &&
-			    peer->srx.transport_type == SOCK_DGRAM &&
-			    peer->srx.transport.family == AF_INET &&
-			    peer->srx.transport.sin.sin_port == port &&
-			    peer->srx.transport.sin.sin_addr.s_addr == addr)
-				goto found_UDP_peer;
-		}
-
-		goto new_UDP_peer;
-	}
-
-	read_unlock_bh(&rxrpc_peer_lock);
-	_leave(" = -EAFNOSUPPORT");
-	return ERR_PTR(-EAFNOSUPPORT);
-
-found_UDP_peer:
-	_net("Rx UDP DGRAM from peer %d", peer->debug_id);
-	atomic_inc(&peer->usage);
-	read_unlock_bh(&rxrpc_peer_lock);
-	_leave(" = %p", peer);
+	_leave(" = %p {u=%d}", peer, atomic_read(&peer->usage));
 	return peer;
-
-new_UDP_peer:
-	_net("Rx UDP DGRAM from NEW peer");
-	read_unlock_bh(&rxrpc_peer_lock);
-	_leave(" = -EBUSY [new]");
-	return ERR_PTR(-EBUSY);
 }
 
 /*
- * release a remote transport endpoint
+ * Discard a ref on a remote peer record.
  */
-void rxrpc_put_peer(struct rxrpc_peer *peer)
+void __rxrpc_put_peer(struct rxrpc_peer *peer)
 {
-	_enter("%p{u=%d}", peer, atomic_read(&peer->usage));
+	ASSERT(list_empty(&peer->error_targets));
 
-	ASSERTCMP(atomic_read(&peer->usage), >, 0);
-
-	if (likely(!atomic_dec_and_test(&peer->usage))) {
-		_leave(" [in use]");
-		return;
-	}
-
-	rxrpc_queue_work(&peer->destroyer);
-	_leave("");
-}
-
-/*
- * destroy a remote transport endpoint
- */
-static void rxrpc_destroy_peer(struct work_struct *work)
-{
-	struct rxrpc_peer *peer =
-		container_of(work, struct rxrpc_peer, destroyer);
-
-	_enter("%p{%d}", peer, atomic_read(&peer->usage));
-
-	write_lock_bh(&rxrpc_peer_lock);
-	list_del(&peer->link);
-	write_unlock_bh(&rxrpc_peer_lock);
-
-	_net("DESTROY PEER %d", peer->debug_id);
-	kfree(peer);
-
-	if (list_empty(&rxrpc_peers))
-		wake_up_all(&rxrpc_peer_wq);
-	_leave("");
-}
-
-/*
- * preemptively destroy all the peer records from a transport endpoint rather
- * than waiting for them to time out
- */
-void __exit rxrpc_destroy_all_peers(void)
-{
-	DECLARE_WAITQUEUE(myself,current);
-
-	_enter("");
-
-	/* we simply have to wait for them to go away */
-	if (!list_empty(&rxrpc_peers)) {
-		set_current_state(TASK_UNINTERRUPTIBLE);
-		add_wait_queue(&rxrpc_peer_wq, &myself);
-
-		while (!list_empty(&rxrpc_peers)) {
-			schedule();
-			set_current_state(TASK_UNINTERRUPTIBLE);
-		}
-
-		remove_wait_queue(&rxrpc_peer_wq, &myself);
-		set_current_state(TASK_RUNNING);
-	}
+	spin_lock(&rxrpc_peer_hash_lock);
+	hash_del_rcu(&peer->hash_link);
+	spin_unlock(&rxrpc_peer_hash_lock);
 
-	_leave("");
+	kfree_rcu(peer, rcu);
 }

commit 8c3e34a4ff85142ca5dba3f18cbc2061899e2612
Author: David Howells <dhowells@redhat.com>
Date:   Mon Jun 13 12:16:05 2016 +0100

    rxrpc: Rename files matching ar-*.c to git rid of the "ar-" prefix
    
    Rename files matching net/rxrpc/ar-*.c to get rid of the "ar-" prefix.
    This will aid splitting those files by making easier to come up with new
    names.
    
    Note that the not all files are simply renamed from ar-X.c to X.c.  The
    following exceptions are made:
    
     (*) ar-call.c -> call_object.c
         ar-ack.c -> call_event.c
    
         call_object.c is going to contain the core of the call object
         handling.  Call event handling is all going to be in call_event.c.
    
     (*) ar-accept.c -> call_accept.c
    
         Incoming call handling is going to be here.
    
     (*) ar-connection.c -> conn_object.c
         ar-connevent.c -> conn_event.c
    
         The former file is going to have the basic connection object handling,
         but there will likely be some differentiation between client
         connections and service connections in additional files later.  The
         latter file will have all the connection-level event handling.
    
     (*) ar-local.c -> local_object.c
    
         This will have the local endpoint object handling code.  The local
         endpoint event handling code will later be split out into
         local_event.c.
    
     (*) ar-peer.c -> peer_object.c
    
         This will have the peer endpoint object handling code.  Peer event
         handling code will be placed in peer_event.c (for the moment, there is
         none).
    
     (*) ar-error.c -> peer_event.c
    
         This will become the peer event handling code, though for the moment
         it's actually driven from the local endpoint's perspective.
    
    Note that I haven't renamed ar-transport.c to transport_object.c as the
    intention is to delete it when the rxrpc_transport struct is excised.
    
    The only file that actually has its contents changed is net/rxrpc/Makefile.
    
    net/rxrpc/ar-internal.h will need its section marker comments updating, but
    I'll do that in a separate patch to make it easier for git to follow the
    history across the rename.  I may also want to rename ar-internal.h at some
    point - but that would mean updating all the #includes and I'd rather do
    that in a separate step.
    
    Signed-off-by: David Howells <dhowells@redhat.com.

diff --git a/net/rxrpc/peer_object.c b/net/rxrpc/peer_object.c
new file mode 100644
index 000000000000..0b54cda3d8e5
--- /dev/null
+++ b/net/rxrpc/peer_object.c
@@ -0,0 +1,305 @@
+/* RxRPC remote transport endpoint management
+ *
+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.
+ * Written by David Howells (dhowells@redhat.com)
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
+#include <linux/module.h>
+#include <linux/net.h>
+#include <linux/skbuff.h>
+#include <linux/udp.h>
+#include <linux/in.h>
+#include <linux/in6.h>
+#include <linux/icmp.h>
+#include <linux/slab.h>
+#include <net/sock.h>
+#include <net/af_rxrpc.h>
+#include <net/ip.h>
+#include <net/route.h>
+#include "ar-internal.h"
+
+static LIST_HEAD(rxrpc_peers);
+static DEFINE_RWLOCK(rxrpc_peer_lock);
+static DECLARE_WAIT_QUEUE_HEAD(rxrpc_peer_wq);
+
+static void rxrpc_destroy_peer(struct work_struct *work);
+
+/*
+ * assess the MTU size for the network interface through which this peer is
+ * reached
+ */
+static void rxrpc_assess_MTU_size(struct rxrpc_peer *peer)
+{
+	struct rtable *rt;
+	struct flowi4 fl4;
+
+	peer->if_mtu = 1500;
+
+	rt = ip_route_output_ports(&init_net, &fl4, NULL,
+				   peer->srx.transport.sin.sin_addr.s_addr, 0,
+				   htons(7000), htons(7001),
+				   IPPROTO_UDP, 0, 0);
+	if (IS_ERR(rt)) {
+		_leave(" [route err %ld]", PTR_ERR(rt));
+		return;
+	}
+
+	peer->if_mtu = dst_mtu(&rt->dst);
+	dst_release(&rt->dst);
+
+	_leave(" [if_mtu %u]", peer->if_mtu);
+}
+
+/*
+ * allocate a new peer
+ */
+static struct rxrpc_peer *rxrpc_alloc_peer(struct sockaddr_rxrpc *srx,
+					   gfp_t gfp)
+{
+	struct rxrpc_peer *peer;
+
+	_enter("");
+
+	peer = kzalloc(sizeof(struct rxrpc_peer), gfp);
+	if (peer) {
+		INIT_WORK(&peer->destroyer, &rxrpc_destroy_peer);
+		INIT_LIST_HEAD(&peer->link);
+		INIT_LIST_HEAD(&peer->error_targets);
+		spin_lock_init(&peer->lock);
+		atomic_set(&peer->usage, 1);
+		peer->debug_id = atomic_inc_return(&rxrpc_debug_id);
+		memcpy(&peer->srx, srx, sizeof(*srx));
+
+		rxrpc_assess_MTU_size(peer);
+		peer->mtu = peer->if_mtu;
+
+		if (srx->transport.family == AF_INET) {
+			peer->hdrsize = sizeof(struct iphdr);
+			switch (srx->transport_type) {
+			case SOCK_DGRAM:
+				peer->hdrsize += sizeof(struct udphdr);
+				break;
+			default:
+				BUG();
+				break;
+			}
+		} else {
+			BUG();
+		}
+
+		peer->hdrsize += sizeof(struct rxrpc_wire_header);
+		peer->maxdata = peer->mtu - peer->hdrsize;
+	}
+
+	_leave(" = %p", peer);
+	return peer;
+}
+
+/*
+ * obtain a remote transport endpoint for the specified address
+ */
+struct rxrpc_peer *rxrpc_get_peer(struct sockaddr_rxrpc *srx, gfp_t gfp)
+{
+	struct rxrpc_peer *peer, *candidate;
+	const char *new = "old";
+	int usage;
+
+	_enter("{%d,%d,%pI4+%hu}",
+	       srx->transport_type,
+	       srx->transport_len,
+	       &srx->transport.sin.sin_addr,
+	       ntohs(srx->transport.sin.sin_port));
+
+	/* search the peer list first */
+	read_lock_bh(&rxrpc_peer_lock);
+	list_for_each_entry(peer, &rxrpc_peers, link) {
+		_debug("check PEER %d { u=%d t=%d l=%d }",
+		       peer->debug_id,
+		       atomic_read(&peer->usage),
+		       peer->srx.transport_type,
+		       peer->srx.transport_len);
+
+		if (atomic_read(&peer->usage) > 0 &&
+		    peer->srx.transport_type == srx->transport_type &&
+		    peer->srx.transport_len == srx->transport_len &&
+		    memcmp(&peer->srx.transport,
+			   &srx->transport,
+			   srx->transport_len) == 0)
+			goto found_extant_peer;
+	}
+	read_unlock_bh(&rxrpc_peer_lock);
+
+	/* not yet present - create a candidate for a new record and then
+	 * redo the search */
+	candidate = rxrpc_alloc_peer(srx, gfp);
+	if (!candidate) {
+		_leave(" = -ENOMEM");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	write_lock_bh(&rxrpc_peer_lock);
+
+	list_for_each_entry(peer, &rxrpc_peers, link) {
+		if (atomic_read(&peer->usage) > 0 &&
+		    peer->srx.transport_type == srx->transport_type &&
+		    peer->srx.transport_len == srx->transport_len &&
+		    memcmp(&peer->srx.transport,
+			   &srx->transport,
+			   srx->transport_len) == 0)
+			goto found_extant_second;
+	}
+
+	/* we can now add the new candidate to the list */
+	peer = candidate;
+	candidate = NULL;
+	usage = atomic_read(&peer->usage);
+
+	list_add_tail(&peer->link, &rxrpc_peers);
+	write_unlock_bh(&rxrpc_peer_lock);
+	new = "new";
+
+success:
+	_net("PEER %s %d {%d,%u,%pI4+%hu}",
+	     new,
+	     peer->debug_id,
+	     peer->srx.transport_type,
+	     peer->srx.transport.family,
+	     &peer->srx.transport.sin.sin_addr,
+	     ntohs(peer->srx.transport.sin.sin_port));
+
+	_leave(" = %p {u=%d}", peer, usage);
+	return peer;
+
+	/* we found the peer in the list immediately */
+found_extant_peer:
+	usage = atomic_inc_return(&peer->usage);
+	read_unlock_bh(&rxrpc_peer_lock);
+	goto success;
+
+	/* we found the peer on the second time through the list */
+found_extant_second:
+	usage = atomic_inc_return(&peer->usage);
+	write_unlock_bh(&rxrpc_peer_lock);
+	kfree(candidate);
+	goto success;
+}
+
+/*
+ * find the peer associated with a packet
+ */
+struct rxrpc_peer *rxrpc_find_peer(struct rxrpc_local *local,
+				   __be32 addr, __be16 port)
+{
+	struct rxrpc_peer *peer;
+
+	_enter("");
+
+	/* search the peer list */
+	read_lock_bh(&rxrpc_peer_lock);
+
+	if (local->srx.transport.family == AF_INET &&
+	    local->srx.transport_type == SOCK_DGRAM
+	    ) {
+		list_for_each_entry(peer, &rxrpc_peers, link) {
+			if (atomic_read(&peer->usage) > 0 &&
+			    peer->srx.transport_type == SOCK_DGRAM &&
+			    peer->srx.transport.family == AF_INET &&
+			    peer->srx.transport.sin.sin_port == port &&
+			    peer->srx.transport.sin.sin_addr.s_addr == addr)
+				goto found_UDP_peer;
+		}
+
+		goto new_UDP_peer;
+	}
+
+	read_unlock_bh(&rxrpc_peer_lock);
+	_leave(" = -EAFNOSUPPORT");
+	return ERR_PTR(-EAFNOSUPPORT);
+
+found_UDP_peer:
+	_net("Rx UDP DGRAM from peer %d", peer->debug_id);
+	atomic_inc(&peer->usage);
+	read_unlock_bh(&rxrpc_peer_lock);
+	_leave(" = %p", peer);
+	return peer;
+
+new_UDP_peer:
+	_net("Rx UDP DGRAM from NEW peer");
+	read_unlock_bh(&rxrpc_peer_lock);
+	_leave(" = -EBUSY [new]");
+	return ERR_PTR(-EBUSY);
+}
+
+/*
+ * release a remote transport endpoint
+ */
+void rxrpc_put_peer(struct rxrpc_peer *peer)
+{
+	_enter("%p{u=%d}", peer, atomic_read(&peer->usage));
+
+	ASSERTCMP(atomic_read(&peer->usage), >, 0);
+
+	if (likely(!atomic_dec_and_test(&peer->usage))) {
+		_leave(" [in use]");
+		return;
+	}
+
+	rxrpc_queue_work(&peer->destroyer);
+	_leave("");
+}
+
+/*
+ * destroy a remote transport endpoint
+ */
+static void rxrpc_destroy_peer(struct work_struct *work)
+{
+	struct rxrpc_peer *peer =
+		container_of(work, struct rxrpc_peer, destroyer);
+
+	_enter("%p{%d}", peer, atomic_read(&peer->usage));
+
+	write_lock_bh(&rxrpc_peer_lock);
+	list_del(&peer->link);
+	write_unlock_bh(&rxrpc_peer_lock);
+
+	_net("DESTROY PEER %d", peer->debug_id);
+	kfree(peer);
+
+	if (list_empty(&rxrpc_peers))
+		wake_up_all(&rxrpc_peer_wq);
+	_leave("");
+}
+
+/*
+ * preemptively destroy all the peer records from a transport endpoint rather
+ * than waiting for them to time out
+ */
+void __exit rxrpc_destroy_all_peers(void)
+{
+	DECLARE_WAITQUEUE(myself,current);
+
+	_enter("");
+
+	/* we simply have to wait for them to go away */
+	if (!list_empty(&rxrpc_peers)) {
+		set_current_state(TASK_UNINTERRUPTIBLE);
+		add_wait_queue(&rxrpc_peer_wq, &myself);
+
+		while (!list_empty(&rxrpc_peers)) {
+			schedule();
+			set_current_state(TASK_UNINTERRUPTIBLE);
+		}
+
+		remove_wait_queue(&rxrpc_peer_wq, &myself);
+		set_current_state(TASK_RUNNING);
+	}
+
+	_leave("");
+}
