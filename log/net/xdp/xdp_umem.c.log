commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index a0d2b757807f..e97db37354e4 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -254,10 +254,10 @@ static int xdp_umem_pin_pages(struct xdp_umem *umem, unsigned long address)
 	if (!umem->pgs)
 		return -ENOMEM;
 
-	down_read(&current->mm->mmap_sem);
+	mmap_read_lock(current->mm);
 	npgs = pin_user_pages(address, umem->npgs,
 			      gup_flags | FOLL_LONGTERM, &umem->pgs[0], NULL);
-	up_read(&current->mm->mmap_sem);
+	mmap_read_unlock(current->mm);
 
 	if (npgs != umem->npgs) {
 		if (npgs >= 0) {

commit 7d877c35ca84cfa634fd63c2b64bf7b6ae9c71cb
Author: Pavel Machek <pavel@ucw.cz>
Date:   Thu Jun 4 23:42:59 2020 +0200

    net/xdp: use shift instead of 64 bit division
    
    64bit division is kind of expensive, and shift should do the job here.
    
    Signed-off-by: Pavel Machek (CIP) <pavel@denx.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 1bbaf1747e4f..a0d2b757807f 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -336,7 +336,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if ((addr + size) < addr)
 		return -EINVAL;
 
-	npgs = div_u64(size, PAGE_SIZE);
+	npgs = size >> PAGE_SHIFT;
 	if (npgs > U32_MAX)
 		return -EINVAL;
 

commit 1806c13dc2532090d742ce03847b22367fb20ad6
Merge: 1079a34c56c5 bdc48fa11e46
Author: David S. Miller <davem@davemloft.net>
Date:   Sun May 31 17:48:46 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    xdp_umem.c had overlapping changes between the 64-bit math fix
    for the calculation of npgs and the removal of the zerocopy
    memory type which got rid of the chunk_size_nohdr member.
    
    The mlx5 Kconfig conflict is a case where we just take the
    net-next copy of the Kconfig entry dependency as it takes on
    the ESWITCH dependency by one level of indirection which is
    what the 'net' conflicting change is trying to ensure.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b16a87d0aef7a6be766f6618976dc5ff2c689291
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon May 25 10:03:59 2020 +0200

    xsk: Add overflow check for u64 division, stored into u32
    
    The npgs member of struct xdp_umem is an u32 entity, and stores the
    number of pages the UMEM consumes. The calculation of npgs
    
      npgs = size / PAGE_SIZE
    
    can overflow.
    
    To avoid overflow scenarios, the division is now first stored in a
    u64, and the result is verified to fit into 32b.
    
    An alternative would be storing the npgs as a u64, however, this
    wastes memory and is an unrealisticly large packet area.
    
    Fixes: c0c77d8fb787 ("xsk: add user memory registration support sockopt")
    Reported-by: "Minh Bùi Quang" <minhquangbui99@gmail.com>
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Link: https://lore.kernel.org/bpf/CACtPs=GGvV-_Yj6rbpzTVnopgi5nhMoCcTkSkYrJHGQHJWFZMQ@mail.gmail.com/
    Link: https://lore.kernel.org/bpf/20200525080400.13195-1-bjorn.topel@gmail.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index ed7a6060f73c..3889bd9aec46 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -341,8 +341,8 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 {
 	bool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;
 	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
+	u64 npgs, addr = mr->addr, size = mr->len;
 	unsigned int chunks, chunks_per_page;
-	u64 addr = mr->addr, size = mr->len;
 	int err;
 
 	if (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {
@@ -372,6 +372,10 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if ((addr + size) < addr)
 		return -EINVAL;
 
+	npgs = div_u64(size, PAGE_SIZE);
+	if (npgs > U32_MAX)
+		return -EINVAL;
+
 	chunks = (unsigned int)div_u64(size, chunk_size);
 	if (chunks == 0)
 		return -EINVAL;
@@ -391,7 +395,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	umem->size = size;
 	umem->headroom = headroom;
 	umem->chunk_size_nohr = chunk_size - headroom;
-	umem->npgs = size / PAGE_SIZE;
+	umem->npgs = (u32)npgs;
 	umem->pgs = NULL;
 	umem->user = NULL;
 	umem->flags = mr->flags;

commit 0807892ecb35734b7ce6f7c29b078f1b60151c94
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 20 21:21:00 2020 +0200

    xsk: Remove MEM_TYPE_ZERO_COPY and corresponding code
    
    There are no users of MEM_TYPE_ZERO_COPY. Remove all corresponding
    code, including the "handle" member of struct xdp_buff.
    
    rfc->v1: Fixed spelling in commit message. (Björn)
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200520192103.355233-13-bjorn.topel@gmail.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 7f04688045d5..19e59d1a5e9f 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -179,37 +179,6 @@ void xdp_umem_clear_dev(struct xdp_umem *umem)
 	umem->zc = false;
 }
 
-static void xdp_umem_unmap_pages(struct xdp_umem *umem)
-{
-	unsigned int i;
-
-	for (i = 0; i < umem->npgs; i++)
-		if (PageHighMem(umem->pgs[i]))
-			vunmap(umem->pages[i].addr);
-}
-
-static int xdp_umem_map_pages(struct xdp_umem *umem)
-{
-	unsigned int i;
-	void *addr;
-
-	for (i = 0; i < umem->npgs; i++) {
-		if (PageHighMem(umem->pgs[i]))
-			addr = vmap(&umem->pgs[i], 1, VM_MAP, PAGE_KERNEL);
-		else
-			addr = page_address(umem->pgs[i]);
-
-		if (!addr) {
-			xdp_umem_unmap_pages(umem);
-			return -ENOMEM;
-		}
-
-		umem->pages[i].addr = addr;
-	}
-
-	return 0;
-}
-
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
 	unpin_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
@@ -244,14 +213,9 @@ static void xdp_umem_release(struct xdp_umem *umem)
 		umem->cq = NULL;
 	}
 
-	xsk_reuseq_destroy(umem);
 	xp_destroy(umem->pool);
-	xdp_umem_unmap_pages(umem);
 	xdp_umem_unpin_pages(umem);
 
-	kvfree(umem->pages);
-	umem->pages = NULL;
-
 	xdp_umem_unaccount_pages(umem);
 	kfree(umem);
 }
@@ -385,11 +349,8 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (headroom >= chunk_size - XDP_PACKET_HEADROOM)
 		return -EINVAL;
 
-	umem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK
-					    : ~((u64)chunk_size - 1);
 	umem->size = size;
 	umem->headroom = headroom;
-	umem->chunk_size_nohr = chunk_size - headroom;
 	umem->chunk_size = chunk_size;
 	umem->npgs = size / PAGE_SIZE;
 	umem->pgs = NULL;
@@ -408,29 +369,14 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (err)
 		goto out_account;
 
-	umem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),
-			       GFP_KERNEL_ACCOUNT);
-	if (!umem->pages) {
-		err = -ENOMEM;
-		goto out_pin;
-	}
-
-	err = xdp_umem_map_pages(umem);
-	if (err)
-		goto out_pages;
-
 	umem->pool = xp_create(umem->pgs, umem->npgs, chunks, chunk_size,
 			       headroom, size, unaligned_chunks);
 	if (!umem->pool) {
 		err = -ENOMEM;
-		goto out_unmap;
+		goto out_pin;
 	}
 	return 0;
 
-out_unmap:
-	xdp_umem_unmap_pages(umem);
-out_pages:
-	kvfree(umem->pages);
 out_pin:
 	xdp_umem_unpin_pages(umem);
 out_account:

commit 2b43470add8c8ff1e1ee28dffc5c5df97e955d09
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 20 21:20:53 2020 +0200

    xsk: Introduce AF_XDP buffer allocation API
    
    In order to simplify AF_XDP zero-copy enablement for NIC driver
    developers, a new AF_XDP buffer allocation API is added. The
    implementation is based on a single core (single producer/consumer)
    buffer pool for the AF_XDP UMEM.
    
    A buffer is allocated using the xsk_buff_alloc() function, and
    returned using xsk_buff_free(). If a buffer is disassociated with the
    pool, e.g. when a buffer is passed to an AF_XDP socket, a buffer is
    said to be released. Currently, the release function is only used by
    the AF_XDP internals and not visible to the driver.
    
    Drivers using this API should register the XDP memory model with the
    new MEM_TYPE_XSK_BUFF_POOL type.
    
    The API is defined in net/xdp_sock_drv.h.
    
    The buffer type is struct xdp_buff, and follows the lifetime of
    regular xdp_buffs, i.e.  the lifetime of an xdp_buff is restricted to
    a NAPI context. In other words, the API is not replacing xdp_frames.
    
    In addition to introducing the API and implementations, the AF_XDP
    core is migrated to use the new APIs.
    
    rfc->v1: Fixed build errors/warnings for m68k and riscv. (kbuild test
             robot)
             Added headroom/chunk size getter. (Maxim/Björn)
    
    v1->v2: Swapped SoBs. (Maxim)
    
    v2->v3: Initialize struct xdp_buff member frame_sz. (Björn)
            Add API to query the DMA address of a frame. (Maxim)
            Do DMA sync for CPU till the end of the frame to handle
            possible growth (frame_sz). (Maxim)
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200520192103.355233-6-bjorn.topel@gmail.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 37ace3bc0d48..7f04688045d5 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -245,7 +245,7 @@ static void xdp_umem_release(struct xdp_umem *umem)
 	}
 
 	xsk_reuseq_destroy(umem);
-
+	xp_destroy(umem->pool);
 	xdp_umem_unmap_pages(umem);
 	xdp_umem_unpin_pages(umem);
 
@@ -390,6 +390,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	umem->size = size;
 	umem->headroom = headroom;
 	umem->chunk_size_nohr = chunk_size - headroom;
+	umem->chunk_size = chunk_size;
 	umem->npgs = size / PAGE_SIZE;
 	umem->pgs = NULL;
 	umem->user = NULL;
@@ -415,11 +416,21 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	}
 
 	err = xdp_umem_map_pages(umem);
-	if (!err)
-		return 0;
+	if (err)
+		goto out_pages;
 
-	kvfree(umem->pages);
+	umem->pool = xp_create(umem->pgs, umem->npgs, chunks, chunk_size,
+			       headroom, size, unaligned_chunks);
+	if (!umem->pool) {
+		err = -ENOMEM;
+		goto out_unmap;
+	}
+	return 0;
 
+out_unmap:
+	xdp_umem_unmap_pages(umem);
+out_pages:
+	kvfree(umem->pages);
 out_pin:
 	xdp_umem_unpin_pages(umem);
 out_account:

commit 07bf2d97d1f37e7ac8d7be2d84ff108d43556a1d
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Mon May 4 15:33:52 2020 +0200

    xsk: Remove unnecessary member in xdp_umem
    
    Remove the unnecessary member of address in struct xdp_umem as it is
    only used during the umem registration. No need to carry this around
    as it is not used during run-time nor when unregistering the umem.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Link: https://lore.kernel.org/bpf/1588599232-24897-3-git-send-email-magnus.karlsson@intel.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 7211f4572760..37ace3bc0d48 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -279,7 +279,7 @@ void xdp_put_umem(struct xdp_umem *umem)
 	}
 }
 
-static int xdp_umem_pin_pages(struct xdp_umem *umem)
+static int xdp_umem_pin_pages(struct xdp_umem *umem, unsigned long address)
 {
 	unsigned int gup_flags = FOLL_WRITE;
 	long npgs;
@@ -291,7 +291,7 @@ static int xdp_umem_pin_pages(struct xdp_umem *umem)
 		return -ENOMEM;
 
 	down_read(&current->mm->mmap_sem);
-	npgs = pin_user_pages(umem->address, umem->npgs,
+	npgs = pin_user_pages(address, umem->npgs,
 			      gup_flags | FOLL_LONGTERM, &umem->pgs[0], NULL);
 	up_read(&current->mm->mmap_sem);
 
@@ -385,7 +385,6 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (headroom >= chunk_size - XDP_PACKET_HEADROOM)
 		return -EINVAL;
 
-	umem->address = (unsigned long)addr;
 	umem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK
 					    : ~((u64)chunk_size - 1);
 	umem->size = size;
@@ -404,7 +403,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (err)
 		return err;
 
-	err = xdp_umem_pin_pages(umem);
+	err = xdp_umem_pin_pages(umem, (unsigned long)addr);
 	if (err)
 		goto out_account;
 

commit e4e5aefc113510c03d34e182ab30bc0cc196675c
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Mon May 4 15:33:51 2020 +0200

    xsk: Change two variable names for increased clarity
    
    Change two variables names so that it is clearer what they
    represent. The first one is xsk_list that in fact only contains the
    list of AF_XDP sockets with a Tx component. Change this to xsk_tx_list
    for improved clarity. The second variable is size in the ring
    structure. One might think that this is the size of the ring, but it
    is in fact the size of the umem, copied into the ring structure to
    improve performance. Rename this variable umem_size to avoid any
    confusion.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Link: https://lore.kernel.org/bpf/1588599232-24897-2-git-send-email-magnus.karlsson@intel.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index ed7a6060f73c..7211f4572760 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -30,9 +30,9 @@ void xdp_add_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 	if (!xs->tx)
 		return;
 
-	spin_lock_irqsave(&umem->xsk_list_lock, flags);
-	list_add_rcu(&xs->list, &umem->xsk_list);
-	spin_unlock_irqrestore(&umem->xsk_list_lock, flags);
+	spin_lock_irqsave(&umem->xsk_tx_list_lock, flags);
+	list_add_rcu(&xs->list, &umem->xsk_tx_list);
+	spin_unlock_irqrestore(&umem->xsk_tx_list_lock, flags);
 }
 
 void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
@@ -42,9 +42,9 @@ void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 	if (!xs->tx)
 		return;
 
-	spin_lock_irqsave(&umem->xsk_list_lock, flags);
+	spin_lock_irqsave(&umem->xsk_tx_list_lock, flags);
 	list_del_rcu(&xs->list);
-	spin_unlock_irqrestore(&umem->xsk_list_lock, flags);
+	spin_unlock_irqrestore(&umem->xsk_tx_list_lock, flags);
 }
 
 /* The umem is stored both in the _rx struct and the _tx struct as we do
@@ -395,8 +395,8 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	umem->pgs = NULL;
 	umem->user = NULL;
 	umem->flags = mr->flags;
-	INIT_LIST_HEAD(&umem->xsk_list);
-	spin_lock_init(&umem->xsk_list_lock);
+	INIT_LIST_HEAD(&umem->xsk_tx_list);
+	spin_lock_init(&umem->xsk_tx_list_lock);
 
 	refcount_set(&umem->users, 1);
 

commit 99e3a236dd43d06c65af0a2ef9cb44306aef6e02
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Tue Apr 14 09:35:15 2020 +0200

    xsk: Add missing check on user supplied headroom size
    
    Add a check that the headroom cannot be larger than the available
    space in the chunk. In the current code, a malicious user can set the
    headroom to a value larger than the chunk size minus the fixed XDP
    headroom. That way packets with a length larger than the supported
    size in the umem could get accepted and result in an out-of-bounds
    write.
    
    Fixes: c0c77d8fb787 ("xsk: add user memory registration support sockopt")
    Reported-by: Bui Quang Minh <minhquangbui99@gmail.com>
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=207225
    Link: https://lore.kernel.org/bpf/1586849715-23490-1-git-send-email-magnus.karlsson@intel.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index fa7bb5e060d0..ed7a6060f73c 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -343,7 +343,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
 	unsigned int chunks, chunks_per_page;
 	u64 addr = mr->addr, size = mr->len;
-	int size_chk, err;
+	int err;
 
 	if (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {
 		/* Strictly speaking we could support this, if:
@@ -382,8 +382,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 			return -EINVAL;
 	}
 
-	size_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;
-	if (size_chk < 0)
+	if (headroom >= chunk_size - XDP_PACKET_HEADROOM)
 		return -EINVAL;
 
 	umem->address = (unsigned long)addr;

commit f1f6a7dd9b53aafd81b696b9017036e7b08e57ea
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Thu Jan 30 22:13:35 2020 -0800

    mm, tree-wide: rename put_user_page*() to unpin_user_page*()
    
    In order to provide a clearer, more symmetric API for pinning and
    unpinning DMA pages.  This way, pin_user_pages*() calls match up with
    unpin_user_pages*() calls, and the API is a lot closer to being
    self-explanatory.
    
    Link: http://lkml.kernel.org/r/20200107224558.2362728-23-jhubbard@nvidia.com
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Reviewed-by: Jan Kara <jack@suse.cz>
    Cc: Alex Williamson <alex.williamson@redhat.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Björn Töpel <bjorn.topel@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Hans Verkuil <hverkuil-cisco@xs4all.nl>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Leon Romanovsky <leonro@mellanox.com>
    Cc: Mauro Carvalho Chehab <mchehab@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index b17ce9a5534d..fa7bb5e060d0 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -212,7 +212,7 @@ static int xdp_umem_map_pages(struct xdp_umem *umem)
 
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
-	put_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
+	unpin_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
 
 	kfree(umem->pgs);
 	umem->pgs = NULL;

commit fb48b4746a5d77f352b8c1f793e2a6f4211c8ec2
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Thu Jan 30 22:13:17 2020 -0800

    net/xdp: set FOLL_PIN via pin_user_pages()
    
    Convert net/xdp to use the new pin_longterm_pages() call, which sets
    FOLL_PIN.  Setting FOLL_PIN is now required for code that requires
    tracking of pinned pages.
    
    In partial anticipation of this work, the net/xdp code was already calling
    put_user_page() instead of put_page().  Therefore, in order to convert
    from the get_user_pages()/put_page() model, to the
    pin_user_pages()/put_user_page() model, the only change required here is
    to change get_user_pages() to pin_user_pages().
    
    Link: http://lkml.kernel.org/r/20200107224558.2362728-18-jhubbard@nvidia.com
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Cc: Alex Williamson <alex.williamson@redhat.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Daniel Vetter <daniel.vetter@ffwll.ch>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Hans Verkuil <hverkuil-cisco@xs4all.nl>
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Jason Gunthorpe <jgg@mellanox.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jens Axboe <axboe@kernel.dk>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kirill A. Shutemov <kirill@shutemov.name>
    Cc: Leon Romanovsky <leonro@mellanox.com>
    Cc: Mauro Carvalho Chehab <mchehab@kernel.org>
    Cc: Mike Rapoport <rppt@linux.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index f93e917e0929..b17ce9a5534d 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -291,7 +291,7 @@ static int xdp_umem_pin_pages(struct xdp_umem *umem)
 		return -ENOMEM;
 
 	down_read(&current->mm->mmap_sem);
-	npgs = get_user_pages(umem->address, umem->npgs,
+	npgs = pin_user_pages(umem->address, umem->npgs,
 			      gup_flags | FOLL_LONGTERM, &umem->pgs[0], NULL);
 	up_read(&current->mm->mmap_sem);
 

commit d3a56931f9c8644a52e7a6735954c84ea895e8c3
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Tue Jan 14 10:49:25 2020 +0100

    xsk: Support allocations of large umems
    
    When registering a umem area that is sufficiently large (>1G on an
    x86), kmalloc cannot be used to allocate one of the internal data
    structures, as the size requested gets too large. Use kvmalloc instead
    that falls back on vmalloc if the allocation is too large for kmalloc.
    
    Also add accounting for this structure as it is triggered by a user
    space action (the XDP_UMEM_REG setsockopt) and it is by far the
    largest structure of kernel allocated memory in xsk.
    
    Reported-by: Ryan Goodfellow <rgoodfel@isi.edu>
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Link: https://lore.kernel.org/bpf/1578995365-7050-1-git-send-email-magnus.karlsson@intel.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 3049af269fbf..f93e917e0929 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -249,7 +249,7 @@ static void xdp_umem_release(struct xdp_umem *umem)
 	xdp_umem_unmap_pages(umem);
 	xdp_umem_unpin_pages(umem);
 
-	kfree(umem->pages);
+	kvfree(umem->pages);
 	umem->pages = NULL;
 
 	xdp_umem_unaccount_pages(umem);
@@ -409,7 +409,8 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (err)
 		goto out_account;
 
-	umem->pages = kcalloc(umem->npgs, sizeof(*umem->pages), GFP_KERNEL);
+	umem->pages = kvcalloc(umem->npgs, sizeof(*umem->pages),
+			       GFP_KERNEL_ACCOUNT);
 	if (!umem->pages) {
 		err = -ENOMEM;
 		goto out_pin;
@@ -419,7 +420,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (!err)
 		return 0;
 
-	kfree(umem->pages);
+	kvfree(umem->pages);
 
 out_pin:
 	xdp_umem_unpin_pages(umem);

commit 2afd23f78f39da84937006ecd24aa664a4ab052b
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Mon Oct 21 10:16:58 2019 +0200

    xsk: Fix registration of Rx-only sockets
    
    Having Rx-only AF_XDP sockets can potentially lead to a crash in the
    system by a NULL pointer dereference in xsk_umem_consume_tx(). This
    function iterates through a list of all sockets tied to a umem and
    checks if there are any packets to send on the Tx ring. Rx-only
    sockets do not have a Tx ring, so this will cause a NULL pointer
    dereference. This will happen if you have registered one or more
    Rx-only sockets to a umem and the driver is checking the Tx ring even
    on Rx, or if the XDP_SHARED_UMEM mode is used and there is a mix of
    Rx-only and other sockets tied to the same umem.
    
    Fixed by only putting sockets with a Tx component on the list that
    xsk_umem_consume_tx() iterates over.
    
    Fixes: ac98d8aab61b ("xsk: wire upp Tx zero-copy functions")
    Reported-by: Kal Cutter Conley <kal.conley@dectris.com>
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Link: https://lore.kernel.org/bpf/1571645818-16244-1-git-send-email-magnus.karlsson@intel.com

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 16d5f353163a..3049af269fbf 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -27,6 +27,9 @@ void xdp_add_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 {
 	unsigned long flags;
 
+	if (!xs->tx)
+		return;
+
 	spin_lock_irqsave(&umem->xsk_list_lock, flags);
 	list_add_rcu(&xs->list, &umem->xsk_list);
 	spin_unlock_irqrestore(&umem->xsk_list_lock, flags);
@@ -36,6 +39,9 @@ void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 {
 	unsigned long flags;
 
+	if (!xs->tx)
+		return;
+
 	spin_lock_irqsave(&umem->xsk_list_lock, flags);
 	list_del_rcu(&xs->list);
 	spin_unlock_irqrestore(&umem->xsk_list_lock, flags);

commit 02dc96ef6c25f990452c114c59d75c368a1f4c8f
Merge: edf445ad7c8d faeacb6ddb13
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Sep 28 17:47:33 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Pull networking fixes from David Miller:
    
     1) Sanity check URB networking device parameters to avoid divide by
        zero, from Oliver Neukum.
    
     2) Disable global multicast filter in NCSI, otherwise LLDP and IPV6
        don't work properly. Longer term this needs a better fix tho. From
        Vijay Khemka.
    
     3) Small fixes to selftests (use ping when ping6 is not present, etc.)
        from David Ahern.
    
     4) Bring back rt_uses_gateway member of struct rtable, it's semantics
        were not well understood and trying to remove it broke things. From
        David Ahern.
    
     5) Move usbnet snaity checking, ignore endpoints with invalid
        wMaxPacketSize. From Bjørn Mork.
    
     6) Missing Kconfig deps for sja1105 driver, from Mao Wenan.
    
     7) Various small fixes to the mlx5 DR steering code, from Alaa Hleihel,
        Alex Vesker, and Yevgeny Kliteynik
    
     8) Missing CAP_NET_RAW checks in various places, from Ori Nimron.
    
     9) Fix crash when removing sch_cbs entry while offloading is enabled,
        from Vinicius Costa Gomes.
    
    10) Signedness bug fixes, generally in looking at the result given by
        of_get_phy_mode() and friends. From Dan Crapenter.
    
    11) Disable preemption around BPF_PROG_RUN() calls, from Eric Dumazet.
    
    12) Don't create VRF ipv6 rules if ipv6 is disabled, from David Ahern.
    
    13) Fix quantization code in tcp_bbr, from Kevin Yang.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net: (127 commits)
      net: tap: clean up an indentation issue
      nfp: abm: fix memory leak in nfp_abm_u32_knode_replace
      tcp: better handle TCP_USER_TIMEOUT in SYN_SENT state
      sk_buff: drop all skb extensions on free and skb scrubbing
      tcp_bbr: fix quantization code to not raise cwnd if not probing bandwidth
      mlxsw: spectrum_flower: Fail in case user specifies multiple mirror actions
      Documentation: Clarify trap's description
      mlxsw: spectrum: Clear VLAN filters during port initialization
      net: ena: clean up indentation issue
      NFC: st95hf: clean up indentation issue
      net: phy: micrel: add Asym Pause workaround for KSZ9021
      net: socionext: ave: Avoid using netdev_err() before calling register_netdev()
      ptp: correctly disable flags on old ioctls
      lib: dimlib: fix help text typos
      net: dsa: microchip: Always set regmap stride to 1
      nfp: flower: fix memory leak in nfp_flower_spawn_vnic_reprs
      nfp: flower: prevent memory leak in nfp_flower_spawn_phy_reprs
      net/sched: Set default of CONFIG_NET_TC_SKB_EXT to N
      vrf: Do not attempt to create IPv6 mcast rule if IPv6 is disabled
      net: sched: sch_sfb: don't call qdisc_put() while holding tree lock
      ...

commit 1edc97694d0fa95d143e3457be892544e41f794a
Author: John Hubbard <jhubbard@nvidia.com>
Date:   Mon Sep 23 15:35:10 2019 -0700

    net/xdp: convert put_page() to put_user_page*()
    
    For pages that were retained via get_user_pages*(), release those pages
    via the new put_user_page*() routines, instead of via put_page() or
    release_pages().
    
    This is part a tree-wide conversion, as described in fc1d8e7cca2d ("mm:
    introduce put_user_page*(), placeholder versions").
    
    Link: http://lkml.kernel.org/r/20190724044537.10458-4-jhubbard@nvidia.com
    Signed-off-by: John Hubbard <jhubbard@nvidia.com>
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Cc: Björn Töpel <bjorn.topel@intel.com>
    Cc: Magnus Karlsson <magnus.karlsson@intel.com>
    Cc: David S. Miller <davem@davemloft.net>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 947b8ff0227e..bba3104f128f 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -206,14 +206,7 @@ static int xdp_umem_map_pages(struct xdp_umem *umem)
 
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
-	unsigned int i;
-
-	for (i = 0; i < umem->npgs; i++) {
-		struct page *page = umem->pgs[i];
-
-		set_page_dirty_lock(page);
-		put_page(page);
-	}
+	put_user_pages_dirty_lock(umem->pgs, umem->npgs, true);
 
 	kfree(umem->pgs);
 	umem->pgs = NULL;

commit 733ef7f056a5e23b66e8e7bb3508ca882db388f0
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed Sep 18 09:57:39 2019 +0200

    xsk: relax UMEM headroom alignment
    
    This patch removes the 64B alignment of the UMEM headroom. There is
    really no reason for it, and having a headroom less than 64B should be
    valid.
    
    Fixes: c0c77d8fb787 ("xsk: add user memory registration support sockopt")
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 947b8ff0227e..cdaef54d48be 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -383,8 +383,6 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 			return -EINVAL;
 	}
 
-	headroom = ALIGN(headroom, 64);
-
 	size_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;
 	if (size_chk < 0)
 		return -EINVAL;

commit 1e46c09ec10049a9e366153b32e41cc557383fdb
Merge: f9bcfe214b00 593f191a8005
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 6 16:49:17 2019 +0200

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Add the ability to use unaligned chunks in the AF_XDP umem. By
       relaxing where the chunks can be placed, it allows to use an
       arbitrary buffer size and place whenever there is a free
       address in the umem. Helps more seamless DPDK AF_XDP driver
       integration. Support for i40e, ixgbe and mlx5e, from Kevin and
       Maxim.
    
    2) Addition of a wakeup flag for AF_XDP tx and fill rings so the
       application can wake up the kernel for rx/tx processing which
       avoids busy-spinning of the latter, useful when app and driver
       is located on the same core. Support for i40e, ixgbe and mlx5e,
       from Magnus and Maxim.
    
    3) bpftool fixes for printf()-like functions so compiler can actually
       enforce checks, bpftool build system improvements for custom output
       directories, and addition of 'bpftool map freeze' command, from Quentin.
    
    4) Support attaching/detaching XDP programs from 'bpftool net' command,
       from Daniel.
    
    5) Automatic xskmap cleanup when AF_XDP socket is released, and several
       barrier/{read,write}_once fixes in AF_XDP code, from Björn.
    
    6) Relicense of bpf_helpers.h/bpf_endian.h for future libbpf
       inclusion as well as libbpf versioning improvements, from Andrii.
    
    7) Several new BPF kselftests for verifier precision tracking, from Alexei.
    
    8) Several BPF kselftest fixes wrt endianess to run on s390x, from Ilya.
    
    9) And more BPF kselftest improvements all over the place, from Stanislav.
    
    10) Add simple BPF map op cache for nfp driver to batch dumps, from Jakub.
    
    11) AF_XDP socket umem mapping improvements for 32bit archs, from Ivan.
    
    12) Add BPF-to-BPF call and BTF line info support for s390x JIT, from Yauheni.
    
    13) Small optimization in arm64 JIT to spare 1 insns for BPF_MOD, from Jerin.
    
    14) Fix an error check in bpf_tcp_gen_syncookie() helper, from Petar.
    
    15) Various minor fixes and cleanups, from Nathan, Masahiro, Masanari,
        Peter, Wei, Yue.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c05cd3645814724bdeb32a2b4d953b12bdea5f8c
Author: Kevin Laatz <kevin.laatz@intel.com>
Date:   Tue Aug 27 02:25:22 2019 +0000

    xsk: add support to allow unaligned chunk placement
    
    Currently, addresses are chunk size aligned. This means, we are very
    restricted in terms of where we can place chunk within the umem. For
    example, if we have a chunk size of 2k, then our chunks can only be placed
    at 0,2k,4k,6k,8k... and so on (ie. every 2k starting from 0).
    
    This patch introduces the ability to use unaligned chunks. With these
    changes, we are no longer bound to having to place chunks at a 2k (or
    whatever your chunk size is) interval. Since we are no longer dealing with
    aligned chunks, they can now cross page boundaries. Checks for page
    contiguity have been added in order to keep track of which pages are
    followed by a physically contiguous page.
    
    Signed-off-by: Kevin Laatz <kevin.laatz@intel.com>
    Signed-off-by: Ciara Loftus <ciara.loftus@intel.com>
    Signed-off-by: Bruce Richardson <bruce.richardson@intel.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 2d65779282a1..e997b263a0dd 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -340,6 +340,7 @@ static int xdp_umem_account_pages(struct xdp_umem *umem)
 
 static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 {
+	bool unaligned_chunks = mr->flags & XDP_UMEM_UNALIGNED_CHUNK_FLAG;
 	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
 	unsigned int chunks, chunks_per_page;
 	u64 addr = mr->addr, size = mr->len;
@@ -355,7 +356,11 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 		return -EINVAL;
 	}
 
-	if (!is_power_of_2(chunk_size))
+	if (mr->flags & ~(XDP_UMEM_UNALIGNED_CHUNK_FLAG |
+			XDP_UMEM_USES_NEED_WAKEUP))
+		return -EINVAL;
+
+	if (!unaligned_chunks && !is_power_of_2(chunk_size))
 		return -EINVAL;
 
 	if (!PAGE_ALIGNED(addr)) {
@@ -372,9 +377,11 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (chunks == 0)
 		return -EINVAL;
 
-	chunks_per_page = PAGE_SIZE / chunk_size;
-	if (chunks < chunks_per_page || chunks % chunks_per_page)
-		return -EINVAL;
+	if (!unaligned_chunks) {
+		chunks_per_page = PAGE_SIZE / chunk_size;
+		if (chunks < chunks_per_page || chunks % chunks_per_page)
+			return -EINVAL;
+	}
 
 	headroom = ALIGN(headroom, 64);
 
@@ -383,13 +390,15 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 		return -EINVAL;
 
 	umem->address = (unsigned long)addr;
-	umem->chunk_mask = ~((u64)chunk_size - 1);
+	umem->chunk_mask = unaligned_chunks ? XSK_UNALIGNED_BUF_ADDR_MASK
+					    : ~((u64)chunk_size - 1);
 	umem->size = size;
 	umem->headroom = headroom;
 	umem->chunk_size_nohr = chunk_size - headroom;
 	umem->npgs = size / PAGE_SIZE;
 	umem->pgs = NULL;
 	umem->user = NULL;
+	umem->flags = mr->flags;
 	INIT_LIST_HEAD(&umem->xsk_list);
 	spin_lock_init(&umem->xsk_list_lock);
 

commit 68aaf4459556b1f9370c259fd486aecad2257552
Merge: d00ee466a07e 9e8312f5e160
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Aug 27 14:23:31 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Minor conflict in r8169, bug fix had two versions in net
    and net-next, take the net-next hunks.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 624676e788992e4da8de1fad93bfe51563a7f9a0
Author: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
Date:   Thu Aug 15 15:13:55 2019 +0300

    xdp: xdp_umem: replace kmap on vmap for umem map
    
    For 64-bit there is no reason to use vmap/vunmap, so use page_address
    as it was initially. For 32 bits, in some apps, like in samples
    xdpsock_user.c when number of pgs in use is quite big, the kmap
    memory can be not enough, despite on this, kmap looks like is
    deprecated in such cases as it can block and should be used rather
    for dynamic mm.
    
    Signed-off-by: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index cda6feb1edb0..2d65779282a1 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -14,7 +14,7 @@
 #include <linux/netdevice.h>
 #include <linux/rtnetlink.h>
 #include <linux/idr.h>
-#include <linux/highmem.h>
+#include <linux/vmalloc.h>
 
 #include "xdp_umem.h"
 #include "xsk_queue.h"
@@ -178,7 +178,30 @@ static void xdp_umem_unmap_pages(struct xdp_umem *umem)
 	unsigned int i;
 
 	for (i = 0; i < umem->npgs; i++)
-		kunmap(umem->pgs[i]);
+		if (PageHighMem(umem->pgs[i]))
+			vunmap(umem->pages[i].addr);
+}
+
+static int xdp_umem_map_pages(struct xdp_umem *umem)
+{
+	unsigned int i;
+	void *addr;
+
+	for (i = 0; i < umem->npgs; i++) {
+		if (PageHighMem(umem->pgs[i]))
+			addr = vmap(&umem->pgs[i], 1, VM_MAP, PAGE_KERNEL);
+		else
+			addr = page_address(umem->pgs[i]);
+
+		if (!addr) {
+			xdp_umem_unmap_pages(umem);
+			return -ENOMEM;
+		}
+
+		umem->pages[i].addr = addr;
+	}
+
+	return 0;
 }
 
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
@@ -320,7 +343,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
 	unsigned int chunks, chunks_per_page;
 	u64 addr = mr->addr, size = mr->len;
-	int size_chk, err, i;
+	int size_chk, err;
 
 	if (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {
 		/* Strictly speaking we could support this, if:
@@ -386,10 +409,11 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 		goto out_account;
 	}
 
-	for (i = 0; i < umem->npgs; i++)
-		umem->pages[i].addr = kmap(umem->pgs[i]);
+	err = xdp_umem_map_pages(umem);
+	if (!err)
+		return 0;
 
-	return 0;
+	kfree(umem->pages);
 
 out_account:
 	xdp_umem_unaccount_pages(umem);

commit fb89c39455e4b49881c5a42761bd71f03d3ef888
Author: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
Date:   Thu Aug 15 23:56:35 2019 +0300

    xdp: unpin xdp umem pages in error path
    
    Fix mem leak caused by missed unpin routine for umem pages.
    
    Fixes: 8aef7340ae9695 ("xsk: introduce xdp_umem_page")
    Signed-off-by: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 83de74ca729a..688aac7a6943 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -365,7 +365,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	umem->pages = kcalloc(umem->npgs, sizeof(*umem->pages), GFP_KERNEL);
 	if (!umem->pages) {
 		err = -ENOMEM;
-		goto out_account;
+		goto out_pin;
 	}
 
 	for (i = 0; i < umem->npgs; i++)
@@ -373,6 +373,8 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 
 	return 0;
 
+out_pin:
+	xdp_umem_unpin_pages(umem);
 out_account:
 	xdp_umem_unaccount_pages(umem);
 	return err;

commit 77cd0d7b3f257fd0e3096b4fdcff1a7d38e99e10
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Wed Aug 14 09:27:17 2019 +0200

    xsk: add support for need_wakeup flag in AF_XDP rings
    
    This commit adds support for a new flag called need_wakeup in the
    AF_XDP Tx and fill rings. When this flag is set, it means that the
    application has to explicitly wake up the kernel Rx (for the bit in
    the fill ring) or kernel Tx (for bit in the Tx ring) processing by
    issuing a syscall. Poll() can wake up both depending on the flags
    submitted and sendto() will wake up tx processing only.
    
    The main reason for introducing this new flag is to be able to
    efficiently support the case when application and driver is executing
    on the same core. Previously, the driver was just busy-spinning on the
    fill ring if it ran out of buffers in the HW and there were none on
    the fill ring. This approach works when the application is running on
    another core as it can replenish the fill ring while the driver is
    busy-spinning. Though, this is a lousy approach if both of them are
    running on the same core as the probability of the fill ring getting
    more entries when the driver is busy-spinning is zero. With this new
    feature the driver now sets the need_wakeup flag and returns to the
    application. The application can then replenish the fill queue and
    then explicitly wake up the Rx processing in the kernel using the
    syscall poll(). For Tx, the flag is only set to one if the driver has
    no outstanding Tx completion interrupts. If it has some, the flag is
    zero as it will be woken up by a completion interrupt anyway.
    
    As a nice side effect, this new flag also improves the performance of
    the case where application and driver are running on two different
    cores as it reduces the number of syscalls to the kernel. The kernel
    tells user space if it needs to be woken up by a syscall, and this
    eliminates many of the syscalls.
    
    This flag needs some simple driver support. If the driver does not
    support this, the Rx flag is always zero and the Tx flag is always
    one. This makes any application relying on this feature default to the
    old behaviour of not requiring any syscalls in the Rx path and always
    having to call sendto() in the Tx path.
    
    For backwards compatibility reasons, this feature has to be explicitly
    turned on using a new bind flag (XDP_USE_NEED_WAKEUP). I recommend
    that you always turn it on as it so far always have had a positive
    performance impact.
    
    The name and inspiration of the flag has been taken from io_uring by
    Jens Axboe. Details about this feature in io_uring can be found in
    http://kernel.dk/io_uring.pdf, section 8.3.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 6e2d4da4cdcd..cda6feb1edb0 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -106,6 +106,15 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	umem->dev = dev;
 	umem->queue_id = queue_id;
 
+	if (flags & XDP_USE_NEED_WAKEUP) {
+		umem->flags |= XDP_UMEM_USES_NEED_WAKEUP;
+		/* Tx needs to be explicitly woken up the first time.
+		 * Also for supporting drivers that do not implement this
+		 * feature. They will always have to call sendto().
+		 */
+		xsk_set_tx_need_wakeup(umem);
+	}
+
 	dev_hold(dev);
 
 	if (force_copy)

commit 9116e5e2b1fff71dce501d971e86a3695acc3dba
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Wed Aug 14 09:27:16 2019 +0200

    xsk: replace ndo_xsk_async_xmit with ndo_xsk_wakeup
    
    This commit replaces ndo_xsk_async_xmit with ndo_xsk_wakeup. This new
    ndo provides the same functionality as before but with the addition of
    a new flags field that is used to specifiy if Rx, Tx or both should be
    woken up. The previous ndo only woke up Tx, as implied by the
    name. The i40e and ixgbe drivers (which are all the supported ones)
    are updated with this new interface.
    
    This new ndo will be used by the new need_wakeup functionality of XDP
    sockets that need to be able to wake up both Rx and Tx driver
    processing.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index a0607969f8c0..6e2d4da4cdcd 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -112,8 +112,7 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 		/* For copy-mode, we are done. */
 		return 0;
 
-	if (!dev->netdev_ops->ndo_bpf ||
-	    !dev->netdev_ops->ndo_xsk_async_xmit) {
+	if (!dev->netdev_ops->ndo_bpf || !dev->netdev_ops->ndo_xsk_wakeup) {
 		err = -EOPNOTSUPP;
 		goto err_unreg_umem;
 	}

commit d9973cec9d578b381235bb872a2d378c69c54915
Author: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
Date:   Thu Aug 8 12:38:03 2019 +0300

    xdp: xdp_umem: fix umem pages mapping for 32bits systems
    
    Use kmap instead of page_address as it's not always in low memory.
    
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Ivan Khoronzhuk <ivan.khoronzhuk@linaro.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 83de74ca729a..a0607969f8c0 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -14,6 +14,7 @@
 #include <linux/netdevice.h>
 #include <linux/rtnetlink.h>
 #include <linux/idr.h>
+#include <linux/highmem.h>
 
 #include "xdp_umem.h"
 #include "xsk_queue.h"
@@ -164,6 +165,14 @@ void xdp_umem_clear_dev(struct xdp_umem *umem)
 	umem->zc = false;
 }
 
+static void xdp_umem_unmap_pages(struct xdp_umem *umem)
+{
+	unsigned int i;
+
+	for (i = 0; i < umem->npgs; i++)
+		kunmap(umem->pgs[i]);
+}
+
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
 	unsigned int i;
@@ -207,6 +216,7 @@ static void xdp_umem_release(struct xdp_umem *umem)
 
 	xsk_reuseq_destroy(umem);
 
+	xdp_umem_unmap_pages(umem);
 	xdp_umem_unpin_pages(umem);
 
 	kfree(umem->pages);
@@ -369,7 +379,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	}
 
 	for (i = 0; i < umem->npgs; i++)
-		umem->pages[i].addr = page_address(umem->pgs[i]);
+		umem->pages[i].addr = kmap(umem->pgs[i]);
 
 	return 0;
 

commit 5464c3a0e9a037b63d5229cdea08dddc01a98aac
Author: Ilya Maximets <i.maximets@samsung.com>
Date:   Mon Jul 8 14:03:44 2019 +0300

    xdp: fix potential deadlock on socket mutex
    
    There are 2 call chains:
    
      a) xsk_bind --> xdp_umem_assign_dev
      b) unregister_netdevice_queue --> xsk_notifier
    
    with the following locking order:
    
      a) xs->mutex --> rtnl_lock
      b) rtnl_lock --> xdp.lock --> xs->mutex
    
    Different order of taking 'xs->mutex' and 'rtnl_lock' could produce a
    deadlock here. Fix that by moving the 'rtnl_lock' before 'xs->lock' in
    the bind call chain (a).
    
    Reported-by: syzbot+bf64ec93de836d7f4c2c@syzkaller.appspotmail.com
    Fixes: 455302d1c9ae ("xdp: fix hang while unregistering device bound to xdp socket")
    Signed-off-by: Ilya Maximets <i.maximets@samsung.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 20c91f02d3d8..83de74ca729a 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -87,21 +87,20 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	struct netdev_bpf bpf;
 	int err = 0;
 
+	ASSERT_RTNL();
+
 	force_zc = flags & XDP_ZEROCOPY;
 	force_copy = flags & XDP_COPY;
 
 	if (force_zc && force_copy)
 		return -EINVAL;
 
-	rtnl_lock();
-	if (xdp_get_umem_from_qid(dev, queue_id)) {
-		err = -EBUSY;
-		goto out_rtnl_unlock;
-	}
+	if (xdp_get_umem_from_qid(dev, queue_id))
+		return -EBUSY;
 
 	err = xdp_reg_umem_at_qid(dev, umem, queue_id);
 	if (err)
-		goto out_rtnl_unlock;
+		return err;
 
 	umem->dev = dev;
 	umem->queue_id = queue_id;
@@ -110,7 +109,7 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 
 	if (force_copy)
 		/* For copy-mode, we are done. */
-		goto out_rtnl_unlock;
+		return 0;
 
 	if (!dev->netdev_ops->ndo_bpf ||
 	    !dev->netdev_ops->ndo_xsk_async_xmit) {
@@ -125,7 +124,6 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	err = dev->netdev_ops->ndo_bpf(dev, &bpf);
 	if (err)
 		goto err_unreg_umem;
-	rtnl_unlock();
 
 	umem->zc = true;
 	return 0;
@@ -135,8 +133,6 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 		err = 0; /* fallback to copy mode */
 	if (err)
 		xdp_clear_umem_at_qid(dev, queue_id);
-out_rtnl_unlock:
-	rtnl_unlock();
 	return err;
 }
 

commit 455302d1c9ae9318660aaeb9748a01ff414c9741
Author: Ilya Maximets <i.maximets@samsung.com>
Date:   Fri Jun 28 11:04:07 2019 +0300

    xdp: fix hang while unregistering device bound to xdp socket
    
    Device that bound to XDP socket will not have zero refcount until the
    userspace application will not close it. This leads to hang inside
    'netdev_wait_allrefs()' if device unregistering requested:
    
      # ip link del p1
      < hang on recvmsg on netlink socket >
    
      # ps -x | grep ip
      5126  pts/0    D+   0:00 ip link del p1
    
      # journalctl -b
    
      Jun 05 07:19:16 kernel:
      unregister_netdevice: waiting for p1 to become free. Usage count = 1
    
      Jun 05 07:19:27 kernel:
      unregister_netdevice: waiting for p1 to become free. Usage count = 1
      ...
    
    Fix that by implementing NETDEV_UNREGISTER event notification handler
    to properly clean up all the resources and unref device.
    
    This should also allow socket killing via ss(8) utility.
    
    Fixes: 965a99098443 ("xsk: add support for bind for Rx")
    Signed-off-by: Ilya Maximets <i.maximets@samsung.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 267b82a4cbcf..20c91f02d3d8 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -140,11 +140,13 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	return err;
 }
 
-static void xdp_umem_clear_dev(struct xdp_umem *umem)
+void xdp_umem_clear_dev(struct xdp_umem *umem)
 {
 	struct netdev_bpf bpf;
 	int err;
 
+	ASSERT_RTNL();
+
 	if (!umem->dev)
 		return;
 
@@ -153,17 +155,13 @@ static void xdp_umem_clear_dev(struct xdp_umem *umem)
 		bpf.xsk.umem = NULL;
 		bpf.xsk.queue_id = umem->queue_id;
 
-		rtnl_lock();
 		err = umem->dev->netdev_ops->ndo_bpf(umem->dev, &bpf);
-		rtnl_unlock();
 
 		if (err)
 			WARN(1, "failed to disable umem!\n");
 	}
 
-	rtnl_lock();
 	xdp_clear_umem_at_qid(umem->dev, umem->queue_id);
-	rtnl_unlock();
 
 	dev_put(umem->dev);
 	umem->dev = NULL;
@@ -195,7 +193,9 @@ static void xdp_umem_unaccount_pages(struct xdp_umem *umem)
 
 static void xdp_umem_release(struct xdp_umem *umem)
 {
+	rtnl_lock();
 	xdp_umem_clear_dev(umem);
+	rtnl_unlock();
 
 	ida_simple_remove(&umem_ida, umem->id);
 

commit 162c820ed8965bf94d2685f97388aea5aee9e258
Author: Ilya Maximets <i.maximets@samsung.com>
Date:   Fri Jun 28 11:04:06 2019 +0300

    xdp: hold device for umem regardless of zero-copy mode
    
    Device pointer stored in umem regardless of zero-copy mode,
    so we heed to hold the device in all cases.
    
    Fixes: c9b47cc1fabc ("xsk: fix bug when trying to use both copy and zero-copy on one queue id")
    Signed-off-by: Ilya Maximets <i.maximets@samsung.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 9c6de4f114f8..267b82a4cbcf 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -105,6 +105,9 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 
 	umem->dev = dev;
 	umem->queue_id = queue_id;
+
+	dev_hold(dev);
+
 	if (force_copy)
 		/* For copy-mode, we are done. */
 		goto out_rtnl_unlock;
@@ -124,7 +127,6 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 		goto err_unreg_umem;
 	rtnl_unlock();
 
-	dev_hold(dev);
 	umem->zc = true;
 	return 0;
 
@@ -163,10 +165,9 @@ static void xdp_umem_clear_dev(struct xdp_umem *umem)
 	xdp_clear_umem_at_qid(umem->dev, umem->queue_id);
 	rtnl_unlock();
 
-	if (umem->zc) {
-		dev_put(umem->dev);
-		umem->zc = false;
-	}
+	dev_put(umem->dev);
+	umem->dev = NULL;
+	umem->zc = false;
 }
 
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)

commit 01d76b5317003e019ace561a9b775f51aafdfdc4
Author: Ilya Maximets <i.maximets@samsung.com>
Date:   Fri Jun 7 20:27:32 2019 +0300

    xdp: check device pointer before clearing
    
    We should not call 'ndo_bpf()' or 'dev_put()' with NULL argument.
    
    Fixes: c9b47cc1fabc ("xsk: fix bug when trying to use both copy and zero-copy on one queue id")
    Signed-off-by: Ilya Maximets <i.maximets@samsung.com>
    Acked-by: Jonathan Lemon <jonathan.lemon@gmail.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 2b18223e7eb8..9c6de4f114f8 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -143,6 +143,9 @@ static void xdp_umem_clear_dev(struct xdp_umem *umem)
 	struct netdev_bpf bpf;
 	int err;
 
+	if (!umem->dev)
+		return;
+
 	if (umem->zc) {
 		bpf.command = XDP_SETUP_XSK_UMEM;
 		bpf.xsk.umem = NULL;
@@ -156,11 +159,9 @@ static void xdp_umem_clear_dev(struct xdp_umem *umem)
 			WARN(1, "failed to disable umem!\n");
 	}
 
-	if (umem->dev) {
-		rtnl_lock();
-		xdp_clear_umem_at_qid(umem->dev, umem->queue_id);
-		rtnl_unlock();
-	}
+	rtnl_lock();
+	xdp_clear_umem_at_qid(umem->dev, umem->queue_id);
+	rtnl_unlock();
 
 	if (umem->zc) {
 		dev_put(umem->dev);

commit 932f4a630a695212bdc7379b05f9bd0dafc5d968
Author: Ira Weiny <ira.weiny@intel.com>
Date:   Mon May 13 17:17:03 2019 -0700

    mm/gup: replace get_user_pages_longterm() with FOLL_LONGTERM
    
    Pach series "Add FOLL_LONGTERM to GUP fast and use it".
    
    HFI1, qib, and mthca, use get_user_pages_fast() due to its performance
    advantages.  These pages can be held for a significant time.  But
    get_user_pages_fast() does not protect against mapping FS DAX pages.
    
    Introduce FOLL_LONGTERM and use this flag in get_user_pages_fast() which
    retains the performance while also adding the FS DAX checks.  XDP has also
    shown interest in using this functionality.[1]
    
    In addition we change get_user_pages() to use the new FOLL_LONGTERM flag
    and remove the specialized get_user_pages_longterm call.
    
    [1] https://lkml.org/lkml/2019/3/19/939
    
    "longterm" is a relative thing and at this point is probably a misnomer.
    This is really flagging a pin which is going to be given to hardware and
    can't move.  I've thought of a couple of alternative names but I think we
    have to settle on if we are going to use FL_LAYOUT or something else to
    solve the "longterm" problem.  Then I think we can change the flag to a
    better name.
    
    Secondly, it depends on how often you are registering memory.  I have
    spoken with some RDMA users who consider MR in the performance path...
    For the overall application performance.  I don't have the numbers as the
    tests for HFI1 were done a long time ago.  But there was a significant
    advantage.  Some of which is probably due to the fact that you don't have
    to hold mmap_sem.
    
    Finally, architecturally I think it would be good for everyone to use
    *_fast.  There are patches submitted to the RDMA list which would allow
    the use of *_fast (they reworking the use of mmap_sem) and as soon as they
    are accepted I'll submit a patch to convert the RDMA core as well.  Also
    to this point others are looking to use *_fast.
    
    As an aside, Jasons pointed out in my previous submission that *_fast and
    *_unlocked look very much the same.  I agree and I think further cleanup
    will be coming.  But I'm focused on getting the final solution for DAX at
    the moment.
    
    This patch (of 7):
    
    This patch starts a series which aims to support FOLL_LONGTERM in
    get_user_pages_fast().  Some callers who would like to do a longterm (user
    controlled pin) of pages with the fast variant of GUP for performance
    purposes.
    
    Rather than have a separate get_user_pages_longterm() call, introduce
    FOLL_LONGTERM and change the longterm callers to use it.
    
    This patch does not change any functionality.  In the short term
    "longterm" or user controlled pins are unsafe for Filesystems and FS DAX
    in particular has been blocked.  However, callers of get_user_pages_fast()
    were not "protected".
    
    FOLL_LONGTERM can _only_ be supported with get_user_pages[_fast]() as it
    requires vmas to determine if DAX is in use.
    
    NOTE: In merging with the CMA changes we opt to change the
    get_user_pages() call in check_and_migrate_cma_pages() to a call of
    __get_user_pages_locked() on the newly migrated pages.  This makes the
    code read better in that we are calling __get_user_pages_locked() on the
    pages before and after a potential migration.
    
    As a side affect some of the interfaces are cleaned up but this is not the
    primary purpose of the series.
    
    In review[1] it was asked:
    
    <quote>
    > This I don't get - if you do lock down long term mappings performance
    > of the actual get_user_pages call shouldn't matter to start with.
    >
    > What do I miss?
    
    A couple of points.
    
    First "longterm" is a relative thing and at this point is probably a
    misnomer.  This is really flagging a pin which is going to be given to
    hardware and can't move.  I've thought of a couple of alternative names
    but I think we have to settle on if we are going to use FL_LAYOUT or
    something else to solve the "longterm" problem.  Then I think we can
    change the flag to a better name.
    
    Second, It depends on how often you are registering memory.  I have spoken
    with some RDMA users who consider MR in the performance path...  For the
    overall application performance.  I don't have the numbers as the tests
    for HFI1 were done a long time ago.  But there was a significant
    advantage.  Some of which is probably due to the fact that you don't have
    to hold mmap_sem.
    
    Finally, architecturally I think it would be good for everyone to use
    *_fast.  There are patches submitted to the RDMA list which would allow
    the use of *_fast (they reworking the use of mmap_sem) and as soon as they
    are accepted I'll submit a patch to convert the RDMA core as well.  Also
    to this point others are looking to use *_fast.
    
    As an asside, Jasons pointed out in my previous submission that *_fast and
    *_unlocked look very much the same.  I agree and I think further cleanup
    will be coming.  But I'm focused on getting the final solution for DAX at
    the moment.
    
    </quote>
    
    [1] https://lore.kernel.org/lkml/20190220180255.GA12020@iweiny-DESK2.sc.intel.com/T/#md6abad2569f3bf6c1f03686c8097ab6563e94965
    
    [ira.weiny@intel.com: v3]
      Link: http://lkml.kernel.org/r/20190328084422.29911-2-ira.weiny@intel.com
    Link: http://lkml.kernel.org/r/20190328084422.29911-2-ira.weiny@intel.com
    Link: http://lkml.kernel.org/r/20190317183438.2057-2-ira.weiny@intel.com
    Signed-off-by: Ira Weiny <ira.weiny@intel.com>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Cc: Michal Hocko <mhocko@kernel.org>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: "Kirill A. Shutemov" <kirill.shutemov@linux.intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Rich Felker <dalias@libc.org>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: James Hogan <jhogan@kernel.org>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Mike Marshall <hubcap@omnibond.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 989e52386c35..2b18223e7eb8 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -253,8 +253,8 @@ static int xdp_umem_pin_pages(struct xdp_umem *umem)
 		return -ENOMEM;
 
 	down_read(&current->mm->mmap_sem);
-	npgs = get_user_pages_longterm(umem->address, umem->npgs,
-				       gup_flags, &umem->pgs[0], NULL);
+	npgs = get_user_pages(umem->address, umem->npgs,
+			      gup_flags | FOLL_LONGTERM, &umem->pgs[0], NULL);
 	up_read(&current->mm->mmap_sem);
 
 	if (npgs != umem->npgs) {

commit 044175a06706d516aa42874bb44dbbfc3c4d20eb
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed Mar 13 15:15:49 2019 +0100

    xsk: fix umem memory leak on cleanup
    
    When the umem is cleaned up, the task that created it might already be
    gone. If the task was gone, the xdp_umem_release function did not free
    the pages member of struct xdp_umem.
    
    It turned out that the task lookup was not needed at all; The code was
    a left-over when we moved from task accounting to user accounting [1].
    
    This patch fixes the memory leak by removing the task lookup logic
    completely.
    
    [1] https://lore.kernel.org/netdev/20180131135356.19134-3-bjorn.topel@gmail.com/
    
    Link: https://lore.kernel.org/netdev/c1cb2ca8-6a14-3980-8672-f3de0bb38dfd@suse.cz/
    Fixes: c0c77d8fb787 ("xsk: add user memory registration support sockopt")
    Reported-by: Jiri Slaby <jslaby@suse.cz>
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 77520eacee8f..989e52386c35 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -193,9 +193,6 @@ static void xdp_umem_unaccount_pages(struct xdp_umem *umem)
 
 static void xdp_umem_release(struct xdp_umem *umem)
 {
-	struct task_struct *task;
-	struct mm_struct *mm;
-
 	xdp_umem_clear_dev(umem);
 
 	ida_simple_remove(&umem_ida, umem->id);
@@ -214,21 +211,10 @@ static void xdp_umem_release(struct xdp_umem *umem)
 
 	xdp_umem_unpin_pages(umem);
 
-	task = get_pid_task(umem->pid, PIDTYPE_PID);
-	put_pid(umem->pid);
-	if (!task)
-		goto out;
-	mm = get_task_mm(task);
-	put_task_struct(task);
-	if (!mm)
-		goto out;
-
-	mmput(mm);
 	kfree(umem->pages);
 	umem->pages = NULL;
 
 	xdp_umem_unaccount_pages(umem);
-out:
 	kfree(umem);
 }
 
@@ -357,7 +343,6 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if (size_chk < 0)
 		return -EINVAL;
 
-	umem->pid = get_task_pid(current, PIDTYPE_PID);
 	umem->address = (unsigned long)addr;
 	umem->chunk_mask = ~((u64)chunk_size - 1);
 	umem->size = size;
@@ -373,7 +358,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 
 	err = xdp_umem_account_pages(umem);
 	if (err)
-		goto out;
+		return err;
 
 	err = xdp_umem_pin_pages(umem);
 	if (err)
@@ -392,8 +377,6 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 
 out_account:
 	xdp_umem_unaccount_pages(umem);
-out:
-	put_pid(umem->pid);
 	return err;
 }
 

commit 375ca548f7e3ac82acdd0959eddd1fa0e17c35cc
Merge: 58066ac9d7f5 40e196a906d9
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Feb 20 00:34:07 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Two easily resolvable overlapping change conflicts, one in
    TCP and one in the eBPF verifier.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 1e405c1a3f667bf152905127b94e9c8f454a343e
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Tue Feb 12 08:51:14 2019 +0100

    xsk: do not remove umem from netdevice on fall-back to copy-mode
    
    Commit c9b47cc1fabc ("xsk: fix bug when trying to use both copy and
    zero-copy on one queue id") stores the umem into the netdev._rx
    struct. However, the patch incorrectly removed the umem from the
    netdev._rx struct when user-space passed "best-effort" mode
    (i.e. select the fastest possible option available), and zero-copy
    mode was not available. This commit fixes that.
    
    Fixes: c9b47cc1fabc ("xsk: fix bug when trying to use both copy and zero-copy on one queue id")
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 597866e7c441..37e1fe180769 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -125,9 +125,10 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	return 0;
 
 err_unreg_umem:
-	xdp_clear_umem_at_qid(dev, queue_id);
 	if (!force_zc)
 		err = 0; /* fallback to copy mode */
+	if (err)
+		xdp_clear_umem_at_qid(dev, queue_id);
 out_rtnl_unlock:
 	rtnl_unlock();
 	return err;

commit e451eb51068496054d071af10b3530af4002a4f4
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Mon Feb 11 08:15:29 2019 -0800

    xsk: share the mmap_sem for page pinning
    
    Holding mmap_sem exclusively for a gup() is an overkill. Lets
    share the lock and replace the gup call for gup_longterm(), as
    it is better suited for the lifetime of the pinning.
    
    Fixes: c0c77d8fb787 ("xsk: add user memory registration support sockopt")
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Bjorn Topel <bjorn.topel@intel.com>
    Cc: Magnus Karlsson <magnus.karlsson@intel.com>
    CC: netdev@vger.kernel.org
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index d4de871e7d4d..597866e7c441 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -259,10 +259,10 @@ static int xdp_umem_pin_pages(struct xdp_umem *umem)
 	if (!umem->pgs)
 		return -ENOMEM;
 
-	down_write(&current->mm->mmap_sem);
-	npgs = get_user_pages(umem->address, umem->npgs,
-			      gup_flags, &umem->pgs[0], NULL);
-	up_write(&current->mm->mmap_sem);
+	down_read(&current->mm->mmap_sem);
+	npgs = get_user_pages_longterm(umem->address, umem->npgs,
+				       gup_flags, &umem->pgs[0], NULL);
+	up_read(&current->mm->mmap_sem);
 
 	if (npgs != umem->npgs) {
 		if (npgs >= 0) {

commit ec7146db150082737cbfeacaae0f33e42c95cf18
Merge: 343917b410ba 3d2af27a84a8
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jan 28 19:38:33 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2019-01-29
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    The main changes are:
    
    1) Teach verifier dead code removal, this also allows for optimizing /
       removing conditional branches around dead code and to shrink the
       resulting image. Code store constrained architectures like nfp would
       have hard time doing this at JIT level, from Jakub.
    
    2) Add JMP32 instructions to BPF ISA in order to allow for optimizing
       code generation for 32-bit sub-registers. Evaluation shows that this
       can result in code reduction of ~5-20% compared to 64 bit-only code
       generation. Also add implementation for most JITs, from Jiong.
    
    3) Add support for __int128 types in BTF which is also needed for
       vmlinux's BTF conversion to work, from Yonghong.
    
    4) Add a new command to bpftool in order to dump a list of BPF-related
       parameters from the system or for a specific network device e.g. in
       terms of available prog/map types or helper functions, from Quentin.
    
    5) Add AF_XDP sock_diag interface for querying sockets from user
       space which provides information about the RX/TX/fill/completion
       rings, umem, memory usage etc, from Björn.
    
    6) Add skb context access for skb_shared_info->gso_segs field, from Eric.
    
    7) Add support for testing flow dissector BPF programs by extending
       existing BPF_PROG_TEST_RUN infrastructure, from Stanislav.
    
    8) Split BPF kselftest's test_verifier into various subgroups of tests
       in order better deal with merge conflicts in this area, from Jakub.
    
    9) Add support for queue/stack manipulations in bpftool, from Stanislav.
    
    10) Document BTF, from Yonghong.
    
    11) Dump supported ELF section names in libbpf on program load
        failure, from Taeung.
    
    12) Silence a false positive compiler warning in verifier's BTF
        handling, from Peter.
    
    13) Fix help string in bpftool's feature probing, from Prashant.
    
    14) Remove duplicate includes in BPF kselftests, from Yue.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 50e74c0131a5b3a3e387798a5705158c04fb3bd0
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Thu Jan 24 19:59:38 2019 +0100

    xsk: add id to umem
    
    This commit adds an id to the umem structure. The id uniquely
    identifies a umem instance, and will be exposed to user-space via the
    socket monitoring interface.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index a264cf2accd0..eabdb0f59031 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -13,12 +13,15 @@
 #include <linux/mm.h>
 #include <linux/netdevice.h>
 #include <linux/rtnetlink.h>
+#include <linux/idr.h>
 
 #include "xdp_umem.h"
 #include "xsk_queue.h"
 
 #define XDP_UMEM_MIN_CHUNK_SIZE 2048
 
+static DEFINE_IDA(umem_ida);
+
 void xdp_add_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 {
 	unsigned long flags;
@@ -183,6 +186,8 @@ static void xdp_umem_release(struct xdp_umem *umem)
 
 	xdp_umem_clear_dev(umem);
 
+	ida_simple_remove(&umem_ida, umem->id);
+
 	if (umem->fq) {
 		xskq_destroy(umem->fq);
 		umem->fq = NULL;
@@ -389,8 +394,16 @@ struct xdp_umem *xdp_umem_create(struct xdp_umem_reg *mr)
 	if (!umem)
 		return ERR_PTR(-ENOMEM);
 
+	err = ida_simple_get(&umem_ida, 0, 0, GFP_KERNEL);
+	if (err < 0) {
+		kfree(umem);
+		return ERR_PTR(err);
+	}
+	umem->id = err;
+
 	err = xdp_umem_reg(umem, mr);
 	if (err) {
+		ida_simple_remove(&umem_ida, umem->id);
 		kfree(umem);
 		return ERR_PTR(err);
 	}

commit 5f4f3b2d19c0e3a6cb9ed4aa11fed6fbb13d1aa0
Author: Jan Sokolowski <jan.sokolowski@intel.com>
Date:   Tue Dec 18 13:45:13 2018 +0000

    xsk: export xdp_get_umem_from_qid
    
    Export xdp_get_umem_from_qid for other modules to use.
    
    Signed-off-by: Jan Sokolowski <jan.sokolowski@intel.com>
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index d4de871e7d4d..d74dfdc106b5 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -67,6 +67,7 @@ struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
 
 	return NULL;
 }
+EXPORT_SYMBOL(xdp_get_umem_from_qid);
 
 static void xdp_clear_umem_at_qid(struct net_device *dev, u16 queue_id)
 {

commit cc5b5d3565048ae57d14e5674a5fb085b2ab0193
Author: Krzysztof Kazimierczak <krzysztof.kazimierczak@intel.com>
Date:   Thu Jan 10 20:29:02 2019 +0100

    xsk: Check if a queue exists during umem setup
    
    In the xdp_umem_assign_dev() path, the xsk code does not
    check if a queue for which umem is to be created exists.
    It leads to a situation where umem is not assigned to any
    Tx/Rx queue of a netdevice, without notifying the stack
    about an error. This affects both XDP_SKB and XDP_DRV
    modes - in case of XDP_DRV_ZC, queue index is checked by
    the driver.
    
    This patch fixes xsk code, so that in both XDP_SKB and
    XDP_DRV mode of AF_XDP, an error is returned when requested
    queue index exceedes an existing maximum.
    
    Fixes: c9b47cc1fabca ("xsk: fix bug when trying to use both copy and zero-copy on one queue id")
    Reported-by: Jakub Spizewski <jakub.spizewski@intel.com>
    Signed-off-by: Krzysztof Kazimierczak <krzysztof.kazimierczak@intel.com>
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index a264cf2accd0..d4de871e7d4d 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -41,13 +41,20 @@ void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
  * not know if the device has more tx queues than rx, or the opposite.
  * This might also change during run time.
  */
-static void xdp_reg_umem_at_qid(struct net_device *dev, struct xdp_umem *umem,
-				u16 queue_id)
+static int xdp_reg_umem_at_qid(struct net_device *dev, struct xdp_umem *umem,
+			       u16 queue_id)
 {
+	if (queue_id >= max_t(unsigned int,
+			      dev->real_num_rx_queues,
+			      dev->real_num_tx_queues))
+		return -EINVAL;
+
 	if (queue_id < dev->real_num_rx_queues)
 		dev->_rx[queue_id].umem = umem;
 	if (queue_id < dev->real_num_tx_queues)
 		dev->_tx[queue_id].umem = umem;
+
+	return 0;
 }
 
 struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
@@ -88,7 +95,10 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 		goto out_rtnl_unlock;
 	}
 
-	xdp_reg_umem_at_qid(dev, umem, queue_id);
+	err = xdp_reg_umem_at_qid(dev, umem, queue_id);
+	if (err)
+		goto out_rtnl_unlock;
+
 	umem->dev = dev;
 	umem->queue_id = queue_id;
 	if (force_copy)

commit 541d7fdd7694560404c502f64298a90ffe017e6b
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Fri Oct 5 13:25:15 2018 +0200

    xsk: proper AF_XDP socket teardown ordering
    
    The AF_XDP socket struct can exist in three different, implicit
    states: setup, bound and released. Setup is prior the socket has been
    bound to a device. Bound is when the socket is active for receive and
    send. Released is when the process/userspace side of the socket is
    released, but the sock object is still lingering, e.g. when there is a
    reference to the socket in an XSKMAP after process termination.
    
    The Rx fast-path code uses the "dev" member of struct xdp_sock to
    check whether a socket is bound or relased, and the Tx code uses the
    struct xdp_umem "xsk_list" member in conjunction with "dev" to
    determine the state of a socket.
    
    However, the transition from bound to released did not tear the socket
    down in correct order.
    
    On the Rx side "dev" was cleared after synchronize_net() making the
    synchronization useless. On the Tx side, the internal queues were
    destroyed prior removing them from the "xsk_list".
    
    This commit corrects the cleanup order, and by doing so
    xdp_del_sk_umem() can be simplified and one synchronize_net() can be
    removed.
    
    Fixes: 965a99098443 ("xsk: add support for bind for Rx")
    Fixes: ac98d8aab61b ("xsk: wire upp Tx zero-copy functions")
    Reported-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index c6007c58231c..a264cf2accd0 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -32,14 +32,9 @@ void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 {
 	unsigned long flags;
 
-	if (xs->dev) {
-		spin_lock_irqsave(&umem->xsk_list_lock, flags);
-		list_del_rcu(&xs->list);
-		spin_unlock_irqrestore(&umem->xsk_list_lock, flags);
-
-		if (umem->zc)
-			synchronize_net();
-	}
+	spin_lock_irqsave(&umem->xsk_list_lock, flags);
+	list_del_rcu(&xs->list);
+	spin_unlock_irqrestore(&umem->xsk_list_lock, flags);
 }
 
 /* The umem is stored both in the _rx struct and the _tx struct as we do

commit a41b4f3c58dddcb39b7072a97ac8cb9036ce53f6
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Mon Oct 1 14:51:37 2018 +0200

    xsk: simplify xdp_clear_umem_at_qid implementation
    
    As we now do not allow ethtool to deactivate the queue id we are
    running an AF_XDP socket on, we can simplify the implementation of
    xdp_clear_umem_at_qid().
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 773326f682b1..c6007c58231c 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -68,12 +68,9 @@ struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
 
 static void xdp_clear_umem_at_qid(struct net_device *dev, u16 queue_id)
 {
-	/* Zero out the entry independent on how many queues are configured
-	 * at this point in time, as it might be used in the future.
-	 */
-	if (queue_id < dev->num_rx_queues)
+	if (queue_id < dev->real_num_rx_queues)
 		dev->_rx[queue_id].umem = NULL;
-	if (queue_id < dev->num_tx_queues)
+	if (queue_id < dev->real_num_tx_queues)
 		dev->_tx[queue_id].umem = NULL;
 }
 

commit 1661d346628115c364e2b7d5b15a64ca3bd0dbd4
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Oct 1 14:51:36 2018 +0200

    ethtool: don't allow disabling queues with umem installed
    
    We already check the RSS indirection table does not use queues which
    would be disabled by channel reconfiguration. Make sure user does not
    try to disable queues which have a UMEM and zero-copy AF_XDP socket
    installed.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 4d6c6652f5d1..773326f682b1 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -55,8 +55,8 @@ static void xdp_reg_umem_at_qid(struct net_device *dev, struct xdp_umem *umem,
 		dev->_tx[queue_id].umem = umem;
 }
 
-static struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
-					      u16 queue_id)
+struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
+				       u16 queue_id)
 {
 	if (queue_id < dev->real_num_rx_queues)
 		return dev->_rx[queue_id].umem;

commit c9b47cc1fabca533fb8ebaeb417778036c1ce27d
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Mon Oct 1 14:51:34 2018 +0200

    xsk: fix bug when trying to use both copy and zero-copy on one queue id
    
    Previously, the xsk code did not record which umem was bound to a
    specific queue id. This was not required if all drivers were zero-copy
    enabled as this had to be recorded in the driver anyway. So if a user
    tried to bind two umems to the same queue, the driver would say
    no. But if copy-mode was first enabled and then zero-copy mode (or the
    reverse order), we mistakenly enabled both of them on the same umem
    leading to buggy behavior. The main culprit for this is that we did
    not store the association of umem to queue id in the copy case and
    only relied on the driver reporting this. As this relation was not
    stored in the driver for copy mode (it does not rely on the AF_XDP
    NDOs), this obviously could not work.
    
    This patch fixes the problem by always recording the umem to queue id
    relationship in the netdev_queue and netdev_rx_queue structs. This way
    we always know what kind of umem has been bound to a queue id and can
    act appropriately at bind time.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 555427b3e0fe..4d6c6652f5d1 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -42,27 +42,47 @@ void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 	}
 }
 
-int xdp_umem_query(struct net_device *dev, u16 queue_id)
+/* The umem is stored both in the _rx struct and the _tx struct as we do
+ * not know if the device has more tx queues than rx, or the opposite.
+ * This might also change during run time.
+ */
+static void xdp_reg_umem_at_qid(struct net_device *dev, struct xdp_umem *umem,
+				u16 queue_id)
 {
-	struct netdev_bpf bpf;
+	if (queue_id < dev->real_num_rx_queues)
+		dev->_rx[queue_id].umem = umem;
+	if (queue_id < dev->real_num_tx_queues)
+		dev->_tx[queue_id].umem = umem;
+}
 
-	ASSERT_RTNL();
+static struct xdp_umem *xdp_get_umem_from_qid(struct net_device *dev,
+					      u16 queue_id)
+{
+	if (queue_id < dev->real_num_rx_queues)
+		return dev->_rx[queue_id].umem;
+	if (queue_id < dev->real_num_tx_queues)
+		return dev->_tx[queue_id].umem;
 
-	memset(&bpf, 0, sizeof(bpf));
-	bpf.command = XDP_QUERY_XSK_UMEM;
-	bpf.xsk.queue_id = queue_id;
+	return NULL;
+}
 
-	if (!dev->netdev_ops->ndo_bpf)
-		return 0;
-	return dev->netdev_ops->ndo_bpf(dev, &bpf) ?: !!bpf.xsk.umem;
+static void xdp_clear_umem_at_qid(struct net_device *dev, u16 queue_id)
+{
+	/* Zero out the entry independent on how many queues are configured
+	 * at this point in time, as it might be used in the future.
+	 */
+	if (queue_id < dev->num_rx_queues)
+		dev->_rx[queue_id].umem = NULL;
+	if (queue_id < dev->num_tx_queues)
+		dev->_tx[queue_id].umem = NULL;
 }
 
 int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
-			u32 queue_id, u16 flags)
+			u16 queue_id, u16 flags)
 {
 	bool force_zc, force_copy;
 	struct netdev_bpf bpf;
-	int err;
+	int err = 0;
 
 	force_zc = flags & XDP_ZEROCOPY;
 	force_copy = flags & XDP_COPY;
@@ -70,17 +90,23 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	if (force_zc && force_copy)
 		return -EINVAL;
 
-	if (force_copy)
-		return 0;
+	rtnl_lock();
+	if (xdp_get_umem_from_qid(dev, queue_id)) {
+		err = -EBUSY;
+		goto out_rtnl_unlock;
+	}
 
-	if (!dev->netdev_ops->ndo_bpf || !dev->netdev_ops->ndo_xsk_async_xmit)
-		return force_zc ? -EOPNOTSUPP : 0; /* fail or fallback */
+	xdp_reg_umem_at_qid(dev, umem, queue_id);
+	umem->dev = dev;
+	umem->queue_id = queue_id;
+	if (force_copy)
+		/* For copy-mode, we are done. */
+		goto out_rtnl_unlock;
 
-	rtnl_lock();
-	err = xdp_umem_query(dev, queue_id);
-	if (err) {
-		err = err < 0 ? -EOPNOTSUPP : -EBUSY;
-		goto err_rtnl_unlock;
+	if (!dev->netdev_ops->ndo_bpf ||
+	    !dev->netdev_ops->ndo_xsk_async_xmit) {
+		err = -EOPNOTSUPP;
+		goto err_unreg_umem;
 	}
 
 	bpf.command = XDP_SETUP_XSK_UMEM;
@@ -89,18 +115,20 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 
 	err = dev->netdev_ops->ndo_bpf(dev, &bpf);
 	if (err)
-		goto err_rtnl_unlock;
+		goto err_unreg_umem;
 	rtnl_unlock();
 
 	dev_hold(dev);
-	umem->dev = dev;
-	umem->queue_id = queue_id;
 	umem->zc = true;
 	return 0;
 
-err_rtnl_unlock:
+err_unreg_umem:
+	xdp_clear_umem_at_qid(dev, queue_id);
+	if (!force_zc)
+		err = 0; /* fallback to copy mode */
+out_rtnl_unlock:
 	rtnl_unlock();
-	return force_zc ? err : 0; /* fail or fallback */
+	return err;
 }
 
 static void xdp_umem_clear_dev(struct xdp_umem *umem)
@@ -108,7 +136,7 @@ static void xdp_umem_clear_dev(struct xdp_umem *umem)
 	struct netdev_bpf bpf;
 	int err;
 
-	if (umem->dev) {
+	if (umem->zc) {
 		bpf.command = XDP_SETUP_XSK_UMEM;
 		bpf.xsk.umem = NULL;
 		bpf.xsk.queue_id = umem->queue_id;
@@ -119,9 +147,17 @@ static void xdp_umem_clear_dev(struct xdp_umem *umem)
 
 		if (err)
 			WARN(1, "failed to disable umem!\n");
+	}
+
+	if (umem->dev) {
+		rtnl_lock();
+		xdp_clear_umem_at_qid(umem->dev, umem->queue_id);
+		rtnl_unlock();
+	}
 
+	if (umem->zc) {
 		dev_put(umem->dev);
-		umem->dev = NULL;
+		umem->zc = false;
 	}
 }
 

commit f5bd91388e26557f64ca999e0006038c7a919308
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Fri Sep 7 10:18:46 2018 +0200

    net: xsk: add a simple buffer reuse queue
    
    XSK UMEM is strongly single producer single consumer so reuse of
    frames is challenging.  Add a simple "stash" of FILL packets to
    reuse for drivers to optionally make use of.  This is useful
    when driver has to free (ndo_stop) or resize a ring with an active
    AF_XDP ZC socket.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index b3b632c5aeae..555427b3e0fe 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -165,6 +165,8 @@ static void xdp_umem_release(struct xdp_umem *umem)
 		umem->cq = NULL;
 	}
 
+	xsk_reuseq_destroy(umem);
+
 	xdp_umem_unpin_pages(umem);
 
 	task = get_pid_task(umem->pid, PIDTYPE_PID);

commit 93ee30f3e8b412c5fc2d2f7d9d002529d9a209ad
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Fri Aug 31 13:40:02 2018 +0200

    xsk: i40e: get rid of useless struct xdp_umem_props
    
    This commit gets rid of the structure xdp_umem_props. It was there to
    be able to break a dependency at one point, but this is no longer
    needed. The values in the struct are instead stored directly in the
    xdp_umem structure. This simplifies the xsk code as well as af_xdp
    zero-copy drivers and as a bonus gets rid of one internal header file.
    
    The i40e driver is also adapted to the new interface in this commit.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index d179732617dc..b3b632c5aeae 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -312,8 +312,8 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 
 	umem->pid = get_task_pid(current, PIDTYPE_PID);
 	umem->address = (unsigned long)addr;
-	umem->props.chunk_mask = ~((u64)chunk_size - 1);
-	umem->props.size = size;
+	umem->chunk_mask = ~((u64)chunk_size - 1);
+	umem->size = size;
 	umem->headroom = headroom;
 	umem->chunk_size_nohr = chunk_size - headroom;
 	umem->npgs = size / PAGE_SIZE;

commit a29c8bb640ca13f5731510ceaf65a6e1fe6055c8
Author: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
Date:   Fri Aug 31 09:59:16 2018 +0900

    xsk: remove unnecessary assignment
    
    Since xdp_umem_query() was added one assignment of bpf.command was
    missed from cleanup. Removing the assignment statement.
    
    Fixes: 84c6b86875e01a0 ("xsk: don't allow umem replace at stack level")
    Signed-off-by: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index bfe2dbea480b..d179732617dc 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -76,8 +76,6 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	if (!dev->netdev_ops->ndo_bpf || !dev->netdev_ops->ndo_xsk_async_xmit)
 		return force_zc ? -EOPNOTSUPP : 0; /* fail or fallback */
 
-	bpf.command = XDP_QUERY_XSK_UMEM;
-
 	rtnl_lock();
 	err = xdp_umem_query(dev, queue_id);
 	if (err) {

commit 96c26e04581667e3cd17ed74c2fc3499afea49b8
Author: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
Date:   Mon Aug 20 09:54:25 2018 +0900

    xsk: fix return value of xdp_umem_assign_dev()
    
    s/ENOTSUPP/EOPNOTSUPP/ in function umem_assign_dev().
    This function's return value is directly returned by xsk_bind().
    EOPNOTSUPP is bind()'s possible return value.
    
    Fixes: f734607e819b ("xsk: refactor xdp_umem_assign_dev()")
    Signed-off-by: Prashant Bhole <bhole_prashant_q7@lab.ntt.co.jp>
    Acked-by: Song Liu <songliubraving@fb.com>
    Acked-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 911ca6d3cb5a..bfe2dbea480b 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -74,14 +74,14 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 		return 0;
 
 	if (!dev->netdev_ops->ndo_bpf || !dev->netdev_ops->ndo_xsk_async_xmit)
-		return force_zc ? -ENOTSUPP : 0; /* fail or fallback */
+		return force_zc ? -EOPNOTSUPP : 0; /* fail or fallback */
 
 	bpf.command = XDP_QUERY_XSK_UMEM;
 
 	rtnl_lock();
 	err = xdp_umem_query(dev, queue_id);
 	if (err) {
-		err = err < 0 ? -ENOTSUPP : -EBUSY;
+		err = err < 0 ? -EOPNOTSUPP : -EBUSY;
 		goto err_rtnl_unlock;
 	}
 

commit 84c6b86875e01a08a0daa6fdd4a01b36bf0bf0b2
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jul 30 20:43:53 2018 -0700

    xsk: don't allow umem replace at stack level
    
    Currently drivers have to check if they already have a umem
    installed for a given queue and return an error if so.  Make
    better use of XDP_QUERY_XSK_UMEM and move this functionality
    to the core.
    
    We need to keep rtnl across the calls now.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Björn Töpel <bjorn.topel@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index c199d66b5f3f..911ca6d3cb5a 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -11,6 +11,8 @@
 #include <linux/slab.h>
 #include <linux/bpf.h>
 #include <linux/mm.h>
+#include <linux/netdevice.h>
+#include <linux/rtnetlink.h>
 
 #include "xdp_umem.h"
 #include "xsk_queue.h"
@@ -40,6 +42,21 @@ void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
 	}
 }
 
+int xdp_umem_query(struct net_device *dev, u16 queue_id)
+{
+	struct netdev_bpf bpf;
+
+	ASSERT_RTNL();
+
+	memset(&bpf, 0, sizeof(bpf));
+	bpf.command = XDP_QUERY_XSK_UMEM;
+	bpf.xsk.queue_id = queue_id;
+
+	if (!dev->netdev_ops->ndo_bpf)
+		return 0;
+	return dev->netdev_ops->ndo_bpf(dev, &bpf) ?: !!bpf.xsk.umem;
+}
+
 int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 			u32 queue_id, u16 flags)
 {
@@ -62,28 +79,30 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	bpf.command = XDP_QUERY_XSK_UMEM;
 
 	rtnl_lock();
-	err = dev->netdev_ops->ndo_bpf(dev, &bpf);
-	rtnl_unlock();
-
-	if (err)
-		return force_zc ? -ENOTSUPP : 0;
+	err = xdp_umem_query(dev, queue_id);
+	if (err) {
+		err = err < 0 ? -ENOTSUPP : -EBUSY;
+		goto err_rtnl_unlock;
+	}
 
 	bpf.command = XDP_SETUP_XSK_UMEM;
 	bpf.xsk.umem = umem;
 	bpf.xsk.queue_id = queue_id;
 
-	rtnl_lock();
 	err = dev->netdev_ops->ndo_bpf(dev, &bpf);
-	rtnl_unlock();
-
 	if (err)
-		return force_zc ? err : 0; /* fail or fallback */
+		goto err_rtnl_unlock;
+	rtnl_unlock();
 
 	dev_hold(dev);
 	umem->dev = dev;
 	umem->queue_id = queue_id;
 	umem->zc = true;
 	return 0;
+
+err_rtnl_unlock:
+	rtnl_unlock();
+	return force_zc ? err : 0; /* fail or fallback */
 }
 
 static void xdp_umem_clear_dev(struct xdp_umem *umem)

commit f734607e819b951bae3b436b026ec672082e9241
Author: Jakub Kicinski <jakub.kicinski@netronome.com>
Date:   Mon Jul 30 20:43:52 2018 -0700

    xsk: refactor xdp_umem_assign_dev()
    
    Return early and only take the ref on dev once there is no possibility
    of failing.
    
    Signed-off-by: Jakub Kicinski <jakub.kicinski@netronome.com>
    Reviewed-by: Quentin Monnet <quentin.monnet@netronome.com>
    Acked-by: Björn Töpel <bjorn.topel@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index f47abb46c587..c199d66b5f3f 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -56,41 +56,34 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	if (force_copy)
 		return 0;
 
-	dev_hold(dev);
+	if (!dev->netdev_ops->ndo_bpf || !dev->netdev_ops->ndo_xsk_async_xmit)
+		return force_zc ? -ENOTSUPP : 0; /* fail or fallback */
 
-	if (dev->netdev_ops->ndo_bpf && dev->netdev_ops->ndo_xsk_async_xmit) {
-		bpf.command = XDP_QUERY_XSK_UMEM;
+	bpf.command = XDP_QUERY_XSK_UMEM;
 
-		rtnl_lock();
-		err = dev->netdev_ops->ndo_bpf(dev, &bpf);
-		rtnl_unlock();
+	rtnl_lock();
+	err = dev->netdev_ops->ndo_bpf(dev, &bpf);
+	rtnl_unlock();
 
-		if (err) {
-			dev_put(dev);
-			return force_zc ? -ENOTSUPP : 0;
-		}
+	if (err)
+		return force_zc ? -ENOTSUPP : 0;
 
-		bpf.command = XDP_SETUP_XSK_UMEM;
-		bpf.xsk.umem = umem;
-		bpf.xsk.queue_id = queue_id;
+	bpf.command = XDP_SETUP_XSK_UMEM;
+	bpf.xsk.umem = umem;
+	bpf.xsk.queue_id = queue_id;
 
-		rtnl_lock();
-		err = dev->netdev_ops->ndo_bpf(dev, &bpf);
-		rtnl_unlock();
+	rtnl_lock();
+	err = dev->netdev_ops->ndo_bpf(dev, &bpf);
+	rtnl_unlock();
 
-		if (err) {
-			dev_put(dev);
-			return force_zc ? err : 0; /* fail or fallback */
-		}
-
-		umem->dev = dev;
-		umem->queue_id = queue_id;
-		umem->zc = true;
-		return 0;
-	}
+	if (err)
+		return force_zc ? err : 0; /* fail or fallback */
 
-	dev_put(dev);
-	return force_zc ? -ENOTSUPP : 0; /* fail or fallback */
+	dev_hold(dev);
+	umem->dev = dev;
+	umem->queue_id = queue_id;
+	umem->zc = true;
+	return 0;
 }
 
 static void xdp_umem_clear_dev(struct xdp_umem *umem)

commit a343993c518ce252b62ec00ac06bccfb1d17129d
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon Jun 11 13:57:12 2018 +0200

    xsk: silence warning on memory allocation failure
    
    syzkaller reported a warning from xdp_umem_pin_pages():
    
      WARNING: CPU: 1 PID: 4537 at mm/slab_common.c:996 kmalloc_slab+0x56/0x70 mm/slab_common.c:996
      ...
      __do_kmalloc mm/slab.c:3713 [inline]
      __kmalloc+0x25/0x760 mm/slab.c:3727
      kmalloc_array include/linux/slab.h:634 [inline]
      kcalloc include/linux/slab.h:645 [inline]
      xdp_umem_pin_pages net/xdp/xdp_umem.c:205 [inline]
      xdp_umem_reg net/xdp/xdp_umem.c:318 [inline]
      xdp_umem_create+0x5c9/0x10f0 net/xdp/xdp_umem.c:349
      xsk_setsockopt+0x443/0x550 net/xdp/xsk.c:531
      __sys_setsockopt+0x1bd/0x390 net/socket.c:1935
      __do_sys_setsockopt net/socket.c:1946 [inline]
      __se_sys_setsockopt net/socket.c:1943 [inline]
      __x64_sys_setsockopt+0xbe/0x150 net/socket.c:1943
      do_syscall_64+0x1b1/0x800 arch/x86/entry/common.c:287
      entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    This is a warning about attempting to allocate more than
    KMALLOC_MAX_SIZE memory. The request originates from userspace, and if
    the request is too big, the kernel is free to deny its allocation. In
    this patch, the failed allocation attempt is silenced with
    __GFP_NOWARN.
    
    Fixes: c0c77d8fb787 ("xsk: add user memory registration support sockopt")
    Reported-by: syzbot+4abadc5d69117b346506@syzkaller.appspotmail.com
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index b9ef487c4618..f47abb46c587 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -204,7 +204,8 @@ static int xdp_umem_pin_pages(struct xdp_umem *umem)
 	long npgs;
 	int err;
 
-	umem->pgs = kcalloc(umem->npgs, sizeof(*umem->pgs), GFP_KERNEL);
+	umem->pgs = kcalloc(umem->npgs, sizeof(*umem->pgs),
+			    GFP_KERNEL | __GFP_NOWARN);
 	if (!umem->pgs)
 		return -ENOMEM;
 

commit c09290c5637692a9bfe7740e4c5e693efff12810
Author: Daniel Borkmann <daniel@iogearbox.net>
Date:   Fri Jun 8 00:06:01 2018 +0200

    bpf, xdp: fix crash in xdp_umem_unaccount_pages
    
    syzkaller was able to trigger the following panic for AF_XDP:
    
      BUG: KASAN: null-ptr-deref in atomic64_sub include/asm-generic/atomic-instrumented.h:144 [inline]
      BUG: KASAN: null-ptr-deref in atomic_long_sub include/asm-generic/atomic-long.h:199 [inline]
      BUG: KASAN: null-ptr-deref in xdp_umem_unaccount_pages.isra.4+0x3d/0x80 net/xdp/xdp_umem.c:135
      Write of size 8 at addr 0000000000000060 by task syz-executor246/4527
    
      CPU: 1 PID: 4527 Comm: syz-executor246 Not tainted 4.17.0+ #89
      Hardware name: Google Google Compute Engine/Google Compute Engine, BIOS Google 01/01/2011
      Call Trace:
       __dump_stack lib/dump_stack.c:77 [inline]
       dump_stack+0x1b9/0x294 lib/dump_stack.c:113
       kasan_report_error mm/kasan/report.c:352 [inline]
       kasan_report.cold.7+0x6d/0x2fe mm/kasan/report.c:412
       check_memory_region_inline mm/kasan/kasan.c:260 [inline]
       check_memory_region+0x13e/0x1b0 mm/kasan/kasan.c:267
       kasan_check_write+0x14/0x20 mm/kasan/kasan.c:278
       atomic64_sub include/asm-generic/atomic-instrumented.h:144 [inline]
       atomic_long_sub include/asm-generic/atomic-long.h:199 [inline]
       xdp_umem_unaccount_pages.isra.4+0x3d/0x80 net/xdp/xdp_umem.c:135
       xdp_umem_reg net/xdp/xdp_umem.c:334 [inline]
       xdp_umem_create+0xd6c/0x10f0 net/xdp/xdp_umem.c:349
       xsk_setsockopt+0x443/0x550 net/xdp/xsk.c:531
       __sys_setsockopt+0x1bd/0x390 net/socket.c:1935
       __do_sys_setsockopt net/socket.c:1946 [inline]
       __se_sys_setsockopt net/socket.c:1943 [inline]
       __x64_sys_setsockopt+0xbe/0x150 net/socket.c:1943
       do_syscall_64+0x1b1/0x800 arch/x86/entry/common.c:287
       entry_SYSCALL_64_after_hwframe+0x49/0xbe
    
    In xdp_umem_reg() the call to xdp_umem_account_pages() passed
    with CAP_IPC_LOCK where we didn't need to end up charging rlimit
    on memlock for the current user and therefore umem->user continues
    to be NULL. Later on through fault injection syzkaller triggered
    a failure in either umem->pgs or umem->pages allocation such that
    we bail out and undo accounting in xdp_umem_unaccount_pages()
    where we eventually hit the panic since it tries to deref the
    umem->user.
    
    The code is pretty close to mm_account_pinned_pages() and
    mm_unaccount_pinned_pages() pair and potentially could reuse
    it even in a later cleanup, and it appears that the initial
    commit c0c77d8fb787 ("xsk: add user memory registration support
    sockopt") got this right while later follow-up introduced the
    bug via a49049ea2576 ("xsk: simplified umem setup").
    
    Fixes: a49049ea2576 ("xsk: simplified umem setup")
    Reported-by: syzbot+979217770b09ebf5c407@syzkaller.appspotmail.com
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 7eb4948a38d2..b9ef487c4618 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -132,8 +132,10 @@ static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 
 static void xdp_umem_unaccount_pages(struct xdp_umem *umem)
 {
-	atomic_long_sub(umem->npgs, &umem->user->locked_vm);
-	free_uid(umem->user);
+	if (umem->user) {
+		atomic_long_sub(umem->npgs, &umem->user->locked_vm);
+		free_uid(umem->user);
+	}
 }
 
 static void xdp_umem_release(struct xdp_umem *umem)

commit ac98d8aab61baf785eb8f099b36daf34fc76a70e
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Mon Jun 4 14:05:57 2018 +0200

    xsk: wire upp Tx zero-copy functions
    
    Here we add the functionality required to support zero-copy Tx, and
    also exposes various zero-copy related functions for the netdevs.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index f729d79b8d91..7eb4948a38d2 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -17,6 +17,29 @@
 
 #define XDP_UMEM_MIN_CHUNK_SIZE 2048
 
+void xdp_add_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&umem->xsk_list_lock, flags);
+	list_add_rcu(&xs->list, &umem->xsk_list);
+	spin_unlock_irqrestore(&umem->xsk_list_lock, flags);
+}
+
+void xdp_del_sk_umem(struct xdp_umem *umem, struct xdp_sock *xs)
+{
+	unsigned long flags;
+
+	if (xs->dev) {
+		spin_lock_irqsave(&umem->xsk_list_lock, flags);
+		list_del_rcu(&xs->list);
+		spin_unlock_irqrestore(&umem->xsk_list_lock, flags);
+
+		if (umem->zc)
+			synchronize_net();
+	}
+}
+
 int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 			u32 queue_id, u16 flags)
 {
@@ -35,7 +58,7 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 
 	dev_hold(dev);
 
-	if (dev->netdev_ops->ndo_bpf) {
+	if (dev->netdev_ops->ndo_bpf && dev->netdev_ops->ndo_xsk_async_xmit) {
 		bpf.command = XDP_QUERY_XSK_UMEM;
 
 		rtnl_lock();
@@ -70,7 +93,7 @@ int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
 	return force_zc ? -ENOTSUPP : 0; /* fail or fallback */
 }
 
-void xdp_umem_clear_dev(struct xdp_umem *umem)
+static void xdp_umem_clear_dev(struct xdp_umem *umem)
 {
 	struct netdev_bpf bpf;
 	int err;
@@ -283,6 +306,8 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	umem->npgs = size / PAGE_SIZE;
 	umem->pgs = NULL;
 	umem->user = NULL;
+	INIT_LIST_HEAD(&umem->xsk_list);
+	spin_lock_init(&umem->xsk_list_lock);
 
 	refcount_set(&umem->users, 1);
 

commit 173d3adb6f437037f216270955886ca9878187a5
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon Jun 4 14:05:55 2018 +0200

    xsk: add zero-copy support for Rx
    
    Extend the xsk_rcv to support the new MEM_TYPE_ZERO_COPY memory, and
    wireup ndo_bpf call in bind.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index aca826011f6c..f729d79b8d91 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -17,6 +17,81 @@
 
 #define XDP_UMEM_MIN_CHUNK_SIZE 2048
 
+int xdp_umem_assign_dev(struct xdp_umem *umem, struct net_device *dev,
+			u32 queue_id, u16 flags)
+{
+	bool force_zc, force_copy;
+	struct netdev_bpf bpf;
+	int err;
+
+	force_zc = flags & XDP_ZEROCOPY;
+	force_copy = flags & XDP_COPY;
+
+	if (force_zc && force_copy)
+		return -EINVAL;
+
+	if (force_copy)
+		return 0;
+
+	dev_hold(dev);
+
+	if (dev->netdev_ops->ndo_bpf) {
+		bpf.command = XDP_QUERY_XSK_UMEM;
+
+		rtnl_lock();
+		err = dev->netdev_ops->ndo_bpf(dev, &bpf);
+		rtnl_unlock();
+
+		if (err) {
+			dev_put(dev);
+			return force_zc ? -ENOTSUPP : 0;
+		}
+
+		bpf.command = XDP_SETUP_XSK_UMEM;
+		bpf.xsk.umem = umem;
+		bpf.xsk.queue_id = queue_id;
+
+		rtnl_lock();
+		err = dev->netdev_ops->ndo_bpf(dev, &bpf);
+		rtnl_unlock();
+
+		if (err) {
+			dev_put(dev);
+			return force_zc ? err : 0; /* fail or fallback */
+		}
+
+		umem->dev = dev;
+		umem->queue_id = queue_id;
+		umem->zc = true;
+		return 0;
+	}
+
+	dev_put(dev);
+	return force_zc ? -ENOTSUPP : 0; /* fail or fallback */
+}
+
+void xdp_umem_clear_dev(struct xdp_umem *umem)
+{
+	struct netdev_bpf bpf;
+	int err;
+
+	if (umem->dev) {
+		bpf.command = XDP_SETUP_XSK_UMEM;
+		bpf.xsk.umem = NULL;
+		bpf.xsk.queue_id = umem->queue_id;
+
+		rtnl_lock();
+		err = umem->dev->netdev_ops->ndo_bpf(umem->dev, &bpf);
+		rtnl_unlock();
+
+		if (err)
+			WARN(1, "failed to disable umem!\n");
+
+		dev_put(umem->dev);
+		umem->dev = NULL;
+	}
+}
+
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
 	unsigned int i;
@@ -43,6 +118,8 @@ static void xdp_umem_release(struct xdp_umem *umem)
 	struct task_struct *task;
 	struct mm_struct *mm;
 
+	xdp_umem_clear_dev(umem);
+
 	if (umem->fq) {
 		xskq_destroy(umem->fq);
 		umem->fq = NULL;

commit 8aef7340ae9695912a411886452ae9773206e845
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon Jun 4 14:05:52 2018 +0200

    xsk: introduce xdp_umem_page
    
    The xdp_umem_page holds the address for a page. Trade memory for
    faster lookup. Later, we'll add DMA address here as well.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 2793a503223e..aca826011f6c 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -65,6 +65,9 @@ static void xdp_umem_release(struct xdp_umem *umem)
 		goto out;
 
 	mmput(mm);
+	kfree(umem->pages);
+	umem->pages = NULL;
+
 	xdp_umem_unaccount_pages(umem);
 out:
 	kfree(umem);
@@ -155,7 +158,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
 	unsigned int chunks, chunks_per_page;
 	u64 addr = mr->addr, size = mr->len;
-	int size_chk, err;
+	int size_chk, err, i;
 
 	if (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {
 		/* Strictly speaking we could support this, if:
@@ -213,6 +216,16 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	err = xdp_umem_pin_pages(umem);
 	if (err)
 		goto out_account;
+
+	umem->pages = kcalloc(umem->npgs, sizeof(*umem->pages), GFP_KERNEL);
+	if (!umem->pages) {
+		err = -ENOMEM;
+		goto out_account;
+	}
+
+	for (i = 0; i < umem->npgs; i++)
+		umem->pages[i].addr = page_address(umem->pgs[i]);
+
 	return 0;
 
 out_account:

commit e61e62b9e2cc14b336f330f37f517f9d373ff31e
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon Jun 4 14:05:51 2018 +0200

    xsk: moved struct xdp_umem definition
    
    Moved struct xdp_umem to xdp_sock.h, in order to prepare for zero-copy
    support.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 9ad791ff4739..2793a503223e 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -13,6 +13,7 @@
 #include <linux/mm.h>
 
 #include "xdp_umem.h"
+#include "xsk_queue.h"
 
 #define XDP_UMEM_MIN_CHUNK_SIZE 2048
 

commit bbff2f321a864ee07c9d3d1245af498023146951
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon Jun 4 13:57:13 2018 +0200

    xsk: new descriptor addressing scheme
    
    Currently, AF_XDP only supports a fixed frame-size memory scheme where
    each frame is referenced via an index (idx). A user passes the frame
    index to the kernel, and the kernel acts upon the data.  Some NICs,
    however, do not have a fixed frame-size model, instead they have a
    model where a memory window is passed to the hardware and multiple
    frames are filled into that window (referred to as the "type-writer"
    model).
    
    By changing the descriptor format from the current frame index
    addressing scheme, AF_XDP can in the future be extended to support
    these kinds of NICs.
    
    In the index-based model, an idx refers to a frame of size
    frame_size. Addressing a frame in the UMEM is done by offseting the
    UMEM starting address by a global offset, idx * frame_size + offset.
    Communicating via the fill- and completion-rings are done by means of
    idx.
    
    In this commit, the idx is removed in favor of an address (addr),
    which is a relative address ranging over the UMEM. To convert an
    idx-based address to the new addr is simply: addr = idx * frame_size +
    offset.
    
    We also stop referring to the UMEM "frame" as a frame. Instead it is
    simply called a chunk.
    
    To transfer ownership of a chunk to the kernel, the addr of the chunk
    is passed in the fill-ring. Note, that the kernel will mask addr to
    make it chunk aligned, so there is no need for userspace to do
    that. E.g., for a chunk size of 2k, passing an addr of 2048, 2050 or
    3000 to the fill-ring will refer to the same chunk.
    
    On the completion-ring, the addr will match that of the Tx descriptor,
    passed to the kernel.
    
    Changing the descriptor format to use chunks/addr will allow for
    future changes to move to a type-writer based model, where multiple
    frames can reside in one chunk. In this model passing one single chunk
    into the fill-ring, would potentially result in multiple Rx
    descriptors.
    
    This commit changes the uapi of AF_XDP sockets, and updates the
    documentation.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 87998818116f..9ad791ff4739 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -14,7 +14,7 @@
 
 #include "xdp_umem.h"
 
-#define XDP_UMEM_MIN_FRAME_SIZE 2048
+#define XDP_UMEM_MIN_CHUNK_SIZE 2048
 
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
@@ -151,12 +151,12 @@ static int xdp_umem_account_pages(struct xdp_umem *umem)
 
 static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 {
-	u32 frame_size = mr->frame_size, frame_headroom = mr->frame_headroom;
+	u32 chunk_size = mr->chunk_size, headroom = mr->headroom;
+	unsigned int chunks, chunks_per_page;
 	u64 addr = mr->addr, size = mr->len;
-	unsigned int nframes, nfpp;
 	int size_chk, err;
 
-	if (frame_size < XDP_UMEM_MIN_FRAME_SIZE || frame_size > PAGE_SIZE) {
+	if (chunk_size < XDP_UMEM_MIN_CHUNK_SIZE || chunk_size > PAGE_SIZE) {
 		/* Strictly speaking we could support this, if:
 		 * - huge pages, or*
 		 * - using an IOMMU, or
@@ -166,7 +166,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 		return -EINVAL;
 	}
 
-	if (!is_power_of_2(frame_size))
+	if (!is_power_of_2(chunk_size))
 		return -EINVAL;
 
 	if (!PAGE_ALIGNED(addr)) {
@@ -179,33 +179,30 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if ((addr + size) < addr)
 		return -EINVAL;
 
-	nframes = (unsigned int)div_u64(size, frame_size);
-	if (nframes == 0 || nframes > UINT_MAX)
+	chunks = (unsigned int)div_u64(size, chunk_size);
+	if (chunks == 0)
 		return -EINVAL;
 
-	nfpp = PAGE_SIZE / frame_size;
-	if (nframes < nfpp || nframes % nfpp)
+	chunks_per_page = PAGE_SIZE / chunk_size;
+	if (chunks < chunks_per_page || chunks % chunks_per_page)
 		return -EINVAL;
 
-	frame_headroom = ALIGN(frame_headroom, 64);
+	headroom = ALIGN(headroom, 64);
 
-	size_chk = frame_size - frame_headroom - XDP_PACKET_HEADROOM;
+	size_chk = chunk_size - headroom - XDP_PACKET_HEADROOM;
 	if (size_chk < 0)
 		return -EINVAL;
 
 	umem->pid = get_task_pid(current, PIDTYPE_PID);
-	umem->size = (size_t)size;
 	umem->address = (unsigned long)addr;
-	umem->props.frame_size = frame_size;
-	umem->props.nframes = nframes;
-	umem->frame_headroom = frame_headroom;
+	umem->props.chunk_mask = ~((u64)chunk_size - 1);
+	umem->props.size = size;
+	umem->headroom = headroom;
+	umem->chunk_size_nohr = chunk_size - headroom;
 	umem->npgs = size / PAGE_SIZE;
 	umem->pgs = NULL;
 	umem->user = NULL;
 
-	umem->frame_size_log2 = ilog2(frame_size);
-	umem->nfpp_mask = nfpp - 1;
-	umem->nfpplog2 = ilog2(nfpp);
 	refcount_set(&umem->users, 1);
 
 	err = xdp_umem_account_pages(umem);

commit d3b42f1422d9c050bf5a2c660c045af2ab5d3e72
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Tue May 22 09:35:03 2018 +0200

    xsk: convert atomic_t to refcount_t
    
    Introduce refcount_t, in favor of atomic_t.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index faa6ffbaf6ab..87998818116f 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -78,7 +78,7 @@ static void xdp_umem_release_deferred(struct work_struct *work)
 
 void xdp_get_umem(struct xdp_umem *umem)
 {
-	atomic_inc(&umem->users);
+	refcount_inc(&umem->users);
 }
 
 void xdp_put_umem(struct xdp_umem *umem)
@@ -86,7 +86,7 @@ void xdp_put_umem(struct xdp_umem *umem)
 	if (!umem)
 		return;
 
-	if (atomic_dec_and_test(&umem->users)) {
+	if (refcount_dec_and_test(&umem->users)) {
 		INIT_WORK(&umem->work, xdp_umem_release_deferred);
 		schedule_work(&umem->work);
 	}
@@ -206,7 +206,7 @@ static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	umem->frame_size_log2 = ilog2(frame_size);
 	umem->nfpp_mask = nfpp - 1;
 	umem->nfpplog2 = ilog2(nfpp);
-	atomic_set(&umem->users, 1);
+	refcount_set(&umem->users, 1);
 
 	err = xdp_umem_account_pages(umem);
 	if (err)

commit a49049ea257656f27ffe424224f4a362b8b1234a
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Tue May 22 09:35:02 2018 +0200

    xsk: simplified umem setup
    
    As suggested by Daniel Borkmann, the umem setup code was a too
    defensive and complex. Here, we reduce the number of checks. Also, the
    memory pinning is now folded into the umem creation, and we do correct
    locking.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index c47909c74899..faa6ffbaf6ab 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -16,39 +16,25 @@
 
 #define XDP_UMEM_MIN_FRAME_SIZE 2048
 
-int xdp_umem_create(struct xdp_umem **umem)
-{
-	*umem = kzalloc(sizeof(**umem), GFP_KERNEL);
-
-	if (!*umem)
-		return -ENOMEM;
-
-	return 0;
-}
-
 static void xdp_umem_unpin_pages(struct xdp_umem *umem)
 {
 	unsigned int i;
 
-	if (umem->pgs) {
-		for (i = 0; i < umem->npgs; i++) {
-			struct page *page = umem->pgs[i];
-
-			set_page_dirty_lock(page);
-			put_page(page);
-		}
+	for (i = 0; i < umem->npgs; i++) {
+		struct page *page = umem->pgs[i];
 
-		kfree(umem->pgs);
-		umem->pgs = NULL;
+		set_page_dirty_lock(page);
+		put_page(page);
 	}
+
+	kfree(umem->pgs);
+	umem->pgs = NULL;
 }
 
 static void xdp_umem_unaccount_pages(struct xdp_umem *umem)
 {
-	if (umem->user) {
-		atomic_long_sub(umem->npgs, &umem->user->locked_vm);
-		free_uid(umem->user);
-	}
+	atomic_long_sub(umem->npgs, &umem->user->locked_vm);
+	free_uid(umem->user);
 }
 
 static void xdp_umem_release(struct xdp_umem *umem)
@@ -66,22 +52,18 @@ static void xdp_umem_release(struct xdp_umem *umem)
 		umem->cq = NULL;
 	}
 
-	if (umem->pgs) {
-		xdp_umem_unpin_pages(umem);
-
-		task = get_pid_task(umem->pid, PIDTYPE_PID);
-		put_pid(umem->pid);
-		if (!task)
-			goto out;
-		mm = get_task_mm(task);
-		put_task_struct(task);
-		if (!mm)
-			goto out;
+	xdp_umem_unpin_pages(umem);
 
-		mmput(mm);
-		umem->pgs = NULL;
-	}
+	task = get_pid_task(umem->pid, PIDTYPE_PID);
+	put_pid(umem->pid);
+	if (!task)
+		goto out;
+	mm = get_task_mm(task);
+	put_task_struct(task);
+	if (!mm)
+		goto out;
 
+	mmput(mm);
 	xdp_umem_unaccount_pages(umem);
 out:
 	kfree(umem);
@@ -167,16 +149,13 @@ static int xdp_umem_account_pages(struct xdp_umem *umem)
 	return 0;
 }
 
-int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
+static int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 {
 	u32 frame_size = mr->frame_size, frame_headroom = mr->frame_headroom;
 	u64 addr = mr->addr, size = mr->len;
 	unsigned int nframes, nfpp;
 	int size_chk, err;
 
-	if (!umem)
-		return -EINVAL;
-
 	if (frame_size < XDP_UMEM_MIN_FRAME_SIZE || frame_size > PAGE_SIZE) {
 		/* Strictly speaking we could support this, if:
 		 * - huge pages, or*
@@ -245,6 +224,24 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	return err;
 }
 
+struct xdp_umem *xdp_umem_create(struct xdp_umem_reg *mr)
+{
+	struct xdp_umem *umem;
+	int err;
+
+	umem = kzalloc(sizeof(*umem), GFP_KERNEL);
+	if (!umem)
+		return ERR_PTR(-ENOMEM);
+
+	err = xdp_umem_reg(umem, mr);
+	if (err) {
+		kfree(umem);
+		return ERR_PTR(err);
+	}
+
+	return umem;
+}
+
 bool xdp_umem_validate_queues(struct xdp_umem *umem)
 {
 	return umem->fq && umem->cq;

commit da60cf00c1a576a459defed2edfb88c858510b64
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Fri May 18 14:00:23 2018 +0200

    xsk: fixed some cases of unnecessary parentheses
    
    Removed some cases of unnecessary parentheses.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index df4ea97c433b..c47909c74899 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -20,7 +20,7 @@ int xdp_umem_create(struct xdp_umem **umem)
 {
 	*umem = kzalloc(sizeof(**umem), GFP_KERNEL);
 
-	if (!(*umem))
+	if (!*umem)
 		return -ENOMEM;
 
 	return 0;
@@ -247,5 +247,5 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 
 bool xdp_umem_validate_queues(struct xdp_umem *umem)
 {
-	return (umem->fq && umem->cq);
+	return umem->fq && umem->cq;
 }

commit dac09149d992995adbef0f472093fbb6940a8653
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Fri May 18 14:00:21 2018 +0200

    xsk: clean up SPDX headers
    
    Clean up SPDX-License-Identifier and removing licensing leftovers.
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 2b47a1dd7c6c..df4ea97c433b 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -1,15 +1,6 @@
 // SPDX-License-Identifier: GPL-2.0
 /* XDP user-space packet buffer
  * Copyright(c) 2018 Intel Corporation.
- *
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
  */
 
 #include <linux/init.h>

commit ea7e3435297c6ba3c8ae51db6ea48f1ed657dc5c
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Mon May 7 19:43:50 2018 +0200

    xsk: fix 64-bit division
    
    i386 builds report:
      net/xdp/xdp_umem.o: In function `xdp_umem_reg':
      xdp_umem.c:(.text+0x47e): undefined reference to `__udivdi3'
    
    This fix uses div_u64 instead of the GCC built-in.
    
    Fixes: c0c77d8fb787 ("xsk: add user memory registration support sockopt")
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Tested-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 881dfdefe235..2b47a1dd7c6c 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -209,7 +209,7 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	if ((addr + size) < addr)
 		return -EINVAL;
 
-	nframes = size / frame_size;
+	nframes = (unsigned int)div_u64(size, frame_size);
 	if (nframes == 0 || nframes > UINT_MAX)
 		return -EINVAL;
 

commit fe2308328cd2f26ebc986f543796e7d13ae00bc4
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Wed May 2 13:01:31 2018 +0200

    xsk: add umem completion queue support and mmap
    
    Here, we add another setsockopt for registered user memory (umem)
    called XDP_UMEM_COMPLETION_QUEUE. Using this socket option, the
    process can ask the kernel to allocate a queue (ring buffer) and also
    mmap it (XDP_UMEM_PGOFF_COMPLETION_QUEUE) into the process.
    
    The queue is used to explicitly pass ownership of umem frames from the
    kernel to user process. This will be used by the TX path to tell user
    space that a certain frame has been transmitted and user space can use
    it for something else, if it wishes.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index 9bac1ad570fa..881dfdefe235 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -70,6 +70,11 @@ static void xdp_umem_release(struct xdp_umem *umem)
 		umem->fq = NULL;
 	}
 
+	if (umem->cq) {
+		xskq_destroy(umem->cq);
+		umem->cq = NULL;
+	}
+
 	if (umem->pgs) {
 		xdp_umem_unpin_pages(umem);
 
@@ -251,5 +256,5 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 
 bool xdp_umem_validate_queues(struct xdp_umem *umem)
 {
-	return umem->fq;
+	return (umem->fq && umem->cq);
 }

commit 965a990984432cd01a9eb3514c64d86f56704295
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Wed May 2 13:01:26 2018 +0200

    xsk: add support for bind for Rx
    
    Here, the bind syscall is added. Binding an AF_XDP socket, means
    associating the socket to an umem, a netdev and a queue index. This
    can be done in two ways.
    
    The first way, creating a "socket from scratch". Create the umem using
    the XDP_UMEM_REG setsockopt and an associated fill queue with
    XDP_UMEM_FILL_QUEUE. Create the Rx queue using the XDP_RX_QUEUE
    setsockopt. Call bind passing ifindex and queue index ("channel" in
    ethtool speak).
    
    The second way to bind a socket, is simply skipping the
    umem/netdev/queue index, and passing another already setup AF_XDP
    socket. The new socket will then have the same umem/netdev/queue index
    as the parent so it will share the same umem. You must also set the
    flags field in the socket address to XDP_SHARED_UMEM.
    
    v2: Use PTR_ERR instead of passing error variable explicitly.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index e1f627d0cc1c..9bac1ad570fa 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -248,3 +248,8 @@ int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
 	put_pid(umem->pid);
 	return err;
 }
+
+bool xdp_umem_validate_queues(struct xdp_umem *umem)
+{
+	return umem->fq;
+}

commit 423f38329d267969130fb6f2c685f73d72687558
Author: Magnus Karlsson <magnus.karlsson@intel.com>
Date:   Wed May 2 13:01:24 2018 +0200

    xsk: add umem fill queue support and mmap
    
    Here, we add another setsockopt for registered user memory (umem)
    called XDP_UMEM_FILL_QUEUE. Using this socket option, the process can
    ask the kernel to allocate a queue (ring buffer) and also mmap it
    (XDP_UMEM_PGOFF_FILL_QUEUE) into the process.
    
    The queue is used to explicitly pass ownership of umem frames from the
    user process to the kernel. These frames will in a later patch be
    filled in with Rx packet data by the kernel.
    
    v2: Fixed potential crash in xsk_mmap.
    
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
index ec8b3552be44..e1f627d0cc1c 100644
--- a/net/xdp/xdp_umem.c
+++ b/net/xdp/xdp_umem.c
@@ -65,6 +65,11 @@ static void xdp_umem_release(struct xdp_umem *umem)
 	struct task_struct *task;
 	struct mm_struct *mm;
 
+	if (umem->fq) {
+		xskq_destroy(umem->fq);
+		umem->fq = NULL;
+	}
+
 	if (umem->pgs) {
 		xdp_umem_unpin_pages(umem);
 

commit c0c77d8fb787cfe0c3fca689c2a30d1dad4eaba7
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 2 13:01:23 2018 +0200

    xsk: add user memory registration support sockopt
    
    In this commit the base structure of the AF_XDP address family is set
    up. Further, we introduce the abilty register a window of user memory
    to the kernel via the XDP_UMEM_REG setsockopt syscall. The memory
    window is viewed by an AF_XDP socket as a set of equally large
    frames. After a user memory registration all frames are "owned" by the
    user application, and not the kernel.
    
    v2: More robust checks on umem creation and unaccount on error.
        Call set_page_dirty_lock on cleanup.
        Simplified xdp_umem_reg.
    
    Co-authored-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Magnus Karlsson <magnus.karlsson@intel.com>
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/net/xdp/xdp_umem.c b/net/xdp/xdp_umem.c
new file mode 100644
index 000000000000..ec8b3552be44
--- /dev/null
+++ b/net/xdp/xdp_umem.c
@@ -0,0 +1,245 @@
+// SPDX-License-Identifier: GPL-2.0
+/* XDP user-space packet buffer
+ * Copyright(c) 2018 Intel Corporation.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/init.h>
+#include <linux/sched/mm.h>
+#include <linux/sched/signal.h>
+#include <linux/sched/task.h>
+#include <linux/uaccess.h>
+#include <linux/slab.h>
+#include <linux/bpf.h>
+#include <linux/mm.h>
+
+#include "xdp_umem.h"
+
+#define XDP_UMEM_MIN_FRAME_SIZE 2048
+
+int xdp_umem_create(struct xdp_umem **umem)
+{
+	*umem = kzalloc(sizeof(**umem), GFP_KERNEL);
+
+	if (!(*umem))
+		return -ENOMEM;
+
+	return 0;
+}
+
+static void xdp_umem_unpin_pages(struct xdp_umem *umem)
+{
+	unsigned int i;
+
+	if (umem->pgs) {
+		for (i = 0; i < umem->npgs; i++) {
+			struct page *page = umem->pgs[i];
+
+			set_page_dirty_lock(page);
+			put_page(page);
+		}
+
+		kfree(umem->pgs);
+		umem->pgs = NULL;
+	}
+}
+
+static void xdp_umem_unaccount_pages(struct xdp_umem *umem)
+{
+	if (umem->user) {
+		atomic_long_sub(umem->npgs, &umem->user->locked_vm);
+		free_uid(umem->user);
+	}
+}
+
+static void xdp_umem_release(struct xdp_umem *umem)
+{
+	struct task_struct *task;
+	struct mm_struct *mm;
+
+	if (umem->pgs) {
+		xdp_umem_unpin_pages(umem);
+
+		task = get_pid_task(umem->pid, PIDTYPE_PID);
+		put_pid(umem->pid);
+		if (!task)
+			goto out;
+		mm = get_task_mm(task);
+		put_task_struct(task);
+		if (!mm)
+			goto out;
+
+		mmput(mm);
+		umem->pgs = NULL;
+	}
+
+	xdp_umem_unaccount_pages(umem);
+out:
+	kfree(umem);
+}
+
+static void xdp_umem_release_deferred(struct work_struct *work)
+{
+	struct xdp_umem *umem = container_of(work, struct xdp_umem, work);
+
+	xdp_umem_release(umem);
+}
+
+void xdp_get_umem(struct xdp_umem *umem)
+{
+	atomic_inc(&umem->users);
+}
+
+void xdp_put_umem(struct xdp_umem *umem)
+{
+	if (!umem)
+		return;
+
+	if (atomic_dec_and_test(&umem->users)) {
+		INIT_WORK(&umem->work, xdp_umem_release_deferred);
+		schedule_work(&umem->work);
+	}
+}
+
+static int xdp_umem_pin_pages(struct xdp_umem *umem)
+{
+	unsigned int gup_flags = FOLL_WRITE;
+	long npgs;
+	int err;
+
+	umem->pgs = kcalloc(umem->npgs, sizeof(*umem->pgs), GFP_KERNEL);
+	if (!umem->pgs)
+		return -ENOMEM;
+
+	down_write(&current->mm->mmap_sem);
+	npgs = get_user_pages(umem->address, umem->npgs,
+			      gup_flags, &umem->pgs[0], NULL);
+	up_write(&current->mm->mmap_sem);
+
+	if (npgs != umem->npgs) {
+		if (npgs >= 0) {
+			umem->npgs = npgs;
+			err = -ENOMEM;
+			goto out_pin;
+		}
+		err = npgs;
+		goto out_pgs;
+	}
+	return 0;
+
+out_pin:
+	xdp_umem_unpin_pages(umem);
+out_pgs:
+	kfree(umem->pgs);
+	umem->pgs = NULL;
+	return err;
+}
+
+static int xdp_umem_account_pages(struct xdp_umem *umem)
+{
+	unsigned long lock_limit, new_npgs, old_npgs;
+
+	if (capable(CAP_IPC_LOCK))
+		return 0;
+
+	lock_limit = rlimit(RLIMIT_MEMLOCK) >> PAGE_SHIFT;
+	umem->user = get_uid(current_user());
+
+	do {
+		old_npgs = atomic_long_read(&umem->user->locked_vm);
+		new_npgs = old_npgs + umem->npgs;
+		if (new_npgs > lock_limit) {
+			free_uid(umem->user);
+			umem->user = NULL;
+			return -ENOBUFS;
+		}
+	} while (atomic_long_cmpxchg(&umem->user->locked_vm, old_npgs,
+				     new_npgs) != old_npgs);
+	return 0;
+}
+
+int xdp_umem_reg(struct xdp_umem *umem, struct xdp_umem_reg *mr)
+{
+	u32 frame_size = mr->frame_size, frame_headroom = mr->frame_headroom;
+	u64 addr = mr->addr, size = mr->len;
+	unsigned int nframes, nfpp;
+	int size_chk, err;
+
+	if (!umem)
+		return -EINVAL;
+
+	if (frame_size < XDP_UMEM_MIN_FRAME_SIZE || frame_size > PAGE_SIZE) {
+		/* Strictly speaking we could support this, if:
+		 * - huge pages, or*
+		 * - using an IOMMU, or
+		 * - making sure the memory area is consecutive
+		 * but for now, we simply say "computer says no".
+		 */
+		return -EINVAL;
+	}
+
+	if (!is_power_of_2(frame_size))
+		return -EINVAL;
+
+	if (!PAGE_ALIGNED(addr)) {
+		/* Memory area has to be page size aligned. For
+		 * simplicity, this might change.
+		 */
+		return -EINVAL;
+	}
+
+	if ((addr + size) < addr)
+		return -EINVAL;
+
+	nframes = size / frame_size;
+	if (nframes == 0 || nframes > UINT_MAX)
+		return -EINVAL;
+
+	nfpp = PAGE_SIZE / frame_size;
+	if (nframes < nfpp || nframes % nfpp)
+		return -EINVAL;
+
+	frame_headroom = ALIGN(frame_headroom, 64);
+
+	size_chk = frame_size - frame_headroom - XDP_PACKET_HEADROOM;
+	if (size_chk < 0)
+		return -EINVAL;
+
+	umem->pid = get_task_pid(current, PIDTYPE_PID);
+	umem->size = (size_t)size;
+	umem->address = (unsigned long)addr;
+	umem->props.frame_size = frame_size;
+	umem->props.nframes = nframes;
+	umem->frame_headroom = frame_headroom;
+	umem->npgs = size / PAGE_SIZE;
+	umem->pgs = NULL;
+	umem->user = NULL;
+
+	umem->frame_size_log2 = ilog2(frame_size);
+	umem->nfpp_mask = nfpp - 1;
+	umem->nfpplog2 = ilog2(nfpp);
+	atomic_set(&umem->users, 1);
+
+	err = xdp_umem_account_pages(umem);
+	if (err)
+		goto out;
+
+	err = xdp_umem_pin_pages(umem);
+	if (err)
+		goto out_account;
+	return 0;
+
+out_account:
+	xdp_umem_unaccount_pages(umem);
+out:
+	put_pid(umem->pid);
+	return err;
+}
