commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index a2cf8a6236d6..13cf3f9b5938 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -1,10 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /* Xtables module to match packets using a BPF filter.
  * Copyright 2013 Google Inc.
  * Written by Willem de Bruijn <willemb@google.com>
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt

commit b26066447bb8599b393b2dd2bbeb68767e09ba07
Author: Florian Westphal <fw@strlen.de>
Date:   Fri Feb 9 15:52:07 2018 +0100

    netfilter: x_tables: use pr ratelimiting in all remaining spots
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index 06b090d8e901..a2cf8a6236d6 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -7,6 +7,8 @@
  * published by the Free Software Foundation.
  */
 
+#define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
+
 #include <linux/module.h>
 #include <linux/syscalls.h>
 #include <linux/skbuff.h>
@@ -34,7 +36,7 @@ static int __bpf_mt_check_bytecode(struct sock_filter *insns, __u16 len,
 	program.filter = insns;
 
 	if (bpf_prog_create(ret, &program)) {
-		pr_info("bpf: check failed: parse error\n");
+		pr_info_ratelimited("check failed: parse error\n");
 		return -EINVAL;
 	}
 

commit 75d4276e83a353d5e41da676f418d4a12f6f6e6c
Merge: 5b6c02f38315 040ee69226f8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 6 17:13:21 2018 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs fixes from Al Viro:
    
     - untangle sys_close() abuses in xt_bpf
    
     - deal with register_shrinker() failures in sget()
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      fix "netfilter: xt_bpf: Fix XT_BPF_MODE_FD_PINNED mode of 'xt_bpf_info_v1'"
      sget(): handle failures of register_shrinker()
      mm,vmscan: Make unregister_shrinker() no-op if register_shrinker() failed.

commit 040ee69226f8a96b7943645d68f41d5d44b5ff7d
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Dec 2 20:20:38 2017 -0500

    fix "netfilter: xt_bpf: Fix XT_BPF_MODE_FD_PINNED mode of 'xt_bpf_info_v1'"
    
    Descriptor table is a shared object; it's not a place where you can
    stick temporary references to files, especially when we don't need
    an opened file at all.
    
    Cc: stable@vger.kernel.org # v4.14
    Fixes: 98589a0998b8 ("netfilter: xt_bpf: Fix XT_BPF_MODE_FD_PINNED mode of 'xt_bpf_info_v1'")
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index 041da0d9c06f..fa2ca0a13619 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -52,18 +52,8 @@ static int __bpf_mt_check_fd(int fd, struct bpf_prog **ret)
 
 static int __bpf_mt_check_path(const char *path, struct bpf_prog **ret)
 {
-	mm_segment_t oldfs = get_fs();
-	int retval, fd;
-
-	set_fs(KERNEL_DS);
-	fd = bpf_obj_get_user(path, 0);
-	set_fs(oldfs);
-	if (fd < 0)
-		return fd;
-
-	retval = __bpf_mt_check_fd(fd, ret);
-	sys_close(fd);
-	return retval;
+	*ret = bpf_prog_get_type_path(path, BPF_PROG_TYPE_SOCKET_FILTER);
+	return PTR_ERR_OR_ZERO(*ret);
 }
 
 static int bpf_mt_check(const struct xt_mtchk_param *par)

commit 6ab405114b0b229151ef06f4e31c7834dd09d0c0
Author: Jann Horn <jannh@google.com>
Date:   Fri Dec 1 01:46:07 2017 +0100

    netfilter: xt_bpf: add overflow checks
    
    Check whether inputs from userspace are too long (explicit length field too
    big or string not null-terminated) to avoid out-of-bounds reads.
    
    As far as I can tell, this can at worst lead to very limited kernel heap
    memory disclosure or oopses.
    
    This bug can be triggered by an unprivileged user even if the xt_bpf module
    is not loaded: iptables is available in network namespaces, and the xt_bpf
    module can be autoloaded.
    
    Triggering the bug with a classic BPF filter with fake length 0x1000 causes
    the following KASAN report:
    
    ==================================================================
    BUG: KASAN: slab-out-of-bounds in bpf_prog_create+0x84/0xf0
    Read of size 32768 at addr ffff8801eff2c494 by task test/4627
    
    CPU: 0 PID: 4627 Comm: test Not tainted 4.15.0-rc1+ #1
    [...]
    Call Trace:
     dump_stack+0x5c/0x85
     print_address_description+0x6a/0x260
     kasan_report+0x254/0x370
     ? bpf_prog_create+0x84/0xf0
     memcpy+0x1f/0x50
     bpf_prog_create+0x84/0xf0
     bpf_mt_check+0x90/0xd6 [xt_bpf]
    [...]
    Allocated by task 4627:
     kasan_kmalloc+0xa0/0xd0
     __kmalloc_node+0x47/0x60
     xt_alloc_table_info+0x41/0x70 [x_tables]
    [...]
    The buggy address belongs to the object at ffff8801eff2c3c0
                    which belongs to the cache kmalloc-2048 of size 2048
    The buggy address is located 212 bytes inside of
                    2048-byte region [ffff8801eff2c3c0, ffff8801eff2cbc0)
    [...]
    ==================================================================
    
    Fixes: e6f30c731718 ("netfilter: x_tables: add xt_bpf match")
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index 041da0d9c06f..1f7fbd3c7e5a 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -27,6 +27,9 @@ static int __bpf_mt_check_bytecode(struct sock_filter *insns, __u16 len,
 {
 	struct sock_fprog_kern program;
 
+	if (len > XT_BPF_MAX_NUM_INSTR)
+		return -EINVAL;
+
 	program.len = len;
 	program.filter = insns;
 
@@ -55,6 +58,9 @@ static int __bpf_mt_check_path(const char *path, struct bpf_prog **ret)
 	mm_segment_t oldfs = get_fs();
 	int retval, fd;
 
+	if (strnlen(path, XT_BPF_PATH_MAX) == XT_BPF_PATH_MAX)
+		return -EINVAL;
+
 	set_fs(KERNEL_DS);
 	fd = bpf_obj_get_user(path, 0);
 	set_fs(oldfs);

commit 6e71b04a82248ccf13a94b85cbc674a9fefe53f5
Author: Chenbo Feng <fengc@google.com>
Date:   Wed Oct 18 13:00:22 2017 -0700

    bpf: Add file mode configuration into bpf maps
    
    Introduce the map read/write flags to the eBPF syscalls that returns the
    map fd. The flags is used to set up the file mode when construct a new
    file descriptor for bpf maps. To not break the backward capability, the
    f_flags is set to O_RDWR if the flag passed by syscall is 0. Otherwise
    it should be O_RDONLY or O_WRONLY. When the userspace want to modify or
    read the map content, it will check the file mode to see if it is
    allowed to make the change.
    
    Signed-off-by: Chenbo Feng <fengc@google.com>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index 29123934887b..041da0d9c06f 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -56,7 +56,7 @@ static int __bpf_mt_check_path(const char *path, struct bpf_prog **ret)
 	int retval, fd;
 
 	set_fs(KERNEL_DS);
-	fd = bpf_obj_get_user(path);
+	fd = bpf_obj_get_user(path, 0);
 	set_fs(oldfs);
 	if (fd < 0)
 		return fd;

commit 98589a0998b8b13c4a8fa1ccb0e62751a019faa5
Author: Shmulik Ladkani <shmulik.ladkani@gmail.com>
Date:   Mon Oct 9 15:27:15 2017 +0300

    netfilter: xt_bpf: Fix XT_BPF_MODE_FD_PINNED mode of 'xt_bpf_info_v1'
    
    Commit 2c16d6033264 ("netfilter: xt_bpf: support ebpf") introduced
    support for attaching an eBPF object by an fd, with the
    'bpf_mt_check_v1' ABI expecting the '.fd' to be specified upon each
    IPT_SO_SET_REPLACE call.
    
    However this breaks subsequent iptables calls:
    
     # iptables -A INPUT -m bpf --object-pinned /sys/fs/bpf/xxx -j ACCEPT
     # iptables -A INPUT -s 5.6.7.8 -j ACCEPT
     iptables: Invalid argument. Run `dmesg' for more information.
    
    That's because iptables works by loading existing rules using
    IPT_SO_GET_ENTRIES to userspace, then issuing IPT_SO_SET_REPLACE with
    the replacement set.
    
    However, the loaded 'xt_bpf_info_v1' has an arbitrary '.fd' number
    (from the initial "iptables -m bpf" invocation) - so when 2nd invocation
    occurs, userspace passes a bogus fd number, which leads to
    'bpf_mt_check_v1' to fail.
    
    One suggested solution [1] was to hack iptables userspace, to perform a
    "entries fixup" immediatley after IPT_SO_GET_ENTRIES, by opening a new,
    process-local fd per every 'xt_bpf_info_v1' entry seen.
    
    However, in [2] both Pablo Neira Ayuso and Willem de Bruijn suggested to
    depricate the xt_bpf_info_v1 ABI dealing with pinned ebpf objects.
    
    This fix changes the XT_BPF_MODE_FD_PINNED behavior to ignore the given
    '.fd' and instead perform an in-kernel lookup for the bpf object given
    the provided '.path'.
    
    It also defines an alias for the XT_BPF_MODE_FD_PINNED mode, named
    XT_BPF_MODE_PATH_PINNED, to better reflect the fact that the user is
    expected to provide the path of the pinned object.
    
    Existing XT_BPF_MODE_FD_ELF behavior (non-pinned fd mode) is preserved.
    
    References: [1] https://marc.info/?l=netfilter-devel&m=150564724607440&w=2
                [2] https://marc.info/?l=netfilter-devel&m=150575727129880&w=2
    
    Reported-by: Rafael Buchbinder <rafi@rbk.ms>
    Signed-off-by: Shmulik Ladkani <shmulik.ladkani@gmail.com>
    Acked-by: Willem de Bruijn <willemb@google.com>
    Acked-by: Daniel Borkmann <daniel@iogearbox.net>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index 38986a95216c..29123934887b 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -8,6 +8,7 @@
  */
 
 #include <linux/module.h>
+#include <linux/syscalls.h>
 #include <linux/skbuff.h>
 #include <linux/filter.h>
 #include <linux/bpf.h>
@@ -49,6 +50,22 @@ static int __bpf_mt_check_fd(int fd, struct bpf_prog **ret)
 	return 0;
 }
 
+static int __bpf_mt_check_path(const char *path, struct bpf_prog **ret)
+{
+	mm_segment_t oldfs = get_fs();
+	int retval, fd;
+
+	set_fs(KERNEL_DS);
+	fd = bpf_obj_get_user(path);
+	set_fs(oldfs);
+	if (fd < 0)
+		return fd;
+
+	retval = __bpf_mt_check_fd(fd, ret);
+	sys_close(fd);
+	return retval;
+}
+
 static int bpf_mt_check(const struct xt_mtchk_param *par)
 {
 	struct xt_bpf_info *info = par->matchinfo;
@@ -66,9 +83,10 @@ static int bpf_mt_check_v1(const struct xt_mtchk_param *par)
 		return __bpf_mt_check_bytecode(info->bpf_program,
 					       info->bpf_program_num_elem,
 					       &info->filter);
-	else if (info->mode == XT_BPF_MODE_FD_PINNED ||
-		 info->mode == XT_BPF_MODE_FD_ELF)
+	else if (info->mode == XT_BPF_MODE_FD_ELF)
 		return __bpf_mt_check_fd(info->fd, &info->filter);
+	else if (info->mode == XT_BPF_MODE_PATH_PINNED)
+		return __bpf_mt_check_path(info->path, &info->filter);
 	else
 		return -EINVAL;
 }

commit ec23189049651b16dc2ffab35a4371dc1f491aca
Author: Willem de Bruijn <willemb@google.com>
Date:   Mon Jan 2 17:19:46 2017 -0500

    xtables: extend matches and targets with .usersize
    
    In matches and targets that define a kernel-only tail to their
    xt_match and xt_target data structs, add a field .usersize that
    specifies up to where data is to be shared with userspace.
    
    Performed a search for comment "Used internally by the kernel" to find
    relevant matches and targets. Manually inspected the structs to derive
    a valid offsetof.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index 2dedaa23ab0a..38986a95216c 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -110,6 +110,7 @@ static struct xt_match bpf_mt_reg[] __read_mostly = {
 		.match		= bpf_mt,
 		.destroy	= bpf_mt_destroy,
 		.matchsize	= sizeof(struct xt_bpf_info),
+		.usersize	= offsetof(struct xt_bpf_info, filter),
 		.me		= THIS_MODULE,
 	},
 	{
@@ -120,6 +121,7 @@ static struct xt_match bpf_mt_reg[] __read_mostly = {
 		.match		= bpf_mt_v1,
 		.destroy	= bpf_mt_destroy_v1,
 		.matchsize	= sizeof(struct xt_bpf_info_v1),
+		.usersize	= offsetof(struct xt_bpf_info_v1, filter),
 		.me		= THIS_MODULE,
 	},
 };

commit 2c16d60332643e90d4fa244f4a706c454b8c7569
Author: Willem de Bruijn <willemb@google.com>
Date:   Tue Dec 6 16:25:02 2016 -0500

    netfilter: xt_bpf: support ebpf
    
    Add support for attaching an eBPF object by file descriptor.
    
    The iptables binary can be called with a path to an elf object or a
    pinned bpf object. Also pass the mode and path to the kernel to be
    able to return it later for iptables dump and save.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index dffee9d47ec4..2dedaa23ab0a 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -10,6 +10,7 @@
 #include <linux/module.h>
 #include <linux/skbuff.h>
 #include <linux/filter.h>
+#include <linux/bpf.h>
 
 #include <linux/netfilter/xt_bpf.h>
 #include <linux/netfilter/x_tables.h>
@@ -20,15 +21,15 @@ MODULE_LICENSE("GPL");
 MODULE_ALIAS("ipt_bpf");
 MODULE_ALIAS("ip6t_bpf");
 
-static int bpf_mt_check(const struct xt_mtchk_param *par)
+static int __bpf_mt_check_bytecode(struct sock_filter *insns, __u16 len,
+				   struct bpf_prog **ret)
 {
-	struct xt_bpf_info *info = par->matchinfo;
 	struct sock_fprog_kern program;
 
-	program.len = info->bpf_program_num_elem;
-	program.filter = info->bpf_program;
+	program.len = len;
+	program.filter = insns;
 
-	if (bpf_prog_create(&info->filter, &program)) {
+	if (bpf_prog_create(ret, &program)) {
 		pr_info("bpf: check failed: parse error\n");
 		return -EINVAL;
 	}
@@ -36,6 +37,42 @@ static int bpf_mt_check(const struct xt_mtchk_param *par)
 	return 0;
 }
 
+static int __bpf_mt_check_fd(int fd, struct bpf_prog **ret)
+{
+	struct bpf_prog *prog;
+
+	prog = bpf_prog_get_type(fd, BPF_PROG_TYPE_SOCKET_FILTER);
+	if (IS_ERR(prog))
+		return PTR_ERR(prog);
+
+	*ret = prog;
+	return 0;
+}
+
+static int bpf_mt_check(const struct xt_mtchk_param *par)
+{
+	struct xt_bpf_info *info = par->matchinfo;
+
+	return __bpf_mt_check_bytecode(info->bpf_program,
+				       info->bpf_program_num_elem,
+				       &info->filter);
+}
+
+static int bpf_mt_check_v1(const struct xt_mtchk_param *par)
+{
+	struct xt_bpf_info_v1 *info = par->matchinfo;
+
+	if (info->mode == XT_BPF_MODE_BYTECODE)
+		return __bpf_mt_check_bytecode(info->bpf_program,
+					       info->bpf_program_num_elem,
+					       &info->filter);
+	else if (info->mode == XT_BPF_MODE_FD_PINNED ||
+		 info->mode == XT_BPF_MODE_FD_ELF)
+		return __bpf_mt_check_fd(info->fd, &info->filter);
+	else
+		return -EINVAL;
+}
+
 static bool bpf_mt(const struct sk_buff *skb, struct xt_action_param *par)
 {
 	const struct xt_bpf_info *info = par->matchinfo;
@@ -43,31 +80,58 @@ static bool bpf_mt(const struct sk_buff *skb, struct xt_action_param *par)
 	return BPF_PROG_RUN(info->filter, skb);
 }
 
+static bool bpf_mt_v1(const struct sk_buff *skb, struct xt_action_param *par)
+{
+	const struct xt_bpf_info_v1 *info = par->matchinfo;
+
+	return !!bpf_prog_run_save_cb(info->filter, (struct sk_buff *) skb);
+}
+
 static void bpf_mt_destroy(const struct xt_mtdtor_param *par)
 {
 	const struct xt_bpf_info *info = par->matchinfo;
+
+	bpf_prog_destroy(info->filter);
+}
+
+static void bpf_mt_destroy_v1(const struct xt_mtdtor_param *par)
+{
+	const struct xt_bpf_info_v1 *info = par->matchinfo;
+
 	bpf_prog_destroy(info->filter);
 }
 
-static struct xt_match bpf_mt_reg __read_mostly = {
-	.name		= "bpf",
-	.revision	= 0,
-	.family		= NFPROTO_UNSPEC,
-	.checkentry	= bpf_mt_check,
-	.match		= bpf_mt,
-	.destroy	= bpf_mt_destroy,
-	.matchsize	= sizeof(struct xt_bpf_info),
-	.me		= THIS_MODULE,
+static struct xt_match bpf_mt_reg[] __read_mostly = {
+	{
+		.name		= "bpf",
+		.revision	= 0,
+		.family		= NFPROTO_UNSPEC,
+		.checkentry	= bpf_mt_check,
+		.match		= bpf_mt,
+		.destroy	= bpf_mt_destroy,
+		.matchsize	= sizeof(struct xt_bpf_info),
+		.me		= THIS_MODULE,
+	},
+	{
+		.name		= "bpf",
+		.revision	= 1,
+		.family		= NFPROTO_UNSPEC,
+		.checkentry	= bpf_mt_check_v1,
+		.match		= bpf_mt_v1,
+		.destroy	= bpf_mt_destroy_v1,
+		.matchsize	= sizeof(struct xt_bpf_info_v1),
+		.me		= THIS_MODULE,
+	},
 };
 
 static int __init bpf_mt_init(void)
 {
-	return xt_register_match(&bpf_mt_reg);
+	return xt_register_matches(bpf_mt_reg, ARRAY_SIZE(bpf_mt_reg));
 }
 
 static void __exit bpf_mt_exit(void)
 {
-	xt_unregister_match(&bpf_mt_reg);
+	xt_unregister_matches(bpf_mt_reg, ARRAY_SIZE(bpf_mt_reg));
 }
 
 module_init(bpf_mt_init);

commit 7ae457c1e5b45a1b826fad9d62b32191d2bdcfdb
Author: Alexei Starovoitov <ast@plumgrid.com>
Date:   Wed Jul 30 20:34:16 2014 -0700

    net: filter: split 'struct sk_filter' into socket and bpf parts
    
    clean up names related to socket filtering and bpf in the following way:
    - everything that deals with sockets keeps 'sk_*' prefix
    - everything that is pure BPF is changed to 'bpf_*' prefix
    
    split 'struct sk_filter' into
    struct sk_filter {
            atomic_t        refcnt;
            struct rcu_head rcu;
            struct bpf_prog *prog;
    };
    and
    struct bpf_prog {
            u32                     jited:1,
                                    len:31;
            struct sock_fprog_kern  *orig_prog;
            unsigned int            (*bpf_func)(const struct sk_buff *skb,
                                                const struct bpf_insn *filter);
            union {
                    struct sock_filter      insns[0];
                    struct bpf_insn         insnsi[0];
                    struct work_struct      work;
            };
    };
    so that 'struct bpf_prog' can be used independent of sockets and cleans up
    'unattached' bpf use cases
    
    split SK_RUN_FILTER macro into:
        SK_RUN_FILTER to be used with 'struct sk_filter *' and
        BPF_PROG_RUN to be used with 'struct bpf_prog *'
    
    __sk_filter_release(struct sk_filter *) gains
    __bpf_prog_release(struct bpf_prog *) helper function
    
    also perform related renames for the functions that work
    with 'struct bpf_prog *', since they're on the same lines:
    
    sk_filter_size -> bpf_prog_size
    sk_filter_select_runtime -> bpf_prog_select_runtime
    sk_filter_free -> bpf_prog_free
    sk_unattached_filter_create -> bpf_prog_create
    sk_unattached_filter_destroy -> bpf_prog_destroy
    sk_store_orig_filter -> bpf_prog_store_orig_filter
    sk_release_orig_filter -> bpf_release_orig_filter
    __sk_migrate_filter -> bpf_migrate_filter
    __sk_prepare_filter -> bpf_prepare_filter
    
    API for attaching classic BPF to a socket stays the same:
    sk_attach_filter(prog, struct sock *)/sk_detach_filter(struct sock *)
    and SK_RUN_FILTER(struct sk_filter *, ctx) to execute a program
    which is used by sockets, tun, af_packet
    
    API for 'unattached' BPF programs becomes:
    bpf_prog_create(struct bpf_prog **)/bpf_prog_destroy(struct bpf_prog *)
    and BPF_PROG_RUN(struct bpf_prog *, ctx) to execute a program
    which is used by isdn, ppp, team, seccomp, ptp, xt_bpf, cls_bpf, test_bpf
    
    Signed-off-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index bbffdbdaf603..dffee9d47ec4 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -28,7 +28,7 @@ static int bpf_mt_check(const struct xt_mtchk_param *par)
 	program.len = info->bpf_program_num_elem;
 	program.filter = info->bpf_program;
 
-	if (sk_unattached_filter_create(&info->filter, &program)) {
+	if (bpf_prog_create(&info->filter, &program)) {
 		pr_info("bpf: check failed: parse error\n");
 		return -EINVAL;
 	}
@@ -40,13 +40,13 @@ static bool bpf_mt(const struct sk_buff *skb, struct xt_action_param *par)
 {
 	const struct xt_bpf_info *info = par->matchinfo;
 
-	return SK_RUN_FILTER(info->filter, skb);
+	return BPF_PROG_RUN(info->filter, skb);
 }
 
 static void bpf_mt_destroy(const struct xt_mtdtor_param *par)
 {
 	const struct xt_bpf_info *info = par->matchinfo;
-	sk_unattached_filter_destroy(info->filter);
+	bpf_prog_destroy(info->filter);
 }
 
 static struct xt_match bpf_mt_reg __read_mostly = {

commit b1fcd35cf53553a0a3ef949b05106d921446abc3
Author: Daniel Borkmann <dborkman@redhat.com>
Date:   Fri May 23 18:43:58 2014 +0200

    net: filter: let unattached filters use sock_fprog_kern
    
    The sk_unattached_filter_create() API is used by BPF filters that
    are not directly attached or related to sockets, and are used in
    team, ptp, xt_bpf, cls_bpf, etc. As such all users do their own
    internal managment of obtaining filter blocks and thus already
    have them in kernel memory and set up before calling into
    sk_unattached_filter_create(). As a result, due to __user annotation
    in sock_fprog, sparse triggers false positives (incorrect type in
    assignment [different address space]) when filters are set up before
    passing them to sk_unattached_filter_create(). Therefore, let
    sk_unattached_filter_create() API use sock_fprog_kern to overcome
    this issue.
    
    Signed-off-by: Daniel Borkmann <dborkman@redhat.com>
    Acked-by: Alexei Starovoitov <ast@plumgrid.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
index 12d4da8e6c77..bbffdbdaf603 100644
--- a/net/netfilter/xt_bpf.c
+++ b/net/netfilter/xt_bpf.c
@@ -23,10 +23,11 @@ MODULE_ALIAS("ip6t_bpf");
 static int bpf_mt_check(const struct xt_mtchk_param *par)
 {
 	struct xt_bpf_info *info = par->matchinfo;
-	struct sock_fprog program;
+	struct sock_fprog_kern program;
 
 	program.len = info->bpf_program_num_elem;
-	program.filter = (struct sock_filter __user *) info->bpf_program;
+	program.filter = info->bpf_program;
+
 	if (sk_unattached_filter_create(&info->filter, &program)) {
 		pr_info("bpf: check failed: parse error\n");
 		return -EINVAL;

commit e6f30c731718db45cec380964dfee210307cfc4a
Author: Willem de Bruijn <willemb@google.com>
Date:   Fri Jan 18 07:17:30 2013 +0000

    netfilter: x_tables: add xt_bpf match
    
    Support arbitrary linux socket filter (BPF) programs as x_tables
    match rules. This allows for very expressive filters, and on
    platforms with BPF JIT appears competitive with traditional
    hardcoded iptables rules using the u32 match.
    
    The size of the filter has been artificially limited to 64
    instructions maximum to avoid bloating the size of each rule
    using this new match.
    
    Signed-off-by: Willem de Bruijn <willemb@google.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/xt_bpf.c b/net/netfilter/xt_bpf.c
new file mode 100644
index 000000000000..12d4da8e6c77
--- /dev/null
+++ b/net/netfilter/xt_bpf.c
@@ -0,0 +1,73 @@
+/* Xtables module to match packets using a BPF filter.
+ * Copyright 2013 Google Inc.
+ * Written by Willem de Bruijn <willemb@google.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/module.h>
+#include <linux/skbuff.h>
+#include <linux/filter.h>
+
+#include <linux/netfilter/xt_bpf.h>
+#include <linux/netfilter/x_tables.h>
+
+MODULE_AUTHOR("Willem de Bruijn <willemb@google.com>");
+MODULE_DESCRIPTION("Xtables: BPF filter match");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("ipt_bpf");
+MODULE_ALIAS("ip6t_bpf");
+
+static int bpf_mt_check(const struct xt_mtchk_param *par)
+{
+	struct xt_bpf_info *info = par->matchinfo;
+	struct sock_fprog program;
+
+	program.len = info->bpf_program_num_elem;
+	program.filter = (struct sock_filter __user *) info->bpf_program;
+	if (sk_unattached_filter_create(&info->filter, &program)) {
+		pr_info("bpf: check failed: parse error\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static bool bpf_mt(const struct sk_buff *skb, struct xt_action_param *par)
+{
+	const struct xt_bpf_info *info = par->matchinfo;
+
+	return SK_RUN_FILTER(info->filter, skb);
+}
+
+static void bpf_mt_destroy(const struct xt_mtdtor_param *par)
+{
+	const struct xt_bpf_info *info = par->matchinfo;
+	sk_unattached_filter_destroy(info->filter);
+}
+
+static struct xt_match bpf_mt_reg __read_mostly = {
+	.name		= "bpf",
+	.revision	= 0,
+	.family		= NFPROTO_UNSPEC,
+	.checkentry	= bpf_mt_check,
+	.match		= bpf_mt,
+	.destroy	= bpf_mt_destroy,
+	.matchsize	= sizeof(struct xt_bpf_info),
+	.me		= THIS_MODULE,
+};
+
+static int __init bpf_mt_init(void)
+{
+	return xt_register_match(&bpf_mt_reg);
+}
+
+static void __exit bpf_mt_exit(void)
+{
+	xt_unregister_match(&bpf_mt_reg);
+}
+
+module_init(bpf_mt_init);
+module_exit(bpf_mt_exit);
