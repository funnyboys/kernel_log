commit 33d077996a87175b155fe88030e8fec7ca76327e
Author: Stefano Brivio <sbrivio@redhat.com>
Date:   Wed Jun 3 01:50:11 2020 +0200

    netfilter: nft_set_rbtree: Don't account for expired elements on insertion
    
    While checking the validity of insertion in __nft_rbtree_insert(),
    we currently ignore conflicting elements and intervals only if they
    are not active within the next generation.
    
    However, if we consider expired elements and intervals as
    potentially conflicting and overlapping, we'll return error for
    entries that should be added instead. This is particularly visible
    with garbage collection intervals that are comparable with the
    element timeout itself, as reported by Mike Dillinger.
    
    Other than the simple issue of denying insertion of valid entries,
    this might also result in insertion of a single element (opening or
    closing) out of a given interval. With single entries (that are
    inserted as intervals of size 1), this leads in turn to the creation
    of new intervals. For example:
    
      # nft add element t s { 192.0.2.1 }
      # nft list ruleset
      [...]
         elements = { 192.0.2.1-255.255.255.255 }
    
    Always ignore expired elements active in the next generation, while
    checking for conflicts.
    
    It might be more convenient to introduce a new macro that covers
    both inactive and expired items, as this type of check also appears
    quite frequently in other set back-ends. This is however beyond the
    scope of this fix and can be deferred to a separate patch.
    
    Other than the overlap detection cases introduced by commit
    7c84d41416d8 ("netfilter: nft_set_rbtree: Detect partial overlaps
    on insertion"), we also have to cover the original conflict check
    dealing with conflicts between two intervals of size 1, which was
    introduced before support for timeout was introduced. This won't
    return an error to the user as -EEXIST is masked by nft if
    NLM_F_EXCL is not given, but would result in a silent failure
    adding the entry.
    
    Reported-by: Mike Dillinger <miked@softtalker.com>
    Cc: <stable@vger.kernel.org> # 5.6.x
    Fixes: 8d8540c4f5e0 ("netfilter: nft_set_rbtree: add timeout support")
    Fixes: 7c84d41416d8 ("netfilter: nft_set_rbtree: Detect partial overlaps on insertion")
    Signed-off-by: Stefano Brivio <sbrivio@redhat.com>
    Acked-by: Phil Sutter <phil@nwl.cc>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 62f416bc0579..b6aad3fc46c3 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -271,12 +271,14 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 
 			if (nft_rbtree_interval_start(new)) {
 				if (nft_rbtree_interval_end(rbe) &&
-				    nft_set_elem_active(&rbe->ext, genmask))
+				    nft_set_elem_active(&rbe->ext, genmask) &&
+				    !nft_set_elem_expired(&rbe->ext))
 					overlap = false;
 			} else {
 				overlap = nft_rbtree_interval_end(rbe) &&
 					  nft_set_elem_active(&rbe->ext,
-							      genmask);
+							      genmask) &&
+					  !nft_set_elem_expired(&rbe->ext);
 			}
 		} else if (d > 0) {
 			p = &parent->rb_right;
@@ -284,9 +286,11 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			if (nft_rbtree_interval_end(new)) {
 				overlap = nft_rbtree_interval_end(rbe) &&
 					  nft_set_elem_active(&rbe->ext,
-							      genmask);
+							      genmask) &&
+					  !nft_set_elem_expired(&rbe->ext);
 			} else if (nft_rbtree_interval_end(rbe) &&
-				   nft_set_elem_active(&rbe->ext, genmask)) {
+				   nft_set_elem_active(&rbe->ext, genmask) &&
+				   !nft_set_elem_expired(&rbe->ext)) {
 				overlap = true;
 			}
 		} else {
@@ -294,15 +298,18 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			    nft_rbtree_interval_start(new)) {
 				p = &parent->rb_left;
 
-				if (nft_set_elem_active(&rbe->ext, genmask))
+				if (nft_set_elem_active(&rbe->ext, genmask) &&
+				    !nft_set_elem_expired(&rbe->ext))
 					overlap = false;
 			} else if (nft_rbtree_interval_start(rbe) &&
 				   nft_rbtree_interval_end(new)) {
 				p = &parent->rb_right;
 
-				if (nft_set_elem_active(&rbe->ext, genmask))
+				if (nft_set_elem_active(&rbe->ext, genmask) &&
+				    !nft_set_elem_expired(&rbe->ext))
 					overlap = false;
-			} else if (nft_set_elem_active(&rbe->ext, genmask)) {
+			} else if (nft_set_elem_active(&rbe->ext, genmask) &&
+				   !nft_set_elem_expired(&rbe->ext)) {
 				*ext = &rbe->ext;
 				return -EEXIST;
 			} else {

commit 340eaff651160234bdbce07ef34b92a8e45cd540
Author: Phil Sutter <phil@nwl.cc>
Date:   Mon May 11 15:31:41 2020 +0200

    netfilter: nft_set_rbtree: Add missing expired checks
    
    Expired intervals would still match and be dumped to user space until
    garbage collection wiped them out. Make sure they stop matching and
    disappear (from users' perspective) as soon as they expire.
    
    Fixes: 8d8540c4f5e03 ("netfilter: nft_set_rbtree: add timeout support")
    Signed-off-by: Phil Sutter <phil@nwl.cc>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 3ffef454d469..62f416bc0579 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -79,6 +79,10 @@ static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set
 				parent = rcu_dereference_raw(parent->rb_left);
 				continue;
 			}
+
+			if (nft_set_elem_expired(&rbe->ext))
+				return false;
+
 			if (nft_rbtree_interval_end(rbe)) {
 				if (nft_set_is_anonymous(set))
 					return false;
@@ -94,6 +98,7 @@ static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set
 
 	if (set->flags & NFT_SET_INTERVAL && interval != NULL &&
 	    nft_set_elem_active(&interval->ext, genmask) &&
+	    !nft_set_elem_expired(&interval->ext) &&
 	    nft_rbtree_interval_start(interval)) {
 		*ext = &interval->ext;
 		return true;
@@ -154,6 +159,9 @@ static bool __nft_rbtree_get(const struct net *net, const struct nft_set *set,
 				continue;
 			}
 
+			if (nft_set_elem_expired(&rbe->ext))
+				return false;
+
 			if (!nft_set_ext_exists(&rbe->ext, NFT_SET_EXT_FLAGS) ||
 			    (*nft_set_ext_flags(&rbe->ext) & NFT_SET_ELEM_INTERVAL_END) ==
 			    (flags & NFT_SET_ELEM_INTERVAL_END)) {
@@ -170,6 +178,7 @@ static bool __nft_rbtree_get(const struct net *net, const struct nft_set *set,
 
 	if (set->flags & NFT_SET_INTERVAL && interval != NULL &&
 	    nft_set_elem_active(&interval->ext, genmask) &&
+	    !nft_set_elem_expired(&interval->ext) &&
 	    ((!nft_rbtree_interval_end(interval) &&
 	      !(flags & NFT_SET_ELEM_INTERVAL_END)) ||
 	     (nft_rbtree_interval_end(interval) &&
@@ -418,6 +427,8 @@ static void nft_rbtree_walk(const struct nft_ctx *ctx,
 
 		if (iter->count < iter->skip)
 			goto cont;
+		if (nft_set_elem_expired(&rbe->ext))
+			goto cont;
 		if (!nft_set_elem_active(&rbe->ext, iter->genmask))
 			goto cont;
 

commit 72239f2795fab9a58633bd0399698ff7581534a3
Author: Stefano Brivio <sbrivio@redhat.com>
Date:   Wed Apr 1 17:14:38 2020 +0200

    netfilter: nft_set_rbtree: Drop spurious condition for overlap detection on insertion
    
    Case a1. for overlap detection in __nft_rbtree_insert() is not a valid
    one: start-after-start is not needed to detect any type of interval
    overlap and it actually results in a false positive if, while
    descending the tree, this is the only step we hit after starting from
    the root.
    
    This introduced a regression, as reported by Pablo, in Python tests
    cases ip/ip.t and ip/numgen.t:
    
      ip/ip.t: ERROR: line 124: add rule ip test-ip4 input ip hdrlength vmap { 0-4 : drop, 5 : accept, 6 : continue } counter: This rule should not have failed.
      ip/numgen.t: ERROR: line 7: add rule ip test-ip4 pre dnat to numgen inc mod 10 map { 0-5 : 192.168.10.100, 6-9 : 192.168.20.200}: This rule should not have failed.
    
    Drop case a1. and renumber others, so that they are a bit clearer. In
    order for these diagrams to be readily understandable, a bigger rework
    is probably needed, such as an ASCII art of the actual rbtree (instead
    of a flattened version).
    
    Shell script test sets/0044interval_overlap_0 should cover all
    possible cases for false negatives, so I consider that test case still
    sufficient after this change.
    
    v2: Fix comments for cases a3. and b3.
    
    Reported-by: Pablo Neira Ayuso <pablo@netfilter.org>
    Fixes: 7c84d41416d8 ("netfilter: nft_set_rbtree: Detect partial overlaps on insertion")
    Signed-off-by: Stefano Brivio <sbrivio@redhat.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 3a5552e14f75..3ffef454d469 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -218,27 +218,26 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 
 	/* Detect overlaps as we descend the tree. Set the flag in these cases:
 	 *
-	 * a1. |__ _ _?  >|__ _ _  (insert start after existing start)
-	 * a2. _ _ __>|  ?_ _ __|  (insert end before existing end)
-	 * a3. _ _ ___|  ?_ _ _>|  (insert end after existing end)
-	 * a4. >|__ _ _   _ _ __|  (insert start before existing end)
+	 * a1. _ _ __>|  ?_ _ __|  (insert end before existing end)
+	 * a2. _ _ ___|  ?_ _ _>|  (insert end after existing end)
+	 * a3. _ _ ___? >|_ _ __|  (insert start before existing end)
 	 *
 	 * and clear it later on, as we eventually reach the points indicated by
 	 * '?' above, in the cases described below. We'll always meet these
 	 * later, locally, due to tree ordering, and overlaps for the intervals
 	 * that are the closest together are always evaluated last.
 	 *
-	 * b1. |__ _ _!  >|__ _ _  (insert start after existing end)
-	 * b2. _ _ __>|  !_ _ __|  (insert end before existing start)
-	 * b3. !_____>|            (insert end after existing start)
+	 * b1. _ _ __>|  !_ _ __|  (insert end before existing start)
+	 * b2. _ _ ___|  !_ _ _>|  (insert end after existing start)
+	 * b3. _ _ ___! >|_ _ __|  (insert start after existing end)
 	 *
-	 * Case a4. resolves to b1.:
+	 * Case a3. resolves to b3.:
 	 * - if the inserted start element is the leftmost, because the '0'
 	 *   element in the tree serves as end element
 	 * - otherwise, if an existing end is found. Note that end elements are
 	 *   always inserted after corresponding start elements.
 	 *
-	 * For a new, rightmost pair of elements, we'll hit cases b1. and b3.,
+	 * For a new, rightmost pair of elements, we'll hit cases b3. and b2.,
 	 * in that order.
 	 *
 	 * The flag is also cleared in two special cases:
@@ -262,9 +261,9 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			p = &parent->rb_left;
 
 			if (nft_rbtree_interval_start(new)) {
-				overlap = nft_rbtree_interval_start(rbe) &&
-					  nft_set_elem_active(&rbe->ext,
-							      genmask);
+				if (nft_rbtree_interval_end(rbe) &&
+				    nft_set_elem_active(&rbe->ext, genmask))
+					overlap = false;
 			} else {
 				overlap = nft_rbtree_interval_end(rbe) &&
 					  nft_set_elem_active(&rbe->ext,

commit 9fb16955fb661945ddffce4504dcffbe55cd518a
Merge: 1f074e677a34 1b649e0bcae7
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Mar 25 18:58:11 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Overlapping header include additions in macsec.c
    
    A bug fix in 'net' overlapping with the removal of 'version'
    string in ena_netdev.c
    
    Overlapping test additions in selftests Makefile
    
    Overlapping PCI ID table adjustments in iwlwifi driver.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7c84d41416d836ef7e533bd4d64ccbdf40c5ac70
Author: Stefano Brivio <sbrivio@redhat.com>
Date:   Sun Mar 22 03:22:01 2020 +0100

    netfilter: nft_set_rbtree: Detect partial overlaps on insertion
    
    ...and return -ENOTEMPTY to the front-end in this case, instead of
    proceeding. Currently, nft takes care of checking for these cases
    and not sending them to the kernel, but if we drop the set_overlap()
    call in nft we can end up in situations like:
    
     # nft add table t
     # nft add set t s '{ type inet_service ; flags interval ; }'
     # nft add element t s '{ 1 - 5 }'
     # nft add element t s '{ 6 - 10 }'
     # nft add element t s '{ 4 - 7 }'
     # nft list set t s
     table ip t {
            set s {
                    type inet_service
                    flags interval
                    elements = { 1-3, 4-5, 6-7 }
            }
     }
    
    This change has the primary purpose of making the behaviour
    consistent with nft_set_pipapo, but is also functional to avoid
    inconsistent behaviour if userspace sends overlapping elements for
    any reason.
    
    v2: When we meet the same key data in the tree, as start element while
        inserting an end element, or as end element while inserting a start
        element, actually check that the existing element is active, before
        resetting the overlap flag (Pablo Neira Ayuso)
    
    Signed-off-by: Stefano Brivio <sbrivio@redhat.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 85572b2a6051..8617fc16a1ed 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -213,8 +213,43 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 	u8 genmask = nft_genmask_next(net);
 	struct nft_rbtree_elem *rbe;
 	struct rb_node *parent, **p;
+	bool overlap = false;
 	int d;
 
+	/* Detect overlaps as we descend the tree. Set the flag in these cases:
+	 *
+	 * a1. |__ _ _?  >|__ _ _  (insert start after existing start)
+	 * a2. _ _ __>|  ?_ _ __|  (insert end before existing end)
+	 * a3. _ _ ___|  ?_ _ _>|  (insert end after existing end)
+	 * a4. >|__ _ _   _ _ __|  (insert start before existing end)
+	 *
+	 * and clear it later on, as we eventually reach the points indicated by
+	 * '?' above, in the cases described below. We'll always meet these
+	 * later, locally, due to tree ordering, and overlaps for the intervals
+	 * that are the closest together are always evaluated last.
+	 *
+	 * b1. |__ _ _!  >|__ _ _  (insert start after existing end)
+	 * b2. _ _ __>|  !_ _ __|  (insert end before existing start)
+	 * b3. !_____>|            (insert end after existing start)
+	 *
+	 * Case a4. resolves to b1.:
+	 * - if the inserted start element is the leftmost, because the '0'
+	 *   element in the tree serves as end element
+	 * - otherwise, if an existing end is found. Note that end elements are
+	 *   always inserted after corresponding start elements.
+	 *
+	 * For a new, rightmost pair of elements, we'll hit cases b1. and b3.,
+	 * in that order.
+	 *
+	 * The flag is also cleared in two special cases:
+	 *
+	 * b4. |__ _ _!|<_ _ _   (insert start right before existing end)
+	 * b5. |__ _ >|!__ _ _   (insert end right after existing start)
+	 *
+	 * which always happen as last step and imply that no further
+	 * overlapping is possible.
+	 */
+
 	parent = NULL;
 	p = &priv->root.rb_node;
 	while (*p != NULL) {
@@ -223,17 +258,42 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 		d = memcmp(nft_set_ext_key(&rbe->ext),
 			   nft_set_ext_key(&new->ext),
 			   set->klen);
-		if (d < 0)
+		if (d < 0) {
 			p = &parent->rb_left;
-		else if (d > 0)
+
+			if (nft_rbtree_interval_start(new)) {
+				overlap = nft_rbtree_interval_start(rbe) &&
+					  nft_set_elem_active(&rbe->ext,
+							      genmask);
+			} else {
+				overlap = nft_rbtree_interval_end(rbe) &&
+					  nft_set_elem_active(&rbe->ext,
+							      genmask);
+			}
+		} else if (d > 0) {
 			p = &parent->rb_right;
-		else {
+
+			if (nft_rbtree_interval_end(new)) {
+				overlap = nft_rbtree_interval_end(rbe) &&
+					  nft_set_elem_active(&rbe->ext,
+							      genmask);
+			} else if (nft_rbtree_interval_end(rbe) &&
+				   nft_set_elem_active(&rbe->ext, genmask)) {
+				overlap = true;
+			}
+		} else {
 			if (nft_rbtree_interval_end(rbe) &&
 			    nft_rbtree_interval_start(new)) {
 				p = &parent->rb_left;
+
+				if (nft_set_elem_active(&rbe->ext, genmask))
+					overlap = false;
 			} else if (nft_rbtree_interval_start(rbe) &&
 				   nft_rbtree_interval_end(new)) {
 				p = &parent->rb_right;
+
+				if (nft_set_elem_active(&rbe->ext, genmask))
+					overlap = false;
 			} else if (nft_set_elem_active(&rbe->ext, genmask)) {
 				*ext = &rbe->ext;
 				return -EEXIST;
@@ -242,6 +302,10 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			}
 		}
 	}
+
+	if (overlap)
+		return -ENOTEMPTY;
+
 	rb_link_node_rcu(&new->node, parent, p);
 	rb_insert_color(&new->node, &priv->root);
 	return 0;

commit 6f7c9caf017be8ab0fe3b99509580d0793bf0833
Author: Stefano Brivio <sbrivio@redhat.com>
Date:   Sun Mar 22 03:22:00 2020 +0100

    netfilter: nft_set_rbtree: Introduce and use nft_rbtree_interval_start()
    
    Replace negations of nft_rbtree_interval_end() with a new helper,
    nft_rbtree_interval_start(), wherever this helps to visualise the
    problem at hand, that is, for all the occurrences except for the
    comparison against given flags in __nft_rbtree_get().
    
    This gets especially useful in the next patch.
    
    Signed-off-by: Stefano Brivio <sbrivio@redhat.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 5000b938ab1e..85572b2a6051 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -33,6 +33,11 @@ static bool nft_rbtree_interval_end(const struct nft_rbtree_elem *rbe)
 	       (*nft_set_ext_flags(&rbe->ext) & NFT_SET_ELEM_INTERVAL_END);
 }
 
+static bool nft_rbtree_interval_start(const struct nft_rbtree_elem *rbe)
+{
+	return !nft_rbtree_interval_end(rbe);
+}
+
 static bool nft_rbtree_equal(const struct nft_set *set, const void *this,
 			     const struct nft_rbtree_elem *interval)
 {
@@ -64,7 +69,7 @@ static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set
 			if (interval &&
 			    nft_rbtree_equal(set, this, interval) &&
 			    nft_rbtree_interval_end(rbe) &&
-			    !nft_rbtree_interval_end(interval))
+			    nft_rbtree_interval_start(interval))
 				continue;
 			interval = rbe;
 		} else if (d > 0)
@@ -89,7 +94,7 @@ static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set
 
 	if (set->flags & NFT_SET_INTERVAL && interval != NULL &&
 	    nft_set_elem_active(&interval->ext, genmask) &&
-	    !nft_rbtree_interval_end(interval)) {
+	    nft_rbtree_interval_start(interval)) {
 		*ext = &interval->ext;
 		return true;
 	}
@@ -224,9 +229,9 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			p = &parent->rb_right;
 		else {
 			if (nft_rbtree_interval_end(rbe) &&
-			    !nft_rbtree_interval_end(new)) {
+			    nft_rbtree_interval_start(new)) {
 				p = &parent->rb_left;
-			} else if (!nft_rbtree_interval_end(rbe) &&
+			} else if (nft_rbtree_interval_start(rbe) &&
 				   nft_rbtree_interval_end(new)) {
 				p = &parent->rb_right;
 			} else if (nft_set_elem_active(&rbe->ext, genmask)) {
@@ -317,10 +322,10 @@ static void *nft_rbtree_deactivate(const struct net *net,
 			parent = parent->rb_right;
 		else {
 			if (nft_rbtree_interval_end(rbe) &&
-			    !nft_rbtree_interval_end(this)) {
+			    nft_rbtree_interval_start(this)) {
 				parent = parent->rb_left;
 				continue;
-			} else if (!nft_rbtree_interval_end(rbe) &&
+			} else if (nft_rbtree_interval_start(rbe) &&
 				   nft_rbtree_interval_end(this)) {
 				parent = parent->rb_right;
 				continue;

commit 24d19826fcbd97144908ae32019ee67d358c5879
Author: Florian Westphal <fw@strlen.de>
Date:   Tue Feb 18 11:59:27 2020 +0100

    netfilter: nf_tables: make all set structs const
    
    They do not need to be writeable anymore.
    
    v2: remove left-over __read_mostly annotation in set_pipapo.c (Stefano)
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 5000b938ab1e..172ef8189f99 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -481,8 +481,7 @@ static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 	return true;
 }
 
-struct nft_set_type nft_set_rbtree_type __read_mostly = {
-	.owner		= THIS_MODULE,
+const struct nft_set_type nft_set_rbtree_type = {
 	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT | NFT_SET_TIMEOUT,
 	.ops		= {
 		.privsize	= nft_rbtree_privsize,

commit f3a2181e16f1dcbf5446ed43f6b5d9f56c459f85
Author: Stefano Brivio <sbrivio@redhat.com>
Date:   Wed Jan 22 00:17:53 2020 +0100

    netfilter: nf_tables: Support for sets with multiple ranged fields
    
    Introduce a new nested netlink attribute, NFTA_SET_DESC_CONCAT, used
    to specify the length of each field in a set concatenation.
    
    This allows set implementations to support concatenation of multiple
    ranged items, as they can divide the input key into matching data for
    every single field. Such set implementations would be selected as
    they specify support for NFT_SET_INTERVAL and allow desc->field_count
    to be greater than one. Explicitly disallow this for nft_set_rbtree.
    
    In order to specify the interval for a set entry, userspace would
    include in NFTA_SET_DESC_CONCAT attributes field lengths, and pass
    range endpoints as two separate keys, represented by attributes
    NFTA_SET_ELEM_KEY and NFTA_SET_ELEM_KEY_END.
    
    While at it, export the number of 32-bit registers available for
    packet matching, as nftables will need this to know the maximum
    number of field lengths that can be specified.
    
    For example, "packets with an IPv4 address between 192.0.2.0 and
    192.0.2.42, with destination port between 22 and 25", can be
    expressed as two concatenated elements:
    
      NFTA_SET_ELEM_KEY:            192.0.2.0 . 22
      NFTA_SET_ELEM_KEY_END:        192.0.2.42 . 25
    
    and NFTA_SET_DESC_CONCAT attribute would contain:
    
      NFTA_LIST_ELEM
        NFTA_SET_FIELD_LEN:         4
      NFTA_LIST_ELEM
        NFTA_SET_FIELD_LEN:         2
    
    v4: No changes
    v3: Complete rework, NFTA_SET_DESC_CONCAT instead of NFTA_SET_SUBKEY
    v2: No changes
    
    Signed-off-by: Stefano Brivio <sbrivio@redhat.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index a9f804f7a04a..5000b938ab1e 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -466,6 +466,9 @@ static void nft_rbtree_destroy(const struct nft_set *set)
 static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 				struct nft_set_estimate *est)
 {
+	if (desc->field_count > 1)
+		return false;
+
 	if (desc->size)
 		est->size = sizeof(struct nft_rbtree) +
 			    desc->size * sizeof(struct nft_rbtree_elem);

commit db3b665dd77b34e34df00e17d7b299c98fcfb2c5
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Fri Dec 6 20:23:29 2019 +0100

    netfilter: nft_set_rbtree: bogus lookup/get on consecutive elements in named sets
    
    The existing rbtree implementation might store consecutive elements
    where the closing element and the opening element might overlap, eg.
    
            [ a, a+1) [ a+1, a+2)
    
    This patch removes the optimization for non-anonymous sets in the exact
    matching case, where it is assumed to stop searching in case that the
    closing element is found. Instead, invalidate candidate interval and
    keep looking further in the tree.
    
    The lookup/get operation might return false, while there is an element
    in the rbtree. Moreover, the get operation returns true as if a+2 would
    be in the tree. This happens with named sets after several set updates.
    
    The existing lookup optimization (that only works for the anonymous
    sets) might not reach the opening [ a+1,... element if the closing
    ...,a+1) is found in first place when walking over the rbtree. Hence,
    walking the full tree in that case is needed.
    
    This patch fixes the lookup and get operations.
    
    Fixes: e701001e7cbe ("netfilter: nft_rbtree: allow adjacent intervals with dynamic updates")
    Fixes: ba0e4d9917b4 ("netfilter: nf_tables: get set elements via netlink")
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 57123259452f..a9f804f7a04a 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -74,8 +74,13 @@ static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set
 				parent = rcu_dereference_raw(parent->rb_left);
 				continue;
 			}
-			if (nft_rbtree_interval_end(rbe))
-				goto out;
+			if (nft_rbtree_interval_end(rbe)) {
+				if (nft_set_is_anonymous(set))
+					return false;
+				parent = rcu_dereference_raw(parent->rb_left);
+				interval = NULL;
+				continue;
+			}
 
 			*ext = &rbe->ext;
 			return true;
@@ -88,7 +93,7 @@ static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set
 		*ext = &interval->ext;
 		return true;
 	}
-out:
+
 	return false;
 }
 
@@ -139,8 +144,10 @@ static bool __nft_rbtree_get(const struct net *net, const struct nft_set *set,
 			if (flags & NFT_SET_ELEM_INTERVAL_END)
 				interval = rbe;
 		} else {
-			if (!nft_set_elem_active(&rbe->ext, genmask))
+			if (!nft_set_elem_active(&rbe->ext, genmask)) {
 				parent = rcu_dereference_raw(parent->rb_left);
+				continue;
+			}
 
 			if (!nft_set_ext_exists(&rbe->ext, NFT_SET_EXT_FLAGS) ||
 			    (*nft_set_ext_flags(&rbe->ext) & NFT_SET_ELEM_INTERVAL_END) ==
@@ -148,7 +155,11 @@ static bool __nft_rbtree_get(const struct net *net, const struct nft_set *set,
 				*elem = rbe;
 				return true;
 			}
-			return false;
+
+			if (nft_rbtree_interval_end(rbe))
+				interval = NULL;
+
+			parent = rcu_dereference_raw(parent->rb_left);
 		}
 	}
 

commit 5785cf15fd74ec3b1a076fd39bc67382a8455fe7
Author: Valdis Klētnieks <valdis.kletnieks@vt.edu>
Date:   Thu Aug 8 01:28:08 2019 -0400

    netfilter: nf_tables: add missing prototypes.
    
    Sparse rightly complains about undeclared symbols.
    
      CHECK   net/netfilter/nft_set_hash.c
    net/netfilter/nft_set_hash.c:647:21: warning: symbol 'nft_set_rhash_type' was not declared. Should it be static?
    net/netfilter/nft_set_hash.c:670:21: warning: symbol 'nft_set_hash_type' was not declared. Should it be static?
    net/netfilter/nft_set_hash.c:690:21: warning: symbol 'nft_set_hash_fast_type' was not declared. Should it be static?
      CHECK   net/netfilter/nft_set_bitmap.c
    net/netfilter/nft_set_bitmap.c:296:21: warning: symbol 'nft_set_bitmap_type' was not declared. Should it be static?
      CHECK   net/netfilter/nft_set_rbtree.c
    net/netfilter/nft_set_rbtree.c:470:21: warning: symbol 'nft_set_rbtree_type' was not declared. Should it be static?
    
    Include nf_tables_core.h rather than nf_tables.h to pick up the additional definitions.
    
    Signed-off-by: Valdis Kletnieks <valdis.kletnieks@vt.edu>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 419d58ef802b..57123259452f 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -13,7 +13,7 @@
 #include <linux/netlink.h>
 #include <linux/netfilter.h>
 #include <linux/netfilter/nf_tables.h>
-#include <net/netfilter/nf_tables.h>
+#include <net/netfilter/nf_tables_core.h>
 
 struct nft_rbtree {
 	struct rb_root		root;

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 321a0036fdf5..419d58ef802b 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -1,10 +1,7 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Copyright (c) 2008-2009 Patrick McHardy <kaber@trash.net>
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
- *
  * Development of this code funded by Astaro AG (http://www.astaro.com/)
  */
 

commit 05b7639da55f5555b9866a1f4b7e8995232a6323
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Tue Mar 12 12:10:59 2019 +0100

    netfilter: nft_set_rbtree: check for inactive element after flag mismatch
    
    Otherwise, we hit bogus ENOENT when removing elements.
    
    Fixes: e701001e7cbe ("netfilter: nft_rbtree: allow adjacent intervals with dynamic updates")
    Reported-by: Václav Zindulka <vaclav.zindulka@tlapnet.cz>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index fa61208371f8..321a0036fdf5 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -308,10 +308,6 @@ static void *nft_rbtree_deactivate(const struct net *net,
 		else if (d > 0)
 			parent = parent->rb_right;
 		else {
-			if (!nft_set_elem_active(&rbe->ext, genmask)) {
-				parent = parent->rb_left;
-				continue;
-			}
 			if (nft_rbtree_interval_end(rbe) &&
 			    !nft_rbtree_interval_end(this)) {
 				parent = parent->rb_left;
@@ -320,6 +316,9 @@ static void *nft_rbtree_deactivate(const struct net *net,
 				   nft_rbtree_interval_end(this)) {
 				parent = parent->rb_right;
 				continue;
+			} else if (!nft_set_elem_active(&rbe->ext, genmask)) {
+				parent = parent->rb_left;
+				continue;
 			}
 			nft_rbtree_flush(net, set, rbe);
 			return rbe;

commit 3b18d5eba491b2328b31efa4235724a2354af010
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon Oct 1 13:27:32 2018 +0200

    netfilter: nft_set_rbtree: allow loose matching of closing element in interval
    
    Allow to find closest matching for the right side of an interval (end
    flag set on) so we allow lookups in inner ranges, eg. 10-20 in 5-25.
    
    Fixes: ba0e4d9917b4 ("netfilter: nf_tables: get set elements via netlink")
    Reported-by: Phil Sutter <phil@nwl.cc>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 0e5ec126f6ad..fa61208371f8 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -135,9 +135,12 @@ static bool __nft_rbtree_get(const struct net *net, const struct nft_set *set,
 		d = memcmp(this, key, set->klen);
 		if (d < 0) {
 			parent = rcu_dereference_raw(parent->rb_left);
-			interval = rbe;
+			if (!(flags & NFT_SET_ELEM_INTERVAL_END))
+				interval = rbe;
 		} else if (d > 0) {
 			parent = rcu_dereference_raw(parent->rb_right);
+			if (flags & NFT_SET_ELEM_INTERVAL_END)
+				interval = rbe;
 		} else {
 			if (!nft_set_elem_active(&rbe->ext, genmask))
 				parent = rcu_dereference_raw(parent->rb_left);
@@ -154,7 +157,10 @@ static bool __nft_rbtree_get(const struct net *net, const struct nft_set *set,
 
 	if (set->flags & NFT_SET_INTERVAL && interval != NULL &&
 	    nft_set_elem_active(&interval->ext, genmask) &&
-	    !nft_rbtree_interval_end(interval)) {
+	    ((!nft_rbtree_interval_end(interval) &&
+	      !(flags & NFT_SET_ELEM_INTERVAL_END)) ||
+	     (nft_rbtree_interval_end(interval) &&
+	      (flags & NFT_SET_ELEM_INTERVAL_END)))) {
 		*elem = interval;
 		return true;
 	}

commit a13f814a67b12a2f29d1decf4b4f4e700658a517
Author: Taehee Yoo <ap420073@gmail.com>
Date:   Thu Aug 30 17:56:52 2018 +0900

    netfilter: nft_set_rbtree: add missing rb_erase() in GC routine
    
    The nft_set_gc_batch_check() checks whether gc buffer is full.
    If gc buffer is full, gc buffer is released by
    the nft_set_gc_batch_complete() internally.
    In case of rbtree, the rb_erase() should be called before calling the
    nft_set_gc_batch_complete(). therefore the rb_erase() should
    be called before calling the nft_set_gc_batch_check() too.
    
    test commands:
       table ip filter {
               set set1 {
                       type ipv4_addr; flags interval, timeout;
                       gc-interval 10s;
                       timeout 1s;
                       elements = {
                               1-2,
                               3-4,
                               5-6,
                               ...
                               10000-10001,
                       }
               }
       }
       %nft -f test.nft
    
    splat looks like:
    [  430.273885] kasan: GPF could be caused by NULL-ptr deref or user memory access
    [  430.282158] general protection fault: 0000 [#1] SMP DEBUG_PAGEALLOC KASAN PTI
    [  430.283116] CPU: 1 PID: 190 Comm: kworker/1:2 Tainted: G    B             4.18.0+ #7
    [  430.283116] Workqueue: events_power_efficient nft_rbtree_gc [nf_tables_set]
    [  430.313559] RIP: 0010:rb_next+0x81/0x130
    [  430.313559] Code: 08 49 bd 00 00 00 00 00 fc ff df 48 bb 00 00 00 00 00 fc ff df 48 85 c0 75 05 eb 58 48 89 d4
    [  430.313559] RSP: 0018:ffff88010cdb7680 EFLAGS: 00010207
    [  430.313559] RAX: 0000000000b84854 RBX: dffffc0000000000 RCX: ffffffff83f01973
    [  430.313559] RDX: 000000000017090c RSI: 0000000000000008 RDI: 0000000000b84864
    [  430.313559] RBP: ffff8801060d4588 R08: fffffbfff09bc349 R09: fffffbfff09bc349
    [  430.313559] R10: 0000000000000001 R11: fffffbfff09bc348 R12: ffff880100f081a8
    [  430.313559] R13: dffffc0000000000 R14: ffff880100ff8688 R15: dffffc0000000000
    [  430.313559] FS:  0000000000000000(0000) GS:ffff88011b400000(0000) knlGS:0000000000000000
    [  430.313559] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [  430.313559] CR2: 0000000001551008 CR3: 000000005dc16000 CR4: 00000000001006e0
    [  430.313559] Call Trace:
    [  430.313559]  nft_rbtree_gc+0x112/0x5c0 [nf_tables_set]
    [  430.313559]  process_one_work+0xc13/0x1ec0
    [  430.313559]  ? _raw_spin_unlock_irq+0x29/0x40
    [  430.313559]  ? pwq_dec_nr_in_flight+0x3c0/0x3c0
    [  430.313559]  ? set_load_weight+0x270/0x270
    [  430.313559]  ? __switch_to_asm+0x34/0x70
    [  430.313559]  ? __switch_to_asm+0x40/0x70
    [  430.313559]  ? __switch_to_asm+0x34/0x70
    [  430.313559]  ? __switch_to_asm+0x34/0x70
    [  430.313559]  ? __switch_to_asm+0x40/0x70
    [  430.313559]  ? __switch_to_asm+0x34/0x70
    [  430.313559]  ? __switch_to_asm+0x40/0x70
    [  430.313559]  ? __switch_to_asm+0x34/0x70
    [  430.313559]  ? __switch_to_asm+0x34/0x70
    [  430.313559]  ? __switch_to_asm+0x40/0x70
    [  430.313559]  ? __switch_to_asm+0x34/0x70
    [  430.313559]  ? __schedule+0x6d3/0x1f50
    [  430.313559]  ? find_held_lock+0x39/0x1c0
    [  430.313559]  ? __sched_text_start+0x8/0x8
    [  430.313559]  ? cyc2ns_read_end+0x10/0x10
    [  430.313559]  ? save_trace+0x300/0x300
    [  430.313559]  ? sched_clock_local+0xd4/0x140
    [  430.313559]  ? find_held_lock+0x39/0x1c0
    [  430.313559]  ? worker_thread+0x353/0x1120
    [  430.313559]  ? worker_thread+0x353/0x1120
    [  430.313559]  ? lock_contended+0xe70/0xe70
    [  430.313559]  ? __lock_acquire+0x4500/0x4500
    [  430.535635]  ? do_raw_spin_unlock+0xa5/0x330
    [  430.535635]  ? do_raw_spin_trylock+0x101/0x1a0
    [  430.535635]  ? do_raw_spin_lock+0x1f0/0x1f0
    [  430.535635]  ? _raw_spin_lock_irq+0x10/0x70
    [  430.535635]  worker_thread+0x15d/0x1120
    [ ... ]
    
    Fixes: 8d8540c4f5e0 ("netfilter: nft_set_rbtree: add timeout support")
    Signed-off-by: Taehee Yoo <ap420073@gmail.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 55e2d9215c0d..0e5ec126f6ad 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -355,12 +355,11 @@ static void nft_rbtree_walk(const struct nft_ctx *ctx,
 
 static void nft_rbtree_gc(struct work_struct *work)
 {
+	struct nft_rbtree_elem *rbe, *rbe_end = NULL, *rbe_prev = NULL;
 	struct nft_set_gc_batch *gcb = NULL;
-	struct rb_node *node, *prev = NULL;
-	struct nft_rbtree_elem *rbe;
 	struct nft_rbtree *priv;
+	struct rb_node *node;
 	struct nft_set *set;
-	int i;
 
 	priv = container_of(work, struct nft_rbtree, gc_work.work);
 	set  = nft_set_container_of(priv);
@@ -371,7 +370,7 @@ static void nft_rbtree_gc(struct work_struct *work)
 		rbe = rb_entry(node, struct nft_rbtree_elem, node);
 
 		if (nft_rbtree_interval_end(rbe)) {
-			prev = node;
+			rbe_end = rbe;
 			continue;
 		}
 		if (!nft_set_elem_expired(&rbe->ext))
@@ -379,29 +378,30 @@ static void nft_rbtree_gc(struct work_struct *work)
 		if (nft_set_elem_mark_busy(&rbe->ext))
 			continue;
 
+		if (rbe_prev) {
+			rb_erase(&rbe_prev->node, &priv->root);
+			rbe_prev = NULL;
+		}
 		gcb = nft_set_gc_batch_check(set, gcb, GFP_ATOMIC);
 		if (!gcb)
 			break;
 
 		atomic_dec(&set->nelems);
 		nft_set_gc_batch_add(gcb, rbe);
+		rbe_prev = rbe;
 
-		if (prev) {
-			rbe = rb_entry(prev, struct nft_rbtree_elem, node);
+		if (rbe_end) {
 			atomic_dec(&set->nelems);
-			nft_set_gc_batch_add(gcb, rbe);
-			prev = NULL;
+			nft_set_gc_batch_add(gcb, rbe_end);
+			rb_erase(&rbe_end->node, &priv->root);
+			rbe_end = NULL;
 		}
 		node = rb_next(node);
 		if (!node)
 			break;
 	}
-	if (gcb) {
-		for (i = 0; i < gcb->head.cnt; i++) {
-			rbe = gcb->elems[i];
-			rb_erase(&rbe->node, &priv->root);
-		}
-	}
+	if (rbe_prev)
+		rb_erase(&rbe_prev->node, &priv->root);
 	write_seqcount_end(&priv->count);
 	write_unlock_bh(&priv->lock);
 

commit 4ef360dd6a65f6ef337645e1b65e744034754b19
Author: Taehee Yoo <ap420073@gmail.com>
Date:   Thu Jul 26 00:39:51 2018 +0900

    netfilter: nft_set: fix allocation size overflow in privsize callback.
    
    In order to determine allocation size of set, ->privsize is invoked.
    At this point, both desc->size and size of each data structure of set
    are used. desc->size means number of element that is given by user.
    desc->size is u32 type. so that upperlimit of set element is 4294967295.
    but return type of ->privsize is also u32. hence overflow can occurred.
    
    test commands:
       %nft add table ip filter
       %nft add set ip filter hash1 { type ipv4_addr \; size 4294967295 \; }
       %nft list ruleset
    
    splat looks like:
    [ 1239.202910] kasan: CONFIG_KASAN_INLINE enabled
    [ 1239.208788] kasan: GPF could be caused by NULL-ptr deref or user memory access
    [ 1239.217625] general protection fault: 0000 [#1] SMP DEBUG_PAGEALLOC KASAN PTI
    [ 1239.219329] CPU: 0 PID: 1603 Comm: nft Not tainted 4.18.0-rc5+ #7
    [ 1239.229091] RIP: 0010:nft_hash_walk+0x1d2/0x310 [nf_tables_set]
    [ 1239.229091] Code: 84 d2 7f 10 4c 89 e7 89 44 24 38 e8 d8 5a 17 e0 8b 44 24 38 48 8d 7b 10 41 0f b6 0c 24 48 89 fa 48 89 fe 48 c1 ea 03 83 e6 07 <42> 0f b6 14 3a 40 38 f2 7f 1a 84 d2 74 16
    [ 1239.229091] RSP: 0018:ffff8801118cf358 EFLAGS: 00010246
    [ 1239.229091] RAX: 0000000000000000 RBX: 0000000000020400 RCX: 0000000000000001
    [ 1239.229091] RDX: 0000000000004082 RSI: 0000000000000000 RDI: 0000000000020410
    [ 1239.229091] RBP: ffff880114d5a988 R08: 0000000000007e94 R09: ffff880114dd8030
    [ 1239.229091] R10: ffff880114d5a988 R11: ffffed00229bb006 R12: ffff8801118cf4d0
    [ 1239.229091] R13: ffff8801118cf4d8 R14: 0000000000000000 R15: dffffc0000000000
    [ 1239.229091] FS:  00007f5a8fe0b700(0000) GS:ffff88011b600000(0000) knlGS:0000000000000000
    [ 1239.229091] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 1239.229091] CR2: 00007f5a8ecc27b0 CR3: 000000010608e000 CR4: 00000000001006f0
    [ 1239.229091] Call Trace:
    [ 1239.229091]  ? nft_hash_remove+0xf0/0xf0 [nf_tables_set]
    [ 1239.229091]  ? memset+0x1f/0x40
    [ 1239.229091]  ? __nla_reserve+0x9f/0xb0
    [ 1239.229091]  ? memcpy+0x34/0x50
    [ 1239.229091]  nf_tables_dump_set+0x9a1/0xda0 [nf_tables]
    [ 1239.229091]  ? __kmalloc_reserve.isra.29+0x2e/0xa0
    [ 1239.229091]  ? nft_chain_hash_obj+0x630/0x630 [nf_tables]
    [ 1239.229091]  ? nf_tables_commit+0x2c60/0x2c60 [nf_tables]
    [ 1239.229091]  netlink_dump+0x470/0xa20
    [ 1239.229091]  __netlink_dump_start+0x5ae/0x690
    [ 1239.229091]  nft_netlink_dump_start_rcu+0xd1/0x160 [nf_tables]
    [ 1239.229091]  nf_tables_getsetelem+0x2e5/0x4b0 [nf_tables]
    [ 1239.229091]  ? nft_get_set_elem+0x440/0x440 [nf_tables]
    [ 1239.229091]  ? nft_chain_hash_obj+0x630/0x630 [nf_tables]
    [ 1239.229091]  ? nf_tables_dump_obj_done+0x70/0x70 [nf_tables]
    [ 1239.229091]  ? nla_parse+0xab/0x230
    [ 1239.229091]  ? nft_get_set_elem+0x440/0x440 [nf_tables]
    [ 1239.229091]  nfnetlink_rcv_msg+0x7f0/0xab0 [nfnetlink]
    [ 1239.229091]  ? nfnetlink_bind+0x1d0/0x1d0 [nfnetlink]
    [ 1239.229091]  ? debug_show_all_locks+0x290/0x290
    [ 1239.229091]  ? sched_clock_cpu+0x132/0x170
    [ 1239.229091]  ? find_held_lock+0x39/0x1b0
    [ 1239.229091]  ? sched_clock_local+0x10d/0x130
    [ 1239.229091]  netlink_rcv_skb+0x211/0x320
    [ 1239.229091]  ? nfnetlink_bind+0x1d0/0x1d0 [nfnetlink]
    [ 1239.229091]  ? netlink_ack+0x7b0/0x7b0
    [ 1239.229091]  ? ns_capable_common+0x6e/0x110
    [ 1239.229091]  nfnetlink_rcv+0x2d1/0x310 [nfnetlink]
    [ 1239.229091]  ? nfnetlink_rcv_batch+0x10f0/0x10f0 [nfnetlink]
    [ 1239.229091]  ? netlink_deliver_tap+0x829/0x930
    [ 1239.229091]  ? lock_acquire+0x265/0x2e0
    [ 1239.229091]  netlink_unicast+0x406/0x520
    [ 1239.509725]  ? netlink_attachskb+0x5b0/0x5b0
    [ 1239.509725]  ? find_held_lock+0x39/0x1b0
    [ 1239.509725]  netlink_sendmsg+0x987/0xa20
    [ 1239.509725]  ? netlink_unicast+0x520/0x520
    [ 1239.509725]  ? _copy_from_user+0xa9/0xc0
    [ 1239.509725]  __sys_sendto+0x21a/0x2c0
    [ 1239.509725]  ? __ia32_sys_getpeername+0xa0/0xa0
    [ 1239.509725]  ? retint_kernel+0x10/0x10
    [ 1239.509725]  ? sched_clock_cpu+0x132/0x170
    [ 1239.509725]  ? find_held_lock+0x39/0x1b0
    [ 1239.509725]  ? lock_downgrade+0x540/0x540
    [ 1239.509725]  ? up_read+0x1c/0x100
    [ 1239.509725]  ? __do_page_fault+0x763/0x970
    [ 1239.509725]  ? retint_user+0x18/0x18
    [ 1239.509725]  __x64_sys_sendto+0x177/0x180
    [ 1239.509725]  do_syscall_64+0xaa/0x360
    [ 1239.509725]  entry_SYSCALL_64_after_hwframe+0x49/0xbe
    [ 1239.509725] RIP: 0033:0x7f5a8f468e03
    [ 1239.509725] Code: 00 f7 d8 64 89 02 48 c7 c0 ff ff ff ff eb d0 0f 1f 84 00 00 00 00 00 83 3d 49 c9 2b 00 00 75 13 49 89 ca b8 2c 00 00 00 0f 05 <48> 3d 01 f0 ff ff 73 34 c3 48 83 ec 08 e8
    [ 1239.509725] RSP: 002b:00007ffd78d0b778 EFLAGS: 00000246 ORIG_RAX: 000000000000002c
    [ 1239.509725] RAX: ffffffffffffffda RBX: 00007ffd78d0c890 RCX: 00007f5a8f468e03
    [ 1239.509725] RDX: 0000000000000034 RSI: 00007ffd78d0b7e0 RDI: 0000000000000003
    [ 1239.509725] RBP: 00007ffd78d0b7d0 R08: 00007f5a8f15c160 R09: 000000000000000c
    [ 1239.509725] R10: 0000000000000000 R11: 0000000000000246 R12: 00007ffd78d0b7e0
    [ 1239.509725] R13: 0000000000000034 R14: 00007f5a8f9aff60 R15: 00005648040094b0
    [ 1239.509725] Modules linked in: nf_tables_set nf_tables nfnetlink ip_tables x_tables
    [ 1239.670713] ---[ end trace 39375adcda140f11 ]---
    [ 1239.676016] RIP: 0010:nft_hash_walk+0x1d2/0x310 [nf_tables_set]
    [ 1239.682834] Code: 84 d2 7f 10 4c 89 e7 89 44 24 38 e8 d8 5a 17 e0 8b 44 24 38 48 8d 7b 10 41 0f b6 0c 24 48 89 fa 48 89 fe 48 c1 ea 03 83 e6 07 <42> 0f b6 14 3a 40 38 f2 7f 1a 84 d2 74 16
    [ 1239.705108] RSP: 0018:ffff8801118cf358 EFLAGS: 00010246
    [ 1239.711115] RAX: 0000000000000000 RBX: 0000000000020400 RCX: 0000000000000001
    [ 1239.719269] RDX: 0000000000004082 RSI: 0000000000000000 RDI: 0000000000020410
    [ 1239.727401] RBP: ffff880114d5a988 R08: 0000000000007e94 R09: ffff880114dd8030
    [ 1239.735530] R10: ffff880114d5a988 R11: ffffed00229bb006 R12: ffff8801118cf4d0
    [ 1239.743658] R13: ffff8801118cf4d8 R14: 0000000000000000 R15: dffffc0000000000
    [ 1239.751785] FS:  00007f5a8fe0b700(0000) GS:ffff88011b600000(0000) knlGS:0000000000000000
    [ 1239.760993] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 1239.767560] CR2: 00007f5a8ecc27b0 CR3: 000000010608e000 CR4: 00000000001006f0
    [ 1239.775679] Kernel panic - not syncing: Fatal exception
    [ 1239.776630] Kernel Offset: 0x1f000000 from 0xffffffff81000000 (relocation range: 0xffffffff80000000-0xffffffffbfffffff)
    [ 1239.776630] Rebooting in 5 seconds..
    
    Fixes: 20a69341f2d0 ("netfilter: nf_tables: add netlink set API")
    Signed-off-by: Taehee Yoo <ap420073@gmail.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 9873d734b494..55e2d9215c0d 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -411,8 +411,8 @@ static void nft_rbtree_gc(struct work_struct *work)
 			   nft_set_gc_interval(set));
 }
 
-static unsigned int nft_rbtree_privsize(const struct nlattr * const nla[],
-					const struct nft_set_desc *desc)
+static u64 nft_rbtree_privsize(const struct nlattr * const nla[],
+			       const struct nft_set_desc *desc)
 {
 	return sizeof(struct nft_rbtree);
 }

commit c293ac959f809ee1cd31609d9e62bccf6804b2e6
Author: Taehee Yoo <ap420073@gmail.com>
Date:   Tue Jul 10 23:22:01 2018 +0900

    netfilter: nft_set_rbtree: fix panic when destroying set by GC
    
    This patch fixes below.
    1. check null pointer of rb_next.
     rb_next can return null. so null check routine should be added.
    2. add rcu_barrier in destroy routine.
     GC uses call_rcu to remove elements. but all elements should be
     removed before destroying set and chains. so that rcu_barrier is added.
    
    test script:
       %cat test.nft
       table inet aa {
               map map1 {
                       type ipv4_addr : verdict; flags interval, timeout;
                       elements = {
                               0-1 : jump a0,
                               3-4 : jump a0,
                               6-7 : jump a0,
                               9-10 : jump a0,
                               12-13 : jump a0,
                               15-16 : jump a0,
                               18-19 : jump a0,
                               21-22 : jump a0,
                               24-25 : jump a0,
                               27-28 : jump a0,
                       }
                       timeout 1s;
               }
               chain a0 {
               }
       }
       flush ruleset
       table inet aa {
               map map1 {
                       type ipv4_addr : verdict; flags interval, timeout;
                       elements = {
                               0-1 : jump a0,
                               3-4 : jump a0,
                               6-7 : jump a0,
                               9-10 : jump a0,
                               12-13 : jump a0,
                               15-16 : jump a0,
                               18-19 : jump a0,
                               21-22 : jump a0,
                               24-25 : jump a0,
                               27-28 : jump a0,
                       }
                       timeout 1s;
               }
               chain a0 {
               }
       }
       flush ruleset
    
    splat looks like:
    [ 2402.419838] kasan: GPF could be caused by NULL-ptr deref or user memory access
    [ 2402.428433] general protection fault: 0000 [#1] SMP DEBUG_PAGEALLOC KASAN PTI
    [ 2402.429343] CPU: 1 PID: 1350 Comm: kworker/1:1 Not tainted 4.18.0-rc2+ #1
    [ 2402.429343] Hardware name: To be filled by O.E.M. To be filled by O.E.M./Aptio CRB, BIOS 5.6.5 03/23/2017
    [ 2402.429343] Workqueue: events_power_efficient nft_rbtree_gc [nft_set_rbtree]
    [ 2402.429343] RIP: 0010:rb_next+0x1e/0x130
    [ 2402.429343] Code: e9 de f2 ff ff 0f 1f 80 00 00 00 00 41 55 48 89 fa 41 54 55 53 48 c1 ea 03 48 b8 00 00 00 0
    [ 2402.429343] RSP: 0018:ffff880105f77678 EFLAGS: 00010296
    [ 2402.429343] RAX: dffffc0000000000 RBX: ffff8801143e3428 RCX: 1ffff1002287c69c
    [ 2402.429343] RDX: 0000000000000000 RSI: 0000000000000004 RDI: 0000000000000000
    [ 2402.429343] RBP: 0000000000000000 R08: ffffed0016aabc24 R09: ffffed0016aabc24
    [ 2402.429343] R10: 0000000000000001 R11: ffffed0016aabc23 R12: 0000000000000000
    [ 2402.429343] R13: ffff8800b6933388 R14: dffffc0000000000 R15: ffff8801143e3440
    [ 2402.534486] kasan: CONFIG_KASAN_INLINE enabled
    [ 2402.534212] FS:  0000000000000000(0000) GS:ffff88011b600000(0000) knlGS:0000000000000000
    [ 2402.534212] CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
    [ 2402.534212] CR2: 0000000000863008 CR3: 00000000a3c16000 CR4: 00000000001006e0
    [ 2402.534212] Call Trace:
    [ 2402.534212]  nft_rbtree_gc+0x2b5/0x5f0 [nft_set_rbtree]
    [ 2402.534212]  process_one_work+0xc1b/0x1ee0
    [ 2402.540329] kasan: GPF could be caused by NULL-ptr deref or user memory access
    [ 2402.534212]  ? _raw_spin_unlock_irq+0x29/0x40
    [ 2402.534212]  ? pwq_dec_nr_in_flight+0x3e0/0x3e0
    [ 2402.534212]  ? set_load_weight+0x270/0x270
    [ 2402.534212]  ? __schedule+0x6ea/0x1fb0
    [ 2402.534212]  ? __sched_text_start+0x8/0x8
    [ 2402.534212]  ? save_trace+0x320/0x320
    [ 2402.534212]  ? sched_clock_local+0xe2/0x150
    [ 2402.534212]  ? find_held_lock+0x39/0x1c0
    [ 2402.534212]  ? worker_thread+0x35f/0x1150
    [ 2402.534212]  ? lock_contended+0xe90/0xe90
    [ 2402.534212]  ? __lock_acquire+0x4520/0x4520
    [ 2402.534212]  ? do_raw_spin_unlock+0xb1/0x350
    [ 2402.534212]  ? do_raw_spin_trylock+0x111/0x1b0
    [ 2402.534212]  ? do_raw_spin_lock+0x1f0/0x1f0
    [ 2402.534212]  worker_thread+0x169/0x1150
    
    Fixes: 8d8540c4f5e0("netfilter: nft_set_rbtree: add timeout support")
    Signed-off-by: Taehee Yoo <ap420073@gmail.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 1f8f257cb518..9873d734b494 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -381,7 +381,7 @@ static void nft_rbtree_gc(struct work_struct *work)
 
 		gcb = nft_set_gc_batch_check(set, gcb, GFP_ATOMIC);
 		if (!gcb)
-			goto out;
+			break;
 
 		atomic_dec(&set->nelems);
 		nft_set_gc_batch_add(gcb, rbe);
@@ -390,10 +390,12 @@ static void nft_rbtree_gc(struct work_struct *work)
 			rbe = rb_entry(prev, struct nft_rbtree_elem, node);
 			atomic_dec(&set->nelems);
 			nft_set_gc_batch_add(gcb, rbe);
+			prev = NULL;
 		}
 		node = rb_next(node);
+		if (!node)
+			break;
 	}
-out:
 	if (gcb) {
 		for (i = 0; i < gcb->head.cnt; i++) {
 			rbe = gcb->elems[i];
@@ -440,6 +442,7 @@ static void nft_rbtree_destroy(const struct nft_set *set)
 	struct rb_node *node;
 
 	cancel_delayed_work_sync(&priv->gc_work);
+	rcu_barrier();
 	while ((node = priv->root.rb_node) != NULL) {
 		rb_erase(node, &priv->root);
 		rbe = rb_entry(node, struct nft_rbtree_elem, node);

commit e240cd0df48185a28c153f83a39ba3940e3e9b86
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Fri Jul 6 19:06:43 2018 +0200

    netfilter: nf_tables: place all set backends in one single module
    
    This patch disallows rbtree with single elements, which is causing
    problems with the recent timeout support. Before this patch, you
    could opt out individual set representations per module, which is
    just adding extra complexity.
    
    Fixes: 8d8540c4f5e0("netfilter: nft_set_rbtree: add timeout support")
    Reported-by: Taehee Yoo <ap420073@gmail.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 7f3a9a211034..1f8f257cb518 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -462,7 +462,7 @@ static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 	return true;
 }
 
-static struct nft_set_type nft_rbtree_type __read_mostly = {
+struct nft_set_type nft_set_rbtree_type __read_mostly = {
 	.owner		= THIS_MODULE,
 	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT | NFT_SET_TIMEOUT,
 	.ops		= {
@@ -481,20 +481,3 @@ static struct nft_set_type nft_rbtree_type __read_mostly = {
 		.get		= nft_rbtree_get,
 	},
 };
-
-static int __init nft_rbtree_module_init(void)
-{
-	return nft_register_set(&nft_rbtree_type);
-}
-
-static void __exit nft_rbtree_module_exit(void)
-{
-	nft_unregister_set(&nft_rbtree_type);
-}
-
-module_init(nft_rbtree_module_init);
-module_exit(nft_rbtree_module_exit);
-
-MODULE_LICENSE("GPL");
-MODULE_AUTHOR("Patrick McHardy <kaber@trash.net>");
-MODULE_ALIAS_NFT_SET();

commit a08ce73ba0a89be9f1418cba45fe94b39f9b73af
Merge: 349b71d6f427 d8e87fc6d11c
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jun 11 14:24:32 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/pablo/nf
    
    Pablo Neira Ayuso says:
    
    ====================
    Netfilter/IPVS fixes for net
    
    The following patchset contains Netfilter/IPVS fixes for your net tree:
    
    1) Reject non-null terminated helper names from xt_CT, from Gao Feng.
    
    2) Fix KASAN splat due to out-of-bound access from commit phase, from
       Alexey Kodanev.
    
    3) Missing conntrack hook registration on IPVS FTP helper, from Julian
       Anastasov.
    
    4) Incorrect skbuff allocation size in bridge nft_reject, from Taehee Yoo.
    
    5) Fix inverted check on packet xmit to non-local addresses, also from
       Julian.
    
    6) Fix ebtables alignment compat problems, from Alin Nastac.
    
    7) Hook mask checks are not correct in xt_set, from Serhey Popovych.
    
    8) Fix timeout listing of element in ipsets, from Jozsef.
    
    9) Cap maximum timeout value in ipset, also from Jozsef.
    
    10) Don't allow family option for hash:mac sets, from Florent Fourcot.
    
    11) Restrict ebtables to work with NFPROTO_BRIDGE targets only, this
        Florian.
    
    12) Another bug reported by KASAN in the rbtree set backend, from
        Taehee Yoo.
    
    13) Missing __IPS_MAX_BIT update doesn't include IPS_OFFLOAD_BIT.
        From Gao Feng.
    
    14) Missing initialization of match/target in ebtables, from Florian
        Westphal.
    
    15) Remove useless nft_dup.h file in include path, from C. Labbe.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 82e20b44477ffe90a5866caa209ecc9df818c6a1
Author: Taehee Yoo <ap420073@gmail.com>
Date:   Thu Jun 7 02:05:12 2018 +0900

    netfilter: nft_set_rbtree: fix parameter of __nft_rbtree_lookup()
    
    The parameter this doesn't have a flags value. so that it can't be
    used by nft_rbtree_interval_end().
    
    test commands:
       %nft add table ip filter
       %nft add set ip filter s { type ipv4_addr \; flags interval \; }
       %nft add element ip filter s {0-1}
       %nft add element ip filter s {2-10}
       %nft add chain ip filter input { type filter hook input priority 0\; }
       %nft add rule ip filter input ip saddr @s
    
    Splat looks like:
    [  246.752502] BUG: KASAN: slab-out-of-bounds in __nft_rbtree_lookup+0x677/0x6a0 [nft_set_rbtree]
    [  246.752502] Read of size 1 at addr ffff88010d9efa47 by task http/1092
    
    [  246.752502] CPU: 1 PID: 1092 Comm: http Not tainted 4.17.0-rc6+ #185
    [  246.752502] Call Trace:
    [  246.752502]  <IRQ>
    [  246.752502]  dump_stack+0x74/0xbb
    [  246.752502]  ? __nft_rbtree_lookup+0x677/0x6a0 [nft_set_rbtree]
    [  246.752502]  print_address_description+0xc7/0x290
    [  246.752502]  ? __nft_rbtree_lookup+0x677/0x6a0 [nft_set_rbtree]
    [  246.752502]  kasan_report+0x22c/0x350
    [  246.752502]  __nft_rbtree_lookup+0x677/0x6a0 [nft_set_rbtree]
    [  246.752502]  nft_rbtree_lookup+0xc9/0x2d2 [nft_set_rbtree]
    [  246.752502]  ? sched_clock_cpu+0x144/0x180
    [  246.752502]  nft_lookup_eval+0x149/0x3a0 [nf_tables]
    [  246.752502]  ? __lock_acquire+0xcea/0x4ed0
    [  246.752502]  ? nft_lookup_init+0x6b0/0x6b0 [nf_tables]
    [  246.752502]  nft_do_chain+0x263/0xf50 [nf_tables]
    [  246.752502]  ? __nft_trace_packet+0x1a0/0x1a0 [nf_tables]
    [  246.752502]  ? sched_clock_cpu+0x144/0x180
    [ ... ]
    
    Fixes: f9121355eb6f ("netfilter: nft_set_rbtree: incorrect assumption on lower interval lookups")
    Signed-off-by: Taehee Yoo <ap420073@gmail.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index e6f08bc5f359..26fa93b23805 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -65,7 +65,7 @@ static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set
 			parent = rcu_dereference_raw(parent->rb_left);
 			if (interval &&
 			    nft_rbtree_equal(set, this, interval) &&
-			    nft_rbtree_interval_end(this) &&
+			    nft_rbtree_interval_end(rbe) &&
 			    !nft_rbtree_interval_end(interval))
 				continue;
 			interval = rbe;

commit 8d8540c4f5e03d847c004e71d6a577bf4f8c78cd
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed May 16 22:58:34 2018 +0200

    netfilter: nft_set_rbtree: add timeout support
    
    Add garbage collection logic to expire elements stored in the rb-tree
    representation.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 22c57d7612c4..d260ce2d6671 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -22,6 +22,7 @@ struct nft_rbtree {
 	struct rb_root		root;
 	rwlock_t		lock;
 	seqcount_t		count;
+	struct delayed_work	gc_work;
 };
 
 struct nft_rbtree_elem {
@@ -265,6 +266,7 @@ static void nft_rbtree_activate(const struct net *net,
 	struct nft_rbtree_elem *rbe = elem->priv;
 
 	nft_set_elem_change_active(net, set, &rbe->ext);
+	nft_set_elem_clear_busy(&rbe->ext);
 }
 
 static bool nft_rbtree_flush(const struct net *net,
@@ -272,8 +274,12 @@ static bool nft_rbtree_flush(const struct net *net,
 {
 	struct nft_rbtree_elem *rbe = priv;
 
-	nft_set_elem_change_active(net, set, &rbe->ext);
-	return true;
+	if (!nft_set_elem_mark_busy(&rbe->ext) ||
+	    !nft_is_active(net, &rbe->ext)) {
+		nft_set_elem_change_active(net, set, &rbe->ext);
+		return true;
+	}
+	return false;
 }
 
 static void *nft_rbtree_deactivate(const struct net *net,
@@ -347,6 +353,62 @@ static void nft_rbtree_walk(const struct nft_ctx *ctx,
 	read_unlock_bh(&priv->lock);
 }
 
+static void nft_rbtree_gc(struct work_struct *work)
+{
+	struct nft_set_gc_batch *gcb = NULL;
+	struct rb_node *node, *prev = NULL;
+	struct nft_rbtree_elem *rbe;
+	struct nft_rbtree *priv;
+	struct nft_set *set;
+	int i;
+
+	priv = container_of(work, struct nft_rbtree, gc_work.work);
+	set  = nft_set_container_of(priv);
+
+	write_lock_bh(&priv->lock);
+	write_seqcount_begin(&priv->count);
+	for (node = rb_first(&priv->root); node != NULL; node = rb_next(node)) {
+		rbe = rb_entry(node, struct nft_rbtree_elem, node);
+
+		if (nft_rbtree_interval_end(rbe)) {
+			prev = node;
+			continue;
+		}
+		if (!nft_set_elem_expired(&rbe->ext))
+			continue;
+		if (nft_set_elem_mark_busy(&rbe->ext))
+			continue;
+
+		gcb = nft_set_gc_batch_check(set, gcb, GFP_ATOMIC);
+		if (!gcb)
+			goto out;
+
+		atomic_dec(&set->nelems);
+		nft_set_gc_batch_add(gcb, rbe);
+
+		if (prev) {
+			rbe = rb_entry(prev, struct nft_rbtree_elem, node);
+			atomic_dec(&set->nelems);
+			nft_set_gc_batch_add(gcb, rbe);
+		}
+		node = rb_next(node);
+	}
+out:
+	if (gcb) {
+		for (i = 0; i < gcb->head.cnt; i++) {
+			rbe = gcb->elems[i];
+			rb_erase(&rbe->node, &priv->root);
+		}
+	}
+	write_seqcount_end(&priv->count);
+	write_unlock_bh(&priv->lock);
+
+	nft_set_gc_batch_complete(gcb);
+
+	queue_delayed_work(system_power_efficient_wq, &priv->gc_work,
+			   nft_set_gc_interval(set));
+}
+
 static unsigned int nft_rbtree_privsize(const struct nlattr * const nla[],
 					const struct nft_set_desc *desc)
 {
@@ -362,6 +424,12 @@ static int nft_rbtree_init(const struct nft_set *set,
 	rwlock_init(&priv->lock);
 	seqcount_init(&priv->count);
 	priv->root = RB_ROOT;
+
+	INIT_DEFERRABLE_WORK(&priv->gc_work, nft_rbtree_gc);
+	if (set->flags & NFT_SET_TIMEOUT)
+		queue_delayed_work(system_power_efficient_wq, &priv->gc_work,
+				   nft_set_gc_interval(set));
+
 	return 0;
 }
 
@@ -371,6 +439,7 @@ static void nft_rbtree_destroy(const struct nft_set *set)
 	struct nft_rbtree_elem *rbe;
 	struct rb_node *node;
 
+	cancel_delayed_work_sync(&priv->gc_work);
 	while ((node = priv->root.rb_node) != NULL) {
 		rb_erase(node, &priv->root);
 		rbe = rb_entry(node, struct nft_rbtree_elem, node);
@@ -395,7 +464,7 @@ static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 
 static struct nft_set_type nft_rbtree_type __read_mostly = {
 	.owner		= THIS_MODULE,
-	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT,
+	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT | NFT_SET_TIMEOUT,
 	.ops		= {
 		.privsize	= nft_rbtree_privsize,
 		.elemsize	= offsetof(struct nft_rbtree_elem, ext),

commit 71cc0873e0e0a4c6dca899c42e3ac143f7960d8e
Author: Phil Sutter <phil@nwl.cc>
Date:   Tue Apr 3 23:15:39 2018 +0200

    netfilter: nf_tables: Simplify set backend selection
    
    Drop nft_set_type's ability to act as a container of multiple backend
    implementations it chooses from. Instead consolidate the whole selection
    logic in nft_select_set_ops() and the actual backend provided estimate()
    callback.
    
    This turns nf_tables_set_types into a list containing all available
    backends which is traversed when selecting one matching userspace
    requested criteria.
    
    Also, this change allows to embed nft_set_ops structure into
    nft_set_type and pull flags field into the latter as it's only used
    during selection phase.
    
    A crucial part of this change is to make sure the new layout respects
    hash backend constraints formerly enforced by nft_hash_select_ops()
    function: This is achieved by introduction of a specific estimate()
    callback for nft_hash_fast_ops which returns false for key lengths != 4.
    In turn, nft_hash_estimate() is changed to return false for key lengths
    == 4 so it won't be chosen by accident. Also, both callbacks must return
    false for unbounded sets as their size estimate depends on a known
    maximum element count.
    
    Note that this patch partially reverts commit 4f2921ca21b71 ("netfilter:
    nf_tables: meter: pick a set backend that supports updates") by making
    nft_set_ops_candidate() not explicitly look for an update callback but
    make NFT_SET_EVAL a regular backend feature flag which is checked along
    with the others. This way all feature requirements are checked in one
    go.
    
    Signed-off-by: Phil Sutter <phil@nwl.cc>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index e6f08bc5f359..22c57d7612c4 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -393,28 +393,24 @@ static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 	return true;
 }
 
-static struct nft_set_type nft_rbtree_type;
-static struct nft_set_ops nft_rbtree_ops __read_mostly = {
-	.type		= &nft_rbtree_type,
-	.privsize	= nft_rbtree_privsize,
-	.elemsize	= offsetof(struct nft_rbtree_elem, ext),
-	.estimate	= nft_rbtree_estimate,
-	.init		= nft_rbtree_init,
-	.destroy	= nft_rbtree_destroy,
-	.insert		= nft_rbtree_insert,
-	.remove		= nft_rbtree_remove,
-	.deactivate	= nft_rbtree_deactivate,
-	.flush		= nft_rbtree_flush,
-	.activate	= nft_rbtree_activate,
-	.lookup		= nft_rbtree_lookup,
-	.walk		= nft_rbtree_walk,
-	.get		= nft_rbtree_get,
-	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT,
-};
-
 static struct nft_set_type nft_rbtree_type __read_mostly = {
-	.ops		= &nft_rbtree_ops,
 	.owner		= THIS_MODULE,
+	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT,
+	.ops		= {
+		.privsize	= nft_rbtree_privsize,
+		.elemsize	= offsetof(struct nft_rbtree_elem, ext),
+		.estimate	= nft_rbtree_estimate,
+		.init		= nft_rbtree_init,
+		.destroy	= nft_rbtree_destroy,
+		.insert		= nft_rbtree_insert,
+		.remove		= nft_rbtree_remove,
+		.deactivate	= nft_rbtree_deactivate,
+		.flush		= nft_rbtree_flush,
+		.activate	= nft_rbtree_activate,
+		.lookup		= nft_rbtree_lookup,
+		.walk		= nft_rbtree_walk,
+		.get		= nft_rbtree_get,
+	},
 };
 
 static int __init nft_rbtree_module_init(void)

commit ba0e4d9917b43dfa746cbbcb4477da59aae73bd6
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon Oct 9 19:52:28 2017 +0200

    netfilter: nf_tables: get set elements via netlink
    
    This patch adds a new get operation to look up for specific elements in
    a set via netlink interface. You can also use it to check if an interval
    already exists.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index d83a4ec5900d..e6f08bc5f359 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -113,6 +113,78 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 	return ret;
 }
 
+static bool __nft_rbtree_get(const struct net *net, const struct nft_set *set,
+			     const u32 *key, struct nft_rbtree_elem **elem,
+			     unsigned int seq, unsigned int flags, u8 genmask)
+{
+	struct nft_rbtree_elem *rbe, *interval = NULL;
+	struct nft_rbtree *priv = nft_set_priv(set);
+	const struct rb_node *parent;
+	const void *this;
+	int d;
+
+	parent = rcu_dereference_raw(priv->root.rb_node);
+	while (parent != NULL) {
+		if (read_seqcount_retry(&priv->count, seq))
+			return false;
+
+		rbe = rb_entry(parent, struct nft_rbtree_elem, node);
+
+		this = nft_set_ext_key(&rbe->ext);
+		d = memcmp(this, key, set->klen);
+		if (d < 0) {
+			parent = rcu_dereference_raw(parent->rb_left);
+			interval = rbe;
+		} else if (d > 0) {
+			parent = rcu_dereference_raw(parent->rb_right);
+		} else {
+			if (!nft_set_elem_active(&rbe->ext, genmask))
+				parent = rcu_dereference_raw(parent->rb_left);
+
+			if (!nft_set_ext_exists(&rbe->ext, NFT_SET_EXT_FLAGS) ||
+			    (*nft_set_ext_flags(&rbe->ext) & NFT_SET_ELEM_INTERVAL_END) ==
+			    (flags & NFT_SET_ELEM_INTERVAL_END)) {
+				*elem = rbe;
+				return true;
+			}
+			return false;
+		}
+	}
+
+	if (set->flags & NFT_SET_INTERVAL && interval != NULL &&
+	    nft_set_elem_active(&interval->ext, genmask) &&
+	    !nft_rbtree_interval_end(interval)) {
+		*elem = interval;
+		return true;
+	}
+
+	return false;
+}
+
+static void *nft_rbtree_get(const struct net *net, const struct nft_set *set,
+			    const struct nft_set_elem *elem, unsigned int flags)
+{
+	struct nft_rbtree *priv = nft_set_priv(set);
+	unsigned int seq = read_seqcount_begin(&priv->count);
+	struct nft_rbtree_elem *rbe = ERR_PTR(-ENOENT);
+	const u32 *key = (const u32 *)&elem->key.val;
+	u8 genmask = nft_genmask_cur(net);
+	bool ret;
+
+	ret = __nft_rbtree_get(net, set, key, &rbe, seq, flags, genmask);
+	if (ret || !read_seqcount_retry(&priv->count, seq))
+		return rbe;
+
+	read_lock_bh(&priv->lock);
+	seq = read_seqcount_begin(&priv->count);
+	ret = __nft_rbtree_get(net, set, key, &rbe, seq, flags, genmask);
+	if (!ret)
+		rbe = ERR_PTR(-ENOENT);
+	read_unlock_bh(&priv->lock);
+
+	return rbe;
+}
+
 static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			       struct nft_rbtree_elem *new,
 			       struct nft_set_ext **ext)
@@ -336,6 +408,7 @@ static struct nft_set_ops nft_rbtree_ops __read_mostly = {
 	.activate	= nft_rbtree_activate,
 	.lookup		= nft_rbtree_lookup,
 	.walk		= nft_rbtree_walk,
+	.get		= nft_rbtree_get,
 	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT,
 };
 

commit 9b7e26aee7cf27ffb37bb2f17229cecc89a833bd
Author: Florian Westphal <fw@strlen.de>
Date:   Fri Jul 28 10:34:42 2017 +0200

    netfilter: nft_set_rbtree: use seqcount to avoid lock in most cases
    
    switch to lockless lockup. write side now also increments sequence
    counter.  On lookup, sample counter value and only take the lock
    if we did not find a match and the counter has changed.
    
    This avoids need to write to private area in normal (lookup) cases.
    
    In case we detect a writer (seqretry is true) we fall back to taking
    the readlock.
    
    The readlock is also used during dumps to ensure we get a consistent
    tree walk.
    
    Similar technique (rbtree+seqlock) was used by David Howells in rxrpc.
    
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index bce5382f1d49..d83a4ec5900d 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -19,8 +19,9 @@
 #include <net/netfilter/nf_tables.h>
 
 struct nft_rbtree {
-	rwlock_t		lock;
 	struct rb_root		root;
+	rwlock_t		lock;
+	seqcount_t		count;
 };
 
 struct nft_rbtree_elem {
@@ -40,8 +41,9 @@ static bool nft_rbtree_equal(const struct nft_set *set, const void *this,
 	return memcmp(this, nft_set_ext_key(&interval->ext), set->klen) == 0;
 }
 
-static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
-			      const u32 *key, const struct nft_set_ext **ext)
+static bool __nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
+				const u32 *key, const struct nft_set_ext **ext,
+				unsigned int seq)
 {
 	struct nft_rbtree *priv = nft_set_priv(set);
 	const struct nft_rbtree_elem *rbe, *interval = NULL;
@@ -50,15 +52,17 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 	const void *this;
 	int d;
 
-	read_lock_bh(&priv->lock);
-	parent = priv->root.rb_node;
+	parent = rcu_dereference_raw(priv->root.rb_node);
 	while (parent != NULL) {
+		if (read_seqcount_retry(&priv->count, seq))
+			return false;
+
 		rbe = rb_entry(parent, struct nft_rbtree_elem, node);
 
 		this = nft_set_ext_key(&rbe->ext);
 		d = memcmp(this, key, set->klen);
 		if (d < 0) {
-			parent = parent->rb_left;
+			parent = rcu_dereference_raw(parent->rb_left);
 			if (interval &&
 			    nft_rbtree_equal(set, this, interval) &&
 			    nft_rbtree_interval_end(this) &&
@@ -66,15 +70,14 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 				continue;
 			interval = rbe;
 		} else if (d > 0)
-			parent = parent->rb_right;
+			parent = rcu_dereference_raw(parent->rb_right);
 		else {
 			if (!nft_set_elem_active(&rbe->ext, genmask)) {
-				parent = parent->rb_left;
+				parent = rcu_dereference_raw(parent->rb_left);
 				continue;
 			}
 			if (nft_rbtree_interval_end(rbe))
 				goto out;
-			read_unlock_bh(&priv->lock);
 
 			*ext = &rbe->ext;
 			return true;
@@ -84,15 +87,32 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 	if (set->flags & NFT_SET_INTERVAL && interval != NULL &&
 	    nft_set_elem_active(&interval->ext, genmask) &&
 	    !nft_rbtree_interval_end(interval)) {
-		read_unlock_bh(&priv->lock);
 		*ext = &interval->ext;
 		return true;
 	}
 out:
-	read_unlock_bh(&priv->lock);
 	return false;
 }
 
+static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
+			      const u32 *key, const struct nft_set_ext **ext)
+{
+	struct nft_rbtree *priv = nft_set_priv(set);
+	unsigned int seq = read_seqcount_begin(&priv->count);
+	bool ret;
+
+	ret = __nft_rbtree_lookup(net, set, key, ext, seq);
+	if (ret || !read_seqcount_retry(&priv->count, seq))
+		return ret;
+
+	read_lock_bh(&priv->lock);
+	seq = read_seqcount_begin(&priv->count);
+	ret = __nft_rbtree_lookup(net, set, key, ext, seq);
+	read_unlock_bh(&priv->lock);
+
+	return ret;
+}
+
 static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			       struct nft_rbtree_elem *new,
 			       struct nft_set_ext **ext)
@@ -130,7 +150,7 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			}
 		}
 	}
-	rb_link_node(&new->node, parent, p);
+	rb_link_node_rcu(&new->node, parent, p);
 	rb_insert_color(&new->node, &priv->root);
 	return 0;
 }
@@ -144,7 +164,9 @@ static int nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 	int err;
 
 	write_lock_bh(&priv->lock);
+	write_seqcount_begin(&priv->count);
 	err = __nft_rbtree_insert(net, set, rbe, ext);
+	write_seqcount_end(&priv->count);
 	write_unlock_bh(&priv->lock);
 
 	return err;
@@ -158,7 +180,9 @@ static void nft_rbtree_remove(const struct net *net,
 	struct nft_rbtree_elem *rbe = elem->priv;
 
 	write_lock_bh(&priv->lock);
+	write_seqcount_begin(&priv->count);
 	rb_erase(&rbe->node, &priv->root);
+	write_seqcount_end(&priv->count);
 	write_unlock_bh(&priv->lock);
 }
 
@@ -264,6 +288,7 @@ static int nft_rbtree_init(const struct nft_set *set,
 	struct nft_rbtree *priv = nft_set_priv(set);
 
 	rwlock_init(&priv->lock);
+	seqcount_init(&priv->count);
 	priv->root = RB_ROOT;
 	return 0;
 }

commit 52a623bd6189b6ea8f06a0d7594c7604deaab24a
Merge: fcce2fdbf478 04ba724b659c
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jun 30 06:27:09 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/pablo/nf-next
    
    Pablo Neira Ayuso says:
    
    ====================
    Netfilter updates for net-next
    
    The following patchset contains Netfilter updates for your net-next
    tree. This batch contains connection tracking updates for the cleanup
    iteration path, patches from Florian Westphal:
    
    X) Skip unconfirmed conntracks in nf_ct_iterate_cleanup_net(), just set
       dying bit to let the CPU release them.
    
    X) Add nf_ct_iterate_destroy() to be used on module removal, to kill
       conntrack from all namespace.
    
    X) Restart iteration on hashtable resizing, since both may occur at
       the same time.
    
    X) Use the new nf_ct_iterate_destroy() to remove conntrack with NAT
       mapping on module removal.
    
    X) Use nf_ct_iterate_destroy() to remove conntrack entries helper
       module removal, from Liping Zhang.
    
    X) Use nf_ct_iterate_cleanup_net() to remove the timeout extension
       if user requests this, also from Liping.
    
    X) Add net_ns_barrier() and use it from FTP helper, so make sure
       no concurrent namespace removal happens at the same time while
       the helper module is being removed.
    
    X) Use NFPROTO_MAX in layer 3 conntrack protocol array, to reduce
       module size. Same thing in nf_tables.
    
    Updates for the nf_tables infrastructure:
    
    X) Prepare usage of the extended ACK reporting infrastructure for
       nf_tables.
    
    X) Remove unnecessary forward declaration in nf_tables hash set.
    
    X) Skip set size estimation if number of element is not specified.
    
    X) Changes to accomodate a (faster) unresizable hash set implementation,
       for anonymous sets and dynamic size fixed sets with no timeouts.
    
    X) Faster lookup function for unresizable hash table for 2 and 4
       bytes key.
    
    And, finally, a bunch of asorted small updates and cleanups:
    
    X) Do not hold reference to netdev from ipt_CLUSTER, instead subscribe
       to device events and look up for index from the packet path, this
       is fixing an issue that is present since the very beginning, patch
       from Xin Long.
    
    X) Use nf_register_net_hook() in ipt_CLUSTER, from Florian Westphal.
    
    X) Use ebt_invalid_target() whenever possible in the ebtables tree,
       from Gao Feng.
    
    X) Calm down compilation warning in nf_dup infrastructure, patch from
       stephen hemminger.
    
    X) Statify functions in nftables rt expression, also from stephen.
    
    X) Update Makefile to use canonical method to specify nf_tables-objs.
       From Jike Song.
    
    X) Use nf_conntrack_helpers_register() in amanda and H323.
    
    X) Space cleanup for ctnetlink, from linzhang.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 347b408d59e7eadcd09f97eba96fa4c270eb3b23
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon May 22 17:47:54 2017 +0100

    netfilter: nf_tables: pass set description to ->privsize
    
    The new non-resizable hashtable variant needs this to calculate the
    size of the bucket array.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 29d41d378339..491e805d3ca2 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -251,7 +251,8 @@ static void nft_rbtree_walk(const struct nft_ctx *ctx,
 	read_unlock_bh(&priv->lock);
 }
 
-static unsigned int nft_rbtree_privsize(const struct nlattr * const nla[])
+static unsigned int nft_rbtree_privsize(const struct nlattr * const nla[],
+					const struct nft_set_desc *desc)
 {
 	return sizeof(struct nft_rbtree);
 }

commit 2b664957c27fe708035b217c908edd1048be355e
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon May 22 17:47:51 2017 +0100

    netfilter: nf_tables: select set backend flavour depending on description
    
    This patch adds the infrastructure to support several implementations of
    the same set type. This selection will be based on the set description
    and the features available for this set. This allow us to select set
    backend implementation that will result in better performance numbers.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index fbfb3cbb3916..29d41d378339 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -295,7 +295,9 @@ static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 	return true;
 }
 
+static struct nft_set_type nft_rbtree_type;
 static struct nft_set_ops nft_rbtree_ops __read_mostly = {
+	.type		= &nft_rbtree_type,
 	.privsize	= nft_rbtree_privsize,
 	.elemsize	= offsetof(struct nft_rbtree_elem, ext),
 	.estimate	= nft_rbtree_estimate,
@@ -309,17 +311,21 @@ static struct nft_set_ops nft_rbtree_ops __read_mostly = {
 	.lookup		= nft_rbtree_lookup,
 	.walk		= nft_rbtree_walk,
 	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT,
+};
+
+static struct nft_set_type nft_rbtree_type __read_mostly = {
+	.ops		= &nft_rbtree_ops,
 	.owner		= THIS_MODULE,
 };
 
 static int __init nft_rbtree_module_init(void)
 {
-	return nft_register_set(&nft_rbtree_ops);
+	return nft_register_set(&nft_rbtree_type);
 }
 
 static void __exit nft_rbtree_module_exit(void)
 {
-	nft_unregister_set(&nft_rbtree_ops);
+	nft_unregister_set(&nft_rbtree_type);
 }
 
 module_init(nft_rbtree_module_init);

commit 080ed636a559e960010b714dee035dddacbe73b9
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon May 22 17:47:45 2017 +0100

    netfilter: nf_tables: no size estimation if number of set elements is unknown
    
    This size estimation is ignored by the existing set backend selection
    logic, since this estimation structure is stack allocated, set this to
    ~0 to make it easier to catch bugs in future changes.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index e97e2fb53f0a..fbfb3cbb3916 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -283,13 +283,11 @@ static void nft_rbtree_destroy(const struct nft_set *set)
 static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 				struct nft_set_estimate *est)
 {
-	unsigned int nsize;
-
-	nsize = sizeof(struct nft_rbtree_elem);
 	if (desc->size)
-		est->size = sizeof(struct nft_rbtree) + desc->size * nsize;
+		est->size = sizeof(struct nft_rbtree) +
+			    desc->size * sizeof(struct nft_rbtree_elem);
 	else
-		est->size = nsize;
+		est->size = ~0;
 
 	est->lookup = NFT_SET_CLASS_O_LOG_N;
 	est->space  = NFT_SET_CLASS_O_N;

commit d2df92e98a34a5619dadd29c6291113c009181e7
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Sun May 21 00:37:10 2017 +0200

    netfilter: nft_set_rbtree: handle element re-addition after deletion
    
    The existing code selects no next branch to be inspected when
    re-inserting an inactive element into the rb-tree, looping endlessly.
    This patch restricts the check for active elements to the EEXIST case
    only.
    
    Fixes: e701001e7cbe ("netfilter: nft_rbtree: allow adjacent intervals with dynamic updates")
    Reported-by: Wolfgang Bumiller <w.bumiller@proxmox.com>
    Tested-by: Wolfgang Bumiller <w.bumiller@proxmox.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index e97e2fb53f0a..fbdbaa00dd5f 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -116,17 +116,17 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 		else if (d > 0)
 			p = &parent->rb_right;
 		else {
-			if (nft_set_elem_active(&rbe->ext, genmask)) {
-				if (nft_rbtree_interval_end(rbe) &&
-				    !nft_rbtree_interval_end(new))
-					p = &parent->rb_left;
-				else if (!nft_rbtree_interval_end(rbe) &&
-					 nft_rbtree_interval_end(new))
-					p = &parent->rb_right;
-				else {
-					*ext = &rbe->ext;
-					return -EEXIST;
-				}
+			if (nft_rbtree_interval_end(rbe) &&
+			    !nft_rbtree_interval_end(new)) {
+				p = &parent->rb_left;
+			} else if (!nft_rbtree_interval_end(rbe) &&
+				   nft_rbtree_interval_end(new)) {
+				p = &parent->rb_right;
+			} else if (nft_set_elem_active(&rbe->ext, genmask)) {
+				*ext = &rbe->ext;
+				return -EEXIST;
+			} else {
+				p = &parent->rb_left;
 			}
 		}
 	}

commit 03e5fd0e9bcc1f34b7a542786b34b8f771e7c260
Author: Liping Zhang <zlpnobody@gmail.com>
Date:   Sun Mar 12 19:38:47 2017 +0800

    netfilter: nft_set_rbtree: use per-set rwlock to improve the scalability
    
    Karel Rericha reported that in his test case, ICMP packets going through
    boxes had normally about 5ms latency. But when running nft, actually
    listing the sets with interval flags, latency would go up to 30-100ms.
    This was observed when router throughput is from 600Mbps to 2Gbps.
    
    This is because we use a single global spinlock to protect the whole
    rbtree sets, so "dumping sets" will race with the "key lookup" inevitably.
    But actually they are all _readers_, so it's ok to convert the spinlock
    to rwlock to avoid competition between them. Also use per-set rwlock since
    each set is independent.
    
    Reported-by: Karel Rericha <karel@unitednetworks.cz>
    Tested-by: Karel Rericha <karel@unitednetworks.cz>
    Signed-off-by: Liping Zhang <zlpnobody@gmail.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 78dfbf9588b3..e97e2fb53f0a 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -18,9 +18,8 @@
 #include <linux/netfilter/nf_tables.h>
 #include <net/netfilter/nf_tables.h>
 
-static DEFINE_SPINLOCK(nft_rbtree_lock);
-
 struct nft_rbtree {
+	rwlock_t		lock;
 	struct rb_root		root;
 };
 
@@ -44,14 +43,14 @@ static bool nft_rbtree_equal(const struct nft_set *set, const void *this,
 static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 			      const u32 *key, const struct nft_set_ext **ext)
 {
-	const struct nft_rbtree *priv = nft_set_priv(set);
+	struct nft_rbtree *priv = nft_set_priv(set);
 	const struct nft_rbtree_elem *rbe, *interval = NULL;
 	u8 genmask = nft_genmask_cur(net);
 	const struct rb_node *parent;
 	const void *this;
 	int d;
 
-	spin_lock_bh(&nft_rbtree_lock);
+	read_lock_bh(&priv->lock);
 	parent = priv->root.rb_node;
 	while (parent != NULL) {
 		rbe = rb_entry(parent, struct nft_rbtree_elem, node);
@@ -75,7 +74,7 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 			}
 			if (nft_rbtree_interval_end(rbe))
 				goto out;
-			spin_unlock_bh(&nft_rbtree_lock);
+			read_unlock_bh(&priv->lock);
 
 			*ext = &rbe->ext;
 			return true;
@@ -85,12 +84,12 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 	if (set->flags & NFT_SET_INTERVAL && interval != NULL &&
 	    nft_set_elem_active(&interval->ext, genmask) &&
 	    !nft_rbtree_interval_end(interval)) {
-		spin_unlock_bh(&nft_rbtree_lock);
+		read_unlock_bh(&priv->lock);
 		*ext = &interval->ext;
 		return true;
 	}
 out:
-	spin_unlock_bh(&nft_rbtree_lock);
+	read_unlock_bh(&priv->lock);
 	return false;
 }
 
@@ -140,12 +139,13 @@ static int nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 			     const struct nft_set_elem *elem,
 			     struct nft_set_ext **ext)
 {
+	struct nft_rbtree *priv = nft_set_priv(set);
 	struct nft_rbtree_elem *rbe = elem->priv;
 	int err;
 
-	spin_lock_bh(&nft_rbtree_lock);
+	write_lock_bh(&priv->lock);
 	err = __nft_rbtree_insert(net, set, rbe, ext);
-	spin_unlock_bh(&nft_rbtree_lock);
+	write_unlock_bh(&priv->lock);
 
 	return err;
 }
@@ -157,9 +157,9 @@ static void nft_rbtree_remove(const struct net *net,
 	struct nft_rbtree *priv = nft_set_priv(set);
 	struct nft_rbtree_elem *rbe = elem->priv;
 
-	spin_lock_bh(&nft_rbtree_lock);
+	write_lock_bh(&priv->lock);
 	rb_erase(&rbe->node, &priv->root);
-	spin_unlock_bh(&nft_rbtree_lock);
+	write_unlock_bh(&priv->lock);
 }
 
 static void nft_rbtree_activate(const struct net *net,
@@ -224,12 +224,12 @@ static void nft_rbtree_walk(const struct nft_ctx *ctx,
 			    struct nft_set *set,
 			    struct nft_set_iter *iter)
 {
-	const struct nft_rbtree *priv = nft_set_priv(set);
+	struct nft_rbtree *priv = nft_set_priv(set);
 	struct nft_rbtree_elem *rbe;
 	struct nft_set_elem elem;
 	struct rb_node *node;
 
-	spin_lock_bh(&nft_rbtree_lock);
+	read_lock_bh(&priv->lock);
 	for (node = rb_first(&priv->root); node != NULL; node = rb_next(node)) {
 		rbe = rb_entry(node, struct nft_rbtree_elem, node);
 
@@ -242,13 +242,13 @@ static void nft_rbtree_walk(const struct nft_ctx *ctx,
 
 		iter->err = iter->fn(ctx, set, iter, &elem);
 		if (iter->err < 0) {
-			spin_unlock_bh(&nft_rbtree_lock);
+			read_unlock_bh(&priv->lock);
 			return;
 		}
 cont:
 		iter->count++;
 	}
-	spin_unlock_bh(&nft_rbtree_lock);
+	read_unlock_bh(&priv->lock);
 }
 
 static unsigned int nft_rbtree_privsize(const struct nlattr * const nla[])
@@ -262,6 +262,7 @@ static int nft_rbtree_init(const struct nft_set *set,
 {
 	struct nft_rbtree *priv = nft_set_priv(set);
 
+	rwlock_init(&priv->lock);
 	priv->root = RB_ROOT;
 	return 0;
 }

commit f9121355eb6f9babadb97bf5b34ab0cce7764406
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Mar 1 18:15:11 2017 +0100

    netfilter: nft_set_rbtree: incorrect assumption on lower interval lookups
    
    In case of adjacent ranges, we may indeed see either the high part of
    the range in first place or the low part of it. Remove this incorrect
    assumption, let's make sure we annotate the low part of the interval in
    case of we have adjacent interva intervals so we hit a matching in
    lookups.
    
    Reported-by: Simon Hanisch <hanisch@wh2.tu-dresden.de>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 71e8fb886a73..78dfbf9588b3 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -60,11 +60,10 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 		d = memcmp(this, key, set->klen);
 		if (d < 0) {
 			parent = parent->rb_left;
-			/* In case of adjacent ranges, we always see the high
-			 * part of the range in first place, before the low one.
-			 * So don't update interval if the keys are equal.
-			 */
-			if (interval && nft_rbtree_equal(set, this, interval))
+			if (interval &&
+			    nft_rbtree_equal(set, this, interval) &&
+			    nft_rbtree_interval_end(this) &&
+			    !nft_rbtree_interval_end(interval))
 				continue;
 			interval = rbe;
 		} else if (d > 0)

commit 7286ff7fde9f963736c7e575572899d8e16b06b7
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Fri Feb 10 19:59:36 2017 +0100

    netfilter: nf_tables: honor NFT_SET_OBJECT in set backend selection
    
    Check for NFT_SET_OBJECT feature flag, otherwise we may end up selecting
    the wrong set backend.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 3387ed7dd231..71e8fb886a73 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -310,7 +310,7 @@ static struct nft_set_ops nft_rbtree_ops __read_mostly = {
 	.activate	= nft_rbtree_activate,
 	.lookup		= nft_rbtree_lookup,
 	.walk		= nft_rbtree_walk,
-	.features	= NFT_SET_INTERVAL | NFT_SET_MAP,
+	.features	= NFT_SET_INTERVAL | NFT_SET_MAP | NFT_SET_OBJECT,
 	.owner		= THIS_MODULE,
 };
 

commit 0b5a78749260560f41e3b7c1f60f2c7dd9aff4f0
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Jan 18 18:30:12 2017 +0100

    netfilter: nf_tables: add space notation to sets
    
    The space notation allows us to classify the set backend implementation
    based on the amount of required memory. This provides an order of the
    set representation scalability in terms of memory. The size field is
    still left in place so use this if the userspace provides no explicit
    number of elements, so we cannot calculate the real memory that this set
    needs. This also helps us break ties in the set backend selection
    routine, eg. two backend implementations provide the same performance.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 2b6ea10c4bbd..3387ed7dd231 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -292,6 +292,7 @@ static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 		est->size = nsize;
 
 	est->lookup = NFT_SET_CLASS_O_LOG_N;
+	est->space  = NFT_SET_CLASS_O_N;
 
 	return true;
 }

commit 55af753cd9fda9c5300f5318253b08bd15fb412e
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Jan 18 18:30:11 2017 +0100

    netfilter: nf_tables: rename struct nft_set_estimate class field
    
    Use lookup as field name instead, to prepare the introduction of the
    memory class in a follow up patch.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 81b8a4c2c061..2b6ea10c4bbd 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -291,7 +291,7 @@ static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
 	else
 		est->size = nsize;
 
-	est->class = NFT_SET_CLASS_O_LOG_N;
+	est->lookup = NFT_SET_CLASS_O_LOG_N;
 
 	return true;
 }

commit 1ba1c41408df8a9d2f8b9b67e4c9e6f59b29d8ee
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Jan 18 18:30:09 2017 +0100

    netfilter: nf_tables: rename deactivate_one() to flush()
    
    Although semantics are similar to deactivate() with no implicit element
    lookup, this is only called from the set flush path, so better rename
    this to flush().
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 9fbd70da1633..81b8a4c2c061 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -172,8 +172,8 @@ static void nft_rbtree_activate(const struct net *net,
 	nft_set_elem_change_active(net, set, &rbe->ext);
 }
 
-static bool nft_rbtree_deactivate_one(const struct net *net,
-				      const struct nft_set *set, void *priv)
+static bool nft_rbtree_flush(const struct net *net,
+			     const struct nft_set *set, void *priv)
 {
 	struct nft_rbtree_elem *rbe = priv;
 
@@ -214,7 +214,7 @@ static void *nft_rbtree_deactivate(const struct net *net,
 				parent = parent->rb_right;
 				continue;
 			}
-			nft_rbtree_deactivate_one(net, set, rbe);
+			nft_rbtree_flush(net, set, rbe);
 			return rbe;
 		}
 	}
@@ -305,7 +305,7 @@ static struct nft_set_ops nft_rbtree_ops __read_mostly = {
 	.insert		= nft_rbtree_insert,
 	.remove		= nft_rbtree_remove,
 	.deactivate	= nft_rbtree_deactivate,
-	.deactivate_one	= nft_rbtree_deactivate_one,
+	.flush		= nft_rbtree_flush,
 	.activate	= nft_rbtree_activate,
 	.lookup		= nft_rbtree_lookup,
 	.walk		= nft_rbtree_walk,

commit 5cb82a38c6b5152b1deaba0c1596ce63222a4710
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Jan 18 18:30:07 2017 +0100

    netfilter: nf_tables: pass netns to set->ops->remove()
    
    This new parameter is required by the new bitmap set type that comes in a
    follow up patch.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index f06f55ee516d..9fbd70da1633 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -151,7 +151,8 @@ static int nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 	return err;
 }
 
-static void nft_rbtree_remove(const struct nft_set *set,
+static void nft_rbtree_remove(const struct net *net,
+			      const struct nft_set *set,
 			      const struct nft_set_elem *elem)
 {
 	struct nft_rbtree *priv = nft_set_priv(set);

commit de70185de0333783154863278ac87bfbbc54e384
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Tue Jan 24 00:51:41 2017 +0100

    netfilter: nf_tables: deconstify walk callback function
    
    The flush operation needs to modify set and element objects, so let's
    deconstify this.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 08376e50f6cd..f06f55ee516d 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -221,7 +221,7 @@ static void *nft_rbtree_deactivate(const struct net *net,
 }
 
 static void nft_rbtree_walk(const struct nft_ctx *ctx,
-			    const struct nft_set *set,
+			    struct nft_set *set,
 			    struct nft_set_iter *iter)
 {
 	const struct nft_rbtree *priv = nft_set_priv(set);

commit 8411b6442e59810fe0750a2f321b9dcb7d0a3d17
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon Dec 5 23:35:50 2016 +0100

    netfilter: nf_tables: support for set flushing
    
    This patch adds support for set flushing, that consists of walking over
    the set elements if the NFTA_SET_ELEM_LIST_ELEMENTS attribute is set.
    This patch requires the following changes:
    
    1) Add set->ops->deactivate_one() operation: This allows us to
       deactivate an element from the set element walk path, given we can
       skip the lookup that happens in ->deactivate().
    
    2) Add a new nft_trans_alloc_gfp() function since we need to allocate
       transactions using GFP_ATOMIC given the set walk path happens with
       held rcu_read_lock.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 5580bb64dc0f..08376e50f6cd 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -304,6 +304,7 @@ static struct nft_set_ops nft_rbtree_ops __read_mostly = {
 	.insert		= nft_rbtree_insert,
 	.remove		= nft_rbtree_remove,
 	.deactivate	= nft_rbtree_deactivate,
+	.deactivate_one	= nft_rbtree_deactivate_one,
 	.activate	= nft_rbtree_activate,
 	.lookup		= nft_rbtree_lookup,
 	.walk		= nft_rbtree_walk,

commit 37df5301a3ae903c5b1aa90cae37c6c669dfc386
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Mon Dec 5 23:35:49 2016 +0100

    netfilter: nft_set: introduce nft_{hash, rbtree}_deactivate_one()
    
    This new function allows us to deactivate one single element, this is
    required by the set flush command that comes in a follow up patch.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 36493a7cae88..5580bb64dc0f 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -171,6 +171,15 @@ static void nft_rbtree_activate(const struct net *net,
 	nft_set_elem_change_active(net, set, &rbe->ext);
 }
 
+static bool nft_rbtree_deactivate_one(const struct net *net,
+				      const struct nft_set *set, void *priv)
+{
+	struct nft_rbtree_elem *rbe = priv;
+
+	nft_set_elem_change_active(net, set, &rbe->ext);
+	return true;
+}
+
 static void *nft_rbtree_deactivate(const struct net *net,
 				   const struct nft_set *set,
 				   const struct nft_set_elem *elem)
@@ -204,7 +213,7 @@ static void *nft_rbtree_deactivate(const struct net *net,
 				parent = parent->rb_right;
 				continue;
 			}
-			nft_set_elem_change_active(net, set, &rbe->ext);
+			nft_rbtree_deactivate_one(net, set, rbe);
 			return rbe;
 		}
 	}

commit 61f9e2924f4981d626b3a931fed935f2fa3cb4de
Author: Liping Zhang <zlpnobody@gmail.com>
Date:   Sat Oct 22 18:51:25 2016 +0800

    netfilter: nf_tables: fix *leak* when expr clone fail
    
    When nft_expr_clone failed, a series of problems will happen:
    
    1. module refcnt will leak, we call __module_get at the beginning but
       we forget to put it back if ops->clone returns fail
    2. memory will be leaked, if clone fail, we just return NULL and forget
       to free the alloced element
    3. set->nelems will become incorrect when set->size is specified. If
       clone fail, we should decrease the set->nelems
    
    Now this patch fixes these problems. And fortunately, clone fail will
    only happen on counter expression when memory is exhausted.
    
    Fixes: 086f332167d6 ("netfilter: nf_tables: add clone interface to expression operations")
    Signed-off-by: Liping Zhang <zlpnobody@gmail.com>
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 38b5bda242f8..36493a7cae88 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -266,7 +266,7 @@ static void nft_rbtree_destroy(const struct nft_set *set)
 	while ((node = priv->root.rb_node) != NULL) {
 		rb_erase(node, &priv->root);
 		rbe = rb_entry(node, struct nft_rbtree_elem, node);
-		nft_set_elem_destroy(set, rbe);
+		nft_set_elem_destroy(set, rbe, true);
 	}
 }
 

commit 60175ccdf46ac5076725cb3e66f6bc2e2766ad2b
Merge: 2f5281ba2a8f 779994fa3636
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Sep 6 12:45:26 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/pablo/nf-next
    
    Pablo Neira Ayuso says:
    
    ====================
    Netfilter updates for net-next
    
    The following patchset contains Netfilter updates for your net-next
    tree.  Most relevant updates are the removal of per-conntrack timers to
    use a workqueue/garbage collection approach instead from Florian
    Westphal, the hash and numgen expression for nf_tables from Laura
    Garcia, updates on nf_tables hash set to honor the NLM_F_EXCL flag,
    removal of ip_conntrack sysctl and many other incremental updates on our
    Netfilter codebase.
    
    More specifically, they are:
    
    1) Retrieve only 4 bytes to fetch ports in case of non-linear skb
       transport area in dccp, sctp, tcp, udp and udplite protocol
       conntrackers, from Gao Feng.
    
    2) Missing whitespace on error message in physdev match, from Hangbin Liu.
    
    3) Skip redundant IPv4 checksum calculation in nf_dup_ipv4, from Liping Zhang.
    
    4) Add nf_ct_expires() helper function and use it, from Florian Westphal.
    
    5) Replace opencoded nf_ct_kill() call in IPVS conntrack support, also
       from Florian.
    
    6) Rename nf_tables set implementation to nft_set_{name}.c
    
    7) Introduce the hash expression to allow arbitrary hashing of selector
       concatenations, from Laura Garcia Liebana.
    
    8) Remove ip_conntrack sysctl backward compatibility code, this code has
       been around for long time already, and we have two interfaces to do
       this already: nf_conntrack sysctl and ctnetlink.
    
    9) Use nf_conntrack_get_ht() helper function whenever possible, instead
       of opencoding fetch of hashtable pointer and size, patch from Liping Zhang.
    
    10) Add quota expression for nf_tables.
    
    11) Add number generator expression for nf_tables, this supports
        incremental and random generators that can be combined with maps,
        very useful for load balancing purpose, again from Laura Garcia Liebana.
    
    12) Fix a typo in a debug message in FTP conntrack helper, from Colin Ian King.
    
    13) Introduce a nft_chain_parse_hook() helper function to parse chain hook
        configuration, this is used by a follow up patch to perform better chain
        update validation.
    
    14) Add rhashtable_lookup_get_insert_key() to rhashtable and use it from the
        nft_set_hash implementation to honor the NLM_F_EXCL flag.
    
    15) Missing nulls check in nf_conntrack from nf_conntrack_tuple_taken(),
        patch from Florian Westphal.
    
    16) Don't use the DYING bit to know if the conntrack event has been already
        delivered, instead a state variable to track event re-delivery
        states, also from Florian.
    
    17) Remove the per-conntrack timer, use the workqueue approach that was
        discussed during the NFWS, from Florian Westphal.
    
    18) Use the netlink conntrack table dump path to kill stale entries,
        again from Florian.
    
    19) Add a garbage collector to get rid of stale conntracks, from
        Florian.
    
    20) Reschedule garbage collector if eviction rate is high.
    
    21) Get rid of the __nf_ct_kill_acct() helper.
    
    22) Use ARPHRD_ETHER instead of hardcoded 1 from ARP logger.
    
    23) Make nf_log_set() interface assertive on unsupported families.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c016c7e45ddfa5085b35b644e659ec014969740d
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Wed Aug 24 12:41:54 2016 +0200

    netfilter: nf_tables: honor NLM_F_EXCL flag in set element insertion
    
    If the NLM_F_EXCL flag is set, then new elements that clash with an
    existing one return EEXIST. In case you try to add an element whose
    data area differs from what we have, then this returns EBUSY. If no
    flag is specified at all, then this returns success to userspace.
    
    This patch also update the set insert operation so we can fetch the
    existing element that clashes with the one you want to add, we need
    this to make sure the element data doesn't differ.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
index 6473936d05c6..038682d48261 100644
--- a/net/netfilter/nft_set_rbtree.c
+++ b/net/netfilter/nft_set_rbtree.c
@@ -94,7 +94,8 @@ static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
 }
 
 static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
-			       struct nft_rbtree_elem *new)
+			       struct nft_rbtree_elem *new,
+			       struct nft_set_ext **ext)
 {
 	struct nft_rbtree *priv = nft_set_priv(set);
 	u8 genmask = nft_genmask_next(net);
@@ -122,8 +123,10 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 				else if (!nft_rbtree_interval_end(rbe) &&
 					 nft_rbtree_interval_end(new))
 					p = &parent->rb_right;
-				else
+				else {
+					*ext = &rbe->ext;
 					return -EEXIST;
+				}
 			}
 		}
 	}
@@ -133,13 +136,14 @@ static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
 }
 
 static int nft_rbtree_insert(const struct net *net, const struct nft_set *set,
-			     const struct nft_set_elem *elem)
+			     const struct nft_set_elem *elem,
+			     struct nft_set_ext **ext)
 {
 	struct nft_rbtree_elem *rbe = elem->priv;
 	int err;
 
 	spin_lock_bh(&nft_rbtree_lock);
-	err = __nft_rbtree_insert(net, set, rbe);
+	err = __nft_rbtree_insert(net, set, rbe, ext);
 	spin_unlock_bh(&nft_rbtree_lock);
 
 	return err;

commit 0ed6389c483dc77cdbdd48de0ca7ce41723dd667
Author: Pablo Neira Ayuso <pablo@netfilter.org>
Date:   Tue Aug 9 16:11:46 2016 +0200

    netfilter: nf_tables: rename set implementations
    
    Use nft_set_* prefix for backend set implementations, thus we can use
    nft_hash for the new hash expression.
    
    Signed-off-by: Pablo Neira Ayuso <pablo@netfilter.org>

diff --git a/net/netfilter/nft_set_rbtree.c b/net/netfilter/nft_set_rbtree.c
new file mode 100644
index 000000000000..6473936d05c6
--- /dev/null
+++ b/net/netfilter/nft_set_rbtree.c
@@ -0,0 +1,314 @@
+/*
+ * Copyright (c) 2008-2009 Patrick McHardy <kaber@trash.net>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * Development of this code funded by Astaro AG (http://www.astaro.com/)
+ */
+
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/list.h>
+#include <linux/rbtree.h>
+#include <linux/netlink.h>
+#include <linux/netfilter.h>
+#include <linux/netfilter/nf_tables.h>
+#include <net/netfilter/nf_tables.h>
+
+static DEFINE_SPINLOCK(nft_rbtree_lock);
+
+struct nft_rbtree {
+	struct rb_root		root;
+};
+
+struct nft_rbtree_elem {
+	struct rb_node		node;
+	struct nft_set_ext	ext;
+};
+
+static bool nft_rbtree_interval_end(const struct nft_rbtree_elem *rbe)
+{
+	return nft_set_ext_exists(&rbe->ext, NFT_SET_EXT_FLAGS) &&
+	       (*nft_set_ext_flags(&rbe->ext) & NFT_SET_ELEM_INTERVAL_END);
+}
+
+static bool nft_rbtree_equal(const struct nft_set *set, const void *this,
+			     const struct nft_rbtree_elem *interval)
+{
+	return memcmp(this, nft_set_ext_key(&interval->ext), set->klen) == 0;
+}
+
+static bool nft_rbtree_lookup(const struct net *net, const struct nft_set *set,
+			      const u32 *key, const struct nft_set_ext **ext)
+{
+	const struct nft_rbtree *priv = nft_set_priv(set);
+	const struct nft_rbtree_elem *rbe, *interval = NULL;
+	u8 genmask = nft_genmask_cur(net);
+	const struct rb_node *parent;
+	const void *this;
+	int d;
+
+	spin_lock_bh(&nft_rbtree_lock);
+	parent = priv->root.rb_node;
+	while (parent != NULL) {
+		rbe = rb_entry(parent, struct nft_rbtree_elem, node);
+
+		this = nft_set_ext_key(&rbe->ext);
+		d = memcmp(this, key, set->klen);
+		if (d < 0) {
+			parent = parent->rb_left;
+			/* In case of adjacent ranges, we always see the high
+			 * part of the range in first place, before the low one.
+			 * So don't update interval if the keys are equal.
+			 */
+			if (interval && nft_rbtree_equal(set, this, interval))
+				continue;
+			interval = rbe;
+		} else if (d > 0)
+			parent = parent->rb_right;
+		else {
+found:
+			if (!nft_set_elem_active(&rbe->ext, genmask)) {
+				parent = parent->rb_left;
+				continue;
+			}
+			if (nft_rbtree_interval_end(rbe))
+				goto out;
+			spin_unlock_bh(&nft_rbtree_lock);
+
+			*ext = &rbe->ext;
+			return true;
+		}
+	}
+
+	if (set->flags & NFT_SET_INTERVAL && interval != NULL) {
+		rbe = interval;
+		goto found;
+	}
+out:
+	spin_unlock_bh(&nft_rbtree_lock);
+	return false;
+}
+
+static int __nft_rbtree_insert(const struct net *net, const struct nft_set *set,
+			       struct nft_rbtree_elem *new)
+{
+	struct nft_rbtree *priv = nft_set_priv(set);
+	u8 genmask = nft_genmask_next(net);
+	struct nft_rbtree_elem *rbe;
+	struct rb_node *parent, **p;
+	int d;
+
+	parent = NULL;
+	p = &priv->root.rb_node;
+	while (*p != NULL) {
+		parent = *p;
+		rbe = rb_entry(parent, struct nft_rbtree_elem, node);
+		d = memcmp(nft_set_ext_key(&rbe->ext),
+			   nft_set_ext_key(&new->ext),
+			   set->klen);
+		if (d < 0)
+			p = &parent->rb_left;
+		else if (d > 0)
+			p = &parent->rb_right;
+		else {
+			if (nft_set_elem_active(&rbe->ext, genmask)) {
+				if (nft_rbtree_interval_end(rbe) &&
+				    !nft_rbtree_interval_end(new))
+					p = &parent->rb_left;
+				else if (!nft_rbtree_interval_end(rbe) &&
+					 nft_rbtree_interval_end(new))
+					p = &parent->rb_right;
+				else
+					return -EEXIST;
+			}
+		}
+	}
+	rb_link_node(&new->node, parent, p);
+	rb_insert_color(&new->node, &priv->root);
+	return 0;
+}
+
+static int nft_rbtree_insert(const struct net *net, const struct nft_set *set,
+			     const struct nft_set_elem *elem)
+{
+	struct nft_rbtree_elem *rbe = elem->priv;
+	int err;
+
+	spin_lock_bh(&nft_rbtree_lock);
+	err = __nft_rbtree_insert(net, set, rbe);
+	spin_unlock_bh(&nft_rbtree_lock);
+
+	return err;
+}
+
+static void nft_rbtree_remove(const struct nft_set *set,
+			      const struct nft_set_elem *elem)
+{
+	struct nft_rbtree *priv = nft_set_priv(set);
+	struct nft_rbtree_elem *rbe = elem->priv;
+
+	spin_lock_bh(&nft_rbtree_lock);
+	rb_erase(&rbe->node, &priv->root);
+	spin_unlock_bh(&nft_rbtree_lock);
+}
+
+static void nft_rbtree_activate(const struct net *net,
+				const struct nft_set *set,
+				const struct nft_set_elem *elem)
+{
+	struct nft_rbtree_elem *rbe = elem->priv;
+
+	nft_set_elem_change_active(net, set, &rbe->ext);
+}
+
+static void *nft_rbtree_deactivate(const struct net *net,
+				   const struct nft_set *set,
+				   const struct nft_set_elem *elem)
+{
+	const struct nft_rbtree *priv = nft_set_priv(set);
+	const struct rb_node *parent = priv->root.rb_node;
+	struct nft_rbtree_elem *rbe, *this = elem->priv;
+	u8 genmask = nft_genmask_next(net);
+	int d;
+
+	while (parent != NULL) {
+		rbe = rb_entry(parent, struct nft_rbtree_elem, node);
+
+		d = memcmp(nft_set_ext_key(&rbe->ext), &elem->key.val,
+					   set->klen);
+		if (d < 0)
+			parent = parent->rb_left;
+		else if (d > 0)
+			parent = parent->rb_right;
+		else {
+			if (!nft_set_elem_active(&rbe->ext, genmask)) {
+				parent = parent->rb_left;
+				continue;
+			}
+			if (nft_rbtree_interval_end(rbe) &&
+			    !nft_rbtree_interval_end(this)) {
+				parent = parent->rb_left;
+				continue;
+			} else if (!nft_rbtree_interval_end(rbe) &&
+				   nft_rbtree_interval_end(this)) {
+				parent = parent->rb_right;
+				continue;
+			}
+			nft_set_elem_change_active(net, set, &rbe->ext);
+			return rbe;
+		}
+	}
+	return NULL;
+}
+
+static void nft_rbtree_walk(const struct nft_ctx *ctx,
+			    const struct nft_set *set,
+			    struct nft_set_iter *iter)
+{
+	const struct nft_rbtree *priv = nft_set_priv(set);
+	struct nft_rbtree_elem *rbe;
+	struct nft_set_elem elem;
+	struct rb_node *node;
+
+	spin_lock_bh(&nft_rbtree_lock);
+	for (node = rb_first(&priv->root); node != NULL; node = rb_next(node)) {
+		rbe = rb_entry(node, struct nft_rbtree_elem, node);
+
+		if (iter->count < iter->skip)
+			goto cont;
+		if (!nft_set_elem_active(&rbe->ext, iter->genmask))
+			goto cont;
+
+		elem.priv = rbe;
+
+		iter->err = iter->fn(ctx, set, iter, &elem);
+		if (iter->err < 0) {
+			spin_unlock_bh(&nft_rbtree_lock);
+			return;
+		}
+cont:
+		iter->count++;
+	}
+	spin_unlock_bh(&nft_rbtree_lock);
+}
+
+static unsigned int nft_rbtree_privsize(const struct nlattr * const nla[])
+{
+	return sizeof(struct nft_rbtree);
+}
+
+static int nft_rbtree_init(const struct nft_set *set,
+			   const struct nft_set_desc *desc,
+			   const struct nlattr * const nla[])
+{
+	struct nft_rbtree *priv = nft_set_priv(set);
+
+	priv->root = RB_ROOT;
+	return 0;
+}
+
+static void nft_rbtree_destroy(const struct nft_set *set)
+{
+	struct nft_rbtree *priv = nft_set_priv(set);
+	struct nft_rbtree_elem *rbe;
+	struct rb_node *node;
+
+	while ((node = priv->root.rb_node) != NULL) {
+		rb_erase(node, &priv->root);
+		rbe = rb_entry(node, struct nft_rbtree_elem, node);
+		nft_set_elem_destroy(set, rbe);
+	}
+}
+
+static bool nft_rbtree_estimate(const struct nft_set_desc *desc, u32 features,
+				struct nft_set_estimate *est)
+{
+	unsigned int nsize;
+
+	nsize = sizeof(struct nft_rbtree_elem);
+	if (desc->size)
+		est->size = sizeof(struct nft_rbtree) + desc->size * nsize;
+	else
+		est->size = nsize;
+
+	est->class = NFT_SET_CLASS_O_LOG_N;
+
+	return true;
+}
+
+static struct nft_set_ops nft_rbtree_ops __read_mostly = {
+	.privsize	= nft_rbtree_privsize,
+	.elemsize	= offsetof(struct nft_rbtree_elem, ext),
+	.estimate	= nft_rbtree_estimate,
+	.init		= nft_rbtree_init,
+	.destroy	= nft_rbtree_destroy,
+	.insert		= nft_rbtree_insert,
+	.remove		= nft_rbtree_remove,
+	.deactivate	= nft_rbtree_deactivate,
+	.activate	= nft_rbtree_activate,
+	.lookup		= nft_rbtree_lookup,
+	.walk		= nft_rbtree_walk,
+	.features	= NFT_SET_INTERVAL | NFT_SET_MAP,
+	.owner		= THIS_MODULE,
+};
+
+static int __init nft_rbtree_module_init(void)
+{
+	return nft_register_set(&nft_rbtree_ops);
+}
+
+static void __exit nft_rbtree_module_exit(void)
+{
+	nft_unregister_set(&nft_rbtree_ops);
+}
+
+module_init(nft_rbtree_module_init);
+module_exit(nft_rbtree_module_exit);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Patrick McHardy <kaber@trash.net>");
+MODULE_ALIAS_NFT_SET();
