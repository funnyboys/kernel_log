commit 0372007f5a79d61d3cb48a507717b9afb5d6addd
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Mar 4 11:05:22 2020 +0100

    context_tracking: Ensure that the critical path cannot be instrumented
    
    context tracking lacks a few protection mechanisms against instrumentation:
    
     - While the core functions are marked NOKPROBE they lack protection
       against function tracing which is required as the function entry/exit
       points can be utilized by BPF.
    
     - static functions invoked from the protected functions need to be marked
       as well as they can be instrumented otherwise.
    
     - using plain inline allows the compiler to emit traceable and probable
       functions.
    
    Fix this by marking the functions noinstr and converting the plain inlines
    to __always_inline.
    
    The NOKPROBE_SYMBOL() annotations are removed as the .noinstr.text section
    is already excluded from being probed.
    
    Cures the following objtool warnings:
    
     vmlinux.o: warning: objtool: enter_from_user_mode()+0x34: call to __context_tracking_exit() leaves .noinstr.text section
     vmlinux.o: warning: objtool: prepare_exit_to_usermode()+0x29: call to __context_tracking_enter() leaves .noinstr.text section
     vmlinux.o: warning: objtool: syscall_return_slowpath()+0x29: call to __context_tracking_enter() leaves .noinstr.text section
     vmlinux.o: warning: objtool: do_syscall_64()+0x7f: call to __context_tracking_enter() leaves .noinstr.text section
     vmlinux.o: warning: objtool: do_int80_syscall_32()+0x3d: call to __context_tracking_enter() leaves .noinstr.text section
     vmlinux.o: warning: objtool: do_fast_syscall_32()+0x9c: call to __context_tracking_enter() leaves .noinstr.text section
    
    and generates new ones...
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200505134340.811520478@linutronix.de

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index ce430885c26c..36a98c48aedc 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -31,7 +31,7 @@ EXPORT_SYMBOL_GPL(context_tracking_key);
 DEFINE_PER_CPU(struct context_tracking, context_tracking);
 EXPORT_SYMBOL_GPL(context_tracking);
 
-static bool context_tracking_recursion_enter(void)
+static noinstr bool context_tracking_recursion_enter(void)
 {
 	int recursion;
 
@@ -45,7 +45,7 @@ static bool context_tracking_recursion_enter(void)
 	return false;
 }
 
-static void context_tracking_recursion_exit(void)
+static __always_inline void context_tracking_recursion_exit(void)
 {
 	__this_cpu_dec(context_tracking.recursion);
 }
@@ -59,7 +59,7 @@ static void context_tracking_recursion_exit(void)
  * instructions to execute won't use any RCU read side critical section
  * because this function sets RCU in extended quiescent state.
  */
-void __context_tracking_enter(enum ctx_state state)
+void noinstr __context_tracking_enter(enum ctx_state state)
 {
 	/* Kernel threads aren't supposed to go to userspace */
 	WARN_ON_ONCE(!current->mm);
@@ -77,8 +77,10 @@ void __context_tracking_enter(enum ctx_state state)
 			 * on the tick.
 			 */
 			if (state == CONTEXT_USER) {
+				instrumentation_begin();
 				trace_user_enter(0);
 				vtime_user_enter(current);
+				instrumentation_end();
 			}
 			rcu_user_enter();
 		}
@@ -99,7 +101,6 @@ void __context_tracking_enter(enum ctx_state state)
 	}
 	context_tracking_recursion_exit();
 }
-NOKPROBE_SYMBOL(__context_tracking_enter);
 EXPORT_SYMBOL_GPL(__context_tracking_enter);
 
 void context_tracking_enter(enum ctx_state state)
@@ -142,7 +143,7 @@ NOKPROBE_SYMBOL(context_tracking_user_enter);
  * This call supports re-entrancy. This way it can be called from any exception
  * handler without needing to know if we came from userspace or not.
  */
-void __context_tracking_exit(enum ctx_state state)
+void noinstr __context_tracking_exit(enum ctx_state state)
 {
 	if (!context_tracking_recursion_enter())
 		return;
@@ -155,15 +156,16 @@ void __context_tracking_exit(enum ctx_state state)
 			 */
 			rcu_user_exit();
 			if (state == CONTEXT_USER) {
+				instrumentation_begin();
 				vtime_user_exit(current);
 				trace_user_exit(0);
+				instrumentation_end();
 			}
 		}
 		__this_cpu_write(context_tracking.state, CONTEXT_KERNEL);
 	}
 	context_tracking_recursion_exit();
 }
-NOKPROBE_SYMBOL(__context_tracking_exit);
 EXPORT_SYMBOL_GPL(__context_tracking_exit);
 
 void context_tracking_exit(enum ctx_state state)

commit 490f561b783dac2c4825e288e6dbbf83481eea34
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Mon Jan 27 16:41:52 2020 +0100

    context-tracking: Introduce CONFIG_HAVE_TIF_NOHZ
    
    A few archs (x86, arm, arm64) don't rely anymore on TIF_NOHZ to call
    into context tracking on user entry/exit but instead use static keys
    (or not) to optimize those calls. Ideally every arch should migrate to
    that behaviour in the long run.
    
    Settle a config option to let those archs remove their TIF_NOHZ
    definitions.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Russell King <linux@armlinux.org.uk>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Will Deacon <will@kernel.org>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Paul Burton <paulburton@kernel.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: David S. Miller <davem@davemloft.net>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 0296b4bda8f1..ce430885c26c 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -198,11 +198,13 @@ void __init context_tracking_cpu_set(int cpu)
 	if (initialized)
 		return;
 
+#ifdef CONFIG_HAVE_TIF_NOHZ
 	/*
 	 * Set TIF_NOHZ to init/0 and let it propagate to all tasks through fork
 	 * This assumes that init is the only task at this early boot stage.
 	 */
 	set_tsk_thread_flag(&init_task, TIF_NOHZ);
+#endif
 	WARN_ON_ONCE(!tasklist_empty());
 
 	initialized = true;

commit 74c578759f15cb5a0d0107759bdad671d7b52ab9
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Oct 16 04:56:51 2019 +0200

    context_tracking: Rename context_tracking_is_enabled() => context_tracking_enabled()
    
    Remove the superfluous "is" in the middle of the name. We want to
    standardize the naming so that it can be expanded through suffixes:
    
            context_tracking_enabled()
            context_tracking_enabled_cpu()
            context_tracking_enabled_this_cpu()
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jacek Anaszewski <jacek.anaszewski@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Pavel Machek <pavel@ucw.cz>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rjw@rjwysocki.net>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Wanpeng Li <wanpengli@tencent.com>
    Cc: Yauheni Kaliuta <yauheni.kaliuta@redhat.com>
    Link: https://lkml.kernel.org/r/20191016025700.31277-6-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index be01a4d627c9..0296b4bda8f1 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -25,8 +25,8 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/context_tracking.h>
 
-DEFINE_STATIC_KEY_FALSE(context_tracking_enabled);
-EXPORT_SYMBOL_GPL(context_tracking_enabled);
+DEFINE_STATIC_KEY_FALSE(context_tracking_key);
+EXPORT_SYMBOL_GPL(context_tracking_key);
 
 DEFINE_PER_CPU(struct context_tracking, context_tracking);
 EXPORT_SYMBOL_GPL(context_tracking);
@@ -192,7 +192,7 @@ void __init context_tracking_cpu_set(int cpu)
 
 	if (!per_cpu(context_tracking.active, cpu)) {
 		per_cpu(context_tracking.active, cpu) = true;
-		static_branch_inc(&context_tracking_enabled);
+		static_branch_inc(&context_tracking_key);
 	}
 
 	if (initialized)

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 9ad37b9e44a7..be01a4d627c9 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Context tracking: Probe on high level context boundaries such as kernel
  * and userspace. This includes syscalls and exceptions entry/exit.

commit ed11a7f1b3bd482bd7d6ef7bc2859c41fb43b9ee
Author: Andy Lutomirski <luto@kernel.org>
Date:   Thu Nov 12 12:59:01 2015 -0800

    context_tracking: Switch to new static_branch API
    
    This is much less error-prone than the old code.
    
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/812df7e64f120c5c7c08481f36a8caa9f53b2199.1447361906.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index d8560ee3bab7..9ad37b9e44a7 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -24,7 +24,7 @@
 #define CREATE_TRACE_POINTS
 #include <trace/events/context_tracking.h>
 
-struct static_key context_tracking_enabled = STATIC_KEY_INIT_FALSE;
+DEFINE_STATIC_KEY_FALSE(context_tracking_enabled);
 EXPORT_SYMBOL_GPL(context_tracking_enabled);
 
 DEFINE_PER_CPU(struct context_tracking, context_tracking);
@@ -191,7 +191,7 @@ void __init context_tracking_cpu_set(int cpu)
 
 	if (!per_cpu(context_tracking.active, cpu)) {
 		per_cpu(context_tracking.active, cpu) = true;
-		static_key_slow_inc(&context_tracking_enabled);
+		static_branch_inc(&context_tracking_enabled);
 	}
 
 	if (initialized)

commit d0e536d89395ecd8ab78fe999dc4d6f5d140ce46
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 28 02:39:56 2015 +0100

    context_tracking: avoid irq_save/irq_restore on guest entry and exit
    
    guest_enter and guest_exit must be called with interrupts disabled,
    since they take the vtime_seqlock with write_seq{lock,unlock}.
    Therefore, it is not necessary to check for exceptions, nor to
    save/restore the IRQ state, when context tracking functions are
    called by guest_enter and guest_exit.
    
    Split the body of context_tracking_entry and context_tracking_exit
    out to __-prefixed functions, and use them from KVM.
    
    Rik van Riel has measured this to speed up a tight vmentry/vmexit
    loop by about 2%.
    
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Tested-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 6d4c6ce21275..d8560ee3bab7 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -58,27 +58,13 @@ static void context_tracking_recursion_exit(void)
  * instructions to execute won't use any RCU read side critical section
  * because this function sets RCU in extended quiescent state.
  */
-void context_tracking_enter(enum ctx_state state)
+void __context_tracking_enter(enum ctx_state state)
 {
-	unsigned long flags;
-
-	/*
-	 * Some contexts may involve an exception occuring in an irq,
-	 * leading to that nesting:
-	 * rcu_irq_enter() rcu_user_exit() rcu_user_exit() rcu_irq_exit()
-	 * This would mess up the dyntick_nesting count though. And rcu_irq_*()
-	 * helpers are enough to protect RCU uses inside the exception. So
-	 * just return immediately if we detect we are in an IRQ.
-	 */
-	if (in_interrupt())
-		return;
-
 	/* Kernel threads aren't supposed to go to userspace */
 	WARN_ON_ONCE(!current->mm);
 
-	local_irq_save(flags);
 	if (!context_tracking_recursion_enter())
-		goto out_irq_restore;
+		return;
 
 	if ( __this_cpu_read(context_tracking.state) != state) {
 		if (__this_cpu_read(context_tracking.active)) {
@@ -111,7 +97,27 @@ void context_tracking_enter(enum ctx_state state)
 		__this_cpu_write(context_tracking.state, state);
 	}
 	context_tracking_recursion_exit();
-out_irq_restore:
+}
+NOKPROBE_SYMBOL(__context_tracking_enter);
+EXPORT_SYMBOL_GPL(__context_tracking_enter);
+
+void context_tracking_enter(enum ctx_state state)
+{
+	unsigned long flags;
+
+	/*
+	 * Some contexts may involve an exception occuring in an irq,
+	 * leading to that nesting:
+	 * rcu_irq_enter() rcu_user_exit() rcu_user_exit() rcu_irq_exit()
+	 * This would mess up the dyntick_nesting count though. And rcu_irq_*()
+	 * helpers are enough to protect RCU uses inside the exception. So
+	 * just return immediately if we detect we are in an IRQ.
+	 */
+	if (in_interrupt())
+		return;
+
+	local_irq_save(flags);
+	__context_tracking_enter(state);
 	local_irq_restore(flags);
 }
 NOKPROBE_SYMBOL(context_tracking_enter);
@@ -135,16 +141,10 @@ NOKPROBE_SYMBOL(context_tracking_user_enter);
  * This call supports re-entrancy. This way it can be called from any exception
  * handler without needing to know if we came from userspace or not.
  */
-void context_tracking_exit(enum ctx_state state)
+void __context_tracking_exit(enum ctx_state state)
 {
-	unsigned long flags;
-
-	if (in_interrupt())
-		return;
-
-	local_irq_save(flags);
 	if (!context_tracking_recursion_enter())
-		goto out_irq_restore;
+		return;
 
 	if (__this_cpu_read(context_tracking.state) == state) {
 		if (__this_cpu_read(context_tracking.active)) {
@@ -161,7 +161,19 @@ void context_tracking_exit(enum ctx_state state)
 		__this_cpu_write(context_tracking.state, CONTEXT_KERNEL);
 	}
 	context_tracking_recursion_exit();
-out_irq_restore:
+}
+NOKPROBE_SYMBOL(__context_tracking_exit);
+EXPORT_SYMBOL_GPL(__context_tracking_exit);
+
+void context_tracking_exit(enum ctx_state state)
+{
+	unsigned long flags;
+
+	if (in_interrupt())
+		return;
+
+	local_irq_save(flags);
+	__context_tracking_exit(state);
 	local_irq_restore(flags);
 }
 NOKPROBE_SYMBOL(context_tracking_exit);

commit f70cd6b07e629f367bb9b1ac9d0e3e669eb325c0
Author: Paolo Bonzini <pbonzini@redhat.com>
Date:   Wed Oct 28 02:39:55 2015 +0100

    context_tracking: remove duplicate enabled check
    
    All calls to context_tracking_enter and context_tracking_exit
    are already checking context_tracking_is_enabled, except the
    context_tracking_user_enter and context_tracking_user_exit
    functions left in for the benefit of assembly calls.
    
    Pull the check up to those functions, by making them simple
    wrappers around the user_enter and user_exit inline functions.
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Tested-by: Rik van Riel <riel@redhat.com>
    Acked-by: Andy Lutomirski <luto@kernel.org>
    Signed-off-by: Paolo Bonzini <pbonzini@redhat.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 0a495ab35bc7..6d4c6ce21275 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -62,15 +62,6 @@ void context_tracking_enter(enum ctx_state state)
 {
 	unsigned long flags;
 
-	/*
-	 * Repeat the user_enter() check here because some archs may be calling
-	 * this from asm and if no CPU needs context tracking, they shouldn't
-	 * go further. Repeat the check here until they support the inline static
-	 * key check.
-	 */
-	if (!context_tracking_is_enabled())
-		return;
-
 	/*
 	 * Some contexts may involve an exception occuring in an irq,
 	 * leading to that nesting:
@@ -128,7 +119,7 @@ EXPORT_SYMBOL_GPL(context_tracking_enter);
 
 void context_tracking_user_enter(void)
 {
-	context_tracking_enter(CONTEXT_USER);
+	user_enter();
 }
 NOKPROBE_SYMBOL(context_tracking_user_enter);
 
@@ -148,9 +139,6 @@ void context_tracking_exit(enum ctx_state state)
 {
 	unsigned long flags;
 
-	if (!context_tracking_is_enabled())
-		return;
-
 	if (in_interrupt())
 		return;
 
@@ -181,7 +169,7 @@ EXPORT_SYMBOL_GPL(context_tracking_exit);
 
 void context_tracking_user_exit(void)
 {
-	context_tracking_exit(CONTEXT_USER);
+	user_exit();
 }
 NOKPROBE_SYMBOL(context_tracking_user_exit);
 

commit fafe870f31212a72f3c2d74e7b90e4ef39e83ee1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed May 6 18:04:24 2015 +0200

    context_tracking: Inherit TIF_NOHZ through forks instead of context switches
    
    TIF_NOHZ is used by context_tracking to force syscall slow-path
    on every task in order to track userspace roundtrips. As such,
    it must be set on all running tasks.
    
    It's currently explicitly inherited through context switches.
    There is no need to do it in this fast-path though. The flag
    could simply be set once for all on all tasks, whether they are
    running or not.
    
    Lets do this by setting the flag for the init task on early boot,
    and let it propagate through fork inheritance.
    
    While at it, mark context_tracking_cpu_set() as init code, we
    only need it at early boot time.
    
    Suggested-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1430928266-24888-3-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 5b11a10e196a..0a495ab35bc7 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -30,14 +30,6 @@ EXPORT_SYMBOL_GPL(context_tracking_enabled);
 DEFINE_PER_CPU(struct context_tracking, context_tracking);
 EXPORT_SYMBOL_GPL(context_tracking);
 
-void context_tracking_cpu_set(int cpu)
-{
-	if (!per_cpu(context_tracking.active, cpu)) {
-		per_cpu(context_tracking.active, cpu) = true;
-		static_key_slow_inc(&context_tracking_enabled);
-	}
-}
-
 static bool context_tracking_recursion_enter(void)
 {
 	int recursion;
@@ -193,24 +185,26 @@ void context_tracking_user_exit(void)
 }
 NOKPROBE_SYMBOL(context_tracking_user_exit);
 
-/**
- * __context_tracking_task_switch - context switch the syscall callbacks
- * @prev: the task that is being switched out
- * @next: the task that is being switched in
- *
- * The context tracking uses the syscall slow path to implement its user-kernel
- * boundaries probes on syscalls. This way it doesn't impact the syscall fast
- * path on CPUs that don't do context tracking.
- *
- * But we need to clear the flag on the previous task because it may later
- * migrate to some CPU that doesn't do the context tracking. As such the TIF
- * flag may not be desired there.
- */
-void __context_tracking_task_switch(struct task_struct *prev,
-				    struct task_struct *next)
+void __init context_tracking_cpu_set(int cpu)
 {
-	clear_tsk_thread_flag(prev, TIF_NOHZ);
-	set_tsk_thread_flag(next, TIF_NOHZ);
+	static __initdata bool initialized = false;
+
+	if (!per_cpu(context_tracking.active, cpu)) {
+		per_cpu(context_tracking.active, cpu) = true;
+		static_key_slow_inc(&context_tracking_enabled);
+	}
+
+	if (initialized)
+		return;
+
+	/*
+	 * Set TIF_NOHZ to init/0 and let it propagate to all tasks through fork
+	 * This assumes that init is the only task at this early boot stage.
+	 */
+	set_tsk_thread_flag(&init_task, TIF_NOHZ);
+	WARN_ON_ONCE(!tasklist_empty());
+
+	initialized = true;
 }
 
 #ifdef CONFIG_CONTEXT_TRACKING_FORCE

commit aed5ed47724f6a7453fa62e3c90f3cee93edbfe3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed May 6 18:04:23 2015 +0200

    context_tracking: Protect against recursion
    
    Context tracking recursion can happen when an exception triggers
    in the middle of a call to a context tracking probe.
    
    This special case can be caused by vmalloc faults. If an access
    to a memory area allocated by vmalloc happens in the middle of
    context_tracking_enter(), we may run into an endless fault loop
    because the exception in turn calls context_tracking_enter()
    which faults on the same vmalloc'ed memory, triggering an
    exception again, etc...
    
    Some rare crashes have been reported so lets protect against
    this with a recursion counter.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1430928266-24888-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 72d59a1a6eb6..5b11a10e196a 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -38,6 +38,25 @@ void context_tracking_cpu_set(int cpu)
 	}
 }
 
+static bool context_tracking_recursion_enter(void)
+{
+	int recursion;
+
+	recursion = __this_cpu_inc_return(context_tracking.recursion);
+	if (recursion == 1)
+		return true;
+
+	WARN_ONCE((recursion < 1), "Invalid context tracking recursion value %d\n", recursion);
+	__this_cpu_dec(context_tracking.recursion);
+
+	return false;
+}
+
+static void context_tracking_recursion_exit(void)
+{
+	__this_cpu_dec(context_tracking.recursion);
+}
+
 /**
  * context_tracking_enter - Inform the context tracking that the CPU is going
  *                          enter user or guest space mode.
@@ -75,6 +94,9 @@ void context_tracking_enter(enum ctx_state state)
 	WARN_ON_ONCE(!current->mm);
 
 	local_irq_save(flags);
+	if (!context_tracking_recursion_enter())
+		goto out_irq_restore;
+
 	if ( __this_cpu_read(context_tracking.state) != state) {
 		if (__this_cpu_read(context_tracking.active)) {
 			/*
@@ -105,6 +127,8 @@ void context_tracking_enter(enum ctx_state state)
 		 */
 		__this_cpu_write(context_tracking.state, state);
 	}
+	context_tracking_recursion_exit();
+out_irq_restore:
 	local_irq_restore(flags);
 }
 NOKPROBE_SYMBOL(context_tracking_enter);
@@ -139,6 +163,9 @@ void context_tracking_exit(enum ctx_state state)
 		return;
 
 	local_irq_save(flags);
+	if (!context_tracking_recursion_enter())
+		goto out_irq_restore;
+
 	if (__this_cpu_read(context_tracking.state) == state) {
 		if (__this_cpu_read(context_tracking.active)) {
 			/*
@@ -153,6 +180,8 @@ void context_tracking_exit(enum ctx_state state)
 		}
 		__this_cpu_write(context_tracking.state, CONTEXT_KERNEL);
 	}
+	context_tracking_recursion_exit();
+out_irq_restore:
 	local_irq_restore(flags);
 }
 NOKPROBE_SYMBOL(context_tracking_exit);

commit efc1e2c9bcbab73797d7bc214014cb916d6a8eb5
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 10 15:27:53 2015 -0500

    context_tracking: Export context_tracking_user_enter/exit
    
    Export context_tracking_user_enter/exit so it can be used by KVM.
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Will deacon <will.deacon@arm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index a2c0866384e8..72d59a1a6eb6 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -108,6 +108,7 @@ void context_tracking_enter(enum ctx_state state)
 	local_irq_restore(flags);
 }
 NOKPROBE_SYMBOL(context_tracking_enter);
+EXPORT_SYMBOL_GPL(context_tracking_enter);
 
 void context_tracking_user_enter(void)
 {
@@ -155,6 +156,7 @@ void context_tracking_exit(enum ctx_state state)
 	local_irq_restore(flags);
 }
 NOKPROBE_SYMBOL(context_tracking_exit);
+EXPORT_SYMBOL_GPL(context_tracking_exit);
 
 void context_tracking_user_exit(void)
 {

commit 19fdd98b6253404c6bdd6927bde9f962729376f7
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 10 15:27:52 2015 -0500

    context_tracking: Run vtime_user_enter/exit only when state == CONTEXT_USER
    
    Only run vtime_user_enter, vtime_user_exit, and the user enter & exit
    trace points when we are entering or exiting user state, respectively.
    
    The KVM code in guest_enter and guest_exit already take care of calling
    vtime_guest_enter and vtime_guest_exit, respectively.
    
    The RCU code only distinguishes between "idle" and "not idle or kernel".
    There should be no need to add an additional (unused) state there.
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Will deacon <will.deacon@arm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 17715d811b71..a2c0866384e8 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -77,7 +77,6 @@ void context_tracking_enter(enum ctx_state state)
 	local_irq_save(flags);
 	if ( __this_cpu_read(context_tracking.state) != state) {
 		if (__this_cpu_read(context_tracking.active)) {
-			trace_user_enter(0);
 			/*
 			 * At this stage, only low level arch entry code remains and
 			 * then we'll run in userspace. We can assume there won't be
@@ -85,7 +84,10 @@ void context_tracking_enter(enum ctx_state state)
 			 * user_exit() or rcu_irq_enter(). Let's remove RCU's dependency
 			 * on the tick.
 			 */
-			vtime_user_enter(current);
+			if (state == CONTEXT_USER) {
+				trace_user_enter(0);
+				vtime_user_enter(current);
+			}
 			rcu_user_enter();
 		}
 		/*
@@ -143,8 +145,10 @@ void context_tracking_exit(enum ctx_state state)
 			 * RCU core about that (ie: we may need the tick again).
 			 */
 			rcu_user_exit();
-			vtime_user_exit(current);
-			trace_user_exit(0);
+			if (state == CONTEXT_USER) {
+				vtime_user_exit(current);
+				trace_user_exit(0);
+			}
 		}
 		__this_cpu_write(context_tracking.state, CONTEXT_KERNEL);
 	}

commit 3aab4f50bff89bdea5066a05d4f3c5fa25bc37c7
Author: Rik van Riel <riel@redhat.com>
Date:   Tue Feb 10 15:27:50 2015 -0500

    context_tracking: Generalize context tracking APIs to support user and guest
    
    Generalize the context tracking APIs to support various nature of
    contexts. This is performed by splitting out the mechanism from
    context_tracking_user_enter and context_tracking_user_exit into
    context_tracking_enter and context_tracking_exit.
    
    The nature of the context we track is now detailed in a ctx_state
    parameter pushed to these APIs, allowing the same functions to not just
    track kernel <> user space switching, but also kernel <> guest transitions.
    
    But leave the old functions in order to avoid breaking ARM, which calls
    these functions from assembler code, and cannot easily use C enum
    parameters.
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Will deacon <will.deacon@arm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 8ad53c9d38b6..17715d811b71 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -39,15 +39,15 @@ void context_tracking_cpu_set(int cpu)
 }
 
 /**
- * context_tracking_user_enter - Inform the context tracking that the CPU is going to
- *                               enter userspace mode.
+ * context_tracking_enter - Inform the context tracking that the CPU is going
+ *                          enter user or guest space mode.
  *
  * This function must be called right before we switch from the kernel
- * to userspace, when it's guaranteed the remaining kernel instructions
- * to execute won't use any RCU read side critical section because this
- * function sets RCU in extended quiescent state.
+ * to user or guest space, when it's guaranteed the remaining kernel
+ * instructions to execute won't use any RCU read side critical section
+ * because this function sets RCU in extended quiescent state.
  */
-void context_tracking_user_enter(void)
+void context_tracking_enter(enum ctx_state state)
 {
 	unsigned long flags;
 
@@ -75,7 +75,7 @@ void context_tracking_user_enter(void)
 	WARN_ON_ONCE(!current->mm);
 
 	local_irq_save(flags);
-	if ( __this_cpu_read(context_tracking.state) != CONTEXT_USER) {
+	if ( __this_cpu_read(context_tracking.state) != state) {
 		if (__this_cpu_read(context_tracking.active)) {
 			trace_user_enter(0);
 			/*
@@ -101,24 +101,31 @@ void context_tracking_user_enter(void)
 		 * OTOH we can spare the calls to vtime and RCU when context_tracking.active
 		 * is false because we know that CPU is not tickless.
 		 */
-		__this_cpu_write(context_tracking.state, CONTEXT_USER);
+		__this_cpu_write(context_tracking.state, state);
 	}
 	local_irq_restore(flags);
 }
+NOKPROBE_SYMBOL(context_tracking_enter);
+
+void context_tracking_user_enter(void)
+{
+	context_tracking_enter(CONTEXT_USER);
+}
 NOKPROBE_SYMBOL(context_tracking_user_enter);
 
 /**
- * context_tracking_user_exit - Inform the context tracking that the CPU is
- *                              exiting userspace mode and entering the kernel.
+ * context_tracking_exit - Inform the context tracking that the CPU is
+ *                         exiting user or guest mode and entering the kernel.
  *
- * This function must be called after we entered the kernel from userspace
- * before any use of RCU read side critical section. This potentially include
- * any high level kernel code like syscalls, exceptions, signal handling, etc...
+ * This function must be called after we entered the kernel from user or
+ * guest space before any use of RCU read side critical section. This
+ * potentially include any high level kernel code like syscalls, exceptions,
+ * signal handling, etc...
  *
  * This call supports re-entrancy. This way it can be called from any exception
  * handler without needing to know if we came from userspace or not.
  */
-void context_tracking_user_exit(void)
+void context_tracking_exit(enum ctx_state state)
 {
 	unsigned long flags;
 
@@ -129,7 +136,7 @@ void context_tracking_user_exit(void)
 		return;
 
 	local_irq_save(flags);
-	if (__this_cpu_read(context_tracking.state) == CONTEXT_USER) {
+	if (__this_cpu_read(context_tracking.state) == state) {
 		if (__this_cpu_read(context_tracking.active)) {
 			/*
 			 * We are going to run code that may use RCU. Inform
@@ -143,6 +150,12 @@ void context_tracking_user_exit(void)
 	}
 	local_irq_restore(flags);
 }
+NOKPROBE_SYMBOL(context_tracking_exit);
+
+void context_tracking_user_exit(void)
+{
+	context_tracking_exit(CONTEXT_USER);
+}
 NOKPROBE_SYMBOL(context_tracking_user_exit);
 
 /**

commit c467ea763fd5d8795b7d1b5a78eb94b6ad8f66ad
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Mar 4 18:06:33 2015 +0100

    context_tracking: Rename context symbols to prepare for transition state
    
    Current context tracking symbols are designed to express living state.
    As such they are prefixed with "IN_": IN_USER, IN_KERNEL.
    
    Now we are going to use these symbols to also express state transitions
    such as context_tracking_enter(IN_USER) or context_tracking_exit(IN_USER).
    But while the "IN_" prefix works well to express entering a context, it's
    confusing to depict a context exit: context_tracking_exit(IN_USER)
    could mean two things:
            1) We are exiting the current context to enter user context.
            2) We are exiting the user context
    We want 2) but the reviewer may be confused and understand 1)
    
    So lets disambiguate these symbols and rename them to CONTEXT_USER and
    CONTEXT_KERNEL.
    
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Will deacon <will.deacon@arm.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 937ecdfdf258..8ad53c9d38b6 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -75,7 +75,7 @@ void context_tracking_user_enter(void)
 	WARN_ON_ONCE(!current->mm);
 
 	local_irq_save(flags);
-	if ( __this_cpu_read(context_tracking.state) != IN_USER) {
+	if ( __this_cpu_read(context_tracking.state) != CONTEXT_USER) {
 		if (__this_cpu_read(context_tracking.active)) {
 			trace_user_enter(0);
 			/*
@@ -101,7 +101,7 @@ void context_tracking_user_enter(void)
 		 * OTOH we can spare the calls to vtime and RCU when context_tracking.active
 		 * is false because we know that CPU is not tickless.
 		 */
-		__this_cpu_write(context_tracking.state, IN_USER);
+		__this_cpu_write(context_tracking.state, CONTEXT_USER);
 	}
 	local_irq_restore(flags);
 }
@@ -129,7 +129,7 @@ void context_tracking_user_exit(void)
 		return;
 
 	local_irq_save(flags);
-	if (__this_cpu_read(context_tracking.state) == IN_USER) {
+	if (__this_cpu_read(context_tracking.state) == CONTEXT_USER) {
 		if (__this_cpu_read(context_tracking.active)) {
 			/*
 			 * We are going to run code that may use RCU. Inform
@@ -139,7 +139,7 @@ void context_tracking_user_exit(void)
 			vtime_user_exit(current);
 			trace_user_exit(0);
 		}
-		__this_cpu_write(context_tracking.state, IN_KERNEL);
+		__this_cpu_write(context_tracking.state, CONTEXT_KERNEL);
 	}
 	local_irq_restore(flags);
 }

commit 009f60e2763568cdcd75bd1cf360c7c7165e2e60
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Oct 5 22:23:22 2014 +0200

    sched: stop the unbound recursion in preempt_schedule_context()
    
    preempt_schedule_context() does preempt_enable_notrace() at the end
    and this can call the same function again; exception_exit() is heavy
    and it is quite possible that need-resched is true again.
    
    1. Change this code to dec preempt_count() and check need_resched()
       by hand.
    
    2. As Linus suggested, we can use the PREEMPT_ACTIVE bit and avoid
       the enable/disable dance around __schedule(). But in this case
       we need to move into sched/core.c.
    
    3. Cosmetic, but x86 forgets to declare this function. This doesn't
       really matter because it is only called by asm helpers, still it
       make sense to add the declaration into asm/preempt.h to match
       preempt_schedule().
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alexander Graf <agraf@suse.de>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Anvin <hpa@zytor.com>
    Cc: Andy Lutomirski <luto@amacapital.net>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: Chuck Ebbert <cebbert.lkml@gmail.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20141005202322.GB27962@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 5664985c46a0..937ecdfdf258 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -107,46 +107,6 @@ void context_tracking_user_enter(void)
 }
 NOKPROBE_SYMBOL(context_tracking_user_enter);
 
-#ifdef CONFIG_PREEMPT
-/**
- * preempt_schedule_context - preempt_schedule called by tracing
- *
- * The tracing infrastructure uses preempt_enable_notrace to prevent
- * recursion and tracing preempt enabling caused by the tracing
- * infrastructure itself. But as tracing can happen in areas coming
- * from userspace or just about to enter userspace, a preempt enable
- * can occur before user_exit() is called. This will cause the scheduler
- * to be called when the system is still in usermode.
- *
- * To prevent this, the preempt_enable_notrace will use this function
- * instead of preempt_schedule() to exit user context if needed before
- * calling the scheduler.
- */
-asmlinkage __visible void __sched notrace preempt_schedule_context(void)
-{
-	enum ctx_state prev_ctx;
-
-	if (likely(!preemptible()))
-		return;
-
-	/*
-	 * Need to disable preemption in case user_exit() is traced
-	 * and the tracer calls preempt_enable_notrace() causing
-	 * an infinite recursion.
-	 */
-	preempt_disable_notrace();
-	prev_ctx = exception_enter();
-	preempt_enable_no_resched_notrace();
-
-	preempt_schedule();
-
-	preempt_disable_notrace();
-	exception_exit(prev_ctx);
-	preempt_enable_notrace();
-}
-EXPORT_SYMBOL_GPL(preempt_schedule_context);
-#endif /* CONFIG_PREEMPT */
-
 /**
  * context_tracking_user_exit - Inform the context tracking that the CPU is
  *                              exiting userspace mode and entering the kernel.

commit 4cdf77a828b056258f48a9f6078bd2f77d9704bb
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Sat Jun 14 06:47:12 2014 +0000

    x86/kprobes: Fix build errors and blacklist context_track_user
    
    This essentially reverts commit:
    
      ecd50f714c42 ("kprobes, x86: Call exception_enter after kprobes handled")
    
    since it causes build errors with CONFIG_CONTEXT_TRACKING and
    that has been made from misunderstandings;
    context_track_user_*() don't involve much in interrupt context,
    it just returns if in_interrupt() is true.
    
    Instead of changing the do_debug/int3(), this just adds
    context_track_user_*() to kprobes blacklist, since those are
    still can be called right before kprobes handles int3 and debug
    exceptions, and probing those will cause an infinite loop.
    
    Reported-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Kees Cook <keescook@chromium.org>
    Link: http://lkml.kernel.org/r/20140614064711.7865.45957.stgit@kbuild-fedora.novalocal
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 019d45008448..5664985c46a0 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -19,6 +19,7 @@
 #include <linux/sched.h>
 #include <linux/hardirq.h>
 #include <linux/export.h>
+#include <linux/kprobes.h>
 
 #define CREATE_TRACE_POINTS
 #include <trace/events/context_tracking.h>
@@ -104,6 +105,7 @@ void context_tracking_user_enter(void)
 	}
 	local_irq_restore(flags);
 }
+NOKPROBE_SYMBOL(context_tracking_user_enter);
 
 #ifdef CONFIG_PREEMPT
 /**
@@ -181,6 +183,7 @@ void context_tracking_user_exit(void)
 	}
 	local_irq_restore(flags);
 }
+NOKPROBE_SYMBOL(context_tracking_user_exit);
 
 /**
  * __context_tracking_task_switch - context switch the syscall callbacks

commit 722a9f9299ca720a3f14660e7c0dce7b76a9cb42
Author: Andi Kleen <ak@linux.intel.com>
Date:   Fri May 2 00:44:38 2014 +0200

    asmlinkage: Add explicit __visible to drivers/*, lib/*, kernel/*
    
    As requested by Linus add explicit __visible to the asmlinkage users.
    This marks functions visible to assembler.
    
    Tree sweep for rest of tree.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Link: http://lkml.kernel.org/r/1398984278-29319-4-git-send-email-andi@firstfloor.org
    Signed-off-by: H. Peter Anvin <hpa@linux.intel.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 6cb20d2e7ee0..019d45008448 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -120,7 +120,7 @@ void context_tracking_user_enter(void)
  * instead of preempt_schedule() to exit user context if needed before
  * calling the scheduler.
  */
-asmlinkage void __sched notrace preempt_schedule_context(void)
+asmlinkage __visible void __sched notrace preempt_schedule_context(void)
 {
 	enum ctx_state prev_ctx;
 

commit 58135f574f1b791c926622387780ed3d090116d6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 6 14:45:57 2013 +0100

    context_tracking: Wrap static key check into more intuitive function name
    
    Use a function with a meaningful name to check the global context
    tracking state. static_key_false() is a bit confusing for reviewers.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index e5f3917aa05b..6cb20d2e7ee0 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -53,10 +53,10 @@ void context_tracking_user_enter(void)
 	/*
 	 * Repeat the user_enter() check here because some archs may be calling
 	 * this from asm and if no CPU needs context tracking, they shouldn't
-	 * go further. Repeat the check here until they support the static key
-	 * check.
+	 * go further. Repeat the check here until they support the inline static
+	 * key check.
 	 */
-	if (!static_key_false(&context_tracking_enabled))
+	if (!context_tracking_is_enabled())
 		return;
 
 	/*
@@ -160,7 +160,7 @@ void context_tracking_user_exit(void)
 {
 	unsigned long flags;
 
-	if (!static_key_false(&context_tracking_enabled))
+	if (!context_tracking_is_enabled())
 		return;
 
 	if (in_interrupt())

commit 37bf06375c90a42fe07b9bebdb07bc316ae5a0ce
Merge: 6bfa687c19b7 d0e639c9e06d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Oct 9 12:36:13 2013 +0200

    Merge tag 'v3.12-rc4' into sched/core
    
    Merge Linux v3.12-rc4 to fix a conflict and also to refresh the tree
    before applying more scheduler patches.
    
    Conflicts:
            arch/avr32/include/asm/Kbuild
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0c06a5d4b13cd66c833805a0d1db76b977944aac
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 10 00:54:17 2013 +0200

    arm: Fix build error with context tracking calls
    
    ad65782fba50 (context_tracking: Optimize main APIs off case
    with static key) converted context tracking main APIs to inline
    function and left ARM asm callers behind.
    
    This can be easily fixed by making ARM calling the post static
    keys context tracking function. We just need to replicate the
    static key checks there. We'll remove these later when ARM will
    support the context tracking static keys.
    
    Reported-by: Guenter Roeck <linux@roeck-us.net>
    Reported-by: Russell King <linux@arm.linux.org.uk>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: Anil Kumar <anilk4.v@gmail.com>
    Cc: Tony Lindgren <tony@atomide.com>
    Cc: Benoit Cousson <b-cousson@ti.com>
    Cc: Guenter Roeck <linux@roeck-us.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 247091bf0587..859c8dfd78a1 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -50,6 +50,15 @@ void context_tracking_user_enter(void)
 {
 	unsigned long flags;
 
+	/*
+	 * Repeat the user_enter() check here because some archs may be calling
+	 * this from asm and if no CPU needs context tracking, they shouldn't
+	 * go further. Repeat the check here until they support the static key
+	 * check.
+	 */
+	if (!static_key_false(&context_tracking_enabled))
+		return;
+
 	/*
 	 * Some contexts may involve an exception occuring in an irq,
 	 * leading to that nesting:
@@ -151,6 +160,9 @@ void context_tracking_user_exit(void)
 {
 	unsigned long flags;
 
+	if (!static_key_false(&context_tracking_enabled))
+		return;
+
 	if (in_interrupt())
 		return;
 

commit bdb43806589096ac4272fe1307e789846ac08d7c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 10 12:15:23 2013 +0200

    sched: Extract the basic add/sub preempt_count modifiers
    
    Rewrite the preempt_count macros in order to extract the 3 basic
    preempt_count value modifiers:
    
      __preempt_count_add()
      __preempt_count_sub()
    
    and the new:
    
      __preempt_count_dec_and_test()
    
    And since we're at it anyway, replace the unconventional
    $op_preempt_count names with the more conventional preempt_count_$op.
    
    Since these basic operators are equivalent to the previous _notrace()
    variants, do away with the _notrace() versions.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-ewbpdbupy9xpsjhg960zwbv8@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 247091bf0587..013161f1c807 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -111,7 +111,7 @@ void context_tracking_user_enter(void)
  * instead of preempt_schedule() to exit user context if needed before
  * calling the scheduler.
  */
-void __sched notrace preempt_schedule_context(void)
+asmlinkage void __sched notrace preempt_schedule_context(void)
 {
 	enum ctx_state prev_ctx;
 

commit 1b6a259aa5ab16d8b215bfc19ff7c9ffa8858f10
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 11 20:27:43 2013 +0200

    context_tracking: User/kernel broundary cross trace events
    
    This can be useful to track all kernel/user round trips.
    And it's also helpful to debug the context tracking subsystem.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index c17822673c39..247091bf0587 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -20,6 +20,9 @@
 #include <linux/hardirq.h>
 #include <linux/export.h>
 
+#define CREATE_TRACE_POINTS
+#include <trace/events/context_tracking.h>
+
 struct static_key context_tracking_enabled = STATIC_KEY_INIT_FALSE;
 EXPORT_SYMBOL_GPL(context_tracking_enabled);
 
@@ -64,6 +67,7 @@ void context_tracking_user_enter(void)
 	local_irq_save(flags);
 	if ( __this_cpu_read(context_tracking.state) != IN_USER) {
 		if (__this_cpu_read(context_tracking.active)) {
+			trace_user_enter(0);
 			/*
 			 * At this stage, only low level arch entry code remains and
 			 * then we'll run in userspace. We can assume there won't be
@@ -159,6 +163,7 @@ void context_tracking_user_exit(void)
 			 */
 			rcu_user_exit();
 			vtime_user_exit(current);
+			trace_user_exit(0);
 		}
 		__this_cpu_write(context_tracking.state, IN_KERNEL);
 	}

commit 73d424f9af7b571276e6284617cb59726d47bf12
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 11 19:42:13 2013 +0200

    context_tracking: Optimize context switch off case with static keys
    
    No need for syscall slowpath if no CPU is full dynticks,
    rather nop this in this case.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index b6a186c4b886..c17822673c39 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -166,7 +166,7 @@ void context_tracking_user_exit(void)
 }
 
 /**
- * context_tracking_task_switch - context switch the syscall callbacks
+ * __context_tracking_task_switch - context switch the syscall callbacks
  * @prev: the task that is being switched out
  * @next: the task that is being switched in
  *
@@ -178,8 +178,8 @@ void context_tracking_user_exit(void)
  * migrate to some CPU that doesn't do the context tracking. As such the TIF
  * flag may not be desired there.
  */
-void context_tracking_task_switch(struct task_struct *prev,
-			     struct task_struct *next)
+void __context_tracking_task_switch(struct task_struct *prev,
+				    struct task_struct *next)
 {
 	clear_tsk_thread_flag(prev, TIF_NOHZ);
 	set_tsk_thread_flag(next, TIF_NOHZ);

commit 48d6a816a8bf36e2a197c322697323003bdc1cfe
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 10 02:44:35 2013 +0200

    context_tracking: Optimize guest APIs off case with static key
    
    Optimize guest entry/exit APIs with static keys. This minimize
    the overhead for those who enable CONFIG_NO_HZ_FULL without
    always using it. Having no range passed to nohz_full= should
    result in the probes overhead to be minimized.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 6e89e094c80e..b6a186c4b886 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -21,8 +21,10 @@
 #include <linux/export.h>
 
 struct static_key context_tracking_enabled = STATIC_KEY_INIT_FALSE;
+EXPORT_SYMBOL_GPL(context_tracking_enabled);
 
 DEFINE_PER_CPU(struct context_tracking, context_tracking);
+EXPORT_SYMBOL_GPL(context_tracking);
 
 void context_tracking_cpu_set(int cpu)
 {
@@ -163,27 +165,6 @@ void context_tracking_user_exit(void)
 	local_irq_restore(flags);
 }
 
-#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
-void guest_enter(void)
-{
-	if (vtime_accounting_enabled())
-		vtime_guest_enter(current);
-	else
-		current->flags |= PF_VCPU;
-}
-EXPORT_SYMBOL_GPL(guest_enter);
-
-void guest_exit(void)
-{
-	if (vtime_accounting_enabled())
-		vtime_guest_exit(current);
-	else
-		current->flags &= ~PF_VCPU;
-}
-EXPORT_SYMBOL_GPL(guest_exit);
-#endif /* CONFIG_VIRT_CPU_ACCOUNTING_GEN */
-
-
 /**
  * context_tracking_task_switch - context switch the syscall callbacks
  * @prev: the task that is being switched out

commit ad65782fba507d91a0a98f519b59e79cac1b474c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 10 02:44:35 2013 +0200

    context_tracking: Optimize main APIs off case with static key
    
    Optimize user and exception entry/exit APIs with static
    keys. This minimize the overhead for those who enable
    CONFIG_NO_HZ_FULL without always using it. Having no range
    passed to nohz_full= should result in the probes to be nopped
    (at least we hope so...).
    
    If this proves not be enough in the long term, we'll need
    to bring an exception slow path by re-routing the exception
    handlers.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 839d377d0da5..6e89e094c80e 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -33,15 +33,15 @@ void context_tracking_cpu_set(int cpu)
 }
 
 /**
- * user_enter - Inform the context tracking that the CPU is going to
- *              enter userspace mode.
+ * context_tracking_user_enter - Inform the context tracking that the CPU is going to
+ *                               enter userspace mode.
  *
  * This function must be called right before we switch from the kernel
  * to userspace, when it's guaranteed the remaining kernel instructions
  * to execute won't use any RCU read side critical section because this
  * function sets RCU in extended quiescent state.
  */
-void user_enter(void)
+void context_tracking_user_enter(void)
 {
 	unsigned long flags;
 
@@ -131,8 +131,8 @@ EXPORT_SYMBOL_GPL(preempt_schedule_context);
 #endif /* CONFIG_PREEMPT */
 
 /**
- * user_exit - Inform the context tracking that the CPU is
- *             exiting userspace mode and entering the kernel.
+ * context_tracking_user_exit - Inform the context tracking that the CPU is
+ *                              exiting userspace mode and entering the kernel.
  *
  * This function must be called after we entered the kernel from userspace
  * before any use of RCU read side critical section. This potentially include
@@ -141,7 +141,7 @@ EXPORT_SYMBOL_GPL(preempt_schedule_context);
  * This call supports re-entrancy. This way it can be called from any exception
  * handler without needing to know if we came from userspace or not.
  */
-void user_exit(void)
+void context_tracking_user_exit(void)
 {
 	unsigned long flags;
 

commit 65f382fd0c8fa483713c0971de9f1dfb4cf1ad9c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 11 19:12:32 2013 +0200

    context_tracking: Ground setup for static key use
    
    Prepare for using a static key in the context tracking subsystem.
    This will help optimizing the off case on its many users:
    
    * user_enter, user_exit, exception_enter, exception_exit, guest_enter,
      guest_exit, vtime_*()
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 72bcb2570d3e..839d377d0da5 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -20,15 +20,16 @@
 #include <linux/hardirq.h>
 #include <linux/export.h>
 
-DEFINE_PER_CPU(struct context_tracking, context_tracking) = {
-#ifdef CONFIG_CONTEXT_TRACKING_FORCE
-	.active = true,
-#endif
-};
+struct static_key context_tracking_enabled = STATIC_KEY_INIT_FALSE;
+
+DEFINE_PER_CPU(struct context_tracking, context_tracking);
 
 void context_tracking_cpu_set(int cpu)
 {
-	per_cpu(context_tracking.active, cpu) = true;
+	if (!per_cpu(context_tracking.active, cpu)) {
+		per_cpu(context_tracking.active, cpu) = true;
+		static_key_slow_inc(&context_tracking_enabled);
+	}
 }
 
 /**
@@ -202,3 +203,13 @@ void context_tracking_task_switch(struct task_struct *prev,
 	clear_tsk_thread_flag(prev, TIF_NOHZ);
 	set_tsk_thread_flag(next, TIF_NOHZ);
 }
+
+#ifdef CONFIG_CONTEXT_TRACKING_FORCE
+void __init context_tracking_init(void)
+{
+	int cpu;
+
+	for_each_possible_cpu(cpu)
+		context_tracking_cpu_set(cpu);
+}
+#endif

commit 2e70933866ace52091a3c11a5c104c063ab0c445
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 10 00:55:25 2013 +0200

    nohz: Only enable context tracking on full dynticks CPUs
    
    The context tracking subsystem has the ability to selectively
    enable the tracking on any defined subset of CPU. This means that
    we can define a CPU range that doesn't run the context tracking
    and another range that does.
    
    Now what we want in practice is to enable the tracking on full
    dynticks CPUs only. In order to perform this, we just need to pass
    our full dynticks CPU range selection from the full dynticks
    subsystem to the context tracking.
    
    This way we can spare the overhead of RCU user extended quiescent
    state and vtime maintainance on the CPUs that are outside the
    full dynticks range. Just keep in mind the raw context tracking
    itself is still necessary everywhere.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 7b095de356c5..72bcb2570d3e 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -26,6 +26,11 @@ DEFINE_PER_CPU(struct context_tracking, context_tracking) = {
 #endif
 };
 
+void context_tracking_cpu_set(int cpu)
+{
+	per_cpu(context_tracking.active, cpu) = true;
+}
+
 /**
  * user_enter - Inform the context tracking that the CPU is going to
  *              enter userspace mode.

commit d65ec12127a5b6c6d7f5331c78157dab98a20ff0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 11 23:59:33 2013 +0200

    context_tracking: Fix runtime CPU off-case
    
    As long as the context tracking is enabled on any CPU, even
    a single one, all other CPUs need to keep track of their
    user <-> kernel boundaries cross as well.
    
    This is because a task can sleep while servicing an exception
    that happened in the kernel or in userspace. Then when the task
    eventually wakes up and return from the exception, the CPU needs
    to know if we resume in userspace or in the kernel. exception_exit()
    get this information from exception_enter() that saved the previous
    state.
    
    If the CPU where the exception happened didn't keep track of
    these informations, exception_exit() doesn't know which state
    tracking to restore on the CPU where the task got migrated
    and we may return to userspace with the context tracking
    subsystem thinking that we are in kernel mode.
    
    This can be fixed in the long term if we move our context tracking
    probes on very low level arch fast path user <-> kernel boundary,
    although even that is worrisome as an exception can still happen
    in the few instructions between the probe and the actual iret.
    
    Also we are not yet ready to set these probes in the fast path given
    the potential overhead problem it induces.
    
    So let's fix this by always enable context tracking even on CPUs
    that are not in the full dynticks range. OTOH we can spare the
    rcu_user_*() and vtime_user_*() calls there because the tick runs
    on these CPUs and we can handle RCU state machine and cputime
    accounting through it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 1f47119c5b09..7b095de356c5 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -54,17 +54,31 @@ void user_enter(void)
 	WARN_ON_ONCE(!current->mm);
 
 	local_irq_save(flags);
-	if (__this_cpu_read(context_tracking.active) &&
-	    __this_cpu_read(context_tracking.state) != IN_USER) {
+	if ( __this_cpu_read(context_tracking.state) != IN_USER) {
+		if (__this_cpu_read(context_tracking.active)) {
+			/*
+			 * At this stage, only low level arch entry code remains and
+			 * then we'll run in userspace. We can assume there won't be
+			 * any RCU read-side critical section until the next call to
+			 * user_exit() or rcu_irq_enter(). Let's remove RCU's dependency
+			 * on the tick.
+			 */
+			vtime_user_enter(current);
+			rcu_user_enter();
+		}
 		/*
-		 * At this stage, only low level arch entry code remains and
-		 * then we'll run in userspace. We can assume there won't be
-		 * any RCU read-side critical section until the next call to
-		 * user_exit() or rcu_irq_enter(). Let's remove RCU's dependency
-		 * on the tick.
+		 * Even if context tracking is disabled on this CPU, because it's outside
+		 * the full dynticks mask for example, we still have to keep track of the
+		 * context transitions and states to prevent inconsistency on those of
+		 * other CPUs.
+		 * If a task triggers an exception in userspace, sleep on the exception
+		 * handler and then migrate to another CPU, that new CPU must know where
+		 * the exception returns by the time we call exception_exit().
+		 * This information can only be provided by the previous CPU when it called
+		 * exception_enter().
+		 * OTOH we can spare the calls to vtime and RCU when context_tracking.active
+		 * is false because we know that CPU is not tickless.
 		 */
-		vtime_user_enter(current);
-		rcu_user_enter();
 		__this_cpu_write(context_tracking.state, IN_USER);
 	}
 	local_irq_restore(flags);
@@ -130,12 +144,14 @@ void user_exit(void)
 
 	local_irq_save(flags);
 	if (__this_cpu_read(context_tracking.state) == IN_USER) {
-		/*
-		 * We are going to run code that may use RCU. Inform
-		 * RCU core about that (ie: we may need the tick again).
-		 */
-		rcu_user_exit();
-		vtime_user_exit(current);
+		if (__this_cpu_read(context_tracking.active)) {
+			/*
+			 * We are going to run code that may use RCU. Inform
+			 * RCU core about that (ie: we may need the tick again).
+			 */
+			rcu_user_exit();
+			vtime_user_exit(current);
+		}
 		__this_cpu_write(context_tracking.state, IN_KERNEL);
 	}
 	local_irq_restore(flags);
@@ -178,8 +194,6 @@ EXPORT_SYMBOL_GPL(guest_exit);
 void context_tracking_task_switch(struct task_struct *prev,
 			     struct task_struct *next)
 {
-	if (__this_cpu_read(context_tracking.active)) {
-		clear_tsk_thread_flag(prev, TIF_NOHZ);
-		set_tsk_thread_flag(next, TIF_NOHZ);
-	}
+	clear_tsk_thread_flag(prev, TIF_NOHZ);
+	set_tsk_thread_flag(next, TIF_NOHZ);
 }

commit 2d854e5738cded368a0759f85b1197f5c044513d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 12 19:02:30 2013 +0200

    context_tracing: Fix guest accounting with native vtime
    
    1) If context tracking is enabled with native vtime accounting (which
    combo is useless except for dev testing), we call vtime_guest_enter()
    and vtime_guest_exit() on host <-> guest switches. But those are stubs
    in this configurations. As a result, cputime is not correctly flushed
    on kvm context switches.
    
    2) If context tracking runs but is disabled on some CPUs, those
    CPUs end up calling __guest_enter/__guest_exit which in turn
    call vtime_account_system(). We don't want to call this because we
    run in tick based accounting for these CPUs.
    
    Refactor the guest_enter/guest_exit code such that all combinations
    finally work.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 942835c12ae5..1f47119c5b09 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -141,12 +141,13 @@ void user_exit(void)
 	local_irq_restore(flags);
 }
 
+#ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
 void guest_enter(void)
 {
 	if (vtime_accounting_enabled())
 		vtime_guest_enter(current);
 	else
-		__guest_enter();
+		current->flags |= PF_VCPU;
 }
 EXPORT_SYMBOL_GPL(guest_enter);
 
@@ -155,9 +156,10 @@ void guest_exit(void)
 	if (vtime_accounting_enabled())
 		vtime_guest_exit(current);
 	else
-		__guest_exit();
+		current->flags &= ~PF_VCPU;
 }
 EXPORT_SYMBOL_GPL(guest_exit);
+#endif /* CONFIG_VIRT_CPU_ACCOUNTING_GEN */
 
 
 /**

commit fbb00b568bc93073452d2a0f9f06e7c33d16eece
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jun 19 23:56:22 2013 +0200

    sched: Consolidate open coded preemptible() checks
    
    preempt_schedule() and preempt_schedule_context() open
    code their preemptability checks.
    
    Use the standard API instead for consolidation.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Alex Shi <alex.shi@intel.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 383f8231e436..942835c12ae5 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -87,10 +87,9 @@ void user_enter(void)
  */
 void __sched notrace preempt_schedule_context(void)
 {
-	struct thread_info *ti = current_thread_info();
 	enum ctx_state prev_ctx;
 
-	if (likely(ti->preempt_count || irqs_disabled()))
+	if (likely(!preemptible()))
 		return;
 
 	/*

commit a3d5c3460a86f52ea435b3fb98be112bd18faabc
Merge: 86c76676cfdb 29bb9e5a7568
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 20 08:18:35 2013 -1000

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Two smaller fixes - plus a context tracking tracing fix that is a bit
      bigger"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      tracing/context-tracking: Add preempt_schedule_context() for tracing
      sched: Fix clear NOHZ_BALANCE_KICK
      sched/x86: Construct all sibling maps if smt

commit 29bb9e5a75684106a37593ad75ec75ff8312731b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri May 24 15:23:40 2013 -0400

    tracing/context-tracking: Add preempt_schedule_context() for tracing
    
    Dave Jones hit the following bug report:
    
     ===============================
     [ INFO: suspicious RCU usage. ]
     3.10.0-rc2+ #1 Not tainted
     -------------------------------
     include/linux/rcupdate.h:771 rcu_read_lock() used illegally while idle!
     other info that might help us debug this:
     RCU used illegally from idle CPU! rcu_scheduler_active = 1, debug_locks = 0
     RCU used illegally from extended quiescent state!
     2 locks held by cc1/63645:
      #0:  (&rq->lock){-.-.-.}, at: [<ffffffff816b39fd>] __schedule+0xed/0x9b0
      #1:  (rcu_read_lock){.+.+..}, at: [<ffffffff8109d645>] cpuacct_charge+0x5/0x1f0
    
     CPU: 1 PID: 63645 Comm: cc1 Not tainted 3.10.0-rc2+ #1 [loadavg: 40.57 27.55 13.39 25/277 64369]
     Hardware name: Gigabyte Technology Co., Ltd. GA-MA78GM-S2H/GA-MA78GM-S2H, BIOS F12a 04/23/2010
      0000000000000000 ffff88010f78fcf8 ffffffff816ae383 ffff88010f78fd28
      ffffffff810b698d ffff88011c092548 000000000023d073 ffff88011c092500
      0000000000000001 ffff88010f78fd60 ffffffff8109d7c5 ffffffff8109d645
     Call Trace:
      [<ffffffff816ae383>] dump_stack+0x19/0x1b
      [<ffffffff810b698d>] lockdep_rcu_suspicious+0xfd/0x130
      [<ffffffff8109d7c5>] cpuacct_charge+0x185/0x1f0
      [<ffffffff8109d645>] ? cpuacct_charge+0x5/0x1f0
      [<ffffffff8108dffc>] update_curr+0xec/0x240
      [<ffffffff8108f528>] put_prev_task_fair+0x228/0x480
      [<ffffffff816b3a71>] __schedule+0x161/0x9b0
      [<ffffffff816b4721>] preempt_schedule+0x51/0x80
      [<ffffffff816b4800>] ? __cond_resched_softirq+0x60/0x60
      [<ffffffff816b6824>] ? retint_careful+0x12/0x2e
      [<ffffffff810ff3cc>] ftrace_ops_control_func+0x1dc/0x210
      [<ffffffff816be280>] ftrace_call+0x5/0x2f
      [<ffffffff816b681d>] ? retint_careful+0xb/0x2e
      [<ffffffff816b4805>] ? schedule_user+0x5/0x70
      [<ffffffff816b4805>] ? schedule_user+0x5/0x70
      [<ffffffff816b6824>] ? retint_careful+0x12/0x2e
     ------------[ cut here ]------------
    
    What happened was that the function tracer traced the schedule_user() code
    that tells RCU that the system is coming back from userspace, and to
    add the CPU back to the RCU monitoring.
    
    Because the function tracer does a preempt_disable/enable_notrace() calls
    the preempt_enable_notrace() checks the NEED_RESCHED flag. If it is set,
    then preempt_schedule() is called. But this is called before the user_exit()
    function can inform the kernel that the CPU is no longer in user mode and
    needs to be accounted for by RCU.
    
    The fix is to create a new preempt_schedule_context() that checks if
    the kernel is still in user mode and if so to switch it to kernel mode
    before calling schedule. It also switches back to user mode coming back
    from schedule in need be.
    
    The only user of this currently is the preempt_enable_notrace(), which is
    only used by the tracing subsystem.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1369423420.6828.226.camel@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 65349f07b878..66677003e223 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -71,6 +71,46 @@ void user_enter(void)
 	local_irq_restore(flags);
 }
 
+#ifdef CONFIG_PREEMPT
+/**
+ * preempt_schedule_context - preempt_schedule called by tracing
+ *
+ * The tracing infrastructure uses preempt_enable_notrace to prevent
+ * recursion and tracing preempt enabling caused by the tracing
+ * infrastructure itself. But as tracing can happen in areas coming
+ * from userspace or just about to enter userspace, a preempt enable
+ * can occur before user_exit() is called. This will cause the scheduler
+ * to be called when the system is still in usermode.
+ *
+ * To prevent this, the preempt_enable_notrace will use this function
+ * instead of preempt_schedule() to exit user context if needed before
+ * calling the scheduler.
+ */
+void __sched notrace preempt_schedule_context(void)
+{
+	struct thread_info *ti = current_thread_info();
+	enum ctx_state prev_ctx;
+
+	if (likely(ti->preempt_count || irqs_disabled()))
+		return;
+
+	/*
+	 * Need to disable preemption in case user_exit() is traced
+	 * and the tracer calls preempt_enable_notrace() causing
+	 * an infinite recursion.
+	 */
+	preempt_disable_notrace();
+	prev_ctx = exception_enter();
+	preempt_enable_no_resched_notrace();
+
+	preempt_schedule();
+
+	preempt_disable_notrace();
+	exception_exit(prev_ctx);
+	preempt_enable_notrace();
+}
+EXPORT_SYMBOL_GPL(preempt_schedule_context);
+#endif /* CONFIG_PREEMPT */
 
 /**
  * user_exit - Inform the context tracking that the CPU is

commit 521921bad1192fb1b8f9b6a5aa673635848b8b5f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 16 01:21:38 2013 +0200

    kvm: Move guest entry/exit APIs to context_tracking
    
    The kvm_host.h header file doesn't handle well
    inclusion when archs don't support KVM.
    
    This results in build crashes for such archs when they
    want to implement context tracking because this subsystem
    includes kvm_host.h in order to implement the
    guest_enter/exit APIs but it doesn't handle KVM off case.
    
    To fix this, move the guest_enter()/guest_exit()
    declarations and generic implementation to the context
    tracking headers. These generic APIs actually belong to
    this subsystem, besides other domains boundary tracking
    like user_enter() et al.
    
    KVM now properly becomes a user of this library, not the
    other buggy way around.
    
    Reported-by: Kevin Hilman <khilman@linaro.org>
    Reviewed-by: Kevin Hilman <khilman@linaro.org>
    Tested-by: Kevin Hilman <khilman@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 65349f07b878..85bdde1137eb 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -15,7 +15,6 @@
  */
 
 #include <linux/context_tracking.h>
-#include <linux/kvm_host.h>
 #include <linux/rcupdate.h>
 #include <linux/sched.h>
 #include <linux/hardirq.h>

commit d652e1eb8e7b739fccbfb503a3da3e9f640fbf3d
Merge: 8f55cea410db 77852fea6e24
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 19 18:19:48 2013 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "Main changes:
    
       - scheduler side full-dynticks (user-space execution is undisturbed
         and receives no timer IRQs) preparation changes that convert the
         cputime accounting code to be full-dynticks ready, from Frederic
         Weisbecker.
    
       - Initial sched.h split-up changes, by Clark Williams
    
       - select_idle_sibling() performance improvement by Mike Galbraith:
    
            " 1 tbench pair (worst case) in a 10 core + SMT package:
    
              pre   15.22 MB/sec 1 procs
              post 252.01 MB/sec 1 procs "
    
      - sched_rr_get_interval() ABI fix/change.  We think this detail is not
        used by apps (so it's not an ABI in practice), but lets keep it
        under observation.
    
      - misc RT scheduling cleanups, optimizations"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (24 commits)
      sched/rt: Add <linux/sched/rt.h> header to <linux/init_task.h>
      cputime: Remove irqsave from seqlock readers
      sched, powerpc: Fix sched.h split-up build failure
      cputime: Restore CPU_ACCOUNTING config defaults for PPC64
      sched/rt: Move rt specific bits into new header file
      sched/rt: Add a tuning knob to allow changing SCHED_RR timeslice
      sched: Move sched.h sysctl bits into separate header
      sched: Fix signedness bug in yield_to()
      sched: Fix select_idle_sibling() bouncing cow syndrome
      sched/rt: Further simplify pick_rt_task()
      sched/rt: Do not account zero delta_exec in update_curr_rt()
      cputime: Safely read cputime of full dynticks CPUs
      kvm: Prepare to add generic guest entry/exit callbacks
      cputime: Use accessors to read task cputime stats
      cputime: Allow dynamic switch between tick/virtual based cputime accounting
      cputime: Generic on-demand virtual cputime accounting
      cputime: Move default nsecs_to_cputime() to jiffies based cputime file
      cputime: Librarize per nsecs resolution cputime definitions
      cputime: Avoid multiplication overflow on utime scaling
      context_tracking: Export context state for generic vtime
      ...
    
    Fix up conflict in kernel/context_tracking.c due to comment additions.

commit 6a61671bb2f3a1bd12cd17b8fca811a624782632
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 16 20:00:34 2012 +0100

    cputime: Safely read cputime of full dynticks CPUs
    
    While remotely reading the cputime of a task running in a
    full dynticks CPU, the values stored in utime/stime fields
    of struct task_struct may be stale. Its values may be those
    of the last kernel <-> user transition time snapshot and
    we need to add the tickless time spent since this snapshot.
    
    To fix this, flush the cputime of the dynticks CPUs on
    kernel <-> user transition and record the time / context
    where we did this. Then on top of this snapshot and the current
    time, perform the fixup on the reader side from task_times()
    accessors.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [fixed kvm module related build errors]
    Signed-off-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 9002e92e6372..74f68f4dc6c2 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -1,8 +1,9 @@
 #include <linux/context_tracking.h>
+#include <linux/kvm_host.h>
 #include <linux/rcupdate.h>
 #include <linux/sched.h>
 #include <linux/hardirq.h>
-
+#include <linux/export.h>
 
 DEFINE_PER_CPU(struct context_tracking, context_tracking) = {
 #ifdef CONFIG_CONTEXT_TRACKING_FORCE
@@ -61,6 +62,24 @@ void user_exit(void)
 	local_irq_restore(flags);
 }
 
+void guest_enter(void)
+{
+	if (vtime_accounting_enabled())
+		vtime_guest_enter(current);
+	else
+		__guest_enter();
+}
+EXPORT_SYMBOL_GPL(guest_enter);
+
+void guest_exit(void)
+{
+	if (vtime_accounting_enabled())
+		vtime_guest_exit(current);
+	else
+		__guest_exit();
+}
+EXPORT_SYMBOL_GPL(guest_exit);
+
 void context_tracking_task_switch(struct task_struct *prev,
 			     struct task_struct *next)
 {

commit abf917cd91cbb73952758f9741e2fa65002a48ee
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 25 07:56:04 2012 +0200

    cputime: Generic on-demand virtual cputime accounting
    
    If we want to stop the tick further idle, we need to be
    able to account the cputime without using the tick.
    
    Virtual based cputime accounting solves that problem by
    hooking into kernel/user boundaries.
    
    However implementing CONFIG_VIRT_CPU_ACCOUNTING require
    low level hooks and involves more overhead. But we already
    have a generic context tracking subsystem that is required
    for RCU needs by archs which plan to shut down the tick
    outside idle.
    
    This patch implements a generic virtual based cputime
    accounting that relies on these generic kernel/user hooks.
    
    There are some upsides of doing this:
    
    - This requires no arch code to implement CONFIG_VIRT_CPU_ACCOUNTING
    if context tracking is already built (already necessary for RCU in full
    tickless mode).
    
    - We can rely on the generic context tracking subsystem to dynamically
    (de)activate the hooks, so that we can switch anytime between virtual
    and tick based accounting. This way we don't have the overhead
    of the virtual accounting when the tick is running periodically.
    
    And one downside:
    
    - There is probably more overhead than a native virtual based cputime
    accounting. But this relies on hooks that are already set anyway.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index 54f471e536dc..9002e92e6372 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -30,8 +30,9 @@ void user_enter(void)
 	local_irq_save(flags);
 	if (__this_cpu_read(context_tracking.active) &&
 	    __this_cpu_read(context_tracking.state) != IN_USER) {
-		__this_cpu_write(context_tracking.state, IN_USER);
+		vtime_user_enter(current);
 		rcu_user_enter();
+		__this_cpu_write(context_tracking.state, IN_USER);
 	}
 	local_irq_restore(flags);
 }
@@ -53,8 +54,9 @@ void user_exit(void)
 
 	local_irq_save(flags);
 	if (__this_cpu_read(context_tracking.state) == IN_USER) {
-		__this_cpu_write(context_tracking.state, IN_KERNEL);
 		rcu_user_exit();
+		vtime_user_exit(current);
+		__this_cpu_write(context_tracking.state, IN_KERNEL);
 	}
 	local_irq_restore(flags);
 }

commit 4eacdf18374e5d7d21a728b46dfec269ac8ef55c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jan 16 17:16:37 2013 +0100

    context_tracking: Add comments on interface and internals
    
    This subsystem lacks many explanations on its purpose and
    design. Add these missing comments.
    
    v4: Document function parameter to be more kernel-doc
    friendly, as per Namhyung suggestion.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Alessio Igor Bogani <abogani@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index e0e07fd55508..d566aba7e801 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -1,3 +1,19 @@
+/*
+ * Context tracking: Probe on high level context boundaries such as kernel
+ * and userspace. This includes syscalls and exceptions entry/exit.
+ *
+ * This is used by RCU to remove its dependency on the timer tick while a CPU
+ * runs in userspace.
+ *
+ *  Started by Frederic Weisbecker:
+ *
+ * Copyright (C) 2012 Red Hat, Inc., Frederic Weisbecker <fweisbec@redhat.com>
+ *
+ * Many thanks to Gilad Ben-Yossef, Paul McKenney, Ingo Molnar, Andrew Morton,
+ * Steven Rostedt, Peter Zijlstra for suggestions and improvements.
+ *
+ */
+
 #include <linux/context_tracking.h>
 #include <linux/rcupdate.h>
 #include <linux/sched.h>
@@ -6,8 +22,8 @@
 
 struct context_tracking {
 	/*
-	 * When active is false, hooks are not set to
-	 * minimize overhead: TIF flags are cleared
+	 * When active is false, probes are unset in order
+	 * to minimize overhead: TIF flags are cleared
 	 * and calls to user_enter/exit are ignored. This
 	 * may be further optimized using static keys.
 	 */
@@ -24,6 +40,15 @@ static DEFINE_PER_CPU(struct context_tracking, context_tracking) = {
 #endif
 };
 
+/**
+ * user_enter - Inform the context tracking that the CPU is going to
+ *              enter userspace mode.
+ *
+ * This function must be called right before we switch from the kernel
+ * to userspace, when it's guaranteed the remaining kernel instructions
+ * to execute won't use any RCU read side critical section because this
+ * function sets RCU in extended quiescent state.
+ */
 void user_enter(void)
 {
 	unsigned long flags;
@@ -39,40 +64,70 @@ void user_enter(void)
 	if (in_interrupt())
 		return;
 
+	/* Kernel threads aren't supposed to go to userspace */
 	WARN_ON_ONCE(!current->mm);
 
 	local_irq_save(flags);
 	if (__this_cpu_read(context_tracking.active) &&
 	    __this_cpu_read(context_tracking.state) != IN_USER) {
 		__this_cpu_write(context_tracking.state, IN_USER);
+		/*
+		 * At this stage, only low level arch entry code remains and
+		 * then we'll run in userspace. We can assume there won't be
+		 * any RCU read-side critical section until the next call to
+		 * user_exit() or rcu_irq_enter(). Let's remove RCU's dependency
+		 * on the tick.
+		 */
 		rcu_user_enter();
 	}
 	local_irq_restore(flags);
 }
 
+
+/**
+ * user_exit - Inform the context tracking that the CPU is
+ *             exiting userspace mode and entering the kernel.
+ *
+ * This function must be called after we entered the kernel from userspace
+ * before any use of RCU read side critical section. This potentially include
+ * any high level kernel code like syscalls, exceptions, signal handling, etc...
+ *
+ * This call supports re-entrancy. This way it can be called from any exception
+ * handler without needing to know if we came from userspace or not.
+ */
 void user_exit(void)
 {
 	unsigned long flags;
 
-	/*
-	 * Some contexts may involve an exception occuring in an irq,
-	 * leading to that nesting:
-	 * rcu_irq_enter() rcu_user_exit() rcu_user_exit() rcu_irq_exit()
-	 * This would mess up the dyntick_nesting count though. And rcu_irq_*()
-	 * helpers are enough to protect RCU uses inside the exception. So
-	 * just return immediately if we detect we are in an IRQ.
-	 */
 	if (in_interrupt())
 		return;
 
 	local_irq_save(flags);
 	if (__this_cpu_read(context_tracking.state) == IN_USER) {
 		__this_cpu_write(context_tracking.state, IN_KERNEL);
+		/*
+		 * We are going to run code that may use RCU. Inform
+		 * RCU core about that (ie: we may need the tick again).
+		 */
 		rcu_user_exit();
 	}
 	local_irq_restore(flags);
 }
 
+
+/**
+ * context_tracking_task_switch - context switch the syscall callbacks
+ * @prev: the task that is being switched out
+ * @next: the task that is being switched in
+ *
+ * The context tracking uses the syscall slow path to implement its user-kernel
+ * boundaries probes on syscalls. This way it doesn't impact the syscall fast
+ * path on CPUs that don't do context tracking.
+ *
+ * But we need to clear the flag on the previous task because it may later
+ * migrate to some CPU that doesn't do the context tracking. As such the TIF
+ * flag may not be desired there.
+ */
 void context_tracking_task_switch(struct task_struct *prev,
 			     struct task_struct *next)
 {

commit 95a79fd458b85132c25e351d45037ec9643312b2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Jan 7 18:12:14 2013 +0100

    context_tracking: Export context state for generic vtime
    
    Export the context state: whether we run in user / kernel
    from the context tracking subsystem point of view.
    
    This is going to be used by the generic virtual cputime
    accounting subsystem that is needed to implement the full
    dynticks.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
index e0e07fd55508..54f471e536dc 100644
--- a/kernel/context_tracking.c
+++ b/kernel/context_tracking.c
@@ -1,24 +1,10 @@
 #include <linux/context_tracking.h>
 #include <linux/rcupdate.h>
 #include <linux/sched.h>
-#include <linux/percpu.h>
 #include <linux/hardirq.h>
 
-struct context_tracking {
-	/*
-	 * When active is false, hooks are not set to
-	 * minimize overhead: TIF flags are cleared
-	 * and calls to user_enter/exit are ignored. This
-	 * may be further optimized using static keys.
-	 */
-	bool active;
-	enum {
-		IN_KERNEL = 0,
-		IN_USER,
-	} state;
-};
 
-static DEFINE_PER_CPU(struct context_tracking, context_tracking) = {
+DEFINE_PER_CPU(struct context_tracking, context_tracking) = {
 #ifdef CONFIG_CONTEXT_TRACKING_FORCE
 	.active = true,
 #endif

commit 91d1aa43d30505b0b825db8898ffc80a8eca96c7
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 27 19:33:25 2012 +0100

    context_tracking: New context tracking susbsystem
    
    Create a new subsystem that probes on kernel boundaries
    to keep track of the transitions between level contexts
    with two basic initial contexts: user or kernel.
    
    This is an abstraction of some RCU code that use such tracking
    to implement its userspace extended quiescent state.
    
    We need to pull this up from RCU into this new level of indirection
    because this tracking is also going to be used to implement an "on
    demand" generic virtual cputime accounting. A necessary step to
    shutdown the tick while still accounting the cputime.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Gilad Ben-Yossef <gilad@benyossef.com>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    [ paulmck: fix whitespace error and email address. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c
new file mode 100644
index 000000000000..e0e07fd55508
--- /dev/null
+++ b/kernel/context_tracking.c
@@ -0,0 +1,83 @@
+#include <linux/context_tracking.h>
+#include <linux/rcupdate.h>
+#include <linux/sched.h>
+#include <linux/percpu.h>
+#include <linux/hardirq.h>
+
+struct context_tracking {
+	/*
+	 * When active is false, hooks are not set to
+	 * minimize overhead: TIF flags are cleared
+	 * and calls to user_enter/exit are ignored. This
+	 * may be further optimized using static keys.
+	 */
+	bool active;
+	enum {
+		IN_KERNEL = 0,
+		IN_USER,
+	} state;
+};
+
+static DEFINE_PER_CPU(struct context_tracking, context_tracking) = {
+#ifdef CONFIG_CONTEXT_TRACKING_FORCE
+	.active = true,
+#endif
+};
+
+void user_enter(void)
+{
+	unsigned long flags;
+
+	/*
+	 * Some contexts may involve an exception occuring in an irq,
+	 * leading to that nesting:
+	 * rcu_irq_enter() rcu_user_exit() rcu_user_exit() rcu_irq_exit()
+	 * This would mess up the dyntick_nesting count though. And rcu_irq_*()
+	 * helpers are enough to protect RCU uses inside the exception. So
+	 * just return immediately if we detect we are in an IRQ.
+	 */
+	if (in_interrupt())
+		return;
+
+	WARN_ON_ONCE(!current->mm);
+
+	local_irq_save(flags);
+	if (__this_cpu_read(context_tracking.active) &&
+	    __this_cpu_read(context_tracking.state) != IN_USER) {
+		__this_cpu_write(context_tracking.state, IN_USER);
+		rcu_user_enter();
+	}
+	local_irq_restore(flags);
+}
+
+void user_exit(void)
+{
+	unsigned long flags;
+
+	/*
+	 * Some contexts may involve an exception occuring in an irq,
+	 * leading to that nesting:
+	 * rcu_irq_enter() rcu_user_exit() rcu_user_exit() rcu_irq_exit()
+	 * This would mess up the dyntick_nesting count though. And rcu_irq_*()
+	 * helpers are enough to protect RCU uses inside the exception. So
+	 * just return immediately if we detect we are in an IRQ.
+	 */
+	if (in_interrupt())
+		return;
+
+	local_irq_save(flags);
+	if (__this_cpu_read(context_tracking.state) == IN_USER) {
+		__this_cpu_write(context_tracking.state, IN_KERNEL);
+		rcu_user_exit();
+	}
+	local_irq_restore(flags);
+}
+
+void context_tracking_task_switch(struct task_struct *prev,
+			     struct task_struct *next)
+{
+	if (__this_cpu_read(context_tracking.active)) {
+		clear_tsk_thread_flag(prev, TIF_NOHZ);
+		set_tsk_thread_flag(next, TIF_NOHZ);
+	}
+}
