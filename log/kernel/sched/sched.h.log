commit 739f70b476cf05c5a424b42a8b5728914345610c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jun 22 12:01:24 2020 +0200

    sched/core: s/WF_ON_RQ/WQ_ON_CPU/
    
    Use a better name for this poorly named flag, to avoid confusion...
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Link: https://lkml.kernel.org/r/20200622100825.785115830@infradead.org

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1d4e94c1e5fe..877fb08eb1b0 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1682,7 +1682,7 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 #define WF_SYNC			0x01		/* Waker goes to sleep after wakeup */
 #define WF_FORK			0x02		/* Child wakeup after fork */
 #define WF_MIGRATED		0x04		/* Internal use, task got migrated */
-#define WF_ON_RQ		0x08		/* Wakee is on_rq */
+#define WF_ON_CPU		0x08		/* Wakee is on_cpu */
 
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution

commit a148866489fbe243c936fe43e4525d8dbfa0318f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:04 2020 +0200

    sched: Replace rq::wake_list
    
    The recent commit: 90b5363acd47 ("sched: Clean up scheduler_ipi()")
    got smp_call_function_single_async() subtly wrong. Even though it will
    return -EBUSY when trying to re-use a csd, that condition is not
    atomic and still requires external serialization.
    
    The change in ttwu_queue_remote() got this wrong.
    
    While on first reading ttwu_queue_remote() has an atomic test-and-set
    that appears to serialize the use, the matching 'release' is not in
    the right place to actually guarantee this serialization.
    
    The actual race is vs the sched_ttwu_pending() call in the idle loop;
    that can run the wakeup-list without consuming the CSD.
    
    Instead of trying to chain the lists, merge them.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161908.129371594@infradead.org

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c86fc94c54e5..1d4e94c1e5fe 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1023,11 +1023,6 @@ struct rq {
 	unsigned int		ttwu_local;
 #endif
 
-#ifdef CONFIG_SMP
-	call_single_data_t	wake_csd;
-	struct llist_head	wake_list;
-#endif
-
 #ifdef CONFIG_CPU_IDLE
 	/* Must be inspected within a rcu lock section */
 	struct cpuidle_state	*idle_state;
@@ -1371,8 +1366,6 @@ queue_balance_callback(struct rq *rq,
 	rq->balance_callback = head;
 }
 
-extern void sched_ttwu_pending(void);
-
 #define rcu_dereference_check_sched_domain(p) \
 	rcu_dereference_check((p), \
 			      lockdep_is_held(&sched_domains_mutex))
@@ -1512,7 +1505,6 @@ extern void flush_smp_call_function_from_idle(void);
 
 #else /* !CONFIG_SMP: */
 static inline void flush_smp_call_function_from_idle(void) { }
-static inline void sched_ttwu_pending(void) { }
 #endif
 
 #include "stats.h"

commit 126c2092e5c8b28623cb890cd2930aa292410676
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:03 2020 +0200

    sched: Add rq::ttwu_pending
    
    In preparation of removing rq->wake_list, replace the
    !list_empty(rq->wake_list) with rq->ttwu_pending. This is not fully
    equivalent as this new variable is racy.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161908.070399698@infradead.org

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 75b062999c43..c86fc94c54e5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -895,7 +895,9 @@ struct rq {
 	atomic_t		nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 
-	unsigned long		nr_load_updates;
+#ifdef CONFIG_SMP
+	unsigned int		ttwu_pending;
+#endif
 	u64			nr_switches;
 
 #ifdef CONFIG_UCLAMP_TASK

commit b2a02fc43a1f40ef4eb2fb2b06357382608d4d84
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:01 2020 +0200

    smp: Optimize send_call_function_single_ipi()
    
    Just like the ttwu_queue_remote() IPI, make use of _TIF_POLLING_NRFLAG
    to avoid sending IPIs to idle CPUs.
    
    [ mingo: Fix UP build bug. ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161907.953304789@infradead.org

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3c163cb5493f..75b062999c43 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1506,11 +1506,12 @@ static inline void unregister_sched_domain_sysctl(void)
 }
 #endif
 
-#else
+extern void flush_smp_call_function_from_idle(void);
 
+#else /* !CONFIG_SMP: */
+static inline void flush_smp_call_function_from_idle(void) { }
 static inline void sched_ttwu_pending(void) { }
-
-#endif /* CONFIG_SMP */
+#endif
 
 #include "stats.h"
 #include "autogroup.h"

commit 19a1f5ec699954d21be10f74ff71c2a7079e99ad
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:10:58 2020 +0200

    sched: Fix smp_call_function_single_async() usage for ILB
    
    The recent commit: 90b5363acd47 ("sched: Clean up scheduler_ipi()")
    got smp_call_function_single_async() subtly wrong. Even though it will
    return -EBUSY when trying to re-use a csd, that condition is not
    atomic and still requires external serialization.
    
    The change in kick_ilb() got this wrong.
    
    While on first reading kick_ilb() has an atomic test-and-set that
    appears to serialize the use, the matching 'release' is not in the
    right place to actually guarantee this serialization.
    
    Rework the nohz_idle_balance() trigger so that the release is in the
    IPI callback and thus guarantees the required serialization for the
    CSD.
    
    Fixes: 90b5363acd47 ("sched: Clean up scheduler_ipi()")
    Reported-by: Qian Cai <cai@lca.pw>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Cc: mgorman@techsingularity.net
    Link: https://lore.kernel.org/r/20200526161907.778543557@infradead.org

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4b32cff0dcbe..3c163cb5493f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -951,6 +951,7 @@ struct rq {
 
 	struct callback_head	*balance_callback;
 
+	unsigned char		nohz_idle_balance;
 	unsigned char		idle_balance;
 
 	unsigned long		misfit_task_load;

commit 2ebb17717550607bcd85fb8cf7d24ac870e9d762
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Sun May 24 21:29:56 2020 +0100

    sched/core: Offload wakee task activation if it the wakee is descheduling
    
    The previous commit:
    
      c6e7bd7afaeb: ("sched/core: Optimize ttwu() spinning on p->on_cpu")
    
    avoids spinning on p->on_rq when the task is descheduling, but only if the
    wakee is on a CPU that does not share cache with the waker.
    
    This patch offloads the activation of the wakee to the CPU that is about to
    go idle if the task is the only one on the runqueue. This potentially allows
    the waker task to continue making progress when the wakeup is not strictly
    synchronous.
    
    This is very obvious with netperf UDP_STREAM running on localhost. The
    waker is sending packets as quickly as possible without waiting for any
    reply. It frequently wakes the server for the processing of packets and
    when netserver is using local memory, it quickly completes the processing
    and goes back to idle. The waker often observes that netserver is on_rq
    and spins excessively leading to a drop in throughput.
    
    This is a comparison of 5.7-rc6 against "sched: Optimize ttwu() spinning
    on p->on_cpu" and against this patch labeled vanilla, optttwu-v1r1 and
    localwakelist-v1r2 respectively.
    
                                      5.7.0-rc6              5.7.0-rc6              5.7.0-rc6
                                        vanilla           optttwu-v1r1     localwakelist-v1r2
    Hmean     send-64         251.49 (   0.00%)      258.05 *   2.61%*      305.59 *  21.51%*
    Hmean     send-128        497.86 (   0.00%)      519.89 *   4.43%*      600.25 *  20.57%*
    Hmean     send-256        944.90 (   0.00%)      997.45 *   5.56%*     1140.19 *  20.67%*
    Hmean     send-1024      3779.03 (   0.00%)     3859.18 *   2.12%*     4518.19 *  19.56%*
    Hmean     send-2048      7030.81 (   0.00%)     7315.99 *   4.06%*     8683.01 *  23.50%*
    Hmean     send-3312     10847.44 (   0.00%)    11149.43 *   2.78%*    12896.71 *  18.89%*
    Hmean     send-4096     13436.19 (   0.00%)    13614.09 (   1.32%)    15041.09 *  11.94%*
    Hmean     send-8192     22624.49 (   0.00%)    23265.32 *   2.83%*    24534.96 *   8.44%*
    Hmean     send-16384    34441.87 (   0.00%)    36457.15 *   5.85%*    35986.21 *   4.48%*
    
    Note that this benefit is not universal to all wakeups, it only applies
    to the case where the waker often spins on p->on_rq.
    
    The impact can be seen from a "perf sched latency" report generated from
    a single iteration of one packet size:
    
       -----------------------------------------------------------------------------------------------------------------
        Task                  |   Runtime ms  | Switches | Average delay ms | Maximum delay ms | Maximum delay at       |
       -----------------------------------------------------------------------------------------------------------------
    
      vanilla
        netperf:4337          |  21709.193 ms |     2932 | avg:    0.002 ms | max:    0.041 ms | max at:    112.154512 s
        netserver:4338        |  14629.459 ms |  5146990 | avg:    0.001 ms | max: 1615.864 ms | max at:    140.134496 s
    
      localwakelist-v1r2
        netperf:4339          |  29789.717 ms |     2460 | avg:    0.002 ms | max:    0.059 ms | max at:    138.205389 s
        netserver:4340        |  18858.767 ms |  7279005 | avg:    0.001 ms | max:    0.362 ms | max at:    135.709683 s
       -----------------------------------------------------------------------------------------------------------------
    
    Note that the average wakeup delay is quite small on both the vanilla
    kernel and with the two patches applied. However, there are significant
    outliers with the vanilla kernel with the maximum one measured as 1615
    milliseconds with a vanilla kernel but never worse than 0.362 ms with
    both patches applied and a much higher rate of context switching.
    
    Similarly a separate profile of cycles showed that 2.83% of all cycles
    were spent in try_to_wake_up() with almost half of the cycles spent
    on spinning on p->on_rq. With the two patches, the percentage of cycles
    spent in try_to_wake_up() drops to 1.13%
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Jirka Hladky <jhladky@redhat.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: valentin.schneider@arm.com
    Cc: Hillf Danton <hdanton@sina.com>
    Cc: Rik van Riel <riel@surriel.com>
    Link: https://lore.kernel.org/r/20200524202956.27665-3-mgorman@techsingularity.net

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f7ab6334e992..4b32cff0dcbe 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1685,7 +1685,8 @@ static inline int task_on_rq_migrating(struct task_struct *p)
  */
 #define WF_SYNC			0x01		/* Waker goes to sleep after wakeup */
 #define WF_FORK			0x02		/* Child wakeup after fork */
-#define WF_MIGRATED		0x4		/* Internal use, task got migrated */
+#define WF_MIGRATED		0x04		/* Internal use, task got migrated */
+#define WF_ON_RQ		0x08		/* Wakee is on_rq */
 
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution

commit d505b8af58912ae1e1a211fabc9995b19bd40828
Author: Huaixin Chang <changhuaixin@linux.alibaba.com>
Date:   Sat Apr 25 18:52:48 2020 +0800

    sched: Defend cfs and rt bandwidth quota against overflow
    
    When users write some huge number into cpu.cfs_quota_us or
    cpu.rt_runtime_us, overflow might happen during to_ratio() shifts of
    schedulable checks.
    
    to_ratio() could be altered to avoid unnecessary internal overflow, but
    min_cfs_quota_period is less than 1 << BW_SHIFT, so a cutoff would still
    be needed. Set a cap MAX_BW for cfs_quota_us and rt_runtime_us to
    prevent overflow.
    
    Signed-off-by: Huaixin Chang <changhuaixin@linux.alibaba.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Link: https://lkml.kernel.org/r/20200425105248.60093-1-changhuaixin@linux.alibaba.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2bd2a222318a..f7ab6334e992 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1915,6 +1915,8 @@ extern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);
 #define BW_SHIFT		20
 #define BW_UNIT			(1 << BW_SHIFT)
 #define RATIO_SHIFT		8
+#define MAX_BW_BITS		(64 - BW_SHIFT)
+#define MAX_BW			((1ULL << MAX_BW_BITS) - 1)
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);

commit 04f5c362ec6d3ff0e14f1c05230b550da7f528a4
Author: Gustavo A. R. Silva <gustavoars@kernel.org>
Date:   Thu May 7 14:21:41 2020 -0500

    sched/fair: Replace zero-length array with flexible-array
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    sizeof(flexible-array-member) triggers a warning because flexible array
    members have incomplete type[1]. There are some instances of code in
    which the sizeof operator is being incorrectly/erroneously applied to
    zero-length arrays and the result is zero. Such instances may be hiding
    some bugs. So, this work (flexible-array member conversions) will also
    help to get completely rid of those sorts of issues.
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Signed-off-by: Gustavo A. R. Silva <gustavoars@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200507192141.GA16183@embeddedor

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 21416b30c520..2bd2a222318a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1462,7 +1462,7 @@ struct sched_group {
 	 * by attaching extra space to the end of the structure,
 	 * depending on how many CPUs the kernel has booted up with)
 	 */
-	unsigned long		cpumask[0];
+	unsigned long		cpumask[];
 };
 
 static inline struct cpumask *sched_group_span(struct sched_group *sg)

commit 90b5363acd4739769c3f38c1aff16171bd133e8c
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Fri Mar 27 11:44:56 2020 +0100

    sched: Clean up scheduler_ipi()
    
    The scheduler IPI has grown weird and wonderful over the years, time
    for spring cleaning.
    
    Move all the non-trivial stuff out of it and into a regular smp function
    call IPI. This then reduces the schedule_ipi() to most of it's former NOP
    glory and ensures to keep the interrupt vector lean and mean.
    
    Aside of that avoiding the full irq_enter() in the x86 IPI implementation
    is incorrect as scheduler_ipi() can be instrumented. To work around that
    scheduler_ipi() had an irq_enter/exit() hack when heavy work was
    pending. This is gone now.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Alexandre Chartre <alexandre.chartre@oracle.com>
    Link: https://lkml.kernel.org/r/20200505134058.361859938@linutronix.de

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 978c6fac8cb8..21416b30c520 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -889,9 +889,10 @@ struct rq {
 #ifdef CONFIG_SMP
 	unsigned long		last_blocked_load_update_tick;
 	unsigned int		has_blocked_load;
+	call_single_data_t	nohz_csd;
 #endif /* CONFIG_SMP */
 	unsigned int		nohz_tick_stopped;
-	atomic_t nohz_flags;
+	atomic_t		nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 
 	unsigned long		nr_load_updates;
@@ -978,7 +979,7 @@ struct rq {
 
 	/* This is used to determine avg_idle's max value */
 	u64			max_idle_balance_cost;
-#endif
+#endif /* CONFIG_SMP */
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 	u64			prev_irq_time;
@@ -1020,6 +1021,7 @@ struct rq {
 #endif
 
 #ifdef CONFIG_SMP
+	call_single_data_t	wake_csd;
 	struct llist_head	wake_list;
 #endif
 

commit d91cecc156620ec75d94c55369509c807c3d07e6
Author: Chen Yu <yu.c.chen@intel.com>
Date:   Tue Apr 21 18:50:34 2020 +0800

    sched: Make newidle_balance() static again
    
    After Commit 6e2df0581f56 ("sched: Fix pick_next_task() vs 'change'
    pattern race"), there is no need to expose newidle_balance() as it
    is only used within fair.c file. Change this function back to static again.
    
    No functional change.
    
    Reported-by: kbuild test robot <lkp@intel.com>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Chen Yu <yu.c.chen@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/83cd3030b031ca5d646cd5e225be10e7a0fdd8f5.1587464698.git.yu.c.chen@intel.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7198683b2869..978c6fac8cb8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1503,14 +1503,10 @@ static inline void unregister_sched_domain_sysctl(void)
 }
 #endif
 
-extern int newidle_balance(struct rq *this_rq, struct rq_flags *rf);
-
 #else
 
 static inline void sched_ttwu_pending(void) { }
 
-static inline int newidle_balance(struct rq *this_rq, struct rq_flags *rf) { return 0; }
-
 #endif /* CONFIG_SMP */
 
 #include "stats.h"

commit ab93a4bc955b3980c699430bc0b633f0d8b607be
Author: Josh Don <joshdon@google.com>
Date:   Fri Apr 10 15:52:08 2020 -0700

    sched/fair: Remove distribute_running from CFS bandwidth
    
    This is mostly a revert of commit:
    
      baa9be4ffb55 ("sched/fair: Fix throttle_list starvation with low CFS quota")
    
    The primary use of distribute_running was to determine whether to add
    throttled entities to the head or the tail of the throttled list. Now
    that we always add to the tail, we can remove this field.
    
    The other use of distribute_running is in the slack_timer, so that we
    don't start a distribution while one is already running. However, even
    in the event that this race occurs, it is fine to have two distributions
    running (especially now that distribute grabs the cfs_b->lock to
    determine remaining quota before assigning).
    
    Signed-off-by: Josh Don <joshdon@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Phil Auld <pauld@redhat.com>
    Tested-by: Phil Auld <pauld@redhat.com>
    Link: https://lkml.kernel.org/r/20200410225208.109717-3-joshdon@google.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index db3a57675ccf..7198683b2869 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -349,7 +349,6 @@ struct cfs_bandwidth {
 
 	u8			idle;
 	u8			period_active;
-	u8			distribute_running;
 	u8			slack_started;
 	struct hrtimer		period_timer;
 	struct hrtimer		slack_timer;

commit 275b2f6723ab9173484e1055ae138d4c2dd9d7c5
Author: Vincent Donnefort <vincent.donnefort@arm.com>
Date:   Fri Mar 20 13:21:35 2020 +0000

    sched/core: Remove unused rq::last_load_update_tick
    
    The following commit:
    
      5e83eafbfd3b ("sched/fair: Remove the rq->cpu_load[] update code")
    
    eliminated the last use case for rq->last_load_update_tick, so remove
    the field as well.
    
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Vincent Donnefort <vincent.donnefort@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/1584710495-308969-1-git-send-email-vincent.donnefort@arm.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index cd008147eccb..db3a57675ccf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -888,7 +888,6 @@ struct rq {
 #endif
 #ifdef CONFIG_NO_HZ_COMMON
 #ifdef CONFIG_SMP
-	unsigned long		last_load_update_tick;
 	unsigned long		last_blocked_load_update_tick;
 	unsigned int		has_blocked_load;
 #endif /* CONFIG_SMP */

commit d76343c6b2b79f5e89c392bc9ce9dabc4c9e90cb
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Mon Mar 30 10:01:27 2020 +0100

    sched/fair: Align rq->avg_idle and rq->avg_scan_cost
    
    sched/core.c uses update_avg() for rq->avg_idle and sched/fair.c uses an
    open-coded version (with the exact same decay factor) for
    rq->avg_scan_cost. On top of that, select_idle_cpu() expects to be able to
    compare these two fields.
    
    The only difference between the two is that rq->avg_scan_cost is computed
    using a pure division rather than a shift. Turns out it actually matters,
    first of all because the shifted value can be negative, and the standard
    has this to say about it:
    
      """
      The result of E1 >> E2 is E1 right-shifted E2 bit positions. [...] If E1
      has a signed type and a negative value, the resulting value is
      implementation-defined.
      """
    
    Not only this, but (arithmetic) right shifting a negative value (using 2's
    complement) is *not* equivalent to dividing it by the corresponding power
    of 2. Let's look at a few examples:
    
      -4      -> 0xF..FC
      -4 >> 3 -> 0xF..FF == -1 != -4 / 8
    
      -8      -> 0xF..F8
      -8 >> 3 -> 0xF..FF == -1 == -8 / 8
    
      -9      -> 0xF..F7
      -9 >> 3 -> 0xF..FE == -2 != -9 / 8
    
    Make update_avg() use a division, and export it to the private scheduler
    header to reuse it where relevant. Note that this still lets compilers use
    a shift here, but should prevent any unwanted surprise. The disassembly of
    select_idle_cpu() remains unchanged on arm64, and ttwu_do_wakeup() gains 2
    instructions; the diff sort of looks like this:
    
      - sub x1, x1, x0
      + subs x1, x1, x0 // set condition codes
      + add x0, x1, #0x7
      + csel x0, x0, x1, mi // x0 = x1 < 0 ? x0 : x1
        add x0, x3, x0, asr #3
    
    which does the right thing (i.e. gives us the expected result while still
    using an arithmetic shift)
    
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200330090127.16294-1-valentin.schneider@arm.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0f616bf7bce3..cd008147eccb 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -195,6 +195,12 @@ static inline int task_has_dl_policy(struct task_struct *p)
 
 #define cap_scale(v, s) ((v)*(s) >> SCHED_CAPACITY_SHIFT)
 
+static inline void update_avg(u64 *avg, u64 sample)
+{
+	s64 diff = sample - *avg;
+	*avg += diff / 8;
+}
+
 /*
  * !! For sched_setattr_nocheck() (kernel) only !!
  *

commit 992a1a3b45b5c0b6e69ecc2a3f32b0d02da28d58
Merge: 2d385336afcc e98eac6ff1b4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 18:06:39 2020 -0700

    Merge tag 'smp-core-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull core SMP updates from Thomas Gleixner:
     "CPU (hotplug) updates:
    
       - Support for locked CSD objects in smp_call_function_single_async()
         which allows to simplify callsites in the scheduler core and MIPS
    
       - Treewide consolidation of CPU hotplug functions which ensures the
         consistency between the sysfs interface and kernel state. The low
         level functions cpu_up/down() are now confined to the core code and
         not longer accessible from random code"
    
    * tag 'smp-core-2020-03-30' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (22 commits)
      cpu/hotplug: Ignore pm_wakeup_pending() for disable_nonboot_cpus()
      cpu/hotplug: Hide cpu_up/down()
      cpu/hotplug: Move bringup of secondary CPUs out of smp_init()
      torture: Replace cpu_up/down() with add/remove_cpu()
      firmware: psci: Replace cpu_up/down() with add/remove_cpu()
      xen/cpuhotplug: Replace cpu_up/down() with device_online/offline()
      parisc: Replace cpu_up/down() with add/remove_cpu()
      sparc: Replace cpu_up/down() with add/remove_cpu()
      powerpc: Replace cpu_up/down() with add/remove_cpu()
      x86/smp: Replace cpu_up/down() with add/remove_cpu()
      arm64: hibernate: Use bringup_hibernate_cpu()
      cpu/hotplug: Provide bringup_hibernate_cpu()
      arm64: Use reboot_cpu instead of hardconding it to 0
      arm64: Don't use disable_nonboot_cpus()
      ARM: Use reboot_cpu instead of hardcoding it to 0
      ARM: Don't use disable_nonboot_cpus()
      ia64: Replace cpu_down() with smp_shutdown_nonboot_cpus()
      cpu/hotplug: Create a new function to shutdown nonboot cpus
      cpu/hotplug: Add new {add,remove}_cpu() functions
      sched/core: Remove rq.hrtick_csd_pending
      ...

commit 642e53ead6aea8740a219ede509a5d138fd4f780
Merge: 9b82f05f869a 313f16e2e35a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 30 17:01:51 2020 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle are:
    
       - Various NUMA scheduling updates: harmonize the load-balancer and
         NUMA placement logic to not work against each other. The intended
         result is better locality, better utilization and fewer migrations.
    
       - Introduce Thermal Pressure tracking and optimizations, to improve
         task placement on thermally overloaded systems.
    
       - Implement frequency invariant scheduler accounting on (some) x86
         CPUs. This is done by observing and sampling the 'recent' CPU
         frequency average at ~tick boundaries. The CPU provides this data
         via the APERF/MPERF MSRs. This hopefully makes our capacity
         estimates more precise and keeps tasks on the same CPU better even
         if it might seem overloaded at a lower momentary frequency. (As
         usual, turbo mode is a complication that we resolve by observing
         the maximum frequency and renormalizing to it.)
    
       - Add asymmetric CPU capacity wakeup scan to improve capacity
         utilization on asymmetric topologies. (big.LITTLE systems)
    
       - PSI fixes and optimizations.
    
       - RT scheduling capacity awareness fixes & improvements.
    
       - Optimize the CONFIG_RT_GROUP_SCHED constraints code.
    
       - Misc fixes, cleanups and optimizations - see the changelog for
         details"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (62 commits)
      threads: Update PID limit comment according to futex UAPI change
      sched/fair: Fix condition of avg_load calculation
      sched/rt: cpupri_find: Trigger a full search as fallback
      kthread: Do not preempt current task if it is going to call schedule()
      sched/fair: Improve spreading of utilization
      sched: Avoid scale real weight down to zero
      psi: Move PF_MEMSTALL out of task->flags
      MAINTAINERS: Add maintenance information for psi
      psi: Optimize switching tasks inside shared cgroups
      psi: Fix cpu.pressure for cpu.max and competing cgroups
      sched/core: Distribute tasks within affinity masks
      sched/fair: Fix enqueue_task_fair warning
      thermal/cpu-cooling, sched/core: Move the arch_set_thermal_pressure() API to generic scheduler code
      sched/rt: Remove unnecessary push for unfit tasks
      sched/rt: Allow pulling unfitting task
      sched/rt: Optimize cpupri_find() on non-heterogenous systems
      sched/rt: Re-instate old behavior in select_task_rq_rt()
      sched/rt: cpupri_find: Implement fallback mechanism for !fit case
      sched/fair: Fix reordering of enqueue/dequeue_task_fair()
      sched/fair: Fix runnable_avg for throttled cfs
      ...

commit b3212fe2bc06fa1014b3063b85b2bac4332a1c28
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 21 12:25:59 2020 +0100

    sched/swait: Prepare usage in completions
    
    As a preparation to use simple wait queues for completions:
    
      - Provide swake_up_all_locked() to support complete_all()
      - Make __prepare_to_swait() public available
    
    This is done to enable the usage of complete() within truly atomic contexts
    on a PREEMPT_RT enabled kernel.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200321113242.228481202@linutronix.de

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9ea647835fd6..fdc77e796324 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2492,3 +2492,6 @@ static inline bool is_per_cpu_kthread(struct task_struct *p)
 	return true;
 }
 #endif
+
+void swake_up_all_locked(struct swait_queue_head *q);
+void __prepare_to_swait(struct swait_queue_head *q, struct swait_queue *wait);

commit 26cf52229efc87e2effa9d788f9b33c40fb3358a
Author: Michael Wang <yun.wang@linux.alibaba.com>
Date:   Wed Mar 18 10:15:15 2020 +0800

    sched: Avoid scale real weight down to zero
    
    During our testing, we found a case that shares no longer
    working correctly, the cgroup topology is like:
    
      /sys/fs/cgroup/cpu/A          (shares=102400)
      /sys/fs/cgroup/cpu/A/B        (shares=2)
      /sys/fs/cgroup/cpu/A/B/C      (shares=1024)
    
      /sys/fs/cgroup/cpu/D          (shares=1024)
      /sys/fs/cgroup/cpu/D/E        (shares=1024)
      /sys/fs/cgroup/cpu/D/E/F      (shares=1024)
    
    The same benchmark is running in group C & F, no other tasks are
    running, the benchmark is capable to consumed all the CPUs.
    
    We suppose the group C will win more CPU resources since it could
    enjoy all the shares of group A, but it's F who wins much more.
    
    The reason is because we have group B with shares as 2, since
    A->cfs_rq.load.weight == B->se.load.weight == B->shares/nr_cpus,
    so A->cfs_rq.load.weight become very small.
    
    And in calc_group_shares() we calculate shares as:
    
      load = max(scale_load_down(cfs_rq->load.weight), cfs_rq->avg.load_avg);
      shares = (tg_shares * load) / tg_weight;
    
    Since the 'cfs_rq->load.weight' is too small, the load become 0
    after scale down, although 'tg_shares' is 102400, shares of the se
    which stand for group A on root cfs_rq become 2.
    
    While the se of D on root cfs_rq is far more bigger than 2, so it
    wins the battle.
    
    Thus when scale_load_down() scale real weight down to 0, it's no
    longer telling the real story, the caller will have the wrong
    information and the calculation will be buggy.
    
    This patch add check in scale_load_down(), so the real weight will
    be >= MIN_SHARES after scale, after applied the group C wins as
    expected.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Michael Wang <yun.wang@linux.alibaba.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
    Link: https://lkml.kernel.org/r/38e8e212-59a1-64b2-b247-b6d0b52d8dc1@linux.alibaba.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9e173fad0425..1e72d1b3d3ce 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -118,7 +118,13 @@ extern long calc_load_fold_active(struct rq *this_rq, long adjust);
 #ifdef CONFIG_64BIT
 # define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)
 # define scale_load(w)		((w) << SCHED_FIXEDPOINT_SHIFT)
-# define scale_load_down(w)	((w) >> SCHED_FIXEDPOINT_SHIFT)
+# define scale_load_down(w) \
+({ \
+	unsigned long __w = (w); \
+	if (__w) \
+		__w = max(2UL, __w >> SCHED_FIXEDPOINT_SHIFT); \
+	__w; \
+})
 #else
 # define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT)
 # define scale_load(w)		(w)

commit fd3eafda8f146d4ad8f95f91a8c2b9a5319ff6b2
Author: Peter Xu <peterx@redhat.com>
Date:   Mon Dec 16 16:31:25 2019 -0500

    sched/core: Remove rq.hrtick_csd_pending
    
    Now smp_call_function_single_async() provides the protection that
    we'll return with -EBUSY if the csd object is still pending, then we
    don't need the rq.hrtick_csd_pending any more.
    
    Signed-off-by: Peter Xu <peterx@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20191216213125.9536-4-peterx@redhat.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9ea647835fd6..38e60b84e3b4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -967,7 +967,6 @@ struct rq {
 
 #ifdef CONFIG_SCHED_HRTICK
 #ifdef CONFIG_SMP
-	int			hrtick_csd_pending;
 	call_single_data_t	hrtick_csd;
 #endif
 	struct hrtimer		hrtick_timer;

commit ba4f7bc1dee318a0fd9c0e3bd46227aca21ac2f2
Author: Yu Chen <chen.yu@easystack.cn>
Date:   Fri Feb 28 18:03:29 2020 +0800

    sched/deadline: Make two functions static
    
    Since commit 06a76fe08d4 ("sched/deadline: Move DL related code
    from sched/core.c to sched/deadline.c"), DL related code moved to
    deadline.c.
    
    Make the following two functions static since they're only used in
    deadline.c:
    
            dl_change_utilization()
            init_dl_rq_bw_ratio()
    
    Signed-off-by: Yu Chen <chen.yu@easystack.cn>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200228100329.16927-1-chen.yu@easystack.cn

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7f1a85bd540d..9e173fad0425 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -305,7 +305,6 @@ bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
 	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
 }
 
-extern void dl_change_utilization(struct task_struct *p, u64 new_bw);
 extern void init_dl_bw(struct dl_bw *dl_b);
 extern int  sched_dl_global_validate(void);
 extern void sched_dl_do_global(void);
@@ -1905,7 +1904,6 @@ extern struct dl_bandwidth def_dl_bandwidth;
 extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
 extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 extern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);
-extern void init_dl_rq_bw_ratio(struct dl_rq *dl_rq);
 
 #define BW_SHIFT		20
 #define BW_UNIT			(1 << BW_SHIFT)

commit 05289b90c2e40ae80f5c70431cd0be4cc8a6038d
Author: Thara Gopinath <thara.gopinath@linaro.org>
Date:   Fri Feb 21 19:52:13 2020 -0500

    sched/fair: Enable tuning of decay period
    
    Thermal pressure follows pelt signals which means the decay period for
    thermal pressure is the default pelt decay period. Depending on SoC
    characteristics and thermal activity, it might be beneficial to decay
    thermal pressure slower, but still in-tune with the pelt signals.  One way
    to achieve this is to provide a command line parameter to set a decay
    shift parameter to an integer between 0 and 10.
    
    Signed-off-by: Thara Gopinath <thara.gopinath@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200222005213.3873-10-thara.gopinath@linaro.org

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6c839f829a25..7f1a85bd540d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1127,6 +1127,24 @@ static inline u64 rq_clock_task(struct rq *rq)
 	return rq->clock_task;
 }
 
+/**
+ * By default the decay is the default pelt decay period.
+ * The decay shift can change the decay period in
+ * multiples of 32.
+ *  Decay shift		Decay period(ms)
+ *	0			32
+ *	1			64
+ *	2			128
+ *	3			256
+ *	4			512
+ */
+extern int sched_thermal_decay_shift;
+
+static inline u64 rq_clock_thermal(struct rq *rq)
+{
+	return rq_clock_task(rq) >> sched_thermal_decay_shift;
+}
+
 static inline void rq_clock_skip_update(struct rq *rq)
 {
 	lockdep_assert_held(&rq->lock);

commit 765047932f153265db6ef15be208d6cbfc03dc62
Author: Thara Gopinath <thara.gopinath@linaro.org>
Date:   Fri Feb 21 19:52:05 2020 -0500

    sched/pelt: Add support to track thermal pressure
    
    Extrapolating on the existing framework to track rt/dl utilization using
    pelt signals, add a similar mechanism to track thermal pressure. The
    difference here from rt/dl utilization tracking is that, instead of
    tracking time spent by a CPU running a RT/DL task through util_avg, the
    average thermal pressure is tracked through load_avg. This is because
    thermal pressure signal is weighted time "delta" capacity unlike util_avg
    which is binary. "delta capacity" here means delta between the actual
    capacity of a CPU and the decreased capacity a CPU due to a thermal event.
    
    In order to track average thermal pressure, a new sched_avg variable
    avg_thermal is introduced. Function update_thermal_load_avg can be called
    to do the periodic bookkeeping (accumulate, decay and average) of the
    thermal pressure.
    
    Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Thara Gopinath <thara.gopinath@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200222005213.3873-2-thara.gopinath@linaro.org

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2a0caf394dd4..6c839f829a25 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -960,6 +960,9 @@ struct rq {
 	struct sched_avg	avg_dl;
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
 	struct sched_avg	avg_irq;
+#endif
+#ifdef CONFIG_SCHED_THERMAL_PRESSURE
+	struct sched_avg	avg_thermal;
 #endif
 	u64			idle_stamp;
 	u64			avg_idle;

commit 9f68395333ad7f5bfe2f83473fed363d4229f11c
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Feb 24 09:52:18 2020 +0000

    sched/pelt: Add a new runnable average signal
    
    Now that runnable_load_avg has been removed, we can replace it by a new
    signal that will highlight the runnable pressure on a cfs_rq. This signal
    track the waiting time of tasks on rq and can help to better define the
    state of rqs.
    
    At now, only util_avg is used to define the state of a rq:
      A rq with more that around 80% of utilization and more than 1 tasks is
      considered as overloaded.
    
    But the util_avg signal of a rq can become temporaly low after that a task
    migrated onto another rq which can bias the classification of the rq.
    
    When tasks compete for the same rq, their runnable average signal will be
    higher than util_avg as it will include the waiting time and we can use
    this signal to better classify cfs_rqs.
    
    The new runnable_avg will track the runnable time of a task which simply
    adds the waiting time to the running time. The runnable _avg of cfs_rq
    will be the /Sum of se's runnable_avg and the runnable_avg of group entity
    will follow the one of the rq similarly to util_avg.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Link: https://lore.kernel.org/r/20200224095223.13361-9-mgorman@techsingularity.net

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ce27e588fa7c..2a0caf394dd4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -527,7 +527,7 @@ struct cfs_rq {
 		int		nr;
 		unsigned long	load_avg;
 		unsigned long	util_avg;
-		unsigned long	runnable_sum;
+		unsigned long	runnable_avg;
 	} removed;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -688,9 +688,29 @@ struct dl_rq {
 /* An entity is a task if it doesn't "own" a runqueue */
 #define entity_is_task(se)	(!se->my_q)
 
+static inline void se_update_runnable(struct sched_entity *se)
+{
+	if (!entity_is_task(se))
+		se->runnable_weight = se->my_q->h_nr_running;
+}
+
+static inline long se_runnable(struct sched_entity *se)
+{
+	if (entity_is_task(se))
+		return !!se->on_rq;
+	else
+		return se->runnable_weight;
+}
+
 #else
 #define entity_is_task(se)	1
 
+static inline void se_update_runnable(struct sched_entity *se) {}
+
+static inline long se_runnable(struct sched_entity *se)
+{
+	return !!se->on_rq;
+}
 #endif
 
 #ifdef CONFIG_SMP

commit 0dacee1bfa70e171be3a12a30414c228453048d2
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Feb 24 09:52:17 2020 +0000

    sched/pelt: Remove unused runnable load average
    
    Now that runnable_load_avg is no more used, we can remove it to make
    space for a new signal.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Link: https://lore.kernel.org/r/20200224095223.13361-8-mgorman@techsingularity.net

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 12bf82d86156..ce27e588fa7c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -489,7 +489,6 @@ struct cfs_bandwidth { };
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
 	struct load_weight	load;
-	unsigned long		runnable_weight;
 	unsigned int		nr_running;
 	unsigned int		h_nr_running;      /* SCHED_{NORMAL,BATCH,IDLE} */
 	unsigned int		idle_h_nr_running; /* SCHED_IDLE */
@@ -688,8 +687,10 @@ struct dl_rq {
 #ifdef CONFIG_FAIR_GROUP_SCHED
 /* An entity is a task if it doesn't "own" a runqueue */
 #define entity_is_task(se)	(!se->my_q)
+
 #else
 #define entity_is_task(se)	1
+
 #endif
 
 #ifdef CONFIG_SMP
@@ -701,10 +702,6 @@ static inline long se_weight(struct sched_entity *se)
 	return scale_load_down(se->load.weight);
 }
 
-static inline long se_runnable(struct sched_entity *se)
-{
-	return scale_load_down(se->runnable_weight);
-}
 
 static inline bool sched_asym_prefer(int a, int b)
 {

commit 546121b65f47384e11ec1fa2e55449fc9f4846b2
Merge: 000619680c37 f8788d86ab28
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 24 11:36:09 2020 +0100

    Merge tag 'v5.6-rc3' into sched/core, to pick up fixes and dependent patches
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f8459197e75b045d8d1d87b9856486b39e375721
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Thu Feb 6 19:19:56 2020 +0000

    sched/core: Remove for_each_lower_domain()
    
    The last remaining user of this macro has just been removed, get rid of it.
    
    Suggested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Quentin Perret <qperret@google.com>
    Link: https://lkml.kernel.org/r/20200206191957.12325-4-valentin.schneider@arm.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0844e81964e5..878910e8b299 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1337,8 +1337,6 @@ extern void sched_ttwu_pending(void);
 	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); \
 			__sd; __sd = __sd->parent)
 
-#define for_each_lower_domain(sd) for (; sd; sd = sd->child)
-
 /**
  * highest_flag_domain - Return highest sched_domain containing flag.
  * @cpu:	The CPU whose highest level of sched domain is to

commit 4104a562e0ca62e971089db9d3c47794a0d7d4eb
Author: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
Date:   Sat Feb 1 18:28:03 2020 +0530

    sched/core: Annotate curr pointer in rq with __rcu
    
    This patch fixes the following sparse warnings in sched/core.c
    and sched/membarrier.c:
    
      kernel/sched/core.c:2372:27: error: incompatible types in comparison expression
      kernel/sched/core.c:4061:17: error: incompatible types in comparison expression
      kernel/sched/core.c:6067:9: error: incompatible types in comparison expression
      kernel/sched/membarrier.c:108:21: error: incompatible types in comparison expression
      kernel/sched/membarrier.c:177:21: error: incompatible types in comparison expression
      kernel/sched/membarrier.c:243:21: error: incompatible types in comparison expression
    
    Signed-off-by: Madhuparna Bhowmik <madhuparnabhowmik10@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200201125803.20245-1-madhuparnabhowmik10@gmail.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5876e6ba5903..9ea647835fd6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -896,7 +896,7 @@ struct rq {
 	 */
 	unsigned long		nr_uninterruptible;
 
-	struct task_struct	*curr;
+	struct task_struct __rcu	*curr;
 	struct task_struct	*idle;
 	struct task_struct	*stop;
 	unsigned long		next_balance;

commit 52262ee567ad14c9606be25f3caddcefa3c514e4
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Tue Jan 28 15:40:06 2020 +0000

    sched/fair: Allow a per-CPU kthread waking a task to stack on the same CPU, to fix XFS performance regression
    
    The following XFS commit:
    
      8ab39f11d974 ("xfs: prevent CIL push holdoff in log recovery")
    
    changed the logic from using bound workqueues to using unbound
    workqueues. Functionally this makes sense but it was observed at the
    time that the dbench performance dropped quite a lot and CPU migrations
    were increased.
    
    The current pattern of the task migration is straight-forward. With XFS,
    an IO issuer delegates work to xlog_cil_push_work ()on an unbound kworker.
    This runs on a nearby CPU and on completion, dbench wakes up on its old CPU
    as it is still idle and no migration occurs. dbench then queues the real
    IO on the blk_mq_requeue_work() work item which runs on a bound kworker
    which is forced to run on the same CPU as dbench. When IO completes,
    the bound kworker wakes dbench but as the kworker is a bound but,
    real task, the CPU is not considered idle and dbench gets migrated by
    select_idle_sibling() to a new CPU. dbench may ping-pong between two CPUs
    for a while but ultimately it starts a round-robin of all CPUs sharing
    the same LLC. High-frequency migration on each IO completion has poor
    performance overall. It has negative implications both in commication
    costs and power management. mpstat confirmed that at low thread counts
    that all CPUs sharing an LLC has low level of activity.
    
    Note that even if the CIL patch was reverted, there still would
    be migrations but the impact is less noticeable. It turns out that
    individually the scheduler, XFS, blk-mq and workqueues all made sensible
    decisions but in combination, the overall effect was sub-optimal.
    
    This patch special cases the IO issue/completion pattern and allows
    a bound kworker waker and a task wakee to stack on the same CPU if
    there is a strong chance they are directly related. The expectation
    is that the kworker is likely going back to sleep shortly. This is not
    guaranteed as the IO could be queued asynchronously but there is a very
    strong relationship between the task and kworker in this case that would
    justify stacking on the same CPU instead of migrating. There should be
    few concerns about kworker starvation given that the special casing is
    only when the kworker is the waker.
    
    DBench on XFS
    MMTests config: io-dbench4-async modified to run on a fresh XFS filesystem
    
    UMA machine with 8 cores sharing LLC
                              5.5.0-rc7              5.5.0-rc7
                      tipsched-20200124           kworkerstack
    Amean     1        22.63 (   0.00%)       20.54 *   9.23%*
    Amean     2        25.56 (   0.00%)       23.40 *   8.44%*
    Amean     4        28.63 (   0.00%)       27.85 *   2.70%*
    Amean     8        37.66 (   0.00%)       37.68 (  -0.05%)
    Amean     64      469.47 (   0.00%)      468.26 (   0.26%)
    Stddev    1         1.00 (   0.00%)        0.72 (  28.12%)
    Stddev    2         1.62 (   0.00%)        1.97 ( -21.54%)
    Stddev    4         2.53 (   0.00%)        3.58 ( -41.19%)
    Stddev    8         5.30 (   0.00%)        5.20 (   1.92%)
    Stddev    64       86.36 (   0.00%)       94.53 (  -9.46%)
    
    NUMA machine, 48 CPUs total, 24 CPUs share cache
                               5.5.0-rc7              5.5.0-rc7
                       tipsched-20200124      kworkerstack-v1r2
    Amean     1         58.69 (   0.00%)       30.21 *  48.53%*
    Amean     2         60.90 (   0.00%)       35.29 *  42.05%*
    Amean     4         66.77 (   0.00%)       46.55 *  30.28%*
    Amean     8         81.41 (   0.00%)       68.46 *  15.91%*
    Amean     16       113.29 (   0.00%)      107.79 *   4.85%*
    Amean     32       199.10 (   0.00%)      198.22 *   0.44%*
    Amean     64       478.99 (   0.00%)      477.06 *   0.40%*
    Amean     128     1345.26 (   0.00%)     1372.64 *  -2.04%*
    Stddev    1          2.64 (   0.00%)        4.17 ( -58.08%)
    Stddev    2          4.35 (   0.00%)        5.38 ( -23.73%)
    Stddev    4          6.77 (   0.00%)        6.56 (   3.00%)
    Stddev    8         11.61 (   0.00%)       10.91 (   6.04%)
    Stddev    16        18.63 (   0.00%)       19.19 (  -3.01%)
    Stddev    32        38.71 (   0.00%)       38.30 (   1.06%)
    Stddev    64       100.28 (   0.00%)       91.24 (   9.02%)
    Stddev    128      186.87 (   0.00%)      160.34 (  14.20%)
    
    Dbench has been modified to report the time to complete a single "load
    file". This is a more meaningful metric for dbench that a throughput
    metric as the benchmark makes many different system calls that are not
    throughput-related
    
    Patch shows a 9.23% and 48.53% reduction in the time to process a load
    file with the difference partially explained by the number of CPUs sharing
    a LLC. In a separate run, task migrations were almost eliminated by the
    patch for low client counts. In case people have issue with the metric
    used for the benchmark, this is a comparison of the throughputs as
    reported by dbench on the NUMA machine.
    
    dbench4 Throughput (misleading but traditional)
                               5.5.0-rc7              5.5.0-rc7
                       tipsched-20200124      kworkerstack-v1r2
    Hmean     1        321.41 (   0.00%)      617.82 *  92.22%*
    Hmean     2        622.87 (   0.00%)     1066.80 *  71.27%*
    Hmean     4       1134.56 (   0.00%)     1623.74 *  43.12%*
    Hmean     8       1869.96 (   0.00%)     2212.67 *  18.33%*
    Hmean     16      2673.11 (   0.00%)     2806.13 *   4.98%*
    Hmean     32      3032.74 (   0.00%)     3039.54 (   0.22%)
    Hmean     64      2514.25 (   0.00%)     2498.96 *  -0.61%*
    Hmean     128     1778.49 (   0.00%)     1746.05 *  -1.82%*
    
    Note that this is somewhat specific to XFS and ext4 shows no performance
    difference as it does not rely on kworkers in the same way. No major
    problem was observed running other workloads on different machines although
    not all tests have completed yet.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200128154006.GD3466@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1a88dc8ad11b..5876e6ba5903 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2479,3 +2479,16 @@ static inline void membarrier_switch_mm(struct rq *rq,
 {
 }
 #endif
+
+#ifdef CONFIG_SMP
+static inline bool is_per_cpu_kthread(struct task_struct *p)
+{
+	if (!(p->flags & PF_KTHREAD))
+		return false;
+
+	if (p->nr_cpus_allowed != 1)
+		return false;
+
+	return true;
+}
+#endif

commit 1567c3e3467cddeb019a7b53ec632f834b6a9239
Author: Giovanni Gherdovich <ggherdovich@suse.cz>
Date:   Wed Jan 22 16:16:12 2020 +0100

    x86, sched: Add support for frequency invariance
    
    Implement arch_scale_freq_capacity() for 'modern' x86. This function
    is used by the scheduler to correctly account usage in the face of
    DVFS.
    
    The present patch addresses Intel processors specifically and has positive
    performance and performance-per-watt implications for the schedutil cpufreq
    governor, bringing it closer to, if not on-par with, the powersave governor
    from the intel_pstate driver/framework.
    
    Large performance gains are obtained when the machine is lightly loaded and
    no regression are observed at saturation. The benchmarks with the largest
    gains are kernel compilation, tbench (the networking version of dbench) and
    shell-intensive workloads.
    
    1. FREQUENCY INVARIANCE: MOTIVATION
       * Without it, a task looks larger if the CPU runs slower
    
    2. PECULIARITIES OF X86
       * freq invariance accounting requires knowing the ratio freq_curr/freq_max
       2.1 CURRENT FREQUENCY
           * Use delta_APERF / delta_MPERF * freq_base (a.k.a "BusyMHz")
       2.2 MAX FREQUENCY
           * It varies with time (turbo). As an approximation, we set it to a
             constant, i.e. 4-cores turbo frequency.
    
    3. EFFECTS ON THE SCHEDUTIL FREQUENCY GOVERNOR
       * The invariant schedutil's formula has no feedback loop and reacts faster
         to utilization changes
    
    4. KNOWN LIMITATIONS
       * In some cases tasks can't reach max util despite how hard they try
    
    5. PERFORMANCE TESTING
       5.1 MACHINES
           * Skylake, Broadwell, Haswell
       5.2 SETUP
           * baseline Linux v5.2 w/ non-invariant schedutil. Tested freq_max = 1-2-3-4-8-12
             active cores turbo w/ invariant schedutil, and intel_pstate/powersave
       5.3 BENCHMARK RESULTS
           5.3.1 NEUTRAL BENCHMARKS
                 * NAS Parallel Benchmark (HPC), hackbench
           5.3.2 NON-NEUTRAL BENCHMARKS
                 * tbench (10-30% better), kernbench (10-15% better),
                   shell-intensive-scripts (30-50% better)
                 * no regressions
           5.3.3 SELECTION OF DETAILED RESULTS
           5.3.4 POWER CONSUMPTION, PERFORMANCE-PER-WATT
                 * dbench (5% worse on one machine), kernbench (3% worse),
                   tbench (5-10% better), shell-intensive-scripts (10-40% better)
    
    6. MICROARCH'ES ADDRESSED HERE
       * Xeon Core before Scalable Performance processors line (Xeon Gold/Platinum
         etc have different MSRs semantic for querying turbo levels)
    
    7. REFERENCES
       * MMTests performance testing framework, github.com/gormanm/mmtests
    
     +-------------------------------------------------------------------------+
     | 1. FREQUENCY INVARIANCE: MOTIVATION
     +-------------------------------------------------------------------------+
    
    For example; suppose a CPU has two frequencies: 500 and 1000 Mhz. When
    running a task that would consume 1/3rd of a CPU at 1000 MHz, it would
    appear to consume 2/3rd (or 66.6%) when running at 500 MHz, giving the
    false impression this CPU is almost at capacity, even though it can go
    faster [*]. In a nutshell, without frequency scale-invariance tasks look
    larger just because the CPU is running slower.
    
    [*] (footnote: this assumes a linear frequency/performance relation; which
    everybody knows to be false, but given realities its the best approximation
    we can make.)
    
     +-------------------------------------------------------------------------+
     | 2. PECULIARITIES OF X86
     +-------------------------------------------------------------------------+
    
    Accounting for frequency changes in PELT signals requires the computation of
    the ratio freq_curr / freq_max. On x86 neither of those terms is readily
    available.
    
    2.1 CURRENT FREQUENCY
    ====================
    
    Since modern x86 has hardware control over the actual frequency we run
    at (because amongst other things, Turbo-Mode), we cannot simply use
    the frequency as requested through cpufreq.
    
    Instead we use the APERF/MPERF MSRs to compute the effective frequency
    over the recent past. Also, because reading MSRs is expensive, don't
    do so every time we need the value, but amortize the cost by doing it
    every tick.
    
    2.2 MAX FREQUENCY
    =================
    
    Obtaining freq_max is also non-trivial because at any time the hardware can
    provide a frequency boost to a selected subset of cores if the package has
    enough power to spare (eg: Turbo Boost). This means that the maximum frequency
    available to a given core changes with time.
    
    The approach taken in this change is to arbitrarily set freq_max to a constant
    value at boot. The value chosen is the "4-cores (4C) turbo frequency" on most
    microarchitectures, after evaluating the following candidates:
    
        * 1-core (1C) turbo frequency (the fastest turbo state available)
        * around base frequency (a.k.a. max P-state)
        * something in between, such as 4C turbo
    
    To interpret these options, consider that this is the denominator in
    freq_curr/freq_max, and that ratio will be used to scale PELT signals such as
    util_avg and load_avg. A large denominator will undershoot (util_avg looks a
    bit smaller than it really is), viceversa with a smaller denominator PELT
    signals will tend to overshoot. Given that PELT drives frequency selection
    in the schedutil governor, we will have:
    
        freq_max set to     | effect on DVFS
        --------------------+------------------
        1C turbo            | power efficiency (lower freq choices)
        base freq           | performance (higher util_avg, higher freq requests)
        4C turbo            | a bit of both
    
    4C turbo proves to be a good compromise in a number of benchmarks (see below).
    
     +-------------------------------------------------------------------------+
     | 3. EFFECTS ON THE SCHEDUTIL FREQUENCY GOVERNOR
     +-------------------------------------------------------------------------+
    
    Once an architecture implements a frequency scale-invariant utilization (the
    PELT signal util_avg), schedutil switches its frequency selection formula from
    
        freq_next = 1.25 * freq_curr * util            [non-invariant util signal]
    
    to
    
        freq_next = 1.25 * freq_max * util             [invariant util signal]
    
    where, in the second formula, freq_max is set to the 1C turbo frequency (max
    turbo). The advantage of the second formula, whose usage we unlock with this
    patch, is that freq_next doesn't depend on the current frequency in an
    iterative fashion, but can jump to any frequency in a single update. This
    absence of feedback in the formula makes it quicker to react to utilization
    changes and more robust against pathological instabilities.
    
    Compare it to the update formula of intel_pstate/powersave:
    
        freq_next = 1.25 * freq_max * Busy%
    
    where again freq_max is 1C turbo and Busy% is the percentage of time not spent
    idling (calculated with delta_MPERF / delta_TSC); essentially the same as
    invariant schedutil, and largely responsible for intel_pstate/powersave good
    reputation. The non-invariant schedutil formula is derived from the invariant
    one by approximating util_inv with util_raw * freq_curr / freq_max, but this
    has limitations.
    
    Testing shows improved performances due to better frequency selections when
    the machine is lightly loaded, and essentially no change in behaviour at
    saturation / overutilization.
    
     +-------------------------------------------------------------------------+
     | 4. KNOWN LIMITATIONS
     +-------------------------------------------------------------------------+
    
    It's been shown that it is possible to create pathological scenarios where a
    CPU-bound task cannot reach max utilization, if the normalizing factor
    freq_max is fixed to a constant value (see [Lelli-2018]).
    
    If freq_max is set to 4C turbo as we do here, one needs to peg at least 5
    cores in a package doing some busywork, and observe that none of those task
    will ever reach max util (1024) because they're all running at less than the
    4C turbo frequency.
    
    While this concern still applies, we believe the performance benefit of
    frequency scale-invariant PELT signals outweights the cost of this limitation.
    
     [Lelli-2018]
     https://lore.kernel.org/lkml/20180517150418.GF22493@localhost.localdomain/
    
     +-------------------------------------------------------------------------+
     | 5. PERFORMANCE TESTING
     +-------------------------------------------------------------------------+
    
    5.1 MACHINES
    ============
    
    We tested the patch on three machines, with Skylake, Broadwell and Haswell
    CPUs. The details are below, together with the available turbo ratios as
    reported by the appropriate MSRs.
    
    * 8x-SKYLAKE-UMA:
      Single socket E3-1240 v5, Skylake 4 cores/8 threads
      Max EFFiciency, BASE frequency and available turbo levels (MHz):
    
        EFFIC    800 |********
        BASE    3500 |***********************************
        4C      3700 |*************************************
        3C      3800 |**************************************
        2C      3900 |***************************************
        1C      3900 |***************************************
    
    * 80x-BROADWELL-NUMA:
      Two sockets E5-2698 v4, 2x Broadwell 20 cores/40 threads
      Max EFFiciency, BASE frequency and available turbo levels (MHz):
    
        EFFIC   1200 |************
        BASE    2200 |**********************
        8C      2900 |*****************************
        7C      3000 |******************************
        6C      3100 |*******************************
        5C      3200 |********************************
        4C      3300 |*********************************
        3C      3400 |**********************************
        2C      3600 |************************************
        1C      3600 |************************************
    
    * 48x-HASWELL-NUMA
      Two sockets E5-2670 v3, 2x Haswell 12 cores/24 threads
      Max EFFiciency, BASE frequency and available turbo levels (MHz):
    
        EFFIC   1200 |************
        BASE    2300 |***********************
        12C     2600 |**************************
        11C     2600 |**************************
        10C     2600 |**************************
        9C      2600 |**************************
        8C      2600 |**************************
        7C      2600 |**************************
        6C      2600 |**************************
        5C      2700 |***************************
        4C      2800 |****************************
        3C      2900 |*****************************
        2C      3100 |*******************************
        1C      3100 |*******************************
    
    5.2 SETUP
    =========
    
    * The baseline is Linux v5.2 with schedutil (non-invariant) and the intel_pstate
      driver in passive mode.
    * The rationale for choosing the various freq_max values to test have been to
      try all the 1-2-3-4C turbo levels (note that 1C and 2C turbo are identical
      on all machines), plus one more value closer to base_freq but still in the
      turbo range (8C turbo for both 80x-BROADWELL-NUMA and 48x-HASWELL-NUMA).
    * In addition we've run all tests with intel_pstate/powersave for comparison.
    * The filesystem is always XFS, the userspace is openSUSE Leap 15.1.
    * 8x-SKYLAKE-UMA is capable of HWP (Hardware-Managed P-States), so the runs
      with active intel_pstate on this machine use that.
    
    This gives, in terms of combinations tested on each machine:
    
    * 8x-SKYLAKE-UMA
      * Baseline: Linux v5.2, non-invariant schedutil, intel_pstate passive
      * intel_pstate active + powersave + HWP
      * invariant schedutil, freq_max = 1C turbo
      * invariant schedutil, freq_max = 3C turbo
      * invariant schedutil, freq_max = 4C turbo
    
    * both 80x-BROADWELL-NUMA and 48x-HASWELL-NUMA
      * [same as 8x-SKYLAKE-UMA, but no HWP capable]
      * invariant schedutil, freq_max = 8C turbo
        (which on 48x-HASWELL-NUMA is the same as 12C turbo, or "all cores turbo")
    
    5.3 BENCHMARK RESULTS
    =====================
    
    5.3.1 NEUTRAL BENCHMARKS
    ------------------------
    
    Tests that didn't show any measurable difference in performance on any of the
    test machines between non-invariant schedutil and our patch are:
    
    * NAS Parallel Benchmarks (NPB) using either MPI or openMP for IPC, any
      computational kernel
    * flexible I/O (FIO)
    * hackbench (using threads or processes, and using pipes or sockets)
    
    5.3.2 NON-NEUTRAL BENCHMARKS
    ----------------------------
    
    What follow are summary tables where each benchmark result is given a score.
    
    * A tilde (~) means a neutral result, i.e. no difference from baseline.
    * Scores are computed with the ratio result_new / result_baseline, so a tilde
      means a score of 1.00.
    * The results in the score ratio are the geometric means of results running
      the benchmark with different parameters (eg: for kernbench: using 1, 2, 4,
      ... number of processes; for pgbench: varying the number of clients, and so
      on).
    * The first three tables show higher-is-better kind of tests (i.e. measured in
      operations/second), the subsequent three show lower-is-better kind of tests
      (i.e. the workload is fixed and we measure elapsed time, think kernbench).
    * "gitsource" is a name we made up for the test consisting in running the
      entire unit tests suite of the Git SCM and measuring how long it takes. We
      take it as a typical example of shell-intensive serialized workload.
    * In the "I_PSTATE" column we have the results for intel_pstate/powersave. Other
      columns show invariant schedutil for different values of freq_max. 4C turbo
      is circled as it's the value we've chosen for the final implementation.
    
    80x-BROADWELL-NUMA (comparison ratio; higher is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  8C
    pgbench-ro           1.14   ~      ~     | 1.11 |  1.14
    pgbench-rw           ~      ~      ~     | ~    |  ~
    netperf-udp          1.06   ~      1.06  | 1.05 |  1.07
    netperf-tcp          ~      1.03   ~     | 1.01 |  1.02
    tbench4              1.57   1.18   1.22  | 1.30 |  1.56
                                             +------+
    
    8x-SKYLAKE-UMA (comparison ratio; higher is better)
                                             +------+
                 I_PSTATE/HWP   1C     3C    | 4C   |
    pgbench-ro           ~      ~      ~     | ~    |
    pgbench-rw           ~      ~      ~     | ~    |
    netperf-udp          ~      ~      ~     | ~    |
    netperf-tcp          ~      ~      ~     | ~    |
    tbench4              1.30   1.14   1.14  | 1.16 |
                                             +------+
    
    48x-HASWELL-NUMA (comparison ratio; higher is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  12C
    pgbench-ro           1.15   ~      ~     | 1.06 |  1.16
    pgbench-rw           ~      ~      ~     | ~    |  ~
    netperf-udp          1.05   0.97   1.04  | 1.04 |  1.02
    netperf-tcp          0.96   1.01   1.01  | 1.01 |  1.01
    tbench4              1.50   1.05   1.13  | 1.13 |  1.25
                                             +------+
    
    In the table above we see that active intel_pstate is slightly better than our
    4C-turbo patch (both in reference to the baseline non-invariant schedutil) on
    read-only pgbench and much better on tbench. Both cases are notable in which
    it shows that lowering our freq_max (to 8C-turbo and 12C-turbo on
    80x-BROADWELL-NUMA and 48x-HASWELL-NUMA respectively) helps invariant
    schedutil to get closer.
    
    If we ignore active intel_pstate and focus on the comparison with baseline
    alone, there are several instances of double-digit performance improvement.
    
    80x-BROADWELL-NUMA (comparison ratio; lower is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  8C
    dbench4              1.23   0.95   0.95  | 0.95 |  0.95
    kernbench            0.93   0.83   0.83  | 0.83 |  0.82
    gitsource            0.98   0.49   0.49  | 0.49 |  0.48
                                             +------+
    
    8x-SKYLAKE-UMA (comparison ratio; lower is better)
                                             +------+
                 I_PSTATE/HWP   1C     3C    | 4C   |
    dbench4              ~      ~      ~     | ~    |
    kernbench            ~      ~      ~     | ~    |
    gitsource            0.92   0.55   0.55  | 0.55 |
                                             +------+
    
    48x-HASWELL-NUMA (comparison ratio; lower is better)
                                             +------+
                     I_PSTATE   1C     3C    | 4C   |  8C
    dbench4              ~      ~      ~     | ~    |  ~
    kernbench            0.94   0.90   0.89  | 0.90 |  0.90
    gitsource            0.97   0.69   0.69  | 0.69 |  0.69
                                             +------+
    
    dbench is not very remarkable here, unless we notice how poorly active
    intel_pstate is performing on 80x-BROADWELL-NUMA: 23% regression versus
    non-invariant schedutil. We repeated that run getting consistent results. Out
    of scope for the patch at hand, but deserving future investigation. Other than
    that, we previously ran this campaign with Linux v5.0 and saw the patch doing
    better on dbench a the time. We haven't checked closely and can only speculate
    at this point.
    
    On the NUMA boxes kernbench gets 10-15% improvements on average; we'll see in
    the detailed tables that the gains concentrate on low process counts (lightly
    loaded machines).
    
    The test we call "gitsource" (running the git unit test suite, a long-running
    single-threaded shell script) appears rather spectacular in this table (gains
    of 30-50% depending on the machine). It is to be noted, however, that
    gitsource has no adjustable parameters (such as the number of jobs in
    kernbench, which we average over in order to get a single-number summary
    score) and is exactly the kind of low-parallelism workload that benefits the
    most from this patch. When looking at the detailed tables of kernbench or
    tbench4, at low process or client counts one can see similar numbers.
    
    5.3.3 SELECTION OF DETAILED RESULTS
    -----------------------------------
    
    Machine            : 48x-HASWELL-NUMA
    Benchmark          : tbench4 (i.e. dbench4 over the network, actually loopback)
    Varying parameter  : number of clients
    Unit               : MB/sec (higher is better)
    
                       5.2.0 vanilla (BASELINE)               5.2.0 intel_pstate                   5.2.0 1C-turbo
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Hmean  1        126.73  +- 0.31% (        )      315.91  +- 0.66% ( 149.28%)      125.03  +- 0.76% (  -1.34%)
    Hmean  2        258.04  +- 0.62% (        )      614.16  +- 0.51% ( 138.01%)      269.58  +- 1.45% (   4.47%)
    Hmean  4        514.30  +- 0.67% (        )     1146.58  +- 0.54% ( 122.94%)      533.84  +- 1.99% (   3.80%)
    Hmean  8       1111.38  +- 2.52% (        )     2159.78  +- 0.38% (  94.33%)     1359.92  +- 1.56% (  22.36%)
    Hmean  16      2286.47  +- 1.36% (        )     3338.29  +- 0.21% (  46.00%)     2720.20  +- 0.52% (  18.97%)
    Hmean  32      4704.84  +- 0.35% (        )     4759.03  +- 0.43% (   1.15%)     4774.48  +- 0.30% (   1.48%)
    Hmean  64      7578.04  +- 0.27% (        )     7533.70  +- 0.43% (  -0.59%)     7462.17  +- 0.65% (  -1.53%)
    Hmean  128     6998.52  +- 0.16% (        )     6987.59  +- 0.12% (  -0.16%)     6909.17  +- 0.14% (  -1.28%)
    Hmean  192     6901.35  +- 0.25% (        )     6913.16  +- 0.10% (   0.17%)     6855.47  +- 0.21% (  -0.66%)
    
                                 5.2.0 3C-turbo                   5.2.0 4C-turbo                  5.2.0 12C-turbo
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Hmean  1        128.43  +- 0.28% (   1.34%)      130.64  +- 3.81% (   3.09%)      153.71  +- 5.89% (  21.30%)
    Hmean  2        311.70  +- 6.15% (  20.79%)      281.66  +- 3.40% (   9.15%)      305.08  +- 5.70% (  18.23%)
    Hmean  4        641.98  +- 2.32% (  24.83%)      623.88  +- 5.28% (  21.31%)      906.84  +- 4.65% (  76.32%)
    Hmean  8       1633.31  +- 1.56% (  46.96%)     1714.16  +- 0.93% (  54.24%)     2095.74  +- 0.47% (  88.57%)
    Hmean  16      3047.24  +- 0.42% (  33.27%)     3155.02  +- 0.30% (  37.99%)     3634.58  +- 0.15% (  58.96%)
    Hmean  32      4734.31  +- 0.60% (   0.63%)     4804.38  +- 0.23% (   2.12%)     4674.62  +- 0.27% (  -0.64%)
    Hmean  64      7699.74  +- 0.35% (   1.61%)     7499.72  +- 0.34% (  -1.03%)     7659.03  +- 0.25% (   1.07%)
    Hmean  128     6935.18  +- 0.15% (  -0.91%)     6942.54  +- 0.10% (  -0.80%)     7004.85  +- 0.12% (   0.09%)
    Hmean  192     6901.62  +- 0.12% (   0.00%)     6856.93  +- 0.10% (  -0.64%)     6978.74  +- 0.10% (   1.12%)
    
    This is one of the cases where the patch still can't surpass active
    intel_pstate, not even when freq_max is as low as 12C-turbo. Otherwise, gains are
    visible up to 16 clients and the saturated scenario is the same as baseline.
    
    The scores in the summary table from the previous sections are ratios of
    geometric means of the results over different clients, as seen in this table.
    
    Machine            : 80x-BROADWELL-NUMA
    Benchmark          : kernbench (kernel compilation)
    Varying parameter  : number of jobs
    Unit               : seconds (lower is better)
    
                       5.2.0 vanilla (BASELINE)               5.2.0 intel_pstate                   5.2.0 1C-turbo
    - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean  2        379.68  +- 0.06% (        )      330.20  +- 0.43% (  13.03%)      285.93  +- 0.07% (  24.69%)
    Amean  4        200.15  +- 0.24% (        )      175.89  +- 0.22% (  12.12%)      153.78  +- 0.25% (  23.17%)
    Amean  8        106.20  +- 0.31% (        )       95.54  +- 0.23% (  10.03%)       86.74  +- 0.10% (  18.32%)
    Amean  16        56.96  +- 1.31% (        )       53.25  +- 1.22% (   6.50%)       48.34  +- 1.73% (  15.13%)
    Amean  32        34.80  +- 2.46% (        )       33.81  +- 0.77% (   2.83%)       30.28  +- 1.59% (  12.99%)
    Amean  64        26.11  +- 1.63% (        )       25.04  +- 1.07% (   4.10%)       22.41  +- 2.37% (  14.16%)
    Amean  128       24.80  +- 1.36% (        )       23.57  +- 1.23% (   4.93%)       21.44  +- 1.37% (  13.55%)
    Amean  160       24.85  +- 0.56% (        )       23.85  +- 1.17% (   4.06%)       21.25  +- 1.12% (  14.49%)
    
                                 5.2.0 3C-turbo                   5.2.0 4C-turbo                   5.2.0 8C-turbo
    - - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean  2        284.08  +- 0.13% (  25.18%)      283.96  +- 0.51% (  25.21%)      285.05  +- 0.21% (  24.92%)
    Amean  4        153.18  +- 0.22% (  23.47%)      154.70  +- 1.64% (  22.71%)      153.64  +- 0.30% (  23.24%)
    Amean  8         87.06  +- 0.28% (  18.02%)       86.77  +- 0.46% (  18.29%)       86.78  +- 0.22% (  18.28%)
    Amean  16        48.03  +- 0.93% (  15.68%)       47.75  +- 1.99% (  16.17%)       47.52  +- 1.61% (  16.57%)
    Amean  32        30.23  +- 1.20% (  13.14%)       30.08  +- 1.67% (  13.57%)       30.07  +- 1.67% (  13.60%)
    Amean  64        22.59  +- 2.02% (  13.50%)       22.63  +- 0.81% (  13.32%)       22.42  +- 0.76% (  14.12%)
    Amean  128       21.37  +- 0.67% (  13.82%)       21.31  +- 1.15% (  14.07%)       21.17  +- 1.93% (  14.63%)
    Amean  160       21.68  +- 0.57% (  12.76%)       21.18  +- 1.74% (  14.77%)       21.22  +- 1.00% (  14.61%)
    
    The patch outperform active intel_pstate (and baseline) by a considerable
    margin; the summary table from the previous section says 4C turbo and active
    intel_pstate are 0.83 and 0.93 against baseline respectively, so 4C turbo is
    0.83/0.93=0.89 against intel_pstate (~10% better on average). There is no
    noticeable difference with regard to the value of freq_max.
    
    Machine            : 8x-SKYLAKE-UMA
    Benchmark          : gitsource (time to run the git unit test suite)
    Varying parameter  : none
    Unit               : seconds (lower is better)
    
                                5.2.0 vanilla           5.2.0 intel_pstate/hwp         5.2.0 1C-turbo
    - - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean         858.85  +- 1.16% (        )      791.94  +- 0.21% (   7.79%)      474.95 (  44.70%)
    
                               5.2.0 3C-turbo                   5.2.0 4C-turbo
    - - - - - - - -  - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
    Amean         475.26  +- 0.20% (  44.66%)      474.34  +- 0.13% (  44.77%)
    
    In this test, which is of interest as representing shell-intensive
    (i.e. fork-intensive) serialized workloads, invariant schedutil outperforms
    intel_pstate/powersave by a whopping 40% margin.
    
    5.3.4 POWER CONSUMPTION, PERFORMANCE-PER-WATT
    ---------------------------------------------
    
    The following table shows average power consumption in watt for each
    benchmark. Data comes from turbostat (package average), which in turn is read
    from the RAPL interface on CPUs. We know the patch affects CPU frequencies so
    it's reasonable to ignore other power consumers (such as memory or I/O). Also,
    we don't have a power meter available in the lab so RAPL is the best we have.
    
    turbostat sampled average power every 10 seconds for the entire duration of
    each benchmark. We took all those values and averaged them (i.e. with don't
    have detail on a per-parameter granularity, only on whole benchmarks).
    
    80x-BROADWELL-NUMA (power consumption, watts)
                                                        +--------+
                   BASELINE I_PSTATE       1C       3C  |     4C |      8C
    pgbench-ro       130.01   142.77   131.11   132.45  | 134.65 |  136.84
    pgbench-rw        68.30    60.83    71.45    71.70  |  71.65 |   72.54
    dbench4           90.25    59.06   101.43    99.89  | 101.10 |  102.94
    netperf-udp       65.70    69.81    66.02    68.03  |  68.27 |   68.95
    netperf-tcp       88.08    87.96    88.97    88.89  |  88.85 |   88.20
    tbench4          142.32   176.73   153.02   163.91  | 165.58 |  176.07
    kernbench         92.94   101.95   114.91   115.47  | 115.52 |  115.10
    gitsource         40.92    41.87    75.14    75.20  |  75.40 |   75.70
                                                        +--------+
    8x-SKYLAKE-UMA (power consumption, watts)
                                                        +--------+
                  BASELINE I_PSTATE/HWP    1C       3C  |     4C |
    pgbench-ro        46.49    46.68    46.56    46.59  |  46.52 |
    pgbench-rw        29.34    31.38    30.98    31.00  |  31.00 |
    dbench4           27.28    27.37    27.49    27.41  |  27.38 |
    netperf-udp       22.33    22.41    22.36    22.35  |  22.36 |
    netperf-tcp       27.29    27.29    27.30    27.31  |  27.33 |
    tbench4           41.13    45.61    43.10    43.33  |  43.56 |
    kernbench         42.56    42.63    43.01    43.01  |  43.01 |
    gitsource         13.32    13.69    17.33    17.30  |  17.35 |
                                                        +--------+
    48x-HASWELL-NUMA (power consumption, watts)
                                                        +--------+
                   BASELINE I_PSTATE       1C       3C  |     4C |     12C
    pgbench-ro       128.84   136.04   129.87   132.43  | 132.30 |  134.86
    pgbench-rw        37.68    37.92    37.17    37.74  |  37.73 |   37.31
    dbench4           28.56    28.73    28.60    28.73  |  28.70 |   28.79
    netperf-udp       56.70    60.44    56.79    57.42  |  57.54 |   57.52
    netperf-tcp       75.49    75.27    75.87    76.02  |  76.01 |   75.95
    tbench4          115.44   139.51   119.53   123.07  | 123.97 |  130.22
    kernbench         83.23    91.55    95.58    95.69  |  95.72 |   96.04
    gitsource         36.79    36.99    39.99    40.34  |  40.35 |   40.23
                                                        +--------+
    
    A lower power consumption isn't necessarily better, it depends on what is done
    with that energy. Here are tables with the ratio of performance-per-watt on
    each machine and benchmark. Higher is always better; a tilde (~) means a
    neutral ratio (i.e. 1.00).
    
    80x-BROADWELL-NUMA (performance-per-watt ratios; higher is better)
                                         +------+
                 I_PSTATE     1C     3C  |   4C |    8C
    pgbench-ro       1.04   1.06   0.94  | 1.07 |  1.08
    pgbench-rw       1.10   0.97   0.96  | 0.96 |  0.97
    dbench4          1.24   0.94   0.95  | 0.94 |  0.92
    netperf-udp      ~      1.02   1.02  | ~    |  1.02
    netperf-tcp      ~      1.02   ~     | ~    |  1.02
    tbench4          1.26   1.10   1.06  | 1.12 |  1.26
    kernbench        0.98   0.97   0.97  | 0.97 |  0.98
    gitsource        ~      1.11   1.11  | 1.11 |  1.13
                                         +------+
    
    8x-SKYLAKE-UMA (performance-per-watt ratios; higher is better)
                                         +------+
             I_PSTATE/HWP     1C     3C  |   4C |
    pgbench-ro       ~      ~      ~     | ~    |
    pgbench-rw       0.95   0.97   0.96  | 0.96 |
    dbench4          ~      ~      ~     | ~    |
    netperf-udp      ~      ~      ~     | ~    |
    netperf-tcp      ~      ~      ~     | ~    |
    tbench4          1.17   1.09   1.08  | 1.10 |
    kernbench        ~      ~      ~     | ~    |
    gitsource        1.06   1.40   1.40  | 1.40 |
                                         +------+
    
    48x-HASWELL-NUMA  (performance-per-watt ratios; higher is better)
                                         +------+
                 I_PSTATE     1C     3C  |   4C |   12C
    pgbench-ro       1.09   ~      1.09  | 1.03 |  1.11
    pgbench-rw       ~      0.86   ~     | ~    |  0.86
    dbench4          ~      1.02   1.02  | 1.02 |  ~
    netperf-udp      ~      0.97   1.03  | 1.02 |  ~
    netperf-tcp      0.96   ~      ~     | ~    |  ~
    tbench4          1.24   ~      1.06  | 1.05 |  1.11
    kernbench        0.97   0.97   0.98  | 0.97 |  0.96
    gitsource        1.03   1.33   1.32  | 1.32 |  1.33
                                         +------+
    
    These results are overall pleasing: in plenty of cases we observe
    performance-per-watt improvements. The few regressions (read/write pgbench and
    dbench on the Broadwell machine) are of small magnitude. kernbench loses a few
    percentage points (it has a 10-15% performance improvement, but apparently the
    increase in power consumption is larger than that). tbench4 and gitsource, which
    benefit the most from the patch, keep a positive score in this table which is
    a welcome surprise; that suggests that in those particular workloads the
    non-invariant schedutil (and active intel_pstate, too) makes some rather
    suboptimal frequency selections.
    
    +-------------------------------------------------------------------------+
    | 6. MICROARCH'ES ADDRESSED HERE
    +-------------------------------------------------------------------------+
    
    The patch addresses Xeon Core processors that use MSR_PLATFORM_INFO and
    MSR_TURBO_RATIO_LIMIT to advertise their base frequency and turbo frequencies
    respectively. This excludes the recent Xeon Scalable Performance processors
    line (Xeon Gold, Platinum etc) whose MSRs have to be parsed differently.
    
    Subsequent patches will address:
    
    * Xeon Scalable Performance processors and Atom Goldmont/Goldmont Plus
    * Xeon Phi (Knights Landing, Knights Mill)
    * Atom Silvermont
    
    +-------------------------------------------------------------------------+
    | 7. REFERENCES
    +-------------------------------------------------------------------------+
    
    Tests have been run with the help of the MMTests performance testing
    framework, see github.com/gormanm/mmtests. The configuration file names for
    the benchmark used are:
    
        db-pgbench-timed-ro-small-xfs
        db-pgbench-timed-rw-small-xfs
        io-dbench4-async-xfs
        network-netperf-unbound
        network-tbench
        scheduler-unbound
        workload-kerndevel-xfs
        workload-shellscripts-xfs
        hpc-nas-c-class-mpi-full-xfs
        hpc-nas-c-class-omp-full
    
    All those benchmarks are generally available on the web:
    
    pgbench: https://www.postgresql.org/docs/10/pgbench.html
    netperf: https://hewlettpackard.github.io/netperf/
    dbench/tbench: https://dbench.samba.org/
    gitsource: git unit test suite, github.com/git/git
    NAS Parallel Benchmarks: https://www.nas.nasa.gov/publications/npb.html
    hackbench: https://people.redhat.com/mingo/cfs-scheduler/tools/hackbench.c
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Giovanni Gherdovich <ggherdovich@suse.cz>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Acked-by: Doug Smythies <dsmythies@telus.net>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Link: https://lkml.kernel.org/r/20200122151617.531-2-ggherdovich@suse.cz

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1a88dc8ad11b..0844e81964e5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1968,6 +1968,13 @@ static inline int hrtick_enabled(struct rq *rq)
 
 #endif /* CONFIG_SCHED_HRTICK */
 
+#ifndef arch_scale_freq_tick
+static __always_inline
+void arch_scale_freq_tick(void)
+{
+}
+#endif
+
 #ifndef arch_scale_freq_capacity
 static __always_inline
 unsigned long arch_scale_freq_capacity(int cpu)

commit d2b58a286e89824900d501db0be1d4f6aed474fc
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Dec 11 11:38:49 2019 +0000

    sched/uclamp: Rename uclamp_util_with() into uclamp_rq_util_with()
    
    The current helper returns (CPU) rq utilization with uclamp restrictions
    taken into account. A uclamp task utilization helper would be quite
    helpful, but this requires some renaming.
    
    Prepare the code for the introduction of a uclamp_task_util() by renaming
    the existing uclamp_util_with() to uclamp_rq_util_with().
    
    Tested-By: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Quentin Perret <qperret@google.com>
    Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191211113851.24241-4-valentin.schneider@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b478474ea847..1a88dc8ad11b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2303,8 +2303,8 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 unsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);
 
 static __always_inline
-unsigned long uclamp_util_with(struct rq *rq, unsigned long util,
-			       struct task_struct *p)
+unsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,
+				  struct task_struct *p)
 {
 	unsigned long min_util = READ_ONCE(rq->uclamp[UCLAMP_MIN].value);
 	unsigned long max_util = READ_ONCE(rq->uclamp[UCLAMP_MAX].value);
@@ -2325,8 +2325,9 @@ unsigned long uclamp_util_with(struct rq *rq, unsigned long util,
 	return clamp(util, min_util, max_util);
 }
 #else /* CONFIG_UCLAMP_TASK */
-static inline unsigned long uclamp_util_with(struct rq *rq, unsigned long util,
-					     struct task_struct *p)
+static inline
+unsigned long uclamp_rq_util_with(struct rq *rq, unsigned long util,
+				  struct task_struct *p)
 {
 	return util;
 }

commit 686516b55e98edf18c2a02d36aaaa6f4c0f6c39c
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Dec 11 11:38:48 2019 +0000

    sched/uclamp: Make uclamp util helpers use and return UL values
    
    Vincent pointed out recently that the canonical type for utilization
    values is 'unsigned long'. Internally uclamp uses 'unsigned int' values for
    cache optimization, but this doesn't have to be exported to its users.
    
    Make the uclamp helpers that deal with utilization use and return unsigned
    long values.
    
    Tested-By: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Quentin Perret <qperret@google.com>
    Reviewed-by: Vincent Guittot <vincent.guittot@linaro.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191211113851.24241-3-valentin.schneider@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d9b24513d71d..b478474ea847 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2300,14 +2300,14 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef CONFIG_UCLAMP_TASK
-unsigned int uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);
+unsigned long uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);
 
 static __always_inline
-unsigned int uclamp_util_with(struct rq *rq, unsigned int util,
-			      struct task_struct *p)
+unsigned long uclamp_util_with(struct rq *rq, unsigned long util,
+			       struct task_struct *p)
 {
-	unsigned int min_util = READ_ONCE(rq->uclamp[UCLAMP_MIN].value);
-	unsigned int max_util = READ_ONCE(rq->uclamp[UCLAMP_MAX].value);
+	unsigned long min_util = READ_ONCE(rq->uclamp[UCLAMP_MIN].value);
+	unsigned long max_util = READ_ONCE(rq->uclamp[UCLAMP_MAX].value);
 
 	if (p) {
 		min_util = max(min_util, uclamp_eff_value(p, UCLAMP_MIN));
@@ -2325,8 +2325,8 @@ unsigned int uclamp_util_with(struct rq *rq, unsigned int util,
 	return clamp(util, min_util, max_util);
 }
 #else /* CONFIG_UCLAMP_TASK */
-static inline unsigned int uclamp_util_with(struct rq *rq, unsigned int util,
-					    struct task_struct *p)
+static inline unsigned long uclamp_util_with(struct rq *rq, unsigned long util,
+					     struct task_struct *p)
 {
 	return util;
 }

commit 59fe675248ffc37d4167e9ec6920a2f3d5ec67bb
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Dec 11 11:38:47 2019 +0000

    sched/uclamp: Remove uclamp_util()
    
    The sole user of uclamp_util(), schedutil_cpu_util(), was made to use
    uclamp_util_with() instead in commit:
    
      af24bde8df20 ("sched/uclamp: Add uclamp support to energy_compute()")
    
    From then on, uclamp_util() has remained unused. Being a simple wrapper
    around uclamp_util_with(), we can get rid of it and win back a few lines.
    
    Tested-By: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Suggested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191211113851.24241-2-valentin.schneider@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 280a3c735935..d9b24513d71d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2324,21 +2324,12 @@ unsigned int uclamp_util_with(struct rq *rq, unsigned int util,
 
 	return clamp(util, min_util, max_util);
 }
-
-static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
-{
-	return uclamp_util_with(rq, util, NULL);
-}
 #else /* CONFIG_UCLAMP_TASK */
 static inline unsigned int uclamp_util_with(struct rq *rq, unsigned int util,
 					    struct task_struct *p)
 {
 	return util;
 }
-static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
-{
-	return util;
-}
 #endif /* CONFIG_UCLAMP_TASK */
 
 #ifdef arch_scale_freq_capacity

commit 7763baace1b738d65efa46d68326c9406311c6bf
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Fri Nov 15 10:39:08 2019 +0000

    sched/uclamp: Fix overzealous type replacement
    
    Some uclamp helpers had their return type changed from 'unsigned int' to
    'enum uclamp_id' by commit
    
      0413d7f33e60 ("sched/uclamp: Always use 'enum uclamp_id' for clamp_id values")
    
    but it happens that some do return a value in the [0, SCHED_CAPACITY_SCALE]
    range, which should really be unsigned int. The affected helpers are
    uclamp_none(), uclamp_rq_max_value() and uclamp_eff_value(). Fix those up.
    
    Note that this doesn't lead to any obj diff using a relatively recent
    aarch64 compiler (8.3-2019.03). The current code of e.g. uclamp_eff_value()
    properly returns an 11 bit value (bits_per(1024)) and doesn't seem to do
    anything funny. I'm still marking this as fixing the above commit to be on
    the safe side.
    
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Reviewed-by: Qais Yousef <qais.yousef@arm.com>
    Acked-by: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Dietmar.Eggemann@arm.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: patrick.bellasi@matbug.net
    Cc: qperret@google.com
    Cc: surenb@google.com
    Cc: tj@kernel.org
    Fixes: 0413d7f33e60 ("sched/uclamp: Always use 'enum uclamp_id' for clamp_id values")
    Link: https://lkml.kernel.org/r/20191115103908.27610-1-valentin.schneider@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 05c282775f21..280a3c735935 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2300,7 +2300,7 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef CONFIG_UCLAMP_TASK
-enum uclamp_id uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);
+unsigned int uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);
 
 static __always_inline
 unsigned int uclamp_util_with(struct rq *rq, unsigned int util,

commit a0e813f26ebcb25c0b5e504498fbd796cca1a4ba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 14:16:00 2019 +0100

    sched/core: Further clarify sched_class::set_next_task()
    
    It turns out there really is something special to the first
    set_next_task() invocation. In specific the 'change' pattern really
    should not cause balance callbacks.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: ktkhai@virtuozzo.com
    Cc: mgorman@suse.de
    Cc: qais.yousef@arm.com
    Cc: qperret@google.com
    Cc: rostedt@goodmis.org
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Fixes: f95d4eaee6d0 ("sched/{rt,deadline}: Fix set_next_task vs pick_next_task")
    Link: https://lkml.kernel.org/r/20191108131909.775434698@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 75d96cce1492..05c282775f21 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1716,7 +1716,7 @@ struct sched_class {
 	struct task_struct *(*pick_next_task)(struct rq *rq);
 
 	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
-	void (*set_next_task)(struct rq *rq, struct task_struct *p);
+	void (*set_next_task)(struct rq *rq, struct task_struct *p, bool first);
 
 #ifdef CONFIG_SMP
 	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
@@ -1768,7 +1768,7 @@ static inline void put_prev_task(struct rq *rq, struct task_struct *prev)
 static inline void set_next_task(struct rq *rq, struct task_struct *next)
 {
 	WARN_ON_ONCE(rq->curr != next);
-	next->sched_class->set_next_task(rq, next);
+	next->sched_class->set_next_task(rq, next, false);
 }
 
 #ifdef CONFIG_SMP

commit 98c2f700edb413e4baa4a0368c5861d96211a775
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 14:15:58 2019 +0100

    sched/core: Simplify sched_class::pick_next_task()
    
    Now that the indirect class call never uses the last two arguments of
    pick_next_task(), remove them.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: ktkhai@virtuozzo.com
    Cc: mgorman@suse.de
    Cc: qais.yousef@arm.com
    Cc: qperret@google.com
    Cc: rostedt@goodmis.org
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: https://lkml.kernel.org/r/20191108131909.660595546@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 66172a337675..75d96cce1492 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1713,20 +1713,8 @@ struct sched_class {
 
 	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);
 
-	/*
-	 * Both @prev and @rf are optional and may be NULL, in which case the
-	 * caller must already have invoked put_prev_task(rq, prev, rf).
-	 *
-	 * Otherwise it is the responsibility of the pick_next_task() to call
-	 * put_prev_task() on the @prev task or something equivalent, IFF it
-	 * returns a next task.
-	 *
-	 * In that case (@rf != NULL) it may return RETRY_TASK when it finds a
-	 * higher prio class has runnable tasks.
-	 */
-	struct task_struct * (*pick_next_task)(struct rq *rq,
-					       struct task_struct *prev,
-					       struct rq_flags *rf);
+	struct task_struct *(*pick_next_task)(struct rq *rq);
+
 	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
 	void (*set_next_task)(struct rq *rq, struct task_struct *p);
 
@@ -1822,7 +1810,7 @@ static inline bool sched_fair_runnable(struct rq *rq)
 }
 
 extern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
-extern struct task_struct *pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
+extern struct task_struct *pick_next_task_idle(struct rq *rq);
 
 #ifdef CONFIG_SMP
 

commit 5d7d605642b28a5911198a405a6072f091bfbee6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 14:15:57 2019 +0100

    sched/core: Optimize pick_next_task()
    
    Ever since we moved the sched_class definitions into their own files,
    the constant expression {fair,idle}_sched_class.pick_next_task() is
    not in fact a compile time constant anymore and results in an indirect
    call (barring LTO).
    
    Fix that by exposing pick_next_task_{fair,idle}() directly, this gets
    rid of the indirect call (and RETPOLINE) on the fast path.
    
    Also remove the unlikely() from the idle case, it is in fact /the/ way
    we select idle -- and that is a very common thing to do.
    
    Performance for will-it-scale/sched_yield improves by 2% (as reported
    by 0-day).
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: ktkhai@virtuozzo.com
    Cc: mgorman@suse.de
    Cc: qais.yousef@arm.com
    Cc: qperret@google.com
    Cc: rostedt@goodmis.org
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: https://lkml.kernel.org/r/20191108131909.603037345@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c8870c5bd7df..66172a337675 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1821,6 +1821,9 @@ static inline bool sched_fair_runnable(struct rq *rq)
 	return rq->cfs.nr_running > 0;
 }
 
+extern struct task_struct *pick_next_task_fair(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
+extern struct task_struct *pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
+
 #ifdef CONFIG_SMP
 
 extern void update_group_capacity(struct sched_domain *sd, int cpu);

commit 6e2df0581f569038719cf2bc2b3baa3fcc83cab4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 11:11:52 2019 +0100

    sched: Fix pick_next_task() vs 'change' pattern race
    
    Commit 67692435c411 ("sched: Rework pick_next_task() slow-path")
    inadvertly introduced a race because it changed a previously
    unexplored dependency between dropping the rq->lock and
    sched_class::put_prev_task().
    
    The comments about dropping rq->lock, in for example
    newidle_balance(), only mentions the task being current and ->on_cpu
    being set. But when we look at the 'change' pattern (in for example
    sched_setnuma()):
    
            queued = task_on_rq_queued(p); /* p->on_rq == TASK_ON_RQ_QUEUED */
            running = task_current(rq, p); /* rq->curr == p */
    
            if (queued)
                    dequeue_task(...);
            if (running)
                    put_prev_task(...);
    
            /* change task properties */
    
            if (queued)
                    enqueue_task(...);
            if (running)
                    set_next_task(...);
    
    It becomes obvious that if we do this after put_prev_task() has
    already been called on @p, things go sideways. This is exactly what
    the commit in question allows to happen when it does:
    
            prev->sched_class->put_prev_task(rq, prev, rf);
            if (!rq->nr_running)
                    newidle_balance(rq, rf);
    
    The newidle_balance() call will drop rq->lock after we've called
    put_prev_task() and that allows the above 'change' pattern to
    interleave and mess up the state.
    
    Furthermore, it turns out we lost the RT-pull when we put the last DL
    task.
    
    Fix both problems by extracting the balancing from put_prev_task() and
    doing a multi-class balance() pass before put_prev_task().
    
    Fixes: 67692435c411 ("sched: Rework pick_next_task() slow-path")
    Reported-by: Quentin Perret <qperret@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Quentin Perret <qperret@google.com>
    Tested-by: Valentin Schneider <valentin.schneider@arm.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0db2c1b3361e..c8870c5bd7df 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1727,10 +1727,11 @@ struct sched_class {
 	struct task_struct * (*pick_next_task)(struct rq *rq,
 					       struct task_struct *prev,
 					       struct rq_flags *rf);
-	void (*put_prev_task)(struct rq *rq, struct task_struct *p, struct rq_flags *rf);
+	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
 	void (*set_next_task)(struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
+	int (*balance)(struct rq *rq, struct task_struct *prev, struct rq_flags *rf);
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
 	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);
 
@@ -1773,7 +1774,7 @@ struct sched_class {
 static inline void put_prev_task(struct rq *rq, struct task_struct *prev)
 {
 	WARN_ON_ONCE(rq->curr != prev);
-	prev->sched_class->put_prev_task(rq, prev, NULL);
+	prev->sched_class->put_prev_task(rq, prev);
 }
 
 static inline void set_next_task(struct rq *rq, struct task_struct *next)
@@ -1787,8 +1788,12 @@ static inline void set_next_task(struct rq *rq, struct task_struct *next)
 #else
 #define sched_class_highest (&dl_sched_class)
 #endif
+
+#define for_class_range(class, _from, _to) \
+	for (class = (_from); class != (_to); class = class->next)
+
 #define for_each_class(class) \
-   for (class = sched_class_highest; class; class = class->next)
+	for_class_range(class, sched_class_highest, NULL)
 
 extern const struct sched_class stop_sched_class;
 extern const struct sched_class dl_sched_class;
@@ -1796,6 +1801,25 @@ extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
 
+static inline bool sched_stop_runnable(struct rq *rq)
+{
+	return rq->stop && task_on_rq_queued(rq->stop);
+}
+
+static inline bool sched_dl_runnable(struct rq *rq)
+{
+	return rq->dl.dl_nr_running > 0;
+}
+
+static inline bool sched_rt_runnable(struct rq *rq)
+{
+	return rq->rt.rt_queued > 0;
+}
+
+static inline bool sched_fair_runnable(struct rq *rq)
+{
+	return rq->cfs.nr_running > 0;
+}
 
 #ifdef CONFIG_SMP
 

commit 227a4aadc75ba22fcb6c4e1c078817b8cbaae4ce
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Thu Sep 19 13:37:02 2019 -0400

    sched/membarrier: Fix p->mm->membarrier_state racy load
    
    The membarrier_state field is located within the mm_struct, which
    is not guaranteed to exist when used from runqueue-lock-free iteration
    on runqueues by the membarrier system call.
    
    Copy the membarrier_state from the mm_struct into the scheduler runqueue
    when the scheduler switches between mm.
    
    When registering membarrier for mm, after setting the registration bit
    in the mm membarrier state, issue a synchronize_rcu() to ensure the
    scheduler observes the change. In order to take care of the case
    where a runqueue keeps executing the target mm without swapping to
    other mm, iterate over each runqueue and issue an IPI to copy the
    membarrier_state from the mm_struct into each runqueue which have the
    same mm which state has just been modified.
    
    Move the mm membarrier_state field closer to pgd in mm_struct to use
    a cache line already touched by the scheduler switch_mm.
    
    The membarrier_execve() (now membarrier_exec_mmap) hook now needs to
    clear the runqueue's membarrier state in addition to clear the mm
    membarrier state, so move its implementation into the scheduler
    membarrier code so it can access the runqueue structure.
    
    Add memory barrier in membarrier_exec_mmap() prior to clearing
    the membarrier state, ensuring memory accesses executed prior to exec
    are not reordered with the stores clearing the membarrier state.
    
    As suggested by Linus, move all membarrier.c RCU read-side locks outside
    of the for each cpu loops.
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Russell King - ARM Linux admin <linux@armlinux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190919173705.2181-5-mathieu.desnoyers@efficios.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b3cb895d14a2..0db2c1b3361e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -911,6 +911,10 @@ struct rq {
 
 	atomic_t		nr_iowait;
 
+#ifdef CONFIG_MEMBARRIER
+	int membarrier_state;
+#endif
+
 #ifdef CONFIG_SMP
 	struct root_domain		*rd;
 	struct sched_domain __rcu	*sd;
@@ -2438,3 +2442,33 @@ static inline bool sched_energy_enabled(void)
 static inline bool sched_energy_enabled(void) { return false; }
 
 #endif /* CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL */
+
+#ifdef CONFIG_MEMBARRIER
+/*
+ * The scheduler provides memory barriers required by membarrier between:
+ * - prior user-space memory accesses and store to rq->membarrier_state,
+ * - store to rq->membarrier_state and following user-space memory accesses.
+ * In the same way it provides those guarantees around store to rq->curr.
+ */
+static inline void membarrier_switch_mm(struct rq *rq,
+					struct mm_struct *prev_mm,
+					struct mm_struct *next_mm)
+{
+	int membarrier_state;
+
+	if (prev_mm == next_mm)
+		return;
+
+	membarrier_state = atomic_read(&next_mm->membarrier_state);
+	if (READ_ONCE(rq->membarrier_state) == membarrier_state)
+		return;
+
+	WRITE_ONCE(rq->membarrier_state, membarrier_state);
+}
+#else
+static inline void membarrier_switch_mm(struct rq *rq,
+					struct mm_struct *prev_mm,
+					struct mm_struct *next_mm)
+{
+}
+#endif

commit 563c4f85f9f0d63b712081d5b4522152cdcb8b6b
Merge: 4adcdcea717c 09c7e8b21d67
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Sep 16 14:04:28 2019 +0200

    Merge branch 'sched/rt' into sched/core, to pick up -rt changes
    
    Pick up the first couple of patches working towards PREEMPT_RT.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 0413d7f33e60751570fd6c179546bde2f7d82dcb
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Thu Aug 22 14:28:11 2019 +0100

    sched/uclamp: Always use 'enum uclamp_id' for clamp_id values
    
    The supported clamp indexes are defined in 'enum clamp_id', however, because
    of the code logic in some of the first utilization clamping series version,
    sometimes we needed to use 'unsigned int' to represent indices.
    
    This is not more required since the final version of the uclamp_* APIs can
    always use the proper enum uclamp_id type.
    
    Fix it with a bulk rename now that we have all the bits merged.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Michal Koutny <mkoutny@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190822132811.31294-7-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5b343112a47b..00ff5b57e9cd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2281,7 +2281,7 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef CONFIG_UCLAMP_TASK
-unsigned int uclamp_eff_value(struct task_struct *p, unsigned int clamp_id);
+enum uclamp_id uclamp_eff_value(struct task_struct *p, enum uclamp_id clamp_id);
 
 static __always_inline
 unsigned int uclamp_util_with(struct rq *rq, unsigned int util,

commit 0b60ba2dd342016e4e717dbaa4ca9af3a43f4434
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Thu Aug 22 14:28:07 2019 +0100

    sched/uclamp: Propagate parent clamps
    
    In order to properly support hierarchical resources control, the cgroup
    delegation model requires that attribute writes from a child group never
    fail but still are locally consistent and constrained based on parent's
    assigned resources. This requires to properly propagate and aggregate
    parent attributes down to its descendants.
    
    Implement this mechanism by adding a new "effective" clamp value for each
    task group. The effective clamp value is defined as the smaller value
    between the clamp value of a group and the effective clamp value of its
    parent. This is the actual clamp value enforced on tasks in a task group.
    
    Since it's possible for a cpu.uclamp.min value to be bigger than the
    cpu.uclamp.max value, ensure local consistency by restricting each
    "protection" (i.e. min utilization) with the corresponding "limit"
    (i.e. max utilization).
    
    Do that at effective clamps propagation to ensure all user-space write
    never fails while still always tracking the most restrictive values.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Michal Koutny <mkoutny@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190822132811.31294-3-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ae1be61fb279..5b343112a47b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -397,6 +397,8 @@ struct task_group {
 	unsigned int		uclamp_pct[UCLAMP_CNT];
 	/* Clamp values requested for a task group */
 	struct uclamp_se	uclamp_req[UCLAMP_CNT];
+	/* Effective clamp values used for a task group */
+	struct uclamp_se	uclamp[UCLAMP_CNT];
 #endif
 
 };

commit 2480c093130f64ac3a410504fa8b3db1fc4b87ce
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Thu Aug 22 14:28:06 2019 +0100

    sched/uclamp: Extend CPU's cgroup controller
    
    The cgroup CPU bandwidth controller allows to assign a specified
    (maximum) bandwidth to the tasks of a group. However this bandwidth is
    defined and enforced only on a temporal base, without considering the
    actual frequency a CPU is running on. Thus, the amount of computation
    completed by a task within an allocated bandwidth can be very different
    depending on the actual frequency the CPU is running that task.
    The amount of computation can be affected also by the specific CPU a
    task is running on, especially when running on asymmetric capacity
    systems like Arm's big.LITTLE.
    
    With the availability of schedutil, the scheduler is now able
    to drive frequency selections based on actual task utilization.
    Moreover, the utilization clamping support provides a mechanism to
    bias the frequency selection operated by schedutil depending on
    constraints assigned to the tasks currently RUNNABLE on a CPU.
    
    Giving the mechanisms described above, it is now possible to extend the
    cpu controller to specify the minimum (or maximum) utilization which
    should be considered for tasks RUNNABLE on a cpu.
    This makes it possible to better defined the actual computational
    power assigned to task groups, thus improving the cgroup CPU bandwidth
    controller which is currently based just on time constraints.
    
    Extend the CPU controller with a couple of new attributes uclamp.{min,max}
    which allow to enforce utilization boosting and capping for all the
    tasks in a group.
    
    Specifically:
    
    - uclamp.min: defines the minimum utilization which should be considered
                  i.e. the RUNNABLE tasks of this group will run at least at a
                  minimum frequency which corresponds to the uclamp.min
                  utilization
    
    - uclamp.max: defines the maximum utilization which should be considered
                  i.e. the RUNNABLE tasks of this group will run up to a
                  maximum frequency which corresponds to the uclamp.max
                  utilization
    
    These attributes:
    
    a) are available only for non-root nodes, both on default and legacy
       hierarchies, while system wide clamps are defined by a generic
       interface which does not depends on cgroups. This system wide
       interface enforces constraints on tasks in the root node.
    
    b) enforce effective constraints at each level of the hierarchy which
       are a restriction of the group requests considering its parent's
       effective constraints. Root group effective constraints are defined
       by the system wide interface.
       This mechanism allows each (non-root) level of the hierarchy to:
       - request whatever clamp values it would like to get
       - effectively get only up to the maximum amount allowed by its parent
    
    c) have higher priority than task-specific clamps, defined via
       sched_setattr(), thus allowing to control and restrict task requests.
    
    Add two new attributes to the cpu controller to collect "requested"
    clamp values. Allow that at each non-root level of the hierarchy.
    Keep it simple by not caring now about "effective" values computation
    and propagation along the hierarchy.
    
    Update sysctl_sched_uclamp_handler() to use the newly introduced
    uclamp_mutex so that we serialize system default updates with cgroup
    relate updates.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Michal Koutny <mkoutny@suse.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190822132811.31294-2-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7111e3a1eeb4..ae1be61fb279 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -391,6 +391,14 @@ struct task_group {
 #endif
 
 	struct cfs_bandwidth	cfs_bandwidth;
+
+#ifdef CONFIG_UCLAMP_TASK_GROUP
+	/* The two decimal precision [%] value requested from user-space */
+	unsigned int		uclamp_pct[UCLAMP_CNT];
+	/* Clamp values requested for a task group */
+	struct uclamp_se	uclamp_req[UCLAMP_CNT];
+#endif
+
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED

commit 67692435c411e5c53a1c588ecca2037aebd81f2e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 29 20:36:44 2019 +0000

    sched: Rework pick_next_task() slow-path
    
    Avoid the RETRY_TASK case in the pick_next_task() slow path.
    
    By doing the put_prev_task() early, we get the rt/deadline pull done,
    and by testing rq->nr_running we know if we need newidle_balance().
    
    This then gives a stable state to pick a task from.
    
    Since the fast-path is fair only; it means the other classes will
    always have pick_next_task(.prev=NULL, .rf=NULL) and we can simplify.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>
    Link: https://lkml.kernel.org/r/aa34d24b36547139248f32a30138791ac6c02bd6.1559129225.git.vpillai@digitalocean.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e085cffb8004..7111e3a1eeb4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1700,12 +1700,15 @@ struct sched_class {
 	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);
 
 	/*
-	 * It is the responsibility of the pick_next_task() method that will
-	 * return the next task to call put_prev_task() on the @prev task or
-	 * something equivalent.
+	 * Both @prev and @rf are optional and may be NULL, in which case the
+	 * caller must already have invoked put_prev_task(rq, prev, rf).
 	 *
-	 * May return RETRY_TASK when it finds a higher prio class has runnable
-	 * tasks.
+	 * Otherwise it is the responsibility of the pick_next_task() to call
+	 * put_prev_task() on the @prev task or something equivalent, IFF it
+	 * returns a next task.
+	 *
+	 * In that case (@rf != NULL) it may return RETRY_TASK when it finds a
+	 * higher prio class has runnable tasks.
 	 */
 	struct task_struct * (*pick_next_task)(struct rq *rq,
 					       struct task_struct *prev,

commit 5f2a45fc9e89e022233085e6f0f352eb6ff770bb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 29 20:36:43 2019 +0000

    sched: Allow put_prev_task() to drop rq->lock
    
    Currently the pick_next_task() loop is convoluted and ugly because of
    how it can drop the rq->lock and needs to restart the picking.
    
    For the RT/Deadline classes, it is put_prev_task() where we do
    balancing, and we could do this before the picking loop. Make this
    possible.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>
    Link: https://lkml.kernel.org/r/e4519f6850477ab7f3d257062796e6425ee4ba7c.1559129225.git.vpillai@digitalocean.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 304d98e712bf..e085cffb8004 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1710,7 +1710,7 @@ struct sched_class {
 	struct task_struct * (*pick_next_task)(struct rq *rq,
 					       struct task_struct *prev,
 					       struct rq_flags *rf);
-	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
+	void (*put_prev_task)(struct rq *rq, struct task_struct *p, struct rq_flags *rf);
 	void (*set_next_task)(struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
@@ -1756,7 +1756,7 @@ struct sched_class {
 static inline void put_prev_task(struct rq *rq, struct task_struct *prev)
 {
 	WARN_ON_ONCE(rq->curr != prev);
-	prev->sched_class->put_prev_task(rq, prev);
+	prev->sched_class->put_prev_task(rq, prev, NULL);
 }
 
 static inline void set_next_task(struct rq *rq, struct task_struct *next)

commit 5ba553eff0c3a7c099b1e29a740277a82c0c3314
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 29 20:36:42 2019 +0000

    sched/fair: Expose newidle_balance()
    
    For pick_next_task_fair() it is the newidle balance that requires
    dropping the rq->lock; provided we do put_prev_task() early, we can
    also detect the condition for doing newidle early.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>
    Link: https://lkml.kernel.org/r/9e3eb1859b946f03d7e500453a885725b68957ba.1559129225.git.vpillai@digitalocean.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f3c50445bf22..304d98e712bf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1445,10 +1445,14 @@ static inline void unregister_sched_domain_sysctl(void)
 }
 #endif
 
+extern int newidle_balance(struct rq *this_rq, struct rq_flags *rf);
+
 #else
 
 static inline void sched_ttwu_pending(void) { }
 
+static inline int newidle_balance(struct rq *this_rq, struct rq_flags *rf) { return 0; }
+
 #endif /* CONFIG_SMP */
 
 #include "stats.h"

commit 03b7fad167efca3b7abbbb39733933f9df56e79c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 29 20:36:41 2019 +0000

    sched: Add task_struct pointer to sched_class::set_curr_task
    
    In preparation of further separating pick_next_task() and
    set_curr_task() we have to pass the actual task into it, while there,
    rename the thing to better pair with put_prev_task().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>
    Link: https://lkml.kernel.org/r/a96d1bcdd716db4a4c5da2fece647a1456c0ed78.1559129225.git.vpillai@digitalocean.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b3449d0dd7f0..f3c50445bf22 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1707,6 +1707,7 @@ struct sched_class {
 					       struct task_struct *prev,
 					       struct rq_flags *rf);
 	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
+	void (*set_next_task)(struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
@@ -1721,7 +1722,6 @@ struct sched_class {
 	void (*rq_offline)(struct rq *rq);
 #endif
 
-	void (*set_curr_task)(struct rq *rq);
 	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
 	void (*task_fork)(struct task_struct *p);
 	void (*task_dead)(struct task_struct *p);
@@ -1755,9 +1755,10 @@ static inline void put_prev_task(struct rq *rq, struct task_struct *prev)
 	prev->sched_class->put_prev_task(rq, prev);
 }
 
-static inline void set_curr_task(struct rq *rq, struct task_struct *curr)
+static inline void set_next_task(struct rq *rq, struct task_struct *next)
 {
-	curr->sched_class->set_curr_task(rq);
+	WARN_ON_ONCE(rq->curr != next);
+	next->sched_class->set_next_task(rq, next);
 }
 
 #ifdef CONFIG_SMP

commit 10e7071b2f491b0fb981717ea0a585c441906ede
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 6 15:13:17 2019 +0200

    sched: Rework CPU hotplug task selection
    
    The CPU hotplug task selection is the only place where we used
    put_prev_task() on a task that is not current. While looking at that,
    it occured to me that we can simplify all that by by using a custom
    pick loop.
    
    Since we don't need to put current, we can do away with the fake task
    too.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ea48aa5daeee..b3449d0dd7f0 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1751,6 +1751,7 @@ struct sched_class {
 
 static inline void put_prev_task(struct rq *rq, struct task_struct *prev)
 {
+	WARN_ON_ONCE(rq->curr != prev);
 	prev->sched_class->put_prev_task(rq, prev);
 }
 

commit de53fd7aedb100f03e5d2231cfce0e4993282425
Author: Dave Chiluk <chiluk+linux@indeed.com>
Date:   Tue Jul 23 11:44:26 2019 -0500

    sched/fair: Fix low cpu usage with high throttling by removing expiration of cpu-local slices
    
    It has been observed, that highly-threaded, non-cpu-bound applications
    running under cpu.cfs_quota_us constraints can hit a high percentage of
    periods throttled while simultaneously not consuming the allocated
    amount of quota. This use case is typical of user-interactive non-cpu
    bound applications, such as those running in kubernetes or mesos when
    run on multiple cpu cores.
    
    This has been root caused to cpu-local run queue being allocated per cpu
    bandwidth slices, and then not fully using that slice within the period.
    At which point the slice and quota expires. This expiration of unused
    slice results in applications not being able to utilize the quota for
    which they are allocated.
    
    The non-expiration of per-cpu slices was recently fixed by
    'commit 512ac999d275 ("sched/fair: Fix bandwidth timer clock drift
    condition")'. Prior to that it appears that this had been broken since
    at least 'commit 51f2176d74ac ("sched/fair: Fix unlocked reads of some
    cfs_b->quota/period")' which was introduced in v3.16-rc1 in 2014. That
    added the following conditional which resulted in slices never being
    expired.
    
    if (cfs_rq->runtime_expires != cfs_b->runtime_expires) {
            /* extend local deadline, drift is bounded above by 2 ticks */
            cfs_rq->runtime_expires += TICK_NSEC;
    
    Because this was broken for nearly 5 years, and has recently been fixed
    and is now being noticed by many users running kubernetes
    (https://github.com/kubernetes/kubernetes/issues/67577) it is my opinion
    that the mechanisms around expiring runtime should be removed
    altogether.
    
    This allows quota already allocated to per-cpu run-queues to live longer
    than the period boundary. This allows threads on runqueues that do not
    use much CPU to continue to use their remaining slice over a longer
    period of time than cpu.cfs_period_us. However, this helps prevent the
    above condition of hitting throttling while also not fully utilizing
    your cpu quota.
    
    This theoretically allows a machine to use slightly more than its
    allotted quota in some periods. This overflow would be bounded by the
    remaining quota left on each per-cpu runqueueu. This is typically no
    more than min_cfs_rq_runtime=1ms per cpu. For CPU bound tasks this will
    change nothing, as they should theoretically fully utilize all of their
    quota in each period. For user-interactive tasks as described above this
    provides a much better user/application experience as their cpu
    utilization will more closely match the amount they requested when they
    hit throttling. This means that cpu limits no longer strictly apply per
    period for non-cpu bound applications, but that they are still accurate
    over longer timeframes.
    
    This greatly improves performance of high-thread-count, non-cpu bound
    applications with low cfs_quota_us allocation on high-core-count
    machines. In the case of an artificial testcase (10ms/100ms of quota on
    80 CPU machine), this commit resulted in almost 30x performance
    improvement, while still maintaining correct cpu quota restrictions.
    That testcase is available at https://github.com/indeedeng/fibtest.
    
    Fixes: 512ac999d275 ("sched/fair: Fix bandwidth timer clock drift condition")
    Signed-off-by: Dave Chiluk <chiluk+linux@indeed.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Phil Auld <pauld@redhat.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: John Hammond <jhammond@indeed.com>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Kyle Anderson <kwa@yelp.com>
    Cc: Gabriel Munos <gmunoz@netflix.com>
    Cc: Peter Oskolkov <posk@posk.io>
    Cc: Cong Wang <xiyou.wangcong@gmail.com>
    Cc: Brendan Gregg <bgregg@netflix.com>
    Link: https://lkml.kernel.org/r/1563900266-19734-2-git-send-email-chiluk+linux@indeed.com

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7583faddba33..ea48aa5daeee 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -335,8 +335,6 @@ struct cfs_bandwidth {
 	u64			quota;
 	u64			runtime;
 	s64			hierarchical_quota;
-	u64			runtime_expires;
-	int			expires_seq;
 
 	u8			idle;
 	u8			period_active;
@@ -557,8 +555,6 @@ struct cfs_rq {
 
 #ifdef CONFIG_CFS_BANDWIDTH
 	int			runtime_enabled;
-	int			expires_seq;
-	u64			runtime_expires;
 	s64			runtime_remaining;
 
 	u64			throttled_clock;

commit c1a280b68d4e6b6db4a65aa7865c22d8789ddf09
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 26 23:19:37 2019 +0200

    sched/preempt: Use CONFIG_PREEMPTION where appropriate
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by
    CONFIG_PREEMPT_RT. Both PREEMPT and PREEMPT_RT require the same
    functionality which today depends on CONFIG_PREEMPT.
    
    Switch the preemption code, scheduler and init task over to use
    CONFIG_PREEMPTION.
    
    That's the first step towards RT in that area. The more complex changes are
    coming separately.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20190726212124.117528401@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 802b1f3405f2..f2ce6ba1c5d5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1943,7 +1943,7 @@ unsigned long arch_scale_freq_capacity(int cpu)
 #endif
 
 #ifdef CONFIG_SMP
-#ifdef CONFIG_PREEMPT
+#ifdef CONFIG_PREEMPTION
 
 static inline void double_rq_lock(struct rq *rq1, struct rq *rq2);
 
@@ -1995,7 +1995,7 @@ static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
 	return ret;
 }
 
-#endif /* CONFIG_PREEMPT */
+#endif /* CONFIG_PREEMPTION */
 
 /*
  * double_lock_balance - lock the busiest runqueue, this_rq is locked already.

commit f9a25f776d780bfa3279f0b6e5f5cf3224997976
Author: Mathieu Poirier <mathieu.poirier@linaro.org>
Date:   Fri Jul 19 15:59:55 2019 +0200

    cpusets: Rebuild root domain deadline accounting information
    
    When the topology of root domains is modified by CPUset or CPUhotplug
    operations information about the current deadline bandwidth held in the
    root domain is lost.
    
    This patch addresses the issue by recalculating the lost deadline
    bandwidth information by circling through the deadline tasks held in
    CPUsets and adding their current load to the root domain they are
    associated with.
    
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Mathieu Poirier <mathieu.poirier@linaro.org>
    Signed-off-by: Juri Lelli <juri.lelli@redhat.com>
    [ Various additional modifications. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bristot@redhat.com
    Cc: claudio@evidence.eu.com
    Cc: lizefan@huawei.com
    Cc: longman@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: rostedt@goodmis.org
    Cc: tj@kernel.org
    Cc: tommaso.cucinotta@santannapisa.it
    Link: https://lkml.kernel.org/r/20190719140000.31694-4-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 16126efd14ed..7583faddba33 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -778,9 +778,6 @@ struct root_domain {
 	struct perf_domain __rcu *pd;
 };
 
-extern struct root_domain def_root_domain;
-extern struct mutex sched_domains_mutex;
-
 extern void init_defrootdomain(void);
 extern int sched_init_domains(const struct cpumask *cpu_map);
 extern void rq_attach_root(struct rq *rq, struct root_domain *rd);

commit e0e8d4911ed2695b12c3a01c15634000ede9bc73
Author: Wanpeng Li <wanpengli@tencent.com>
Date:   Fri Jun 28 16:51:41 2019 +0800

    sched/isolation: Prefer housekeeping CPU in local node
    
    In real product setup, there will be houseeking CPUs in each nodes, it
    is prefer to do housekeeping from local node, fallback to global online
    cpumask if failed to find houseeking CPU from local node.
    
    Signed-off-by: Wanpeng Li <wanpengli@tencent.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/1561711901-4755-2-git-send-email-wanpengli@tencent.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index aaca0e743776..16126efd14ed 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1262,16 +1262,18 @@ enum numa_topology_type {
 extern enum numa_topology_type sched_numa_topology_type;
 extern int sched_max_numa_distance;
 extern bool find_numa_distance(int distance);
-#endif
-
-#ifdef CONFIG_NUMA
 extern void sched_init_numa(void);
 extern void sched_domains_numa_masks_set(unsigned int cpu);
 extern void sched_domains_numa_masks_clear(unsigned int cpu);
+extern int sched_numa_find_closest(const struct cpumask *cpus, int cpu);
 #else
 static inline void sched_init_numa(void) { }
 static inline void sched_domains_numa_masks_set(unsigned int cpu) { }
 static inline void sched_domains_numa_masks_clear(unsigned int cpu) { }
+static inline int sched_numa_find_closest(const struct cpumask *cpus, int cpu)
+{
+	return nr_cpu_ids;
+}
 #endif
 
 #ifdef CONFIG_NUMA_BALANCING

commit 43e9f7f231e40e4534fc3a735da152911a085c16
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Wed Jun 26 10:36:29 2019 +0530

    sched/fair: Start tracking SCHED_IDLE tasks count in cfs_rq
    
    Track how many tasks are present with SCHED_IDLE policy in each cfs_rq.
    This will be used by later commits.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: chris.redpath@arm.com
    Cc: quentin.perret@linaro.org
    Cc: songliubraving@fb.com
    Cc: steven.sistare@oracle.com
    Cc: subhra.mazumdar@oracle.com
    Cc: tkjos@google.com
    Link: https://lkml.kernel.org/r/0d3cdc427fc68808ad5bccc40e86ed0bf9da8bb4.1561523542.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 802b1f3405f2..aaca0e743776 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -483,7 +483,8 @@ struct cfs_rq {
 	struct load_weight	load;
 	unsigned long		runnable_weight;
 	unsigned int		nr_running;
-	unsigned int		h_nr_running;
+	unsigned int		h_nr_running;      /* SCHED_{NORMAL,BATCH,IDLE} */
+	unsigned int		idle_h_nr_running; /* SCHED_IDLE */
 
 	u64			exec_clock;
 	u64			min_vruntime;

commit af24bde8df2029f067dc46aff0393c8f18ff6e2f
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:12 2019 +0100

    sched/uclamp: Add uclamp support to energy_compute()
    
    The Energy Aware Scheduler (EAS) estimates the energy impact of waking
    up a task on a given CPU. This estimation is based on:
    
     a) an (active) power consumption defined for each CPU frequency
     b) an estimation of which frequency will be used on each CPU
     c) an estimation of the busy time (utilization) of each CPU
    
    Utilization clamping can affect both b) and c).
    
    A CPU is expected to run:
    
     - on an higher than required frequency, but for a shorter time, in case
       its estimated utilization will be smaller than the minimum utilization
       enforced by uclamp
     - on a smaller than required frequency, but for a longer time, in case
       its estimated utilization is bigger than the maximum utilization
       enforced by uclamp
    
    While compute_energy() already accounts clamping effects on busy time,
    the clamping effects on frequency selection are currently ignored.
    
    Fix it by considering how CPU clamp values will be affected by a
    task waking up and being RUNNABLE on that CPU.
    
    Do that by refactoring schedutil_freq_util() to take an additional
    task_struct* which allows EAS to evaluate the impact on clamp values of
    a task being eventually queued in a CPU. Clamp values are applied to the
    RT+CFS utilization only when a FREQUENCY_UTIL is required by
    compute_energy().
    
    Do note that switching from ENERGY_UTIL to FREQUENCY_UTIL in the
    computation of the cpu_util signal implies that we are more likely to
    estimate the highest OPP when a RT task is running in another CPU of
    the same performance domain. This can have an impact on energy
    estimation but:
    
     - it's not easy to say which approach is better, since it depends on
       the use case
     - the original approach could still be obtained by setting a smaller
       task-specific util_min whenever required
    
    Since we are at that:
    
     - rename schedutil_freq_util() into schedutil_cpu_util(),
       since it's not only used for frequency selection.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-12-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1783f6b4c2e0..802b1f3405f2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2322,7 +2322,6 @@ static inline unsigned long capacity_orig_of(int cpu)
 }
 #endif
 
-#ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
 /**
  * enum schedutil_type - CPU utilization type
  * @FREQUENCY_UTIL:	Utilization used to select frequency
@@ -2338,15 +2337,11 @@ enum schedutil_type {
 	ENERGY_UTIL,
 };
 
-unsigned long schedutil_freq_util(int cpu, unsigned long util_cfs,
-				  unsigned long max, enum schedutil_type type);
+#ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
 
-static inline unsigned long schedutil_energy_util(int cpu, unsigned long cfs)
-{
-	unsigned long max = arch_scale_cpu_capacity(cpu);
-
-	return schedutil_freq_util(cpu, cfs, max, ENERGY_UTIL);
-}
+unsigned long schedutil_cpu_util(int cpu, unsigned long util_cfs,
+				 unsigned long max, enum schedutil_type type,
+				 struct task_struct *p);
 
 static inline unsigned long cpu_bw_dl(struct rq *rq)
 {
@@ -2375,11 +2370,13 @@ static inline unsigned long cpu_util_rt(struct rq *rq)
 	return READ_ONCE(rq->avg_rt.util_avg);
 }
 #else /* CONFIG_CPU_FREQ_GOV_SCHEDUTIL */
-static inline unsigned long schedutil_energy_util(int cpu, unsigned long cfs)
+static inline unsigned long schedutil_cpu_util(int cpu, unsigned long util_cfs,
+				 unsigned long max, enum schedutil_type type,
+				 struct task_struct *p)
 {
-	return cfs;
+	return 0;
 }
-#endif
+#endif /* CONFIG_CPU_FREQ_GOV_SCHEDUTIL */
 
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ
 static inline unsigned long cpu_util_irq(struct rq *rq)

commit 9d20ad7dfc9a5cc64e33d725902d3863d350a66a
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:11 2019 +0100

    sched/uclamp: Add uclamp_util_with()
    
    So far uclamp_util() allows to clamp a specified utilization considering
    the clamp values requested by RUNNABLE tasks in a CPU. For the Energy
    Aware Scheduler (EAS) it is interesting to test how clamp values will
    change when a task is becoming RUNNABLE on a given CPU.
    For example, EAS is interested in comparing the energy impact of
    different scheduling decisions and the clamp values can play a role on
    that.
    
    Add uclamp_util_with() which allows to clamp a given utilization by
    considering the possible impact on CPU clamp values of a specified task.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-11-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9b0c77a99346..1783f6b4c2e0 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2266,11 +2266,20 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef CONFIG_UCLAMP_TASK
-static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
+unsigned int uclamp_eff_value(struct task_struct *p, unsigned int clamp_id);
+
+static __always_inline
+unsigned int uclamp_util_with(struct rq *rq, unsigned int util,
+			      struct task_struct *p)
 {
 	unsigned int min_util = READ_ONCE(rq->uclamp[UCLAMP_MIN].value);
 	unsigned int max_util = READ_ONCE(rq->uclamp[UCLAMP_MAX].value);
 
+	if (p) {
+		min_util = max(min_util, uclamp_eff_value(p, UCLAMP_MIN));
+		max_util = max(max_util, uclamp_eff_value(p, UCLAMP_MAX));
+	}
+
 	/*
 	 * Since CPU's {min,max}_util clamps are MAX aggregated considering
 	 * RUNNABLE tasks with _different_ clamps, we can end up with an
@@ -2281,7 +2290,17 @@ static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
 
 	return clamp(util, min_util, max_util);
 }
+
+static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
+{
+	return uclamp_util_with(rq, util, NULL);
+}
 #else /* CONFIG_UCLAMP_TASK */
+static inline unsigned int uclamp_util_with(struct rq *rq, unsigned int util,
+					    struct task_struct *p)
+{
+	return util;
+}
 static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
 {
 	return util;

commit 982d9cdc22c9f6df5ad790caa229ff74fb1d95e7
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:10 2019 +0100

    sched/cpufreq, sched/uclamp: Add clamps for FAIR and RT tasks
    
    Each time a frequency update is required via schedutil, a frequency is
    selected to (possibly) satisfy the utilization reported by each
    scheduling class and irqs. However, when utilization clamping is in use,
    the frequency selection should consider userspace utilization clamping
    hints.  This will allow, for example, to:
    
     - boost tasks which are directly affecting the user experience
       by running them at least at a minimum "requested" frequency
    
     - cap low priority tasks not directly affecting the user experience
       by running them only up to a maximum "allowed" frequency
    
    These constraints are meant to support a per-task based tuning of the
    frequency selection thus supporting a fine grained definition of
    performance boosting vs energy saving strategies in kernel space.
    
    Add support to clamp the utilization of RUNNABLE FAIR and RT tasks
    within the boundaries defined by their aggregated utilization clamp
    constraints.
    
    Do that by considering the max(min_util, max_util) to give boosted tasks
    the performance they need even when they happen to be co-scheduled with
    other capped tasks.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-10-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0d2ba8bb2cb3..9b0c77a99346 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2265,6 +2265,29 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
 static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
+#ifdef CONFIG_UCLAMP_TASK
+static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
+{
+	unsigned int min_util = READ_ONCE(rq->uclamp[UCLAMP_MIN].value);
+	unsigned int max_util = READ_ONCE(rq->uclamp[UCLAMP_MAX].value);
+
+	/*
+	 * Since CPU's {min,max}_util clamps are MAX aggregated considering
+	 * RUNNABLE tasks with _different_ clamps, we can end up with an
+	 * inversion. Fix it now when the clamps are applied.
+	 */
+	if (unlikely(min_util >= max_util))
+		return min_util;
+
+	return clamp(util, min_util, max_util);
+}
+#else /* CONFIG_UCLAMP_TASK */
+static inline unsigned int uclamp_util(struct rq *rq, unsigned int util)
+{
+	return util;
+}
+#endif /* CONFIG_UCLAMP_TASK */
+
 #ifdef arch_scale_freq_capacity
 # ifndef arch_scale_freq_invariant
 #  define arch_scale_freq_invariant()	true

commit e496187da71070687b55ff455e7d8d7d7f0ae0b9
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:04 2019 +0100

    sched/uclamp: Enforce last task's UCLAMP_MAX
    
    When a task sleeps it removes its max utilization clamp from its CPU.
    However, the blocked utilization on that CPU can be higher than the max
    clamp value enforced while the task was running. This allows undesired
    CPU frequency increases while a CPU is idle, for example, when another
    CPU on the same frequency domain triggers a frequency update, since
    schedutil can now see the full not clamped blocked utilization of the
    idle CPU.
    
    Fix this by using:
    
      uclamp_rq_dec_id(p, rq, UCLAMP_MAX)
        uclamp_rq_max_value(rq, UCLAMP_MAX, clamp_value)
    
    to detect when a CPU has no more RUNNABLE clamped tasks and to flag this
    condition.
    
    Don't track any minimum utilization clamps since an idle CPU never
    requires a minimum frequency. The decay of the blocked utilization is
    good enough to reduce the CPU frequency.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-4-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index cecc6baaba93..0d2ba8bb2cb3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -870,6 +870,8 @@ struct rq {
 #ifdef CONFIG_UCLAMP_TASK
 	/* Utilization clamp values based on CPU's RUNNABLE tasks */
 	struct uclamp_rq	uclamp[UCLAMP_CNT] ____cacheline_aligned;
+	unsigned int		uclamp_flags;
+#define UCLAMP_FLAG_IDLE 0x01
 #endif
 
 	struct cfs_rq		cfs;

commit 69842cba9ace84849bb9b8edcdf2cefccd97901c
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Jun 21 09:42:02 2019 +0100

    sched/uclamp: Add CPU's clamp buckets refcounting
    
    Utilization clamping allows to clamp the CPU's utilization within a
    [util_min, util_max] range, depending on the set of RUNNABLE tasks on
    that CPU. Each task references two "clamp buckets" defining its minimum
    and maximum (util_{min,max}) utilization "clamp values". A CPU's clamp
    bucket is active if there is at least one RUNNABLE tasks enqueued on
    that CPU and refcounting that bucket.
    
    When a task is {en,de}queued {on,from} a rq, the set of active clamp
    buckets on that CPU can change. If the set of active clamp buckets
    changes for a CPU a new "aggregated" clamp value is computed for that
    CPU. This is because each clamp bucket enforces a different utilization
    clamp value.
    
    Clamp values are always MAX aggregated for both util_min and util_max.
    This ensures that no task can affect the performance of other
    co-scheduled tasks which are more boosted (i.e. with higher util_min
    clamp) or less capped (i.e. with higher util_max clamp).
    
    A task has:
       task_struct::uclamp[clamp_id]::bucket_id
    to track the "bucket index" of the CPU's clamp bucket it refcounts while
    enqueued, for each clamp index (clamp_id).
    
    A runqueue has:
       rq::uclamp[clamp_id]::bucket[bucket_id].tasks
    to track how many RUNNABLE tasks on that CPU refcount each
    clamp bucket (bucket_id) of a clamp index (clamp_id).
    It also has a:
       rq::uclamp[clamp_id]::bucket[bucket_id].value
    to track the clamp value of each clamp bucket (bucket_id) of a clamp
    index (clamp_id).
    
    The rq::uclamp::bucket[clamp_id][] array is scanned every time it's
    needed to find a new MAX aggregated clamp value for a clamp_id. This
    operation is required only when it's dequeued the last task of a clamp
    bucket tracking the current MAX aggregated clamp value. In this case,
    the CPU is either entering IDLE or going to schedule a less boosted or
    more clamped task.
    The expected number of different clamp values configured at build time
    is small enough to fit the full unordered array into a single cache
    line, for configurations of up to 7 buckets.
    
    Add to struct rq the basic data structures required to refcount the
    number of RUNNABLE tasks for each clamp bucket. Add also the max
    aggregation required to update the rq's clamp value at each
    enqueue/dequeue event.
    
    Use a simple linear mapping of clamp values into clamp buckets.
    Pre-compute and cache bucket_id to avoid integer divisions at
    enqueue/dequeue time.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alessio Balsini <balsini@android.com>
    Cc: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Suren Baghdasaryan <surenb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: https://lkml.kernel.org/r/20190621084217.8167-2-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e58ab597ec88..cecc6baaba93 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -791,6 +791,48 @@ extern void rto_push_irq_work_func(struct irq_work *work);
 #endif
 #endif /* CONFIG_SMP */
 
+#ifdef CONFIG_UCLAMP_TASK
+/*
+ * struct uclamp_bucket - Utilization clamp bucket
+ * @value: utilization clamp value for tasks on this clamp bucket
+ * @tasks: number of RUNNABLE tasks on this clamp bucket
+ *
+ * Keep track of how many tasks are RUNNABLE for a given utilization
+ * clamp value.
+ */
+struct uclamp_bucket {
+	unsigned long value : bits_per(SCHED_CAPACITY_SCALE);
+	unsigned long tasks : BITS_PER_LONG - bits_per(SCHED_CAPACITY_SCALE);
+};
+
+/*
+ * struct uclamp_rq - rq's utilization clamp
+ * @value: currently active clamp values for a rq
+ * @bucket: utilization clamp buckets affecting a rq
+ *
+ * Keep track of RUNNABLE tasks on a rq to aggregate their clamp values.
+ * A clamp value is affecting a rq when there is at least one task RUNNABLE
+ * (or actually running) with that value.
+ *
+ * There are up to UCLAMP_CNT possible different clamp values, currently there
+ * are only two: minimum utilization and maximum utilization.
+ *
+ * All utilization clamping values are MAX aggregated, since:
+ * - for util_min: we want to run the CPU at least at the max of the minimum
+ *   utilization required by its currently RUNNABLE tasks.
+ * - for util_max: we want to allow the CPU to run up to the max of the
+ *   maximum utilization allowed by its currently RUNNABLE tasks.
+ *
+ * Since on each system we expect only a limited number of different
+ * utilization clamp values (UCLAMP_BUCKETS), use a simple array to track
+ * the metrics required to compute all the per-rq utilization clamp values.
+ */
+struct uclamp_rq {
+	unsigned int value;
+	struct uclamp_bucket bucket[UCLAMP_BUCKETS];
+};
+#endif /* CONFIG_UCLAMP_TASK */
+
 /*
  * This is the main, per-CPU runqueue data structure.
  *
@@ -825,6 +867,11 @@ struct rq {
 	unsigned long		nr_load_updates;
 	u64			nr_switches;
 
+#ifdef CONFIG_UCLAMP_TASK
+	/* Utilization clamp values based on CPU's RUNNABLE tasks */
+	struct uclamp_rq	uclamp[UCLAMP_CNT] ____cacheline_aligned;
+#endif
+
 	struct cfs_rq		cfs;
 	struct rt_rq		rt;
 	struct dl_rq		dl;
@@ -1639,6 +1686,10 @@ extern const u32		sched_prio_to_wmult[40];
 struct sched_class {
 	const struct sched_class *next;
 
+#ifdef CONFIG_UCLAMP_TASK
+	int uclamp_enabled;
+#endif
+
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
 	void (*yield_task)   (struct rq *rq);

commit 8ec59c0f5f4966f89f4e3e3cab81710c7fa959d0
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Jun 17 17:00:17 2019 +0200

    sched/topology: Remove unused 'sd' parameter from arch_scale_cpu_capacity()
    
    The 'struct sched_domain *sd' parameter to arch_scale_cpu_capacity() is
    unused since commit:
    
      765d0af19f5f ("sched/topology: Remove the ::smt_gain field from 'struct sched_domain'")
    
    Remove it.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: gregkh@linuxfoundation.org
    Cc: linux@armlinux.org.uk
    Cc: quentin.perret@arm.com
    Cc: rafael@kernel.org
    Link: https://lkml.kernel.org/r/1560783617-5827-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b08dee29ef5e..e58ab597ec88 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2248,7 +2248,7 @@ unsigned long schedutil_freq_util(int cpu, unsigned long util_cfs,
 
 static inline unsigned long schedutil_energy_util(int cpu, unsigned long cfs)
 {
-	unsigned long max = arch_scale_cpu_capacity(NULL, cpu);
+	unsigned long max = arch_scale_cpu_capacity(cpu);
 
 	return schedutil_freq_util(cpu, cfs, max, ENERGY_UTIL);
 }

commit 66567fcbaecac455caa1b13643155d686b51ce63
Author: bsegall@google.com <bsegall@google.com>
Date:   Thu Jun 6 10:21:01 2019 -0700

    sched/fair: Don't push cfs_bandwith slack timers forward
    
    When a cfs_rq sleeps and returns its quota, we delay for 5ms before
    waking any throttled cfs_rqs to coalesce with other cfs_rqs going to
    sleep, as this has to be done outside of the rq lock we hold.
    
    The current code waits for 5ms without any sleeps, instead of waiting
    for 5ms from the first sleep, which can delay the unthrottle more than
    we want. Switch this around so that we can't push this forward forever.
    
    This requires an extra flag rather than using hrtimer_active, since we
    need to start a new timer if the current one is in the process of
    finishing.
    
    Signed-off-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Xunlei Pang <xlpang@linux.alibaba.com>
    Acked-by: Phil Auld <pauld@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/xm26a7euy6iq.fsf_-_@bsegall-linux.svl.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 607859a18b2a..b08dee29ef5e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -338,8 +338,10 @@ struct cfs_bandwidth {
 	u64			runtime_expires;
 	int			expires_seq;
 
-	short			idle;
-	short			period_active;
+	u8			idle;
+	u8			period_active;
+	u8			distribute_running;
+	u8			slack_started;
 	struct hrtimer		period_timer;
 	struct hrtimer		slack_timer;
 	struct list_head	throttled_cfs_rq;
@@ -348,8 +350,6 @@ struct cfs_bandwidth {
 	int			nr_periods;
 	int			nr_throttled;
 	u64			throttled_time;
-
-	bool                    distribute_running;
 #endif
 };
 

commit 55627e3cd22c315c4a02fe3bbbb7234ec439cb1d
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Mon May 27 07:21:13 2019 +0100

    sched/core: Remove rq->cpu_load[]
    
    The per rq load array values also disappear from the cpu#X sections in
    /proc/sched_debug.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@surriel.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Patrick Bellasi <patrick.bellasi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: https://lkml.kernel.org/r/20190527062116.11512-5-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3750b5e53792..607859a18b2a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -812,8 +812,6 @@ struct rq {
 	unsigned int		nr_preferred_running;
 	unsigned int		numa_migrate_on;
 #endif
-	#define CPU_LOAD_IDX_MAX 5
-	unsigned long		cpu_load[CPU_LOAD_IDX_MAX];
 #ifdef CONFIG_NO_HZ_COMMON
 #ifdef CONFIG_SMP
 	unsigned long		last_load_update_tick;

commit 5e83eafbfd3b351537c0d74467fc43e8a88f4ae4
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Mon May 27 07:21:10 2019 +0100

    sched/fair: Remove the rq->cpu_load[] update code
    
    With LB_BIAS disabled, there is no need to update the rq->cpu_load[idx]
    any more.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@surriel.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Patrick Bellasi <patrick.bellasi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: https://lkml.kernel.org/r/20190527062116.11512-2-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c308410675ed..3750b5e53792 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -96,12 +96,6 @@ extern atomic_long_t calc_load_tasks;
 extern void calc_global_load_tick(struct rq *this_rq);
 extern long calc_load_fold_active(struct rq *this_rq, long adjust);
 
-#ifdef CONFIG_SMP
-extern void cpu_load_update_active(struct rq *this_rq);
-#else
-static inline void cpu_load_update_active(struct rq *this_rq) { }
-#endif
-
 /*
  * Helpers for converting nanosecond timing to jiffy resolution
  */

commit f2bedc4705659216bd60948029ad8dfedf923ad9
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Wed Apr 24 09:45:56 2019 +0100

    sched/fair: Remove rq->load
    
    The CFS class is the only one maintaining and using the CPU wide load
    (rq->load(.weight)). The last use case of the CPU wide load in CFS's
    set_next_entity() can be replaced by using the load of the CFS class
    (rq->cfs.load(.weight)) instead.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190424084556.604-1-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b52ed1ada0be..c308410675ed 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -830,8 +830,6 @@ struct rq {
 	atomic_t nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 
-	/* capture load from *all* tasks on this CPU: */
-	struct load_weight	load;
 	unsigned long		nr_load_updates;
 	u64			nr_switches;
 

commit 7ba7319f9e3898101bff5d63cbae5a6cc174c8c9
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Wed Mar 20 20:34:26 2019 -0400

    sched/core: Annotate perf_domain pointer with __rcu
    
    This fixes the following sparse errors in sched/fair.c:
    
      fair.c:6506:14: error: incompatible types in comparison expression
      fair.c:8642:21: error: incompatible types in comparison expression
    
    Using __rcu will also help sparse catch any future bugs.
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [ From an RCU perspective. ]
    Reviewed-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: keescook@chromium.org
    Cc: kernel-hardening@lists.openwall.com
    Cc: kernel-team@android.com
    Link: https://lkml.kernel.org/r/20190321003426.160260-5-joel@joelfernandes.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2b452d68ab2e..b52ed1ada0be 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -780,7 +780,7 @@ struct root_domain {
 	 * NULL-terminated list of performance domains intersecting with the
 	 * CPUs of the rd. Protected by RCU.
 	 */
-	struct perf_domain	*pd;
+	struct perf_domain __rcu *pd;
 };
 
 extern struct root_domain def_root_domain;

commit 994aeb7a93e43d28f6074195ccb03a384342e1bf
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Wed Mar 20 20:34:24 2019 -0400

    sched_domain: Annotate RCU pointers properly
    
    The scheduler uses RCU API in various places to access sched_domain
    pointers. These cause sparse errors as below.
    
    Many new errors show up because of an annotation check I added to
    rcu_assign_pointer(). Let us annotate the pointers correctly which also
    will help sparse catch any potential future bugs.
    
    This fixes the following sparse errors:
    
      rt.c:1681:9: error: incompatible types in comparison expression
      deadline.c:1904:9: error: incompatible types in comparison expression
      core.c:519:9: error: incompatible types in comparison expression
      core.c:1634:17: error: incompatible types in comparison expression
      fair.c:6193:14: error: incompatible types in comparison expression
      fair.c:9883:22: error: incompatible types in comparison expression
      fair.c:9897:9: error: incompatible types in comparison expression
      sched.h:1287:9: error: incompatible types in comparison expression
      topology.c:612:9: error: incompatible types in comparison expression
      topology.c:615:9: error: incompatible types in comparison expression
      sched.h:1300:9: error: incompatible types in comparison expression
      topology.c:618:9: error: incompatible types in comparison expression
      sched.h:1287:9: error: incompatible types in comparison expression
      topology.c:621:9: error: incompatible types in comparison expression
      sched.h:1300:9: error: incompatible types in comparison expression
      topology.c:624:9: error: incompatible types in comparison expression
      topology.c:671:9: error: incompatible types in comparison expression
      stats.c:45:17: error: incompatible types in comparison expression
      fair.c:5998:15: error: incompatible types in comparison expression
      fair.c:5989:15: error: incompatible types in comparison expression
      fair.c:5998:15: error: incompatible types in comparison expression
      fair.c:5989:15: error: incompatible types in comparison expression
      fair.c:6120:19: error: incompatible types in comparison expression
      fair.c:6506:14: error: incompatible types in comparison expression
      fair.c:6515:14: error: incompatible types in comparison expression
      fair.c:6623:9: error: incompatible types in comparison expression
      fair.c:5970:17: error: incompatible types in comparison expression
      fair.c:8642:21: error: incompatible types in comparison expression
      fair.c:9253:9: error: incompatible types in comparison expression
      fair.c:9331:9: error: incompatible types in comparison expression
      fair.c:9519:15: error: incompatible types in comparison expression
      fair.c:9533:14: error: incompatible types in comparison expression
      fair.c:9542:14: error: incompatible types in comparison expression
      fair.c:9567:14: error: incompatible types in comparison expression
      fair.c:9597:14: error: incompatible types in comparison expression
      fair.c:9421:16: error: incompatible types in comparison expression
      fair.c:9421:16: error: incompatible types in comparison expression
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [ From an RCU perspective. ]
    Reviewed-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: keescook@chromium.org
    Cc: kernel-hardening@lists.openwall.com
    Cc: kernel-team@android.com
    Link: https://lkml.kernel.org/r/20190321003426.160260-3-joel@joelfernandes.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 713715dd00cf..2b452d68ab2e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -869,8 +869,8 @@ struct rq {
 	atomic_t		nr_iowait;
 
 #ifdef CONFIG_SMP
-	struct root_domain	*rd;
-	struct sched_domain	*sd;
+	struct root_domain		*rd;
+	struct sched_domain __rcu	*sd;
 
 	unsigned long		cpu_capacity;
 	unsigned long		cpu_capacity_orig;
@@ -1324,13 +1324,13 @@ static inline struct sched_domain *lowest_flag_domain(int cpu, int flag)
 	return sd;
 }
 
-DECLARE_PER_CPU(struct sched_domain *, sd_llc);
+DECLARE_PER_CPU(struct sched_domain __rcu *, sd_llc);
 DECLARE_PER_CPU(int, sd_llc_size);
 DECLARE_PER_CPU(int, sd_llc_id);
-DECLARE_PER_CPU(struct sched_domain_shared *, sd_llc_shared);
-DECLARE_PER_CPU(struct sched_domain *, sd_numa);
-DECLARE_PER_CPU(struct sched_domain *, sd_asym_packing);
-DECLARE_PER_CPU(struct sched_domain *, sd_asym_cpucapacity);
+DECLARE_PER_CPU(struct sched_domain_shared __rcu *, sd_llc_shared);
+DECLARE_PER_CPU(struct sched_domain __rcu *, sd_numa);
+DECLARE_PER_CPU(struct sched_domain __rcu *, sd_asym_packing);
+DECLARE_PER_CPU(struct sched_domain __rcu *, sd_asym_cpucapacity);
 extern struct static_key_false sched_asym_cpucapacity;
 
 struct sched_group_capacity {

commit b10abd0a8859493a93c6b8020f2be2587557749d
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Wed Mar 20 20:34:23 2019 -0400

    sched/cpufreq: Annotate cpufreq_update_util_data pointer with __rcu
    
    Recently I added an RCU annotation check to rcu_assign_pointer(). All
    pointers assigned to RCU protected data are to be annotated with __rcu
    inorder to be able to use rcu_assign_pointer() similar to checks in
    other RCU APIs.
    
    This resulted in a sparse error:
    
      kernel//sched/cpufreq.c:41:9: sparse: error: incompatible types in comparison expression (different address spaces)
    
    Fix this by annotating cpufreq_update_util_data pointer with __rcu. This
    will also help sparse catch any future RCU misuage bugs.
    
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    [ From an RCU perspective. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Josh Triplett <josh@joshtriplett.org>
    Cc: Lai Jiangshan <jiangshanlai@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luc Van Oostenryck <luc.vanoostenryck@gmail.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: keescook@chromium.org
    Cc: kernel-hardening@lists.openwall.com
    Cc: kernel-team@android.com
    Link: https://lkml.kernel.org/r/20190321003426.160260-2-joel@joelfernandes.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index efa686eeff26..713715dd00cf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2185,7 +2185,7 @@ static inline u64 irq_time_read(int cpu)
 #endif /* CONFIG_IRQ_TIME_ACCOUNTING */
 
 #ifdef CONFIG_CPU_FREQ
-DECLARE_PER_CPU(struct update_util_data *, cpufreq_update_util_data);
+DECLARE_PER_CPU(struct update_util_data __rcu *, cpufreq_update_util_data);
 
 /**
  * cpufreq_update_util - Take a note about CPU utilization changes.

commit 45802da05e666a81b421422d3e302930c0e24e77
Merge: 203b6609e0ed ad01423aedaa
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 6 08:14:05 2019 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - refcount conversions
    
       - Solve the rq->leaf_cfs_rq_list can of worms for real.
    
       - improve power-aware scheduling
    
       - add sysctl knob for Energy Aware Scheduling
    
       - documentation updates
    
       - misc other changes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (34 commits)
      kthread: Do not use TIMER_IRQSAFE
      kthread: Convert worker lock to raw spinlock
      sched/fair: Use non-atomic cpumask_{set,clear}_cpu()
      sched/fair: Remove unused 'sd' parameter from select_idle_smt()
      sched/wait: Use freezable_schedule() when possible
      sched/fair: Prune, fix and simplify the nohz_balancer_kick() comment block
      sched/fair: Explain LLC nohz kick condition
      sched/fair: Simplify nohz_balancer_kick()
      sched/topology: Fix percpu data types in struct sd_data & struct s_data
      sched/fair: Simplify post_init_entity_util_avg() by calling it with a task_struct pointer argument
      sched/fair: Fix O(nr_cgroups) in the load balancing path
      sched/fair: Optimize update_blocked_averages()
      sched/fair: Fix insertion in rq->leaf_cfs_rq_list
      sched/fair: Add tmp_alone_branch assertion
      sched/core: Use READ_ONCE()/WRITE_ONCE() in move_queued_task()/task_rq_lock()
      sched/debug: Initialize sd_sysctl_cpus if !CONFIG_CPUMASK_OFFSTACK
      sched/pelt: Skip updating util_est when utilization is higher than CPU's capacity
      sched/fair: Update scale invariance of PELT
      sched/fair: Move the rq_of() helper function
      sched/core: Convert task_struct.stack_refcount to refcount_t
      ...

commit d0fe0b9c45c144e4ac60cf7f07f7e8ae86d3536d
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Tue Jan 22 16:25:01 2019 +0000

    sched/fair: Simplify post_init_entity_util_avg() by calling it with a task_struct pointer argument
    
    Since commit:
    
      d03266910a53 ("sched/fair: Fix task group initialization")
    
    the utilization of a sched entity representing a task group is no longer
    initialized to any other value than 0. So post_init_entity_util_avg() is
    only used for tasks, not for sched_entities.
    
    Make this clear by calling it with a task_struct pointer argument which
    also eliminates the entity_is_task(se) if condition in the fork path and
    get rid of the stale comment in remove_entity_load_avg() accordingly.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Patrick Bellasi <patrick.bellasi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: https://lkml.kernel.org/r/20190122162501.12000-1-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c688ef5012e5..71208b67e58a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1800,7 +1800,7 @@ extern void init_dl_rq_bw_ratio(struct dl_rq *dl_rq);
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);
-extern void post_init_entity_util_avg(struct sched_entity *se);
+extern void post_init_entity_util_avg(struct task_struct *p);
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool sched_can_stop_tick(struct rq *rq);

commit c546951d9c9300065bad253ecdf1ac59ce9d06c8
Author: Andrea Parri <andrea.parri@amarulasolutions.com>
Date:   Mon Jan 21 16:52:40 2019 +0100

    sched/core: Use READ_ONCE()/WRITE_ONCE() in move_queued_task()/task_rq_lock()
    
    move_queued_task() synchronizes with task_rq_lock() as follows:
    
            move_queued_task()              task_rq_lock()
    
            [S] ->on_rq = MIGRATING         [L] rq = task_rq()
            WMB (__set_task_cpu())          ACQUIRE (rq->lock);
            [S] ->cpu = new_cpu             [L] ->on_rq
    
    where "[L] rq = task_rq()" is ordered before "ACQUIRE (rq->lock)" by an
    address dependency and, in turn, "ACQUIRE (rq->lock)" is ordered before
    "[L] ->on_rq" by the ACQUIRE itself.
    
    Use READ_ONCE() to load ->cpu in task_rq() (c.f., task_cpu()) to honor
    this address dependency.  Also, mark the accesses to ->cpu and ->on_rq
    with READ_ONCE()/WRITE_ONCE() to comply with the LKMM.
    
    Signed-off-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Link: https://lkml.kernel.org/r/20190121155240.27173-1-andrea.parri@amarulasolutions.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 99e2a7772d16..c688ef5012e5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1479,9 +1479,9 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 	 */
 	smp_wmb();
 #ifdef CONFIG_THREAD_INFO_IN_TASK
-	p->cpu = cpu;
+	WRITE_ONCE(p->cpu, cpu);
 #else
-	task_thread_info(p)->cpu = cpu;
+	WRITE_ONCE(task_thread_info(p)->cpu, cpu);
 #endif
 	p->wake_cpu = cpu;
 #endif
@@ -1582,7 +1582,7 @@ static inline int task_on_rq_queued(struct task_struct *p)
 
 static inline int task_on_rq_migrating(struct task_struct *p)
 {
-	return p->on_rq == TASK_ON_RQ_MIGRATING;
+	return READ_ONCE(p->on_rq) == TASK_ON_RQ_MIGRATING;
 }
 
 /*

commit 10a35e6812aa0953f02a956c499d23fe4e68af4a
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed Jan 23 16:26:54 2019 +0100

    sched/pelt: Skip updating util_est when utilization is higher than CPU's capacity
    
    util_est is mainly meant to be a lower-bound for tasks utilization.
    That's why task_util_est() returns the actual util_avg when it's higher
    than the estimated utilization.
    
    With new invaraince signal and without any special check on samples
    collection, if a task is limited because of thermal capping for
    example, we could end up overestimating its utilization and thus
    perhaps generating an unwanted frequency spike when the capping is
    relaxed... and (even worst) it will take some more activations for the
    estimated utilization to converge back to the actual utilization.
    
    Since we cannot easily know if there is idle time in a CPU when a task
    completes an activation with a utilization higher then the CPU capacity,
    we skip the sampling when utilization is higher than CPU's capacity.
    
    Suggested-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: pjt@google.com
    Cc: pkondeti@codeaurora.org
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Link: https://lkml.kernel.org/r/1548257214-13745-4-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fe31bc472f3e..99e2a7772d16 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2230,6 +2230,13 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 # define arch_scale_freq_invariant()	false
 #endif
 
+#ifdef CONFIG_SMP
+static inline unsigned long capacity_orig_of(int cpu)
+{
+	return cpu_rq(cpu)->cpu_capacity_orig;
+}
+#endif
+
 #ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
 /**
  * enum schedutil_type - CPU utilization type

commit 23127296889fe84b0762b191b5d041e8ba6f2599
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed Jan 23 16:26:53 2019 +0100

    sched/fair: Update scale invariance of PELT
    
    The current implementation of load tracking invariance scales the
    contribution with current frequency and uarch performance (only for
    utilization) of the CPU. One main result of this formula is that the
    figures are capped by current capacity of CPU. Another one is that the
    load_avg is not invariant because not scaled with uarch.
    
    The util_avg of a periodic task that runs r time slots every p time slots
    varies in the range :
    
        U * (1-y^r)/(1-y^p) * y^i < Utilization < U * (1-y^r)/(1-y^p)
    
    with U is the max util_avg value = SCHED_CAPACITY_SCALE
    
    At a lower capacity, the range becomes:
    
        U * C * (1-y^r')/(1-y^p) * y^i' < Utilization <  U * C * (1-y^r')/(1-y^p)
    
    with C reflecting the compute capacity ratio between current capacity and
    max capacity.
    
    so C tries to compensate changes in (1-y^r') but it can't be accurate.
    
    Instead of scaling the contribution value of PELT algo, we should scale the
    running time. The PELT signal aims to track the amount of computation of
    tasks and/or rq so it seems more correct to scale the running time to
    reflect the effective amount of computation done since the last update.
    
    In order to be fully invariant, we need to apply the same amount of
    running time and idle time whatever the current capacity. Because running
    at lower capacity implies that the task will run longer, we have to ensure
    that the same amount of idle time will be applied when system becomes idle
    and no idle time has been "stolen". But reaching the maximum utilization
    value (SCHED_CAPACITY_SCALE) means that the task is seen as an
    always-running task whatever the capacity of the CPU (even at max compute
    capacity). In this case, we can discard this "stolen" idle times which
    becomes meaningless.
    
    In order to achieve this time scaling, a new clock_pelt is created per rq.
    The increase of this clock scales with current capacity when something
    is running on rq and synchronizes with clock_task when rq is idle. With
    this mechanism, we ensure the same running and idle time whatever the
    current capacity. This also enables to simplify the pelt algorithm by
    removing all references of uarch and frequency and applying the same
    contribution to utilization and loads. Furthermore, the scaling is done
    only once per update of clock (update_rq_clock_task()) instead of during
    each update of sched_entities and cfs/rt/dl_rq of the rq like the current
    implementation. This is interesting when cgroup are involved as shown in
    the results below:
    
    On a hikey (octo Arm64 platform).
    Performance cpufreq governor and only shallowest c-state to remove variance
    generated by those power features so we only track the impact of pelt algo.
    
    each test runs 16 times:
    
            ./perf bench sched pipe
            (higher is better)
            kernel  tip/sched/core     + patch
                    ops/seconds        ops/seconds         diff
            cgroup
            root    59652(+/- 0.18%)   59876(+/- 0.24%)    +0.38%
            level1  55608(+/- 0.27%)   55923(+/- 0.24%)    +0.57%
            level2  52115(+/- 0.29%)   52564(+/- 0.22%)    +0.86%
    
            hackbench -l 1000
            (lower is better)
            kernel  tip/sched/core     + patch
                    duration(sec)      duration(sec)        diff
            cgroup
            root    4.453(+/- 2.37%)   4.383(+/- 2.88%)     -1.57%
            level1  4.859(+/- 8.50%)   4.830(+/- 7.07%)     -0.60%
            level2  5.063(+/- 9.83%)   4.928(+/- 9.66%)     -2.66%
    
    Then, the responsiveness of PELT is improved when CPU is not running at max
    capacity with this new algorithm. I have put below some examples of
    duration to reach some typical load values according to the capacity of the
    CPU with current implementation and with this patch. These values has been
    computed based on the geometric series and the half period value:
    
      Util (%)     max capacity  half capacity(mainline)  half capacity(w/ patch)
      972 (95%)    138ms         not reachable            276ms
      486 (47.5%)  30ms          138ms                     60ms
      256 (25%)    13ms           32ms                     26ms
    
    On my hikey (octo Arm64 platform) with schedutil governor, the time to
    reach max OPP when starting from a null utilization, decreases from 223ms
    with current scale invariance down to 121ms with the new algorithm.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pjt@google.com
    Cc: pkondeti@codeaurora.org
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Link: https://lkml.kernel.org/r/1548257214-13745-3-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0ed130fae2a9..fe31bc472f3e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -861,7 +861,10 @@ struct rq {
 
 	unsigned int		clock_update_flags;
 	u64			clock;
-	u64			clock_task;
+	/* Ensure that all clocks are in the same cache line */
+	u64			clock_task ____cacheline_aligned;
+	u64			clock_pelt;
+	unsigned long		lost_idle_time;
 
 	atomic_t		nr_iowait;
 

commit 62478d9911fab9694c195f0ca8e4701de09be98e
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed Jan 23 16:26:52 2019 +0100

    sched/fair: Move the rq_of() helper function
    
    Move rq_of() helper function so it can be used in pelt.c
    
    [ mingo: Improve readability while at it. ]
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pjt@google.com
    Cc: pkondeti@codeaurora.org
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Link: https://lkml.kernel.org/r/1548257214-13745-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d27c1a5d4e25..0ed130fae2a9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -951,6 +951,22 @@ struct rq {
 #endif
 };
 
+#ifdef CONFIG_FAIR_GROUP_SCHED
+
+/* CPU runqueue to which this cfs_rq is attached */
+static inline struct rq *rq_of(struct cfs_rq *cfs_rq)
+{
+	return cfs_rq->rq;
+}
+
+#else
+
+static inline struct rq *rq_of(struct cfs_rq *cfs_rq)
+{
+	return container_of(cfs_rq, struct rq, cfs);
+}
+#endif
+
 static inline int cpu_of(struct rq *rq)
 {
 #ifdef CONFIG_SMP

commit f8a696f25ba09a1821dc6ca3db56f41c264fb896
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Dec 5 11:23:56 2018 +0100

    sched/core: Give DCE a fighting chance
    
    All that fancy new Energy-Aware scheduling foo is hidden behind a
    static_key, which is awesome if you have the stuff enabled in your
    config.
    
    However, when you lack all the prerequisites it doesn't make any sense
    to pretend we'll ever actually run this, so provide a little more clue
    to the compiler so it can more agressively delete the code.
    
       text    data     bss     dec     hex filename
      50297     976      96   51369    c8a9 defconfig-build/kernel/sched/fair.o
      49227     944      96   50267    c45b defconfig-build/kernel/sched/fair.o
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d04530bf251f..d27c1a5d4e25 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2299,11 +2299,19 @@ unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned
 #endif
 
 #if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
+
 #define perf_domain_span(pd) (to_cpumask(((pd)->em_pd->cpus)))
-#else
+
+DECLARE_STATIC_KEY_FALSE(sched_energy_present);
+
+static inline bool sched_energy_enabled(void)
+{
+	return static_branch_unlikely(&sched_energy_present);
+}
+
+#else /* ! (CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL) */
+
 #define perf_domain_span(pd) NULL
-#endif
+static inline bool sched_energy_enabled(void) { return false; }
 
-#ifdef CONFIG_SMP
-extern struct static_key_false sched_energy_present;
-#endif
+#endif /* CONFIG_ENERGY_MODEL && CONFIG_CPU_FREQ_GOV_SCHEDUTIL */

commit 337e9b07db3b8c7f7d68b849df32f434a1a3b831
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Tue Nov 6 19:10:53 2018 -0800

    sched: Replace call_rcu_sched() with call_rcu()
    
    Now that call_rcu()'s callback is not invoked until after all
    preempt-disable regions of code have completed (in addition to explicitly
    marked RCU read-side critical sections), call_rcu() can be used in place
    of call_rcu_sched().  This commit therefore makes that change.
    
    While in the area, this commit also updates an outdated header comment
    for for_each_domain().
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d04530bf251f..6665b9c02e2f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1260,7 +1260,7 @@ extern void sched_ttwu_pending(void);
 
 /*
  * The domain tree (rq->sd) is protected by RCU's quiescent state transition.
- * See detach_destroy_domains: synchronize_sched for details.
+ * See destroy_sched_domains: call_rcu for details.
  *
  * The domain tree of any CPU may only be accessed from within
  * preempt-disabled sections.

commit e9666d10a5677a494260d60d1fa0b73cc7646eb3
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Dec 31 00:14:15 2018 +0900

    jump_label: move 'asm goto' support test to Kconfig
    
    Currently, CONFIG_JUMP_LABEL just means "I _want_ to use jump label".
    
    The jump label is controlled by HAVE_JUMP_LABEL, which is defined
    like this:
    
      #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
      # define HAVE_JUMP_LABEL
      #endif
    
    We can improve this by testing 'asm goto' support in Kconfig, then
    make JUMP_LABEL depend on CC_HAS_ASM_GOTO.
    
    Ugly #ifdef HAVE_JUMP_LABEL will go away, and CONFIG_JUMP_LABEL will
    match to the real kernel capability.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0ba08924e017..d04530bf251f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1488,7 +1488,7 @@ enum {
 
 #undef SCHED_FEAT
 
-#if defined(CONFIG_SCHED_DEBUG) && defined(HAVE_JUMP_LABEL)
+#if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_JUMP_LABEL)
 
 /*
  * To support run-time toggling of sched features, all the translation units
@@ -1508,7 +1508,7 @@ static __always_inline bool static_branch_##name(struct static_key *key) \
 extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #define sched_feat(x) (static_branch_##x(&sched_feat_keys[__SCHED_FEAT_##x]))
 
-#else /* !(SCHED_DEBUG && HAVE_JUMP_LABEL) */
+#else /* !(SCHED_DEBUG && CONFIG_JUMP_LABEL) */
 
 /*
  * Each translation unit has its own copy of sysctl_sched_features to allow
@@ -1524,7 +1524,7 @@ static const_debug __maybe_unused unsigned int sysctl_sched_features =
 
 #define sched_feat(x) !!(sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
 
-#endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
+#endif /* SCHED_DEBUG && CONFIG_JUMP_LABEL */
 
 extern struct static_key_false sched_numa_balancing;
 extern struct static_key_false sched_schedstats;

commit 2802bf3cd936fe2c8033a696d375a4d9d3974de4
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Mon Dec 3 09:56:25 2018 +0000

    sched/fair: Add over-utilization/tipping point indicator
    
    Energy-aware scheduling is only meant to be active while the system is
    _not_ over-utilized. That is, there are spare cycles available to shift
    tasks around based on their actual utilization to get a more
    energy-efficient task distribution without depriving any tasks. When
    above the tipping point task placement is done the traditional way based
    on load_avg, spreading the tasks across as many cpus as possible based
    on priority scaled load to preserve smp_nice. Below the tipping point we
    want to use util_avg instead. We need to define a criteria for when we
    make the switch.
    
    The util_avg for each cpu converges towards 100% regardless of how many
    additional tasks we may put on it. If we define over-utilized as:
    
    sum_{cpus}(rq.cfs.avg.util_avg) + margin > sum_{cpus}(rq.capacity)
    
    some individual cpus may be over-utilized running multiple tasks even
    when the above condition is false. That should be okay as long as we try
    to spread the tasks out to avoid per-cpu over-utilization as much as
    possible and if all tasks have the _same_ priority. If the latter isn't
    true, we have to consider priority to preserve smp_nice.
    
    For example, we could have n_cpus nice=-10 util_avg=55% tasks and
    n_cpus/2 nice=0 util_avg=60% tasks. Balancing based on util_avg we are
    likely to end up with nice=-10 tasks sharing cpus and nice=0 tasks
    getting their own as we 1.5*n_cpus tasks in total and 55%+55% is less
    over-utilized than 55%+60% for those cpus that have to be shared. The
    system utilization is only 85% of the system capacity, but we are
    breaking smp_nice.
    
    To be sure not to break smp_nice, we have defined over-utilization
    conservatively as when any cpu in the system is fully utilized at its
    highest frequency instead:
    
    cpu_rq(any).cfs.avg.util_avg + margin > cpu_rq(any).capacity
    
    IOW, as soon as one cpu is (nearly) 100% utilized, we switch to load_avg
    to factor in priority to preserve smp_nice.
    
    With this definition, we can skip periodic load-balance as no cpu has an
    always-running task when the system is not over-utilized. All tasks will
    be periodic and we can balance them at wake-up. This conservative
    condition does however mean that some scenarios that could benefit from
    energy-aware decisions even if one cpu is fully utilized would not get
    those benefits.
    
    For systems where some cpus might have reduced capacity on some cpus
    (RT-pressure and/or big.LITTLE), we want periodic load-balance checks as
    soon a just a single cpu is fully utilized as it might one of those with
    reduced capacity and in that case we want to migrate it.
    
    [ peterz: Added a comment explaining why new tasks are not accounted during
              overutilization detection. ]
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-13-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d4d984846924..0ba08924e017 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -718,6 +718,7 @@ struct perf_domain {
 
 /* Scheduling group status flags */
 #define SG_OVERLOAD		0x1 /* More than one runnable task on a CPU. */
+#define SG_OVERUTILIZED		0x2 /* One or more CPUs are over-utilized. */
 
 /*
  * We add the notion of a root-domain which will be used to define per-domain
@@ -741,6 +742,9 @@ struct root_domain {
 	 */
 	int			overload;
 
+	/* Indicate one or more cpus over-utilized (tipping point) */
+	int			overutilized;
+
 	/*
 	 * The bit corresponding to a CPU gets set here if such CPU has more
 	 * than one runnable -deadline task (as it is below for RT tasks).

commit 630246a06ae2a7a12d1fce85f1e5681032982791
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:24 2018 +0000

    sched/fair: Clean-up update_sg_lb_stats parameters
    
    In preparation for the introduction of a new root domain flag which can
    be set during load balance (the 'overutilized' flag), clean-up the set
    of parameters passed to update_sg_lb_stats(). More specifically, the
    'local_group' and 'local_idx' parameters can be removed since they can
    easily be reconstructed from within the function.
    
    While at it, transform the 'overload' parameter into a flag stored in
    the 'sg_status' parameter hence facilitating the definition of new flags
    when needed.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Suggested-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-12-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2b3cf356e958..d4d984846924 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -716,6 +716,9 @@ struct perf_domain {
 	struct rcu_head rcu;
 };
 
+/* Scheduling group status flags */
+#define SG_OVERLOAD		0x1 /* More than one runnable task on a CPU. */
+
 /*
  * We add the notion of a root-domain which will be used to define per-domain
  * variables. Each exclusive cpuset essentially defines an island domain by

commit 1f74de8798c93ce14801cc4e772603e51c841c33
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:22 2018 +0000

    sched/toplogy: Introduce the 'sched_energy_present' static key
    
    In order to make sure Energy Aware Scheduling (EAS) will not impact
    systems where no Energy Model is available, introduce a static key
    guarding the access to EAS code. Since EAS is enabled on a
    per-root-domain basis, the static key is enabled when at least one root
    domain meets all conditions for EAS.
    
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-10-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fd84900b0b21..2b3cf356e958 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2296,3 +2296,7 @@ unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned
 #else
 #define perf_domain_span(pd) NULL
 #endif
+
+#ifdef CONFIG_SMP
+extern struct static_key_false sched_energy_present;
+#endif

commit 531b5c9f5cd05ead53324f419b32685a22eebe8b
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:21 2018 +0000

    sched/topology: Make Energy Aware Scheduling depend on schedutil
    
    Energy Aware Scheduling (EAS) is designed with the assumption that
    frequencies of CPUs follow their utilization value. When using a CPUFreq
    governor other than schedutil, the chances of this assumption being true
    are small, if any. When schedutil is being used, EAS' predictions are at
    least consistent with the frequency requests. Although those requests
    have no guarantees to be honored by the hardware, they should at least
    guide DVFS in the right direction and provide some hope in regards to the
    EAS model being accurate.
    
    To make sure EAS is only used in a sane configuration, create a strong
    dependency on schedutil being used. Since having sugov compiled-in does
    not provide that guarantee, make CPUFreq call a scheduler function on
    governor changes hence letting it rebuild the scheduling domains, check
    the governors of the online CPUs, and enable/disable EAS accordingly.
    
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-9-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 75c403674706..fd84900b0b21 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2291,10 +2291,8 @@ unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned
 }
 #endif
 
-#ifdef CONFIG_SMP
-#ifdef CONFIG_ENERGY_MODEL
+#if defined(CONFIG_ENERGY_MODEL) && defined(CONFIG_CPU_FREQ_GOV_SCHEDUTIL)
 #define perf_domain_span(pd) (to_cpumask(((pd)->em_pd->cpus)))
 #else
 #define perf_domain_span(pd) NULL
 #endif
-#endif

commit 011b27bb5d3139e8b5fe9ceff1fc7f6dc3145071
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:19 2018 +0000

    sched/topology: Add lowest CPU asymmetry sched_domain level pointer
    
    Add another member to the family of per-cpu sched_domain shortcut
    pointers. This one, sd_asym_cpucapacity, points to the lowest level
    at which the SD_ASYM_CPUCAPACITY flag is set. While at it, rename the
    sd_asym shortcut to sd_asym_packing to avoid confusions.
    
    Generally speaking, the largest opportunity to save energy via
    scheduling comes from a smarter exploitation of heterogeneous platforms
    (i.e. big.LITTLE). Consequently, the sd_asym_cpucapacity shortcut will
    be used at first as the lowest domain where Energy-Aware Scheduling
    (EAS) should be applied. For example, it is possible to apply EAS within
    a socket on a multi-socket system, as long as each socket has an
    asymmetric topology. Energy-aware cross-sockets wake-up balancing will
    only happen when the system is over-utilized, or this_cpu and prev_cpu
    are in different sockets.
    
    Suggested-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-7-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 808a565187b1..75c403674706 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1303,7 +1303,8 @@ DECLARE_PER_CPU(int, sd_llc_size);
 DECLARE_PER_CPU(int, sd_llc_id);
 DECLARE_PER_CPU(struct sched_domain_shared *, sd_llc_shared);
 DECLARE_PER_CPU(struct sched_domain *, sd_numa);
-DECLARE_PER_CPU(struct sched_domain *, sd_asym);
+DECLARE_PER_CPU(struct sched_domain *, sd_asym_packing);
+DECLARE_PER_CPU(struct sched_domain *, sd_asym_cpucapacity);
 extern struct static_key_false sched_asym_cpucapacity;
 
 struct sched_group_capacity {

commit 6aa140fa4508933a6ac6717d65a403eb904d6c02
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:18 2018 +0000

    sched/topology: Reference the Energy Model of CPUs when available
    
    The existing scheduling domain hierarchy is defined to map to the cache
    topology of the system. However, Energy Aware Scheduling (EAS) requires
    more knowledge about the platform, and specifically needs to know about
    the span of Performance Domains (PD), which do not always align with
    caches.
    
    To address this issue, use the Energy Model (EM) of the system to extend
    the scheduler topology code with a representation of the PDs, alongside
    the scheduling domains. More specifically, a linked list of PDs is
    attached to each root domain. When multiple root domains are in use,
    each list contains only the PDs covering the CPUs of its root domain. If
    a PD spans over CPUs of multiple different root domains, it will be
    duplicated in all lists.
    
    The lists are fully maintained by the scheduler from
    partition_sched_domains() in order to cope with hotplug and cpuset
    changes. As for scheduling domains, the list are protected by RCU to
    ensure safe concurrent updates.
    
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-6-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2eafa228aebf..808a565187b1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -45,6 +45,7 @@
 #include <linux/ctype.h>
 #include <linux/debugfs.h>
 #include <linux/delayacct.h>
+#include <linux/energy_model.h>
 #include <linux/init_task.h>
 #include <linux/kprobes.h>
 #include <linux/kthread.h>
@@ -709,6 +710,12 @@ static inline bool sched_asym_prefer(int a, int b)
 	return arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);
 }
 
+struct perf_domain {
+	struct em_perf_domain *em_pd;
+	struct perf_domain *next;
+	struct rcu_head rcu;
+};
+
 /*
  * We add the notion of a root-domain which will be used to define per-domain
  * variables. Each exclusive cpuset essentially defines an island domain by
@@ -761,6 +768,12 @@ struct root_domain {
 	struct cpupri		cpupri;
 
 	unsigned long		max_cpu_capacity;
+
+	/*
+	 * NULL-terminated list of performance domains intersecting with the
+	 * CPUs of the rd. Protected by RCU.
+	 */
+	struct perf_domain	*pd;
 };
 
 extern struct root_domain def_root_domain;
@@ -2276,3 +2289,11 @@ unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned
 	return util;
 }
 #endif
+
+#ifdef CONFIG_SMP
+#ifdef CONFIG_ENERGY_MODEL
+#define perf_domain_span(pd) (to_cpumask(((pd)->em_pd->cpus)))
+#else
+#define perf_domain_span(pd) NULL
+#endif
+#endif

commit 938e5e4b0d1502a93e787985cb95b136b40717b7
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:15 2018 +0000

    sched/cpufreq: Prepare schedutil for Energy Aware Scheduling
    
    Schedutil requests frequency by aggregating utilization signals from
    the scheduler (CFS, RT, DL, IRQ) and applying a 25% margin on top of
    them. Since Energy Aware Scheduling (EAS) needs to be able to predict
    the frequency requests, it needs to forecast the decisions made by the
    governor.
    
    In order to prepare the introduction of EAS, introduce
    schedutil_freq_util() to centralize the aforementioned signal
    aggregation and make it available to both schedutil and EAS. Since
    frequency selection and energy estimation still need to deal with RT and
    DL signals slightly differently, schedutil_freq_util() is called with a
    different 'type' parameter in those two contexts, and returns an
    aggregated utilization signal accordingly. While at it, introduce the
    map_util_freq() function which is designed to make schedutil's 25%
    margin usable easily for both sugov and EAS.
    
    As EAS will be able to predict schedutil's frequency requests more
    accurately than any other governor by design, it'd be sensible to make
    sure EAS cannot be used without schedutil. This will be done later, once
    EAS has actually been introduced.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-3-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 66067152a831..2eafa228aebf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2191,6 +2191,31 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif
 
 #ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
+/**
+ * enum schedutil_type - CPU utilization type
+ * @FREQUENCY_UTIL:	Utilization used to select frequency
+ * @ENERGY_UTIL:	Utilization used during energy calculation
+ *
+ * The utilization signals of all scheduling classes (CFS/RT/DL) and IRQ time
+ * need to be aggregated differently depending on the usage made of them. This
+ * enum is used within schedutil_freq_util() to differentiate the types of
+ * utilization expected by the callers, and adjust the aggregation accordingly.
+ */
+enum schedutil_type {
+	FREQUENCY_UTIL,
+	ENERGY_UTIL,
+};
+
+unsigned long schedutil_freq_util(int cpu, unsigned long util_cfs,
+				  unsigned long max, enum schedutil_type type);
+
+static inline unsigned long schedutil_energy_util(int cpu, unsigned long cfs)
+{
+	unsigned long max = arch_scale_cpu_capacity(NULL, cpu);
+
+	return schedutil_freq_util(cpu, cfs, max, ENERGY_UTIL);
+}
+
 static inline unsigned long cpu_bw_dl(struct rq *rq)
 {
 	return (rq->dl.running_bw * SCHED_CAPACITY_SCALE) >> BW_SHIFT;
@@ -2217,6 +2242,11 @@ static inline unsigned long cpu_util_rt(struct rq *rq)
 {
 	return READ_ONCE(rq->avg_rt.util_avg);
 }
+#else /* CONFIG_CPU_FREQ_GOV_SCHEDUTIL */
+static inline unsigned long schedutil_energy_util(int cpu, unsigned long cfs)
+{
+	return cfs;
+}
 #endif
 
 #ifdef CONFIG_HAVE_SCHED_AVG_IRQ

commit 5bd0988be12733a42a1a3d50e3e2ddfd79e57518
Author: Quentin Perret <quentin.perret@arm.com>
Date:   Mon Dec 3 09:56:14 2018 +0000

    sched/topology: Relocate arch_scale_cpu_capacity() to the internal header
    
    By default, arch_scale_cpu_capacity() is only visible from within the
    kernel/sched folder. Relocate it to include/linux/sched/topology.h to
    make it visible to other clients needing to know about the capacity of
    CPUs, such as the Energy Model framework.
    
    This also shrinks the <linux/sched/topology.h> public header.
    
    Signed-off-by: Quentin Perret <quentin.perret@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: adharmap@codeaurora.org
    Cc: chris.redpath@arm.com
    Cc: currojerez@riseup.net
    Cc: dietmar.eggemann@arm.com
    Cc: edubezval@gmail.com
    Cc: gregkh@linuxfoundation.org
    Cc: javi.merino@kernel.org
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: pkondeti@codeaurora.org
    Cc: rjw@rjwysocki.net
    Cc: skannan@codeaurora.org
    Cc: smuckle@google.com
    Cc: srinivas.pandruvada@linux.intel.com
    Cc: thara.gopinath@linaro.org
    Cc: tkjos@google.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: https://lkml.kernel.org/r/20181203095628.11858-2-quentin.perret@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ceb896404869..66067152a831 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1859,24 +1859,6 @@ unsigned long arch_scale_freq_capacity(int cpu)
 }
 #endif
 
-#ifdef CONFIG_SMP
-#ifndef arch_scale_cpu_capacity
-static __always_inline
-unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
-{
-	return SCHED_CAPACITY_SCALE;
-}
-#endif
-#else
-#ifndef arch_scale_cpu_capacity
-static __always_inline
-unsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)
-{
-	return SCHED_CAPACITY_SCALE;
-}
-#endif
-#endif
-
 #ifdef CONFIG_SMP
 #ifdef CONFIG_PREEMPT
 

commit 765d0af19f5f388a34bf4533378f8398b72ded46
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed Aug 29 15:19:11 2018 +0200

    sched/topology: Remove the ::smt_gain field from 'struct sched_domain'
    
    ::smt_gain is used to compute the capacity of CPUs of a SMT core with the
    constraint 1 < ::smt_gain < 2 in order to be able to compute number of CPUs
    per core. The field has_free_capacity of struct numa_stat, which was the
    last user of this computation of number of CPUs per core, has been removed
    by:
    
      2d4056fafa19 ("sched/numa: Remove numa_has_capacity()")
    
    We can now remove this constraint on core capacity and use the defautl value
    SCHED_CAPACITY_SCALE for SMT CPUs. With this remove, SCHED_CAPACITY_SCALE
    becomes the maximum compute capacity of CPUs on every systems. This should
    help to simplify some code and remove fields like rd->max_cpu_capacity
    
    Furthermore, arch_scale_cpu_capacity() is used with a NULL sd in several other
    places in the code when it wants the capacity of a CPUs to scale
    some metrics like in pelt, deadline or schedutil. In case on SMT, the value
    returned is not the capacity of SMT CPUs but default SCHED_CAPACITY_SCALE.
    
    So remove it.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1535548752-4434-4-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9bde60a11805..ceb896404869 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1864,9 +1864,6 @@ unsigned long arch_scale_freq_capacity(int cpu)
 static __always_inline
 unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
 {
-	if (sd && (sd->flags & SD_SHARE_CPUCAPACITY) && (sd->span_weight > 1))
-		return sd->smt_gain / sd->span_weight;
-
 	return SCHED_CAPACITY_SCALE;
 }
 #endif

commit dfcb245e28481256a10a9133441baf2a93d26642
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 3 10:05:56 2018 +0100

    sched: Fix various typos in comments
    
    Go over the scheduler source code and fix common typos
    in comments - and a typo in an actual variable name.
    
    No change in functionality intended.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 71cd8b710599..9bde60a11805 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -637,7 +637,7 @@ struct dl_rq {
 	/*
 	 * Deadline values of the currently executing and the
 	 * earliest ready task on this rq. Caching these facilitates
-	 * the decision wether or not a ready but not running task
+	 * the decision whether or not a ready but not running task
 	 * should migrate somewhere else.
 	 */
 	struct {
@@ -1434,7 +1434,7 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 #ifdef CONFIG_SMP
 	/*
 	 * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be
-	 * successfuly executed on another CPU. We must ensure that updates of
+	 * successfully executed on another CPU. We must ensure that updates of
 	 * per-task data have been completed by this moment.
 	 */
 	smp_wmb();

commit 5f675231e456cb599b283f8361f01cf34b0617df
Merge: 3e184501083c 2595646791c3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Dec 3 11:42:17 2018 +0100

    Merge tag 'v4.20-rc5' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 321a874a7ef85655e93b3206d0f36b4a6097f948
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun Nov 25 19:33:38 2018 +0100

    sched/smt: Expose sched_smt_present static key
    
    Make the scheduler's 'sched_smt_present' static key globaly available, so
    it can be used in the x86 speculation control code.
    
    Provide a query function and a stub for the CONFIG_SMP=n case.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jiri Kosina <jkosina@suse.cz>
    Cc: Tom Lendacky <thomas.lendacky@amd.com>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: David Woodhouse <dwmw@amazon.co.uk>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Casey Schaufler <casey.schaufler@intel.com>
    Cc: Asit Mallick <asit.k.mallick@intel.com>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Jon Masters <jcm@redhat.com>
    Cc: Waiman Long <longman9394@gmail.com>
    Cc: Greg KH <gregkh@linuxfoundation.org>
    Cc: Dave Stewart <david.c.stewart@intel.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: stable@vger.kernel.org
    Link: https://lkml.kernel.org/r/20181125185004.430168326@linutronix.de

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 618577fc9aa8..4e524ab589c9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -23,6 +23,7 @@
 #include <linux/sched/prio.h>
 #include <linux/sched/rt.h>
 #include <linux/sched/signal.h>
+#include <linux/sched/smt.h>
 #include <linux/sched/stat.h>
 #include <linux/sched/sysctl.h>
 #include <linux/sched/task.h>
@@ -936,9 +937,6 @@ static inline int cpu_of(struct rq *rq)
 
 
 #ifdef CONFIG_SCHED_SMT
-
-extern struct static_key_false sched_smt_present;
-
 extern void __update_idle_core(struct rq *rq);
 
 static inline void update_idle_core(struct rq *rq)

commit 3e184501083c38fa091f640acb13af17a21fd228
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Nov 6 11:12:57 2018 +0530

    sched/core: Clean up the #ifdef block in add_nr_running()
    
    There is no point in keeping the conditional statement of the #if block
    outside of the #ifdef block, while all of its body is contained within
    the #ifdef block.
    
    Move the conditional statement under the #ifdef block as well.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: http://lkml.kernel.org/r/78cbd78a615d6f9fdcd3327f1ead68470f92593e.1541482935.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b7a3147874e3..e0e052a50fcd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1801,12 +1801,12 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 
 	rq->nr_running = prev_nr + count;
 
-	if (prev_nr < 2 && rq->nr_running >= 2) {
 #ifdef CONFIG_SMP
+	if (prev_nr < 2 && rq->nr_running >= 2) {
 		if (!READ_ONCE(rq->rd->overload))
 			WRITE_ONCE(rq->rd->overload, 1);
-#endif
 	}
+#endif
 
 	sched_update_tick_dependency(rq);
 }

commit 1da1843f9f0334e2428308945d396ffecc2acfe1
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Nov 5 16:51:55 2018 +0530

    sched/core: Create task_has_idle_policy() helper
    
    We already have task_has_rt_policy() and task_has_dl_policy() helpers,
    create task_has_idle_policy() as well and update sched core to start
    using it.
    
    While at it, use task_has_dl_policy() at one more place.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: http://lkml.kernel.org/r/ce3915d5b490fc81af926a3b6bfb775e7188e005.1541416894.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 618577fc9aa8..b7a3147874e3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -176,6 +176,11 @@ static inline bool valid_policy(int policy)
 		rt_policy(policy) || dl_policy(policy);
 }
 
+static inline int task_has_idle_policy(struct task_struct *p)
+{
+	return idle_policy(p->policy);
+}
+
 static inline int task_has_rt_policy(struct task_struct *p)
 {
 	return rt_policy(p->policy);

commit eb414681d5a07d28d2ff90dc05f69ec6b232ebd2
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:27 2018 -0700

    psi: pressure stall information for CPU, memory, and IO
    
    When systems are overcommitted and resources become contended, it's hard
    to tell exactly the impact this has on workload productivity, or how close
    the system is to lockups and OOM kills.  In particular, when machines work
    multiple jobs concurrently, the impact of overcommit in terms of latency
    and throughput on the individual job can be enormous.
    
    In order to maximize hardware utilization without sacrificing individual
    job health or risk complete machine lockups, this patch implements a way
    to quantify resource pressure in the system.
    
    A kernel built with CONFIG_PSI=y creates files in /proc/pressure/ that
    expose the percentage of time the system is stalled on CPU, memory, or IO,
    respectively.  Stall states are aggregate versions of the per-task delay
    accounting delays:
    
           cpu: some tasks are runnable but not executing on a CPU
           memory: tasks are reclaiming, or waiting for swapin or thrashing cache
           io: tasks are waiting for io completions
    
    These percentages of walltime can be thought of as pressure percentages,
    and they give a general sense of system health and productivity loss
    incurred by resource overcommit.  They can also indicate when the system
    is approaching lockup scenarios and OOMs.
    
    To do this, psi keeps track of the task states associated with each CPU
    and samples the time they spend in stall states.  Every 2 seconds, the
    samples are averaged across CPUs - weighted by the CPUs' non-idle time to
    eliminate artifacts from unused CPUs - and translated into percentages of
    walltime.  A running average of those percentages is maintained over 10s,
    1m, and 5m periods (similar to the loadaverage).
    
    [hannes@cmpxchg.org: doc fixlet, per Randy]
      Link: http://lkml.kernel.org/r/20180828205625.GA14030@cmpxchg.org
    [hannes@cmpxchg.org: code optimization]
      Link: http://lkml.kernel.org/r/20180907175015.GA8479@cmpxchg.org
    [hannes@cmpxchg.org: rename psi_clock() to psi_update_work(), per Peter]
      Link: http://lkml.kernel.org/r/20180907145404.GB11088@cmpxchg.org
    [hannes@cmpxchg.org: fix build]
      Link: http://lkml.kernel.org/r/20180913014222.GA2370@cmpxchg.org
    Link: http://lkml.kernel.org/r/20180828172258.3185-9-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1de189bb9209..618577fc9aa8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -54,6 +54,7 @@
 #include <linux/proc_fs.h>
 #include <linux/prefetch.h>
 #include <linux/profile.h>
+#include <linux/psi.h>
 #include <linux/rcupdate_wait.h>
 #include <linux/security.h>
 #include <linux/stop_machine.h>
@@ -319,6 +320,7 @@ extern bool dl_cpu_busy(unsigned int cpu);
 #ifdef CONFIG_CGROUP_SCHED
 
 #include <linux/cgroup.h>
+#include <linux/psi.h>
 
 struct cfs_rq;
 struct rt_rq;

commit 246b3b3342c9b0a2e24cda2178be87bc36e1c874
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:23 2018 -0700

    sched: introduce this_rq_lock_irq()
    
    do_sched_yield() disables IRQs, looks up this_rq() and locks it.  The next
    patch is adding another site with the same pattern, so provide a
    convenience function for it.
    
    Link: http://lkml.kernel.org/r/20180828172258.3185-8-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 65a75b317935..1de189bb9209 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1157,6 +1157,18 @@ rq_unlock(struct rq *rq, struct rq_flags *rf)
 	raw_spin_unlock(&rq->lock);
 }
 
+static inline struct rq *
+this_rq_lock_irq(struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+	local_irq_disable();
+	rq = this_rq();
+	rq_lock(rq, rf);
+	return rq;
+}
+
 #ifdef CONFIG_NUMA
 enum numa_topology_type {
 	NUMA_DIRECT,

commit 1f351d7f7590857ea281579c26e6045b4c548ef4
Author: Johannes Weiner <hannes@cmpxchg.org>
Date:   Fri Oct 26 15:06:19 2018 -0700

    sched: sched.h: make rq locking and clock functions available in stats.h
    
    kernel/sched/sched.h includes "stats.h" half-way through the file.  The
    next patch introduces users of sched.h's rq locking functions and
    update_rq_clock() in kernel/sched/stats.h.  Move those definitions up in
    the file so they are available in stats.h.
    
    Link: http://lkml.kernel.org/r/20180828172258.3185-7-hannes@cmpxchg.org
    Signed-off-by: Johannes Weiner <hannes@cmpxchg.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Suren Baghdasaryan <surenb@google.com>
    Tested-by: Daniel Drake <drake@endlessm.com>
    Cc: Christopher Lameter <cl@linux.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <jweiner@fb.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Enderborg <peter.enderborg@sony.com>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Shakeel Butt <shakeelb@google.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Vinayak Menon <vinmenon@codeaurora.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b8c007713b3b..65a75b317935 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -957,6 +957,8 @@ DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 #define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
 #define raw_rq()		raw_cpu_ptr(&runqueues)
 
+extern void update_rq_clock(struct rq *rq);
+
 static inline u64 __rq_clock_broken(struct rq *rq)
 {
 	return READ_ONCE(rq->clock);
@@ -1075,6 +1077,86 @@ static inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)
 #endif
 }
 
+struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
+	__acquires(rq->lock);
+
+struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
+	__acquires(p->pi_lock)
+	__acquires(rq->lock);
+
+static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)
+	__releases(rq->lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock(&rq->lock);
+}
+
+static inline void
+task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
+	__releases(rq->lock)
+	__releases(p->pi_lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock(&rq->lock);
+	raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
+}
+
+static inline void
+rq_lock_irqsave(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock_irqsave(&rq->lock, rf->flags);
+	rq_pin_lock(rq, rf);
+}
+
+static inline void
+rq_lock_irq(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock_irq(&rq->lock);
+	rq_pin_lock(rq, rf);
+}
+
+static inline void
+rq_lock(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock(&rq->lock);
+	rq_pin_lock(rq, rf);
+}
+
+static inline void
+rq_relock(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock(&rq->lock);
+	rq_repin_lock(rq, rf);
+}
+
+static inline void
+rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
+	__releases(rq->lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock_irqrestore(&rq->lock, rf->flags);
+}
+
+static inline void
+rq_unlock_irq(struct rq *rq, struct rq_flags *rf)
+	__releases(rq->lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock_irq(&rq->lock);
+}
+
+static inline void
+rq_unlock(struct rq *rq, struct rq_flags *rf)
+	__releases(rq->lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock(&rq->lock);
+}
+
 #ifdef CONFIG_NUMA
 enum numa_topology_type {
 	NUMA_DIRECT,
@@ -1717,8 +1799,6 @@ static inline void sub_nr_running(struct rq *rq, unsigned count)
 	sched_update_tick_dependency(rq);
 }
 
-extern void update_rq_clock(struct rq *rq);
-
 extern void activate_task(struct rq *rq, struct task_struct *p, int flags);
 extern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);
 
@@ -1783,86 +1863,6 @@ unsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)
 #endif
 #endif
 
-struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
-	__acquires(rq->lock);
-
-struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
-	__acquires(p->pi_lock)
-	__acquires(rq->lock);
-
-static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)
-	__releases(rq->lock)
-{
-	rq_unpin_lock(rq, rf);
-	raw_spin_unlock(&rq->lock);
-}
-
-static inline void
-task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
-	__releases(rq->lock)
-	__releases(p->pi_lock)
-{
-	rq_unpin_lock(rq, rf);
-	raw_spin_unlock(&rq->lock);
-	raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
-}
-
-static inline void
-rq_lock_irqsave(struct rq *rq, struct rq_flags *rf)
-	__acquires(rq->lock)
-{
-	raw_spin_lock_irqsave(&rq->lock, rf->flags);
-	rq_pin_lock(rq, rf);
-}
-
-static inline void
-rq_lock_irq(struct rq *rq, struct rq_flags *rf)
-	__acquires(rq->lock)
-{
-	raw_spin_lock_irq(&rq->lock);
-	rq_pin_lock(rq, rf);
-}
-
-static inline void
-rq_lock(struct rq *rq, struct rq_flags *rf)
-	__acquires(rq->lock)
-{
-	raw_spin_lock(&rq->lock);
-	rq_pin_lock(rq, rf);
-}
-
-static inline void
-rq_relock(struct rq *rq, struct rq_flags *rf)
-	__acquires(rq->lock)
-{
-	raw_spin_lock(&rq->lock);
-	rq_repin_lock(rq, rf);
-}
-
-static inline void
-rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
-	__releases(rq->lock)
-{
-	rq_unpin_lock(rq, rf);
-	raw_spin_unlock_irqrestore(&rq->lock, rf->flags);
-}
-
-static inline void
-rq_unlock_irq(struct rq *rq, struct rq_flags *rf)
-	__releases(rq->lock)
-{
-	rq_unpin_lock(rq, rf);
-	raw_spin_unlock_irq(&rq->lock);
-}
-
-static inline void
-rq_unlock(struct rq *rq, struct rq_flags *rf)
-	__releases(rq->lock)
-{
-	rq_unpin_lock(rq, rf);
-	raw_spin_unlock(&rq->lock);
-}
-
 #ifdef CONFIG_SMP
 #ifdef CONFIG_PREEMPT
 

commit 99792e0cea1ed733cdc8d0758677981e0cbebfed
Merge: 382d72a9aa52 977e4be5eb71
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 17:05:28 2018 +0100

    Merge branch 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 mm updates from Ingo Molnar:
     "Lots of changes in this cycle:
    
       - Lots of CPA (change page attribute) optimizations and related
         cleanups (Thomas Gleixner, Peter Zijstra)
    
       - Make lazy TLB mode even lazier (Rik van Riel)
    
       - Fault handler cleanups and improvements (Dave Hansen)
    
       - kdump, vmcore: Enable kdumping encrypted memory with AMD SME
         enabled (Lianbo Jiang)
    
       - Clean up VM layout documentation (Baoquan He, Ingo Molnar)
    
       - ... plus misc other fixes and enhancements"
    
    * 'x86-mm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (51 commits)
      x86/stackprotector: Remove the call to boot_init_stack_canary() from cpu_startup_entry()
      x86/mm: Kill stray kernel fault handling comment
      x86/mm: Do not warn about PCI BIOS W+X mappings
      resource: Clean it up a bit
      resource: Fix find_next_iomem_res() iteration issue
      resource: Include resource end in walk_*() interfaces
      x86/kexec: Correct KEXEC_BACKUP_SRC_END off-by-one error
      x86/mm: Remove spurious fault pkey check
      x86/mm/vsyscall: Consider vsyscall page part of user address space
      x86/mm: Add vsyscall address helper
      x86/mm: Fix exception table comments
      x86/mm: Add clarifying comments for user addr space
      x86/mm: Break out user address space handling
      x86/mm: Break out kernel address space handling
      x86/mm: Clarify hardware vs. software "error_code"
      x86/mm/tlb: Make lazy TLB mode lazier
      x86/mm/tlb: Add freed_tables element to flush_tlb_info
      x86/mm/tlb: Add freed_tables argument to flush_tlb_mm_range
      smp,cpumask: introduce on_each_cpu_cond_mask
      smp: use __cpumask_set_cpu in on_each_cpu_cond
      ...

commit 42f52e1c59bdb78cad945b2dd34fa1f892239a39
Merge: 0d1b82cd8ac2 11e13696a08e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 23 15:00:03 2018 +0100

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes are:
    
       - Migrate CPU-intense 'misfit' tasks on asymmetric capacity systems,
         to better utilize (much) faster 'big core' CPUs. (Morten Rasmussen,
         Valentin Schneider)
    
       - Topology handling improvements, in particular when CPU capacity
         changes and related load-balancing fixes/improvements (Morten
         Rasmussen)
    
       - ... plus misc other improvements, fixes and updates"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (28 commits)
      sched/completions/Documentation: Add recommendation for dynamic and ONSTACK completions
      sched/completions/Documentation: Clean up the document some more
      sched/completions/Documentation: Fix a couple of punctuation nits
      cpu/SMT: State SMT is disabled even with nosmt and without "=force"
      sched/core: Fix comment regarding nr_iowait_cpu() and get_iowait_load()
      sched/fair: Remove setting task's se->runnable_weight during PELT update
      sched/fair: Disable LB_BIAS by default
      sched/pelt: Fix warning and clean up IRQ PELT config
      sched/topology: Make local variables static
      sched/debug: Use symbolic names for task state constants
      sched/numa: Remove unused numa_stats::nr_running field
      sched/numa: Remove unused code from update_numa_stats()
      sched/debug: Explicitly cast sched_feat() to bool
      sched/core: Disable SD_PREFER_SIBLING on asymmetric CPU capacity domains
      sched/fair: Don't move tasks to lower capacity CPUs unless necessary
      sched/fair: Set rq->rd->overload when misfit
      sched/fair: Wrap rq->rd->overload accesses with READ/WRITE_ONCE()
      sched/core: Change root_domain->overload type to int
      sched/fair: Change 'prefer_sibling' type to bool
      sched/fair: Kick nohz balance if rq->misfit_task_load
      ...

commit 977e4be5eb714c48a67afc26a6c477f24130a1f2
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Oct 20 09:26:49 2018 +0200

    x86/stackprotector: Remove the call to boot_init_stack_canary() from cpu_startup_entry()
    
    The following commit:
    
      d7880812b359 ("idle: Add the stack canary init to cpu_startup_entry()")
    
    ... added an x86 specific boot_init_stack_canary() call to the generic
    cpu_startup_entry() as a temporary hack, with the intention to remove
    the #ifdef CONFIG_X86 later.
    
    More than 5 years later let's finally realize that plan! :-)
    
    While implementing stack protector support for PowerPC, we found
    that calling boot_init_stack_canary() is also needed for PowerPC
    which uses per task (TLS) stack canary like the X86.
    
    However, calling boot_init_stack_canary() would break architectures
    using a global stack canary (ARM, SH, MIPS and XTENSA).
    
    Instead of modifying the #ifdef CONFIG_X86 to an even messier:
    
       #if defined(CONFIG_X86) || defined(CONFIG_PPC)
    
    PowerPC implemented the call to boot_init_stack_canary() in the function
    calling cpu_startup_entry().
    
    Let's try the same cleanup on the x86 side as well.
    
    On x86 we have two functions calling cpu_startup_entry():
    
     - start_secondary()
     - cpu_bringup_and_idle()
    
    start_secondary() already calls boot_init_stack_canary(), so
    it's good, and this patch adds the call to boot_init_stack_canary()
    in cpu_bringup_and_idle().
    
    I.e. now x86 catches up to the rest of the world and the ugly init
    sequence in init/main.c can be removed from cpu_startup_entry().
    
    As a final benefit we can also remove the <linux/stackprotector.h>
    dependency from <linux/sched.h>.
    
    [ mingo: Improved the changelog a bit, added language explaining x86 borkage and sched.h change. ]
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20181020072649.5B59310483E@pc16082vm.idsi0.si.c-s.fr
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4a2e8cae63c4..5b00a816a4b3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -56,7 +56,6 @@
 #include <linux/profile.h>
 #include <linux/rcupdate_wait.h>
 #include <linux/security.h>
-#include <linux/stackprotector.h>
 #include <linux/stop_machine.h>
 #include <linux/suspend.h>
 #include <linux/swait.h>

commit baa9be4ffb55876923dc9716abc0a448e510ba30
Author: Phil Auld <pauld@redhat.com>
Date:   Mon Oct 8 10:36:40 2018 -0400

    sched/fair: Fix throttle_list starvation with low CFS quota
    
    With a very low cpu.cfs_quota_us setting, such as the minimum of 1000,
    distribute_cfs_runtime may not empty the throttled_list before it runs
    out of runtime to distribute. In that case, due to the change from
    c06f04c7048 to put throttled entries at the head of the list, later entries
    on the list will starve.  Essentially, the same X processes will get pulled
    off the list, given CPU time and then, when expired, get put back on the
    head of the list where distribute_cfs_runtime will give runtime to the same
    set of processes leaving the rest.
    
    Fix the issue by setting a bit in struct cfs_bandwidth when
    distribute_cfs_runtime is running, so that the code in throttle_cfs_rq can
    decide to put the throttled entry on the tail or the head of the list.  The
    bit is set/cleared by the callers of distribute_cfs_runtime while they hold
    cfs_bandwidth->lock.
    
    This is easy to reproduce with a handful of CPU consumers. I use 'crash' on
    the live system. In some cases you can simply look at the throttled list and
    see the later entries are not changing:
    
      crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
        1     ffff90b56cb2d200  -976050
        2     ffff90b56cb2cc00  -484925
        3     ffff90b56cb2bc00  -658814
        4     ffff90b56cb2ba00  -275365
        5     ffff90b166a45600  -135138
        6     ffff90b56cb2da00  -282505
        7     ffff90b56cb2e000  -148065
        8     ffff90b56cb2fa00  -872591
        9     ffff90b56cb2c000  -84687
       10     ffff90b56cb2f000  -87237
       11     ffff90b166a40a00  -164582
    
      crash> list cfs_rq.throttled_list -H 0xffff90b54f6ade40 -s cfs_rq.runtime_remaining | paste - - | awk '{print $1"  "$4}' | pr -t -n3
        1     ffff90b56cb2d200  -994147
        2     ffff90b56cb2cc00  -306051
        3     ffff90b56cb2bc00  -961321
        4     ffff90b56cb2ba00  -24490
        5     ffff90b166a45600  -135138
        6     ffff90b56cb2da00  -282505
        7     ffff90b56cb2e000  -148065
        8     ffff90b56cb2fa00  -872591
        9     ffff90b56cb2c000  -84687
       10     ffff90b56cb2f000  -87237
       11     ffff90b166a40a00  -164582
    
    Sometimes it is easier to see by finding a process getting starved and looking
    at the sched_info:
    
      crash> task ffff8eb765994500 sched_info
      PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
        sched_info = {
          pcount = 8,
          run_delay = 697094208,
          last_arrival = 240260125039,
          last_queued = 240260327513
        },
      crash> task ffff8eb765994500 sched_info
      PID: 7800   TASK: ffff8eb765994500  CPU: 16  COMMAND: "cputest"
        sched_info = {
          pcount = 8,
          run_delay = 697094208,
          last_arrival = 240260125039,
          last_queued = 240260327513
        },
    
    Signed-off-by: Phil Auld <pauld@redhat.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: stable@vger.kernel.org
    Fixes: c06f04c70489 ("sched: Fix potential near-infinite distribute_cfs_runtime() loop")
    Link: http://lkml.kernel.org/r/20181008143639.GA4019@pauld.bos.csb
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 455fa330de04..9683f458aec7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -346,6 +346,8 @@ struct cfs_bandwidth {
 	int			nr_periods;
 	int			nr_throttled;
 	u64			throttled_time;
+
+	bool                    distribute_running;
 #endif
 };
 

commit 11d4afd4ff667f9b6178ee8c142c36cb78bd84db
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Sep 25 11:17:42 2018 +0200

    sched/pelt: Fix warning and clean up IRQ PELT config
    
    Create a config for enabling irq load tracking in the scheduler.
    irq load tracking is useful only when irq or paravirtual time is
    accounted but it's only possible with SMP for now.
    
    Also use __maybe_unused to remove the compilation warning in
    update_rq_clock_task() that has been introduced by:
    
      2e62c4743adc ("sched/fair: Remove #ifdefs from scale_rt_capacity()")
    
    Suggested-by: Ingo Molnar <mingo@redhat.com>
    Reported-by: Dou Liyang <douly.fnst@cn.fujitsu.com>
    Reported-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bp@alien8.de
    Cc: dou_liyang@163.com
    Fixes: 2e62c4743adc ("sched/fair: Remove #ifdefs from scale_rt_capacity()")
    Link: http://lkml.kernel.org/r/1537867062-27285-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 632804fa0b12..798b1afd5092 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -862,8 +862,7 @@ struct rq {
 
 	struct sched_avg	avg_rt;
 	struct sched_avg	avg_dl;
-#if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
-#define HAVE_SCHED_AVG_IRQ
+#ifdef CONFIG_HAVE_SCHED_AVG_IRQ
 	struct sched_avg	avg_irq;
 #endif
 	u64			idle_stamp;
@@ -2223,7 +2222,7 @@ static inline unsigned long cpu_util_rt(struct rq *rq)
 }
 #endif
 
-#ifdef HAVE_SCHED_AVG_IRQ
+#ifdef CONFIG_HAVE_SCHED_AVG_IRQ
 static inline unsigned long cpu_util_irq(struct rq *rq)
 {
 	return rq->avg_irq.util_avg;

commit b429f71bca5a4ddd914350a39572692e2ea211e0
Merge: ace8031099f9 6fd98e775f24
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Oct 2 09:43:39 2018 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 1327237a5978b00bcc665c33046c9bae75da1154
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Fri Sep 21 23:18:57 2018 +0530

    sched/numa: Pass destination CPU as a parameter to migrate_task_rq
    
    This additional parameter (new_cpu) is used later for identifying if
    task migration is across nodes.
    
    No functional change.
    
    Specjbb2005 results (8 warehouses)
    Higher bops are better
    
    2 Socket - 2  Node Haswell - X86
    JVMS  Prev    Current  %Change
    4     203353  200668   -1.32036
    1     328205  321791   -1.95427
    
    2 Socket - 4 Node Power8 - PowerNV
    JVMS  Prev    Current  %Change
    1     214384  204848   -4.44809
    
    2 Socket - 2  Node Power9 - PowerNV
    JVMS  Prev    Current  %Change
    4     188553  188098   -0.241311
    1     196273  200351   2.07772
    
    4 Socket - 4  Node Power7 - PowerVM
    JVMS  Prev     Current  %Change
    8     57581.2  58145.9  0.980702
    1     103468   103798   0.318939
    
    Brings out the variance between different specjbb2005 runs.
    
    Some events stats before and after applying the patch.
    
    perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
    Event                     Before          After
    cs                        13,941,377      13,912,183
    migrations                1,157,323       1,155,931
    faults                    382,175         367,139
    cache-misses              54,993,823,500  54,240,196,814
    sched:sched_move_numa     2,005           1,571
    sched:sched_stick_numa    14              9
    sched:sched_swap_numa     529             463
    migrate:mm_migrate_pages  1,573           703
    
    vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
    Event                   Before  After
    numa_hint_faults        67099   50155
    numa_hint_faults_local  58456   45264
    numa_hit                240416  239652
    numa_huge_pte_updates   18      36
    numa_interleave         65      68
    numa_local              240339  239576
    numa_other              77      76
    numa_pages_migrated     1574    680
    numa_pte_updates        77182   71146
    
    perf stats 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
    Event                     Before          After
    cs                        3,176,453       3,156,720
    migrations                30,238          30,354
    faults                    87,869          97,261
    cache-misses              12,544,479,391  12,400,026,826
    sched:sched_move_numa     23              4
    sched:sched_stick_numa    0               0
    sched:sched_swap_numa     6               1
    migrate:mm_migrate_pages  10              20
    
    vmstat 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
    Event                   Before  After
    numa_hint_faults        236     272
    numa_hint_faults_local  201     186
    numa_hit                72293   71362
    numa_huge_pte_updates   0       0
    numa_interleave         26      23
    numa_local              72233   71299
    numa_other              60      63
    numa_pages_migrated     8       2
    numa_pte_updates        0       0
    
    perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                     Before       After
    cs                        8,478,820    8,606,824
    migrations                171,323      155,352
    faults                    307,499      301,409
    cache-misses              240,353,599  157,759,224
    sched:sched_move_numa     214          168
    sched:sched_stick_numa    0            0
    sched:sched_swap_numa     4            3
    migrate:mm_migrate_pages  89           125
    
    vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                   Before  After
    numa_hint_faults        5301    4650
    numa_hint_faults_local  4745    3946
    numa_hit                92943   90489
    numa_huge_pte_updates   0       0
    numa_interleave         899     892
    numa_local              92345   90034
    numa_other              598     455
    numa_pages_migrated     88      124
    numa_pte_updates        5505    4818
    
    perf stats 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                     Before      After
    cs                        2,066,172   2,113,167
    migrations                11,076      10,533
    faults                    149,544     142,727
    cache-misses              10,398,067  5,594,192
    sched:sched_move_numa     43          10
    sched:sched_stick_numa    0           0
    sched:sched_swap_numa     0           0
    migrate:mm_migrate_pages  6           6
    
    vmstat 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                   Before  After
    numa_hint_faults        3552    744
    numa_hint_faults_local  3347    584
    numa_hit                25611   25551
    numa_huge_pte_updates   0       0
    numa_interleave         213     263
    numa_local              25583   25302
    numa_other              28      249
    numa_pages_migrated     6       6
    numa_pte_updates        3535    744
    
    perf stats 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                     Before           After
    cs                        99,358,136       101,227,352
    migrations                4,041,607        4,151,829
    faults                    749,653          745,233
    cache-misses              225,562,543,251  224,669,561,766
    sched:sched_move_numa     771              617
    sched:sched_stick_numa    14               2
    sched:sched_swap_numa     204              187
    migrate:mm_migrate_pages  1,180            316
    
    vmstat 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                   Before  After
    numa_hint_faults        27409   24195
    numa_hint_faults_local  20677   21639
    numa_hit                239988  238331
    numa_huge_pte_updates   0       0
    numa_interleave         0       0
    numa_local              239983  238331
    numa_other              5       0
    numa_pages_migrated     1016    204
    numa_pte_updates        27916   24561
    
    perf stats 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                     Before          After
    cs                        60,899,307      62,738,978
    migrations                544,668         562,702
    faults                    270,834         228,465
    cache-misses              74,543,455,635  75,778,067,952
    sched:sched_move_numa     735             648
    sched:sched_stick_numa    25              13
    sched:sched_swap_numa     174             137
    migrate:mm_migrate_pages  816             733
    
    vmstat 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                   Before  After
    numa_hint_faults        11059   10281
    numa_hint_faults_local  4733    3242
    numa_hit                41384   36338
    numa_huge_pte_updates   0       0
    numa_interleave         0       0
    numa_local              41383   36338
    numa_other              1       0
    numa_pages_migrated     815     706
    numa_pte_updates        11323   10176
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Jirka Hladky <jhladky@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@surriel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1537552141-27815-3-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0b9161241bda..455fa330de04 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1524,7 +1524,7 @@ struct sched_class {
 
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
-	void (*migrate_task_rq)(struct task_struct *p);
+	void (*migrate_task_rq)(struct task_struct *p, int new_cpu);
 
 	void (*task_woken)(struct rq *this_rq, struct task_struct *task);
 

commit a4739eca4456e3d140cc656c5331d42b7465f91d
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Fri Sep 21 23:18:56 2018 +0530

    sched/numa: Stop multiple tasks from moving to the CPU at the same time
    
    Task migration under NUMA balancing can happen in parallel. More than
    one task might choose to migrate to the same CPU at the same time. This
    can result in:
    
    - During task swap, choosing a task that was not part of the evaluation.
    - During task swap, task which just got moved into its preferred node,
      moving to a completely different node.
    - During task swap, task failing to move to the preferred node, will have
      to wait an extra interval for the next migrate opportunity.
    - During task movement, multiple task movements can cause load imbalance.
    
    This problem is more likely if there are more cores per node or more
    nodes in the system.
    
    Use a per run-queue variable to check if NUMA-balance is active on the
    run-queue.
    
    Specjbb2005 results (8 warehouses)
    Higher bops are better
    
    2 Socket - 2  Node Haswell - X86
    JVMS  Prev    Current  %Change
    4     200194  203353   1.57797
    1     311331  328205   5.41995
    
    2 Socket - 4 Node Power8 - PowerNV
    JVMS  Prev    Current  %Change
    1     197654  214384   8.46429
    
    2 Socket - 2  Node Power9 - PowerNV
    JVMS  Prev    Current  %Change
    4     192605  188553   -2.10379
    1     213402  196273   -8.02664
    
    4 Socket - 4  Node Power7 - PowerVM
    JVMS  Prev     Current  %Change
    8     52227.1  57581.2  10.2516
    1     102529   103468   0.915838
    
    There is a regression on power 9 box. If we look at the details,
    that box has a sudden jump in cache-misses with this patch.
    All other parameters seem to be pointing towards NUMA
    consolidation.
    
    perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
    Event                     Before          After
    cs                        13,345,784      13,941,377
    migrations                1,127,820       1,157,323
    faults                    374,736         382,175
    cache-misses              55,132,054,603  54,993,823,500
    sched:sched_move_numa     1,923           2,005
    sched:sched_stick_numa    52              14
    sched:sched_swap_numa     595             529
    migrate:mm_migrate_pages  1,932           1,573
    
    vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Haswell - X86
    Event                   Before  After
    numa_hint_faults        60605   67099
    numa_hint_faults_local  51804   58456
    numa_hit                239945  240416
    numa_huge_pte_updates   14      18
    numa_interleave         60      65
    numa_local              239865  240339
    numa_other              80      77
    numa_pages_migrated     1931    1574
    numa_pte_updates        67823   77182
    
    perf stats 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
    Event                     Before          After
    cs                        3,016,467       3,176,453
    migrations                37,326          30,238
    faults                    115,342         87,869
    cache-misses              11,692,155,554  12,544,479,391
    sched:sched_move_numa     965             23
    sched:sched_stick_numa    8               0
    sched:sched_swap_numa     35              6
    migrate:mm_migrate_pages  1,168           10
    
    vmstat 8th warehouse Single JVM 2 Socket - 2  Node Haswell - X86
    Event                   Before  After
    numa_hint_faults        16286   236
    numa_hint_faults_local  11863   201
    numa_hit                112482  72293
    numa_huge_pte_updates   33      0
    numa_interleave         20      26
    numa_local              112419  72233
    numa_other              63      60
    numa_pages_migrated     1144    8
    numa_pte_updates        32859   0
    
    perf stats 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                     Before       After
    cs                        8,629,724    8,478,820
    migrations                221,052      171,323
    faults                    308,661      307,499
    cache-misses              135,574,913  240,353,599
    sched:sched_move_numa     147          214
    sched:sched_stick_numa    0            0
    sched:sched_swap_numa     2            4
    migrate:mm_migrate_pages  64           89
    
    vmstat 8th warehouse Multi JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                   Before  After
    numa_hint_faults        11481   5301
    numa_hint_faults_local  10968   4745
    numa_hit                89773   92943
    numa_huge_pte_updates   0       0
    numa_interleave         1116    899
    numa_local              89220   92345
    numa_other              553     598
    numa_pages_migrated     62      88
    numa_pte_updates        11694   5505
    
    perf stats 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                     Before     After
    cs                        2,272,887  2,066,172
    migrations                12,206     11,076
    faults                    163,704    149,544
    cache-misses              4,801,186  10,398,067
    sched:sched_move_numa     44         43
    sched:sched_stick_numa    0          0
    sched:sched_swap_numa     0          0
    migrate:mm_migrate_pages  17         6
    
    vmstat 8th warehouse Single JVM 2 Socket - 2  Node Power9 - PowerNV
    Event                   Before  After
    numa_hint_faults        2261    3552
    numa_hint_faults_local  1993    3347
    numa_hit                25726   25611
    numa_huge_pte_updates   0       0
    numa_interleave         239     213
    numa_local              25498   25583
    numa_other              228     28
    numa_pages_migrated     17      6
    numa_pte_updates        2266    3535
    
    perf stats 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                     Before           After
    cs                        117,980,962      99,358,136
    migrations                3,950,220        4,041,607
    faults                    736,979          749,653
    cache-misses              224,976,072,879  225,562,543,251
    sched:sched_move_numa     504              771
    sched:sched_stick_numa    50               14
    sched:sched_swap_numa     239              204
    migrate:mm_migrate_pages  1,260            1,180
    
    vmstat 8th warehouse Multi JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                   Before  After
    numa_hint_faults        18293   27409
    numa_hint_faults_local  11969   20677
    numa_hit                240854  239988
    numa_huge_pte_updates   0       0
    numa_interleave         0       0
    numa_local              240851  239983
    numa_other              3       5
    numa_pages_migrated     1190    1016
    numa_pte_updates        18106   27916
    
    perf stats 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                     Before          After
    cs                        61,053,158      60,899,307
    migrations                551,586         544,668
    faults                    244,174         270,834
    cache-misses              74,326,766,973  74,543,455,635
    sched:sched_move_numa     344             735
    sched:sched_stick_numa    24              25
    sched:sched_swap_numa     140             174
    migrate:mm_migrate_pages  568             816
    
    vmstat 8th warehouse Single JVM 4 Socket - 4  Node Power7 - PowerVM
    Event                   Before  After
    numa_hint_faults        6461    11059
    numa_hint_faults_local  2283    4733
    numa_hit                35661   41384
    numa_huge_pte_updates   0       0
    numa_interleave         0       0
    numa_local              35661   41383
    numa_other              0       1
    numa_pages_migrated     568     815
    numa_pte_updates        6518    11323
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Jirka Hladky <jhladky@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1537552141-27815-2-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4a2e8cae63c4..0b9161241bda 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -783,6 +783,7 @@ struct rq {
 #ifdef CONFIG_NUMA_BALANCING
 	unsigned int		nr_numa_running;
 	unsigned int		nr_preferred_running;
+	unsigned int		numa_migrate_on;
 #endif
 	#define CPU_LOAD_IDX_MAX 5
 	unsigned long		cpu_load[CPU_LOAD_IDX_MAX];

commit 7e6f4c5d600c1c8e2a1d900e65cab319d9b6782e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 29 11:45:21 2018 +0200

    sched/debug: Explicitly cast sched_feat() to bool
    
    LLVM has a warning that tags expressions like:
    
            if (foo && non-bool-const)
    
    This pattern triggers for CONFIG_SCHED_DEBUG=n where sched_feat() ends
    up being whatever bit we select. Avoid the warning with an explicit
    cast to bool.
    
    Reported-by: Philipp Klocke <philipp97kl@gmail.com>
    Tested-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 85b3a2bf6c2b..3a4ef8f73f08 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1401,7 +1401,7 @@ static const_debug __maybe_unused unsigned int sysctl_sched_features =
 	0;
 #undef SCHED_FEAT
 
-#define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
+#define sched_feat(x) !!(sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
 
 #endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
 

commit 757ffdd705ee942fc8150b17942d968601d2a15b
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Jul 4 11:17:47 2018 +0100

    sched/fair: Set rq->rd->overload when misfit
    
    Idle balance is a great opportunity to pull a misfit task. However,
    there are scenarios where misfit tasks are present but idle balance is
    prevented by the overload flag.
    
    A good example of this is a workload of n identical tasks. Let's suppose
    we have a 2+2 Arm big.LITTLE system. We then spawn 4 fairly
    CPU-intensive tasks - for the sake of simplicity let's say they are just
    CPU hogs, even when running on big CPUs.
    
    They are identical tasks, so on an SMP system they should all end at
    (roughly) the same time. However, in our case the LITTLE CPUs are less
    performing than the big CPUs, so tasks running on the LITTLEs will have
    a longer completion time.
    
    This means that the big CPUs will complete their work earlier, at which
    point they should pull the tasks from the LITTLEs. What we want to
    happen is summarized as follows:
    
    a,b,c,d are our CPU-hogging tasks _ signifies idling
    
      LITTLE_0 | a a a a _ _
      LITTLE_1 | b b b b _ _
      ---------|-------------
        big_0  | c c c c a a
        big_1  | d d d d b b
                        ^
                        ^
          Tasks end on the big CPUs, idle balance happens
          and the misfit tasks are pulled straight away
    
    This however won't happen, because currently the overload flag is only
    set when there is any CPU that has more than one runnable task - which
    may very well not be the case here if our CPU-hogging workload is all
    there is to run.
    
    As such, this commit sets the overload flag in update_sg_lb_stats when
    a group is flagged as having a misfit task.
    
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: gaku.inami.xh@renesas.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1530699470-29808-10-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 938063639793..85b3a2bf6c2b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -715,7 +715,11 @@ struct root_domain {
 	cpumask_var_t		span;
 	cpumask_var_t		online;
 
-	/* Indicate more than one runnable task for any CPU */
+	/*
+	 * Indicate pullable load on at least one CPU, e.g:
+	 * - More than one runnable task
+	 * - Running task is misfit
+	 */
 	int			overload;
 
 	/*

commit e90c8fe15a3bf93a23088bcf1a56a0fa391d4e50
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Jul 4 11:17:46 2018 +0100

    sched/fair: Wrap rq->rd->overload accesses with READ/WRITE_ONCE()
    
    This variable can be read and set locklessly within update_sd_lb_stats().
    As such, READ/WRITE_ONCE() are added to make sure nothing terribly wrong
    can happen because of the compiler.
    
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: gaku.inami.xh@renesas.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1530699470-29808-9-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4d181478c5b8..938063639793 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1697,8 +1697,8 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 
 	if (prev_nr < 2 && rq->nr_running >= 2) {
 #ifdef CONFIG_SMP
-		if (!rq->rd->overload)
-			rq->rd->overload = 1;
+		if (!READ_ONCE(rq->rd->overload))
+			WRITE_ONCE(rq->rd->overload, 1);
 #endif
 	}
 

commit 575638d1047eb057a5cdf95cc0b3c084e1279508
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Jul 4 11:17:45 2018 +0100

    sched/core: Change root_domain->overload type to int
    
    sizeof(_Bool) is implementation defined, so let's just go with 'int' as
    is done for other structures e.g. sched_domain_shared->has_idle_cores.
    
    The local 'overload' variable used in update_sd_lb_stats can remain
    bool, as it won't impact any struct layout and can be assigned to the
    root_domain field.
    
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: gaku.inami.xh@renesas.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1530699470-29808-8-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fe17e0be2d7b..4d181478c5b8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -716,7 +716,7 @@ struct root_domain {
 	cpumask_var_t		online;
 
 	/* Indicate more than one runnable task for any CPU */
-	bool			overload;
+	int			overload;
 
 	/*
 	 * The bit corresponding to a CPU gets set here if such CPU has more
@@ -1698,7 +1698,7 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 	if (prev_nr < 2 && rq->nr_running >= 2) {
 #ifdef CONFIG_SMP
 		if (!rq->rd->overload)
-			rq->rd->overload = true;
+			rq->rd->overload = 1;
 #endif
 	}
 

commit e3d6d0cb66f2351cbfd09fbae04eb9804afe9577
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Wed Jul 4 11:17:41 2018 +0100

    sched/fair: Add sched_group per-CPU max capacity
    
    The current sg->min_capacity tracks the lowest per-CPU compute capacity
    available in the sched_group when rt/irq pressure is taken into account.
    Minimum capacity isn't the ideal metric for tracking if a sched_group
    needs offloading to another sched_group for some scenarios, e.g. a
    sched_group with multiple CPUs if only one is under heavy pressure.
    Tracking maximum capacity isn't perfect either but a better choice for
    some situations as it indicates that the sched_group definitely compute
    capacity constrained either due to rt/irq pressure on all CPUs or
    asymmetric CPU capacities (e.g. big.LITTLE).
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: gaku.inami.xh@renesas.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1530699470-29808-4-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7dbf67d147a2..fe17e0be2d7b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1197,6 +1197,7 @@ struct sched_group_capacity {
 	 */
 	unsigned long		capacity;
 	unsigned long		min_capacity;		/* Min per-CPU capacity in group */
+	unsigned long		max_capacity;		/* Max per-CPU capacity in group */
 	unsigned long		next_update;
 	int			imbalance;		/* XXX unrelated to capacity but shared group state */
 

commit 3b1baa6496e6b7ad016342a9d256bdfb072ce902
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Wed Jul 4 11:17:40 2018 +0100

    sched/fair: Add 'group_misfit_task' load-balance type
    
    To maximize throughput in systems with asymmetric CPU capacities (e.g.
    ARM big.LITTLE) load-balancing has to consider task and CPU utilization
    as well as per-CPU compute capacity when load-balancing in addition to
    the current average load based load-balancing policy. Tasks with high
    utilization that are scheduled on a lower capacity CPU need to be
    identified and migrated to a higher capacity CPU if possible to maximize
    throughput.
    
    To implement this additional policy an additional group_type
    (load-balance scenario) is added: 'group_misfit_task'. This represents
    scenarios where a sched_group has one or more tasks that are not
    suitable for its per-CPU capacity. 'group_misfit_task' is only considered
    if the system is not overloaded or imbalanced ('group_imbalanced' or
    'group_overloaded').
    
    Identifying misfit tasks requires the rq lock to be held. To avoid
    taking remote rq locks to examine source sched_groups for misfit tasks,
    each CPU is responsible for tracking misfit tasks themselves and update
    the rq->misfit_task flag. This means checking task utilization when
    tasks are scheduled and on sched_tick.
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: gaku.inami.xh@renesas.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1530699470-29808-3-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0f36adc31ba5..7dbf67d147a2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -842,6 +842,8 @@ struct rq {
 
 	unsigned char		idle_balance;
 
+	unsigned long		misfit_task_load;
+
 	/* For active balancing */
 	int			active_balance;
 	int			push_cpu;

commit df054e8445a4011e3d693c2268129c0456108663
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Wed Jul 4 11:17:39 2018 +0100

    sched/topology: Add static_key for asymmetric CPU capacity optimizations
    
    The existing asymmetric CPU capacity code should cause minimal overhead
    for others. Putting it behind a static_key, it has been done for SMT
    optimizations, would make it easier to extend and improve without
    causing harm to others moving forward.
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: gaku.inami.xh@renesas.com
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1530699470-29808-2-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4a2e8cae63c4..0f36adc31ba5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1185,6 +1185,7 @@ DECLARE_PER_CPU(int, sd_llc_id);
 DECLARE_PER_CPU(struct sched_domain_shared *, sd_llc_shared);
 DECLARE_PER_CPU(struct sched_domain *, sd_numa);
 DECLARE_PER_CPU(struct sched_domain *, sd_asym);
+extern struct static_key_false sched_asym_cpucapacity;
 
 struct sched_group_capacity {
 	atomic_t		ref;

commit 0ad4e3dfe6cf3f207e61cbd8e3e4a943f8c1ad20
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Wed Jun 20 22:32:50 2018 +0530

    sched/numa: Modify migrate_swap() to accept additional parameters
    
    There are checks in migrate_swap_stop() that check if the task/CPU
    combination is as per migrate_swap_arg before migrating.
    
    However atleast one of the two tasks to be swapped by migrate_swap() could
    have migrated to a completely different CPU before updating the
    migrate_swap_arg. The new CPU where the task is currently running could
    be a different node too. If the task has migrated, numa balancer might
    end up placing a task in a wrong node.  Instead of achieving node
    consolidation, it may end up spreading the load across nodes.
    
    To avoid that pass the CPUs as additional parameters.
    
    While here, place migrate_swap under CONFIG_NUMA_BALANCING.
    
    Running SPECjbb2005 on a 4 node machine and comparing bops/JVM
    JVMS  LAST_PATCH  WITH_PATCH  %CHANGE
    16    25377.3     25226.6     -0.59
    1     72287       73326       1.437
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1529514181-9842-10-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 614170d9b1aa..4a2e8cae63c4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1099,7 +1099,8 @@ enum numa_faults_stats {
 };
 extern void sched_setnuma(struct task_struct *p, int node);
 extern int migrate_task_to(struct task_struct *p, int cpu);
-extern int migrate_swap(struct task_struct *, struct task_struct *);
+extern int migrate_swap(struct task_struct *p, struct task_struct *t,
+			int cpu, int scpu);
 extern void init_numa_balancing(unsigned long clone_flags, struct task_struct *p);
 #else
 static inline void

commit 2e62c4743adc4c7bfcbc1f45118fc7bec58cf30a
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jul 19 14:00:06 2018 +0200

    sched/fair: Remove #ifdefs from scale_rt_capacity()
    
    Reuse cpu_util_irq() that has been defined for schedutil and set irq util
    to 0 when !CONFIG_IRQ_TIME_ACCOUNTING.
    
    But the compiler is not able to optimize the sequence (at least with
    aarch64 GCC 7.2.1):
    
            free *= (max - irq);
            free /= max;
    
    when irq is fixed to 0
    
    Add a new inline function scale_irq_capacity() that will scale utilization
    when irq is accounted. Reuse this funciton in schedutil which applies
    similar formula.
    
    Suggested-by: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: rjw@rjwysocki.net
    Link: http://lkml.kernel.org/r/1532001606-6689-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ebb4b3c3ece7..614170d9b1aa 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -856,6 +856,7 @@ struct rq {
 	struct sched_avg	avg_rt;
 	struct sched_avg	avg_dl;
 #if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
+#define HAVE_SCHED_AVG_IRQ
 	struct sched_avg	avg_irq;
 #endif
 	u64			idle_stamp;
@@ -2210,17 +2211,32 @@ static inline unsigned long cpu_util_rt(struct rq *rq)
 {
 	return READ_ONCE(rq->avg_rt.util_avg);
 }
+#endif
 
-#if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
+#ifdef HAVE_SCHED_AVG_IRQ
 static inline unsigned long cpu_util_irq(struct rq *rq)
 {
 	return rq->avg_irq.util_avg;
 }
+
+static inline
+unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)
+{
+	util *= (max - irq);
+	util /= max;
+
+	return util;
+
+}
 #else
 static inline unsigned long cpu_util_irq(struct rq *rq)
 {
 	return 0;
 }
 
-#endif
+static inline
+unsigned long scale_irq_capacity(unsigned long util, unsigned long irq, unsigned long max)
+{
+	return util;
+}
 #endif

commit 5fd778915ad29184a5ff8eb82d1118f6916b79e4
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:14 2018 +0200

    sched/sysctl: Remove unused sched_time_avg_ms sysctl
    
    /proc/sys/kernel/sched_time_avg_ms entry is not used anywhere,
    remove it.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Luis R. Rodriguez <mcgrof@kernel.org>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/1530200714-4504-12-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 14aac2d2de80..ebb4b3c3ece7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1713,7 +1713,6 @@ extern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);
 
 extern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);
 
-extern const_debug unsigned int sysctl_sched_time_avg;
 extern const_debug unsigned int sysctl_sched_nr_migrate;
 extern const_debug unsigned int sysctl_sched_migration_cost;
 

commit bbb62c0b024a1c721232667fa1d625cf6b3a555b
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:13 2018 +0200

    sched/core: Remove the rt_avg code
    
    rt_avg is not used anywhere anymore, so we can remove all related code.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/1530200714-4504-11-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 061d51fb5b44..14aac2d2de80 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -853,8 +853,6 @@ struct rq {
 
 	struct list_head cfs_tasks;
 
-	u64			rt_avg;
-	u64			age_stamp;
 	struct sched_avg	avg_rt;
 	struct sched_avg	avg_dl;
 #if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
@@ -1719,11 +1717,6 @@ extern const_debug unsigned int sysctl_sched_time_avg;
 extern const_debug unsigned int sysctl_sched_nr_migrate;
 extern const_debug unsigned int sysctl_sched_migration_cost;
 
-static inline u64 sched_avg_period(void)
-{
-	return (u64)sysctl_sched_time_avg * NSEC_PER_MSEC / 2;
-}
-
 #ifdef CONFIG_SCHED_HRTICK
 
 /*
@@ -1760,8 +1753,6 @@ unsigned long arch_scale_freq_capacity(int cpu)
 #endif
 
 #ifdef CONFIG_SMP
-extern void sched_avg_update(struct rq *rq);
-
 #ifndef arch_scale_cpu_capacity
 static __always_inline
 unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
@@ -1772,12 +1763,6 @@ unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
 	return SCHED_CAPACITY_SCALE;
 }
 #endif
-
-static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta)
-{
-	rq->rt_avg += rt_delta * arch_scale_freq_capacity(cpu_of(rq));
-	sched_avg_update(rq);
-}
 #else
 #ifndef arch_scale_cpu_capacity
 static __always_inline
@@ -1786,8 +1771,6 @@ unsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)
 	return SCHED_CAPACITY_SCALE;
 }
 #endif
-static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }
-static inline void sched_avg_update(struct rq *rq) { }
 #endif
 
 struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)

commit dfa444dc2ff62edbaf1ff95ed22dd2ce8a5715da
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:11 2018 +0200

    sched/cpufreq: Remove sugov_aggregate_util()
    
    There is no reason why sugov_get_util() and sugov_aggregate_util()
    were in fact separate functions.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    [ Rebased after adding irq tracking and fixed some compilation errors. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Link: http://lkml.kernel.org/r/1530200714-4504-9-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b2833e2b4b6a..061d51fb5b44 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2226,7 +2226,7 @@ static inline unsigned long cpu_util_cfs(struct rq *rq)
 
 static inline unsigned long cpu_util_rt(struct rq *rq)
 {
-	return rq->avg_rt.util_avg;
+	return READ_ONCE(rq->avg_rt.util_avg);
 }
 
 #if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)

commit 9033ea11889f88f243445495f72441e22256d5e9
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:10 2018 +0200

    cpufreq/schedutil: Take time spent in interrupts into account
    
    The time spent executing IRQ handlers can be significant but it is not reflected
    in the utilization of CPU when deciding to choose an OPP. Now that we have
    access to this metric, schedutil can take it into account when selecting
    the OPP for a CPU.
    
    RQS utilization don't see the time spend under interrupt context and report
    their value in the normal context time window. We need to compensate this when
    adding interrupt utilization
    
    The CPU utilization is:
    
      IRQ util_avg + (1 - IRQ util_avg / max capacity ) * /Sum rq util_avg
    
    A test with iperf on hikey (octo arm64) gives the following speedup:
    
     iperf -c server_address -r -t 5
    
     w/o patch              w/ patch
     Tx 276 Mbits/sec       304 Mbits/sec +10%
     Rx 299 Mbits/sec       328 Mbits/sec  +9%
    
     8 iterations
     stdev is lower than 1%
    
    Only WFI idle state is enabled (shallowest idle state).
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Link: http://lkml.kernel.org/r/1530200714-4504-8-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b26d0c9948dd..b2833e2b4b6a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2228,4 +2228,17 @@ static inline unsigned long cpu_util_rt(struct rq *rq)
 {
 	return rq->avg_rt.util_avg;
 }
+
+#if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
+static inline unsigned long cpu_util_irq(struct rq *rq)
+{
+	return rq->avg_irq.util_avg;
+}
+#else
+static inline unsigned long cpu_util_irq(struct rq *rq)
+{
+	return 0;
+}
+
+#endif
 #endif

commit 91c27493e78df6849baaa21a9d66e26de8b875c0
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:09 2018 +0200

    sched/irq: Add IRQ utilization tracking
    
    interrupt and steal time are the only remaining activities tracked by
    rt_avg. Like for sched classes, we can use PELT to track their average
    utilization of the CPU. But unlike sched class, we don't track when
    entering/leaving interrupt; Instead, we take into account the time spent
    under interrupt context when we update rqs' clock (rq_clock_task).
    This also means that we have to decay the normal context time and account
    for interrupt time during the update.
    
    That's also important to note that because:
    
      rq_clock == rq_clock_task + interrupt time
    
    and rq_clock_task is used by a sched class to compute its utilization, the
    util_avg of a sched class only reflects the utilization of the time spent
    in normal context and not of the whole time of the CPU. The utilization of
    interrupt gives an more accurate level of utilization of CPU.
    
    The CPU utilization is:
    
      avg_irq + (1 - avg_irq / max capacity) * /Sum avg_rq
    
    Most of the time, avg_irq is small and neglictible so the use of the
    approximation CPU utilization = /Sum avg_rq was enough.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/1530200714-4504-7-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9028f268f867..b26d0c9948dd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -857,6 +857,9 @@ struct rq {
 	u64			age_stamp;
 	struct sched_avg	avg_rt;
 	struct sched_avg	avg_dl;
+#if defined(CONFIG_IRQ_TIME_ACCOUNTING) || defined(CONFIG_PARAVIRT_TIME_ACCOUNTING)
+	struct sched_avg	avg_irq;
+#endif
 	u64			idle_stamp;
 	u64			avg_idle;
 

commit 8cc90515a4fa419ccfc4703ff127699cdcb96839
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:08 2018 +0200

    cpufreq/schedutil: Use DL utilization tracking
    
    Now that we have both the DL class bandwidth requirement and the DL class
    utilization, we can detect when CPU is fully used so we should run at max.
    Otherwise, we keep using the DL bandwidth requirement to define the
    utilization of the CPU.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Link: http://lkml.kernel.org/r/1530200714-4504-6-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ab8b5296b5f6..9028f268f867 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2199,11 +2199,16 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif
 
 #ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
-static inline unsigned long cpu_util_dl(struct rq *rq)
+static inline unsigned long cpu_bw_dl(struct rq *rq)
 {
 	return (rq->dl.running_bw * SCHED_CAPACITY_SCALE) >> BW_SHIFT;
 }
 
+static inline unsigned long cpu_util_dl(struct rq *rq)
+{
+	return READ_ONCE(rq->avg_dl.util_avg);
+}
+
 static inline unsigned long cpu_util_cfs(struct rq *rq)
 {
 	unsigned long util = READ_ONCE(rq->cfs.avg.util_avg);

commit 3727e0e16340cbdf83818f5bf0113505c6876057
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:07 2018 +0200

    sched/dl: Add dl_rq utilization tracking
    
    Similarly to what happens with RT tasks, CFS tasks can be preempted by DL
    tasks and the CFS's utilization might no longer describes the real
    utilization level.
    
    Current DL bandwidth reflects the requirements to meet deadline when tasks are
    enqueued but not the current utilization of the DL sched class. We track
    DL class utilization to estimate the system utilization.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/1530200714-4504-5-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 405dd9ba6b39..ab8b5296b5f6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -856,6 +856,7 @@ struct rq {
 	u64			rt_avg;
 	u64			age_stamp;
 	struct sched_avg	avg_rt;
+	struct sched_avg	avg_dl;
 	u64			idle_stamp;
 	u64			avg_idle;
 

commit 371bf42732694d142b0de026e152266c039b97d3
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:05 2018 +0200

    sched/rt: Add rt_rq utilization tracking
    
    schedutil governor relies on cfs_rq's util_avg to choose the OPP when CFS
    tasks are running. When the CPU is overloaded by CFS and RT tasks, CFS tasks
    are preempted by RT tasks and in this case util_avg reflects the remaining
    capacity but not what CFS want to use. In such case, schedutil can select a
    lower OPP whereas the CPU is overloaded. In order to have a more accurate
    view of the utilization of the CPU, we track the utilization of RT tasks.
    Only util_avg is correctly tracked but not load_avg and runnable_load_avg
    which are useless for rt_rq.
    
    rt_rq uses rq_clock_task and cfs_rq uses cfs_rq_clock_task but they are
    the same at the root group level, so the PELT windows of the util_sum are
    aligned.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/1530200714-4504-3-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 00d6f2594c4e..405dd9ba6b39 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -594,6 +594,7 @@ struct rt_rq {
 	unsigned long		rt_nr_total;
 	int			overloaded;
 	struct plist_head	pushable_tasks;
+
 #endif /* CONFIG_SMP */
 	int			rt_queued;
 
@@ -854,6 +855,7 @@ struct rq {
 
 	u64			rt_avg;
 	u64			age_stamp;
+	struct sched_avg	avg_rt;
 	u64			idle_stamp;
 	u64			avg_idle;
 
@@ -2212,4 +2214,9 @@ static inline unsigned long cpu_util_cfs(struct rq *rq)
 
 	return util;
 }
+
+static inline unsigned long cpu_util_rt(struct rq *rq)
+{
+	return rq->avg_rt.util_avg;
+}
 #endif

commit c079629862b20c101e8336362a8b042ec7d942fe
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Jun 28 17:45:04 2018 +0200

    sched/pelt: Move PELT related code in a dedicated file
    
    We want to track rt_rq's utilization as a part of the estimation of the
    whole rq's utilization. This is necessary because rt tasks can steal
    utilization to cfs tasks and make them lighter than they are.
    As we want to use the same load tracking mecanism for both and prevent
    useless dependency between cfs and rt code, PELT code is moved in a
    dedicated file.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: claudio@evidence.eu.com
    Cc: daniel.lezcano@linaro.org
    Cc: dietmar.eggemann@arm.com
    Cc: joel@joelfernandes.org
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: patrick.bellasi@arm.com
    Cc: quentin.perret@arm.com
    Cc: rjw@rjwysocki.net
    Cc: valentin.schneider@arm.com
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/1530200714-4504-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c7742dcc136c..00d6f2594c4e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -673,7 +673,26 @@ struct dl_rq {
 	u64			bw_ratio;
 };
 
+#ifdef CONFIG_FAIR_GROUP_SCHED
+/* An entity is a task if it doesn't "own" a runqueue */
+#define entity_is_task(se)	(!se->my_q)
+#else
+#define entity_is_task(se)	1
+#endif
+
 #ifdef CONFIG_SMP
+/*
+ * XXX we want to get rid of these helpers and use the full load resolution.
+ */
+static inline long se_weight(struct sched_entity *se)
+{
+	return scale_load_down(se->load.weight);
+}
+
+static inline long se_runnable(struct sched_entity *se)
+{
+	return scale_load_down(se->runnable_weight);
+}
 
 static inline bool sched_asym_prefer(int a, int b)
 {

commit 512ac999d2755d2b7109e996a76b6fb8b888631d
Author: Xunlei Pang <xlpang@linux.alibaba.com>
Date:   Wed Jun 20 18:18:33 2018 +0800

    sched/fair: Fix bandwidth timer clock drift condition
    
    I noticed that cgroup task groups constantly get throttled even
    if they have low CPU usage, this causes some jitters on the response
    time to some of our business containers when enabling CPU quotas.
    
    It's very simple to reproduce:
    
      mkdir /sys/fs/cgroup/cpu/test
      cd /sys/fs/cgroup/cpu/test
      echo 100000 > cpu.cfs_quota_us
      echo $$ > tasks
    
    then repeat:
    
      cat cpu.stat | grep nr_throttled  # nr_throttled will increase steadily
    
    After some analysis, we found that cfs_rq::runtime_remaining will
    be cleared by expire_cfs_rq_runtime() due to two equal but stale
    "cfs_{b|q}->runtime_expires" after period timer is re-armed.
    
    The current condition to judge clock drift in expire_cfs_rq_runtime()
    is wrong, the two runtime_expires are actually the same when clock
    drift happens, so this condtion can never hit. The orginal design was
    correctly done by this commit:
    
      a9cf55b28610 ("sched: Expire invalid runtime")
    
    ... but was changed to be the current implementation due to its locking bug.
    
    This patch introduces another way, it adds a new field in both structures
    cfs_rq and cfs_bandwidth to record the expiration update sequence, and
    uses them to figure out if clock drift happens (true if they are equal).
    
    Signed-off-by: Xunlei Pang <xlpang@linux.alibaba.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 51f2176d74ac ("sched/fair: Fix unlocked reads of some cfs_b->quota/period")
    Link: http://lkml.kernel.org/r/20180620101834.24455-1-xlpang@linux.alibaba.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 27ddec334601..c7742dcc136c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -334,9 +334,10 @@ struct cfs_bandwidth {
 	u64			runtime;
 	s64			hierarchical_quota;
 	u64			runtime_expires;
+	int			expires_seq;
 
-	int			idle;
-	int			period_active;
+	short			idle;
+	short			period_active;
 	struct hrtimer		period_timer;
 	struct hrtimer		slack_timer;
 	struct list_head	throttled_cfs_rq;
@@ -551,6 +552,7 @@ struct cfs_rq {
 
 #ifdef CONFIG_CFS_BANDWIDTH
 	int			runtime_enabled;
+	int			expires_seq;
 	u64			runtime_expires;
 	s64			runtime_remaining;
 

commit 296b2ffe7fa9ed756c41415c6b1512bc4ad687b1
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Jun 26 15:53:22 2018 +0200

    sched/rt: Fix call to cpufreq_update_util()
    
    With commit:
    
      8f111bc357aa ("cpufreq/schedutil: Rewrite CPUFREQ_RT support")
    
    the schedutil governor uses rq->rt.rt_nr_running to detect whether an
    RT task is currently running on the CPU and to set frequency to max
    if necessary.
    
    cpufreq_update_util() is called in enqueue/dequeue_top_rt_rq() but
    rq->rt.rt_nr_running has not been updated yet when dequeue_top_rt_rq() is
    called so schedutil still considers that an RT task is running when the
    last task is dequeued. The update of rq->rt.rt_nr_running happens later
    in dequeue_rt_stack().
    
    In fact, we can take advantage of the sequence that the dequeue then
    re-enqueue rt entities when a rt task is enqueued or dequeued;
    As a result enqueue_top_rt_rq() is always called when a task is
    enqueued or dequeued and also when groups are throttled or unthrottled.
    The only place that not use enqueue_top_rt_rq() is when root rt_rq is
    throttled.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: efault@gmx.de
    Cc: juri.lelli@redhat.com
    Cc: patrick.bellasi@arm.com
    Cc: viresh.kumar@linaro.org
    Fixes: 8f111bc357aa ('cpufreq/schedutil: Rewrite CPUFREQ_RT support')
    Link: http://lkml.kernel.org/r/1530021202-21695-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6601baf2361c..27ddec334601 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -609,6 +609,11 @@ struct rt_rq {
 #endif
 };
 
+static inline bool rt_rq_is_runnable(struct rt_rq *rt_rq)
+{
+	return rt_rq->rt_queued && rt_rq->rt_nr_running;
+}
+
 /* Deadline class' related fields in a runqueue */
 struct dl_rq {
 	/* runqueue is an rbtree, ordered by deadline */

commit f7f4e7fc6c517708738d1d1984b170e9475a130f
Merge: d9b446e294f2 2539fc82aa9b
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 4 17:45:38 2018 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - power-aware scheduling improvements (Patrick Bellasi)
    
     - NUMA balancing improvements (Mel Gorman)
    
     - vCPU scheduling fixes (Rohit Jain)
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/fair: Update util_est before updating schedutil
      sched/cpufreq: Modify aggregate utilization to always include blocked FAIR utilization
      sched/deadline/Documentation: Add overrun signal and GRUB-PA documentation
      sched/core: Distinguish between idle_cpu() calls based on desired effect, introduce available_idle_cpu()
      sched/wait: Include <linux/wait.h> in <linux/swait.h>
      sched/numa: Stagger NUMA balancing scan periods for new threads
      sched/core: Don't schedule threads on pre-empted vCPUs
      sched/fair: Avoid calling sync_entity_load_avg() unnecessarily
      sched/fair: Rearrange select_task_rq_fair() to optimize it

commit 595058b6675e4d2a70dcd867c84d922975f9d22b
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed May 30 15:49:40 2018 -0700

    sched/headers: Fix typo
    
    I cannot spell 'throttling'.
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180530224940.17839-1-dave@stgolabs.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1f0a4bc6a39d..cb467c221b15 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -983,7 +983,7 @@ static inline void rq_clock_skip_update(struct rq *rq)
 }
 
 /*
- * See rt task throttoling, which is the only time a skip
+ * See rt task throttling, which is the only time a skip
  * request is cancelled.
  */
 static inline void rq_clock_cancel_skipupdate(struct rq *rq)

commit 0548dc5cde19e88b8495cb74e3893d8c8713392a
Merge: bb4e30a48045 4ff648decf47
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri May 25 08:04:39 2018 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit f6a3463063f42d9fb2c78f386437a822e0ad1792
Author: Mathieu Malaterre <malat@debian.org>
Date:   Wed May 16 21:53:47 2018 +0200

    sched/debug: Move the print_rt_rq() and print_dl_rq() declarations to kernel/sched/sched.h
    
    In the following commit:
    
      6b55c9654fcc ("sched/debug: Move print_cfs_rq() declaration to kernel/sched/sched.h")
    
    the print_cfs_rq() prototype was added to <kernel/sched/sched.h>,
    right next to the prototypes for print_cfs_stats(), print_rt_stats()
    and print_dl_stats().
    
    Finish this previous commit and also move related prototypes for
    print_rt_rq() and print_dl_rq().
    
    Remove existing extern declarations now that they not needed anymore.
    
    Silences the following GCC warning, triggered by W=1:
    
      kernel/sched/debug.c:573:6: warning: no previous prototype for ‘print_rt_rq’ [-Wmissing-prototypes]
      kernel/sched/debug.c:603:6: warning: no previous prototype for ‘print_dl_rq’ [-Wmissing-prototypes]
    
    Signed-off-by: Mathieu Malaterre <malat@debian.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180516195348.30426-1-malat@debian.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 15750c222ca2..1f0a4bc6a39d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2025,8 +2025,9 @@ extern bool sched_debug_enabled;
 extern void print_cfs_stats(struct seq_file *m, int cpu);
 extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void print_dl_stats(struct seq_file *m, int cpu);
-extern void
-print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
+extern void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
+extern void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq);
+extern void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq);
 #ifdef CONFIG_NUMA_BALANCING
 extern void
 show_numa_stats(struct task_struct *p, struct seq_file *m);

commit 1378447598432513d94ce2c607c412dc4f260f31
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri May 4 16:41:09 2018 +0100

    sched/numa: Stagger NUMA balancing scan periods for new threads
    
    Threads share an address space and each can change the protections of the
    same address space to trap NUMA faults. This is redundant and potentially
    counter-productive as any thread doing the update will suffice. Potentially
    only one thread is required but that thread may be idle or it may not have
    any locality concerns and pick an unsuitable scan rate.
    
    This patch uses independent scan period but they are staggered based on
    the number of address space users when the thread is created.  The intent
    is that threads will avoid scanning at the same time and have a chance
    to adapt their scan rate later if necessary. This reduces the total scan
    activity early in the lifetime of the threads.
    
    The different in headline performance across a range of machines and
    workloads is marginal but the system CPU usage is reduced as well as overall
    scan activity.  The following is the time reported by NAS Parallel Benchmark
    using unbound openmp threads and a D size class:
    
                                  4.17.0-rc1             4.17.0-rc1
                                     vanilla           stagger-v1r1
            Time bt.D      442.77 (   0.00%)      419.70 (   5.21%)
            Time cg.D      171.90 (   0.00%)      180.85 (  -5.21%)
            Time ep.D       33.10 (   0.00%)       32.90 (   0.60%)
            Time is.D        9.59 (   0.00%)        9.42 (   1.77%)
            Time lu.D      306.75 (   0.00%)      304.65 (   0.68%)
            Time mg.D       54.56 (   0.00%)       52.38 (   4.00%)
            Time sp.D     1020.03 (   0.00%)      903.77 (  11.40%)
            Time ua.D      400.58 (   0.00%)      386.49 (   3.52%)
    
    Note it's not a universal win but we have no prior knowledge of which
    thread matters but the number of threads created often exceeds the size
    of the node when the threads are not bound. However, there is a reducation
    of overall system CPU usage:
    
                                        4.17.0-rc1             4.17.0-rc1
                                           vanilla           stagger-v1r1
            sys-time-bt.D         48.78 (   0.00%)       48.22 (   1.15%)
            sys-time-cg.D         25.31 (   0.00%)       26.63 (  -5.22%)
            sys-time-ep.D          1.65 (   0.00%)        0.62 (  62.42%)
            sys-time-is.D         40.05 (   0.00%)       24.45 (  38.95%)
            sys-time-lu.D         37.55 (   0.00%)       29.02 (  22.72%)
            sys-time-mg.D         47.52 (   0.00%)       34.92 (  26.52%)
            sys-time-sp.D        119.01 (   0.00%)      109.05 (   8.37%)
            sys-time-ua.D         51.52 (   0.00%)       45.13 (  12.40%)
    
    NUMA scan activity is also reduced:
    
            NUMA alloc local               1042828     1342670
            NUMA base PTE updates        140481138    93577468
            NUMA huge PMD updates           272171      180766
            NUMA page range updates      279832690   186129660
            NUMA hint faults               1395972     1193897
            NUMA hint local faults          877925      855053
            NUMA hint local percent             62          71
            NUMA pages migrated           12057909     9158023
    
    Similar observations are made for other thread-intensive workloads. System
    CPU usage is lower even though the headline gains in performance tend to be
    small. For example, specjbb 2005 shows almost no difference in performance
    but scan activity is reduced by a third on a 4-socket box. I didn't find
    a workload (thread intensive or otherwise) that suffered badly.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20180504154109.mvrha2qo5wdl65vr@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 15750c222ca2..c9895d35c5f7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1069,6 +1069,12 @@ enum numa_faults_stats {
 extern void sched_setnuma(struct task_struct *p, int node);
 extern int migrate_task_to(struct task_struct *p, int cpu);
 extern int migrate_swap(struct task_struct *, struct task_struct *);
+extern void init_numa_balancing(unsigned long clone_flags, struct task_struct *p);
+#else
+static inline void
+init_numa_balancing(unsigned long clone_flags, struct task_struct *p)
+{
+}
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_SMP

commit adcc8da8859bee9548bb6d323b1e8de8a7252acd
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Wed Apr 4 09:15:39 2018 -0700

    sched/core: Simplify helpers for rq clock update skip requests
    
    By renaming the functions we can get rid of the skip parameter
    and have better code redability. It makes zero sense to have
    things such as:
    
      rq_clock_skip_update(rq, false)
    
    When the skip request is in fact not going to happen. Ever. Rename
    things such that we end up with:
    
      rq_clock_skip_update(rq)
      rq_clock_cancel_skipupdate(rq)
    
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: matt@codeblueprint.co.uk
    Cc: rostedt@goodmis.org
    Link: http://lkml.kernel.org/r/20180404161539.nhadkff2aats74jh@linux-n805
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c3deaee7a7a2..15750c222ca2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -976,13 +976,20 @@ static inline u64 rq_clock_task(struct rq *rq)
 	return rq->clock_task;
 }
 
-static inline void rq_clock_skip_update(struct rq *rq, bool skip)
+static inline void rq_clock_skip_update(struct rq *rq)
 {
 	lockdep_assert_held(&rq->lock);
-	if (skip)
-		rq->clock_update_flags |= RQCF_REQ_SKIP;
-	else
-		rq->clock_update_flags &= ~RQCF_REQ_SKIP;
+	rq->clock_update_flags |= RQCF_REQ_SKIP;
+}
+
+/*
+ * See rt task throttoling, which is the only time a skip
+ * request is cancelled.
+ */
+static inline void rq_clock_cancel_skipupdate(struct rq *rq)
+{
+	lockdep_assert_held(&rq->lock);
+	rq->clock_update_flags &= ~RQCF_REQ_SKIP;
 }
 
 struct rq_flags {

commit a07630b8b2c16f82fd5b71d890079f4dd7599c1d
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Mar 9 09:52:44 2018 +0000

    sched/cpufreq/schedutil: Use util_est for OPP selection
    
    When schedutil looks at the CPU utilization, the current PELT value for
    that CPU is returned straight away. In certain scenarios this can have
    undesired side effects and delays on frequency selection.
    
    For example, since the task utilization is decayed at wakeup time, a
    long sleeping big task newly enqueued does not add immediately a
    significant contribution to the target CPU. This introduces some latency
    before schedutil will be able to detect the best frequency required by
    that task.
    
    Moreover, the PELT signal build-up time is a function of the current
    frequency, because of the scale invariant load tracking support. Thus,
    starting from a lower frequency, the utilization build-up time will
    increase even more and further delays the selection of the actual
    frequency which better serves the task requirements.
    
    In order to reduce these kind of latencies, we integrate the usage
    of the CPU's estimated utilization in the sugov_get_util function.
    
    This allows to properly consider the expected utilization of a CPU which,
    for example, has just got a big task running after a long sleep period.
    Ultimately this allows to select the best frequency to run a task
    right after its wake-up.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Acked-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@android.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: http://lkml.kernel.org/r/20180309095245.11071-4-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 22909ffc04fb..c3deaee7a7a2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2163,6 +2163,13 @@ static inline unsigned long cpu_util_dl(struct rq *rq)
 
 static inline unsigned long cpu_util_cfs(struct rq *rq)
 {
-	return rq->cfs.avg.util_avg;
+	unsigned long util = READ_ONCE(rq->cfs.avg.util_avg);
+
+	if (sched_feat(UTIL_EST)) {
+		util = max_t(unsigned long, util,
+			     READ_ONCE(rq->cfs.avg.util_est.enqueued));
+	}
+
+	return util;
 }
 #endif

commit f643ea2207010db26f17fca99db031bad87c8461
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Feb 13 11:31:17 2018 +0100

    sched/nohz: Stop NOHZ stats when decayed
    
    Stopped the periodic update of blocked load when all idle CPUs have fully
    decayed. We introduce a new nohz.has_blocked that reflect if some idle
    CPUs has blocked load that have to be periodiccally updated. nohz.has_blocked
    is set everytime that a Idle CPU can have blocked load and it is then clear
    when no more blocked load has been detected during an update. We don't need
    atomic operation but only to make cure of the right ordering when updating
    nohz.idle_cpus_mask and nohz.has_blocked.
    
    Suggested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: brendan.jackman@arm.com
    Cc: dietmar.eggemann@arm.com
    Cc: morten.rasmussen@foss.arm.com
    Cc: valentin.schneider@arm.com
    Link: http://lkml.kernel.org/r/1518517879-2280-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 818f22dbc7ea..22909ffc04fb 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -763,6 +763,7 @@ struct rq {
 #ifdef CONFIG_SMP
 	unsigned long		last_load_update_tick;
 	unsigned long		last_blocked_load_update_tick;
+	unsigned int		has_blocked_load;
 #endif /* CONFIG_SMP */
 	unsigned int		nohz_tick_stopped;
 	atomic_t nohz_flags;

commit 00357f5ec5d67a52a175da6f29f85c2c19d59bc8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 21 15:06:50 2017 +0100

    sched/nohz: Clean up nohz enter/exit
    
    The primary observation is that nohz enter/exit is always from the
    current CPU, therefore NOHZ_TICK_STOPPED does not in fact need to be
    an atomic.
    
    Secondary is that we appear to have 2 nearly identical hooks in the
    nohz enter code, set_cpu_sd_state_idle() and
    nohz_balance_enter_idle(). Fold the whole set_cpu_sd_state thing into
    nohz_balance_{enter,exit}_idle.
    
    Removes an atomic op from both enter and exit paths.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 21381d276709..818f22dbc7ea 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -764,6 +764,7 @@ struct rq {
 	unsigned long		last_load_update_tick;
 	unsigned long		last_blocked_load_update_tick;
 #endif /* CONFIG_SMP */
+	unsigned int		nohz_tick_stopped;
 	atomic_t nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 
@@ -2035,11 +2036,9 @@ extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);
 
 #ifdef CONFIG_NO_HZ_COMMON
-#define NOHZ_TICK_STOPPED_BIT	0
-#define NOHZ_BALANCE_KICK_BIT	1
-#define NOHZ_STATS_KICK_BIT	2
+#define NOHZ_BALANCE_KICK_BIT	0
+#define NOHZ_STATS_KICK_BIT	1
 
-#define NOHZ_TICK_STOPPED	BIT(NOHZ_TICK_STOPPED_BIT)
 #define NOHZ_BALANCE_KICK	BIT(NOHZ_BALANCE_KICK_BIT)
 #define NOHZ_STATS_KICK		BIT(NOHZ_STATS_KICK_BIT)
 
@@ -2047,9 +2046,9 @@ extern void cfs_bandwidth_usage_dec(void);
 
 #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
 
-extern void nohz_balance_exit_idle(unsigned int cpu);
+extern void nohz_balance_exit_idle(struct rq *rq);
 #else
-static inline void nohz_balance_exit_idle(unsigned int cpu) { }
+static inline void nohz_balance_exit_idle(struct rq *rq) { }
 #endif
 
 

commit e022e0d38ad475fc650f22efa3deb2fb96e62542
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 21 11:20:23 2017 +0100

    sched/fair: Update blocked load from NEWIDLE
    
    Since we already iterate CPUs looking for work on NEWIDLE, use this
    iteration to age the blocked load. If the domain for which this is
    done completely spand the idle set, we can push the ILB based aging
    forward.
    
    Suggested-by: Brendan Jackman <brendan.jackman@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5295f274053b..21381d276709 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -762,6 +762,7 @@ struct rq {
 #ifdef CONFIG_NO_HZ_COMMON
 #ifdef CONFIG_SMP
 	unsigned long		last_load_update_tick;
+	unsigned long		last_blocked_load_update_tick;
 #endif /* CONFIG_SMP */
 	atomic_t nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */

commit b7031a02ec753bf9b52a94a966b05e1abad3b7a9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 21 10:11:09 2017 +0100

    sched/fair: Add NOHZ_STATS_KICK
    
    Split the NOHZ idle balancer into doing two separate actions:
    
     - update blocked load statistic
    
     - actually load-balance
    
    Since the latter requires the former, ensure this happens. For now
    always tag both bits at the same time.
    
    Prepares for a future where we can toggle only the STATS bit.
    
    Suggested-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d98e761b962f..5295f274053b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2036,9 +2036,13 @@ extern void cfs_bandwidth_usage_dec(void);
 #ifdef CONFIG_NO_HZ_COMMON
 #define NOHZ_TICK_STOPPED_BIT	0
 #define NOHZ_BALANCE_KICK_BIT	1
+#define NOHZ_STATS_KICK_BIT	2
 
 #define NOHZ_TICK_STOPPED	BIT(NOHZ_TICK_STOPPED_BIT)
 #define NOHZ_BALANCE_KICK	BIT(NOHZ_BALANCE_KICK_BIT)
+#define NOHZ_STATS_KICK		BIT(NOHZ_STATS_KICK_BIT)
+
+#define NOHZ_KICK_MASK	(NOHZ_BALANCE_KICK | NOHZ_STATS_KICK)
 
 #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
 

commit a22e47a4e3f5a9e50a827c5d94705ace3b1eac0b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Dec 21 10:01:24 2017 +0100

    sched/core: Convert nohz_flags to atomic_t
    
    Using atomic_t allows us to use the more flexible bitops provided
    there. Also its smaller.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 23ba4dd76ac4..d98e761b962f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -763,7 +763,7 @@ struct rq {
 #ifdef CONFIG_SMP
 	unsigned long		last_load_update_tick;
 #endif /* CONFIG_SMP */
-	unsigned long		nohz_flags;
+	atomic_t nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 
 	/* capture load from *all* tasks on this CPU: */
@@ -2034,10 +2034,11 @@ extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);
 
 #ifdef CONFIG_NO_HZ_COMMON
-enum rq_nohz_flag_bits {
-	NOHZ_TICK_STOPPED,
-	NOHZ_BALANCE_KICK,
-};
+#define NOHZ_TICK_STOPPED_BIT	0
+#define NOHZ_BALANCE_KICK_BIT	1
+
+#define NOHZ_TICK_STOPPED	BIT(NOHZ_TICK_STOPPED_BIT)
+#define NOHZ_BALANCE_KICK	BIT(NOHZ_BALANCE_KICK_BIT)
 
 #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
 

commit 325ea10c0809406ce23f038602abbc454f3f761d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 12:20:47 2018 +0100

    sched/headers: Simplify and clean up header usage in the scheduler
    
    Do the following cleanups and simplifications:
    
     - sched/sched.h already includes <asm/paravirt.h>, so no need to
       include it in sched/core.c again.
    
     - order the <linux/sched/*.h> headers alphabetically
    
     - add all <linux/sched/*.h> headers to kernel/sched/sched.h
    
     - remove all unnecessary includes from the .c files that
       are already included in kernel/sched/sched.h.
    
    Finally, make all scheduler .c files use a single common header:
    
      #include "sched.h"
    
    ... which now contains a union of the relied upon headers.
    
    This makes the various .c files easier to read and easier to handle.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index bd1461ae06e4..23ba4dd76ac4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -3,39 +3,71 @@
  * Scheduler internal types and methods:
  */
 #include <linux/sched.h>
+
 #include <linux/sched/autogroup.h>
-#include <linux/sched/sysctl.h>
-#include <linux/sched/topology.h>
-#include <linux/sched/rt.h>
-#include <linux/sched/deadline.h>
 #include <linux/sched/clock.h>
-#include <linux/sched/wake_q.h>
-#include <linux/sched/signal.h>
-#include <linux/sched/numa_balancing.h>
-#include <linux/sched/mm.h>
+#include <linux/sched/coredump.h>
 #include <linux/sched/cpufreq.h>
-#include <linux/sched/stat.h>
-#include <linux/sched/nohz.h>
+#include <linux/sched/cputime.h>
+#include <linux/sched/deadline.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/hotplug.h>
+#include <linux/sched/idle.h>
+#include <linux/sched/init.h>
+#include <linux/sched/isolation.h>
+#include <linux/sched/jobctl.h>
+#include <linux/sched/loadavg.h>
+#include <linux/sched/mm.h>
+#include <linux/sched/nohz.h>
+#include <linux/sched/numa_balancing.h>
+#include <linux/sched/prio.h>
+#include <linux/sched/rt.h>
+#include <linux/sched/signal.h>
+#include <linux/sched/stat.h>
+#include <linux/sched/sysctl.h>
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
-#include <linux/sched/cputime.h>
-#include <linux/sched/init.h>
+#include <linux/sched/topology.h>
+#include <linux/sched/user.h>
+#include <linux/sched/wake_q.h>
+#include <linux/sched/xacct.h>
+
+#include <uapi/linux/sched/types.h>
 
-#include <linux/u64_stats_sync.h>
-#include <linux/kernel_stat.h>
 #include <linux/binfmts.h>
-#include <linux/mutex.h>
-#include <linux/spinlock.h>
+#include <linux/blkdev.h>
+#include <linux/compat.h>
+#include <linux/context_tracking.h>
+#include <linux/cpufreq.h>
+#include <linux/cpuidle.h>
+#include <linux/cpuset.h>
+#include <linux/ctype.h>
+#include <linux/debugfs.h>
+#include <linux/delayacct.h>
+#include <linux/init_task.h>
+#include <linux/kprobes.h>
+#include <linux/kthread.h>
+#include <linux/membarrier.h>
+#include <linux/migrate.h>
+#include <linux/mmu_context.h>
+#include <linux/nmi.h>
+#include <linux/proc_fs.h>
+#include <linux/prefetch.h>
+#include <linux/profile.h>
+#include <linux/rcupdate_wait.h>
+#include <linux/security.h>
+#include <linux/stackprotector.h>
 #include <linux/stop_machine.h>
-#include <linux/irq_work.h>
-#include <linux/tick.h>
-#include <linux/slab.h>
-#include <linux/cgroup.h>
+#include <linux/suspend.h>
+#include <linux/swait.h>
+#include <linux/syscalls.h>
+#include <linux/task_work.h>
+#include <linux/tsacct_kern.h>
+
+#include <asm/tlb.h>
 
 #ifdef CONFIG_PARAVIRT
-#include <asm/paravirt.h>
+# include <asm/paravirt.h>
 #endif
 
 #include "cpupri.h"
@@ -1357,13 +1389,6 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 	return p->on_rq == TASK_ON_RQ_MIGRATING;
 }
 
-#ifndef prepare_arch_switch
-# define prepare_arch_switch(next)	do { } while (0)
-#endif
-#ifndef finish_arch_post_lock_switch
-# define finish_arch_post_lock_switch()	do { } while (0)
-#endif
-
 /*
  * wake flags
  */

commit 97fb7a0a8944bd6d2c5634e1e0fa689a5c40bc22
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 14:01:12 2018 +0100

    sched: Clean up and harmonize the coding style of the scheduler code base
    
    A good number of small style inconsistencies have accumulated
    in the scheduler core, so do a pass over them to harmonize
    all these details:
    
     - fix speling in comments,
    
     - use curly braces for multi-line statements,
    
     - remove unnecessary parentheses from integer literals,
    
     - capitalize consistently,
    
     - remove stray newlines,
    
     - add comments where necessary,
    
     - remove invalid/unnecessary comments,
    
     - align structure definitions and other data types vertically,
    
     - add missing newlines for increased readability,
    
     - fix vertical tabulation where it's misaligned,
    
     - harmonize preprocessor conditional block labeling
       and vertical alignment,
    
     - remove line-breaks where they uglify the code,
    
     - add newline after local variable definitions,
    
    No change in functionality:
    
      md5:
         1191fa0a890cfa8132156d2959d7e9e2  built-in.o.before.asm
         1191fa0a890cfa8132156d2959d7e9e2  built-in.o.after.asm
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index dc6c8b5a24ad..bd1461ae06e4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1,5 +1,7 @@
 /* SPDX-License-Identifier: GPL-2.0 */
-
+/*
+ * Scheduler internal types and methods:
+ */
 #include <linux/sched.h>
 #include <linux/sched/autogroup.h>
 #include <linux/sched/sysctl.h>
@@ -79,11 +81,11 @@ static inline void cpu_load_update_active(struct rq *this_rq) { }
  * and does not change the user-interface for setting shares/weights.
  *
  * We increase resolution only if we have enough bits to allow this increased
- * resolution (i.e. 64bit). The costs for increasing resolution when 32bit are
- * pretty high and the returns do not justify the increased costs.
+ * resolution (i.e. 64-bit). The costs for increasing resolution when 32-bit
+ * are pretty high and the returns do not justify the increased costs.
  *
- * Really only required when CONFIG_FAIR_GROUP_SCHED is also set, but to
- * increase coverage and consistency always enable it on 64bit platforms.
+ * Really only required when CONFIG_FAIR_GROUP_SCHED=y is also set, but to
+ * increase coverage and consistency always enable it on 64-bit platforms.
  */
 #ifdef CONFIG_64BIT
 # define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)
@@ -111,16 +113,12 @@ static inline void cpu_load_update_active(struct rq *this_rq) { }
  * 10 -> just above 1us
  * 9  -> just above 0.5us
  */
-#define DL_SCALE (10)
-
-/*
- * These are the 'tuning knobs' of the scheduler:
- */
+#define DL_SCALE		10
 
 /*
- * single value that denotes runtime == period, ie unlimited time.
+ * Single value that denotes runtime == period, ie unlimited time.
  */
-#define RUNTIME_INF	((u64)~0ULL)
+#define RUNTIME_INF		((u64)~0ULL)
 
 static inline int idle_policy(int policy)
 {
@@ -235,9 +233,9 @@ void __dl_clear_params(struct task_struct *p);
  * control.
  */
 struct dl_bandwidth {
-	raw_spinlock_t dl_runtime_lock;
-	u64 dl_runtime;
-	u64 dl_period;
+	raw_spinlock_t		dl_runtime_lock;
+	u64			dl_runtime;
+	u64			dl_period;
 };
 
 static inline int dl_bandwidth_enabled(void)
@@ -246,8 +244,9 @@ static inline int dl_bandwidth_enabled(void)
 }
 
 struct dl_bw {
-	raw_spinlock_t lock;
-	u64 bw, total_bw;
+	raw_spinlock_t		lock;
+	u64			bw;
+	u64			total_bw;
 };
 
 static inline void __dl_update(struct dl_bw *dl_b, s64 bw);
@@ -273,20 +272,17 @@ bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
 	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
 }
 
-void dl_change_utilization(struct task_struct *p, u64 new_bw);
+extern void dl_change_utilization(struct task_struct *p, u64 new_bw);
 extern void init_dl_bw(struct dl_bw *dl_b);
-extern int sched_dl_global_validate(void);
+extern int  sched_dl_global_validate(void);
 extern void sched_dl_do_global(void);
-extern int sched_dl_overflow(struct task_struct *p, int policy,
-			     const struct sched_attr *attr);
+extern int  sched_dl_overflow(struct task_struct *p, int policy, const struct sched_attr *attr);
 extern void __setparam_dl(struct task_struct *p, const struct sched_attr *attr);
 extern void __getparam_dl(struct task_struct *p, struct sched_attr *attr);
 extern bool __checkparam_dl(const struct sched_attr *attr);
 extern bool dl_param_changed(struct task_struct *p, const struct sched_attr *attr);
-extern int dl_task_can_attach(struct task_struct *p,
-			      const struct cpumask *cs_cpus_allowed);
-extern int dl_cpuset_cpumask_can_shrink(const struct cpumask *cur,
-					const struct cpumask *trial);
+extern int  dl_task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);
+extern int  dl_cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);
 extern bool dl_cpu_busy(unsigned int cpu);
 
 #ifdef CONFIG_CGROUP_SCHED
@@ -300,32 +296,36 @@ extern struct list_head task_groups;
 
 struct cfs_bandwidth {
 #ifdef CONFIG_CFS_BANDWIDTH
-	raw_spinlock_t lock;
-	ktime_t period;
-	u64 quota, runtime;
-	s64 hierarchical_quota;
-	u64 runtime_expires;
-
-	int idle, period_active;
-	struct hrtimer period_timer, slack_timer;
-	struct list_head throttled_cfs_rq;
-
-	/* statistics */
-	int nr_periods, nr_throttled;
-	u64 throttled_time;
+	raw_spinlock_t		lock;
+	ktime_t			period;
+	u64			quota;
+	u64			runtime;
+	s64			hierarchical_quota;
+	u64			runtime_expires;
+
+	int			idle;
+	int			period_active;
+	struct hrtimer		period_timer;
+	struct hrtimer		slack_timer;
+	struct list_head	throttled_cfs_rq;
+
+	/* Statistics: */
+	int			nr_periods;
+	int			nr_throttled;
+	u64			throttled_time;
 #endif
 };
 
-/* task group related information */
+/* Task group related information */
 struct task_group {
 	struct cgroup_subsys_state css;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	/* schedulable entities of this group on each cpu */
-	struct sched_entity **se;
-	/* runqueue "owned" by this group on each cpu */
-	struct cfs_rq **cfs_rq;
-	unsigned long shares;
+	/* schedulable entities of this group on each CPU */
+	struct sched_entity	**se;
+	/* runqueue "owned" by this group on each CPU */
+	struct cfs_rq		**cfs_rq;
+	unsigned long		shares;
 
 #ifdef	CONFIG_SMP
 	/*
@@ -333,29 +333,29 @@ struct task_group {
 	 * it in its own cacheline separated from the fields above which
 	 * will also be accessed at each tick.
 	 */
-	atomic_long_t load_avg ____cacheline_aligned;
+	atomic_long_t		load_avg ____cacheline_aligned;
 #endif
 #endif
 
 #ifdef CONFIG_RT_GROUP_SCHED
-	struct sched_rt_entity **rt_se;
-	struct rt_rq **rt_rq;
+	struct sched_rt_entity	**rt_se;
+	struct rt_rq		**rt_rq;
 
-	struct rt_bandwidth rt_bandwidth;
+	struct rt_bandwidth	rt_bandwidth;
 #endif
 
-	struct rcu_head rcu;
-	struct list_head list;
+	struct rcu_head		rcu;
+	struct list_head	list;
 
-	struct task_group *parent;
-	struct list_head siblings;
-	struct list_head children;
+	struct task_group	*parent;
+	struct list_head	siblings;
+	struct list_head	children;
 
 #ifdef CONFIG_SCHED_AUTOGROUP
-	struct autogroup *autogroup;
+	struct autogroup	*autogroup;
 #endif
 
-	struct cfs_bandwidth cfs_bandwidth;
+	struct cfs_bandwidth	cfs_bandwidth;
 };
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -369,8 +369,8 @@ struct task_group {
  * (The default weight is 1024 - so there's no practical
  *  limitation from this.)
  */
-#define MIN_SHARES	(1UL <<  1)
-#define MAX_SHARES	(1UL << 18)
+#define MIN_SHARES		(1UL <<  1)
+#define MAX_SHARES		(1UL << 18)
 #endif
 
 typedef int (*tg_visitor)(struct task_group *, void *);
@@ -443,35 +443,39 @@ struct cfs_bandwidth { };
 
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
-	struct load_weight load;
-	unsigned long runnable_weight;
-	unsigned int nr_running, h_nr_running;
+	struct load_weight	load;
+	unsigned long		runnable_weight;
+	unsigned int		nr_running;
+	unsigned int		h_nr_running;
 
-	u64 exec_clock;
-	u64 min_vruntime;
+	u64			exec_clock;
+	u64			min_vruntime;
 #ifndef CONFIG_64BIT
-	u64 min_vruntime_copy;
+	u64			min_vruntime_copy;
 #endif
 
-	struct rb_root_cached tasks_timeline;
+	struct rb_root_cached	tasks_timeline;
 
 	/*
 	 * 'curr' points to currently running entity on this cfs_rq.
 	 * It is set to NULL otherwise (i.e when none are currently running).
 	 */
-	struct sched_entity *curr, *next, *last, *skip;
+	struct sched_entity	*curr;
+	struct sched_entity	*next;
+	struct sched_entity	*last;
+	struct sched_entity	*skip;
 
 #ifdef	CONFIG_SCHED_DEBUG
-	unsigned int nr_spread_over;
+	unsigned int		nr_spread_over;
 #endif
 
 #ifdef CONFIG_SMP
 	/*
 	 * CFS load tracking
 	 */
-	struct sched_avg avg;
+	struct sched_avg	avg;
 #ifndef CONFIG_64BIT
-	u64 load_last_update_time_copy;
+	u64			load_last_update_time_copy;
 #endif
 	struct {
 		raw_spinlock_t	lock ____cacheline_aligned;
@@ -482,9 +486,9 @@ struct cfs_rq {
 	} removed;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	unsigned long tg_load_avg_contrib;
-	long propagate;
-	long prop_runnable_sum;
+	unsigned long		tg_load_avg_contrib;
+	long			propagate;
+	long			prop_runnable_sum;
 
 	/*
 	 *   h_load = weight * f(tg)
@@ -492,36 +496,38 @@ struct cfs_rq {
 	 * Where f(tg) is the recursive weight fraction assigned to
 	 * this group.
 	 */
-	unsigned long h_load;
-	u64 last_h_load_update;
-	struct sched_entity *h_load_next;
+	unsigned long		h_load;
+	u64			last_h_load_update;
+	struct sched_entity	*h_load_next;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */
+	struct rq		*rq;	/* CPU runqueue to which this cfs_rq is attached */
 
 	/*
 	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
 	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
 	 * (like users, containers etc.)
 	 *
-	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This
-	 * list is used during load balance.
+	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a CPU.
+	 * This list is used during load balance.
 	 */
-	int on_list;
-	struct list_head leaf_cfs_rq_list;
-	struct task_group *tg;	/* group that "owns" this runqueue */
+	int			on_list;
+	struct list_head	leaf_cfs_rq_list;
+	struct task_group	*tg;	/* group that "owns" this runqueue */
 
 #ifdef CONFIG_CFS_BANDWIDTH
-	int runtime_enabled;
-	u64 runtime_expires;
-	s64 runtime_remaining;
-
-	u64 throttled_clock, throttled_clock_task;
-	u64 throttled_clock_task_time;
-	int throttled, throttle_count;
-	struct list_head throttled_list;
+	int			runtime_enabled;
+	u64			runtime_expires;
+	s64			runtime_remaining;
+
+	u64			throttled_clock;
+	u64			throttled_clock_task;
+	u64			throttled_clock_task_time;
+	int			throttled;
+	int			throttle_count;
+	struct list_head	throttled_list;
 #endif /* CONFIG_CFS_BANDWIDTH */
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 };
@@ -538,45 +544,45 @@ static inline int rt_bandwidth_enabled(void)
 
 /* Real-Time classes' related field in a runqueue: */
 struct rt_rq {
-	struct rt_prio_array active;
-	unsigned int rt_nr_running;
-	unsigned int rr_nr_running;
+	struct rt_prio_array	active;
+	unsigned int		rt_nr_running;
+	unsigned int		rr_nr_running;
 #if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
 	struct {
-		int curr; /* highest queued rt task prio */
+		int		curr; /* highest queued rt task prio */
 #ifdef CONFIG_SMP
-		int next; /* next highest */
+		int		next; /* next highest */
 #endif
 	} highest_prio;
 #endif
 #ifdef CONFIG_SMP
-	unsigned long rt_nr_migratory;
-	unsigned long rt_nr_total;
-	int overloaded;
-	struct plist_head pushable_tasks;
+	unsigned long		rt_nr_migratory;
+	unsigned long		rt_nr_total;
+	int			overloaded;
+	struct plist_head	pushable_tasks;
 #endif /* CONFIG_SMP */
-	int rt_queued;
+	int			rt_queued;
 
-	int rt_throttled;
-	u64 rt_time;
-	u64 rt_runtime;
+	int			rt_throttled;
+	u64			rt_time;
+	u64			rt_runtime;
 	/* Nests inside the rq lock: */
-	raw_spinlock_t rt_runtime_lock;
+	raw_spinlock_t		rt_runtime_lock;
 
 #ifdef CONFIG_RT_GROUP_SCHED
-	unsigned long rt_nr_boosted;
+	unsigned long		rt_nr_boosted;
 
-	struct rq *rq;
-	struct task_group *tg;
+	struct rq		*rq;
+	struct task_group	*tg;
 #endif
 };
 
 /* Deadline class' related fields in a runqueue */
 struct dl_rq {
 	/* runqueue is an rbtree, ordered by deadline */
-	struct rb_root_cached root;
+	struct rb_root_cached	root;
 
-	unsigned long dl_nr_running;
+	unsigned long		dl_nr_running;
 
 #ifdef CONFIG_SMP
 	/*
@@ -586,28 +592,28 @@ struct dl_rq {
 	 * should migrate somewhere else.
 	 */
 	struct {
-		u64 curr;
-		u64 next;
+		u64		curr;
+		u64		next;
 	} earliest_dl;
 
-	unsigned long dl_nr_migratory;
-	int overloaded;
+	unsigned long		dl_nr_migratory;
+	int			overloaded;
 
 	/*
 	 * Tasks on this rq that can be pushed away. They are kept in
 	 * an rb-tree, ordered by tasks' deadlines, with caching
 	 * of the leftmost (earliest deadline) element.
 	 */
-	struct rb_root_cached pushable_dl_tasks_root;
+	struct rb_root_cached	pushable_dl_tasks_root;
 #else
-	struct dl_bw dl_bw;
+	struct dl_bw		dl_bw;
 #endif
 	/*
 	 * "Active utilization" for this runqueue: increased when a
 	 * task wakes up (becomes TASK_RUNNING) and decreased when a
 	 * task blocks
 	 */
-	u64 running_bw;
+	u64			running_bw;
 
 	/*
 	 * Utilization of the tasks "assigned" to this runqueue (including
@@ -618,14 +624,14 @@ struct dl_rq {
 	 * This is needed to compute the "inactive utilization" for the
 	 * runqueue (inactive utilization = this_bw - running_bw).
 	 */
-	u64 this_bw;
-	u64 extra_bw;
+	u64			this_bw;
+	u64			extra_bw;
 
 	/*
 	 * Inverse of the fraction of CPU utilization that can be reclaimed
 	 * by the GRUB algorithm.
 	 */
-	u64 bw_ratio;
+	u64			bw_ratio;
 };
 
 #ifdef CONFIG_SMP
@@ -638,51 +644,51 @@ static inline bool sched_asym_prefer(int a, int b)
 /*
  * We add the notion of a root-domain which will be used to define per-domain
  * variables. Each exclusive cpuset essentially defines an island domain by
- * fully partitioning the member cpus from any other cpuset. Whenever a new
+ * fully partitioning the member CPUs from any other cpuset. Whenever a new
  * exclusive cpuset is created, we also create and attach a new root-domain
  * object.
  *
  */
 struct root_domain {
-	atomic_t refcount;
-	atomic_t rto_count;
-	struct rcu_head rcu;
-	cpumask_var_t span;
-	cpumask_var_t online;
+	atomic_t		refcount;
+	atomic_t		rto_count;
+	struct rcu_head		rcu;
+	cpumask_var_t		span;
+	cpumask_var_t		online;
 
 	/* Indicate more than one runnable task for any CPU */
-	bool overload;
+	bool			overload;
 
 	/*
 	 * The bit corresponding to a CPU gets set here if such CPU has more
 	 * than one runnable -deadline task (as it is below for RT tasks).
 	 */
-	cpumask_var_t dlo_mask;
-	atomic_t dlo_count;
-	struct dl_bw dl_bw;
-	struct cpudl cpudl;
+	cpumask_var_t		dlo_mask;
+	atomic_t		dlo_count;
+	struct dl_bw		dl_bw;
+	struct cpudl		cpudl;
 
 #ifdef HAVE_RT_PUSH_IPI
 	/*
 	 * For IPI pull requests, loop across the rto_mask.
 	 */
-	struct irq_work rto_push_work;
-	raw_spinlock_t rto_lock;
+	struct irq_work		rto_push_work;
+	raw_spinlock_t		rto_lock;
 	/* These are only updated and read within rto_lock */
-	int rto_loop;
-	int rto_cpu;
+	int			rto_loop;
+	int			rto_cpu;
 	/* These atomics are updated outside of a lock */
-	atomic_t rto_loop_next;
-	atomic_t rto_loop_start;
+	atomic_t		rto_loop_next;
+	atomic_t		rto_loop_start;
 #endif
 	/*
 	 * The "RT overload" flag: it gets set if a CPU has more than
 	 * one runnable RT task.
 	 */
-	cpumask_var_t rto_mask;
-	struct cpupri cpupri;
+	cpumask_var_t		rto_mask;
+	struct cpupri		cpupri;
 
-	unsigned long max_cpu_capacity;
+	unsigned long		max_cpu_capacity;
 };
 
 extern struct root_domain def_root_domain;
@@ -708,39 +714,39 @@ extern void rto_push_irq_work_func(struct irq_work *work);
  */
 struct rq {
 	/* runqueue lock: */
-	raw_spinlock_t lock;
+	raw_spinlock_t		lock;
 
 	/*
 	 * nr_running and cpu_load should be in the same cacheline because
 	 * remote CPUs use both these fields when doing load calculation.
 	 */
-	unsigned int nr_running;
+	unsigned int		nr_running;
 #ifdef CONFIG_NUMA_BALANCING
-	unsigned int nr_numa_running;
-	unsigned int nr_preferred_running;
+	unsigned int		nr_numa_running;
+	unsigned int		nr_preferred_running;
 #endif
 	#define CPU_LOAD_IDX_MAX 5
-	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
+	unsigned long		cpu_load[CPU_LOAD_IDX_MAX];
 #ifdef CONFIG_NO_HZ_COMMON
 #ifdef CONFIG_SMP
-	unsigned long last_load_update_tick;
+	unsigned long		last_load_update_tick;
 #endif /* CONFIG_SMP */
-	unsigned long nohz_flags;
+	unsigned long		nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 
-	/* capture load from *all* tasks on this cpu: */
-	struct load_weight load;
-	unsigned long nr_load_updates;
-	u64 nr_switches;
+	/* capture load from *all* tasks on this CPU: */
+	struct load_weight	load;
+	unsigned long		nr_load_updates;
+	u64			nr_switches;
 
-	struct cfs_rq cfs;
-	struct rt_rq rt;
-	struct dl_rq dl;
+	struct cfs_rq		cfs;
+	struct rt_rq		rt;
+	struct dl_rq		dl;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	/* list of leaf cfs_rq on this cpu: */
-	struct list_head leaf_cfs_rq_list;
-	struct list_head *tmp_alone_branch;
+	/* list of leaf cfs_rq on this CPU: */
+	struct list_head	leaf_cfs_rq_list;
+	struct list_head	*tmp_alone_branch;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 	/*
@@ -749,94 +755,98 @@ struct rq {
 	 * one CPU and if it got migrated afterwards it may decrease
 	 * it on another CPU. Always updated under the runqueue lock:
 	 */
-	unsigned long nr_uninterruptible;
+	unsigned long		nr_uninterruptible;
 
-	struct task_struct *curr, *idle, *stop;
-	unsigned long next_balance;
-	struct mm_struct *prev_mm;
+	struct task_struct	*curr;
+	struct task_struct	*idle;
+	struct task_struct	*stop;
+	unsigned long		next_balance;
+	struct mm_struct	*prev_mm;
 
-	unsigned int clock_update_flags;
-	u64 clock;
-	u64 clock_task;
+	unsigned int		clock_update_flags;
+	u64			clock;
+	u64			clock_task;
 
-	atomic_t nr_iowait;
+	atomic_t		nr_iowait;
 
 #ifdef CONFIG_SMP
-	struct root_domain *rd;
-	struct sched_domain *sd;
+	struct root_domain	*rd;
+	struct sched_domain	*sd;
+
+	unsigned long		cpu_capacity;
+	unsigned long		cpu_capacity_orig;
 
-	unsigned long cpu_capacity;
-	unsigned long cpu_capacity_orig;
+	struct callback_head	*balance_callback;
 
-	struct callback_head *balance_callback;
+	unsigned char		idle_balance;
 
-	unsigned char idle_balance;
 	/* For active balancing */
-	int active_balance;
-	int push_cpu;
-	struct cpu_stop_work active_balance_work;
-	/* cpu of this runqueue: */
-	int cpu;
-	int online;
+	int			active_balance;
+	int			push_cpu;
+	struct cpu_stop_work	active_balance_work;
+
+	/* CPU of this runqueue: */
+	int			cpu;
+	int			online;
 
 	struct list_head cfs_tasks;
 
-	u64 rt_avg;
-	u64 age_stamp;
-	u64 idle_stamp;
-	u64 avg_idle;
+	u64			rt_avg;
+	u64			age_stamp;
+	u64			idle_stamp;
+	u64			avg_idle;
 
 	/* This is used to determine avg_idle's max value */
-	u64 max_idle_balance_cost;
+	u64			max_idle_balance_cost;
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
-	u64 prev_irq_time;
+	u64			prev_irq_time;
 #endif
 #ifdef CONFIG_PARAVIRT
-	u64 prev_steal_time;
+	u64			prev_steal_time;
 #endif
 #ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
-	u64 prev_steal_time_rq;
+	u64			prev_steal_time_rq;
 #endif
 
 	/* calc_load related fields */
-	unsigned long calc_load_update;
-	long calc_load_active;
+	unsigned long		calc_load_update;
+	long			calc_load_active;
 
 #ifdef CONFIG_SCHED_HRTICK
 #ifdef CONFIG_SMP
-	int hrtick_csd_pending;
-	call_single_data_t hrtick_csd;
+	int			hrtick_csd_pending;
+	call_single_data_t	hrtick_csd;
 #endif
-	struct hrtimer hrtick_timer;
+	struct hrtimer		hrtick_timer;
 #endif
 
 #ifdef CONFIG_SCHEDSTATS
 	/* latency stats */
-	struct sched_info rq_sched_info;
-	unsigned long long rq_cpu_time;
+	struct sched_info	rq_sched_info;
+	unsigned long long	rq_cpu_time;
 	/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */
 
 	/* sys_sched_yield() stats */
-	unsigned int yld_count;
+	unsigned int		yld_count;
 
 	/* schedule() stats */
-	unsigned int sched_count;
-	unsigned int sched_goidle;
+	unsigned int		sched_count;
+	unsigned int		sched_goidle;
 
 	/* try_to_wake_up() stats */
-	unsigned int ttwu_count;
-	unsigned int ttwu_local;
+	unsigned int		ttwu_count;
+	unsigned int		ttwu_local;
 #endif
 
 #ifdef CONFIG_SMP
-	struct llist_head wake_list;
+	struct llist_head	wake_list;
 #endif
 
 #ifdef CONFIG_CPU_IDLE
 	/* Must be inspected within a rcu lock section */
-	struct cpuidle_state *idle_state;
+	struct cpuidle_state	*idle_state;
 #endif
 };
 
@@ -902,9 +912,9 @@ static inline u64 __rq_clock_broken(struct rq *rq)
  * one position though, because the next rq_unpin_lock() will shift it
  * back.
  */
-#define RQCF_REQ_SKIP	0x01
-#define RQCF_ACT_SKIP	0x02
-#define RQCF_UPDATED	0x04
+#define RQCF_REQ_SKIP		0x01
+#define RQCF_ACT_SKIP		0x02
+#define RQCF_UPDATED		0x04
 
 static inline void assert_clock_updated(struct rq *rq)
 {
@@ -1057,12 +1067,12 @@ extern void sched_ttwu_pending(void);
 
 /**
  * highest_flag_domain - Return highest sched_domain containing flag.
- * @cpu:	The cpu whose highest level of sched domain is to
+ * @cpu:	The CPU whose highest level of sched domain is to
  *		be returned.
  * @flag:	The flag to check for the highest sched_domain
- *		for the given cpu.
+ *		for the given CPU.
  *
- * Returns the highest sched_domain of a cpu which contains the given flag.
+ * Returns the highest sched_domain of a CPU which contains the given flag.
  */
 static inline struct sched_domain *highest_flag_domain(int cpu, int flag)
 {
@@ -1097,30 +1107,30 @@ DECLARE_PER_CPU(struct sched_domain *, sd_numa);
 DECLARE_PER_CPU(struct sched_domain *, sd_asym);
 
 struct sched_group_capacity {
-	atomic_t ref;
+	atomic_t		ref;
 	/*
 	 * CPU capacity of this group, SCHED_CAPACITY_SCALE being max capacity
 	 * for a single CPU.
 	 */
-	unsigned long capacity;
-	unsigned long min_capacity; /* Min per-CPU capacity in group */
-	unsigned long next_update;
-	int imbalance; /* XXX unrelated to capacity but shared group state */
+	unsigned long		capacity;
+	unsigned long		min_capacity;		/* Min per-CPU capacity in group */
+	unsigned long		next_update;
+	int			imbalance;		/* XXX unrelated to capacity but shared group state */
 
 #ifdef CONFIG_SCHED_DEBUG
-	int id;
+	int			id;
 #endif
 
-	unsigned long cpumask[0]; /* balance mask */
+	unsigned long		cpumask[0];		/* Balance mask */
 };
 
 struct sched_group {
-	struct sched_group *next;	/* Must be a circular list */
-	atomic_t ref;
+	struct sched_group	*next;			/* Must be a circular list */
+	atomic_t		ref;
 
-	unsigned int group_weight;
+	unsigned int		group_weight;
 	struct sched_group_capacity *sgc;
-	int asym_prefer_cpu;		/* cpu of highest priority in group */
+	int			asym_prefer_cpu;	/* CPU of highest priority in group */
 
 	/*
 	 * The CPUs this group covers.
@@ -1129,7 +1139,7 @@ struct sched_group {
 	 * by attaching extra space to the end of the structure,
 	 * depending on how many CPUs the kernel has booted up with)
 	 */
-	unsigned long cpumask[0];
+	unsigned long		cpumask[0];
 };
 
 static inline struct cpumask *sched_group_span(struct sched_group *sg)
@@ -1146,8 +1156,8 @@ static inline struct cpumask *group_balance_mask(struct sched_group *sg)
 }
 
 /**
- * group_first_cpu - Returns the first cpu in the cpumask of a sched_group.
- * @group: The group whose first cpu is to be returned.
+ * group_first_cpu - Returns the first CPU in the cpumask of a sched_group.
+ * @group: The group whose first CPU is to be returned.
  */
 static inline unsigned int group_first_cpu(struct sched_group *group)
 {
@@ -1357,9 +1367,9 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 /*
  * wake flags
  */
-#define WF_SYNC		0x01		/* waker goes to sleep after wakeup */
-#define WF_FORK		0x02		/* child wakeup after fork */
-#define WF_MIGRATED	0x4		/* internal use, task got migrated */
+#define WF_SYNC			0x01		/* Waker goes to sleep after wakeup */
+#define WF_FORK			0x02		/* Child wakeup after fork */
+#define WF_MIGRATED		0x4		/* Internal use, task got migrated */
 
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution
@@ -1370,11 +1380,11 @@ static inline int task_on_rq_migrating(struct task_struct *p)
  * slice expiry etc.
  */
 
-#define WEIGHT_IDLEPRIO                3
-#define WMULT_IDLEPRIO         1431655765
+#define WEIGHT_IDLEPRIO		3
+#define WMULT_IDLEPRIO		1431655765
 
-extern const int sched_prio_to_weight[40];
-extern const u32 sched_prio_to_wmult[40];
+extern const int		sched_prio_to_weight[40];
+extern const u32		sched_prio_to_wmult[40];
 
 /*
  * {de,en}queue flags:
@@ -1396,9 +1406,9 @@ extern const u32 sched_prio_to_wmult[40];
  */
 
 #define DEQUEUE_SLEEP		0x01
-#define DEQUEUE_SAVE		0x02 /* matches ENQUEUE_RESTORE */
-#define DEQUEUE_MOVE		0x04 /* matches ENQUEUE_MOVE */
-#define DEQUEUE_NOCLOCK		0x08 /* matches ENQUEUE_NOCLOCK */
+#define DEQUEUE_SAVE		0x02 /* Matches ENQUEUE_RESTORE */
+#define DEQUEUE_MOVE		0x04 /* Matches ENQUEUE_MOVE */
+#define DEQUEUE_NOCLOCK		0x08 /* Matches ENQUEUE_NOCLOCK */
 
 #define ENQUEUE_WAKEUP		0x01
 #define ENQUEUE_RESTORE		0x02
@@ -1420,10 +1430,10 @@ struct sched_class {
 
 	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
 	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
-	void (*yield_task) (struct rq *rq);
-	bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
+	void (*yield_task)   (struct rq *rq);
+	bool (*yield_to_task)(struct rq *rq, struct task_struct *p, bool preempt);
 
-	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
+	void (*check_preempt_curr)(struct rq *rq, struct task_struct *p, int flags);
 
 	/*
 	 * It is the responsibility of the pick_next_task() method that will
@@ -1433,16 +1443,16 @@ struct sched_class {
 	 * May return RETRY_TASK when it finds a higher prio class has runnable
 	 * tasks.
 	 */
-	struct task_struct * (*pick_next_task) (struct rq *rq,
-						struct task_struct *prev,
-						struct rq_flags *rf);
-	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
+	struct task_struct * (*pick_next_task)(struct rq *rq,
+					       struct task_struct *prev,
+					       struct rq_flags *rf);
+	void (*put_prev_task)(struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
 	void (*migrate_task_rq)(struct task_struct *p);
 
-	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
+	void (*task_woken)(struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,
 				 const struct cpumask *newmask);
@@ -1451,31 +1461,31 @@ struct sched_class {
 	void (*rq_offline)(struct rq *rq);
 #endif
 
-	void (*set_curr_task) (struct rq *rq);
-	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
-	void (*task_fork) (struct task_struct *p);
-	void (*task_dead) (struct task_struct *p);
+	void (*set_curr_task)(struct rq *rq);
+	void (*task_tick)(struct rq *rq, struct task_struct *p, int queued);
+	void (*task_fork)(struct task_struct *p);
+	void (*task_dead)(struct task_struct *p);
 
 	/*
 	 * The switched_from() call is allowed to drop rq->lock, therefore we
 	 * cannot assume the switched_from/switched_to pair is serliazed by
 	 * rq->lock. They are however serialized by p->pi_lock.
 	 */
-	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
-	void (*switched_to) (struct rq *this_rq, struct task_struct *task);
+	void (*switched_from)(struct rq *this_rq, struct task_struct *task);
+	void (*switched_to)  (struct rq *this_rq, struct task_struct *task);
 	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
-			     int oldprio);
+			      int oldprio);
 
-	unsigned int (*get_rr_interval) (struct rq *rq,
-					 struct task_struct *task);
+	unsigned int (*get_rr_interval)(struct rq *rq,
+					struct task_struct *task);
 
-	void (*update_curr) (struct rq *rq);
+	void (*update_curr)(struct rq *rq);
 
-#define TASK_SET_GROUP  0
-#define TASK_MOVE_GROUP	1
+#define TASK_SET_GROUP		0
+#define TASK_MOVE_GROUP		1
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	void (*task_change_group) (struct task_struct *p, int type);
+	void (*task_change_group)(struct task_struct *p, int type);
 #endif
 };
 
@@ -1524,6 +1534,7 @@ static inline void idle_set_state(struct rq *rq,
 static inline struct cpuidle_state *idle_get_state(struct rq *rq)
 {
 	SCHED_WARN_ON(!rcu_read_lock_held());
+
 	return rq->idle_state;
 }
 #else
@@ -1562,9 +1573,9 @@ extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 extern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);
 extern void init_dl_rq_bw_ratio(struct dl_rq *dl_rq);
 
-#define BW_SHIFT	20
-#define BW_UNIT		(1 << BW_SHIFT)
-#define RATIO_SHIFT	8
+#define BW_SHIFT		20
+#define BW_UNIT			(1 << BW_SHIFT)
+#define RATIO_SHIFT		8
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);
@@ -1814,8 +1825,8 @@ static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
 /*
  * Unfair double_lock_balance: Optimizes throughput at the expense of
  * latency by eliminating extra atomic operations when the locks are
- * already in proper order on entry.  This favors lower cpu-ids and will
- * grant the double lock to lower cpus over higher ids under contention,
+ * already in proper order on entry.  This favors lower CPU-ids and will
+ * grant the double lock to lower CPUs over higher ids under contention,
  * regardless of entry order into the function.
  */
 static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
@@ -1847,7 +1858,7 @@ static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
 static inline int double_lock_balance(struct rq *this_rq, struct rq *busiest)
 {
 	if (unlikely(!irqs_disabled())) {
-		/* printk() doesn't work good under rq->lock */
+		/* printk() doesn't work well under rq->lock */
 		raw_spin_unlock(&this_rq->lock);
 		BUG_ON(1);
 	}
@@ -2106,15 +2117,14 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef arch_scale_freq_capacity
-#ifndef arch_scale_freq_invariant
-#define arch_scale_freq_invariant()	(true)
-#endif
-#else /* arch_scale_freq_capacity */
-#define arch_scale_freq_invariant()	(false)
+# ifndef arch_scale_freq_invariant
+#  define arch_scale_freq_invariant()	true
+# endif
+#else
+# define arch_scale_freq_invariant()	false
 #endif
 
 #ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
-
 static inline unsigned long cpu_util_dl(struct rq *rq)
 {
 	return (rq->dl.running_bw * SCHED_CAPACITY_SCALE) >> BW_SHIFT;
@@ -2124,5 +2134,4 @@ static inline unsigned long cpu_util_cfs(struct rq *rq)
 {
 	return rq->cfs.avg.util_avg;
 }
-
 #endif

commit dcdedb24159be3487e3dbbe1faa79ae7d00c92ac
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Feb 21 05:17:28 2018 +0100

    sched/nohz: Remove the 1 Hz tick code
    
    Now that the 1Hz tick is offloaded to workqueues, we can safely remove
    the residual code that used to handle it locally.
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1519186649-3242-7-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c1c7c788da1c..dc6c8b5a24ad 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -727,9 +727,7 @@ struct rq {
 #endif /* CONFIG_SMP */
 	unsigned long nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
-#ifdef CONFIG_NO_HZ_FULL
-	unsigned long last_sched_tick;
-#endif
+
 	/* capture load from *all* tasks on this cpu: */
 	struct load_weight load;
 	unsigned long nr_load_updates;
@@ -1626,13 +1624,6 @@ static inline void sub_nr_running(struct rq *rq, unsigned count)
 	sched_update_tick_dependency(rq);
 }
 
-static inline void rq_last_tick_reset(struct rq *rq)
-{
-#ifdef CONFIG_NO_HZ_FULL
-	rq->last_sched_tick = jiffies;
-#endif
-}
-
 extern void update_rq_clock(struct rq *rq);
 
 extern void activate_task(struct rq *rq, struct task_struct *p, int flags);

commit d84b31313ef8a8de55a2cbfb72f76f36d8c927fb
Author: Frederic Weisbecker <frederic@kernel.org>
Date:   Wed Feb 21 05:17:27 2018 +0100

    sched/isolation: Offload residual 1Hz scheduler tick
    
    When a CPU runs in full dynticks mode, a 1Hz tick remains in order to
    keep the scheduler stats alive. However this residual tick is a burden
    for bare metal tasks that can't stand any interruption at all, or want
    to minimize them.
    
    The usual boot parameters "nohz_full=" or "isolcpus=nohz" will now
    outsource these scheduler ticks to the global workqueue so that a
    housekeeping CPU handles those remotely. The sched_class::task_tick()
    implementations have been audited and look safe to be called remotely
    as the target runqueue and its current task are passed in parameter
    and don't seem to be accessed locally.
    
    Note that in the case of using isolcpus, it's still up to the user to
    affine the global workqueues to the housekeeping CPUs through
    /sys/devices/virtual/workqueue/cpumask or domains isolation
    "isolcpus=nohz,domain".
    
    Signed-off-by: Frederic Weisbecker <frederic@kernel.org>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chris Metcalf <cmetcalf@mellanox.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Wanpeng Li <kernellwp@gmail.com>
    Link: http://lkml.kernel.org/r/1519186649-3242-6-git-send-email-frederic@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fb5fc458547f..c1c7c788da1c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1574,6 +1574,7 @@ extern void post_init_entity_util_avg(struct sched_entity *se);
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool sched_can_stop_tick(struct rq *rq);
+extern int __init sched_tick_offload_init(void);
 
 /*
  * Tick may be needed by tasks in the runqueue depending on their policy and
@@ -1598,6 +1599,7 @@ static inline void sched_update_tick_dependency(struct rq *rq)
 		tick_nohz_dep_set_cpu(cpu, TICK_DEP_BIT_SCHED);
 }
 #else
+static inline int sched_tick_offload_init(void) { return 0; }
 static inline void sched_update_tick_dependency(struct rq *rq) { }
 #endif
 

commit 364f56653708ba8bcdefd4f0da2a42904baa8eeb
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Jan 23 20:45:38 2018 -0500

    sched/rt: Up the root domain ref count when passing it around via IPIs
    
    When issuing an IPI RT push, where an IPI is sent to each CPU that has more
    than one RT task scheduled on it, it references the root domain's rto_mask,
    that contains all the CPUs within the root domain that has more than one RT
    task in the runable state. The problem is, after the IPIs are initiated, the
    rq->lock is released. This means that the root domain that is associated to
    the run queue could be freed while the IPIs are going around.
    
    Add a sched_get_rd() and a sched_put_rd() that will increment and decrement
    the root domain's ref count respectively. This way when initiating the IPIs,
    the scheduler will up the root domain's ref count before releasing the
    rq->lock, ensuring that the root domain does not go away until the IPI round
    is complete.
    
    Reported-by: Pavan Kondeti <pkondeti@codeaurora.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 4bdced5c9a292 ("sched/rt: Simplify the IPI based RT balancing logic")
    Link: http://lkml.kernel.org/r/CAEU1=PkiHO35Dzna8EQqNSKW1fr1y1zRQ5y66X117MG06sQtNA@mail.gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2e95505e23c6..fb5fc458547f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -691,6 +691,8 @@ extern struct mutex sched_domains_mutex;
 extern void init_defrootdomain(void);
 extern int sched_init_domains(const struct cpumask *cpu_map);
 extern void rq_attach_root(struct rq *rq, struct root_domain *rd);
+extern void sched_get_rd(struct root_domain *rd);
+extern void sched_put_rd(struct root_domain *rd);
 
 #ifdef HAVE_RT_PUSH_IPI
 extern void rto_push_irq_work_func(struct irq_work *work);

commit 07881166a892fa4908ac4924660a7793f75d6544
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Mon Dec 4 11:23:25 2017 +0100

    sched/deadline: Make bandwidth enforcement scale-invariant
    
    Apply frequency and CPU scale-invariance correction factor to bandwidth
    enforcement (similar to what we already do to fair utilization tracking).
    
    Each delta_exec gets scaled considering current frequency and maximum
    CPU capacity; which means that the reservation runtime parameter (that
    need to be specified profiling the task execution at max frequency on
    biggest capacity core) gets thus scaled accordingly.
    
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Claudio Scordino <claudio@evidence.eu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: alessio.balsini@arm.com
    Cc: bristot@redhat.com
    Cc: dietmar.eggemann@arm.com
    Cc: joelaf@google.com
    Cc: juri.lelli@redhat.com
    Cc: mathieu.poirier@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: rjw@rjwysocki.net
    Cc: rostedt@goodmis.org
    Cc: tkjos@android.com
    Cc: tommaso.cucinotta@santannapisa.it
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/20171204102325.5110-9-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e122c89bdbdd..2e95505e23c6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -156,6 +156,8 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
+#define cap_scale(v, s) ((v)*(s) >> SCHED_CAPACITY_SHIFT)
+
 /*
  * !! For sched_setattr_nocheck() (kernel) only !!
  *

commit 7e1a9208f6c7e66bb4e5d2ed18dfd191230f431b
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Mon Dec 4 11:23:24 2017 +0100

    sched/cpufreq: Move arch_scale_{freq,cpu}_capacity() outside of #ifdef CONFIG_SMP
    
    Currently, frequency and cpu capacity scaling is only performed on
    CONFIG_SMP systems (as CFS PELT signals are only present for such
    systems). However, other scheduling classes want to do freq/cpu scaling,
    and for !CONFIG_SMP configurations as well.
    
    arch_scale_freq_capacity() is useful to implement frequency scaling even
    on !CONFIG_SMP platforms, so we simply move it outside CONFIG_SMP
    ifdeffery.
    
    Even if arch_scale_cpu_capacity() is not useful on !CONFIG_SMP platforms,
    we make a default implementation available for such configurations anyway
    to simplify scheduler code doing CPU scale invariance.
    
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: alessio.balsini@arm.com
    Cc: bristot@redhat.com
    Cc: claudio@evidence.eu.com
    Cc: dietmar.eggemann@arm.com
    Cc: joelaf@google.com
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: mathieu.poirier@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: rjw@rjwysocki.net
    Cc: tkjos@android.com
    Cc: tommaso.cucinotta@santannapisa.it
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/20171204102325.5110-8-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b7100192ecd3..e122c89bdbdd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1670,9 +1670,6 @@ static inline int hrtick_enabled(struct rq *rq)
 
 #endif /* CONFIG_SCHED_HRTICK */
 
-#ifdef CONFIG_SMP
-extern void sched_avg_update(struct rq *rq);
-
 #ifndef arch_scale_freq_capacity
 static __always_inline
 unsigned long arch_scale_freq_capacity(int cpu)
@@ -1681,6 +1678,9 @@ unsigned long arch_scale_freq_capacity(int cpu)
 }
 #endif
 
+#ifdef CONFIG_SMP
+extern void sched_avg_update(struct rq *rq);
+
 #ifndef arch_scale_cpu_capacity
 static __always_inline
 unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
@@ -1698,6 +1698,13 @@ static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta)
 	sched_avg_update(rq);
 }
 #else
+#ifndef arch_scale_cpu_capacity
+static __always_inline
+unsigned long arch_scale_cpu_capacity(void __always_unused *sd, int cpu)
+{
+	return SCHED_CAPACITY_SCALE;
+}
+#endif
 static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }
 static inline void sched_avg_update(struct rq *rq) { }
 #endif

commit 7673c8a4c75d1cac2cd47156b9768f462683a09d
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Mon Dec 4 11:23:23 2017 +0100

    sched/cpufreq: Remove arch_scale_freq_capacity()'s 'sd' parameter
    
    The 'sd' parameter is never used in arch_scale_freq_capacity() (and it's hard to
    see where information coming from scheduling domains might help doing
    frequency invariance scaling).
    
    Remove it; also in anticipation of moving arch_scale_freq_capacity()
    outside CONFIG_SMP.
    
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: alessio.balsini@arm.com
    Cc: bristot@redhat.com
    Cc: claudio@evidence.eu.com
    Cc: dietmar.eggemann@arm.com
    Cc: joelaf@google.com
    Cc: juri.lelli@redhat.com
    Cc: luca.abeni@santannapisa.it
    Cc: mathieu.poirier@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: rjw@rjwysocki.net
    Cc: rostedt@goodmis.org
    Cc: tkjos@android.com
    Cc: tommaso.cucinotta@santannapisa.it
    Cc: vincent.guittot@linaro.org
    Cc: viresh.kumar@linaro.org
    Link: http://lkml.kernel.org/r/20171204102325.5110-7-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c5197338ac47..b7100192ecd3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1675,7 +1675,7 @@ extern void sched_avg_update(struct rq *rq);
 
 #ifndef arch_scale_freq_capacity
 static __always_inline
-unsigned long arch_scale_freq_capacity(struct sched_domain *sd, int cpu)
+unsigned long arch_scale_freq_capacity(int cpu)
 {
 	return SCHED_CAPACITY_SCALE;
 }
@@ -1694,7 +1694,7 @@ unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
 
 static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta)
 {
-	rq->rt_avg += rt_delta * arch_scale_freq_capacity(NULL, cpu_of(rq));
+	rq->rt_avg += rt_delta * arch_scale_freq_capacity(cpu_of(rq));
 	sched_avg_update(rq);
 }
 #else

commit 794a56ebd9a57db12abaec63f038c6eb073461f7
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Mon Dec 4 11:23:20 2017 +0100

    sched/cpufreq: Change the worker kthread to SCHED_DEADLINE
    
    Worker kthread needs to be able to change frequency for all other
    threads.
    
    Make it special, just under STOP class.
    
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Claudio Scordino <claudio@evidence.eu.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: alessio.balsini@arm.com
    Cc: bristot@redhat.com
    Cc: dietmar.eggemann@arm.com
    Cc: joelaf@google.com
    Cc: juri.lelli@redhat.com
    Cc: mathieu.poirier@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: rjw@rjwysocki.net
    Cc: rostedt@goodmis.org
    Cc: tkjos@android.com
    Cc: tommaso.cucinotta@santannapisa.it
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/20171204102325.5110-4-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 863964fbcfd2..c5197338ac47 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -156,13 +156,37 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
+/*
+ * !! For sched_setattr_nocheck() (kernel) only !!
+ *
+ * This is actually gross. :(
+ *
+ * It is used to make schedutil kworker(s) higher priority than SCHED_DEADLINE
+ * tasks, but still be able to sleep. We need this on platforms that cannot
+ * atomically change clock frequency. Remove once fast switching will be
+ * available on such platforms.
+ *
+ * SUGOV stands for SchedUtil GOVernor.
+ */
+#define SCHED_FLAG_SUGOV	0x10000000
+
+static inline bool dl_entity_is_special(struct sched_dl_entity *dl_se)
+{
+#ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
+	return unlikely(dl_se->flags & SCHED_FLAG_SUGOV);
+#else
+	return false;
+#endif
+}
+
 /*
  * Tells if entity @a should preempt entity @b.
  */
 static inline bool
 dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
 {
-	return dl_time_before(a->deadline, b->deadline);
+	return dl_entity_is_special(a) ||
+	       dl_time_before(a->deadline, b->deadline);
 }
 
 /*
@@ -2085,6 +2109,8 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #define arch_scale_freq_invariant()	(false)
 #endif
 
+#ifdef CONFIG_CPU_FREQ_GOV_SCHEDUTIL
+
 static inline unsigned long cpu_util_dl(struct rq *rq)
 {
 	return (rq->dl.running_bw * SCHED_CAPACITY_SCALE) >> BW_SHIFT;
@@ -2094,3 +2120,5 @@ static inline unsigned long cpu_util_cfs(struct rq *rq)
 {
 	return rq->cfs.avg.util_avg;
 }
+
+#endif

commit e0367b12674bf4420870cd0237e3ebafb2ec9593
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Mon Dec 4 11:23:19 2017 +0100

    sched/deadline: Move CPU frequency selection triggering points
    
    Since SCHED_DEADLINE doesn't track utilization signal (but reserves a
    fraction of CPU bandwidth to tasks admitted to the system), there is no
    point in evaluating frequency changes during each tick event.
    
    Move frequency selection triggering points to where running_bw changes.
    
    Co-authored-by: Claudio Scordino <claudio@evidence.eu.com>
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: alessio.balsini@arm.com
    Cc: bristot@redhat.com
    Cc: dietmar.eggemann@arm.com
    Cc: joelaf@google.com
    Cc: juri.lelli@redhat.com
    Cc: mathieu.poirier@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: rjw@rjwysocki.net
    Cc: rostedt@goodmis.org
    Cc: tkjos@android.com
    Cc: tommaso.cucinotta@santannapisa.it
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/20171204102325.5110-3-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 136ab500daeb..863964fbcfd2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2055,14 +2055,14 @@ DECLARE_PER_CPU(struct update_util_data *, cpufreq_update_util_data);
  * The way cpufreq is currently arranged requires it to evaluate the CPU
  * performance state (frequency/voltage) on a regular basis to prevent it from
  * being stuck in a completely inadequate performance level for too long.
- * That is not guaranteed to happen if the updates are only triggered from CFS,
- * though, because they may not be coming in if RT or deadline tasks are active
- * all the time (or there are RT and DL tasks only).
+ * That is not guaranteed to happen if the updates are only triggered from CFS
+ * and DL, though, because they may not be coming in if only RT tasks are
+ * active all the time (or there are RT tasks only).
  *
- * As a workaround for that issue, this function is called by the RT and DL
- * sched classes to trigger extra cpufreq updates to prevent it from stalling,
+ * As a workaround for that issue, this function is called periodically by the
+ * RT sched class to trigger extra cpufreq updates to prevent it from stalling,
  * but that really is a band-aid.  Going forward it should be replaced with
- * solutions targeted more specifically at RT and DL tasks.
+ * solutions targeted more specifically at RT tasks.
  */
 static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
 {

commit d4edd662ac1657126df7ffd74a278958b133a77d
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Mon Dec 4 11:23:18 2017 +0100

    sched/cpufreq: Use the DEADLINE utilization signal
    
    SCHED_DEADLINE tracks active utilization signal with a per dl_rq
    variable named running_bw.
    
    Make use of that to drive CPU frequency selection: add up FAIR and
    DEADLINE contribution to get the required CPU capacity to handle both
    requirements (while RT still selects max frequency).
    
    Co-authored-by: Claudio Scordino <claudio@evidence.eu.com>
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@santannapisa.it>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: alessio.balsini@arm.com
    Cc: bristot@redhat.com
    Cc: dietmar.eggemann@arm.com
    Cc: joelaf@google.com
    Cc: juri.lelli@redhat.com
    Cc: mathieu.poirier@linaro.org
    Cc: morten.rasmussen@arm.com
    Cc: patrick.bellasi@arm.com
    Cc: rjw@rjwysocki.net
    Cc: rostedt@goodmis.org
    Cc: tkjos@android.com
    Cc: tommaso.cucinotta@santannapisa.it
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/20171204102325.5110-2-juri.lelli@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 43f5d6e936bb..136ab500daeb 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2084,3 +2084,13 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
 #else /* arch_scale_freq_capacity */
 #define arch_scale_freq_invariant()	(false)
 #endif
+
+static inline unsigned long cpu_util_dl(struct rq *rq)
+{
+	return (rq->dl.running_bw * SCHED_CAPACITY_SCALE) >> BW_SHIFT;
+}
+
+static inline unsigned long cpu_util_cfs(struct rq *rq)
+{
+	return rq->cfs.avg.util_avg;
+}

commit 31cb1bc0dc94882a588930f4d007b570c481fd17
Author: rodrigosiqueira <rodrigosiqueiramelo@gmail.com>
Date:   Fri Dec 15 12:06:03 2017 -0200

    sched/core: Rework and clarify prepare_lock_switch()
    
    The prepare_lock_switch() function has an unused parameter, and also the
    function name was not descriptive. To improve readability and remove
    the extra parameter, do the following changes:
    
    * Move prepare_lock_switch() from kernel/sched/sched.h to
      kernel/sched/core.c, rename it to prepare_task(), and remove the
      unused parameter.
    
    * Split the smp_store_release() out from finish_lock_switch() to a
      function named finish_task.
    
    * Comments ajdustments.
    
    Signed-off-by: Rodrigo Siqueira <rodrigosiqueiramelo@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171215140603.gxe5i2y6fg5ojfpp@smtp.gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b19552a212de..43f5d6e936bb 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1328,47 +1328,6 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 # define finish_arch_post_lock_switch()	do { } while (0)
 #endif
 
-static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
-{
-#ifdef CONFIG_SMP
-	/*
-	 * We can optimise this out completely for !SMP, because the
-	 * SMP rebalancing from interrupt is the only thing that cares
-	 * here.
-	 */
-	next->on_cpu = 1;
-#endif
-}
-
-static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
-{
-#ifdef CONFIG_SMP
-	/*
-	 * After ->on_cpu is cleared, the task can be moved to a different CPU.
-	 * We must ensure this doesn't happen until the switch is completely
-	 * finished.
-	 *
-	 * In particular, the load of prev->state in finish_task_switch() must
-	 * happen before this.
-	 *
-	 * Pairs with the smp_cond_load_acquire() in try_to_wake_up().
-	 */
-	smp_store_release(&prev->on_cpu, 0);
-#endif
-#ifdef CONFIG_DEBUG_SPINLOCK
-	/* this is a valid case when another task releases the spinlock */
-	rq->lock.owner = current;
-#endif
-	/*
-	 * If we are tracking spinlock dependencies then we have to
-	 * fix up the runqueue lock - which gets 'carried over' from
-	 * prev into current:
-	 */
-	spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
-
-	raw_spin_unlock_irq(&rq->lock);
-}
-
 /*
  * wake flags
  */

commit 22714a2ba4b55737cd7d5299db7aaf1fa8287354
Merge: 766ec76a27aa 5f2e673405b7
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Nov 15 14:29:44 2017 -0800

    Merge branch 'for-4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "Cgroup2 cpu controller support is finally merged.
    
       - Basic cpu statistics support to allow monitoring by default without
         the CPU controller enabled.
    
       - cgroup2 cpu controller support.
    
       - /sys/kernel/cgroup files to help dealing with new / optional
         features"
    
    * 'for-4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: export list of cgroups v2 features using sysfs
      cgroup: export list of delegatable control files using sysfs
      cgroup: mark @cgrp __maybe_unused in cpu_stat_show()
      MAINTAINERS: relocate cpuset.c
      cgroup, sched: Move basic cpu stats from cgroup.stat to cpu.stat
      sched: Implement interface for cgroup unified hierarchy
      sched: Misc preps for cgroup unified hierarchy interface
      sched/cputime: Add dummy cputime_adjust() implementation for CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
      cgroup: statically initialize init_css_set->dfl_cgrp
      cgroup: Implement cgroup2 basic CPU usage accounting
      cpuacct: Introduce cgroup_account_cputime[_field]()
      sched/cputime: Expose cputime_adjust()

commit 765cc3a4b224e22bf524fabe40284a524f37cdd0
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Wed Nov 8 18:41:01 2017 +0000

    sched/core: Optimize sched_feat() for !CONFIG_SCHED_DEBUG builds
    
    When the kernel is compiled with !CONFIG_SCHED_DEBUG support, we expect that
    all SCHED_FEAT are turned into compile time constants being propagated
    to support compiler optimizations.
    
    Specifically, we expect that code blocks like this:
    
       if (sched_feat(FEATURE_NAME) [&& <other_conditions>]) {
            /* FEATURE CODE */
       }
    
    are turned into dead-code in case FEATURE_NAME defaults to FALSE, and thus
    being removed by the compiler from the finale image.
    
    For this mechanism to properly work it's required for the compiler to
    have full access, from each translation unit, to whatever is the value
    defined by the sched_feat macro. This macro is defined as:
    
       #define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
    
    and thus, the compiler can optimize that code only if the value of
    sysctl_sched_features is visible within each translation unit.
    
    Since:
    
       029632fbb ("sched: Make separate sched*.c translation units")
    
    the scheduler code has been split into separate translation units
    however the definition of sysctl_sched_features is part of
    kernel/sched/core.c while, for all the other scheduler modules, it is
    visible only via kernel/sched/sched.h as an:
    
       extern const_debug unsigned int sysctl_sched_features
    
    Unfortunately, an extern reference does not allow the compiler to apply
    constants propagation. Thus, on !CONFIG_SCHED_DEBUG kernel we still end up
    with code to load a memory reference and (eventually) doing an unconditional
    jump of a chunk of code.
    
    This mechanism is unavoidable when sched_features can be turned on and off at
    run-time. However, this is not the case for "production" kernels compiled with
    !CONFIG_SCHED_DEBUG. In this case, sysctl_sched_features is just a constant value
    which cannot be changed at run-time and thus memory loads and jumps can be
    avoided altogether.
    
    This patch fixes the case of !CONFIG_SCHED_DEBUG kernel by declaring a local version
    of the sysctl_sched_features constant for each translation unit. This will
    ultimately allow the compiler to perform constants propagation and dead-code
    pruning.
    
    Tests have been done, with !CONFIG_SCHED_DEBUG on a v4.14-rc8 with and without
    the patch, by running 30 iterations of:
    
       perf bench sched messaging --pipe --thread --group 4 --loop 50000
    
    on a 40 cores Intel(R) Xeon(R) CPU E5-2690 v2 @ 3.00GHz using the
    powersave governor to rule out variations due to frequency scaling.
    
    Statistics on the reported completion time:
    
                       count     mean       std     min       99%     max
      v4.14-rc8         30.0  15.7831  0.176032  15.442  16.01226  16.014
      v4.14-rc8+patch   30.0  15.5033  0.189681  15.232  15.93938  15.962
    
    ... show a 1.8% speedup on average completion time and 0.5% speedup in the
    99 percentile.
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Chris Redpath <chris.redpath@arm.com>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Reviewed-by: Brendan Jackman <brendan.jackman@arm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: http://lkml.kernel.org/r/20171108184101.16006-1-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 58787e3631c7..45ab0bf564e7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1233,8 +1233,6 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 # define const_debug const
 #endif
 
-extern const_debug unsigned int sysctl_sched_features;
-
 #define SCHED_FEAT(name, enabled)	\
 	__SCHED_FEAT_##name ,
 
@@ -1246,6 +1244,13 @@ enum {
 #undef SCHED_FEAT
 
 #if defined(CONFIG_SCHED_DEBUG) && defined(HAVE_JUMP_LABEL)
+
+/*
+ * To support run-time toggling of sched features, all the translation units
+ * (but core.c) reference the sysctl_sched_features defined in core.c.
+ */
+extern const_debug unsigned int sysctl_sched_features;
+
 #define SCHED_FEAT(name, enabled)					\
 static __always_inline bool static_branch_##name(struct static_key *key) \
 {									\
@@ -1253,13 +1258,27 @@ static __always_inline bool static_branch_##name(struct static_key *key) \
 }
 
 #include "features.h"
-
 #undef SCHED_FEAT
 
 extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #define sched_feat(x) (static_branch_##x(&sched_feat_keys[__SCHED_FEAT_##x]))
+
 #else /* !(SCHED_DEBUG && HAVE_JUMP_LABEL) */
+
+/*
+ * Each translation unit has its own copy of sysctl_sched_features to allow
+ * constants propagation at compile time and compiler optimization based on
+ * features default.
+ */
+#define SCHED_FEAT(name, enabled)	\
+	(1UL << __SCHED_FEAT_##name) * enabled |
+static const_debug __maybe_unused unsigned int sysctl_sched_features =
+#include "features.h"
+	0;
+#undef SCHED_FEAT
+
 #define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
+
 #endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
 
 extern struct static_key_false sched_numa_balancing;

commit 8a103df440afea30c91ebd42e61dc644e647f4bd
Merge: a9903f04e0a4 fbc3edf7d773
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Nov 8 10:17:15 2017 +0100

    Merge branch 'linus' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 14db76cd496f..3b448ba82225 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 
 #include <linux/sched.h>
 #include <linux/sched/autogroup.h>

commit 4bdced5c9a2922521e325896a7bbbf0132c94e56
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Oct 6 14:05:04 2017 -0400

    sched/rt: Simplify the IPI based RT balancing logic
    
    When a CPU lowers its priority (schedules out a high priority task for a
    lower priority one), a check is made to see if any other CPU has overloaded
    RT tasks (more than one). It checks the rto_mask to determine this and if so
    it will request to pull one of those tasks to itself if the non running RT
    task is of higher priority than the new priority of the next task to run on
    the current CPU.
    
    When we deal with large number of CPUs, the original pull logic suffered
    from large lock contention on a single CPU run queue, which caused a huge
    latency across all CPUs. This was caused by only having one CPU having
    overloaded RT tasks and a bunch of other CPUs lowering their priority. To
    solve this issue, commit:
    
      b6366f048e0c ("sched/rt: Use IPI to trigger RT task push migration instead of pulling")
    
    changed the way to request a pull. Instead of grabbing the lock of the
    overloaded CPU's runqueue, it simply sent an IPI to that CPU to do the work.
    
    Although the IPI logic worked very well in removing the large latency build
    up, it still could suffer from a large number of IPIs being sent to a single
    CPU. On a 80 CPU box, I measured over 200us of processing IPIs. Worse yet,
    when I tested this on a 120 CPU box, with a stress test that had lots of
    RT tasks scheduling on all CPUs, it actually triggered the hard lockup
    detector! One CPU had so many IPIs sent to it, and due to the restart
    mechanism that is triggered when the source run queue has a priority status
    change, the CPU spent minutes! processing the IPIs.
    
    Thinking about this further, I realized there's no reason for each run queue
    to send its own IPI. As all CPUs with overloaded tasks must be scanned
    regardless if there's one or many CPUs lowering their priority, because
    there's no current way to find the CPU with the highest priority task that
    can schedule to one of these CPUs, there really only needs to be one IPI
    being sent around at a time.
    
    This greatly simplifies the code!
    
    The new approach is to have each root domain have its own irq work, as the
    rto_mask is per root domain. The root domain has the following fields
    attached to it:
    
      rto_push_work  - the irq work to process each CPU set in rto_mask
      rto_lock       - the lock to protect some of the other rto fields
      rto_loop_start - an atomic that keeps contention down on rto_lock
                        the first CPU scheduling in a lower priority task
                        is the one to kick off the process.
      rto_loop_next  - an atomic that gets incremented for each CPU that
                        schedules in a lower priority task.
      rto_loop       - a variable protected by rto_lock that is used to
                        compare against rto_loop_next
      rto_cpu        - The cpu to send the next IPI to, also protected by
                        the rto_lock.
    
    When a CPU schedules in a lower priority task and wants to make sure
    overloaded CPUs know about it. It increments the rto_loop_next. Then it
    atomically sets rto_loop_start with a cmpxchg. If the old value is not "0",
    then it is done, as another CPU is kicking off the IPI loop. If the old
    value is "0", then it will take the rto_lock to synchronize with a possible
    IPI being sent around to the overloaded CPUs.
    
    If rto_cpu is greater than or equal to nr_cpu_ids, then there's either no
    IPI being sent around, or one is about to finish. Then rto_cpu is set to the
    first CPU in rto_mask and an IPI is sent to that CPU. If there's no CPUs set
    in rto_mask, then there's nothing to be done.
    
    When the CPU receives the IPI, it will first try to push any RT tasks that is
    queued on the CPU but can't run because a higher priority RT task is
    currently running on that CPU.
    
    Then it takes the rto_lock and looks for the next CPU in the rto_mask. If it
    finds one, it simply sends an IPI to that CPU and the process continues.
    
    If there's no more CPUs in the rto_mask, then rto_loop is compared with
    rto_loop_next. If they match, everything is done and the process is over. If
    they do not match, then a CPU scheduled in a lower priority task as the IPI
    was being passed around, and the process needs to start again. The first CPU
    in rto_mask is sent the IPI.
    
    This change removes this duplication of work in the IPI logic, and greatly
    lowers the latency caused by the IPIs. This removed the lockup happening on
    the 120 CPU machine. It also simplifies the code tremendously. What else
    could anyone ask for?
    
    Thanks to Peter Zijlstra for simplifying the rto_loop_start atomic logic and
    supplying me with the rto_start_trylock() and rto_start_unlock() helper
    functions.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott Wood <swood@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170424114732.1aac6dc4@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index a81c9782e98c..8aa24b41f652 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -505,7 +505,7 @@ static inline int rt_bandwidth_enabled(void)
 }
 
 /* RT IPI pull logic requires IRQ_WORK */
-#ifdef CONFIG_IRQ_WORK
+#if defined(CONFIG_IRQ_WORK) && defined(CONFIG_SMP)
 # define HAVE_RT_PUSH_IPI
 #endif
 
@@ -527,12 +527,6 @@ struct rt_rq {
 	unsigned long rt_nr_total;
 	int overloaded;
 	struct plist_head pushable_tasks;
-#ifdef HAVE_RT_PUSH_IPI
-	int push_flags;
-	int push_cpu;
-	struct irq_work push_work;
-	raw_spinlock_t push_lock;
-#endif
 #endif /* CONFIG_SMP */
 	int rt_queued;
 
@@ -641,6 +635,19 @@ struct root_domain {
 	struct dl_bw dl_bw;
 	struct cpudl cpudl;
 
+#ifdef HAVE_RT_PUSH_IPI
+	/*
+	 * For IPI pull requests, loop across the rto_mask.
+	 */
+	struct irq_work rto_push_work;
+	raw_spinlock_t rto_lock;
+	/* These are only updated and read within rto_lock */
+	int rto_loop;
+	int rto_cpu;
+	/* These atomics are updated outside of a lock */
+	atomic_t rto_loop_next;
+	atomic_t rto_loop_start;
+#endif
 	/*
 	 * The "RT overload" flag: it gets set if a CPU has more than
 	 * one runnable RT task.
@@ -658,6 +665,9 @@ extern void init_defrootdomain(void);
 extern int sched_init_domains(const struct cpumask *cpu_map);
 extern void rq_attach_root(struct rq *rq, struct root_domain *rd);
 
+#ifdef HAVE_RT_PUSH_IPI
+extern void rto_push_irq_work_func(struct irq_work *work);
+#endif
 #endif /* CONFIG_SMP */
 
 /*

commit 8c0944cee7af55291df0b28e6e2eeac0930e93c9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 7 12:09:30 2017 +0200

    sched/deadline: Rename __dl_clear() to __dl_sub()
    
    __dl_sub() is more meaningful as a name, and is more consistent
    with the naming of the dual function (__dl_add()).
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1504778971-13573-4-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9d5aa18fc9bf..a81c9782e98c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -226,7 +226,7 @@ struct dl_bw {
 static inline void __dl_update(struct dl_bw *dl_b, s64 bw);
 
 static inline
-void __dl_clear(struct dl_bw *dl_b, u64 tsk_bw, int cpus)
+void __dl_sub(struct dl_bw *dl_b, u64 tsk_bw, int cpus)
 {
 	dl_b->total_bw -= tsk_bw;
 	__dl_update(dl_b, (s32)tsk_bw / cpus);

commit e964d3501b64d6930aaa4dd18955a8cd086ccb92
Author: luca abeni <luca.abeni@santannapisa.it>
Date:   Thu Sep 7 12:09:28 2017 +0200

    sched/headers: Remove duplicate prototype of __dl_clear_params()
    
    Signed-off-by: luca abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1504778971-13573-2-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e83d1b8be611..9d5aa18fc9bf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -255,7 +255,6 @@ extern int sched_dl_overflow(struct task_struct *p, int policy,
 extern void __setparam_dl(struct task_struct *p, const struct sched_attr *attr);
 extern void __getparam_dl(struct task_struct *p, struct sched_attr *attr);
 extern bool __checkparam_dl(const struct sched_attr *attr);
-extern void __dl_clear_params(struct task_struct *p);
 extern bool dl_param_changed(struct task_struct *p, const struct sched_attr *attr);
 extern int dl_task_can_attach(struct task_struct *p,
 			      const struct cpumask *cs_cpus_allowed);

commit 1ea6c46a23f1213d1972bfae220db5c165e27bba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat May 6 15:59:54 2017 +0200

    sched/fair: Propagate an effective runnable_load_avg
    
    The load balancer uses runnable_load_avg as load indicator. For
    !cgroup this is:
    
      runnable_load_avg = \Sum se->avg.load_avg ; where se->on_rq
    
    That is, a direct sum of all runnable tasks on that runqueue. As
    opposed to load_avg, which is a sum of all tasks on the runqueue,
    which includes a blocked component.
    
    However, in the cgroup case, this comes apart since the group entities
    are always runnable, even if most of their constituent entities are
    blocked.
    
    Therefore introduce a runnable_weight which for task entities is the
    same as the regular weight, but for group entities is a fraction of
    the entity weight and represents the runnable part of the group
    runqueue.
    
    Then propagate this load through the PELT hierarchy to arrive at an
    effective runnable load avgerage -- which we should not confuse with
    the canonical runnable load average.
    
    Suggested-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5bcb86eb026b..e83d1b8be611 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -418,6 +418,7 @@ struct cfs_bandwidth { };
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
 	struct load_weight load;
+	unsigned long runnable_weight;
 	unsigned int nr_running, h_nr_running;
 
 	u64 exec_clock;
@@ -443,8 +444,6 @@ struct cfs_rq {
 	 * CFS load tracking
 	 */
 	struct sched_avg avg;
-	u64 runnable_load_sum;
-	unsigned long runnable_load_avg;
 #ifndef CONFIG_64BIT
 	u64 load_last_update_time_copy;
 #endif

commit 0e2d2aaaae52c247c047d14999b93486bdbd3431
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 8 17:30:46 2017 +0200

    sched/fair: Rewrite PELT migration propagation
    
    When an entity migrates in (or out) of a runqueue, we need to add (or
    remove) its contribution from the entire PELT hierarchy, because even
    non-runnable entities are included in the load average sums.
    
    In order to do this we have some propagation logic that updates the
    PELT tree, however the way it 'propagates' the runnable (or load)
    change is (more or less):
    
                         tg->weight * grq->avg.load_avg
      ge->avg.load_avg = ------------------------------
                                   tg->load_avg
    
    But that is the expression for ge->weight, and per the definition of
    load_avg:
    
      ge->avg.load_avg := ge->weight * ge->avg.runnable_avg
    
    That destroys the runnable_avg (by setting it to 1) we wanted to
    propagate.
    
    Instead directly propagate runnable_sum.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2fd350a12bb7..5bcb86eb026b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -447,19 +447,20 @@ struct cfs_rq {
 	unsigned long runnable_load_avg;
 #ifndef CONFIG_64BIT
 	u64 load_last_update_time_copy;
-#endif
-#ifdef CONFIG_FAIR_GROUP_SCHED
-	unsigned long tg_load_avg_contrib;
-	unsigned long propagate_avg;
 #endif
 	struct {
 		raw_spinlock_t	lock ____cacheline_aligned;
 		int		nr;
 		unsigned long	load_avg;
 		unsigned long	util_avg;
+		unsigned long	runnable_sum;
 	} removed;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
+	unsigned long tg_load_avg_contrib;
+	long propagate;
+	long prop_runnable_sum;
+
 	/*
 	 *   h_load = weight * f(tg)
 	 *

commit 2a2f5d4e44ed160a5ed822c94e04f918f9fbb487
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 8 16:51:41 2017 +0200

    sched/fair: Rewrite cfs_rq->removed_*avg
    
    Since on wakeup migration we don't hold the rq->lock for the old CPU
    we cannot update its state. Instead we add the removed 'load' to an
    atomic variable and have the next update on that CPU collect and
    process it.
    
    Currently we have 2 atomic variables; which already have the issue
    that they can be read out-of-sync. Also, two atomic ops on a single
    cacheline is already more expensive than an uncontended lock.
    
    Since we want to add more, convert the thing over to an explicit
    cacheline with a lock in.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index a5d97460ee4e..2fd350a12bb7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -445,14 +445,19 @@ struct cfs_rq {
 	struct sched_avg avg;
 	u64 runnable_load_sum;
 	unsigned long runnable_load_avg;
+#ifndef CONFIG_64BIT
+	u64 load_last_update_time_copy;
+#endif
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	unsigned long tg_load_avg_contrib;
 	unsigned long propagate_avg;
 #endif
-	atomic_long_t removed_load_avg, removed_util_avg;
-#ifndef CONFIG_64BIT
-	u64 load_last_update_time_copy;
-#endif
+	struct {
+		raw_spinlock_t	lock ____cacheline_aligned;
+		int		nr;
+		unsigned long	load_avg;
+		unsigned long	util_avg;
+	} removed;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/*

commit 9059393e4ec1c8c6623a120b405ef2c90b968d80
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Wed May 17 11:50:45 2017 +0200

    sched/fair: Use reweight_entity() for set_user_nice()
    
    Now that we directly change load_avg and propagate that change into
    the sums, sys_nice() and co should do the same, otherwise its possible
    to confuse load accounting when we migrate near the weight change.
    
    Fixes-by: Josef Bacik <josef@toxicpanda.com>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    [ Added changelog, fixed the call condition. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20170517095045.GA8420@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 14db76cd496f..a5d97460ee4e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1529,6 +1529,8 @@ extern void init_sched_dl_class(void);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
 
+extern void reweight_task(struct task_struct *p, int prio);
+
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 

commit d2cc5ed6949085cfba30ec5228816cf6eb1d02b9
Author: Tejun Heo <tj@kernel.org>
Date:   Mon Sep 25 08:12:04 2017 -0700

    cpuacct: Introduce cgroup_account_cputime[_field]()
    
    Introduce cgroup_account_cputime[_field]() which wrap cpuacct_charge()
    and cgroup_account_field().  This doesn't introduce any functional
    changes and will be used to add cgroup basic resource accounting.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@redhat.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 14db76cd496f..f0b98f978843 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -29,6 +29,7 @@
 #include <linux/irq_work.h>
 #include <linux/tick.h>
 #include <linux/slab.h>
+#include <linux/cgroup.h>
 
 #ifdef CONFIG_PARAVIRT
 #include <asm/paravirt.h>
@@ -36,7 +37,6 @@
 
 #include "cpupri.h"
 #include "cpudeadline.h"
-#include "cpuacct.h"
 
 #ifdef CONFIG_SCHED_DEBUG
 # define SCHED_WARN_ON(x)	WARN_ONCE(x, #x)

commit ec846ecd6350857a8b8b9a6b78c763d45e0f09b8
Merge: b5df1b3a5637 9469eb01db89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 13 12:22:32 2017 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Three CPU hotplug related fixes and a debugging improvement"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/debug: Add debugfs knob for "sched_debug"
      sched/core: WARN() when migrating to an offline CPU
      sched/fair: Plug hole between hotplug and active_load_balance()
      sched/fair: Avoid newidle balance for !active CPUs

commit 9469eb01db891b55367ee7539f1b9f7f6fd2819d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 7 17:03:53 2017 +0200

    sched/debug: Add debugfs knob for "sched_debug"
    
    I'm forever late for editing my kernel cmdline, add a runtime knob to
    disable the "sched_debug" thing.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170907150614.142924283@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ab1c7f5409a0..7ea2a0339771 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1954,6 +1954,8 @@ extern struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq);
 extern struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq);
 
 #ifdef	CONFIG_SCHED_DEBUG
+extern bool sched_debug_enabled;
+
 extern void print_cfs_stats(struct seq_file *m, int cpu);
 extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void print_dl_stats(struct seq_file *m, int cpu);

commit 2161573ecd6931565936cb66793b2d2bf805c088
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:14:58 2017 -0700

    sched/deadline: replace earliest dl and rq leftmost caching
    
    ... with the generic rbtree flavor instead. No changes
    in semantics whatsoever.
    
    Link: http://lkml.kernel.org/r/20170719014603.19029-9-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c30c57563dbc..746ac78ff492 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -549,8 +549,7 @@ struct rt_rq {
 /* Deadline class' related fields in a runqueue */
 struct dl_rq {
 	/* runqueue is an rbtree, ordered by deadline */
-	struct rb_root rb_root;
-	struct rb_node *rb_leftmost;
+	struct rb_root_cached root;
 
 	unsigned long dl_nr_running;
 
@@ -574,8 +573,7 @@ struct dl_rq {
 	 * an rb-tree, ordered by tasks' deadlines, with caching
 	 * of the leftmost (earliest deadline) element.
 	 */
-	struct rb_root pushable_dl_tasks_root;
-	struct rb_node *pushable_dl_tasks_leftmost;
+	struct rb_root_cached pushable_dl_tasks_root;
 #else
 	struct dl_bw dl_bw;
 #endif

commit bfb068892d30dcf0a32b89302fe293347adeaaaa
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:14:55 2017 -0700

    sched/fair: replace cfs_rq->rb_leftmost
    
    ... with the generic rbtree flavor instead. No changes
    in semantics whatsoever.
    
    Link: http://lkml.kernel.org/r/20170719014603.19029-8-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6ed7962dc896..c30c57563dbc 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -426,8 +426,7 @@ struct cfs_rq {
 	u64 min_vruntime_copy;
 #endif
 
-	struct rb_root tasks_timeline;
-	struct rb_node *rb_leftmost;
+	struct rb_root_cached tasks_timeline;
 
 	/*
 	 * 'curr' points to currently running entity on this cfs_rq.

commit 439644096c1a6afb9bd9953130f4444a856f76c5
Merge: b42a362e6d10 d97561f461e4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 5 12:19:08 2017 -0700

    Merge tag 'pm-4.14-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "This time (again) cpufreq gets the majority of changes which mostly
      are driver updates (including a major consolidation of intel_pstate),
      some schedutil governor modifications and core cleanups.
    
      There also are some changes in the system suspend area, mostly related
      to diagnostics and debug messages plus some renames of things related
      to suspend-to-idle. One major change here is that suspend-to-idle is
      now going to be preferred over S3 on systems where the ACPI tables
      indicate to do so and provide requsite support (the Low Power Idle S0
      _DSM in particular). The system sleep documentation and the tools
      related to it are updated too.
    
      The rest is a few cpuidle changes (nothing major), devfreq updates,
      generic power domains (genpd) framework updates and a few assorted
      modifications elsewhere.
    
      Specifics:
    
       - Drop the P-state selection algorithm based on a PID controller from
         intel_pstate and make it use the same P-state selection method
         (based on the CPU load) for all types of systems in the active mode
         (Rafael Wysocki, Srinivas Pandruvada).
    
       - Rework the cpufreq core and governors to make it possible to take
         cross-CPU utilization updates into account and modify the schedutil
         governor to actually do so (Viresh Kumar).
    
       - Clean up the handling of transition latency information in the
         cpufreq core and untangle it from the information on which drivers
         cannot do dynamic frequency switching (Viresh Kumar).
    
       - Add support for new SoCs (MT2701/MT7623 and MT7622) to the mediatek
         cpufreq driver and update its DT bindings (Sean Wang).
    
       - Modify the cpufreq dt-platdev driver to autimatically create
         cpufreq devices for the new (v2) Operating Performance Points (OPP)
         DT bindings and update its whitelist of supported systems (Viresh
         Kumar, Shubhrajyoti Datta, Marc Gonzalez, Khiem Nguyen, Finley
         Xiao).
    
       - Add support for Ux500 to the cpufreq-dt driver and drop the
         obsolete dbx500 cpufreq driver (Linus Walleij, Arnd Bergmann).
    
       - Add new SoC (R8A7795) support to the cpufreq rcar driver (Khiem
         Nguyen).
    
       - Fix and clean up assorted issues in the cpufreq drivers and core
         (Arvind Yadav, Christophe Jaillet, Colin Ian King, Gustavo Silva,
         Julia Lawall, Leonard Crestez, Rob Herring, Sudeep Holla).
    
       - Update the IO-wait boost handling in the schedutil governor to make
         it less aggressive (Joel Fernandes).
    
       - Rework system suspend diagnostics to make it print fewer messages
         to the kernel log by default, add a sysfs knob to allow more
         suspend-related messages to be printed and add Low Power S0 Idle
         constraints checks to the ACPI suspend-to-idle code (Rafael
         Wysocki, Srinivas Pandruvada).
    
       - Prefer suspend-to-idle over S3 on ACPI-based systems with the
         ACPI_FADT_LOW_POWER_S0 flag set and the Low Power Idle S0 _DSM
         interface present in the ACPI tables (Rafael Wysocki).
    
       - Update documentation related to system sleep and rename a number of
         items in the code to make it cleare that they are related to
         suspend-to-idle (Rafael Wysocki).
    
       - Export a variable allowing device drivers to check the target
         system sleep state from the core system suspend code (Florian
         Fainelli).
    
       - Clean up the cpuidle subsystem to handle the polling state on x86
         in a more straightforward way and to use %pOF instead of full_name
         (Rafael Wysocki, Rob Herring).
    
       - Update the devfreq framework to fix and clean up a few minor issues
         (Chanwoo Choi, Rob Herring).
    
       - Extend diagnostics in the generic power domains (genpd) framework
         and clean it up slightly (Thara Gopinath, Rob Herring).
    
       - Fix and clean up a couple of issues in the operating performance
         points (OPP) framework (Viresh Kumar, Waldemar Rymarkiewicz).
    
       - Add support for RV1108 to the rockchip-io Adaptive Voltage Scaling
         (AVS) driver (David Wu).
    
       - Fix the usage of notifiers in CPU power management on some
         platforms (Alex Shi).
    
       - Update the pm-graph system suspend/hibernation and boot profiling
         utility (Todd Brandt).
    
       - Make it possible to run the cpupower utility without CPU0 (Prarit
         Bhargava)"
    
    * tag 'pm-4.14-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (87 commits)
      cpuidle: Make drivers initialize polling state
      cpuidle: Move polling state initialization code to separate file
      cpuidle: Eliminate the CPUIDLE_DRIVER_STATE_START symbol
      cpufreq: imx6q: Fix imx6sx low frequency support
      cpufreq: speedstep-lib: make several arrays static, makes code smaller
      PM: docs: Delete the obsolete states.txt document
      PM: docs: Describe high-level PM strategies and sleep states
      PM / devfreq: Fix memory leak when fail to register device
      PM / devfreq: Add dependency on PM_OPP
      PM / devfreq: Move private devfreq_update_stats() into devfreq
      PM / devfreq: Convert to using %pOF instead of full_name
      PM / AVS: rockchip-io: add io selectors and supplies for RV1108
      cpufreq: ti: Fix 'of_node_put' being called twice in error handling path
      cpufreq: dt-platdev: Drop few entries from whitelist
      cpufreq: dt-platdev: Automatically create cpufreq device with OPP v2
      ARM: ux500: don't select CPUFREQ_DT
      cpuidle: Convert to using %pOF instead of full_name
      cpufreq: Convert to using %pOF instead of full_name
      PM / Domains: Convert to using %pOF instead of full_name
      cpufreq: Cap the default transition delay value to 10 ms
      ...

commit 5f82e71a001d14824a7728ad9e49f6aea420f161
Merge: 6c51e67b64d1 edc2988c548d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 11:52:29 2017 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - Add 'cross-release' support to lockdep, which allows APIs like
       completions, where it's not the 'owner' who releases the lock, to be
       tracked. It's all activated automatically under
       CONFIG_PROVE_LOCKING=y.
    
     - Clean up (restructure) the x86 atomics op implementation to be more
       readable, in preparation of KASAN annotations. (Dmitry Vyukov)
    
     - Fix static keys (Paolo Bonzini)
    
     - Add killable versions of down_read() et al (Kirill Tkhai)
    
     - Rework and fix jump_label locking (Marc Zyngier, Paolo Bonzini)
    
     - Rework (and fix) tlb_flush_pending() barriers (Peter Zijlstra)
    
     - Remove smp_mb__before_spinlock() and convert its usages, introduce
       smp_mb__after_spinlock() (Peter Zijlstra)
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (56 commits)
      locking/lockdep/selftests: Fix mixed read-write ABBA tests
      sched/completion: Avoid unnecessary stack allocation for COMPLETION_INITIALIZER_ONSTACK()
      acpi/nfit: Fix COMPLETION_INITIALIZER_ONSTACK() abuse
      locking/pvqspinlock: Relax cmpxchg's to improve performance on some architectures
      smp: Avoid using two cache lines for struct call_single_data
      locking/lockdep: Untangle xhlock history save/restore from task independence
      locking/refcounts, x86/asm: Disable CONFIG_ARCH_HAS_REFCOUNT for the time being
      futex: Remove duplicated code and fix undefined behaviour
      Documentation/locking/atomic: Finish the document...
      locking/lockdep: Fix workqueue crossrelease annotation
      workqueue/lockdep: 'Fix' flush_work() annotation
      locking/lockdep/selftests: Add mixed read-write ABBA tests
      mm, locking/barriers: Clarify tlb_flush_pending() barriers
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE and CONFIG_LOCKDEP_COMPLETIONS truly non-interactive
      locking/lockdep: Explicitly initialize wq_barrier::done::map
      locking/lockdep: Rename CONFIG_LOCKDEP_COMPLETE to CONFIG_LOCKDEP_COMPLETIONS
      locking/lockdep: Reword title of LOCKDEP_CROSSRELEASE config
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE part of CONFIG_PROVE_LOCKING
      locking/refcounts, x86/asm: Implement fast refcount overflow protection
      locking/lockdep: Fix the rollback and overwrite detection logic in crossrelease
      ...

commit 966a967116e699762dbf4af7f9e0d1955c25aa37
Author: Ying Huang <ying.huang@intel.com>
Date:   Tue Aug 8 12:30:00 2017 +0800

    smp: Avoid using two cache lines for struct call_single_data
    
    struct call_single_data is used in IPIs to transfer information between
    CPUs.  Its size is bigger than sizeof(unsigned long) and less than
    cache line size.  Currently it is not allocated with any explicit alignment
    requirements.  This makes it possible for allocated call_single_data to
    cross two cache lines, which results in double the number of the cache lines
    that need to be transferred among CPUs.
    
    This can be fixed by requiring call_single_data to be aligned with the
    size of call_single_data. Currently the size of call_single_data is the
    power of 2.  If we add new fields to call_single_data, we may need to
    add padding to make sure the size of new definition is the power of 2
    as well.
    
    Fortunately, this is enforced by GCC, which will report bad sizes.
    
    To set alignment requirements of call_single_data to the size of
    call_single_data, a struct definition and a typedef is used.
    
    To test the effect of the patch, I used the vm-scalability multiple
    thread swap test case (swap-w-seq-mt).  The test will create multiple
    threads and each thread will eat memory until all RAM and part of swap
    is used, so that huge number of IPIs are triggered when unmapping
    memory.  In the test, the throughput of memory writing improves ~5%
    compared with misaligned call_single_data, because of faster IPIs.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Huang, Ying <ying.huang@intel.com>
    [ Add call_single_data_t and align with size of call_single_data. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lu@intel.com>
    Cc: Borislav Petkov <bp@suse.de>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Juergen Gross <jgross@suse.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/87bmnqd6lz.fsf@yhuang-mobile.sh.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index eeef1a3086d1..f29a7d2b57e1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -769,7 +769,7 @@ struct rq {
 #ifdef CONFIG_SCHED_HRTICK
 #ifdef CONFIG_SMP
 	int hrtick_csd_pending;
-	struct call_single_data hrtick_csd;
+	call_single_data_t hrtick_csd;
 #endif
 	struct hrtimer hrtick_timer;
 #endif

commit bbdacdfed2f5fa50a2cc9f500a36e05990a0837d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Aug 10 17:10:26 2017 +0200

    sched/debug: Optimize sched_domain sysctl generation
    
    Currently we unconditionally destroy all sysctl bits and regenerate
    them after we've rebuild the domains (even if that rebuild is a
    no-op).
    
    And since we unconditionally (re)build the sysctl for all possible
    CPUs, onlining all CPUs gets us O(n^2) time. Instead change this to
    only rebuild the bits for CPUs we've actually installed new domains
    on.
    
    Reported-by: Ofer Levi(SW) <oferle@mellanox.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index eeef1a3086d1..25e5cb1107f3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1120,11 +1120,15 @@ extern int group_balance_cpu(struct sched_group *sg);
 
 #if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_SYSCTL)
 void register_sched_domain_sysctl(void);
+void dirty_sched_domain_sysctl(int cpu);
 void unregister_sched_domain_sysctl(void);
 #else
 static inline void register_sched_domain_sysctl(void)
 {
 }
+static inline void dirty_sched_domain_sysctl(int cpu)
+{
+}
 static inline void unregister_sched_domain_sysctl(void)
 {
 }

commit 674e75411fc260b0d4532701228cfe12fc090da8
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Fri Jul 28 12:16:38 2017 +0530

    sched: cpufreq: Allow remote cpufreq callbacks
    
    With Android UI and benchmarks the latency of cpufreq response to
    certain scheduling events can become very critical. Currently, callbacks
    into cpufreq governors are only made from the scheduler if the target
    CPU of the event is the same as the current CPU. This means there are
    certain situations where a target CPU may not run the cpufreq governor
    for some time.
    
    One testcase to show this behavior is where a task starts running on
    CPU0, then a new task is also spawned on CPU0 by a task on CPU1. If the
    system is configured such that the new tasks should receive maximum
    demand initially, this should result in CPU0 increasing frequency
    immediately. But because of the above mentioned limitation though, this
    does not occur.
    
    This patch updates the scheduler core to call the cpufreq callbacks for
    remote CPUs as well.
    
    The schedutil, ondemand and conservative governors are updated to
    process cpufreq utilization update hooks called for remote CPUs where
    the remote CPU is managed by the cpufreq policy of the local CPU.
    
    The intel_pstate driver is updated to always reject remote callbacks.
    
    This is tested with couple of usecases (Android: hackbench, recentfling,
    galleryfling, vellamo, Ubuntu: hackbench) on ARM hikey board (64 bit
    octa-core, single policy). Only galleryfling showed minor improvements,
    while others didn't had much deviation.
    
    The reason being that this patch only targets a corner case, where
    following are required to be true to improve performance and that
    doesn't happen too often with these tests:
    
    - Task is migrated to another CPU.
    - The task has high demand, and should take the target CPU to higher
      OPPs.
    - And the target CPU doesn't call into the cpufreq governor until the
      next tick.
    
    Based on initial work from Steve Muckle.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Saravana Kannan <skannan@codeaurora.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index eeef1a3086d1..aa9d5b87b4f8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2070,19 +2070,13 @@ static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
 {
 	struct update_util_data *data;
 
-	data = rcu_dereference_sched(*this_cpu_ptr(&cpufreq_update_util_data));
+	data = rcu_dereference_sched(*per_cpu_ptr(&cpufreq_update_util_data,
+						  cpu_of(rq)));
 	if (data)
 		data->func(data, rq_clock(rq), flags);
 }
-
-static inline void cpufreq_update_this_cpu(struct rq *rq, unsigned int flags)
-{
-	if (cpu_of(rq) == smp_processor_id())
-		cpufreq_update_util(rq, flags);
-}
 #else
 static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
-static inline void cpufreq_update_this_cpu(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef arch_scale_freq_capacity

commit 8887cd99038bf242fb47f2d07fa0cf9371efa643
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Jun 21 14:22:02 2017 -0400

    sched/rt: Move RT related code from sched/core.c to sched/rt.c
    
    This helps making sched/core.c smaller and hopefully easier to understand and maintain.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170621182203.30626-3-nicolas.pitre@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d4eb3f67529d..eeef1a3086d1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -383,6 +383,11 @@ extern int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent
 extern void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,
 		struct sched_rt_entity *rt_se, int cpu,
 		struct sched_rt_entity *parent);
+extern int sched_group_set_rt_runtime(struct task_group *tg, long rt_runtime_us);
+extern int sched_group_set_rt_period(struct task_group *tg, u64 rt_period_us);
+extern long sched_group_rt_runtime(struct task_group *tg);
+extern long sched_group_rt_period(struct task_group *tg);
+extern int sched_rt_can_attach(struct task_group *tg, struct task_struct *tsk);
 
 extern struct task_group *sched_create_group(struct task_group *parent);
 extern void sched_online_group(struct task_group *tg,

commit 06a76fe08d4daaeea01ca0f175ad29f40c781ece
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Wed Jun 21 14:22:01 2017 -0400

    sched/deadline: Move DL related code from sched/core.c to sched/deadline.c
    
    This helps making sched/core.c smaller and hopefully easier to understand and maintain.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170621182203.30626-2-nicolas.pitre@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e0329d10bdb8..d4eb3f67529d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -218,9 +218,6 @@ static inline int dl_bandwidth_enabled(void)
 	return sysctl_sched_rt_runtime >= 0;
 }
 
-extern struct dl_bw *dl_bw_of(int i);
-extern int dl_bw_cpus(int i);
-
 struct dl_bw {
 	raw_spinlock_t lock;
 	u64 bw, total_bw;
@@ -251,6 +248,20 @@ bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
 
 void dl_change_utilization(struct task_struct *p, u64 new_bw);
 extern void init_dl_bw(struct dl_bw *dl_b);
+extern int sched_dl_global_validate(void);
+extern void sched_dl_do_global(void);
+extern int sched_dl_overflow(struct task_struct *p, int policy,
+			     const struct sched_attr *attr);
+extern void __setparam_dl(struct task_struct *p, const struct sched_attr *attr);
+extern void __getparam_dl(struct task_struct *p, struct sched_attr *attr);
+extern bool __checkparam_dl(const struct sched_attr *attr);
+extern void __dl_clear_params(struct task_struct *p);
+extern bool dl_param_changed(struct task_struct *p, const struct sched_attr *attr);
+extern int dl_task_can_attach(struct task_struct *p,
+			      const struct cpumask *cs_cpus_allowed);
+extern int dl_cpuset_cpumask_can_shrink(const struct cpumask *cur,
+					const struct cpumask *trial);
+extern bool dl_cpu_busy(unsigned int cpu);
 
 #ifdef CONFIG_CGROUP_SCHED
 

commit 6d3aed3d8a0573d0a6eb1160ccd0a0713f4dbc2f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jun 20 12:24:42 2017 +0200

    sched/debug: Fix SCHED_WARN_ON() to return a value on !CONFIG_SCHED_DEBUG as well
    
    This definition of SCHED_WARN_ON():
    
     #define SCHED_WARN_ON(x)        ((void)(x))
    
    is not fully compatible with the 'real' WARN_ON_ONCE() primitive, as it
    has no return value, so it cannot be used in conditionals.
    
    Fix it.
    
    Cc: Daniel Axtens <dja@axtens.net>
    Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f2ef759a4cb6..e0329d10bdb8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -39,9 +39,9 @@
 #include "cpuacct.h"
 
 #ifdef CONFIG_SCHED_DEBUG
-#define SCHED_WARN_ON(x)	WARN_ONCE(x, #x)
+# define SCHED_WARN_ON(x)	WARN_ONCE(x, #x)
 #else
-#define SCHED_WARN_ON(x)	((void)(x))
+# define SCHED_WARN_ON(x)	({ (void)(x), 0; })
 #endif
 
 struct rq;

commit f5832c1998af2ca8d9947792d1c8e1816ab58e57
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon May 29 17:02:57 2017 -0400

    sched/core: Omit building stop_sched_class when !SMP
    
    The stop class is invoked through stop_machine only.
    This is dead code on UP builds.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170529210302.26868-3-nicolas.pitre@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f1e400c6403c..f2ef759a4cb6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1453,7 +1453,11 @@ static inline void set_curr_task(struct rq *rq, struct task_struct *curr)
 	curr->sched_class->set_curr_task(rq);
 }
 
+#ifdef CONFIG_SMP
 #define sched_class_highest (&stop_sched_class)
+#else
+#define sched_class_highest (&dl_sched_class)
+#endif
 #define for_each_class(class) \
    for (class = sched_class_highest; class; class = class->next)
 

commit daec5798367012951cdb54fdb5c006e4379c9ae9
Author: Luca Abeni <luca.abeni@santannapisa.it>
Date:   Thu May 18 22:13:36 2017 +0200

    sched/deadline: Reclaim bandwidth not used by dl tasks
    
    This commit introduces a per-runqueue "extra utilization" that can be
    reclaimed by deadline tasks. In this way, the maximum fraction of CPU
    time that can reclaimed by deadline tasks is fixed (and configurable)
    and does not depend on the total deadline utilization.
    The GRUB accounting rule is modified to add this "extra utilization"
    to the inactive utilization of the runqueue, and to avoid reclaiming
    more than a maximum fraction of the CPU time.
    
    Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Claudio Scordino <claudio@evidence.eu.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1495138417-6203-10-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b7321dac03c1..f1e400c6403c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -219,22 +219,27 @@ static inline int dl_bandwidth_enabled(void)
 }
 
 extern struct dl_bw *dl_bw_of(int i);
+extern int dl_bw_cpus(int i);
 
 struct dl_bw {
 	raw_spinlock_t lock;
 	u64 bw, total_bw;
 };
 
+static inline void __dl_update(struct dl_bw *dl_b, s64 bw);
+
 static inline
-void __dl_clear(struct dl_bw *dl_b, u64 tsk_bw)
+void __dl_clear(struct dl_bw *dl_b, u64 tsk_bw, int cpus)
 {
 	dl_b->total_bw -= tsk_bw;
+	__dl_update(dl_b, (s32)tsk_bw / cpus);
 }
 
 static inline
-void __dl_add(struct dl_bw *dl_b, u64 tsk_bw)
+void __dl_add(struct dl_bw *dl_b, u64 tsk_bw, int cpus)
 {
 	dl_b->total_bw += tsk_bw;
+	__dl_update(dl_b, -((s32)tsk_bw / cpus));
 }
 
 static inline
@@ -576,6 +581,7 @@ struct dl_rq {
 	 * runqueue (inactive utilization = this_bw - running_bw).
 	 */
 	u64 this_bw;
+	u64 extra_bw;
 
 	/*
 	 * Inverse of the fraction of CPU utilization that can be reclaimed
@@ -1958,6 +1964,33 @@ extern void nohz_balance_exit_idle(unsigned int cpu);
 static inline void nohz_balance_exit_idle(unsigned int cpu) { }
 #endif
 
+
+#ifdef CONFIG_SMP
+static inline
+void __dl_update(struct dl_bw *dl_b, s64 bw)
+{
+	struct root_domain *rd = container_of(dl_b, struct root_domain, dl_bw);
+	int i;
+
+	RCU_LOCKDEP_WARN(!rcu_read_lock_sched_held(),
+			 "sched RCU must be held");
+	for_each_cpu_and(i, rd->span, cpu_active_mask) {
+		struct rq *rq = cpu_rq(i);
+
+		rq->dl.extra_bw += bw;
+	}
+}
+#else
+static inline
+void __dl_update(struct dl_bw *dl_b, s64 bw)
+{
+	struct dl_rq *dl = container_of(dl_b, struct dl_rq, dl_bw);
+
+	dl->extra_bw += bw;
+}
+#endif
+
+
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 struct irqtime {
 	u64			total;

commit 8fd27231c3302e0c7e1907df1252db97b65eb241
Author: Luca Abeni <luca.abeni@santannapisa.it>
Date:   Thu May 18 22:13:34 2017 +0200

    sched/deadline: Track the "total rq utilization" too
    
    The total rq utilization is defined as the sum of the utilisations of
    tasks that are "assigned" to a runqueue, independently from their state
    (TASK_RUNNING or blocked)
    
    Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Claudio Scordino <claudio@evidence.eu.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1495138417-6203-8-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 878fe757d6ad..b7321dac03c1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -566,6 +566,17 @@ struct dl_rq {
 	 */
 	u64 running_bw;
 
+	/*
+	 * Utilization of the tasks "assigned" to this runqueue (including
+	 * the tasks that are in runqueue and the tasks that executed on this
+	 * CPU and blocked). Increased when a task moves to this runqueue, and
+	 * decreased when the task moves away (migrates, changes scheduling
+	 * policy, or terminates).
+	 * This is needed to compute the "inactive utilization" for the
+	 * runqueue (inactive utilization = this_bw - running_bw).
+	 */
+	u64 this_bw;
+
 	/*
 	 * Inverse of the fraction of CPU utilization that can be reclaimed
 	 * by the GRUB algorithm.

commit 4da3abcefe178c650033f371e94fa10e80bce167
Author: Luca Abeni <luca.abeni@santannapisa.it>
Date:   Thu May 18 22:13:32 2017 +0200

    sched/deadline: Do not reclaim the whole CPU bandwidth
    
    Original GRUB tends to reclaim 100% of the CPU time... And this
    allows a CPU hog to starve non-deadline tasks.
    To address this issue, allow the scheduler to reclaim only a
    specified fraction of CPU time, stored in the new "bw_ratio"
    field of the dl runqueue structure.
    
    Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Claudio Scordino <claudio@evidence.eu.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1495138417-6203-6-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index bb409ef40120..878fe757d6ad 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -565,6 +565,12 @@ struct dl_rq {
 	 * task blocks
 	 */
 	u64 running_bw;
+
+	/*
+	 * Inverse of the fraction of CPU utilization that can be reclaimed
+	 * by the GRUB algorithm.
+	 */
+	u64 bw_ratio;
 };
 
 #ifdef CONFIG_SMP
@@ -1495,9 +1501,11 @@ extern struct dl_bandwidth def_dl_bandwidth;
 extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
 extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 extern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);
+extern void init_dl_rq_bw_ratio(struct dl_rq *dl_rq);
 
 #define BW_SHIFT	20
 #define BW_UNIT		(1 << BW_SHIFT)
+#define RATIO_SHIFT	8
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);

commit c52f14d384628db0217a7a9080ab800d5ffb2d72
Author: Luca Abeni <luca.abeni@santannapisa.it>
Date:   Thu May 18 22:13:31 2017 +0200

    sched/deadline: Implement GRUB accounting
    
    According to the GRUB (Greedy Reclaimation of Unused Bandwidth)
    reclaiming algorithm, the runtime is not decreased as "dq = -dt",
    but as "dq = -Uact dt" (where Uact is the per-runqueue active
    utilization).
    Hence, this commit modifies the runtime accounting rule in
    update_curr_dl() to implement the GRUB rule.
    
    Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Claudio Scordino <claudio@evidence.eu.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1495138417-6203-5-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c58f38905e0a..bb409ef40120 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1496,6 +1496,8 @@ extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime
 extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 extern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);
 
+#define BW_SHIFT	20
+#define BW_UNIT		(1 << BW_SHIFT)
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);

commit 209a0cbda7a01d2ea32a8b631d35e873bee498e9
Author: Luca Abeni <luca.abeni@santannapisa.it>
Date:   Thu May 18 22:13:29 2017 +0200

    sched/deadline: Improve the tracking of active utilization
    
    This patch implements a more theoretically sound algorithm for
    tracking active utilization: instead of decreasing it when a
    task blocks, use a timer (the "inactive timer", named after the
    "Inactive" task state of the GRUB algorithm) to decrease the
    active utilization at the so called "0-lag time".
    
    Tested-by: Claudio Scordino <claudio@evidence.eu.com>
    Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Luca Abeni <luca.abeni@santannapisa.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1495138417-6203-3-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ee26867da339..c58f38905e0a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -244,6 +244,7 @@ bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
 	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
 }
 
+void dl_change_utilization(struct task_struct *p, u64 new_bw);
 extern void init_dl_bw(struct dl_bw *dl_b);
 
 #ifdef CONFIG_CGROUP_SCHED
@@ -1493,6 +1494,7 @@ extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime
 extern struct dl_bandwidth def_dl_bandwidth;
 extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
 extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
+extern void init_dl_inactive_task_timer(struct sched_dl_entity *dl_se);
 
 unsigned long to_ratio(u64 period, u64 runtime);
 

commit e36d8677bfa55054e4194ec3683189b882a538f6
Author: Luca Abeni <luca.abeni@unitn.it>
Date:   Thu May 18 22:13:28 2017 +0200

    sched/deadline: Track the active utilization
    
    Active utilization is defined as the total utilization of active
    (TASK_RUNNING) tasks queued on a runqueue. Hence, it is increased
    when a task wakes up and is decreased when a task blocks.
    
    When a task is migrated from CPUi to CPUj, immediately subtract the
    task's utilization from CPUi and add it to CPUj. This mechanism is
    implemented by modifying the pull and push functions.
    Note: this is not fully correct from the theoretical point of view
    (the utilization should be removed from CPUi only at the 0 lag
    time), a more theoretically sound solution is presented in the
    next patches.
    
    Tested-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Signed-off-by: Luca Abeni <luca.abeni@unitn.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Juri Lelli <juri.lelli@arm.com>
    Cc: Claudio Scordino <claudio@evidence.eu.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Poirier <mathieu.poirier@linaro.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Link: http://lkml.kernel.org/r/1495138417-6203-2-git-send-email-luca.abeni@santannapisa.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f8cf1d87f065..ee26867da339 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -558,6 +558,12 @@ struct dl_rq {
 #else
 	struct dl_bw dl_bw;
 #endif
+	/*
+	 * "Active utilization" for this runqueue: increased when a
+	 * task wakes up (becomes TASK_RUNNING) and decreased when a
+	 * task blocks
+	 */
+	u64 running_bw;
 };
 
 #ifdef CONFIG_SMP

commit ae4df9d6c935105857d9d166b615e3f17531ce6b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 1 11:03:12 2017 +0200

    sched/topology: Rename sched_group_cpus()
    
    There's a discrepancy in naming between the sched_domain and
    sched_group cpumask accessor. Since we're doing changes, fix it.
    
      $ git grep sched_group_cpus | wc -l
      28
      $ git grep sched_domain_span | wc -l
      38
    
    Suggests changing sched_group_cpus() into sched_group_span():
    
      for i  in `git grep -l sched_group_cpus`
      do
        sed -ie 's/sched_group_cpus/sched_group_span/g' $i
      done
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f7c70575ae34..f8cf1d87f065 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1048,7 +1048,7 @@ struct sched_group {
 	unsigned long cpumask[0];
 };
 
-static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
+static inline struct cpumask *sched_group_span(struct sched_group *sg)
 {
 	return to_cpumask(sg->cpumask);
 }
@@ -1067,7 +1067,7 @@ static inline struct cpumask *group_balance_mask(struct sched_group *sg)
  */
 static inline unsigned int group_first_cpu(struct sched_group *group)
 {
-	return cpumask_first(sched_group_cpus(group));
+	return cpumask_first(sched_group_span(group));
 }
 
 extern int group_balance_cpu(struct sched_group *sg);

commit e5c14b1fb89213ff718261e6fb1bb29c5ffbbe99
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 1 10:47:02 2017 +0200

    sched/topology: Rename sched_group_mask()
    
    Since sched_group_mask() is now an independent cpumask (it no longer
    masks sched_group_cpus()), rename the thing.
    
    Suggested-by: Lauro Ramos Venancio <lvenanci@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4312b2adfb02..f7c70575ae34 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1027,7 +1027,7 @@ struct sched_group_capacity {
 	int id;
 #endif
 
-	unsigned long cpumask[0]; /* iteration mask */
+	unsigned long cpumask[0]; /* balance mask */
 };
 
 struct sched_group {
@@ -1054,10 +1054,9 @@ static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
 }
 
 /*
- * cpumask masking which cpus in the group are allowed to iterate up the domain
- * tree.
+ * See build_balance_mask().
  */
-static inline struct cpumask *sched_group_mask(struct sched_group *sg)
+static inline struct cpumask *group_balance_mask(struct sched_group *sg)
 {
 	return to_cpumask(sg->sgc->cpumask);
 }

commit 005f874dd2843116e2ea079e3679f4f318f12fee
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 26 17:35:35 2017 +0200

    sched/topology: Add sched_group_capacity debugging
    
    Add sgc::id to easier spot domain construction issues.
    
    Take the opportunity to slightly rework the group printing, because
    adding more "(id: %d)" strings makes the entire thing very hard to
    read. Also the individual groups are very hard to separate, so add
    explicit visual grouping, which allows replacing all the "(%s: %d)"
    format things with shorter "%s=%d" variants.
    
    Then fix up some inconsistencies in surrounding prints for domains.
    
    The end result looks like:
    
      [] CPU0 attaching sched-domain(s):
      []  domain-0: span=0,4 level=DIE
      []   groups: 0:{ span=0 }, 4:{ span=4 }
      []   domain-1: span=0-1,3-5,7 level=NUMA
      []    groups: 0:{ span=0,4 mask=0,4 cap=2048 }, 1:{ span=1,5 mask=1,5 cap=2048 }, 3:{ span=3,7 mask=3,7 cap=2048 }
      []    domain-2: span=0-7 level=NUMA
      []     groups: 0:{ span=0-1,3-5,7 mask=0,4 cap=6144 }, 2:{ span=1-3,5-7 mask=2,6 cap=6144 }
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6e1eae717a24..4312b2adfb02 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1023,6 +1023,10 @@ struct sched_group_capacity {
 	unsigned long next_update;
 	int imbalance; /* XXX unrelated to capacity but shared group state */
 
+#ifdef CONFIG_SCHED_DEBUG
+	int id;
+#endif
+
 	unsigned long cpumask[0]; /* iteration mask */
 };
 

commit 8d5dc5126bb2bbcebf0b1e061cca2fc02c935620
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Apr 25 15:29:40 2017 +0200

    sched/topology: Small cleanup
    
    Move the allocation of topology specific cpumasks into the topology
    code.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6dda2aab731e..6e1eae717a24 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -606,11 +606,9 @@ struct root_domain {
 
 extern struct root_domain def_root_domain;
 extern struct mutex sched_domains_mutex;
-extern cpumask_var_t fallback_doms;
-extern cpumask_var_t sched_domains_tmpmask;
 
 extern void init_defrootdomain(void);
-extern int init_sched_domains(const struct cpumask *cpu_map);
+extern int sched_init_domains(const struct cpumask *cpu_map);
 extern void rq_attach_root(struct rq *rq, struct root_domain *rd);
 
 #endif /* CONFIG_SMP */

commit 8663effb24f9430394d3bf1ed2dac42a771421d1
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Apr 14 08:48:09 2017 -0400

    sched/core: Call __schedule() from do_idle() without enabling preemption
    
    I finally got around to creating trampolines for dynamically allocated
    ftrace_ops with using synchronize_rcu_tasks(). For users of the ftrace
    function hook callbacks, like perf, that allocate the ftrace_ops
    descriptor via kmalloc() and friends, ftrace was not able to optimize
    the functions being traced to use a trampoline because they would also
    need to be allocated dynamically. The problem is that they cannot be
    freed when CONFIG_PREEMPT is set, as there's no way to tell if a task
    was preempted on the trampoline. That was before Paul McKenney
    implemented synchronize_rcu_tasks() that would make sure all tasks
    (except idle) have scheduled out or have entered user space.
    
    While testing this, I triggered this bug:
    
     BUG: unable to handle kernel paging request at ffffffffa0230077
     ...
     RIP: 0010:0xffffffffa0230077
     ...
     Call Trace:
      schedule+0x5/0xe0
      schedule_preempt_disabled+0x18/0x30
      do_idle+0x172/0x220
    
    What happened was that the idle task was preempted on the trampoline.
    As synchronize_rcu_tasks() ignores the idle thread, there's nothing
    that lets ftrace know that the idle task was preempted on a trampoline.
    
    The idle task shouldn't need to ever enable preemption. The idle task
    is simply a loop that calls schedule or places the cpu into idle mode.
    In fact, having preemption enabled is inefficient, because it can
    happen when idle is just about to call schedule anyway, which would
    cause schedule to be called twice. Once for when the interrupt came in
    and was returning back to normal context, and then again in the normal
    path that the idle loop is running in, which would be pointless, as it
    had already scheduled.
    
    The only reason schedule_preempt_disable() enables preemption is to be
    able to call sched_submit_work(), which requires preemption enabled. As
    this is a nop when the task is in the RUNNING state, and idle is always
    in the running state, there's no reason that idle needs to enable
    preemption. But that means it cannot use schedule_preempt_disable() as
    other callers of that function require calling sched_submit_work().
    
    Adding a new function local to kernel/sched/ that allows idle to call
    the scheduler without enabling preemption, fixes the
    synchronize_rcu_tasks() issue, as well as removes the pointless spurious
    schedule calls caused by interrupts happening in the brief window where
    preemption is enabled just before it calls schedule.
    
    Reviewed: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170414084809.3dacde2a@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7808ab050599..6dda2aab731e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1467,6 +1467,8 @@ static inline struct cpuidle_state *idle_get_state(struct rq *rq)
 }
 #endif
 
+extern void schedule_idle(void);
+
 extern void sysrq_sched_debug_show(void);
 extern void sched_init_granularity(void);
 extern void update_max_interval(void);

commit 3527d3e9514f013f361fba29fd71858d9361049d
Merge: 3711c94fd659 21173d0b4d2a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 1 19:12:53 2017 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - another round of rq-clock handling debugging, robustization and
         fixes
    
       - PELT accounting improvements
    
       - CPU hotplug related ->cpus_allowed affinity handling fixes all
         around the tree
    
       - ... plus misc fixes, cleanups and updates"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (35 commits)
      sched/x86: Update reschedule warning text
      crypto: N2 - Replace racy task affinity logic
      cpufreq/sparc-us2e: Replace racy task affinity logic
      cpufreq/sparc-us3: Replace racy task affinity logic
      cpufreq/sh: Replace racy task affinity logic
      cpufreq/ia64: Replace racy task affinity logic
      ACPI/processor: Replace racy task affinity logic
      ACPI/processor: Fix error handling in __acpi_processor_start()
      sparc/sysfs: Replace racy task affinity logic
      powerpc/smp: Replace open coded task affinity logic
      ia64/sn/hwperf: Replace racy task affinity logic
      ia64/salinfo: Replace racy task affinity logic
      workqueue: Provide work_on_cpu_safe()
      ia64/topology: Remove cpus_allowed manipulation
      sched/fair: Move the PELT constants into a generated header
      sched/fair: Increase PELT accuracy for small tasks
      sched/fair: Fix comments
      sched/Documentation: Add 'sched-pelt' tool
      sched/fair: Fix corner case in __accumulate_sum()
      sched/core: Remove 'task' parameter and rename tsk_restore_flags() to current_restore_flags()
      ...

commit 25e2d8c1b9e327ed260edd13169cc22bc7a78bc6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Apr 25 16:10:48 2017 +0200

    sched/cputime: Fix ksoftirqd cputime accounting regression
    
    irq_time_read() returns the irqtime minus the ksoftirqd time. This
    is necessary because irq_time_read() is used to substract the IRQ time
    from the sum_exec_runtime of a task. If we were to include the softirq
    time of ksoftirqd, this task would substract its own CPU time everytime
    it updates ksoftirqd->sum_exec_runtime which would therefore never
    progress.
    
    But this behaviour got broken by:
    
      a499a5a14db ("sched/cputime: Increment kcpustat directly on irqtime account")
    
    ... which now includes ksoftirqd softirq time in the time returned by
    irq_time_read().
    
    This has resulted in wrong ksoftirqd cputime reported to userspace
    through /proc/stat and thus "top" not showing ksoftirqd when it should
    after intense networking load.
    
    ksoftirqd->stime happens to be correct but it gets scaled down by
    sum_exec_runtime through task_cputime_adjusted().
    
    To fix this, just account the strict IRQ time in a separate counter and
    use it to report the IRQ time.
    
    Reported-and-tested-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1493129448-5356-1-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5cbf92214ad8..767aab3505a8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1869,6 +1869,7 @@ static inline void nohz_balance_exit_idle(unsigned int cpu) { }
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 struct irqtime {
+	u64			total;
 	u64			tick_delta;
 	u64			irq_start_time;
 	struct u64_stats_sync	sync;
@@ -1876,16 +1877,20 @@ struct irqtime {
 
 DECLARE_PER_CPU(struct irqtime, cpu_irqtime);
 
+/*
+ * Returns the irqtime minus the softirq time computed by ksoftirqd.
+ * Otherwise ksoftirqd's sum_exec_runtime is substracted its own runtime
+ * and never move forward.
+ */
 static inline u64 irq_time_read(int cpu)
 {
 	struct irqtime *irqtime = &per_cpu(cpu_irqtime, cpu);
-	u64 *cpustat = kcpustat_cpu(cpu).cpustat;
 	unsigned int seq;
 	u64 total;
 
 	do {
 		seq = __u64_stats_fetch_begin(&irqtime->sync);
-		total = cpustat[CPUTIME_SOFTIRQ] + cpustat[CPUTIME_IRQ];
+		total = irqtime->total;
 	} while (__u64_stats_fetch_retry(&irqtime->sync, seq));
 
 	return total;

commit 0a67d1ee30ef1efe6a412b3590e08734902aed43
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 4 16:29:45 2016 +0200

    sched/core: Add {EN,DE}QUEUE_NOCLOCK flags
    
    Currently {en,de}queue_task() do an unconditional update_rq_clock().
    However since we want to avoid duplicate updates, so that each
    rq->lock section appears atomic in time, we need to be able to skip
    these clock updates.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7d4f69329634..de4b934ba974 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1331,15 +1331,17 @@ extern const u32 sched_prio_to_wmult[40];
 #define DEQUEUE_SLEEP		0x01
 #define DEQUEUE_SAVE		0x02 /* matches ENQUEUE_RESTORE */
 #define DEQUEUE_MOVE		0x04 /* matches ENQUEUE_MOVE */
+#define DEQUEUE_NOCLOCK		0x08 /* matches ENQUEUE_NOCLOCK */
 
 #define ENQUEUE_WAKEUP		0x01
 #define ENQUEUE_RESTORE		0x02
 #define ENQUEUE_MOVE		0x04
+#define ENQUEUE_NOCLOCK		0x08
 
-#define ENQUEUE_HEAD		0x08
-#define ENQUEUE_REPLENISH	0x10
+#define ENQUEUE_HEAD		0x10
+#define ENQUEUE_REPLENISH	0x20
 #ifdef CONFIG_SMP
-#define ENQUEUE_MIGRATED	0x20
+#define ENQUEUE_MIGRATED	0x40
 #else
 #define ENQUEUE_MIGRATED	0x00
 #endif

commit 8a8c69c32778865affcedc2111bb5d938b50516f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 4 16:04:35 2016 +0200

    sched/core: Add rq->lock wrappers
    
    The missing update_rq_clock() check can work with partial rq->lock
    wrappery, since a missing wrapper can cause the warning to not be
    emitted when it should have, but cannot cause the warning to trigger
    when it should not have.
    
    The duplicate update_rq_clock() check however can cause false warnings
    to trigger. Therefore add more comprehensive rq->lock wrappery.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5cbf92214ad8..7d4f69329634 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1624,6 +1624,7 @@ static inline void sched_avg_update(struct rq *rq) { }
 
 struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 	__acquires(rq->lock);
+
 struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 	__acquires(p->pi_lock)
 	__acquires(rq->lock);
@@ -1645,6 +1646,62 @@ task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
 	raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
 }
 
+static inline void
+rq_lock_irqsave(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock_irqsave(&rq->lock, rf->flags);
+	rq_pin_lock(rq, rf);
+}
+
+static inline void
+rq_lock_irq(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock_irq(&rq->lock);
+	rq_pin_lock(rq, rf);
+}
+
+static inline void
+rq_lock(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock(&rq->lock);
+	rq_pin_lock(rq, rf);
+}
+
+static inline void
+rq_relock(struct rq *rq, struct rq_flags *rf)
+	__acquires(rq->lock)
+{
+	raw_spin_lock(&rq->lock);
+	rq_repin_lock(rq, rf);
+}
+
+static inline void
+rq_unlock_irqrestore(struct rq *rq, struct rq_flags *rf)
+	__releases(rq->lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock_irqrestore(&rq->lock, rf->flags);
+}
+
+static inline void
+rq_unlock_irq(struct rq *rq, struct rq_flags *rf)
+	__releases(rq->lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock_irq(&rq->lock);
+}
+
+static inline void
+rq_unlock(struct rq *rq, struct rq_flags *rf)
+	__releases(rq->lock)
+{
+	rq_unpin_lock(rq, rf);
+	raw_spin_unlock(&rq->lock);
+}
+
 #ifdef CONFIG_SMP
 #ifdef CONFIG_PREEMPT
 

commit 1777e4635507265ba53d8dc4cd248e7d7c306fa0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 14:47:12 2017 +0100

    sched/headers: Prepare to move _init() prototypes from <linux/sched.h> to <linux/sched/init.h>
    
    But first introduce a trivial header and update usage sites.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 76e3af3f39f4..5cbf92214ad8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -18,6 +18,7 @@
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
 #include <linux/sched/cputime.h>
+#include <linux/sched/init.h>
 
 #include <linux/u64_stats_sync.h>
 #include <linux/kernel_stat.h>

commit 32ef5517c298042ed58408545f475df43afe1f24
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Feb 5 11:48:36 2017 +0100

    sched/headers: Prepare to move cputime functionality from <linux/sched.h> into <linux/sched/cputime.h>
    
    Introduce a trivial, mostly empty <linux/sched/cputime.h> header
    to prepare for the moving of cputime functionality out of sched.h.
    
    Update all code that relies on these facilities.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b1f1c8443837..76e3af3f39f4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -17,6 +17,7 @@
 #include <linux/sched/hotplug.h>
 #include <linux/sched/task.h>
 #include <linux/sched/task_stack.h>
+#include <linux/sched/cputime.h>
 
 #include <linux/u64_stats_sync.h>
 #include <linux/kernel_stat.h>

commit dfc3401a33086a3fd465468e171ea0e82430569b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Feb 3 23:15:21 2017 +0100

    sched/headers: Prepare to move the 'root_task_group' declaration to <linux/sched/autogroup.h>
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4e2fec3f12a0..b1f1c8443837 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1,5 +1,6 @@
 
 #include <linux/sched.h>
+#include <linux/sched/autogroup.h>
 #include <linux/sched/sysctl.h>
 #include <linux/sched/topology.h>
 #include <linux/sched/rt.h>

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6b7155ae5c33..4e2fec3f12a0 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -15,6 +15,7 @@
 #include <linux/sched/debug.h>
 #include <linux/sched/hotplug.h>
 #include <linux/sched/task.h>
+#include <linux/sched/task_stack.h>
 
 #include <linux/u64_stats_sync.h>
 #include <linux/kernel_stat.h>

commit 299300258d1bc4e997b7db340a2e06636757fe2e
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task.h>
    
    We are going to split <linux/sched/task.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0974eb2ef50d..6b7155ae5c33 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -14,6 +14,7 @@
 #include <linux/sched/nohz.h>
 #include <linux/sched/debug.h>
 #include <linux/sched/hotplug.h>
+#include <linux/sched/task.h>
 
 #include <linux/u64_stats_sync.h>
 #include <linux/kernel_stat.h>

commit ef8bd77f332bb0a4e467d7171bbfc6c57aa08a88
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:36 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/hotplug.h>
    
    We are going to split <linux/sched/hotplug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/hotplug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e1e819f731b2..0974eb2ef50d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -3,6 +3,7 @@
 #include <linux/sched/sysctl.h>
 #include <linux/sched/topology.h>
 #include <linux/sched/rt.h>
+#include <linux/sched/deadline.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/wake_q.h>
 #include <linux/sched/signal.h>
@@ -12,8 +13,9 @@
 #include <linux/sched/stat.h>
 #include <linux/sched/nohz.h>
 #include <linux/sched/debug.h>
+#include <linux/sched/hotplug.h>
+
 #include <linux/u64_stats_sync.h>
-#include <linux/sched/deadline.h>
 #include <linux/kernel_stat.h>
 #include <linux/binfmts.h>
 #include <linux/mutex.h>

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5979f47c422c..e1e819f731b2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -11,6 +11,7 @@
 #include <linux/sched/cpufreq.h>
 #include <linux/sched/stat.h>
 #include <linux/sched/nohz.h>
+#include <linux/sched/debug.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
 #include <linux/kernel_stat.h>
@@ -1830,7 +1831,6 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void print_dl_stats(struct seq_file *m, int cpu);
 extern void
 print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
-
 #ifdef CONFIG_NUMA_BALANCING
 extern void
 show_numa_stats(struct task_struct *p, struct seq_file *m);

commit 370c91355c76cbcaad8ff79b4bb15a7f2ea59433
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/nohz.h>
    
    We are going to split <linux/sched/nohz.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/nohz.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6c8a1db44dde..5979f47c422c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -10,6 +10,7 @@
 #include <linux/sched/mm.h>
 #include <linux/sched/cpufreq.h>
 #include <linux/sched/stat.h>
+#include <linux/sched/nohz.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
 #include <linux/kernel_stat.h>

commit 03441a3482a31462c93509939a388877e3cd9261
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/stat.h>
    
    We are going to split <linux/sched/stat.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/stat.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 04f376cf7ba9..6c8a1db44dde 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -9,6 +9,7 @@
 #include <linux/sched/numa_balancing.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/cpufreq.h>
+#include <linux/sched/stat.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
 #include <linux/kernel_stat.h>

commit 7fce777cd4eacc0bdcb33017e5a4c495d28afed1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 14:47:27 2017 +0100

    sched/headers: Prepare header dependency changes, move the <asm/paravirt.h> include to kernel/sched/sched.h
    
    Recent header reorganizations unearthed this hidden dependency:
    
      kernel/sched/core.c:199:25: error: 'paravirt_steal_rq_enabled' undeclared (first use in this function)
      kernel/sched/core.c:200:11: error: implicit declaration of function 'paravirt_steal_clock' [-Werror=implicit-function-declaration]
    
    So move the asm/paravirt.h include from kernel/sched/cpuclock.c to kernel/sched/sched.h.
    
    ( NOTE: We do this change before doing the changes that introduce the build failure,
            so the series remains fully bisectable. )
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7e8ce0347fbf..04f376cf7ba9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -20,6 +20,10 @@
 #include <linux/tick.h>
 #include <linux/slab.h>
 
+#ifdef CONFIG_PARAVIRT
+#include <asm/paravirt.h>
+#endif
+
 #include "cpupri.h"
 #include "cpudeadline.h"
 #include "cpuacct.h"

commit 6a3827d7509cbf96b7e961f8957c1f01d1bcf894
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:31 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/numa_balancing.h>
    
    We are going to split <linux/sched/numa_balancing.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/numa_balancing.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7dfeb14fa43c..7e8ce0347fbf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -6,6 +6,7 @@
 #include <linux/sched/clock.h>
 #include <linux/sched/wake_q.h>
 #include <linux/sched/signal.h>
+#include <linux/sched/numa_balancing.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/cpufreq.h>
 #include <linux/u64_stats_sync.h>

commit 55687da166bf51129ed6b110d7711f4c7560abe2
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:31 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/cpufreq.h>
    
    We are going to split <linux/sched/cpufreq.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/cpufreq.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 641249471952..7dfeb14fa43c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -7,6 +7,7 @@
 #include <linux/sched/wake_q.h>
 #include <linux/sched/signal.h>
 #include <linux/sched/mm.h>
+#include <linux/sched/cpufreq.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
 #include <linux/kernel_stat.h>

commit 3f07c0144132e4f59d88055ac8ff3e691a5fa2b8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/signal.h>
    
    We are going to split <linux/sched/signal.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/signal.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e2307a6c29f1..641249471952 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -5,6 +5,7 @@
 #include <linux/sched/rt.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/wake_q.h>
+#include <linux/sched/signal.h>
 #include <linux/sched/mm.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>

commit 6e84f31522f931027bf695752087ece278c10d3f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:29 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/mm.h>
    
    We are going to split <linux/sched/mm.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/mm.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    The APIs that are going to be moved first are:
    
       mm_alloc()
       __mmdrop()
       mmdrop()
       mmdrop_async_fn()
       mmdrop_async()
       mmget_not_zero()
       mmput()
       mmput_async()
       get_task_mm()
       mm_access()
       mm_release()
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4d386461f13a..e2307a6c29f1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -5,6 +5,7 @@
 #include <linux/sched/rt.h>
 #include <linux/sched/clock.h>
 #include <linux/sched/wake_q.h>
+#include <linux/sched/mm.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
 #include <linux/kernel_stat.h>

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 683570739f46..4d386461f13a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -3,6 +3,7 @@
 #include <linux/sched/sysctl.h>
 #include <linux/sched/topology.h>
 #include <linux/sched/rt.h>
+#include <linux/sched/clock.h>
 #include <linux/sched/wake_q.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>

commit 84f001e15737f8214b0f5f0f7dfec0fb1027938f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/wake_q.h>
    
    We are going to split <linux/sched/wake_q.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/wake_q.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9b9cb260d9cf..683570739f46 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -3,6 +3,7 @@
 #include <linux/sched/sysctl.h>
 #include <linux/sched/topology.h>
 #include <linux/sched/rt.h>
+#include <linux/sched/wake_q.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
 #include <linux/kernel_stat.h>

commit 105ab3d8ce7269887d24d224054677125e18037c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/topology.h>
    
    We are going to split <linux/sched/topology.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/topology.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 71b10a9b73cf..9b9cb260d9cf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1,6 +1,7 @@
 
 #include <linux/sched.h>
 #include <linux/sched/sysctl.h>
+#include <linux/sched/topology.h>
 #include <linux/sched/rt.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>

commit 1051408f7ecdcd1baf86f5dd5fdc44740be3b23d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:42:41 2017 +0100

    sched/autogroup: Rename auto_group.[ch] to autogroup.[ch]
    
    The names are all 'autogroup', not 'auto_group' - so rename
    the kernel/sched/auto_group.[ch] to match the existing
    nomenclature.
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 17ed94b9b413..71b10a9b73cf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1069,7 +1069,7 @@ static inline void sched_ttwu_pending(void) { }
 #endif /* CONFIG_SMP */
 
 #include "stats.h"
-#include "auto_group.h"
+#include "autogroup.h"
 
 #ifdef CONFIG_CGROUP_SCHED
 

commit f2cb13609d5397cdd747f3ed6fb651233851717d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 13:10:18 2017 +0100

    sched/topology: Split out scheduler topology code from core.c into topology.c
    
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 8ff5cc539e8a..17ed94b9b413 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -223,7 +223,7 @@ bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
 	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
 }
 
-extern struct mutex sched_domains_mutex;
+extern void init_dl_bw(struct dl_bw *dl_b);
 
 #ifdef CONFIG_CGROUP_SCHED
 
@@ -584,6 +584,13 @@ struct root_domain {
 };
 
 extern struct root_domain def_root_domain;
+extern struct mutex sched_domains_mutex;
+extern cpumask_var_t fallback_doms;
+extern cpumask_var_t sched_domains_tmpmask;
+
+extern void init_defrootdomain(void);
+extern int init_sched_domains(const struct cpumask *cpu_map);
+extern void rq_attach_root(struct rq *rq, struct root_domain *rd);
 
 #endif /* CONFIG_SMP */
 
@@ -886,6 +893,16 @@ extern int sched_max_numa_distance;
 extern bool find_numa_distance(int distance);
 #endif
 
+#ifdef CONFIG_NUMA
+extern void sched_init_numa(void);
+extern void sched_domains_numa_masks_set(unsigned int cpu);
+extern void sched_domains_numa_masks_clear(unsigned int cpu);
+#else
+static inline void sched_init_numa(void) { }
+static inline void sched_domains_numa_masks_set(unsigned int cpu) { }
+static inline void sched_domains_numa_masks_clear(unsigned int cpu) { }
+#endif
+
 #ifdef CONFIG_NUMA_BALANCING
 /* The regions in numa_faults array from task_struct */
 enum numa_faults_stats {
@@ -1752,6 +1769,10 @@ static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
 		__release(rq2->lock);
 }
 
+extern void set_rq_online (struct rq *rq);
+extern void set_rq_offline(struct rq *rq);
+extern bool sched_smp_initialized;
+
 #else /* CONFIG_SMP */
 
 /*

commit a499a5a14dbd1d0315a96fc62a8798059325e9e6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jan 31 04:09:32 2017 +0100

    sched/cputime: Increment kcpustat directly on irqtime account
    
    The irqtime is accounted is nsecs and stored in
    cpu_irq_time.hardirq_time and cpu_irq_time.softirq_time. Once the
    accumulated amount reaches a new jiffy, this one gets accounted to the
    kcpustat.
    
    This was necessary when kcpustat was stored in cputime_t, which could at
    worst have jiffies granularity. But now kcpustat is stored in nsecs
    so this whole discretization game with temporary irqtime storage has
    become unnecessary.
    
    We can now directly account the irqtime to the kcpustat.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Stanislaw Gruszka <sgruszka@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1485832191-26889-17-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6eeae7ebd99b..8ff5cc539e8a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -4,6 +4,7 @@
 #include <linux/sched/rt.h>
 #include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
+#include <linux/kernel_stat.h>
 #include <linux/binfmts.h>
 #include <linux/mutex.h>
 #include <linux/spinlock.h>
@@ -1827,8 +1828,7 @@ static inline void nohz_balance_exit_idle(unsigned int cpu) { }
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
 struct irqtime {
-	u64			hardirq_time;
-	u64			softirq_time;
+	u64			tick_delta;
 	u64			irq_start_time;
 	struct u64_stats_sync	sync;
 };
@@ -1838,12 +1838,13 @@ DECLARE_PER_CPU(struct irqtime, cpu_irqtime);
 static inline u64 irq_time_read(int cpu)
 {
 	struct irqtime *irqtime = &per_cpu(cpu_irqtime, cpu);
+	u64 *cpustat = kcpustat_cpu(cpu).cpustat;
 	unsigned int seq;
 	u64 total;
 
 	do {
 		seq = __u64_stats_fetch_begin(&irqtime->sync);
-		total = irqtime->softirq_time + irqtime->hardirq_time;
+		total = cpustat[CPUTIME_SOFTIRQ] + cpustat[CPUTIME_IRQ];
 	} while (__u64_stats_fetch_retry(&irqtime->sync, seq));
 
 	return total;

commit cb42c9a3ebbbb23448c3f9a25417fae6309b1a92
Author: Matt Fleming <matt@codeblueprint.co.uk>
Date:   Wed Sep 21 14:38:13 2016 +0100

    sched/core: Add debugging code to catch missing update_rq_clock() calls
    
    There's no diagnostic checks for figuring out when we've accidentally
    missed update_rq_clock() calls. Let's add some by piggybacking on the
    rq_*pin_lock() wrappers.
    
    The idea behind the diagnostic checks is that upon pining rq lock the
    rq clock should be updated, via update_rq_clock(), before anybody
    reads the clock with rq_clock() or rq_clock_task().
    
    The exception to this rule is when updates have explicitly been
    disabled with the rq_clock_skip_update() optimisation.
    
    There are some functions that only unpin the rq lock in order to grab
    some other lock and avoid deadlock. In that case we don't need to
    update the clock again and the previous diagnostic state can be
    carried over in rq_repin_lock() by saving the state in the rq_flags
    context.
    
    Since this patch adds a new clock update flag and some already exist
    in rq::clock_skip_update, that field has now been renamed. An attempt
    has been made to keep the flag manipulation code small and fast since
    it's used in the heart of the __schedule() fast path.
    
    For the !CONFIG_SCHED_DEBUG case the only object code change (other
    than addresses) is the following change to reset RQCF_ACT_SKIP inside
    of __schedule(),
    
      -       c7 83 38 09 00 00 00    movl   $0x0,0x938(%rbx)
      -       00 00 00
      +       83 a3 38 09 00 00 fc    andl   $0xfffffffc,0x938(%rbx)
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@unitn.it>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Cc: Yuyang Du <yuyang.du@intel.com>
    Link: http://lkml.kernel.org/r/20160921133813.31976-8-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 98e7eee07237..6eeae7ebd99b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -644,7 +644,7 @@ struct rq {
 	unsigned long next_balance;
 	struct mm_struct *prev_mm;
 
-	unsigned int clock_skip_update;
+	unsigned int clock_update_flags;
 	u64 clock;
 	u64 clock_task;
 
@@ -768,48 +768,110 @@ static inline u64 __rq_clock_broken(struct rq *rq)
 	return READ_ONCE(rq->clock);
 }
 
+/*
+ * rq::clock_update_flags bits
+ *
+ * %RQCF_REQ_SKIP - will request skipping of clock update on the next
+ *  call to __schedule(). This is an optimisation to avoid
+ *  neighbouring rq clock updates.
+ *
+ * %RQCF_ACT_SKIP - is set from inside of __schedule() when skipping is
+ *  in effect and calls to update_rq_clock() are being ignored.
+ *
+ * %RQCF_UPDATED - is a debug flag that indicates whether a call has been
+ *  made to update_rq_clock() since the last time rq::lock was pinned.
+ *
+ * If inside of __schedule(), clock_update_flags will have been
+ * shifted left (a left shift is a cheap operation for the fast path
+ * to promote %RQCF_REQ_SKIP to %RQCF_ACT_SKIP), so you must use,
+ *
+ *	if (rq-clock_update_flags >= RQCF_UPDATED)
+ *
+ * to check if %RQCF_UPADTED is set. It'll never be shifted more than
+ * one position though, because the next rq_unpin_lock() will shift it
+ * back.
+ */
+#define RQCF_REQ_SKIP	0x01
+#define RQCF_ACT_SKIP	0x02
+#define RQCF_UPDATED	0x04
+
+static inline void assert_clock_updated(struct rq *rq)
+{
+	/*
+	 * The only reason for not seeing a clock update since the
+	 * last rq_pin_lock() is if we're currently skipping updates.
+	 */
+	SCHED_WARN_ON(rq->clock_update_flags < RQCF_ACT_SKIP);
+}
+
 static inline u64 rq_clock(struct rq *rq)
 {
 	lockdep_assert_held(&rq->lock);
+	assert_clock_updated(rq);
+
 	return rq->clock;
 }
 
 static inline u64 rq_clock_task(struct rq *rq)
 {
 	lockdep_assert_held(&rq->lock);
+	assert_clock_updated(rq);
+
 	return rq->clock_task;
 }
 
-#define RQCF_REQ_SKIP	0x01
-#define RQCF_ACT_SKIP	0x02
-
 static inline void rq_clock_skip_update(struct rq *rq, bool skip)
 {
 	lockdep_assert_held(&rq->lock);
 	if (skip)
-		rq->clock_skip_update |= RQCF_REQ_SKIP;
+		rq->clock_update_flags |= RQCF_REQ_SKIP;
 	else
-		rq->clock_skip_update &= ~RQCF_REQ_SKIP;
+		rq->clock_update_flags &= ~RQCF_REQ_SKIP;
 }
 
 struct rq_flags {
 	unsigned long flags;
 	struct pin_cookie cookie;
+#ifdef CONFIG_SCHED_DEBUG
+	/*
+	 * A copy of (rq::clock_update_flags & RQCF_UPDATED) for the
+	 * current pin context is stashed here in case it needs to be
+	 * restored in rq_repin_lock().
+	 */
+	unsigned int clock_update_flags;
+#endif
 };
 
 static inline void rq_pin_lock(struct rq *rq, struct rq_flags *rf)
 {
 	rf->cookie = lockdep_pin_lock(&rq->lock);
+
+#ifdef CONFIG_SCHED_DEBUG
+	rq->clock_update_flags &= (RQCF_REQ_SKIP|RQCF_ACT_SKIP);
+	rf->clock_update_flags = 0;
+#endif
 }
 
 static inline void rq_unpin_lock(struct rq *rq, struct rq_flags *rf)
 {
+#ifdef CONFIG_SCHED_DEBUG
+	if (rq->clock_update_flags > RQCF_ACT_SKIP)
+		rf->clock_update_flags = RQCF_UPDATED;
+#endif
+
 	lockdep_unpin_lock(&rq->lock, rf->cookie);
 }
 
 static inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)
 {
 	lockdep_repin_lock(&rq->lock, rf->cookie);
+
+#ifdef CONFIG_SCHED_DEBUG
+	/*
+	 * Restore the value we stashed in @rf for this pin context.
+	 */
+	rq->clock_update_flags |= rf->clock_update_flags;
+#endif
 }
 
 #ifdef CONFIG_NUMA

commit d8ac897137a230ec351269f6378017f2decca512
Author: Matt Fleming <matt@codeblueprint.co.uk>
Date:   Wed Sep 21 14:38:10 2016 +0100

    sched/core: Add wrappers for lockdep_(un)pin_lock()
    
    In preparation for adding diagnostic checks to catch missing calls to
    update_rq_clock(), provide wrappers for (re)pinning and unpinning
    rq->lock.
    
    Because the pending diagnostic checks allow state to be maintained in
    rq_flags across pin contexts, swap the 'struct pin_cookie' arguments
    for 'struct rq_flags *'.
    
    Signed-off-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jan Kara <jack@suse.cz>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luca Abeni <luca.abeni@unitn.it>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Petr Mladek <pmladek@suse.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Sergey Senozhatsky <sergey.senozhatsky.work@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Cc: Yuyang Du <yuyang.du@intel.com>
    Link: http://lkml.kernel.org/r/20160921133813.31976-5-matt@codeblueprint.co.uk
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7b34c7826ca5..98e7eee07237 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -792,6 +792,26 @@ static inline void rq_clock_skip_update(struct rq *rq, bool skip)
 		rq->clock_skip_update &= ~RQCF_REQ_SKIP;
 }
 
+struct rq_flags {
+	unsigned long flags;
+	struct pin_cookie cookie;
+};
+
+static inline void rq_pin_lock(struct rq *rq, struct rq_flags *rf)
+{
+	rf->cookie = lockdep_pin_lock(&rq->lock);
+}
+
+static inline void rq_unpin_lock(struct rq *rq, struct rq_flags *rf)
+{
+	lockdep_unpin_lock(&rq->lock, rf->cookie);
+}
+
+static inline void rq_repin_lock(struct rq *rq, struct rq_flags *rf)
+{
+	lockdep_repin_lock(&rq->lock, rf->cookie);
+}
+
 #ifdef CONFIG_NUMA
 enum numa_topology_type {
 	NUMA_DIRECT,
@@ -1245,7 +1265,7 @@ struct sched_class {
 	 */
 	struct task_struct * (*pick_next_task) (struct rq *rq,
 						struct task_struct *prev,
-						struct pin_cookie cookie);
+						struct rq_flags *rf);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
@@ -1501,11 +1521,6 @@ static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }
 static inline void sched_avg_update(struct rq *rq) { }
 #endif
 
-struct rq_flags {
-	unsigned long flags;
-	struct pin_cookie cookie;
-};
-
 struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 	__acquires(rq->lock);
 struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
@@ -1515,7 +1530,7 @@ struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)
 	__releases(rq->lock)
 {
-	lockdep_unpin_lock(&rq->lock, rf->cookie);
+	rq_unpin_lock(rq, rf);
 	raw_spin_unlock(&rq->lock);
 }
 
@@ -1524,7 +1539,7 @@ task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
 	__releases(rq->lock)
 	__releases(p->pi_lock)
 {
-	lockdep_unpin_lock(&rq->lock, rf->cookie);
+	rq_unpin_lock(rq, rf);
 	raw_spin_unlock(&rq->lock);
 	raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
 }

commit afe06efdf07c12fd9370d5cce5383398cedf6c90
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Tue Nov 22 12:23:53 2016 -0800

    sched: Extend scheduler's asym packing
    
    We generalize the scheduler's asym packing to provide an ordering
    of the cpu beyond just the cpu number.  This allows the use of the
    ASYM_PACKING scheduler machinery to move loads to preferred CPU in a
    sched domain. The preference is defined with the cpu priority
    given by arch_asym_cpu_priority(cpu).
    
    We also record the most preferred cpu in a sched group when
    we build the cpu's capacity for fast lookup of preferred cpu
    during load balancing.
    
    Co-developed-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linux-pm@vger.kernel.org
    Cc: jolsa@redhat.com
    Cc: rjw@rjwysocki.net
    Cc: linux-acpi@vger.kernel.org
    Cc: Srinivas Pandruvada <srinivas.pandruvada@linux.intel.com>
    Cc: bp@suse.de
    Link: http://lkml.kernel.org/r/0e73ae12737dfaafa46c07066cc7c5d3f1675e46.1479844244.git.tim.c.chen@linux.intel.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d7e39317d688..7b34c7826ca5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -540,6 +540,11 @@ struct dl_rq {
 
 #ifdef CONFIG_SMP
 
+static inline bool sched_asym_prefer(int a, int b)
+{
+	return arch_asym_cpu_priority(a) > arch_asym_cpu_priority(b);
+}
+
 /*
  * We add the notion of a root-domain which will be used to define per-domain
  * variables. Each exclusive cpuset essentially defines an island domain by
@@ -908,6 +913,7 @@ struct sched_group {
 
 	unsigned int group_weight;
 	struct sched_group_capacity *sgc;
+	int asym_prefer_cpu;		/* cpu of highest priority in group */
 
 	/*
 	 * The CPUs this group covers.

commit 09a43ace1f986b003c118fdf6ddf1fd685692d49
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Nov 8 10:53:45 2016 +0100

    sched/fair: Propagate load during synchronous attach/detach
    
    When a task moves from/to a cfs_rq, we set a flag which is then used to
    propagate the change at parent level (sched_entity and cfs_rq) during
    next update. If the cfs_rq is throttled, the flag will stay pending until
    the cfs_rq is unthrottled.
    
    For propagating the utilization, we copy the utilization of group cfs_rq to
    the sched_entity.
    
    For propagating the load, we have to take into account the load of the
    whole task group in order to evaluate the load of the sched_entity.
    Similarly to what was done before the rewrite of PELT, we add a correction
    factor in case the task group's load is greater than its share so it will
    contribute the same load of a task of equal weight.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: kernellwp@gmail.com
    Cc: pjt@google.com
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/1478598827-32372-5-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 36f30e0aa266..d7e39317d688 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -404,6 +404,7 @@ struct cfs_rq {
 	unsigned long runnable_load_avg;
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	unsigned long tg_load_avg_contrib;
+	unsigned long propagate_avg;
 #endif
 	atomic_long_t removed_load_avg, removed_util_avg;
 #ifndef CONFIG_64BIT

commit 9c2791f936ef5fd04a118b5c284f2c9a95f4a647
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Nov 8 10:53:43 2016 +0100

    sched/fair: Fix hierarchical order in rq->leaf_cfs_rq_list
    
    Fix the insertion of cfs_rq in rq->leaf_cfs_rq_list to ensure that a
    child will always be called before its parent.
    
    The hierarchical order in shares update list has been introduced by
    commit:
    
      67e86250f8ea ("sched: Introduce hierarchal order on shares update list")
    
    With the current implementation a child can be still put after its
    parent.
    
    Lets take the example of:
    
           root
            \
             b
             /\
             c d*
               |
               e*
    
    with root -> b -> c already enqueued but not d -> e so the
    leaf_cfs_rq_list looks like: head -> c -> b -> root -> tail
    
    The branch d -> e will be added the first time that they are enqueued,
    starting with e then d.
    
    When e is added, its parents is not already on the list so e is put at
    the tail : head -> c -> b -> root -> e -> tail
    
    Then, d is added at the head because its parent is already on the
    list: head -> d -> c -> b -> root -> e -> tail
    
    e is not placed at the right position and will be called the last
    whereas it should be called at the beginning.
    
    Because it follows the bottom-up enqueue sequence, we are sure that we
    will finished to add either a cfs_rq without parent or a cfs_rq with a
    parent that is already on the list. We can use this event to detect
    when we have finished to add a new branch. For the others, whose
    parents are not already added, we have to ensure that they will be
    added after their children that have just been inserted the steps
    before, and after any potential parents that are already in the list.
    The easiest way is to put the cfs_rq just after the last inserted one
    and to keep track of it untl the branch is fully added.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: kernellwp@gmail.com
    Cc: pjt@google.com
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/1478598827-32372-3-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 345c1ccaba34..36f30e0aa266 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -623,6 +623,7 @@ struct rq {
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
 	struct list_head leaf_cfs_rq_list;
+	struct list_head *tmp_alone_branch;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 	/*

commit bf475ce0a3dd75b5d1df6c6c14ae25168caa15ac
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Fri Oct 14 14:41:09 2016 +0100

    sched/fair: Add per-CPU min capacity to sched_group_capacity
    
    struct sched_group_capacity currently represents the compute capacity
    sum of all CPUs in the sched_group.
    
    Unless it is divided by the group_weight to get the average capacity
    per CPU, it hides differences in CPU capacity for mixed capacity systems
    (e.g. high RT/IRQ utilization or ARM big.LITTLE).
    
    But even the average may not be sufficient if the group covers CPUs of
    different capacities.
    
    Instead, by extending struct sched_group_capacity to indicate min per-CPU
    capacity in the group a suitable group for a given task utilization can
    more easily be found such that CPUs with reduced capacity can be avoided
    for tasks with high utilization (not implemented by this patch).
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: freedom.tan@mediatek.com
    Cc: keita.kobayashi.ym@renesas.com
    Cc: mgalbraith@suse.de
    Cc: sgurrappadi@nvidia.com
    Cc: vincent.guittot@linaro.org
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/1476452472-24740-4-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 055f935d4421..345c1ccaba34 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -892,7 +892,8 @@ struct sched_group_capacity {
 	 * CPU capacity of this group, SCHED_CAPACITY_SCALE being max capacity
 	 * for a single CPU.
 	 */
-	unsigned int capacity;
+	unsigned long capacity;
+	unsigned long min_capacity; /* Min per-CPU capacity in group */
 	unsigned long next_update;
 	int imbalance; /* XXX unrelated to capacity but shared group state */
 

commit 1a4a2bc460721bc8f91e4c1294d39b38e5af132f
Merge: 110a9e42b687 1ef55be16ed6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 16:13:28 2016 -0700

    Merge branch 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull low-level x86 updates from Ingo Molnar:
     "In this cycle this topic tree has become one of those 'super topics'
      that accumulated a lot of changes:
    
       - Add CONFIG_VMAP_STACK=y support to the core kernel and enable it on
         x86 - preceded by an array of changes. v4.8 saw preparatory changes
         in this area already - this is the rest of the work. Includes the
         thread stack caching performance optimization. (Andy Lutomirski)
    
       - switch_to() cleanups and all around enhancements. (Brian Gerst)
    
       - A large number of dumpstack infrastructure enhancements and an
         unwinder abstraction. The secret long term plan is safe(r) live
         patching plus maybe another attempt at debuginfo based unwinding -
         but all these current bits are standalone enhancements in a frame
         pointer based debug environment as well. (Josh Poimboeuf)
    
       - More __ro_after_init and const annotations. (Kees Cook)
    
       - Enable KASLR for the vmemmap memory region. (Thomas Garnier)"
    
    [ The virtually mapped stack changes are pretty fundamental, and not
      x86-specific per se, even if they are only used on x86 right now. ]
    
    * 'x86-asm-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (70 commits)
      x86/asm: Get rid of __read_cr4_safe()
      thread_info: Use unsigned long for flags
      x86/alternatives: Add stack frame dependency to alternative_call_2()
      x86/dumpstack: Fix show_stack() task pointer regression
      x86/dumpstack: Remove dump_trace() and related callbacks
      x86/dumpstack: Convert show_trace_log_lvl() to use the new unwinder
      oprofile/x86: Convert x86_backtrace() to use the new unwinder
      x86/stacktrace: Convert save_stack_trace_*() to use the new unwinder
      perf/x86: Convert perf_callchain_kernel() to use the new unwinder
      x86/unwind: Add new unwind interface and implementations
      x86/dumpstack: Remove NULL task pointer convention
      fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y
      sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
      lib/syscall: Pin the task stack in collect_syscall()
      x86/process: Pin the target stack in get_wchan()
      x86/dumpstack: Pin the target stack when dumping it
      kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function
      sched/core: Add try_get_task_stack() and put_task_stack()
      x86/entry/64: Fix a minor comment rebase error
      iommu/amd: Don't put completion-wait semaphore on stack
      ...

commit af79ad2b1f337a00aa150b993635b10bc68dc842
Merge: e606d81d2d95 447976ef4fd0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Oct 3 13:39:00 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes from Ingo Molnar:
     "The main changes are:
    
       - irqtime accounting cleanups and enhancements. (Frederic Weisbecker)
    
       - schedstat debugging enhancements, make it more broadly runtime
         available. (Josh Poimboeuf)
    
       - More work on asymmetric topology/capacity scheduling. (Morten
         Rasmussen)
    
       - sched/wait fixes and cleanups. (Oleg Nesterov)
    
       - PELT (per entity load tracking) improvements. (Peter Zijlstra)
    
       - Rewrite and enhance select_idle_siblings(). (Peter Zijlstra)
    
       - sched/numa enhancements/fixes (Rik van Riel)
    
       - sched/cputime scalability improvements (Stanislaw Gruszka)
    
       - Load calculation arithmetics fixes. (Dietmar Eggemann)
    
       - sched/deadline enhancements (Tommaso Cucinotta)
    
       - Fix utilization accounting when switching to the SCHED_NORMAL
         policy. (Vincent Guittot)
    
       - ... plus misc cleanups and enhancements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (64 commits)
      sched/irqtime: Consolidate irqtime flushing code
      sched/irqtime: Consolidate accounting synchronization with u64_stats API
      u64_stats: Introduce IRQs disabled helpers
      sched/irqtime: Remove needless IRQs disablement on kcpustat update
      sched/irqtime: No need for preempt-safe accessors
      sched/fair: Fix min_vruntime tracking
      sched/debug: Add SCHED_WARN_ON()
      sched/core: Fix set_user_nice()
      sched/fair: Introduce set_curr_task() helper
      sched/core, ia64: Rename set_curr_task()
      sched/core: Fix incorrect utilization accounting when switching to fair class
      sched/core: Optimize SCHED_SMT
      sched/core: Rewrite and improve select_idle_siblings()
      sched/core: Replace sd_busy/nr_busy_cpus with sched_domain_shared
      sched/core: Introduce 'struct sched_domain_shared'
      sched/core: Restructure destroy_sched_domain()
      sched/core: Remove unused @cpu argument from destroy_sched_domain*()
      sched/wait: Introduce init_wait_entry()
      sched/wait: Avoid abort_exclusive_wait() in __wait_on_bit_lock()
      sched/wait: Avoid abort_exclusive_wait() in ___wait_event()
      ...

commit 19d23dbfeb10724675152915e76e03d771f23d9d
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Sep 26 02:29:20 2016 +0200

    sched/irqtime: Consolidate accounting synchronization with u64_stats API
    
    The irqtime accounting currently implement its own ad hoc implementation
    of u64_stats API. Lets rather consolidate it with the appropriate
    library.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Wanpeng Li <wanpeng.li@hotmail.com>
    Link: http://lkml.kernel.org/r/1474849761-12678-5-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5489d07a4643..19b99869809d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2,6 +2,7 @@
 #include <linux/sched.h>
 #include <linux/sched/sysctl.h>
 #include <linux/sched/rt.h>
+#include <linux/u64_stats_sync.h>
 #include <linux/sched/deadline.h>
 #include <linux/binfmts.h>
 #include <linux/mutex.h>
@@ -1735,52 +1736,28 @@ static inline void nohz_balance_exit_idle(unsigned int cpu) { }
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING
+struct irqtime {
+	u64			hardirq_time;
+	u64			softirq_time;
+	u64			irq_start_time;
+	struct u64_stats_sync	sync;
+};
 
-DECLARE_PER_CPU(u64, cpu_hardirq_time);
-DECLARE_PER_CPU(u64, cpu_softirq_time);
-
-#ifndef CONFIG_64BIT
-DECLARE_PER_CPU(seqcount_t, irq_time_seq);
-
-static inline void irq_time_write_begin(void)
-{
-	__this_cpu_inc(irq_time_seq.sequence);
-	smp_wmb();
-}
-
-static inline void irq_time_write_end(void)
-{
-	smp_wmb();
-	__this_cpu_inc(irq_time_seq.sequence);
-}
+DECLARE_PER_CPU(struct irqtime, cpu_irqtime);
 
 static inline u64 irq_time_read(int cpu)
 {
-	u64 irq_time;
-	unsigned seq;
+	struct irqtime *irqtime = &per_cpu(cpu_irqtime, cpu);
+	unsigned int seq;
+	u64 total;
 
 	do {
-		seq = read_seqcount_begin(&per_cpu(irq_time_seq, cpu));
-		irq_time = per_cpu(cpu_softirq_time, cpu) +
-			   per_cpu(cpu_hardirq_time, cpu);
-	} while (read_seqcount_retry(&per_cpu(irq_time_seq, cpu), seq));
-
-	return irq_time;
-}
-#else /* CONFIG_64BIT */
-static inline void irq_time_write_begin(void)
-{
-}
+		seq = __u64_stats_fetch_begin(&irqtime->sync);
+		total = irqtime->softirq_time + irqtime->hardirq_time;
+	} while (__u64_stats_fetch_retry(&irqtime->sync, seq));
 
-static inline void irq_time_write_end(void)
-{
-}
-
-static inline u64 irq_time_read(int cpu)
-{
-	return per_cpu(cpu_softirq_time, cpu) + per_cpu(cpu_hardirq_time, cpu);
+	return total;
 }
-#endif /* CONFIG_64BIT */
 #endif /* CONFIG_IRQ_TIME_ACCOUNTING */
 
 #ifdef CONFIG_CPU_FREQ

commit 9148a3a10e0b74c5722174a0bbef16d821f8a48b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 20 22:34:51 2016 +0200

    sched/debug: Add SCHED_WARN_ON()
    
    Provide SCHED_WARN_ON as wrapper for WARN_ON_ONCE() to avoid
    CONFIG_SCHED_DEBUG wrappery.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fc6ae04ec080..5489d07a4643 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -15,6 +15,12 @@
 #include "cpudeadline.h"
 #include "cpuacct.h"
 
+#ifdef CONFIG_SCHED_DEBUG
+#define SCHED_WARN_ON(x)	WARN_ONCE(x, #x)
+#else
+#define SCHED_WARN_ON(x)	((void)(x))
+#endif
+
 struct rq;
 struct cpuidle_state;
 
@@ -1309,7 +1315,7 @@ static inline void idle_set_state(struct rq *rq,
 
 static inline struct cpuidle_state *idle_get_state(struct rq *rq)
 {
-	WARN_ON(!rcu_read_lock_held());
+	SCHED_WARN_ON(!rcu_read_lock_held());
 	return rq->idle_state;
 }
 #else

commit b2bf6c314e3a9e227925240d92ecd6e9b0110170
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 20 22:00:38 2016 +0200

    sched/fair: Introduce set_curr_task() helper
    
    Now that the ia64 only set_curr_task() symbol is gone, provide a
    helper just like put_prev_task().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 01b5189235f2..fc6ae04ec080 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1274,6 +1274,11 @@ static inline void put_prev_task(struct rq *rq, struct task_struct *prev)
 	prev->sched_class->put_prev_task(rq, prev);
 }
 
+static inline void set_curr_task(struct rq *rq, struct task_struct *curr)
+{
+	curr->sched_class->set_curr_task(rq);
+}
+
 #define sched_class_highest (&stop_sched_class)
 #define for_each_class(class) \
    for (class = sched_class_highest; class; class = class->next)

commit 1b568f0aabf280555125bc7cefc08321ff0ebaba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 9 10:38:41 2016 +0200

    sched/core: Optimize SCHED_SMT
    
    Avoid pointless SCHED_SMT code when running on !SMT hardware.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c917dcad82ad..01b5189235f2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -36,12 +36,6 @@ extern void cpu_load_update_active(struct rq *this_rq);
 static inline void cpu_load_update_active(struct rq *this_rq) { }
 #endif
 
-#ifdef CONFIG_SCHED_SMT
-extern void update_idle_core(struct rq *rq);
-#else
-static inline void update_idle_core(struct rq *rq) { }
-#endif
-
 /*
  * Helpers for converting nanosecond timing to jiffy resolution
  */
@@ -730,6 +724,23 @@ static inline int cpu_of(struct rq *rq)
 #endif
 }
 
+
+#ifdef CONFIG_SCHED_SMT
+
+extern struct static_key_false sched_smt_present;
+
+extern void __update_idle_core(struct rq *rq);
+
+static inline void update_idle_core(struct rq *rq)
+{
+	if (static_branch_unlikely(&sched_smt_present))
+		__update_idle_core(rq);
+}
+
+#else
+static inline void update_idle_core(struct rq *rq) { }
+#endif
+
 DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 
 #define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))

commit 10e2f1acd0106c05229f94c70a344ce3a2c8008b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 9 10:38:05 2016 +0200

    sched/core: Rewrite and improve select_idle_siblings()
    
    select_idle_siblings() is a known pain point for a number of
    workloads; it either does too much or not enough and sometimes just
    does plain wrong.
    
    This rewrite attempts to address a number of issues (but sadly not
    all).
    
    The current code does an unconditional sched_domain iteration; with
    the intent of finding an idle core (on SMT hardware). The problems
    which this patch tries to address are:
    
     - its pointless to look for idle cores if the machine is real busy;
       at which point you're just wasting cycles.
    
     - it's behaviour is inconsistent between SMT and !SMT hardware in
       that !SMT hardware ends up doing a scan for any idle CPU in the LLC
       domain, while SMT hardware does a scan for idle cores and if that
       fails, falls back to a scan for idle threads on the 'target' core.
    
    The new code replaces the sched_domain scan with 3 explicit scans:
    
     1) search for an idle core in the LLC
     2) search for an idle CPU in the LLC
     3) search for an idle thread in the 'target' core
    
    where 1 and 3 are conditional on SMT support and 1 and 2 have runtime
    heuristics to skip the step.
    
    Step 1) is conditional on sd_llc_shared->has_idle_cores; when a cpu
    goes idle and sd_llc_shared->has_idle_cores is false, we scan all SMT
    siblings of the CPU going idle. Similarly, we clear
    sd_llc_shared->has_idle_cores when we fail to find an idle core.
    
    Step 2) tracks the average cost of the scan and compares this to the
    average idle time guestimate for the CPU doing the wakeup. There is a
    significant fudge factor involved to deal with the variability of the
    averages. Esp. hackbench was sensitive to this.
    
    Step 3) is unconditional; we assume (also per step 1) that scanning
    all SMT siblings in a core is 'cheap'.
    
    With this; SMT systems gain step 2, which cures a few benchmarks --
    notably one from Facebook.
    
    One 'feature' of the sched_domain iteration, which we preserve in the
    new code, is that it would start scanning from the 'target' CPU,
    instead of scanning the cpumask in cpu id order. This avoids multiple
    CPUs in the LLC scanning for idle to gang up and find the same CPU
    quite as much. The down side is that tasks can end up hopping across
    the LLC for no apparent reason.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4fc6e9876d9c..c917dcad82ad 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -36,6 +36,12 @@ extern void cpu_load_update_active(struct rq *this_rq);
 static inline void cpu_load_update_active(struct rq *this_rq) { }
 #endif
 
+#ifdef CONFIG_SCHED_SMT
+extern void update_idle_core(struct rq *rq);
+#else
+static inline void update_idle_core(struct rq *rq) { }
+#endif
+
 /*
  * Helpers for converting nanosecond timing to jiffy resolution
  */

commit 0e369d757578b23ac50b893f920aa50fdbc45fb6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 9 10:38:01 2016 +0200

    sched/core: Replace sd_busy/nr_busy_cpus with sched_domain_shared
    
    Move the nr_busy_cpus thing from its hacky sd->parent->groups->sgc
    location into the much more natural sched_domain_shared location.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 420c05d099c3..4fc6e9876d9c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -858,8 +858,8 @@ static inline struct sched_domain *lowest_flag_domain(int cpu, int flag)
 DECLARE_PER_CPU(struct sched_domain *, sd_llc);
 DECLARE_PER_CPU(int, sd_llc_size);
 DECLARE_PER_CPU(int, sd_llc_id);
+DECLARE_PER_CPU(struct sched_domain_shared *, sd_llc_shared);
 DECLARE_PER_CPU(struct sched_domain *, sd_numa);
-DECLARE_PER_CPU(struct sched_domain *, sd_busy);
 DECLARE_PER_CPU(struct sched_domain *, sd_asym);
 
 struct sched_group_capacity {
@@ -871,10 +871,6 @@ struct sched_group_capacity {
 	unsigned int capacity;
 	unsigned long next_update;
 	int imbalance; /* XXX unrelated to capacity but shared group state */
-	/*
-	 * Number of busy cpus in this group.
-	 */
-	atomic_t nr_busy_cpus;
 
 	unsigned long cpumask[0]; /* iteration mask */
 };

commit c65eacbe290b8141554c71b2c94489e73ade8c8d
Author: Andy Lutomirski <luto@kernel.org>
Date:   Tue Sep 13 14:29:24 2016 -0700

    sched/core: Allow putting thread_info into task_struct
    
    If an arch opts in by setting CONFIG_THREAD_INFO_IN_TASK_STRUCT,
    then thread_info is defined as a single 'u32 flags' and is the first
    entry of task_struct.  thread_info::task is removed (it serves no
    purpose if thread_info is embedded in task_struct), and
    thread_info::cpu gets its own slot in task_struct.
    
    This is heavily based on a patch written by Linus.
    
    Originally-from: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jann Horn <jann@thejh.net>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/a0898196f0476195ca02713691a5037a14f2aac5.1473801993.git.luto@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c64fc5114004..3655c9625e5b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1000,7 +1000,11 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 	 * per-task data have been completed by this moment.
 	 */
 	smp_wmb();
+#ifdef CONFIG_THREAD_INFO_IN_TASK
+	p->cpu = cpu;
+#else
 	task_thread_info(p)->cpu = cpu;
+#endif
 	p->wake_cpu = cpu;
 #endif
 }

commit cd92bfd3b8cb0ec2ee825e55a3aee704cd55aea9
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Mon Aug 1 19:53:35 2016 +0100

    sched/core: Store maximum per-CPU capacity in root domain
    
    To be able to compare the capacity of the target CPU with the highest
    available CPU capacity, store the maximum per-CPU capacity in the root
    domain.
    
    The max per-CPU capacity should be 1024 for all systems except SMT,
    where the capacity is currently based on smt_gain and the number of
    hardware threads and is <1024. If SMT can be brought to work with a
    per-thread capacity of 1024, this patch can be dropped and replaced by a
    hard-coded max capacity of 1024 (=SCHED_CAPACITY_SCALE).
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: freedom.tan@mediatek.com
    Cc: keita.kobayashi.ym@renesas.com
    Cc: mgalbraith@suse.de
    Cc: sgurrappadi@nvidia.com
    Cc: vincent.guittot@linaro.org
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/26c69258-9947-f830-a53e-0c54e7750646@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index afe76d04e916..420c05d099c3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -565,6 +565,8 @@ struct root_domain {
 	 */
 	cpumask_var_t rto_mask;
 	struct cpupri cpupri;
+
+	unsigned long max_cpu_capacity;
 };
 
 extern struct root_domain def_root_domain;

commit 1fc770d5899c995db8e22d35eb918a2cb79559d9
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Aug 15 12:14:10 2016 -0400

    sched: Remove struct rq::nohz_stamp
    
    The nohz_stamp member of struct rq has been unused since 2010,
    when this commit removed the code that referenced it:
    
      396e894d289d ("sched: Revert nohz_ratelimit() for now")
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160815121410.5ea1c98f@annuminas.surriel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c64fc5114004..afe76d04e916 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -597,7 +597,6 @@ struct rq {
 #ifdef CONFIG_SMP
 	unsigned long last_load_update_tick;
 #endif /* CONFIG_SMP */
-	u64 nohz_stamp;
 	unsigned long nohz_flags;
 #endif /* CONFIG_NO_HZ_COMMON */
 #ifdef CONFIG_NO_HZ_FULL

commit 12bde33dbb3eadd60343a8a71c39766073c1d752
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Aug 10 03:11:17 2016 +0200

    cpufreq / sched: Pass runqueue pointer to cpufreq_update_util()
    
    All of the callers of cpufreq_update_util() pass rq_clock(rq) to it
    as the time argument and some of them check whether or not cpu_of(rq)
    is equal to smp_processor_id() before calling it, so rework it to
    take a runqueue pointer as the argument and move the rq_clock(rq)
    evaluation into it.
    
    Additionally, provide a wrapper checking cpu_of(rq) against
    smp_processor_id() for the cpufreq_update_util() callers that
    need it.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 82fc5542708c..b7fc1ced4380 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1763,7 +1763,7 @@ DECLARE_PER_CPU(struct update_util_data *, cpufreq_update_util_data);
 
 /**
  * cpufreq_update_util - Take a note about CPU utilization changes.
- * @time: Current time.
+ * @rq: Runqueue to carry out the update for.
  * @flags: Update reason flags.
  *
  * This function is called by the scheduler on the CPU whose utilization is
@@ -1783,16 +1783,23 @@ DECLARE_PER_CPU(struct update_util_data *, cpufreq_update_util_data);
  * but that really is a band-aid.  Going forward it should be replaced with
  * solutions targeted more specifically at RT and DL tasks.
  */
-static inline void cpufreq_update_util(u64 time, unsigned int flags)
+static inline void cpufreq_update_util(struct rq *rq, unsigned int flags)
 {
 	struct update_util_data *data;
 
 	data = rcu_dereference_sched(*this_cpu_ptr(&cpufreq_update_util_data));
 	if (data)
-		data->func(data, time, flags);
+		data->func(data, rq_clock(rq), flags);
+}
+
+static inline void cpufreq_update_this_cpu(struct rq *rq, unsigned int flags)
+{
+	if (cpu_of(rq) == smp_processor_id())
+		cpufreq_update_util(rq, flags);
 }
 #else
-static inline void cpufreq_update_util(u64 time, unsigned int flags) {}
+static inline void cpufreq_update_util(struct rq *rq, unsigned int flags) {}
+static inline void cpufreq_update_this_cpu(struct rq *rq, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef arch_scale_freq_capacity

commit 58919e83c85c3a3c5fb34025dc0e95ddd998c478
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Aug 16 22:14:55 2016 +0200

    cpufreq / sched: Pass flags to cpufreq_update_util()
    
    It is useful to know the reason why cpufreq_update_util() has just
    been called and that can be passed as flags to cpufreq_update_util()
    and to the ->func() callback in struct update_util_data.  However,
    doing that in addition to passing the util and max arguments they
    already take would be clumsy, so avoid it.
    
    Instead, use the observation that the schedutil governor is part
    of the scheduler proper, so it can access scheduler data directly.
    This allows the util and max arguments of cpufreq_update_util()
    and the ->func() callback in struct update_util_data to be replaced
    with a flags one, but schedutil has to be modified to follow.
    
    Thus make the schedutil governor obtain the CFS utilization
    information from the scheduler and use the "RT" and "DL" flags
    instead of the special utilization value of ULONG_MAX to track
    updates from the RT and DL sched classes.  Make it non-modular
    too to avoid having to export scheduler variables to modules at
    large.
    
    Next, update all of the other users of cpufreq_update_util()
    and the ->func() callback in struct update_util_data accordingly.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c64fc5114004..82fc5542708c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1764,26 +1764,12 @@ DECLARE_PER_CPU(struct update_util_data *, cpufreq_update_util_data);
 /**
  * cpufreq_update_util - Take a note about CPU utilization changes.
  * @time: Current time.
- * @util: Current utilization.
- * @max: Utilization ceiling.
+ * @flags: Update reason flags.
  *
- * This function is called by the scheduler on every invocation of
- * update_load_avg() on the CPU whose utilization is being updated.
+ * This function is called by the scheduler on the CPU whose utilization is
+ * being updated.
  *
  * It can only be called from RCU-sched read-side critical sections.
- */
-static inline void cpufreq_update_util(u64 time, unsigned long util, unsigned long max)
-{
-       struct update_util_data *data;
-
-       data = rcu_dereference_sched(*this_cpu_ptr(&cpufreq_update_util_data));
-       if (data)
-               data->func(data, time, util, max);
-}
-
-/**
- * cpufreq_trigger_update - Trigger CPU performance state evaluation if needed.
- * @time: Current time.
  *
  * The way cpufreq is currently arranged requires it to evaluate the CPU
  * performance state (frequency/voltage) on a regular basis to prevent it from
@@ -1797,13 +1783,16 @@ static inline void cpufreq_update_util(u64 time, unsigned long util, unsigned lo
  * but that really is a band-aid.  Going forward it should be replaced with
  * solutions targeted more specifically at RT and DL tasks.
  */
-static inline void cpufreq_trigger_update(u64 time)
+static inline void cpufreq_update_util(u64 time, unsigned int flags)
 {
-	cpufreq_update_util(time, ULONG_MAX, 0);
+	struct update_util_data *data;
+
+	data = rcu_dereference_sched(*this_cpu_ptr(&cpufreq_update_util_data));
+	if (data)
+		data->func(data, time, flags);
 }
 #else
-static inline void cpufreq_update_util(u64 time, unsigned long util, unsigned long max) {}
-static inline void cpufreq_trigger_update(u64 time) {}
+static inline void cpufreq_update_util(u64 time, unsigned int flags) {}
 #endif /* CONFIG_CPU_FREQ */
 
 #ifdef arch_scale_freq_capacity

commit cca08cd66ce6cc37812b6b36986ba7eaabd33e0b
Merge: 7e4dc77b2869 748c7201e622
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 13:59:34 2016 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - introduce and use task_rcu_dereference()/try_get_task_struct() to fix
       and generalize task_struct handling (Oleg Nesterov)
    
     - do various per entity load tracking (PELT) fixes and optimizations
       (Peter Zijlstra)
    
     - cputime virt-steal time accounting enhancements/fixes (Wanpeng Li)
    
     - introduce consolidated cputime output file cpuacct.usage_all and
       related refactorings (Zhao Lei)
    
     - ... plus misc fixes and enhancements
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/core: Panic on scheduling while atomic bugs if kernel.panic_on_warn is set
      sched/cpuacct: Introduce cpuacct.usage_all to show all CPU stats together
      sched/cpuacct: Use loop to consolidate code in cpuacct_stats_show()
      sched/cpuacct: Merge cpuacct_usage_index and cpuacct_stat_index enums
      sched/fair: Rework throttle_count sync
      sched/core: Fix sched_getaffinity() return value kerneldoc comment
      sched/fair: Reorder cgroup creation code
      sched/fair: Apply more PELT fixes
      sched/fair: Fix PELT integrity for new tasks
      sched/cgroup: Fix cpu_cgroup_fork() handling
      sched/fair: Fix PELT integrity for new groups
      sched/fair: Fix and optimize the fork() path
      sched/cputime: Add steal time support to full dynticks CPU time accounting
      sched/cputime: Fix prev steal time accouting during CPU hotplug
      KVM: Fix steal clock warp during guest CPU hotplug
      sched/debug: Always show 'nr_migrations'
      sched/fair: Use task_rcu_dereference()
      sched/api: Introduce task_rcu_dereference() and try_get_task_struct()
      sched/idle: Optimize the generic idle loop
      sched/fair: Fix the wrong throttled clock time for cfs_rq_clock_task()

commit c86ad14d305d2429c3da19462440bac50c183def
Merge: a2303849a6b4 f06628638cf6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 25 12:41:29 2016 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
     "The locking tree was busier in this cycle than the usual pattern - a
      couple of major projects happened to coincide.
    
      The main changes are:
    
       - implement the atomic_fetch_{add,sub,and,or,xor}() API natively
         across all SMP architectures (Peter Zijlstra)
    
       - add atomic_fetch_{inc/dec}() as well, using the generic primitives
         (Davidlohr Bueso)
    
       - optimize various aspects of rwsems (Jason Low, Davidlohr Bueso,
         Waiman Long)
    
       - optimize smp_cond_load_acquire() on arm64 and implement LSE based
         atomic{,64}_fetch_{add,sub,and,andnot,or,xor}{,_relaxed,_acquire,_release}()
         on arm64 (Will Deacon)
    
       - introduce smp_acquire__after_ctrl_dep() and fix various barrier
         mis-uses and bugs (Peter Zijlstra)
    
       - after discovering ancient spin_unlock_wait() barrier bugs in its
         implementation and usage, strengthen its semantics and update/fix
         usage sites (Peter Zijlstra)
    
       - optimize mutex_trylock() fastpath (Peter Zijlstra)
    
       - ... misc fixes and cleanups"
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (67 commits)
      locking/atomic: Introduce inc/dec variants for the atomic_fetch_$op() API
      locking/barriers, arch/arm64: Implement LDXR+WFE based smp_cond_load_acquire()
      locking/static_keys: Fix non static symbol Sparse warning
      locking/qspinlock: Use __this_cpu_dec() instead of full-blown this_cpu_dec()
      locking/atomic, arch/tile: Fix tilepro build
      locking/atomic, arch/m68k: Remove comment
      locking/atomic, arch/arc: Fix build
      locking/Documentation: Clarify limited control-dependency scope
      locking/atomic, arch/rwsem: Employ atomic_long_fetch_add()
      locking/atomic, arch/qrwlock: Employ atomic_fetch_add_acquire()
      locking/atomic, arch/mips: Convert to _relaxed atomics
      locking/atomic, arch/alpha: Convert to _relaxed atomics
      locking/atomic: Remove the deprecated atomic_{set,clear}_mask() functions
      locking/atomic: Remove linux/atomic.h:atomic_fetch_or()
      locking/atomic: Implement atomic{,64,_long}_fetch_{add,sub,and,andnot,or,xor}{,_relaxed,_acquire,_release}()
      locking/atomic: Fix atomic64_relaxed() bits
      locking/atomic, arch/xtensa: Implement atomic_fetch_{add,sub,and,or,xor}()
      locking/atomic, arch/x86: Implement atomic{,64}_fetch_{add,sub,and,or,xor}()
      locking/atomic, arch/tile: Implement atomic{,64}_fetch_{add,sub,and,or,xor}()
      locking/atomic, arch/sparc: Implement atomic{,64}_fetch_{add,sub,and,or,xor}()
      ...

commit d60585c5766e9620d5d83e2b25dc042c7bdada2c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jul 12 18:33:56 2016 +0200

    sched/core: Correct off by one bug in load migration calculation
    
    The move of calc_load_migrate() from CPU_DEAD to CPU_DYING did not take into
    account that the function is now called from a thread running on the outgoing
    CPU. As a result a cpu unplug leakes a load of 1 into the global load
    accounting mechanism.
    
    Fix it by adjusting for the currently running thread which calls
    calc_load_migrate().
    
    Reported-by: Anton Blanchard <anton@samba.org>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Vaidyanathan Srinivasan <svaidy@linux.vnet.ibm.com>
    Cc: rt@linutronix.de
    Cc: shreyas@linux.vnet.ibm.com
    Fixes: e9cd8fa4fcfd: ("sched/migration: Move calc_load_migrate() into CPU_DYING")
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1607121744350.4083@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7cbeb92a1cb9..898c0d2f18fe 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -28,7 +28,7 @@ extern unsigned long calc_load_update;
 extern atomic_long_t calc_load_tasks;
 
 extern void calc_global_load_tick(struct rq *this_rq);
-extern long calc_load_fold_active(struct rq *this_rq);
+extern long calc_load_fold_active(struct rq *this_rq, long adjust);
 
 #ifdef CONFIG_SMP
 extern void cpu_load_update_active(struct rq *this_rq);

commit 55e16d30bd99510900caec913c90f53bc2b35cba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 22 15:14:26 2016 +0200

    sched/fair: Rework throttle_count sync
    
    Since we already take rq->lock when creating a cgroup, use it to also
    sync the throttle_count and avoid the extra state and enqueue path
    branch.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: linux-kernel@vger.kernel.org
    [ Fixed build warning. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 28c42b789f70..f44da95c70cd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -438,7 +438,7 @@ struct cfs_rq {
 
 	u64 throttled_clock, throttled_clock_task;
 	u64 throttled_clock_task_time;
-	int throttled, throttle_count, throttle_uptodate;
+	int throttled, throttle_count;
 	struct list_head throttled_list;
 #endif /* CONFIG_CFS_BANDWIDTH */
 #endif /* CONFIG_FAIR_GROUP_SCHED */

commit 8663e24d56dc1f093232783c23ea17f2a6f61c03
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 22 14:58:02 2016 +0200

    sched/fair: Reorder cgroup creation code
    
    A future patch needs rq->lock held _after_ we link the task_group into
    the hierarchy. In order to avoid taking every rq->lock twice, reorder
    things a little and create online_fair_sched_group() to be called
    after we link the task_group.
    
    All this code is still ran from css_alloc() so css_online() isn't in
    fact used for this.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 307bd0418095..28c42b789f70 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -321,6 +321,7 @@ extern int tg_nop(struct task_group *tg, void *data);
 
 extern void free_fair_sched_group(struct task_group *tg);
 extern int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent);
+extern void online_fair_sched_group(struct task_group *tg);
 extern void unregister_fair_sched_group(struct task_group *tg);
 extern void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,
 			struct sched_entity *se, int cpu,

commit ea86cb4b7621e1298a37197005bf0abcc86348d4
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Jun 17 13:38:55 2016 +0200

    sched/cgroup: Fix cpu_cgroup_fork() handling
    
    A new fair task is detached and attached from/to task_group with:
    
      cgroup_post_fork()
        ss->fork(child) := cpu_cgroup_fork()
          sched_move_task()
            task_move_group_fair()
    
    Which is wrong, because at this point in fork() the task isn't fully
    initialized and it cannot 'move' to another group, because its not
    attached to any group as yet.
    
    In fact, cpu_cgroup_fork() needs a small part of sched_move_task() so we
    can just call this small part directly instead sched_move_task(). And
    the task doesn't really migrate because it is not yet attached so we
    need the following sequence:
    
      do_fork()
        sched_fork()
          __set_task_cpu()
    
        cgroup_post_fork()
          set_task_rq() # set task group and runqueue
    
        wake_up_new_task()
          select_task_rq() can select a new cpu
          __set_task_cpu
          post_init_entity_util_avg
            attach_task_cfs_rq()
          activate_task
            enqueue_task
    
    This patch makes that happen.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    [ Added TASK_SET_GROUP to set depth properly. ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 71ce9862abc3..307bd0418095 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1246,8 +1246,11 @@ struct sched_class {
 
 	void (*update_curr) (struct rq *rq);
 
+#define TASK_SET_GROUP  0
+#define TASK_MOVE_GROUP	1
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	void (*task_move_group) (struct task_struct *p);
+	void (*task_change_group) (struct task_struct *p, int type);
 #endif
 };
 

commit 630741fb60ac4e286f5396403c0d864d924c02bc
Merge: 807e5b80687c ea1dc6fc6242
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 27 11:35:02 2016 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 094f469172e00d6ab0a3130b0e01c83b3cf3a98d
Author: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
Date:   Thu Jun 16 15:57:01 2016 +0300

    sched/fair: Initialize throttle_count for new task-groups lazily
    
    Cgroup created inside throttled group must inherit current throttle_count.
    Broken throttle_count allows to nominate throttled entries as a next buddy,
    later this leads to null pointer dereference in pick_next_task_fair().
    
    This patch initialize cfs_rq->throttle_count at first enqueue: laziness
    allows to skip locking all rq at group creation. Lazy approach also allows
    to skip full sub-tree scan at throttling hierarchy (not in this patch).
    
    Signed-off-by: Konstantin Khlebnikov <khlebnikov@yandex-team.ru>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Link: http://lkml.kernel.org/r/146608182119.21870.8439834428248129633.stgit@buzz
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 72f1f3087b04..7cbeb92a1cb9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -437,7 +437,7 @@ struct cfs_rq {
 
 	u64 throttled_clock, throttled_clock_task;
 	u64 throttled_clock_task_time;
-	int throttled, throttle_count;
+	int throttled, throttle_count, throttle_uptodate;
 	struct list_head throttled_list;
 #endif /* CONFIG_CFS_BANDWIDTH */
 #endif /* CONFIG_FAIR_GROUP_SCHED */

commit 1f03e8d2919270bd6ef64f39a45ce8df8a9f012a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Apr 4 10:57:12 2016 +0200

    locking/barriers: Replace smp_cond_acquire() with smp_cond_load_acquire()
    
    This new form allows using hardware assisted waiting.
    
    Some hardware (ARM64 and x86) allow monitoring an address for changes,
    so by providing a pointer we can use this to replace the cpu_relax()
    with hardware optimized methods in the future.
    
    Requested-by: Will Deacon <will.deacon@arm.com>
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 72f1f3087b04..425bf5ddaa5a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1113,7 +1113,7 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 	 * In particular, the load of prev->state in finish_task_switch() must
 	 * happen before this.
 	 *
-	 * Pairs with the smp_cond_acquire() in try_to_wake_up().
+	 * Pairs with the smp_cond_load_acquire() in try_to_wake_up().
 	 */
 	smp_store_release(&prev->on_cpu, 0);
 #endif

commit 3d89e5478bf550a50c99e93adf659369798263b0
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Mon Jun 13 18:32:45 2016 +0800

    sched/cputime: Fix prev steal time accouting during CPU hotplug
    
    Commit:
    
      e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")
    
    ... set rq->prev_* to 0 after a CPU hotplug comes back, in order to
    fix the case where (after CPU hotplug) steal time is smaller than
    rq->prev_steal_time.
    
    However, this should never happen. Steal time was only smaller because of the
    KVM-specific bug fixed by the previous patch.  Worse, the previous patch
    triggers a bug on CPU hot-unplug/plug operation: because
    rq->prev_steal_time is cleared, all of the CPU's past steal time will be
    accounted again on hot-plug.
    
    Since the root cause has been fixed, we can just revert commit e9532e69b8d1.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Radim Krčmář <rkrcmar@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: 'commit e9532e69b8d1 ("sched/cputime: Fix steal time accounting vs. CPU hotplug")'
    Link: http://lkml.kernel.org/r/1465813966-3116-3-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 72f1f3087b04..de607e4febd9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1809,16 +1809,3 @@ static inline void cpufreq_trigger_update(u64 time) {}
 #else /* arch_scale_freq_capacity */
 #define arch_scale_freq_invariant()	(false)
 #endif
-
-static inline void account_reset_rq(struct rq *rq)
-{
-#ifdef CONFIG_IRQ_TIME_ACCOUNTING
-	rq->prev_irq_time = 0;
-#endif
-#ifdef CONFIG_PARAVIRT
-	rq->prev_steal_time = 0;
-#endif
-#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
-	rq->prev_steal_time_rq = 0;
-#endif
-}

commit d57d39431924d1628ac9b93a2de7f806fc80680a
Merge: 3e21e5dda490 27c4a1c5ef61
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon May 16 19:17:22 2016 -0700

    Merge tag 'pm-4.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "The majority of changes go into the cpufreq subsystem this time.
    
      To me, quite obviously, the biggest ticket item is the new "schedutil"
      governor.  Interestingly enough, it's the first new cpufreq governor
      since the beginning of the git era (except for some out-of-the-tree
      ones).
    
      There are two main differences between it and the existing governors.
      First, it uses the information provided by the scheduler directly for
      making its decisions, so it doesn't have to track anything by itself.
      Second, it can invoke drivers (supporting that feature) to adjust CPU
      performance right away without having to spawn work items to be
      executed in process context or similar.  Currently, the acpi-cpufreq
      driver is the only one supporting that mode of operation, but then it
      is used on a large number of systems.
    
      The "schedutil" governor as included here is very simple and mostly
      regarded as a foundation for future work on the integration of the
      scheduler with CPU power management (in fact, there is work in
      progress on top of it already).  Nevertheless it works and the
      preliminary results obtained with it are encouraging.
    
      There also is some consolidation of CPU frequency management for ARM
      platforms that can add their machine IDs the the new stub dt-platdev
      driver now and that will take care of creating the requisite platform
      device for cpufreq-dt, so it is not necessary to do that in platform
      code any more.  Several ARM platforms are switched over to using this
      generic mechanism.
    
      In addition to that, the intel_pstate driver is now going to respect
      CPU frequency limits set by the platform firmware (or a BMC) and
      provided via the ACPI _PPC object.
    
      The devfreq subsystem is getting a new "passive" governor for SoCs
      subsystems that will depend on somebody else to manage their voltage
      rails and its support for Samsung Exynos SoCs is consolidated.
    
      The rest is support for new hardware (Intel Broxton support in
      intel_idle for one example), bug fixes, optimizations and cleanups in
      a number of places.
    
      Specifics:
    
       - New cpufreq "schedutil" governor (making decisions based on CPU
         utilization information provided by the scheduler and capable of
         switching CPU frequencies right away if the underlying driver
         supports that) and support for fast frequency switching in the
         acpi-cpufreq driver (Rafael Wysocki)
    
       - Consolidation of CPU frequency management on ARM platforms allowing
         them to get rid of some platform-specific boilerplate code if they
         are going to use the cpufreq-dt driver (Viresh Kumar, Finley Xiao,
         Marc Gonzalez)
    
       - Support for ACPI _PPC and CPU frequency limits in the intel_pstate
         driver (Srinivas Pandruvada)
    
       - Fixes and cleanups in the cpufreq core and generic governor code
         (Rafael Wysocki, Sai Gurrappadi)
    
       - intel_pstate driver optimizations and cleanups (Rafael Wysocki,
         Philippe Longepe, Chen Yu, Joe Perches)
    
       - cpufreq powernv driver fixes and cleanups (Akshay Adiga, Shilpasri
         Bhat)
    
       - cpufreq qoriq driver fixes and cleanups (Jia Hongtao)
    
       - ACPI cpufreq driver cleanups (Viresh Kumar)
    
       - Assorted cpufreq driver updates (Ashwin Chaugule, Geliang Tang,
         Javier Martinez Canillas, Paul Gortmaker, Sudeep Holla)
    
       - Assorted cpufreq fixes and cleanups (Joe Perches, Arnd Bergmann)
    
       - Fixes and cleanups in the OPP (Operating Performance Points)
         framework, mostly related to OPP sharing, and reorganization of
         OF-dependent code in it (Viresh Kumar, Arnd Bergmann, Sudeep Holla)
    
       - New "passive" governor for devfreq (for SoC subsystems that will
         rely on someone else for the management of their power resources)
         and consolidation of devfreq support for Exynos platforms, coding
         style and typo fixes for devfreq (Chanwoo Choi, MyungJoo Ham)
    
       - PM core fixes and cleanups, mostly to make it work better with the
         generic power domains (genpd) framework, and updates for that
         framework (Ulf Hansson, Thierry Reding, Colin Ian King)
    
       - Intel Broxton support for the intel_idle driver (Len Brown)
    
       - cpuidle core optimization and fix (Daniel Lezcano, Dave Gerlach)
    
       - ARM cpuidle cleanups (Jisheng Zhang)
    
       - Intel Kabylake support for the RAPL power capping driver (Jacob
         Pan)
    
       - AVS (Adaptive Voltage Switching) rockchip-io driver update (Heiko
         Stuebner)
    
       - Updates for the cpupower tool (Arjun Sreedharan, Colin Ian King,
         Mattia Dongili, Thomas Renninger)"
    
    * tag 'pm-4.7-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (112 commits)
      intel_pstate: Clean up get_target_pstate_use_performance()
      intel_pstate: Use sample.core_avg_perf in get_avg_pstate()
      intel_pstate: Clarify average performance computation
      intel_pstate: Avoid unnecessary synchronize_sched() during initialization
      cpufreq: schedutil: Make default depend on CONFIG_SMP
      cpufreq: powernv: del_timer_sync when global and local pstate are equal
      cpufreq: powernv: Move smp_call_function_any() out of irq safe block
      intel_pstate: Clean up intel_pstate_get()
      cpufreq: schedutil: Make it depend on CONFIG_SMP
      cpufreq: governor: Fix handling of special cases in dbs_update()
      PM / OPP: Move CONFIG_OF dependent code in a separate file
      cpufreq: intel_pstate: Ignore _PPC processing under HWP
      cpufreq: arm_big_little: use generic OPP functions for {init, free}_opp_table
      PM / OPP: add non-OF versions of dev_pm_opp_{cpumask_, }remove_table
      cpufreq: tango: Use generic platdev driver
      PM / OPP: pass cpumask by reference
      cpufreq: Fix GOV_LIMITS handling for the userspace governor
      cpupower: fix potential memory leak
      PM / devfreq: style/typo fixes
      PM / devfreq: exynos: Add the detailed correlation for Exynos5422 bus
      ..

commit 59efa0bac9cf8b2ef8d08f7632826c6d90f6a9bb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 10 18:24:37 2016 +0200

    sched/core: Kill sched_class::task_waking to clean up the migration logic
    
    With sched_class::task_waking being called only when we do
    set_task_cpu(), we can make sched_class::migrate_task_rq() do the work
    and eliminate sched_class::task_waking entirely.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Hunter <ahh@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Pavan Kondeti <pkondeti@codeaurora.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: byungchul.park@lge.com
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ab6adb159e23..e51145e76807 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1168,7 +1168,7 @@ extern const u32 sched_prio_to_wmult[40];
  *
  * ENQUEUE_HEAD      - place at front of runqueue (tail if not specified)
  * ENQUEUE_REPLENISH - CBS (replenish runtime and postpone deadline)
- * ENQUEUE_WAKING    - sched_class::task_waking was called
+ * ENQUEUE_MIGRATED  - the task was migrated during wakeup
  *
  */
 
@@ -1183,9 +1183,9 @@ extern const u32 sched_prio_to_wmult[40];
 #define ENQUEUE_HEAD		0x08
 #define ENQUEUE_REPLENISH	0x10
 #ifdef CONFIG_SMP
-#define ENQUEUE_WAKING		0x20
+#define ENQUEUE_MIGRATED	0x20
 #else
-#define ENQUEUE_WAKING		0x00
+#define ENQUEUE_MIGRATED	0x00
 #endif
 
 #define RETRY_TASK		((void *)-1UL)
@@ -1217,7 +1217,6 @@ struct sched_class {
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
 	void (*migrate_task_rq)(struct task_struct *p);
 
-	void (*task_waking) (struct task_struct *task);
 	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
 
 	void (*set_cpus_allowed)(struct task_struct *p,

commit 4eb867651721228ee2eeae142c53378375303e8b
Merge: eb60b3e5e8df e5ef27d0f5ac
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu May 12 09:51:36 2016 +0200

    Merge branch 'smp/hotplug' into sched/core, to resolve conflicts
    
    Conflicts:
            kernel/sched/core.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 20a5c8cc74ade5027c2b0e2bc724278afd6054f3
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Mar 10 12:54:20 2016 +0100

    sched/fair: Make ilb_notifier an explicit call
    
    No need for an extra notifier.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: rt@linutronix.de
    Link: http://lkml.kernel.org/r/20160310120025.693720241@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ec2e8d23527e..16a27b624ee5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1743,6 +1743,10 @@ enum rq_nohz_flag_bits {
 };
 
 #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
+
+extern void nohz_balance_exit_idle(unsigned int cpu);
+#else
+static inline void nohz_balance_exit_idle(unsigned int cpu) { }
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING

commit 172895e6b5216eba3e0880460829a8baeefd55f3
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Tue Apr 5 12:12:27 2016 +0800

    sched/fair: Rename SCHED_LOAD_SHIFT to NICE_0_LOAD_SHIFT and remove SCHED_LOAD_SCALE
    
    After cleaning up the sched metrics, there are two definitions that are
    ambiguous and confusing: SCHED_LOAD_SHIFT and SCHED_LOAD_SHIFT.
    
    Resolve this:
    
     - Rename SCHED_LOAD_SHIFT to NICE_0_LOAD_SHIFT, which better reflects what
       it is.
    
     - Replace SCHED_LOAD_SCALE use with SCHED_CAPACITY_SCALE and remove SCHED_LOAD_SCALE.
    
    Suggested-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dietmar.eggemann@arm.com
    Cc: lizefan@huawei.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1459829551-21625-3-git-send-email-yuyang.du@intel.com
    [ Rewrote the changelog and fixed the build on 32-bit kernels. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ad83361f9e67..d24e91b0a722 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -56,25 +56,25 @@ static inline void cpu_load_update_active(struct rq *this_rq) { }
  * increase coverage and consistency always enable it on 64bit platforms.
  */
 #ifdef CONFIG_64BIT
-# define SCHED_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)
+# define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)
 # define scale_load(w)		((w) << SCHED_FIXEDPOINT_SHIFT)
 # define scale_load_down(w)	((w) >> SCHED_FIXEDPOINT_SHIFT)
 #else
-# define SCHED_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT)
+# define NICE_0_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT)
 # define scale_load(w)		(w)
 # define scale_load_down(w)	(w)
 #endif
 
-#define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
-
 /*
- * NICE_0's weight (visible to users) and its load (invisible to users) have
- * independent ranges, but they should be well calibrated. We use scale_load()
- * and scale_load_down(w) to convert between them, and the following must be true:
- * scale_load(sched_prio_to_weight[20]) == NICE_0_LOAD
+ * Task weight (visible to users) and its load (invisible to users) have
+ * independent resolution, but they should be well calibrated. We use
+ * scale_load() and scale_load_down(w) to convert between them. The
+ * following must be true:
+ *
+ *  scale_load(sched_prio_to_weight[USER_PRIO(NICE_TO_PRIO(0))]) == NICE_0_LOAD
+ *
  */
-#define NICE_0_LOAD		SCHED_LOAD_SCALE
-#define NICE_0_SHIFT		SCHED_LOAD_SHIFT
+#define NICE_0_LOAD		(1L << NICE_0_LOAD_SHIFT)
 
 /*
  * Single value that decides SCHED_DEADLINE internal math precision.
@@ -863,7 +863,7 @@ DECLARE_PER_CPU(struct sched_domain *, sd_asym);
 struct sched_group_capacity {
 	atomic_t ref;
 	/*
-	 * CPU capacity of this group, SCHED_LOAD_SCALE being max capacity
+	 * CPU capacity of this group, SCHED_CAPACITY_SCALE being max capacity
 	 * for a single CPU.
 	 */
 	unsigned int capacity;

commit 6ecdd74962f246dfe8750b7bea481a1c0816315d
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Tue Apr 5 12:12:26 2016 +0800

    sched/fair: Generalize the load/util averages resolution definition
    
    Integer metric needs fixed point arithmetic. In sched/fair, a few
    metrics, e.g., weight, load, load_avg, util_avg, freq, and capacity,
    may have different fixed point ranges, which makes their update and
    usage error-prone.
    
    In order to avoid the errors relating to the fixed point range, we
    definie a basic fixed point range, and then formalize all metrics to
    base on the basic range.
    
    The basic range is 1024 or (1 << 10). Further, one can recursively
    apply the basic range to have larger range.
    
    Pointed out by Ben Segall, weight (visible to user, e.g., NICE-0 has
    1024) and load (e.g., NICE_0_LOAD) have independent ranges, but they
    must be well calibrated.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: lizefan@huawei.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1459829551-21625-2-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 066a4c2d2695..ad83361f9e67 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -56,18 +56,23 @@ static inline void cpu_load_update_active(struct rq *this_rq) { }
  * increase coverage and consistency always enable it on 64bit platforms.
  */
 #ifdef CONFIG_64BIT
-# define SCHED_LOAD_RESOLUTION	10
-# define scale_load(w)		((w) << SCHED_LOAD_RESOLUTION)
-# define scale_load_down(w)	((w) >> SCHED_LOAD_RESOLUTION)
+# define SCHED_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT + SCHED_FIXEDPOINT_SHIFT)
+# define scale_load(w)		((w) << SCHED_FIXEDPOINT_SHIFT)
+# define scale_load_down(w)	((w) >> SCHED_FIXEDPOINT_SHIFT)
 #else
-# define SCHED_LOAD_RESOLUTION	0
+# define SCHED_LOAD_SHIFT	(SCHED_FIXEDPOINT_SHIFT)
 # define scale_load(w)		(w)
 # define scale_load_down(w)	(w)
 #endif
 
-#define SCHED_LOAD_SHIFT	(10 + SCHED_LOAD_RESOLUTION)
 #define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
 
+/*
+ * NICE_0's weight (visible to users) and its load (invisible to users) have
+ * independent ranges, but they should be well calibrated. We use scale_load()
+ * and scale_load_down(w) to convert between them, and the following must be true:
+ * scale_load(sched_prio_to_weight[20]) == NICE_0_LOAD
+ */
 #define NICE_0_LOAD		SCHED_LOAD_SCALE
 #define NICE_0_SHIFT		SCHED_LOAD_SHIFT
 

commit 2159197d66770ec01f75c93fb11dc66df81fd45b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Apr 28 12:49:38 2016 +0200

    sched/core: Enable increased load resolution on 64-bit kernels
    
    Mike ran into the low load resolution limitation on his big machine.
    
    So reenable these bits; nobody could ever reproduce/analyze the
    reported power usage claim and Google has been running with this for
    years as well.
    
    Reported-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Tested-by: Mike Galbraith <umgwanakikbuti@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0b6a838e9e73..066a4c2d2695 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -49,11 +49,13 @@ static inline void cpu_load_update_active(struct rq *this_rq) { }
  * and does not change the user-interface for setting shares/weights.
  *
  * We increase resolution only if we have enough bits to allow this increased
- * resolution (i.e. BITS_PER_LONG > 32). The costs for increasing resolution
- * when BITS_PER_LONG <= 32 are pretty high and the returns do not justify the
- * increased costs.
+ * resolution (i.e. 64bit). The costs for increasing resolution when 32bit are
+ * pretty high and the returns do not justify the increased costs.
+ *
+ * Really only required when CONFIG_FAIR_GROUP_SCHED is also set, but to
+ * increase coverage and consistency always enable it on 64bit platforms.
  */
-#if 0 /* BITS_PER_LONG > 32 -- currently broken: it increases power usage under light load  */
+#ifdef CONFIG_64BIT
 # define SCHED_LOAD_RESOLUTION	10
 # define scale_load(w)		((w) << SCHED_LOAD_RESOLUTION)
 # define scale_load_down(w)	((w) >> SCHED_LOAD_RESOLUTION)

commit e7904a28f5331c21d17af638cb477c83662e3cb6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Aug 1 19:25:08 2015 +0200

    locking/lockdep, sched/core: Implement a better lock pinning scheme
    
    The problem with the existing lock pinning is that each pin is of
    value 1; this mean you can simply unpin if you know its pinned,
    without having any extra information.
    
    This scheme generates a random (16 bit) cookie for each pin and
    requires this same cookie to unpin. This means you have to keep the
    cookie in context.
    
    No objsize difference for !LOCKDEP kernels.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index a5eecb1e5e4b..0b6a838e9e73 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1202,7 +1202,8 @@ struct sched_class {
 	 * tasks.
 	 */
 	struct task_struct * (*pick_next_task) (struct rq *rq,
-						struct task_struct *prev);
+						struct task_struct *prev,
+						struct pin_cookie cookie);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
@@ -1453,6 +1454,7 @@ static inline void sched_avg_update(struct rq *rq) { }
 
 struct rq_flags {
 	unsigned long flags;
+	struct pin_cookie cookie;
 };
 
 struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
@@ -1464,7 +1466,7 @@ struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)
 	__releases(rq->lock)
 {
-	lockdep_unpin_lock(&rq->lock);
+	lockdep_unpin_lock(&rq->lock, rf->cookie);
 	raw_spin_unlock(&rq->lock);
 }
 
@@ -1473,7 +1475,7 @@ task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
 	__releases(rq->lock)
 	__releases(p->pi_lock)
 {
-	lockdep_unpin_lock(&rq->lock);
+	lockdep_unpin_lock(&rq->lock, rf->cookie);
 	raw_spin_unlock(&rq->lock);
 	raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
 }

commit eb58075149b7f0300ff19142e6245fe75db2a081
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jul 31 21:28:18 2015 +0200

    sched/core: Introduce 'struct rq_flags'
    
    In order to be able to pass around more than just the IRQ flags in the
    future, add a rq_flags structure.
    
    No difference in code generation for the x86_64-defconfig build I
    tested.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index aab4cf05d48a..a5eecb1e5e4b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1451,13 +1451,17 @@ static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }
 static inline void sched_avg_update(struct rq *rq) { }
 #endif
 
-struct rq *__task_rq_lock(struct task_struct *p)
+struct rq_flags {
+	unsigned long flags;
+};
+
+struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 	__acquires(rq->lock);
-struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)
+struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)
 	__acquires(p->pi_lock)
 	__acquires(rq->lock);
 
-static inline void __task_rq_unlock(struct rq *rq)
+static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)
 	__releases(rq->lock)
 {
 	lockdep_unpin_lock(&rq->lock);
@@ -1465,13 +1469,13 @@ static inline void __task_rq_unlock(struct rq *rq)
 }
 
 static inline void
-task_rq_unlock(struct rq *rq, struct task_struct *p, unsigned long *flags)
+task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)
 	__releases(rq->lock)
 	__releases(p->pi_lock)
 {
 	lockdep_unpin_lock(&rq->lock);
 	raw_spin_unlock(&rq->lock);
-	raw_spin_unlock_irqrestore(&p->pi_lock, *flags);
+	raw_spin_unlock_irqrestore(&p->pi_lock, rf->flags);
 }
 
 #ifdef CONFIG_SMP

commit 3e71a462dd483ce508a723356b293731e7d788ea
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Apr 28 16:16:33 2016 +0200

    sched/core: Move task_rq_lock() out of line
    
    Its a rather large function, inline doesn't seems to make much sense:
    
     $ size defconfig-build/kernel/sched/core.o{.orig,}
        text    data     bss     dec     hex filename
       56533   21037    2320   79890   13812 defconfig-build/kernel/sched/core.o.orig
       55733   21037    2320   79090   134f2 defconfig-build/kernel/sched/core.o
    
    The 'perf bench sched messaging' micro-benchmark shows a visible improvement
    of 4-5%:
    
      $ for i in /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor ; do echo performance > $i ; done
      $ perf stat --null --repeat 25 -- perf bench sched messaging -g 40 -l 5000
    
      pre:
           4.582798193 seconds time elapsed          ( +-  1.41% )
           4.733374877 seconds time elapsed          ( +-  2.10% )
           4.560955136 seconds time elapsed          ( +-  1.43% )
           4.631062303 seconds time elapsed          ( +-  1.40% )
    
      post:
           4.364765213 seconds time elapsed          ( +-  0.91% )
           4.454442734 seconds time elapsed          ( +-  1.18% )
           4.448893817 seconds time elapsed          ( +-  1.41% )
           4.424346872 seconds time elapsed          ( +-  0.97% )
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 69da6fcaa0e8..aab4cf05d48a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1451,70 +1451,11 @@ static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }
 static inline void sched_avg_update(struct rq *rq) { }
 #endif
 
-/*
- * __task_rq_lock - lock the rq @p resides on.
- */
-static inline struct rq *__task_rq_lock(struct task_struct *p)
-	__acquires(rq->lock)
-{
-	struct rq *rq;
-
-	lockdep_assert_held(&p->pi_lock);
-
-	for (;;) {
-		rq = task_rq(p);
-		raw_spin_lock(&rq->lock);
-		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {
-			lockdep_pin_lock(&rq->lock);
-			return rq;
-		}
-		raw_spin_unlock(&rq->lock);
-
-		while (unlikely(task_on_rq_migrating(p)))
-			cpu_relax();
-	}
-}
-
-/*
- * task_rq_lock - lock p->pi_lock and lock the rq @p resides on.
- */
-static inline struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)
+struct rq *__task_rq_lock(struct task_struct *p)
+	__acquires(rq->lock);
+struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)
 	__acquires(p->pi_lock)
-	__acquires(rq->lock)
-{
-	struct rq *rq;
-
-	for (;;) {
-		raw_spin_lock_irqsave(&p->pi_lock, *flags);
-		rq = task_rq(p);
-		raw_spin_lock(&rq->lock);
-		/*
-		 *	move_queued_task()		task_rq_lock()
-		 *
-		 *	ACQUIRE (rq->lock)
-		 *	[S] ->on_rq = MIGRATING		[L] rq = task_rq()
-		 *	WMB (__set_task_cpu())		ACQUIRE (rq->lock);
-		 *	[S] ->cpu = new_cpu		[L] task_rq()
-		 *					[L] ->on_rq
-		 *	RELEASE (rq->lock)
-		 *
-		 * If we observe the old cpu in task_rq_lock, the acquire of
-		 * the old rq->lock will fully serialize against the stores.
-		 *
-		 * If we observe the new cpu in task_rq_lock, the acquire will
-		 * pair with the WMB to ensure we must then also see migrating.
-		 */
-		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {
-			lockdep_pin_lock(&rq->lock);
-			return rq;
-		}
-		raw_spin_unlock(&rq->lock);
-		raw_spin_unlock_irqrestore(&p->pi_lock, *flags);
-
-		while (unlikely(task_on_rq_migrating(p)))
-			cpu_relax();
-	}
-}
+	__acquires(rq->lock);
 
 static inline void __task_rq_unlock(struct rq *rq)
 	__releases(rq->lock)

commit 9fd81dd5ce0b12341c9f83346f8d32ac68bd3841
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Apr 19 17:36:51 2016 +0200

    sched/fair: Optimize !CONFIG_NO_HZ_COMMON CPU load updates
    
    Some code in CPU load update only concern NO_HZ configs but it is
    built on all configurations. When NO_HZ isn't built, that code is harmless
    but just happens to take some useless ressources in CPU and memory:
    
    1) one useless field in struct rq
    2) jiffies record on every tick that is never used (cpu_load_update_periodic)
    3) decay_load_missed is called two times on every tick to eventually
       return immediately with no action taken. And that function is dead
       code.
    
    For pure optimization purposes, lets conditionally build the NO_HZ
    related code.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1461080211-16271-1-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 32d9e22cfacf..69da6fcaa0e8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -585,11 +585,13 @@ struct rq {
 #endif
 	#define CPU_LOAD_IDX_MAX 5
 	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
-	unsigned long last_load_update_tick;
 #ifdef CONFIG_NO_HZ_COMMON
+#ifdef CONFIG_SMP
+	unsigned long last_load_update_tick;
+#endif /* CONFIG_SMP */
 	u64 nohz_stamp;
 	unsigned long nohz_flags;
-#endif
+#endif /* CONFIG_NO_HZ_COMMON */
 #ifdef CONFIG_NO_HZ_FULL
 	unsigned long last_sched_tick;
 #endif

commit cee1afce3053e7aa0793fbd5f2e845fa2cef9e33
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Apr 13 15:56:50 2016 +0200

    sched/fair: Gather CPU load functions under a more conventional namespace
    
    The CPU load update related functions have a weak naming convention
    currently, starting with update_cpu_load_*() which isn't ideal as
    "update" is a very generic concept.
    
    Since two of these functions are public already (and a third is to come)
    that's enough to introduce a more conventional naming scheme. So let's
    do the following rename instead:
    
            update_cpu_load_*() -> cpu_load_update_*()
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E . McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1460555812-25375-2-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index a7cbad7b3ad2..32d9e22cfacf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -31,9 +31,9 @@ extern void calc_global_load_tick(struct rq *this_rq);
 extern long calc_load_fold_active(struct rq *this_rq);
 
 #ifdef CONFIG_SMP
-extern void update_cpu_load_active(struct rq *this_rq);
+extern void cpu_load_update_active(struct rq *this_rq);
 #else
-static inline void update_cpu_load_active(struct rq *this_rq) { }
+static inline void cpu_load_update_active(struct rq *this_rq) { }
 #endif
 
 /*

commit 9bdcb44e391da5c41b98573bf0305a0e0b1c9569
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Apr 2 01:09:12 2016 +0200

    cpufreq: schedutil: New governor based on scheduler utilization data
    
    Add a new cpufreq scaling governor, called "schedutil", that uses
    scheduler-provided CPU utilization information as input for making
    its decisions.
    
    Doing that is possible after commit 34e2c555f3e1 (cpufreq: Add
    mechanism for registering utilization update callbacks) that
    introduced cpufreq_update_util() called by the scheduler on
    utilization changes (from CFS) and RT/DL task status updates.
    In particular, CPU frequency scaling decisions may be based on
    the the utilization data passed to cpufreq_update_util() by CFS.
    
    The new governor is relatively simple.
    
    The frequency selection formula used by it depends on whether or not
    the utilization is frequency-invariant.  In the frequency-invariant
    case the new CPU frequency is given by
    
            next_freq = 1.25 * max_freq * util / max
    
    where util and max are the last two arguments of cpufreq_update_util().
    In turn, if util is not frequency-invariant, the maximum frequency in
    the above formula is replaced with the current frequency of the CPU:
    
            next_freq = 1.25 * curr_freq * util / max
    
    The coefficient 1.25 corresponds to the frequency tipping point at
    (util / max) = 0.8.
    
    All of the computations are carried out in the utilization update
    handlers provided by the new governor.  One of those handlers is
    used for cpufreq policies shared between multiple CPUs and the other
    one is for policies with one CPU only (and therefore it doesn't need
    to use any extra synchronization means).
    
    The governor supports fast frequency switching if that is supported
    by the cpufreq driver in use and possible for the given policy.
    In the fast switching case, all operations of the governor take
    place in its utilization update handlers.  If fast switching cannot
    be used, the frequency switch operations are carried out with the
    help of a work item which only calls __cpufreq_driver_target()
    (under a mutex) to trigger a frequency update (to a value already
    computed beforehand in one of the utilization update handlers).
    
    Currently, the governor treats all of the RT and DL tasks as
    "unknown utilization" and sets the frequency to the allowed
    maximum when updated from the RT or DL sched classes.  That
    heavy-handed approach should be replaced with something more
    subtle and specifically targeted at RT and DL tasks.
    
    The governor shares some tunables management code with the
    "ondemand" and "conservative" governors and uses some common
    definitions from cpufreq_governor.h, but apart from that it
    is stand-alone.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ec2e8d23527e..921d6e5d33b7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1842,6 +1842,14 @@ static inline void cpufreq_update_util(u64 time, unsigned long util, unsigned lo
 static inline void cpufreq_trigger_update(u64 time) {}
 #endif /* CONFIG_CPU_FREQ */
 
+#ifdef arch_scale_freq_capacity
+#ifndef arch_scale_freq_invariant
+#define arch_scale_freq_invariant()	(true)
+#endif
+#else /* arch_scale_freq_capacity */
+#define arch_scale_freq_invariant()	(false)
+#endif
+
 static inline void account_reset_rq(struct rq *rq)
 {
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING

commit 2b8c41daba327c633228169e8bd8ec067ab443f8
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Mar 30 04:30:56 2016 +0800

    sched/fair: Initiate a new task's util avg to a bounded value
    
    A new task's util_avg is set to full utilization of a CPU (100% time
    running). This accelerates a new task's utilization ramp-up, useful to
    boost its execution in early time. However, it may result in
    (insanely) high utilization for a transient time period when a flood
    of tasks are spawned. Importantly, it violates the "fundamentally
    bounded" CPU utilization, and its side effect is negative if we don't
    take any measure to bound it.
    
    This patch proposes an algorithm to address this issue. It has
    two methods to approach a sensible initial util_avg:
    
    (1) An expected (or average) util_avg based on its cfs_rq's util_avg:
    
      util_avg = cfs_rq->util_avg / (cfs_rq->load_avg + 1) * se.load.weight
    
    (2) A trajectory of how successive new tasks' util develops, which
    gives 1/2 of the left utilization budget to a new task such that
    the additional util is noticeably large (when overall util is low) or
    unnoticeably small (when overall util is high enough). In the meantime,
    the aggregate utilization is well bounded:
    
      util_avg_cap = (1024 - cfs_rq->avg.util_avg) / 2^n
    
    where n denotes the nth task.
    
    If util_avg is larger than util_avg_cap, then the effective util is
    clamped to the util_avg_cap.
    
    Reported-by: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: steve.muckle@linaro.org
    Link: http://lkml.kernel.org/r/1459283456-21682-1-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ec2e8d23527e..a7cbad7b3ad2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1313,6 +1313,7 @@ extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);
+extern void post_init_entity_util_avg(struct sched_entity *se);
 
 #ifdef CONFIG_NO_HZ_FULL
 extern bool sched_can_stop_tick(struct rq *rq);

commit be53f58fa0fcd97c62a84f2eb98cff528f8b2443
Merge: 19d6f04cd374 73e6aafd9ea8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 24 09:42:50 2016 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Misc fixes: a cgroup fix, a fair-scheduler migration accounting fix, a
      cputime fix and two cpuacct cleanups"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/cpuacct: Simplify the cpuacct code
      sched/cpuacct: Rename parameter in cpuusage_write() for readability
      sched/fair: Add comments to explain select_idle_sibling()
      sched/fair: Fix fairness issue on migration
      sched/cgroup: Fix/cleanup cgroup teardown/init
      sched/cputime: Fix steal time accounting vs. CPU hotplug

commit 42e405f7b1d252c90a2468dd2140f47b8142b7a0
Merge: e9532e69b8d1 710d60cbf1b3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Mar 21 10:47:40 2016 +0100

    Merge branch 'linus' into sched/urgent, to pick up dependencies
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 277edbabf6fece057b14fb6db5e3a34e00f42f42
Merge: 271ecc5253e2 0d571b62dd8e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Mar 16 14:10:53 2016 -0700

    Merge tag 'pm+acpi-4.6-rc1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management and ACPI updates from Rafael Wysocki:
     "This time the majority of changes go into cpufreq and they are
      significant.
    
      First off, the way CPU frequency updates are triggered is different
      now.  Instead of having to set up and manage a deferrable timer for
      each CPU in the system to evaluate and possibly change its frequency
      periodically, cpufreq governors set up callbacks to be invoked by the
      scheduler on a regular basis (basically on utilization updates).  The
      "old" governors, "ondemand" and "conservative", still do all of their
      work in process context (although that is triggered by the scheduler
      now), but intel_pstate does it all in the callback invoked by the
      scheduler with no need for any additional asynchronous processing.
    
      Of course, this eliminates the overhead related to the management of
      all those timers, but also it allows the cpufreq governor code to be
      simplified quite a bit.  On top of that, the common code and data
      structures used by the "ondemand" and "conservative" governors are
      cleaned up and made more straightforward and some long-standing and
      quite annoying problems are addressed.  In particular, the handling of
      governor sysfs attributes is modified and the related locking becomes
      more fine grained which allows some concurrency problems to be avoided
      (particularly deadlocks with the core cpufreq code).
    
      In principle, the new mechanism for triggering frequency updates
      allows utilization information to be passed from the scheduler to
      cpufreq.  Although the current code doesn't make use of it, in the
      works is a new cpufreq governor that will make decisions based on the
      scheduler's utilization data.  That should allow the scheduler and
      cpufreq to work more closely together in the long run.
    
      In addition to the core and governor changes, cpufreq drivers are
      updated too.  Fixes and optimizations go into intel_pstate, the
      cpufreq-dt driver is updated on top of some modification in the
      Operating Performance Points (OPP) framework and there are fixes and
      other updates in the powernv cpufreq driver.
    
      Apart from the cpufreq updates there is some new ACPICA material,
      including a fix for a problem introduced by previous ACPICA updates,
      and some less significant changes in the ACPI code, like CPPC code
      optimizations, ACPI processor driver cleanups and support for loading
      ACPI tables from initrd.
    
      Also updated are the generic power domains framework, the Intel RAPL
      power capping driver and the turbostat utility and we have a bunch of
      traditional assorted fixes and cleanups.
    
      Specifics:
    
       - Redesign of cpufreq governors and the intel_pstate driver to make
         them use callbacks invoked by the scheduler to trigger CPU
         frequency evaluation instead of using per-CPU deferrable timers for
         that purpose (Rafael Wysocki).
    
       - Reorganization and cleanup of cpufreq governor code to make it more
         straightforward and fix some concurrency problems in it (Rafael
         Wysocki, Viresh Kumar).
    
       - Cleanup and improvements of locking in the cpufreq core (Viresh
         Kumar).
    
       - Assorted cleanups in the cpufreq core (Rafael Wysocki, Viresh
         Kumar, Eric Biggers).
    
       - intel_pstate driver updates including fixes, optimizations and a
         modification to make it enable enable hardware-coordinated P-state
         selection (HWP) by default if supported by the processor (Philippe
         Longepe, Srinivas Pandruvada, Rafael Wysocki, Viresh Kumar, Felipe
         Franciosi).
    
       - Operating Performance Points (OPP) framework updates to improve its
         handling of voltage regulators and device clocks and updates of the
         cpufreq-dt driver on top of that (Viresh Kumar, Jon Hunter).
    
       - Updates of the powernv cpufreq driver to fix initialization and
         cleanup problems in it and correct its worker thread handling with
         respect to CPU offline, new powernv_throttle tracepoint (Shilpasri
         Bhat).
    
       - ACPI cpufreq driver optimization and cleanup (Rafael Wysocki).
    
       - ACPICA updates including one fix for a regression introduced by
         previos changes in the ACPICA code (Bob Moore, Lv Zheng, David Box,
         Colin Ian King).
    
       - Support for installing ACPI tables from initrd (Lv Zheng).
    
       - Optimizations of the ACPI CPPC code (Prashanth Prakash, Ashwin
         Chaugule).
    
       - Support for _HID(ACPI0010) devices (ACPI processor containers) and
         ACPI processor driver cleanups (Sudeep Holla).
    
       - Support for ACPI-based enumeration of the AMBA bus (Graeme Gregory,
         Aleksey Makarov).
    
       - Modification of the ACPI PCI IRQ management code to make it treat
         255 in the Interrupt Line register as "not connected" on x86 (as
         per the specification) and avoid attempts to use that value as a
         valid interrupt vector (Chen Fan).
    
       - ACPI APEI fixes related to resource leaks (Josh Hunt).
    
       - Removal of modularity from a few ACPI drivers (BGRT, GHES,
         intel_pmic_crc) that cannot be built as modules in practice (Paul
         Gortmaker).
    
       - PNP framework update to make it treat ACPI_RESOURCE_TYPE_SERIAL_BUS
         as a valid resource type (Harb Abdulhamid).
    
       - New device ID (future AMD I2C controller) in the ACPI driver for
         AMD SoCs (APD) and in the designware I2C driver (Xiangliang Yu).
    
       - Assorted ACPI cleanups (Colin Ian King, Kaiyen Chang, Oleg Drokin).
    
       - cpuidle menu governor optimization to avoid a square root
         computation in it (Rasmus Villemoes).
    
       - Fix for potential use-after-free in the generic device properties
         framework (Heikki Krogerus).
    
       - Updates of the generic power domains (genpd) framework including
         support for multiple power states of a domain, fixes and debugfs
         output improvements (Axel Haslam, Jon Hunter, Laurent Pinchart,
         Geert Uytterhoeven).
    
       - Intel RAPL power capping driver updates to reduce IPI overhead in
         it (Jacob Pan).
    
       - System suspend/hibernation code cleanups (Eric Biggers, Saurabh
         Sengar).
    
       - Year 2038 fix for the process freezer (Abhilash Jindal).
    
       - turbostat utility updates including new features (decoding of more
         registers and CPUID fields, sub-second intervals support, GFX MHz
         and RC6 printout, --out command line option), fixes (syscall jitter
         detection and workaround, reductioin of the number of syscalls
         made, fixes related to Xeon x200 processors, compiler warning
         fixes) and cleanups (Len Brown, Hubert Chrzaniuk, Chen Yu)"
    
    * tag 'pm+acpi-4.6-rc1-1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (182 commits)
      tools/power turbostat: bugfix: TDP MSRs print bits fixing
      tools/power turbostat: correct output for MSR_NHM_SNB_PKG_CST_CFG_CTL dump
      tools/power turbostat: call __cpuid() instead of __get_cpuid()
      tools/power turbostat: indicate SMX and SGX support
      tools/power turbostat: detect and work around syscall jitter
      tools/power turbostat: show GFX%rc6
      tools/power turbostat: show GFXMHz
      tools/power turbostat: show IRQs per CPU
      tools/power turbostat: make fewer systems calls
      tools/power turbostat: fix compiler warnings
      tools/power turbostat: add --out option for saving output in a file
      tools/power turbostat: re-name "%Busy" field to "Busy%"
      tools/power turbostat: Intel Xeon x200: fix turbo-ratio decoding
      tools/power turbostat: Intel Xeon x200: fix erroneous bclk value
      tools/power turbostat: allow sub-sec intervals
      ACPI / APEI: ERST: Fixed leaked resources in erst_init
      ACPI / APEI: Fix leaked resources
      intel_pstate: Do not skip samples partially
      intel_pstate: Remove freq calculation from intel_pstate_calc_busy()
      intel_pstate: Move intel_pstate_calc_busy() into get_target_pstate_use_performance()
      ...

commit e23604edac2a7be6a8808a5d13fac6b9df4eb9a8
Merge: d4e796152a04 1f25184656a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Mar 14 19:44:38 2016 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull NOHZ updates from Ingo Molnar:
     "NOHZ enhancements, by Frederic Weisbecker, which reorganizes/refactors
      the NOHZ 'can the tick be stopped?' infrastructure and related code to
      be data driven, and harmonizes the naming and handling of all the
      various properties"
    
    [ This makes the ugly "fetch_or()" macro that the scheduler used
      internally a new generic helper, and does a bad job at it.
    
      I'm pulling it, but I've asked Ingo and Frederic to get this
      fixed up ]
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched-clock: Migrate to use new tick dependency mask model
      posix-cpu-timers: Migrate to use new tick dependency mask model
      sched: Migrate sched to use new tick dependency mask model
      sched: Account rr tasks
      perf: Migrate perf to use new tick dependency mask model
      nohz: Use enum code for tick stop failure tracing message
      nohz: New tick dependency mask
      nohz: Implement wide kick on top of irq work
      atomic: Export fetch_or()

commit adaf9fcd136970e480d7ca834c0cf25ce922ea74
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 10 20:44:47 2016 +0100

    cpufreq: Move scheduler-related code to the sched directory
    
    Create cpufreq.c under kernel/sched/ and move the cpufreq code
    related to the scheduler to that file and to sched.h.
    
    Redefine cpufreq_update_util() as a static inline function to avoid
    function calls at its call sites in the scheduler code (as suggested
    by Peter Zijlstra).
    
    Also move the definition of struct update_util_data and declaration
    of cpufreq_set_update_util_data() from include/linux/cpufreq.h to
    include/linux/sched.h.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f042190c8002..faf7e2758dd0 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -9,7 +9,6 @@
 #include <linux/irq_work.h>
 #include <linux/tick.h>
 #include <linux/slab.h>
-#include <linux/cpufreq.h>
 
 #include "cpupri.h"
 #include "cpudeadline.h"
@@ -1739,3 +1738,51 @@ static inline u64 irq_time_read(int cpu)
 }
 #endif /* CONFIG_64BIT */
 #endif /* CONFIG_IRQ_TIME_ACCOUNTING */
+
+#ifdef CONFIG_CPU_FREQ
+DECLARE_PER_CPU(struct update_util_data *, cpufreq_update_util_data);
+
+/**
+ * cpufreq_update_util - Take a note about CPU utilization changes.
+ * @time: Current time.
+ * @util: Current utilization.
+ * @max: Utilization ceiling.
+ *
+ * This function is called by the scheduler on every invocation of
+ * update_load_avg() on the CPU whose utilization is being updated.
+ *
+ * It can only be called from RCU-sched read-side critical sections.
+ */
+static inline void cpufreq_update_util(u64 time, unsigned long util, unsigned long max)
+{
+       struct update_util_data *data;
+
+       data = rcu_dereference_sched(*this_cpu_ptr(&cpufreq_update_util_data));
+       if (data)
+               data->func(data, time, util, max);
+}
+
+/**
+ * cpufreq_trigger_update - Trigger CPU performance state evaluation if needed.
+ * @time: Current time.
+ *
+ * The way cpufreq is currently arranged requires it to evaluate the CPU
+ * performance state (frequency/voltage) on a regular basis to prevent it from
+ * being stuck in a completely inadequate performance level for too long.
+ * That is not guaranteed to happen if the updates are only triggered from CFS,
+ * though, because they may not be coming in if RT or deadline tasks are active
+ * all the time (or there are RT and DL tasks only).
+ *
+ * As a workaround for that issue, this function is called by the RT and DL
+ * sched classes to trigger extra cpufreq updates to prevent it from stalling,
+ * but that really is a band-aid.  Going forward it should be replaced with
+ * solutions targeted more specifically at RT and DL tasks.
+ */
+static inline void cpufreq_trigger_update(u64 time)
+{
+	cpufreq_update_util(time, ULONG_MAX, 0);
+}
+#else
+static inline void cpufreq_update_util(u64 time, unsigned long util, unsigned long max) {}
+static inline void cpufreq_trigger_update(u64 time) {}
+#endif /* CONFIG_CPU_FREQ */

commit 34e2c555f3e13c90e9284e23d00f03be8a6e06c5
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Feb 15 20:20:42 2016 +0100

    cpufreq: Add mechanism for registering utilization update callbacks
    
    Introduce a mechanism by which parts of the cpufreq subsystem
    ("setpolicy" drivers or the core) can register callbacks to be
    executed from cpufreq_update_util() which is invoked by the
    scheduler's update_load_avg() on CPU utilization changes.
    
    This allows the "setpolicy" drivers to dispense with their timers
    and do all of the computations they need and frequency/voltage
    adjustments in the update_load_avg() code path, among other things.
    
    The update_load_avg() changes were suggested by Peter Zijlstra.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Viresh Kumar <viresh.kumar@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 10f16374df7f..f042190c8002 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -9,6 +9,7 @@
 #include <linux/irq_work.h>
 #include <linux/tick.h>
 #include <linux/slab.h>
+#include <linux/cpufreq.h>
 
 #include "cpupri.h"
 #include "cpudeadline.h"

commit e9532e69b8d1d1284e8ecf8d2586de34aec61244
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Mar 4 15:59:42 2016 +0100

    sched/cputime: Fix steal time accounting vs. CPU hotplug
    
    On CPU hotplug the steal time accounting can keep a stale rq->prev_steal_time
    value over CPU down and up. So after the CPU comes up again the delta
    calculation in steal_account_process_tick() wreckages itself due to the
    unsigned math:
    
             u64 steal = paravirt_steal_clock(smp_processor_id());
    
             steal -= this_rq()->prev_steal_time;
    
    So if steal is smaller than rq->prev_steal_time we end up with an insane large
    value which then gets added to rq->prev_steal_time, resulting in a permanent
    wreckage of the accounting. As a consequence the per CPU stats in /proc/stat
    become stale.
    
    Nice trick to tell the world how idle the system is (100%) while the CPU is
    100% busy running tasks. Though we prefer realistic numbers.
    
    None of the accounting values which use a previous value to account for
    fractions is reset at CPU hotplug time. update_rq_clock_task() has a sanity
    check for prev_irq_time and prev_steal_time_rq, but that sanity check solely
    deals with clock warps and limits the /proc/stat visible wreckage. The
    prev_time values are still wrong.
    
    Solution is simple: Reset rq->prev_*_time when the CPU is plugged in again.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: <stable@vger.kernel.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Glauber Costa <glommer@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Fixes: commit 095c0aa83e52 "sched: adjust scheduler cpu power for stolen time"
    Fixes: commit aa483808516c "sched: Remove irq time from available CPU power"
    Fixes: commit e6e6685accfa "KVM guest: Steal time accounting"
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1603041539490.3686@nanos
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 30ea2d871ba7..4f6598ae4c31 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1738,3 +1738,16 @@ static inline u64 irq_time_read(int cpu)
 }
 #endif /* CONFIG_64BIT */
 #endif /* CONFIG_IRQ_TIME_ACCOUNTING */
+
+static inline void account_reset_rq(struct rq *rq)
+{
+#ifdef CONFIG_IRQ_TIME_ACCOUNTING
+	rq->prev_irq_time = 0;
+#endif
+#ifdef CONFIG_PARAVIRT
+	rq->prev_steal_time = 0;
+#endif
+#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
+	rq->prev_steal_time_rq = 0;
+#endif
+}

commit 76d92ac305f23cada3a9b3c48a7ccea5f71019cb
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 17 22:25:49 2015 +0200

    sched: Migrate sched to use new tick dependency mask model
    
    Instead of providing asynchronous checks for the nohz subsystem to verify
    sched tick dependency, migrate sched to the new mask.
    
    Everytime a task is enqueued or dequeued, we evaluate the state of the
    tick dependency on top of the policy of the tasks in the runqueue, by
    order of priority:
    
    SCHED_DEADLINE: Need the tick in order to periodically check for runtime
    SCHED_FIFO    : Don't need the tick (no round-robin)
    SCHED_RR      : Need the tick if more than 1 task of the same priority
                    for round robin (simplified with checking if more than
                    one SCHED_RR task no matter what priority).
    SCHED_NORMAL  : Need the tick if more than 1 task for round-robin.
    
    We could optimize that further with one flag per sched policy on the tick
    dependency mask and perform only the checks relevant to the policy
    concerned by an enqueue/dequeue operation.
    
    Since the checks aren't based on the current task anymore, we could get
    rid of the task switch hook but it's still needed for posix cpu
    timers.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f0abfce14044..4f0bca770108 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1279,6 +1279,35 @@ unsigned long to_ratio(u64 period, u64 runtime);
 
 extern void init_entity_runnable_average(struct sched_entity *se);
 
+#ifdef CONFIG_NO_HZ_FULL
+extern bool sched_can_stop_tick(struct rq *rq);
+
+/*
+ * Tick may be needed by tasks in the runqueue depending on their policy and
+ * requirements. If tick is needed, lets send the target an IPI to kick it out of
+ * nohz mode if necessary.
+ */
+static inline void sched_update_tick_dependency(struct rq *rq)
+{
+	int cpu;
+
+	if (!tick_nohz_full_enabled())
+		return;
+
+	cpu = cpu_of(rq);
+
+	if (!tick_nohz_full_cpu(cpu))
+		return;
+
+	if (sched_can_stop_tick(rq))
+		tick_nohz_dep_clear_cpu(cpu, TICK_DEP_BIT_SCHED);
+	else
+		tick_nohz_dep_set_cpu(cpu, TICK_DEP_BIT_SCHED);
+}
+#else
+static inline void sched_update_tick_dependency(struct rq *rq) { }
+#endif
+
 static inline void add_nr_running(struct rq *rq, unsigned count)
 {
 	unsigned prev_nr = rq->nr_running;
@@ -1290,26 +1319,16 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 		if (!rq->rd->overload)
 			rq->rd->overload = true;
 #endif
-
-#ifdef CONFIG_NO_HZ_FULL
-		if (tick_nohz_full_cpu(rq->cpu)) {
-			/*
-			 * Tick is needed if more than one task runs on a CPU.
-			 * Send the target an IPI to kick it out of nohz mode.
-			 *
-			 * We assume that IPI implies full memory barrier and the
-			 * new value of rq->nr_running is visible on reception
-			 * from the target.
-			 */
-			tick_nohz_full_kick_cpu(rq->cpu);
-		}
-#endif
 	}
+
+	sched_update_tick_dependency(rq);
 }
 
 static inline void sub_nr_running(struct rq *rq, unsigned count)
 {
 	rq->nr_running -= count;
+	/* Check if we still need preemption */
+	sched_update_tick_dependency(rq);
 }
 
 static inline void rq_last_tick_reset(struct rq *rq)

commit 01d36d0ac390895e719d0dd8ab91ebbbf506d28e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 4 18:17:10 2015 +0100

    sched: Account rr tasks
    
    In order to evaluate the scheduler tick dependency without probing
    context switches, we need to know how much SCHED_RR and SCHED_FIFO tasks
    are enqueued as those policies don't have the same preemption
    requirements.
    
    To prepare for that, let's account SCHED_RR tasks, we'll be able to
    deduce SCHED_FIFO tasks as well from it and the total RT tasks in the
    runqueue.
    
    Reviewed-by: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Chris Metcalf <cmetcalf@ezchip.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 10f16374df7f..f0abfce14044 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -450,6 +450,7 @@ static inline int rt_bandwidth_enabled(void)
 struct rt_rq {
 	struct rt_prio_array active;
 	unsigned int rt_nr_running;
+	unsigned int rr_nr_running;
 #if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
 	struct {
 		int curr; /* highest queued rt task prio */

commit 3866e845ed522258c77da2eaa9f849ef55206ca2
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Feb 22 16:26:51 2016 -0500

    sched/debug: Move sched_domain_sysctl to debug.c
    
    The sched_domain_sysctl setup is only enabled when SCHED_DEBUG is
    configured. As debug.c is only compiled when SCHED_DEBUG is configured as
    well, move the setup of sched_domain_sysctl into that file.
    
    Note, the (un)register_sched_domain_sysctl() functions had to be changed
    from static to allow access to them from core.c.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160222212825.599278093@goodmis.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d3606e40ea0d..ef5875fff5b7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -3,6 +3,7 @@
 #include <linux/sched/sysctl.h>
 #include <linux/sched/rt.h>
 #include <linux/sched/deadline.h>
+#include <linux/binfmts.h>
 #include <linux/mutex.h>
 #include <linux/spinlock.h>
 #include <linux/stop_machine.h>
@@ -908,6 +909,18 @@ static inline unsigned int group_first_cpu(struct sched_group *group)
 
 extern int group_balance_cpu(struct sched_group *sg);
 
+#if defined(CONFIG_SCHED_DEBUG) && defined(CONFIG_SYSCTL)
+void register_sched_domain_sysctl(void);
+void unregister_sched_domain_sysctl(void);
+#else
+static inline void register_sched_domain_sysctl(void)
+{
+}
+static inline void unregister_sched_domain_sysctl(void)
+{
+}
+#endif
+
 #else
 
 static inline void sched_ttwu_pending(void) { }

commit ff77e468535987b3d21b7bd4da15608ea3ce7d0b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 18 15:27:07 2016 +0100

    sched/rt: Fix PI handling vs. sched_setscheduler()
    
    Andrea Parri reported:
    
    > I found that the following scenario (with CONFIG_RT_GROUP_SCHED=y) is not
    > handled correctly:
    >
    >     T1 (prio = 20)
    >        lock(rtmutex);
    >
    >     T2 (prio = 20)
    >        blocks on rtmutex  (rt_nr_boosted = 0 on T1's rq)
    >
    >     T1 (prio = 20)
    >        sys_set_scheduler(prio = 0)
    >           [new_effective_prio == oldprio]
    >           T1 prio = 20    (rt_nr_boosted = 0 on T1's rq)
    >
    > The last step is incorrect as T1 is now boosted (c.f., rt_se_boosted());
    > in particular, if we continue with
    >
    >    T1 (prio = 20)
    >       unlock(rtmutex)
    >          wakeup(T2)
    >          adjust_prio(T1)
    >             [prio != rt_mutex_getprio(T1)]
    >           dequeue(T1)
    >              rt_nr_boosted = (unsigned long)(-1)
    >              ...
    >             T1 prio = 0
    >
    > then we end up leaving rt_nr_boosted in an "inconsistent" state.
    >
    > The simple program attached could reproduce the previous scenario; note
    > that, as a consequence of the presence of this state, the "assertion"
    >
    >     WARN_ON(!rt_nr_running && rt_nr_boosted)
    >
    > from dec_rt_group() may trigger.
    
    So normally we dequeue/enqueue tasks in sched_setscheduler(), which
    would ensure the accounting stays correct. However in the early PI path
    we fail to do so.
    
    So this was introduced at around v3.14, by:
    
      c365c292d059 ("sched: Consider pi boosting in setscheduler()")
    
    which fixed another problem exactly because that dequeue/enqueue, joy.
    
    Fix this by teaching rt about DEQUEUE_SAVE/ENQUEUE_RESTORE and have it
    preserve runqueue location with that option. This requires decoupling
    the on_rt_rq() state from being on the list.
    
    In order to allow for explicit movement during the SAVE/RESTORE,
    introduce {DE,EN}QUEUE_MOVE. We still must use SAVE/RESTORE in these
    cases to preserve other invariants.
    
    Respecting the SAVE/RESTORE flags also has the (nice) side-effect that
    things like sys_nice()/sys_sched_setaffinity() also do not reorder
    FIFO tasks (whereas they used to before this patch).
    
    Reported-by: Andrea Parri <parri.andrea@gmail.com>
    Tested-by: Andrea Parri <parri.andrea@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3dc7b8b3f94c..d3606e40ea0d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1130,18 +1130,40 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 extern const int sched_prio_to_weight[40];
 extern const u32 sched_prio_to_wmult[40];
 
+/*
+ * {de,en}queue flags:
+ *
+ * DEQUEUE_SLEEP  - task is no longer runnable
+ * ENQUEUE_WAKEUP - task just became runnable
+ *
+ * SAVE/RESTORE - an otherwise spurious dequeue/enqueue, done to ensure tasks
+ *                are in a known state which allows modification. Such pairs
+ *                should preserve as much state as possible.
+ *
+ * MOVE - paired with SAVE/RESTORE, explicitly does not preserve the location
+ *        in the runqueue.
+ *
+ * ENQUEUE_HEAD      - place at front of runqueue (tail if not specified)
+ * ENQUEUE_REPLENISH - CBS (replenish runtime and postpone deadline)
+ * ENQUEUE_WAKING    - sched_class::task_waking was called
+ *
+ */
+
+#define DEQUEUE_SLEEP		0x01
+#define DEQUEUE_SAVE		0x02 /* matches ENQUEUE_RESTORE */
+#define DEQUEUE_MOVE		0x04 /* matches ENQUEUE_MOVE */
+
 #define ENQUEUE_WAKEUP		0x01
-#define ENQUEUE_HEAD		0x02
+#define ENQUEUE_RESTORE		0x02
+#define ENQUEUE_MOVE		0x04
+
+#define ENQUEUE_HEAD		0x08
+#define ENQUEUE_REPLENISH	0x10
 #ifdef CONFIG_SMP
-#define ENQUEUE_WAKING		0x04	/* sched_class::task_waking was called */
+#define ENQUEUE_WAKING		0x20
 #else
 #define ENQUEUE_WAKING		0x00
 #endif
-#define ENQUEUE_REPLENISH	0x08
-#define ENQUEUE_RESTORE	0x10
-
-#define DEQUEUE_SLEEP		0x01
-#define DEQUEUE_SAVE		0x02
 
 #define RETRY_TASK		((void *)-1UL)
 

commit 41d93397334f3c3374810f45e7bcce9007d1a7bb
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Wed Jan 13 16:42:38 2016 +0800

    sched/core: Remove duplicated sched_group_set_shares() prototype
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <lizefan@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1452674558-31897-1-git-send-email-yangds.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 91f0a7752058..3dc7b8b3f94c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -318,7 +318,6 @@ extern void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,
 			struct sched_entity *se, int cpu,
 			struct sched_entity *parent);
 extern void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
-extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 
 extern void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b);
 extern void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b);

commit 6aa447bcbb444cd1b738613a20627f288d631665
Merge: abedf8e2419f 48be3a67da74
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 29 09:42:07 2016 +0100

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before applying new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6fe1f348b3dd1f700f9630562b7d38afd6949568
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 21 22:24:16 2016 +0100

    sched/cgroup: Fix cgroup entity load tracking tear-down
    
    When a cgroup's CPU runqueue is destroyed, it should remove its
    remaining load accounting from its parent cgroup.
    
    The current site for doing so it unsuited because its far too late and
    unordered against other cgroup removal (->css_free() will be, but we're also
    in an RCU callback).
    
    Put it in the ->css_offline() callback, which is the start of cgroup
    destruction, right after the group has been made unavailable to
    userspace. The ->css_offline() callbacks are called in hierarchical order
    after the following v4.4 commit:
    
      aa226ff4a1ce ("cgroup: make sure a parent css isn't offlined before its children")
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Christian Borntraeger <borntraeger@de.ibm.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160121212416.GL6357@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 10f16374df7f..30ea2d871ba7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -313,7 +313,7 @@ extern int tg_nop(struct task_group *tg, void *data);
 
 extern void free_fair_sched_group(struct task_group *tg);
 extern int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent);
-extern void unregister_fair_sched_group(struct task_group *tg, int cpu);
+extern void unregister_fair_sched_group(struct task_group *tg);
 extern void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,
 			struct sched_entity *se, int cpu,
 			struct sched_entity *parent);

commit cb2517653fccaf9f9b4ae968c7ee005c1bbacdc5
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Feb 5 09:08:36 2016 +0000

    sched/debug: Make schedstats a runtime tunable that is disabled by default
    
    schedstats is very useful during debugging and performance tuning but it
    incurs overhead to calculate the stats. As such, even though it can be
    disabled at build time, it is often enabled as the information is useful.
    
    This patch adds a kernel command-line and sysctl tunable to enable or
    disable schedstats on demand (when it's built in). It is disabled
    by default as someone who knows they need it can also learn to enable
    it when necessary.
    
    The benefits are dependent on how scheduler-intensive the workload is.
    If it is then the patch reduces the number of cycles spent calculating
    the stats with a small benefit from reducing the cache footprint of the
    scheduler.
    
    These measurements were taken from a 48-core 2-socket
    machine with Xeon(R) E5-2670 v3 cpus although they were also tested on a
    single socket machine 8-core machine with Intel i7-3770 processors.
    
    netperf-tcp
                               4.5.0-rc1             4.5.0-rc1
                                 vanilla          nostats-v3r1
    Hmean    64         560.45 (  0.00%)      575.98 (  2.77%)
    Hmean    128        766.66 (  0.00%)      795.79 (  3.80%)
    Hmean    256        950.51 (  0.00%)      981.50 (  3.26%)
    Hmean    1024      1433.25 (  0.00%)     1466.51 (  2.32%)
    Hmean    2048      2810.54 (  0.00%)     2879.75 (  2.46%)
    Hmean    3312      4618.18 (  0.00%)     4682.09 (  1.38%)
    Hmean    4096      5306.42 (  0.00%)     5346.39 (  0.75%)
    Hmean    8192     10581.44 (  0.00%)    10698.15 (  1.10%)
    Hmean    16384    18857.70 (  0.00%)    18937.61 (  0.42%)
    
    Small gains here, UDP_STREAM showed nothing intresting and neither did
    the TCP_RR tests. The gains on the 8-core machine were very similar.
    
    tbench4
                                     4.5.0-rc1             4.5.0-rc1
                                       vanilla          nostats-v3r1
    Hmean    mb/sec-1         500.85 (  0.00%)      522.43 (  4.31%)
    Hmean    mb/sec-2         984.66 (  0.00%)     1018.19 (  3.41%)
    Hmean    mb/sec-4        1827.91 (  0.00%)     1847.78 (  1.09%)
    Hmean    mb/sec-8        3561.36 (  0.00%)     3611.28 (  1.40%)
    Hmean    mb/sec-16       5824.52 (  0.00%)     5929.03 (  1.79%)
    Hmean    mb/sec-32      10943.10 (  0.00%)    10802.83 ( -1.28%)
    Hmean    mb/sec-64      15950.81 (  0.00%)    16211.31 (  1.63%)
    Hmean    mb/sec-128     15302.17 (  0.00%)    15445.11 (  0.93%)
    Hmean    mb/sec-256     14866.18 (  0.00%)    15088.73 (  1.50%)
    Hmean    mb/sec-512     15223.31 (  0.00%)    15373.69 (  0.99%)
    Hmean    mb/sec-1024    14574.25 (  0.00%)    14598.02 (  0.16%)
    Hmean    mb/sec-2048    13569.02 (  0.00%)    13733.86 (  1.21%)
    Hmean    mb/sec-3072    12865.98 (  0.00%)    13209.23 (  2.67%)
    
    Small gains of 2-4% at low thread counts and otherwise flat.  The
    gains on the 8-core machine were slightly different
    
    tbench4 on 8-core i7-3770 single socket machine
    Hmean    mb/sec-1        442.59 (  0.00%)      448.73 (  1.39%)
    Hmean    mb/sec-2        796.68 (  0.00%)      794.39 ( -0.29%)
    Hmean    mb/sec-4       1322.52 (  0.00%)     1343.66 (  1.60%)
    Hmean    mb/sec-8       2611.65 (  0.00%)     2694.86 (  3.19%)
    Hmean    mb/sec-16      2537.07 (  0.00%)     2609.34 (  2.85%)
    Hmean    mb/sec-32      2506.02 (  0.00%)     2578.18 (  2.88%)
    Hmean    mb/sec-64      2511.06 (  0.00%)     2569.16 (  2.31%)
    Hmean    mb/sec-128     2313.38 (  0.00%)     2395.50 (  3.55%)
    Hmean    mb/sec-256     2110.04 (  0.00%)     2177.45 (  3.19%)
    Hmean    mb/sec-512     2072.51 (  0.00%)     2053.97 ( -0.89%)
    
    In constract, this shows a relatively steady 2-3% gain at higher thread
    counts. Due to the nature of the patch and the type of workload, it's
    not a surprise that the result will depend on the CPU used.
    
    hackbench-pipes
                             4.5.0-rc1             4.5.0-rc1
                               vanilla          nostats-v3r1
    Amean    1        0.0637 (  0.00%)      0.0660 ( -3.59%)
    Amean    4        0.1229 (  0.00%)      0.1181 (  3.84%)
    Amean    7        0.1921 (  0.00%)      0.1911 (  0.52%)
    Amean    12       0.3117 (  0.00%)      0.2923 (  6.23%)
    Amean    21       0.4050 (  0.00%)      0.3899 (  3.74%)
    Amean    30       0.4586 (  0.00%)      0.4433 (  3.33%)
    Amean    48       0.5910 (  0.00%)      0.5694 (  3.65%)
    Amean    79       0.8663 (  0.00%)      0.8626 (  0.43%)
    Amean    110      1.1543 (  0.00%)      1.1517 (  0.22%)
    Amean    141      1.4457 (  0.00%)      1.4290 (  1.16%)
    Amean    172      1.7090 (  0.00%)      1.6924 (  0.97%)
    Amean    192      1.9126 (  0.00%)      1.9089 (  0.19%)
    
    Some small gains and losses and while the variance data is not included,
    it's close to the noise. The UMA machine did not show anything particularly
    different
    
    pipetest
                                 4.5.0-rc1             4.5.0-rc1
                                   vanilla          nostats-v2r2
    Min         Time        4.13 (  0.00%)        3.99 (  3.39%)
    1st-qrtle   Time        4.38 (  0.00%)        4.27 (  2.51%)
    2nd-qrtle   Time        4.46 (  0.00%)        4.39 (  1.57%)
    3rd-qrtle   Time        4.56 (  0.00%)        4.51 (  1.10%)
    Max-90%     Time        4.67 (  0.00%)        4.60 (  1.50%)
    Max-93%     Time        4.71 (  0.00%)        4.65 (  1.27%)
    Max-95%     Time        4.74 (  0.00%)        4.71 (  0.63%)
    Max-99%     Time        4.88 (  0.00%)        4.79 (  1.84%)
    Max         Time        4.93 (  0.00%)        4.83 (  2.03%)
    Mean        Time        4.48 (  0.00%)        4.39 (  1.91%)
    Best99%Mean Time        4.47 (  0.00%)        4.39 (  1.91%)
    Best95%Mean Time        4.46 (  0.00%)        4.38 (  1.93%)
    Best90%Mean Time        4.45 (  0.00%)        4.36 (  1.98%)
    Best50%Mean Time        4.36 (  0.00%)        4.25 (  2.49%)
    Best10%Mean Time        4.23 (  0.00%)        4.10 (  3.13%)
    Best5%Mean  Time        4.19 (  0.00%)        4.06 (  3.20%)
    Best1%Mean  Time        4.13 (  0.00%)        4.00 (  3.39%)
    
    Small improvement and similar gains were seen on the UMA machine.
    
    The gain is small but it stands to reason that doing less work in the
    scheduler is a good thing. The downside is that the lack of schedstats and
    tracepoints may be surprising to experts doing performance analysis until
    they find the existence of the schedstats= parameter or schedstats sysctl.
    It will be automatically activated for latencytop and sleep profiling to
    alleviate the problem. For tracepoints, there is a simple warning as it's
    not safe to activate schedstats in the context when it's known the tracepoint
    may be wanted but is unavailable.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454663316-22048-1-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 10f16374df7f..1d583870e1a6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1022,6 +1022,7 @@ extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
 
 extern struct static_key_false sched_numa_balancing;
+extern struct static_key_false sched_schedstats;
 
 static inline u64 global_rt_period(void)
 {

commit af345201ea948d0976d775958d8aa22fe5e5ba58
Merge: 4bd20db2c027 0905f04eb21f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jan 11 15:13:38 2016 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - tickless load average calculation enhancements (Byungchul Park)
    
       - vtime handling enhancements (Frederic Weisbecker)
    
       - scalability improvement via properly aligning a key structure field
         (Jiri Olsa)
    
       - various stop_machine() fixes (Oleg Nesterov)
    
       - sched/numa enhancement (Rik van Riel)
    
       - various fixes and improvements (Andi Kleen, Dietmar Eggemann,
         Geliang Tang, Hiroshi Shimamoto, Joonwoo Park, Peter Zijlstra,
         Waiman Long, Wanpeng Li, Yuyang Du)"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (32 commits)
      sched/fair: Fix new task's load avg removed from source CPU in wake_up_new_task()
      sched/core: Move sched_entity::avg into separate cache line
      x86/fpu: Properly align size in CHECK_MEMBER_AT_END_OF() macro
      sched/deadline: Fix the earliest_dl.next logic
      sched/fair: Disable the task group load_avg update for the root_task_group
      sched/fair: Move the cache-hot 'load_avg' variable into its own cacheline
      sched/fair: Avoid redundant idle_cpu() call in update_sg_lb_stats()
      sched/core: Move the sched_to_prio[] arrays out of line
      sched/cputime: Convert vtime_seqlock to seqcount
      sched/cputime: Introduce vtime accounting check for readers
      sched/cputime: Rename vtime_accounting_enabled() to vtime_accounting_cpu_enabled()
      sched/cputime: Correctly handle task guest time on housekeepers
      sched/cputime: Clarify vtime symbols and document them
      sched/cputime: Remove extra cost in task_cputime()
      sched/fair: Make it possible to account fair load avg consistently
      sched/fair: Modify the comment about lock assumptions in migrate_task_rq_fair()
      stop_machine: Clean up the usage of the preemption counter in cpu_stopper_thread()
      stop_machine: Shift the 'done != NULL' check from cpu_stop_signal_done() to callers
      stop_machine: Kill cpu_stop_done->executed
      stop_machine: Change __stop_cpus() to rely on cpu_stop_queue_work()
      ...

commit b0367629acf62a78404c467cd09df447c2fea804
Author: Waiman Long <Waiman.Long@hpe.com>
Date:   Wed Dec 2 13:41:49 2015 -0500

    sched/fair: Move the cache-hot 'load_avg' variable into its own cacheline
    
    If a system with large number of sockets was driven to full
    utilization, it was found that the clock tick handling occupied a
    rather significant proportion of CPU time when fair group scheduling
    and autogroup were enabled.
    
    Running a java benchmark on a 16-socket IvyBridge-EX system, the perf
    profile looked like:
    
      10.52%   0.00%  java   [kernel.vmlinux]  [k] smp_apic_timer_interrupt
       9.66%   0.05%  java   [kernel.vmlinux]  [k] hrtimer_interrupt
       8.65%   0.03%  java   [kernel.vmlinux]  [k] tick_sched_timer
       8.56%   0.00%  java   [kernel.vmlinux]  [k] update_process_times
       8.07%   0.03%  java   [kernel.vmlinux]  [k] scheduler_tick
       6.91%   1.78%  java   [kernel.vmlinux]  [k] task_tick_fair
       5.24%   5.04%  java   [kernel.vmlinux]  [k] update_cfs_shares
    
    In particular, the high CPU time consumed by update_cfs_shares()
    was mostly due to contention on the cacheline that contained the
    task_group's load_avg statistical counter. This cacheline may also
    contains variables like shares, cfs_rq & se which are accessed rather
    frequently during clock tick processing.
    
    This patch moves the load_avg variable into another cacheline
    separated from the other frequently accessed variables. It also
    creates a cacheline aligned kmemcache for task_group to make sure
    that all the allocated task_group's are cacheline aligned.
    
    By doing so, the perf profile became:
    
       9.44%   0.00%  java   [kernel.vmlinux]  [k] smp_apic_timer_interrupt
       8.74%   0.01%  java   [kernel.vmlinux]  [k] hrtimer_interrupt
       7.83%   0.03%  java   [kernel.vmlinux]  [k] tick_sched_timer
       7.74%   0.00%  java   [kernel.vmlinux]  [k] update_process_times
       7.27%   0.03%  java   [kernel.vmlinux]  [k] scheduler_tick
       5.94%   1.74%  java   [kernel.vmlinux]  [k] task_tick_fair
       4.15%   3.92%  java   [kernel.vmlinux]  [k] update_cfs_shares
    
    The %cpu time is still pretty high, but it is better than before. The
    benchmark results before and after the patch was as follows:
    
      Before patch - Max-jOPs: 907533    Critical-jOps: 134877
      After patch  - Max-jOPs: 916011    Critical-jOps: 142366
    
    Signed-off-by: Waiman Long <Waiman.Long@hpe.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Douglas Hatch <doug.hatch@hpe.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Scott J Norton <scott.norton@hpe.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yuyang Du <yuyang.du@intel.com>
    Link: http://lkml.kernel.org/r/1449081710-20185-3-git-send-email-Waiman.Long@hpe.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 472cd142e4f4..a5a6b3e60868 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -248,7 +248,12 @@ struct task_group {
 	unsigned long shares;
 
 #ifdef	CONFIG_SMP
-	atomic_long_t load_avg;
+	/*
+	 * load_avg can be heavily contended at clock tick time, so put
+	 * it in its own cacheline separated from the fields above which
+	 * will also be accessed at each tick.
+	 */
+	atomic_long_t load_avg ____cacheline_aligned;
 #endif
 #endif
 

commit ed82b8a1ff76ed7b2709e36ed361ddd022fe2407
Author: Andi Kleen <ak@linux.intel.com>
Date:   Sun Nov 29 20:59:43 2015 -0800

    sched/core: Move the sched_to_prio[] arrays out of line
    
    When building a kernel with a gcc 6 snapshot the compiler complains
    about unused const static variables for prio_to_weight and prio_to_mult
    for multiple scheduler files (all but core.c and autogroup.c)
    
    The way the array is currently declared it will be duplicated in
    every scheduler file that includes sched.h, which seems rather wasteful.
    
    Move the array out of line into core.c. I also added a sched_ prefix
    to avoid any potential name space collisions.
    
    Signed-off-by: Andi Kleen <ak@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1448859583-3252-1-git-send-email-andi@firstfloor.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9a029fadcfbe..472cd142e4f4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1122,46 +1122,8 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 #define WEIGHT_IDLEPRIO                3
 #define WMULT_IDLEPRIO         1431655765
 
-/*
- * Nice levels are multiplicative, with a gentle 10% change for every
- * nice level changed. I.e. when a CPU-bound task goes from nice 0 to
- * nice 1, it will get ~10% less CPU time than another CPU-bound task
- * that remained on nice 0.
- *
- * The "10% effect" is relative and cumulative: from _any_ nice level,
- * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
- * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
- * If a task goes up by ~10% and another task goes down by ~10% then
- * the relative distance between them is ~25%.)
- */
-static const int prio_to_weight[40] = {
- /* -20 */     88761,     71755,     56483,     46273,     36291,
- /* -15 */     29154,     23254,     18705,     14949,     11916,
- /* -10 */      9548,      7620,      6100,      4904,      3906,
- /*  -5 */      3121,      2501,      1991,      1586,      1277,
- /*   0 */      1024,       820,       655,       526,       423,
- /*   5 */       335,       272,       215,       172,       137,
- /*  10 */       110,        87,        70,        56,        45,
- /*  15 */        36,        29,        23,        18,        15,
-};
-
-/*
- * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.
- *
- * In cases where the weight does not change often, we can use the
- * precalculated inverse to speed up arithmetics by turning divisions
- * into multiplications:
- */
-static const u32 prio_to_wmult[40] = {
- /* -20 */     48388,     59856,     76040,     92818,    118348,
- /* -15 */    147320,    184698,    229616,    287308,    360437,
- /* -10 */    449829,    563644,    704093,    875809,   1099582,
- /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
- /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
- /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
- /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
- /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
-};
+extern const int sched_prio_to_weight[40];
+extern const u32 sched_prio_to_wmult[40];
 
 #define ENQUEUE_WAKEUP		0x01
 #define ENQUEUE_HEAD		0x02

commit ad936d8658fd348338cb7d42c577dac77892b074
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Sat Oct 24 01:16:19 2015 +0900

    sched/fair: Make it possible to account fair load avg consistently
    
    The current code accounts for the time a task was absent from the fair
    class (per ATTACH_AGE_LOAD). However it does not work correctly when a
    task got migrated or moved to another cgroup while outside of the fair
    class.
    
    This patch tries to address that by aging on migration. We locklessly
    read the 'last_update_time' stamp from both the old and new cfs_rq,
    ages the load upto the old time, and sets it to the new time.
    
    These timestamps should in general not be more than 1 tick apart from
    one another, so there is a definite bound on things.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    [ Changelog, a few edits and !SMP build fix ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1445616981-29904-2-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index cdae23dabfdc..9a029fadcfbe 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -335,7 +335,15 @@ extern void sched_move_task(struct task_struct *tsk);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
-#endif
+
+#ifdef CONFIG_SMP
+extern void set_task_rq_fair(struct sched_entity *se,
+			     struct cfs_rq *prev, struct cfs_rq *next);
+#else /* !CONFIG_SMP */
+static inline void set_task_rq_fair(struct sched_entity *se,
+			     struct cfs_rq *prev, struct cfs_rq *next) { }
+#endif /* CONFIG_SMP */
+#endif /* CONFIG_FAIR_GROUP_SCHED */
 
 #else /* CONFIG_CGROUP_SCHED */
 
@@ -933,6 +941,7 @@ static inline void set_task_rq(struct task_struct *p, unsigned int cpu)
 #endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
+	set_task_rq_fair(&p->se, p->se.cfs_rq, tg->cfs_rq[cpu]);
 	p->se.cfs_rq = tg->cfs_rq[cpu];
 	p->se.parent = tg->se[cpu];
 #endif

commit b3e0b1b6d841a4b2f64fc09ea728913da8218424
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Oct 16 14:39:38 2015 +0200

    locking, sched: Introduce smp_cond_acquire() and use it
    
    Introduce smp_cond_acquire() which combines a control dependency and a
    read barrier to form acquire semantics.
    
    This primitive has two benefits:
    
     - it documents control dependencies,
     - its typically cheaper than using smp_load_acquire() in a loop.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b242775bf670..1e0bb4afe3fd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1076,7 +1076,7 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 	 * In particular, the load of prev->state in finish_task_switch() must
 	 * happen before this.
 	 *
-	 * Pairs with the control dependency and rmb in try_to_wake_up().
+	 * Pairs with the smp_cond_acquire() in try_to_wake_up().
 	 */
 	smp_store_release(&prev->on_cpu, 0);
 #endif

commit 467386fbbf085e716570813a5d3be3927c348e11
Merge: 525628c73bd6 ecf7d01c229d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Dec 4 10:27:36 2015 +0100

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before applying new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b75a22531588e77aa8c2daf228c9723916ae2cd0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 6 14:36:17 2015 +0200

    sched/core: Better document the try_to_wake_up() barriers
    
    Explain how the control dependency and smp_rmb() end up providing
    ACQUIRE semantics and pair with smp_store_release() in
    finish_lock_switch().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index efd3bfc7e347..b242775bf670 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1073,6 +1073,9 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 	 * We must ensure this doesn't happen until the switch is completely
 	 * finished.
 	 *
+	 * In particular, the load of prev->state in finish_task_switch() must
+	 * happen before this.
+	 *
 	 * Pairs with the control dependency and rmb in try_to_wake_up().
 	 */
 	smp_store_release(&prev->on_cpu, 0);

commit 38c6ade2dd4dcc3bca06c981e2a1b91289046177
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Tue Oct 20 13:04:41 2015 +0100

    sched/fair: Remove empty idle enter and exit functions
    
    Commit cd126afe838d ("sched/fair: Remove rq's runnable avg") got rid of
    rq->avg and so there is no need to update it any more when entering or
    exiting idle.
    
    Remove the now empty functions idle_{enter|exit}_fair().
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Yuyang Du <yuyang.du@intel.com>
    Link: http://lkml.kernel.org/r/1445342681-17171-1-git-send-email-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index efd3bfc7e347..2eb2002aa336 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1249,16 +1249,8 @@ extern void update_group_capacity(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
 
-extern void idle_enter_fair(struct rq *this_rq);
-extern void idle_exit_fair(struct rq *this_rq);
-
 extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask);
 
-#else
-
-static inline void idle_enter_fair(struct rq *rq) { }
-static inline void idle_exit_fair(struct rq *rq) { }
-
 #endif
 
 #ifdef CONFIG_CPU_IDLE

commit 5a4fd0368517bc5b5399ef958f6d30cbff492918
Author: xiaofeng.yan <yanxiaofeng@inspur.com>
Date:   Wed Sep 23 14:55:59 2015 +0800

    sched/core: Remove a parameter in the migrate_task_rq() function
    
    The parameter "int next_cpu" in the following function is unused:
    
      migrate_task_rq(struct task_struct *p, int next_cpu)
    
    Remove it.
    
    Signed-off-by: xiaofeng.yan <yanxiaofeng@inspur.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1442991360-31945-1-git-send-email-yanxiaofeng@inspur.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e08cc4cf68e2..efd3bfc7e347 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1190,7 +1190,7 @@ struct sched_class {
 
 #ifdef CONFIG_SMP
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
-	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
+	void (*migrate_task_rq)(struct task_struct *p);
 
 	void (*task_waking) (struct task_struct *task);
 	void (*task_woken) (struct rq *this_rq, struct task_struct *task);

commit 1de64443d755f83af8ba8b558fded0c61afaef47
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Sep 30 17:44:13 2015 +0200

    sched/core: Fix task and run queue sched_info::run_delay inconsistencies
    
    Mike Meyer reported the following bug:
    
    > During evaluation of some performance data, it was discovered thread
    > and run queue run_delay accounting data was inconsistent with the other
    > accounting data that was collected.  Further investigation found under
    > certain circumstances execution time was leaking into the task and
    > run queue accounting of run_delay.
    >
    > Consider the following sequence:
    >
    >     a. thread is running.
    >     b. thread moves beween cgroups, changes scheduling class or priority.
    >     c. thread sleeps OR
    >     d. thread involuntarily gives up cpu.
    >
    > a. implies:
    >
    >     thread->sched_info.last_queued = 0
    >
    > a. and b. results in the following:
    >
    >     1. dequeue_task(rq, thread)
    >
    >            sched_info_dequeued(rq, thread)
    >                delta = 0
    >
    >                sched_info_reset_dequeued(thread)
    >                    thread->sched_info.last_queued = 0
    >
    >                thread->sched_info.run_delay += delta
    >
    >     2. enqueue_task(rq, thread)
    >
    >            sched_info_queued(rq, thread)
    >
    >                /* thread is still on cpu at this point. */
    >                thread->sched_info.last_queued = task_rq(thread)->clock;
    >
    > c. results in:
    >
    >     dequeue_task(rq, thread)
    >
    >         sched_info_dequeued(rq, thread)
    >
    >             /* delta is execution time not run_delay. */
    >             delta = task_rq(thread)->clock - thread->sched_info.last_queued
    >
    >         sched_info_reset_dequeued(thread)
    >             thread->sched_info.last_queued = 0
    >
    >         thread->sched_info.run_delay += delta
    >
    >     Since thread was running between enqueue_task(rq, thread) and
    >     dequeue_task(rq, thread), the delta above is really execution
    >     time and not run_delay.
    >
    > d. results in:
    >
    >     __sched_info_switch(thread, next_thread)
    >
    >         sched_info_depart(rq, thread)
    >
    >             sched_info_queued(rq, thread)
    >
    >                 /* last_queued not updated due to being non-zero */
    >                 return
    >
    >     Since thread was running between enqueue_task(rq, thread) and
    >     __sched_info_switch(thread, next_thread), the execution time
    >     between enqueue_task(rq, thread) and
    >     __sched_info_switch(thread, next_thread) now will become
    >     associated with run_delay due to when last_queued was last updated.
    >
    
    This alternative patch solves the problem by not calling
    sched_info_{de,}queued() in {de,en}queue_task(). Therefore the
    sched_info state is preserved and things work as expected.
    
    By inlining the {de,en}queue_task() functions the new condition
    becomes (mostly) a compile-time constant and we'll not emit any new
    branch instructions.
    
    It even shrinks the code (due to inlining {en,de}queue_task()):
    
    $ size defconfig-build/kernel/sched/core.o defconfig-build/kernel/sched/core.o.orig
       text    data     bss     dec     hex filename
      64019   23378    2344   89741   15e8d defconfig-build/kernel/sched/core.o
      64149   23378    2344   89871   15f0f defconfig-build/kernel/sched/core.o.orig
    
    Reported-by: Mike Meyer <Mike.Meyer@Teradata.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/20150930154413.GO3604@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 046242feff3a..e08cc4cf68e2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1151,16 +1151,18 @@ static const u32 prio_to_wmult[40] = {
  /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
 };
 
-#define ENQUEUE_WAKEUP		1
-#define ENQUEUE_HEAD		2
+#define ENQUEUE_WAKEUP		0x01
+#define ENQUEUE_HEAD		0x02
 #ifdef CONFIG_SMP
-#define ENQUEUE_WAKING		4	/* sched_class::task_waking was called */
+#define ENQUEUE_WAKING		0x04	/* sched_class::task_waking was called */
 #else
-#define ENQUEUE_WAKING		0
+#define ENQUEUE_WAKING		0x00
 #endif
-#define ENQUEUE_REPLENISH	8
+#define ENQUEUE_REPLENISH	0x08
+#define ENQUEUE_RESTORE	0x10
 
-#define DEQUEUE_SLEEP		1
+#define DEQUEUE_SLEEP		0x01
+#define DEQUEUE_SAVE		0x02
 
 #define RETRY_TASK		((void *)-1UL)
 

commit fe19159225d8516f3f57a5fe8f735c01684f0ddd
Merge: c6e1e7b5b7f0 95913d97914f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Oct 6 17:05:36 2015 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before applying new changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 95913d97914f44db2b81271c2e2ebd4d2ac2df83
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 29 14:45:09 2015 +0200

    sched/core: Fix TASK_DEAD race in finish_task_switch()
    
    So the problem this patch is trying to address is as follows:
    
            CPU0                            CPU1
    
            context_switch(A, B)
                                            ttwu(A)
                                              LOCK A->pi_lock
                                              A->on_cpu == 0
            finish_task_switch(A)
              prev_state = A->state  <-.
              WMB                      |
              A->on_cpu = 0;           |
              UNLOCK rq0->lock         |
                                       |    context_switch(C, A)
                                       `--  A->state = TASK_DEAD
              prev_state == TASK_DEAD
                put_task_struct(A)
                                            context_switch(A, C)
                                            finish_task_switch(A)
                                              A->state == TASK_DEAD
                                                put_task_struct(A)
    
    The argument being that the WMB will allow the load of A->state on CPU0
    to cross over and observe CPU1's store of A->state, which will then
    result in a double-drop and use-after-free.
    
    Now the comment states (and this was true once upon a long time ago)
    that we need to observe A->state while holding rq->lock because that
    will order us against the wakeup; however the wakeup will not in fact
    acquire (that) rq->lock; it takes A->pi_lock these days.
    
    We can obviously fix this by upgrading the WMB to an MB, but that is
    expensive, so we'd rather avoid that.
    
    The alternative this patch takes is: smp_store_release(&A->on_cpu, 0),
    which avoids the MB on some archs, but not important ones like ARM.
    
    Reported-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: <stable@vger.kernel.org> # v3.1+
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Cc: manfred@colorfullife.com
    Cc: will.deacon@arm.com
    Fixes: e4a52bcb9a18 ("sched: Remove rq->lock from the first half of ttwu()")
    Link: http://lkml.kernel.org/r/20150929124509.GG3816@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 68cda117574c..6d2a119c7ad9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1078,9 +1078,10 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 	 * After ->on_cpu is cleared, the task can be moved to a different CPU.
 	 * We must ensure this doesn't happen until the switch is completely
 	 * finished.
+	 *
+	 * Pairs with the control dependency and rmb in try_to_wake_up().
 	 */
-	smp_wmb();
-	prev->on_cpu = 0;
+	smp_store_release(&prev->on_cpu, 0);
 #endif
 #ifdef CONFIG_DEBUG_SPINLOCK
 	/* this is a valid case when another task releases the spinlock */

commit 2726d6ce389788c7fe724961a6e1bfe569560088
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Wed Sep 2 11:01:34 2015 +0100

    sched/deadline: Unify dl_time_before() usage
    
    Move dl_time_before() static definition in include/linux/sched/deadline.h
    so that it can be used by different parties without being re-defined.
    
    Reported-by: Luca Abeni <luca.abeni@unitn.it>
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1441188096-23021-3-git-send-email-juri.lelli@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3845a711c65e..af6f252e7e34 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -118,11 +118,6 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
-static inline bool dl_time_before(u64 a, u64 b)
-{
-	return (s64)(a - b) < 0;
-}
-
 /*
  * Tells if entity @a should preempt entity @b.
  */

commit 20f9cd2acb1d74a8bf4b4087267f586e6ecdbc03
Author: Henrik Austad <henrik@austad.us>
Date:   Wed Sep 9 17:00:41 2015 +0200

    sched/core: Make policy-testing consistent
    
    Most of the policy-tests are done via the <class>_policy() helpers with
    the notable exception of idle. A new wrapper for valid_policy() has also
    been added to improve readability  in set_load_weight().
    
    This commit does not change the logical behavior of the scheduler core.
    
    Signed-off-by: Henrik Austad <henrik@austad.us>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/1441810841-4756-1-git-send-email-henrik@austad.us
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 167ab4844ee6..3845a711c65e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -84,6 +84,10 @@ static inline void update_cpu_load_active(struct rq *this_rq) { }
  */
 #define RUNTIME_INF	((u64)~0ULL)
 
+static inline int idle_policy(int policy)
+{
+	return policy == SCHED_IDLE;
+}
 static inline int fair_policy(int policy)
 {
 	return policy == SCHED_NORMAL || policy == SCHED_BATCH;
@@ -98,6 +102,11 @@ static inline int dl_policy(int policy)
 {
 	return policy == SCHED_DEADLINE;
 }
+static inline bool valid_policy(int policy)
+{
+	return idle_policy(policy) || fair_policy(policy) ||
+		rt_policy(policy) || dl_policy(policy);
+}
 
 static inline int task_has_rt_policy(struct task_struct *p)
 {

commit e3279a2e6d697e00e74f905851ee7cf532f72b2d
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Sat Aug 15 00:04:41 2015 +0100

    sched/fair: Make utilization tracking CPU scale-invariant
    
    Besides the existing frequency scale-invariance correction factor, apply
    CPU scale-invariance correction factor to utilization tracking to
    compensate for any differences in compute capacity. This could be due to
    micro-architectural differences (i.e. instructions per seconds) between
    cpus in HMP systems (e.g. big.LITTLE), and/or differences in the current
    maximum frequency supported by individual cpus in SMP systems. In the
    existing implementation utilization isn't comparable between cpus as it
    is relative to the capacity of each individual CPU.
    
    Each segment of the sched_avg.util_sum geometric series is now scaled
    by the CPU performance factor too so the sched_avg.util_avg of each
    sched entity will be invariant from the particular CPU of the HMP/SMP
    system on which the sched entity is scheduled.
    
    With this patch, the utilization of a CPU stays relative to the max CPU
    performance of the fastest CPU in the system.
    
    In contrast to utilization (sched_avg.util_sum), load
    (sched_avg.load_sum) should not be scaled by compute capacity. The
    utilization metric is based on running time which only makes sense when
    cpus are _not_ fully utilized (utilization cannot go beyond 100% even if
    more tasks are added), where load is runnable time which isn't limited
    by the capacity of the CPU and therefore is a better metric for
    overloaded scenarios. If we run two nice-0 busy loops on two cpus with
    different compute capacity their load should be similar since their
    compute demands are the same. We have to assume that the compute demand
    of any task running on a fully utilized CPU (no spare cycles = 100%
    utilization) is high and the same no matter of the compute capacity of
    its current CPU, hence we shouldn't scale load by CPU capacity.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/55CE7409.1000700@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c0726d5fd6a3..167ab4844ee6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1398,7 +1398,7 @@ unsigned long arch_scale_freq_capacity(struct sched_domain *sd, int cpu)
 static __always_inline
 unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
 {
-	if ((sd->flags & SD_SHARE_CPUCAPACITY) && (sd->span_weight > 1))
+	if (sd && (sd->flags & SD_SHARE_CPUCAPACITY) && (sd->span_weight > 1))
 		return sd->smt_gain / sd->span_weight;
 
 	return SCHED_CAPACITY_SCALE;

commit 8cd5601c50603caa195ce86cc465cb04079ed488
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Fri Aug 14 17:23:10 2015 +0100

    sched/fair: Convert arch_scale_cpu_capacity() from weak function to #define
    
    Bring arch_scale_cpu_capacity() in line with the recent change of its
    arch_scale_freq_capacity() sibling in commit dfbca41f3479 ("sched:
    Optimize freq invariant accounting") from weak function to #define to
    allow inlining of the function.
    
    While at it, remove the ARCH_CAPACITY sched_feature as well. With the
    change to #define there isn't a straightforward way to allow runtime
    switch between an arch implementation and the default implementation of
    arch_scale_cpu_capacity() using sched_feature. The default was to use
    the arch-specific implementation, but only the arm architecture provides
    one and that is essentially equivalent to the default implementation.
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Dietmar Eggemann <Dietmar.Eggemann@arm.com>
    Cc: Juri Lelli <Juri.Lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: daniel.lezcano@linaro.org
    Cc: mturquette@baylibre.com
    Cc: pang.xunlei@zte.com.cn
    Cc: rjw@rjwysocki.net
    Cc: sgurrappadi@nvidia.com
    Cc: vincent.guittot@linaro.org
    Cc: yuyang.du@intel.com
    Link: http://lkml.kernel.org/r/1439569394-11974-3-git-send-email-morten.rasmussen@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2e8530d02b02..c0726d5fd6a3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1394,6 +1394,17 @@ unsigned long arch_scale_freq_capacity(struct sched_domain *sd, int cpu)
 }
 #endif
 
+#ifndef arch_scale_cpu_capacity
+static __always_inline
+unsigned long arch_scale_cpu_capacity(struct sched_domain *sd, int cpu)
+{
+	if ((sd->flags & SD_SHARE_CPUCAPACITY) && (sd->span_weight > 1))
+		return sd->smt_gain / sd->span_weight;
+
+	return SCHED_CAPACITY_SCALE;
+}
+#endif
+
 static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta)
 {
 	rq->rt_avg += rt_delta * arch_scale_freq_capacity(NULL, cpu_of(rq));

commit 2a595721a1fa6b684c1c818f379bef834ac3d65e
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Aug 11 21:54:21 2015 +0530

    sched/numa: Convert sched_numa_balancing to a static_branch
    
    Variable sched_numa_balancing toggles numa_balancing feature. Hence
    moving from a simple read mostly variable to a more apt static_branch.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1439310261-16124-1-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0d8f885b215b..2e8530d02b02 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1003,11 +1003,7 @@ extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
 #endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
 
-#ifdef CONFIG_NUMA_BALANCING
-extern bool sched_numa_balancing;
-#else
-#define sched_numa_balancing (0)
-#endif /* CONFIG_NUMA_BALANCING */
+extern struct static_key_false sched_numa_balancing;
 
 static inline u64 global_rt_period(void)
 {

commit c3b9bc5bbfc3750570d788afffd431263ef695c6
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Aug 11 16:30:12 2015 +0530

    sched/numa: Disable sched_numa_balancing on UMA systems
    
    Commit 2a1ed24 ("sched/numa: Prefer NUMA hotness over cache hotness")
    sets sched feature NUMA to true. However this can enable NUMA hinting
    faults on a UMA system.
    
    This commit ensures that NUMA hinting faults occur only on a NUMA system
    by setting/resetting sched_numa_balancing.
    
    This commit:
    
      - Makes sched_numa_balancing common to CONFIG_SCHED_DEBUG and
        !CONFIG_SCHED_DEBUG. Earlier it was only in !CONFIG_SCHED_DEBUG.
    
      - Checks for sched_numa_balancing instead of sched_feat(NUMA).
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1439290813-6683-3-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d0b303d675d6..0d8f885b215b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1004,14 +1004,8 @@ extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
 
 #ifdef CONFIG_NUMA_BALANCING
-#define sched_feat_numa(x) sched_feat(x)
-#ifdef CONFIG_SCHED_DEBUG
-#define sched_numa_balancing sched_feat_numa(NUMA)
-#else
 extern bool sched_numa_balancing;
-#endif /* CONFIG_SCHED_DEBUG */
 #else
-#define sched_feat_numa(x) (0)
 #define sched_numa_balancing (0)
 #endif /* CONFIG_NUMA_BALANCING */
 

commit 78a9c54649ea220065aad9902460a1d137c7eafd
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Tue Aug 11 16:30:11 2015 +0530

    sched/numa: Rename numabalancing_enabled to sched_numa_balancing
    
    Simple rename of the 'numabalancing_enabled' variable to 'sched_numa_balancing'.
    No functional changes.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1439290813-6683-2-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 637d5aeaa55a..d0b303d675d6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1006,13 +1006,13 @@ extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #ifdef CONFIG_NUMA_BALANCING
 #define sched_feat_numa(x) sched_feat(x)
 #ifdef CONFIG_SCHED_DEBUG
-#define numabalancing_enabled sched_feat_numa(NUMA)
+#define sched_numa_balancing sched_feat_numa(NUMA)
 #else
-extern bool numabalancing_enabled;
+extern bool sched_numa_balancing;
 #endif /* CONFIG_SCHED_DEBUG */
 #else
 #define sched_feat_numa(x) (0)
-#define numabalancing_enabled (0)
+#define sched_numa_balancing (0)
 #endif /* CONFIG_NUMA_BALANCING */
 
 static inline u64 global_rt_period(void)

commit bc54da2176cd38cedea767eff637229a191a2383
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 31 17:13:55 2015 +0200

    sched/core: Remove unused argument from sched_class::task_move_group
    
    The previous patches made the second argument go unused, remove it.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 68cda117574c..637d5aeaa55a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1226,7 +1226,7 @@ struct sched_class {
 	void (*update_curr) (struct rq *rq);
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	void (*task_move_group) (struct task_struct *p, int on_rq);
+	void (*task_move_group) (struct task_struct *p);
 #endif
 };
 

commit c5b2803840817115e9b568d5054e5007ae36176b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri May 15 17:43:35 2015 +0200

    sched: Make sched_class::set_cpus_allowed() unconditional
    
    Give every class a set_cpus_allowed() method, this enables some small
    optimization in the RT,DL implementation by avoiding a double
    cpumask_weight() call.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: dedekind1@gmail.com
    Cc: juri.lelli@arm.com
    Cc: mgorman@suse.de
    Cc: riel@redhat.com
    Cc: rostedt@goodmis.org
    Link: http://lkml.kernel.org/r/20150515154833.614517487@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 22ccc5556c42..68cda117574c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1255,6 +1255,8 @@ extern void trigger_load_balance(struct rq *rq);
 extern void idle_enter_fair(struct rq *this_rq);
 extern void idle_exit_fair(struct rq *this_rq);
 
+extern void set_cpus_allowed_common(struct task_struct *p, const struct cpumask *new_mask);
+
 #else
 
 static inline void idle_enter_fair(struct rq *rq) { }

commit 3c8e4793556981a7f532599959aa3303968056f0
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jul 29 17:31:47 2015 +0200

    sched: Remove finish_arch_switch()
    
    One less arch hook..
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ab0b05cc3f37..22ccc5556c42 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1055,9 +1055,6 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 #ifndef prepare_arch_switch
 # define prepare_arch_switch(next)	do { } while (0)
 #endif
-#ifndef finish_arch_switch
-# define finish_arch_switch(prev)	do { } while (0)
-#endif
 #ifndef finish_arch_post_lock_switch
 # define finish_arch_post_lock_switch()	do { } while (0)
 #endif

commit 139622343ef31941effc6de6a5a9320371a00e62
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:41 2015 +0800

    sched/fair: Provide runnable_load_avg back to cfs_rq
    
    The cfs_rq's load_avg is composed of runnable_load_avg and blocked_load_avg.
    Before this series, sometimes the runnable_load_avg is used, and sometimes
    the load_avg is used. Completely replacing all uses of runnable_load_avg
    with load_avg may be too big a leap, i.e., the blocked_load_avg is concerned
    to result in overrated load. Therefore, we get runnable_load_avg back.
    
    The new cfs_rq's runnable_load_avg is improved to be updated with all of the
    runnable sched_eneities at the same time, so the one sched_entity updated and
    the others stale problem is solved.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-7-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4d139e0bc206..ab0b05cc3f37 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -368,6 +368,8 @@ struct cfs_rq {
 	 * CFS load tracking
 	 */
 	struct sched_avg avg;
+	u64 runnable_load_sum;
+	unsigned long runnable_load_avg;
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	unsigned long tg_load_avg_contrib;
 #endif

commit 540247fb5ddf6d2364f90387fa1f8f428d15e683
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:39 2015 +0800

    sched/fair: Init cfs_rq's sched_entity load average
    
    The runnable load and utilization averages of cfs_rq's sched_entity
    were not initiated. Like done to a task, give new cfs_rq' sched_entity
    start values to heavy its load in infant time.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-5-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index dcde941a585b..4d139e0bc206 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1307,7 +1307,7 @@ extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 
 unsigned long to_ratio(u64 period, u64 runtime);
 
-extern void init_task_runnable_average(struct task_struct *p);
+extern void init_entity_runnable_average(struct sched_entity *se);
 
 static inline void add_nr_running(struct rq *rq, unsigned count)
 {

commit 9d89c257dfb9c51a532d69397f6eed75e5168c35
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:37 2015 +0800

    sched/fair: Rewrite runnable load and utilization average tracking
    
    The idea of runnable load average (let runnable time contribute to weight)
    was proposed by Paul Turner and Ben Segall, and it is still followed by
    this rewrite. This rewrite aims to solve the following issues:
    
    1. cfs_rq's load average (namely runnable_load_avg and blocked_load_avg) is
       updated at the granularity of an entity at a time, which results in the
       cfs_rq's load average is stale or partially updated: at any time, only
       one entity is up to date, all other entities are effectively lagging
       behind. This is undesirable.
    
       To illustrate, if we have n runnable entities in the cfs_rq, as time
       elapses, they certainly become outdated:
    
         t0: cfs_rq { e1_old, e2_old, ..., en_old }
    
       and when we update:
    
         t1: update e1, then we have cfs_rq { e1_new, e2_old, ..., en_old }
    
         t2: update e2, then we have cfs_rq { e1_old, e2_new, ..., en_old }
    
         ...
    
       We solve this by combining all runnable entities' load averages together
       in cfs_rq's avg, and update the cfs_rq's avg as a whole. This is based
       on the fact that if we regard the update as a function, then:
    
       w * update(e) = update(w * e) and
    
       update(e1) + update(e2) = update(e1 + e2), then
    
       w1 * update(e1) + w2 * update(e2) = update(w1 * e1 + w2 * e2)
    
       therefore, by this rewrite, we have an entirely updated cfs_rq at the
       time we update it:
    
         t1: update cfs_rq { e1_new, e2_new, ..., en_new }
    
         t2: update cfs_rq { e1_new, e2_new, ..., en_new }
    
         ...
    
    2. cfs_rq's load average is different between top rq->cfs_rq and other
       task_group's per CPU cfs_rqs in whether or not blocked_load_average
       contributes to the load.
    
       The basic idea behind runnable load average (the same for utilization)
       is that the blocked state is taken into account as opposed to only
       accounting for the currently runnable state. Therefore, the average
       should include both the runnable/running and blocked load averages.
       This rewrite does that.
    
       In addition, we also combine runnable/running and blocked averages
       of all entities into the cfs_rq's average, and update it together at
       once. This is based on the fact that:
    
         update(runnable) + update(blocked) = update(runnable + blocked)
    
       This significantly reduces the code as we don't need to separately
       maintain/update runnable/running load and blocked load.
    
    3. How task_group entities' share is calculated is complex and imprecise.
    
       We reduce the complexity in this rewrite to allow a very simple rule:
       the task_group's load_avg is aggregated from its per CPU cfs_rqs's
       load_avgs. Then group entity's weight is simply proportional to its
       own cfs_rq's load_avg / task_group's load_avg. To illustrate,
    
       if a task_group has { cfs_rq1, cfs_rq2, ..., cfs_rqn }, then,
    
       task_group_avg = cfs_rq1_avg + cfs_rq2_avg + ... + cfs_rqn_avg, then
    
       cfs_rqx's entity's share = cfs_rqx_avg / task_group_avg * task_group's share
    
    To sum up, this rewrite in principle is equivalent to the current one, but
    fixes the issues described above. Turns out, it significantly reduces the
    code complexity and hence increases clarity and efficiency. In addition,
    the new averages are more smooth/continuous (no spurious spikes and valleys)
    and updated more consistently and quickly to reflect the load dynamics.
    
    As a result, we have less load tracking overhead, better performance,
    and especially better power efficiency due to more balanced load.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-3-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e13210cce7e8..dcde941a585b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -245,7 +245,6 @@ struct task_group {
 
 #ifdef	CONFIG_SMP
 	atomic_long_t load_avg;
-	atomic_t runnable_avg;
 #endif
 #endif
 
@@ -366,27 +365,18 @@ struct cfs_rq {
 
 #ifdef CONFIG_SMP
 	/*
-	 * CFS Load tracking
-	 * Under CFS, load is tracked on a per-entity basis and aggregated up.
-	 * This allows for the description of both thread and group usage (in
-	 * the FAIR_GROUP_SCHED case).
-	 * runnable_load_avg is the sum of the load_avg_contrib of the
-	 * sched_entities on the rq.
-	 * blocked_load_avg is similar to runnable_load_avg except that its
-	 * the blocked sched_entities on the rq.
-	 * utilization_load_avg is the sum of the average running time of the
-	 * sched_entities on the rq.
+	 * CFS load tracking
 	 */
-	unsigned long runnable_load_avg, blocked_load_avg, utilization_load_avg;
-	atomic64_t decay_counter;
-	u64 last_decay;
-	atomic_long_t removed_load;
-
+	struct sched_avg avg;
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	/* Required to track per-cpu representation of a task_group */
-	u32 tg_runnable_contrib;
-	unsigned long tg_load_contrib;
+	unsigned long tg_load_avg_contrib;
+#endif
+	atomic_long_t removed_load_avg, removed_util_avg;
+#ifndef CONFIG_64BIT
+	u64 load_last_update_time_copy;
+#endif
 
+#ifdef CONFIG_FAIR_GROUP_SCHED
 	/*
 	 *   h_load = weight * f(tg)
 	 *

commit cd126afe838d7ea9b971cdea087fd498a7293c7f
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:36 2015 +0800

    sched/fair: Remove rq's runnable avg
    
    The current rq->avg is not used at all since its merge into the kernel,
    and the code is in the scheduler's hot path, so remove it.
    
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-2-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 84d48790bb6d..e13210cce7e8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -595,8 +595,6 @@ struct rq {
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
 	struct list_head leaf_cfs_rq_list;
-
-	struct sched_avg avg;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 	/*

commit 22a093b2fb52fb656658a32adc80c24ddc200ca4
Merge: c1776a18e3b5 397f2378f136
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jul 4 08:56:53 2015 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Debug info and other statistics fixes and related enhancements"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/numa: Fix numa balancing stats in /proc/pid/sched
      sched/numa: Show numa_group ID in /proc/sched_debug task listings
      sched/debug: Move print_cfs_rq() declaration to kernel/sched/sched.h
      sched/stat: Expose /proc/pid/schedstat if CONFIG_SCHED_INFO=y
      sched/stat: Simplify the sched_info accounting dependency

commit 397f2378f136128623fc237746157aa2564d1082
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jun 25 22:51:43 2015 +0530

    sched/numa: Fix numa balancing stats in /proc/pid/sched
    
    Commit 44dba3d5d6a1 ("sched: Refactor task_struct to use
    numa_faults instead of numa_* pointers") modified the way
    tsk->numa_faults stats are accounted.
    
    However that commit never touched show_numa_stats() that is displayed
    in /proc/pid/sched and thus the numbers displayed in /proc/pid/sched
    don't match the actual numbers.
    
    Fix it by making sure that /proc/pid/sched reflects the task
    fault numbers. Also add group fault stats too.
    
    Also couple of more modifications are added here:
    
    1. Format changes:
    
      - Previously we would list two entries per node, one for private
        and one for shared. Also the home node info was listed in each entry.
    
      - Now preferred node, total_faults and current node are
        displayed separately.
    
      - Now there is one entry per node, that lists private,shared task and
        group faults.
    
    2. Unit changes:
    
      - p->numa_pages_migrated was getting reset after every read of
        /proc/pid/sched. It's more useful to have absolute numbers since
        differential migrations between two accesses can be more easily
        calculated.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Iulia Manda <iulia.manda21@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1435252903-1081-4-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7d5895258fe3..7ef596837dac 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1675,7 +1675,15 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void print_dl_stats(struct seq_file *m, int cpu);
 extern void
 print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
-#endif
+
+#ifdef CONFIG_NUMA_BALANCING
+extern void
+show_numa_stats(struct task_struct *p, struct seq_file *m);
+extern void
+print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
+	unsigned long tpf, unsigned long gsf, unsigned long gpf);
+#endif /* CONFIG_NUMA_BALANCING */
+#endif /* CONFIG_SCHED_DEBUG */
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq);

commit 6b55c9654fccf69ae7ace23ca101dc37b903181b
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jun 25 22:51:41 2015 +0530

    sched/debug: Move print_cfs_rq() declaration to kernel/sched/sched.h
    
    Currently print_cfs_rq() is declared in include/linux/sched.h.
    However it's not used outside kernel/sched. Hence move the
    declaration to kernel/sched/sched.h
    
    Also some functions are only available for CONFIG_SCHED_DEBUG=y.
    Hence move the declarations to within the #ifdef.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Iulia Manda <iulia.manda21@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1435252903-1081-2-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index aea7c1f393cb..7d5895258fe3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1668,9 +1668,14 @@ static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
 
 extern struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq);
 extern struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq);
+
+#ifdef	CONFIG_SCHED_DEBUG
 extern void print_cfs_stats(struct seq_file *m, int cpu);
 extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void print_dl_stats(struct seq_file *m, int cpu);
+extern void
+print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq);
+#endif
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq);

commit 98ec21a01896751b673b6c731ca8881daa8b2c6d
Merge: a262948335bc cbce1a686700
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jun 24 15:09:40 2015 -0700

    Merge branch 'sched-hrtimers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Thomas Gleixner:
     "This series of scheduler updates depends on sched/core and timers/core
      branches, which are already in your tree:
    
       - Scheduler balancing overhaul to plug a hard to trigger race which
         causes an oops in the balancer (Peter Zijlstra)
    
       - Lockdep updates which are related to the balancing updates (Peter
         Zijlstra)"
    
    * 'sched-hrtimers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched,lockdep: Employ lock pinning
      lockdep: Implement lock pinning
      lockdep: Simplify lock_release()
      sched: Streamline the task migration locking a little
      sched: Move code around
      sched,dl: Fix sched class hopping CBS hole
      sched, dl: Convert switched_{from, to}_dl() / prio_changed_dl() to balance callbacks
      sched,dl: Remove return value from pull_dl_task()
      sched, rt: Convert switched_{from, to}_rt() / prio_changed_rt() to balance callbacks
      sched,rt: Remove return value from pull_rt_task()
      sched: Allow balance callbacks for check_class_changed()
      sched: Use replace normalize_task() with __sched_setscheduler()
      sched: Replace post_schedule with a balance callback list

commit 43224b96af3154cedd7220f7b90094905f07ac78
Merge: d70b3ef54cea 1cb6c2151850
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 18:57:44 2015 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "A rather largish update for everything time and timer related:
    
       - Cache footprint optimizations for both hrtimers and timer wheel
    
       - Lower the NOHZ impact on systems which have NOHZ or timer migration
         disabled at runtime.
    
       - Optimize run time overhead of hrtimer interrupt by making the clock
         offset updates smarter
    
       - hrtimer cleanups and removal of restrictions to tackle some
         problems in sched/perf
    
       - Some more leap second tweaks
    
       - Another round of changes addressing the 2038 problem
    
       - First step to change the internals of clock event devices by
         introducing the necessary infrastructure
    
       - Allow constant folding for usecs/msecs_to_jiffies()
    
       - The usual pile of clockevent/clocksource driver updates
    
      The hrtimer changes contain updates to sched, perf and x86 as they
      depend on them plus changes all over the tree to cleanup API changes
      and redundant code, which got copied all over the place.  The y2038
      changes touch s390 to remove the last non 2038 safe code related to
      boot/persistant clock"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      clocksource: Increase dependencies of timer-stm32 to limit build wreckage
      timer: Minimize nohz off overhead
      timer: Reduce timer migration overhead if disabled
      timer: Stats: Simplify the flags handling
      timer: Replace timer base by a cpu index
      timer: Use hlist for the timer wheel hash buckets
      timer: Remove FIFO "guarantee"
      timers: Sanitize catchup_timer_jiffies() usage
      hrtimer: Allow hrtimer::function() to free the timer
      seqcount: Introduce raw_write_seqcount_barrier()
      seqcount: Rename write_seqcount_barrier()
      hrtimer: Fix hrtimer_is_queued() hole
      hrtimer: Remove HRTIMER_STATE_MIGRATE
      selftest: Timers: Avoid signal deadlock in leap-a-day
      timekeeping: Copy the shadow-timekeeper over the real timekeeper last
      clockevents: Check state instead of mode in suspend/resume path
      selftests: timers: Add leap-second timer edge testing to leap-a-day.c
      ntp: Do leapsecond adjustment in adjtimex read path
      time: Prevent early expiry of hrtimers[CLOCK_REALTIME] at the leap second edge
      ntp: Introduce and use SECS_PER_DAY macro instead of 86400
      ...

commit 178a4d23e4e6a0a90b086dad86697676b49db60a
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Wed May 13 14:01:05 2015 +0800

    sched/deadline: Drop duplicate init_sched_dl_class() declaration
    
    There are two init_sched_dl_class() declarations, this patch drops
    the duplicate.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1431496867-4194-5-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d85455539d5c..d62b2882232b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1290,7 +1290,6 @@ extern void update_max_interval(void);
 extern void init_sched_dl_class(void);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
-extern void init_sched_dl_class(void);
 
 extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);

commit cbce1a686700595de65ee363b9b3283ae85d8fc5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 11 14:46:54 2015 +0200

    sched,lockdep: Employ lock pinning
    
    Employ the new lockdep lock pinning annotation to ensure no
    'accidental' lock-breaks happen with rq->lock.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: ktkhai@parallels.com
    Cc: rostedt@goodmis.org
    Cc: juri.lelli@gmail.com
    Cc: pang.xunlei@linaro.org
    Cc: oleg@redhat.com
    Cc: wanpeng.li@linux.intel.com
    Cc: umgwanakikbuti@gmail.com
    Link: http://lkml.kernel.org/r/20150611124744.003233193@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 62949ab06bc2..ef02d11654cd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1439,8 +1439,10 @@ static inline struct rq *__task_rq_lock(struct task_struct *p)
 	for (;;) {
 		rq = task_rq(p);
 		raw_spin_lock(&rq->lock);
-		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p)))
+		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {
+			lockdep_pin_lock(&rq->lock);
 			return rq;
+		}
 		raw_spin_unlock(&rq->lock);
 
 		while (unlikely(task_on_rq_migrating(p)))
@@ -1477,8 +1479,10 @@ static inline struct rq *task_rq_lock(struct task_struct *p, unsigned long *flag
 		 * If we observe the new cpu in task_rq_lock, the acquire will
 		 * pair with the WMB to ensure we must then also see migrating.
 		 */
-		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p)))
+		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p))) {
+			lockdep_pin_lock(&rq->lock);
 			return rq;
+		}
 		raw_spin_unlock(&rq->lock);
 		raw_spin_unlock_irqrestore(&p->pi_lock, *flags);
 
@@ -1490,6 +1494,7 @@ static inline struct rq *task_rq_lock(struct task_struct *p, unsigned long *flag
 static inline void __task_rq_unlock(struct rq *rq)
 	__releases(rq->lock)
 {
+	lockdep_unpin_lock(&rq->lock);
 	raw_spin_unlock(&rq->lock);
 }
 
@@ -1498,6 +1503,7 @@ task_rq_unlock(struct rq *rq, struct task_struct *p, unsigned long *flags)
 	__releases(rq->lock)
 	__releases(p->pi_lock)
 {
+	lockdep_unpin_lock(&rq->lock);
 	raw_spin_unlock(&rq->lock);
 	raw_spin_unlock_irqrestore(&p->pi_lock, *flags);
 }

commit e3fca9e7cbfb72694a21c886fcdf9f059cfded9c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jun 11 14:46:37 2015 +0200

    sched: Replace post_schedule with a balance callback list
    
    Generalize the post_schedule() stuff into a balance callback list.
    This allows us to more easily use it outside of schedule() and cross
    sched_class.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: ktkhai@parallels.com
    Cc: rostedt@goodmis.org
    Cc: juri.lelli@gmail.com
    Cc: pang.xunlei@linaro.org
    Cc: oleg@redhat.com
    Cc: wanpeng.li@linux.intel.com
    Cc: umgwanakikbuti@gmail.com
    Link: http://lkml.kernel.org/r/20150611124742.424032725@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f10a445910c9..62949ab06bc2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -624,9 +624,10 @@ struct rq {
 	unsigned long cpu_capacity;
 	unsigned long cpu_capacity_orig;
 
+	struct callback_head *balance_callback;
+
 	unsigned char idle_balance;
 	/* For active balancing */
-	int post_schedule;
 	int active_balance;
 	int push_cpu;
 	struct cpu_stop_work active_balance_work;
@@ -767,6 +768,21 @@ extern int migrate_swap(struct task_struct *, struct task_struct *);
 
 #ifdef CONFIG_SMP
 
+static inline void
+queue_balance_callback(struct rq *rq,
+		       struct callback_head *head,
+		       void (*func)(struct rq *rq))
+{
+	lockdep_assert_held(&rq->lock);
+
+	if (unlikely(head->next))
+		return;
+
+	head->func = (void (*)(struct callback_head *))func;
+	head->next = rq->balance_callback;
+	rq->balance_callback = head;
+}
+
 extern void sched_ttwu_pending(void);
 
 #define rcu_dereference_check_sched_domain(p) \
@@ -1192,7 +1208,6 @@ struct sched_class {
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
 	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
 
-	void (*post_schedule) (struct rq *this_rq);
 	void (*task_waking) (struct task_struct *task);
 	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
 

commit 624bbdfac99c50bf03dff9a0023f666b8e965627
Merge: 6f9aad0bc372 887d9dc989eb
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jun 19 00:17:47 2015 +0200

    Merge branch 'timers/core' into sched/hrtimers
    
    Merge sched/core and timers/core so we can apply the sched balancing
    patch queue, which depends on both.

commit 4cfafd3082afc707653aeb82e9f8e7b596fbbfd6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu May 14 12:23:11 2015 +0200

    sched,perf: Fix periodic timers
    
    In the below two commits (see Fixes) we have periodic timers that can
    stop themselves when they're no longer required, but need to be
    (re)-started when their idle condition changes.
    
    Further complications is that we want the timer handler to always do
    the forward such that it will always correctly deal with the overruns,
    and we do not want to race such that the handler has already decided
    to stop, but the (external) restart sees the timer still active and we
    end up with a 'lost' timer.
    
    The problem with the current code is that the re-start can come before
    the callback does the forward, at which point the forward from the
    callback will WARN about forwarding an enqueued timer.
    
    Now, conceptually its easy to detect if you're before or after the fwd
    by comparing the expiration time against the current time. Of course,
    that's expensive (and racy) because we don't have the current time.
    
    Alternatively one could cache this state inside the timer, but then
    everybody pays the overhead of maintaining this extra state, and that
    is undesired.
    
    The only other option that I could see is the external timer_active
    variable, which I tried to kill before. I would love a nicer interface
    for this seemingly simple 'problem' but alas.
    
    Fixes: 272325c4821f ("perf: Fix mux_interval hrtimer wreckage")
    Fixes: 77a4d1a1b9a1 ("sched: Cleanup bandwidth timers")
    Cc: pjt@google.com
    Cc: tglx@linutronix.de
    Cc: klamm@yandex-team.ru
    Cc: mingo@kernel.org
    Cc: bsegall@google.com
    Cc: hpa@zytor.com
    Cc: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20150514102311.GX21418@twins.programming.kicks-ass.net

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 08606a1f8c4d..f9a58ef373b4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -131,6 +131,7 @@ struct rt_bandwidth {
 	ktime_t			rt_period;
 	u64			rt_runtime;
 	struct hrtimer		rt_period_timer;
+	unsigned int		rt_period_active;
 };
 
 void __dl_clear_params(struct task_struct *p);
@@ -215,7 +216,7 @@ struct cfs_bandwidth {
 	s64 hierarchical_quota;
 	u64 runtime_expires;
 
-	int idle;
+	int idle, period_active;
 	struct hrtimer period_timer, slack_timer;
 	struct list_head throttled_cfs_rq;
 
@@ -1406,8 +1407,6 @@ static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }
 static inline void sched_avg_update(struct rq *rq) { }
 #endif
 
-extern void start_bandwidth_timer(struct hrtimer *period_timer, ktime_t period);
-
 /*
  * __task_rq_lock - lock the rq @p resides on.
  */

commit 316c1608d15c736439d4065ed12f306db554b3da
Author: Jason Low <jason.low2@hp.com>
Date:   Tue Apr 28 13:00:20 2015 -0700

    sched, timer: Convert usages of ACCESS_ONCE() in the scheduler to READ_ONCE()/WRITE_ONCE()
    
    ACCESS_ONCE doesn't work reliably on non-scalar types. This patch removes
    the rest of the existing usages of ACCESS_ONCE() in the scheduler, and use
    the new READ_ONCE() and WRITE_ONCE() APIs as appropriate.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Waiman Long <Waiman.Long@hp.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Aswin Chandramouleeswaran <aswin@hp.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Davidlohr Bueso <dave@stgolabs.net>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Cc: Scott J Norton <scott.norton@hp.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1430251224-5764-2-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 09ed26a89f31..d85455539d5c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -713,7 +713,7 @@ DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 
 static inline u64 __rq_clock_broken(struct rq *rq)
 {
-	return ACCESS_ONCE(rq->clock);
+	return READ_ONCE(rq->clock);
 }
 
 static inline u64 rq_clock(struct rq *rq)

commit 3289bdb429884c0279bf9ab72dff7b934f19dfc6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Apr 14 13:19:42 2015 +0200

    sched: Move the loadavg code to a more obvious location
    
    I could not find the loadavg code.. turns out it was hidden in a file
    called proc.c. It further got mingled up with the cruft per rq load
    indexes (which we really want to get rid of).
    
    Move the per rq load indexes into the fair.c load-balance code (that's
    the only thing that uses them) and rename proc.c to loadavg.c so we
    can find it again.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    [ Did minor cleanups to the code. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e0e129993958..09ed26a89f31 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -26,8 +26,14 @@ extern __read_mostly int scheduler_running;
 extern unsigned long calc_load_update;
 extern atomic_long_t calc_load_tasks;
 
+extern void calc_global_load_tick(struct rq *this_rq);
 extern long calc_load_fold_active(struct rq *this_rq);
+
+#ifdef CONFIG_SMP
 extern void update_cpu_load_active(struct rq *this_rq);
+#else
+static inline void update_cpu_load_active(struct rq *this_rq) { }
+#endif
 
 /*
  * Helpers for converting nanosecond timing to jiffy resolution
@@ -1298,8 +1304,6 @@ extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 
 unsigned long to_ratio(u64 period, u64 runtime);
 
-extern void update_idle_cpu_load(struct rq *this_rq);
-
 extern void init_task_runnable_average(struct task_struct *p);
 
 static inline void add_nr_running(struct rq *rq, unsigned count)

commit 77a4d1a1b9a122ca1fa3507bd30aec1520d7a8a4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Apr 15 11:41:57 2015 +0200

    sched: Cleanup bandwidth timers
    
    Roman reported a 3 cpu lockup scenario involving __start_cfs_bandwidth().
    
    The more I look at that code the more I'm convinced its crack, that
    entire __start_cfs_bandwidth() thing is brain melting, we don't need to
    cancel a timer before starting it, *hrtimer_start*() will happily remove
    the timer for you if its still enqueued.
    
    Removing that, removes a big part of the problem, no more ugly cancel
    loop to get stuck in.
    
    So now, if I understand things right, the entire reason you have this
    cfs_b->lock guarded ->timer_active nonsense is to make sure we don't
    accidentally lose the timer.
    
    It appears to me that it should be possible to guarantee that same by
    unconditionally (re)starting the timer when !queued. Because regardless
    what hrtimer::function will return, if we beat it to (re)enqueue the
    timer, it doesn't matter.
    
    Now, because hrtimers don't come with any serialization guarantees we
    must ensure both handler and (re)start loop serialize their access to
    the hrtimer to avoid both trying to forward the timer at the same
    time.
    
    Update the rt bandwidth timer to match.
    
    This effectively reverts: 09dc4ab03936 ("sched/fair: Fix
    tg_set_cfs_bandwidth() deadlock on rq->lock").
    
    Reported-by: Roman Gushchin <klamm@yandex-team.ru>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20150415095011.804589208@infradead.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e0e129993958..08606a1f8c4d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -215,7 +215,7 @@ struct cfs_bandwidth {
 	s64 hierarchical_quota;
 	u64 runtime_expires;
 
-	int idle, timer_active;
+	int idle;
 	struct hrtimer period_timer, slack_timer;
 	struct list_head throttled_cfs_rq;
 
@@ -306,7 +306,7 @@ extern void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 
 extern void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b);
-extern void __start_cfs_bandwidth(struct cfs_bandwidth *cfs_b, bool force);
+extern void start_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
 extern void unthrottle_cfs_rq(struct cfs_rq *cfs_rq);
 
 extern void free_rt_sched_group(struct task_group *tg);

commit 07c54f7a7ff77bb47bae26e566969e9c4b6fb0c6
Author: Abel Vesa <abelvesa@gmail.com>
Date:   Tue Mar 3 13:50:27 2015 +0200

    sched/core: Remove unused argument from init_[rt|dl]_rq()
    
    Obviously, 'rq' is not used in these two functions, therefore,
    there is no reason for it to be passed as an argument.
    
    Signed-off-by: Abel Vesa <abelvesa@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1425383427-26244-1-git-send-email-abelvesa@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 91c6736d2522..e0e129993958 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1671,8 +1671,8 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void print_dl_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
-extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
-extern void init_dl_rq(struct dl_rq *dl_rq, struct rq *rq);
+extern void init_rt_rq(struct rt_rq *rt_rq);
+extern void init_dl_rq(struct dl_rq *dl_rq);
 
 extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);

commit dfbca41f347997e57048a53755611c8e2d792924
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Mar 23 14:19:05 2015 +0100

    sched: Optimize freq invariant accounting
    
    Currently the freq invariant accounting (in
    __update_entity_runnable_avg() and sched_rt_avg_update()) get the
    scale factor from a weak function call, this means that even for archs
    that default on their implementation the compiler cannot see into this
    function and optimize the extra scaling math away.
    
    This is sad, esp. since its a 64-bit multiplication which can be quite
    costly on some platforms.
    
    So replace the weak function with #ifdef and __always_inline goo. This
    is not quite as nice from an arch support PoV but should at least
    result in compile time errors if done wrong.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Morten.Rasmussen@arm.com
    Cc: Paul Turner <pjt@google.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/20150323131905.GF23123@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index dd532c558ad4..91c6736d2522 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1387,7 +1387,14 @@ static inline int hrtick_enabled(struct rq *rq)
 
 #ifdef CONFIG_SMP
 extern void sched_avg_update(struct rq *rq);
-extern unsigned long arch_scale_freq_capacity(struct sched_domain *sd, int cpu);
+
+#ifndef arch_scale_freq_capacity
+static __always_inline
+unsigned long arch_scale_freq_capacity(struct sched_domain *sd, int cpu)
+{
+	return SCHED_CAPACITY_SCALE;
+}
+#endif
 
 static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta)
 {

commit dc7ff76eadb4b89fd39bb466b8f3773e5467c11d
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Mar 3 11:35:03 2015 +0100

    sched: Remove unused struct sched_group_capacity::capacity_orig
    
    The 'struct sched_group_capacity::capacity_orig' field is no longer used
    in the scheduler so we can remove it.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Morten.Rasmussen@arm.com
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1425378903-5349-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index be56dfd645b2..dd532c558ad4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -826,7 +826,7 @@ struct sched_group_capacity {
 	 * CPU capacity of this group, SCHED_LOAD_SCALE being max capacity
 	 * for a single CPU.
 	 */
-	unsigned int capacity, capacity_orig;
+	unsigned int capacity;
 	unsigned long next_update;
 	int imbalance; /* XXX unrelated to capacity but shared group state */
 	/*

commit ca6d75e6908efbc350d536e0b496ebdac36b20d2
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Feb 27 16:54:09 2015 +0100

    sched: Add struct rq::cpu_capacity_orig
    
    This new field 'cpu_capacity_orig' reflects the original capacity of a CPU
    before being altered by rt tasks and/or IRQ
    
    The cpu_capacity_orig will be used:
    
      - to detect when the capacity of a CPU has been noticeably reduced so we can
        trig load balance to look for a CPU with better capacity. As an example, we
        can detect when a CPU handles a significant amount of irq
        (with CONFIG_IRQ_TIME_ACCOUNTING) but this CPU is seen as an idle CPU by
        scheduler whereas CPUs, which are really idle, are available.
    
      - evaluate the available capacity for CFS tasks
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Acked-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Morten.Rasmussen@arm.com
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: linaro-kernel@lists.linaro.org
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1425052454-25797-7-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 36000029f33b..be56dfd645b2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -615,6 +615,7 @@ struct rq {
 	struct sched_domain *sd;
 
 	unsigned long cpu_capacity;
+	unsigned long cpu_capacity_orig;
 
 	unsigned char idle_balance;
 	/* For active balancing */

commit b5b4860d1d61ddc5308c7d492cbeaa3a6e508d7f
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Feb 27 16:54:08 2015 +0100

    sched: Make scale_rt invariant with frequency
    
    The average running time of RT tasks is used to estimate the remaining compute
    capacity for CFS tasks. This remaining capacity is the original capacity scaled
    down by a factor (aka scale_rt_capacity). This estimation of available capacity
    must also be invariant with frequency scaling.
    
    A frequency scaling factor is applied on the running time of the RT tasks for
    computing scale_rt_capacity.
    
    In sched_rt_avg_update(), we now scale the RT execution time like below:
    
      rq->rt_avg += rt_delta * arch_scale_freq_capacity() >> SCHED_CAPACITY_SHIFT
    
    Then, scale_rt_capacity can be summarized by:
    
      scale_rt_capacity = SCHED_CAPACITY_SCALE * available / total
    
    with available = total - rq->rt_avg
    
    This has been been optimized in current code by:
    
      scale_rt_capacity = available / (total >> SCHED_CAPACITY_SHIFT)
    
    But we can also developed the equation like below:
    
      scale_rt_capacity = SCHED_CAPACITY_SCALE - ((rq->rt_avg << SCHED_CAPACITY_SHIFT) / total)
    
    and we can optimize the equation by removing SCHED_CAPACITY_SHIFT shift in
    the computation of rq->rt_avg and scale_rt_capacity().
    
    so rq->rt_avg += rt_delta * arch_scale_freq_capacity()
    and
    scale_rt_capacity = SCHED_CAPACITY_SCALE - (rq->rt_avg / total)
    
    arch_scale_frequency_capacity() will be called in the hot path of the scheduler
    which implies to have a short and efficient function.
    
    As an example, arch_scale_frequency_capacity() should return a cached value that
    is updated periodically outside of the hot path.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Morten.Rasmussen@arm.com
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1425052454-25797-6-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4c95cc2e0be2..36000029f33b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1386,9 +1386,11 @@ static inline int hrtick_enabled(struct rq *rq)
 
 #ifdef CONFIG_SMP
 extern void sched_avg_update(struct rq *rq);
+extern unsigned long arch_scale_freq_capacity(struct sched_domain *sd, int cpu);
+
 static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta)
 {
-	rq->rt_avg += rt_delta;
+	rq->rt_avg += rt_delta * arch_scale_freq_capacity(NULL, cpu_of(rq));
 	sched_avg_update(rq);
 }
 #else

commit 36ee28e45df50c2c8624b978335516e42d84ae1f
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Feb 27 16:54:04 2015 +0100

    sched: Add sched_avg::utilization_avg_contrib
    
    Add new statistics which reflect the average time a task is running on the CPU
    and the sum of these running time of the tasks on a runqueue. The latter is
    named utilization_load_avg.
    
    This patch is based on the usage metric that was proposed in the 1st
    versions of the per-entity load tracking patchset by Paul Turner
    <pjt@google.com> but that has be removed afterwards. This version differs from
    the original one in the sense that it's not linked to task_group.
    
    The rq's utilization_load_avg will be used to check if a rq is overloaded or
    not instead of trying to compute how many tasks a group of CPUs can handle.
    
    Rename runnable_avg_period into avg_period as it is now used with both
    runnable_avg_sum and running_avg_sum.
    
    Add some descriptions of the variables to explain their differences.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Morten.Rasmussen@arm.com
    Cc: Paul Turner <pjt@google.com>
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1425052454-25797-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c2c0d7bd5027..4c95cc2e0be2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -363,8 +363,14 @@ struct cfs_rq {
 	 * Under CFS, load is tracked on a per-entity basis and aggregated up.
 	 * This allows for the description of both thread and group usage (in
 	 * the FAIR_GROUP_SCHED case).
+	 * runnable_load_avg is the sum of the load_avg_contrib of the
+	 * sched_entities on the rq.
+	 * blocked_load_avg is similar to runnable_load_avg except that its
+	 * the blocked sched_entities on the rq.
+	 * utilization_load_avg is the sum of the average running time of the
+	 * sched_entities on the rq.
 	 */
-	unsigned long runnable_load_avg, blocked_load_avg;
+	unsigned long runnable_load_avg, blocked_load_avg, utilization_load_avg;
 	atomic64_t decay_counter;
 	u64 last_decay;
 	atomic_long_t removed_load;

commit b6366f048e0caff28af5335b7af2031266e1b06b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Mar 18 14:49:46 2015 -0400

    sched/rt: Use IPI to trigger RT task push migration instead of pulling
    
    When debugging the latencies on a 40 core box, where we hit 300 to
    500 microsecond latencies, I found there was a huge contention on the
    runqueue locks.
    
    Investigating it further, running ftrace, I found that it was due to
    the pulling of RT tasks.
    
    The test that was run was the following:
    
     cyclictest --numa -p95 -m -d0 -i100
    
    This created a thread on each CPU, that would set its wakeup in iterations
    of 100 microseconds. The -d0 means that all the threads had the same
    interval (100us). Each thread sleeps for 100us and wakes up and measures
    its latencies.
    
    cyclictest is maintained at:
     git://git.kernel.org/pub/scm/linux/kernel/git/clrkwllms/rt-tests.git
    
    What happened was another RT task would be scheduled on one of the CPUs
    that was running our test, when the other CPU tests went to sleep and
    scheduled idle. This caused the "pull" operation to execute on all
    these CPUs. Each one of these saw the RT task that was overloaded on
    the CPU of the test that was still running, and each one tried
    to grab that task in a thundering herd way.
    
    To grab the task, each thread would do a double rq lock grab, grabbing
    its own lock as well as the rq of the overloaded CPU. As the sched
    domains on this box was rather flat for its size, I saw up to 12 CPUs
    block on this lock at once. This caused a ripple affect with the
    rq locks especially since the taking was done via a double rq lock, which
    means that several of the CPUs had their own rq locks held while trying
    to take this rq lock. As these locks were blocked, any wakeups or load
    balanceing on these CPUs would also block on these locks, and the wait
    time escalated.
    
    I've tried various methods to lessen the load, but things like an
    atomic counter to only let one CPU grab the task wont work, because
    the task may have a limited affinity, and we may pick the wrong
    CPU to take that lock and do the pull, to only find out that the
    CPU we picked isn't in the task's affinity.
    
    Instead of doing the PULL, I now have the CPUs that want the pull to
    send over an IPI to the overloaded CPU, and let that CPU pick what
    CPU to push the task to. No more need to grab the rq lock, and the
    push/pull algorithm still works fine.
    
    With this patch, the latency dropped to just 150us over a 20 hour run.
    Without the patch, the huge latencies would trigger in seconds.
    
    I've created a new sched feature called RT_PUSH_IPI, which is enabled
    by default.
    
    When RT_PUSH_IPI is not enabled, the old method of grabbing the rq locks
    and having the pulling CPU do the work is implemented. When RT_PUSH_IPI
    is enabled, the IPI is sent to the overloaded CPU to do a push.
    
    To enabled or disable this at run time:
    
     # mount -t debugfs nodev /sys/kernel/debug
     # echo RT_PUSH_IPI > /sys/kernel/debug/sched_features
    or
     # echo NO_RT_PUSH_IPI > /sys/kernel/debug/sched_features
    
    Update: This original patch would send an IPI to all CPUs in the RT overload
    list. But that could theoretically cause the reverse issue. That is, there
    could be lots of overloaded RT queues and one CPU lowers its priority. It would
    then send an IPI to all the overloaded RT queues and they could then all try
    to grab the rq lock of the CPU lowering its priority, and then we have the
    same problem.
    
    The latest design sends out only one IPI to the first overloaded CPU. It tries to
    push any tasks that it can, and then looks for the next overloaded CPU that can
    push to the source CPU. The IPIs stop when all overloaded CPUs that have pushable
    tasks that have priorities greater than the source CPU are covered. In case the
    source CPU lowers its priority again, a flag is set to tell the IPI traversal to
    restart with the first RT overloaded CPU after the source CPU.
    
    Parts-suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Joern Engel <joern@purestorage.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20150318144946.2f3cc982@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index dc0f435a2779..c2c0d7bd5027 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -6,6 +6,7 @@
 #include <linux/mutex.h>
 #include <linux/spinlock.h>
 #include <linux/stop_machine.h>
+#include <linux/irq_work.h>
 #include <linux/tick.h>
 #include <linux/slab.h>
 
@@ -418,6 +419,11 @@ static inline int rt_bandwidth_enabled(void)
 	return sysctl_sched_rt_runtime >= 0;
 }
 
+/* RT IPI pull logic requires IRQ_WORK */
+#ifdef CONFIG_IRQ_WORK
+# define HAVE_RT_PUSH_IPI
+#endif
+
 /* Real-Time classes' related field in a runqueue: */
 struct rt_rq {
 	struct rt_prio_array active;
@@ -435,7 +441,13 @@ struct rt_rq {
 	unsigned long rt_nr_total;
 	int overloaded;
 	struct plist_head pushable_tasks;
+#ifdef HAVE_RT_PUSH_IPI
+	int push_flags;
+	int push_cpu;
+	struct irq_work push_work;
+	raw_spinlock_t push_lock;
 #endif
+#endif /* CONFIG_SMP */
 	int rt_queued;
 
 	int rt_throttled;

commit 3960c8c0c7891dfc0f7be687cbdabb0d6916d614
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Feb 17 13:22:25 2015 +0100

    sched: Make dl_task_time() use task_rq_lock()
    
    Kirill reported that a dl task can be throttled and dequeued at the
    same time. This happens, when it becomes throttled in schedule(),
    which is called to go to sleep:
    
    current->state = TASK_INTERRUPTIBLE;
    schedule()
        deactivate_task()
            dequeue_task_dl()
                update_curr_dl()
                    start_dl_timer()
                __dequeue_task_dl()
        prev->on_rq = 0;
    
    This invalidates the assumption from commit 0f397f2c90ce ("sched/dl:
    Fix race in dl_task_timer()"):
    
      "The only reason we don't strictly need ->pi_lock now is because
       we're guaranteed to have p->state == TASK_RUNNING here and are
       thus free of ttwu races".
    
    And therefore we have to use the full task_rq_lock() here.
    
    This further amends the fact that we forgot to update the rq lock loop
    for TASK_ON_RQ_MIGRATE, from commit cca26e8009d1 ("sched: Teach
    scheduler to understand TASK_ON_RQ_MIGRATING state").
    
    Reported-by: Kirill Tkhai <ktkhai@parallels.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Link: http://lkml.kernel.org/r/20150217123139.GN5029@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0870db23d79c..dc0f435a2779 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1380,6 +1380,82 @@ static inline void sched_avg_update(struct rq *rq) { }
 
 extern void start_bandwidth_timer(struct hrtimer *period_timer, ktime_t period);
 
+/*
+ * __task_rq_lock - lock the rq @p resides on.
+ */
+static inline struct rq *__task_rq_lock(struct task_struct *p)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+	lockdep_assert_held(&p->pi_lock);
+
+	for (;;) {
+		rq = task_rq(p);
+		raw_spin_lock(&rq->lock);
+		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p)))
+			return rq;
+		raw_spin_unlock(&rq->lock);
+
+		while (unlikely(task_on_rq_migrating(p)))
+			cpu_relax();
+	}
+}
+
+/*
+ * task_rq_lock - lock p->pi_lock and lock the rq @p resides on.
+ */
+static inline struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)
+	__acquires(p->pi_lock)
+	__acquires(rq->lock)
+{
+	struct rq *rq;
+
+	for (;;) {
+		raw_spin_lock_irqsave(&p->pi_lock, *flags);
+		rq = task_rq(p);
+		raw_spin_lock(&rq->lock);
+		/*
+		 *	move_queued_task()		task_rq_lock()
+		 *
+		 *	ACQUIRE (rq->lock)
+		 *	[S] ->on_rq = MIGRATING		[L] rq = task_rq()
+		 *	WMB (__set_task_cpu())		ACQUIRE (rq->lock);
+		 *	[S] ->cpu = new_cpu		[L] task_rq()
+		 *					[L] ->on_rq
+		 *	RELEASE (rq->lock)
+		 *
+		 * If we observe the old cpu in task_rq_lock, the acquire of
+		 * the old rq->lock will fully serialize against the stores.
+		 *
+		 * If we observe the new cpu in task_rq_lock, the acquire will
+		 * pair with the WMB to ensure we must then also see migrating.
+		 */
+		if (likely(rq == task_rq(p) && !task_on_rq_migrating(p)))
+			return rq;
+		raw_spin_unlock(&rq->lock);
+		raw_spin_unlock_irqrestore(&p->pi_lock, *flags);
+
+		while (unlikely(task_on_rq_migrating(p)))
+			cpu_relax();
+	}
+}
+
+static inline void __task_rq_unlock(struct rq *rq)
+	__releases(rq->lock)
+{
+	raw_spin_unlock(&rq->lock);
+}
+
+static inline void
+task_rq_unlock(struct rq *rq, struct task_struct *p, unsigned long *flags)
+	__releases(rq->lock)
+	__releases(p->pi_lock)
+{
+	raw_spin_unlock(&rq->lock);
+	raw_spin_unlock_irqrestore(&p->pi_lock, *flags);
+}
+
 #ifdef CONFIG_SMP
 #ifdef CONFIG_PREEMPT
 

commit 9edfbfed3f544a7830d99b341f0c175995a02950
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 5 11:18:11 2015 +0100

    sched/core: Rework rq->clock update skips
    
    The original purpose of rq::skip_clock_update was to avoid 'costly' clock
    updates for back to back wakeup-preempt pairs. The big problem with it
    has always been that the rq variable is unaware of the context and
    causes indiscrimiate clock skips.
    
    Rework the entire thing and create a sense of context by only allowing
    schedule() to skip clock updates. (XXX can we measure the cost of the
    added store?)
    
    By ensuring only schedule can ever skip an update, we guarantee we're
    never more than 1 tick behind on the update.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: umgwanakikbuti@gmail.com
    Link: http://lkml.kernel.org/r/20150105103554.432381549@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index bd2373273a9e..0870db23d79c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -558,8 +558,6 @@ struct rq {
 #ifdef CONFIG_NO_HZ_FULL
 	unsigned long last_sched_tick;
 #endif
-	int skip_clock_update;
-
 	/* capture load from *all* tasks on this cpu: */
 	struct load_weight load;
 	unsigned long nr_load_updates;
@@ -588,6 +586,7 @@ struct rq {
 	unsigned long next_balance;
 	struct mm_struct *prev_mm;
 
+	unsigned int clock_skip_update;
 	u64 clock;
 	u64 clock_task;
 
@@ -704,6 +703,18 @@ static inline u64 rq_clock_task(struct rq *rq)
 	return rq->clock_task;
 }
 
+#define RQCF_REQ_SKIP	0x01
+#define RQCF_ACT_SKIP	0x02
+
+static inline void rq_clock_skip_update(struct rq *rq, bool skip)
+{
+	lockdep_assert_held(&rq->lock);
+	if (skip)
+		rq->clock_skip_update |= RQCF_REQ_SKIP;
+	else
+		rq->clock_skip_update &= ~RQCF_REQ_SKIP;
+}
+
 #ifdef CONFIG_NUMA
 enum numa_topology_type {
 	NUMA_DIRECT,

commit cebde6d681aa45f96111cfcffc1544cf2a0454ff
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 5 11:18:10 2015 +0100

    sched/core: Validate rq_clock*() serialization
    
    rq->clock{,_task} are serialized by rq->lock, verify this.
    
    One immediate fail is the usage in scale_rt_capability, so 'annotate'
    that for now, there's more 'funny' there. Maybe change rq->lock into a
    raw_seqlock_t?
    
    (Only 32-bit is affected)
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20150105103554.361872747@infradead.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: umgwanakikbuti@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9a2a45c970e7..bd2373273a9e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -687,13 +687,20 @@ DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 #define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
 #define raw_rq()		raw_cpu_ptr(&runqueues)
 
+static inline u64 __rq_clock_broken(struct rq *rq)
+{
+	return ACCESS_ONCE(rq->clock);
+}
+
 static inline u64 rq_clock(struct rq *rq)
 {
+	lockdep_assert_held(&rq->lock);
 	return rq->clock;
 }
 
 static inline u64 rq_clock_task(struct rq *rq)
 {
+	lockdep_assert_held(&rq->lock);
 	return rq->clock_task;
 }
 

commit e9ac5f0fa8549dffe2a15870217a9c2e7cd557ec
Merge: 44dba3d5d6a1 6e998916dfe3
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sun Nov 16 10:50:25 2014 +0100

    Merge branch 'sched/urgent' into sched/core, to pick up fixes before applying more changes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6e998916dfe327e785e7c2447959b2c1a3ea4930
Author: Stanislaw Gruszka <sgruszka@redhat.com>
Date:   Wed Nov 12 16:58:44 2014 +0100

    sched/cputime: Fix clock_nanosleep()/clock_gettime() inconsistency
    
    Commit d670ec13178d0 "posix-cpu-timers: Cure SMP wobbles" fixes one glibc
    test case in cost of breaking another one. After that commit, calling
    clock_nanosleep(TIMER_ABSTIME, X) and then clock_gettime(&Y) can result
    of Y time being smaller than X time.
    
    Reproducer/tester can be found further below, it can be compiled and ran by:
    
            gcc -o tst-cpuclock2 tst-cpuclock2.c -pthread
            while ./tst-cpuclock2 ; do : ; done
    
    This reproducer, when running on a buggy kernel, will complain
    about "clock_gettime difference too small".
    
    Issue happens because on start in thread_group_cputimer() we initialize
    sum_exec_runtime of cputimer with threads runtime not yet accounted and
    then add the threads runtime to running cputimer again on scheduler
    tick, making it's sum_exec_runtime bigger than actual threads runtime.
    
    KOSAKI Motohiro posted a fix for this problem, but that patch was never
    applied: https://lkml.org/lkml/2013/5/26/191 .
    
    This patch takes different approach to cure the problem. It calls
    update_curr() when cputimer starts, that assure we will have updated
    stats of running threads and on the next schedule tick we will account
    only the runtime that elapsed from cputimer start. That also assure we
    have consistent state between cpu times of individual threads and cpu
    time of the process consisted by those threads.
    
    Full reproducer (tst-cpuclock2.c):
    
            #define _GNU_SOURCE
            #include <unistd.h>
            #include <sys/syscall.h>
            #include <stdio.h>
            #include <time.h>
            #include <pthread.h>
            #include <stdint.h>
            #include <inttypes.h>
    
            /* Parameters for the Linux kernel ABI for CPU clocks.  */
            #define CPUCLOCK_SCHED          2
            #define MAKE_PROCESS_CPUCLOCK(pid, clock) \
                    ((~(clockid_t) (pid) << 3) | (clockid_t) (clock))
    
            static pthread_barrier_t barrier;
    
            /* Help advance the clock.  */
            static void *chew_cpu(void *arg)
            {
                    pthread_barrier_wait(&barrier);
                    while (1) ;
    
                    return NULL;
            }
    
            /* Don't use the glibc wrapper.  */
            static int do_nanosleep(int flags, const struct timespec *req)
            {
                    clockid_t clock_id = MAKE_PROCESS_CPUCLOCK(0, CPUCLOCK_SCHED);
    
                    return syscall(SYS_clock_nanosleep, clock_id, flags, req, NULL);
            }
    
            static int64_t tsdiff(const struct timespec *before, const struct timespec *after)
            {
                    int64_t before_i = before->tv_sec * 1000000000ULL + before->tv_nsec;
                    int64_t after_i = after->tv_sec * 1000000000ULL + after->tv_nsec;
    
                    return after_i - before_i;
            }
    
            int main(void)
            {
                    int result = 0;
                    pthread_t th;
    
                    pthread_barrier_init(&barrier, NULL, 2);
    
                    if (pthread_create(&th, NULL, chew_cpu, NULL) != 0) {
                            perror("pthread_create");
                            return 1;
                    }
    
                    pthread_barrier_wait(&barrier);
    
                    /* The test.  */
                    struct timespec before, after, sleeptimeabs;
                    int64_t sleepdiff, diffabs;
                    const struct timespec sleeptime = {.tv_sec = 0,.tv_nsec = 100000000 };
    
                    /* The relative nanosleep.  Not sure why this is needed, but its presence
                       seems to make it easier to reproduce the problem.  */
                    if (do_nanosleep(0, &sleeptime) != 0) {
                            perror("clock_nanosleep");
                            return 1;
                    }
    
                    /* Get the current time.  */
                    if (clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &before) < 0) {
                            perror("clock_gettime[2]");
                            return 1;
                    }
    
                    /* Compute the absolute sleep time based on the current time.  */
                    uint64_t nsec = before.tv_nsec + sleeptime.tv_nsec;
                    sleeptimeabs.tv_sec = before.tv_sec + nsec / 1000000000;
                    sleeptimeabs.tv_nsec = nsec % 1000000000;
    
                    /* Sleep for the computed time.  */
                    if (do_nanosleep(TIMER_ABSTIME, &sleeptimeabs) != 0) {
                            perror("absolute clock_nanosleep");
                            return 1;
                    }
    
                    /* Get the time after the sleep.  */
                    if (clock_gettime(CLOCK_PROCESS_CPUTIME_ID, &after) < 0) {
                            perror("clock_gettime[3]");
                            return 1;
                    }
    
                    /* The time after sleep should always be equal to or after the absolute sleep
                       time passed to clock_nanosleep.  */
                    sleepdiff = tsdiff(&sleeptimeabs, &after);
                    if (sleepdiff < 0) {
                            printf("absolute clock_nanosleep woke too early: %" PRId64 "\n", sleepdiff);
                            result = 1;
    
                            printf("Before %llu.%09llu\n", before.tv_sec, before.tv_nsec);
                            printf("After  %llu.%09llu\n", after.tv_sec, after.tv_nsec);
                            printf("Sleep  %llu.%09llu\n", sleeptimeabs.tv_sec, sleeptimeabs.tv_nsec);
                    }
    
                    /* The difference between the timestamps taken before and after the
                       clock_nanosleep call should be equal to or more than the duration of the
                       sleep.  */
                    diffabs = tsdiff(&before, &after);
                    if (diffabs < sleeptime.tv_nsec) {
                            printf("clock_gettime difference too small: %" PRId64 "\n", diffabs);
                            result = 1;
                    }
    
                    pthread_cancel(th);
    
                    return result;
            }
    
    Signed-off-by: Stanislaw Gruszka <sgruszka@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20141112155843.GA24803@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 24156c8434d1..2df8ef067cc5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1135,6 +1135,8 @@ struct sched_class {
 	unsigned int (*get_rr_interval) (struct rq *rq,
 					 struct task_struct *task);
 
+	void (*update_curr) (struct rq *rq);
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	void (*task_move_group) (struct task_struct *p, int on_rq);
 #endif

commit 44dba3d5d6a10685fb15bd1954e62016334825e0
Author: Iulia Manda <iulia.manda21@gmail.com>
Date:   Fri Oct 31 02:13:31 2014 +0200

    sched: Refactor task_struct to use numa_faults instead of numa_* pointers
    
    This patch simplifies task_struct by removing the four numa_* pointers
    in the same array and replacing them with the array pointer. By doing this,
    on x86_64, the size of task_struct is reduced by 3 ulong pointers (24 bytes on
    x86_64).
    
    A new parameter is added to the task_faults_idx function so that it can return
    an index to the correct offset, corresponding with the old precalculated
    pointers.
    
    All of the code in sched/ that depended on task_faults_idx and numa_* was
    changed in order to match the new logic.
    
    Signed-off-by: Iulia Manda <iulia.manda21@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: mgorman@suse.de
    Cc: dave@stgolabs.net
    Cc: riel@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20141031001331.GA30662@winterfell
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7e5c1eebc110..31f1e4d2996a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -709,6 +709,13 @@ extern bool find_numa_distance(int distance);
 #endif
 
 #ifdef CONFIG_NUMA_BALANCING
+/* The regions in numa_faults array from task_struct */
+enum numa_faults_stats {
+	NUMA_MEM = 0,
+	NUMA_CPU,
+	NUMA_MEMBUF,
+	NUMA_CPUBUF
+};
 extern void sched_setnuma(struct task_struct *p, int node);
 extern int migrate_task_to(struct task_struct *p, int cpu);
 extern int migrate_swap(struct task_struct *, struct task_struct *);

commit acb32132ec0433c03bed750f3e9508dc29db0328
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Fri Oct 31 06:39:33 2014 +0800

    sched/deadline: Add deadline rq status print
    
    This patch add deadline rq status print.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Kirill Tkhai <ktkhai@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1414708776-124078-3-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 49b941fe2cc2..7e5c1eebc110 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1537,6 +1537,7 @@ extern struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq);
 extern struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq);
 extern void print_cfs_stats(struct seq_file *m, int cpu);
 extern void print_rt_stats(struct seq_file *m, int cpu);
+extern void print_dl_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);

commit 67dfa1b756f250972bde31d65e3f8fde6aeddc5b
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Mon Oct 27 17:40:52 2014 +0300

    sched/deadline: Implement cancel_dl_timer() to use in switched_from_dl()
    
    Currently used hrtimer_try_to_cancel() is racy:
    
    raw_spin_lock(&rq->lock)
    ...                            dl_task_timer                 raw_spin_lock(&rq->lock)
    ...                               raw_spin_lock(&rq->lock)   ...
       switched_from_dl()             ...                        ...
          hrtimer_try_to_cancel()     ...                        ...
       switched_to_fair()             ...                        ...
    ...                               ...                        ...
    ...                               ...                        ...
    raw_spin_unlock(&rq->lock)        ...                        (asquired)
    ...                               ...                        ...
    ...                               ...                        ...
    do_exit()                         ...                        ...
       schedule()                     ...                        ...
          raw_spin_lock(&rq->lock)    ...                        raw_spin_unlock(&rq->lock)
          ...                         ...                        ...
          raw_spin_unlock(&rq->lock)  ...                        raw_spin_lock(&rq->lock)
          ...                         ...                        (asquired)
          put_task_struct()           ...                        ...
              free_task_struct()      ...                        ...
          ...                         ...                        raw_spin_unlock(&rq->lock)
    ...                               (asquired)                 ...
    ...                               ...                        ...
    ...                               (use after free)           ...
    
    So, let's implement 100% guaranteed way to cancel the timer and let's
    be sure we are safe even in very unlikely situations.
    
    rq unlocking does not limit the area of switched_from_dl() use, because
    this has already been possible in pull_dl_task() below.
    
    Let's consider the safety of of this unlocking. New code in the patch
    is working when hrtimer_try_to_cancel() fails. This means the callback
    is running. In this case hrtimer_cancel() is just waiting till the
    callback is finished. Two
    
    1) Since we are in switched_from_dl(), new class is not dl_sched_class and
    new prio is not less MAX_DL_PRIO. So, the callback returns early; it's
    right after !dl_task() check. After that hrtimer_cancel() returns back too.
    
    The above is:
    
    raw_spin_lock(rq->lock);                  ...
    ...                                       dl_task_timer()
    ...                                          raw_spin_lock(rq->lock);
       switched_from_dl()                        ...
           hrtimer_try_to_cancel()               ...
              raw_spin_unlock(rq->lock);         ...
              hrtimer_cancel()                   ...
              ...                                raw_spin_unlock(rq->lock);
              ...                                return HRTIMER_NORESTART;
              ...                             ...
              raw_spin_lock(rq->lock);        ...
    
    2) But the below is also possible:
                                       dl_task_timer()
                                          raw_spin_lock(rq->lock);
                                          ...
                                          raw_spin_unlock(rq->lock);
    raw_spin_lock(rq->lock);              ...
       switched_from_dl()                 ...
           hrtimer_try_to_cancel()        ...
           ...                            return HRTIMER_NORESTART;
           raw_spin_unlock(rq->lock);  ...
           hrtimer_cancel();           ...
           raw_spin_lock(rq->lock);    ...
    
    In this case hrtimer_cancel() returns immediately. Very unlikely case,
    just to mention.
    
    Nobody can manipulate the task, because check_class_changed() is
    always called with pi_lock locked. Nobody can force the task to
    participate in (concurrent) priority inheritance schemes (the same reason).
    
    All concurrent task operations require pi_lock, which is held by us.
    No deadlocks with dl_task_timer() are possible, because it returns
    right after !dl_task() check (it does nothing).
    
    If we receive a new dl_task during the time of unlocked rq, we just
    don't have to do pull_dl_task() in switched_from_dl() further.
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    [ Added comments]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Juri Lelli <juri.lelli@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1414420852.19914.186.camel@tkhai
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ec3917c5f898..49b941fe2cc2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1157,6 +1157,11 @@ struct sched_class {
 	void (*task_fork) (struct task_struct *p);
 	void (*task_dead) (struct task_struct *p);
 
+	/*
+	 * The switched_from() call is allowed to drop rq->lock, therefore we
+	 * cannot assume the switched_from/switched_to pair is serliazed by
+	 * rq->lock. They are however serialized by p->pi_lock.
+	 */
 	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
 	void (*switched_to) (struct rq *this_rq, struct task_struct *task);
 	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,

commit 7f51412a415d87ea8598d14722fb31e4f5701257
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Fri Sep 19 10:22:40 2014 +0100

    sched/deadline: Fix bandwidth check/update when migrating tasks between exclusive cpusets
    
    Exclusive cpusets are the only way users can restrict SCHED_DEADLINE tasks
    affinity (performing what is commonly called clustered scheduling).
    Unfortunately, such thing is currently broken for two reasons:
    
     - No check is performed when the user tries to attach a task to
       an exlusive cpuset (recall that exclusive cpusets have an
       associated maximum allowed bandwidth).
    
     - Bandwidths of source and destination cpusets are not correctly
       updated after a task is migrated between them.
    
    This patch fixes both things at once, as they are opposite faces
    of the same coin.
    
    The check is performed in cpuset_can_attach(), as there aren't any
    points of failure after that function. The updated is split in two
    halves. We first reserve bandwidth in the destination cpuset, after
    we pass the check in cpuset_can_attach(). And we then release
    bandwidth from the source cpuset when the task's affinity is
    actually changed. Even if there can be time windows when sched_setattr()
    may erroneously fail in the source cpuset, we are fine with it, as
    we can't perfom an atomic update of both cpusets at once.
    
    Reported-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Reported-by: Vincent Legout <vincent@legout.info>
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Dario Faggioli <raistlin@linux.it>
    Cc: Michael Trimarchi <michael@amarulasolutions.com>
    Cc: Fabio Checconi <fchecconi@gmail.com>
    Cc: michael@amarulasolutions.com
    Cc: luca.abeni@unitn.it
    Cc: Li Zefan <lizefan@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: cgroups@vger.kernel.org
    Link: http://lkml.kernel.org/r/1411118561-26323-3-git-send-email-juri.lelli@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 57aacea1cbdf..ec3917c5f898 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -176,6 +176,25 @@ struct dl_bw {
 	u64 bw, total_bw;
 };
 
+static inline
+void __dl_clear(struct dl_bw *dl_b, u64 tsk_bw)
+{
+	dl_b->total_bw -= tsk_bw;
+}
+
+static inline
+void __dl_add(struct dl_bw *dl_b, u64 tsk_bw)
+{
+	dl_b->total_bw += tsk_bw;
+}
+
+static inline
+bool __dl_overflow(struct dl_bw *dl_b, int cpus, u64 old_bw, u64 new_bw)
+{
+	return dl_b->bw != -1 &&
+	       dl_b->bw * cpus < dl_b->total_bw - old_bw + new_bw;
+}
+
 extern struct mutex sched_domains_mutex;
 
 #ifdef CONFIG_CGROUP_SCHED

commit e3fe70b1f72e3f83a00d9c332ec09ab347a981e2
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Oct 17 03:29:50 2014 -0400

    sched/numa: Classify the NUMA topology of a system
    
    Smaller NUMA systems tend to have all NUMA nodes directly connected
    to each other. This includes the degenerate case of a system with just
    one node, ie. a non-NUMA system.
    
    Larger systems can have two kinds of NUMA topology, which affects how
    tasks and memory should be placed on the system.
    
    On glueless mesh systems, nodes that are not directly connected to
    each other will bounce traffic through intermediary nodes. Task groups
    can be run closer to each other by moving tasks from a node to an
    intermediary node between it and the task's preferred node.
    
    On NUMA systems with backplane controllers, the intermediary hops
    are incapable of running programs. This creates "islands" of nodes
    that are at an equal distance to anywhere else in the system.
    
    Each kind of topology requires a slightly different placement
    algorithm; this patch provides the mechanism to detect the kind
    of NUMA topology of a system.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Tested-by: Chegu Vinod <chegu_vinod@hp.com>
    [ Changed to use kernel/sched/sched.h ]
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: mgorman@suse.de
    Cc: chegu_vinod@hp.com
    Link: http://lkml.kernel.org/r/1413530994-9732-3-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 443d6e152a03..57aacea1cbdf 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -679,6 +679,12 @@ static inline u64 rq_clock_task(struct rq *rq)
 }
 
 #ifdef CONFIG_NUMA
+enum numa_topology_type {
+	NUMA_DIRECT,
+	NUMA_GLUELESS_MESH,
+	NUMA_BACKPLANE,
+};
+extern enum numa_topology_type sched_numa_topology_type;
 extern int sched_max_numa_distance;
 extern bool find_numa_distance(int distance);
 #endif

commit 9942f79baaaf111d63ebf0862a819278d84fccc4
Author: Rik van Riel <riel@redhat.com>
Date:   Fri Oct 17 03:29:49 2014 -0400

    sched/numa: Export info needed for NUMA balancing on complex topologies
    
    Export some information that is necessary to do placement of
    tasks on systems with multi-level NUMA topologies.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: mgorman@suse.de
    Cc: chegu_vinod@hp.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1413530994-9732-2-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 24156c8434d1..443d6e152a03 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -678,6 +678,11 @@ static inline u64 rq_clock_task(struct rq *rq)
 	return rq->clock_task;
 }
 
+#ifdef CONFIG_NUMA
+extern int sched_max_numa_distance;
+extern bool find_numa_distance(int distance);
+#endif
+
 #ifdef CONFIG_NUMA_BALANCING
 extern void sched_setnuma(struct task_struct *p, int node);
 extern int migrate_task_to(struct task_struct *p, int cpu);

commit 0429fbc0bdc297d64188483ba029a23773ae07b0
Merge: 6929c358972f 513d1a2884a4
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Oct 15 07:48:18 2014 +0200

    Merge branch 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    Pull percpu consistent-ops changes from Tejun Heo:
     "Way back, before the current percpu allocator was implemented, static
      and dynamic percpu memory areas were allocated and handled separately
      and had their own accessors.  The distinction has been gone for many
      years now; however, the now duplicate two sets of accessors remained
      with the pointer based ones - this_cpu_*() - evolving various other
      operations over time.  During the process, we also accumulated other
      inconsistent operations.
    
      This pull request contains Christoph's patches to clean up the
      duplicate accessor situation.  __get_cpu_var() uses are replaced with
      with this_cpu_ptr() and __this_cpu_ptr() with raw_cpu_ptr().
    
      Unfortunately, the former sometimes is tricky thanks to C being a bit
      messy with the distinction between lvalues and pointers, which led to
      a rather ugly solution for cpumask_var_t involving the introduction of
      this_cpu_cpumask_var_ptr().
    
      This converts most of the uses but not all.  Christoph will follow up
      with the remaining conversions in this merge window and hopefully
      remove the obsolete accessors"
    
    * 'for-3.18-consistent-ops' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (38 commits)
      irqchip: Properly fetch the per cpu offset
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t -fix
      ia64: sn_nodepda cannot be assigned to after this_cpu conversion. Use __this_cpu_write.
      percpu: Resolve ambiguities in __get_cpu_var/cpumask_var_t
      Revert "powerpc: Replace __get_cpu_var uses"
      percpu: Remove __this_cpu_ptr
      clocksource: Replace __this_cpu_ptr with raw_cpu_ptr
      sparc: Replace __get_cpu_var uses
      avr32: Replace __get_cpu_var with __this_cpu_write
      blackfin: Replace __get_cpu_var uses
      tile: Use this_cpu_ptr() for hardware counters
      tile: Replace __get_cpu_var uses
      powerpc: Replace __get_cpu_var uses
      alpha: Replace __get_cpu_var
      ia64: Replace __get_cpu_var uses
      s390: cio driver &__get_cpu_var replacements
      s390: Replace __get_cpu_var uses
      mips: Replace __get_cpu_var uses
      MIPS: Replace __get_cpu_var uses in FPU emulator.
      arm: Replace __this_cpu_ptr with raw_cpu_ptr
      ...

commit c55f5158f5606f8a62e694b7e009f59b92ac6258
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Sep 23 17:06:41 2014 +0200

    sched, mips, ia64: Remove __ARCH_WANT_UNLOCKED_CTXSW
    
    Kirill found that there's a subtle race in the
    __ARCH_WANT_UNLOCKED_CTXSW code, and instead of fixing it, remove the
    entire exception because neither arch that uses it seems to actually
    still require it.
    
    Boot tested on mips64el (qemu) only.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: James Hogan <james.hogan@imgtec.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Burton <paul.burton@imgtec.com>
    Cc: Qais Yousef <qais.yousef@imgtec.com>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: oleg@redhat.com
    Cc: linux@roeck-us.net
    Cc: linux-ia64@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-mips@linux-mips.org
    Link: http://lkml.kernel.org/r/20140923150641.GH3312@worktop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 16e1ca9cb7e8..6130251de280 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -975,7 +975,6 @@ static inline int task_on_rq_migrating(struct task_struct *p)
 # define finish_arch_post_lock_switch()	do { } while (0)
 #endif
 
-#ifndef __ARCH_WANT_UNLOCKED_CTXSW
 static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
 {
 #ifdef CONFIG_SMP
@@ -1013,35 +1012,6 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 	raw_spin_unlock_irq(&rq->lock);
 }
 
-#else /* __ARCH_WANT_UNLOCKED_CTXSW */
-static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
-{
-#ifdef CONFIG_SMP
-	/*
-	 * We can optimise this out completely for !SMP, because the
-	 * SMP rebalancing from interrupt is the only thing that cares
-	 * here.
-	 */
-	next->on_cpu = 1;
-#endif
-	raw_spin_unlock(&rq->lock);
-}
-
-static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
-{
-#ifdef CONFIG_SMP
-	/*
-	 * After ->on_cpu is cleared, the task can be moved to a different CPU.
-	 * We must ensure this doesn't happen until the switch is completely
-	 * finished.
-	 */
-	smp_wmb();
-	prev->on_cpu = 0;
-#endif
-	local_irq_enable();
-}
-#endif /* __ARCH_WANT_UNLOCKED_CTXSW */
-
 /*
  * wake flags
  */

commit 442bf3aaf55a91ebfec71da46a4ee10a3c905bcc
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Thu Sep 4 11:32:09 2014 -0400

    sched: Let the scheduler see CPU idle states
    
    When the cpu enters idle, it stores the cpuidle state pointer in its
    struct rq instance which in turn could be used to make a better decision
    when balancing tasks.
    
    As soon as the cpu exits its idle state, the struct rq reference is
    cleared.
    
    There are a couple of situations where the idle state pointer could be changed
    while it is being consulted:
    
    1. For x86/acpi with dynamic c-states, when a laptop switches from battery
       to AC that could result on removing the deeper idle state. The acpi driver
       triggers:
            'acpi_processor_cst_has_changed'
                    'cpuidle_pause_and_lock'
                            'cpuidle_uninstall_idle_handler'
                                    'kick_all_cpus_sync'.
    
    All cpus will exit their idle state and the pointed object will be set to
    NULL.
    
    2. The cpuidle driver is unloaded. Logically that could happen but not
    in practice because the drivers are always compiled in and 95% of them are
    not coded to unregister themselves.  In any case, the unloading code must
    call 'cpuidle_unregister_device', that calls 'cpuidle_pause_and_lock'
    leading to 'kick_all_cpus_sync' as mentioned above.
    
    A race can happen if we use the pointer and then one of these two scenarios
    occurs at the same moment.
    
    In order to be safe, the idle state pointer stored in the rq must be
    used inside a rcu_read_lock section where we are protected with the
    'rcu_barrier' in the 'cpuidle_uninstall_idle_handler' function. The
    idle_get_state() and idle_put_state() accessors should be used to that
    effect.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linux-pm@vger.kernel.org
    Cc: linaro-kernel@lists.linaro.org
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 76f3a38a401c..16e1ca9cb7e8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -14,6 +14,7 @@
 #include "cpuacct.h"
 
 struct rq;
+struct cpuidle_state;
 
 /* task_struct::on_rq states: */
 #define TASK_ON_RQ_QUEUED	1
@@ -643,6 +644,11 @@ struct rq {
 #ifdef CONFIG_SMP
 	struct llist_head wake_list;
 #endif
+
+#ifdef CONFIG_CPU_IDLE
+	/* Must be inspected within a rcu lock section */
+	struct cpuidle_state *idle_state;
+#endif
 };
 
 static inline int cpu_of(struct rq *rq)
@@ -1196,6 +1202,30 @@ static inline void idle_exit_fair(struct rq *rq) { }
 
 #endif
 
+#ifdef CONFIG_CPU_IDLE
+static inline void idle_set_state(struct rq *rq,
+				  struct cpuidle_state *idle_state)
+{
+	rq->idle_state = idle_state;
+}
+
+static inline struct cpuidle_state *idle_get_state(struct rq *rq)
+{
+	WARN_ON(!rcu_read_lock_held());
+	return rq->idle_state;
+}
+#else
+static inline void idle_set_state(struct rq *rq,
+				  struct cpuidle_state *idle_state)
+{
+}
+
+static inline struct cpuidle_state *idle_get_state(struct rq *rq)
+{
+	return NULL;
+}
+#endif
+
 extern void sysrq_sched_debug_show(void);
 extern void sched_init_granularity(void);
 extern void update_max_interval(void);

commit a5e7be3b28a235108c59561bea55eea1072b23b0
Author: Juri Lelli <juri.lelli@arm.com>
Date:   Fri Sep 19 10:22:39 2014 +0100

    sched/deadline: Clear dl_entity params when setscheduling to different class
    
    When a task is using SCHED_DEADLINE and the user setschedules it to a
    different class its sched_dl_entity static parameters are not cleaned
    up. This causes a bug if the user sets it back to SCHED_DEADLINE with
    the same parameters again.  The problem resides in the check we
    perform at the very beginning of dl_overflow():
    
            if (new_bw == p->dl.dl_bw)
                    return 0;
    
    This condition is met in the case depicted above, so the function
    returns and dl_b->total_bw is not updated (the p->dl.dl_bw is not
    added to it). After this, admission control is broken.
    
    This patch fixes the thing, properly clearing static parameters for a
    task that ceases to use SCHED_DEADLINE.
    
    Reported-by: Daniele Alessandrelli <daniele.alessandrelli@gmail.com>
    Reported-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Reported-by: Vincent Legout <vincent@legout.info>
    Tested-by: Luca Abeni <luca.abeni@unitn.it>
    Tested-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Tested-by: Vincent Legout <vincent@legout.info>
    Signed-off-by: Juri Lelli <juri.lelli@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Fabio Checconi <fchecconi@gmail.com>
    Cc: Dario Faggioli <raistlin@linux.it>
    Cc: Michael Trimarchi <michael@amarulasolutions.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1411118561-26323-2-git-send-email-juri.lelli@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1bc6aad1391a..76f3a38a401c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -130,6 +130,9 @@ struct rt_bandwidth {
 	u64			rt_runtime;
 	struct hrtimer		rt_period_timer;
 };
+
+void __dl_clear_params(struct task_struct *p);
+
 /*
  * To keep the bandwidth of -deadline tasks and groups under control
  * we need some place where:

commit 9c58c79a8a76c510cd3a5012c536d4fe3c81ec3b
Author: Zhihui Zhang <zzhsuny@gmail.com>
Date:   Sat Sep 20 21:24:36 2014 -0400

    sched: Clean up some typos and grammatical errors in code/comments
    
    Signed-off-by: Zhihui Zhang <zzhsuny@gmail.com>
    Cc: peterz@infradead.org
    Link: http://lkml.kernel.org/r/1411262676-19928-1-git-send-email-zzhsuny@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index aa0f73ba3777..1bc6aad1391a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -188,7 +188,7 @@ struct cfs_bandwidth {
 	raw_spinlock_t lock;
 	ktime_t period;
 	u64 quota, runtime;
-	s64 hierarchal_quota;
+	s64 hierarchical_quota;
 	u64 runtime_expires;
 
 	int idle, timer_active;

commit 4a32fea9d78f2d2315c0072757b197d5a304dc8b
Author: Christoph Lameter <cl@linux.com>
Date:   Sun Aug 17 12:30:27 2014 -0500

    scheduler: Replace __get_cpu_var with this_cpu_ptr
    
    Convert all uses of __get_cpu_var for address calculation to use
    this_cpu_ptr instead.
    
    [Uses of __get_cpu_var with cpumask_var_t are no longer
    handled by this patch]
    
    Cc: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 579712f4e9d5..77d92f8130e8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -650,10 +650,10 @@ static inline int cpu_of(struct rq *rq)
 DECLARE_PER_CPU(struct rq, runqueues);
 
 #define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
-#define this_rq()		(&__get_cpu_var(runqueues))
+#define this_rq()		this_cpu_ptr(&runqueues)
 #define task_rq(p)		cpu_rq(task_cpu(p))
 #define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
-#define raw_rq()		(&__raw_get_cpu_var(runqueues))
+#define raw_rq()		raw_cpu_ptr(&runqueues)
 
 static inline u64 rq_clock(struct rq *rq)
 {

commit cca26e8009d1939a6a5bf0200d276fa26f03e536
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Wed Aug 20 13:47:42 2014 +0400

    sched: Teach scheduler to understand TASK_ON_RQ_MIGRATING state
    
    This is a new p->on_rq state which will be used to indicate that a task
    is in a process of migrating between two RQs. It allows to get
    rid of double_rq_lock(), which we used to use to change a rq of
    a queued task before.
    
    Let's consider an example. To move a task between src_rq and
    dst_rq we will do the following:
    
            raw_spin_lock(&src_rq->lock);
            /* p is a task which is queued on src_rq */
            p = ...;
    
            dequeue_task(src_rq, p, 0);
            p->on_rq = TASK_ON_RQ_MIGRATING;
            set_task_cpu(p, dst_cpu);
            raw_spin_unlock(&src_rq->lock);
    
            /*
             * Both RQs are unlocked here.
             * Task p is dequeued from src_rq
             * but its on_rq value is not zero.
             */
    
            raw_spin_lock(&dst_rq->lock);
            p->on_rq = TASK_ON_RQ_QUEUED;
            enqueue_task(dst_rq, p, 0);
            raw_spin_unlock(&dst_rq->lock);
    
    While p->on_rq is TASK_ON_RQ_MIGRATING, task is considered as
    "migrating", and other parallel scheduler actions with it are
    not available to parallel callers. The parallel caller is
    spining till migration is completed.
    
    The unavailable actions are changing of cpu affinity, changing
    of priority etc, in other words all the functionality which used
    to require task_rq(p)->lock before (and related to the task).
    
    To implement TASK_ON_RQ_MIGRATING support we primarily are using
    the following fact. Most of scheduler users (from which we are
    protecting a migrating task) use task_rq_lock() and
    __task_rq_lock() to get the lock of task_rq(p). These primitives
    know that task's cpu may change, and they are spining while the
    lock of the right RQ is not held. We add one more condition into
    them, so they will be also spinning until the migration is
    finished.
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1408528062.23412.88.camel@tkhai
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 26566d0c67ac..aa0f73ba3777 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -17,6 +17,7 @@ struct rq;
 
 /* task_struct::on_rq states: */
 #define TASK_ON_RQ_QUEUED	1
+#define TASK_ON_RQ_MIGRATING	2
 
 extern __read_mostly int scheduler_running;
 
@@ -950,6 +951,11 @@ static inline int task_on_rq_queued(struct task_struct *p)
 	return p->on_rq == TASK_ON_RQ_QUEUED;
 }
 
+static inline int task_on_rq_migrating(struct task_struct *p)
+{
+	return p->on_rq == TASK_ON_RQ_MIGRATING;
+}
+
 #ifndef prepare_arch_switch
 # define prepare_arch_switch(next)	do { } while (0)
 #endif

commit da0c1e65b51a289540159663aa4b90ba2366bc21
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Wed Aug 20 13:47:32 2014 +0400

    sched: Add wrapper for checking task_struct::on_rq
    
    Implement task_on_rq_queued() and use it everywhere instead of
    on_rq check. No functional changes.
    
    The only exception is we do not use the wrapper in
    check_for_tasks(), because it requires to export
    task_on_rq_queued() in global header files. Next patch in series
    would return it back, so we do not twist it from here to there.
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Tim Chen <tim.c.chen@linux.intel.com>
    Cc: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1408528052.23412.87.camel@tkhai
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4c2b87fd5f52..26566d0c67ac 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -15,6 +15,9 @@
 
 struct rq;
 
+/* task_struct::on_rq states: */
+#define TASK_ON_RQ_QUEUED	1
+
 extern __read_mostly int scheduler_running;
 
 extern unsigned long calc_load_update;
@@ -942,6 +945,10 @@ static inline int task_running(struct rq *rq, struct task_struct *p)
 #endif
 }
 
+static inline int task_on_rq_queued(struct task_struct *p)
+{
+	return p->on_rq == TASK_ON_RQ_QUEUED;
+}
 
 #ifndef prepare_arch_switch
 # define prepare_arch_switch(next)	do { } while (0)

commit 8b06c55bdb8b402cb4814e83dc4b1cb245fcc9f5
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Wed Aug 13 13:28:12 2014 -0400

    sched: Match declaration with definition
    
    Match the declaration of runqueues with the definition.
    
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1407950893-32731-1-git-send-email-bobby.prani@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 579712f4e9d5..4c2b87fd5f52 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -647,7 +647,7 @@ static inline int cpu_of(struct rq *rq)
 #endif
 }
 
-DECLARE_PER_CPU(struct rq, runqueues);
+DECLARE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues);
 
 #define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
 #define this_rq()		(&__get_cpu_var(runqueues))

commit 6e76ea8a8209386c3cc7ee5594e6ea5d25525cf2
Author: Jason Baron <jbaron@akamai.com>
Date:   Wed Jul 2 15:52:41 2014 +0000

    sched: Remove extra static_key*() function indirection
    
    I think its a bit simpler without having to follow an extra layer of static
    inline fuctions. No functional change just cosmetic.
    
    Signed-off-by: Jason Baron <jbaron@akamai.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: rostedt@goodmis.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/2ce52233ce200faad93b6029d90f1411cd926667.1404315388.git.jbaron@akamai.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1283945d1ace..579712f4e9d5 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -887,20 +887,10 @@ enum {
 #undef SCHED_FEAT
 
 #if defined(CONFIG_SCHED_DEBUG) && defined(HAVE_JUMP_LABEL)
-static __always_inline bool static_branch__true(struct static_key *key)
-{
-	return static_key_true(key); /* Not out of line branch. */
-}
-
-static __always_inline bool static_branch__false(struct static_key *key)
-{
-	return static_key_false(key); /* Out of line branch. */
-}
-
 #define SCHED_FEAT(name, enabled)					\
 static __always_inline bool static_branch_##name(struct static_key *key) \
 {									\
-	return static_branch__##enabled(key);				\
+	return static_key_##enabled(key);				\
 }
 
 #include "features.h"

commit 8875125efe8402c4d84b08291e68f1281baba8e2
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Sun Jun 29 00:03:57 2014 +0400

    sched: Transform resched_task() into resched_curr()
    
    We always use resched_task() with rq->curr argument.
    It's not possible to reschedule any task but rq's current.
    
    The patch introduces resched_curr(struct rq *) to
    replace all of the repeating patterns. The main aim
    is cleanup, but there is a little size profit too:
    
      (before)
            $ size kernel/sched/built-in.o
               text    data     bss     dec     hex filename
            155274    16445    7042  178761   2ba49 kernel/sched/built-in.o
    
            $ size vmlinux
               text    data     bss     dec     hex filename
            7411490 1178376  991232 9581098  92322a vmlinux
    
      (after)
            $ size kernel/sched/built-in.o
               text    data     bss     dec     hex filename
            155130    16445    7042  178617   2b9b9 kernel/sched/built-in.o
    
            $ size vmlinux
               text    data     bss     dec     hex filename
            7411362 1178376  991232 9580970  9231aa vmlinux
    
            I was choosing between resched_curr() and resched_rq(),
            and the first name looks better for me.
    
    A little lie in Documentation/trace/ftrace.txt. I have not
    actually collected the tracing again. With a hope the patch
    won't make execution times much worse :)
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Randy Dunlap <rdunlap@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20140628200219.1778.18735.stgit@localhost
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0191ed563bdd..1283945d1ace 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1199,7 +1199,7 @@ extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
 extern void init_sched_dl_class(void);
 
-extern void resched_task(struct task_struct *p);
+extern void resched_curr(struct rq *rq);
 extern void resched_cpu(int cpu);
 
 extern struct rt_bandwidth def_rt_bandwidth;

commit 4486edd12b5ac8a9af7a5e16e4b9eeb3b8339c10
Author: Tim Chen <tim.c.chen@linux.intel.com>
Date:   Mon Jun 23 12:16:49 2014 -0700

    sched/fair: Implement fast idling of CPUs when the system is partially loaded
    
    When a system is lightly loaded (i.e. no more than 1 job per cpu),
    attempt to pull job to a cpu before putting it to idle is unnecessary and
    can be skipped.  This patch adds an indicator so the scheduler can know
    when there's no more than 1 active job is on any CPU in the system to
    skip needless job pulls.
    
    On a 4 socket machine with a request/response kind of workload from
    clients, we saw about 0.13 msec delay when we go through a full load
    balance to try pull job from all the other cpus.  While 0.1 msec was
    spent on processing the request and generating a response, the 0.13 msec
    load balance overhead was actually more than the actual work being done.
    This overhead can be skipped much of the time for lightly loaded systems.
    
    With this patch, we tested with a netperf request/response workload that
    has the server busy with half the cpus in a 4 socket system.  We found
    the patch eliminated 75% of the load balance attempts before idling a cpu.
    
    The overhead of setting/clearing the indicator is low as we already gather
    the necessary info while we call add_nr_running() and update_sd_lb_stats.()
    We switch to full load balance load immediately if any cpu got more than
    one job on its run queue in add_nr_running.  We'll clear the indicator
    to avoid load balance when we detect no cpu's have more than one job
    when we scan the work queues in update_sg_lb_stats().  We are aggressive
    in turning on the load balance and opportunistic in skipping the load
    balance.
    
    Signed-off-by: Tim Chen <tim.c.chen@linux.intel.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Acked-by: Jason Low <jason.low2@hp.com>
    Cc: "Paul E.McKenney" <paulmck@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Davidlohr Bueso <davidlohr@hp.com>
    Cc: Alex Shi <alex.shi@linaro.org>
    Cc: Michel Lespinasse <walken@google.com>
    Cc: Peter Hurley <peter@hurleysoftware.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1403551009.2970.613.camel@schen9-DESK
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index eb8567610295..0191ed563bdd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -477,6 +477,9 @@ struct root_domain {
 	cpumask_var_t span;
 	cpumask_var_t online;
 
+	/* Indicate more than one runnable task for any CPU */
+	bool overload;
+
 	/*
 	 * The bit corresponding to a CPU gets set here if such CPU has more
 	 * than one runnable -deadline task (as it is below for RT tasks).
@@ -1218,8 +1221,13 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 
 	rq->nr_running = prev_nr + count;
 
-#ifdef CONFIG_NO_HZ_FULL
 	if (prev_nr < 2 && rq->nr_running >= 2) {
+#ifdef CONFIG_SMP
+		if (!rq->rd->overload)
+			rq->rd->overload = true;
+#endif
+
+#ifdef CONFIG_NO_HZ_FULL
 		if (tick_nohz_full_cpu(rq->cpu)) {
 			/*
 			 * Tick is needed if more than one task runs on a CPU.
@@ -1231,8 +1239,8 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 			 */
 			tick_nohz_full_kick_cpu(rq->cpu);
 		}
-       }
 #endif
+	}
 }
 
 static inline void sub_nr_running(struct rq *rq, unsigned count)

commit 3882ec643997757824cd5f25180cd8a787b9dbe1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Mar 18 22:54:04 2014 +0100

    nohz: Use IPI implicit full barrier against rq->nr_running r/w
    
    A full dynticks CPU is allowed to stop its tick when a single task runs.
    Meanwhile when a new task gets enqueued, the CPU must be notified so that
    it can restart its tick to maintain local fairness and other accounting
    details.
    
    This notification is performed by way of an IPI. Then when the target
    receives the IPI, we expect it to see the new value of rq->nr_running.
    
    Hence the following ordering scenario:
    
       CPU 0                   CPU 1
    
       write rq->running       get IPI
       smp_wmb()               smp_rmb()
       send IPI                read rq->nr_running
    
    But Paul Mckenney says that nowadays IPIs imply a full barrier on
    all architectures. So we can safely remove this pair and rely on the
    implicit barriers that come along IPI send/receive. Lets
    just comment on this new assumption.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 599a72aff5ea..eb8567610295 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1221,8 +1221,14 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 #ifdef CONFIG_NO_HZ_FULL
 	if (prev_nr < 2 && rq->nr_running >= 2) {
 		if (tick_nohz_full_cpu(rq->cpu)) {
-			/* Order rq->nr_running write against the IPI */
-			smp_wmb();
+			/*
+			 * Tick is needed if more than one task runs on a CPU.
+			 * Send the target an IPI to kick it out of nohz mode.
+			 *
+			 * We assume that IPI implies full memory barrier and the
+			 * new value of rq->nr_running is visible on reception
+			 * from the target.
+			 */
 			tick_nohz_full_kick_cpu(rq->cpu);
 		}
        }

commit fd2ac4f4a65a7f34b0bc6433fcca1192d7ba8b8e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Mar 18 21:12:53 2014 +0100

    nohz: Use nohz own full kick on 2nd task enqueue
    
    Now that we have a nohz full remote kick based on irq work, lets use
    it to notify a CPU that it's exiting single task mode.
    
    This unbloats a bit the scheduler IPI that the nohz code was abusing
    for its cool "callable anywhere/anytime" properties.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 31cc02ebc54e..599a72aff5ea 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1223,7 +1223,7 @@ static inline void add_nr_running(struct rq *rq, unsigned count)
 		if (tick_nohz_full_cpu(rq->cpu)) {
 			/* Order rq->nr_running write against the IPI */
 			smp_wmb();
-			smp_send_reschedule(rq->cpu);
+			tick_nohz_full_kick_cpu(rq->cpu);
 		}
        }
 #endif

commit b2e09f633a3994ee97fa6bc734b533d9c8e6ea0f
Merge: 3737a1276163 535560d841b2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jun 12 19:42:15 2014 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull more scheduler updates from Ingo Molnar:
     "Second round of scheduler changes:
       - try-to-wakeup and IPI reduction speedups, from Andy Lutomirski
       - continued power scheduling cleanups and refactorings, from Nicolas
         Pitre
       - misc fixes and enhancements"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/deadline: Delete extraneous extern for to_ratio()
      sched/idle: Optimize try-to-wake-up IPI
      sched/idle: Simplify wake_up_idle_cpu()
      sched/idle: Clear polling before descheduling the idle thread
      sched, trace: Add a tracepoint for IPI-less remote wakeups
      cpuidle: Set polling in poll_idle
      sched: Remove redundant assignment to "rt_rq" in update_curr_rt(...)
      sched: Rename capacity related flags
      sched: Final power vs. capacity cleanups
      sched: Remove remaining dubious usage of "power"
      sched: Let 'struct sched_group_power' care about CPU capacity
      sched/fair: Disambiguate existing/remaining "capacity" usage
      sched/fair: Change "has_capacity" to "has_free_capacity"
      sched/fair: Remove "power" from 'struct numa_stats'
      sched: Fix signedness bug in yield_to()
      sched/fair: Use time_after() in record_wakee()
      sched/balancing: Reduce the rate of needless idle load balancing
      sched/fair: Fix unlocked reads of some cfs_b->quota/period

commit 3f17ea6dea8ba5668873afa54628a91aaa3fb1c0
Merge: 1860e379875d 1a5700bc2d10
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jun 8 11:31:16 2014 -0700

    Merge branch 'next' (accumulated 3.16 merge window patches) into master
    
    Now that 3.15 is released, this merges the 'next' branch into 'master',
    bringing us to the normal situation where my 'master' branch is the
    merge window.
    
    * accumulated work in next: (6809 commits)
      ufs: sb mutex merge + mutex_destroy
      powerpc: update comments for generic idle conversion
      cris: update comments for generic idle conversion
      idle: remove cpu_idle() forward declarations
      nbd: zero from and len fields in NBD_CMD_DISCONNECT.
      mm: convert some level-less printks to pr_*
      MAINTAINERS: adi-buildroot-devel is moderated
      MAINTAINERS: add linux-api for review of API/ABI changes
      mm/kmemleak-test.c: use pr_fmt for logging
      fs/dlm/debug_fs.c: replace seq_printf by seq_puts
      fs/dlm/lockspace.c: convert simple_str to kstr
      fs/dlm/config.c: convert simple_str to kstr
      mm: mark remap_file_pages() syscall as deprecated
      mm: memcontrol: remove unnecessary memcg argument from soft limit functions
      mm: memcontrol: clean up memcg zoneinfo lookup
      mm/memblock.c: call kmemleak directly from memblock_(alloc|free)
      mm/mempool.c: update the kmemleak stack trace for mempool allocations
      lib/radix-tree.c: update the kmemleak stack trace for radix tree allocations
      mm: introduce kmemleak_update_trace()
      mm/kmemleak.c: use %u to print ->checksum
      ...

commit e3baac47f0e82c4be632f4f97215bb93bf16b342
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 4 10:31:18 2014 -0700

    sched/idle: Optimize try-to-wake-up IPI
    
    [ This series reduces the number of IPIs on Andy's workload by something like
      99%. It's down from many hundreds per second to very few.
    
      The basic idea behind this series is to make TIF_POLLING_NRFLAG be a
      reliable indication that the idle task is polling.  Once that's done,
      the rest is reasonably straightforward. ]
    
    When enqueueing tasks on remote LLC domains, we send an IPI to do the
    work 'locally' and avoid bouncing all the cachelines over.
    
    However, when the remote CPU is idle (and polling, say x86 mwait), we
    don't need to send an IPI, we can simply kick the TIF word to wake it
    up and have the 'idle' loop do the work.
    
    So when _TIF_POLLING_NRFLAG is set, but _TIF_NEED_RESCHED is not (yet)
    set, set _TIF_NEED_RESCHED and avoid sending the IPI.
    
    Much-requested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [Edited by Andy Lutomirski, but this is mostly Peter Zijlstra's code.]
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: nicolas.pitre@linaro.org
    Cc: daniel.lezcano@linaro.org
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: umgwanakikbuti@gmail.com
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/ce06f8b02e7e337be63e97597fc4b248d3aa6f9b.1401902905.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 956b8ca24893..2f8636199b83 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -670,6 +670,8 @@ extern int migrate_swap(struct task_struct *, struct task_struct *);
 
 #ifdef CONFIG_SMP
 
+extern void sched_ttwu_pending(void);
+
 #define rcu_dereference_check_sched_domain(p) \
 	rcu_dereference_check((p), \
 			      lockdep_is_held(&sched_domains_mutex))
@@ -787,6 +789,10 @@ static inline unsigned int group_first_cpu(struct sched_group *group)
 
 extern int group_balance_cpu(struct sched_group *sg);
 
+#else
+
+static inline void sched_ttwu_pending(void) { }
+
 #endif /* CONFIG_SMP */
 
 #include "stats.h"

commit ced549fa5fc1fdaf7fac93894dc673092eb3dc20
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon May 26 18:19:38 2014 -0400

    sched: Remove remaining dubious usage of "power"
    
    It is better not to think about compute capacity as being equivalent
    to "CPU power".  The upcoming "power aware" scheduler work may create
    confusion with the notion of energy consumption if "power" is used too
    liberally.
    
    This is the remaining "power" -> "capacity" rename for local symbols.
    Those symbols visible to the rest of the kernel are not included yet.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/n/tip-yyyhohzhkwnaotr3lx8zd5aa@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index a5b957d53c92..956b8ca24893 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -567,7 +567,7 @@ struct rq {
 	struct root_domain *rd;
 	struct sched_domain *sd;
 
-	unsigned long cpu_power;
+	unsigned long cpu_capacity;
 
 	unsigned char idle_balance;
 	/* For active balancing */

commit 63b2ca30bdb3dbf60bc7ac5f46713c0d32308261
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Mon May 26 18:19:37 2014 -0400

    sched: Let 'struct sched_group_power' care about CPU capacity
    
    It is better not to think about compute capacity as being equivalent
    to "CPU power".  The upcoming "power aware" scheduler work may create
    confusion with the notion of energy consumption if "power" is used too
    liberally.
    
    Since struct sched_group_power is really about compute capacity of sched
    groups, let's rename it to struct sched_group_capacity. Similarly sgp
    becomes sgc. Related variables and functions dealing with groups are also
    adjusted accordingly.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linaro-kernel@lists.linaro.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/n/tip-5yeix833vvgf2uyj5o36hpu9@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 600e2291a75c..a5b957d53c92 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -728,15 +728,15 @@ DECLARE_PER_CPU(struct sched_domain *, sd_numa);
 DECLARE_PER_CPU(struct sched_domain *, sd_busy);
 DECLARE_PER_CPU(struct sched_domain *, sd_asym);
 
-struct sched_group_power {
+struct sched_group_capacity {
 	atomic_t ref;
 	/*
-	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
-	 * single CPU.
+	 * CPU capacity of this group, SCHED_LOAD_SCALE being max capacity
+	 * for a single CPU.
 	 */
-	unsigned int power, power_orig;
+	unsigned int capacity, capacity_orig;
 	unsigned long next_update;
-	int imbalance; /* XXX unrelated to power but shared group state */
+	int imbalance; /* XXX unrelated to capacity but shared group state */
 	/*
 	 * Number of busy cpus in this group.
 	 */
@@ -750,7 +750,7 @@ struct sched_group {
 	atomic_t ref;
 
 	unsigned int group_weight;
-	struct sched_group_power *sgp;
+	struct sched_group_capacity *sgc;
 
 	/*
 	 * The CPUs this group covers.
@@ -773,7 +773,7 @@ static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
  */
 static inline struct cpumask *sched_group_mask(struct sched_group *sg)
 {
-	return to_cpumask(sg->sgp->cpumask);
+	return to_cpumask(sg->sgc->cpumask);
 }
 
 /**
@@ -1167,7 +1167,7 @@ extern const struct sched_class idle_sched_class;
 
 #ifdef CONFIG_SMP
 
-extern void update_group_power(struct sched_domain *sd, int cpu);
+extern void update_group_capacity(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
 

commit 09dc4ab03936df5c5aa711d27c81283c6d09f495
Author: Roman Gushchin <klamm@yandex-team.ru>
Date:   Mon May 19 15:10:09 2014 +0400

    sched/fair: Fix tg_set_cfs_bandwidth() deadlock on rq->lock
    
    tg_set_cfs_bandwidth() sets cfs_b->timer_active to 0 to
    force the period timer restart. It's not safe, because
    can lead to deadlock, described in commit 927b54fccbf0:
    "__start_cfs_bandwidth calls hrtimer_cancel while holding rq->lock,
    waiting for the hrtimer to finish. However, if sched_cfs_period_timer
    runs for another loop iteration, the hrtimer can attempt to take
    rq->lock, resulting in deadlock."
    
    Three CPUs must be involved:
    
      CPU0               CPU1                         CPU2
      take rq->lock      period timer fired
      ...                take cfs_b lock
      ...                ...                          tg_set_cfs_bandwidth()
      throttle_cfs_rq()  release cfs_b lock           take cfs_b lock
      ...                distribute_cfs_runtime()     timer_active = 0
      take cfs_b->lock   wait for rq->lock            ...
      __start_cfs_bandwidth()
      {wait for timer callback
       break if timer_active == 1}
    
    So, CPU0 and CPU1 are deadlocked.
    
    Instead of resetting cfs_b->timer_active, tg_set_cfs_bandwidth can
    wait for period timer callbacks (ignoring cfs_b->timer_active) and
    restart the timer explicitly.
    
    Signed-off-by: Roman Gushchin <klamm@yandex-team.ru>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/87wqdi9g8e.wl\%klamm@yandex-team.ru
    Cc: pjt@google.com
    Cc: chris.j.arges@canonical.com
    Cc: gregkh@linuxfoundation.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 456e492a3dca..369b4d663c42 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -278,7 +278,7 @@ extern void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
 extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
 
 extern void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b);
-extern void __start_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
+extern void __start_cfs_bandwidth(struct cfs_bandwidth *cfs_b, bool force);
 extern void unthrottle_cfs_rq(struct cfs_rq *cfs_rq);
 
 extern void free_rt_sched_group(struct task_group *tg);

commit 72465447867b9de6b5cdea5d10f9781585136270
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Fri May 9 03:00:14 2014 +0400

    sched, nohz: Change rq->nr_running to always use wrappers
    
    Sometimes ->nr_running may cross 2 but interrupt is not being
    sent to rq's cpu. In this case we don't reenable the timer.
    Looks like this may be the reason for rare unexpected effects,
    if nohz is enabled.
    
    Patch replaces all places of direct changing of nr_running
    and makes add_nr_running() caring about crossing border.
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140508225830.2469.97461.stgit@localhost
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b2cbe81308af..600e2291a75c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1206,12 +1206,14 @@ extern void update_idle_cpu_load(struct rq *this_rq);
 
 extern void init_task_runnable_average(struct task_struct *p);
 
-static inline void inc_nr_running(struct rq *rq)
+static inline void add_nr_running(struct rq *rq, unsigned count)
 {
-	rq->nr_running++;
+	unsigned prev_nr = rq->nr_running;
+
+	rq->nr_running = prev_nr + count;
 
 #ifdef CONFIG_NO_HZ_FULL
-	if (rq->nr_running == 2) {
+	if (prev_nr < 2 && rq->nr_running >= 2) {
 		if (tick_nohz_full_cpu(rq->cpu)) {
 			/* Order rq->nr_running write against the IPI */
 			smp_wmb();
@@ -1221,9 +1223,9 @@ static inline void inc_nr_running(struct rq *rq)
 #endif
 }
 
-static inline void dec_nr_running(struct rq *rq)
+static inline void sub_nr_running(struct rq *rq, unsigned count)
 {
-	rq->nr_running--;
+	rq->nr_running -= count;
 }
 
 static inline void rq_last_tick_reset(struct rq *rq)

commit 46383648b3c769fa74794ae6425ab993fc113bdb
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Sat Mar 15 02:15:07 2014 +0400

    sched: Revert commit 4c6c4e38c4e9 ("sched/core: Fix endless loop in pick_next_task()")
    
    This reverts commit 4c6c4e38c4e9 ("sched/core: Fix endless loop in
    pick_next_task()"), which is not necessary after ("sched/rt: Substract number
    of tasks of throttled queues from rq->nr_running").
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    [conflict resolution with stop task checking patch]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1394835307.18748.34.camel@HP-250-G1-Notebook-PC
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c8d9ee418ca7..b2cbe81308af 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -425,18 +425,6 @@ struct rt_rq {
 #endif
 };
 
-#ifdef CONFIG_RT_GROUP_SCHED
-static inline int rt_rq_throttled(struct rt_rq *rt_rq)
-{
-	return rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;
-}
-#else
-static inline int rt_rq_throttled(struct rt_rq *rt_rq)
-{
-	return rt_rq->rt_throttled;
-}
-#endif
-
 /* Deadline class' related fields in a runqueue */
 struct dl_rq {
 	/* runqueue is an rbtree, ordered by deadline */

commit f4ebcbc0d7e009783256c9daf76bc4b90e645c14
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Sat Mar 15 02:15:00 2014 +0400

    sched/rt: Substract number of tasks of throttled queues from rq->nr_running
    
    Now rq->rt becomes to be able to be in dequeued or enqueued state.
    We add new member rt_rq->rt_queued, which is used to indicate this.
    The member is used only for top queue rq->rt_rq.
    
    The goal is to fit generic scheme which is used in deadline and
    fair classes, i.e. throttled rt_rq's rt_nr_running is beeing
    substracted from rq->nr_running.
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1394835300.18748.33.camel@HP-250-G1-Notebook-PC
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 456e492a3dca..c8d9ee418ca7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -409,6 +409,8 @@ struct rt_rq {
 	int overloaded;
 	struct plist_head pushable_tasks;
 #endif
+	int rt_queued;
+
 	int rt_throttled;
 	u64 rt_time;
 	u64 rt_runtime;

commit 60e69eed85bb7b5198ef70643b5895c26ad76ef7
Author: Mike Galbraith <bitbucket@online.de>
Date:   Mon Apr 7 10:55:15 2014 +0200

    sched/numa: Fix task_numa_free() lockdep splat
    
    Sasha reported that lockdep claims that the following commit:
    made numa_group.lock interrupt unsafe:
    
      156654f491dd ("sched/numa: Move task_numa_free() to __put_task_struct()")
    
    While I don't see how that could be, given the commit in question moved
    task_numa_free() from one irq enabled region to another, the below does
    make both gripes and lockups upon gripe with numa=fake=4 go away.
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com>
    Fixes: 156654f491dd ("sched/numa: Move task_numa_free() to __put_task_struct()")
    Signed-off-by: Mike Galbraith <bitbucket@online.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: torvalds@linux-foundation.org
    Cc: mgorman@suse.com
    Cc: akpm@linux-foundation.org
    Cc: Dave Jones <davej@redhat.com>
    Link: http://lkml.kernel.org/r/1396860915.5170.5.camel@marge.simpson.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c9007f28d3a2..456e492a3dca 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1385,6 +1385,15 @@ static inline void double_lock(spinlock_t *l1, spinlock_t *l2)
 	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
 }
 
+static inline void double_lock_irq(spinlock_t *l1, spinlock_t *l2)
+{
+	if (l1 > l2)
+		swap(l1, l2);
+
+	spin_lock_irq(l1);
+	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+}
+
 static inline void double_raw_lock(raw_spinlock_t *l1, raw_spinlock_t *l2)
 {
 	if (l1 > l2)

commit a21e40877ad130de837b0394583e4f68dc2ab6c5
Merge: b9b16a792241 073d8224d299
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 1 10:16:10 2014 -0700

    Merge branch 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Ingo Molnar:
     "The main purpose is to fix a full dynticks bug related to
      virtualization, where steal time accounting appears to be zero in
      /proc/stat even after a few seconds of competing guests running busy
      loops in a same host CPU.  It's not a regression though as it was
      there since the beginning.
    
      The other commits are preparatory work to fix the bug and various
      cleanups"
    
    * 'timers-nohz-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      arch: Remove stub cputime.h headers
      sched: Remove needless round trip nsecs <-> tick conversion of steal time
      cputime: Fix jiffies based cputime assumption on steal accounting
      cputime: Bring cputime -> nsecs conversion
      cputime: Default implementation of nsecs -> cputime conversion
      cputime: Fix nsecs_to_cputime() return type cast

commit 300a9d887ea221f344962506f724e02101bacc08
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Mar 5 17:05:57 2014 +0100

    sched: Remove needless round trip nsecs <-> tick conversion of steal time
    
    When update_rq_clock_task() accounts the pending steal time for a task,
    it converts the steal delta from nsecs to tick then from tick to nsecs.
    
    There is no apparent good reason for doing that though because both
    the task clock and the prev steal delta are u64 and store values
    in nsecs.
    
    So lets remove the needless conversion.
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c2119fd20f8b..5ec991010122 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1214,16 +1214,6 @@ extern void update_idle_cpu_load(struct rq *this_rq);
 
 extern void init_task_runnable_average(struct task_struct *p);
 
-#ifdef CONFIG_PARAVIRT
-static inline u64 steal_ticks(u64 steal)
-{
-	if (unlikely(steal > NSEC_PER_SEC))
-		return div_u64(steal, TICK_NSEC);
-
-	return __iter_div_u64_rem(steal, TICK_NSEC, &steal);
-}
-#endif
-
 static inline void inc_nr_running(struct rq *rq)
 {
 	rq->nr_running++;

commit 4c6c4e38c4e9a454889298dcc498174968d14a09
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Thu Mar 6 13:32:01 2014 +0400

    sched/core: Fix endless loop in pick_next_task()
    
    1) Single cpu machine case.
    
    When rq has only RT tasks, but no one of them can be picked
    because of throttling, we enter in endless loop.
    
    pick_next_task_{dl,rt} return NULL.
    
    In pick_next_task_fair() we permanently go to retry
    
            if (rq->nr_running != rq->cfs.h_nr_running)
                    return RETRY_TASK;
    
    (rq->nr_running is not being decremented when rt_rq becomes
    throttled).
    
    No chances to unthrottle any rt_rq or to wake fair here,
    because of rq is locked permanently and interrupts are
    disabled.
    
    2) In case of SMP this can cause a hang too. Although we unlock
       rq in idle_balance(), interrupts are still disabled.
    
    The solution is to check for available tasks in DL and RT
    classes instead of checking for sum.
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1394098321.19290.11.camel@tkhai
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 378bff76267f..f2de7a175620 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -423,6 +423,18 @@ struct rt_rq {
 #endif
 };
 
+#ifdef CONFIG_RT_GROUP_SCHED
+static inline int rt_rq_throttled(struct rt_rq *rt_rq)
+{
+	return rt_rq->rt_throttled && !rt_rq->rt_nr_boosted;
+}
+#else
+static inline int rt_rq_throttled(struct rt_rq *rt_rq)
+{
+	return rt_rq->rt_throttled;
+}
+#endif
+
 /* Deadline class' related fields in a runqueue */
 struct dl_rq {
 	/* runqueue is an rbtree, ordered by deadline */

commit a02ed5e3e05ec5e8af21e645cccc77f3a6480aaf
Merge: 2b3942e4bb20 96b3d28bf4b0
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Mar 11 11:34:27 2014 +0100

    Merge branch 'sched/urgent' into sched/core
    
    Pick up fixes before queueing up new changes.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 37e117c07b89194aae7062bc63bde1104c03db02
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Feb 14 12:25:08 2014 +0100

    sched: Guarantee task priority in pick_next_task()
    
    Michael spotted that the idle_balance() push down created a task
    priority problem.
    
    Previously, when we called idle_balance() before pick_next_task() it
    wasn't a problem when -- because of the rq->lock droppage -- an rt/dl
    task slipped in.
    
    Similarly for pre_schedule(), rt pre-schedule could have a dl task
    slip in.
    
    But by pulling it into the pick_next_task() loop, we'll not try a
    higher task priority again.
    
    Cure this by creating a re-start condition in pick_next_task(); and
    triggering this from pick_next_task_{rt,fair}().
    
    It also fixes a live-lock where we get stuck in pick_next_task_fair()
    due to idle_balance() seeing !0 nr_running but there not actually
    being any fair tasks about.
    
    Reported-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Fixes: 38033c37faab ("sched: Push down pre_schedule() and idle_balance()")
    Tested-by: Sasha Levin <sasha.levin@oracle.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20140224121218.GR15586@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 046084ebb1fb..1929deb3f29d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1091,6 +1091,8 @@ static const u32 prio_to_wmult[40] = {
 
 #define DEQUEUE_SLEEP		1
 
+#define RETRY_TASK		((void *)-1UL)
+
 struct sched_class {
 	const struct sched_class *next;
 
@@ -1105,6 +1107,9 @@ struct sched_class {
 	 * It is the responsibility of the pick_next_task() method that will
 	 * return the next task to call put_prev_task() on the @prev task or
 	 * something equivalent.
+	 *
+	 * May return RETRY_TASK when it finds a higher prio class has runnable
+	 * tasks.
 	 */
 	struct task_struct * (*pick_next_task) (struct rq *rq,
 						struct task_struct *prev);

commit f5f9739d7a0ccbdcf913a0b3604b134129d14f7e
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Wed Feb 26 11:19:33 2014 +0000

    sched: Put rq's sched_avg under CONFIG_FAIR_GROUP_SCHED
    
    The struct sched_avg of struct rq is only used in case group
    scheduling is enabled inside __update_tg_runnable_avg() to update
    per-cpu representation of a task group.  I.e. that there is no need to
    maintain the runnable avg of a rq in the !CONFIG_FAIR_GROUP_SCHED case.
    
    This patch guards struct sched_avg of struct rq and
    update_rq_runnable_avg() with CONFIG_FAIR_GROUP_SCHED.
    
    There is an extra empty definition for update_rq_runnable_avg()
    necessary for the !CONFIG_FAIR_GROUP_SCHED && CONFIG_SMP case.
    
    The function print_cfs_group_stats() which prints out struct sched_avg
    of struct rq is already guarded with CONFIG_FAIR_GROUP_SCHED.
    
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/530DCDC5.1060406@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d608125b36ef..046084ebb1fb 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -541,6 +541,8 @@ struct rq {
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
 	struct list_head leaf_cfs_rq_list;
+
+	struct sched_avg avg;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 	/*
@@ -630,8 +632,6 @@ struct rq {
 #ifdef CONFIG_SMP
 	struct llist_head wake_list;
 #endif
-
-	struct sched_avg avg;
 };
 
 static inline int cpu_of(struct rq *rq)

commit d82fd25356b902703152c1800845661835541878
Author: Li Zefan <lizefan@huawei.com>
Date:   Sat Feb 8 14:17:26 2014 +0800

    sched/rt: Remove 'leaf_rt_rq_list' from 'struct rq'
    
    This is a leftover from commit e23ee74777f389369431d77390c4b09332ce026a
    ("sched/rt: Simplify pull_rt_task() logic and remove .leaf_rt_rq_list").
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/52F5CBF6.4060901@huawei.com
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index caf4abda45e3..d608125b36ef 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -543,10 +543,6 @@ struct rq {
 	struct list_head leaf_cfs_rq_list;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
-#ifdef CONFIG_RT_GROUP_SCHED
-	struct list_head leaf_rt_rq_list;
-#endif
-
 	/*
 	 * This is part of a global counter where only the total sum
 	 * over all CPUs matters. A task can increase this counter on

commit dc87734106bb6e97c92d8bd81f261fb71976ec2c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 12 15:47:29 2014 +0100

    sched: Remove some #ifdeffery
    
    Remove a few gratuitous #ifdefs in pick_next_task*().
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-nnzddp5c4fijyzzxxrwlxghf@git.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d276147ba5e4..caf4abda45e3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1172,6 +1172,11 @@ extern void trigger_load_balance(struct rq *rq);
 extern void idle_enter_fair(struct rq *this_rq);
 extern void idle_exit_fair(struct rq *this_rq);
 
+#else
+
+static inline void idle_enter_fair(struct rq *rq) { }
+static inline void idle_exit_fair(struct rq *rq) { }
+
 #endif
 
 extern void sysrq_sched_debug_show(void);

commit 3f1d2a318171bf61850d4e5a72031271e5aada76
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Feb 12 10:49:30 2014 +0100

    sched: Fix hotplug task migration
    
    Dan Carpenter reported:
    
    > kernel/sched/rt.c:1347 pick_next_task_rt() warn: variable dereferenced before check 'prev' (see line 1338)
    > kernel/sched/deadline.c:1011 pick_next_task_dl() warn: variable dereferenced before check 'prev' (see line 1005)
    
    Kirill also spotted that migrate_tasks() will have an instant NULL
    deref because pick_next_task() will immediately deref prev.
    
    Instead of fixing all the corner cases because migrate_tasks() can
    pass in a NULL prev task in the unlikely case of hot-un-plug, provide
    a fake task such that we can remove all the NULL checks from the far
    more common paths.
    
    A further problem; not previously spotted; is that because we pushed
    pre_schedule() and idle_balance() into pick_next_task() we now need to
    avoid those getting called and pulling more tasks on our dying CPU.
    
    We avoid pull_{dl,rt}_task() by setting fake_task.prio to MAX_PRIO+1.
    We also note that since we call pick_next_task() exactly the amount of
    times we have runnable tasks present, we should never land in
    idle_balance().
    
    Fixes: 38033c37faab ("sched: Push down pre_schedule() and idle_balance()")
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Reported-by: Kirill Tkhai <tkhai@yandex.ru>
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140212094930.GB3545@laptop.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 92018f9821e8..d276147ba5e4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1147,6 +1147,11 @@ struct sched_class {
 #endif
 };
 
+static inline void put_prev_task(struct rq *rq, struct task_struct *prev)
+{
+	prev->sched_class->put_prev_task(rq, prev);
+}
+
 #define sched_class_highest (&stop_sched_class)
 #define for_each_class(class) \
    for (class = sched_class_highest; class; class = class->next)

commit 6e83125c6b151afa139c8852c099d6d92954fe3b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Feb 11 16:11:48 2014 +0100

    sched/fair: Remove idle_balance() declaration in sched.h
    
    Remove idle_balance() from the public life; also reduce some #ifdef
    clutter by folding the pick_next_task_fair() idle path into
    idle_balance().
    
    Cc: mingo@kernel.org
    Reported-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20140211151148.GP27965@twins.programming.kicks-ass.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1bf34c257d3b..92018f9821e8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1163,17 +1163,10 @@ extern const struct sched_class idle_sched_class;
 extern void update_group_power(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
-extern int idle_balance(struct rq *this_rq);
 
 extern void idle_enter_fair(struct rq *this_rq);
 extern void idle_exit_fair(struct rq *this_rq);
 
-#else	/* CONFIG_SMP */
-
-static inline void idle_balance(int cpu, struct rq *rq)
-{
-}
-
 #endif
 
 extern void sysrq_sched_debug_show(void);

commit 995b9ea440862def83e8fcb1b498e68f93d4af59
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Tue Feb 18 02:24:13 2014 +0400

    sched/deadline: Remove useless dl_nr_total
    
    In deadline class we do not have group scheduling like in RT.
    
    dl_nr_total is the same as dl_nr_running. So, one of them should
    be removed.
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/368631392675853@web20h.yandex.ru
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c2119fd20f8b..f964add50f38 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -462,7 +462,6 @@ struct dl_rq {
 	} earliest_dl;
 
 	unsigned long dl_nr_migratory;
-	unsigned long dl_nr_total;
 	int overloaded;
 
 	/*

commit 38033c37faab850ed5d33bb675c4de6c66be84d8
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jan 23 20:32:21 2014 +0100

    sched: Push down pre_schedule() and idle_balance()
    
    This patch both merged idle_balance() and pre_schedule() and pushes
    both of them into pick_next_task().
    
    Conceptually pre_schedule() and idle_balance() are rather similar,
    both are used to pull more work onto the current CPU.
    
    We cannot however first move idle_balance() into pre_schedule_fair()
    since there is no guarantee the last runnable task is a fair task, and
    thus we would miss newidle balances.
    
    Similarly, the dl and rt pre_schedule calls must be ran before
    idle_balance() since their respective tasks have higher priority and
    it would not do to delay their execution searching for less important
    tasks first.
    
    However, by noticing that pick_next_tasks() already traverses the
    sched_class hierarchy in the right order, we can get the right
    behaviour and do away with both calls.
    
    We must however change the special case optimization to also require
    that prev is of sched_class_fair, otherwise we can miss doing a dl or
    rt pull where we needed one.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/n/tip-a8k6vvaebtn64nie345kx1je@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c534cf4181ab..1bf34c257d3b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1118,7 +1118,6 @@ struct sched_class {
 	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
 	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
 
-	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);
 	void (*task_waking) (struct task_struct *task);
 	void (*task_woken) (struct rq *this_rq, struct task_struct *task);

commit 606dba2e289446600a0b68422ed2019af5355c12
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat Feb 11 06:05:00 2012 +0100

    sched: Push put_prev_task() into pick_next_task()
    
    In order to avoid having to do put/set on a whole cgroup hierarchy
    when we context switch, push the put into pick_next_task() so that
    both operations are in the same function. Further changes then allow
    us to possibly optimize away redundant work.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1328936700.2476.17.camel@laptop
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index bb89991ee409..c534cf4181ab 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1105,7 +1105,13 @@ struct sched_class {
 
 	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
 
-	struct task_struct * (*pick_next_task) (struct rq *rq);
+	/*
+	 * It is the responsibility of the pick_next_task() method that will
+	 * return the next task to call put_prev_task() on the @prev task or
+	 * something equivalent.
+	 */
+	struct task_struct * (*pick_next_task) (struct rq *rq,
+						struct task_struct *prev);
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP

commit 3c4017c13f91069194fce3160944efec50f15a6e
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri Jan 17 10:04:03 2014 +0100

    sched: Move rq->idle_stamp up to the core
    
    idle_balance() modifies the rq->idle_stamp field, making this information
    shared across core.c and fair.c.
    
    As we know if the cpu is going to idle or not with the previous patch, let's
    encapsulate the rq->idle_stamp information in core.c by moving it up to the
    caller.
    
    The idle_balance() function returns true in case a balancing occured and the
    cpu won't be idle, false if no balance happened and the cpu is going idle.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: alex.shi@linaro.org
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1389949444-14821-3-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 82c0e02f2a58..bb89991ee409 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1158,7 +1158,7 @@ extern const struct sched_class idle_sched_class;
 extern void update_group_power(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
-extern void idle_balance(struct rq *this_rq);
+extern int idle_balance(struct rq *this_rq);
 
 extern void idle_enter_fair(struct rq *this_rq);
 extern void idle_exit_fair(struct rq *this_rq);

commit b4f2ab43615e5b36c48fffa99f26aca381839ac6
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri Jan 17 10:04:01 2014 +0100

    sched: Remove 'cpu' parameter from idle_balance()
    
    The cpu parameter passed to idle_balance() is not needed as it could
    be retrieved from 'struct rq.'
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: alex.shi@linaro.org
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1389949444-14821-1-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b44720d38ae9..82c0e02f2a58 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1158,7 +1158,7 @@ extern const struct sched_class idle_sched_class;
 extern void update_group_power(struct sched_domain *sd, int cpu);
 
 extern void trigger_load_balance(struct rq *rq);
-extern void idle_balance(int this_cpu, struct rq *this_rq);
+extern void idle_balance(struct rq *this_rq);
 
 extern void idle_enter_fair(struct rq *this_rq);
 extern void idle_exit_fair(struct rq *this_rq);

commit 6b6350f155afdfdf888e18c7bf26950a6d10b0c2
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Mon Jan 27 17:15:38 2014 -0500

    sched: Expose some macros related to priority
    
    Some macros in kernel/sched/sched.h about priority are
    private to kernel/sched. But they are useful to other
    parts of the core kernel.
    
    This patch moves these macros from kernel/sched/sched.h to
    include/linux/sched/prio.h so that they are available to
    other subsystems.
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Cc: raistlin@linux.it
    Cc: juri.lelli@gmail.com
    Cc: clark.williams@gmail.com
    Cc: rostedt@goodmis.org
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/2b022810905b52d13238466807f4b2a691577180.1390859827.git.yangds.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c2119fd20f8b..b44720d38ae9 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -23,24 +23,6 @@ extern atomic_long_t calc_load_tasks;
 extern long calc_load_fold_active(struct rq *this_rq);
 extern void update_cpu_load_active(struct rq *this_rq);
 
-/*
- * Convert user-nice values [ -20 ... 0 ... 19 ]
- * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
- * and back.
- */
-#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
-#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
-#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
-
-/*
- * 'User priority' is the nice value converted to something we
- * can work with better when scaling various scheduler parameters,
- * it's a [ 0 ... 39 ] range.
- */
-#define USER_PRIO(p)		((p)-MAX_RT_PRIO)
-#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
-#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
-
 /*
  * Helpers for converting nanosecond timing to jiffy resolution
  */

commit 7caff66f361c44d0fbc74ed1cfa60a357fc84cf2
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Jan 6 12:34:38 2014 +0100

    sched: Reduce trigger_load_balance() parameters
    
    The cpu information is already stored in the struct rq, so no need to pass it
    as parameter to the trigger_load_balance function.
    
    Cc: linaro-kernel@lists.linaro.org
    Cc: preeti.lkml@gmail.com
    Cc: mingo@redhat.com
    Cc: peterz@infradead.org
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1389008085-9069-2-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 890339099550..c2119fd20f8b 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1175,7 +1175,7 @@ extern const struct sched_class idle_sched_class;
 
 extern void update_group_power(struct sched_domain *sd, int cpu);
 
-extern void trigger_load_balance(struct rq *rq, int cpu);
+extern void trigger_load_balance(struct rq *rq);
 extern void idle_balance(int this_cpu, struct rq *this_rq);
 
 extern void idle_enter_fair(struct rq *this_rq);

commit 1724813d9f2c7ff702b46d3e4a4f6d9b10a8f8c2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Dec 17 12:44:49 2013 +0100

    sched/deadline: Remove the sysctl_sched_dl knobs
    
    Remove the deadline specific sysctls for now. The problem with them is
    that the interaction with the exisiting rt knobs is nearly impossible
    to get right.
    
    The current (as per before this patch) situation is that the rt and dl
    bandwidth is completely separate and we enforce rt+dl < 100%. This is
    undesirable because this means that the rt default of 95% leaves us
    hardly any room, even though dl tasks are saver than rt tasks.
    
    Another proposed solution was (a discarted patch) to have the dl
    bandwidth be a fraction of the rt bandwidth. This is highly
    confusing imo.
    
    Furthermore neither proposal is consistent with the situation we
    actually want; which is rt tasks ran from a dl server. In which case
    the rt bandwidth is a direct subset of dl.
    
    So whichever way we go, the introduction of dl controls at this point
    is painful. Therefore remove them and instead share the rt budget.
    
    This means that for now the rt knobs are used for dl admission control
    and the dl runtime is accounted against the rt runtime. I realise that
    this isn't entirely desirable either; but whatever we do we appear to
    need to change the interface later, so better have a small interface
    for now.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-zpyqbqds1r0vyxtxza1e7rdc@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 2b7421db6c41..890339099550 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -176,7 +176,7 @@ struct dl_bandwidth {
 
 static inline int dl_bandwidth_enabled(void)
 {
-	return sysctl_sched_dl_runtime >= 0;
+	return sysctl_sched_rt_runtime >= 0;
 }
 
 extern struct dl_bw *dl_bw_of(int i);
@@ -186,9 +186,6 @@ struct dl_bw {
 	u64 bw, total_bw;
 };
 
-static inline u64 global_dl_period(void);
-static inline u64 global_dl_runtime(void);
-
 extern struct mutex sched_domains_mutex;
 
 #ifdef CONFIG_CGROUP_SCHED
@@ -953,19 +950,6 @@ static inline u64 global_rt_runtime(void)
 	return (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;
 }
 
-static inline u64 global_dl_period(void)
-{
-	return (u64)sysctl_sched_dl_period * NSEC_PER_USEC;
-}
-
-static inline u64 global_dl_runtime(void)
-{
-	if (sysctl_sched_dl_runtime < 0)
-		return RUNTIME_INF;
-
-	return (u64)sysctl_sched_dl_runtime * NSEC_PER_USEC;
-}
-
 static inline int task_current(struct rq *rq, struct task_struct *p)
 {
 	return rq->curr == p;

commit 6bfd6d72f51c51177676f2b1ba113fe0a85fdae4
Author: Juri Lelli <juri.lelli@gmail.com>
Date:   Thu Nov 7 14:43:47 2013 +0100

    sched/deadline: speed up SCHED_DEADLINE pushes with a push-heap
    
    Data from tests confirmed that the original active load balancing
    logic didn't scale neither in the number of CPU nor in the number of
    tasks (as sched_rt does).
    
    Here we provide a global data structure to keep track of deadlines
    of the running tasks in the system. The structure is composed by
    a bitmask showing the free CPUs and a max-heap, needed when the system
    is heavily loaded.
    
    The implementation and concurrent access scheme are kept simple by
    design. However, our measurements show that we can compete with sched_rt
    on large multi-CPUs machines [1].
    
    Only the push path is addressed, the extension to use this structure
    also for pull decisions is straightforward. However, we are currently
    evaluating different (in order to decrease/avoid contention) data
    structures to solve possibly both problems. We are also going to re-run
    tests considering recent changes inside cpupri [2].
    
     [1] http://retis.sssup.it/~jlelli/papers/Ospert11Lelli.pdf
     [2] http://www.spinics.net/lists/linux-rt-users/msg06778.html
    
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-14-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ad4f4fbd002e..2b7421db6c41 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -10,6 +10,7 @@
 #include <linux/slab.h>
 
 #include "cpupri.h"
+#include "cpudeadline.h"
 #include "cpuacct.h"
 
 struct rq;
@@ -503,6 +504,7 @@ struct root_domain {
 	cpumask_var_t dlo_mask;
 	atomic_t dlo_count;
 	struct dl_bw dl_bw;
+	struct cpudl cpudl;
 
 	/*
 	 * The "RT overload" flag: it gets set if a CPU has more than

commit 332ac17ef5bfcff4766dfdfd3b4cdf10b8f8f155
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:45 2013 +0100

    sched/deadline: Add bandwidth management for SCHED_DEADLINE tasks
    
    In order of deadline scheduling to be effective and useful, it is
    important that some method of having the allocation of the available
    CPU bandwidth to tasks and task groups under control.
    This is usually called "admission control" and if it is not performed
    at all, no guarantee can be given on the actual scheduling of the
    -deadline tasks.
    
    Since when RT-throttling has been introduced each task group have a
    bandwidth associated to itself, calculated as a certain amount of
    runtime over a period. Moreover, to make it possible to manipulate
    such bandwidth, readable/writable controls have been added to both
    procfs (for system wide settings) and cgroupfs (for per-group
    settings).
    
    Therefore, the same interface is being used for controlling the
    bandwidth distrubution to -deadline tasks and task groups, i.e.,
    new controls but with similar names, equivalent meaning and with
    the same usage paradigm are added.
    
    However, more discussion is needed in order to figure out how
    we want to manage SCHED_DEADLINE bandwidth at the task group level.
    Therefore, this patch adds a less sophisticated, but actually
    very sensible, mechanism to ensure that a certain utilization
    cap is not overcome per each root_domain (the single rq for !SMP
    configurations).
    
    Another main difference between deadline bandwidth management and
    RT-throttling is that -deadline tasks have bandwidth on their own
    (while -rt ones doesn't!), and thus we don't need an higher level
    throttling mechanism to enforce the desired bandwidth.
    
    This patch, therefore:
    
     - adds system wide deadline bandwidth management by means of:
        * /proc/sys/kernel/sched_dl_runtime_us,
        * /proc/sys/kernel/sched_dl_period_us,
       that determine (i.e., runtime / period) the total bandwidth
       available on each CPU of each root_domain for -deadline tasks;
    
     - couples the RT and deadline bandwidth management, i.e., enforces
       that the sum of how much bandwidth is being devoted to -rt
       -deadline tasks to stay below 100%.
    
    This means that, for a root_domain comprising M CPUs, -deadline tasks
    can be created until the sum of their bandwidths stay below:
    
        M * (sched_dl_runtime_us / sched_dl_period_us)
    
    It is also possible to disable this bandwidth management logic, and
    be thus free of oversubscribing the system up to any arbitrary level.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-12-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 52453a2d0a79..ad4f4fbd002e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -73,6 +73,13 @@ extern void update_cpu_load_active(struct rq *this_rq);
 #define NICE_0_LOAD		SCHED_LOAD_SCALE
 #define NICE_0_SHIFT		SCHED_LOAD_SHIFT
 
+/*
+ * Single value that decides SCHED_DEADLINE internal math precision.
+ * 10 -> just above 1us
+ * 9  -> just above 0.5us
+ */
+#define DL_SCALE (10)
+
 /*
  * These are the 'tuning knobs' of the scheduler:
  */
@@ -107,7 +114,7 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
-static inline int dl_time_before(u64 a, u64 b)
+static inline bool dl_time_before(u64 a, u64 b)
 {
 	return (s64)(a - b) < 0;
 }
@@ -115,8 +122,8 @@ static inline int dl_time_before(u64 a, u64 b)
 /*
  * Tells if entity @a should preempt entity @b.
  */
-static inline
-int dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
+static inline bool
+dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
 {
 	return dl_time_before(a->deadline, b->deadline);
 }
@@ -136,6 +143,50 @@ struct rt_bandwidth {
 	u64			rt_runtime;
 	struct hrtimer		rt_period_timer;
 };
+/*
+ * To keep the bandwidth of -deadline tasks and groups under control
+ * we need some place where:
+ *  - store the maximum -deadline bandwidth of the system (the group);
+ *  - cache the fraction of that bandwidth that is currently allocated.
+ *
+ * This is all done in the data structure below. It is similar to the
+ * one used for RT-throttling (rt_bandwidth), with the main difference
+ * that, since here we are only interested in admission control, we
+ * do not decrease any runtime while the group "executes", neither we
+ * need a timer to replenish it.
+ *
+ * With respect to SMP, the bandwidth is given on a per-CPU basis,
+ * meaning that:
+ *  - dl_bw (< 100%) is the bandwidth of the system (group) on each CPU;
+ *  - dl_total_bw array contains, in the i-eth element, the currently
+ *    allocated bandwidth on the i-eth CPU.
+ * Moreover, groups consume bandwidth on each CPU, while tasks only
+ * consume bandwidth on the CPU they're running on.
+ * Finally, dl_total_bw_cpu is used to cache the index of dl_total_bw
+ * that will be shown the next time the proc or cgroup controls will
+ * be red. It on its turn can be changed by writing on its own
+ * control.
+ */
+struct dl_bandwidth {
+	raw_spinlock_t dl_runtime_lock;
+	u64 dl_runtime;
+	u64 dl_period;
+};
+
+static inline int dl_bandwidth_enabled(void)
+{
+	return sysctl_sched_dl_runtime >= 0;
+}
+
+extern struct dl_bw *dl_bw_of(int i);
+
+struct dl_bw {
+	raw_spinlock_t lock;
+	u64 bw, total_bw;
+};
+
+static inline u64 global_dl_period(void);
+static inline u64 global_dl_runtime(void);
 
 extern struct mutex sched_domains_mutex;
 
@@ -423,6 +474,8 @@ struct dl_rq {
 	 */
 	struct rb_root pushable_dl_tasks_root;
 	struct rb_node *pushable_dl_tasks_leftmost;
+#else
+	struct dl_bw dl_bw;
 #endif
 };
 
@@ -449,6 +502,7 @@ struct root_domain {
 	 */
 	cpumask_var_t dlo_mask;
 	atomic_t dlo_count;
+	struct dl_bw dl_bw;
 
 	/*
 	 * The "RT overload" flag: it gets set if a CPU has more than
@@ -897,7 +951,18 @@ static inline u64 global_rt_runtime(void)
 	return (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;
 }
 
+static inline u64 global_dl_period(void)
+{
+	return (u64)sysctl_sched_dl_period * NSEC_PER_USEC;
+}
+
+static inline u64 global_dl_runtime(void)
+{
+	if (sysctl_sched_dl_runtime < 0)
+		return RUNTIME_INF;
 
+	return (u64)sysctl_sched_dl_runtime * NSEC_PER_USEC;
+}
 
 static inline int task_current(struct rq *rq, struct task_struct *p)
 {
@@ -1145,6 +1210,7 @@ extern void update_max_interval(void);
 extern void init_sched_dl_class(void);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
+extern void init_sched_dl_class(void);
 
 extern void resched_task(struct task_struct *p);
 extern void resched_cpu(int cpu);
@@ -1152,8 +1218,12 @@ extern void resched_cpu(int cpu);
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
+extern struct dl_bandwidth def_dl_bandwidth;
+extern void init_dl_bandwidth(struct dl_bandwidth *dl_b, u64 period, u64 runtime);
 extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
 
+unsigned long to_ratio(u64 period, u64 runtime);
+
 extern void update_idle_cpu_load(struct rq *this_rq);
 
 extern void init_task_runnable_average(struct task_struct *p);

commit 2d3d891d3344159d5b452a645e355bbe29591e8b
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:44 2013 +0100

    sched/deadline: Add SCHED_DEADLINE inheritance logic
    
    Some method to deal with rt-mutexes and make sched_dl interact with
    the current PI-coded is needed, raising all but trivial issues, that
    needs (according to us) to be solved with some restructuring of
    the pi-code (i.e., going toward a proxy execution-ish implementation).
    
    This is under development, in the meanwhile, as a temporary solution,
    what this commits does is:
    
     - ensure a pi-lock owner with waiters is never throttled down. Instead,
       when it runs out of runtime, it immediately gets replenished and it's
       deadline is postponed;
    
     - the scheduling parameters (relative deadline and default runtime)
       used for that replenishments --during the whole period it holds the
       pi-lock-- are the ones of the waiting task with earliest deadline.
    
    Acting this way, we provide some kind of boosting to the lock-owner,
    still by using the existing (actually, slightly modified by the previous
    commit) pi-architecture.
    
    We would stress the fact that this is only a surely needed, all but
    clean solution to the problem. In the end it's only a way to re-start
    discussion within the community. So, as always, comments, ideas, rants,
    etc.. are welcome! :-)
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    [ Added !RT_MUTEXES build fix. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-11-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 93ea62754f11..52453a2d0a79 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -107,6 +107,20 @@ static inline int task_has_dl_policy(struct task_struct *p)
 	return dl_policy(p->policy);
 }
 
+static inline int dl_time_before(u64 a, u64 b)
+{
+	return (s64)(a - b) < 0;
+}
+
+/*
+ * Tells if entity @a should preempt entity @b.
+ */
+static inline
+int dl_entity_preempt(struct sched_dl_entity *a, struct sched_dl_entity *b)
+{
+	return dl_time_before(a->deadline, b->deadline);
+}
+
 /*
  * This is the priority-queue data structure of the RT scheduling class:
  */

commit 1baca4ce16b8cc7d4f50be1f7914799af30a2861
Author: Juri Lelli <juri.lelli@gmail.com>
Date:   Thu Nov 7 14:43:38 2013 +0100

    sched/deadline: Add SCHED_DEADLINE SMP-related data structures & logic
    
    Introduces data structures relevant for implementing dynamic
    migration of -deadline tasks and the logic for checking if
    runqueues are overloaded with -deadline tasks and for choosing
    where a task should migrate, when it is the case.
    
    Adds also dynamic migrations to SCHED_DEADLINE, so that tasks can
    be moved among CPUs when necessary. It is also possible to bind a
    task to a (set of) CPU(s), thus restricting its capability of
    migrating, or forbidding migrations at all.
    
    The very same approach used in sched_rt is utilised:
     - -deadline tasks are kept into CPU-specific runqueues,
     - -deadline tasks are migrated among runqueues to achieve the
       following:
        * on an M-CPU system the M earliest deadline ready tasks
          are always running;
        * affinity/cpusets settings of all the -deadline tasks is
          always respected.
    
    Therefore, this very special form of "load balancing" is done with
    an active method, i.e., the scheduler pushes or pulls tasks between
    runqueues when they are woken up and/or (de)scheduled.
    IOW, every time a preemption occurs, the descheduled task might be sent
    to some other CPU (depending on its deadline) to continue executing
    (push). On the other hand, every time a CPU becomes idle, it might pull
    the second earliest deadline ready task from some other CPU.
    
    To enforce this, a pull operation is always attempted before taking any
    scheduling decision (pre_schedule()), as well as a push one after each
    scheduling decision (post_schedule()). In addition, when a task arrives
    or wakes up, the best CPU where to resume it is selected taking into
    account its affinity mask, the system topology, but also its deadline.
    E.g., from the scheduling point of view, the best CPU where to wake
    up (and also where to push) a task is the one which is running the task
    with the latest deadline among the M executing ones.
    
    In order to facilitate these decisions, per-runqueue "caching" of the
    deadlines of the currently running and of the first ready task is used.
    Queued but not running tasks are also parked in another rb-tree to
    speed-up pushes.
    
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-5-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 83eb5390f753..93ea62754f11 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -385,6 +385,31 @@ struct dl_rq {
 	struct rb_node *rb_leftmost;
 
 	unsigned long dl_nr_running;
+
+#ifdef CONFIG_SMP
+	/*
+	 * Deadline values of the currently executing and the
+	 * earliest ready task on this rq. Caching these facilitates
+	 * the decision wether or not a ready but not running task
+	 * should migrate somewhere else.
+	 */
+	struct {
+		u64 curr;
+		u64 next;
+	} earliest_dl;
+
+	unsigned long dl_nr_migratory;
+	unsigned long dl_nr_total;
+	int overloaded;
+
+	/*
+	 * Tasks on this rq that can be pushed away. They are kept in
+	 * an rb-tree, ordered by tasks' deadlines, with caching
+	 * of the leftmost (earliest deadline) element.
+	 */
+	struct rb_root pushable_dl_tasks_root;
+	struct rb_node *pushable_dl_tasks_leftmost;
+#endif
 };
 
 #ifdef CONFIG_SMP
@@ -404,6 +429,13 @@ struct root_domain {
 	cpumask_var_t span;
 	cpumask_var_t online;
 
+	/*
+	 * The bit corresponding to a CPU gets set here if such CPU has more
+	 * than one runnable -deadline task (as it is below for RT tasks).
+	 */
+	cpumask_var_t dlo_mask;
+	atomic_t dlo_count;
+
 	/*
 	 * The "RT overload" flag: it gets set if a CPU has more than
 	 * one runnable RT task.
@@ -1095,6 +1127,8 @@ static inline void idle_balance(int cpu, struct rq *rq)
 extern void sysrq_sched_debug_show(void);
 extern void sched_init_granularity(void);
 extern void update_max_interval(void);
+
+extern void init_sched_dl_class(void);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
 

commit aab03e05e8f7e26f51dee792beddcb5cca9215a5
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 28 11:14:43 2013 +0100

    sched/deadline: Add SCHED_DEADLINE structures & implementation
    
    Introduces the data structures, constants and symbols needed for
    SCHED_DEADLINE implementation.
    
    Core data structure of SCHED_DEADLINE are defined, along with their
    initializers. Hooks for checking if a task belong to the new policy
    are also added where they are needed.
    
    Adds a scheduling class, in sched/dl.c and a new policy called
    SCHED_DEADLINE. It is an implementation of the Earliest Deadline
    First (EDF) scheduling algorithm, augmented with a mechanism (called
    Constant Bandwidth Server, CBS) that makes it possible to isolate
    the behaviour of tasks between each other.
    
    The typical -deadline task will be made up of a computation phase
    (instance) which is activated on a periodic or sporadic fashion. The
    expected (maximum) duration of such computation is called the task's
    runtime; the time interval by which each instance need to be completed
    is called the task's relative deadline. The task's absolute deadline
    is dynamically calculated as the time instant a task (better, an
    instance) activates plus the relative deadline.
    
    The EDF algorithms selects the task with the smallest absolute
    deadline as the one to be executed first, while the CBS ensures each
    task to run for at most its runtime every (relative) deadline
    length time interval, avoiding any interference between different
    tasks (bandwidth isolation).
    Thanks to this feature, also tasks that do not strictly comply with
    the computational model sketched above can effectively use the new
    policy.
    
    To summarize, this patch:
     - introduces the data structures, constants and symbols needed;
     - implements the core logic of the scheduling algorithm in the new
       scheduling class file;
     - provides all the glue code between the new scheduling class and
       the core scheduler and refines the interactions between sched/dl
       and the other existing scheduling classes.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Signed-off-by: Michael Trimarchi <michael@amarulasolutions.com>
    Signed-off-by: Fabio Checconi <fchecconi@gmail.com>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-4-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index df023db7721c..83eb5390f753 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -2,6 +2,7 @@
 #include <linux/sched.h>
 #include <linux/sched/sysctl.h>
 #include <linux/sched/rt.h>
+#include <linux/sched/deadline.h>
 #include <linux/mutex.h>
 #include <linux/spinlock.h>
 #include <linux/stop_machine.h>
@@ -91,11 +92,21 @@ static inline int rt_policy(int policy)
 	return policy == SCHED_FIFO || policy == SCHED_RR;
 }
 
+static inline int dl_policy(int policy)
+{
+	return policy == SCHED_DEADLINE;
+}
+
 static inline int task_has_rt_policy(struct task_struct *p)
 {
 	return rt_policy(p->policy);
 }
 
+static inline int task_has_dl_policy(struct task_struct *p)
+{
+	return dl_policy(p->policy);
+}
+
 /*
  * This is the priority-queue data structure of the RT scheduling class:
  */
@@ -367,6 +378,15 @@ struct rt_rq {
 #endif
 };
 
+/* Deadline class' related fields in a runqueue */
+struct dl_rq {
+	/* runqueue is an rbtree, ordered by deadline */
+	struct rb_root rb_root;
+	struct rb_node *rb_leftmost;
+
+	unsigned long dl_nr_running;
+};
+
 #ifdef CONFIG_SMP
 
 /*
@@ -435,6 +455,7 @@ struct rq {
 
 	struct cfs_rq cfs;
 	struct rt_rq rt;
+	struct dl_rq dl;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
@@ -991,6 +1012,7 @@ static const u32 prio_to_wmult[40] = {
 #else
 #define ENQUEUE_WAKING		0
 #endif
+#define ENQUEUE_REPLENISH	8
 
 #define DEQUEUE_SLEEP		1
 
@@ -1046,6 +1068,7 @@ struct sched_class {
    for (class = sched_class_highest; class; class = class->next)
 
 extern const struct sched_class stop_sched_class;
+extern const struct sched_class dl_sched_class;
 extern const struct sched_class rt_sched_class;
 extern const struct sched_class fair_sched_class;
 extern const struct sched_class idle_sched_class;
@@ -1081,6 +1104,8 @@ extern void resched_cpu(int cpu);
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
+extern void init_dl_task_timer(struct sched_dl_entity *dl_se);
+
 extern void update_idle_cpu_load(struct rq *this_rq);
 
 extern void init_task_runnable_average(struct task_struct *p);
@@ -1357,6 +1382,7 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void init_dl_rq(struct dl_rq *dl_rq, struct rq *rq);
 
 extern void cfs_bandwidth_usage_inc(void);
 extern void cfs_bandwidth_usage_dec(void);

commit d50dde5a10f305253cbc3855307f608f8a3c5f73
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:36 2013 +0100

    sched: Add new scheduler syscalls to support an extended scheduling parameters ABI
    
    Add the syscalls needed for supporting scheduling algorithms
    with extended scheduling parameters (e.g., SCHED_DEADLINE).
    
    In general, it makes possible to specify a periodic/sporadic task,
    that executes for a given amount of runtime at each instance, and is
    scheduled according to the urgency of their own timing constraints,
    i.e.:
    
     - a (maximum/typical) instance execution time,
     - a minimum interval between consecutive instances,
     - a time constraint by which each instance must be completed.
    
    Thus, both the data structure that holds the scheduling parameters of
    the tasks and the system calls dealing with it must be extended.
    Unfortunately, modifying the existing struct sched_param would break
    the ABI and result in potentially serious compatibility issues with
    legacy binaries.
    
    For these reasons, this patch:
    
     - defines the new struct sched_attr, containing all the fields
       that are necessary for specifying a task in the computational
       model described above;
    
     - defines and implements the new scheduling related syscalls that
       manipulate it, i.e., sched_setattr() and sched_getattr().
    
    Syscalls are introduced for x86 (32 and 64 bits) and ARM only, as a
    proof of concept and for developing and testing purposes. Making them
    available on other architectures is straightforward.
    
    Since no "user" for these new parameters is introduced in this patch,
    the implementation of the new system calls is just identical to their
    already existing counterpart. Future patches that implement scheduling
    policies able to exploit the new data structure must also take care of
    modifying the sched_*attr() calls accordingly with their own purposes.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    [ Rewrote to use sched_attr. ]
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    [ Removed sched_setscheduler2() for now. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-3-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b3b4a4953efc..df023db7721c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -81,11 +81,14 @@ extern void update_cpu_load_active(struct rq *this_rq);
  */
 #define RUNTIME_INF	((u64)~0ULL)
 
+static inline int fair_policy(int policy)
+{
+	return policy == SCHED_NORMAL || policy == SCHED_BATCH;
+}
+
 static inline int rt_policy(int policy)
 {
-	if (policy == SCHED_FIFO || policy == SCHED_RR)
-		return 1;
-	return 0;
+	return policy == SCHED_FIFO || policy == SCHED_RR;
 }
 
 static inline int task_has_rt_policy(struct task_struct *p)

commit e6c390f2dfd04c165ce45b0032f73fba85b1f282
Author: Dario Faggioli <raistlin@linux.it>
Date:   Thu Nov 7 14:43:35 2013 +0100

    sched: Add sched_class->task_dead() method
    
    Add a new function to the scheduling class interface. It is called
    at the end of a context switch, if the prev task is in TASK_DEAD state.
    
    It will be useful for the scheduling classes that want to be notified
    when one of their tasks dies, e.g. to perform some cleanup actions,
    such as SCHED_DEADLINE.
    
    Signed-off-by: Dario Faggioli <raistlin@linux.it>
    Reviewed-by: Paul Turner <pjt@google.com>
    Signed-off-by: Juri Lelli <juri.lelli@gmail.com>
    Cc: bruce.ashfield@windriver.com
    Cc: claudio@evidence.eu.com
    Cc: darren@dvhart.com
    Cc: dhaval.giani@gmail.com
    Cc: fchecconi@gmail.com
    Cc: fweisbec@gmail.com
    Cc: harald.gustafsson@ericsson.com
    Cc: hgu1972@gmail.com
    Cc: insop.song@gmail.com
    Cc: jkacur@redhat.com
    Cc: johan.eker@ericsson.com
    Cc: liming.wang@windriver.com
    Cc: luca.abeni@unitn.it
    Cc: michael@amarulasolutions.com
    Cc: nicola.manica@disi.unitn.it
    Cc: oleg@redhat.com
    Cc: paulmck@linux.vnet.ibm.com
    Cc: p.faure@akatech.ch
    Cc: rostedt@goodmis.org
    Cc: tommaso.cucinotta@sssup.it
    Cc: vincent.guittot@linaro.org
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1383831828-15501-2-git-send-email-juri.lelli@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 88c85b21d633..b3b4a4953efc 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1023,6 +1023,7 @@ struct sched_class {
 	void (*set_curr_task) (struct rq *rq);
 	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
 	void (*task_fork) (struct task_struct *p);
+	void (*task_dead) (struct task_struct *p);
 
 	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
 	void (*switched_to) (struct rq *this_rq, struct task_struct *task);

commit 37dc6b50cee97954c4e6edcd5b1fa614b76038ee
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Wed Oct 30 08:42:52 2013 +0530

    sched: Remove unnecessary iteration over sched domains to update nr_busy_cpus
    
    nr_busy_cpus parameter is used by nohz_kick_needed() to find out the
    number of busy cpus in a sched domain which has SD_SHARE_PKG_RESOURCES
    flag set.  Therefore instead of updating nr_busy_cpus at every level
    of sched domain, since it is irrelevant, we can update this parameter
    only at the parent domain of the sd which has this flag set. Introduce
    a per-cpu parameter sd_busy which represents this parent domain.
    
    In nohz_kick_needed() we directly query the nr_busy_cpus parameter
    associated with the groups of sd_busy.
    
    By associating sd_busy with the highest domain which has
    SD_SHARE_PKG_RESOURCES flag set, we cover all lower level domains
    which could have this flag set and trigger nohz_idle_balancing if any
    of the levels have more than one busy cpu.
    
    sd_busy is irrelevant for asymmetric load balancing. However sd_asym
    has been introduced to represent the highest sched domain which has
    SD_ASYM_PACKING flag set so that it can be queried directly when
    required.
    
    While we are at it, we might as well change the nohz_idle parameter to
    be updated at the sd_busy domain level alone and not the base domain
    level of a CPU.  This will unify the concept of busy cpus at just one
    level of sched domain where it is currently used.
    
    Signed-off-by: Preeti U Murthy<preeti@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: svaidy@linux.vnet.ibm.com
    Cc: vincent.guittot@linaro.org
    Cc: bitbucket@online.de
    Cc: benh@kernel.crashing.org
    Cc: anton@samba.org
    Cc: Morten.Rasmussen@arm.com
    Cc: pjt@google.com
    Cc: peterz@infradead.org
    Cc: mikey@neuling.org
    Link: http://lkml.kernel.org/r/20131030031252.23426.4417.stgit@preeti.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4e650acffed7..88c85b21d633 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -623,6 +623,8 @@ DECLARE_PER_CPU(struct sched_domain *, sd_llc);
 DECLARE_PER_CPU(int, sd_llc_size);
 DECLARE_PER_CPU(int, sd_llc_id);
 DECLARE_PER_CPU(struct sched_domain *, sd_numa);
+DECLARE_PER_CPU(struct sched_domain *, sd_busy);
+DECLARE_PER_CPU(struct sched_domain *, sd_asym);
 
 struct sched_group_power {
 	atomic_t ref;

commit 1ee14e6c8cddeeb8a490d7b54cd9016e4bb900b4
Author: Ben Segall <bsegall@google.com>
Date:   Wed Oct 16 11:16:12 2013 -0700

    sched: Fix race on toggling cfs_bandwidth_used
    
    When we transition cfs_bandwidth_used to false, any currently
    throttled groups will incorrectly return false from cfs_rq_throttled.
    While tg_set_cfs_bandwidth will unthrottle them eventually, currently
    running code (including at least dequeue_task_fair and
    distribute_cfs_runtime) will cause errors.
    
    Fix this by turning off cfs_bandwidth_used only after unthrottling all
    cfs_rqs.
    
    Tested: toggle bandwidth back and forth on a loaded cgroup. Caused
    crashes in minutes without the patch, hasn't crashed with it.
    
    Signed-off-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: pjt@google.com
    Link: http://lkml.kernel.org/r/20131016181611.22647.80365.stgit@sword-of-the-dawn.mtv.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ffc708717b70..4e650acffed7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1352,7 +1352,8 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
 
-extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
+extern void cfs_bandwidth_usage_inc(void);
+extern void cfs_bandwidth_usage_dec(void);
 
 #ifdef CONFIG_NO_HZ_COMMON
 enum rq_nohz_flag_bits {

commit 746023159c40c523b08a3bc3d213dac212385895
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 10 20:17:22 2013 +0200

    sched: Fix race in migrate_swap_stop()
    
    There is a subtle race in migrate_swap, when task P, on CPU A, decides to swap
    places with task T, on CPU B.
    
    Task P:
      - call migrate_swap
    Task T:
      - go to sleep, removing itself from the runqueue
    Task P:
      - double lock the runqueues on CPU A & B
    Task T:
      - get woken up, place itself on the runqueue of CPU C
    Task P:
      - see that task T is on a runqueue, and pretend to remove it
        from the runqueue on CPU B
    
    Now CPUs B & C both have corrupted scheduler data structures.
    
    This patch fixes it, by holding the pi_lock for both of the tasks
    involved in the migrate swap. This prevents task T from waking up,
    and placing itself onto another runqueue, until after migrate_swap
    has released all locks.
    
    This means that, when migrate_swap checks, task T will be either
    on the runqueue where it was originally seen, or not on any
    runqueue at all. Migrate_swap deals correctly with of those cases.
    
    Tested-by: Joe Mario <jmario@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: hannes@cmpxchg.org
    Cc: aarcange@redhat.com
    Cc: srikar@linux.vnet.ibm.com
    Cc: tglx@linutronix.de
    Cc: hpa@zytor.com
    Link: http://lkml.kernel.org/r/20131010181722.GO13848@laptop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d69cb325c27e..ffc708717b70 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1249,6 +1249,24 @@ static inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)
 	lock_set_subclass(&this_rq->lock.dep_map, 0, _RET_IP_);
 }
 
+static inline void double_lock(spinlock_t *l1, spinlock_t *l2)
+{
+	if (l1 > l2)
+		swap(l1, l2);
+
+	spin_lock(l1);
+	spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+}
+
+static inline void double_raw_lock(raw_spinlock_t *l1, raw_spinlock_t *l2)
+{
+	if (l1 > l2)
+		swap(l1, l2);
+
+	raw_spin_lock(l1);
+	raw_spin_lock_nested(l2, SINGLE_DEPTH_NESTING);
+}
+
 /*
  * double_rq_lock - safely lock two runqueues
  *

commit 0ec8aa00f2b4dc457836ef4e2662b02483e94fb7
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 7 11:29:33 2013 +0100

    sched/numa: Avoid migrating tasks that are placed on their preferred node
    
    This patch classifies scheduler domains and runqueues into types depending
    the number of tasks that are about their NUMA placement and the number
    that are currently running on their preferred node. The types are
    
    regular: There are tasks running that do not care about their NUMA
            placement.
    
    remote: There are tasks running that care about their placement but are
            currently running on a node remote to their ideal placement
    
    all: No distinction
    
    To implement this the patch tracks the number of tasks that are optimally
    NUMA placed (rq->nr_preferred_running) and the number of tasks running
    that care about their placement (nr_numa_running). The load balancer
    uses this information to avoid migrating idea placed NUMA tasks as long
    as better options for load balancing exists. For example, it will not
    consider balancing between a group whose tasks are all perfectly placed
    and a group with remote tasks.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1381141781-10992-56-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index eeb1923812a1..d69cb325c27e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -409,6 +409,10 @@ struct rq {
 	 * remote CPUs use both these fields when doing load calculation.
 	 */
 	unsigned int nr_running;
+#ifdef CONFIG_NUMA_BALANCING
+	unsigned int nr_numa_running;
+	unsigned int nr_preferred_running;
+#endif
 	#define CPU_LOAD_IDX_MAX 5
 	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
 	unsigned long last_load_update_tick;
@@ -557,6 +561,7 @@ static inline u64 rq_clock_task(struct rq *rq)
 }
 
 #ifdef CONFIG_NUMA_BALANCING
+extern void sched_setnuma(struct task_struct *p, int node);
 extern int migrate_task_to(struct task_struct *p, int cpu);
 extern int migrate_swap(struct task_struct *, struct task_struct *);
 #endif /* CONFIG_NUMA_BALANCING */

commit 82727018b0d33d188e9916bcf76f18387484cb04
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Oct 7 11:29:28 2013 +0100

    sched/numa: Call task_numa_free() from do_execve()
    
    It is possible for a task in a numa group to call exec, and
    have the new (unrelated) executable inherit the numa group
    association from its former self.
    
    This has the potential to break numa grouping, and is trivial
    to fix.
    
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-51-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 8037b10a256f..eeb1923812a1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -559,11 +559,6 @@ static inline u64 rq_clock_task(struct rq *rq)
 #ifdef CONFIG_NUMA_BALANCING
 extern int migrate_task_to(struct task_struct *p, int cpu);
 extern int migrate_swap(struct task_struct *, struct task_struct *);
-extern void task_numa_free(struct task_struct *p);
-#else /* CONFIG_NUMA_BALANCING */
-static inline void task_numa_free(struct task_struct *p)
-{
-}
 #endif /* CONFIG_NUMA_BALANCING */
 
 #ifdef CONFIG_SMP

commit 8c8a743c5087bac9caac8155b8f3b367e75cdd0b
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 7 11:29:21 2013 +0100

    sched/numa: Use {cpu, pid} to create task groups for shared faults
    
    While parallel applications tend to align their data on the cache
    boundary, they tend not to align on the page or THP boundary.
    Consequently tasks that partition their data can still "false-share"
    pages presenting a problem for optimal NUMA placement.
    
    This patch uses NUMA hinting faults to chain tasks together into
    numa_groups. As well as storing the NID a task was running on when
    accessing a page a truncated representation of the faulting PID is
    stored. If subsequent faults are from different PIDs it is reasonable
    to assume that those two tasks share a page and are candidates for
    being grouped together. Note that this patch makes no scheduling
    decisions based on the grouping information.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1381141781-10992-44-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 691e96964dcc..8037b10a256f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -559,10 +559,7 @@ static inline u64 rq_clock_task(struct rq *rq)
 #ifdef CONFIG_NUMA_BALANCING
 extern int migrate_task_to(struct task_struct *p, int cpu);
 extern int migrate_swap(struct task_struct *, struct task_struct *);
-static inline void task_numa_free(struct task_struct *p)
-{
-	kfree(p->numa_faults);
-}
+extern void task_numa_free(struct task_struct *p);
 #else /* CONFIG_NUMA_BALANCING */
 static inline void task_numa_free(struct task_struct *p)
 {

commit fb13c7ee0ed387bd6bec4b4024a4d49b1bd504f1
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:17 2013 +0100

    sched/numa: Use a system-wide search to find swap/migration candidates
    
    This patch implements a system-wide search for swap/migration candidates
    based on total NUMA hinting faults. It has a balance limit, however it
    doesn't properly consider total node balance.
    
    In the old scheme a task selected a preferred node based on the highest
    number of private faults recorded on the node. In this scheme, the preferred
    node is based on the total number of faults. If the preferred node for a
    task changes then task_numa_migrate will search the whole system looking
    for tasks to swap with that would improve both the overall compute
    balance and minimise the expected number of remote NUMA hinting faults.
    
    Not there is no guarantee that the node the source task is placed
    on by task_numa_migrate() has any relationship to the newly selected
    task->numa_preferred_nid due to compute overloading.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    [ Do not swap with tasks that cannot run on source cpu]
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    [ Fixed compiler warning on UP. ]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-40-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4dc92d016aef..691e96964dcc 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -610,9 +610,22 @@ static inline struct sched_domain *highest_flag_domain(int cpu, int flag)
 	return hsd;
 }
 
+static inline struct sched_domain *lowest_flag_domain(int cpu, int flag)
+{
+	struct sched_domain *sd;
+
+	for_each_domain(cpu, sd) {
+		if (sd->flags & flag)
+			break;
+	}
+
+	return sd;
+}
+
 DECLARE_PER_CPU(struct sched_domain *, sd_llc);
 DECLARE_PER_CPU(int, sd_llc_size);
 DECLARE_PER_CPU(int, sd_llc_id);
+DECLARE_PER_CPU(struct sched_domain *, sd_numa);
 
 struct sched_group_power {
 	atomic_t ref;

commit ac66f5477239ebd3c4e2cbf2f591ef387aa09884
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Oct 7 11:29:16 2013 +0100

    sched/numa: Introduce migrate_swap()
    
    Use the new stop_two_cpus() to implement migrate_swap(), a function that
    flips two tasks between their respective cpus.
    
    I'm fairly sure there's a less crude way than employing the stop_two_cpus()
    method, but everything I tried either got horribly fragile and/or complex. So
    keep it simple for now.
    
    The notable detail is how we 'migrate' tasks that aren't runnable
    anymore. We'll make it appear like we migrated them before they went to
    sleep. The sole difference is the previous cpu in the wakeup path, so we
    override this.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Link: http://lkml.kernel.org/r/1381141781-10992-39-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 66458c902d84..4dc92d016aef 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -558,6 +558,7 @@ static inline u64 rq_clock_task(struct rq *rq)
 
 #ifdef CONFIG_NUMA_BALANCING
 extern int migrate_task_to(struct task_struct *p, int cpu);
+extern int migrate_swap(struct task_struct *, struct task_struct *);
 static inline void task_numa_free(struct task_struct *p)
 {
 	kfree(p->numa_faults);
@@ -736,6 +737,7 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
 	 */
 	smp_wmb();
 	task_thread_info(p)->cpu = cpu;
+	p->wake_cpu = cpu;
 #endif
 }
 
@@ -991,7 +993,7 @@ struct sched_class {
 	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 
 #ifdef CONFIG_SMP
-	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
+	int  (*select_task_rq)(struct task_struct *p, int task_cpu, int sd_flag, int flags);
 	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
 
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);

commit e6628d5b0a2979f3e0ee6f7783ede5df50cb9ede
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:29:02 2013 +0100

    sched/numa: Reschedule task on preferred NUMA node once selected
    
    A preferred node is selected based on the node the most NUMA hinting
    faults was incurred on. There is no guarantee that the task is running
    on that node at the time so this patch rescheules the task to run on
    the most idle CPU of the selected node when selected. This avoids
    waiting for the balancer to make a decision.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-25-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 199099c7aa22..66458c902d84 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -557,6 +557,7 @@ static inline u64 rq_clock_task(struct rq *rq)
 }
 
 #ifdef CONFIG_NUMA_BALANCING
+extern int migrate_task_to(struct task_struct *p, int cpu);
 static inline void task_numa_free(struct task_struct *p)
 {
 	kfree(p->numa_faults);

commit f809ca9a554dda49fb264c79e31c722e0b063ff8
Author: Mel Gorman <mgorman@suse.de>
Date:   Mon Oct 7 11:28:57 2013 +0100

    sched/numa: Track NUMA hinting faults on per-node basis
    
    This patch tracks what nodes numa hinting faults were incurred on.
    This information is later used to schedule a task on the node storing
    the pages most frequently faulted by the task.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-20-git-send-email-mgorman@suse.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e82484db7699..199099c7aa22 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -6,6 +6,7 @@
 #include <linux/spinlock.h>
 #include <linux/stop_machine.h>
 #include <linux/tick.h>
+#include <linux/slab.h>
 
 #include "cpupri.h"
 #include "cpuacct.h"
@@ -555,6 +556,17 @@ static inline u64 rq_clock_task(struct rq *rq)
 	return rq->clock_task;
 }
 
+#ifdef CONFIG_NUMA_BALANCING
+static inline void task_numa_free(struct task_struct *p)
+{
+	kfree(p->numa_faults);
+}
+#else /* CONFIG_NUMA_BALANCING */
+static inline void task_numa_free(struct task_struct *p)
+{
+}
+#endif /* CONFIG_NUMA_BALANCING */
+
 #ifdef CONFIG_SMP
 
 #define rcu_dereference_check_sched_domain(p) \

commit 9bd721c55c8a886b938a45198aab0ccb52f1f7fa
Author: Jason Low <jason.low2@hp.com>
Date:   Fri Sep 13 11:26:52 2013 -0700

    sched/balancing: Consider max cost of idle balance per sched domain
    
    In this patch, we keep track of the max cost we spend doing idle load balancing
    for each sched domain. If the avg time the CPU remains idle is less then the
    time we have already spent on idle balancing + the max cost of idle balancing
    in the sched domain, then we don't continue to attempt the balance. We also
    keep a per rq variable, max_idle_balance_cost, which keeps track of the max
    time spent on newidle load balances throughout all its domains so that we can
    determine the avg_idle's max value.
    
    By using the max, we avoid overrunning the average. This further reduces the
    chance we attempt balancing when the CPU is not idle for longer than the cost
    to balance.
    
    Signed-off-by: Jason Low <jason.low2@hp.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1379096813-3032-3-git-send-email-jason.low2@hp.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0d7544c3dba7..e82484db7699 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -476,6 +476,9 @@ struct rq {
 	u64 age_stamp;
 	u64 idle_stamp;
 	u64 avg_idle;
+
+	/* This is used to determine avg_idle's max value */
+	u64 max_idle_balance_cost;
 #endif
 
 #ifdef CONFIG_IRQ_TIME_ACCOUNTING

commit 6263322c5e8ffdaf5eaaa29e9d02d84a786aa970
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Aug 19 12:41:09 2013 +0200

    sched/fair: Rewrite group_imb trigger
    
    Change the group_imb detection from the old 'load-spike' detector to
    an actual imbalance detector. We set it from the lower domain balance
    pass when it fails to create a balance in the presence of task
    affinities.
    
    The advantage is that this should no longer generate the false
    positive group_imb conditions generated by transient load spikes from
    the normal balancing/bulk-wakeup etc. behaviour.
    
    While I haven't actually observed those they could happen.
    
    I'm not entirely happy with this patch; it somehow feels a little
    fragile.
    
    Nor does it solve the biggest issue I have with the group_imb code; it
    it still a fragile construct in that once we 'fixed' the imbalance
    we'll not detect the group_imb again and could end up re-creating it.
    
    That said, this patch does seem to preserve behaviour for the
    described degenerate case. In particular on my 2*6*2 wsm-ep:
    
      taskset -c 3-11 bash -c 'for ((i=0;i<9;i++)) do while :; do :; done & done'
    
    ends up with 9 spinners, each on their own CPU; whereas if you disable
    the group_imb code that typically doesn't happen (you'll get one pair
    sharing a CPU most of the time).
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/tip-36fpbgl39dv4u51b6yz2ypz5@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index b3c5653e1dca..0d7544c3dba7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -605,6 +605,7 @@ struct sched_group_power {
 	 */
 	unsigned int power, power_orig;
 	unsigned long next_update;
+	int imbalance; /* XXX unrelated to power but shared group state */
 	/*
 	 * Number of busy cpus in this group.
 	 */

commit 0d99b7087324978b09b59d8c7a0736214c4a42b1
Merge: 4689550bb278 ae23bff1d71f 61bf86ad8644
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 4 08:25:35 2013 -0700

    Merge branches 'perf-urgent-for-linus' and 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf changes from Ingo Molnar:
     "As a first remark I'd like to point out that the obsolete '-f'
      (--force) option, which has not done anything for several releases,
      has been removed from 'perf record' and related utilities.  Everyone
      please update muscle memory accordingly! :-)
    
      Main changes on the perf kernel side:
    
       - Performance optimizations:
            . for trace events, by Steve Rostedt.
            . for time values, by Peter Zijlstra
    
       - New hardware support:
            . for Intel Silvermont (22nm Atom) CPUs, by Zheng Yan
            . for Intel SNB-EP uncore PMUs, by Zheng Yan
    
       - Enhanced hardware support:
            . for Intel uncore PMUs: add filter support for QPI boxes, by Zheng Yan
    
       - Core perf events code enhancements and fixes:
            . for full-nohz feature handling, by Frederic Weisbecker
            . for group events, by Jiri Olsa
            . for call chains, by Frederic Weisbecker
            . for event stream parsing, by Adrian Hunter
    
       - New ABI details:
            . Add attr->mmap2 attribute, by Stephane Eranian
            . Add PERF_EVENT_IOC_ID ioctl to return event ID, by Jiri Olsa
            . Export u64 time_zero on the mmap header page to allow TSC
              calculation, by Adrian Hunter
            . Add dummy software event, by Adrian Hunter.
            . Add a new PERF_SAMPLE_IDENTIFIER to make samples always
              parseable, by Adrian Hunter.
            . Make Power7 events available via sysfs, by Runzhen Wang.
    
       - Code cleanups and refactorings:
            . for nohz-full, by Frederic Weisbecker
            . for group events, by Jiri Olsa
    
       - Documentation updates:
            . for perf_event_type, by Peter Zijlstra
    
      Main changes on the perf tooling side (some of these tooling changes
      utilize the above kernel side changes):
    
       - Lots of 'perf trace' enhancements:
    
            . Make 'perf trace' command line arguments consistent with
              'perf record', by David Ahern.
    
            . Allow specifying syscalls a la strace, by Arnaldo Carvalho de Melo.
    
            . Add --verbose and -o/--output options, by Arnaldo Carvalho de Melo.
    
            . Support ! in -e expressions, to filter a list of syscalls,
              by Arnaldo Carvalho de Melo.
    
            . Arg formatting improvements to allow masking arguments in
              syscalls such as futex and open, where the some arguments are
              ignored and thus should not be printed depending on other args,
              by Arnaldo Carvalho de Melo.
    
            . Beautify futex open, openat, open_by_handle_at, lseek and futex
              syscalls, by Arnaldo Carvalho de Melo.
    
            . Add option to analyze events in a file versus live, so that
              one can do:
    
               [root@zoo ~]# perf record -a -e raw_syscalls:* sleep 1
               [ perf record: Woken up 0 times to write data ]
               [ perf record: Captured and wrote 25.150 MB perf.data (~1098836 samples) ]
               [root@zoo ~]# perf trace -i perf.data -e futex --duration 1
                  17.799 ( 1.020 ms): 7127 futex(uaddr: 0x7fff3f6c6674, op: 393, val: 1, utime: 0x7fff3f6c6470, ua
                 113.344 (95.429 ms): 7127 futex(uaddr: 0x7fff3f6c6674, op: 393, val: 1, utime: 0x7fff3f6c6470, uaddr2: 0x7fff3f6c6648, val3: 4294967
                 133.778 ( 1.042 ms): 18004 futex(uaddr: 0x7fff3f6c6674, op: 393, val: 1, utime: 0x7fff3f6c6470, uaddr2: 0x7fff3f6c6648, val3: 429496
               [root@zoo ~]#
    
              By David Ahern.
    
            . Honor target pid / tid options when analyzing a file, by David Ahern.
    
            . Introduce better formatting of syscall arguments, including so
              far beautifiers for mmap, madvise, syscall return values,
              by Arnaldo Carvalho de Melo.
    
            . Handle HUGEPAGE defines in the mmap beautifier, by David Ahern.
    
       - 'perf report/top' enhancements:
    
            . Do annotation using /proc/kcore and /proc/kallsyms when
              available, removing the forced need for a vmlinux file kernel
              assembly annotation. This also improves this use case because
              vmlinux has just the initial kernel image, not what is actually
              in use after various code patchings by things like alternatives.
              By Adrian Hunter.
    
            . Add --ignore-callees=<regex> option to collapse undesired parts
              of call graphs, by Greg Price.
    
            . Simplify symbol filtering by doing it at machine class level,
              by Adrian Hunter.
    
            . Add support for callchains in the gtk UI, by Namhyung Kim.
    
            . Add --objdump option to 'perf top', by Sukadev Bhattiprolu.
    
       - 'perf kvm' enhancements:
    
            . Add option to print only events that exceed a specified time
              duration, by David Ahern.
    
            . Improve stack trace printing, by David Ahern.
    
            . Update documentation of the live command, by David Ahern
    
            . Add perf kvm stat live mode that combines aspects of 'perf kvm
              stat' record and report, by David Ahern.
    
            . Add option to analyze specific VM in perf kvm stat report, by
              David Ahern.
    
            . Do not require /lib/modules/* on a guest, by Jason Wessel.
    
       - 'perf script' enhancements:
    
            . Fix symbol offset computation for some dsos, by David Ahern.
    
            . Fix named threads support, by David Ahern.
    
            . Don't install scripting files files when perl/python support
              is disabled, by Arnaldo Carvalho de Melo.
    
       - 'perf test' enhancements:
    
            . Add various improvements and fixes to the "vmlinux matches
              kallsyms" 'perf test' entry, related to the /proc/kcore
              annotation feature. By Adrian Hunter.
    
            . Add sample parsing test, by Adrian Hunter.
    
            . Add test for reading object code, by Adrian Hunter.
    
            . Add attr record group sampling test, by Jiri Olsa.
    
            . Misc testing infrastructure improvements and other details,
              by Jiri Olsa.
    
       - 'perf list' enhancements:
    
            . Skip unsupported hardware events, by Namhyung Kim.
    
            . List pmu events, by Andi Kleen.
    
       - 'perf diff' enhancements:
    
            . Add support for more than two files comparison, by Jiri Olsa.
    
       - 'perf sched' enhancements:
    
            . Various improvements, including removing reliance on some
              scheduler tracepoints that provide the same information as the
              PERF_RECORD_{FORK,EXIT} events. By David Ahern.
    
            . Remove odd build stall by moving a large struct initialization
              from a local variable to a global one, by Namhyung Kim.
    
       - 'perf stat' enhancements:
    
            . Add --initial-delay option to skip measuring for a defined
              startup phase, by Andi Kleen.
    
       - Generic perf tooling infrastructure/plumbing changes:
    
            . Tidy up sample parsing validation, by Adrian Hunter.
    
            . Fix up jobserver setup in libtraceevent Makefile.
              by Arnaldo Carvalho de Melo.
    
            . Debug improvements, by Adrian Hunter.
    
            . Fix correlation of samples coming after PERF_RECORD_EXIT event,
              by David Ahern.
    
            . Improve robustness of the topology parsing code,
              by Stephane Eranian.
    
            . Add group leader sampling, that allows just one event in a group
              to sample while the other events have just its values read,
              by Jiri Olsa.
    
            . Add support for a new modifier "D", which requests that the
              event, or group of events, be pinned to the PMU.
              By Michael Ellerman.
    
            . Support callchain sorting based on addresses, by Andi Kleen
    
            . Prep work for multi perf data file storage, by Jiri Olsa.
    
            . libtraceevent cleanups, by Namhyung Kim.
    
      And lots and lots of other fixes and code reorganizations that did not
      make it into the list, see the shortlog, diffstat and the Git log for
      details!"
    
    [ Also merge a leftover from the 3.11 cycle ]
    
    * 'perf-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      perf: Prevent race in unthrottling code
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (237 commits)
      perf trace: Tell arg formatters the arg index
      perf trace: Add beautifier for open's flags arg
      perf trace: Add beautifier for lseek's whence arg
      perf tools: Fix symbol offset computation for some dsos
      perf list: Skip unsupported events
      perf tests: Add 'keep tracking' test
      perf tools: Add support for PERF_COUNT_SW_DUMMY
      perf: Add a dummy software event to keep tracking
      perf trace: Add beautifier for futex 'operation' parm
      perf trace: Allow syscall arg formatters to mask args
      perf: Convert kmalloc_node(...GFP_ZERO...) to kzalloc_node()
      perf: Export struct perf_branch_entry to userspace
      perf: Add attr->mmap2 attribute to an event
      perf/x86: Add Silvermont (22nm Atom) support
      perf/x86: use INTEL_UEVENT_EXTRA_REG to define MSR_OFFCORE_RSP_X
      perf trace: Handle missing HUGEPAGE defines
      perf trace: Honor target pid / tid options when analyzing a file
      perf trace: Add option to analyze events in a file versus live
      perf evlist: Add tracepoint lookup by name
      perf tests: Add a sample parsing test
      ...

commit 8af01f56a03e9cbd91a55d688fce1315021efba8
Author: Tejun Heo <tj@kernel.org>
Date:   Thu Aug 8 20:11:22 2013 -0400

    cgroup: s/cgroup_subsys_state/cgroup_css/ s/task_subsys_state/task_css/
    
    The names of the two struct cgroup_subsys_state accessors -
    cgroup_subsys_state() and task_subsys_state() - are somewhat awkward.
    The former clashes with the type name and the latter doesn't even
    indicate it's somehow related to cgroup.
    
    We're about to revamp large portion of cgroup API, so, let's rename
    them so that they're less awkward.  Most per-controller usages of the
    accessors are localized in accessor wrappers and given the amount of
    scheduled changes, this isn't gonna add any noticeable headache.
    
    Rename cgroup_subsys_state() to cgroup_css() and task_subsys_state()
    to task_css().  This patch is pure rename.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Li Zefan <lizefan@huawei.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ef0a7b2439dd..471a56db05ea 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -665,9 +665,9 @@ extern int group_balance_cpu(struct sched_group *sg);
 /*
  * Return the group to which this tasks belongs.
  *
- * We cannot use task_subsys_state() and friends because the cgroup
- * subsystem changes that value before the cgroup_subsys::attach() method
- * is called, therefore we cannot pin it and might observe the wrong value.
+ * We cannot use task_css() and friends because the cgroup subsystem
+ * changes that value before the cgroup_subsys::attach() method is called,
+ * therefore we cannot pin it and might observe the wrong value.
  *
  * The same is true for autogroup's p->signal->autogroup->tg, the autogroup
  * core changes this before calling sched_move_task().

commit 7d9ffa8961482232d964173cccba6e14d2d543b2
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Jul 4 12:56:46 2013 +0800

    sched: Micro-optimize the smart wake-affine logic
    
    Smart wake-affine is using node-size as the factor currently, but the overhead
    of the mask operation is high.
    
    Thus, this patch introduce the 'sd_llc_size' percpu variable, which will record
    the highest cache-share domain size, and make it to be the new factor, in order
    to reduce the overhead and make it more reasonable.
    
    Tested-by: Davidlohr Bueso <davidlohr.bueso@hp.com>
    Tested-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Michael Wang <wangyun@linux.vnet.ibm.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Link: http://lkml.kernel.org/r/51D5008E.6030102@linux.vnet.ibm.com
    [ Tidied up the changelog. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5e129efb84ce..4c1cb8029feb 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -594,6 +594,7 @@ static inline struct sched_domain *highest_flag_domain(int cpu, int flag)
 }
 
 DECLARE_PER_CPU(struct sched_domain *, sd_llc);
+DECLARE_PER_CPU(int, sd_llc_size);
 DECLARE_PER_CPU(int, sd_llc_id);
 
 struct sched_group_power {

commit 685207963be973fbb73550db6edaf920a283e1a7
Author: Vladimir Davydov <vdavydov@parallels.com>
Date:   Mon Jul 15 17:49:19 2013 +0400

    sched: Move h_load calculation to task_h_load()
    
    The bad thing about update_h_load(), which computes hierarchical load
    factor for task groups, is that it is called for each task group in the
    system before every load balancer run, and since rebalance can be
    triggered very often, this function can eat really a lot of cpu time if
    there are many cpu cgroups in the system.
    
    Although the situation was improved significantly by commit a35b646
    ('sched, cgroup: Reduce rq->lock hold times for large cgroup
    hierarchies'), the problem still can arise under some kinds of loads,
    e.g. when cpus are switching from idle to busy and back very frequently.
    
    For instance, when I start 1000 of processes that wake up every
    millisecond on my 8 cpus host, 'top' and 'perf top' show:
    
    Cpu(s): 17.8%us, 24.3%sy,  0.0%ni, 57.9%id,  0.0%wa,  0.0%hi,  0.0%si
    Events: 243K cycles
      7.57%  [kernel]               [k] __schedule
      7.08%  [kernel]               [k] timerqueue_add
      6.13%  libc-2.12.so           [.] usleep
    
    Then if I create 10000 *idle* cpu cgroups (no processes in them), cpu
    usage increases significantly although the 'wakers' are still executing
    in the root cpu cgroup:
    
    Cpu(s): 19.1%us, 48.7%sy,  0.0%ni, 31.6%id,  0.0%wa,  0.0%hi,  0.7%si
    Events: 230K cycles
     24.56%  [kernel]            [k] tg_load_down
      5.76%  [kernel]            [k] __schedule
    
    This happens because this particular kind of load triggers 'new idle'
    rebalance very frequently, which requires calling update_h_load(),
    which, in turn, calls tg_load_down() for every *idle* cpu cgroup even
    though it is absolutely useless, because idle cpu cgroups have no tasks
    to pull.
    
    This patch tries to improve the situation by making h_load calculation
    proceed only when h_load is really necessary. To achieve this, it
    substitutes update_h_load() with update_cfs_rq_h_load(), which computes
    h_load only for a given cfs_rq and all its ascendants, and makes the
    load balancer call this function whenever it considers if a task should
    be pulled, i.e. it moves h_load calculations directly to task_h_load().
    For h_load of the same cfs_rq not to be updated multiple times (in case
    several tasks in the same cgroup are considered during the same balance
    run), the patch keeps the time of the last h_load update for each cfs_rq
    and breaks calculation when it finds h_load to be uptodate.
    
    The benefit of it is that h_load is computed only for those cfs_rq's,
    which really need it, in particular all idle task groups are skipped.
    Although this, in fact, moves h_load calculation under rq lock, it
    should not affect latency much, because the amount of work done under rq
    lock while trying to pull tasks is limited by sched_nr_migrate.
    
    After the patch applied with the setup described above (1000 wakers in
    the root cgroup and 10000 idle cgroups), I get:
    
    Cpu(s): 16.9%us, 24.8%sy,  0.0%ni, 58.4%id,  0.0%wa,  0.0%hi,  0.0%si
    Events: 242K cycles
      7.57%  [kernel]                  [k] __schedule
      6.70%  [kernel]                  [k] timerqueue_add
      5.93%  libc-2.12.so              [.] usleep
    
    Signed-off-by: Vladimir Davydov <vdavydov@parallels.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1373896159-1278-1-git-send-email-vdavydov@parallels.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ef0a7b2439dd..5e129efb84ce 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -285,7 +285,6 @@ struct cfs_rq {
 	/* Required to track per-cpu representation of a task_group */
 	u32 tg_runnable_contrib;
 	unsigned long tg_load_contrib;
-#endif /* CONFIG_FAIR_GROUP_SCHED */
 
 	/*
 	 *   h_load = weight * f(tg)
@@ -294,6 +293,9 @@ struct cfs_rq {
 	 * this group.
 	 */
 	unsigned long h_load;
+	u64 last_h_load_update;
+	struct sched_entity *h_load_next;
+#endif /* CONFIG_FAIR_GROUP_SCHED */
 #endif /* CONFIG_SMP */
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
@@ -429,9 +431,6 @@ struct rq {
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
 	struct list_head leaf_cfs_rq_list;
-#ifdef CONFIG_SMP
-	unsigned long h_load_throttle;
-#endif /* CONFIG_SMP */
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 #ifdef CONFIG_RT_GROUP_SCHED

commit a9cef46a10cc1b84bf2cdf4060766d858c0439d8
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:56 2013 +0800

    sched/tg: Remove tg.load_weight
    
    Since no one use it.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    Tested-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-13-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 705991906fbe..ef0a7b2439dd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -148,7 +148,6 @@ struct task_group {
 	struct cfs_rq **cfs_rq;
 	unsigned long shares;
 
-	atomic_t load_weight;
 #ifdef	CONFIG_SMP
 	atomic_long_t load_avg;
 	atomic_t runnable_avg;

commit 2509940fd71c2e2915a05052bbdbf2d478364184
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:55 2013 +0800

    sched/cfs_rq: Change atomic64_t removed_load to atomic_long_t
    
    Similar to runnable_load_avg, blocked_load_avg variable, long type is
    enough for removed_load in 64 bit or 32 bit machine.
    
    Then we avoid the expensive atomic64 operations on 32 bit machine.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    Tested-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-12-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 5585eb25e9a3..705991906fbe 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -278,8 +278,9 @@ struct cfs_rq {
 	 * the FAIR_GROUP_SCHED case).
 	 */
 	unsigned long runnable_load_avg, blocked_load_avg;
-	atomic64_t decay_counter, removed_load;
+	atomic64_t decay_counter;
 	u64 last_decay;
+	atomic_long_t removed_load;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* Required to track per-cpu representation of a task_group */

commit bf5b986ed4d20428eeec3df4a03dbfebb9b6538c
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:54 2013 +0800

    sched/tg: Use 'unsigned long' for load variable in task group
    
    Since tg->load_avg is smaller than tg->load_weight, we don't need a
    atomic64_t variable for load_avg in 32 bit machine.
    The same reason for cfs_rq->tg_load_contrib.
    
    The atomic_long_t/unsigned long variable type are more efficient and
    convenience for them.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Tested-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-11-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9eb12d9edd35..5585eb25e9a3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -150,7 +150,7 @@ struct task_group {
 
 	atomic_t load_weight;
 #ifdef	CONFIG_SMP
-	atomic64_t load_avg;
+	atomic_long_t load_avg;
 	atomic_t runnable_avg;
 #endif
 #endif
@@ -284,7 +284,7 @@ struct cfs_rq {
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* Required to track per-cpu representation of a task_group */
 	u32 tg_runnable_contrib;
-	u64 tg_load_contrib;
+	unsigned long tg_load_contrib;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
 
 	/*

commit 72a4cf20cb71a327c636c7042fdacc25abffc87c
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:53 2013 +0800

    sched: Change cfs_rq load avg to unsigned long
    
    Since the 'u64 runnable_load_avg, blocked_load_avg' in cfs_rq struct are
    smaller than 'unsigned long' cfs_rq->load.weight. We don't need u64
    vaiables to describe them. unsigned long is more efficient and convenience.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    Tested-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-10-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 9c65d46504b1..9eb12d9edd35 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -277,7 +277,7 @@ struct cfs_rq {
 	 * This allows for the description of both thread and group usage (in
 	 * the FAIR_GROUP_SCHED case).
 	 */
-	u64 runnable_load_avg, blocked_load_avg;
+	unsigned long runnable_load_avg, blocked_load_avg;
 	atomic64_t decay_counter, removed_load;
 	u64 last_decay;
 

commit a75cdaa915e42ef0e6f38dc7f2a6a1deca91d648
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:47 2013 +0800

    sched: Set an initial value of runnable avg for new forked task
    
    We need to initialize the se.avg.{decay_count, load_avg_contrib} for a
    new forked task. Otherwise random values of above variables cause a
    mess when a new task is enqueued:
    
        enqueue_task_fair
            enqueue_entity
                enqueue_entity_load_avg
    
    and make fork balancing imbalance due to incorrect load_avg_contrib.
    
    Further more, Morten Rasmussen notice some tasks were not launched at
    once after created. So Paul and Peter suggest giving a start value for
    new task runnable avg time same as sched_slice().
    
    PeterZ said:
    
    > So the 'problem' is that our running avg is a 'floating' average; ie. it
    > decays with time. Now we have to guess about the future of our newly
    > spawned task -- something that is nigh impossible seeing these CPU
    > vendors keep refusing to implement the crystal ball instruction.
    >
    > So there's two asymptotic cases we want to deal well with; 1) the case
    > where the newly spawned program will be 'nearly' idle for its lifetime;
    > and 2) the case where its cpu-bound.
    >
    > Since we have to guess, we'll go for worst case and assume its
    > cpu-bound; now we don't want to make the avg so heavy adjusting to the
    > near-idle case takes forever. We want to be able to quickly adjust and
    > lower our running avg.
    >
    > Now we also don't want to make our avg too light, such that it gets
    > decremented just for the new task not having had a chance to run yet --
    > even if when it would run, it would be more cpu-bound than not.
    >
    > So what we do is we make the initial avg of the same duration as that we
    > guess it takes to run each task on the system at least once -- aka
    > sched_slice().
    >
    > Of course we can defeat this with wakeup/fork bombs, but in the 'normal'
    > case it should be good enough.
    
    Paul also contributed most of the code comments in this commit.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Reviewed-by: Gu Zheng <guz.fnst@cn.fujitsu.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    [peterz; added explanation of sched_slice() usage]
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-4-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 31d25f80a7c6..9c65d46504b1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1048,6 +1048,8 @@ extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime
 
 extern void update_idle_cpu_load(struct rq *this_rq);
 
+extern void init_task_runnable_average(struct task_struct *p);
+
 #ifdef CONFIG_PARAVIRT
 static inline u64 steal_ticks(u64 steal)
 {

commit fa6bddeb14d59d701f846b174b59c9982e926e66
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:46 2013 +0800

    sched: Move a few runnable tg variables into CONFIG_SMP
    
    The following 2 variables are only used under CONFIG_SMP, so its
    better to move their definiation into CONFIG_SMP too.
    
            atomic64_t load_avg;
            atomic_t runnable_avg;
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-3-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 77ce668ba302..31d25f80a7c6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -149,9 +149,11 @@ struct task_group {
 	unsigned long shares;
 
 	atomic_t load_weight;
+#ifdef	CONFIG_SMP
 	atomic64_t load_avg;
 	atomic_t runnable_avg;
 #endif
+#endif
 
 #ifdef CONFIG_RT_GROUP_SCHED
 	struct sched_rt_entity **rt_se;

commit 141965c7494d984b2bf24efd361a3125278869c6
Author: Alex Shi <alex.shi@intel.com>
Date:   Wed Jun 26 13:05:39 2013 +0800

    Revert "sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking"
    
    Remove CONFIG_FAIR_GROUP_SCHED that covers the runnable info, then
    we can use runnable load variables.
    
    Also remove 2 CONFIG_FAIR_GROUP_SCHED setting which is not in reverted
    patch(introduced in 9ee474f), but also need to revert.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/51CA76A3.3050207@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 029601a61587..77ce668ba302 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -269,12 +269,6 @@ struct cfs_rq {
 #endif
 
 #ifdef CONFIG_SMP
-/*
- * Load-tracking only depends on SMP, FAIR_GROUP_SCHED dependency below may be
- * removed when useful for applications beyond shares distribution (e.g.
- * load-balance).
- */
-#ifdef CONFIG_FAIR_GROUP_SCHED
 	/*
 	 * CFS Load tracking
 	 * Under CFS, load is tracked on a per-entity basis and aggregated up.
@@ -284,9 +278,9 @@ struct cfs_rq {
 	u64 runnable_load_avg, blocked_load_avg;
 	atomic64_t decay_counter, removed_load;
 	u64 last_decay;
-#endif /* CONFIG_FAIR_GROUP_SCHED */
-/* These always depend on CONFIG_FAIR_GROUP_SCHED */
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
+	/* Required to track per-cpu representation of a task_group */
 	u32 tg_runnable_contrib;
 	u64 tg_load_contrib;
 #endif /* CONFIG_FAIR_GROUP_SCHED */
@@ -1027,17 +1021,8 @@ extern void update_group_power(struct sched_domain *sd, int cpu);
 extern void trigger_load_balance(struct rq *rq, int cpu);
 extern void idle_balance(int this_cpu, struct rq *this_rq);
 
-/*
- * Only depends on SMP, FAIR_GROUP_SCHED may be removed when runnable_avg
- * becomes useful in lb
- */
-#if defined(CONFIG_FAIR_GROUP_SCHED)
 extern void idle_enter_fair(struct rq *this_rq);
 extern void idle_exit_fair(struct rq *this_rq);
-#else
-static inline void idle_enter_fair(struct rq *this_rq) {}
-static inline void idle_exit_fair(struct rq *this_rq) {}
-#endif
 
 #else	/* CONFIG_SMP */
 

commit e23ee74777f389369431d77390c4b09332ce026a
Author: Kirill Tkhai <tkhai@yandex.ru>
Date:   Fri Jun 7 15:37:43 2013 -0400

    sched/rt: Simplify pull_rt_task() logic and remove .leaf_rt_rq_list
    
    [ Peter, this is based off of some of my work, I ran it though a few
      tests and it passed. I also reviewed it, and added my SOB as I am
      somewhat a co-author to it. ]
    
    Based on the patch by Steven Rostedt from previous year:
    
    https://lkml.org/lkml/2012/4/18/517
    
    1)Simplify pull_rt_task() logic: search in pushable tasks of dest runqueue.
    The only pullable tasks are the tasks which are pushable in their local rq,
    and no others.
    
    2)Remove .leaf_rt_rq_list member of struct rt_rq and functions connected
    with it: nobody uses it since now.
    
    Signed-off-by: Kirill Tkhai <tkhai@yandex.ru>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/287571370557898@web7d.yandex.ru
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 74ff659e964f..029601a61587 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -361,7 +361,6 @@ struct rt_rq {
 	unsigned long rt_nr_boosted;
 
 	struct rq *rq;
-	struct list_head leaf_rt_rq_list;
 	struct task_group *tg;
 #endif
 };

commit 78becc27097585c6aec7043834cadde950ae79f2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Apr 12 01:51:02 2013 +0200

    sched: Use an accessor to read the rq clock
    
    Read the runqueue clock through an accessor. This
    prepares for adding a debugging infrastructure to
    detect missing or redundant calls to update_rq_clock()
    between a scheduler's entry and exit point.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1365724262-20142-6-git-send-email-fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c806c61a1261..74ff659e964f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -548,6 +548,16 @@ DECLARE_PER_CPU(struct rq, runqueues);
 #define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
 #define raw_rq()		(&__raw_get_cpu_var(runqueues))
 
+static inline u64 rq_clock(struct rq *rq)
+{
+	return rq->clock;
+}
+
+static inline u64 rq_clock_task(struct rq *rq)
+{
+	return rq->clock_task;
+}
+
 #ifdef CONFIG_SMP
 
 #define rcu_dereference_check_sched_domain(p) \

commit c5405a495e88d93cf9b4f4cc91507c7f4afcb901
Author: Neil Zhang <zhangwm@marvell.com>
Date:   Thu Apr 11 21:04:59 2013 +0800

    sched: Remove redundant update_runtime notifier
    
    migration_call() will do all the things that update_runtime() does.
    So let's remove it.
    
    Furthermore, there is potential risk that the current code will catch
    BUG_ON at line 689 of rt.c when do cpu hotplug while there are realtime
    threads running because of enabling runtime twice while the rt_runtime
    may already changed.
    
    Signed-off-by: Neil Zhang <zhangwm@marvell.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1365685499-26515-1-git-send-email-zhangwm@marvell.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f1f6256c1224..c806c61a1261 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1041,7 +1041,6 @@ static inline void idle_balance(int cpu, struct rq *rq)
 extern void sysrq_sched_debug_show(void);
 extern void sched_init_granularity(void);
 extern void update_max_interval(void);
-extern int update_runtime(struct notifier_block *nfb, unsigned long action, void *hcpu);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);
 

commit 8527632dc95472adb571701e852479531c0567a2
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Apr 19 15:10:50 2013 -0400

    sched: Move update_load_*() methods from sched.h to fair.c
    
    These inlines are only used by kernel/sched/fair.c so they do
    not need to be present in the main kernel/sched/sched.h file.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/1366398650-31599-3-git-send-email-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index a38ee0a0650e..f1f6256c1224 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -892,24 +892,6 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 #define WF_FORK		0x02		/* child wakeup after fork */
 #define WF_MIGRATED	0x4		/* internal use, task got migrated */
 
-static inline void update_load_add(struct load_weight *lw, unsigned long inc)
-{
-	lw->weight += inc;
-	lw->inv_weight = 0;
-}
-
-static inline void update_load_sub(struct load_weight *lw, unsigned long dec)
-{
-	lw->weight -= dec;
-	lw->inv_weight = 0;
-}
-
-static inline void update_load_set(struct load_weight *lw, unsigned long w)
-{
-	lw->weight = w;
-	lw->inv_weight = 0;
-}
-
 /*
  * To aid in avoiding the subversion of "niceness" due to uneven distribution
  * of tasks with abnormal "nice" values across CPUs the contribution that

commit 45ceebf77653975815d82fcf7cec0a164215ae11
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Fri Apr 19 15:10:49 2013 -0400

    sched: Factor out load calculation code from sched/core.c --> sched/proc.c
    
    This large chunk of load calculation code can be easily divorced
    from the main core.c scheduler file, with only a couple
    prototypes and externs added to a kernel/sched header.
    
    Some recent commits expanded the code and the documentation of
    it, making it large enough to warrant separation.  For example,
    see:
    
      556061b, "sched/nohz: Fix rq->cpu_load[] calculations"
      5aaa0b7, "sched/nohz: Fix rq->cpu_load calculations some more"
      5167e8d, "sched/nohz: Rewrite and fix load-avg computation -- again"
    
    More importantly, it helps reduce the size of the main
    sched/core.c by yet another significant amount (~600 lines).
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/1366398650-31599-2-git-send-email-paul.gortmaker@windriver.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ce39224d6155..a38ee0a0650e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -10,8 +10,16 @@
 #include "cpupri.h"
 #include "cpuacct.h"
 
+struct rq;
+
 extern __read_mostly int scheduler_running;
 
+extern unsigned long calc_load_update;
+extern atomic_long_t calc_load_tasks;
+
+extern long calc_load_fold_active(struct rq *this_rq);
+extern void update_cpu_load_active(struct rq *this_rq);
+
 /*
  * Convert user-nice values [ -20 ... 0 ... 19 ]
  * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],

commit 265f22a975c1e4cc3a4d1f94a3ec53ffbb6f5b9f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri May 3 03:39:05 2013 +0200

    sched: Keep at least 1 tick per second for active dynticks tasks
    
    The scheduler doesn't yet fully support environments
    with a single task running without a periodic tick.
    
    In order to ensure we still maintain the duties of scheduler_tick(),
    keep at least 1 tick per second.
    
    This makes sure that we keep the progression of various scheduler
    accounting and background maintainance even with a very low granularity.
    Examples include cpu load, sched average, CFS entity vruntime,
    avenrun and events such as load balancing, amongst other details
    handled in sched_class::task_tick().
    
    This limitation will be removed in the future once we get
    these individual items to work in full dynticks CPUs.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 24dc29897749..ce39224d6155 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -409,6 +409,9 @@ struct rq {
 #ifdef CONFIG_NO_HZ_COMMON
 	u64 nohz_stamp;
 	unsigned long nohz_flags;
+#endif
+#ifdef CONFIG_NO_HZ_FULL
+	unsigned long last_sched_tick;
 #endif
 	int skip_clock_update;
 
@@ -1090,6 +1093,13 @@ static inline void dec_nr_running(struct rq *rq)
 	rq->nr_running--;
 }
 
+static inline void rq_last_tick_reset(struct rq *rq)
+{
+#ifdef CONFIG_NO_HZ_FULL
+	rq->last_sched_tick = jiffies;
+#endif
+}
+
 extern void update_rq_clock(struct rq *rq);
 
 extern void activate_task(struct rq *rq, struct task_struct *p, int flags);

commit c032862fba51a3ca504752d3a25186b324c5ce83
Merge: fda76e074c77 8700c95adb03
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 2 17:37:49 2013 +0200

    Merge commit '8700c95adb03' into timers/nohz
    
    The full dynticks tree needs the latest RCU and sched
    upstream updates in order to fix some dependencies.
    
    Merge a common upstream merge point that has these
    updates.
    
    Conflicts:
            include/linux/perf_event.h
            kernel/rcutree.h
            kernel/rcutree_plugin.h
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 25f55d9d01ad7a7ad248fd5af1d22675ffd202c5
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Tue Apr 23 16:59:02 2013 +0200

    sched: Fix init NOHZ_IDLE flag
    
    On my SMP platform which is made of 5 cores in 2 clusters, I
    have the nr_busy_cpu field of sched_group_power struct that is
    not null when the platform is fully idle - which makes the
    scheduler unhappy.
    
    The root cause is:
    
    During the boot sequence, some CPUs reach the idle loop and set
    their NOHZ_IDLE flag while waiting for others CPUs to boot. But
    the nr_busy_cpus field is initialized later with the assumption
    that all CPUs are in the busy state whereas some CPUs have
    already set their NOHZ_IDLE flag.
    
    More generally, the NOHZ_IDLE flag must be initialized when new
    sched_domains are created in order to ensure that NOHZ_IDLE and
    nr_busy_cpus are aligned.
    
    This condition can be ensured by adding a synchronize_rcu()
    between the destruction of old sched_domains and the creation of
    new ones so the NOHZ_IDLE flag will not be updated with old
    sched_domain once it has been initialized. But this solution
    introduces a additionnal latency in the rebuild sequence that is
    called during cpu hotplug.
    
    As suggested by Frederic Weisbecker, another solution is to have
    the same rcu lifecycle for both NOHZ_IDLE and sched_domain
    struct. A new nohz_idle field is added to sched_domain so both
    status and sched_domain will share the same RCU lifecycle and
    will be always synchronized. In addition, there is no more need
    to protect nohz_idle against concurrent access as it is only
    modified by 2 exclusive functions called by local cpu.
    
    This solution has been prefered to the creation of a new struct
    with an extra pointer indirection for sched_domain.
    
    The synchronization is done at the cost of :
    
     - An additional indirection and a rcu_dereference for accessing nohz_idle.
     - We use only the nohz_idle field of the top sched_domain.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: linaro-kernel@lists.linaro.org
    Cc: peterz@infradead.org
    Cc: fweisbec@gmail.com
    Cc: pjt@google.com
    Cc: rostedt@goodmis.org
    Cc: efault@gmx.de
    Link: http://lkml.kernel.org/r/1366729142-14662-1-git-send-email-vincent.guittot@linaro.org
    [ Fixed !NO_HZ build bug. ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 605426a63588..4c225c4c7111 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1303,7 +1303,6 @@ extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
 enum rq_nohz_flag_bits {
 	NOHZ_TICK_STOPPED,
 	NOHZ_BALANCE_KICK,
-	NOHZ_IDLE,
 };
 
 #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)

commit 9f3660c2c1a221c886474587103c69f6034d3e4f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Apr 20 14:35:09 2013 +0200

    sched: Kick full dynticks CPU that have more than one task enqueued.
    
    Kick the tick on full dynticks CPUs when they get more
    than one task running on their queue. This makes sure that
    local fairness is maintained by the tick on the destination.
    
    This is done regardless of these tasks' class. We should
    be able to be more clever in the future depending on these. eg:
    a CPU that runs a SCHED_FIFO task doesn't need to maintain
    fairness against local pending tasks of the fair class.
    
    But keep things simple for now.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 889904dd6d77..eb363aa5d83c 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -5,6 +5,7 @@
 #include <linux/mutex.h>
 #include <linux/spinlock.h>
 #include <linux/stop_machine.h>
+#include <linux/tick.h>
 
 #include "cpupri.h"
 
@@ -1106,6 +1107,16 @@ static inline u64 steal_ticks(u64 steal)
 static inline void inc_nr_running(struct rq *rq)
 {
 	rq->nr_running++;
+
+#ifdef CONFIG_NO_HZ_FULL
+	if (rq->nr_running == 2) {
+		if (tick_nohz_full_cpu(rq->cpu)) {
+			/* Order rq->nr_running write against the IPI */
+			smp_wmb();
+			smp_send_reschedule(rq->cpu);
+		}
+       }
+#endif
 }
 
 static inline void dec_nr_running(struct rq *rq)

commit 642dbc39ab1ea00f47e0fee1b8e8a27da036d940
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Thu Apr 18 18:34:26 2013 +0200

    sched: Fix wrong rq's runnable_avg update with rt tasks
    
    The current update of the rq's load can be erroneous when RT
    tasks are involved.
    
    The update of the load of a rq that becomes idle, is done only
    if the avg_idle is less than sysctl_sched_migration_cost. If RT
    tasks and short idle duration alternate, the runnable_avg will
    not be updated correctly and the time will be accounted as idle
    time when a CFS task wakes up.
    
    A new idle_enter function is called when the next task is the
    idle function so the elapsed time will be accounted as run time
    in the load of the rq, whatever the average idle time is. The
    function update_rq_runnable_avg is removed from idle_balance.
    
    When a RT task is scheduled on an idle CPU, the update of the
    rq's load is not done when the rq exit idle state because CFS's
    functions are not called. Then, the idle_balance, which is
    called just before entering the idle function, updates the rq's
    load and makes the assumption that the elapsed time since the
    last update, was only running time.
    
    As a consequence, the rq's load of a CPU that only runs a
    periodic RT task, is close to LOAD_AVG_MAX whatever the running
    duration of the RT task is.
    
    A new idle_exit function is called when the prev task is the
    idle function so the elapsed time will be accounted as idle time
    in the rq's load.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: linaro-kernel@lists.linaro.org
    Cc: peterz@infradead.org
    Cc: pjt@google.com
    Cc: fweisbec@gmail.com
    Cc: efault@gmx.de
    Link: http://lkml.kernel.org/r/1366302867-5055-1-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 8116cf8e350f..605426a63588 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1024,6 +1024,18 @@ extern void update_group_power(struct sched_domain *sd, int cpu);
 extern void trigger_load_balance(struct rq *rq, int cpu);
 extern void idle_balance(int this_cpu, struct rq *this_rq);
 
+/*
+ * Only depends on SMP, FAIR_GROUP_SCHED may be removed when runnable_avg
+ * becomes useful in lb
+ */
+#if defined(CONFIG_FAIR_GROUP_SCHED)
+extern void idle_enter_fair(struct rq *this_rq);
+extern void idle_exit_fair(struct rq *this_rq);
+#else
+static inline void idle_enter_fair(struct rq *this_rq) {}
+static inline void idle_exit_fair(struct rq *this_rq) {}
+#endif
+
 #else	/* CONFIG_SMP */
 
 static inline void idle_balance(int cpu, struct rq *rq)

commit 60fed7891d4115be0ed7ff9ce6851eda80533c64
Author: Li Zefan <lizefan@huawei.com>
Date:   Fri Mar 29 14:36:43 2013 +0800

    sched: Split cpuacct code out of sched.h
    
    Add cpuacct.h and let sched.h include it.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5155367B.2060506@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3bd15a43eebc..8116cf8e350f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -7,6 +7,7 @@
 #include <linux/stop_machine.h>
 
 #include "cpupri.h"
+#include "cpuacct.h"
 
 extern __read_mostly int scheduler_running;
 
@@ -950,14 +951,6 @@ static const u32 prio_to_wmult[40] = {
  /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
 };
 
-/* Time spent by the tasks of the cpu accounting group executing in ... */
-enum cpuacct_stat_index {
-	CPUACCT_STAT_USER,	/* ... user mode */
-	CPUACCT_STAT_SYSTEM,	/* ... kernel mode */
-
-	CPUACCT_STAT_NSTATS,
-};
-
 #define ENQUEUE_WAKEUP		1
 #define ENQUEUE_HEAD		2
 #ifdef CONFIG_SMP
@@ -1054,45 +1047,6 @@ extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime
 
 extern void update_idle_cpu_load(struct rq *this_rq);
 
-#ifdef CONFIG_CGROUP_CPUACCT
-#include <linux/cgroup.h>
-/* track cpu usage of a group of tasks and its child groups */
-struct cpuacct {
-	struct cgroup_subsys_state css;
-	/* cpuusage holds pointer to a u64-type object on every cpu */
-	u64 __percpu *cpuusage;
-	struct kernel_cpustat __percpu *cpustat;
-};
-
-extern struct cgroup_subsys cpuacct_subsys;
-extern struct cpuacct root_cpuacct;
-
-/* return cpu accounting group corresponding to this container */
-static inline struct cpuacct *cgroup_ca(struct cgroup *cgrp)
-{
-	return container_of(cgroup_subsys_state(cgrp, cpuacct_subsys_id),
-			    struct cpuacct, css);
-}
-
-/* return cpu accounting group to which this task belongs */
-static inline struct cpuacct *task_ca(struct task_struct *tsk)
-{
-	return container_of(task_subsys_state(tsk, cpuacct_subsys_id),
-			    struct cpuacct, css);
-}
-
-static inline struct cpuacct *parent_ca(struct cpuacct *ca)
-{
-	if (!ca || !ca->css.cgroup->parent)
-		return NULL;
-	return cgroup_ca(ca->css.cgroup->parent);
-}
-
-extern void cpuacct_charge(struct task_struct *tsk, u64 cputime);
-#else
-static inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}
-#endif
-
 #ifdef CONFIG_PARAVIRT
 static inline u64 steal_ticks(u64 steal)
 {

commit 3451d0243c3cdfd729b36f9684a14659d4895ca3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Aug 10 23:21:01 2011 +0200

    nohz: Rename CONFIG_NO_HZ to CONFIG_NO_HZ_COMMON
    
    We are planning to convert the dynticks Kconfig options layout
    into a choice menu. The user must be able to easily pick
    any of the following implementations: constant periodic tick,
    idle dynticks, full dynticks.
    
    As this implies a mutual exclusion, the two dynticks implementions
    need to converge on the selection of a common Kconfig option in order
    to ease the sharing of a common infrastructure.
    
    It would thus seem pretty natural to reuse CONFIG_NO_HZ to
    that end. It already implements all the idle dynticks code
    and the full dynticks depends on all that code for now.
    So ideally the choice menu would propose CONFIG_NO_HZ_IDLE and
    CONFIG_NO_HZ_EXTENDED then both would select CONFIG_NO_HZ.
    
    On the other hand we want to stay backward compatible: if
    CONFIG_NO_HZ is set in an older config file, we want to
    enable CONFIG_NO_HZ_IDLE by default.
    
    But we can't afford both at the same time or we run into
    a circular dependency:
    
    1) CONFIG_NO_HZ_IDLE and CONFIG_NO_HZ_EXTENDED both select
       CONFIG_NO_HZ
    2) If CONFIG_NO_HZ is set, we default to CONFIG_NO_HZ_IDLE
    
    We might be able to support that from Kconfig/Kbuild but it
    may not be wise to introduce such a confusing behaviour.
    
    So to solve this, create a new CONFIG_NO_HZ_COMMON option
    which gathers the common code between idle and full dynticks
    (that common code for now is simply the idle dynticks code)
    and select it from their referring Kconfig.
    
    Then we'll later create CONFIG_NO_HZ_IDLE and map CONFIG_NO_HZ
    to it for backward compatibility.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Chris Metcalf <cmetcalf@tilera.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Geoff Levand <geoff@infradead.org>
    Cc: Gilad Ben Yossef <gilad@benyossef.com>
    Cc: Hakan Akkan <hakanakkan@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Kevin Hilman <khilman@linaro.org>
    Cc: Li Zhong <zhong@linux.vnet.ibm.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 3bd15a43eebc..889904dd6d77 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -404,7 +404,7 @@ struct rq {
 	#define CPU_LOAD_IDX_MAX 5
 	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
 	unsigned long last_load_update_tick;
-#ifdef CONFIG_NO_HZ
+#ifdef CONFIG_NO_HZ_COMMON
 	u64 nohz_stamp;
 	unsigned long nohz_flags;
 #endif
@@ -1333,7 +1333,7 @@ extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
 
-#ifdef CONFIG_NO_HZ
+#ifdef CONFIG_NO_HZ_COMMON
 enum rq_nohz_flag_bits {
 	NOHZ_TICK_STOPPED,
 	NOHZ_BALANCE_KICK,

commit b719203b846284e77f5c50fca04b458b6484aeae
Author: Li Zefan <lizefan@huawei.com>
Date:   Thu Mar 7 10:00:26 2013 +0800

    sched: Fix update_group_power() prototype placement to fix build warning when !CONFIG_SMP
    
    All warnings:
    
       In file included from kernel/sched/core.c:85:0:
       kernel/sched/sched.h:1036:39: warning: 'struct sched_domain' declared inside parameter list
       kernel/sched/sched.h:1036:39: warning: its scope is only this definition or declaration, which is probably not what you want
    
    It's because struct sched_domain is defined inside #if CONFIG_SMP,
    while update_group_power() is declared unconditionally.
    
    Fix this warning by declaring update_group_power() only if
    CONFIG_SMP=n.
    
    Build tested with CONFIG_SMP enabled and then disabled.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5137F4BA.2060101@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 30bebb955023..3bd15a43eebc 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1026,6 +1026,8 @@ extern const struct sched_class idle_sched_class;
 
 #ifdef CONFIG_SMP
 
+extern void update_group_power(struct sched_domain *sd, int cpu);
+
 extern void trigger_load_balance(struct rq *rq, int cpu);
 extern void idle_balance(int this_cpu, struct rq *this_rq);
 
@@ -1040,7 +1042,6 @@ static inline void idle_balance(int cpu, struct rq *rq)
 extern void sysrq_sched_debug_show(void);
 extern void sched_init_granularity(void);
 extern void update_max_interval(void);
-extern void update_group_power(struct sched_domain *sd, int cpu);
 extern int update_runtime(struct notifier_block *nfb, unsigned long action, void *hcpu);
 extern void init_sched_rt_class(void);
 extern void init_sched_fair_class(void);

commit 27b4b9319a3c2e8654d45df99ce584c7c2cfe100
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:07:52 2013 +0800

    sched: Remove double declaration of root_task_group
    
    It's already declared in include/linux/sched.h
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A7D8.7000107@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 304fc1c77143..30bebb955023 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -179,11 +179,6 @@ struct task_group {
 #define MAX_SHARES	(1UL << 18)
 #endif
 
-/* Default task group.
- *	Every task in system belong to this group at bootup.
- */
-extern struct task_group root_task_group;
-
 typedef int (*tg_visitor)(struct task_group *, void *);
 
 extern int walk_tg_tree_from(struct task_group *from,

commit 25cc7da7e6336d3bb6a5bad3d3fa96fce9a81d5b
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:07:33 2013 +0800

    sched: Move group scheduling functions out of include/linux/sched.h
    
    - Make sched_group_{set_,}runtime(), sched_group_{set_,}period()
    and sched_rt_can_attach() static.
    
    - Move sched_{create,destroy,online,offline}_group() to
    kernel/sched/sched.h.
    
    - Remove declaration of sched_group_shares().
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A7C5.3000708@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index eca526d7afbd..304fc1c77143 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -221,6 +221,18 @@ extern void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,
 		struct sched_rt_entity *rt_se, int cpu,
 		struct sched_rt_entity *parent);
 
+extern struct task_group *sched_create_group(struct task_group *parent);
+extern void sched_online_group(struct task_group *tg,
+			       struct task_group *parent);
+extern void sched_destroy_group(struct task_group *tg);
+extern void sched_offline_group(struct task_group *tg);
+
+extern void sched_move_task(struct task_struct *tsk);
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
+#endif
+
 #else /* CONFIG_CGROUP_SCHED */
 
 struct cfs_bandwidth { };

commit c82ba9fa7588dfd02d4dc99ad1af486304bc424c
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:55 2013 +0800

    sched: Move struct sched_class to kernel/sched/sched.h
    
    It's used internally only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A79F.8090502@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 4e5c2afdac91..eca526d7afbd 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -951,6 +951,61 @@ enum cpuacct_stat_index {
 	CPUACCT_STAT_NSTATS,
 };
 
+#define ENQUEUE_WAKEUP		1
+#define ENQUEUE_HEAD		2
+#ifdef CONFIG_SMP
+#define ENQUEUE_WAKING		4	/* sched_class::task_waking was called */
+#else
+#define ENQUEUE_WAKING		0
+#endif
+
+#define DEQUEUE_SLEEP		1
+
+struct sched_class {
+	const struct sched_class *next;
+
+	void (*enqueue_task) (struct rq *rq, struct task_struct *p, int flags);
+	void (*dequeue_task) (struct rq *rq, struct task_struct *p, int flags);
+	void (*yield_task) (struct rq *rq);
+	bool (*yield_to_task) (struct rq *rq, struct task_struct *p, bool preempt);
+
+	void (*check_preempt_curr) (struct rq *rq, struct task_struct *p, int flags);
+
+	struct task_struct * (*pick_next_task) (struct rq *rq);
+	void (*put_prev_task) (struct rq *rq, struct task_struct *p);
+
+#ifdef CONFIG_SMP
+	int  (*select_task_rq)(struct task_struct *p, int sd_flag, int flags);
+	void (*migrate_task_rq)(struct task_struct *p, int next_cpu);
+
+	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
+	void (*post_schedule) (struct rq *this_rq);
+	void (*task_waking) (struct task_struct *task);
+	void (*task_woken) (struct rq *this_rq, struct task_struct *task);
+
+	void (*set_cpus_allowed)(struct task_struct *p,
+				 const struct cpumask *newmask);
+
+	void (*rq_online)(struct rq *rq);
+	void (*rq_offline)(struct rq *rq);
+#endif
+
+	void (*set_curr_task) (struct rq *rq);
+	void (*task_tick) (struct rq *rq, struct task_struct *p, int queued);
+	void (*task_fork) (struct task_struct *p);
+
+	void (*switched_from) (struct rq *this_rq, struct task_struct *task);
+	void (*switched_to) (struct rq *this_rq, struct task_struct *task);
+	void (*prio_changed) (struct rq *this_rq, struct task_struct *task,
+			     int oldprio);
+
+	unsigned int (*get_rr_interval) (struct rq *rq,
+					 struct task_struct *task);
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	void (*task_move_group) (struct task_struct *p, int on_rq);
+#endif
+};
 
 #define sched_class_highest (&stop_sched_class)
 #define for_each_class(class) \

commit b13095f07f25464de65f5ce5ea94e16813d67488
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:38 2013 +0800

    sched: Move wake flags to kernel/sched/sched.h
    
    They are used internally only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A78E.7040609@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 1a4a2b19c2f4..4e5c2afdac91 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -865,6 +865,13 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 }
 #endif /* __ARCH_WANT_UNLOCKED_CTXSW */
 
+/*
+ * wake flags
+ */
+#define WF_SYNC		0x01		/* waker goes to sleep after wakeup */
+#define WF_FORK		0x02		/* child wakeup after fork */
+#define WF_MIGRATED	0x4		/* internal use, task got migrated */
+
 static inline void update_load_add(struct load_weight *lw, unsigned long inc)
 {
 	lw->weight += inc;

commit 5e6521eaa1ee581a13b904f35b80c5efeb2baccb
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:23 2013 +0800

    sched: Move struct sched_group to kernel/sched/sched.h
    
    Move struct sched_group_power and sched_group and related inline
    functions to kernel/sched/sched.h, as they are used internally
    only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A77F.2010705@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 709a30cdfd85..1a4a2b19c2f4 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -572,6 +572,62 @@ static inline struct sched_domain *highest_flag_domain(int cpu, int flag)
 DECLARE_PER_CPU(struct sched_domain *, sd_llc);
 DECLARE_PER_CPU(int, sd_llc_id);
 
+struct sched_group_power {
+	atomic_t ref;
+	/*
+	 * CPU power of this group, SCHED_LOAD_SCALE being max power for a
+	 * single CPU.
+	 */
+	unsigned int power, power_orig;
+	unsigned long next_update;
+	/*
+	 * Number of busy cpus in this group.
+	 */
+	atomic_t nr_busy_cpus;
+
+	unsigned long cpumask[0]; /* iteration mask */
+};
+
+struct sched_group {
+	struct sched_group *next;	/* Must be a circular list */
+	atomic_t ref;
+
+	unsigned int group_weight;
+	struct sched_group_power *sgp;
+
+	/*
+	 * The CPUs this group covers.
+	 *
+	 * NOTE: this field is variable length. (Allocated dynamically
+	 * by attaching extra space to the end of the structure,
+	 * depending on how many CPUs the kernel has booted up with)
+	 */
+	unsigned long cpumask[0];
+};
+
+static inline struct cpumask *sched_group_cpus(struct sched_group *sg)
+{
+	return to_cpumask(sg->cpumask);
+}
+
+/*
+ * cpumask masking which cpus in the group are allowed to iterate up the domain
+ * tree.
+ */
+static inline struct cpumask *sched_group_mask(struct sched_group *sg)
+{
+	return to_cpumask(sg->sgp->cpumask);
+}
+
+/**
+ * group_first_cpu - Returns the first cpu in the cpumask of a sched_group.
+ * @group: The group whose first cpu is to be returned.
+ */
+static inline unsigned int group_first_cpu(struct sched_group *group)
+{
+	return cpumask_first(sched_group_cpus(group));
+}
+
 extern int group_balance_cpu(struct sched_group *sg);
 
 #endif /* CONFIG_SMP */

commit cc1f4b1f3faed9f2040eff2a75f510b424b3cf18
Author: Li Zefan <lizefan@huawei.com>
Date:   Tue Mar 5 16:06:09 2013 +0800

    sched: Move SCHED_LOAD_SHIFT macros to kernel/sched/sched.h
    
    They are used internally only.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/5135A771.4070104@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index cc03cfdf469f..709a30cdfd85 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -33,6 +33,31 @@ extern __read_mostly int scheduler_running;
  */
 #define NS_TO_JIFFIES(TIME)	((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))
 
+/*
+ * Increase resolution of nice-level calculations for 64-bit architectures.
+ * The extra resolution improves shares distribution and load balancing of
+ * low-weight task groups (eg. nice +19 on an autogroup), deeper taskgroup
+ * hierarchies, especially on larger systems. This is not a user-visible change
+ * and does not change the user-interface for setting shares/weights.
+ *
+ * We increase resolution only if we have enough bits to allow this increased
+ * resolution (i.e. BITS_PER_LONG > 32). The costs for increasing resolution
+ * when BITS_PER_LONG <= 32 are pretty high and the returns do not justify the
+ * increased costs.
+ */
+#if 0 /* BITS_PER_LONG > 32 -- currently broken: it increases power usage under light load  */
+# define SCHED_LOAD_RESOLUTION	10
+# define scale_load(w)		((w) << SCHED_LOAD_RESOLUTION)
+# define scale_load_down(w)	((w) >> SCHED_LOAD_RESOLUTION)
+#else
+# define SCHED_LOAD_RESOLUTION	0
+# define scale_load(w)		(w)
+# define scale_load_down(w)	(w)
+#endif
+
+#define SCHED_LOAD_SHIFT	(10 + SCHED_LOAD_RESOLUTION)
+#define SCHED_LOAD_SCALE	(1L << SCHED_LOAD_SHIFT)
+
 #define NICE_0_LOAD		SCHED_LOAD_SCALE
 #define NICE_0_SHIFT		SCHED_LOAD_SHIFT
 
@@ -784,7 +809,6 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 }
 #endif /* __ARCH_WANT_UNLOCKED_CTXSW */
 
-
 static inline void update_load_add(struct load_weight *lw, unsigned long inc)
 {
 	lw->weight += inc;

commit 8bd75c77b7c6a3954140dd2e20346aef3efe4a35
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:47:07 2013 -0600

    sched/rt: Move rt specific bits into new header file
    
    Move rt scheduler definitions out of include/linux/sched.h into
    new file include/linux/sched/rt.h
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094707.7b9f825f@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ed8de30a040e..cc03cfdf469f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1,6 +1,7 @@
 
 #include <linux/sched.h>
 #include <linux/sched/sysctl.h>
+#include <linux/sched/rt.h>
 #include <linux/mutex.h>
 #include <linux/spinlock.h>
 #include <linux/stop_machine.h>

commit cf4aebc292fac7f34f8345664320e9d4a42ca76c
Author: Clark Williams <williams@redhat.com>
Date:   Thu Feb 7 09:46:59 2013 -0600

    sched: Move sched.h sysctl bits into separate header
    
    Move the sysctl-related bits from include/linux/sched.h into
    a new file: include/linux/sched/sysctl.h. Then update source
    files requiring access to those bits by including the new
    header file.
    
    Signed-off-by: Clark Williams <williams@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20130207094659.06dced96@riff.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fc886441436a..ed8de30a040e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1,5 +1,6 @@
 
 #include <linux/sched.h>
+#include <linux/sched/sysctl.h>
 #include <linux/mutex.h>
 #include <linux/spinlock.h>
 #include <linux/stop_machine.h>

commit 3d59eebc5e137bd89c6351e4c70e90ba1d0dc234
Merge: 11520e5e7c18 4fc3f1d66b1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 16 14:33:25 2012 -0800

    Merge tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma
    
    Pull Automatic NUMA Balancing bare-bones from Mel Gorman:
     "There are three implementations for NUMA balancing, this tree
      (balancenuma), numacore which has been developed in tip/master and
      autonuma which is in aa.git.
    
      In almost all respects balancenuma is the dumbest of the three because
      its main impact is on the VM side with no attempt to be smart about
      scheduling.  In the interest of getting the ball rolling, it would be
      desirable to see this much merged for 3.8 with the view to building
      scheduler smarts on top and adapting the VM where required for 3.9.
    
      The most recent set of comparisons available from different people are
    
        mel:    https://lkml.org/lkml/2012/12/9/108
        mingo:  https://lkml.org/lkml/2012/12/7/331
        tglx:   https://lkml.org/lkml/2012/12/10/437
        srikar: https://lkml.org/lkml/2012/12/10/397
    
      The results are a mixed bag.  In my own tests, balancenuma does
      reasonably well.  It's dumb as rocks and does not regress against
      mainline.  On the other hand, Ingo's tests shows that balancenuma is
      incapable of converging for this workloads driven by perf which is bad
      but is potentially explained by the lack of scheduler smarts.  Thomas'
      results show balancenuma improves on mainline but falls far short of
      numacore or autonuma.  Srikar's results indicate we all suffer on a
      large machine with imbalanced node sizes.
    
      My own testing showed that recent numacore results have improved
      dramatically, particularly in the last week but not universally.
      We've butted heads heavily on system CPU usage and high levels of
      migration even when it shows that overall performance is better.
      There are also cases where it regresses.  Of interest is that for
      specjbb in some configurations it will regress for lower numbers of
      warehouses and show gains for higher numbers which is not reported by
      the tool by default and sometimes missed in treports.  Recently I
      reported for numacore that the JVM was crashing with
      NullPointerExceptions but currently it's unclear what the source of
      this problem is.  Initially I thought it was in how numacore batch
      handles PTEs but I'm no longer think this is the case.  It's possible
      numacore is just able to trigger it due to higher rates of migration.
    
      These reports were quite late in the cycle so I/we would like to start
      with this tree as it contains much of the code we can agree on and has
      not changed significantly over the last 2-3 weeks."
    
    * tag 'balancenuma-v11' of git://git.kernel.org/pub/scm/linux/kernel/git/mel/linux-balancenuma: (50 commits)
      mm/rmap, migration: Make rmap_walk_anon() and try_to_unmap_anon() more scalable
      mm/rmap: Convert the struct anon_vma::mutex to an rwsem
      mm: migrate: Account a transhuge page properly when rate limiting
      mm: numa: Account for failed allocations and isolations as migration failures
      mm: numa: Add THP migration for the NUMA working set scanning fault case build fix
      mm: numa: Add THP migration for the NUMA working set scanning fault case.
      mm: sched: numa: Delay PTE scanning until a task is scheduled on a new node
      mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
      mm: sched: numa: Control enabling and disabling of NUMA balancing
      mm: sched: Adapt the scanning rate if a NUMA hinting fault does not migrate
      mm: numa: Use a two-stage filter to restrict pages being migrated for unlikely task<->node relationships
      mm: numa: migrate: Set last_nid on newly allocated page
      mm: numa: split_huge_page: Transfer last_nid on tail page
      mm: numa: Introduce last_nid to the page frame
      sched: numa: Slowly increase the scanning period as NUMA faults are handled
      mm: numa: Rate limit setting of pte_numa if node is saturated
      mm: numa: Rate limit the amount of memory that is migrated between nodes
      mm: numa: Structures for Migrate On Fault per NUMA migration rate limiting
      mm: numa: Migrate pages handled during a pmd_numa hinting fault
      mm: numa: Migrate on reference policy
      ...

commit 3105b86a9fee7d2c2e76edb53bbbc4027599628f
Author: Mel Gorman <mgorman@suse.de>
Date:   Fri Nov 23 11:23:49 2012 +0000

    mm: sched: numa: Control enabling and disabling of NUMA balancing if !SCHED_DEBUG
    
    The "mm: sched: numa: Control enabling and disabling of NUMA balancing"
    depends on scheduling debug being enabled but it's perfectly legimate to
    disable automatic NUMA balancing even without this option. This should
    take care of it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ae31c051ff2f..4f93d031875a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -650,9 +650,15 @@ extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 
 #ifdef CONFIG_NUMA_BALANCING
 #define sched_feat_numa(x) sched_feat(x)
+#ifdef CONFIG_SCHED_DEBUG
+#define numabalancing_enabled sched_feat_numa(NUMA)
+#else
+extern bool numabalancing_enabled;
+#endif /* CONFIG_SCHED_DEBUG */
 #else
 #define sched_feat_numa(x) (0)
-#endif
+#define numabalancing_enabled (0)
+#endif /* CONFIG_NUMA_BALANCING */
 
 static inline u64 global_rt_period(void)
 {

commit cbee9f88ec1b8dd6b58f25f54e4f52c82ed77690
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Oct 25 14:16:43 2012 +0200

    mm: numa: Add fault driven placement and migration
    
    NOTE: This patch is based on "sched, numa, mm: Add fault driven
            placement and migration policy" but as it throws away all the policy
            to just leave a basic foundation I had to drop the signed-offs-by.
    
    This patch creates a bare-bones method for setting PTEs pte_numa in the
    context of the scheduler that when faulted later will be faulted onto the
    node the CPU is running on.  In itself this does nothing useful but any
    placement policy will fundamentally depend on receiving hints on placement
    from fault context and doing something intelligent about it.
    
    Signed-off-by: Mel Gorman <mgorman@suse.de>
    Acked-by: Rik van Riel <riel@redhat.com>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7a7db09cfabc..ae31c051ff2f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -648,6 +648,12 @@ extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
 #endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
 
+#ifdef CONFIG_NUMA_BALANCING
+#define sched_feat_numa(x) sched_feat(x)
+#else
+#define sched_feat_numa(x) (0)
+#endif
+
 static inline u64 global_rt_period(void)
 {
 	return (u64)sysctl_sched_rt_period * NSEC_PER_USEC;

commit f4e26b120b9de84cb627bc7361ba43cfdc51341f
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:32 2012 +0200

    sched: Introduce temporary FAIR_GROUP_SCHED dependency for load-tracking
    
    While per-entity load-tracking is generally useful, beyond computing shares
    distribution, e.g. runnable based load-balance (in progress), governors,
    power-management, etc.
    
    These facilities are not yet consumers of this data.  This may be trivially
    reverted when the information is required; but avoid paying the overhead for
    calculations we will not use until then.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.422162369@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 0a75a430ca77..5eca173b563f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -225,6 +225,12 @@ struct cfs_rq {
 #endif
 
 #ifdef CONFIG_SMP
+/*
+ * Load-tracking only depends on SMP, FAIR_GROUP_SCHED dependency below may be
+ * removed when useful for applications beyond shares distribution (e.g.
+ * load-balance).
+ */
+#ifdef CONFIG_FAIR_GROUP_SCHED
 	/*
 	 * CFS Load tracking
 	 * Under CFS, load is tracked on a per-entity basis and aggregated up.
@@ -234,7 +240,8 @@ struct cfs_rq {
 	u64 runnable_load_avg, blocked_load_avg;
 	atomic64_t decay_counter, removed_load;
 	u64 last_decay;
-
+#endif /* CONFIG_FAIR_GROUP_SCHED */
+/* These always depend on CONFIG_FAIR_GROUP_SCHED */
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	u32 tg_runnable_contrib;
 	u64 tg_load_contrib;

commit 82958366cfea1a50e7e90907b2d55ae29ed69974
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Replace update_shares weight distribution with per-entity computation
    
    Now that the machinery in place is in place to compute contributed load in a
    bottom up fashion; replace the shares distribution code within update_shares()
    accordingly.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.061208672@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d13bce7a44ef..0a75a430ca77 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -234,11 +234,21 @@ struct cfs_rq {
 	u64 runnable_load_avg, blocked_load_avg;
 	atomic64_t decay_counter, removed_load;
 	u64 last_decay;
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	u32 tg_runnable_contrib;
 	u64 tg_load_contrib;
-#endif
-#endif
+#endif /* CONFIG_FAIR_GROUP_SCHED */
+
+	/*
+	 *   h_load = weight * f(tg)
+	 *
+	 * Where f(tg) is the recursive weight fraction assigned to
+	 * this group.
+	 */
+	unsigned long h_load;
+#endif /* CONFIG_SMP */
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */
 
@@ -254,28 +264,6 @@ struct cfs_rq {
 	struct list_head leaf_cfs_rq_list;
 	struct task_group *tg;	/* group that "owns" this runqueue */
 
-#ifdef CONFIG_SMP
-	/*
-	 *   h_load = weight * f(tg)
-	 *
-	 * Where f(tg) is the recursive weight fraction assigned to
-	 * this group.
-	 */
-	unsigned long h_load;
-
-	/*
-	 * Maintaining per-cpu shares distribution for group scheduling
-	 *
-	 * load_stamp is the last time we updated the load average
-	 * load_last is the last time we updated the load average and saw load
-	 * load_unacc_exec_time is currently unaccounted execution time
-	 */
-	u64 load_avg;
-	u64 load_period;
-	u64 load_stamp, load_last, load_unacc_exec_time;
-
-	unsigned long load_contribution;
-#endif /* CONFIG_SMP */
 #ifdef CONFIG_CFS_BANDWIDTH
 	int runtime_enabled;
 	u64 runtime_expires;

commit f1b17280efbd21873d1db8631117bdbccbcb39a2
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Maintain runnable averages across throttled periods
    
    With bandwidth control tracked entities may cease execution according to user
    specified bandwidth limits.  Charging this time as either throttled or blocked
    however, is incorrect and would falsely skew in either direction.
    
    What we actually want is for any throttled periods to be "invisible" to
    load-tracking as they are removed from the system for that interval and
    contribute normally otherwise.
    
    Do this by moderating the progression of time to omit any periods in which the
    entity belonged to a throttled hierarchy.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.998912151@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 134928dc6f05..d13bce7a44ef 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -281,7 +281,8 @@ struct cfs_rq {
 	u64 runtime_expires;
 	s64 runtime_remaining;
 
-	u64 throttled_timestamp;
+	u64 throttled_clock, throttled_clock_task;
+	u64 throttled_clock_task_time;
 	int throttled, throttle_count;
 	struct list_head throttled_list;
 #endif /* CONFIG_CFS_BANDWIDTH */

commit bb17f65571e97a7ec0297571fb1154fbd107ad00
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Normalize tg load contributions against runnable time
    
    Entities of equal weight should receive equitable distribution of cpu time.
    This is challenging in the case of a task_group's shares as execution may be
    occurring on multiple cpus simultaneously.
    
    To handle this we divide up the shares into weights proportionate with the load
    on each cfs_rq.  This does not however, account for the fact that the sum of
    the parts may be less than one cpu and so we need to normalize:
      load(tg) = min(runnable_avg(tg), 1) * tg->shares
    Where runnable_avg is the aggregate time in which the task_group had runnable
    children.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>.
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.930124292@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 924a99094888..134928dc6f05 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -113,6 +113,7 @@ struct task_group {
 
 	atomic_t load_weight;
 	atomic64_t load_avg;
+	atomic_t runnable_avg;
 #endif
 
 #ifdef CONFIG_RT_GROUP_SCHED
@@ -234,6 +235,7 @@ struct cfs_rq {
 	atomic64_t decay_counter, removed_load;
 	u64 last_decay;
 #ifdef CONFIG_FAIR_GROUP_SCHED
+	u32 tg_runnable_contrib;
 	u64 tg_load_contrib;
 #endif
 #endif

commit c566e8e9e44b72b53091da20e2dedefc730f2ee2
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Aggregate total task_group load
    
    Maintain a global running sum of the average load seen on each cfs_rq belonging
    to each task group so that it may be used in calculating an appropriate
    shares:weight distribution.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.792901086@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 30236ab4edc0..924a99094888 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -112,6 +112,7 @@ struct task_group {
 	unsigned long shares;
 
 	atomic_t load_weight;
+	atomic64_t load_avg;
 #endif
 
 #ifdef CONFIG_RT_GROUP_SCHED
@@ -232,6 +233,9 @@ struct cfs_rq {
 	u64 runnable_load_avg, blocked_load_avg;
 	atomic64_t decay_counter, removed_load;
 	u64 last_decay;
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	u64 tg_load_contrib;
+#endif
 #endif
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */

commit aff3e498844441fa71c5ee1bbc470e1dff9548d9
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Account for blocked load waking back up
    
    When a running entity blocks we migrate its tracked load to
    cfs_rq->blocked_runnable_avg.  In the sleep case this occurs while holding
    rq->lock and so is a natural transition.  Wake-ups however, are potentially
    asynchronous in the presence of migration and so special care must be taken.
    
    We use an atomic counter to track such migrated load, taking care to match this
    with the previously introduced decay counters so that we don't migrate too much
    load.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.726077467@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 664ff39195f7..30236ab4edc0 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -230,7 +230,7 @@ struct cfs_rq {
 	 * the FAIR_GROUP_SCHED case).
 	 */
 	u64 runnable_load_avg, blocked_load_avg;
-	atomic64_t decay_counter;
+	atomic64_t decay_counter, removed_load;
 	u64 last_decay;
 #endif
 #ifdef CONFIG_FAIR_GROUP_SCHED

commit 9ee474f55664ff63111c843099d365e7ecffb56f
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Maintain the load contribution of blocked entities
    
    We are currently maintaining:
    
      runnable_load(cfs_rq) = \Sum task_load(t)
    
    For all running children t of cfs_rq.  While this can be naturally updated for
    tasks in a runnable state (as they are scheduled); this does not account for
    the load contributed by blocked task entities.
    
    This can be solved by introducing a separate accounting for blocked load:
    
      blocked_load(cfs_rq) = \Sum runnable(b) * weight(b)
    
    Obviously we do not want to iterate over all blocked entities to account for
    their decay, we instead observe that:
    
      runnable_load(t) = \Sum p_i*y^i
    
    and that to account for an additional idle period we only need to compute:
    
      y*runnable_load(t).
    
    This means that we can compute all blocked entities at once by evaluating:
    
      blocked_load(cfs_rq)` = y * blocked_load(cfs_rq)
    
    Finally we maintain a decay counter so that when a sleeping entity re-awakens
    we can determine how much of its load should be removed from the blocked sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.585389902@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index e6539736af58..664ff39195f7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -229,7 +229,9 @@ struct cfs_rq {
 	 * This allows for the description of both thread and group usage (in
 	 * the FAIR_GROUP_SCHED case).
 	 */
-	u64 runnable_load_avg;
+	u64 runnable_load_avg, blocked_load_avg;
+	atomic64_t decay_counter;
+	u64 last_decay;
 #endif
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */

commit 2dac754e10a5d41d94d2d2365c0345d4f215a266
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Aggregate load contributed by task entities on parenting cfs_rq
    
    For a given task t, we can compute its contribution to load as:
    
      task_load(t) = runnable_avg(t) * weight(t)
    
    On a parenting cfs_rq we can then aggregate:
    
      runnable_load(cfs_rq) = \Sum task_load(t), for all runnable children t
    
    Maintain this bottom up, with task entities adding their contributed load to
    the parenting cfs_rq sum.  When a task entity's load changes we add the same
    delta to the maintained sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.514678907@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 14b571968713..e6539736af58 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -222,6 +222,15 @@ struct cfs_rq {
 	unsigned int nr_spread_over;
 #endif
 
+#ifdef CONFIG_SMP
+	/*
+	 * CFS Load tracking
+	 * Under CFS, load is tracked on a per-entity basis and aggregated up.
+	 * This allows for the description of both thread and group usage (in
+	 * the FAIR_GROUP_SCHED case).
+	 */
+	u64 runnable_load_avg;
+#endif
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */
 
@@ -1214,4 +1223,3 @@ static inline u64 irq_time_read(int cpu)
 }
 #endif /* CONFIG_64BIT */
 #endif /* CONFIG_IRQ_TIME_ACCOUNTING */
-

commit 18bf2805d9b30cb823d4919b42cd230f59c7ce1f
Author: Ben Segall <bsegall@google.com>
Date:   Thu Oct 4 12:51:20 2012 +0200

    sched: Maintain per-rq runnable averages
    
    Since runqueues do not have a corresponding sched_entity we instead embed a
    sched_avg structure directly.
    
    Signed-off-by: Ben Segall <bsegall@google.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.442637130@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7a7db09cfabc..14b571968713 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -467,6 +467,8 @@ struct rq {
 #ifdef CONFIG_SMP
 	struct llist_head wake_list;
 #endif
+
+	struct sched_avg avg;
 };
 
 static inline int cpu_of(struct rq *rq)

commit f3e947867478af9a12b9956bcd000ac7613a8a95
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Sep 12 11:22:00 2012 +0200

    sched: Remove __ARCH_WANT_INTERRUPTS_ON_CTXSW
    
    Now that the last architecture to use this has stopped doing so (ARM,
    thanks Catalin!) we can remove this complexity from the scheduler
    core.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Link: http://lkml.kernel.org/n/tip-g9p2a1w81xxbrze25v9zpzbf@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 09871698e80c..7a7db09cfabc 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -737,11 +737,7 @@ static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
 	 */
 	next->on_cpu = 1;
 #endif
-#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
-	raw_spin_unlock_irq(&rq->lock);
-#else
 	raw_spin_unlock(&rq->lock);
-#endif
 }
 
 static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
@@ -755,9 +751,7 @@ static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
 	smp_wmb();
 	prev->on_cpu = 0;
 #endif
-#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW
 	local_irq_enable();
-#endif
 }
 #endif /* __ARCH_WANT_UNLOCKED_CTXSW */
 

commit 59f979455d7209171ab10a72c8df5c2512976cb4
Merge: b9bb50db9126 9450d57eab5c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Sep 4 14:31:00 2012 +0200

    Merge branch 'sched/urgent' into sched/core
    
    Merge in the current fixes branch, we are going to apply dependent patches.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit a4c96ae319b8047f62dedbe1eac79e321c185749
Author: Peter Boonstoppel <pboonstoppel@nvidia.com>
Date:   Thu Aug 9 15:34:47 2012 -0700

    sched: Unthrottle rt runqueues in __disable_runtime()
    
    migrate_tasks() uses _pick_next_task_rt() to get tasks from the
    real-time runqueues to be migrated. When rt_rq is throttled
    _pick_next_task_rt() won't return anything, in which case
    migrate_tasks() can't move all threads over and gets stuck in an
    infinite loop.
    
    Instead unthrottle rt runqueues before migrating tasks.
    
    Additionally: move unthrottle_offline_cfs_rqs() to rq_offline_fair()
    
    Signed-off-by: Peter Boonstoppel <pboonstoppel@nvidia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/5FBF8E85CA34454794F0F7ECBA79798F379D3648B7@HQMAIL04.nvidia.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f6714d009e77..0848fa36c383 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1144,7 +1144,6 @@ extern void print_rt_stats(struct seq_file *m, int cpu);
 
 extern void init_cfs_rq(struct cfs_rq *cfs_rq);
 extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
-extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
 

commit 73fbec604432e1fbfeb1bc59a110dac1f98160f6
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Jun 16 15:57:37 2012 +0200

    sched: Move cputime code to its own file
    
    Extract cputime code from the giant sched/core.c and
    put it in its own file. This make it easier to deal with
    this particular area and de-bloat a bit more core.c
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Fenghua Yu <fenghua.yu@intel.com>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index f6714d009e77..804c2e5e7872 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -891,6 +891,9 @@ struct cpuacct {
 	struct kernel_cpustat __percpu *cpustat;
 };
 
+extern struct cgroup_subsys cpuacct_subsys;
+extern struct cpuacct root_cpuacct;
+
 /* return cpu accounting group corresponding to this container */
 static inline struct cpuacct *cgroup_ca(struct cgroup *cgrp)
 {
@@ -917,6 +920,16 @@ extern void cpuacct_charge(struct task_struct *tsk, u64 cputime);
 static inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}
 #endif
 
+#ifdef CONFIG_PARAVIRT
+static inline u64 steal_ticks(u64 steal)
+{
+	if (unlikely(steal > NSEC_PER_SEC))
+		return div_u64(steal, TICK_NSEC);
+
+	return __iter_div_u64_rem(steal, TICK_NSEC, &steal);
+}
+#endif
+
 static inline void inc_nr_running(struct rq *rq)
 {
 	rq->nr_running++;
@@ -1157,3 +1170,53 @@ enum rq_nohz_flag_bits {
 
 #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
 #endif
+
+#ifdef CONFIG_IRQ_TIME_ACCOUNTING
+
+DECLARE_PER_CPU(u64, cpu_hardirq_time);
+DECLARE_PER_CPU(u64, cpu_softirq_time);
+
+#ifndef CONFIG_64BIT
+DECLARE_PER_CPU(seqcount_t, irq_time_seq);
+
+static inline void irq_time_write_begin(void)
+{
+	__this_cpu_inc(irq_time_seq.sequence);
+	smp_wmb();
+}
+
+static inline void irq_time_write_end(void)
+{
+	smp_wmb();
+	__this_cpu_inc(irq_time_seq.sequence);
+}
+
+static inline u64 irq_time_read(int cpu)
+{
+	u64 irq_time;
+	unsigned seq;
+
+	do {
+		seq = read_seqcount_begin(&per_cpu(irq_time_seq, cpu));
+		irq_time = per_cpu(cpu_softirq_time, cpu) +
+			   per_cpu(cpu_hardirq_time, cpu);
+	} while (read_seqcount_retry(&per_cpu(irq_time_seq, cpu), seq));
+
+	return irq_time;
+}
+#else /* CONFIG_64BIT */
+static inline void irq_time_write_begin(void)
+{
+}
+
+static inline void irq_time_write_end(void)
+{
+}
+
+static inline u64 irq_time_read(int cpu)
+{
+	return per_cpu(cpu_softirq_time, cpu) + per_cpu(cpu_hardirq_time, cpu);
+}
+#endif /* CONFIG_64BIT */
+#endif /* CONFIG_IRQ_TIME_ACCOUNTING */
+

commit 35cf4e50b16331def6cfcbee11e49270b6db07f5
Author: Mike Galbraith <efault@gmx.de>
Date:   Tue Aug 7 05:00:13 2012 +0200

    sched,cgroup: Fix up task_groups list
    
    With multiple instances of task_groups, for_each_rt_rq() is a noop,
    no task groups having been added to the rt.c list instance.  This
    renders __enable/disable_runtime() and print_rt_stats() noop, the
    user (non) visible effect being that rt task groups are missing in
    /proc/sched_debug.
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    Cc: stable@kernel.org # v3.3+
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1344308413.6846.7.camel@marge.simpson.net
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 531411b1044e..f6714d009e77 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -80,7 +80,7 @@ extern struct mutex sched_domains_mutex;
 struct cfs_rq;
 struct rt_rq;
 
-static LIST_HEAD(task_groups);
+extern struct list_head task_groups;
 
 struct cfs_bandwidth {
 #ifdef CONFIG_CFS_BANDWIDTH

commit a35b6466aabb051568b844e8c63f87a356d3d129
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Aug 8 21:46:40 2012 +0200

    sched, cgroup: Reduce rq->lock hold times for large cgroup hierarchies
    
    Peter Portante reported that for large cgroup hierarchies (and or on
    large CPU counts) we get immense lock contention on rq->lock and stuff
    stops working properly.
    
    His workload was a ton of processes, each in their own cgroup,
    everybody idling except for a sporadic wakeup once every so often.
    
    It was found that:
    
      schedule()
        idle_balance()
          load_balance()
            local_irq_save()
            double_rq_lock()
            update_h_load()
              walk_tg_tree(tg_load_down)
                tg_load_down()
    
    Results in an entire cgroup hierarchy walk under rq->lock for every
    new-idle balance and since new-idle balance isn't throttled this
    results in a lot of work while holding the rq->lock.
    
    This patch does two things, it removes the work from under rq->lock
    based on the good principle of race and pray which is widely employed
    in the load-balancer as a whole. And secondly it throttles the
    update_h_load() calculation to max once per jiffy.
    
    I considered excluding update_h_load() for new-idle balance
    all-together, but purely relying on regular balance passes to update
    this data might not work out under some rare circumstances where the
    new-idle busiest isn't the regular busiest for a while (unlikely, but
    a nightmare to debug if someone hits it and suffers).
    
    Cc: pjt@google.com
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Reported-by: Peter Portante <pportant@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-aaarrzfpnaam7pqrekofu8a6@git.kernel.org
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c35a1a7dd4d6..531411b1044e 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -374,7 +374,11 @@ struct rq {
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	/* list of leaf cfs_rq on this cpu: */
 	struct list_head leaf_cfs_rq_list;
-#endif
+#ifdef CONFIG_SMP
+	unsigned long h_load_throttle;
+#endif /* CONFIG_SMP */
+#endif /* CONFIG_FAIR_GROUP_SCHED */
+
 #ifdef CONFIG_RT_GROUP_SCHED
 	struct list_head leaf_rt_rq_list;
 #endif

commit 8323f26ce3425460769605a6aece7a174edaa7d1
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Jun 22 13:36:05 2012 +0200

    sched: Fix race in task_group()
    
    Stefan reported a crash on a kernel before a3e5d1091c1 ("sched:
    Don't call task_group() too many times in set_task_rq()"), he
    found the reason to be that the multiple task_group()
    invocations in set_task_rq() returned different values.
    
    Looking at all that I found a lack of serialization and plain
    wrong comments.
    
    The below tries to fix it using an extra pointer which is
    updated under the appropriate scheduler locks. Its not pretty,
    but I can't really see another way given how all the cgroup
    stuff works.
    
    Reported-and-tested-by: Stefan Bader <stefan.bader@canonical.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1340364965.18025.71.camel@twins
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 55844f24435a..c35a1a7dd4d6 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -538,22 +538,19 @@ extern int group_balance_cpu(struct sched_group *sg);
 /*
  * Return the group to which this tasks belongs.
  *
- * We use task_subsys_state_check() and extend the RCU verification with
- * pi->lock and rq->lock because cpu_cgroup_attach() holds those locks for each
- * task it moves into the cgroup. Therefore by holding either of those locks,
- * we pin the task to the current cgroup.
+ * We cannot use task_subsys_state() and friends because the cgroup
+ * subsystem changes that value before the cgroup_subsys::attach() method
+ * is called, therefore we cannot pin it and might observe the wrong value.
+ *
+ * The same is true for autogroup's p->signal->autogroup->tg, the autogroup
+ * core changes this before calling sched_move_task().
+ *
+ * Instead we use a 'copy' which is updated from sched_move_task() while
+ * holding both task_struct::pi_lock and rq::lock.
  */
 static inline struct task_group *task_group(struct task_struct *p)
 {
-	struct task_group *tg;
-	struct cgroup_subsys_state *css;
-
-	css = task_subsys_state_check(p, cpu_cgroup_subsys_id,
-			lockdep_is_held(&p->pi_lock) ||
-			lockdep_is_held(&task_rq(p)->lock));
-	tg = container_of(css, struct task_group, css);
-
-	return autogroup_task_group(p, tg);
+	return p->sched_task_group;
 }
 
 /* Change a task's cfs_rq and parent entity if it moves across CPUs/groups */

commit 5167e8d5417bf5c322a703d2927daec727ea40dd
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri Jun 22 15:52:09 2012 +0200

    sched/nohz: Rewrite and fix load-avg computation -- again
    
    Thanks to Charles Wang for spotting the defects in the current code:
    
     - If we go idle during the sample window -- after sampling, we get a
       negative bias because we can negate our own sample.
    
     - If we wake up during the sample window we get a positive bias
       because we push the sample to a known active period.
    
    So rewrite the entire nohz load-avg muck once again, now adding
    copious documentation to the code.
    
    Reported-and-tested-by: Doug Smythies <dsmythies@telus.net>
    Reported-and-tested-by: Charles Wang <muming.wq@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: stable@kernel.org
    Link: http://lkml.kernel.org/r/1340373782.18025.74.camel@twins
    [ minor edits ]
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 6d52cea7f33d..55844f24435a 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -942,8 +942,6 @@ static inline u64 sched_avg_period(void)
 	return (u64)sysctl_sched_time_avg * NSEC_PER_MSEC / 2;
 }
 
-void calc_load_account_idle(struct rq *this_rq);
-
 #ifdef CONFIG_SCHED_HRTICK
 
 /*

commit c1174876874dcf8986806e4dad3d7d07af20b439
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu May 31 14:47:33 2012 +0200

    sched: Fix domain iteration
    
    Weird topologies can lead to asymmetric domain setups. This needs
    further consideration since these setups are typically non-minimal
    too.
    
    For now, make it work by adding an extra mask selecting which CPUs
    are allowed to iterate up.
    
    The topology that triggered it is the one from David Rientjes:
    
            10 20 20 30
            20 10 20 20
            20 20 10 20
            30 20 20 10
    
    resulting in boxes that wouldn't even boot.
    
    Reported-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-3p86l9cuaqnxz7uxsojmz5rm@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index ba9dccfd24ce..6d52cea7f33d 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -526,6 +526,8 @@ static inline struct sched_domain *highest_flag_domain(int cpu, int flag)
 DECLARE_PER_CPU(struct sched_domain *, sd_llc);
 DECLARE_PER_CPU(int, sd_llc_id);
 
+extern int group_balance_cpu(struct sched_group *sg);
+
 #endif /* CONFIG_SMP */
 
 #include "stats.h"

commit 556061b00c9f2fd6a5524b6bde823ef12f299ecf
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 11 17:31:26 2012 +0200

    sched/nohz: Fix rq->cpu_load[] calculations
    
    While investigating why the load-balancer did funny I found that the
    rq->cpu_load[] tables were completely screwy.. a bit more digging
    revealed that the updates that got through were missing ticks followed
    by a catchup of 2 ticks.
    
    The catchup assumes the cpu was idle during that time (since only nohz
    can cause missed ticks and the machine is idle etc..) this means that
    esp. the higher indices were significantly lower than they ought to
    be.
    
    The reason for this is that its not correct to compare against jiffies
    on every jiffy on any other cpu than the cpu that updates jiffies.
    
    This patch cludges around it by only doing the catch-up stuff from
    nohz_idle_balance() and doing the regular stuff unconditionally from
    the tick.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: pjt@google.com
    Cc: Venkatesh Pallipadi <venki@google.com>
    Link: http://lkml.kernel.org/n/tip-tp4kj18xdd5aj4vvj0qg55s2@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 7282e7b5f4c7..ba9dccfd24ce 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -876,7 +876,7 @@ extern void resched_cpu(int cpu);
 extern struct rt_bandwidth def_rt_bandwidth;
 extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
 
-extern void update_cpu_load(struct rq *this_rq);
+extern void update_idle_cpu_load(struct rq *this_rq);
 
 #ifdef CONFIG_CGROUP_CPUACCT
 #include <linux/cgroup.h>

commit c82513e513556a04f81aa511cd890acd23349c48
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Apr 26 13:12:27 2012 +0200

    sched: Change rq->nr_running to unsigned int
    
    Since there's a PID space limit of 30bits (see
    futex.h:FUTEX_TID_MASK) and allocating that many tasks (assuming a
    lower bound of 2 pages per task) would still take 8T of memory it
    seems reasonable to say that unsigned int is sufficient for
    rq->nr_running.
    
    When we do get anywhere near that amount of tasks I suspect other
    things would go funny, load-balancer load computations would really
    need to be hoisted to 128bit etc.
    
    So save a few bytes and convert rq->nr_running and friends to
    unsigned int.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-y3tvyszjdmbibade5bw8zl81@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index fb3acba4d52e..7282e7b5f4c7 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -201,7 +201,7 @@ struct cfs_bandwidth { };
 /* CFS-related fields in a runqueue */
 struct cfs_rq {
 	struct load_weight load;
-	unsigned long nr_running, h_nr_running;
+	unsigned int nr_running, h_nr_running;
 
 	u64 exec_clock;
 	u64 min_vruntime;
@@ -279,7 +279,7 @@ static inline int rt_bandwidth_enabled(void)
 /* Real-Time classes' related field in a runqueue: */
 struct rt_rq {
 	struct rt_prio_array active;
-	unsigned long rt_nr_running;
+	unsigned int rt_nr_running;
 #if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
 	struct {
 		int curr; /* highest queued rt task prio */
@@ -353,7 +353,7 @@ struct rq {
 	 * nr_running and cpu_load should be in the same cacheline because
 	 * remote CPUs use both these fields when doing load calculation.
 	 */
-	unsigned long nr_running;
+	unsigned int nr_running;
 	#define CPU_LOAD_IDX_MAX 5
 	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
 	unsigned long last_load_update_tick;

commit f22e08a79f3765fecf060b225a46931c94fb0a92
Merge: f187e9fd6857 e3831edd59ed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 31 13:35:31 2012 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar.
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched: Fix incorrect usage of for_each_cpu_mask() in select_fallback_rq()
      sched: Fix __schedule_bug() output when called from an interrupt
      sched/arch: Introduce the finish_arch_post_lock_switch() scheduler callback

commit 1f56ee7b68fecd45d25bdcf2eda7507797594424
Merge: 6135fc1eb4b1 01f23e1630d9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Mar 29 12:48:15 2012 +0200

    Merge branch 'sched/arch' into sched/urgent
    
    Merge reason: It has not gone upstream via the ARM tree, merge it via
                  the scheduler tree.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2ba68940c893c8f0bfc8573c041254251bb6aeab
Merge: 9c2b957db177 600e14588280
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Mar 20 10:31:44 2012 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler changes for v3.4 from Ingo Molnar
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      printk: Make it compile with !CONFIG_PRINTK
      sched/x86: Fix overflow in cyc2ns_offset
      sched: Fix nohz load accounting -- again!
      sched: Update yield() docs
      printk/sched: Introduce special printk_sched() for those awkward moments
      sched/nohz: Correctly initialize 'next_balance' in 'nohz' idle balancer
      sched: Cleanup cpu_active madness
      sched: Fix load-balance wreckage
      sched: Clean up parameter passing of proc_sched_autogroup_set_nice()
      sched: Ditch per cgroup task lists for load-balancing
      sched: Rename load-balancing fields
      sched: Move load-balancing arguments into helper struct
      sched/rt: Do not submit new work when PI-blocked
      sched/rt: Prevent idle task boosting
      sched/wait: Add __wake_up_all_locked() API
      sched/rt: Document scheduler related skip-resched-check sites
      sched/rt: Use schedule_preempt_disabled()
      sched/rt: Add schedule_preempt_disabled()
      sched/rt: Do not throttle when PI boosting
      sched/rt: Keep period timer ticking when rt throttling is active
      ...

commit 01f23e1630d944f7085cd8fd5793e31ea91c03d8
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Sun Nov 27 21:43:10 2011 +0000

    sched/arch: Introduce the finish_arch_post_lock_switch() scheduler callback
    
    This callback is called by the scheduler after rq->lock has been released
    and interrupts enabled. It will be used in subsequent patches on the ARM
    architecture.
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Reviewed-by: Will Deacon <will.deacon@arm.com>
    Reviewed-by: Frank Rowand <frank.rowand@am.sony.com>
    Tested-by: Will Deacon <will.deacon@arm.com>
    Tested-by: Marc Zyngier <Marc.Zyngier@arm.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/n/20120313110840.7b444deb6b1bb902c15f3cdf@canb.auug.org.au
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 98c0c2623db8..d72483d07c9f 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -692,6 +692,9 @@ static inline int task_running(struct rq *rq, struct task_struct *p)
 #ifndef finish_arch_switch
 # define finish_arch_switch(prev)	do { } while (0)
 #endif
+#ifndef finish_arch_post_lock_switch
+# define finish_arch_post_lock_switch()	do { } while (0)
+#endif
 
 #ifndef __ARCH_WANT_UNLOCKED_CTXSW
 static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)

commit 367456c756a6b84f493ca9cc5b17b1f5d38ef466
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon Feb 20 21:49:09 2012 +0100

    sched: Ditch per cgroup task lists for load-balancing
    
    Per cgroup load-balance has numerous problems, chief amongst them that
    there is no real sane order in them. So stop pretending it makes sense
    and enqueue all tasks on a single list.
    
    This also allows us to more easily fix the fwd progress issue
    uncovered by the lock-break stuff. Rotate the list on failure to
    migreate and limit the total iterations to nr_running (which with
    releasing the lock isn't strictly accurate but close enough).
    
    Also add a filter that skips very light tasks on the first attempt
    around the list, this attempts to avoid shooting whole cgroups around
    without affecting over balance.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: pjt@google.com
    Link: http://lkml.kernel.org/n/tip-tx8yqydc7eimgq7i4rkc3a4g@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c0660a1a0cd1..753bdd567416 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -212,9 +212,6 @@ struct cfs_rq {
 	struct rb_root tasks_timeline;
 	struct rb_node *rb_leftmost;
 
-	struct list_head tasks;
-	struct list_head *balance_iterator;
-
 	/*
 	 * 'curr' points to currently running entity on this cfs_rq.
 	 * It is set to NULL otherwise (i.e when none are currently running).
@@ -241,11 +238,6 @@ struct cfs_rq {
 	struct task_group *tg;	/* group that "owns" this runqueue */
 
 #ifdef CONFIG_SMP
-	/*
-	 * the part of load.weight contributed by tasks
-	 */
-	unsigned long task_weight;
-
 	/*
 	 *   h_load = weight * f(tg)
 	 *
@@ -420,6 +412,8 @@ struct rq {
 	int cpu;
 	int online;
 
+	struct list_head cfs_tasks;
+
 	u64 rt_avg;
 	u64 age_stamp;
 	u64 idle_stamp;

commit c5905afb0ee6550b42c49213da1c22d67316c194
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Feb 24 08:31:31 2012 +0100

    static keys: Introduce 'struct static_key', static_key_true()/false() and static_key_slow_[inc|dec]()
    
    So here's a boot tested patch on top of Jason's series that does
    all the cleanups I talked about and turns jump labels into a
    more intuitive to use facility. It should also address the
    various misconceptions and confusions that surround jump labels.
    
    Typical usage scenarios:
    
            #include <linux/static_key.h>
    
            struct static_key key = STATIC_KEY_INIT_TRUE;
    
            if (static_key_false(&key))
                    do unlikely code
            else
                    do likely code
    
    Or:
    
            if (static_key_true(&key))
                    do likely code
            else
                    do unlikely code
    
    The static key is modified via:
    
            static_key_slow_inc(&key);
            ...
            static_key_slow_dec(&key);
    
    The 'slow' prefix makes it abundantly clear that this is an
    expensive operation.
    
    I've updated all in-kernel code to use this everywhere. Note
    that I (intentionally) have not pushed through the rename
    blindly through to the lowest levels: the actual jump-label
    patching arch facility should be named like that, so we want to
    decouple jump labels from the static-key facility a bit.
    
    On non-jump-label enabled architectures static keys default to
    likely()/unlikely() branches.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Jason Baron <jbaron@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: a.p.zijlstra@chello.nl
    Cc: mathieu.desnoyers@efficios.com
    Cc: davem@davemloft.net
    Cc: ddaney.cavm@gmail.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20120222085809.GA26397@elte.hu
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 98c0c2623db8..b4cd6d8ea150 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -611,7 +611,7 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
  * Tunables that become constants when CONFIG_SCHED_DEBUG is off:
  */
 #ifdef CONFIG_SCHED_DEBUG
-# include <linux/jump_label.h>
+# include <linux/static_key.h>
 # define const_debug __read_mostly
 #else
 # define const_debug const
@@ -630,18 +630,18 @@ enum {
 #undef SCHED_FEAT
 
 #if defined(CONFIG_SCHED_DEBUG) && defined(HAVE_JUMP_LABEL)
-static __always_inline bool static_branch__true(struct jump_label_key *key)
+static __always_inline bool static_branch__true(struct static_key *key)
 {
-	return likely(static_branch(key)); /* Not out of line branch. */
+	return static_key_true(key); /* Not out of line branch. */
 }
 
-static __always_inline bool static_branch__false(struct jump_label_key *key)
+static __always_inline bool static_branch__false(struct static_key *key)
 {
-	return unlikely(static_branch(key)); /* Out of line branch. */
+	return static_key_false(key); /* Out of line branch. */
 }
 
 #define SCHED_FEAT(name, enabled)					\
-static __always_inline bool static_branch_##name(struct jump_label_key *key) \
+static __always_inline bool static_branch_##name(struct static_key *key) \
 {									\
 	return static_branch__##enabled(key);				\
 }
@@ -650,7 +650,7 @@ static __always_inline bool static_branch_##name(struct jump_label_key *key) \
 
 #undef SCHED_FEAT
 
-extern struct jump_label_key sched_feat_keys[__SCHED_FEAT_NR];
+extern struct static_key sched_feat_keys[__SCHED_FEAT_NR];
 #define sched_feat(x) (static_branch_##x(&sched_feat_keys[__SCHED_FEAT_##x]))
 #else /* !(SCHED_DEBUG && HAVE_JUMP_LABEL) */
 #define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))

commit de5bdff7a72acc281219be2b8edeeca1fd81c542
Author: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
Date:   Thu Feb 16 14:52:21 2012 +0900

    sched: Make initial SCHED_RR timeslace DEF_TIMESLICE
    
    Current the initial SCHED_RR timeslice of init_task is HZ, which means
    1s, and is not same as the default SCHED_RR timeslice DEF_TIMESLICE.
    
    Change that initial timeslice to the DEF_TIMESLICE.
    
    Signed-off-by: Hiroshi Shimamoto <h-shimamoto@ct.jp.nec.com>
    [ s/DEF_TIMESLICE/RR_TIMESLICE/g ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/4F3C9995.3010800@ct.jp.nec.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 8a2c768d2f98..c0660a1a0cd1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -36,11 +36,7 @@ extern __read_mostly int scheduler_running;
 
 /*
  * These are the 'tuning knobs' of the scheduler:
- *
- * default timeslice is 100 msecs (used only for SCHED_RR tasks).
- * Timeslices get refilled after they expire.
  */
-#define DEF_TIMESLICE		(100 * HZ / 1000)
 
 /*
  * single value that denotes runtime == period, ie unlimited time.

commit 30fd049afcfed50e022704036e8629d6bdfe84e6
Author: Rakib Mullick <rakib.mullick@gmail.com>
Date:   Tue Jan 24 22:33:56 2012 +0600

    sched: Remove sched_switch
    
    Currently we don't utilize the sched_switch field anymore.
    
    But, simply removing sched_switch field from the middle of the
    sched_stat output will break tools.
    
    So, to stay compatible we hardcode it to zero and remove the
    field from the scheduler data structures.
    
    Update the schedstat documentation accordingly.
    
    Signed-off-by: Rakib Mullick <rakib.mullick@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1327422836.27181.5.camel@localhost.localdomain
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 98c0c2623db8..8a2c768d2f98 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -462,7 +462,6 @@ struct rq {
 	unsigned int yld_count;
 
 	/* schedule() stats */
-	unsigned int sched_switch;
 	unsigned int sched_count;
 	unsigned int sched_goidle;
 

commit 518cd62341786aa4e3839810832af2fbc0de1ea4
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Dec 7 15:07:31 2011 +0100

    sched: Only queue remote wakeups when crossing cache boundaries
    
    Mike reported a 13% drop in netperf TCP_RR performance due to the
    new remote wakeup code. Suresh too noticed some performance issues
    with it.
    
    Reducing the IPIs to only cross cache domains solves the observed
    performance issues.
    
    Reported-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Reported-by: Mike Galbraith <efault@gmx.de>
    Acked-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Acked-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Chris Mason <chris.mason@oracle.com>
    Cc: Dave Kleikamp <dave.kleikamp@oracle.com>
    Link: http://lkml.kernel.org/r/1323338531.17673.7.camel@twins
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d8d3613a4055..98c0c2623db8 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -487,6 +487,14 @@ static inline int cpu_of(struct rq *rq)
 
 DECLARE_PER_CPU(struct rq, runqueues);
 
+#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
+#define this_rq()		(&__get_cpu_var(runqueues))
+#define task_rq(p)		cpu_rq(task_cpu(p))
+#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
+#define raw_rq()		(&__raw_get_cpu_var(runqueues))
+
+#ifdef CONFIG_SMP
+
 #define rcu_dereference_check_sched_domain(p) \
 	rcu_dereference_check((p), \
 			      lockdep_is_held(&sched_domains_mutex))
@@ -499,15 +507,37 @@ DECLARE_PER_CPU(struct rq, runqueues);
  * preempt-disabled sections.
  */
 #define for_each_domain(cpu, __sd) \
-	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)
+	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); \
+			__sd; __sd = __sd->parent)
 
 #define for_each_lower_domain(sd) for (; sd; sd = sd->child)
 
-#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
-#define this_rq()		(&__get_cpu_var(runqueues))
-#define task_rq(p)		cpu_rq(task_cpu(p))
-#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
-#define raw_rq()		(&__raw_get_cpu_var(runqueues))
+/**
+ * highest_flag_domain - Return highest sched_domain containing flag.
+ * @cpu:	The cpu whose highest level of sched domain is to
+ *		be returned.
+ * @flag:	The flag to check for the highest sched_domain
+ *		for the given cpu.
+ *
+ * Returns the highest sched_domain of a cpu which contains the given flag.
+ */
+static inline struct sched_domain *highest_flag_domain(int cpu, int flag)
+{
+	struct sched_domain *sd, *hsd = NULL;
+
+	for_each_domain(cpu, sd) {
+		if (!(sd->flags & flag))
+			break;
+		hsd = sd;
+	}
+
+	return hsd;
+}
+
+DECLARE_PER_CPU(struct sched_domain *, sd_llc);
+DECLARE_PER_CPU(int, sd_llc_id);
+
+#endif /* CONFIG_SMP */
 
 #include "stats.h"
 #include "auto_group.h"

commit f8b6d1cc7dc15cf3de538b864eefaedad7a84d85
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jul 6 14:20:14 2011 +0200

    sched: Use jump_labels for sched_feat
    
    Now that we initialize jump_labels before sched_init() we can use them
    for the debug features without having to worry about a window where
    they have the wrong setting.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-vpreo4hal9e0kzqmg5y0io2k@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c24801636219..d8d3613a4055 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -581,6 +581,7 @@ static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
  * Tunables that become constants when CONFIG_SCHED_DEBUG is off:
  */
 #ifdef CONFIG_SCHED_DEBUG
+# include <linux/jump_label.h>
 # define const_debug __read_mostly
 #else
 # define const_debug const
@@ -593,11 +594,37 @@ extern const_debug unsigned int sysctl_sched_features;
 
 enum {
 #include "features.h"
+	__SCHED_FEAT_NR,
 };
 
 #undef SCHED_FEAT
 
+#if defined(CONFIG_SCHED_DEBUG) && defined(HAVE_JUMP_LABEL)
+static __always_inline bool static_branch__true(struct jump_label_key *key)
+{
+	return likely(static_branch(key)); /* Not out of line branch. */
+}
+
+static __always_inline bool static_branch__false(struct jump_label_key *key)
+{
+	return unlikely(static_branch(key)); /* Out of line branch. */
+}
+
+#define SCHED_FEAT(name, enabled)					\
+static __always_inline bool static_branch_##name(struct jump_label_key *key) \
+{									\
+	return static_branch__##enabled(key);				\
+}
+
+#include "features.h"
+
+#undef SCHED_FEAT
+
+extern struct jump_label_key sched_feat_keys[__SCHED_FEAT_NR];
+#define sched_feat(x) (static_branch_##x(&sched_feat_keys[__SCHED_FEAT_##x]))
+#else /* !(SCHED_DEBUG && HAVE_JUMP_LABEL) */
 #define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
+#endif /* SCHED_DEBUG && HAVE_JUMP_LABEL */
 
 static inline u64 global_rt_period(void)
 {

commit 54c707e98de9ca899e6552a47c797c62c45885ee
Author: Glauber Costa <glommer@parallels.com>
Date:   Mon Nov 28 14:45:19 2011 -0200

    sched/accounting: Re-use scheduler statistics for the root cgroup
    
    Right now, after we collect tick statistics for user and system and store them
    in a well known location, we keep the same statistics again for cpuacct.
    Since cpuacct is hierarchical, the numbers for the root cgroup should be
    absolutely equal to the system-wide numbers.
    
    So it would be better to just use it: this patch changes cpuacct accounting
    in a way that the cpustat statistics are kept in a struct kernel_cpustat percpu
    array. In the root cgroup case, we just point it to the main array. The rest of
    the hierarchy walk can be totally disabled later with a static branch - but I am
    not doing it here.
    
    Signed-off-by: Glauber Costa <glommer@parallels.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Tuner <pjt@google.com>
    Link: http://lkml.kernel.org/r/1322498719-2255-4-git-send-email-glommer@parallels.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index d88545c667e3..c24801636219 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -830,13 +830,39 @@ extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime
 extern void update_cpu_load(struct rq *this_rq);
 
 #ifdef CONFIG_CGROUP_CPUACCT
+#include <linux/cgroup.h>
+/* track cpu usage of a group of tasks and its child groups */
+struct cpuacct {
+	struct cgroup_subsys_state css;
+	/* cpuusage holds pointer to a u64-type object on every cpu */
+	u64 __percpu *cpuusage;
+	struct kernel_cpustat __percpu *cpustat;
+};
+
+/* return cpu accounting group corresponding to this container */
+static inline struct cpuacct *cgroup_ca(struct cgroup *cgrp)
+{
+	return container_of(cgroup_subsys_state(cgrp, cpuacct_subsys_id),
+			    struct cpuacct, css);
+}
+
+/* return cpu accounting group to which this task belongs */
+static inline struct cpuacct *task_ca(struct task_struct *tsk)
+{
+	return container_of(task_subsys_state(tsk, cpuacct_subsys_id),
+			    struct cpuacct, css);
+}
+
+static inline struct cpuacct *parent_ca(struct cpuacct *ca)
+{
+	if (!ca || !ca->css.cgroup->parent)
+		return NULL;
+	return cgroup_ca(ca->css.cgroup->parent);
+}
+
 extern void cpuacct_charge(struct task_struct *tsk, u64 cputime);
-extern void cpuacct_update_stats(struct task_struct *tsk,
-		enum cpuacct_stat_index idx, cputime_t val);
 #else
 static inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}
-static inline void cpuacct_update_stats(struct task_struct *tsk,
-		enum cpuacct_stat_index idx, cputime_t val) {}
 #endif
 
 static inline void inc_nr_running(struct rq *rq)

commit b39e66eaf9c573f38133e894256caeaf9fd2a528
Author: Mike Galbraith <mgalbraith@suse.de>
Date:   Tue Nov 22 15:20:07 2011 +0100

    sched: Save some hrtick_start_fair cycles
    
    hrtick_start_fair() shows up in profiles even when disabled.
    
    v3.0.6
    
    taskset -c 3 pipe-test
    
       PerfTop:     997 irqs/sec  kernel:89.5%  exact:  0.0% [1000Hz cycles],  (all, CPU: 3)
    ------------------------------------------------------------------------------------------------
    
                 Virgin                                    Patched
                 samples  pcnt function                    samples  pcnt function
                 _______ _____ ___________________________ _______ _____ ___________________________
    
                 2880.00 10.2% __schedule                  3136.00 11.3% __schedule
                 1634.00  5.8% pipe_read                   1615.00  5.8% pipe_read
                 1458.00  5.2% system_call                 1534.00  5.5% system_call
                 1382.00  4.9% _raw_spin_lock_irqsave      1412.00  5.1% _raw_spin_lock_irqsave
                 1202.00  4.3% pipe_write                  1255.00  4.5% copy_user_generic_string
                 1164.00  4.1% copy_user_generic_string    1241.00  4.5% __switch_to
                 1097.00  3.9% __switch_to                  929.00  3.3% mutex_lock
                  872.00  3.1% mutex_lock                   846.00  3.0% mutex_unlock
                  687.00  2.4% mutex_unlock                 804.00  2.9% pipe_write
                  682.00  2.4% native_sched_clock           713.00  2.6% native_sched_clock
                  643.00  2.3% system_call_after_swapgs     653.00  2.3% _raw_spin_unlock_irqrestore
                  617.00  2.2% sched_clock_local            633.00  2.3% fsnotify
                  612.00  2.2% fsnotify                     605.00  2.2% sched_clock_local
                  596.00  2.1% _raw_spin_unlock_irqrestore  593.00  2.1% system_call_after_swapgs
                  542.00  1.9% sysret_check                 559.00  2.0% sysret_check
                  467.00  1.7% fget_light                   472.00  1.7% fget_light
                  462.00  1.6% finish_task_switch           461.00  1.7% finish_task_switch
                  437.00  1.5% vfs_write                    442.00  1.6% vfs_write
                  431.00  1.5% do_sync_write                428.00  1.5% do_sync_write
                  413.00  1.5% select_task_rq_fair          404.00  1.5% _raw_spin_lock_irq
                  386.00  1.4% update_curr                  402.00  1.4% update_curr
                  385.00  1.4% rw_verify_area               389.00  1.4% do_sync_read
                  377.00  1.3% _raw_spin_lock_irq           378.00  1.4% vfs_read
                  369.00  1.3% do_sync_read                 340.00  1.2% pipe_iov_copy_from_user
                  360.00  1.3% vfs_read                     316.00  1.1% __wake_up_sync_key
    *             342.00  1.2% hrtick_start_fair            313.00  1.1% __wake_up_common
    
    Signed-off-by: Mike Galbraith <efault@gmx.de>
    [ fixed !CONFIG_SCHED_HRTICK borkage ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1321971607.6855.17.camel@marge.simson.net
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 91810f0ee3af..d88545c667e3 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -885,6 +885,13 @@ static inline int hrtick_enabled(struct rq *rq)
 
 void hrtick_start(struct rq *rq, u64 delay);
 
+#else
+
+static inline int hrtick_enabled(struct rq *rq)
+{
+	return 0;
+}
+
 #endif /* CONFIG_SCHED_HRTICK */
 
 #ifdef CONFIG_SMP

commit 69e1e811dcc436a6b129dbef273ad9ec22d095ce
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Dec 1 17:07:33 2011 -0800

    sched, nohz: Track nr_busy_cpus in the sched_group_power
    
    Introduce nr_busy_cpus in the struct sched_group_power [Not in sched_group
    because sched groups are duplicated for the SD_OVERLAP scheduler domain]
    and for each cpu that enters and exits idle, this parameter will
    be updated in each scheduler group of the scheduler domain that this cpu
    belongs to.
    
    To avoid the frequent update of this state as the cpu enters
    and exits idle, the update of the stat during idle exit is
    delayed to the first timer tick that happens after the cpu becomes busy.
    This is done using NOHZ_IDLE flag in the struct rq's nohz_flags.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20111202010832.555984323@sbsiddha-desk.sc.intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index cf7d02662bc2..91810f0ee3af 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -1069,6 +1069,7 @@ extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
 enum rq_nohz_flag_bits {
 	NOHZ_TICK_STOPPED,
 	NOHZ_BALANCE_KICK,
+	NOHZ_IDLE,
 };
 
 #define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)

commit 1c792db7f7957e2e34b9a164f08200e36a25dfd0
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Dec 1 17:07:32 2011 -0800

    sched, nohz: Introduce nohz_flags in 'struct rq'
    
    Introduce nohz_flags in the struct rq, which will track these two flags
    for now.
    
    NOHZ_TICK_STOPPED keeps track of the tick stopped status that gets set when
    the tick is stopped. It will be used to update the nohz idle load balancer data
    structures during the first busy tick after the tick is restarted. At this
    first busy tick after tickless idle, NOHZ_TICK_STOPPED flag will be reset.
    This will minimize the nohz idle load balancer status updates that currently
    happen for every tickless exit, making it more scalable when there
    are many logical cpu's that enter and exit idle often.
    
    NOHZ_BALANCE_KICK will track the need for nohz idle load balance
    on this rq. This will replace the nohz_balance_kick in the rq, which was
    not being updated atomically.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20111202010832.499438999@sbsiddha-desk.sc.intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index 8715055979d1..cf7d02662bc2 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -371,7 +371,7 @@ struct rq {
 	unsigned long last_load_update_tick;
 #ifdef CONFIG_NO_HZ
 	u64 nohz_stamp;
-	unsigned char nohz_balance_kick;
+	unsigned long nohz_flags;
 #endif
 	int skip_clock_update;
 
@@ -1064,3 +1064,12 @@ extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
 extern void unthrottle_offline_cfs_rqs(struct rq *rq);
 
 extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
+
+#ifdef CONFIG_NO_HZ
+enum rq_nohz_flag_bits {
+	NOHZ_TICK_STOPPED,
+	NOHZ_BALANCE_KICK,
+};
+
+#define nohz_flags(cpu)	(&cpu_rq(cpu)->nohz_flags)
+#endif

commit 77e81365e0b7d7479fc444a21cea0cd4def70b45
Author: Suresh Siddha <suresh.b.siddha@intel.com>
Date:   Thu Nov 17 11:08:23 2011 -0800

    sched: Clean up domain traversal in select_idle_sibling()
    
    Instead of going through the scheduler domain hierarchy multiple times
    (for giving priority to an idle core over an idle SMT sibling in a busy
    core), start with the highest scheduler domain with the SD_SHARE_PKG_RESOURCES
    flag and traverse the domain hierarchy down till we find an idle group.
    
    This cleanup also addresses an issue reported by Mike where the recent
    changes returned the busy thread even in the presence of an idle SMT
    sibling in single socket platforms.
    
    Signed-off-by: Suresh Siddha <suresh.b.siddha@intel.com>
    Tested-by: Mike Galbraith <efault@gmx.de>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1321556904.15339.25.camel@sbsiddha-desk.sc.intel.com
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
index c2e780234c31..8715055979d1 100644
--- a/kernel/sched/sched.h
+++ b/kernel/sched/sched.h
@@ -501,6 +501,8 @@ DECLARE_PER_CPU(struct rq, runqueues);
 #define for_each_domain(cpu, __sd) \
 	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)
 
+#define for_each_lower_domain(sd) for (; sd; sd = sd->child)
+
 #define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
 #define this_rq()		(&__get_cpu_var(runqueues))
 #define task_rq(p)		cpu_rq(task_cpu(p))

commit 391e43da797a96aeb65410281891f6d0b0e9611c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Nov 15 17:14:39 2011 +0100

    sched: Move all scheduler bits into kernel/sched/
    
    There's too many sched*.[ch] files in kernel/, give them their own
    directory.
    
    (No code changed, other than Makefile glue added.)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h
new file mode 100644
index 000000000000..c2e780234c31
--- /dev/null
+++ b/kernel/sched/sched.h
@@ -0,0 +1,1064 @@
+
+#include <linux/sched.h>
+#include <linux/mutex.h>
+#include <linux/spinlock.h>
+#include <linux/stop_machine.h>
+
+#include "cpupri.h"
+
+extern __read_mostly int scheduler_running;
+
+/*
+ * Convert user-nice values [ -20 ... 0 ... 19 ]
+ * to static priority [ MAX_RT_PRIO..MAX_PRIO-1 ],
+ * and back.
+ */
+#define NICE_TO_PRIO(nice)	(MAX_RT_PRIO + (nice) + 20)
+#define PRIO_TO_NICE(prio)	((prio) - MAX_RT_PRIO - 20)
+#define TASK_NICE(p)		PRIO_TO_NICE((p)->static_prio)
+
+/*
+ * 'User priority' is the nice value converted to something we
+ * can work with better when scaling various scheduler parameters,
+ * it's a [ 0 ... 39 ] range.
+ */
+#define USER_PRIO(p)		((p)-MAX_RT_PRIO)
+#define TASK_USER_PRIO(p)	USER_PRIO((p)->static_prio)
+#define MAX_USER_PRIO		(USER_PRIO(MAX_PRIO))
+
+/*
+ * Helpers for converting nanosecond timing to jiffy resolution
+ */
+#define NS_TO_JIFFIES(TIME)	((unsigned long)(TIME) / (NSEC_PER_SEC / HZ))
+
+#define NICE_0_LOAD		SCHED_LOAD_SCALE
+#define NICE_0_SHIFT		SCHED_LOAD_SHIFT
+
+/*
+ * These are the 'tuning knobs' of the scheduler:
+ *
+ * default timeslice is 100 msecs (used only for SCHED_RR tasks).
+ * Timeslices get refilled after they expire.
+ */
+#define DEF_TIMESLICE		(100 * HZ / 1000)
+
+/*
+ * single value that denotes runtime == period, ie unlimited time.
+ */
+#define RUNTIME_INF	((u64)~0ULL)
+
+static inline int rt_policy(int policy)
+{
+	if (policy == SCHED_FIFO || policy == SCHED_RR)
+		return 1;
+	return 0;
+}
+
+static inline int task_has_rt_policy(struct task_struct *p)
+{
+	return rt_policy(p->policy);
+}
+
+/*
+ * This is the priority-queue data structure of the RT scheduling class:
+ */
+struct rt_prio_array {
+	DECLARE_BITMAP(bitmap, MAX_RT_PRIO+1); /* include 1 bit for delimiter */
+	struct list_head queue[MAX_RT_PRIO];
+};
+
+struct rt_bandwidth {
+	/* nests inside the rq lock: */
+	raw_spinlock_t		rt_runtime_lock;
+	ktime_t			rt_period;
+	u64			rt_runtime;
+	struct hrtimer		rt_period_timer;
+};
+
+extern struct mutex sched_domains_mutex;
+
+#ifdef CONFIG_CGROUP_SCHED
+
+#include <linux/cgroup.h>
+
+struct cfs_rq;
+struct rt_rq;
+
+static LIST_HEAD(task_groups);
+
+struct cfs_bandwidth {
+#ifdef CONFIG_CFS_BANDWIDTH
+	raw_spinlock_t lock;
+	ktime_t period;
+	u64 quota, runtime;
+	s64 hierarchal_quota;
+	u64 runtime_expires;
+
+	int idle, timer_active;
+	struct hrtimer period_timer, slack_timer;
+	struct list_head throttled_cfs_rq;
+
+	/* statistics */
+	int nr_periods, nr_throttled;
+	u64 throttled_time;
+#endif
+};
+
+/* task group related information */
+struct task_group {
+	struct cgroup_subsys_state css;
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	/* schedulable entities of this group on each cpu */
+	struct sched_entity **se;
+	/* runqueue "owned" by this group on each cpu */
+	struct cfs_rq **cfs_rq;
+	unsigned long shares;
+
+	atomic_t load_weight;
+#endif
+
+#ifdef CONFIG_RT_GROUP_SCHED
+	struct sched_rt_entity **rt_se;
+	struct rt_rq **rt_rq;
+
+	struct rt_bandwidth rt_bandwidth;
+#endif
+
+	struct rcu_head rcu;
+	struct list_head list;
+
+	struct task_group *parent;
+	struct list_head siblings;
+	struct list_head children;
+
+#ifdef CONFIG_SCHED_AUTOGROUP
+	struct autogroup *autogroup;
+#endif
+
+	struct cfs_bandwidth cfs_bandwidth;
+};
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+#define ROOT_TASK_GROUP_LOAD	NICE_0_LOAD
+
+/*
+ * A weight of 0 or 1 can cause arithmetics problems.
+ * A weight of a cfs_rq is the sum of weights of which entities
+ * are queued on this cfs_rq, so a weight of a entity should not be
+ * too large, so as the shares value of a task group.
+ * (The default weight is 1024 - so there's no practical
+ *  limitation from this.)
+ */
+#define MIN_SHARES	(1UL <<  1)
+#define MAX_SHARES	(1UL << 18)
+#endif
+
+/* Default task group.
+ *	Every task in system belong to this group at bootup.
+ */
+extern struct task_group root_task_group;
+
+typedef int (*tg_visitor)(struct task_group *, void *);
+
+extern int walk_tg_tree_from(struct task_group *from,
+			     tg_visitor down, tg_visitor up, void *data);
+
+/*
+ * Iterate the full tree, calling @down when first entering a node and @up when
+ * leaving it for the final time.
+ *
+ * Caller must hold rcu_lock or sufficient equivalent.
+ */
+static inline int walk_tg_tree(tg_visitor down, tg_visitor up, void *data)
+{
+	return walk_tg_tree_from(&root_task_group, down, up, data);
+}
+
+extern int tg_nop(struct task_group *tg, void *data);
+
+extern void free_fair_sched_group(struct task_group *tg);
+extern int alloc_fair_sched_group(struct task_group *tg, struct task_group *parent);
+extern void unregister_fair_sched_group(struct task_group *tg, int cpu);
+extern void init_tg_cfs_entry(struct task_group *tg, struct cfs_rq *cfs_rq,
+			struct sched_entity *se, int cpu,
+			struct sched_entity *parent);
+extern void init_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
+extern int sched_group_set_shares(struct task_group *tg, unsigned long shares);
+
+extern void __refill_cfs_bandwidth_runtime(struct cfs_bandwidth *cfs_b);
+extern void __start_cfs_bandwidth(struct cfs_bandwidth *cfs_b);
+extern void unthrottle_cfs_rq(struct cfs_rq *cfs_rq);
+
+extern void free_rt_sched_group(struct task_group *tg);
+extern int alloc_rt_sched_group(struct task_group *tg, struct task_group *parent);
+extern void init_tg_rt_entry(struct task_group *tg, struct rt_rq *rt_rq,
+		struct sched_rt_entity *rt_se, int cpu,
+		struct sched_rt_entity *parent);
+
+#else /* CONFIG_CGROUP_SCHED */
+
+struct cfs_bandwidth { };
+
+#endif	/* CONFIG_CGROUP_SCHED */
+
+/* CFS-related fields in a runqueue */
+struct cfs_rq {
+	struct load_weight load;
+	unsigned long nr_running, h_nr_running;
+
+	u64 exec_clock;
+	u64 min_vruntime;
+#ifndef CONFIG_64BIT
+	u64 min_vruntime_copy;
+#endif
+
+	struct rb_root tasks_timeline;
+	struct rb_node *rb_leftmost;
+
+	struct list_head tasks;
+	struct list_head *balance_iterator;
+
+	/*
+	 * 'curr' points to currently running entity on this cfs_rq.
+	 * It is set to NULL otherwise (i.e when none are currently running).
+	 */
+	struct sched_entity *curr, *next, *last, *skip;
+
+#ifdef	CONFIG_SCHED_DEBUG
+	unsigned int nr_spread_over;
+#endif
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	struct rq *rq;	/* cpu runqueue to which this cfs_rq is attached */
+
+	/*
+	 * leaf cfs_rqs are those that hold tasks (lowest schedulable entity in
+	 * a hierarchy). Non-leaf lrqs hold other higher schedulable entities
+	 * (like users, containers etc.)
+	 *
+	 * leaf_cfs_rq_list ties together list of leaf cfs_rq's in a cpu. This
+	 * list is used during load balance.
+	 */
+	int on_list;
+	struct list_head leaf_cfs_rq_list;
+	struct task_group *tg;	/* group that "owns" this runqueue */
+
+#ifdef CONFIG_SMP
+	/*
+	 * the part of load.weight contributed by tasks
+	 */
+	unsigned long task_weight;
+
+	/*
+	 *   h_load = weight * f(tg)
+	 *
+	 * Where f(tg) is the recursive weight fraction assigned to
+	 * this group.
+	 */
+	unsigned long h_load;
+
+	/*
+	 * Maintaining per-cpu shares distribution for group scheduling
+	 *
+	 * load_stamp is the last time we updated the load average
+	 * load_last is the last time we updated the load average and saw load
+	 * load_unacc_exec_time is currently unaccounted execution time
+	 */
+	u64 load_avg;
+	u64 load_period;
+	u64 load_stamp, load_last, load_unacc_exec_time;
+
+	unsigned long load_contribution;
+#endif /* CONFIG_SMP */
+#ifdef CONFIG_CFS_BANDWIDTH
+	int runtime_enabled;
+	u64 runtime_expires;
+	s64 runtime_remaining;
+
+	u64 throttled_timestamp;
+	int throttled, throttle_count;
+	struct list_head throttled_list;
+#endif /* CONFIG_CFS_BANDWIDTH */
+#endif /* CONFIG_FAIR_GROUP_SCHED */
+};
+
+static inline int rt_bandwidth_enabled(void)
+{
+	return sysctl_sched_rt_runtime >= 0;
+}
+
+/* Real-Time classes' related field in a runqueue: */
+struct rt_rq {
+	struct rt_prio_array active;
+	unsigned long rt_nr_running;
+#if defined CONFIG_SMP || defined CONFIG_RT_GROUP_SCHED
+	struct {
+		int curr; /* highest queued rt task prio */
+#ifdef CONFIG_SMP
+		int next; /* next highest */
+#endif
+	} highest_prio;
+#endif
+#ifdef CONFIG_SMP
+	unsigned long rt_nr_migratory;
+	unsigned long rt_nr_total;
+	int overloaded;
+	struct plist_head pushable_tasks;
+#endif
+	int rt_throttled;
+	u64 rt_time;
+	u64 rt_runtime;
+	/* Nests inside the rq lock: */
+	raw_spinlock_t rt_runtime_lock;
+
+#ifdef CONFIG_RT_GROUP_SCHED
+	unsigned long rt_nr_boosted;
+
+	struct rq *rq;
+	struct list_head leaf_rt_rq_list;
+	struct task_group *tg;
+#endif
+};
+
+#ifdef CONFIG_SMP
+
+/*
+ * We add the notion of a root-domain which will be used to define per-domain
+ * variables. Each exclusive cpuset essentially defines an island domain by
+ * fully partitioning the member cpus from any other cpuset. Whenever a new
+ * exclusive cpuset is created, we also create and attach a new root-domain
+ * object.
+ *
+ */
+struct root_domain {
+	atomic_t refcount;
+	atomic_t rto_count;
+	struct rcu_head rcu;
+	cpumask_var_t span;
+	cpumask_var_t online;
+
+	/*
+	 * The "RT overload" flag: it gets set if a CPU has more than
+	 * one runnable RT task.
+	 */
+	cpumask_var_t rto_mask;
+	struct cpupri cpupri;
+};
+
+extern struct root_domain def_root_domain;
+
+#endif /* CONFIG_SMP */
+
+/*
+ * This is the main, per-CPU runqueue data structure.
+ *
+ * Locking rule: those places that want to lock multiple runqueues
+ * (such as the load balancing or the thread migration code), lock
+ * acquire operations must be ordered by ascending &runqueue.
+ */
+struct rq {
+	/* runqueue lock: */
+	raw_spinlock_t lock;
+
+	/*
+	 * nr_running and cpu_load should be in the same cacheline because
+	 * remote CPUs use both these fields when doing load calculation.
+	 */
+	unsigned long nr_running;
+	#define CPU_LOAD_IDX_MAX 5
+	unsigned long cpu_load[CPU_LOAD_IDX_MAX];
+	unsigned long last_load_update_tick;
+#ifdef CONFIG_NO_HZ
+	u64 nohz_stamp;
+	unsigned char nohz_balance_kick;
+#endif
+	int skip_clock_update;
+
+	/* capture load from *all* tasks on this cpu: */
+	struct load_weight load;
+	unsigned long nr_load_updates;
+	u64 nr_switches;
+
+	struct cfs_rq cfs;
+	struct rt_rq rt;
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	/* list of leaf cfs_rq on this cpu: */
+	struct list_head leaf_cfs_rq_list;
+#endif
+#ifdef CONFIG_RT_GROUP_SCHED
+	struct list_head leaf_rt_rq_list;
+#endif
+
+	/*
+	 * This is part of a global counter where only the total sum
+	 * over all CPUs matters. A task can increase this counter on
+	 * one CPU and if it got migrated afterwards it may decrease
+	 * it on another CPU. Always updated under the runqueue lock:
+	 */
+	unsigned long nr_uninterruptible;
+
+	struct task_struct *curr, *idle, *stop;
+	unsigned long next_balance;
+	struct mm_struct *prev_mm;
+
+	u64 clock;
+	u64 clock_task;
+
+	atomic_t nr_iowait;
+
+#ifdef CONFIG_SMP
+	struct root_domain *rd;
+	struct sched_domain *sd;
+
+	unsigned long cpu_power;
+
+	unsigned char idle_balance;
+	/* For active balancing */
+	int post_schedule;
+	int active_balance;
+	int push_cpu;
+	struct cpu_stop_work active_balance_work;
+	/* cpu of this runqueue: */
+	int cpu;
+	int online;
+
+	u64 rt_avg;
+	u64 age_stamp;
+	u64 idle_stamp;
+	u64 avg_idle;
+#endif
+
+#ifdef CONFIG_IRQ_TIME_ACCOUNTING
+	u64 prev_irq_time;
+#endif
+#ifdef CONFIG_PARAVIRT
+	u64 prev_steal_time;
+#endif
+#ifdef CONFIG_PARAVIRT_TIME_ACCOUNTING
+	u64 prev_steal_time_rq;
+#endif
+
+	/* calc_load related fields */
+	unsigned long calc_load_update;
+	long calc_load_active;
+
+#ifdef CONFIG_SCHED_HRTICK
+#ifdef CONFIG_SMP
+	int hrtick_csd_pending;
+	struct call_single_data hrtick_csd;
+#endif
+	struct hrtimer hrtick_timer;
+#endif
+
+#ifdef CONFIG_SCHEDSTATS
+	/* latency stats */
+	struct sched_info rq_sched_info;
+	unsigned long long rq_cpu_time;
+	/* could above be rq->cfs_rq.exec_clock + rq->rt_rq.rt_runtime ? */
+
+	/* sys_sched_yield() stats */
+	unsigned int yld_count;
+
+	/* schedule() stats */
+	unsigned int sched_switch;
+	unsigned int sched_count;
+	unsigned int sched_goidle;
+
+	/* try_to_wake_up() stats */
+	unsigned int ttwu_count;
+	unsigned int ttwu_local;
+#endif
+
+#ifdef CONFIG_SMP
+	struct llist_head wake_list;
+#endif
+};
+
+static inline int cpu_of(struct rq *rq)
+{
+#ifdef CONFIG_SMP
+	return rq->cpu;
+#else
+	return 0;
+#endif
+}
+
+DECLARE_PER_CPU(struct rq, runqueues);
+
+#define rcu_dereference_check_sched_domain(p) \
+	rcu_dereference_check((p), \
+			      lockdep_is_held(&sched_domains_mutex))
+
+/*
+ * The domain tree (rq->sd) is protected by RCU's quiescent state transition.
+ * See detach_destroy_domains: synchronize_sched for details.
+ *
+ * The domain tree of any CPU may only be accessed from within
+ * preempt-disabled sections.
+ */
+#define for_each_domain(cpu, __sd) \
+	for (__sd = rcu_dereference_check_sched_domain(cpu_rq(cpu)->sd); __sd; __sd = __sd->parent)
+
+#define cpu_rq(cpu)		(&per_cpu(runqueues, (cpu)))
+#define this_rq()		(&__get_cpu_var(runqueues))
+#define task_rq(p)		cpu_rq(task_cpu(p))
+#define cpu_curr(cpu)		(cpu_rq(cpu)->curr)
+#define raw_rq()		(&__raw_get_cpu_var(runqueues))
+
+#include "stats.h"
+#include "auto_group.h"
+
+#ifdef CONFIG_CGROUP_SCHED
+
+/*
+ * Return the group to which this tasks belongs.
+ *
+ * We use task_subsys_state_check() and extend the RCU verification with
+ * pi->lock and rq->lock because cpu_cgroup_attach() holds those locks for each
+ * task it moves into the cgroup. Therefore by holding either of those locks,
+ * we pin the task to the current cgroup.
+ */
+static inline struct task_group *task_group(struct task_struct *p)
+{
+	struct task_group *tg;
+	struct cgroup_subsys_state *css;
+
+	css = task_subsys_state_check(p, cpu_cgroup_subsys_id,
+			lockdep_is_held(&p->pi_lock) ||
+			lockdep_is_held(&task_rq(p)->lock));
+	tg = container_of(css, struct task_group, css);
+
+	return autogroup_task_group(p, tg);
+}
+
+/* Change a task's cfs_rq and parent entity if it moves across CPUs/groups */
+static inline void set_task_rq(struct task_struct *p, unsigned int cpu)
+{
+#if defined(CONFIG_FAIR_GROUP_SCHED) || defined(CONFIG_RT_GROUP_SCHED)
+	struct task_group *tg = task_group(p);
+#endif
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	p->se.cfs_rq = tg->cfs_rq[cpu];
+	p->se.parent = tg->se[cpu];
+#endif
+
+#ifdef CONFIG_RT_GROUP_SCHED
+	p->rt.rt_rq  = tg->rt_rq[cpu];
+	p->rt.parent = tg->rt_se[cpu];
+#endif
+}
+
+#else /* CONFIG_CGROUP_SCHED */
+
+static inline void set_task_rq(struct task_struct *p, unsigned int cpu) { }
+static inline struct task_group *task_group(struct task_struct *p)
+{
+	return NULL;
+}
+
+#endif /* CONFIG_CGROUP_SCHED */
+
+static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)
+{
+	set_task_rq(p, cpu);
+#ifdef CONFIG_SMP
+	/*
+	 * After ->cpu is set up to a new value, task_rq_lock(p, ...) can be
+	 * successfuly executed on another CPU. We must ensure that updates of
+	 * per-task data have been completed by this moment.
+	 */
+	smp_wmb();
+	task_thread_info(p)->cpu = cpu;
+#endif
+}
+
+/*
+ * Tunables that become constants when CONFIG_SCHED_DEBUG is off:
+ */
+#ifdef CONFIG_SCHED_DEBUG
+# define const_debug __read_mostly
+#else
+# define const_debug const
+#endif
+
+extern const_debug unsigned int sysctl_sched_features;
+
+#define SCHED_FEAT(name, enabled)	\
+	__SCHED_FEAT_##name ,
+
+enum {
+#include "features.h"
+};
+
+#undef SCHED_FEAT
+
+#define sched_feat(x) (sysctl_sched_features & (1UL << __SCHED_FEAT_##x))
+
+static inline u64 global_rt_period(void)
+{
+	return (u64)sysctl_sched_rt_period * NSEC_PER_USEC;
+}
+
+static inline u64 global_rt_runtime(void)
+{
+	if (sysctl_sched_rt_runtime < 0)
+		return RUNTIME_INF;
+
+	return (u64)sysctl_sched_rt_runtime * NSEC_PER_USEC;
+}
+
+
+
+static inline int task_current(struct rq *rq, struct task_struct *p)
+{
+	return rq->curr == p;
+}
+
+static inline int task_running(struct rq *rq, struct task_struct *p)
+{
+#ifdef CONFIG_SMP
+	return p->on_cpu;
+#else
+	return task_current(rq, p);
+#endif
+}
+
+
+#ifndef prepare_arch_switch
+# define prepare_arch_switch(next)	do { } while (0)
+#endif
+#ifndef finish_arch_switch
+# define finish_arch_switch(prev)	do { } while (0)
+#endif
+
+#ifndef __ARCH_WANT_UNLOCKED_CTXSW
+static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * We can optimise this out completely for !SMP, because the
+	 * SMP rebalancing from interrupt is the only thing that cares
+	 * here.
+	 */
+	next->on_cpu = 1;
+#endif
+}
+
+static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * After ->on_cpu is cleared, the task can be moved to a different CPU.
+	 * We must ensure this doesn't happen until the switch is completely
+	 * finished.
+	 */
+	smp_wmb();
+	prev->on_cpu = 0;
+#endif
+#ifdef CONFIG_DEBUG_SPINLOCK
+	/* this is a valid case when another task releases the spinlock */
+	rq->lock.owner = current;
+#endif
+	/*
+	 * If we are tracking spinlock dependencies then we have to
+	 * fix up the runqueue lock - which gets 'carried over' from
+	 * prev into current:
+	 */
+	spin_acquire(&rq->lock.dep_map, 0, 0, _THIS_IP_);
+
+	raw_spin_unlock_irq(&rq->lock);
+}
+
+#else /* __ARCH_WANT_UNLOCKED_CTXSW */
+static inline void prepare_lock_switch(struct rq *rq, struct task_struct *next)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * We can optimise this out completely for !SMP, because the
+	 * SMP rebalancing from interrupt is the only thing that cares
+	 * here.
+	 */
+	next->on_cpu = 1;
+#endif
+#ifdef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+	raw_spin_unlock_irq(&rq->lock);
+#else
+	raw_spin_unlock(&rq->lock);
+#endif
+}
+
+static inline void finish_lock_switch(struct rq *rq, struct task_struct *prev)
+{
+#ifdef CONFIG_SMP
+	/*
+	 * After ->on_cpu is cleared, the task can be moved to a different CPU.
+	 * We must ensure this doesn't happen until the switch is completely
+	 * finished.
+	 */
+	smp_wmb();
+	prev->on_cpu = 0;
+#endif
+#ifndef __ARCH_WANT_INTERRUPTS_ON_CTXSW
+	local_irq_enable();
+#endif
+}
+#endif /* __ARCH_WANT_UNLOCKED_CTXSW */
+
+
+static inline void update_load_add(struct load_weight *lw, unsigned long inc)
+{
+	lw->weight += inc;
+	lw->inv_weight = 0;
+}
+
+static inline void update_load_sub(struct load_weight *lw, unsigned long dec)
+{
+	lw->weight -= dec;
+	lw->inv_weight = 0;
+}
+
+static inline void update_load_set(struct load_weight *lw, unsigned long w)
+{
+	lw->weight = w;
+	lw->inv_weight = 0;
+}
+
+/*
+ * To aid in avoiding the subversion of "niceness" due to uneven distribution
+ * of tasks with abnormal "nice" values across CPUs the contribution that
+ * each task makes to its run queue's load is weighted according to its
+ * scheduling class and "nice" value. For SCHED_NORMAL tasks this is just a
+ * scaled version of the new time slice allocation that they receive on time
+ * slice expiry etc.
+ */
+
+#define WEIGHT_IDLEPRIO                3
+#define WMULT_IDLEPRIO         1431655765
+
+/*
+ * Nice levels are multiplicative, with a gentle 10% change for every
+ * nice level changed. I.e. when a CPU-bound task goes from nice 0 to
+ * nice 1, it will get ~10% less CPU time than another CPU-bound task
+ * that remained on nice 0.
+ *
+ * The "10% effect" is relative and cumulative: from _any_ nice level,
+ * if you go up 1 level, it's -10% CPU usage, if you go down 1 level
+ * it's +10% CPU usage. (to achieve that we use a multiplier of 1.25.
+ * If a task goes up by ~10% and another task goes down by ~10% then
+ * the relative distance between them is ~25%.)
+ */
+static const int prio_to_weight[40] = {
+ /* -20 */     88761,     71755,     56483,     46273,     36291,
+ /* -15 */     29154,     23254,     18705,     14949,     11916,
+ /* -10 */      9548,      7620,      6100,      4904,      3906,
+ /*  -5 */      3121,      2501,      1991,      1586,      1277,
+ /*   0 */      1024,       820,       655,       526,       423,
+ /*   5 */       335,       272,       215,       172,       137,
+ /*  10 */       110,        87,        70,        56,        45,
+ /*  15 */        36,        29,        23,        18,        15,
+};
+
+/*
+ * Inverse (2^32/x) values of the prio_to_weight[] array, precalculated.
+ *
+ * In cases where the weight does not change often, we can use the
+ * precalculated inverse to speed up arithmetics by turning divisions
+ * into multiplications:
+ */
+static const u32 prio_to_wmult[40] = {
+ /* -20 */     48388,     59856,     76040,     92818,    118348,
+ /* -15 */    147320,    184698,    229616,    287308,    360437,
+ /* -10 */    449829,    563644,    704093,    875809,   1099582,
+ /*  -5 */   1376151,   1717300,   2157191,   2708050,   3363326,
+ /*   0 */   4194304,   5237765,   6557202,   8165337,  10153587,
+ /*   5 */  12820798,  15790321,  19976592,  24970740,  31350126,
+ /*  10 */  39045157,  49367440,  61356676,  76695844,  95443717,
+ /*  15 */ 119304647, 148102320, 186737708, 238609294, 286331153,
+};
+
+/* Time spent by the tasks of the cpu accounting group executing in ... */
+enum cpuacct_stat_index {
+	CPUACCT_STAT_USER,	/* ... user mode */
+	CPUACCT_STAT_SYSTEM,	/* ... kernel mode */
+
+	CPUACCT_STAT_NSTATS,
+};
+
+
+#define sched_class_highest (&stop_sched_class)
+#define for_each_class(class) \
+   for (class = sched_class_highest; class; class = class->next)
+
+extern const struct sched_class stop_sched_class;
+extern const struct sched_class rt_sched_class;
+extern const struct sched_class fair_sched_class;
+extern const struct sched_class idle_sched_class;
+
+
+#ifdef CONFIG_SMP
+
+extern void trigger_load_balance(struct rq *rq, int cpu);
+extern void idle_balance(int this_cpu, struct rq *this_rq);
+
+#else	/* CONFIG_SMP */
+
+static inline void idle_balance(int cpu, struct rq *rq)
+{
+}
+
+#endif
+
+extern void sysrq_sched_debug_show(void);
+extern void sched_init_granularity(void);
+extern void update_max_interval(void);
+extern void update_group_power(struct sched_domain *sd, int cpu);
+extern int update_runtime(struct notifier_block *nfb, unsigned long action, void *hcpu);
+extern void init_sched_rt_class(void);
+extern void init_sched_fair_class(void);
+
+extern void resched_task(struct task_struct *p);
+extern void resched_cpu(int cpu);
+
+extern struct rt_bandwidth def_rt_bandwidth;
+extern void init_rt_bandwidth(struct rt_bandwidth *rt_b, u64 period, u64 runtime);
+
+extern void update_cpu_load(struct rq *this_rq);
+
+#ifdef CONFIG_CGROUP_CPUACCT
+extern void cpuacct_charge(struct task_struct *tsk, u64 cputime);
+extern void cpuacct_update_stats(struct task_struct *tsk,
+		enum cpuacct_stat_index idx, cputime_t val);
+#else
+static inline void cpuacct_charge(struct task_struct *tsk, u64 cputime) {}
+static inline void cpuacct_update_stats(struct task_struct *tsk,
+		enum cpuacct_stat_index idx, cputime_t val) {}
+#endif
+
+static inline void inc_nr_running(struct rq *rq)
+{
+	rq->nr_running++;
+}
+
+static inline void dec_nr_running(struct rq *rq)
+{
+	rq->nr_running--;
+}
+
+extern void update_rq_clock(struct rq *rq);
+
+extern void activate_task(struct rq *rq, struct task_struct *p, int flags);
+extern void deactivate_task(struct rq *rq, struct task_struct *p, int flags);
+
+extern void check_preempt_curr(struct rq *rq, struct task_struct *p, int flags);
+
+extern const_debug unsigned int sysctl_sched_time_avg;
+extern const_debug unsigned int sysctl_sched_nr_migrate;
+extern const_debug unsigned int sysctl_sched_migration_cost;
+
+static inline u64 sched_avg_period(void)
+{
+	return (u64)sysctl_sched_time_avg * NSEC_PER_MSEC / 2;
+}
+
+void calc_load_account_idle(struct rq *this_rq);
+
+#ifdef CONFIG_SCHED_HRTICK
+
+/*
+ * Use hrtick when:
+ *  - enabled by features
+ *  - hrtimer is actually high res
+ */
+static inline int hrtick_enabled(struct rq *rq)
+{
+	if (!sched_feat(HRTICK))
+		return 0;
+	if (!cpu_active(cpu_of(rq)))
+		return 0;
+	return hrtimer_is_hres_active(&rq->hrtick_timer);
+}
+
+void hrtick_start(struct rq *rq, u64 delay);
+
+#endif /* CONFIG_SCHED_HRTICK */
+
+#ifdef CONFIG_SMP
+extern void sched_avg_update(struct rq *rq);
+static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta)
+{
+	rq->rt_avg += rt_delta;
+	sched_avg_update(rq);
+}
+#else
+static inline void sched_rt_avg_update(struct rq *rq, u64 rt_delta) { }
+static inline void sched_avg_update(struct rq *rq) { }
+#endif
+
+extern void start_bandwidth_timer(struct hrtimer *period_timer, ktime_t period);
+
+#ifdef CONFIG_SMP
+#ifdef CONFIG_PREEMPT
+
+static inline void double_rq_lock(struct rq *rq1, struct rq *rq2);
+
+/*
+ * fair double_lock_balance: Safely acquires both rq->locks in a fair
+ * way at the expense of forcing extra atomic operations in all
+ * invocations.  This assures that the double_lock is acquired using the
+ * same underlying policy as the spinlock_t on this architecture, which
+ * reduces latency compared to the unfair variant below.  However, it
+ * also adds more overhead and therefore may reduce throughput.
+ */
+static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
+	__releases(this_rq->lock)
+	__acquires(busiest->lock)
+	__acquires(this_rq->lock)
+{
+	raw_spin_unlock(&this_rq->lock);
+	double_rq_lock(this_rq, busiest);
+
+	return 1;
+}
+
+#else
+/*
+ * Unfair double_lock_balance: Optimizes throughput at the expense of
+ * latency by eliminating extra atomic operations when the locks are
+ * already in proper order on entry.  This favors lower cpu-ids and will
+ * grant the double lock to lower cpus over higher ids under contention,
+ * regardless of entry order into the function.
+ */
+static inline int _double_lock_balance(struct rq *this_rq, struct rq *busiest)
+	__releases(this_rq->lock)
+	__acquires(busiest->lock)
+	__acquires(this_rq->lock)
+{
+	int ret = 0;
+
+	if (unlikely(!raw_spin_trylock(&busiest->lock))) {
+		if (busiest < this_rq) {
+			raw_spin_unlock(&this_rq->lock);
+			raw_spin_lock(&busiest->lock);
+			raw_spin_lock_nested(&this_rq->lock,
+					      SINGLE_DEPTH_NESTING);
+			ret = 1;
+		} else
+			raw_spin_lock_nested(&busiest->lock,
+					      SINGLE_DEPTH_NESTING);
+	}
+	return ret;
+}
+
+#endif /* CONFIG_PREEMPT */
+
+/*
+ * double_lock_balance - lock the busiest runqueue, this_rq is locked already.
+ */
+static inline int double_lock_balance(struct rq *this_rq, struct rq *busiest)
+{
+	if (unlikely(!irqs_disabled())) {
+		/* printk() doesn't work good under rq->lock */
+		raw_spin_unlock(&this_rq->lock);
+		BUG_ON(1);
+	}
+
+	return _double_lock_balance(this_rq, busiest);
+}
+
+static inline void double_unlock_balance(struct rq *this_rq, struct rq *busiest)
+	__releases(busiest->lock)
+{
+	raw_spin_unlock(&busiest->lock);
+	lock_set_subclass(&this_rq->lock.dep_map, 0, _RET_IP_);
+}
+
+/*
+ * double_rq_lock - safely lock two runqueues
+ *
+ * Note this does not disable interrupts like task_rq_lock,
+ * you need to do so manually before calling.
+ */
+static inline void double_rq_lock(struct rq *rq1, struct rq *rq2)
+	__acquires(rq1->lock)
+	__acquires(rq2->lock)
+{
+	BUG_ON(!irqs_disabled());
+	if (rq1 == rq2) {
+		raw_spin_lock(&rq1->lock);
+		__acquire(rq2->lock);	/* Fake it out ;) */
+	} else {
+		if (rq1 < rq2) {
+			raw_spin_lock(&rq1->lock);
+			raw_spin_lock_nested(&rq2->lock, SINGLE_DEPTH_NESTING);
+		} else {
+			raw_spin_lock(&rq2->lock);
+			raw_spin_lock_nested(&rq1->lock, SINGLE_DEPTH_NESTING);
+		}
+	}
+}
+
+/*
+ * double_rq_unlock - safely unlock two runqueues
+ *
+ * Note this does not restore interrupts like task_rq_unlock,
+ * you need to do so manually after calling.
+ */
+static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
+	__releases(rq1->lock)
+	__releases(rq2->lock)
+{
+	raw_spin_unlock(&rq1->lock);
+	if (rq1 != rq2)
+		raw_spin_unlock(&rq2->lock);
+	else
+		__release(rq2->lock);
+}
+
+#else /* CONFIG_SMP */
+
+/*
+ * double_rq_lock - safely lock two runqueues
+ *
+ * Note this does not disable interrupts like task_rq_lock,
+ * you need to do so manually before calling.
+ */
+static inline void double_rq_lock(struct rq *rq1, struct rq *rq2)
+	__acquires(rq1->lock)
+	__acquires(rq2->lock)
+{
+	BUG_ON(!irqs_disabled());
+	BUG_ON(rq1 != rq2);
+	raw_spin_lock(&rq1->lock);
+	__acquire(rq2->lock);	/* Fake it out ;) */
+}
+
+/*
+ * double_rq_unlock - safely unlock two runqueues
+ *
+ * Note this does not restore interrupts like task_rq_unlock,
+ * you need to do so manually after calling.
+ */
+static inline void double_rq_unlock(struct rq *rq1, struct rq *rq2)
+	__releases(rq1->lock)
+	__releases(rq2->lock)
+{
+	BUG_ON(rq1 != rq2);
+	raw_spin_unlock(&rq1->lock);
+	__release(rq2->lock);
+}
+
+#endif
+
+extern struct sched_entity *__pick_first_entity(struct cfs_rq *cfs_rq);
+extern struct sched_entity *__pick_last_entity(struct cfs_rq *cfs_rq);
+extern void print_cfs_stats(struct seq_file *m, int cpu);
+extern void print_rt_stats(struct seq_file *m, int cpu);
+
+extern void init_cfs_rq(struct cfs_rq *cfs_rq);
+extern void init_rt_rq(struct rt_rq *rt_rq, struct rq *rq);
+extern void unthrottle_offline_cfs_rqs(struct rq *rq);
+
+extern void account_cfs_bandwidth_used(int enabled, int was_enabled);
