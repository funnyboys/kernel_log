commit 126c2092e5c8b28623cb890cd2930aa292410676
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:03 2020 +0200

    sched: Add rq::ttwu_pending
    
    In preparation of removing rq->wake_list, replace the
    !list_empty(rq->wake_list) with rq->ttwu_pending. This is not fully
    equivalent as this new variable is racy.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161908.070399698@infradead.org

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 1c24a6bbdae2..36c54265bb2b 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -638,7 +638,6 @@ do {									\
 
 	P(nr_running);
 	P(nr_switches);
-	P(nr_load_updates);
 	P(nr_uninterruptible);
 	PN(next_balance);
 	SEQ_printf(m, "  .%-30s: %ld\n", "curr->pid", (long)(task_pid_nr(rq->curr)));

commit 9013196a467e770e1470cccee6c0fe435ef37c66
Merge: 2a0a24ebb499 39f23ce07b93
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 19 20:34:12 2020 +0200

    Merge branch 'sched/urgent'

commit ad32bb41fca67936c0c1d6d0bdd6d3e2e9c5432f
Author: Pavankumar Kondeti <pkondeti@codeaurora.org>
Date:   Sun May 10 18:26:41 2020 +0530

    sched/debug: Fix requested task uclamp values shown in procfs
    
    The intention of commit 96e74ebf8d59 ("sched/debug: Add task uclamp
    values to SCHED_DEBUG procfs") was to print requested and effective
    task uclamp values. The requested values printed are read from p->uclamp,
    which holds the last effective values. Fix this by printing the values
    from p->uclamp_req.
    
    Fixes: 96e74ebf8d59 ("sched/debug: Add task uclamp values to SCHED_DEBUG procfs")
    Signed-off-by: Pavankumar Kondeti <pkondeti@codeaurora.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Valentin Schneider <valentin.schneider@arm.com>
    Tested-by: Valentin Schneider <valentin.schneider@arm.com>
    Link: https://lkml.kernel.org/r/1589115401-26391-1-git-send-email-pkondeti@codeaurora.org

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index a562df57a86e..239970b991c0 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -948,8 +948,8 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	P(se.avg.util_est.enqueued);
 #endif
 #ifdef CONFIG_UCLAMP_TASK
-	__PS("uclamp.min", p->uclamp[UCLAMP_MIN].value);
-	__PS("uclamp.max", p->uclamp[UCLAMP_MAX].value);
+	__PS("uclamp.min", p->uclamp_req[UCLAMP_MIN].value);
+	__PS("uclamp.max", p->uclamp_req[UCLAMP_MAX].value);
 	__PS("effective uclamp.min", uclamp_eff_value(p, UCLAMP_MIN));
 	__PS("effective uclamp.max", uclamp_eff_value(p, UCLAMP_MAX));
 #endif

commit 9818427c6270a9ce8c52c8621026fe9cebae0f92
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Apr 15 22:05:05 2020 +0100

    sched/debug: Make sd->flags sysctl read-only
    
    Writing to the sysctl of a sched_domain->flags directly updates the value of
    the field, and goes nowhere near update_top_cache_domain(). This means that
    the cached domain pointers can end up containing stale data (e.g. the
    domain pointed to doesn't have the relevant flag set anymore).
    
    Explicit domain walks that check for flags will be affected by
    the write, but this won't be in sync with the cached pointers which will
    still point to the domains that were cached at the last sched_domain
    build.
    
    In other words, writing to this interface is playing a dangerous game. It
    could be made to trigger an update of the cached sched_domain pointers when
    written to, but this does not seem to be worth the trouble. Make it
    read-only.
    
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200415210512.805-3-valentin.schneider@arm.com

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index b3ac1c10b032..c6cc02a6aad0 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -258,7 +258,7 @@ sd_alloc_ctl_domain_table(struct sched_domain *sd)
 	set_table_entry(&table[2], "busy_factor",	  &sd->busy_factor,	    sizeof(int),  0644, proc_dointvec_minmax);
 	set_table_entry(&table[3], "imbalance_pct",	  &sd->imbalance_pct,	    sizeof(int),  0644, proc_dointvec_minmax);
 	set_table_entry(&table[4], "cache_nice_tries",	  &sd->cache_nice_tries,    sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[5], "flags",		  &sd->flags,		    sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[5], "flags",		  &sd->flags,		    sizeof(int),  0444, proc_dointvec_minmax);
 	set_table_entry(&table[6], "max_newidle_lb_cost", &sd->max_newidle_lb_cost, sizeof(long), 0644, proc_doulongvec_minmax);
 	set_table_entry(&table[7], "name",		  sd->name,	       CORENAME_MAX_SIZE, 0444, proc_dostring);
 	/* &table[8] is terminator */

commit f080d93e1d419099a99d7473ed532289ca8dc717
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Tue Apr 14 20:57:21 2020 +0800

    sched/debug: Fix trival print_task() format
    
    Ensure leave one space between state and task name.
    
    w/o patch:
    runnable tasks:
     S           task   PID         tree-key  switches  prio     wait
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200414125721.195801-1-xiexiuqi@huawei.com

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index a562df57a86e..b3ac1c10b032 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -437,7 +437,7 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 	else
 		SEQ_printf(m, " %c", task_state_to_char(p));
 
-	SEQ_printf(m, "%15s %5d %9Ld.%06ld %9Ld %5d ",
+	SEQ_printf(m, " %15s %5d %9Ld.%06ld %9Ld %5d ",
 		p->comm, task_pid_nr(p),
 		SPLIT_NS(p->se.vruntime),
 		(long long)(p->nvcsw + p->nivcsw),
@@ -464,10 +464,10 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 
 	SEQ_printf(m, "\n");
 	SEQ_printf(m, "runnable tasks:\n");
-	SEQ_printf(m, " S           task   PID         tree-key  switches  prio"
+	SEQ_printf(m, " S            task   PID         tree-key  switches  prio"
 		   "     wait-time             sum-exec        sum-sleep\n");
 	SEQ_printf(m, "-------------------------------------------------------"
-		   "----------------------------------------------------\n");
+		   "------------------------------------------------------\n");
 
 	rcu_read_lock();
 	for_each_process_thread(g, p) {

commit 96e74ebf8d594496f3dda5f8e26af6b4e161e4e9
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Feb 26 12:45:43 2020 +0000

    sched/debug: Add task uclamp values to SCHED_DEBUG procfs
    
    Requested and effective uclamp values can be a bit tricky to decipher when
    playing with cgroup hierarchies. Add them to a task's procfs when
    SCHED_DEBUG is enabled.
    
    Reviewed-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200226124543.31986-4-valentin.schneider@arm.com

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 315ef6de3cc4..a562df57a86e 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -946,6 +946,12 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	P(se.avg.last_update_time);
 	P(se.avg.util_est.ewma);
 	P(se.avg.util_est.enqueued);
+#endif
+#ifdef CONFIG_UCLAMP_TASK
+	__PS("uclamp.min", p->uclamp[UCLAMP_MIN].value);
+	__PS("uclamp.max", p->uclamp[UCLAMP_MAX].value);
+	__PS("effective uclamp.min", uclamp_eff_value(p, UCLAMP_MIN));
+	__PS("effective uclamp.max", uclamp_eff_value(p, UCLAMP_MAX));
 #endif
 	P(policy);
 	P(prio);

commit 9e3bf9469c29f7e4e49c5c0d8fecaf8ac57d1fe4
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Feb 26 12:45:42 2020 +0000

    sched/debug: Factor out printing formats into common macros
    
    The printing macros in debug.c keep redefining the same output
    format. Collect each output format in a single definition, and reuse that
    definition in the other macros. While at it, add a layer of parentheses and
    replace printf's  with the newly introduced macros.
    
    Reviewed-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200226124543.31986-3-valentin.schneider@arm.com

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 4670151eb131..315ef6de3cc4 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -816,10 +816,12 @@ static int __init init_sched_debug_procfs(void)
 
 __initcall(init_sched_debug_procfs);
 
-#define __P(F)	SEQ_printf(m, "%-45s:%21Ld\n",	     #F, (long long)F)
-#define   P(F)	SEQ_printf(m, "%-45s:%21Ld\n",	     #F, (long long)p->F)
-#define __PN(F)	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
-#define   PN(F)	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
+#define __PS(S, F) SEQ_printf(m, "%-45s:%21Ld\n", S, (long long)(F))
+#define __P(F) __PS(#F, F)
+#define   P(F) __PS(#F, p->F)
+#define __PSN(S, F) SEQ_printf(m, "%-45s:%14Ld.%06ld\n", S, SPLIT_NS((long long)(F)))
+#define __PN(F) __PSN(#F, F)
+#define   PN(F) __PSN(#F, p->F)
 
 
 #ifdef CONFIG_NUMA_BALANCING
@@ -868,10 +870,9 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	SEQ_printf(m,
 		"---------------------------------------------------------"
 		"----------\n");
-#define P_SCHEDSTAT(F) \
-	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)schedstat_val(p->F))
-#define PN_SCHEDSTAT(F) \
-	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)schedstat_val(p->F)))
+
+#define P_SCHEDSTAT(F)  __PS(#F, schedstat_val(p->F))
+#define PN_SCHEDSTAT(F) __PSN(#F, schedstat_val(p->F))
 
 	PN(se.exec_start);
 	PN(se.vruntime);
@@ -931,10 +932,8 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	}
 
 	__P(nr_switches);
-	SEQ_printf(m, "%-45s:%21Ld\n",
-		   "nr_voluntary_switches", (long long)p->nvcsw);
-	SEQ_printf(m, "%-45s:%21Ld\n",
-		   "nr_involuntary_switches", (long long)p->nivcsw);
+	__PS("nr_voluntary_switches", p->nvcsw);
+	__PS("nr_involuntary_switches", p->nivcsw);
 
 	P(se.load.weight);
 #ifdef CONFIG_SMP
@@ -963,8 +962,7 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 
 		t0 = cpu_clock(this_cpu);
 		t1 = cpu_clock(this_cpu);
-		SEQ_printf(m, "%-45s:%21Ld\n",
-			   "clock-delta", (long long)(t1-t0));
+		__PS("clock-delta", t1-t0);
 	}
 
 	sched_show_numa(p, m);

commit c745a6212c9923eb2253f4229e5d7277ca3d9d8e
Author: Valentin Schneider <valentin.schneider@arm.com>
Date:   Wed Feb 26 12:45:41 2020 +0000

    sched/debug: Remove redundant macro define
    
    Most printing macros for procfs are defined globally in debug.c, and they
    are re-defined (to the exact same thing) within proc_sched_show_task().
    
    Get rid of the duplicate defines.
    
    Reviewed-by: Qais Yousef <qais.yousef@arm.com>
    Signed-off-by: Valentin Schneider <valentin.schneider@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lkml.kernel.org/r/20200226124543.31986-2-valentin.schneider@arm.com

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 8331bc04aea2..4670151eb131 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -868,16 +868,8 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	SEQ_printf(m,
 		"---------------------------------------------------------"
 		"----------\n");
-#define __P(F) \
-	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)F)
-#define P(F) \
-	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)p->F)
 #define P_SCHEDSTAT(F) \
 	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)schedstat_val(p->F))
-#define __PN(F) \
-	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
-#define PN(F) \
-	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
 #define PN_SCHEDSTAT(F) \
 	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)schedstat_val(p->F)))
 
@@ -963,11 +955,7 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 		P(dl.deadline);
 	}
 #undef PN_SCHEDSTAT
-#undef PN
-#undef __PN
 #undef P_SCHEDSTAT
-#undef P
-#undef __P
 
 	{
 		unsigned int this_cpu = raw_smp_processor_id();

commit 9f68395333ad7f5bfe2f83473fed363d4229f11c
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Feb 24 09:52:18 2020 +0000

    sched/pelt: Add a new runnable average signal
    
    Now that runnable_load_avg has been removed, we can replace it by a new
    signal that will highlight the runnable pressure on a cfs_rq. This signal
    track the waiting time of tasks on rq and can help to better define the
    state of rqs.
    
    At now, only util_avg is used to define the state of a rq:
      A rq with more that around 80% of utilization and more than 1 tasks is
      considered as overloaded.
    
    But the util_avg signal of a rq can become temporaly low after that a task
    migrated onto another rq which can bias the classification of the rq.
    
    When tasks compete for the same rq, their runnable average signal will be
    higher than util_avg as it will include the waiting time and we can use
    this signal to better classify cfs_rqs.
    
    The new runnable_avg will track the runnable time of a task which simply
    adds the waiting time to the running time. The runnable _avg of cfs_rq
    will be the /Sum of se's runnable_avg and the runnable_avg of group entity
    will follow the one of the rq similarly to util_avg.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Link: https://lore.kernel.org/r/20200224095223.13361-9-mgorman@techsingularity.net

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index cfecaad387c0..8331bc04aea2 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -405,6 +405,7 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 #ifdef CONFIG_SMP
 	P(se->avg.load_avg);
 	P(se->avg.util_avg);
+	P(se->avg.runnable_avg);
 #endif
 
 #undef PN_SCHEDSTAT
@@ -524,6 +525,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #ifdef CONFIG_SMP
 	SEQ_printf(m, "  .%-30s: %lu\n", "load_avg",
 			cfs_rq->avg.load_avg);
+	SEQ_printf(m, "  .%-30s: %lu\n", "runnable_avg",
+			cfs_rq->avg.runnable_avg);
 	SEQ_printf(m, "  .%-30s: %lu\n", "util_avg",
 			cfs_rq->avg.util_avg);
 	SEQ_printf(m, "  .%-30s: %u\n", "util_est_enqueued",
@@ -532,8 +535,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->removed.load_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "removed.util_avg",
 			cfs_rq->removed.util_avg);
-	SEQ_printf(m, "  .%-30s: %ld\n", "removed.runnable_sum",
-			cfs_rq->removed.runnable_sum);
+	SEQ_printf(m, "  .%-30s: %ld\n", "removed.runnable_avg",
+			cfs_rq->removed.runnable_avg);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	SEQ_printf(m, "  .%-30s: %lu\n", "tg_load_avg_contrib",
 			cfs_rq->tg_load_avg_contrib);
@@ -944,8 +947,10 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	P(se.load.weight);
 #ifdef CONFIG_SMP
 	P(se.avg.load_sum);
+	P(se.avg.runnable_sum);
 	P(se.avg.util_sum);
 	P(se.avg.load_avg);
+	P(se.avg.runnable_avg);
 	P(se.avg.util_avg);
 	P(se.avg.last_update_time);
 	P(se.avg.util_est.ewma);

commit 0dacee1bfa70e171be3a12a30414c228453048d2
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Mon Feb 24 09:52:17 2020 +0000

    sched/pelt: Remove unused runnable load average
    
    Now that runnable_load_avg is no more used, we can remove it to make
    space for a new signal.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: "Dietmar Eggemann <dietmar.eggemann@arm.com>"
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Hillf Danton <hdanton@sina.com>
    Link: https://lore.kernel.org/r/20200224095223.13361-8-mgorman@techsingularity.net

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 879d3ccf3806..cfecaad387c0 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -402,11 +402,9 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	}
 
 	P(se->load.weight);
-	P(se->runnable_weight);
 #ifdef CONFIG_SMP
 	P(se->avg.load_avg);
 	P(se->avg.util_avg);
-	P(se->avg.runnable_load_avg);
 #endif
 
 #undef PN_SCHEDSTAT
@@ -524,11 +522,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", cfs_rq->nr_running);
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
 #ifdef CONFIG_SMP
-	SEQ_printf(m, "  .%-30s: %ld\n", "runnable_weight", cfs_rq->runnable_weight);
 	SEQ_printf(m, "  .%-30s: %lu\n", "load_avg",
 			cfs_rq->avg.load_avg);
-	SEQ_printf(m, "  .%-30s: %lu\n", "runnable_load_avg",
-			cfs_rq->avg.runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lu\n", "util_avg",
 			cfs_rq->avg.util_avg);
 	SEQ_printf(m, "  .%-30s: %u\n", "util_est_enqueued",
@@ -947,13 +942,10 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 		   "nr_involuntary_switches", (long long)p->nivcsw);
 
 	P(se.load.weight);
-	P(se.runnable_weight);
 #ifdef CONFIG_SMP
 	P(se.avg.load_sum);
-	P(se.avg.runnable_load_sum);
 	P(se.avg.util_sum);
 	P(se.avg.load_avg);
-	P(se.avg.runnable_load_avg);
 	P(se.avg.util_avg);
 	P(se.avg.last_update_time);
 	P(se.avg.util_est.ewma);

commit 02d4ac5885a18d326b500b94808f0956dcce2832
Author: Wei Li <liwei391@huawei.com>
Date:   Thu Dec 26 16:52:24 2019 +0800

    sched/debug: Reset watchdog on all CPUs while processing sysrq-t
    
    Lengthy output of sysrq-t may take a lot of time on slow serial console
    with lots of processes and CPUs.
    
    So we need to reset NMI-watchdog to avoid spurious lockup messages, and
    we also reset softlockup watchdogs on all other CPUs since another CPU
    might be blocked waiting for us to process an IPI or stop_machine.
    
    Add to sysrq_sched_debug_show() as what we did in show_state_filter().
    
    Signed-off-by: Wei Li <liwei391@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Link: https://lkml.kernel.org/r/20191226085224.48942-1-liwei391@huawei.com

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index f7e4579e746c..879d3ccf3806 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -751,9 +751,16 @@ void sysrq_sched_debug_show(void)
 	int cpu;
 
 	sched_debug_header(NULL);
-	for_each_online_cpu(cpu)
+	for_each_online_cpu(cpu) {
+		/*
+		 * Need to reset softlockup watchdogs on all CPUs, because
+		 * another CPU might be blocked waiting for us to process
+		 * an IPI or stop_machine.
+		 */
+		touch_nmi_watchdog();
+		touch_all_softlockup_watchdogs();
 		print_cpu(NULL, cpu);
-
+	}
 }
 
 /*

commit d2abae71ebcc409828b24ce9da402548ecdf1311
Merge: 66567fcbaeca 4b972a01a7da
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jun 24 19:19:53 2019 +0200

    Merge tag 'v5.2-rc6' into sched/core, to refresh the branch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit d2912cb15bdda8ba4a5dd73396ad62641af2f520
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Tue Jun 4 10:11:33 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 500
    
    Based on 2 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license version 2 as
      published by the free software foundation #
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-only
    
    has been chosen to replace the boilerplate/reference in 4122 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Enrico Weigelt <info@metux.net>
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190604081206.933168790@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 678bfb9bd87f..14c6a8716ba1 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -1,13 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * kernel/sched/debug.c
  *
  * Print the CFS rbtree and other debugging details
  *
  * Copyright(C) 2007, Red Hat, Inc., Ingo Molnar
- *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License version 2 as
- * published by the Free Software Foundation.
  */
 #include "sched.h"
 

commit 0e1fef63d92d61ed561e504c3a078a827a0f9bfe
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Mon May 27 07:21:14 2019 +0100

    sched/core: Remove sd->*_idx
    
    The sched domain per rq load index files also disappear from the
    /proc/sys/kernel/sched_domain/cpuX/domainY directories.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@surriel.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Patrick Bellasi <patrick.bellasi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: https://lkml.kernel.org/r/20190527062116.11512-6-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index a0b0d6e21e5b..7ffde8ce82fd 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -251,25 +251,20 @@ set_table_entry(struct ctl_table *entry,
 static struct ctl_table *
 sd_alloc_ctl_domain_table(struct sched_domain *sd)
 {
-	struct ctl_table *table = sd_alloc_ctl_entry(14);
+	struct ctl_table *table = sd_alloc_ctl_entry(9);
 
 	if (table == NULL)
 		return NULL;
 
-	set_table_entry(&table[0],  "min_interval",	   &sd->min_interval,	     sizeof(long), 0644, proc_doulongvec_minmax);
-	set_table_entry(&table[1],  "max_interval",	   &sd->max_interval,	     sizeof(long), 0644, proc_doulongvec_minmax);
-	set_table_entry(&table[2],  "busy_idx",		   &sd->busy_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[3],  "idle_idx",		   &sd->idle_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[4],  "newidle_idx",	   &sd->newidle_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[5],  "wake_idx",		   &sd->wake_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[6],  "forkexec_idx",	   &sd->forkexec_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[7],  "busy_factor",	   &sd->busy_factor,	     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[8],  "imbalance_pct",	   &sd->imbalance_pct,	     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[9],  "cache_nice_tries",	   &sd->cache_nice_tries,    sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[10], "flags",		   &sd->flags,		     sizeof(int),  0644, proc_dointvec_minmax);
-	set_table_entry(&table[11], "max_newidle_lb_cost", &sd->max_newidle_lb_cost, sizeof(long), 0644, proc_doulongvec_minmax);
-	set_table_entry(&table[12], "name",		   sd->name,		CORENAME_MAX_SIZE, 0444, proc_dostring);
-	/* &table[13] is terminator */
+	set_table_entry(&table[0], "min_interval",	  &sd->min_interval,	    sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[1], "max_interval",	  &sd->max_interval,	    sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[2], "busy_factor",	  &sd->busy_factor,	    sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[3], "imbalance_pct",	  &sd->imbalance_pct,	    sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[4], "cache_nice_tries",	  &sd->cache_nice_tries,    sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[5], "flags",		  &sd->flags,		    sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[6], "max_newidle_lb_cost", &sd->max_newidle_lb_cost, sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[7], "name",		  sd->name,	       CORENAME_MAX_SIZE, 0444, proc_dostring);
+	/* &table[8] is terminator */
 
 	return table;
 }

commit 55627e3cd22c315c4a02fe3bbbb7234ec439cb1d
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Mon May 27 07:21:13 2019 +0100

    sched/core: Remove rq->cpu_load[]
    
    The per rq load array values also disappear from the cpu#X sections in
    /proc/sched_debug.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@surriel.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Patrick Bellasi <patrick.bellasi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: https://lkml.kernel.org/r/20190527062116.11512-5-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 5c7b066d7de6..a0b0d6e21e5b 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -654,11 +654,6 @@ do {									\
 	SEQ_printf(m, "  .%-30s: %ld\n", "curr->pid", (long)(task_pid_nr(rq->curr)));
 	PN(clock);
 	PN(clock_task);
-	P(cpu_load[0]);
-	P(cpu_load[1]);
-	P(cpu_load[2]);
-	P(cpu_load[3]);
-	P(cpu_load[4]);
 #undef P
 #undef PN
 

commit 3d8d53554405952993bb0279ef3ebebc51740074
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Mon May 27 07:21:12 2019 +0100

    sched/debug: Remove sd->*_idx range on sysctl
    
    This reverts:
    
      commit 201c373e8e48 ("sched/debug: Limit sd->*_idx range on sysctl")
    
    Load indexes (sd->*_idx) are no longer needed without rq->cpu_load[].
    The range check for load indexes can be removed as well. Get rid of it
    before the rq->cpu_load[] since it uses CPU_LOAD_IDX_MAX.
    
    At the same time, fix the following coding style issues detected by
    scripts/checkpatch.pl:
    
      ERROR: space prohibited before that ','
      ERROR: space prohibited before that close parenthesis ')'
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Rik van Riel <riel@surriel.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Patrick Bellasi <patrick.bellasi@arm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Quentin Perret <quentin.perret@arm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: https://lkml.kernel.org/r/20190527062116.11512-4-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 150043e1d716..5c7b066d7de6 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -236,25 +236,16 @@ static void sd_free_ctl_entry(struct ctl_table **tablep)
 	*tablep = NULL;
 }
 
-static int min_load_idx = 0;
-static int max_load_idx = CPU_LOAD_IDX_MAX-1;
-
 static void
 set_table_entry(struct ctl_table *entry,
 		const char *procname, void *data, int maxlen,
-		umode_t mode, proc_handler *proc_handler,
-		bool load_idx)
+		umode_t mode, proc_handler *proc_handler)
 {
 	entry->procname = procname;
 	entry->data = data;
 	entry->maxlen = maxlen;
 	entry->mode = mode;
 	entry->proc_handler = proc_handler;
-
-	if (load_idx) {
-		entry->extra1 = &min_load_idx;
-		entry->extra2 = &max_load_idx;
-	}
 }
 
 static struct ctl_table *
@@ -265,19 +256,19 @@ sd_alloc_ctl_domain_table(struct sched_domain *sd)
 	if (table == NULL)
 		return NULL;
 
-	set_table_entry(&table[0] , "min_interval",	   &sd->min_interval,	     sizeof(long), 0644, proc_doulongvec_minmax, false);
-	set_table_entry(&table[1] , "max_interval",	   &sd->max_interval,	     sizeof(long), 0644, proc_doulongvec_minmax, false);
-	set_table_entry(&table[2] , "busy_idx",		   &sd->busy_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
-	set_table_entry(&table[3] , "idle_idx",		   &sd->idle_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
-	set_table_entry(&table[4] , "newidle_idx",	   &sd->newidle_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
-	set_table_entry(&table[5] , "wake_idx",		   &sd->wake_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
-	set_table_entry(&table[6] , "forkexec_idx",	   &sd->forkexec_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
-	set_table_entry(&table[7] , "busy_factor",	   &sd->busy_factor,	     sizeof(int) , 0644, proc_dointvec_minmax,   false);
-	set_table_entry(&table[8] , "imbalance_pct",	   &sd->imbalance_pct,	     sizeof(int) , 0644, proc_dointvec_minmax,   false);
-	set_table_entry(&table[9] , "cache_nice_tries",	   &sd->cache_nice_tries,    sizeof(int) , 0644, proc_dointvec_minmax,   false);
-	set_table_entry(&table[10], "flags",		   &sd->flags,		     sizeof(int) , 0644, proc_dointvec_minmax,   false);
-	set_table_entry(&table[11], "max_newidle_lb_cost", &sd->max_newidle_lb_cost, sizeof(long), 0644, proc_doulongvec_minmax, false);
-	set_table_entry(&table[12], "name",		   sd->name,		CORENAME_MAX_SIZE, 0444, proc_dostring,		 false);
+	set_table_entry(&table[0],  "min_interval",	   &sd->min_interval,	     sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[1],  "max_interval",	   &sd->max_interval,	     sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[2],  "busy_idx",		   &sd->busy_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[3],  "idle_idx",		   &sd->idle_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[4],  "newidle_idx",	   &sd->newidle_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[5],  "wake_idx",		   &sd->wake_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[6],  "forkexec_idx",	   &sd->forkexec_idx,	     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[7],  "busy_factor",	   &sd->busy_factor,	     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[8],  "imbalance_pct",	   &sd->imbalance_pct,	     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[9],  "cache_nice_tries",	   &sd->cache_nice_tries,    sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[10], "flags",		   &sd->flags,		     sizeof(int),  0644, proc_dointvec_minmax);
+	set_table_entry(&table[11], "max_newidle_lb_cost", &sd->max_newidle_lb_cost, sizeof(long), 0644, proc_doulongvec_minmax);
+	set_table_entry(&table[12], "name",		   sd->name,		CORENAME_MAX_SIZE, 0444, proc_dostring);
 	/* &table[13] is terminator */
 
 	return table;

commit f2bedc4705659216bd60948029ad8dfedf923ad9
Author: Dietmar Eggemann <dietmar.eggemann@arm.com>
Date:   Wed Apr 24 09:45:56 2019 +0100

    sched/fair: Remove rq->load
    
    The CFS class is the only one maintaining and using the CPU wide load
    (rq->load(.weight)). The last use case of the CPU wide load in CFS's
    set_next_entity() can be replaced by using the load of the CFS class
    (rq->cfs.load(.weight)) instead.
    
    Signed-off-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190424084556.604-1-dietmar.eggemann@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 678bfb9bd87f..150043e1d716 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -656,8 +656,6 @@ do {									\
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(rq->x))
 
 	P(nr_running);
-	SEQ_printf(m, "  .%-30s: %lu\n", "load",
-		   rq->load.weight);
 	P(nr_switches);
 	P(nr_load_updates);
 	P(nr_uninterruptible);

commit ad2e379def135ebc079f89a0e0b1d987d243f949
Author: Colin Ian King <colin.king@canonical.com>
Date:   Wed Nov 28 15:23:50 2018 +0000

    sched/debug: Fix spelling mistake "logaritmic" -> "logarithmic"
    
    Signed-off-by: Colin Ian King <colin.king@canonical.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel-janitors@vger.kernel.org
    Link: http://lkml.kernel.org/r/20181128152350.13622-1-colin.king@canonical.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 8039d62ae36e..678bfb9bd87f 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -702,7 +702,7 @@ do {									\
 
 static const char *sched_tunable_scaling_names[] = {
 	"none",
-	"logaritmic",
+	"logarithmic",
 	"linear"
 };
 

commit 1ca4fa3ab604734e38e2a3000c9abf788512ffa7
Author: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
Date:   Tue Jan 29 10:12:45 2019 -0500

    sched/debug: Initialize sd_sysctl_cpus if !CONFIG_CPUMASK_OFFSTACK
    
    register_sched_domain_sysctl() copies the cpu_possible_mask into
    sd_sysctl_cpus, but only if sd_sysctl_cpus hasn't already been
    allocated (ie, CONFIG_CPUMASK_OFFSTACK is set).  However, when
    CONFIG_CPUMASK_OFFSTACK is not set, sd_sysctl_cpus is left
    uninitialized (all zeroes) and the kernel may fail to initialize
    sched_domain sysctl entries for all possible CPUs.
    
    This is visible to the user if the kernel is booted with maxcpus=n, or
    if ACPI tables have been modified to leave CPUs offline, and then
    checking for missing /proc/sys/kernel/sched_domain/cpu* entries.
    
    Fix this by separating the allocation and initialization, and adding a
    flag to initialize the possible CPU entries while system booting only.
    
    Tested-by: Syuuichirou Ishii <ishii.shuuichir@jp.fujitsu.com>
    Tested-by: Tarumizu, Kohei <tarumizu.kohei@jp.fujitsu.com>
    Signed-off-by: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Masayoshi Mizuma <m.mizuma@jp.fujitsu.com>
    Acked-by: Joe Lawrence <joe.lawrence@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masayoshi Mizuma <msys.mizuma@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20190129151245.5073-1-msys.mizuma@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index de3de997e245..8039d62ae36e 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -315,6 +315,7 @@ void register_sched_domain_sysctl(void)
 {
 	static struct ctl_table *cpu_entries;
 	static struct ctl_table **cpu_idx;
+	static bool init_done = false;
 	char buf[32];
 	int i;
 
@@ -344,7 +345,10 @@ void register_sched_domain_sysctl(void)
 	if (!cpumask_available(sd_sysctl_cpus)) {
 		if (!alloc_cpumask_var(&sd_sysctl_cpus, GFP_KERNEL))
 			return;
+	}
 
+	if (!init_done) {
+		init_done = true;
 		/* init to possible to not have holes in @cpu_entries */
 		cpumask_copy(sd_sysctl_cpus, cpu_possible_mask);
 	}

commit e9666d10a5677a494260d60d1fa0b73cc7646eb3
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Mon Dec 31 00:14:15 2018 +0900

    jump_label: move 'asm goto' support test to Kconfig
    
    Currently, CONFIG_JUMP_LABEL just means "I _want_ to use jump label".
    
    The jump label is controlled by HAVE_JUMP_LABEL, which is defined
    like this:
    
      #if defined(CC_HAVE_ASM_GOTO) && defined(CONFIG_JUMP_LABEL)
      # define HAVE_JUMP_LABEL
      #endif
    
    We can improve this by testing 'asm goto' support in Kconfig, then
    make JUMP_LABEL depend on CC_HAS_ASM_GOTO.
    
    Ugly #ifdef HAVE_JUMP_LABEL will go away, and CONFIG_JUMP_LABEL will
    match to the real kernel capability.
    
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au> (powerpc)
    Tested-by: Sedat Dilek <sedat.dilek@gmail.com>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 02bd5f969b21..de3de997e245 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -73,7 +73,7 @@ static int sched_feat_show(struct seq_file *m, void *v)
 	return 0;
 }
 
-#ifdef HAVE_JUMP_LABEL
+#ifdef CONFIG_JUMP_LABEL
 
 #define jump_label_key__true  STATIC_KEY_INIT_TRUE
 #define jump_label_key__false STATIC_KEY_INIT_FALSE
@@ -99,7 +99,7 @@ static void sched_feat_enable(int i)
 #else
 static void sched_feat_disable(int i) { };
 static void sched_feat_enable(int i) { };
-#endif /* HAVE_JUMP_LABEL */
+#endif /* CONFIG_JUMP_LABEL */
 
 static int sched_feat_set(char *cmp)
 {

commit 1da1843f9f0334e2428308945d396ffecc2acfe1
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Mon Nov 5 16:51:55 2018 +0530

    sched/core: Create task_has_idle_policy() helper
    
    We already have task_has_rt_policy() and task_has_dl_policy() helpers,
    create task_has_idle_policy() as well and update sched core to start
    using it.
    
    While at it, use task_has_dl_policy() at one more place.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Link: http://lkml.kernel.org/r/ce3915d5b490fc81af926a3b6bfb775e7188e005.1541416894.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 6383aa6a60ca..02bd5f969b21 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -974,7 +974,7 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 #endif
 	P(policy);
 	P(prio);
-	if (p->policy == SCHED_DEADLINE) {
+	if (task_has_dl_policy(p)) {
 		P(dl.runtime);
 		P(dl.deadline);
 	}

commit e73e81975f2447e6f556100cada64a18ec631cbb
Author: Jiada Wang <jiada_wang@mentor.com>
Date:   Tue Jul 31 21:12:22 2018 +0900

    sched/debug: Fix potential deadlock when writing to sched_features
    
    The following lockdep report can be triggered by writing to /sys/kernel/debug/sched_features:
    
      ======================================================
      WARNING: possible circular locking dependency detected
      4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18 Not tainted
      ------------------------------------------------------
      sh/3358 is trying to acquire lock:
      000000004ad3989d (cpu_hotplug_lock.rw_sem){++++}, at: static_key_enable+0x14/0x30
      but task is already holding lock:
      00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428
      which lock already depends on the new lock.
      the existing dependency chain (in reverse order) is:
      -> #3 (&sb->s_type->i_mutex_key#3){+.+.}:
             lock_acquire+0xb8/0x148
             down_write+0xac/0x140
             start_creating+0x5c/0x168
             debugfs_create_dir+0x18/0x220
             opp_debug_register+0x8c/0x120
             _add_opp_dev+0x104/0x1f8
             dev_pm_opp_get_opp_table+0x174/0x340
             _of_add_opp_table_v2+0x110/0x760
             dev_pm_opp_of_add_table+0x5c/0x240
             dev_pm_opp_of_cpumask_add_table+0x5c/0x100
             cpufreq_init+0x160/0x430
             cpufreq_online+0x1cc/0xe30
             cpufreq_add_dev+0x78/0x198
             subsys_interface_register+0x168/0x270
             cpufreq_register_driver+0x1c8/0x278
             dt_cpufreq_probe+0xdc/0x1b8
             platform_drv_probe+0xb4/0x168
             driver_probe_device+0x318/0x4b0
             __device_attach_driver+0xfc/0x1f0
             bus_for_each_drv+0xf8/0x180
             __device_attach+0x164/0x200
             device_initial_probe+0x10/0x18
             bus_probe_device+0x110/0x178
             device_add+0x6d8/0x908
             platform_device_add+0x138/0x3d8
             platform_device_register_full+0x1cc/0x1f8
             cpufreq_dt_platdev_init+0x174/0x1bc
             do_one_initcall+0xb8/0x310
             kernel_init_freeable+0x4b8/0x56c
             kernel_init+0x10/0x138
             ret_from_fork+0x10/0x18
      -> #2 (opp_table_lock){+.+.}:
             lock_acquire+0xb8/0x148
             __mutex_lock+0x104/0xf50
             mutex_lock_nested+0x1c/0x28
             _of_add_opp_table_v2+0xb4/0x760
             dev_pm_opp_of_add_table+0x5c/0x240
             dev_pm_opp_of_cpumask_add_table+0x5c/0x100
             cpufreq_init+0x160/0x430
             cpufreq_online+0x1cc/0xe30
             cpufreq_add_dev+0x78/0x198
             subsys_interface_register+0x168/0x270
             cpufreq_register_driver+0x1c8/0x278
             dt_cpufreq_probe+0xdc/0x1b8
             platform_drv_probe+0xb4/0x168
             driver_probe_device+0x318/0x4b0
             __device_attach_driver+0xfc/0x1f0
             bus_for_each_drv+0xf8/0x180
             __device_attach+0x164/0x200
             device_initial_probe+0x10/0x18
             bus_probe_device+0x110/0x178
             device_add+0x6d8/0x908
             platform_device_add+0x138/0x3d8
             platform_device_register_full+0x1cc/0x1f8
             cpufreq_dt_platdev_init+0x174/0x1bc
             do_one_initcall+0xb8/0x310
             kernel_init_freeable+0x4b8/0x56c
             kernel_init+0x10/0x138
             ret_from_fork+0x10/0x18
      -> #1 (subsys mutex#6){+.+.}:
             lock_acquire+0xb8/0x148
             __mutex_lock+0x104/0xf50
             mutex_lock_nested+0x1c/0x28
             subsys_interface_register+0xd8/0x270
             cpufreq_register_driver+0x1c8/0x278
             dt_cpufreq_probe+0xdc/0x1b8
             platform_drv_probe+0xb4/0x168
             driver_probe_device+0x318/0x4b0
             __device_attach_driver+0xfc/0x1f0
             bus_for_each_drv+0xf8/0x180
             __device_attach+0x164/0x200
             device_initial_probe+0x10/0x18
             bus_probe_device+0x110/0x178
             device_add+0x6d8/0x908
             platform_device_add+0x138/0x3d8
             platform_device_register_full+0x1cc/0x1f8
             cpufreq_dt_platdev_init+0x174/0x1bc
             do_one_initcall+0xb8/0x310
             kernel_init_freeable+0x4b8/0x56c
             kernel_init+0x10/0x138
             ret_from_fork+0x10/0x18
      -> #0 (cpu_hotplug_lock.rw_sem){++++}:
             __lock_acquire+0x203c/0x21d0
             lock_acquire+0xb8/0x148
             cpus_read_lock+0x58/0x1c8
             static_key_enable+0x14/0x30
             sched_feat_write+0x314/0x428
             full_proxy_write+0xa0/0x138
             __vfs_write+0xd8/0x388
             vfs_write+0xdc/0x318
             ksys_write+0xb4/0x138
             sys_write+0xc/0x18
             __sys_trace_return+0x0/0x4
      other info that might help us debug this:
      Chain exists of:
        cpu_hotplug_lock.rw_sem --> opp_table_lock --> &sb->s_type->i_mutex_key#3
       Possible unsafe locking scenario:
             CPU0                    CPU1
             ----                    ----
        lock(&sb->s_type->i_mutex_key#3);
                                     lock(opp_table_lock);
                                     lock(&sb->s_type->i_mutex_key#3);
        lock(cpu_hotplug_lock.rw_sem);
       *** DEADLOCK ***
      2 locks held by sh/3358:
       #0: 00000000a8c4b363 (sb_writers#10){.+.+}, at: vfs_write+0x238/0x318
       #1: 00000000c1b31a88 (&sb->s_type->i_mutex_key#3){+.+.}, at: sched_feat_write+0x160/0x428
      stack backtrace:
      CPU: 5 PID: 3358 Comm: sh Not tainted 4.18.0-rc6-00152-gcd3f77d74ac3-dirty #18
      Hardware name: Renesas H3ULCB Kingfisher board based on r8a7795 ES2.0+ (DT)
      Call trace:
       dump_backtrace+0x0/0x288
       show_stack+0x14/0x20
       dump_stack+0x13c/0x1ac
       print_circular_bug.isra.10+0x270/0x438
       check_prev_add.constprop.16+0x4dc/0xb98
       __lock_acquire+0x203c/0x21d0
       lock_acquire+0xb8/0x148
       cpus_read_lock+0x58/0x1c8
       static_key_enable+0x14/0x30
       sched_feat_write+0x314/0x428
       full_proxy_write+0xa0/0x138
       __vfs_write+0xd8/0x388
       vfs_write+0xdc/0x318
       ksys_write+0xb4/0x138
       sys_write+0xc/0x18
       __sys_trace_return+0x0/0x4
    
    This is because when loading the cpufreq_dt module we first acquire
    cpu_hotplug_lock.rw_sem lock, then in cpufreq_init(), we are taking
    the &sb->s_type->i_mutex_key lock.
    
    But when writing to /sys/kernel/debug/sched_features, the
    cpu_hotplug_lock.rw_sem lock depends on the &sb->s_type->i_mutex_key lock.
    
    To fix this bug, reverse the lock acquisition order when writing to
    sched_features, this way cpu_hotplug_lock.rw_sem no longer depends on
    &sb->s_type->i_mutex_key.
    
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Jiada Wang <jiada_wang@mentor.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Eugeniu Rosca <erosca@de.adit-jv.com>
    Cc: George G. Davis <george_davis@mentor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180731121222.26195-1-jiada_wang@mentor.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 60caf1fb94e0..6383aa6a60ca 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -89,12 +89,12 @@ struct static_key sched_feat_keys[__SCHED_FEAT_NR] = {
 
 static void sched_feat_disable(int i)
 {
-	static_key_disable(&sched_feat_keys[i]);
+	static_key_disable_cpuslocked(&sched_feat_keys[i]);
 }
 
 static void sched_feat_enable(int i)
 {
-	static_key_enable(&sched_feat_keys[i]);
+	static_key_enable_cpuslocked(&sched_feat_keys[i]);
 }
 #else
 static void sched_feat_disable(int i) { };
@@ -146,9 +146,11 @@ sched_feat_write(struct file *filp, const char __user *ubuf,
 
 	/* Ensure the static_key remains in a consistent state */
 	inode = file_inode(filp);
+	cpus_read_lock();
 	inode_lock(inode);
 	ret = sched_feat_set(cmp);
 	inode_unlock(inode);
+	cpus_read_unlock();
 	if (ret < 0)
 		return ret;
 

commit 13e091b6dd0e78a518a7d8756607d3acb8215768
Merge: eac341194426 1088c6eef261
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 13 18:28:19 2018 -0700

    Merge branch 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull x86 timer updates from Thomas Gleixner:
     "Early TSC based time stamping to allow better boot time analysis.
    
      This comes with a general cleanup of the TSC calibration code which
      grew warts and duct taping over the years and removes 250 lines of
      code. Initiated and mostly implemented by Pavel with help from various
      folks"
    
    * 'x86-timers-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (37 commits)
      x86/kvmclock: Mark kvm_get_preset_lpj() as __init
      x86/tsc: Consolidate init code
      sched/clock: Disable interrupts when calling generic_sched_clock_init()
      timekeeping: Prevent false warning when persistent clock is not available
      sched/clock: Close a hole in sched_clock_init()
      x86/tsc: Make use of tsc_calibrate_cpu_early()
      x86/tsc: Split native_calibrate_cpu() into early and late parts
      sched/clock: Use static key for sched_clock_running
      sched/clock: Enable sched clock early
      sched/clock: Move sched clock initialization and merge with generic clock
      x86/tsc: Use TSC as sched clock early
      x86/tsc: Initialize cyc2ns when tsc frequency is determined
      x86/tsc: Calibrate tsc only once
      ARM/time: Remove read_boot_clock64()
      s390/time: Remove read_boot_clock64()
      timekeeping: Default boot time offset to local_clock()
      timekeeping: Replace read_boot_clock64() with read_persistent_wall_and_boot_offset()
      s390/time: Add read_persistent_wall_and_boot_offset()
      x86/xen/time: Output xen sched_clock time from 0
      x86/xen/time: Initialize pv xen time in init_hypervisor_platform()
      ...

commit 67d9f6c256cd66e15f85c92670f52a7ad4689cff
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Wed Jun 20 22:32:47 2018 +0530

    sched/debug: Reverse the order of printing faults
    
    Fix the order in which the private and shared numa faults are getting
    printed.
    
    No functional changes.
    
    Running SPECjbb2005 on a 4 node machine and comparing bops/JVM
    JVMS  LAST_PATCH  WITH_PATCH  %CHANGE
    16    25215.7     25375.3     0.63
    1     72107       72617       0.70
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Rik van Riel <riel@surriel.com>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1529514181-9842-7-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index c96e89cc4bc7..870d4f3da285 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -842,8 +842,8 @@ void print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
 		unsigned long tpf, unsigned long gsf, unsigned long gpf)
 {
 	SEQ_printf(m, "numa_faults node=%d ", node);
-	SEQ_printf(m, "task_private=%lu task_shared=%lu ", tsf, tpf);
-	SEQ_printf(m, "group_private=%lu group_shared=%lu\n", gsf, gpf);
+	SEQ_printf(m, "task_private=%lu task_shared=%lu ", tpf, tsf);
+	SEQ_printf(m, "group_private=%lu group_shared=%lu\n", gpf, gsf);
 }
 #endif
 

commit 46457ea464f5341d1f9dad8dd213805d45f7f117
Author: Pavel Tatashin <pasha.tatashin@oracle.com>
Date:   Thu Jul 19 16:55:43 2018 -0400

    sched/clock: Use static key for sched_clock_running
    
    sched_clock_running may be read every time sched_clock_cpu() is called.
    Yet, this variable is updated only twice during boot, and never changes
    again, therefore it is better to make it a static key.
    
    Signed-off-by: Pavel Tatashin <pasha.tatashin@oracle.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: steven.sistare@oracle.com
    Cc: daniel.m.jordan@oracle.com
    Cc: linux@armlinux.org.uk
    Cc: schwidefsky@de.ibm.com
    Cc: heiko.carstens@de.ibm.com
    Cc: john.stultz@linaro.org
    Cc: sboyd@codeaurora.org
    Cc: hpa@zytor.com
    Cc: douly.fnst@cn.fujitsu.com
    Cc: prarit@redhat.com
    Cc: feng.tang@intel.com
    Cc: pmladek@suse.com
    Cc: gnomes@lxorguk.ukuu.org.uk
    Cc: linux-s390@vger.kernel.org
    Cc: boris.ostrovsky@oracle.com
    Cc: jgross@suse.com
    Cc: pbonzini@redhat.com
    Link: https://lkml.kernel.org/r/20180719205545.16512-25-pasha.tatashin@oracle.com

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index e593b4118578..b0212f489a33 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -623,8 +623,6 @@ void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq)
 #undef PU
 }
 
-extern __read_mostly int sched_clock_running;
-
 static void print_cpu(struct seq_file *m, int cpu)
 {
 	struct rq *rq = cpu_rq(cpu);

commit 8f894bf47dc9e8b77166125a084a7217693a28cd
Author: Yisheng Xie <xieyisheng1@huawei.com>
Date:   Thu May 31 19:11:19 2018 +0800

    sched/debug: Use match_string() helper instead of open-coded logic
    
    match_string() returns the index of an array for a matching string,
    which can be used instead of the open coded variant.
    
    Signed-off-by: Yisheng Xie <xieyisheng1@huawei.com>
    Reviewed-by: Andy Shevchenko <andy.shevchenko@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lore.kernel.org/lkml/1527765086-19873-15-git-send-email-xieyisheng1@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index e593b4118578..c96e89cc4bc7 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -111,20 +111,19 @@ static int sched_feat_set(char *cmp)
 		cmp += 3;
 	}
 
-	for (i = 0; i < __SCHED_FEAT_NR; i++) {
-		if (strcmp(cmp, sched_feat_names[i]) == 0) {
-			if (neg) {
-				sysctl_sched_features &= ~(1UL << i);
-				sched_feat_disable(i);
-			} else {
-				sysctl_sched_features |= (1UL << i);
-				sched_feat_enable(i);
-			}
-			break;
-		}
+	i = match_string(sched_feat_names, __SCHED_FEAT_NR, cmp);
+	if (i < 0)
+		return i;
+
+	if (neg) {
+		sysctl_sched_features &= ~(1UL << i);
+		sched_feat_disable(i);
+	} else {
+		sysctl_sched_features |= (1UL << i);
+		sched_feat_enable(i);
 	}
 
-	return i;
+	return 0;
 }
 
 static ssize_t
@@ -133,7 +132,7 @@ sched_feat_write(struct file *filp, const char __user *ubuf,
 {
 	char buf[64];
 	char *cmp;
-	int i;
+	int ret;
 	struct inode *inode;
 
 	if (cnt > 63)
@@ -148,10 +147,10 @@ sched_feat_write(struct file *filp, const char __user *ubuf,
 	/* Ensure the static_key remains in a consistent state */
 	inode = file_inode(filp);
 	inode_lock(inode);
-	i = sched_feat_set(cmp);
+	ret = sched_feat_set(cmp);
 	inode_unlock(inode);
-	if (i == __SCHED_FEAT_NR)
-		return -EINVAL;
+	if (ret < 0)
+		return ret;
 
 	*ppos += cnt;
 

commit fddda2b7b521185f3aa018f9559eb33b0aee53a9
Author: Christoph Hellwig <hch@lst.de>
Date:   Fri Apr 13 19:44:18 2018 +0200

    proc: introduce proc_create_seq{,_data}
    
    Variants of proc_create{,_data} that directly take a struct seq_operations
    argument and drastically reduces the boilerplate code in the callers.
    
    All trivial callers converted over.
    
    Signed-off-by: Christoph Hellwig <hch@lst.de>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 15b10e210a6b..e593b4118578 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -823,35 +823,9 @@ static const struct seq_operations sched_debug_sops = {
 	.show		= sched_debug_show,
 };
 
-static int sched_debug_release(struct inode *inode, struct file *file)
-{
-	seq_release(inode, file);
-
-	return 0;
-}
-
-static int sched_debug_open(struct inode *inode, struct file *filp)
-{
-	int ret = 0;
-
-	ret = seq_open(filp, &sched_debug_sops);
-
-	return ret;
-}
-
-static const struct file_operations sched_debug_fops = {
-	.open		= sched_debug_open,
-	.read		= seq_read,
-	.llseek		= seq_lseek,
-	.release	= sched_debug_release,
-};
-
 static int __init init_sched_debug_procfs(void)
 {
-	struct proc_dir_entry *pe;
-
-	pe = proc_create("sched_debug", 0444, NULL, &sched_debug_fops);
-	if (!pe)
+	if (!proc_create_seq("sched_debug", 0444, NULL, &sched_debug_sops))
 		return -ENOMEM;
 	return 0;
 }

commit 46e0d28bdb8e6d00e27a0fe9e1d15df6098f0ffb
Merge: 86bbbebac193 b720342849fe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 2 11:49:41 2018 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main scheduler changes in this cycle were:
    
       - NUMA balancing improvements (Mel Gorman)
    
       - Further load tracking improvements (Patrick Bellasi)
    
       - Various NOHZ balancing cleanups and optimizations (Peter Zijlstra)
    
       - Improve blocked load handling, in particular we can now reduce and
         eventually stop periodic load updates on 'very idle' CPUs. (Vincent
         Guittot)
    
       - On isolated CPUs offload the final 1Hz scheduler tick as well, plus
         related cleanups and reorganization. (Frederic Weisbecker)
    
       - Core scheduler code cleanups (Ingo Molnar)"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (45 commits)
      sched/core: Update preempt_notifier_key to modern API
      sched/cpufreq: Rate limits for SCHED_DEADLINE
      sched/fair: Update util_est only on util_avg updates
      sched/cpufreq/schedutil: Use util_est for OPP selection
      sched/fair: Use util_est in LB and WU paths
      sched/fair: Add util_est on top of PELT
      sched/core: Remove TASK_ALL
      sched/completions: Use bool in try_wait_for_completion()
      sched/fair: Update blocked load when newly idle
      sched/fair: Move idle_balance()
      sched/nohz: Merge CONFIG_NO_HZ_COMMON blocks
      sched/fair: Move rebalance_domains()
      sched/nohz: Optimize nohz_idle_balance()
      sched/fair: Reduce the periodic update duration
      sched/nohz: Stop NOHZ stats when decayed
      sched/cpufreq: Provide migration hint
      sched/nohz: Clean up nohz enter/exit
      sched/fair: Update blocked load from NEWIDLE
      sched/fair: Add NOHZ stats balancing
      sched/fair: Restructure nohz_balance_kick()
      ...

commit e9ca267096674eadd1fd479279bcb58df1486049
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Mon Mar 19 14:35:55 2018 -0400

    sched/debug: Adjust newlines for better alignment
    
    Scheduler debug stats include newlines that display out of alignment
    when prefixed by timestamps.  For example, the dmesg utility:
    
      % echo t > /proc/sysrq-trigger
      % dmesg
      ...
      [   83.124251]
      runnable tasks:
       S           task   PID         tree-key  switches  prio     wait-time
      sum-exec        sum-sleep
      -----------------------------------------------------------------------------------------------------------
    
    At the same time, some syslog utilities (like rsyslog by default) don't
    like the additional newlines control characters, saving lines like this
    to /var/log/messages:
    
      Mar 16 16:02:29 localhost kernel: #012runnable tasks:#012 S           task   PID         tree-key ...
                                        ^^^^               ^^^^
    Clean these up by moving newline characters to their own SEQ_printf
    invocation.  This leaves the /proc/sched_debug unchanged, but brings the
    entire output into alignment when prefixed:
    
      % echo t > /proc/sysrq-trigger
      % dmesg
      ...
      [   62.410368] runnable tasks:
      [   62.410368]  S           task   PID         tree-key  switches  prio     wait-time             sum-exec        sum-sleep
      [   62.410369] -----------------------------------------------------------------------------------------------------------
      [   62.410369]  I  kworker/u12:0     5      1932.215593       332   120         0.000000         3.621252         0.000000 0 0 /
    
    and no escaped control characters from rsyslog in /var/log/messages:
    
      Mar 16 16:15:06 localhost kernel: runnable tasks:
      Mar 16 16:15:06 localhost kernel: S           task   PID         tree-key  ...
    
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1521484555-8620-3-git-send-email-joe.lawrence@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 50026aa2d81e..72c401b3b15c 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -501,12 +501,12 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 {
 	struct task_struct *g, *p;
 
-	SEQ_printf(m,
-	"\nrunnable tasks:\n"
-	" S           task   PID         tree-key  switches  prio"
-	"     wait-time             sum-exec        sum-sleep\n"
-	"-------------------------------------------------------"
-	"----------------------------------------------------\n");
+	SEQ_printf(m, "\n");
+	SEQ_printf(m, "runnable tasks:\n");
+	SEQ_printf(m, " S           task   PID         tree-key  switches  prio"
+		   "     wait-time             sum-exec        sum-sleep\n");
+	SEQ_printf(m, "-------------------------------------------------------"
+		   "----------------------------------------------------\n");
 
 	rcu_read_lock();
 	for_each_process_thread(g, p) {
@@ -527,9 +527,11 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	unsigned long flags;
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	SEQ_printf(m, "\ncfs_rq[%d]:%s\n", cpu, task_group_path(cfs_rq->tg));
+	SEQ_printf(m, "\n");
+	SEQ_printf(m, "cfs_rq[%d]:%s\n", cpu, task_group_path(cfs_rq->tg));
 #else
-	SEQ_printf(m, "\ncfs_rq[%d]:\n", cpu);
+	SEQ_printf(m, "\n");
+	SEQ_printf(m, "cfs_rq[%d]:\n", cpu);
 #endif
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "exec_clock",
 			SPLIT_NS(cfs_rq->exec_clock));
@@ -595,9 +597,11 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)
 {
 #ifdef CONFIG_RT_GROUP_SCHED
-	SEQ_printf(m, "\nrt_rq[%d]:%s\n", cpu, task_group_path(rt_rq->tg));
+	SEQ_printf(m, "\n");
+	SEQ_printf(m, "rt_rq[%d]:%s\n", cpu, task_group_path(rt_rq->tg));
 #else
-	SEQ_printf(m, "\nrt_rq[%d]:\n", cpu);
+	SEQ_printf(m, "\n");
+	SEQ_printf(m, "rt_rq[%d]:\n", cpu);
 #endif
 
 #define P(x) \
@@ -624,7 +628,8 @@ void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq)
 {
 	struct dl_bw *dl_bw;
 
-	SEQ_printf(m, "\ndl_rq[%d]:\n", cpu);
+	SEQ_printf(m, "\n");
+	SEQ_printf(m, "dl_rq[%d]:\n", cpu);
 
 #define PU(x) \
 	SEQ_printf(m, "  .%-30s: %lu\n", #x, (unsigned long)(dl_rq->x))

commit a8c024cd9b9683d25ae1f459525dd2c6bec75e79
Author: Joe Lawrence <joe.lawrence@redhat.com>
Date:   Mon Mar 19 14:35:54 2018 -0400

    sched/debug: Fix per-task line continuation for console output
    
    When the SEQ_printf() macro prints to the console, it runs a simple
    printk() without KERN_CONT "continued" line printing.  The result of
    this is oddly wrapped task info, for example:
    
      % echo t > /proc/sysrq-trigger
      % dmesg
      ...
      runnable tasks:
      ...
      [   29.608611]  I
      [   29.608613]       rcu_sched     8      3252.013846      4087   120
      [   29.608614]         0.000000        29.090111         0.000000
      [   29.608615]  0 0
      [   29.608616]  /
    
    Modify SEQ_printf to use pr_cont() for expected one-line results:
    
      % echo t > /proc/sysrq-trigger
      % dmesg
      ...
      runnable tasks:
      ...
      [  106.716329]  S        cpuhp/5    37      2006.315026        14   120         0.000000         0.496893         0.000000 0 0 /
    
    Signed-off-by: Joe Lawrence <joe.lawrence@redhat.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1521484555-8620-2-git-send-email-joe.lawrence@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 1ca0130ed4f9..50026aa2d81e 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -32,7 +32,7 @@ static DEFINE_SPINLOCK(sched_debug_lock);
 	if (m)					\
 		seq_printf(m, x);		\
 	else					\
-		printk(x);			\
+		pr_cont(x);			\
  } while (0)
 
 /*

commit 7f65ea42eb00bc902f1c37a71e984e4f4064cfa9
Author: Patrick Bellasi <patrick.bellasi@arm.com>
Date:   Fri Mar 9 09:52:42 2018 +0000

    sched/fair: Add util_est on top of PELT
    
    The util_avg signal computed by PELT is too variable for some use-cases.
    For example, a big task waking up after a long sleep period will have its
    utilization almost completely decayed. This introduces some latency before
    schedutil will be able to pick the best frequency to run a task.
    
    The same issue can affect task placement. Indeed, since the task
    utilization is already decayed at wakeup, when the task is enqueued in a
    CPU, this can result in a CPU running a big task as being temporarily
    represented as being almost empty. This leads to a race condition where
    other tasks can be potentially allocated on a CPU which just started to run
    a big task which slept for a relatively long period.
    
    Moreover, the PELT utilization of a task can be updated every [ms], thus
    making it a continuously changing value for certain longer running
    tasks. This means that the instantaneous PELT utilization of a RUNNING
    task is not really meaningful to properly support scheduler decisions.
    
    For all these reasons, a more stable signal can do a better job of
    representing the expected/estimated utilization of a task/cfs_rq.
    Such a signal can be easily created on top of PELT by still using it as
    an estimator which produces values to be aggregated on meaningful
    events.
    
    This patch adds a simple implementation of util_est, a new signal built on
    top of PELT's util_avg where:
    
        util_est(task) = max(task::util_avg, f(task::util_avg@dequeue))
    
    This allows to remember how big a task has been reported by PELT in its
    previous activations via f(task::util_avg@dequeue), which is the new
    _task_util_est(struct task_struct*) function added by this patch.
    
    If a task should change its behavior and it runs longer in a new
    activation, after a certain time its util_est will just track the
    original PELT signal (i.e. task::util_avg).
    
    The estimated utilization of cfs_rq is defined only for root ones.
    That's because the only sensible consumer of this signal are the
    scheduler and schedutil when looking for the overall CPU utilization
    due to FAIR tasks.
    
    For this reason, the estimated utilization of a root cfs_rq is simply
    defined as:
    
        util_est(cfs_rq) = max(cfs_rq::util_avg, cfs_rq::util_est::enqueued)
    
    where:
    
        cfs_rq::util_est::enqueued = sum(_task_util_est(task))
                                     for each RUNNABLE task on that root cfs_rq
    
    It's worth noting that the estimated utilization is tracked only for
    objects of interests, specifically:
    
     - Tasks: to better support tasks placement decisions
     - root cfs_rqs: to better support both tasks placement decisions as
                     well as frequencies selection
    
    Signed-off-by: Patrick Bellasi <patrick.bellasi@arm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: Juri Lelli <juri.lelli@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Rafael J . Wysocki <rafael.j.wysocki@intel.com>
    Cc: Steve Muckle <smuckle@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Todd Kjos <tkjos@android.com>
    Cc: Vincent Guittot <vincent.guittot@linaro.org>
    Cc: Viresh Kumar <viresh.kumar@linaro.org>
    Link: http://lkml.kernel.org/r/20180309095245.11071-2-patrick.bellasi@arm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 644d9a464380..332303be4beb 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -541,6 +541,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->avg.runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lu\n", "util_avg",
 			cfs_rq->avg.util_avg);
+	SEQ_printf(m, "  .%-30s: %u\n", "util_est_enqueued",
+			cfs_rq->avg.util_est.enqueued);
 	SEQ_printf(m, "  .%-30s: %ld\n", "removed.load_avg",
 			cfs_rq->removed.load_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "removed.util_avg",
@@ -989,6 +991,8 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 	P(se.avg.runnable_load_avg);
 	P(se.avg.util_avg);
 	P(se.avg.last_update_time);
+	P(se.avg.util_est.ewma);
+	P(se.avg.util_est.enqueued);
 #endif
 	P(policy);
 	P(prio);

commit 325ea10c0809406ce23f038602abbc454f3f761d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 12:20:47 2018 +0100

    sched/headers: Simplify and clean up header usage in the scheduler
    
    Do the following cleanups and simplifications:
    
     - sched/sched.h already includes <asm/paravirt.h>, so no need to
       include it in sched/core.c again.
    
     - order the <linux/sched/*.h> headers alphabetically
    
     - add all <linux/sched/*.h> headers to kernel/sched/sched.h
    
     - remove all unnecessary includes from the .c files that
       are already included in kernel/sched/sched.h.
    
    Finally, make all scheduler .c files use a single common header:
    
      #include "sched.h"
    
    ... which now contains a union of the relied upon headers.
    
    This makes the various .c files easier to read and easier to handle.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 7c82a9b88510..644d9a464380 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -1,7 +1,7 @@
 /*
  * kernel/sched/debug.c
  *
- * Print the CFS rbtree
+ * Print the CFS rbtree and other debugging details
  *
  * Copyright(C) 2007, Red Hat, Inc., Ingo Molnar
  *
@@ -9,15 +9,6 @@
  * it under the terms of the GNU General Public License version 2 as
  * published by the Free Software Foundation.
  */
-#include <linux/proc_fs.h>
-#include <linux/sched/mm.h>
-#include <linux/sched/task.h>
-#include <linux/seq_file.h>
-#include <linux/kallsyms.h>
-#include <linux/utsname.h>
-#include <linux/mempolicy.h>
-#include <linux/debugfs.h>
-
 #include "sched.h"
 
 static DEFINE_SPINLOCK(sched_debug_lock);

commit 97fb7a0a8944bd6d2c5634e1e0fa689a5c40bc22
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 14:01:12 2018 +0100

    sched: Clean up and harmonize the coding style of the scheduler code base
    
    A good number of small style inconsistencies have accumulated
    in the scheduler core, so do a pass over them to harmonize
    all these details:
    
     - fix speling in comments,
    
     - use curly braces for multi-line statements,
    
     - remove unnecessary parentheses from integer literals,
    
     - capitalize consistently,
    
     - remove stray newlines,
    
     - add comments where necessary,
    
     - remove invalid/unnecessary comments,
    
     - align structure definitions and other data types vertically,
    
     - add missing newlines for increased readability,
    
     - fix vertical tabulation where it's misaligned,
    
     - harmonize preprocessor conditional block labeling
       and vertical alignment,
    
     - remove line-breaks where they uglify the code,
    
     - add newline after local variable definitions,
    
    No change in functionality:
    
      md5:
         1191fa0a890cfa8132156d2959d7e9e2  built-in.o.before.asm
         1191fa0a890cfa8132156d2959d7e9e2  built-in.o.after.asm
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 1ca0130ed4f9..7c82a9b88510 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -9,7 +9,6 @@
  * it under the terms of the GNU General Public License version 2 as
  * published by the Free Software Foundation.
  */
-
 #include <linux/proc_fs.h>
 #include <linux/sched/mm.h>
 #include <linux/sched/task.h>
@@ -274,34 +273,19 @@ sd_alloc_ctl_domain_table(struct sched_domain *sd)
 	if (table == NULL)
 		return NULL;
 
-	set_table_entry(&table[0], "min_interval", &sd->min_interval,
-		sizeof(long), 0644, proc_doulongvec_minmax, false);
-	set_table_entry(&table[1], "max_interval", &sd->max_interval,
-		sizeof(long), 0644, proc_doulongvec_minmax, false);
-	set_table_entry(&table[2], "busy_idx", &sd->busy_idx,
-		sizeof(int), 0644, proc_dointvec_minmax, true);
-	set_table_entry(&table[3], "idle_idx", &sd->idle_idx,
-		sizeof(int), 0644, proc_dointvec_minmax, true);
-	set_table_entry(&table[4], "newidle_idx", &sd->newidle_idx,
-		sizeof(int), 0644, proc_dointvec_minmax, true);
-	set_table_entry(&table[5], "wake_idx", &sd->wake_idx,
-		sizeof(int), 0644, proc_dointvec_minmax, true);
-	set_table_entry(&table[6], "forkexec_idx", &sd->forkexec_idx,
-		sizeof(int), 0644, proc_dointvec_minmax, true);
-	set_table_entry(&table[7], "busy_factor", &sd->busy_factor,
-		sizeof(int), 0644, proc_dointvec_minmax, false);
-	set_table_entry(&table[8], "imbalance_pct", &sd->imbalance_pct,
-		sizeof(int), 0644, proc_dointvec_minmax, false);
-	set_table_entry(&table[9], "cache_nice_tries",
-		&sd->cache_nice_tries,
-		sizeof(int), 0644, proc_dointvec_minmax, false);
-	set_table_entry(&table[10], "flags", &sd->flags,
-		sizeof(int), 0644, proc_dointvec_minmax, false);
-	set_table_entry(&table[11], "max_newidle_lb_cost",
-		&sd->max_newidle_lb_cost,
-		sizeof(long), 0644, proc_doulongvec_minmax, false);
-	set_table_entry(&table[12], "name", sd->name,
-		CORENAME_MAX_SIZE, 0444, proc_dostring, false);
+	set_table_entry(&table[0] , "min_interval",	   &sd->min_interval,	     sizeof(long), 0644, proc_doulongvec_minmax, false);
+	set_table_entry(&table[1] , "max_interval",	   &sd->max_interval,	     sizeof(long), 0644, proc_doulongvec_minmax, false);
+	set_table_entry(&table[2] , "busy_idx",		   &sd->busy_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
+	set_table_entry(&table[3] , "idle_idx",		   &sd->idle_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
+	set_table_entry(&table[4] , "newidle_idx",	   &sd->newidle_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
+	set_table_entry(&table[5] , "wake_idx",		   &sd->wake_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
+	set_table_entry(&table[6] , "forkexec_idx",	   &sd->forkexec_idx,	     sizeof(int) , 0644, proc_dointvec_minmax,   true );
+	set_table_entry(&table[7] , "busy_factor",	   &sd->busy_factor,	     sizeof(int) , 0644, proc_dointvec_minmax,   false);
+	set_table_entry(&table[8] , "imbalance_pct",	   &sd->imbalance_pct,	     sizeof(int) , 0644, proc_dointvec_minmax,   false);
+	set_table_entry(&table[9] , "cache_nice_tries",	   &sd->cache_nice_tries,    sizeof(int) , 0644, proc_dointvec_minmax,   false);
+	set_table_entry(&table[10], "flags",		   &sd->flags,		     sizeof(int) , 0644, proc_dointvec_minmax,   false);
+	set_table_entry(&table[11], "max_newidle_lb_cost", &sd->max_newidle_lb_cost, sizeof(long), 0644, proc_doulongvec_minmax, false);
+	set_table_entry(&table[12], "name",		   sd->name,		CORENAME_MAX_SIZE, 0444, proc_dostring,		 false);
 	/* &table[13] is terminator */
 
 	return table;
@@ -332,8 +316,8 @@ static struct ctl_table *sd_alloc_ctl_cpu_table(int cpu)
 	return table;
 }
 
-static cpumask_var_t sd_sysctl_cpus;
-static struct ctl_table_header *sd_sysctl_header;
+static cpumask_var_t		sd_sysctl_cpus;
+static struct ctl_table_header	*sd_sysctl_header;
 
 void register_sched_domain_sysctl(void)
 {
@@ -413,14 +397,10 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 {
 	struct sched_entity *se = tg->se[cpu];
 
-#define P(F) \
-	SEQ_printf(m, "  .%-30s: %lld\n", #F, (long long)F)
-#define P_SCHEDSTAT(F) \
-	SEQ_printf(m, "  .%-30s: %lld\n", #F, (long long)schedstat_val(F))
-#define PN(F) \
-	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)F))
-#define PN_SCHEDSTAT(F) \
-	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)schedstat_val(F)))
+#define P(F)		SEQ_printf(m, "  .%-30s: %lld\n",	#F, (long long)F)
+#define P_SCHEDSTAT(F)	SEQ_printf(m, "  .%-30s: %lld\n",	#F, (long long)schedstat_val(F))
+#define PN(F)		SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)F))
+#define PN_SCHEDSTAT(F)	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)schedstat_val(F)))
 
 	if (!se)
 		return;
@@ -428,6 +408,7 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	PN(se->exec_start);
 	PN(se->vruntime);
 	PN(se->sum_exec_runtime);
+
 	if (schedstat_enabled()) {
 		PN_SCHEDSTAT(se->statistics.wait_start);
 		PN_SCHEDSTAT(se->statistics.sleep_start);
@@ -440,6 +421,7 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 		PN_SCHEDSTAT(se->statistics.wait_sum);
 		P_SCHEDSTAT(se->statistics.wait_count);
 	}
+
 	P(se->load.weight);
 	P(se->runnable_weight);
 #ifdef CONFIG_SMP
@@ -464,6 +446,7 @@ static char *task_group_path(struct task_group *tg)
 		return group_path;
 
 	cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
+
 	return group_path;
 }
 #endif
@@ -799,9 +782,9 @@ void sysrq_sched_debug_show(void)
 /*
  * This itererator needs some explanation.
  * It returns 1 for the header position.
- * This means 2 is cpu 0.
- * In a hotplugged system some cpus, including cpu 0, may be missing so we have
- * to use cpumask_* to iterate over the cpus.
+ * This means 2 is CPU 0.
+ * In a hotplugged system some CPUs, including CPU 0, may be missing so we have
+ * to use cpumask_* to iterate over the CPUs.
  */
 static void *sched_debug_start(struct seq_file *file, loff_t *offset)
 {
@@ -821,6 +804,7 @@ static void *sched_debug_start(struct seq_file *file, loff_t *offset)
 
 	if (n < nr_cpu_ids)
 		return (void *)(unsigned long)(n + 2);
+
 	return NULL;
 }
 
@@ -835,10 +819,10 @@ static void sched_debug_stop(struct seq_file *file, void *data)
 }
 
 static const struct seq_operations sched_debug_sops = {
-	.start = sched_debug_start,
-	.next = sched_debug_next,
-	.stop = sched_debug_stop,
-	.show = sched_debug_show,
+	.start		= sched_debug_start,
+	.next		= sched_debug_next,
+	.stop		= sched_debug_stop,
+	.show		= sched_debug_show,
 };
 
 static int sched_debug_release(struct inode *inode, struct file *file)
@@ -876,14 +860,10 @@ static int __init init_sched_debug_procfs(void)
 
 __initcall(init_sched_debug_procfs);
 
-#define __P(F) \
-	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)F)
-#define P(F) \
-	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)p->F)
-#define __PN(F) \
-	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
-#define PN(F) \
-	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
+#define __P(F)	SEQ_printf(m, "%-45s:%21Ld\n",	     #F, (long long)F)
+#define   P(F)	SEQ_printf(m, "%-45s:%21Ld\n",	     #F, (long long)p->F)
+#define __PN(F)	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
+#define   PN(F)	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
 
 
 #ifdef CONFIG_NUMA_BALANCING

commit 1ea6c46a23f1213d1972bfae220db5c165e27bba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Sat May 6 15:59:54 2017 +0200

    sched/fair: Propagate an effective runnable_load_avg
    
    The load balancer uses runnable_load_avg as load indicator. For
    !cgroup this is:
    
      runnable_load_avg = \Sum se->avg.load_avg ; where se->on_rq
    
    That is, a direct sum of all runnable tasks on that runqueue. As
    opposed to load_avg, which is a sum of all tasks on the runqueue,
    which includes a blocked component.
    
    However, in the cgroup case, this comes apart since the group entities
    are always runnable, even if most of their constituent entities are
    blocked.
    
    Therefore introduce a runnable_weight which for task entities is the
    same as the regular weight, but for group entities is a fraction of
    the entity weight and represents the runnable part of the group
    runqueue.
    
    Then propagate this load through the PELT hierarchy to arrive at an
    effective runnable load avgerage -- which we should not confuse with
    the canonical runnable load average.
    
    Suggested-by: Tejun Heo <tj@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2e039a81864c..1ca0130ed4f9 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -441,9 +441,11 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 		P_SCHEDSTAT(se->statistics.wait_count);
 	}
 	P(se->load.weight);
+	P(se->runnable_weight);
 #ifdef CONFIG_SMP
 	P(se->avg.load_avg);
 	P(se->avg.util_avg);
+	P(se->avg.runnable_load_avg);
 #endif
 
 #undef PN_SCHEDSTAT
@@ -558,10 +560,11 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", cfs_rq->nr_running);
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
 #ifdef CONFIG_SMP
+	SEQ_printf(m, "  .%-30s: %ld\n", "runnable_weight", cfs_rq->runnable_weight);
 	SEQ_printf(m, "  .%-30s: %lu\n", "load_avg",
 			cfs_rq->avg.load_avg);
 	SEQ_printf(m, "  .%-30s: %lu\n", "runnable_load_avg",
-			cfs_rq->runnable_load_avg);
+			cfs_rq->avg.runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lu\n", "util_avg",
 			cfs_rq->avg.util_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "removed.load_avg",
@@ -1006,10 +1009,13 @@ void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
 		   "nr_involuntary_switches", (long long)p->nivcsw);
 
 	P(se.load.weight);
+	P(se.runnable_weight);
 #ifdef CONFIG_SMP
 	P(se.avg.load_sum);
+	P(se.avg.runnable_load_sum);
 	P(se.avg.util_sum);
 	P(se.avg.load_avg);
+	P(se.avg.runnable_load_avg);
 	P(se.avg.util_avg);
 	P(se.avg.last_update_time);
 #endif

commit 0e2d2aaaae52c247c047d14999b93486bdbd3431
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 8 17:30:46 2017 +0200

    sched/fair: Rewrite PELT migration propagation
    
    When an entity migrates in (or out) of a runqueue, we need to add (or
    remove) its contribution from the entire PELT hierarchy, because even
    non-runnable entities are included in the load average sums.
    
    In order to do this we have some propagation logic that updates the
    PELT tree, however the way it 'propagates' the runnable (or load)
    change is (more or less):
    
                         tg->weight * grq->avg.load_avg
      ge->avg.load_avg = ------------------------------
                                   tg->load_avg
    
    But that is the expression for ge->weight, and per the definition of
    load_avg:
    
      ge->avg.load_avg := ge->weight * ge->avg.runnable_avg
    
    That destroys the runnable_avg (by setting it to 1) we wanted to
    propagate.
    
    Instead directly propagate runnable_sum.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2f22342c48ff..2e039a81864c 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -568,6 +568,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->removed.load_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "removed.util_avg",
 			cfs_rq->removed.util_avg);
+	SEQ_printf(m, "  .%-30s: %ld\n", "removed.runnable_sum",
+			cfs_rq->removed.runnable_sum);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	SEQ_printf(m, "  .%-30s: %lu\n", "tg_load_avg_contrib",
 			cfs_rq->tg_load_avg_contrib);

commit 2a2f5d4e44ed160a5ed822c94e04f918f9fbb487
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon May 8 16:51:41 2017 +0200

    sched/fair: Rewrite cfs_rq->removed_*avg
    
    Since on wakeup migration we don't hold the rq->lock for the old CPU
    we cannot update its state. Instead we add the removed 'load' to an
    atomic variable and have the next update on that CPU collect and
    process it.
    
    Currently we have 2 atomic variables; which already have the issue
    that they can be read out-of-sync. Also, two atomic ops on a single
    cacheline is already more expensive than an uncontended lock.
    
    Since we want to add more, convert the thing over to an explicit
    cacheline with a lock in.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2f93e4a2d9f6..2f22342c48ff 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -564,10 +564,10 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lu\n", "util_avg",
 			cfs_rq->avg.util_avg);
-	SEQ_printf(m, "  .%-30s: %ld\n", "removed_load_avg",
-			atomic_long_read(&cfs_rq->removed_load_avg));
-	SEQ_printf(m, "  .%-30s: %ld\n", "removed_util_avg",
-			atomic_long_read(&cfs_rq->removed_util_avg));
+	SEQ_printf(m, "  .%-30s: %ld\n", "removed.load_avg",
+			cfs_rq->removed.load_avg);
+	SEQ_printf(m, "  .%-30s: %ld\n", "removed.util_avg",
+			cfs_rq->removed.util_avg);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	SEQ_printf(m, "  .%-30s: %lu\n", "tg_load_avg_contrib",
 			cfs_rq->tg_load_avg_contrib);

commit 65d5dc47fe8530e17e318722fb4df676536c2bfc
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Sep 22 18:14:08 2017 +0200

    sched/debug: Remove unused variable
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 01217fb5a5de..2f93e4a2d9f6 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -466,8 +466,6 @@ static char *task_group_path(struct task_group *tg)
 }
 #endif
 
-static const char stat_nam[] = TASK_STATE_TO_CHAR_STR;
-
 static void
 print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 {

commit ec846ecd6350857a8b8b9a6b78c763d45e0f09b8
Merge: b5df1b3a5637 9469eb01db89
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 13 12:22:32 2017 -0700

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar:
     "Three CPU hotplug related fixes and a debugging improvement"
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      sched/debug: Add debugfs knob for "sched_debug"
      sched/core: WARN() when migrating to an offline CPU
      sched/fair: Plug hole between hotplug and active_load_balance()
      sched/fair: Avoid newidle balance for !active CPUs

commit 9469eb01db891b55367ee7539f1b9f7f6fd2819d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 7 17:03:53 2017 +0200

    sched/debug: Add debugfs knob for "sched_debug"
    
    I'm forever late for editing my kernel cmdline, add a runtime knob to
    disable the "sched_debug" thing.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170907150614.142924283@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 4a23bbc3111b..b19d06ea6e10 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -181,11 +181,16 @@ static const struct file_operations sched_feat_fops = {
 	.release	= single_release,
 };
 
+__read_mostly bool sched_debug_enabled;
+
 static __init int sched_init_debug(void)
 {
 	debugfs_create_file("sched_features", 0644, NULL, NULL,
 			&sched_feat_fops);
 
+	debugfs_create_bool("sched_debug", 0644, NULL,
+			&sched_debug_enabled);
+
 	return 0;
 }
 late_initcall(sched_init_debug);

commit bfb068892d30dcf0a32b89302fe293347adeaaaa
Author: Davidlohr Bueso <dave@stgolabs.net>
Date:   Fri Sep 8 16:14:55 2017 -0700

    sched/fair: replace cfs_rq->rb_leftmost
    
    ... with the generic rbtree flavor instead. No changes
    in semantics whatsoever.
    
    Link: http://lkml.kernel.org/r/20170719014603.19029-8-dave@stgolabs.net
    Signed-off-by: Davidlohr Bueso <dbueso@suse.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 4a23bbc3111b..8e536d963652 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -530,7 +530,7 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			SPLIT_NS(cfs_rq->exec_clock));
 
 	raw_spin_lock_irqsave(&rq->lock, flags);
-	if (cfs_rq->rb_leftmost)
+	if (rb_first_cached(&cfs_rq->tasks_timeline))
 		MIN_vruntime = (__pick_first_entity(cfs_rq))->vruntime;
 	last = __pick_last_entity(cfs_rq);
 	if (last)

commit bbdacdfed2f5fa50a2cc9f500a36e05990a0837d
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Aug 10 17:10:26 2017 +0200

    sched/debug: Optimize sched_domain sysctl generation
    
    Currently we unconditionally destroy all sysctl bits and regenerate
    them after we've rebuild the domains (even if that rebuild is a
    no-op).
    
    And since we unconditionally (re)build the sysctl for all possible
    CPUs, onlining all CPUs gets us O(n^2) time. Instead change this to
    only rebuild the bits for CPUs we've actually installed new domains
    on.
    
    Reported-by: Ofer Levi(SW) <oferle@mellanox.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index cfd84f79e075..4a23bbc3111b 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -327,38 +327,78 @@ static struct ctl_table *sd_alloc_ctl_cpu_table(int cpu)
 	return table;
 }
 
+static cpumask_var_t sd_sysctl_cpus;
 static struct ctl_table_header *sd_sysctl_header;
+
 void register_sched_domain_sysctl(void)
 {
-	int i, cpu_num = num_possible_cpus();
-	struct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);
+	static struct ctl_table *cpu_entries;
+	static struct ctl_table **cpu_idx;
 	char buf[32];
+	int i;
 
-	WARN_ON(sd_ctl_dir[0].child);
-	sd_ctl_dir[0].child = entry;
+	if (!cpu_entries) {
+		cpu_entries = sd_alloc_ctl_entry(num_possible_cpus() + 1);
+		if (!cpu_entries)
+			return;
 
-	if (entry == NULL)
-		return;
+		WARN_ON(sd_ctl_dir[0].child);
+		sd_ctl_dir[0].child = cpu_entries;
+	}
 
-	for_each_possible_cpu(i) {
-		snprintf(buf, 32, "cpu%d", i);
-		entry->procname = kstrdup(buf, GFP_KERNEL);
-		entry->mode = 0555;
-		entry->child = sd_alloc_ctl_cpu_table(i);
-		entry++;
+	if (!cpu_idx) {
+		struct ctl_table *e = cpu_entries;
+
+		cpu_idx = kcalloc(nr_cpu_ids, sizeof(struct ctl_table*), GFP_KERNEL);
+		if (!cpu_idx)
+			return;
+
+		/* deal with sparse possible map */
+		for_each_possible_cpu(i) {
+			cpu_idx[i] = e;
+			e++;
+		}
+	}
+
+	if (!cpumask_available(sd_sysctl_cpus)) {
+		if (!alloc_cpumask_var(&sd_sysctl_cpus, GFP_KERNEL))
+			return;
+
+		/* init to possible to not have holes in @cpu_entries */
+		cpumask_copy(sd_sysctl_cpus, cpu_possible_mask);
+	}
+
+	for_each_cpu(i, sd_sysctl_cpus) {
+		struct ctl_table *e = cpu_idx[i];
+
+		if (e->child)
+			sd_free_ctl_entry(&e->child);
+
+		if (!e->procname) {
+			snprintf(buf, 32, "cpu%d", i);
+			e->procname = kstrdup(buf, GFP_KERNEL);
+		}
+		e->mode = 0555;
+		e->child = sd_alloc_ctl_cpu_table(i);
+
+		__cpumask_clear_cpu(i, sd_sysctl_cpus);
 	}
 
 	WARN_ON(sd_sysctl_header);
 	sd_sysctl_header = register_sysctl_table(sd_ctl_root);
 }
 
+void dirty_sched_domain_sysctl(int cpu)
+{
+	if (cpumask_available(sd_sysctl_cpus))
+		__cpumask_set_cpu(cpu, sd_sysctl_cpus);
+}
+
 /* may be called multiple times per register */
 void unregister_sched_domain_sysctl(void)
 {
 	unregister_sysctl_table(sd_sysctl_header);
 	sd_sysctl_header = NULL;
-	if (sd_ctl_dir[0].child)
-		sd_free_ctl_entry(&sd_ctl_dir[0].child);
 }
 #endif /* CONFIG_SYSCTL */
 #endif /* CONFIG_SMP */

commit 20435d84e5f2041c64c792399ab6f2948a2c2252
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Mon Aug 7 16:44:23 2017 +0800

    sched/debug: Intruduce task_state_to_char() helper function
    
    Now that we have more than one place to get the task state,
    intruduce the task_state_to_char() helper function to save some code.
    
    No functionality changed.
    
    Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <cj.chengjian@huawei.com>
    Cc: <huawei.libin@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1502095463-160172-3-git-send-email-xiexiuqi@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index d8d2ea215b85..cfd84f79e075 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -426,14 +426,10 @@ static const char stat_nam[] = TASK_STATE_TO_CHAR_STR;
 static void
 print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 {
-	unsigned long state;
-
-	if (rq->curr == p) {
+	if (rq->curr == p)
 		SEQ_printf(m, ">R");
-	} else {
-		state = p->state ? __ffs(p->state) + 1 : 0;
-		SEQ_printf(m, " %c", state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');
-	}
+	else
+		SEQ_printf(m, " %c", task_state_to_char(p));
 
 	SEQ_printf(m, "%15s %5d %9Ld.%06ld %9Ld %5d ",
 		p->comm, task_pid_nr(p),

commit e8c164954b926f06f109a42fb8595ed01275b141
Author: Xie XiuQi <xiexiuqi@huawei.com>
Date:   Mon Aug 7 16:44:22 2017 +0800

    sched/debug: Show task state in /proc/sched_debug
    
    Currently we print the runnable task in /proc/sched_debug, but
    there is no task state information.
    
    We don't know which task is in the runqueue and which task is sleeping.
    
    Add task state in the runnable task list, like this:
    
      runnable tasks:
       S           task   PID         tree-key  switches  prio     wait-time             sum-exec        sum-sleep
      -----------------------------------------------------------------------------------------------------------
       S   watchdog/239  1452       -11.917445      2811     0         0.000000         8.949306         0.000000 7 0 /
       S  migration/239  1453     20686.367740         8     0         0.000000     16215.720897         0.000000 7 0 /
       S  ksoftirqd/239  1454    115383.841071        12   120         0.000000         0.200683         0.000000 7 0 /
      >R           test 21287      4872.190970       407   120         0.000000      4874.911790         0.000000 7 0 /autogroup-150
       R           test 21288      4868.385454       401   120         0.000000      3672.341489         0.000000 7 0 /autogroup-150
       R           test 21289      4868.326776       384   120         0.000000      3424.934159         0.000000 7 0 /autogroup-150
    
    Signed-off-by: Xie XiuQi <xiexiuqi@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: <cj.chengjian@huawei.com>
    Cc: <huawei.libin@huawei.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1502095463-160172-2-git-send-email-xiexiuqi@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index ac345115877b..d8d2ea215b85 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -421,13 +421,19 @@ static char *task_group_path(struct task_group *tg)
 }
 #endif
 
+static const char stat_nam[] = TASK_STATE_TO_CHAR_STR;
+
 static void
 print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 {
-	if (rq->curr == p)
-		SEQ_printf(m, "R");
-	else
-		SEQ_printf(m, " ");
+	unsigned long state;
+
+	if (rq->curr == p) {
+		SEQ_printf(m, ">R");
+	} else {
+		state = p->state ? __ffs(p->state) + 1 : 0;
+		SEQ_printf(m, " %c", state < sizeof(stat_nam) - 1 ? stat_nam[state] : '?');
+	}
 
 	SEQ_printf(m, "%15s %5d %9Ld.%06ld %9Ld %5d ",
 		p->comm, task_pid_nr(p),
@@ -456,9 +462,9 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 
 	SEQ_printf(m,
 	"\nrunnable tasks:\n"
-	"            task   PID         tree-key  switches  prio"
+	" S           task   PID         tree-key  switches  prio"
 	"     wait-time             sum-exec        sum-sleep\n"
-	"------------------------------------------------------"
+	"-------------------------------------------------------"
 	"----------------------------------------------------\n");
 
 	rcu_read_lock();

commit 74dc3384fc7983b78cc46ebb1824968a3db85eb1
Author: Aleksa Sarai <asarai@suse.com>
Date:   Sun Aug 6 14:41:41 2017 +1000

    sched/debug: Use task_pid_nr_ns in /proc/$pid/sched
    
    It appears as though the addition of the PID namespace did not update
    the output code for /proc/*/sched, which resulted in it providing PIDs
    that were not self-consistent with the /proc mount. This additionally
    made it trivial to detect whether a process was inside &init_pid_ns from
    userspace, making container detection trivial:
    
       https://github.com/jessfraz/amicontained
    
    This leads to situations such as:
    
      % unshare -pmf
      % mount -t proc proc /proc
      % head -n1 /proc/1/sched
      head (10047, #threads: 1)
    
    Fix this by just using task_pid_nr_ns for the output of /proc/*/sched.
    All of the other uses of task_pid_nr in kernel/sched/debug.c are from a
    sysctl context and thus don't need to be namespaced.
    
    Signed-off-by: Aleksa Sarai <asarai@suse.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Jess Frazelle <acidburn@google.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: cyphar@cyphar.com
    Link: http://lkml.kernel.org/r/20170806044141.5093-1-asarai@suse.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 4fa66de52bd6..ac345115877b 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -872,11 +872,12 @@ static void sched_show_numa(struct task_struct *p, struct seq_file *m)
 #endif
 }
 
-void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
+void proc_sched_show_task(struct task_struct *p, struct pid_namespace *ns,
+						  struct seq_file *m)
 {
 	unsigned long nr_switches;
 
-	SEQ_printf(m, "%s (%d, #threads: %d)\n", p->comm, task_pid_nr(p),
+	SEQ_printf(m, "%s (%d, #threads: %d)\n", p->comm, task_pid_nr_ns(p, ns),
 						get_nr_threads(p));
 	SEQ_printf(m,
 		"---------------------------------------------------------"

commit 48365b38849fdb1ee6dc65beac044ca59f669683
Author: Daniel Bristot de Oliveira <bristot@redhat.com>
Date:   Mon Jun 26 17:07:14 2017 +0200

    sched/debug: Expose the number of RT/DL tasks that can migrate
    
    Add the value of the rt_rq.rt_nr_migratory and dl_rq.dl_nr_migratory
    to the sched_debug output, for instance:
    
     rt_rq[0]:
       .rt_nr_running                 : 2
       .rt_nr_migratory               : 1     <--- Like this
       .rt_throttled                  : 0
       .rt_time                       : 828.645877
       .rt_runtime                    : 1000.000000
    
    This is useful to debug problems related to the RT/DL schedulers.
    
    This also fixes the format of some variables, that were unsigned, rather
    than signed.
    
    Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luis Claudio R. Goncalves <lgoncalv@redhat.com>
    Cc: Luiz Capitulino <lcapitulino@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-rt-users <linux-rt-users@vger.kernel.org>
    Link: http://lkml.kernel.org/r/7896f71cada54ee7dd8507bb666063a2e051c3d4.1498482127.git.bristot@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 38f019324f1a..4fa66de52bd6 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -552,15 +552,21 @@ void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)
 
 #define P(x) \
 	SEQ_printf(m, "  .%-30s: %Ld\n", #x, (long long)(rt_rq->x))
+#define PU(x) \
+	SEQ_printf(m, "  .%-30s: %lu\n", #x, (unsigned long)(rt_rq->x))
 #define PN(x) \
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(rt_rq->x))
 
-	P(rt_nr_running);
+	PU(rt_nr_running);
+#ifdef CONFIG_SMP
+	PU(rt_nr_migratory);
+#endif
 	P(rt_throttled);
 	PN(rt_time);
 	PN(rt_runtime);
 
 #undef PN
+#undef PU
 #undef P
 }
 
@@ -569,14 +575,21 @@ void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq)
 	struct dl_bw *dl_bw;
 
 	SEQ_printf(m, "\ndl_rq[%d]:\n", cpu);
-	SEQ_printf(m, "  .%-30s: %ld\n", "dl_nr_running", dl_rq->dl_nr_running);
+
+#define PU(x) \
+	SEQ_printf(m, "  .%-30s: %lu\n", #x, (unsigned long)(dl_rq->x))
+
+	PU(dl_nr_running);
 #ifdef CONFIG_SMP
+	PU(dl_nr_migratory);
 	dl_bw = &cpu_rq(cpu)->rd->dl_bw;
 #else
 	dl_bw = &dl_rq->dl_bw;
 #endif
 	SEQ_printf(m, "  .%-30s: %lld\n", "dl_bw->bw", dl_bw->bw);
 	SEQ_printf(m, "  .%-30s: %lld\n", "dl_bw->total_bw", dl_bw->total_bw);
+
+#undef PU
 }
 
 extern __read_mostly int sched_clock_running;

commit f719ff9bcee2a422647790f12d53d3755f47c727
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 10:57:33 2017 +0100

    sched/headers: Prepare to move the task_lock()/unlock() APIs to <linux/sched/task.h>
    
    But first update the code that uses these facilities with the
    new header.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index e865d8bfb881..38f019324f1a 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -12,6 +12,7 @@
 
 #include <linux/proc_fs.h>
 #include <linux/sched/mm.h>
+#include <linux/sched/task.h>
 #include <linux/seq_file.h>
 #include <linux/kallsyms.h>
 #include <linux/utsname.h>

commit 589ee62844e042b0b7d19ef57fb4cff77f3ca294
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Feb 4 00:16:44 2017 +0100

    sched/headers: Prepare to remove the <linux/mm_types.h> dependency from <linux/sched.h>
    
    Update code that relied on sched.h including various MM types for them.
    
    This will allow us to remove the <linux/mm_types.h> include from <linux/sched.h>.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 109adc0e9cb9..e865d8bfb881 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -11,7 +11,7 @@
  */
 
 #include <linux/proc_fs.h>
-#include <linux/sched.h>
+#include <linux/sched/mm.h>
 #include <linux/seq_file.h>
 #include <linux/kallsyms.h>
 #include <linux/utsname.h>

commit 59f8c2989283bbd3df9fcfb22494d84f4852e536
Author: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
Date:   Wed Oct 26 11:17:17 2016 +0200

    sched/deadline: Show leftover runtime and abs deadline in /proc/*/sched
    
    This patch allows for reading the current (leftover) runtime and
    absolute deadline of a SCHED_DEADLINE task through /proc/*/sched
    (entries dl.runtime and dl.deadline), while debugging/testing.
    
    Signed-off-by: Tommaso Cucinotta <tommaso.cucinotta@sssup.it>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Juri Lelli <juri.lelli@arm.com>
    Reviewed-by: Luca Abeni <luca.abeni@unitn.it>
    Acked-by: Daniel Bistrot de Oliveira <danielbristot@gmail.com>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1477473437-10346-2-git-send-email-tommaso.cucinotta@sssup.it
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index fa178b62ea79..109adc0e9cb9 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -953,6 +953,10 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 #endif
 	P(policy);
 	P(prio);
+	if (p->policy == SCHED_DEADLINE) {
+		P(dl.runtime);
+		P(dl.deadline);
+	}
 #undef PN_SCHEDSTAT
 #undef PN
 #undef __PN

commit f34d3606f76a8121b9d4940d2dd436bebeb2f9d7
Merge: b6daa51b9a6a bbb427e34249
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 14 12:18:50 2016 -0700

    Merge branch 'for-4.9' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
    
     - tracepoints for basic cgroup management operations added
    
     - kernfs and cgroup path formatting functions updated to behave in the
       style of strlcpy()
    
     - non-critical bug fixes
    
    * 'for-4.9' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      blkcg: Unlock blkcg_pol_mutex only once when cpd == NULL
      cgroup: fix error handling regressions in proc_cgroup_show() and cgroup_release_agent()
      cpuset: fix error handling regression in proc_cpuset_show()
      cgroup: add tracepoints for basic operations
      cgroup: make cgroup_path() and friends behave in the style of strlcpy()
      kernfs: remove kernfs_path_len()
      kernfs: make kernfs_path*() behave in the style of strlcpy()
      kernfs: add dummy implementation of kernfs_path_from_node()

commit 4fa8d299b43a91f871f6d5b00dd5ab33d43bbc2c
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Jun 17 12:43:26 2016 -0500

    sched/debug: Remove several CONFIG_SCHEDSTATS guards
    
    Clean up the sched code by removing several of the CONFIG_SCHEDSTATS
    guards, using schedstat_*() macros where needed.
    
    Code size:
    
      !CONFIG_SCHEDSTATS defconfig:
    
          text         data     bss      dec            hex filename
      10209818      4368184 1105920 15683922         ef5152 vmlinux.before.nostats
      10209818      4368184 1105920 15683922         ef5152 vmlinux.after.nostats
    
      CONFIG_SCHEDSTATS defconfig:
    
          text         data     bss     dec     hex filename
      10214210      4370040 1105920 15690170         ef69ba vmlinux.before.stats
      10214210      4370680 1105920 15690810         ef6c3a vmlinux.after.stats
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/e51e0ebe5af95ac295de720dd252e7c0d2142e4a.1466184592.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 63ffcaa5d57c..13935886a471 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -369,8 +369,12 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 
 #define P(F) \
 	SEQ_printf(m, "  .%-30s: %lld\n", #F, (long long)F)
+#define P_SCHEDSTAT(F) \
+	SEQ_printf(m, "  .%-30s: %lld\n", #F, (long long)schedstat_val(F))
 #define PN(F) \
 	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)F))
+#define PN_SCHEDSTAT(F) \
+	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)schedstat_val(F)))
 
 	if (!se)
 		return;
@@ -378,26 +382,27 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	PN(se->exec_start);
 	PN(se->vruntime);
 	PN(se->sum_exec_runtime);
-#ifdef CONFIG_SCHEDSTATS
 	if (schedstat_enabled()) {
-		PN(se->statistics.wait_start);
-		PN(se->statistics.sleep_start);
-		PN(se->statistics.block_start);
-		PN(se->statistics.sleep_max);
-		PN(se->statistics.block_max);
-		PN(se->statistics.exec_max);
-		PN(se->statistics.slice_max);
-		PN(se->statistics.wait_max);
-		PN(se->statistics.wait_sum);
-		P(se->statistics.wait_count);
+		PN_SCHEDSTAT(se->statistics.wait_start);
+		PN_SCHEDSTAT(se->statistics.sleep_start);
+		PN_SCHEDSTAT(se->statistics.block_start);
+		PN_SCHEDSTAT(se->statistics.sleep_max);
+		PN_SCHEDSTAT(se->statistics.block_max);
+		PN_SCHEDSTAT(se->statistics.exec_max);
+		PN_SCHEDSTAT(se->statistics.slice_max);
+		PN_SCHEDSTAT(se->statistics.wait_max);
+		PN_SCHEDSTAT(se->statistics.wait_sum);
+		P_SCHEDSTAT(se->statistics.wait_count);
 	}
-#endif
 	P(se->load.weight);
 #ifdef CONFIG_SMP
 	P(se->avg.load_avg);
 	P(se->avg.util_avg);
 #endif
+
+#undef PN_SCHEDSTAT
 #undef PN
+#undef P_SCHEDSTAT
 #undef P
 }
 #endif
@@ -626,9 +631,7 @@ do {									\
 #undef P64
 #endif
 
-#ifdef CONFIG_SCHEDSTATS
-#define P(n) SEQ_printf(m, "  .%-30s: %d\n", #n, rq->n);
-
+#define P(n) SEQ_printf(m, "  .%-30s: %d\n", #n, schedstat_val(rq->n));
 	if (schedstat_enabled()) {
 		P(yld_count);
 		P(sched_count);
@@ -636,9 +639,8 @@ do {									\
 		P(ttwu_count);
 		P(ttwu_local);
 	}
-
 #undef P
-#endif
+
 	spin_lock_irqsave(&sched_debug_lock, flags);
 	print_cfs_stats(m, cpu);
 	print_rt_stats(m, cpu);
@@ -868,10 +870,14 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)F)
 #define P(F) \
 	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)p->F)
+#define P_SCHEDSTAT(F) \
+	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)schedstat_val(p->F))
 #define __PN(F) \
 	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
 #define PN(F) \
 	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
+#define PN_SCHEDSTAT(F) \
+	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)schedstat_val(p->F)))
 
 	PN(se.exec_start);
 	PN(se.vruntime);
@@ -881,37 +887,36 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 
 	P(se.nr_migrations);
 
-#ifdef CONFIG_SCHEDSTATS
 	if (schedstat_enabled()) {
 		u64 avg_atom, avg_per_cpu;
 
-		PN(se.statistics.sum_sleep_runtime);
-		PN(se.statistics.wait_start);
-		PN(se.statistics.sleep_start);
-		PN(se.statistics.block_start);
-		PN(se.statistics.sleep_max);
-		PN(se.statistics.block_max);
-		PN(se.statistics.exec_max);
-		PN(se.statistics.slice_max);
-		PN(se.statistics.wait_max);
-		PN(se.statistics.wait_sum);
-		P(se.statistics.wait_count);
-		PN(se.statistics.iowait_sum);
-		P(se.statistics.iowait_count);
-		P(se.statistics.nr_migrations_cold);
-		P(se.statistics.nr_failed_migrations_affine);
-		P(se.statistics.nr_failed_migrations_running);
-		P(se.statistics.nr_failed_migrations_hot);
-		P(se.statistics.nr_forced_migrations);
-		P(se.statistics.nr_wakeups);
-		P(se.statistics.nr_wakeups_sync);
-		P(se.statistics.nr_wakeups_migrate);
-		P(se.statistics.nr_wakeups_local);
-		P(se.statistics.nr_wakeups_remote);
-		P(se.statistics.nr_wakeups_affine);
-		P(se.statistics.nr_wakeups_affine_attempts);
-		P(se.statistics.nr_wakeups_passive);
-		P(se.statistics.nr_wakeups_idle);
+		PN_SCHEDSTAT(se.statistics.sum_sleep_runtime);
+		PN_SCHEDSTAT(se.statistics.wait_start);
+		PN_SCHEDSTAT(se.statistics.sleep_start);
+		PN_SCHEDSTAT(se.statistics.block_start);
+		PN_SCHEDSTAT(se.statistics.sleep_max);
+		PN_SCHEDSTAT(se.statistics.block_max);
+		PN_SCHEDSTAT(se.statistics.exec_max);
+		PN_SCHEDSTAT(se.statistics.slice_max);
+		PN_SCHEDSTAT(se.statistics.wait_max);
+		PN_SCHEDSTAT(se.statistics.wait_sum);
+		P_SCHEDSTAT(se.statistics.wait_count);
+		PN_SCHEDSTAT(se.statistics.iowait_sum);
+		P_SCHEDSTAT(se.statistics.iowait_count);
+		P_SCHEDSTAT(se.statistics.nr_migrations_cold);
+		P_SCHEDSTAT(se.statistics.nr_failed_migrations_affine);
+		P_SCHEDSTAT(se.statistics.nr_failed_migrations_running);
+		P_SCHEDSTAT(se.statistics.nr_failed_migrations_hot);
+		P_SCHEDSTAT(se.statistics.nr_forced_migrations);
+		P_SCHEDSTAT(se.statistics.nr_wakeups);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_sync);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_migrate);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_local);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_remote);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_affine);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_affine_attempts);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_passive);
+		P_SCHEDSTAT(se.statistics.nr_wakeups_idle);
 
 		avg_atom = p->se.sum_exec_runtime;
 		if (nr_switches)
@@ -930,7 +935,7 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 		__PN(avg_atom);
 		__PN(avg_per_cpu);
 	}
-#endif
+
 	__P(nr_switches);
 	SEQ_printf(m, "%-45s:%21Ld\n",
 		   "nr_voluntary_switches", (long long)p->nvcsw);
@@ -947,8 +952,10 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 #endif
 	P(policy);
 	P(prio);
+#undef PN_SCHEDSTAT
 #undef PN
 #undef __PN
+#undef P_SCHEDSTAT
 #undef P
 #undef __P
 

commit 20e1d4863bfa7152e98f94e5bcdda3e7db41d899
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Jun 17 12:43:25 2016 -0500

    sched/debug: Rename 'schedstat_val()' -> 'schedstat_val_or_zero()'
    
    The schedstat_val() macro's behavior is kind of surprising: when
    schedstat is runtime disabled, it returns zero.  Rename it to
    schedstat_val_or_zero().
    
    There's also a need for a similar macro which doesn't have the 'if
    (schedstat_enable())' check, to avoid doing the check twice.  Create a
    new 'schedstat_val()' macro for that.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/3bb1d2367d041fee333b0dde17171e709395b675.1466184592.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 92fa53457b72..63ffcaa5d57c 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -429,9 +429,9 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		p->prio);
 
 	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
-		SPLIT_NS(schedstat_val(p->se.statistics.wait_sum)),
+		SPLIT_NS(schedstat_val_or_zero(p->se.statistics.wait_sum)),
 		SPLIT_NS(p->se.sum_exec_runtime),
-		SPLIT_NS(schedstat_val(p->se.statistics.sum_sleep_runtime)));
+		SPLIT_NS(schedstat_val_or_zero(p->se.statistics.sum_sleep_runtime)));
 
 #ifdef CONFIG_NUMA_BALANCING
 	SEQ_printf(m, " %d %d", task_node(p), task_numa_group_id(p));

commit ae92882e5646d8661a3ca182ba988752fe4b773f
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Jun 17 12:43:24 2016 -0500

    sched/debug: Clean up schedstat macros
    
    The schedstat_*() macros are inconsistent: most of them take a pointer
    and a field which the macro combines, whereas schedstat_set() takes the
    already combined ptr->field.
    
    The already combined ptr->field argument is actually more intuitive and
    easier to use, and there's no reason to require the user to split the
    variable up, so convert the macros to use the combined argument.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Mel Gorman <mgorman@techsingularity.net>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/54953ca25bb579f3a5946432dee409b0e05222c6.1466184592.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2a0a9995256d..92fa53457b72 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -429,9 +429,9 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		p->prio);
 
 	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
-		SPLIT_NS(schedstat_val(p, se.statistics.wait_sum)),
+		SPLIT_NS(schedstat_val(p->se.statistics.wait_sum)),
 		SPLIT_NS(p->se.sum_exec_runtime),
-		SPLIT_NS(schedstat_val(p, se.statistics.sum_sleep_runtime)));
+		SPLIT_NS(schedstat_val(p->se.statistics.sum_sleep_runtime)));
 
 #ifdef CONFIG_NUMA_BALANCING
 	SEQ_printf(m, " %d %d", task_node(p), task_numa_group_id(p));

commit 4c737b41de7f4eef2a593803bad1b918dd718b10
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Aug 10 11:23:44 2016 -0400

    cgroup: make cgroup_path() and friends behave in the style of strlcpy()
    
    cgroup_path() and friends used to format the path from the end and
    thus the resulting path usually didn't start at the start of the
    passed in buffer.  Also, when the buffer was too small, the partial
    result was truncated from the head rather than tail and there was no
    way to tell how long the full path would be.  These make the functions
    less robust and more awkward to use.
    
    With recent updates to kernfs_path(), cgroup_path() and friends can be
    made to behave in strlcpy() style.
    
    * cgroup_path(), cgroup_path_ns[_locked]() and task_cgroup_path() now
      always return the length of the full path.  If buffer is too small,
      it contains nul terminated truncated output.
    
    * All users updated accordingly.
    
    v2: cgroup_path() usage in kernel/sched/debug.c converted.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Serge Hallyn <serge.hallyn@ubuntu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2a0a9995256d..23cb609ba4eb 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -410,7 +410,8 @@ static char *task_group_path(struct task_group *tg)
 	if (autogroup_path(tg, group_path, PATH_MAX))
 		return group_path;
 
-	return cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
+	cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
+	return group_path;
 }
 #endif
 

commit 03c041c5bf6ed584dff36b7cd509e0146a124277
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Jun 3 17:58:41 2016 -0500

    sched/debug: Always show 'nr_migrations'
    
    The nr_migrations field is updated independently of CONFIG_SCHEDSTATS,
    so it can be displayed regardless.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/5b1b04057ae2b14d73c2d03f56582c1d38cfe066.1464994423.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 0368c393a336..2a0a9995256d 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -879,9 +879,9 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 
 	nr_switches = p->nvcsw + p->nivcsw;
 
-#ifdef CONFIG_SCHEDSTATS
 	P(se.nr_migrations);
 
+#ifdef CONFIG_SCHEDSTATS
 	if (schedstat_enabled()) {
 		u64 avg_atom, avg_per_cpu;
 

commit 9c57259117b9c25472a3fa6d5a14d6bb3b647e87
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Fri Jun 3 17:58:40 2016 -0500

    sched/debug: Fix /proc/sched_debug regression
    
    Commit:
    
      cb2517653fcc ("sched/debug: Make schedstats a runtime tunable that is disabled by default")
    
    ... introduced a bug when CONFIG_SCHEDSTATS is enabled and the
    runtime tunable is disabled (which is the default).
    
    The wait-time, sum-exec, and sum-sleep fields are missing from the
    /proc/sched_debug file in the runnable_tasks section.
    
    Fix it with a new schedstat_val() macro which returns the field value
    when schedstats is enabled and zero otherwise.  The macro works with
    both SCHEDSTATS and !SCHEDSTATS.  I put the macro in stats.h since it
    might end up being useful in other places.
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Mel Gorman <mgorman@techsingularity.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Matt Fleming <matt@codeblueprint.co.uk>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: cb2517653fcc ("sched/debug: Make schedstats a runtime tunable that is disabled by default")
    Link: http://lkml.kernel.org/r/bcda7c2790cf2ccbe586a28c02dd7b6fe7749a2b.1464994423.git.jpoimboe@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index cf905f655ba1..0368c393a336 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -427,19 +427,12 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		SPLIT_NS(p->se.vruntime),
 		(long long)(p->nvcsw + p->nivcsw),
 		p->prio);
-#ifdef CONFIG_SCHEDSTATS
-	if (schedstat_enabled()) {
-		SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
-			SPLIT_NS(p->se.statistics.wait_sum),
-			SPLIT_NS(p->se.sum_exec_runtime),
-			SPLIT_NS(p->se.statistics.sum_sleep_runtime));
-	}
-#else
+
 	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
-		0LL, 0L,
+		SPLIT_NS(schedstat_val(p, se.statistics.wait_sum)),
 		SPLIT_NS(p->se.sum_exec_runtime),
-		0LL, 0L);
-#endif
+		SPLIT_NS(schedstat_val(p, se.statistics.sum_sleep_runtime)));
+
 #ifdef CONFIG_NUMA_BALANCING
 	SEQ_printf(m, " %d %d", task_node(p), task_numa_group_id(p));
 #endif

commit db6ea2fb094fb3a6afc36d3e4229bc162638ad24
Author: Wanpeng Li <wanpeng.li@hotmail.com>
Date:   Tue May 3 12:38:25 2016 +0800

    sched/debug: Print out idle balance values even on !CONFIG_SCHEDSTATS kernels
    
    The max_idle_balance_cost and avg_idle values which are tracked and ar used to
    capture short idle incidents, are not associated with schedstats, however the
    information of these two values isn't printed out on !CONFIG_SCHEDSTATS kernels.
    
    Fix this by moving the value printout out of the CONFIG_SCHEDSTATS section.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@hotmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1462250305-4523-1-git-send-email-wanpeng.li@hotmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 4fbc3bd5ff60..cf905f655ba1 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -626,15 +626,16 @@ do {									\
 #undef P
 #undef PN
 
-#ifdef CONFIG_SCHEDSTATS
-#define P(n) SEQ_printf(m, "  .%-30s: %d\n", #n, rq->n);
-#define P64(n) SEQ_printf(m, "  .%-30s: %Ld\n", #n, rq->n);
-
 #ifdef CONFIG_SMP
+#define P64(n) SEQ_printf(m, "  .%-30s: %Ld\n", #n, rq->n);
 	P64(avg_idle);
 	P64(max_idle_balance_cost);
+#undef P64
 #endif
 
+#ifdef CONFIG_SCHEDSTATS
+#define P(n) SEQ_printf(m, "  .%-30s: %d\n", #n, rq->n);
+
 	if (schedstat_enabled()) {
 		P(yld_count);
 		P(sched_count);
@@ -644,7 +645,6 @@ do {									\
 	}
 
 #undef P
-#undef P64
 #endif
 	spin_lock_irqsave(&sched_debug_lock, flags);
 	print_cfs_stats(m, cpu);

commit ef477183d06b0aa41c9e7c02cf5bfec41536e2c4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Feb 22 16:26:52 2016 -0500

    sched/debug: Add deadline scheduler bandwidth ratio to /proc/sched_debug
    
    Playing with SCHED_DEADLINE and cpusets, I found that I was unable to create
    new SCHED_DEADLINE tasks, with the error of EBUSY as if the bandwidth was
    already used up. I then realized there wa no way to see what bandwidth is
    used by the runqueues to debug the issue.
    
    By adding the dl_bw->bw and dl_bw->total_bw to the output of the deadline
    info in /proc/sched_debug, this allows us to see what bandwidth has been
    reserved and where a problem may exist.
    
    For example, before the issue we see the ratio of the bandwidth:
    
     # cat /proc/sys/kernel/sched_rt_runtime_us
     950000
     # cat /proc/sys/kernel/sched_rt_period_us
     1000000
    
      # grep dl /proc/sched_debug
      dl_rq[0]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
      dl_rq[1]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
      dl_rq[2]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
      dl_rq[3]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
      dl_rq[4]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
      dl_rq[5]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
      dl_rq[6]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
      dl_rq[7]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 0
    
    Note: (950000 / 1000000) << 20 == 996147
    
    After I played with cpusets and hit the issue, the result is now:
    
      # grep dl /proc/sched_debug
      dl_rq[0]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : -104857
      dl_rq[1]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 104857
      dl_rq[2]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 104857
      dl_rq[3]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : 104857
      dl_rq[4]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : -104857
      dl_rq[5]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : -104857
      dl_rq[6]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : -104857
      dl_rq[7]:
        .dl_nr_running                 : 0
        .dl_bw->bw                     : 996147
        .dl_bw->total_bw               : -104857
    
    This shows that there is definitely a problem as we should never have a
    negative total bandwidth.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160222212825.756849091@goodmis.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 313c65fe2087..4fbc3bd5ff60 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -566,8 +566,17 @@ void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)
 
 void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq)
 {
+	struct dl_bw *dl_bw;
+
 	SEQ_printf(m, "\ndl_rq[%d]:\n", cpu);
 	SEQ_printf(m, "  .%-30s: %ld\n", "dl_nr_running", dl_rq->dl_nr_running);
+#ifdef CONFIG_SMP
+	dl_bw = &cpu_rq(cpu)->rd->dl_bw;
+#else
+	dl_bw = &dl_rq->dl_bw;
+#endif
+	SEQ_printf(m, "  .%-30s: %lld\n", "dl_bw->bw", dl_bw->bw);
+	SEQ_printf(m, "  .%-30s: %lld\n", "dl_bw->total_bw", dl_bw->total_bw);
 }
 
 extern __read_mostly int sched_clock_running;

commit 3866e845ed522258c77da2eaa9f849ef55206ca2
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Feb 22 16:26:51 2016 -0500

    sched/debug: Move sched_domain_sysctl to debug.c
    
    The sched_domain_sysctl setup is only enabled when SCHED_DEBUG is
    configured. As debug.c is only compiled when SCHED_DEBUG is configured as
    well, move the setup of sched_domain_sysctl into that file.
    
    Note, the (un)register_sched_domain_sysctl() functions had to be changed
    from static to allow access to them from core.c.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160222212825.599278093@goodmis.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index d2dedfcbf84d..313c65fe2087 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -189,6 +189,179 @@ static __init int sched_init_debug(void)
 }
 late_initcall(sched_init_debug);
 
+#ifdef CONFIG_SMP
+
+#ifdef CONFIG_SYSCTL
+
+static struct ctl_table sd_ctl_dir[] = {
+	{
+		.procname	= "sched_domain",
+		.mode		= 0555,
+	},
+	{}
+};
+
+static struct ctl_table sd_ctl_root[] = {
+	{
+		.procname	= "kernel",
+		.mode		= 0555,
+		.child		= sd_ctl_dir,
+	},
+	{}
+};
+
+static struct ctl_table *sd_alloc_ctl_entry(int n)
+{
+	struct ctl_table *entry =
+		kcalloc(n, sizeof(struct ctl_table), GFP_KERNEL);
+
+	return entry;
+}
+
+static void sd_free_ctl_entry(struct ctl_table **tablep)
+{
+	struct ctl_table *entry;
+
+	/*
+	 * In the intermediate directories, both the child directory and
+	 * procname are dynamically allocated and could fail but the mode
+	 * will always be set. In the lowest directory the names are
+	 * static strings and all have proc handlers.
+	 */
+	for (entry = *tablep; entry->mode; entry++) {
+		if (entry->child)
+			sd_free_ctl_entry(&entry->child);
+		if (entry->proc_handler == NULL)
+			kfree(entry->procname);
+	}
+
+	kfree(*tablep);
+	*tablep = NULL;
+}
+
+static int min_load_idx = 0;
+static int max_load_idx = CPU_LOAD_IDX_MAX-1;
+
+static void
+set_table_entry(struct ctl_table *entry,
+		const char *procname, void *data, int maxlen,
+		umode_t mode, proc_handler *proc_handler,
+		bool load_idx)
+{
+	entry->procname = procname;
+	entry->data = data;
+	entry->maxlen = maxlen;
+	entry->mode = mode;
+	entry->proc_handler = proc_handler;
+
+	if (load_idx) {
+		entry->extra1 = &min_load_idx;
+		entry->extra2 = &max_load_idx;
+	}
+}
+
+static struct ctl_table *
+sd_alloc_ctl_domain_table(struct sched_domain *sd)
+{
+	struct ctl_table *table = sd_alloc_ctl_entry(14);
+
+	if (table == NULL)
+		return NULL;
+
+	set_table_entry(&table[0], "min_interval", &sd->min_interval,
+		sizeof(long), 0644, proc_doulongvec_minmax, false);
+	set_table_entry(&table[1], "max_interval", &sd->max_interval,
+		sizeof(long), 0644, proc_doulongvec_minmax, false);
+	set_table_entry(&table[2], "busy_idx", &sd->busy_idx,
+		sizeof(int), 0644, proc_dointvec_minmax, true);
+	set_table_entry(&table[3], "idle_idx", &sd->idle_idx,
+		sizeof(int), 0644, proc_dointvec_minmax, true);
+	set_table_entry(&table[4], "newidle_idx", &sd->newidle_idx,
+		sizeof(int), 0644, proc_dointvec_minmax, true);
+	set_table_entry(&table[5], "wake_idx", &sd->wake_idx,
+		sizeof(int), 0644, proc_dointvec_minmax, true);
+	set_table_entry(&table[6], "forkexec_idx", &sd->forkexec_idx,
+		sizeof(int), 0644, proc_dointvec_minmax, true);
+	set_table_entry(&table[7], "busy_factor", &sd->busy_factor,
+		sizeof(int), 0644, proc_dointvec_minmax, false);
+	set_table_entry(&table[8], "imbalance_pct", &sd->imbalance_pct,
+		sizeof(int), 0644, proc_dointvec_minmax, false);
+	set_table_entry(&table[9], "cache_nice_tries",
+		&sd->cache_nice_tries,
+		sizeof(int), 0644, proc_dointvec_minmax, false);
+	set_table_entry(&table[10], "flags", &sd->flags,
+		sizeof(int), 0644, proc_dointvec_minmax, false);
+	set_table_entry(&table[11], "max_newidle_lb_cost",
+		&sd->max_newidle_lb_cost,
+		sizeof(long), 0644, proc_doulongvec_minmax, false);
+	set_table_entry(&table[12], "name", sd->name,
+		CORENAME_MAX_SIZE, 0444, proc_dostring, false);
+	/* &table[13] is terminator */
+
+	return table;
+}
+
+static struct ctl_table *sd_alloc_ctl_cpu_table(int cpu)
+{
+	struct ctl_table *entry, *table;
+	struct sched_domain *sd;
+	int domain_num = 0, i;
+	char buf[32];
+
+	for_each_domain(cpu, sd)
+		domain_num++;
+	entry = table = sd_alloc_ctl_entry(domain_num + 1);
+	if (table == NULL)
+		return NULL;
+
+	i = 0;
+	for_each_domain(cpu, sd) {
+		snprintf(buf, 32, "domain%d", i);
+		entry->procname = kstrdup(buf, GFP_KERNEL);
+		entry->mode = 0555;
+		entry->child = sd_alloc_ctl_domain_table(sd);
+		entry++;
+		i++;
+	}
+	return table;
+}
+
+static struct ctl_table_header *sd_sysctl_header;
+void register_sched_domain_sysctl(void)
+{
+	int i, cpu_num = num_possible_cpus();
+	struct ctl_table *entry = sd_alloc_ctl_entry(cpu_num + 1);
+	char buf[32];
+
+	WARN_ON(sd_ctl_dir[0].child);
+	sd_ctl_dir[0].child = entry;
+
+	if (entry == NULL)
+		return;
+
+	for_each_possible_cpu(i) {
+		snprintf(buf, 32, "cpu%d", i);
+		entry->procname = kstrdup(buf, GFP_KERNEL);
+		entry->mode = 0555;
+		entry->child = sd_alloc_ctl_cpu_table(i);
+		entry++;
+	}
+
+	WARN_ON(sd_sysctl_header);
+	sd_sysctl_header = register_sysctl_table(sd_ctl_root);
+}
+
+/* may be called multiple times per register */
+void unregister_sched_domain_sysctl(void)
+{
+	unregister_sysctl_table(sd_sysctl_header);
+	sd_sysctl_header = NULL;
+	if (sd_ctl_dir[0].child)
+		sd_free_ctl_entry(&sd_ctl_dir[0].child);
+}
+#endif /* CONFIG_SYSCTL */
+#endif /* CONFIG_SMP */
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group *tg)
 {

commit d6ca41d7922ce0110a840ef4f8ec4afdd5a239d3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Feb 22 16:26:50 2016 -0500

    sched/debug: Move the /sys/kernel/debug/sched_features file setup into debug.c
    
    As /sys/kernel/debug/sched_features is only created when SCHED_DEBUG is enabled, and the file
    debug.c is only compiled when SCHED_DEBUG is enabled, it makes sense to move
    sched_feature setup into that file and get rid of the #ifdef.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Juri Lelli <juri.lelli@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160222212825.464193063@goodmis.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 7cfa87bd8b89..d2dedfcbf84d 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -16,6 +16,7 @@
 #include <linux/kallsyms.h>
 #include <linux/utsname.h>
 #include <linux/mempolicy.h>
+#include <linux/debugfs.h>
 
 #include "sched.h"
 
@@ -58,6 +59,136 @@ static unsigned long nsec_low(unsigned long long nsec)
 
 #define SPLIT_NS(x) nsec_high(x), nsec_low(x)
 
+#define SCHED_FEAT(name, enabled)	\
+	#name ,
+
+static const char * const sched_feat_names[] = {
+#include "features.h"
+};
+
+#undef SCHED_FEAT
+
+static int sched_feat_show(struct seq_file *m, void *v)
+{
+	int i;
+
+	for (i = 0; i < __SCHED_FEAT_NR; i++) {
+		if (!(sysctl_sched_features & (1UL << i)))
+			seq_puts(m, "NO_");
+		seq_printf(m, "%s ", sched_feat_names[i]);
+	}
+	seq_puts(m, "\n");
+
+	return 0;
+}
+
+#ifdef HAVE_JUMP_LABEL
+
+#define jump_label_key__true  STATIC_KEY_INIT_TRUE
+#define jump_label_key__false STATIC_KEY_INIT_FALSE
+
+#define SCHED_FEAT(name, enabled)	\
+	jump_label_key__##enabled ,
+
+struct static_key sched_feat_keys[__SCHED_FEAT_NR] = {
+#include "features.h"
+};
+
+#undef SCHED_FEAT
+
+static void sched_feat_disable(int i)
+{
+	static_key_disable(&sched_feat_keys[i]);
+}
+
+static void sched_feat_enable(int i)
+{
+	static_key_enable(&sched_feat_keys[i]);
+}
+#else
+static void sched_feat_disable(int i) { };
+static void sched_feat_enable(int i) { };
+#endif /* HAVE_JUMP_LABEL */
+
+static int sched_feat_set(char *cmp)
+{
+	int i;
+	int neg = 0;
+
+	if (strncmp(cmp, "NO_", 3) == 0) {
+		neg = 1;
+		cmp += 3;
+	}
+
+	for (i = 0; i < __SCHED_FEAT_NR; i++) {
+		if (strcmp(cmp, sched_feat_names[i]) == 0) {
+			if (neg) {
+				sysctl_sched_features &= ~(1UL << i);
+				sched_feat_disable(i);
+			} else {
+				sysctl_sched_features |= (1UL << i);
+				sched_feat_enable(i);
+			}
+			break;
+		}
+	}
+
+	return i;
+}
+
+static ssize_t
+sched_feat_write(struct file *filp, const char __user *ubuf,
+		size_t cnt, loff_t *ppos)
+{
+	char buf[64];
+	char *cmp;
+	int i;
+	struct inode *inode;
+
+	if (cnt > 63)
+		cnt = 63;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	buf[cnt] = 0;
+	cmp = strstrip(buf);
+
+	/* Ensure the static_key remains in a consistent state */
+	inode = file_inode(filp);
+	inode_lock(inode);
+	i = sched_feat_set(cmp);
+	inode_unlock(inode);
+	if (i == __SCHED_FEAT_NR)
+		return -EINVAL;
+
+	*ppos += cnt;
+
+	return cnt;
+}
+
+static int sched_feat_open(struct inode *inode, struct file *filp)
+{
+	return single_open(filp, sched_feat_show, NULL);
+}
+
+static const struct file_operations sched_feat_fops = {
+	.open		= sched_feat_open,
+	.write		= sched_feat_write,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static __init int sched_init_debug(void)
+{
+	debugfs_create_file("sched_features", 0644, NULL, NULL,
+			&sched_feat_fops);
+
+	return 0;
+}
+late_initcall(sched_init_debug);
+
 #ifdef CONFIG_FAIR_GROUP_SCHED
 static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group *tg)
 {

commit cb2517653fccaf9f9b4ae968c7ee005c1bbacdc5
Author: Mel Gorman <mgorman@techsingularity.net>
Date:   Fri Feb 5 09:08:36 2016 +0000

    sched/debug: Make schedstats a runtime tunable that is disabled by default
    
    schedstats is very useful during debugging and performance tuning but it
    incurs overhead to calculate the stats. As such, even though it can be
    disabled at build time, it is often enabled as the information is useful.
    
    This patch adds a kernel command-line and sysctl tunable to enable or
    disable schedstats on demand (when it's built in). It is disabled
    by default as someone who knows they need it can also learn to enable
    it when necessary.
    
    The benefits are dependent on how scheduler-intensive the workload is.
    If it is then the patch reduces the number of cycles spent calculating
    the stats with a small benefit from reducing the cache footprint of the
    scheduler.
    
    These measurements were taken from a 48-core 2-socket
    machine with Xeon(R) E5-2670 v3 cpus although they were also tested on a
    single socket machine 8-core machine with Intel i7-3770 processors.
    
    netperf-tcp
                               4.5.0-rc1             4.5.0-rc1
                                 vanilla          nostats-v3r1
    Hmean    64         560.45 (  0.00%)      575.98 (  2.77%)
    Hmean    128        766.66 (  0.00%)      795.79 (  3.80%)
    Hmean    256        950.51 (  0.00%)      981.50 (  3.26%)
    Hmean    1024      1433.25 (  0.00%)     1466.51 (  2.32%)
    Hmean    2048      2810.54 (  0.00%)     2879.75 (  2.46%)
    Hmean    3312      4618.18 (  0.00%)     4682.09 (  1.38%)
    Hmean    4096      5306.42 (  0.00%)     5346.39 (  0.75%)
    Hmean    8192     10581.44 (  0.00%)    10698.15 (  1.10%)
    Hmean    16384    18857.70 (  0.00%)    18937.61 (  0.42%)
    
    Small gains here, UDP_STREAM showed nothing intresting and neither did
    the TCP_RR tests. The gains on the 8-core machine were very similar.
    
    tbench4
                                     4.5.0-rc1             4.5.0-rc1
                                       vanilla          nostats-v3r1
    Hmean    mb/sec-1         500.85 (  0.00%)      522.43 (  4.31%)
    Hmean    mb/sec-2         984.66 (  0.00%)     1018.19 (  3.41%)
    Hmean    mb/sec-4        1827.91 (  0.00%)     1847.78 (  1.09%)
    Hmean    mb/sec-8        3561.36 (  0.00%)     3611.28 (  1.40%)
    Hmean    mb/sec-16       5824.52 (  0.00%)     5929.03 (  1.79%)
    Hmean    mb/sec-32      10943.10 (  0.00%)    10802.83 ( -1.28%)
    Hmean    mb/sec-64      15950.81 (  0.00%)    16211.31 (  1.63%)
    Hmean    mb/sec-128     15302.17 (  0.00%)    15445.11 (  0.93%)
    Hmean    mb/sec-256     14866.18 (  0.00%)    15088.73 (  1.50%)
    Hmean    mb/sec-512     15223.31 (  0.00%)    15373.69 (  0.99%)
    Hmean    mb/sec-1024    14574.25 (  0.00%)    14598.02 (  0.16%)
    Hmean    mb/sec-2048    13569.02 (  0.00%)    13733.86 (  1.21%)
    Hmean    mb/sec-3072    12865.98 (  0.00%)    13209.23 (  2.67%)
    
    Small gains of 2-4% at low thread counts and otherwise flat.  The
    gains on the 8-core machine were slightly different
    
    tbench4 on 8-core i7-3770 single socket machine
    Hmean    mb/sec-1        442.59 (  0.00%)      448.73 (  1.39%)
    Hmean    mb/sec-2        796.68 (  0.00%)      794.39 ( -0.29%)
    Hmean    mb/sec-4       1322.52 (  0.00%)     1343.66 (  1.60%)
    Hmean    mb/sec-8       2611.65 (  0.00%)     2694.86 (  3.19%)
    Hmean    mb/sec-16      2537.07 (  0.00%)     2609.34 (  2.85%)
    Hmean    mb/sec-32      2506.02 (  0.00%)     2578.18 (  2.88%)
    Hmean    mb/sec-64      2511.06 (  0.00%)     2569.16 (  2.31%)
    Hmean    mb/sec-128     2313.38 (  0.00%)     2395.50 (  3.55%)
    Hmean    mb/sec-256     2110.04 (  0.00%)     2177.45 (  3.19%)
    Hmean    mb/sec-512     2072.51 (  0.00%)     2053.97 ( -0.89%)
    
    In constract, this shows a relatively steady 2-3% gain at higher thread
    counts. Due to the nature of the patch and the type of workload, it's
    not a surprise that the result will depend on the CPU used.
    
    hackbench-pipes
                             4.5.0-rc1             4.5.0-rc1
                               vanilla          nostats-v3r1
    Amean    1        0.0637 (  0.00%)      0.0660 ( -3.59%)
    Amean    4        0.1229 (  0.00%)      0.1181 (  3.84%)
    Amean    7        0.1921 (  0.00%)      0.1911 (  0.52%)
    Amean    12       0.3117 (  0.00%)      0.2923 (  6.23%)
    Amean    21       0.4050 (  0.00%)      0.3899 (  3.74%)
    Amean    30       0.4586 (  0.00%)      0.4433 (  3.33%)
    Amean    48       0.5910 (  0.00%)      0.5694 (  3.65%)
    Amean    79       0.8663 (  0.00%)      0.8626 (  0.43%)
    Amean    110      1.1543 (  0.00%)      1.1517 (  0.22%)
    Amean    141      1.4457 (  0.00%)      1.4290 (  1.16%)
    Amean    172      1.7090 (  0.00%)      1.6924 (  0.97%)
    Amean    192      1.9126 (  0.00%)      1.9089 (  0.19%)
    
    Some small gains and losses and while the variance data is not included,
    it's close to the noise. The UMA machine did not show anything particularly
    different
    
    pipetest
                                 4.5.0-rc1             4.5.0-rc1
                                   vanilla          nostats-v2r2
    Min         Time        4.13 (  0.00%)        3.99 (  3.39%)
    1st-qrtle   Time        4.38 (  0.00%)        4.27 (  2.51%)
    2nd-qrtle   Time        4.46 (  0.00%)        4.39 (  1.57%)
    3rd-qrtle   Time        4.56 (  0.00%)        4.51 (  1.10%)
    Max-90%     Time        4.67 (  0.00%)        4.60 (  1.50%)
    Max-93%     Time        4.71 (  0.00%)        4.65 (  1.27%)
    Max-95%     Time        4.74 (  0.00%)        4.71 (  0.63%)
    Max-99%     Time        4.88 (  0.00%)        4.79 (  1.84%)
    Max         Time        4.93 (  0.00%)        4.83 (  2.03%)
    Mean        Time        4.48 (  0.00%)        4.39 (  1.91%)
    Best99%Mean Time        4.47 (  0.00%)        4.39 (  1.91%)
    Best95%Mean Time        4.46 (  0.00%)        4.38 (  1.93%)
    Best90%Mean Time        4.45 (  0.00%)        4.36 (  1.98%)
    Best50%Mean Time        4.36 (  0.00%)        4.25 (  2.49%)
    Best10%Mean Time        4.23 (  0.00%)        4.10 (  3.13%)
    Best5%Mean  Time        4.19 (  0.00%)        4.06 (  3.20%)
    Best1%Mean  Time        4.13 (  0.00%)        4.00 (  3.39%)
    
    Small improvement and similar gains were seen on the UMA machine.
    
    The gain is small but it stands to reason that doing less work in the
    scheduler is a good thing. The downside is that the lack of schedstats and
    tracepoints may be surprising to experts doing performance analysis until
    they find the existence of the schedstats= parameter or schedstats sysctl.
    It will be automatically activated for latencytop and sleep profiling to
    alleviate the problem. For tracepoints, there is a simple warning as it's
    not safe to activate schedstats in the context when it's known the tracepoint
    may be wanted but is unavailable.
    
    Signed-off-by: Mel Gorman <mgorman@techsingularity.net>
    Reviewed-by: Matt Fleming <matt@codeblueprint.co.uk>
    Reviewed-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <mgalbraith@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1454663316-22048-1-git-send-email-mgorman@techsingularity.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 641511771ae6..7cfa87bd8b89 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -75,16 +75,18 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	PN(se->vruntime);
 	PN(se->sum_exec_runtime);
 #ifdef CONFIG_SCHEDSTATS
-	PN(se->statistics.wait_start);
-	PN(se->statistics.sleep_start);
-	PN(se->statistics.block_start);
-	PN(se->statistics.sleep_max);
-	PN(se->statistics.block_max);
-	PN(se->statistics.exec_max);
-	PN(se->statistics.slice_max);
-	PN(se->statistics.wait_max);
-	PN(se->statistics.wait_sum);
-	P(se->statistics.wait_count);
+	if (schedstat_enabled()) {
+		PN(se->statistics.wait_start);
+		PN(se->statistics.sleep_start);
+		PN(se->statistics.block_start);
+		PN(se->statistics.sleep_max);
+		PN(se->statistics.block_max);
+		PN(se->statistics.exec_max);
+		PN(se->statistics.slice_max);
+		PN(se->statistics.wait_max);
+		PN(se->statistics.wait_sum);
+		P(se->statistics.wait_count);
+	}
 #endif
 	P(se->load.weight);
 #ifdef CONFIG_SMP
@@ -122,10 +124,12 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		(long long)(p->nvcsw + p->nivcsw),
 		p->prio);
 #ifdef CONFIG_SCHEDSTATS
-	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
-		SPLIT_NS(p->se.statistics.wait_sum),
-		SPLIT_NS(p->se.sum_exec_runtime),
-		SPLIT_NS(p->se.statistics.sum_sleep_runtime));
+	if (schedstat_enabled()) {
+		SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
+			SPLIT_NS(p->se.statistics.wait_sum),
+			SPLIT_NS(p->se.sum_exec_runtime),
+			SPLIT_NS(p->se.statistics.sum_sleep_runtime));
+	}
 #else
 	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
 		0LL, 0L,
@@ -313,17 +317,18 @@ do {									\
 #define P(n) SEQ_printf(m, "  .%-30s: %d\n", #n, rq->n);
 #define P64(n) SEQ_printf(m, "  .%-30s: %Ld\n", #n, rq->n);
 
-	P(yld_count);
-
-	P(sched_count);
-	P(sched_goidle);
 #ifdef CONFIG_SMP
 	P64(avg_idle);
 	P64(max_idle_balance_cost);
 #endif
 
-	P(ttwu_count);
-	P(ttwu_local);
+	if (schedstat_enabled()) {
+		P(yld_count);
+		P(sched_count);
+		P(sched_goidle);
+		P(ttwu_count);
+		P(ttwu_local);
+	}
 
 #undef P
 #undef P64
@@ -569,38 +574,39 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 	nr_switches = p->nvcsw + p->nivcsw;
 
 #ifdef CONFIG_SCHEDSTATS
-	PN(se.statistics.sum_sleep_runtime);
-	PN(se.statistics.wait_start);
-	PN(se.statistics.sleep_start);
-	PN(se.statistics.block_start);
-	PN(se.statistics.sleep_max);
-	PN(se.statistics.block_max);
-	PN(se.statistics.exec_max);
-	PN(se.statistics.slice_max);
-	PN(se.statistics.wait_max);
-	PN(se.statistics.wait_sum);
-	P(se.statistics.wait_count);
-	PN(se.statistics.iowait_sum);
-	P(se.statistics.iowait_count);
 	P(se.nr_migrations);
-	P(se.statistics.nr_migrations_cold);
-	P(se.statistics.nr_failed_migrations_affine);
-	P(se.statistics.nr_failed_migrations_running);
-	P(se.statistics.nr_failed_migrations_hot);
-	P(se.statistics.nr_forced_migrations);
-	P(se.statistics.nr_wakeups);
-	P(se.statistics.nr_wakeups_sync);
-	P(se.statistics.nr_wakeups_migrate);
-	P(se.statistics.nr_wakeups_local);
-	P(se.statistics.nr_wakeups_remote);
-	P(se.statistics.nr_wakeups_affine);
-	P(se.statistics.nr_wakeups_affine_attempts);
-	P(se.statistics.nr_wakeups_passive);
-	P(se.statistics.nr_wakeups_idle);
 
-	{
+	if (schedstat_enabled()) {
 		u64 avg_atom, avg_per_cpu;
 
+		PN(se.statistics.sum_sleep_runtime);
+		PN(se.statistics.wait_start);
+		PN(se.statistics.sleep_start);
+		PN(se.statistics.block_start);
+		PN(se.statistics.sleep_max);
+		PN(se.statistics.block_max);
+		PN(se.statistics.exec_max);
+		PN(se.statistics.slice_max);
+		PN(se.statistics.wait_max);
+		PN(se.statistics.wait_sum);
+		P(se.statistics.wait_count);
+		PN(se.statistics.iowait_sum);
+		P(se.statistics.iowait_count);
+		P(se.statistics.nr_migrations_cold);
+		P(se.statistics.nr_failed_migrations_affine);
+		P(se.statistics.nr_failed_migrations_running);
+		P(se.statistics.nr_failed_migrations_hot);
+		P(se.statistics.nr_forced_migrations);
+		P(se.statistics.nr_wakeups);
+		P(se.statistics.nr_wakeups_sync);
+		P(se.statistics.nr_wakeups_migrate);
+		P(se.statistics.nr_wakeups_local);
+		P(se.statistics.nr_wakeups_remote);
+		P(se.statistics.nr_wakeups_affine);
+		P(se.statistics.nr_wakeups_affine_attempts);
+		P(se.statistics.nr_wakeups_passive);
+		P(se.statistics.nr_wakeups_idle);
+
 		avg_atom = p->se.sum_exec_runtime;
 		if (nr_switches)
 			avg_atom = div64_ul(avg_atom, nr_switches);

commit 139622343ef31941effc6de6a5a9320371a00e62
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:41 2015 +0800

    sched/fair: Provide runnable_load_avg back to cfs_rq
    
    The cfs_rq's load_avg is composed of runnable_load_avg and blocked_load_avg.
    Before this series, sometimes the runnable_load_avg is used, and sometimes
    the load_avg is used. Completely replacing all uses of runnable_load_avg
    with load_avg may be too big a leap, i.e., the blocked_load_avg is concerned
    to result in overrated load. Therefore, we get runnable_load_avg back.
    
    The new cfs_rq's runnable_load_avg is improved to be updated with all of the
    runnable sched_eneities at the same time, so the one sched_entity updated and
    the others stale problem is solved.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-7-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 74f276f5568c..641511771ae6 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -207,6 +207,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #ifdef CONFIG_SMP
 	SEQ_printf(m, "  .%-30s: %lu\n", "load_avg",
 			cfs_rq->avg.load_avg);
+	SEQ_printf(m, "  .%-30s: %lu\n", "runnable_load_avg",
+			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lu\n", "util_avg",
 			cfs_rq->avg.util_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "removed_load_avg",

commit 9d89c257dfb9c51a532d69397f6eed75e5168c35
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:37 2015 +0800

    sched/fair: Rewrite runnable load and utilization average tracking
    
    The idea of runnable load average (let runnable time contribute to weight)
    was proposed by Paul Turner and Ben Segall, and it is still followed by
    this rewrite. This rewrite aims to solve the following issues:
    
    1. cfs_rq's load average (namely runnable_load_avg and blocked_load_avg) is
       updated at the granularity of an entity at a time, which results in the
       cfs_rq's load average is stale or partially updated: at any time, only
       one entity is up to date, all other entities are effectively lagging
       behind. This is undesirable.
    
       To illustrate, if we have n runnable entities in the cfs_rq, as time
       elapses, they certainly become outdated:
    
         t0: cfs_rq { e1_old, e2_old, ..., en_old }
    
       and when we update:
    
         t1: update e1, then we have cfs_rq { e1_new, e2_old, ..., en_old }
    
         t2: update e2, then we have cfs_rq { e1_old, e2_new, ..., en_old }
    
         ...
    
       We solve this by combining all runnable entities' load averages together
       in cfs_rq's avg, and update the cfs_rq's avg as a whole. This is based
       on the fact that if we regard the update as a function, then:
    
       w * update(e) = update(w * e) and
    
       update(e1) + update(e2) = update(e1 + e2), then
    
       w1 * update(e1) + w2 * update(e2) = update(w1 * e1 + w2 * e2)
    
       therefore, by this rewrite, we have an entirely updated cfs_rq at the
       time we update it:
    
         t1: update cfs_rq { e1_new, e2_new, ..., en_new }
    
         t2: update cfs_rq { e1_new, e2_new, ..., en_new }
    
         ...
    
    2. cfs_rq's load average is different between top rq->cfs_rq and other
       task_group's per CPU cfs_rqs in whether or not blocked_load_average
       contributes to the load.
    
       The basic idea behind runnable load average (the same for utilization)
       is that the blocked state is taken into account as opposed to only
       accounting for the currently runnable state. Therefore, the average
       should include both the runnable/running and blocked load averages.
       This rewrite does that.
    
       In addition, we also combine runnable/running and blocked averages
       of all entities into the cfs_rq's average, and update it together at
       once. This is based on the fact that:
    
         update(runnable) + update(blocked) = update(runnable + blocked)
    
       This significantly reduces the code as we don't need to separately
       maintain/update runnable/running load and blocked load.
    
    3. How task_group entities' share is calculated is complex and imprecise.
    
       We reduce the complexity in this rewrite to allow a very simple rule:
       the task_group's load_avg is aggregated from its per CPU cfs_rqs's
       load_avgs. Then group entity's weight is simply proportional to its
       own cfs_rq's load_avg / task_group's load_avg. To illustrate,
    
       if a task_group has { cfs_rq1, cfs_rq2, ..., cfs_rqn }, then,
    
       task_group_avg = cfs_rq1_avg + cfs_rq2_avg + ... + cfs_rqn_avg, then
    
       cfs_rqx's entity's share = cfs_rqx_avg / task_group_avg * task_group's share
    
    To sum up, this rewrite in principle is equivalent to the current one, but
    fixes the issues described above. Turns out, it significantly reduces the
    code complexity and hence increases clarity and efficiency. In addition,
    the new averages are more smooth/continuous (no spurious spikes and valleys)
    and updated more consistently and quickly to reflect the load dynamics.
    
    As a result, we have less load tracking overhead, better performance,
    and especially better power efficiency due to more balanced load.
    
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-3-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 363b7e82554b..74f276f5568c 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -88,12 +88,8 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 #endif
 	P(se->load.weight);
 #ifdef CONFIG_SMP
-	P(se->avg.runnable_avg_sum);
-	P(se->avg.running_avg_sum);
-	P(se->avg.avg_period);
-	P(se->avg.load_avg_contrib);
-	P(se->avg.utilization_avg_contrib);
-	P(se->avg.decay_count);
+	P(se->avg.load_avg);
+	P(se->avg.util_avg);
 #endif
 #undef PN
 #undef P
@@ -209,21 +205,19 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", cfs_rq->nr_running);
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
 #ifdef CONFIG_SMP
-	SEQ_printf(m, "  .%-30s: %ld\n", "runnable_load_avg",
-			cfs_rq->runnable_load_avg);
-	SEQ_printf(m, "  .%-30s: %ld\n", "blocked_load_avg",
-			cfs_rq->blocked_load_avg);
-	SEQ_printf(m, "  .%-30s: %ld\n", "utilization_load_avg",
-			cfs_rq->utilization_load_avg);
+	SEQ_printf(m, "  .%-30s: %lu\n", "load_avg",
+			cfs_rq->avg.load_avg);
+	SEQ_printf(m, "  .%-30s: %lu\n", "util_avg",
+			cfs_rq->avg.util_avg);
+	SEQ_printf(m, "  .%-30s: %ld\n", "removed_load_avg",
+			atomic_long_read(&cfs_rq->removed_load_avg));
+	SEQ_printf(m, "  .%-30s: %ld\n", "removed_util_avg",
+			atomic_long_read(&cfs_rq->removed_util_avg));
 #ifdef CONFIG_FAIR_GROUP_SCHED
-	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_contrib",
-			cfs_rq->tg_load_contrib);
-	SEQ_printf(m, "  .%-30s: %d\n", "tg_runnable_contrib",
-			cfs_rq->tg_runnable_contrib);
+	SEQ_printf(m, "  .%-30s: %lu\n", "tg_load_avg_contrib",
+			cfs_rq->tg_load_avg_contrib);
 	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_avg",
 			atomic_long_read(&cfs_rq->tg->load_avg));
-	SEQ_printf(m, "  .%-30s: %d\n", "tg->runnable_avg",
-			atomic_read(&cfs_rq->tg->runnable_avg));
 #endif
 #endif
 #ifdef CONFIG_CFS_BANDWIDTH
@@ -631,12 +625,11 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 
 	P(se.load.weight);
 #ifdef CONFIG_SMP
-	P(se.avg.runnable_avg_sum);
-	P(se.avg.running_avg_sum);
-	P(se.avg.avg_period);
-	P(se.avg.load_avg_contrib);
-	P(se.avg.utilization_avg_contrib);
-	P(se.avg.decay_count);
+	P(se.avg.load_sum);
+	P(se.avg.util_sum);
+	P(se.avg.load_avg);
+	P(se.avg.util_avg);
+	P(se.avg.last_update_time);
 #endif
 	P(policy);
 	P(prio);

commit cd126afe838d7ea9b971cdea087fd498a7293c7f
Author: Yuyang Du <yuyang.du@intel.com>
Date:   Wed Jul 15 08:04:36 2015 +0800

    sched/fair: Remove rq's runnable avg
    
    The current rq->avg is not used at all since its merge into the kernel,
    and the code is in the scheduler's hot path, so remove it.
    
    Tested-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Signed-off-by: Yuyang Du <yuyang.du@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Dietmar Eggemann <dietmar.eggemann@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: arjan@linux.intel.com
    Cc: bsegall@google.com
    Cc: fengguang.wu@intel.com
    Cc: len.brown@intel.com
    Cc: morten.rasmussen@arm.com
    Cc: pjt@google.com
    Cc: rafael.j.wysocki@intel.com
    Cc: umgwanakikbuti@gmail.com
    Cc: vincent.guittot@linaro.org
    Link: http://lkml.kernel.org/r/1436918682-4971-2-git-send-email-yuyang.du@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 4222ec50ab88..363b7e82554b 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -68,13 +68,8 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 #define PN(F) \
 	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)F))
 
-	if (!se) {
-		struct sched_avg *avg = &cpu_rq(cpu)->avg;
-		P(avg->runnable_avg_sum);
-		P(avg->avg_period);
+	if (!se)
 		return;
-	}
-
 
 	PN(se->exec_start);
 	PN(se->vruntime);

commit 397f2378f136128623fc237746157aa2564d1082
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jun 25 22:51:43 2015 +0530

    sched/numa: Fix numa balancing stats in /proc/pid/sched
    
    Commit 44dba3d5d6a1 ("sched: Refactor task_struct to use
    numa_faults instead of numa_* pointers") modified the way
    tsk->numa_faults stats are accounted.
    
    However that commit never touched show_numa_stats() that is displayed
    in /proc/pid/sched and thus the numbers displayed in /proc/pid/sched
    don't match the actual numbers.
    
    Fix it by making sure that /proc/pid/sched reflects the task
    fault numbers. Also add group fault stats too.
    
    Also couple of more modifications are added here:
    
    1. Format changes:
    
      - Previously we would list two entries per node, one for private
        and one for shared. Also the home node info was listed in each entry.
    
      - Now preferred node, total_faults and current node are
        displayed separately.
    
      - Now there is one entry per node, that lists private,shared task and
        group faults.
    
    2. Unit changes:
    
      - p->numa_pages_migrated was getting reset after every read of
        /proc/pid/sched. It's more useful to have absolute numbers since
        differential migrations between two accesses can be more easily
        calculated.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Iulia Manda <iulia.manda21@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1435252903-1081-4-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index f1dcd1d390c1..4222ec50ab88 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -517,11 +517,21 @@ __initcall(init_sched_debug_procfs);
 	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
 
 
+#ifdef CONFIG_NUMA_BALANCING
+void print_numa_stats(struct seq_file *m, int node, unsigned long tsf,
+		unsigned long tpf, unsigned long gsf, unsigned long gpf)
+{
+	SEQ_printf(m, "numa_faults node=%d ", node);
+	SEQ_printf(m, "task_private=%lu task_shared=%lu ", tsf, tpf);
+	SEQ_printf(m, "group_private=%lu group_shared=%lu\n", gsf, gpf);
+}
+#endif
+
+
 static void sched_show_numa(struct task_struct *p, struct seq_file *m)
 {
 #ifdef CONFIG_NUMA_BALANCING
 	struct mempolicy *pol;
-	int node, i;
 
 	if (p->mm)
 		P(mm->numa_scan_seq);
@@ -533,26 +543,12 @@ static void sched_show_numa(struct task_struct *p, struct seq_file *m)
 	mpol_get(pol);
 	task_unlock(p);
 
-	SEQ_printf(m, "numa_migrations, %ld\n", xchg(&p->numa_pages_migrated, 0));
-
-	for_each_online_node(node) {
-		for (i = 0; i < 2; i++) {
-			unsigned long nr_faults = -1;
-			int cpu_current, home_node;
-
-			if (p->numa_faults)
-				nr_faults = p->numa_faults[2*node + i];
-
-			cpu_current = !i ? (task_node(p) == node) :
-				(pol && node_isset(node, pol->v.nodes));
-
-			home_node = (p->numa_preferred_nid == node);
-
-			SEQ_printf(m, "numa_faults_memory, %d, %d, %d, %d, %ld\n",
-				i, node, cpu_current, home_node, nr_faults);
-		}
-	}
-
+	P(numa_pages_migrated);
+	P(numa_preferred_nid);
+	P(total_numa_faults);
+	SEQ_printf(m, "current_node=%d, numa_group_id=%d\n",
+			task_node(p), task_numa_group_id(p));
+	show_numa_stats(p, m);
 	mpol_put(pol);
 #endif
 }

commit e3d24d0a6048a826de5562d75dedb664d3a2a1b2
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Thu Jun 25 22:51:42 2015 +0530

    sched/numa: Show numa_group ID in /proc/sched_debug task listings
    
    Having the numa group ID in /proc/sched_debug helps to see how
    the numa groups have spread across the system.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Rik van Riel <riel@redhat.com>
    Cc: Iulia Manda <iulia.manda21@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1435252903-1081-3-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 315c68e015d9..f1dcd1d390c1 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -142,7 +142,7 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		0LL, 0L);
 #endif
 #ifdef CONFIG_NUMA_BALANCING
-	SEQ_printf(m, " %d", task_node(p));
+	SEQ_printf(m, " %d %d", task_node(p), task_numa_group_id(p));
 #endif
 #ifdef CONFIG_CGROUP_SCHED
 	SEQ_printf(m, " %s", task_group_path(task_group(p)));

commit 43224b96af3154cedd7220f7b90094905f07ac78
Merge: d70b3ef54cea 1cb6c2151850
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jun 22 18:57:44 2015 -0700

    Merge branch 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull timer updates from Thomas Gleixner:
     "A rather largish update for everything time and timer related:
    
       - Cache footprint optimizations for both hrtimers and timer wheel
    
       - Lower the NOHZ impact on systems which have NOHZ or timer migration
         disabled at runtime.
    
       - Optimize run time overhead of hrtimer interrupt by making the clock
         offset updates smarter
    
       - hrtimer cleanups and removal of restrictions to tackle some
         problems in sched/perf
    
       - Some more leap second tweaks
    
       - Another round of changes addressing the 2038 problem
    
       - First step to change the internals of clock event devices by
         introducing the necessary infrastructure
    
       - Allow constant folding for usecs/msecs_to_jiffies()
    
       - The usual pile of clockevent/clocksource driver updates
    
      The hrtimer changes contain updates to sched, perf and x86 as they
      depend on them plus changes all over the tree to cleanup API changes
      and redundant code, which got copied all over the place.  The y2038
      changes touch s390 to remove the last non 2038 safe code related to
      boot/persistant clock"
    
    * 'timers-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (114 commits)
      clocksource: Increase dependencies of timer-stm32 to limit build wreckage
      timer: Minimize nohz off overhead
      timer: Reduce timer migration overhead if disabled
      timer: Stats: Simplify the flags handling
      timer: Replace timer base by a cpu index
      timer: Use hlist for the timer wheel hash buckets
      timer: Remove FIFO "guarantee"
      timers: Sanitize catchup_timer_jiffies() usage
      hrtimer: Allow hrtimer::function() to free the timer
      seqcount: Introduce raw_write_seqcount_barrier()
      seqcount: Rename write_seqcount_barrier()
      hrtimer: Fix hrtimer_is_queued() hole
      hrtimer: Remove HRTIMER_STATE_MIGRATE
      selftest: Timers: Avoid signal deadlock in leap-a-day
      timekeeping: Copy the shadow-timekeeper over the real timekeeper last
      clockevents: Check state instead of mode in suspend/resume path
      selftests: timers: Add leap-second timer edge testing to leap-a-day.c
      ntp: Do leapsecond adjustment in adjtimex read path
      time: Prevent early expiry of hrtimers[CLOCK_REALTIME] at the leap second edge
      ntp: Introduce and use SECS_PER_DAY macro instead of 86400
      ...

commit 82a0d2762699b95d6ce4114d00dc1865df9b0df3
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Mon Jun 8 13:40:41 2015 +0530

    sched/debug: Add sum_sleep_runtime to /proc/<pid>/sched
    
    When CONFIG_SCHEDSTATS is enabled, /proc/<pid>/sched prints almost all
    sched statistics except sum_sleep_runtime. Since sum_sleep_runtime is
    a good info to collect, add this it to /proc/<pid>/sched.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1433751041-11724-4-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 7dc547eb56d7..704683cc9042 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -584,6 +584,7 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 	nr_switches = p->nvcsw + p->nivcsw;
 
 #ifdef CONFIG_SCHEDSTATS
+	PN(se.statistics.sum_sleep_runtime);
 	PN(se.statistics.wait_start);
 	PN(se.statistics.sleep_start);
 	PN(se.statistics.block_start);

commit c5f3ab1c3b2e277cca6462415038dab02b4ad396
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Mon Jun 8 13:40:40 2015 +0530

    sched/debug: Replace vruntime with wait_sum in /proc/sched_debug
    
    Within runnable tasks in /proc/sched_debug, vruntime is printed twice,
    once as tree-key and again as exec-runtime.
    
    Since exec-runtime isnt populated in !CONFIG_SCHEDSTATS, use this field
    to print wait_sum.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1433751041-11724-3-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 59cb603081a3..7dc547eb56d7 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -132,7 +132,7 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		p->prio);
 #ifdef CONFIG_SCHEDSTATS
 	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
-		SPLIT_NS(p->se.vruntime),
+		SPLIT_NS(p->se.statistics.wait_sum),
 		SPLIT_NS(p->se.sum_exec_runtime),
 		SPLIT_NS(p->se.statistics.sum_sleep_runtime));
 #else
@@ -158,7 +158,7 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 	SEQ_printf(m,
 	"\nrunnable tasks:\n"
 	"            task   PID         tree-key  switches  prio"
-	"     exec-runtime         sum-exec        sum-sleep\n"
+	"     wait-time             sum-exec        sum-sleep\n"
 	"------------------------------------------------------"
 	"----------------------------------------------------\n");
 

commit 33d6176eb12d1b0ae6d2f672b47367fd90726b91
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Mon Jun 8 13:40:39 2015 +0530

    sched/debug: Properly format runnable tasks in /proc/sched_debug
    
    With !CONFIG_SCHEDSTATS, runnable tasks in /proc/sched_debug has too
    many columns than required. Fix this by printing appropriate columns.
    
    While at this, print sum_exec_runtime, since this information is
    available even in !CONFIG_SCHEDSTATS case.
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1433751041-11724-2-git-send-email-srikar@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index a245c1fc6f0a..59cb603081a3 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -136,8 +136,10 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		SPLIT_NS(p->se.sum_exec_runtime),
 		SPLIT_NS(p->se.statistics.sum_sleep_runtime));
 #else
-	SEQ_printf(m, "%15Ld %15Ld %15Ld.%06ld %15Ld.%06ld %15Ld.%06ld",
-		0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);
+	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
+		0LL, 0L,
+		SPLIT_NS(p->se.sum_exec_runtime),
+		0LL, 0L);
 #endif
 #ifdef CONFIG_NUMA_BALANCING
 	SEQ_printf(m, " %d", task_node(p));

commit b484403b9abe5f444ae2fee6a249759bb3c35bcf
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 23 13:58:09 2015 +0200

    sched: debug: Remove the cfs bandwidth timer_active printout
    
    The struct member is gone.
    
    Reported-by: fengguang.wu@intel.com>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index a245c1fc6f0a..f94724eda407 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -230,8 +230,6 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 #endif
 #endif
 #ifdef CONFIG_CFS_BANDWIDTH
-	SEQ_printf(m, "  .%-30s: %d\n", "tg->cfs_bandwidth.timer_active",
-			cfs_rq->tg->cfs_bandwidth.timer_active);
 	SEQ_printf(m, "  .%-30s: %d\n", "throttled",
 			cfs_rq->throttled);
 	SEQ_printf(m, "  .%-30s: %d\n", "throttle_count",

commit 21f4486630b0bd1b6dbcc04f61836987fa54278f
Author: Morten Rasmussen <morten.rasmussen@arm.com>
Date:   Fri Feb 27 16:54:05 2015 +0100

    sched: Track group sched_entity usage contributions
    
    Add usage contribution tracking for group entities. Unlike
    se->avg.load_avg_contrib, se->avg.utilization_avg_contrib for group
    entities is the sum of se->avg.utilization_avg_contrib for all entities on the
    group runqueue.
    
    It is _not_ influenced in any way by the task group h_load. Hence it is
    representing the actual cpu usage of the group, not its intended load
    contribution which may differ significantly from the utilization on
    lightly utilized systems.
    
    Signed-off-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Morten.Rasmussen@arm.com
    Cc: Paul Turner <pjt@google.com>
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1425052454-25797-3-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 578ff83d1d1a..a245c1fc6f0a 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -94,8 +94,10 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	P(se->load.weight);
 #ifdef CONFIG_SMP
 	P(se->avg.runnable_avg_sum);
+	P(se->avg.running_avg_sum);
 	P(se->avg.avg_period);
 	P(se->avg.load_avg_contrib);
+	P(se->avg.utilization_avg_contrib);
 	P(se->avg.decay_count);
 #endif
 #undef PN

commit 36ee28e45df50c2c8624b978335516e42d84ae1f
Author: Vincent Guittot <vincent.guittot@linaro.org>
Date:   Fri Feb 27 16:54:04 2015 +0100

    sched: Add sched_avg::utilization_avg_contrib
    
    Add new statistics which reflect the average time a task is running on the CPU
    and the sum of these running time of the tasks on a runqueue. The latter is
    named utilization_load_avg.
    
    This patch is based on the usage metric that was proposed in the 1st
    versions of the per-entity load tracking patchset by Paul Turner
    <pjt@google.com> but that has be removed afterwards. This version differs from
    the original one in the sense that it's not linked to task_group.
    
    The rq's utilization_load_avg will be used to check if a rq is overloaded or
    not instead of trying to compute how many tasks a group of CPUs can handle.
    
    Rename runnable_avg_period into avg_period as it is now used with both
    runnable_avg_sum and running_avg_sum.
    
    Add some descriptions of the variables to explain their differences.
    
    Signed-off-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Morten Rasmussen <morten.rasmussen@arm.com>
    Cc: Paul Turner <pjt@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Ben Segall <bsegall@google.com>
    Cc: Morten.Rasmussen@arm.com
    Cc: Paul Turner <pjt@google.com>
    Cc: dietmar.eggemann@arm.com
    Cc: efault@gmx.de
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: linaro-kernel@lists.linaro.org
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: riel@redhat.com
    Link: http://lkml.kernel.org/r/1425052454-25797-2-git-send-email-vincent.guittot@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 8baaf858d25c..578ff83d1d1a 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -71,7 +71,7 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	if (!se) {
 		struct sched_avg *avg = &cpu_rq(cpu)->avg;
 		P(avg->runnable_avg_sum);
-		P(avg->runnable_avg_period);
+		P(avg->avg_period);
 		return;
 	}
 
@@ -94,7 +94,7 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	P(se->load.weight);
 #ifdef CONFIG_SMP
 	P(se->avg.runnable_avg_sum);
-	P(se->avg.runnable_avg_period);
+	P(se->avg.avg_period);
 	P(se->avg.load_avg_contrib);
 	P(se->avg.decay_count);
 #endif
@@ -214,6 +214,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "blocked_load_avg",
 			cfs_rq->blocked_load_avg);
+	SEQ_printf(m, "  .%-30s: %ld\n", "utilization_load_avg",
+			cfs_rq->utilization_load_avg);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_contrib",
 			cfs_rq->tg_load_contrib);
@@ -636,8 +638,10 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 	P(se.load.weight);
 #ifdef CONFIG_SMP
 	P(se.avg.runnable_avg_sum);
-	P(se.avg.runnable_avg_period);
+	P(se.avg.running_avg_sum);
+	P(se.avg.avg_period);
 	P(se.avg.load_avg_contrib);
+	P(se.avg.utilization_avg_contrib);
 	P(se.avg.decay_count);
 #endif
 	P(policy);

commit 5a5375977b721503e4d6b37ab8982902cd2d10b3
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Jan 5 11:18:12 2015 +0100

    sched/debug: Print rq->clock_task
    
    We seem to have forgotten adding it to the debug output like
    forever... do so now.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20150105103554.495253233@infradead.org
    Cc: umgwanakikbuti@gmail.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 92cc52001e74..8baaf858d25c 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -305,6 +305,7 @@ do {									\
 	PN(next_balance);
 	SEQ_printf(m, "  .%-30s: %ld\n", "curr->pid", (long)(task_pid_nr(rq->curr)));
 	PN(clock);
+	PN(clock_task);
 	P(cpu_load[0]);
 	P(cpu_load[1]);
 	P(cpu_load[2]);

commit 44dba3d5d6a10685fb15bd1954e62016334825e0
Author: Iulia Manda <iulia.manda21@gmail.com>
Date:   Fri Oct 31 02:13:31 2014 +0200

    sched: Refactor task_struct to use numa_faults instead of numa_* pointers
    
    This patch simplifies task_struct by removing the four numa_* pointers
    in the same array and replacing them with the array pointer. By doing this,
    on x86_64, the size of task_struct is reduced by 3 ulong pointers (24 bytes on
    x86_64).
    
    A new parameter is added to the task_faults_idx function so that it can return
    an index to the correct offset, corresponding with the old precalculated
    pointers.
    
    All of the code in sched/ that depended on task_faults_idx and numa_* was
    changed in order to match the new logic.
    
    Signed-off-by: Iulia Manda <iulia.manda21@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: mgorman@suse.de
    Cc: dave@stgolabs.net
    Cc: riel@redhat.com
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20141031001331.GA30662@winterfell
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index eeb6046d60c7..92cc52001e74 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -535,8 +535,8 @@ static void sched_show_numa(struct task_struct *p, struct seq_file *m)
 			unsigned long nr_faults = -1;
 			int cpu_current, home_node;
 
-			if (p->numa_faults_memory)
-				nr_faults = p->numa_faults_memory[2*node + i];
+			if (p->numa_faults)
+				nr_faults = p->numa_faults[2*node + i];
 
 			cpu_current = !i ? (task_node(p) == node) :
 				(pol && node_isset(node, pol->v.nodes));

commit acb32132ec0433c03bed750f3e9508dc29db0328
Author: Wanpeng Li <wanpeng.li@linux.intel.com>
Date:   Fri Oct 31 06:39:33 2014 +0800

    sched/deadline: Add deadline rq status print
    
    This patch add deadline rq status print.
    
    Signed-off-by: Wanpeng Li <wanpeng.li@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Juri Lelli <juri.lelli@arm.com>
    Cc: Kirill Tkhai <ktkhai@parallels.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1414708776-124078-3-git-send-email-wanpeng.li@linux.intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index ce33780d8f20..eeb6046d60c7 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -261,6 +261,12 @@ void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)
 #undef P
 }
 
+void print_dl_rq(struct seq_file *m, int cpu, struct dl_rq *dl_rq)
+{
+	SEQ_printf(m, "\ndl_rq[%d]:\n", cpu);
+	SEQ_printf(m, "  .%-30s: %ld\n", "dl_nr_running", dl_rq->dl_nr_running);
+}
+
 extern __read_mostly int sched_clock_running;
 
 static void print_cpu(struct seq_file *m, int cpu)
@@ -329,6 +335,7 @@ do {									\
 	spin_lock_irqsave(&sched_debug_lock, flags);
 	print_cfs_stats(m, cpu);
 	print_rt_stats(m, cpu);
+	print_dl_stats(m, cpu);
 
 	print_rq(m, rq, cpu);
 	spin_unlock_irqrestore(&sched_debug_lock, flags);

commit 5bd96ab6fef66ec6b9f54134364e618fd0f8f2f3
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Sep 21 21:33:41 2014 +0200

    sched: print_rq(): Don't use tasklist_lock
    
    read_lock_irqsave(tasklist_lock) in print_rq() looks strange. We do
    not need to disable irqs, and they are already disabled by the caller.
    
    And afaics this lock buys nothing, we can rely on rcu_read_lock().
    In this case it makes sense to also move rcu_read_lock/unlock from
    the caller to print_rq().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Kirill Tkhai <tkhai@yandex.ru>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20140921193341.GA28628@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index c7fe1ea0e8ab..ce33780d8f20 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -150,7 +150,6 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 {
 	struct task_struct *g, *p;
-	unsigned long flags;
 
 	SEQ_printf(m,
 	"\nrunnable tasks:\n"
@@ -159,14 +158,14 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 	"------------------------------------------------------"
 	"----------------------------------------------------\n");
 
-	read_lock_irqsave(&tasklist_lock, flags);
+	rcu_read_lock();
 	for_each_process_thread(g, p) {
 		if (task_cpu(p) != rq_cpu)
 			continue;
 
 		print_task(m, rq, p);
 	}
-	read_unlock_irqrestore(&tasklist_lock, flags);
+	rcu_read_unlock();
 }
 
 void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
@@ -331,9 +330,7 @@ do {									\
 	print_cfs_stats(m, cpu);
 	print_rt_stats(m, cpu);
 
-	rcu_read_lock();
 	print_rq(m, rq, cpu);
-	rcu_read_unlock();
 	spin_unlock_irqrestore(&sched_debug_lock, flags);
 	SEQ_printf(m, "\n");
 }

commit d38e83c715270cc2e137bbf6f25206c8c023896b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Aug 13 21:19:56 2014 +0200

    sched: s/do_each_thread/for_each_process_thread/ in debug.c
    
    Change kernel/sched/debug.c to use for_each_process_thread().
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: Hidetoshi Seto <seto.hidetoshi@jp.fujitsu.com>
    Cc: Frank Mayhar <fmayhar@google.com>
    Cc: Frederic Weisbecker <fweisbec@redhat.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Sanjay Rao <srao@redhat.com>
    Cc: Larry Woodman <lwoodman@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20140813191956.GA19324@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 627b3c34b821..c7fe1ea0e8ab 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -160,14 +160,12 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 	"----------------------------------------------------\n");
 
 	read_lock_irqsave(&tasklist_lock, flags);
-
-	do_each_thread(g, p) {
+	for_each_process_thread(g, p) {
 		if (task_cpu(p) != rq_cpu)
 			continue;
 
 		print_task(m, rq, p);
-	} while_each_thread(g, p);
-
+	}
 	read_unlock_irqrestore(&tasklist_lock, flags);
 }
 

commit b0ab99e7736af88b8ac1b7ae50ea287fffa2badc
Author: Mateusz Guzik <mguzik@redhat.com>
Date:   Sat Jun 14 15:00:09 2014 +0200

    sched: Fix possible divide by zero in avg_atom() calculation
    
    proc_sched_show_task() does:
    
      if (nr_switches)
            do_div(avg_atom, nr_switches);
    
    nr_switches is unsigned long and do_div truncates it to 32 bits, which
    means it can test non-zero on e.g. x86-64 and be truncated to zero for
    division.
    
    Fix the problem by using div64_ul() instead.
    
    As a side effect calculations of avg_atom for big nr_switches are now correct.
    
    Signed-off-by: Mateusz Guzik <mguzik@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: stable@vger.kernel.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1402750809-31991-1-git-send-email-mguzik@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 695f9773bb60..627b3c34b821 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -608,7 +608,7 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 
 		avg_atom = p->se.sum_exec_runtime;
 		if (nr_switches)
-			do_div(avg_atom, nr_switches);
+			avg_atom = div64_ul(avg_atom, nr_switches);
 		else
 			avg_atom = -1LL;
 

commit 32d01dc7be4e725ab85ce1d74e8f4adc02ad68dd
Merge: 68114e5eb862 1ec41830e087
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 3 13:05:42 2014 -0700

    Merge branch 'for-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup updates from Tejun Heo:
     "A lot updates for cgroup:
    
       - The biggest one is cgroup's conversion to kernfs.  cgroup took
         after the long abandoned vfs-entangled sysfs implementation and
         made it even more convoluted over time.  cgroup's internal objects
         were fused with vfs objects which also brought in vfs locking and
         object lifetime rules.  Naturally, there are places where vfs rules
         don't fit and nasty hacks, such as credential switching or lock
         dance interleaving inode mutex and cgroup_mutex with object serial
         number comparison thrown in to decide whether the operation is
         actually necessary, needed to be employed.
    
         After conversion to kernfs, internal object lifetime and locking
         rules are mostly isolated from vfs interactions allowing shedding
         of several nasty hacks and overall simplification.  This will also
         allow implmentation of operations which may affect multiple cgroups
         which weren't possible before as it would have required nesting
         i_mutexes.
    
       - Various simplifications including dropping of module support,
         easier cgroup name/path handling, simplified cgroup file type
         handling and task_cg_lists optimization.
    
       - Prepatory changes for the planned unified hierarchy, which is still
         a patchset away from being actually operational.  The dummy
         hierarchy is updated to serve as the default unified hierarchy.
         Controllers which aren't claimed by other hierarchies are
         associated with it, which BTW was what the dummy hierarchy was for
         anyway.
    
       - Various fixes from Li and others.  This pull request includes some
         patches to add missing slab.h to various subsystems.  This was
         triggered xattr.h include removal from cgroup.h.  cgroup.h
         indirectly got included a lot of files which brought in xattr.h
         which brought in slab.h.
    
      There are several merge commits - one to pull in kernfs updates
      necessary for converting cgroup (already in upstream through
      driver-core), others for interfering changes in the fixes branch"
    
    * 'for-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup: (74 commits)
      cgroup: remove useless argument from cgroup_exit()
      cgroup: fix spurious lockdep warning in cgroup_exit()
      cgroup: Use RCU_INIT_POINTER(x, NULL) in cgroup.c
      cgroup: break kernfs active_ref protection in cgroup directory operations
      cgroup: fix cgroup_taskset walking order
      cgroup: implement CFTYPE_ONLY_ON_DFL
      cgroup: make cgrp_dfl_root mountable
      cgroup: drop const from @buffer of cftype->write_string()
      cgroup: rename cgroup_dummy_root and related names
      cgroup: move ->subsys_mask from cgroupfs_root to cgroup
      cgroup: treat cgroup_dummy_root as an equivalent hierarchy during rebinding
      cgroup: remove NULL checks from [pr_cont_]cgroup_{name|path}()
      cgroup: use cgroup_setup_root() to initialize cgroup_dummy_root
      cgroup: reorganize cgroup bootstrapping
      cgroup: relocate setting of CGRP_DEAD
      cpuset: use rcu_read_lock() to protect task_cs()
      cgroup_freezer: document freezer_fork() subtleties
      cgroup: update cgroup_transfer_tasks() to either succeed or fail
      cgroup: drop task_lock() protection around task->cgroups
      cgroup: update how a newly forked task gets associated with css_set
      ...

commit e61734c55c24cdf11b07e52a74aec4dc4a7f4bd0
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Feb 12 09:29:50 2014 -0500

    cgroup: remove cgroup->name
    
    cgroup->name handling became quite complicated over time involving
    dedicated struct cgroup_name for RCU protection.  Now that cgroup is
    on kernfs, we can drop all of it and simply use kernfs_name/path() and
    friends.  Replace cgroup->name and all related code with kernfs
    name/path constructs.
    
    * Reimplement cgroup_name() and cgroup_path() as thin wrappers on top
      of kernfs counterparts, which involves semantic changes.
      pr_cont_cgroup_name() and pr_cont_cgroup_path() added.
    
    * cgroup->name handling dropped from cgroup_rename().
    
    * All users of cgroup_name/path() updated to the new semantics.  Users
      which were formatting the string just to printk them are converted
      to use pr_cont_cgroup_name/path() instead, which simplifies things
      quite a bit.  As cgroup_name() no longer requires RCU read lock
      around it, RCU lockings which were protecting only cgroup_name() are
      removed.
    
    v2: Comment above oom_info_lock updated as suggested by Michal.
    
    v3: dummy_top doesn't have a kn associated and
        pr_cont_cgroup_name/path() ended up calling the matching kernfs
        functions with NULL kn leading to oops.  Test for NULL kn and
        print "/" if so.  This issue was reported by Fengguang Wu.
    
    v4: Rebased on top of 0ab02ca8f887 ("cgroup: protect modifications to
        cgroup_idr with cgroup_mutex").
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Michal Hocko <mhocko@suse.cz>
    Acked-by: Li Zefan <lizefan@huawei.com>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Balbir Singh <bsingharora@gmail.com>
    Cc: KAMEZAWA Hiroyuki <kamezawa.hiroyu@jp.fujitsu.com>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index dd52e7ffb10e..30eee3b5293d 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -111,8 +111,7 @@ static char *task_group_path(struct task_group *tg)
 	if (autogroup_path(tg, group_path, PATH_MAX))
 		return group_path;
 
-	cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
-	return group_path;
+	return cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
 }
 #endif
 

commit 37e6bae8395a94b4dd934c92b02b9408be992365
Author: Alex Shi <alex.shi@linaro.org>
Date:   Thu Jan 23 18:39:54 2014 +0800

    sched: Add statistic for newidle load balance cost
    
    Tracking rq->max_idle_balance_cost and sd->max_newidle_lb_cost.
    It's useful to know these values in debug mode.
    
    Signed-off-by: Alex Shi <alex.shi@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/52E0F3BF.5020904@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 31b908daaa1b..f3344c31632a 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -321,6 +321,7 @@ do {									\
 	P(sched_goidle);
 #ifdef CONFIG_SMP
 	P64(avg_idle);
+	P64(max_idle_balance_cost);
 #endif
 
 	P(ttwu_count);

commit ff1df896aef8e0ec1556a5c44f424bd45bfa2cbe
Author: Rik van Riel <riel@redhat.com>
Date:   Mon Jan 27 17:03:41 2014 -0500

    sched/numa: Rename p->numa_faults to numa_faults_memory
    
    In order to get a more consistent naming scheme, making it clear
    which fault statistics track memory locality, and which track
    CPU locality, rename the memory fault statistics.
    
    Suggested-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Rik van Riel <riel@redhat.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Chegu Vinod <chegu_vinod@hp.com>
    Link: http://lkml.kernel.org/r/1390860228-21539-3-git-send-email-riel@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index dd52e7ffb10e..31b908daaa1b 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -533,15 +533,15 @@ static void sched_show_numa(struct task_struct *p, struct seq_file *m)
 			unsigned long nr_faults = -1;
 			int cpu_current, home_node;
 
-			if (p->numa_faults)
-				nr_faults = p->numa_faults[2*node + i];
+			if (p->numa_faults_memory)
+				nr_faults = p->numa_faults_memory[2*node + i];
 
 			cpu_current = !i ? (task_node(p) == node) :
 				(pol && node_isset(node, pol->v.nodes));
 
 			home_node = (p->numa_preferred_nid == node);
 
-			SEQ_printf(m, "numa_faults, %d, %d, %d, %d, %ld\n",
+			SEQ_printf(m, "numa_faults_memory, %d, %d, %d, %d, %ld\n",
 				i, node, cpu_current, home_node, nr_faults);
 		}
 	}

commit 35af99e646c7f7ea46dc2977601e9e71a51dadd5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 28 19:38:42 2013 +0100

    sched/clock, x86: Use a static_key for sched_clock_stable
    
    In order to avoid the runtime condition and variable load turn
    sched_clock_stable into a static_key.
    
    Also provide a shorter implementation of local_clock() and
    cpu_clock(int) when sched_clock_stable==1.
    
                            MAINLINE   PRE       POST
    
        sched_clock_stable: 1          1         1
        (cold) sched_clock: 329841     221876    215295
        (cold) local_clock: 301773     234692    220773
        (warm) sched_clock: 38375      25602     25659
        (warm) local_clock: 100371     33265     27242
        (warm) rdtsc:       27340      24214     24208
        sched_clock_stable: 0          0         0
        (cold) sched_clock: 382634     235941    237019
        (cold) local_clock: 396890     297017    294819
        (warm) sched_clock: 38194      25233     25609
        (warm) local_clock: 143452     71234     71232
        (warm) rdtsc:       27345      24245     24243
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-eummbdechzz37mwmpags1gjr@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 374fe04a5e6e..dd52e7ffb10e 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -371,7 +371,7 @@ static void sched_debug_header(struct seq_file *m)
 	PN(cpu_clk);
 	P(jiffies);
 #ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
-	P(sched_clock_stable);
+	P(sched_clock_stable());
 #endif
 #undef PN
 #undef P

commit de1b301a19754778ddd9f908d266ffe1c010b2cf
Author: Wanpeng Li <liwanp@linux.vnet.ibm.com>
Date:   Thu Dec 12 15:23:24 2013 +0800

    sched/numa: Use wrapper function task_node to get node which task is on
    
    Use wrapper function task_node to get node which task is on.
    
    Signed-off-by: Wanpeng Li <liwanp@linux.vnet.ibm.com>
    Acked-by: Mel Gorman <mgorman@suse.de>
    Reviewed-by: Naoya Horiguchi <n-horiguchi@ah.jp.nec.com>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Acked-by: David Rientjes <rientjes@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1386833006-6600-2-git-send-email-liwanp@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 5c34d1817e8f..374fe04a5e6e 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -139,7 +139,7 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);
 #endif
 #ifdef CONFIG_NUMA_BALANCING
-	SEQ_printf(m, " %d", cpu_to_node(task_cpu(p)));
+	SEQ_printf(m, " %d", task_node(p));
 #endif
 #ifdef CONFIG_CGROUP_SCHED
 	SEQ_printf(m, " %s", task_group_path(task_group(p)));

commit f9f9ffc237dd924f048204e8799da74f9ecf40cf
Author: Ben Segall <bsegall@google.com>
Date:   Wed Oct 16 11:16:32 2013 -0700

    sched: Avoid throttle_cfs_rq() racing with period_timer stopping
    
    throttle_cfs_rq() doesn't check to make sure that period_timer is running,
    and while update_curr/assign_cfs_runtime does, a concurrently running
    period_timer on another cpu could cancel itself between this cpu's
    update_curr and throttle_cfs_rq(). If there are no other cfs_rqs running
    in the tg to restart the timer, this causes the cfs_rq to be stranded
    forever.
    
    Fix this by calling __start_cfs_bandwidth() in throttle if the timer is
    inactive.
    
    (Also add some sched_debug lines for cfs_bandwidth.)
    
    Tested: make a run/sleep task in a cgroup, loop switching the cgroup
    between 1ms/100ms quota and unlimited, checking for timer_active=0 and
    throttled=1 as a failure. With the throttle_cfs_rq() change commented out
    this fails, with the full patch it passes.
    
    Signed-off-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: pjt@google.com
    Link: http://lkml.kernel.org/r/20131016181632.22647.84174.stgit@sword-of-the-dawn.mtv.corp.google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index e6ba5e31c7ca..5c34d1817e8f 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -229,6 +229,14 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			atomic_read(&cfs_rq->tg->runnable_avg));
 #endif
 #endif
+#ifdef CONFIG_CFS_BANDWIDTH
+	SEQ_printf(m, "  .%-30s: %d\n", "tg->cfs_bandwidth.timer_active",
+			cfs_rq->tg->cfs_bandwidth.timer_active);
+	SEQ_printf(m, "  .%-30s: %d\n", "throttled",
+			cfs_rq->throttled);
+	SEQ_printf(m, "  .%-30s: %d\n", "throttle_count",
+			cfs_rq->throttle_count);
+#endif
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
 	print_cfs_group_stats(m, cpu, cfs_rq->tg);

commit b32e86b4301e345611f0446265f782a229faadf6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Oct 7 11:29:30 2013 +0100

    sched/numa: Add debugging
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Reviewed-by: Rik van Riel <riel@redhat.com>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Mel Gorman <mgorman@suse.de>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Andrea Arcangeli <aarcange@redhat.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: http://lkml.kernel.org/r/1381141781-10992-53-git-send-email-mgorman@suse.de

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 196559994f7c..e6ba5e31c7ca 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -15,6 +15,7 @@
 #include <linux/seq_file.h>
 #include <linux/kallsyms.h>
 #include <linux/utsname.h>
+#include <linux/mempolicy.h>
 
 #include "sched.h"
 
@@ -137,6 +138,9 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 	SEQ_printf(m, "%15Ld %15Ld %15Ld.%06ld %15Ld.%06ld %15Ld.%06ld",
 		0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);
 #endif
+#ifdef CONFIG_NUMA_BALANCING
+	SEQ_printf(m, " %d", cpu_to_node(task_cpu(p)));
+#endif
 #ifdef CONFIG_CGROUP_SCHED
 	SEQ_printf(m, " %s", task_group_path(task_group(p)));
 #endif
@@ -159,7 +163,7 @@ static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
 	read_lock_irqsave(&tasklist_lock, flags);
 
 	do_each_thread(g, p) {
-		if (!p->on_rq || task_cpu(p) != rq_cpu)
+		if (task_cpu(p) != rq_cpu)
 			continue;
 
 		print_task(m, rq, p);
@@ -345,7 +349,7 @@ static void sched_debug_header(struct seq_file *m)
 	cpu_clk = local_clock();
 	local_irq_restore(flags);
 
-	SEQ_printf(m, "Sched Debug Version: v0.10, %s %.*s\n",
+	SEQ_printf(m, "Sched Debug Version: v0.11, %s %.*s\n",
 		init_utsname()->release,
 		(int)strcspn(init_utsname()->version, " "),
 		init_utsname()->version);
@@ -488,6 +492,56 @@ static int __init init_sched_debug_procfs(void)
 
 __initcall(init_sched_debug_procfs);
 
+#define __P(F) \
+	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)F)
+#define P(F) \
+	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)p->F)
+#define __PN(F) \
+	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
+#define PN(F) \
+	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
+
+
+static void sched_show_numa(struct task_struct *p, struct seq_file *m)
+{
+#ifdef CONFIG_NUMA_BALANCING
+	struct mempolicy *pol;
+	int node, i;
+
+	if (p->mm)
+		P(mm->numa_scan_seq);
+
+	task_lock(p);
+	pol = p->mempolicy;
+	if (pol && !(pol->flags & MPOL_F_MORON))
+		pol = NULL;
+	mpol_get(pol);
+	task_unlock(p);
+
+	SEQ_printf(m, "numa_migrations, %ld\n", xchg(&p->numa_pages_migrated, 0));
+
+	for_each_online_node(node) {
+		for (i = 0; i < 2; i++) {
+			unsigned long nr_faults = -1;
+			int cpu_current, home_node;
+
+			if (p->numa_faults)
+				nr_faults = p->numa_faults[2*node + i];
+
+			cpu_current = !i ? (task_node(p) == node) :
+				(pol && node_isset(node, pol->v.nodes));
+
+			home_node = (p->numa_preferred_nid == node);
+
+			SEQ_printf(m, "numa_faults, %d, %d, %d, %d, %ld\n",
+				i, node, cpu_current, home_node, nr_faults);
+		}
+	}
+
+	mpol_put(pol);
+#endif
+}
+
 void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 {
 	unsigned long nr_switches;
@@ -591,6 +645,8 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 		SEQ_printf(m, "%-45s:%21Ld\n",
 			   "clock-delta", (long long)(t1-t0));
 	}
+
+	sched_show_numa(p, m);
 }
 
 void proc_sched_set_task(struct task_struct *p)

commit fc840914e9b07ab4685c195e1e54e58de4f84c03
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Sep 9 13:01:41 2013 +0200

    sched/debug: Take PID namespace into account
    
    Emmanuel reported that /proc/sched_debug didn't report the right PIDs
    when using namespaces, cure this.
    
    Reported-by: Emmanuel Deloget <emmanuel.deloget@efixo.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20130909110141.GM31370@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index e076bddd4c66..196559994f7c 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -124,7 +124,7 @@ print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
 		SEQ_printf(m, " ");
 
 	SEQ_printf(m, "%15s %5d %9Ld.%06ld %9Ld %5d ",
-		p->comm, p->pid,
+		p->comm, task_pid_nr(p),
 		SPLIT_NS(p->se.vruntime),
 		(long long)(p->nvcsw + p->nivcsw),
 		p->prio);
@@ -289,7 +289,7 @@ do {									\
 	P(nr_load_updates);
 	P(nr_uninterruptible);
 	PN(next_balance);
-	P(curr->pid);
+	SEQ_printf(m, "  .%-30s: %ld\n", "curr->pid", (long)(task_pid_nr(rq->curr)));
 	PN(clock);
 	P(cpu_load[0]);
 	P(cpu_load[1]);
@@ -492,7 +492,7 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 {
 	unsigned long nr_switches;
 
-	SEQ_printf(m, "%s (%d, #threads: %d)\n", p->comm, p->pid,
+	SEQ_printf(m, "%s (%d, #threads: %d)\n", p->comm, task_pid_nr(p),
 						get_nr_threads(p));
 	SEQ_printf(m,
 		"---------------------------------------------------------"

commit 333bb864f192015a53b5060b829089decd0220ef
Author: Alex Shi <alex.shi@intel.com>
Date:   Fri Jun 28 19:10:35 2013 +0800

    sched/debug: Remove CONFIG_FAIR_GROUP_SCHED mask
    
    Now that we are using runnable load avg in sched balance, we don't
    need to keep it under CONFIG_FAIR_GROUP_SCHED.
    
    Also align the code style to #ifdef instead of #if defined() and
    reorder the tg output info.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Cc: pjt@google.com
    Cc: kamalesh@linux.vnet.ibm.com
    Cc: peterz@infradead.org
    Link: http://lkml.kernel.org/r/1372417835-4698-1-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 159561415d13..e076bddd4c66 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -209,22 +209,24 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->nr_spread_over);
 	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", cfs_rq->nr_running);
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
-#ifdef CONFIG_FAIR_GROUP_SCHED
 #ifdef CONFIG_SMP
 	SEQ_printf(m, "  .%-30s: %ld\n", "runnable_load_avg",
 			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "blocked_load_avg",
 			cfs_rq->blocked_load_avg);
-	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_avg",
-			atomic_long_read(&cfs_rq->tg->load_avg));
+#ifdef CONFIG_FAIR_GROUP_SCHED
 	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_contrib",
 			cfs_rq->tg_load_contrib);
 	SEQ_printf(m, "  .%-30s: %d\n", "tg_runnable_contrib",
 			cfs_rq->tg_runnable_contrib);
+	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_avg",
+			atomic_long_read(&cfs_rq->tg->load_avg));
 	SEQ_printf(m, "  .%-30s: %d\n", "tg->runnable_avg",
 			atomic_read(&cfs_rq->tg->runnable_avg));
 #endif
+#endif
 
+#ifdef CONFIG_FAIR_GROUP_SCHED
 	print_cfs_group_stats(m, cpu, cfs_rq->tg);
 #endif
 }
@@ -567,7 +569,7 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 		   "nr_involuntary_switches", (long long)p->nivcsw);
 
 	P(se.load.weight);
-#if defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)
+#ifdef CONFIG_SMP
 	P(se.avg.runnable_avg_sum);
 	P(se.avg.runnable_avg_period);
 	P(se.avg.load_avg_contrib);

commit add332a1523a09cf6d429933f1e2fb4ccdfe6479
Author: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
Date:   Thu Jun 27 22:20:05 2013 +0530

    sched/debug: Fix formatting of /proc/<PID>/sched
    
    This patch alters format string's width, to align all statistics
    at par with the longest struct sched_statistic member name under
    /proc/<PID>/sched.
    
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Cc: peterz@infradead.org
    Link: http://lkml.kernel.org/r/20130627165005.GA15583@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 626320985366..159561415d13 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -493,15 +493,16 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 	SEQ_printf(m, "%s (%d, #threads: %d)\n", p->comm, p->pid,
 						get_nr_threads(p));
 	SEQ_printf(m,
-		"---------------------------------------------------------\n");
+		"---------------------------------------------------------"
+		"----------\n");
 #define __P(F) \
-	SEQ_printf(m, "%-35s:%21Ld\n", #F, (long long)F)
+	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)F)
 #define P(F) \
-	SEQ_printf(m, "%-35s:%21Ld\n", #F, (long long)p->F)
+	SEQ_printf(m, "%-45s:%21Ld\n", #F, (long long)p->F)
 #define __PN(F) \
-	SEQ_printf(m, "%-35s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
+	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
 #define PN(F) \
-	SEQ_printf(m, "%-35s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
+	SEQ_printf(m, "%-45s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
 
 	PN(se.exec_start);
 	PN(se.vruntime);
@@ -560,9 +561,9 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 	}
 #endif
 	__P(nr_switches);
-	SEQ_printf(m, "%-35s:%21Ld\n",
+	SEQ_printf(m, "%-45s:%21Ld\n",
 		   "nr_voluntary_switches", (long long)p->nvcsw);
-	SEQ_printf(m, "%-35s:%21Ld\n",
+	SEQ_printf(m, "%-45s:%21Ld\n",
 		   "nr_involuntary_switches", (long long)p->nivcsw);
 
 	P(se.load.weight);
@@ -585,7 +586,7 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 
 		t0 = cpu_clock(this_cpu);
 		t1 = cpu_clock(this_cpu);
-		SEQ_printf(m, "%-35s:%21Ld\n",
+		SEQ_printf(m, "%-45s:%21Ld\n",
 			   "clock-delta", (long long)(t1-t0));
 	}
 }

commit 939fd731eb88a0cdd9058d0b0143563172a217d7
Author: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
Date:   Tue Jun 25 13:33:36 2013 +0530

    sched/debug: Add load-tracking statistics to task
    
    At present we print per-entity load-tracking statistics for
    cfs_rq of cgroups/runqueues. Given that per task statistics
    is maintained, it can be used to know the contribution made
    by the task to its parenting cfs_rq level.
    
    This patch adds per-task load-tracking statistics to /proc/<PID>/sched.
    
    Signed-off-by: Kamalesh Babulal <kamalesh@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20130625080336.GA20175@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index d803989defc0..626320985366 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -566,6 +566,12 @@ void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
 		   "nr_involuntary_switches", (long long)p->nivcsw);
 
 	P(se.load.weight);
+#if defined(CONFIG_SMP) && defined(CONFIG_FAIR_GROUP_SCHED)
+	P(se.avg.runnable_avg_sum);
+	P(se.avg.runnable_avg_period);
+	P(se.avg.load_avg_contrib);
+	P(se.avg.decay_count);
+#endif
 	P(policy);
 	P(prio);
 #undef PN

commit bf5b986ed4d20428eeec3df4a03dbfebb9b6538c
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:54 2013 +0800

    sched/tg: Use 'unsigned long' for load variable in task group
    
    Since tg->load_avg is smaller than tg->load_weight, we don't need a
    atomic64_t variable for load_avg in 32 bit machine.
    The same reason for cfs_rq->tg_load_contrib.
    
    The atomic_long_t/unsigned long variable type are more efficient and
    convenience for them.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Tested-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-11-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 160afdc5cdff..d803989defc0 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -215,9 +215,9 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %ld\n", "blocked_load_avg",
 			cfs_rq->blocked_load_avg);
-	SEQ_printf(m, "  .%-30s: %lld\n", "tg_load_avg",
-			(unsigned long long)atomic64_read(&cfs_rq->tg->load_avg));
-	SEQ_printf(m, "  .%-30s: %lld\n", "tg_load_contrib",
+	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_avg",
+			atomic_long_read(&cfs_rq->tg->load_avg));
+	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_contrib",
 			cfs_rq->tg_load_contrib);
 	SEQ_printf(m, "  .%-30s: %d\n", "tg_runnable_contrib",
 			cfs_rq->tg_runnable_contrib);

commit 72a4cf20cb71a327c636c7042fdacc25abffc87c
Author: Alex Shi <alex.shi@intel.com>
Date:   Thu Jun 20 10:18:53 2013 +0800

    sched: Change cfs_rq load avg to unsigned long
    
    Since the 'u64 runnable_load_avg, blocked_load_avg' in cfs_rq struct are
    smaller than 'unsigned long' cfs_rq->load.weight. We don't need u64
    vaiables to describe them. unsigned long is more efficient and convenience.
    
    Signed-off-by: Alex Shi <alex.shi@intel.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    Tested-by: Vincent Guittot <vincent.guittot@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1371694737-29336-10-git-send-email-alex.shi@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 75024a673520..160afdc5cdff 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -211,9 +211,9 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 #ifdef CONFIG_SMP
-	SEQ_printf(m, "  .%-30s: %lld\n", "runnable_load_avg",
+	SEQ_printf(m, "  .%-30s: %ld\n", "runnable_load_avg",
 			cfs_rq->runnable_load_avg);
-	SEQ_printf(m, "  .%-30s: %lld\n", "blocked_load_avg",
+	SEQ_printf(m, "  .%-30s: %ld\n", "blocked_load_avg",
 			cfs_rq->blocked_load_avg);
 	SEQ_printf(m, "  .%-30s: %lld\n", "tg_load_avg",
 			(unsigned long long)atomic64_read(&cfs_rq->tg->load_avg));

commit dcad0fceae528e8007610308bad7e5a3370e5c39
Merge: f8ef15d6b9d8 7f6575f1fb96
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 26 19:42:08 2013 -0800

    Merge branch 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler fixes from Ingo Molnar.
    
    * 'sched-urgent-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip:
      cputime: Use local_clock() for full dynticks cputime accounting
      cputime: Constify timeval_to_cputime(timeval) argument
      sched: Move RR_TIMESLICE from sysctl.h to rt.h
      sched: Fix /proc/sched_debug failure on very very large systems
      sched: Fix /proc/sched_stat failure on very very large systems
      sched/core: Remove the obsolete and unused nr_uninterruptible() function

commit bbbfeac92beff40eb86c7f682a7f1395f9f0ae52
Author: Nathan Zimmer <nzimmer@sgi.com>
Date:   Thu Feb 21 15:15:09 2013 -0800

    sched: Fix /proc/sched_debug failure on very very large systems
    
    On systems with 4096 cores attemping to read /proc/sched_debug
    fails because we are trying to push all the data into a single
    kmalloc buffer.
    
    The issue is on these very large machines all the data will not
    fit in 4mb.
    
    A better solution is to not us the single_open mechanism but to
    provide our own seq_operations and treat each cpu as an
    individual record.
    
    The output should be identical to the previous version.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Nathan Zimmer <nzimmer@sgi.com>
    Cc: Peter Zijlstra <peterz@infradead.org>)
    [ Whitespace fixlet]
    [ Fix spello in comment]
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 7ae4c4c5420e..c496eb3c6459 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -269,11 +269,11 @@ static void print_cpu(struct seq_file *m, int cpu)
 	{
 		unsigned int freq = cpu_khz ? : 1;
 
-		SEQ_printf(m, "\ncpu#%d, %u.%03u MHz\n",
+		SEQ_printf(m, "cpu#%d, %u.%03u MHz\n",
 			   cpu, freq / 1000, (freq % 1000));
 	}
 #else
-	SEQ_printf(m, "\ncpu#%d\n", cpu);
+	SEQ_printf(m, "cpu#%d\n", cpu);
 #endif
 
 #define P(x)								\
@@ -330,6 +330,7 @@ do {									\
 	print_rq(m, rq, cpu);
 	rcu_read_unlock();
 	spin_unlock_irqrestore(&sched_debug_lock, flags);
+	SEQ_printf(m, "\n");
 }
 
 static const char *sched_tunable_scaling_names[] = {
@@ -338,11 +339,10 @@ static const char *sched_tunable_scaling_names[] = {
 	"linear"
 };
 
-static int sched_debug_show(struct seq_file *m, void *v)
+static void sched_debug_header(struct seq_file *m)
 {
 	u64 ktime, sched_clk, cpu_clk;
 	unsigned long flags;
-	int cpu;
 
 	local_irq_save(flags);
 	ktime = ktime_to_ns(ktime_get());
@@ -384,33 +384,101 @@ static int sched_debug_show(struct seq_file *m, void *v)
 #undef PN
 #undef P
 
-	SEQ_printf(m, "  .%-40s: %d (%s)\n", "sysctl_sched_tunable_scaling",
+	SEQ_printf(m, "  .%-40s: %d (%s)\n",
+		"sysctl_sched_tunable_scaling",
 		sysctl_sched_tunable_scaling,
 		sched_tunable_scaling_names[sysctl_sched_tunable_scaling]);
+	SEQ_printf(m, "\n");
+}
 
-	for_each_online_cpu(cpu)
-		print_cpu(m, cpu);
+static int sched_debug_show(struct seq_file *m, void *v)
+{
+	int cpu = (unsigned long)(v - 2);
 
-	SEQ_printf(m, "\n");
+	if (cpu != -1)
+		print_cpu(m, cpu);
+	else
+		sched_debug_header(m);
 
 	return 0;
 }
 
 void sysrq_sched_debug_show(void)
 {
-	sched_debug_show(NULL, NULL);
+	int cpu;
+
+	sched_debug_header(NULL);
+	for_each_online_cpu(cpu)
+		print_cpu(NULL, cpu);
+
+}
+
+/*
+ * This itererator needs some explanation.
+ * It returns 1 for the header position.
+ * This means 2 is cpu 0.
+ * In a hotplugged system some cpus, including cpu 0, may be missing so we have
+ * to use cpumask_* to iterate over the cpus.
+ */
+static void *sched_debug_start(struct seq_file *file, loff_t *offset)
+{
+	unsigned long n = *offset;
+
+	if (n == 0)
+		return (void *) 1;
+
+	n--;
+
+	if (n > 0)
+		n = cpumask_next(n - 1, cpu_online_mask);
+	else
+		n = cpumask_first(cpu_online_mask);
+
+	*offset = n + 1;
+
+	if (n < nr_cpu_ids)
+		return (void *)(unsigned long)(n + 2);
+	return NULL;
+}
+
+static void *sched_debug_next(struct seq_file *file, void *data, loff_t *offset)
+{
+	(*offset)++;
+	return sched_debug_start(file, offset);
+}
+
+static void sched_debug_stop(struct seq_file *file, void *data)
+{
+}
+
+static const struct seq_operations sched_debug_sops = {
+	.start = sched_debug_start,
+	.next = sched_debug_next,
+	.stop = sched_debug_stop,
+	.show = sched_debug_show,
+};
+
+static int sched_debug_release(struct inode *inode, struct file *file)
+{
+	seq_release(inode, file);
+
+	return 0;
 }
 
 static int sched_debug_open(struct inode *inode, struct file *filp)
 {
-	return single_open(filp, sched_debug_show, NULL);
+	int ret = 0;
+
+	ret = seq_open(filp, &sched_debug_sops);
+
+	return ret;
 }
 
 static const struct file_operations sched_debug_fops = {
 	.open		= sched_debug_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
-	.release	= single_release,
+	.release	= sched_debug_release,
 };
 
 static int __init init_sched_debug_procfs(void)

commit 502b24c23b44fbaa01cc2cbd86d8035845b7811f
Merge: ece8e0b2f9c9 f169007b2773
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 20 09:16:21 2013 -0800

    Merge branch 'for-3.9' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup
    
    Pull cgroup changes from Tejun Heo:
     "Nothing too drastic.
    
       - Removal of synchronize_rcu() from userland visible paths.
    
       - Various fixes and cleanups from Li.
    
       - cgroup_rightmost_descendant() added which will be used by cpuset
         changes (it will be a separate pull request)."
    
    * 'for-3.9' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/cgroup:
      cgroup: fail if monitored file and event_control are in different cgroup
      cgroup: fix cgroup_rmdir() vs close(eventfd) race
      cpuset: fix cpuset_print_task_mems_allowed() vs rename() race
      cgroup: fix exit() vs rmdir() race
      cgroup: remove bogus comments in cgroup_diput()
      cgroup: remove synchronize_rcu() from cgroup_diput()
      cgroup: remove duplicate RCU free on struct cgroup
      sched: remove redundant NULL cgroup check in task_group_path()
      sched: split out css_online/css_offline from tg creation/destruction
      cgroup: initialize cgrp->dentry before css_alloc()
      cgroup: remove a NULL check in cgroup_exit()
      cgroup: fix bogus kernel warnings when cgroup_create() failed
      cgroup: remove synchronize_rcu() from rebind_subsystems()
      cgroup: remove synchronize_rcu() from cgroup_attach_{task|proc}()
      cgroup: use new hashtable implementation
      cgroups: fix cgroup_event_listener error handling
      cgroups: move cgroup_event_listener.c to tools/cgroup
      cgroup: implement cgroup_rightmost_descendant()
      cgroup: remove unused dummy cgroup_fork_callbacks()

commit cff3c124a7e82ca0ea1d6864b27ef18c403c0773
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Fri Jan 25 14:14:23 2013 +0000

    sched/debug: Fix format string for 32-bit platforms
    
    The type returned from atomic64_t can be either unsigned
    long or unsigned long long, depending on the architecture.
    Using a cast to unsigned long long lets us use the same
    format string for all architectures.
    
    Without this patch, building with scheduler debugging
    enabled results in:
    
      kernel/sched/debug.c: In function 'print_cfs_rq':
      kernel/sched/debug.c:225:2: warning: format '%ld' expects argument of type 'long int', but argument 4 has type 'long long int' [-Wformat]
      kernel/sched/debug.c:225:2: warning: format '%ld' expects argument of type 'long int', but argument 3 has type 'long long int' [-Wformat]
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Turner <pjt@google.com>
    Cc: linux-arm-kernel@list.infradead.org
    Link: http://lkml.kernel.org/r/1359123276-15833-7-git-send-email-arnd@arndb.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2cd3c1b4e582..7ae4c4c5420e 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -222,8 +222,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lld\n", "blocked_load_avg",
 			cfs_rq->blocked_load_avg);
-	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_avg",
-			atomic64_read(&cfs_rq->tg->load_avg));
+	SEQ_printf(m, "  .%-30s: %lld\n", "tg_load_avg",
+			(unsigned long long)atomic64_read(&cfs_rq->tg->load_avg));
 	SEQ_printf(m, "  .%-30s: %lld\n", "tg_load_contrib",
 			cfs_rq->tg_load_contrib);
 	SEQ_printf(m, "  .%-30s: %d\n", "tg_runnable_contrib",

commit 2a73991b76cbd38c4a0c6704449ccc08c89c3ff3
Author: Li Zefan <lizefan@huawei.com>
Date:   Thu Jan 24 14:31:11 2013 +0800

    sched: remove redundant NULL cgroup check in task_group_path()
    
    A task_group won't be online (thus no one can see it) until
    cpu_cgroup_css_online(), and at that time tg->css.cgroup has
    been initialized, so this NULL check is redundant.
    
    Signed-off-by: Li Zefan <lizefan@huawei.com>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2cd3c1b4e582..38df0db96608 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -110,13 +110,6 @@ static char *task_group_path(struct task_group *tg)
 	if (autogroup_path(tg, group_path, PATH_MAX))
 		return group_path;
 
-	/*
-	 * May be NULL if the underlying cgroup isn't fully-created yet
-	 */
-	if (!tg->css.cgroup) {
-		group_path[0] = '\0';
-		return group_path;
-	}
 	cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
 	return group_path;
 }

commit 82958366cfea1a50e7e90907b2d55ae29ed69974
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Replace update_shares weight distribution with per-entity computation
    
    Now that the machinery in place is in place to compute contributed load in a
    bottom up fashion; replace the shares distribution code within update_shares()
    accordingly.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141507.061208672@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 71b0ea325e93..2cd3c1b4e582 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -218,14 +218,6 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 #ifdef CONFIG_SMP
-	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "load_avg",
-			SPLIT_NS(cfs_rq->load_avg));
-	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "load_period",
-			SPLIT_NS(cfs_rq->load_period));
-	SEQ_printf(m, "  .%-30s: %ld\n", "load_contrib",
-			cfs_rq->load_contribution);
-	SEQ_printf(m, "  .%-30s: %d\n", "load_tg",
-			atomic_read(&cfs_rq->tg->load_weight));
 	SEQ_printf(m, "  .%-30s: %lld\n", "runnable_load_avg",
 			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lld\n", "blocked_load_avg",

commit bb17f65571e97a7ec0297571fb1154fbd107ad00
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:31 2012 +0200

    sched: Normalize tg load contributions against runnable time
    
    Entities of equal weight should receive equitable distribution of cpu time.
    This is challenging in the case of a task_group's shares as execution may be
    occurring on multiple cpus simultaneously.
    
    To handle this we divide up the shares into weights proportionate with the load
    on each cfs_rq.  This does not however, account for the fact that the sum of
    the parts may be less than one cpu and so we need to normalize:
      load(tg) = min(runnable_avg(tg), 1) * tg->shares
    Where runnable_avg is the aggregate time in which the task_group had runnable
    children.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>.
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.930124292@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 290892361a09..71b0ea325e93 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -234,6 +234,10 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			atomic64_read(&cfs_rq->tg->load_avg));
 	SEQ_printf(m, "  .%-30s: %lld\n", "tg_load_contrib",
 			cfs_rq->tg_load_contrib);
+	SEQ_printf(m, "  .%-30s: %d\n", "tg_runnable_contrib",
+			cfs_rq->tg_runnable_contrib);
+	SEQ_printf(m, "  .%-30s: %d\n", "tg->runnable_avg",
+			atomic_read(&cfs_rq->tg->runnable_avg));
 #endif
 
 	print_cfs_group_stats(m, cpu, cfs_rq->tg);

commit c566e8e9e44b72b53091da20e2dedefc730f2ee2
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Aggregate total task_group load
    
    Maintain a global running sum of the average load seen on each cfs_rq belonging
    to each task group so that it may be used in calculating an appropriate
    shares:weight distribution.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.792901086@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2d2e2b3c1bef..290892361a09 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -230,6 +230,10 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->runnable_load_avg);
 	SEQ_printf(m, "  .%-30s: %lld\n", "blocked_load_avg",
 			cfs_rq->blocked_load_avg);
+	SEQ_printf(m, "  .%-30s: %ld\n", "tg_load_avg",
+			atomic64_read(&cfs_rq->tg->load_avg));
+	SEQ_printf(m, "  .%-30s: %lld\n", "tg_load_contrib",
+			cfs_rq->tg_load_contrib);
 #endif
 
 	print_cfs_group_stats(m, cpu, cfs_rq->tg);

commit 9ee474f55664ff63111c843099d365e7ecffb56f
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Maintain the load contribution of blocked entities
    
    We are currently maintaining:
    
      runnable_load(cfs_rq) = \Sum task_load(t)
    
    For all running children t of cfs_rq.  While this can be naturally updated for
    tasks in a runnable state (as they are scheduled); this does not account for
    the load contributed by blocked task entities.
    
    This can be solved by introducing a separate accounting for blocked load:
    
      blocked_load(cfs_rq) = \Sum runnable(b) * weight(b)
    
    Obviously we do not want to iterate over all blocked entities to account for
    their decay, we instead observe that:
    
      runnable_load(t) = \Sum p_i*y^i
    
    and that to account for an additional idle period we only need to compute:
    
      y*runnable_load(t).
    
    This means that we can compute all blocked entities at once by evaluating:
    
      blocked_load(cfs_rq)` = y * blocked_load(cfs_rq)
    
    Finally we maintain a decay counter so that when a sleeping entity re-awakens
    we can determine how much of its load should be removed from the blocked sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.585389902@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index c953a89f94aa..2d2e2b3c1bef 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -95,6 +95,7 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	P(se->avg.runnable_avg_sum);
 	P(se->avg.runnable_avg_period);
 	P(se->avg.load_avg_contrib);
+	P(se->avg.decay_count);
 #endif
 #undef PN
 #undef P
@@ -227,6 +228,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			atomic_read(&cfs_rq->tg->load_weight));
 	SEQ_printf(m, "  .%-30s: %lld\n", "runnable_load_avg",
 			cfs_rq->runnable_load_avg);
+	SEQ_printf(m, "  .%-30s: %lld\n", "blocked_load_avg",
+			cfs_rq->blocked_load_avg);
 #endif
 
 	print_cfs_group_stats(m, cpu, cfs_rq->tg);

commit 2dac754e10a5d41d94d2d2365c0345d4f215a266
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:30 2012 +0200

    sched: Aggregate load contributed by task entities on parenting cfs_rq
    
    For a given task t, we can compute its contribution to load as:
    
      task_load(t) = runnable_avg(t) * weight(t)
    
    On a parenting cfs_rq we can then aggregate:
    
      runnable_load(cfs_rq) = \Sum task_load(t), for all runnable children t
    
    Maintain this bottom up, with task entities adding their contributed load to
    the parenting cfs_rq sum.  When a task entity's load changes we add the same
    delta to the maintained sum.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.514678907@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 4240abce4116..c953a89f94aa 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -94,6 +94,7 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 #ifdef CONFIG_SMP
 	P(se->avg.runnable_avg_sum);
 	P(se->avg.runnable_avg_period);
+	P(se->avg.load_avg_contrib);
 #endif
 #undef PN
 #undef P
@@ -224,6 +225,8 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			cfs_rq->load_contribution);
 	SEQ_printf(m, "  .%-30s: %d\n", "load_tg",
 			atomic_read(&cfs_rq->tg->load_weight));
+	SEQ_printf(m, "  .%-30s: %lld\n", "runnable_load_avg",
+			cfs_rq->runnable_load_avg);
 #endif
 
 	print_cfs_group_stats(m, cpu, cfs_rq->tg);

commit 18bf2805d9b30cb823d4919b42cd230f59c7ce1f
Author: Ben Segall <bsegall@google.com>
Date:   Thu Oct 4 12:51:20 2012 +0200

    sched: Maintain per-rq runnable averages
    
    Since runqueues do not have a corresponding sched_entity we instead embed a
    sched_avg structure directly.
    
    Signed-off-by: Ben Segall <bsegall@google.com>
    Reviewed-by: Paul Turner <pjt@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.442637130@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 61f70979153a..4240abce4116 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -61,14 +61,20 @@ static unsigned long nsec_low(unsigned long long nsec)
 static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group *tg)
 {
 	struct sched_entity *se = tg->se[cpu];
-	if (!se)
-		return;
 
 #define P(F) \
 	SEQ_printf(m, "  .%-30s: %lld\n", #F, (long long)F)
 #define PN(F) \
 	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)F))
 
+	if (!se) {
+		struct sched_avg *avg = &cpu_rq(cpu)->avg;
+		P(avg->runnable_avg_sum);
+		P(avg->runnable_avg_period);
+		return;
+	}
+
+
 	PN(se->exec_start);
 	PN(se->vruntime);
 	PN(se->sum_exec_runtime);

commit 9d85f21c94f7f7a84d0ba686c58aa6d9da58fdbb
Author: Paul Turner <pjt@google.com>
Date:   Thu Oct 4 13:18:29 2012 +0200

    sched: Track the runnable average on a per-task entity basis
    
    Instead of tracking averaging the load parented by a cfs_rq, we can track
    entity load directly. With the load for a given cfs_rq then being the sum
    of its children.
    
    To do this we represent the historical contribution to runnable average
    within each trailing 1024us of execution as the coefficients of a
    geometric series.
    
    We can express this for a given task t as:
    
      runnable_sum(t) = \Sum u_i * y^i, runnable_avg_period(t) = \Sum 1024 * y^i
      load(t) = weight_t * runnable_sum(t) / runnable_avg_period(t)
    
    Where: u_i is the usage in the last i`th 1024us period (approximately 1ms)
    ~ms and y is chosen such that y^k = 1/2.  We currently choose k to be 32 which
    roughly translates to about a sched period.
    
    Signed-off-by: Paul Turner <pjt@google.com>
    Reviewed-by: Ben Segall <bsegall@google.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/20120823141506.372695337@google.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 6f79596e0ea9..61f70979153a 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -85,6 +85,10 @@ static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group
 	P(se->statistics.wait_count);
 #endif
 	P(se->load.weight);
+#ifdef CONFIG_SMP
+	P(se->avg.runnable_avg_sum);
+	P(se->avg.runnable_avg_period);
+#endif
 #undef PN
 #undef P
 }

commit 13e099d2f77e1da3e4046860c48d956588633613
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon May 14 14:34:00 2012 +0200

    sched/debug: Fix printing large integers on 32-bit platforms
    
    Some numbers like nr_running and nr_uninterruptible are fundamentally
    unsigned since its impossible to have a negative amount of tasks, yet
    we still print them as signed to easily recognise the underflow
    condition.
    
    rq->nr_uninterruptible has 'special' accounting and can in fact very
    easily become negative on a per-cpu basis.
    
    It was noted that since the P() macro assumes things are long long and
    the promotion of unsigned 'int/long' to long long on 32bit doesn't
    sign extend we print silly large numbers instead of the easier to read
    signed numbers.
    
    Therefore extend the P() macro to not require the sign extention.
    
    Reported-by: Diwakar Tundlam <dtundlam@nvidia.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-gk5tm8t2n4ix2vkpns42uqqp@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 31e4f61a1629..6f79596e0ea9 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -260,8 +260,14 @@ static void print_cpu(struct seq_file *m, int cpu)
 	SEQ_printf(m, "\ncpu#%d\n", cpu);
 #endif
 
-#define P(x) \
-	SEQ_printf(m, "  .%-30s: %Ld\n", #x, (long long)(rq->x))
+#define P(x)								\
+do {									\
+	if (sizeof(rq->x) == 4)						\
+		SEQ_printf(m, "  .%-30s: %ld\n", #x, (long)(rq->x));	\
+	else								\
+		SEQ_printf(m, "  .%-30s: %Ld\n", #x, (long long)(rq->x));\
+} while (0)
+
 #define PN(x) \
 	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(rq->x))
 

commit c82513e513556a04f81aa511cd890acd23349c48
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Apr 26 13:12:27 2012 +0200

    sched: Change rq->nr_running to unsigned int
    
    Since there's a PID space limit of 30bits (see
    futex.h:FUTEX_TID_MASK) and allocating that many tasks (assuming a
    lower bound of 2 pages per task) would still take 8T of memory it
    seems reasonable to say that unsigned int is sufficient for
    rq->nr_running.
    
    When we do get anywhere near that amount of tasks I suspect other
    things would go funny, load-balancer load computations would really
    need to be hoisted to 128bit etc.
    
    So save a few bytes and convert rq->nr_running and friends to
    unsigned int.
    
    Suggested-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/n/tip-y3tvyszjdmbibade5bw8zl81@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 09acaa15161d..31e4f61a1629 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -202,7 +202,7 @@ void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
 			SPLIT_NS(spread0));
 	SEQ_printf(m, "  .%-30s: %d\n", "nr_spread_over",
 			cfs_rq->nr_spread_over);
-	SEQ_printf(m, "  .%-30s: %ld\n", "nr_running", cfs_rq->nr_running);
+	SEQ_printf(m, "  .%-30s: %d\n", "nr_running", cfs_rq->nr_running);
 	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
 #ifdef CONFIG_FAIR_GROUP_SCHED
 #ifdef CONFIG_SMP

commit 30fd049afcfed50e022704036e8629d6bdfe84e6
Author: Rakib Mullick <rakib.mullick@gmail.com>
Date:   Tue Jan 24 22:33:56 2012 +0600

    sched: Remove sched_switch
    
    Currently we don't utilize the sched_switch field anymore.
    
    But, simply removing sched_switch field from the middle of the
    sched_stat output will break tools.
    
    So, to stay compatible we hardcode it to zero and remove the
    field from the scheduler data structures.
    
    Update the schedstat documentation accordingly.
    
    Signed-off-by: Rakib Mullick <rakib.mullick@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1327422836.27181.5.camel@localhost.localdomain
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
index 2a075e10004b..09acaa15161d 100644
--- a/kernel/sched/debug.c
+++ b/kernel/sched/debug.c
@@ -288,7 +288,6 @@ static void print_cpu(struct seq_file *m, int cpu)
 
 	P(yld_count);
 
-	P(sched_switch);
 	P(sched_count);
 	P(sched_goidle);
 #ifdef CONFIG_SMP

commit 391e43da797a96aeb65410281891f6d0b0e9611c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Nov 15 17:14:39 2011 +0100

    sched: Move all scheduler bits into kernel/sched/
    
    There's too many sched*.[ch] files in kernel/, give them their own
    directory.
    
    (No code changed, other than Makefile glue added.)
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/sched/debug.c b/kernel/sched/debug.c
new file mode 100644
index 000000000000..2a075e10004b
--- /dev/null
+++ b/kernel/sched/debug.c
@@ -0,0 +1,510 @@
+/*
+ * kernel/sched/debug.c
+ *
+ * Print the CFS rbtree
+ *
+ * Copyright(C) 2007, Red Hat, Inc., Ingo Molnar
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/proc_fs.h>
+#include <linux/sched.h>
+#include <linux/seq_file.h>
+#include <linux/kallsyms.h>
+#include <linux/utsname.h>
+
+#include "sched.h"
+
+static DEFINE_SPINLOCK(sched_debug_lock);
+
+/*
+ * This allows printing both to /proc/sched_debug and
+ * to the console
+ */
+#define SEQ_printf(m, x...)			\
+ do {						\
+	if (m)					\
+		seq_printf(m, x);		\
+	else					\
+		printk(x);			\
+ } while (0)
+
+/*
+ * Ease the printing of nsec fields:
+ */
+static long long nsec_high(unsigned long long nsec)
+{
+	if ((long long)nsec < 0) {
+		nsec = -nsec;
+		do_div(nsec, 1000000);
+		return -nsec;
+	}
+	do_div(nsec, 1000000);
+
+	return nsec;
+}
+
+static unsigned long nsec_low(unsigned long long nsec)
+{
+	if ((long long)nsec < 0)
+		nsec = -nsec;
+
+	return do_div(nsec, 1000000);
+}
+
+#define SPLIT_NS(x) nsec_high(x), nsec_low(x)
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+static void print_cfs_group_stats(struct seq_file *m, int cpu, struct task_group *tg)
+{
+	struct sched_entity *se = tg->se[cpu];
+	if (!se)
+		return;
+
+#define P(F) \
+	SEQ_printf(m, "  .%-30s: %lld\n", #F, (long long)F)
+#define PN(F) \
+	SEQ_printf(m, "  .%-30s: %lld.%06ld\n", #F, SPLIT_NS((long long)F))
+
+	PN(se->exec_start);
+	PN(se->vruntime);
+	PN(se->sum_exec_runtime);
+#ifdef CONFIG_SCHEDSTATS
+	PN(se->statistics.wait_start);
+	PN(se->statistics.sleep_start);
+	PN(se->statistics.block_start);
+	PN(se->statistics.sleep_max);
+	PN(se->statistics.block_max);
+	PN(se->statistics.exec_max);
+	PN(se->statistics.slice_max);
+	PN(se->statistics.wait_max);
+	PN(se->statistics.wait_sum);
+	P(se->statistics.wait_count);
+#endif
+	P(se->load.weight);
+#undef PN
+#undef P
+}
+#endif
+
+#ifdef CONFIG_CGROUP_SCHED
+static char group_path[PATH_MAX];
+
+static char *task_group_path(struct task_group *tg)
+{
+	if (autogroup_path(tg, group_path, PATH_MAX))
+		return group_path;
+
+	/*
+	 * May be NULL if the underlying cgroup isn't fully-created yet
+	 */
+	if (!tg->css.cgroup) {
+		group_path[0] = '\0';
+		return group_path;
+	}
+	cgroup_path(tg->css.cgroup, group_path, PATH_MAX);
+	return group_path;
+}
+#endif
+
+static void
+print_task(struct seq_file *m, struct rq *rq, struct task_struct *p)
+{
+	if (rq->curr == p)
+		SEQ_printf(m, "R");
+	else
+		SEQ_printf(m, " ");
+
+	SEQ_printf(m, "%15s %5d %9Ld.%06ld %9Ld %5d ",
+		p->comm, p->pid,
+		SPLIT_NS(p->se.vruntime),
+		(long long)(p->nvcsw + p->nivcsw),
+		p->prio);
+#ifdef CONFIG_SCHEDSTATS
+	SEQ_printf(m, "%9Ld.%06ld %9Ld.%06ld %9Ld.%06ld",
+		SPLIT_NS(p->se.vruntime),
+		SPLIT_NS(p->se.sum_exec_runtime),
+		SPLIT_NS(p->se.statistics.sum_sleep_runtime));
+#else
+	SEQ_printf(m, "%15Ld %15Ld %15Ld.%06ld %15Ld.%06ld %15Ld.%06ld",
+		0LL, 0LL, 0LL, 0L, 0LL, 0L, 0LL, 0L);
+#endif
+#ifdef CONFIG_CGROUP_SCHED
+	SEQ_printf(m, " %s", task_group_path(task_group(p)));
+#endif
+
+	SEQ_printf(m, "\n");
+}
+
+static void print_rq(struct seq_file *m, struct rq *rq, int rq_cpu)
+{
+	struct task_struct *g, *p;
+	unsigned long flags;
+
+	SEQ_printf(m,
+	"\nrunnable tasks:\n"
+	"            task   PID         tree-key  switches  prio"
+	"     exec-runtime         sum-exec        sum-sleep\n"
+	"------------------------------------------------------"
+	"----------------------------------------------------\n");
+
+	read_lock_irqsave(&tasklist_lock, flags);
+
+	do_each_thread(g, p) {
+		if (!p->on_rq || task_cpu(p) != rq_cpu)
+			continue;
+
+		print_task(m, rq, p);
+	} while_each_thread(g, p);
+
+	read_unlock_irqrestore(&tasklist_lock, flags);
+}
+
+void print_cfs_rq(struct seq_file *m, int cpu, struct cfs_rq *cfs_rq)
+{
+	s64 MIN_vruntime = -1, min_vruntime, max_vruntime = -1,
+		spread, rq0_min_vruntime, spread0;
+	struct rq *rq = cpu_rq(cpu);
+	struct sched_entity *last;
+	unsigned long flags;
+
+#ifdef CONFIG_FAIR_GROUP_SCHED
+	SEQ_printf(m, "\ncfs_rq[%d]:%s\n", cpu, task_group_path(cfs_rq->tg));
+#else
+	SEQ_printf(m, "\ncfs_rq[%d]:\n", cpu);
+#endif
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "exec_clock",
+			SPLIT_NS(cfs_rq->exec_clock));
+
+	raw_spin_lock_irqsave(&rq->lock, flags);
+	if (cfs_rq->rb_leftmost)
+		MIN_vruntime = (__pick_first_entity(cfs_rq))->vruntime;
+	last = __pick_last_entity(cfs_rq);
+	if (last)
+		max_vruntime = last->vruntime;
+	min_vruntime = cfs_rq->min_vruntime;
+	rq0_min_vruntime = cpu_rq(0)->cfs.min_vruntime;
+	raw_spin_unlock_irqrestore(&rq->lock, flags);
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "MIN_vruntime",
+			SPLIT_NS(MIN_vruntime));
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "min_vruntime",
+			SPLIT_NS(min_vruntime));
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "max_vruntime",
+			SPLIT_NS(max_vruntime));
+	spread = max_vruntime - MIN_vruntime;
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "spread",
+			SPLIT_NS(spread));
+	spread0 = min_vruntime - rq0_min_vruntime;
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "spread0",
+			SPLIT_NS(spread0));
+	SEQ_printf(m, "  .%-30s: %d\n", "nr_spread_over",
+			cfs_rq->nr_spread_over);
+	SEQ_printf(m, "  .%-30s: %ld\n", "nr_running", cfs_rq->nr_running);
+	SEQ_printf(m, "  .%-30s: %ld\n", "load", cfs_rq->load.weight);
+#ifdef CONFIG_FAIR_GROUP_SCHED
+#ifdef CONFIG_SMP
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "load_avg",
+			SPLIT_NS(cfs_rq->load_avg));
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", "load_period",
+			SPLIT_NS(cfs_rq->load_period));
+	SEQ_printf(m, "  .%-30s: %ld\n", "load_contrib",
+			cfs_rq->load_contribution);
+	SEQ_printf(m, "  .%-30s: %d\n", "load_tg",
+			atomic_read(&cfs_rq->tg->load_weight));
+#endif
+
+	print_cfs_group_stats(m, cpu, cfs_rq->tg);
+#endif
+}
+
+void print_rt_rq(struct seq_file *m, int cpu, struct rt_rq *rt_rq)
+{
+#ifdef CONFIG_RT_GROUP_SCHED
+	SEQ_printf(m, "\nrt_rq[%d]:%s\n", cpu, task_group_path(rt_rq->tg));
+#else
+	SEQ_printf(m, "\nrt_rq[%d]:\n", cpu);
+#endif
+
+#define P(x) \
+	SEQ_printf(m, "  .%-30s: %Ld\n", #x, (long long)(rt_rq->x))
+#define PN(x) \
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(rt_rq->x))
+
+	P(rt_nr_running);
+	P(rt_throttled);
+	PN(rt_time);
+	PN(rt_runtime);
+
+#undef PN
+#undef P
+}
+
+extern __read_mostly int sched_clock_running;
+
+static void print_cpu(struct seq_file *m, int cpu)
+{
+	struct rq *rq = cpu_rq(cpu);
+	unsigned long flags;
+
+#ifdef CONFIG_X86
+	{
+		unsigned int freq = cpu_khz ? : 1;
+
+		SEQ_printf(m, "\ncpu#%d, %u.%03u MHz\n",
+			   cpu, freq / 1000, (freq % 1000));
+	}
+#else
+	SEQ_printf(m, "\ncpu#%d\n", cpu);
+#endif
+
+#define P(x) \
+	SEQ_printf(m, "  .%-30s: %Ld\n", #x, (long long)(rq->x))
+#define PN(x) \
+	SEQ_printf(m, "  .%-30s: %Ld.%06ld\n", #x, SPLIT_NS(rq->x))
+
+	P(nr_running);
+	SEQ_printf(m, "  .%-30s: %lu\n", "load",
+		   rq->load.weight);
+	P(nr_switches);
+	P(nr_load_updates);
+	P(nr_uninterruptible);
+	PN(next_balance);
+	P(curr->pid);
+	PN(clock);
+	P(cpu_load[0]);
+	P(cpu_load[1]);
+	P(cpu_load[2]);
+	P(cpu_load[3]);
+	P(cpu_load[4]);
+#undef P
+#undef PN
+
+#ifdef CONFIG_SCHEDSTATS
+#define P(n) SEQ_printf(m, "  .%-30s: %d\n", #n, rq->n);
+#define P64(n) SEQ_printf(m, "  .%-30s: %Ld\n", #n, rq->n);
+
+	P(yld_count);
+
+	P(sched_switch);
+	P(sched_count);
+	P(sched_goidle);
+#ifdef CONFIG_SMP
+	P64(avg_idle);
+#endif
+
+	P(ttwu_count);
+	P(ttwu_local);
+
+#undef P
+#undef P64
+#endif
+	spin_lock_irqsave(&sched_debug_lock, flags);
+	print_cfs_stats(m, cpu);
+	print_rt_stats(m, cpu);
+
+	rcu_read_lock();
+	print_rq(m, rq, cpu);
+	rcu_read_unlock();
+	spin_unlock_irqrestore(&sched_debug_lock, flags);
+}
+
+static const char *sched_tunable_scaling_names[] = {
+	"none",
+	"logaritmic",
+	"linear"
+};
+
+static int sched_debug_show(struct seq_file *m, void *v)
+{
+	u64 ktime, sched_clk, cpu_clk;
+	unsigned long flags;
+	int cpu;
+
+	local_irq_save(flags);
+	ktime = ktime_to_ns(ktime_get());
+	sched_clk = sched_clock();
+	cpu_clk = local_clock();
+	local_irq_restore(flags);
+
+	SEQ_printf(m, "Sched Debug Version: v0.10, %s %.*s\n",
+		init_utsname()->release,
+		(int)strcspn(init_utsname()->version, " "),
+		init_utsname()->version);
+
+#define P(x) \
+	SEQ_printf(m, "%-40s: %Ld\n", #x, (long long)(x))
+#define PN(x) \
+	SEQ_printf(m, "%-40s: %Ld.%06ld\n", #x, SPLIT_NS(x))
+	PN(ktime);
+	PN(sched_clk);
+	PN(cpu_clk);
+	P(jiffies);
+#ifdef CONFIG_HAVE_UNSTABLE_SCHED_CLOCK
+	P(sched_clock_stable);
+#endif
+#undef PN
+#undef P
+
+	SEQ_printf(m, "\n");
+	SEQ_printf(m, "sysctl_sched\n");
+
+#define P(x) \
+	SEQ_printf(m, "  .%-40s: %Ld\n", #x, (long long)(x))
+#define PN(x) \
+	SEQ_printf(m, "  .%-40s: %Ld.%06ld\n", #x, SPLIT_NS(x))
+	PN(sysctl_sched_latency);
+	PN(sysctl_sched_min_granularity);
+	PN(sysctl_sched_wakeup_granularity);
+	P(sysctl_sched_child_runs_first);
+	P(sysctl_sched_features);
+#undef PN
+#undef P
+
+	SEQ_printf(m, "  .%-40s: %d (%s)\n", "sysctl_sched_tunable_scaling",
+		sysctl_sched_tunable_scaling,
+		sched_tunable_scaling_names[sysctl_sched_tunable_scaling]);
+
+	for_each_online_cpu(cpu)
+		print_cpu(m, cpu);
+
+	SEQ_printf(m, "\n");
+
+	return 0;
+}
+
+void sysrq_sched_debug_show(void)
+{
+	sched_debug_show(NULL, NULL);
+}
+
+static int sched_debug_open(struct inode *inode, struct file *filp)
+{
+	return single_open(filp, sched_debug_show, NULL);
+}
+
+static const struct file_operations sched_debug_fops = {
+	.open		= sched_debug_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+	.release	= single_release,
+};
+
+static int __init init_sched_debug_procfs(void)
+{
+	struct proc_dir_entry *pe;
+
+	pe = proc_create("sched_debug", 0444, NULL, &sched_debug_fops);
+	if (!pe)
+		return -ENOMEM;
+	return 0;
+}
+
+__initcall(init_sched_debug_procfs);
+
+void proc_sched_show_task(struct task_struct *p, struct seq_file *m)
+{
+	unsigned long nr_switches;
+
+	SEQ_printf(m, "%s (%d, #threads: %d)\n", p->comm, p->pid,
+						get_nr_threads(p));
+	SEQ_printf(m,
+		"---------------------------------------------------------\n");
+#define __P(F) \
+	SEQ_printf(m, "%-35s:%21Ld\n", #F, (long long)F)
+#define P(F) \
+	SEQ_printf(m, "%-35s:%21Ld\n", #F, (long long)p->F)
+#define __PN(F) \
+	SEQ_printf(m, "%-35s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)F))
+#define PN(F) \
+	SEQ_printf(m, "%-35s:%14Ld.%06ld\n", #F, SPLIT_NS((long long)p->F))
+
+	PN(se.exec_start);
+	PN(se.vruntime);
+	PN(se.sum_exec_runtime);
+
+	nr_switches = p->nvcsw + p->nivcsw;
+
+#ifdef CONFIG_SCHEDSTATS
+	PN(se.statistics.wait_start);
+	PN(se.statistics.sleep_start);
+	PN(se.statistics.block_start);
+	PN(se.statistics.sleep_max);
+	PN(se.statistics.block_max);
+	PN(se.statistics.exec_max);
+	PN(se.statistics.slice_max);
+	PN(se.statistics.wait_max);
+	PN(se.statistics.wait_sum);
+	P(se.statistics.wait_count);
+	PN(se.statistics.iowait_sum);
+	P(se.statistics.iowait_count);
+	P(se.nr_migrations);
+	P(se.statistics.nr_migrations_cold);
+	P(se.statistics.nr_failed_migrations_affine);
+	P(se.statistics.nr_failed_migrations_running);
+	P(se.statistics.nr_failed_migrations_hot);
+	P(se.statistics.nr_forced_migrations);
+	P(se.statistics.nr_wakeups);
+	P(se.statistics.nr_wakeups_sync);
+	P(se.statistics.nr_wakeups_migrate);
+	P(se.statistics.nr_wakeups_local);
+	P(se.statistics.nr_wakeups_remote);
+	P(se.statistics.nr_wakeups_affine);
+	P(se.statistics.nr_wakeups_affine_attempts);
+	P(se.statistics.nr_wakeups_passive);
+	P(se.statistics.nr_wakeups_idle);
+
+	{
+		u64 avg_atom, avg_per_cpu;
+
+		avg_atom = p->se.sum_exec_runtime;
+		if (nr_switches)
+			do_div(avg_atom, nr_switches);
+		else
+			avg_atom = -1LL;
+
+		avg_per_cpu = p->se.sum_exec_runtime;
+		if (p->se.nr_migrations) {
+			avg_per_cpu = div64_u64(avg_per_cpu,
+						p->se.nr_migrations);
+		} else {
+			avg_per_cpu = -1LL;
+		}
+
+		__PN(avg_atom);
+		__PN(avg_per_cpu);
+	}
+#endif
+	__P(nr_switches);
+	SEQ_printf(m, "%-35s:%21Ld\n",
+		   "nr_voluntary_switches", (long long)p->nvcsw);
+	SEQ_printf(m, "%-35s:%21Ld\n",
+		   "nr_involuntary_switches", (long long)p->nivcsw);
+
+	P(se.load.weight);
+	P(policy);
+	P(prio);
+#undef PN
+#undef __PN
+#undef P
+#undef __P
+
+	{
+		unsigned int this_cpu = raw_smp_processor_id();
+		u64 t0, t1;
+
+		t0 = cpu_clock(this_cpu);
+		t1 = cpu_clock(this_cpu);
+		SEQ_printf(m, "%-35s:%21Ld\n",
+			   "clock-delta", (long long)(t1-t0));
+	}
+}
+
+void proc_sched_set_task(struct task_struct *p)
+{
+#ifdef CONFIG_SCHEDSTATS
+	memset(&p->se.statistics, 0, sizeof(p->se.statistics));
+#endif
+}
