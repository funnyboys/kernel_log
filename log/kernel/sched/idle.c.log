commit 10e8b11eb3195e11450c509d4dd3984d707a4167
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Jun 25 13:52:53 2020 +0200

    cpuidle: Rearrange s2idle-specific idle state entry code
    
    Implement call_cpuidle_s2idle() in analogy with call_cpuidle()
    for the s2idle-specific idle state entry and invoke it from
    cpuidle_idle_call() to make the s2idle-specific idle entry code
    path look more similar to the "regular" idle entry one.
    
    No intentional functional impact.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Chen Yu <yu.c.chen@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 05deb81bb3e3..1ae95b9150d3 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -96,6 +96,15 @@ void __cpuidle default_idle_call(void)
 	}
 }
 
+static int call_cpuidle_s2idle(struct cpuidle_driver *drv,
+			       struct cpuidle_device *dev)
+{
+	if (current_clr_polling_and_test())
+		return -EBUSY;
+
+	return cpuidle_enter_s2idle(drv, dev);
+}
+
 static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		      int next_state)
 {
@@ -171,11 +180,9 @@ static void cpuidle_idle_call(void)
 		if (idle_should_enter_s2idle()) {
 			rcu_idle_enter();
 
-			entered_state = cpuidle_enter_s2idle(drv, dev);
-			if (entered_state > 0) {
-				local_irq_enable();
+			entered_state = call_cpuidle_s2idle(drv, dev);
+			if (entered_state > 0)
 				goto exit_idle;
-			}
 
 			rcu_idle_exit();
 

commit a148866489fbe243c936fe43e4525d8dbfa0318f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:04 2020 +0200

    sched: Replace rq::wake_list
    
    The recent commit: 90b5363acd47 ("sched: Clean up scheduler_ipi()")
    got smp_call_function_single_async() subtly wrong. Even though it will
    return -EBUSY when trying to re-use a csd, that condition is not
    atomic and still requires external serialization.
    
    The change in ttwu_queue_remote() got this wrong.
    
    While on first reading ttwu_queue_remote() has an atomic test-and-set
    that appears to serialize the use, the matching 'release' is not in
    the right place to actually guarantee this serialization.
    
    The actual race is vs the sched_ttwu_pending() call in the idle loop;
    that can run the wakeup-list without consuming the CSD.
    
    Instead of trying to chain the lists, merge them.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161908.129371594@infradead.org

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 387fd75ee6f7..05deb81bb3e3 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -294,7 +294,6 @@ static void do_idle(void)
 	 * critical section.
 	 */
 	flush_smp_call_function_from_idle();
-	sched_ttwu_pending();
 	schedule_idle();
 
 	if (unlikely(klp_patch_pending(current)))

commit b2a02fc43a1f40ef4eb2fb2b06357382608d4d84
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 26 18:11:01 2020 +0200

    smp: Optimize send_call_function_single_ipi()
    
    Just like the ttwu_queue_remote() IPI, make use of _TIF_POLLING_NRFLAG
    to avoid sending IPIs to idle CPUs.
    
    [ mingo: Fix UP build bug. ]
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>
    Link: https://lore.kernel.org/r/20200526161907.953304789@infradead.org

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index b743bf38f08f..387fd75ee6f7 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -289,6 +289,11 @@ static void do_idle(void)
 	 */
 	smp_mb__after_atomic();
 
+	/*
+	 * RCU relies on this call to be done outside of an RCU read-side
+	 * critical section.
+	 */
+	flush_smp_call_function_from_idle();
 	sched_ttwu_pending();
 	schedule_idle();
 

commit 3e0de271fff77abb933f1b69c213854c3eda9125
Author: Hewenliang <hewenliang4@huawei.com>
Date:   Thu Jan 9 21:56:04 2020 -0500

    idle: fix spelling mistake "iterrupts" -> "interrupts"
    
    There is a spelling misake in comments of cpuidle_idle_call. Fix it.
    
    Signed-off-by: Hewenliang <hewenliang4@huawei.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Link: https://lkml.kernel.org/r/20200110025604.34373-1-hewenliang4@huawei.com

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index ffa959e91227..b743bf38f08f 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -158,7 +158,7 @@ static void cpuidle_idle_call(void)
 	/*
 	 * Suspend-to-idle ("s2idle") is a system state in which all user space
 	 * has been frozen, all I/O devices have been suspended and the only
-	 * activity happens here and in iterrupts (if any).  In that case bypass
+	 * activity happens here and in interrupts (if any). In that case bypass
 	 * the cpuidle governor and go stratight for the deepest idle state
 	 * available.  Possibly also suspend the local tick and the entire
 	 * timekeeping to prevent timer interrupts from kicking us out of idle

commit 9e7a03233e02afd3ee061e373355f34d7254f1e6
Merge: c2da5bdc66a3 e350b60f4e0f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Nov 26 19:06:44 2019 -0800

    Merge tag 'pm-5.5-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These include cpuidle changes to use nanoseconds (instead of
      microseconds) as the unit of time and to simplify checks for disabled
      idle states in the idle loop, some cpuidle fixes and governor updates,
      assorted cpufreq updates (driver updates mostly and a few core fixes
      and cleanups), devfreq updates (dominated by the tegra30 driver
      changes), new CPU IDs for the RAPL power capping driver, relatively
      minor updates of the generic power domains (genpd) and operation
      performance points (OPP) frameworks, and assorted fixes and cleanups.
    
      There are also two maintainer information updates: Chanwoo Choi will
      be maintaining the devfreq subsystem going forward and Todd Brandt is
      going to maintain the pm-graph utility (created by him).
    
      Specifics:
    
       - Use nanoseconds (instead of microseconds) as the unit of time in
         the cpuidle core and simplify checks for disabled idle states in
         the idle loop (Rafael Wysocki)
    
       - Fix and clean up the teo cpuidle governor (Rafael Wysocki)
    
       - Fix the cpuidle registration error code path (Zhenzhong Duan)
    
       - Avoid excessive vmexits in the ACPI cpuidle driver (Yin Fengwei)
    
       - Extend the idle injection infrastructure to be able to measure the
         requested duration in nanoseconds and to allow an exit latency
         limit for idle states to be specified (Daniel Lezcano)
    
       - Fix cpufreq driver registration and clarify a comment in the
         cpufreq core (Viresh Kumar)
    
       - Add NULL checks to the show() and store() methods of sysfs
         attributes exposed by cpufreq (Kai Shen)
    
       - Update cpufreq drivers:
          * Fix for a plain int as pointer warning from sparse in
            intel_pstate (Jamal Shareef)
          * Fix for a hardcoded number of CPUs and stack bloat in the
            powernv driver (John Hubbard)
          * Updates to the ti-cpufreq driver and DT files to support new
            platforms and migrate bindings from opp-v1 to opp-v2 (Adam Ford,
            H. Nikolaus Schaller)
          * Merging of the arm_big_little and vexpress-spc drivers and
            related cleanup (Sudeep Holla)
          * Fix for imx's default speed grade value (Anson Huang)
          * Minor cleanup of the s3c64xx driver (Nathan Chancellor)
          * CPU speed bin detection fix for sun50i (Ondrej Jirman)
    
       - Appoint Chanwoo Choi as the new devfreq maintainer.
    
       - Update the devfreq core:
          * Check NULL governor in available_governors_show sysfs to prevent
            showing wrong governor information and fix a race condition
            between devfreq_update_status() and trans_stat_show() (Leonard
            Crestez)
          * Add new 'interrupt-driven' flag for devfreq governors to allow
            interrupt-driven governors to prevent the devfreq core from
            polling devices for status (Dmitry Osipenko)
          * Improve an error message in devfreq_add_device() (Matthias
            Kaehlcke)
    
       - Update devfreq drivers:
          * tegra30 driver fixes and cleanups (Dmitry Osipenko)
          * Removal of unused property from dt-binding documentation for the
            exynos-bus driver (Kamil Konieczny)
          * exynos-ppmu cleanup and DT bindings update (Lukasz Luba, Marek
            Szyprowski)
    
       - Add new CPU IDs for CometLake Mobile and Desktop to the Intel RAPL
         power capping driver (Zhang Rui)
    
       - Allow device initialization in the generic power domains (genpd)
         framework to be more straightforward and clean it up (Ulf Hansson)
    
       - Add support for adjusting OPP voltages at run time to the OPP
         framework (Stephen Boyd)
    
       - Avoid freeing memory that has never been allocated in the
         hibernation core (Andy Whitcroft)
    
       - Clean up function headers in a header file and coding style in the
         wakeup IRQs handling code (Ulf Hansson, Xiaofei Tan)
    
       - Clean up the SmartReflex adaptive voltage scaling (AVS) driver for
         ARM (Ben Dooks, Geert Uytterhoeven)
    
       - Wrap power management documentation to fit in 80 columns (Bjorn
         Helgaas)
    
       - Add pm-graph utility entry to MAINTAINERS (Todd Brandt)
    
       - Update the cpupower utility:
          * Fix the handling of set and info subcommands (Abhishek Goel)
          * Fix build warnings (Nathan Chancellor)
          * Improve mperf_monitor handling (Janakarajan Natarajan)"
    
    * tag 'pm-5.5-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (83 commits)
      PM: Wrap documentation to fit in 80 columns
      cpuidle: Pass exit latency limit to cpuidle_use_deepest_state()
      cpuidle: Allow idle injection to apply exit latency limit
      cpuidle: Introduce cpuidle_driver_state_disabled() for driver quirks
      cpuidle: teo: Avoid code duplication in conditionals
      cpufreq: Register drivers only after CPU devices have been registered
      cpuidle: teo: Avoid using "early hits" incorrectly
      cpuidle: teo: Exclude cpuidle overhead from computations
      PM / Domains: Convert to dev_to_genpd_safe() in genpd_syscore_switch()
      mmc: tmio: Avoid boilerplate code in ->runtime_suspend()
      PM / Domains: Implement the ->start() callback for genpd
      PM / Domains: Introduce dev_pm_domain_start()
      ARM: OMAP2+: SmartReflex: add omap_sr_pdata definition
      PM / wakeirq: remove unnecessary parentheses
      power: avs: smartreflex: Remove superfluous cast in debugfs_create_file() call
      cpuidle: Use nanoseconds as the unit of time
      PM / OPP: Support adjusting OPP voltages at runtime
      PM / core: Clean up some function headers in power.h
      cpufreq: Add NULL checks to show() and store() methods of cpufreq
      cpufreq: intel_pstate: Fix plain int as pointer warning from sparse
      ...

commit 62214039525a621aa0eca3fe9160e809ec0e45d3
Merge: 05ff1ba412fd 5aa9ba6312e3
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Nov 26 10:26:26 2019 +0100

    Merge branch 'pm-cpuidle'
    
    * pm-cpuidle:
      cpuidle: Pass exit latency limit to cpuidle_use_deepest_state()
      cpuidle: Allow idle injection to apply exit latency limit
      cpuidle: Introduce cpuidle_driver_state_disabled() for driver quirks
      cpuidle: teo: Avoid code duplication in conditionals
      cpuidle: teo: Avoid using "early hits" incorrectly
      cpuidle: teo: Exclude cpuidle overhead from computations
      cpuidle: Use nanoseconds as the unit of time
      cpuidle: Consolidate disabled state checks
      ACPI: processor_idle: Skip dummy wait if kernel is in guest
      cpuidle: Do not unset the driver if it is there already
      cpuidle: teo: Fix "early hits" handling for disabled idle states
      cpuidle: teo: Consider hits and misses metrics of disabled states
      cpuidle: teo: Rename local variable in teo_select()
      cpuidle: teo: Ignore disabled idle states that are too deep

commit 5aa9ba6312e36c18626e73506b92d1513d815435
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Sat Nov 16 14:16:13 2019 +0100

    cpuidle: Pass exit latency limit to cpuidle_use_deepest_state()
    
    Modify cpuidle_use_deepest_state() to take an additional exit latency
    limit argument to be passed to find_deepest_idle_state() and make
    cpuidle_idle_call() pass dev->forced_idle_latency_limit_ns to it for
    forced idle.
    
    Suggested-by: Rafael J. Wysocki <rafael@kernel.org>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    [ rjw: Rebase and rearrange code, subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index cd05ffa0abfe..fc9604ddd802 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -166,6 +166,8 @@ static void cpuidle_idle_call(void)
 	 */
 
 	if (idle_should_enter_s2idle() || dev->forced_idle_latency_limit_ns) {
+		u64 max_latency_ns;
+
 		if (idle_should_enter_s2idle()) {
 			rcu_idle_enter();
 
@@ -176,12 +178,16 @@ static void cpuidle_idle_call(void)
 			}
 
 			rcu_idle_exit();
+
+			max_latency_ns = U64_MAX;
+		} else {
+			max_latency_ns = dev->forced_idle_latency_limit_ns;
 		}
 
 		tick_nohz_idle_stop_tick();
 		rcu_idle_enter();
 
-		next_state = cpuidle_find_deepest_state(drv, dev);
+		next_state = cpuidle_find_deepest_state(drv, dev, max_latency_ns);
 		call_cpuidle(drv, dev, next_state);
 	} else {
 		bool stop_tick = true;

commit c55b51a06b01d67a99457bb82a8c31081c7faa23
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Sat Nov 16 14:16:12 2019 +0100

    cpuidle: Allow idle injection to apply exit latency limit
    
    In some cases it may be useful to specify an exit latency limit for
    the idle state to be used during CPU idle time injection.
    
    Instead of duplicating the information in struct cpuidle_device
    or propagating the latency limit in the call stack, replace the
    use_deepest_state field with forced_latency_limit_ns to represent
    that limit, so that the deepest idle state with exit latency within
    that limit is forced (i.e. no governors) when it is set.
    
    A zero exit latency limit for forced idle means to use governors in
    the usual way (analogous to use_deepest_state equal to "false" before
    this change).
    
    Additionally, add play_idle_precise() taking two arguments, the
    duration of forced idle and the idle state exit latency limit, both
    in nanoseconds, and redefine play_idle() as a wrapper around that
    new function.
    
    This change is preparatory, no functional impact is expected.
    
    Suggested-by: Rafael J. Wysocki <rafael@kernel.org>
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    [ rjw: Subject, changelog, cpuidle_use_deepest_state() kerneldoc, whitespace ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 1aa260702b38..cd05ffa0abfe 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -165,7 +165,7 @@ static void cpuidle_idle_call(void)
 	 * until a proper wakeup interrupt happens.
 	 */
 
-	if (idle_should_enter_s2idle() || dev->use_deepest_state) {
+	if (idle_should_enter_s2idle() || dev->forced_idle_latency_limit_ns) {
 		if (idle_should_enter_s2idle()) {
 			rcu_idle_enter();
 
@@ -311,7 +311,7 @@ static enum hrtimer_restart idle_inject_timer_fn(struct hrtimer *timer)
 	return HRTIMER_NORESTART;
 }
 
-void play_idle(unsigned long duration_us)
+void play_idle_precise(u64 duration_ns, u64 latency_ns)
 {
 	struct idle_timer it;
 
@@ -323,29 +323,29 @@ void play_idle(unsigned long duration_us)
 	WARN_ON_ONCE(current->nr_cpus_allowed != 1);
 	WARN_ON_ONCE(!(current->flags & PF_KTHREAD));
 	WARN_ON_ONCE(!(current->flags & PF_NO_SETAFFINITY));
-	WARN_ON_ONCE(!duration_us);
+	WARN_ON_ONCE(!duration_ns);
 
 	rcu_sleep_check();
 	preempt_disable();
 	current->flags |= PF_IDLE;
-	cpuidle_use_deepest_state(true);
+	cpuidle_use_deepest_state(latency_ns);
 
 	it.done = 0;
 	hrtimer_init_on_stack(&it.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	it.timer.function = idle_inject_timer_fn;
-	hrtimer_start(&it.timer, ns_to_ktime(duration_us * NSEC_PER_USEC),
+	hrtimer_start(&it.timer, ns_to_ktime(duration_ns),
 		      HRTIMER_MODE_REL_PINNED);
 
 	while (!READ_ONCE(it.done))
 		do_idle();
 
-	cpuidle_use_deepest_state(false);
+	cpuidle_use_deepest_state(0);
 	current->flags &= ~PF_IDLE;
 
 	preempt_fold_need_resched();
 	preempt_enable();
 }
-EXPORT_SYMBOL_GPL(play_idle);
+EXPORT_SYMBOL_GPL(play_idle_precise);
 
 void cpu_startup_entry(enum cpuhp_state state)
 {

commit c1d51f684c72b5eb2aecbbd47be3a2977a2dc903
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Nov 7 15:25:12 2019 +0100

    cpuidle: Use nanoseconds as the unit of time
    
    Currently, the cpuidle subsystem uses microseconds as the unit of
    time which (among other things) causes the idle loop to incur some
    integer division overhead for no clear benefit.
    
    In order to allow cpuidle to measure time in nanoseconds, add two
    new fields, exit_latency_ns and target_residency_ns, to represent the
    exit latency and target residency of an idle state in nanoseconds,
    respectively, to struct cpuidle_state and initialize them with the
    help of the corresponding values in microseconds provided by drivers.
    Additionally, change cpuidle_governor_latency_req() to return the
    idle state exit latency constraint in nanoseconds.
    
    Also meeasure idle state residency (last_residency_ns in struct
    cpuidle_device and time_ns in struct cpuidle_driver) in nanoseconds
    and update the cpuidle core and governors accordingly.
    
    However, the menu governor still computes typical intervals in
    microseconds to avoid integer overflows.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Doug Smythies <dsmythies@telus.net>
    Tested-by: Doug Smythies <dsmythies@telus.net>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 8dad5aa600ea..1aa260702b38 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -104,7 +104,7 @@ static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 * update no idle residency and return.
 	 */
 	if (current_clr_polling_and_test()) {
-		dev->last_residency = 0;
+		dev->last_residency_ns = 0;
 		local_irq_enable();
 		return -EBUSY;
 	}

commit a0e813f26ebcb25c0b5e504498fbd796cca1a4ba
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 14:16:00 2019 +0100

    sched/core: Further clarify sched_class::set_next_task()
    
    It turns out there really is something special to the first
    set_next_task() invocation. In specific the 'change' pattern really
    should not cause balance callbacks.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: ktkhai@virtuozzo.com
    Cc: mgorman@suse.de
    Cc: qais.yousef@arm.com
    Cc: qperret@google.com
    Cc: rostedt@goodmis.org
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Fixes: f95d4eaee6d0 ("sched/{rt,deadline}: Fix set_next_task vs pick_next_task")
    Link: https://lkml.kernel.org/r/20191108131909.775434698@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index f88b79eeee1e..428cd05c0b5d 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -385,7 +385,7 @@ static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
 {
 }
 
-static void set_next_task_idle(struct rq *rq, struct task_struct *next)
+static void set_next_task_idle(struct rq *rq, struct task_struct *next, bool first)
 {
 	update_idle_core(rq);
 	schedstat_inc(rq->sched_goidle);
@@ -395,7 +395,7 @@ struct task_struct *pick_next_task_idle(struct rq *rq)
 {
 	struct task_struct *next = rq->idle;
 
-	set_next_task_idle(rq, next);
+	set_next_task_idle(rq, next, true);
 
 	return next;
 }

commit 98c2f700edb413e4baa4a0368c5861d96211a775
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 14:15:58 2019 +0100

    sched/core: Simplify sched_class::pick_next_task()
    
    Now that the indirect class call never uses the last two arguments of
    pick_next_task(), remove them.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: ktkhai@virtuozzo.com
    Cc: mgorman@suse.de
    Cc: qais.yousef@arm.com
    Cc: qperret@google.com
    Cc: rostedt@goodmis.org
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: https://lkml.kernel.org/r/20191108131909.660595546@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 0fdceac00b70..f88b79eeee1e 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -391,13 +391,10 @@ static void set_next_task_idle(struct rq *rq, struct task_struct *next)
 	schedstat_inc(rq->sched_goidle);
 }
 
-struct task_struct *
-pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+struct task_struct *pick_next_task_idle(struct rq *rq)
 {
 	struct task_struct *next = rq->idle;
 
-	WARN_ON_ONCE(prev || rf);
-
 	set_next_task_idle(rq, next);
 
 	return next;

commit 5d7d605642b28a5911198a405a6072f091bfbee6
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 14:15:57 2019 +0100

    sched/core: Optimize pick_next_task()
    
    Ever since we moved the sched_class definitions into their own files,
    the constant expression {fair,idle}_sched_class.pick_next_task() is
    not in fact a compile time constant anymore and results in an indirect
    call (barring LTO).
    
    Fix that by exposing pick_next_task_{fair,idle}() directly, this gets
    rid of the indirect call (and RETPOLINE) on the fast path.
    
    Also remove the unlikely() from the idle case, it is in fact /the/ way
    we select idle -- and that is a very common thing to do.
    
    Performance for will-it-scale/sched_yield improves by 2% (as reported
    by 0-day).
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: ktkhai@virtuozzo.com
    Cc: mgorman@suse.de
    Cc: qais.yousef@arm.com
    Cc: qperret@google.com
    Cc: rostedt@goodmis.org
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: https://lkml.kernel.org/r/20191108131909.603037345@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 179d1d4ac5a6..0fdceac00b70 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -391,7 +391,7 @@ static void set_next_task_idle(struct rq *rq, struct task_struct *next)
 	schedstat_inc(rq->sched_goidle);
 }
 
-static struct task_struct *
+struct task_struct *
 pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 {
 	struct task_struct *next = rq->idle;

commit f488e1057bb97b881ed317557dc5e62ff8747393
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 14:15:56 2019 +0100

    sched/core: Make pick_next_task_idle() more consistent
    
    Only pick_next_task_fair() needs the @prev and @rf argument; these are
    required to implement the cpu-cgroup optimization. None of the other
    pick_next_task() methods need this. Make pick_next_task_idle() more
    consistent.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: bsegall@google.com
    Cc: dietmar.eggemann@arm.com
    Cc: juri.lelli@redhat.com
    Cc: ktkhai@virtuozzo.com
    Cc: mgorman@suse.de
    Cc: qais.yousef@arm.com
    Cc: qperret@google.com
    Cc: rostedt@goodmis.org
    Cc: valentin.schneider@arm.com
    Cc: vincent.guittot@linaro.org
    Link: https://lkml.kernel.org/r/20191108131909.545730862@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index f65ef1e2f204..179d1d4ac5a6 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -396,8 +396,7 @@ pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf
 {
 	struct task_struct *next = rq->idle;
 
-	if (prev)
-		put_prev_task(rq, prev);
+	WARN_ON_ONCE(prev || rf);
 
 	set_next_task_idle(rq, next);
 

commit 6e2df0581f569038719cf2bc2b3baa3fcc83cab4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Nov 8 11:11:52 2019 +0100

    sched: Fix pick_next_task() vs 'change' pattern race
    
    Commit 67692435c411 ("sched: Rework pick_next_task() slow-path")
    inadvertly introduced a race because it changed a previously
    unexplored dependency between dropping the rq->lock and
    sched_class::put_prev_task().
    
    The comments about dropping rq->lock, in for example
    newidle_balance(), only mentions the task being current and ->on_cpu
    being set. But when we look at the 'change' pattern (in for example
    sched_setnuma()):
    
            queued = task_on_rq_queued(p); /* p->on_rq == TASK_ON_RQ_QUEUED */
            running = task_current(rq, p); /* rq->curr == p */
    
            if (queued)
                    dequeue_task(...);
            if (running)
                    put_prev_task(...);
    
            /* change task properties */
    
            if (queued)
                    enqueue_task(...);
            if (running)
                    set_next_task(...);
    
    It becomes obvious that if we do this after put_prev_task() has
    already been called on @p, things go sideways. This is exactly what
    the commit in question allows to happen when it does:
    
            prev->sched_class->put_prev_task(rq, prev, rf);
            if (!rq->nr_running)
                    newidle_balance(rq, rf);
    
    The newidle_balance() call will drop rq->lock after we've called
    put_prev_task() and that allows the above 'change' pattern to
    interleave and mess up the state.
    
    Furthermore, it turns out we lost the RT-pull when we put the last DL
    task.
    
    Fix both problems by extracting the balancing from put_prev_task() and
    doing a multi-class balance() pass before put_prev_task().
    
    Fixes: 67692435c411 ("sched: Rework pick_next_task() slow-path")
    Reported-by: Quentin Perret <qperret@google.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Quentin Perret <qperret@google.com>
    Tested-by: Valentin Schneider <valentin.schneider@arm.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 8dad5aa600ea..f65ef1e2f204 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -365,6 +365,12 @@ select_task_rq_idle(struct task_struct *p, int cpu, int sd_flag, int flags)
 {
 	return task_cpu(p); /* IDLE tasks as never migrated */
 }
+
+static int
+balance_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+{
+	return WARN_ON_ONCE(1);
+}
 #endif
 
 /*
@@ -375,7 +381,7 @@ static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int fl
 	resched_curr(rq);
 }
 
-static void put_prev_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
 {
 }
 
@@ -460,6 +466,7 @@ const struct sched_class idle_sched_class = {
 	.set_next_task          = set_next_task_idle,
 
 #ifdef CONFIG_SMP
+	.balance		= balance_idle,
 	.select_task_rq		= select_task_rq_idle,
 	.set_cpus_allowed	= set_cpus_allowed_common,
 #endif

commit 13224794cb0832caa403ad583d8605202cabc6bc
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Mon Sep 23 15:35:19 2019 -0700

    mm: remove quicklist page table caches
    
    Patch series "mm: remove quicklist page table caches".
    
    A while ago Nicholas proposed to remove quicklist page table caches [1].
    
    I've rebased his patch on the curren upstream and switched ia64 and sh to
    use generic versions of PTE allocation.
    
    [1] https://lore.kernel.org/linux-mm/20190711030339.20892-1-npiggin@gmail.com
    
    This patch (of 3):
    
    Remove page table allocator "quicklists".  These have been around for a
    long time, but have not got much traction in the last decade and are only
    used on ia64 and sh architectures.
    
    The numbers in the initial commit look interesting but probably don't
    apply anymore.  If anybody wants to resurrect this it's in the git
    history, but it's unhelpful to have this code and divergent allocator
    behaviour for minor archs.
    
    Also it might be better to instead make more general improvements to page
    allocator if this is still so slow.
    
    Link: http://lkml.kernel.org/r/1565250728-21721-2-git-send-email-rppt@linux.ibm.com
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Signed-off-by: Mike Rapoport <rppt@linux.ibm.com>
    Cc: Tony Luck <tony.luck@intel.com>
    Cc: Yoshinori Sato <ysato@users.sourceforge.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index c892c6280c9f..8dad5aa600ea 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -238,7 +238,6 @@ static void do_idle(void)
 	tick_nohz_idle_enter();
 
 	while (!need_resched()) {
-		check_pgt_cache();
 		rmb();
 
 		local_irq_disable();

commit 77dcfe2b9edc98286cf18e03c243c9b999f955d9
Merge: 04cbfba62085 fc6763a2d7e0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 17 19:15:14 2019 -0700

    Merge tag 'pm-5.4-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull power management updates from Rafael Wysocki:
     "These include a rework of the main suspend-to-idle code flow (related
      to the handling of spurious wakeups), a switch over of several users
      of cpufreq notifiers to QoS-based limits, a new devfreq driver for
      Tegra20, a new cpuidle driver and governor for virtualized guests, an
      extension of the wakeup sources framework to expose wakeup sources as
      device objects in sysfs, and more.
    
      Specifics:
    
       - Rework the main suspend-to-idle control flow to avoid repeating
         "noirq" device resume and suspend operations in case of spurious
         wakeups from the ACPI EC and decouple the ACPI EC wakeups support
         from the LPS0 _DSM support (Rafael Wysocki).
    
       - Extend the wakeup sources framework to expose wakeup sources as
         device objects in sysfs (Tri Vo, Stephen Boyd).
    
       - Expose system suspend statistics in sysfs (Kalesh Singh).
    
       - Introduce a new haltpoll cpuidle driver and a new matching governor
         for virtualized guests wanting to do guest-side polling in the idle
         loop (Marcelo Tosatti, Joao Martins, Wanpeng Li, Stephen Rothwell).
    
       - Fix the menu and teo cpuidle governors to allow the scheduler tick
         to be stopped if PM QoS is used to limit the CPU idle state exit
         latency in some cases (Rafael Wysocki).
    
       - Increase the resolution of the play_idle() argument to microseconds
         for more fine-grained injection of CPU idle cycles (Daniel
         Lezcano).
    
       - Switch over some users of cpuidle notifiers to the new QoS-based
         frequency limits and drop the CPUFREQ_ADJUST and CPUFREQ_NOTIFY
         policy notifier events (Viresh Kumar).
    
       - Add new cpufreq driver based on nvmem for sun50i (Yangtao Li).
    
       - Add support for MT8183 and MT8516 to the mediatek cpufreq driver
         (Andrew-sh.Cheng, Fabien Parent).
    
       - Add i.MX8MN support to the imx-cpufreq-dt cpufreq driver (Anson
         Huang).
    
       - Add qcs404 to cpufreq-dt-platdev blacklist (Jorge Ramirez-Ortiz).
    
       - Update the qcom cpufreq driver (among other things, to make it
         easier to extend and to use kryo cpufreq for other nvmem-based
         SoCs) and add qcs404 support to it (Niklas Cassel, Douglas
         RAILLARD, Sibi Sankar, Sricharan R).
    
       - Fix assorted issues and make assorted minor improvements in the
         cpufreq code (Colin Ian King, Douglas RAILLARD, Florian Fainelli,
         Gustavo Silva, Hariprasad Kelam).
    
       - Add new devfreq driver for NVidia Tegra20 (Dmitry Osipenko, Arnd
         Bergmann).
    
       - Add new Exynos PPMU events to devfreq events and extend that
         mechanism (Lukasz Luba).
    
       - Fix and clean up the exynos-bus devfreq driver (Kamil Konieczny).
    
       - Improve devfreq documentation and governor code, fix spelling typos
         in devfreq (Ezequiel Garcia, Krzysztof Kozlowski, Leonard Crestez,
         MyungJoo Ham, Gaël PORTAY).
    
       - Add regulators enable and disable to the OPP (operating performance
         points) framework (Kamil Konieczny).
    
       - Update the OPP framework to support multiple opp-suspend properties
         (Anson Huang).
    
       - Fix assorted issues and make assorted minor improvements in the OPP
         code (Niklas Cassel, Viresh Kumar, Yue Hu).
    
       - Clean up the generic power domains (genpd) framework (Ulf Hansson).
    
       - Clean up assorted pieces of power management code and documentation
         (Akinobu Mita, Amit Kucheria, Chuhong Yuan).
    
       - Update the pm-graph tool to version 5.5 including multiple fixes
         and improvements (Todd Brandt).
    
       - Update the cpupower utility (Benjamin Weis, Geert Uytterhoeven,
         Sébastien Szymanski)"
    
    * tag 'pm-5.4-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (126 commits)
      cpuidle-haltpoll: Enable kvm guest polling when dedicated physical CPUs are available
      cpuidle-haltpoll: do not set an owner to allow modunload
      cpuidle-haltpoll: return -ENODEV on modinit failure
      cpuidle-haltpoll: set haltpoll as preferred governor
      cpuidle: allow governor switch on cpuidle_register_driver()
      PM: runtime: Documentation: add runtime_status ABI document
      pm-graph: make setVal unbuffered again for python2 and python3
      powercap: idle_inject: Use higher resolution for idle injection
      cpuidle: play_idle: Increase the resolution to usec
      cpuidle-haltpoll: vcpu hotplug support
      cpufreq: Add qcs404 to cpufreq-dt-platdev blacklist
      cpufreq: qcom: Add support for qcs404 on nvmem driver
      cpufreq: qcom: Refactor the driver to make it easier to extend
      cpufreq: qcom: Re-organise kryo cpufreq to use it for other nvmem based qcom socs
      dt-bindings: opp: Add qcom-opp bindings with properties needed for CPR
      dt-bindings: opp: qcom-nvmem: Support pstates provided by a power domain
      Documentation: cpufreq: Update policy notifier documentation
      cpufreq: Remove CPUFREQ_ADJUST and CPUFREQ_NOTIFY policy notifier events
      PM / Domains: Verify PM domain type in dev_pm_genpd_set_performance_state()
      PM / Domains: Simplify genpd_lookup_dev()
      ...

commit 7e67a859997aad47727aff9c5a32e160da079ce3
Merge: 772c1d06bd40 563c4f85f9f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 17:25:49 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - MAINTAINERS: Add Mark Rutland as perf submaintainer, Juri Lelli and
       Vincent Guittot as scheduler submaintainers. Add Dietmar Eggemann,
       Steven Rostedt, Ben Segall and Mel Gorman as scheduler reviewers.
    
       As perf and the scheduler is getting bigger and more complex,
       document the status quo of current responsibilities and interests,
       and spread the review pain^H^H^H^H fun via an increase in the Cc:
       linecount generated by scripts/get_maintainer.pl. :-)
    
     - Add another series of patches that brings the -rt (PREEMPT_RT) tree
       closer to mainline: split the monolithic CONFIG_PREEMPT dependencies
       into a new CONFIG_PREEMPTION category that will allow the eventual
       introduction of CONFIG_PREEMPT_RT. Still a few more hundred patches
       to go though.
    
     - Extend the CPU cgroup controller with uclamp.min and uclamp.max to
       allow the finer shaping of CPU bandwidth usage.
    
     - Micro-optimize energy-aware wake-ups from O(CPUS^2) to O(CPUS).
    
     - Improve the behavior of high CPU count, high thread count
       applications running under cpu.cfs_quota_us constraints.
    
     - Improve balancing with SCHED_IDLE (SCHED_BATCH) tasks present.
    
     - Improve CPU isolation housekeeping CPU allocation NUMA locality.
    
     - Fix deadline scheduler bandwidth calculations and logic when cpusets
       rebuilds the topology, or when it gets deadline-throttled while it's
       being offlined.
    
     - Convert the cpuset_mutex to percpu_rwsem, to allow it to be used from
       setscheduler() system calls without creating global serialization.
       Add new synchronization between cpuset topology-changing events and
       the deadline acceptance tests in setscheduler(), which were broken
       before.
    
     - Rework the active_mm state machine to be less confusing and more
       optimal.
    
     - Rework (simplify) the pick_next_task() slowpath.
    
     - Improve load-balancing on AMD EPYC systems.
    
     - ... and misc cleanups, smaller fixes and improvements - please see
       the Git log for more details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (53 commits)
      sched/psi: Correct overly pessimistic size calculation
      sched/fair: Speed-up energy-aware wake-ups
      sched/uclamp: Always use 'enum uclamp_id' for clamp_id values
      sched/uclamp: Update CPU's refcount on TG's clamp changes
      sched/uclamp: Use TG's clamps to restrict TASK's clamps
      sched/uclamp: Propagate system defaults to the root group
      sched/uclamp: Propagate parent clamps
      sched/uclamp: Extend CPU's cgroup controller
      sched/topology: Improve load balancing on AMD EPYC systems
      arch, ia64: Make NUMA select SMP
      sched, perf: MAINTAINERS update, add submaintainers and reviewers
      sched/fair: Use rq_lock/unlock in online_fair_sched_group
      cpufreq: schedutil: fix equation in comment
      sched: Rework pick_next_task() slow-path
      sched: Allow put_prev_task() to drop rq->lock
      sched/fair: Expose newidle_balance()
      sched: Add task_struct pointer to sched_class::set_curr_task
      sched: Rework CPU hotplug task selection
      sched/{rt,deadline}: Fix set_next_task vs pick_next_task
      sched: Fix kerneldoc comment for ia64_set_curr_task
      ...

commit 82e430a6df7f0b5972c7fe717faffea823c6b84a
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Fri Aug 2 19:34:23 2019 +0200

    cpuidle: play_idle: Increase the resolution to usec
    
    The play_idle resolution is 1ms. The intel_powerclamp bases the idle
    duration on jiffies. The idle injection API is also using msec based
    duration but has no user yet.
    
    Unfortunately, msec based time does not fit well when we want to
    inject idle cycle precisely with shallow idle state.
    
    In order to set the scene for the incoming idle injection user, move
    the precision up to usec when calling play_idle.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 80940939b733..b98283fc6914 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -311,7 +311,7 @@ static enum hrtimer_restart idle_inject_timer_fn(struct hrtimer *timer)
 	return HRTIMER_NORESTART;
 }
 
-void play_idle(unsigned long duration_ms)
+void play_idle(unsigned long duration_us)
 {
 	struct idle_timer it;
 
@@ -323,7 +323,7 @@ void play_idle(unsigned long duration_ms)
 	WARN_ON_ONCE(current->nr_cpus_allowed != 1);
 	WARN_ON_ONCE(!(current->flags & PF_KTHREAD));
 	WARN_ON_ONCE(!(current->flags & PF_NO_SETAFFINITY));
-	WARN_ON_ONCE(!duration_ms);
+	WARN_ON_ONCE(!duration_us);
 
 	rcu_sleep_check();
 	preempt_disable();
@@ -333,7 +333,8 @@ void play_idle(unsigned long duration_ms)
 	it.done = 0;
 	hrtimer_init_on_stack(&it.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
 	it.timer.function = idle_inject_timer_fn;
-	hrtimer_start(&it.timer, ms_to_ktime(duration_ms), HRTIMER_MODE_REL_PINNED);
+	hrtimer_start(&it.timer, ns_to_ktime(duration_us * NSEC_PER_USEC),
+		      HRTIMER_MODE_REL_PINNED);
 
 	while (!READ_ONCE(it.done))
 		do_idle();

commit e78a7614f3876ac649b3df608789cb6ef74d0480
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 5 07:46:43 2019 -0700

    idle: Prevent late-arriving interrupts from disrupting offline
    
    Scheduling-clock interrupts can arrive late in the CPU-offline process,
    after idle entry and the subsequent call to cpuhp_report_idle_dead().
    Once execution passes the call to rcu_report_dead(), RCU is ignoring
    the CPU, which results in lockdep complaints when the interrupt handler
    uses RCU:
    
    ------------------------------------------------------------------------
    
    =============================
    WARNING: suspicious RCU usage
    5.2.0-rc1+ #681 Not tainted
    -----------------------------
    kernel/sched/fair.c:9542 suspicious rcu_dereference_check() usage!
    
    other info that might help us debug this:
    
    RCU used illegally from offline CPU!
    rcu_scheduler_active = 2, debug_locks = 1
    no locks held by swapper/5/0.
    
    stack backtrace:
    CPU: 5 PID: 0 Comm: swapper/5 Not tainted 5.2.0-rc1+ #681
    Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS Bochs 01/01/2011
    Call Trace:
     <IRQ>
     dump_stack+0x5e/0x8b
     trigger_load_balance+0xa8/0x390
     ? tick_sched_do_timer+0x60/0x60
     update_process_times+0x3b/0x50
     tick_sched_handle+0x2f/0x40
     tick_sched_timer+0x32/0x70
     __hrtimer_run_queues+0xd3/0x3b0
     hrtimer_interrupt+0x11d/0x270
     ? sched_clock_local+0xc/0x74
     smp_apic_timer_interrupt+0x79/0x200
     apic_timer_interrupt+0xf/0x20
     </IRQ>
    RIP: 0010:delay_tsc+0x22/0x50
    Code: ff 0f 1f 80 00 00 00 00 65 44 8b 05 18 a7 11 48 0f ae e8 0f 31 48 89 d6 48 c1 e6 20 48 09 c6 eb 0e f3 90 65 8b 05 fe a6 11 48 <41> 39 c0 75 18 0f ae e8 0f 31 48 c1 e2 20 48 09 c2 48 89 d0 48 29
    RSP: 0000:ffff8f92c0157ed0 EFLAGS: 00000212 ORIG_RAX: ffffffffffffff13
    RAX: 0000000000000005 RBX: ffff8c861f356400 RCX: ffff8f92c0157e64
    RDX: 000000321214c8cc RSI: 00000032120daa7f RDI: 0000000000260f15
    RBP: 0000000000000005 R08: 0000000000000005 R09: 0000000000000000
    R10: 0000000000000001 R11: 0000000000000001 R12: 0000000000000000
    R13: 0000000000000000 R14: ffff8c861ee18000 R15: ffff8c861ee18000
     cpuhp_report_idle_dead+0x31/0x60
     do_idle+0x1d5/0x200
     ? _raw_spin_unlock_irqrestore+0x2d/0x40
     cpu_startup_entry+0x14/0x20
     start_secondary+0x151/0x170
     secondary_startup_64+0xa4/0xb0
    
    ------------------------------------------------------------------------
    
    This happens rarely, but can be forced by happen more often by
    placing delays in cpuhp_report_idle_dead() following the call to
    rcu_report_dead().  With this in place, the following rcutorture
    scenario reproduces the problem within a few minutes:
    
    tools/testing/selftests/rcutorture/bin/kvm.sh --cpus 8 --duration 5 --kconfig "CONFIG_DEBUG_LOCK_ALLOC=y CONFIG_PROVE_LOCKING=y" --configs "TREE04"
    
    This commit uses the crude but effective expedient of moving the disabling
    of interrupts within the idle loop to precede the cpu_is_offline()
    check.  It also invokes tick_nohz_idle_stop_tick() instead of
    tick_nohz_idle_stop_tick_protected() to shut off the scheduling-clock
    interrupt.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@kernel.org>
    [ paulmck: Revert tick_nohz_idle_stop_tick_protected() removal, new callers. ]
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 80940939b733..e4bc4aa739b8 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -241,13 +241,14 @@ static void do_idle(void)
 		check_pgt_cache();
 		rmb();
 
+		local_irq_disable();
+
 		if (cpu_is_offline(cpu)) {
-			tick_nohz_idle_stop_tick_protected();
+			tick_nohz_idle_stop_tick();
 			cpuhp_report_idle_dead();
 			arch_cpu_idle_dead();
 		}
 
-		local_irq_disable();
 		arch_cpu_idle_enter();
 
 		/*

commit 67692435c411e5c53a1c588ecca2037aebd81f2e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 29 20:36:44 2019 +0000

    sched: Rework pick_next_task() slow-path
    
    Avoid the RETRY_TASK case in the pick_next_task() slow path.
    
    By doing the put_prev_task() early, we get the rt/deadline pull done,
    and by testing rq->nr_running we know if we need newidle_balance().
    
    This then gives a stable state to pick a task from.
    
    Since the fast-path is fair only; it means the other classes will
    always have pick_next_task(.prev=NULL, .rf=NULL) and we can simplify.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>
    Link: https://lkml.kernel.org/r/aa34d24b36547139248f32a30138791ac6c02bd6.1559129225.git.vpillai@digitalocean.com

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 8d59de2e4a6e..7c54550dda6a 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -389,7 +389,9 @@ pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf
 {
 	struct task_struct *next = rq->idle;
 
-	put_prev_task(rq, prev);
+	if (prev)
+		put_prev_task(rq, prev);
+
 	set_next_task_idle(rq, next);
 
 	return next;

commit 5f2a45fc9e89e022233085e6f0f352eb6ff770bb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 29 20:36:43 2019 +0000

    sched: Allow put_prev_task() to drop rq->lock
    
    Currently the pick_next_task() loop is convoluted and ugly because of
    how it can drop the rq->lock and needs to restart the picking.
    
    For the RT/Deadline classes, it is put_prev_task() where we do
    balancing, and we could do this before the picking loop. Make this
    possible.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>
    Link: https://lkml.kernel.org/r/e4519f6850477ab7f3d257062796e6425ee4ba7c.1559129225.git.vpillai@digitalocean.com

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 54194d41035c..8d59de2e4a6e 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -374,7 +374,7 @@ static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int fl
 	resched_curr(rq);
 }
 
-static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
+static void put_prev_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 {
 }
 

commit 03b7fad167efca3b7abbbb39733933f9df56e79c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed May 29 20:36:41 2019 +0000

    sched: Add task_struct pointer to sched_class::set_curr_task
    
    In preparation of further separating pick_next_task() and
    set_curr_task() we have to pass the actual task into it, while there,
    rename the thing to better pair with put_prev_task().
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aaron Lu <aaron.lwe@gmail.com>
    Cc: Valentin Schneider <valentin.schneider@arm.com>
    Cc: mingo@kernel.org
    Cc: Phil Auld <pauld@redhat.com>
    Cc: Julien Desfossez <jdesfossez@digitalocean.com>
    Cc: Nishanth Aravamudan <naravamudan@digitalocean.com>
    Link: https://lkml.kernel.org/r/a96d1bcdd716db4a4c5da2fece647a1456c0ed78.1559129225.git.vpillai@digitalocean.com

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 80940939b733..54194d41035c 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -374,14 +374,25 @@ static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int fl
 	resched_curr(rq);
 }
 
+static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
+{
+}
+
+static void set_next_task_idle(struct rq *rq, struct task_struct *next)
+{
+	update_idle_core(rq);
+	schedstat_inc(rq->sched_goidle);
+}
+
 static struct task_struct *
 pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
 {
+	struct task_struct *next = rq->idle;
+
 	put_prev_task(rq, prev);
-	update_idle_core(rq);
-	schedstat_inc(rq->sched_goidle);
+	set_next_task_idle(rq, next);
 
-	return rq->idle;
+	return next;
 }
 
 /*
@@ -397,10 +408,6 @@ dequeue_task_idle(struct rq *rq, struct task_struct *p, int flags)
 	raw_spin_lock_irq(&rq->lock);
 }
 
-static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
-{
-}
-
 /*
  * scheduler tick hitting a task of our scheduling class.
  *
@@ -413,10 +420,6 @@ static void task_tick_idle(struct rq *rq, struct task_struct *curr, int queued)
 {
 }
 
-static void set_curr_task_idle(struct rq *rq)
-{
-}
-
 static void switched_to_idle(struct rq *rq, struct task_struct *p)
 {
 	BUG();
@@ -451,13 +454,13 @@ const struct sched_class idle_sched_class = {
 
 	.pick_next_task		= pick_next_task_idle,
 	.put_prev_task		= put_prev_task_idle,
+	.set_next_task          = set_next_task_idle,
 
 #ifdef CONFIG_SMP
 	.select_task_rq		= select_task_rq_idle,
 	.set_cpus_allowed	= set_cpus_allowed_common,
 #endif
 
-	.set_curr_task          = set_curr_task_idle,
 	.task_tick		= task_tick_idle,
 
 	.get_rr_interval	= get_rr_interval_idle,

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index f5516bae0c1b..80940939b733 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * Generic entry points for the idle threads and
  * implementation of the idle task scheduling class.

commit 977e4be5eb714c48a67afc26a6c477f24130a1f2
Author: Christophe Leroy <christophe.leroy@c-s.fr>
Date:   Sat Oct 20 09:26:49 2018 +0200

    x86/stackprotector: Remove the call to boot_init_stack_canary() from cpu_startup_entry()
    
    The following commit:
    
      d7880812b359 ("idle: Add the stack canary init to cpu_startup_entry()")
    
    ... added an x86 specific boot_init_stack_canary() call to the generic
    cpu_startup_entry() as a temporary hack, with the intention to remove
    the #ifdef CONFIG_X86 later.
    
    More than 5 years later let's finally realize that plan! :-)
    
    While implementing stack protector support for PowerPC, we found
    that calling boot_init_stack_canary() is also needed for PowerPC
    which uses per task (TLS) stack canary like the X86.
    
    However, calling boot_init_stack_canary() would break architectures
    using a global stack canary (ARM, SH, MIPS and XTENSA).
    
    Instead of modifying the #ifdef CONFIG_X86 to an even messier:
    
       #if defined(CONFIG_X86) || defined(CONFIG_PPC)
    
    PowerPC implemented the call to boot_init_stack_canary() in the function
    calling cpu_startup_entry().
    
    Let's try the same cleanup on the x86 side as well.
    
    On x86 we have two functions calling cpu_startup_entry():
    
     - start_secondary()
     - cpu_bringup_and_idle()
    
    start_secondary() already calls boot_init_stack_canary(), so
    it's good, and this patch adds the call to boot_init_stack_canary()
    in cpu_bringup_and_idle().
    
    I.e. now x86 catches up to the rest of the world and the ugly init
    sequence in init/main.c can be removed from cpu_startup_entry().
    
    As a final benefit we can also remove the <linux/stackprotector.h>
    dependency from <linux/sched.h>.
    
    [ mingo: Improved the changelog a bit, added language explaining x86 borkage and sched.h change. ]
    Signed-off-by: Christophe Leroy <christophe.leroy@c-s.fr>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: xen-devel@lists.xenproject.org
    Link: http://lkml.kernel.org/r/20181020072649.5B59310483E@pc16082vm.idsi0.si.c-s.fr
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 16f84142f2f4..f5516bae0c1b 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -347,21 +347,6 @@ EXPORT_SYMBOL_GPL(play_idle);
 
 void cpu_startup_entry(enum cpuhp_state state)
 {
-	/*
-	 * This #ifdef needs to die, but it's too late in the cycle to
-	 * make this generic (ARM and SH have never invoked the canary
-	 * init for the non boot CPUs!). Will be fixed in 3.11
-	 */
-#ifdef CONFIG_X86
-	/*
-	 * If we're the non-boot CPU, nothing set the stack canary up
-	 * for us. The boot CPU already has it initialized but no harm
-	 * in doing it again. This is a good place for updating it, as
-	 * we wont ever return from this function (so the invalid
-	 * canaries already on the stack wont ever trigger).
-	 */
-	boot_init_stack_canary();
-#endif
 	arch_cpu_idle_prepare();
 	cpuhp_online_idle(state);
 	while (1)

commit 7059b36636beab57c3c43c62104483e5449bee95
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Aug 9 19:08:34 2018 +0200

    sched: idle: Avoid retaining the tick when it has been stopped
    
    If the tick has been stopped already, but the governor has not asked to
    stop it (which it can do sometimes), the idle loop should invoke
    tick_nohz_idle_stop_tick(), to let tick_nohz_stop_tick() take care
    of this case properly.
    
    Fixes: 554c8aa8ecad (sched: idle: Select idle state before stopping the tick)
    Cc: 4.17+ <stable@vger.kernel.org> # 4.17+
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 1a3e9bddd17b..16f84142f2f4 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -190,7 +190,7 @@ static void cpuidle_idle_call(void)
 		 */
 		next_state = cpuidle_select(drv, dev, &stop_tick);
 
-		if (stop_tick)
+		if (stop_tick || tick_nohz_tick_stopped())
 			tick_nohz_idle_stop_tick();
 		else
 			tick_nohz_idle_retain_tick();

commit 554c8aa8ecade210d58a252173bb8f2106552a44
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Tue Apr 3 23:17:11 2018 +0200

    sched: idle: Select idle state before stopping the tick
    
    In order to address the issue with short idle duration predictions
    by the idle governor after the scheduler tick has been stopped,
    reorder the code in cpuidle_idle_call() so that the governor idle
    state selection runs before tick_nohz_idle_go_idle() and use the
    "nohz" hint returned by cpuidle_select() to decide whether or not
    to stop the tick.
    
    This isn't straightforward, because menu_select() invokes
    tick_nohz_get_sleep_length() to get the time to the next timer
    event and the number returned by the latter comes from
    __tick_nohz_idle_stop_tick().  Fortunately, however, it is possible
    to compute that number without actually stopping the tick and with
    the help of the existing code.
    
    Namely, tick_nohz_get_sleep_length() can be made call
    tick_nohz_next_event(), introduced earlier, to get the time to the
    next non-highres timer event.  If that happens, tick_nohz_next_event()
    need not be called by __tick_nohz_idle_stop_tick() again.
    
    If it turns out that the scheduler tick cannot be stopped going
    forward or the next timer event is too close for the tick to be
    stopped, tick_nohz_get_sleep_length() can simply return the time to
    the next event currently programmed into the corresponding clock
    event device.
    
    In addition to knowing the return value of tick_nohz_next_event(),
    however, tick_nohz_get_sleep_length() needs to know the time to the
    next highres timer event, but with the scheduler tick timer excluded,
    which can be computed with the help of hrtimer_get_next_event().
    
    That minimum of that number and the tick_nohz_next_event() return
    value is the total time to the next timer event with the assumption
    that the tick will be stopped.  It can be returned to the idle
    governor which can use it for predicting idle duration (under the
    assumption that the tick will be stopped) and deciding whether or
    not it makes sense to stop the tick before putting the CPU into the
    selected idle state.
    
    With the above, the sleep_length field in struct tick_sched is not
    necessary any more, so drop it.
    
    Link: https://bugzilla.kernel.org/show_bug.cgi?id=199227
    Reported-by: Doug Smythies <dsmythies@telus.net>
    Reported-by: Thomas Ilsche <thomas.ilsche@tu-dresden.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index a966bd2a6fa0..1a3e9bddd17b 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -185,13 +185,18 @@ static void cpuidle_idle_call(void)
 	} else {
 		bool stop_tick = true;
 
-		tick_nohz_idle_stop_tick();
-		rcu_idle_enter();
-
 		/*
 		 * Ask the cpuidle framework to choose a convenient idle state.
 		 */
 		next_state = cpuidle_select(drv, dev, &stop_tick);
+
+		if (stop_tick)
+			tick_nohz_idle_stop_tick();
+		else
+			tick_nohz_idle_retain_tick();
+
+		rcu_idle_enter();
+
 		entered_state = call_cpuidle(drv, dev, next_state);
 		/*
 		 * Give the governor an opportunity to reflect on the outcome

commit 45f1ff59e27ca59d33cc1a317e669d90022ccf7d
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 22 17:50:49 2018 +0100

    cpuidle: Return nohz hint from cpuidle_select()
    
    Add a new pointer argument to cpuidle_select() and to the ->select
    cpuidle governor callback to allow a boolean value indicating
    whether or not the tick should be stopped before entering the
    selected state to be returned from there.
    
    Make the ladder governor ignore that pointer (to preserve its
    current behavior) and make the menu governor return 'false" through
    it if:
     (1) the idle exit latency is constrained at 0, or
     (2) the selected state is a polling one, or
     (3) the expected idle period duration is within the tick period
         range.
    
    In addition to that, the correction factor computations in the menu
    governor need to take the possibility that the tick may not be
    stopped into account to avoid artificially small correction factor
    values.  To that end, add a mechanism to record tick wakeups, as
    suggested by Peter Zijlstra, and use it to modify the menu_update()
    behavior when tick wakeup occurs.  Namely, if the CPU is woken up by
    the tick and the return value of tick_nohz_get_sleep_length() is not
    within the tick boundary, the predicted idle duration is likely too
    short, so make menu_update() try to compensate for that by updating
    the governor statistics as though the CPU was idle for a long time.
    
    Since the value returned through the new argument pointer of
    cpuidle_select() is not used by its caller yet, this change by
    itself is not expected to alter the functionality of the code.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 4f64835d38a8..a966bd2a6fa0 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -183,13 +183,15 @@ static void cpuidle_idle_call(void)
 		next_state = cpuidle_find_deepest_state(drv, dev);
 		call_cpuidle(drv, dev, next_state);
 	} else {
+		bool stop_tick = true;
+
 		tick_nohz_idle_stop_tick();
 		rcu_idle_enter();
 
 		/*
 		 * Ask the cpuidle framework to choose a convenient idle state.
 		 */
-		next_state = cpuidle_select(drv, dev);
+		next_state = cpuidle_select(drv, dev, &stop_tick);
 		entered_state = call_cpuidle(drv, dev, next_state);
 		/*
 		 * Give the governor an opportunity to reflect on the outcome

commit ed98c34919985a9f87c3edacb9a8d8c283c9e243
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 15 23:07:41 2018 +0100

    sched: idle: Do not stop the tick before cpuidle_idle_call()
    
    Make cpuidle_idle_call() decide whether or not to stop the tick.
    
    First, the cpuidle_enter_s2idle() path deals with the tick (and with
    the entire timekeeping for that matter) by itself and it doesn't need
    the tick to be stopped beforehand.
    
    Second, to address the issue with short idle duration predictions
    by the idle governor after the tick has been stopped, it will be
    necessary to change the ordering of cpuidle_select() with respect
    to tick_nohz_idle_stop_tick().  To prepare for that, put a
    tick_nohz_idle_stop_tick() call in the same branch in which
    cpuidle_select() is called.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 3777e83c0b5a..4f64835d38a8 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -141,13 +141,15 @@ static void cpuidle_idle_call(void)
 	}
 
 	/*
-	 * Tell the RCU framework we are entering an idle section,
-	 * so no more rcu read side critical sections and one more
+	 * The RCU framework needs to be told that we are entering an idle
+	 * section, so no more rcu read side critical sections and one more
 	 * step to the grace period
 	 */
-	rcu_idle_enter();
 
 	if (cpuidle_not_available(drv, dev)) {
+		tick_nohz_idle_stop_tick();
+		rcu_idle_enter();
+
 		default_idle_call();
 		goto exit_idle;
 	}
@@ -164,16 +166,26 @@ static void cpuidle_idle_call(void)
 
 	if (idle_should_enter_s2idle() || dev->use_deepest_state) {
 		if (idle_should_enter_s2idle()) {
+			rcu_idle_enter();
+
 			entered_state = cpuidle_enter_s2idle(drv, dev);
 			if (entered_state > 0) {
 				local_irq_enable();
 				goto exit_idle;
 			}
+
+			rcu_idle_exit();
 		}
 
+		tick_nohz_idle_stop_tick();
+		rcu_idle_enter();
+
 		next_state = cpuidle_find_deepest_state(drv, dev);
 		call_cpuidle(drv, dev, next_state);
 	} else {
+		tick_nohz_idle_stop_tick();
+		rcu_idle_enter();
+
 		/*
 		 * Ask the cpuidle framework to choose a convenient idle state.
 		 */
@@ -240,7 +252,6 @@ static void do_idle(void)
 			tick_nohz_idle_restart_tick();
 			cpu_idle_poll();
 		} else {
-			tick_nohz_idle_stop_tick();
 			cpuidle_idle_call();
 		}
 		arch_cpu_idle_exit();

commit 2aaf709a518d26563b80fd7a42379d7aa7ffed4a
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Mar 15 23:05:50 2018 +0100

    sched: idle: Do not stop the tick upfront in the idle loop
    
    Push the decision whether or not to stop the tick somewhat deeper
    into the idle loop.
    
    Stopping the tick upfront leads to unpleasant outcomes in case the
    idle governor doesn't agree with the nohz code on the duration of the
    upcoming idle period.  Specifically, if the tick has been stopped and
    the idle governor predicts short idle, the situation is bad regardless
    of whether or not the prediction is accurate.  If it is accurate, the
    tick has been stopped unnecessarily which means excessive overhead.
    If it is not accurate, the CPU is likely to spend too much time in
    the (shallow, because short idle has been predicted) idle state
    selected by the governor [1].
    
    As the first step towards addressing this problem, change the code
    to make the tick stopping decision inside of the loop in do_idle().
    In particular, do not stop the tick in the cpu_idle_poll() code path.
    Also don't do that in tick_nohz_irq_exit() which doesn't really have
    enough information on whether or not to stop the tick.
    
    Link: https://marc.info/?l=linux-pm&m=150116085925208&w=2 # [1]
    Link: https://tu-dresden.de/zih/forschung/ressourcen/dateien/projekte/haec/powernightmares.pdf
    Suggested-by: Frederic Weisbecker <frederic@kernel.org>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index c0bc111878e6..3777e83c0b5a 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -216,13 +216,13 @@ static void do_idle(void)
 
 	__current_set_polling();
 	tick_nohz_idle_enter();
-	tick_nohz_idle_stop_tick_protected();
 
 	while (!need_resched()) {
 		check_pgt_cache();
 		rmb();
 
 		if (cpu_is_offline(cpu)) {
+			tick_nohz_idle_stop_tick_protected();
 			cpuhp_report_idle_dead();
 			arch_cpu_idle_dead();
 		}
@@ -236,10 +236,13 @@ static void do_idle(void)
 		 * broadcast device expired for us, we don't want to go deep
 		 * idle as we know that the IPI is going to arrive right away.
 		 */
-		if (cpu_idle_force_poll || tick_check_broadcast_expired())
+		if (cpu_idle_force_poll || tick_check_broadcast_expired()) {
+			tick_nohz_idle_restart_tick();
 			cpu_idle_poll();
-		else
+		} else {
+			tick_nohz_idle_stop_tick();
 			cpuidle_idle_call();
+		}
 		arch_cpu_idle_exit();
 	}
 

commit 0e7767687fdabfc58d5046e7488632bf2ecd4d0c
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Apr 5 18:58:27 2018 +0200

    time: tick-sched: Reorganize idle tick management code
    
    Prepare the scheduler tick code for reworking the idle loop to
    avoid stopping the tick in some cases.
    
    The idea is to split the nohz idle entry call to decouple the idle
    time stats accounting and preparatory work from the actual tick stop
    code, in order to later be able to delay the tick stop once we reach
    more power-knowledgeable callers.
    
    Move away the tick_nohz_start_idle() invocation from
    __tick_nohz_idle_enter(), rename the latter to
    __tick_nohz_idle_stop_tick() and define tick_nohz_idle_stop_tick()
    as a wrapper around it for calling it from the outside.
    
    Make tick_nohz_idle_enter() only call tick_nohz_start_idle() instead
    of calling the entire __tick_nohz_idle_enter(), add another wrapper
    disabling and enabling interrupts around tick_nohz_idle_stop_tick()
    and make the current callers of tick_nohz_idle_enter() call it too
    to retain their current functionality.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Frederic Weisbecker <frederic@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 2975f195e1c4..c0bc111878e6 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -216,6 +216,7 @@ static void do_idle(void)
 
 	__current_set_polling();
 	tick_nohz_idle_enter();
+	tick_nohz_idle_stop_tick_protected();
 
 	while (!need_resched()) {
 		check_pgt_cache();

commit a92057e14beb233e8c891f4de075f2a468c71f15
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 15:44:39 2018 +0100

    sched/idle: Merge kernel/sched/idle.c and kernel/sched/idle_task.c
    
    Merge these two small .c modules as they implement two aspects
    of idle task handling.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 2760e0357271..2975f195e1c4 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -1,5 +1,9 @@
 /*
- * Generic entry points for the idle threads
+ * Generic entry points for the idle threads and
+ * implementation of the idle task scheduling class.
+ *
+ * (NOTE: these are not related to SCHED_IDLE batch scheduled
+ *        tasks which are handled in sched/fair.c )
  */
 #include "sched.h"
 
@@ -33,6 +37,7 @@ void cpu_idle_poll_ctrl(bool enable)
 static int __init cpu_idle_poll_setup(char *__unused)
 {
 	cpu_idle_force_poll = 1;
+
 	return 1;
 }
 __setup("nohlt", cpu_idle_poll_setup);
@@ -40,6 +45,7 @@ __setup("nohlt", cpu_idle_poll_setup);
 static int __init cpu_idle_nopoll_setup(char *__unused)
 {
 	cpu_idle_force_poll = 0;
+
 	return 1;
 }
 __setup("hlt", cpu_idle_nopoll_setup);
@@ -51,12 +57,14 @@ static noinline int __cpuidle cpu_idle_poll(void)
 	trace_cpu_idle_rcuidle(0, smp_processor_id());
 	local_irq_enable();
 	stop_critical_timings();
+
 	while (!tif_need_resched() &&
 		(cpu_idle_force_poll || tick_check_broadcast_expired()))
 		cpu_relax();
 	start_critical_timings();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	rcu_idle_exit();
+
 	return 1;
 }
 
@@ -337,3 +345,116 @@ void cpu_startup_entry(enum cpuhp_state state)
 	while (1)
 		do_idle();
 }
+
+/*
+ * idle-task scheduling class.
+ */
+
+#ifdef CONFIG_SMP
+static int
+select_task_rq_idle(struct task_struct *p, int cpu, int sd_flag, int flags)
+{
+	return task_cpu(p); /* IDLE tasks as never migrated */
+}
+#endif
+
+/*
+ * Idle tasks are unconditionally rescheduled:
+ */
+static void check_preempt_curr_idle(struct rq *rq, struct task_struct *p, int flags)
+{
+	resched_curr(rq);
+}
+
+static struct task_struct *
+pick_next_task_idle(struct rq *rq, struct task_struct *prev, struct rq_flags *rf)
+{
+	put_prev_task(rq, prev);
+	update_idle_core(rq);
+	schedstat_inc(rq->sched_goidle);
+
+	return rq->idle;
+}
+
+/*
+ * It is not legal to sleep in the idle task - print a warning
+ * message if some code attempts to do it:
+ */
+static void
+dequeue_task_idle(struct rq *rq, struct task_struct *p, int flags)
+{
+	raw_spin_unlock_irq(&rq->lock);
+	printk(KERN_ERR "bad: scheduling from the idle thread!\n");
+	dump_stack();
+	raw_spin_lock_irq(&rq->lock);
+}
+
+static void put_prev_task_idle(struct rq *rq, struct task_struct *prev)
+{
+}
+
+/*
+ * scheduler tick hitting a task of our scheduling class.
+ *
+ * NOTE: This function can be called remotely by the tick offload that
+ * goes along full dynticks. Therefore no local assumption can be made
+ * and everything must be accessed through the @rq and @curr passed in
+ * parameters.
+ */
+static void task_tick_idle(struct rq *rq, struct task_struct *curr, int queued)
+{
+}
+
+static void set_curr_task_idle(struct rq *rq)
+{
+}
+
+static void switched_to_idle(struct rq *rq, struct task_struct *p)
+{
+	BUG();
+}
+
+static void
+prio_changed_idle(struct rq *rq, struct task_struct *p, int oldprio)
+{
+	BUG();
+}
+
+static unsigned int get_rr_interval_idle(struct rq *rq, struct task_struct *task)
+{
+	return 0;
+}
+
+static void update_curr_idle(struct rq *rq)
+{
+}
+
+/*
+ * Simple, special scheduling class for the per-CPU idle tasks:
+ */
+const struct sched_class idle_sched_class = {
+	/* .next is NULL */
+	/* no enqueue/yield_task for idle tasks */
+
+	/* dequeue is not valid, we print a debug message there: */
+	.dequeue_task		= dequeue_task_idle,
+
+	.check_preempt_curr	= check_preempt_curr_idle,
+
+	.pick_next_task		= pick_next_task_idle,
+	.put_prev_task		= put_prev_task_idle,
+
+#ifdef CONFIG_SMP
+	.select_task_rq		= select_task_rq_idle,
+	.set_cpus_allowed	= set_cpus_allowed_common,
+#endif
+
+	.set_curr_task          = set_curr_task_idle,
+	.task_tick		= task_tick_idle,
+
+	.get_rr_interval	= get_rr_interval_idle,
+
+	.prio_changed		= prio_changed_idle,
+	.switched_to		= switched_to_idle,
+	.update_curr		= update_curr_idle,
+};

commit 325ea10c0809406ce23f038602abbc454f3f761d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 12:20:47 2018 +0100

    sched/headers: Simplify and clean up header usage in the scheduler
    
    Do the following cleanups and simplifications:
    
     - sched/sched.h already includes <asm/paravirt.h>, so no need to
       include it in sched/core.c again.
    
     - order the <linux/sched/*.h> headers alphabetically
    
     - add all <linux/sched/*.h> headers to kernel/sched/sched.h
    
     - remove all unnecessary includes from the .c files that
       are already included in kernel/sched/sched.h.
    
    Finally, make all scheduler .c files use a single common header:
    
      #include "sched.h"
    
    ... which now contains a union of the relied upon headers.
    
    This makes the various .c files easier to read and easier to handle.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 343d25f85477..2760e0357271 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -1,23 +1,10 @@
 /*
  * Generic entry points for the idle threads
  */
-#include <linux/sched.h>
-#include <linux/sched/idle.h>
-#include <linux/cpu.h>
-#include <linux/cpuidle.h>
-#include <linux/cpuhotplug.h>
-#include <linux/tick.h>
-#include <linux/mm.h>
-#include <linux/stackprotector.h>
-#include <linux/suspend.h>
-#include <linux/livepatch.h>
-
-#include <asm/tlb.h>
+#include "sched.h"
 
 #include <trace/events/power.h>
 
-#include "sched.h"
-
 /* Linker adds these: start and end of __cpuidle functions */
 extern char __cpuidle_text_start[], __cpuidle_text_end[];
 

commit 97fb7a0a8944bd6d2c5634e1e0fa689a5c40bc22
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 14:01:12 2018 +0100

    sched: Clean up and harmonize the coding style of the scheduler code base
    
    A good number of small style inconsistencies have accumulated
    in the scheduler core, so do a pass over them to harmonize
    all these details:
    
     - fix speling in comments,
    
     - use curly braces for multi-line statements,
    
     - remove unnecessary parentheses from integer literals,
    
     - capitalize consistently,
    
     - remove stray newlines,
    
     - add comments where necessary,
    
     - remove invalid/unnecessary comments,
    
     - align structure definitions and other data types vertically,
    
     - add missing newlines for increased readability,
    
     - fix vertical tabulation where it's misaligned,
    
     - harmonize preprocessor conditional block labeling
       and vertical alignment,
    
     - remove line-breaks where they uglify the code,
    
     - add newline after local variable definitions,
    
    No change in functionality:
    
      md5:
         1191fa0a890cfa8132156d2959d7e9e2  built-in.o.before.asm
         1191fa0a890cfa8132156d2959d7e9e2  built-in.o.after.asm
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 7dae9eb8c042..343d25f85477 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -1,5 +1,5 @@
 /*
- * Generic entry point for the idle threads
+ * Generic entry points for the idle threads
  */
 #include <linux/sched.h>
 #include <linux/sched/idle.h>
@@ -332,8 +332,8 @@ void cpu_startup_entry(enum cpuhp_state state)
 {
 	/*
 	 * This #ifdef needs to die, but it's too late in the cycle to
-	 * make this generic (arm and sh have never invoked the canary
-	 * init for the non boot cpus!). Will be fixed in 3.11
+	 * make this generic (ARM and SH have never invoked the canary
+	 * init for the non boot CPUs!). Will be fixed in 3.11
 	 */
 #ifdef CONFIG_X86
 	/*

commit 54b933c6c954a8b7b0c2b40a1c4d3f7279d11e22
Author: Cheng Jian <cj.chengjian@huawei.com>
Date:   Wed Oct 25 19:28:27 2017 +0800

    sched/idle: Micro-optimize the idle loop
    
    Move the loop-invariant calculation of 'cpu' in do_idle() out of the loop body,
    because the current CPU is always constant.
    
    This improves the generated code both on x86-64 and ARM64:
    
    x86-64:
    
    Before patch (execution in loop):
            864:       0f ae e8                lfence
            867:       65 8b 05 c2 38 f1 7e    mov %gs:0x7ef138c2(%rip),%eax
            86e:       89 c0                   mov %eax,%eax
            870:       48 0f a3 05 68 19 08    bt  %rax,0x1081968(%rip)
            877:       01
    
    After patch (execution in loop):
            872:       0f ae e8                lfence
            875:       4c 0f a3 25 63 19 08    bt  %r12,0x1081963(%rip)
            87c:       01
    
    ARM64:
    
    Before patch (execution in loop):
            c58:       d5033d9f        dsb     ld
            c5c:       d538d080        mrs     x0, tpidr_el1
            c60:       b8606a61        ldr     w1, [x19,x0]
            c64:       1100fc20        add     w0, w1, #0x3f
            c68:       7100003f        cmp     w1, #0x0
            c6c:       1a81b000        csel    w0, w0, w1, lt
            c70:       13067c00        asr     w0, w0, #6
            c74:       93407c00        sxtw    x0, w0
            c78:       f8607a80        ldr     x0, [x20,x0,lsl #3]
            c7c:       9ac12401        lsr     x1, x0, x1
            c80:       36000581        tbz     w1, #0, d30 <do_idle+0x128>
    
    After patch (execution in loop):
            c84:       d5033d9f        dsb     ld
            c88:       f9400260        ldr     x0, [x19]
            c8c:       ea14001f        tst     x0, x20
            c90:       54000580        b.eq    d40 <do_idle+0x138>
    
    Signed-off-by: Cheng Jian <cj.chengjian@huawei.com>
    [ Rewrote the title and the changelog. ]
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: huawei.libin@huawei.com
    Cc: xiexiuqi@huawei.com
    Link: http://lkml.kernel.org/r/1508930907-107755-1-git-send-email-cj.chengjian@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index b2e8f0afbb42..7dae9eb8c042 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -209,6 +209,7 @@ static void cpuidle_idle_call(void)
  */
 static void do_idle(void)
 {
+	int cpu = smp_processor_id();
 	/*
 	 * If the arch has a polling bit, we maintain an invariant:
 	 *
@@ -225,7 +226,7 @@ static void do_idle(void)
 		check_pgt_cache();
 		rmb();
 
-		if (cpu_is_offline(smp_processor_id())) {
+		if (cpu_is_offline(cpu)) {
 			cpuhp_report_idle_dead();
 			arch_cpu_idle_dead();
 		}

commit 62cb1188ed86a9cf082fd2f757d4dd9b54741f24
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Aug 29 15:07:54 2017 +0200

    sched/idle: Move quiet_vmstate() into the NOHZ code
    
    quiet_vmstat() is an expensive function that only makes sense when we
    go into NOHZ.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: aubrey.li@linux.intel.com
    Cc: cl@linux.com
    Cc: fweisbec@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 257f4f0b4532..b2e8f0afbb42 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -219,7 +219,6 @@ static void do_idle(void)
 	 */
 
 	__current_set_polling();
-	quiet_vmstat();
 	tick_nohz_idle_enter();
 
 	while (!need_resched()) {

commit 28ba086ed30fb3fb714598aa029b894c3754fa7b
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Aug 10 00:14:45 2017 +0200

    PM / s2idle: Rename ->enter_freeze to ->enter_s2idle
    
    Rename the ->enter_freeze cpuidle driver callback to ->enter_s2idle
    to make it clear that it is used for entering suspend-to-idle and
    rename the related functions, variables and so on accordingly.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 2ea4b7b23044..257f4f0b4532 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -169,7 +169,7 @@ static void cpuidle_idle_call(void)
 
 	if (idle_should_enter_s2idle() || dev->use_deepest_state) {
 		if (idle_should_enter_s2idle()) {
-			entered_state = cpuidle_enter_freeze(drv, dev);
+			entered_state = cpuidle_enter_s2idle(drv, dev);
 			if (entered_state > 0) {
 				local_irq_enable();
 				goto exit_idle;

commit f02f4f9d826590f1ef1b087374d3e3cfb7949eac
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Aug 10 00:13:56 2017 +0200

    PM / s2idle: Rename freeze_state enum and related items
    
    Rename the freeze_state enum representing the suspend-to-idle state
    machine states to s2idle_states and rename the related variables and
    functions accordingly.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 6c23e30c0e5c..2ea4b7b23044 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -158,7 +158,7 @@ static void cpuidle_idle_call(void)
 	}
 
 	/*
-	 * Suspend-to-idle ("freeze") is a system state in which all user space
+	 * Suspend-to-idle ("s2idle") is a system state in which all user space
 	 * has been frozen, all I/O devices have been suspended and the only
 	 * activity happens here and in iterrupts (if any).  In that case bypass
 	 * the cpuidle governor and go stratight for the deepest idle state
@@ -167,8 +167,8 @@ static void cpuidle_idle_call(void)
 	 * until a proper wakeup interrupt happens.
 	 */
 
-	if (idle_should_freeze() || dev->use_deepest_state) {
-		if (idle_should_freeze()) {
+	if (idle_should_enter_s2idle() || dev->use_deepest_state) {
+		if (idle_should_enter_s2idle()) {
 			entered_state = cpuidle_enter_freeze(drv, dev);
 			if (entered_state > 0) {
 				local_irq_enable();

commit ebfa4c02fa4806bfef189e88152b833f2a732bff
Author: Aubrey Li <aubrey.li@intel.com>
Date:   Wed Jun 7 10:49:02 2017 +0800

    sched/idle: Add deferrable vmstat_updater back
    
    Deferrable vmstat_updater was missing in commit:
    
      c1de45ca831a ("sched/idle: Add support for tasks that inject idle")
    
    Add it back.
    
    Signed-off-by: Aubrey Li <aubrey.li@linux.intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Aubrey Li <aubrey.li@intel.com>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/1496803742-38274-1-git-send-email-aubrey.li@intel.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index ef63adce0c9c..6c23e30c0e5c 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -219,6 +219,7 @@ static void do_idle(void)
 	 */
 
 	__current_set_polling();
+	quiet_vmstat();
 	tick_nohz_idle_enter();
 
 	while (!need_resched()) {

commit 8663effb24f9430394d3bf1ed2dac42a771421d1
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Apr 14 08:48:09 2017 -0400

    sched/core: Call __schedule() from do_idle() without enabling preemption
    
    I finally got around to creating trampolines for dynamically allocated
    ftrace_ops with using synchronize_rcu_tasks(). For users of the ftrace
    function hook callbacks, like perf, that allocate the ftrace_ops
    descriptor via kmalloc() and friends, ftrace was not able to optimize
    the functions being traced to use a trampoline because they would also
    need to be allocated dynamically. The problem is that they cannot be
    freed when CONFIG_PREEMPT is set, as there's no way to tell if a task
    was preempted on the trampoline. That was before Paul McKenney
    implemented synchronize_rcu_tasks() that would make sure all tasks
    (except idle) have scheduled out or have entered user space.
    
    While testing this, I triggered this bug:
    
     BUG: unable to handle kernel paging request at ffffffffa0230077
     ...
     RIP: 0010:0xffffffffa0230077
     ...
     Call Trace:
      schedule+0x5/0xe0
      schedule_preempt_disabled+0x18/0x30
      do_idle+0x172/0x220
    
    What happened was that the idle task was preempted on the trampoline.
    As synchronize_rcu_tasks() ignores the idle thread, there's nothing
    that lets ftrace know that the idle task was preempted on a trampoline.
    
    The idle task shouldn't need to ever enable preemption. The idle task
    is simply a loop that calls schedule or places the cpu into idle mode.
    In fact, having preemption enabled is inefficient, because it can
    happen when idle is just about to call schedule anyway, which would
    cause schedule to be called twice. Once for when the interrupt came in
    and was returning back to normal context, and then again in the normal
    path that the idle loop is running in, which would be pointless, as it
    had already scheduled.
    
    The only reason schedule_preempt_disable() enables preemption is to be
    able to call sched_submit_work(), which requires preemption enabled. As
    this is a nop when the task is in the RUNNING state, and idle is always
    in the running state, there's no reason that idle needs to enable
    preemption. But that means it cannot use schedule_preempt_disable() as
    other callers of that function require calling sched_submit_work().
    
    Adding a new function local to kernel/sched/ that allows idle to call
    the scheduler without enabling preemption, fixes the
    synchronize_rcu_tasks() issue, as well as removes the pointless spurious
    schedule calls caused by interrupts happening in the brief window where
    preemption is enabled just before it calls schedule.
    
    Reviewed: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170414084809.3dacde2a@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 2a25a9ec2c6e..ef63adce0c9c 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -265,7 +265,7 @@ static void do_idle(void)
 	smp_mb__after_atomic();
 
 	sched_ttwu_pending();
-	schedule_preempt_disabled();
+	schedule_idle();
 
 	if (unlikely(klp_patch_pending(current)))
 		klp_update_patch_state(current);

commit d83a7cb375eec21f04c83542395d08b2f6641da2
Author: Josh Poimboeuf <jpoimboe@redhat.com>
Date:   Mon Feb 13 19:42:40 2017 -0600

    livepatch: change to a per-task consistency model
    
    Change livepatch to use a basic per-task consistency model.  This is the
    foundation which will eventually enable us to patch those ~10% of
    security patches which change function or data semantics.  This is the
    biggest remaining piece needed to make livepatch more generally useful.
    
    This code stems from the design proposal made by Vojtech [1] in November
    2014.  It's a hybrid of kGraft and kpatch: it uses kGraft's per-task
    consistency and syscall barrier switching combined with kpatch's stack
    trace switching.  There are also a number of fallback options which make
    it quite flexible.
    
    Patches are applied on a per-task basis, when the task is deemed safe to
    switch over.  When a patch is enabled, livepatch enters into a
    transition state where tasks are converging to the patched state.
    Usually this transition state can complete in a few seconds.  The same
    sequence occurs when a patch is disabled, except the tasks converge from
    the patched state to the unpatched state.
    
    An interrupt handler inherits the patched state of the task it
    interrupts.  The same is true for forked tasks: the child inherits the
    patched state of the parent.
    
    Livepatch uses several complementary approaches to determine when it's
    safe to patch tasks:
    
    1. The first and most effective approach is stack checking of sleeping
       tasks.  If no affected functions are on the stack of a given task,
       the task is patched.  In most cases this will patch most or all of
       the tasks on the first try.  Otherwise it'll keep trying
       periodically.  This option is only available if the architecture has
       reliable stacks (HAVE_RELIABLE_STACKTRACE).
    
    2. The second approach, if needed, is kernel exit switching.  A
       task is switched when it returns to user space from a system call, a
       user space IRQ, or a signal.  It's useful in the following cases:
    
       a) Patching I/O-bound user tasks which are sleeping on an affected
          function.  In this case you have to send SIGSTOP and SIGCONT to
          force it to exit the kernel and be patched.
       b) Patching CPU-bound user tasks.  If the task is highly CPU-bound
          then it will get patched the next time it gets interrupted by an
          IRQ.
       c) In the future it could be useful for applying patches for
          architectures which don't yet have HAVE_RELIABLE_STACKTRACE.  In
          this case you would have to signal most of the tasks on the
          system.  However this isn't supported yet because there's
          currently no way to patch kthreads without
          HAVE_RELIABLE_STACKTRACE.
    
    3. For idle "swapper" tasks, since they don't ever exit the kernel, they
       instead have a klp_update_patch_state() call in the idle loop which
       allows them to be patched before the CPU enters the idle state.
    
       (Note there's not yet such an approach for kthreads.)
    
    All the above approaches may be skipped by setting the 'immediate' flag
    in the 'klp_patch' struct, which will disable per-task consistency and
    patch all tasks immediately.  This can be useful if the patch doesn't
    change any function or data semantics.  Note that, even with this flag
    set, it's possible that some tasks may still be running with an old
    version of the function, until that function returns.
    
    There's also an 'immediate' flag in the 'klp_func' struct which allows
    you to specify that certain functions in the patch can be applied
    without per-task consistency.  This might be useful if you want to patch
    a common function like schedule(), and the function change doesn't need
    consistency but the rest of the patch does.
    
    For architectures which don't have HAVE_RELIABLE_STACKTRACE, the user
    must set patch->immediate which causes all tasks to be patched
    immediately.  This option should be used with care, only when the patch
    doesn't change any function or data semantics.
    
    In the future, architectures which don't have HAVE_RELIABLE_STACKTRACE
    may be allowed to use per-task consistency if we can come up with
    another way to patch kthreads.
    
    The /sys/kernel/livepatch/<patch>/transition file shows whether a patch
    is in transition.  Only a single patch (the topmost patch on the stack)
    can be in transition at a given time.  A patch can remain in transition
    indefinitely, if any of the tasks are stuck in the initial patch state.
    
    A transition can be reversed and effectively canceled by writing the
    opposite value to the /sys/kernel/livepatch/<patch>/enabled file while
    the transition is in progress.  Then all the tasks will attempt to
    converge back to the original patch state.
    
    [1] https://lkml.kernel.org/r/20141107140458.GA21774@suse.cz
    
    Signed-off-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Acked-by: Miroslav Benes <mbenes@suse.cz>
    Acked-by: Ingo Molnar <mingo@kernel.org>        # for the scheduler changes
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index ac6d5176463d..2a25a9ec2c6e 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -10,6 +10,7 @@
 #include <linux/mm.h>
 #include <linux/stackprotector.h>
 #include <linux/suspend.h>
+#include <linux/livepatch.h>
 
 #include <asm/tlb.h>
 
@@ -265,6 +266,9 @@ static void do_idle(void)
 
 	sched_ttwu_pending();
 	schedule_preempt_disabled();
+
+	if (unlikely(klp_patch_pending(current)))
+		klp_update_patch_state(current);
 }
 
 bool cpu_in_idle(unsigned long pc)

commit 4c822698cba8bdd93724117eded12bf34eb80252
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/idle.h>
    
    We are going to split  <linux/sched/idle.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/idle.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 6a4bae0a649d..ac6d5176463d 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -2,6 +2,7 @@
  * Generic entry point for the idle threads
  */
 #include <linux/sched.h>
+#include <linux/sched/idle.h>
 #include <linux/cpu.h>
 #include <linux/cpuidle.h>
 #include <linux/cpuhotplug.h>

commit c1de45ca831acee9b72c9320dde447edafadb43f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 28 23:03:05 2016 -0800

    sched/idle: Add support for tasks that inject idle
    
    Idle injection drivers such as Intel powerclamp and ACPI PAD drivers use
    realtime tasks to take control of CPU then inject idle. There are two
    issues with this approach:
    
     1. Low efficiency: injected idle task is treated as busy so sched ticks
        do not stop during injected idle period, the result of these
        unwanted wakeups can be ~20% loss in power savings.
    
     2. Idle accounting: injected idle time is presented to user as busy.
    
    This patch addresses the issues by introducing a new PF_IDLE flag which
    allows any given task to be treated as idle task while the flag is set.
    Therefore, idle injection tasks can run through the normal flow of NOHZ
    idle enter/exit to get the correct accounting as well as tick stop when
    possible.
    
    The implication is that idle task is then no longer limited to PID == 0.
    
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 513e4dfeeae7..6a4bae0a649d 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -205,76 +205,65 @@ static void cpuidle_idle_call(void)
  *
  * Called with polling cleared.
  */
-static void cpu_idle_loop(void)
+static void do_idle(void)
 {
-	int cpu = smp_processor_id();
-
-	while (1) {
-		/*
-		 * If the arch has a polling bit, we maintain an invariant:
-		 *
-		 * Our polling bit is clear if we're not scheduled (i.e. if
-		 * rq->curr != rq->idle).  This means that, if rq->idle has
-		 * the polling bit set, then setting need_resched is
-		 * guaranteed to cause the cpu to reschedule.
-		 */
-
-		__current_set_polling();
-		quiet_vmstat();
-		tick_nohz_idle_enter();
+	/*
+	 * If the arch has a polling bit, we maintain an invariant:
+	 *
+	 * Our polling bit is clear if we're not scheduled (i.e. if rq->curr !=
+	 * rq->idle). This means that, if rq->idle has the polling bit set,
+	 * then setting need_resched is guaranteed to cause the CPU to
+	 * reschedule.
+	 */
 
-		while (!need_resched()) {
-			check_pgt_cache();
-			rmb();
+	__current_set_polling();
+	tick_nohz_idle_enter();
 
-			if (cpu_is_offline(cpu)) {
-				cpuhp_report_idle_dead();
-				arch_cpu_idle_dead();
-			}
+	while (!need_resched()) {
+		check_pgt_cache();
+		rmb();
 
-			local_irq_disable();
-			arch_cpu_idle_enter();
-
-			/*
-			 * In poll mode we reenable interrupts and spin.
-			 *
-			 * Also if we detected in the wakeup from idle
-			 * path that the tick broadcast device expired
-			 * for us, we don't want to go deep idle as we
-			 * know that the IPI is going to arrive right
-			 * away
-			 */
-			if (cpu_idle_force_poll || tick_check_broadcast_expired())
-				cpu_idle_poll();
-			else
-				cpuidle_idle_call();
-
-			arch_cpu_idle_exit();
+		if (cpu_is_offline(smp_processor_id())) {
+			cpuhp_report_idle_dead();
+			arch_cpu_idle_dead();
 		}
 
-		/*
-		 * Since we fell out of the loop above, we know
-		 * TIF_NEED_RESCHED must be set, propagate it into
-		 * PREEMPT_NEED_RESCHED.
-		 *
-		 * This is required because for polling idle loops we will
-		 * not have had an IPI to fold the state for us.
-		 */
-		preempt_set_need_resched();
-		tick_nohz_idle_exit();
-		__current_clr_polling();
+		local_irq_disable();
+		arch_cpu_idle_enter();
 
 		/*
-		 * We promise to call sched_ttwu_pending and reschedule
-		 * if need_resched is set while polling is set.  That
-		 * means that clearing polling needs to be visible
-		 * before doing these things.
+		 * In poll mode we reenable interrupts and spin. Also if we
+		 * detected in the wakeup from idle path that the tick
+		 * broadcast device expired for us, we don't want to go deep
+		 * idle as we know that the IPI is going to arrive right away.
 		 */
-		smp_mb__after_atomic();
-
-		sched_ttwu_pending();
-		schedule_preempt_disabled();
+		if (cpu_idle_force_poll || tick_check_broadcast_expired())
+			cpu_idle_poll();
+		else
+			cpuidle_idle_call();
+		arch_cpu_idle_exit();
 	}
+
+	/*
+	 * Since we fell out of the loop above, we know TIF_NEED_RESCHED must
+	 * be set, propagate it into PREEMPT_NEED_RESCHED.
+	 *
+	 * This is required because for polling idle loops we will not have had
+	 * an IPI to fold the state for us.
+	 */
+	preempt_set_need_resched();
+	tick_nohz_idle_exit();
+	__current_clr_polling();
+
+	/*
+	 * We promise to call sched_ttwu_pending() and reschedule if
+	 * need_resched() is set while polling is set. That means that clearing
+	 * polling needs to be visible before doing these things.
+	 */
+	smp_mb__after_atomic();
+
+	sched_ttwu_pending();
+	schedule_preempt_disabled();
 }
 
 bool cpu_in_idle(unsigned long pc)
@@ -283,6 +272,56 @@ bool cpu_in_idle(unsigned long pc)
 		pc < (unsigned long)__cpuidle_text_end;
 }
 
+struct idle_timer {
+	struct hrtimer timer;
+	int done;
+};
+
+static enum hrtimer_restart idle_inject_timer_fn(struct hrtimer *timer)
+{
+	struct idle_timer *it = container_of(timer, struct idle_timer, timer);
+
+	WRITE_ONCE(it->done, 1);
+	set_tsk_need_resched(current);
+
+	return HRTIMER_NORESTART;
+}
+
+void play_idle(unsigned long duration_ms)
+{
+	struct idle_timer it;
+
+	/*
+	 * Only FIFO tasks can disable the tick since they don't need the forced
+	 * preemption.
+	 */
+	WARN_ON_ONCE(current->policy != SCHED_FIFO);
+	WARN_ON_ONCE(current->nr_cpus_allowed != 1);
+	WARN_ON_ONCE(!(current->flags & PF_KTHREAD));
+	WARN_ON_ONCE(!(current->flags & PF_NO_SETAFFINITY));
+	WARN_ON_ONCE(!duration_ms);
+
+	rcu_sleep_check();
+	preempt_disable();
+	current->flags |= PF_IDLE;
+	cpuidle_use_deepest_state(true);
+
+	it.done = 0;
+	hrtimer_init_on_stack(&it.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	it.timer.function = idle_inject_timer_fn;
+	hrtimer_start(&it.timer, ms_to_ktime(duration_ms), HRTIMER_MODE_REL_PINNED);
+
+	while (!READ_ONCE(it.done))
+		do_idle();
+
+	cpuidle_use_deepest_state(false);
+	current->flags &= ~PF_IDLE;
+
+	preempt_fold_need_resched();
+	preempt_enable();
+}
+EXPORT_SYMBOL_GPL(play_idle);
+
 void cpu_startup_entry(enum cpuhp_state state)
 {
 	/*
@@ -302,5 +341,6 @@ void cpu_startup_entry(enum cpuhp_state state)
 #endif
 	arch_cpu_idle_prepare();
 	cpuhp_online_idle(state);
-	cpu_idle_loop();
+	while (1)
+		do_idle();
 }

commit bb8313b603eb8fd52de48a079bfcd72dcab2ef1e
Author: Jacob Pan <jacob.jun.pan@linux.intel.com>
Date:   Mon Nov 28 23:03:04 2016 -0800

    cpuidle: Allow enforcing deepest idle state selection
    
    When idle injection is used to cap power, we need to override the
    governor's choice of idle states.
    
    For this reason, make it possible the deepest idle state selection to
    be enforced by setting a flag on a given CPU to achieve the maximum
    potential power draw reduction.
    
    Signed-off-by: Jacob Pan <jacob.jun.pan@linux.intel.com>
    [ rjw: Subject & changelog ]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 1d8718d5300d..513e4dfeeae7 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -164,11 +164,14 @@ static void cpuidle_idle_call(void)
 	 * timekeeping to prevent timer interrupts from kicking us out of idle
 	 * until a proper wakeup interrupt happens.
 	 */
-	if (idle_should_freeze()) {
-		entered_state = cpuidle_enter_freeze(drv, dev);
-		if (entered_state > 0) {
-			local_irq_enable();
-			goto exit_idle;
+
+	if (idle_should_freeze() || dev->use_deepest_state) {
+		if (idle_should_freeze()) {
+			entered_state = cpuidle_enter_freeze(drv, dev);
+			if (entered_state > 0) {
+				local_irq_enable();
+				goto exit_idle;
+			}
 		}
 
 		next_state = cpuidle_find_deepest_state(drv, dev);

commit 6727ad9e206cc08b80d8000a4d67f8417e53539d
Author: Chris Metcalf <cmetcalf@mellanox.com>
Date:   Fri Oct 7 17:02:55 2016 -0700

    nmi_backtrace: generate one-line reports for idle cpus
    
    When doing an nmi backtrace of many cores, most of which are idle, the
    output is a little overwhelming and very uninformative.  Suppress
    messages for cpus that are idling when they are interrupted and just
    emit one line, "NMI backtrace for N skipped: idling at pc 0xNNN".
    
    We do this by grouping all the cpuidle code together into a new
    .cpuidle.text section, and then checking the address of the interrupted
    PC to see if it lies within that section.
    
    This commit suitably tags x86 and tile idle routines, and only adds in
    the minimal framework for other architectures.
    
    Link: http://lkml.kernel.org/r/1472487169-14923-5-git-send-email-cmetcalf@mellanox.com
    Signed-off-by: Chris Metcalf <cmetcalf@mellanox.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Tested-by: Daniel Thompson <daniel.thompson@linaro.org> [arm]
    Tested-by: Petr Mladek <pmladek@suse.com>
    Cc: Aaron Tomlin <atomlin@redhat.com>
    Cc: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: Russell King <linux@arm.linux.org.uk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 9fb873cfc75c..1d8718d5300d 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -16,6 +16,9 @@
 
 #include "sched.h"
 
+/* Linker adds these: start and end of __cpuidle functions */
+extern char __cpuidle_text_start[], __cpuidle_text_end[];
+
 /**
  * sched_idle_set_state - Record idle state for the current CPU.
  * @idle_state: State to record.
@@ -53,7 +56,7 @@ static int __init cpu_idle_nopoll_setup(char *__unused)
 __setup("hlt", cpu_idle_nopoll_setup);
 #endif
 
-static inline int cpu_idle_poll(void)
+static noinline int __cpuidle cpu_idle_poll(void)
 {
 	rcu_idle_enter();
 	trace_cpu_idle_rcuidle(0, smp_processor_id());
@@ -84,7 +87,7 @@ void __weak arch_cpu_idle(void)
  *
  * To use when the cpuidle framework cannot be used.
  */
-void default_idle_call(void)
+void __cpuidle default_idle_call(void)
 {
 	if (current_clr_polling_and_test()) {
 		local_irq_enable();
@@ -271,6 +274,12 @@ static void cpu_idle_loop(void)
 	}
 }
 
+bool cpu_in_idle(unsigned long pc)
+{
+	return pc >= (unsigned long)__cpuidle_text_start &&
+		pc < (unsigned long)__cpuidle_text_end;
+}
+
 void cpu_startup_entry(enum cpuhp_state state)
 {
 	/*

commit 07f9f22087a94e8162f77ee997c52a23f158aee8
Merge: 03c041c5bf6e b7fa30c9cc48
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jun 14 11:04:13 2016 +0200

    Merge branch 'sched/urgent' into sched/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit df55f462b905f3b2d40ec3fb865891382a6ebfb1
Author: Gaurav Jindal (Gaurav Jindal) <Gaurav.Jindal@spreadtrum.com>
Date:   Thu May 12 10:13:33 2016 +0000

    sched/idle: Optimize the generic idle loop
    
    Currently, smp_processor_id() is used to fetch the current CPU in
    cpu_idle_loop(). Every time the idle thread runs, it fetches the
    current CPU using smp_processor_id().
    
    Since the idle thread is per CPU, the current CPU is constant, so we
    can lift the load out of the loop, saving execution cycles/time in the
    loop.
    
    x86-64:
    
    Before patch (execution in loop):
            148:    0f ae e8                lfence
            14b:    65 8b 04 25 00 00 00 00 mov %gs:0x0,%eax
            152:    00
            153:    89 c0                   mov %eax,%eax
            155:    49 0f a3 04 24          bt %rax,(%r12)
    
    After patch (execution in loop):
            150:    0f ae e8                lfence
            153:    4d 0f a3 34 24          bt %r14,(%r12)
    
    ARM64:
    
    Before patch (execution in loop):
            168:    d5033d9f        dsb     ld
            16c:    b9405661        ldr     w1,[x19,#84]
            170:    1100fc20        add     w0,w1,#0x3f
            174:    6b1f003f        cmp     w1,wzr
            178:    1a81b000        csel    w0,w0,w1,lt
            17c:    130c7000        asr     w0,w0,#6
            180:    937d7c00        sbfiz   x0,x0,#3,#32
            184:    f8606aa0        ldr     x0,[x21,x0]
            188:    9ac12401        lsr     x1,x0,x1
            18c:    36000e61        tbz     w1,#0,358
    
    After patch (execution in loop):
            1a8:    d50339df        dsb     ld
            1ac:    f8776ac0        ldr     x0,[x22,x23]
            ab0:    ea18001f        tst     x0,x24
            1b4:    54000ea0        b.eq    388
    
    Further observance on ARM64 for 4 seconds shows that cpu_idle_loop is
    called 8672 times. Shifting the code will save instructions executed
    in loop and eventually time as well.
    
    Signed-off-by: Gaurav Jindal <gaurav.jindal@spreadtrum.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Sanjeev Yadav <sanjeev.yadav@spreadtrum.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20160512101330.GA488@gauravjindalubtnb.del.spreadtrum.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index bd12c6c714ec..db4ff7c100b9 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -201,6 +201,8 @@ static void cpuidle_idle_call(void)
  */
 static void cpu_idle_loop(void)
 {
+	int cpu = smp_processor_id();
+
 	while (1) {
 		/*
 		 * If the arch has a polling bit, we maintain an invariant:
@@ -219,7 +221,7 @@ static void cpu_idle_loop(void)
 			check_pgt_cache();
 			rmb();
 
-			if (cpu_is_offline(smp_processor_id())) {
+			if (cpu_is_offline(cpu)) {
 				cpuhp_report_idle_dead();
 				arch_cpu_idle_dead();
 			}

commit 9bd616e3dbedfc103f158197c8ad93678849b1ed
Author: Catalin Marinas <catalin.marinas@arm.com>
Date:   Wed Jun 1 18:52:16 2016 +0100

    cpuidle: Do not access cpuidle_devices when !CONFIG_CPU_IDLE
    
    The cpuidle_devices per-CPU variable is only defined when CPU_IDLE is
    enabled. Commit c8cc7d4de7a4 ("sched/idle: Reorganize the idle loop")
    removed the #ifdef CONFIG_CPU_IDLE around cpuidle_idle_call() with the
    compiler optimising away __this_cpu_read(cpuidle_devices). However, with
    CONFIG_UBSAN && !CONFIG_CPU_IDLE, this optimisation no longer happens
    and the kernel fails to link since cpuidle_devices is not defined.
    
    This patch introduces an accessor function for the current CPU cpuidle
    device (returning NULL when !CONFIG_CPU_IDLE) and uses it in
    cpuidle_idle_call().
    
    Signed-off-by: Catalin Marinas <catalin.marinas@arm.com>
    Cc: 4.5+ <stable@vger.kernel.org> # 4.5+
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index bd12c6c714ec..c5aeedf4e93a 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -127,7 +127,7 @@ static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
  */
 static void cpuidle_idle_call(void)
 {
-	struct cpuidle_device *dev = __this_cpu_read(cpuidle_devices);
+	struct cpuidle_device *dev = cpuidle_get_device();
 	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
 	int next_state, entered_state;
 

commit 27d50c7eeb0f03c3d3ca72aac4d2dd487ca1f3f0
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:44 2016 +0000

    rcu: Make CPU_DYING_IDLE an explicit call
    
    Make the RCU CPU_DYING_IDLE callback an explicit function call, so it gets
    invoked at the proper place.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.870167933@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 8abbe89e9114..bd12c6c714ec 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -220,8 +220,6 @@ static void cpu_idle_loop(void)
 			rmb();
 
 			if (cpu_is_offline(smp_processor_id())) {
-				rcu_cpu_notify(NULL, CPU_DYING_IDLE,
-					       (void *)(long)smp_processor_id());
 				cpuhp_report_idle_dead();
 				arch_cpu_idle_dead();
 			}

commit e69aab13117efc1987620090e539b4ebeb33a04c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:43 2016 +0000

    cpu/hotplug: Make wait for dead cpu completion based
    
    Kill the busy spinning on the control side and just wait for the hotplugged
    cpu to tell that it reached the dead state.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.776157858@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index a4b9813afc96..8abbe89e9114 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -194,8 +194,6 @@ static void cpuidle_idle_call(void)
 	rcu_idle_exit();
 }
 
-DEFINE_PER_CPU(bool, cpu_dead_idle);
-
 /*
  * Generic idle loop implementation
  *
@@ -224,8 +222,7 @@ static void cpu_idle_loop(void)
 			if (cpu_is_offline(smp_processor_id())) {
 				rcu_cpu_notify(NULL, CPU_DYING_IDLE,
 					       (void *)(long)smp_processor_id());
-				smp_mb(); /* all activity before dead. */
-				this_cpu_write(cpu_dead_idle, true);
+				cpuhp_report_idle_dead();
 				arch_cpu_idle_dead();
 			}
 

commit 8df3e07e7f21f2ed8d001e6fabf9505946b438aa
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Feb 26 18:43:41 2016 +0000

    cpu/hotplug: Let upcoming cpu bring itself fully up
    
    Let the upcoming cpu kick the hotplug thread and let itself complete the
    bringup. That way the controll side can just wait for the completion or later
    when we made the hotplug machinery async not care at all.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-arch@vger.kernel.org
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rafael Wysocki <rafael.j.wysocki@intel.com>
    Cc: "Srivatsa S. Bhat" <srivatsa@mit.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arjan van de Ven <arjan@linux.intel.com>
    Cc: Sebastian Siewior <bigeasy@linutronix.de>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tejun Heo <tj@kernel.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Paul Turner <pjt@google.com>
    Link: http://lkml.kernel.org/r/20160226182341.697655464@linutronix.de
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 544a7133cbd1..a4b9813afc96 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -4,6 +4,7 @@
 #include <linux/sched.h>
 #include <linux/cpu.h>
 #include <linux/cpuidle.h>
+#include <linux/cpuhotplug.h>
 #include <linux/tick.h>
 #include <linux/mm.h>
 #include <linux/stackprotector.h>
@@ -291,5 +292,6 @@ void cpu_startup_entry(enum cpuhp_state state)
 	boot_init_stack_canary();
 #endif
 	arch_cpu_idle_prepare();
+	cpuhp_online_idle(state);
 	cpu_idle_loop();
 }

commit ad1ac94767aa9d74c6533e33e768a14d2715162f
Merge: 0bce39ccb2ef 75274b33e779 b331bc20d928 a3d09c73492e 993e9fe1e70a
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Jan 29 21:45:17 2016 +0100

    Merge branches 'pm-cpuidle', 'pm-cpufreq', 'pm-domains' and 'pm-sleep'
    
    * pm-cpuidle:
      cpuidle: coupled: remove unused define cpuidle_coupled_lock
      cpuidle: fix fallback mechanism for suspend to idle in absence of enter_freeze
    
    * pm-cpufreq:
      cpufreq: cpufreq-dt: avoid uninitialized variable warnings:
      cpufreq: pxa2xx: fix pxa_cpufreq_change_voltage prototype
      cpufreq: Use list_is_last() to check last entry of the policy list
      cpufreq: Fix NULL reference crash while accessing policy->governor_data
    
    * pm-domains:
      PM / Domains: Fix typo in comment
      PM / Domains: Fix potential deadlock while adding/removing subdomains
      PM / domains: fix lockdep issue for all subdomains
    
    * pm-sleep:
      PM: APM_EMULATION does not depend on PM

commit 6f16886b7c050c934305b1f285c3458ff1b6e4e4
Author: Sudeep Holla <Sudeep.Holla@arm.com>
Date:   Thu Jan 21 11:19:29 2016 +0000

    cpuidle: fix fallback mechanism for suspend to idle in absence of enter_freeze
    
    Commit 51164251f5c3 "sched / idle: Drop default_idle_call() fallback
    from call_cpuidle()" made find_deepest_state() return non-negative
    value and check all the states with index > 0.  Also as a result,
    find_deepest_state() returns 0 even when enter_freeze callbacks are not
    implemented and enter_freeze_proper() is called which ends up crashing
    the kernel.
    
    This patch updates the check for index > 0 in cpuidle_enter_freeze and
    cpuidle_idle_call(when idle_should_freeze is true) to restore the
    suspend-to-idle functionality in absence of enter_freeze callback.
    
    Fixes: 51164251f5c3 "sched / idle: Drop default_idle_call() fallback from call_cpuidle()"
    Signed-off-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 34852ee70d54..30f7b6ad3920 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -162,7 +162,7 @@ static void cpuidle_idle_call(void)
 	 */
 	if (idle_should_freeze()) {
 		entered_state = cpuidle_enter_freeze(drv, dev);
-		if (entered_state >= 0) {
+		if (entered_state > 0) {
 			local_irq_enable();
 			goto exit_idle;
 		}

commit 30f05309bde49295e02e45c7e615f73aa4e0ccc2
Merge: 3549d8227937 db2b52f75250
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jan 20 19:06:49 2016 -0800

    Merge tag 'pm+acpi-4.5-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull more power management and ACPI updates from Rafael Wysocki:
     "This includes fixes on top of the previous batch of PM+ACPI updates
      and some new material as well.
    
      From the new material perspective the most significant are the driver
      core changes that should allow USB devices to stay suspended over
      system suspend/resume cycles if they have been runtime-suspended
      already beforehand.  Apart from that, ACPICA is updated to upstream
      revision 20160108 (cosmetic mostly, but including one fixup on top of
      the previous ACPICA update) and there are some devfreq updates the
      didn't make it before (due to timing).
    
      A few recent regressions are fixed, most importantly in the cpuidle
      menu governor and in the ACPI backlight driver and some x86 platform
      drivers depending on it.
    
      Some more bugs are fixed and cleanups are made on top of that.
    
      Specifics:
    
       - Modify the driver core and the USB subsystem to allow USB devices
         to stay suspended over system suspend/resume cycles if they have
         been runtime-suspended already beforehand and fix some bugs on top
         of these changes (Tomeu Vizoso, Rafael Wysocki).
    
       - Update ACPICA to upstream revision 20160108, including updates of
         the ACPICA's copyright notices, a code fixup resulting from a
         regression fix that was necessary in the upstream code only (the
         regression fixed by it has never been present in Linux) and a
         compiler warning fix (Bob Moore, Lv Zheng).
    
       - Fix a recent regression in the cpuidle menu governor that broke it
         on practically all architectures other than x86 and make a couple
         of optimizations on top of that fix (Rafael Wysocki).
    
       - Clean up the selection of cpuidle governors depending on whether or
         not the kernel is configured for tickless systems (Jean Delvare).
    
       - Revert a recent commit that introduced a regression in the ACPI
         backlight driver, address the problem it attempted to fix in a
         different way and revert one more cosmetic change depending on the
         problematic commit (Hans de Goede).
    
       - Add two more ACPI backlight quirks (Hans de Goede).
    
       - Fix a few minor problems in the core devfreq code, clean it up a
         bit and update the MAINTAINERS information related to it (Chanwoo
         Choi, MyungJoo Ham).
    
       - Improve an error message in the ACPI fan driver (Andy Lutomirski).
    
       - Fix a recent build regression in the cpupower tool (Shreyas
         Prabhu)"
    
    * tag 'pm+acpi-4.5-rc1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (32 commits)
      cpuidle: menu: Avoid pointless checks in menu_select()
      sched / idle: Drop default_idle_call() fallback from call_cpuidle()
      cpupower: Fix build error in cpufreq-info
      cpuidle: Don't enable all governors by default
      cpuidle: Default to ladder governor on ticking systems
      time: nohz: Expose tick_nohz_enabled
      ACPICA: Update version to 20160108
      ACPICA: Silence a -Wbad-function-cast warning when acpi_uintptr_t is 'uintptr_t'
      ACPICA: Additional 2016 copyright changes
      ACPICA: Reduce regression fix divergence from upstream ACPICA
      ACPI / video: Add disable_backlight_sysfs_if quirk for the Toshiba Satellite R830
      ACPI / video: Revert "thinkpad_acpi: Use acpi_video_handles_brightness_key_presses()"
      ACPI / video: Document acpi_video_handles_brightness_key_presses() a bit
      ACPI / video: Fix using an uninitialized mutex / list_head in acpi_video_handles_brightness_key_presses()
      ACPI / video: Revert "ACPI / video: driver must be registered before checking for keypresses"
      ACPI / fan: Improve acpi_device_update_power error message
      ACPI / video: Add disable_backlight_sysfs_if quirk for the Toshiba Portege R700
      cpuidle: menu: Fix menu_select() for CPUIDLE_DRIVER_STATE_START == 0
      MAINTAINERS: Add devfreq-event entry
      MAINTAINERS: Add missing git repository and directory for devfreq
      ...

commit 51164251f5c35e6596130ef0de94ffe65fe441e0
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sat Jan 16 00:54:53 2016 +0100

    sched / idle: Drop default_idle_call() fallback from call_cpuidle()
    
    After commit 9c4b2867ed7c (cpuidle: menu: Fix menu_select() for
    CPUIDLE_DRIVER_STATE_START == 0) it is clear that menu_select()
    cannot return negative values.  Moreover, ladder_select_state()
    will never return a negative value too, so make find_deepest_state()
    return non-negative values too and drop the default_idle_call()
    fallback from call_cpuidle().
    
    This eliminates one branch from the idle loop and makes the governors
    and find_deepest_state() handle the case when all states have been
    disabled from sysfs consistently.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Ingo Molnar <mingo@kernel.org>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 4a2ef5a02fd3..34852ee70d54 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -97,12 +97,6 @@ void default_idle_call(void)
 static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		      int next_state)
 {
-	/* Fall back to the default arch idle method on errors. */
-	if (next_state < 0) {
-		default_idle_call();
-		return next_state;
-	}
-
 	/*
 	 * The idle task must be scheduled, it is pointless to go to idle, just
 	 * update no idle residency and return.

commit 0eb77e9880321915322d42913c3b53241739c8aa
Author: Christoph Lameter <cl@linux.com>
Date:   Thu Jan 14 15:21:40 2016 -0800

    vmstat: make vmstat_updater deferrable again and shut down on idle
    
    Currently the vmstat updater is not deferrable as a result of commit
    ba4877b9ca51 ("vmstat: do not use deferrable delayed work for
    vmstat_update").  This in turn can cause multiple interruptions of the
    applications because the vmstat updater may run at
    
    Make vmstate_update deferrable again and provide a function that folds
    the differentials when the processor is going to idle mode thus
    addressing the issue of the above commit in a clean way.
    
    Note that the shepherd thread will continue scanning the differentials
    from another processor and will reenable the vmstat workers if it
    detects any changes.
    
    Fixes: ba4877b9ca51 ("vmstat: do not use deferrable delayed work for vmstat_update")
    Signed-off-by: Christoph Lameter <cl@linux.com>
    Cc: Michal Hocko <mhocko@suse.cz>
    Cc: Johannes Weiner <hannes@cmpxchg.org>
    Cc: Tetsuo Handa <penguin-kernel@i-love.sakura.ne.jp>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 4a2ef5a02fd3..2489140a7c51 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -219,6 +219,7 @@ static void cpu_idle_loop(void)
 		 */
 
 		__current_set_polling();
+		quiet_vmstat();
 		tick_nohz_idle_enter();
 
 		while (!need_resched()) {

commit 9babcd7929bc8967ae3bb6093f603b93c2f9958f
Author: Daniel Bristot de Oliveira <bristot@redhat.com>
Date:   Thu Oct 8 15:36:06 2015 -0300

    sched, tracing: Stop/start critical timings around the idle=poll idle loop
    
    When using idle=poll, the preemptoff tracer is always showing
    the idle task as the culprit for long latencies. That happens
    because critical timings are not stopped before idle loop. This
    patch stops critical timings before entering the idle loop,
    starting it again after the idle loop.
    
    This problem does not affect the irqsoff tracer because
    interruptions are enabled before entering the idle loop.
    
    Signed-off-by: Daniel Bristot de Oliveira <bristot@redhat.com>
    Reviewed-by: Luis Claudio R. Goncalves <lgoncalv@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/10fc3705874aef11dbe152a068b591a7be1899b4.1444314899.git.bristot@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 8f177c73ae19..4a2ef5a02fd3 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -57,9 +57,11 @@ static inline int cpu_idle_poll(void)
 	rcu_idle_enter();
 	trace_cpu_idle_rcuidle(0, smp_processor_id());
 	local_irq_enable();
+	stop_critical_timings();
 	while (!tif_need_resched() &&
 		(cpu_idle_force_poll || tick_check_broadcast_expired()))
 		cpu_relax();
+	start_critical_timings();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	rcu_idle_exit();
 	return 1;

commit 63caae8480921773b46adec0b6ddac9a844a042f
Author: Lucas Stach <l.stach@pengutronix.de>
Date:   Mon Jul 20 18:34:50 2015 +0200

    sched/idle: Move latency tracing stop/start calls deeper inside the idle loop
    
    Make sure to stop tracing only once we are past a point where
    all latency tracing events have been processed (irqs are not
    enabled again). This has the slight advantage of capturing more
    latency related events in the idle path, but most importantly it
    makes sure that latency tracing doesn't get re-enabled
    inadvertently when new events are coming in.
    
    This makes the irqsoff latency tracer useful again, as we stop
    capturing CPU sleep time as IRQ latency.
    
    Signed-off-by: Lucas Stach <l.stach@pengutronix.de>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Rafael J. Wysocki <rjw@rjwysocki.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: kernel@pengutronix.de
    Cc: patchwork-lst@pengutronix.de
    Link: http://lkml.kernel.org/r/1437410090-3747-1-git-send-email-l.stach@pengutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 594275ed2620..8f177c73ae19 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -83,10 +83,13 @@ void __weak arch_cpu_idle(void)
  */
 void default_idle_call(void)
 {
-	if (current_clr_polling_and_test())
+	if (current_clr_polling_and_test()) {
 		local_irq_enable();
-	else
+	} else {
+		stop_critical_timings();
 		arch_cpu_idle();
+		start_critical_timings();
+	}
 }
 
 static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
@@ -140,12 +143,6 @@ static void cpuidle_idle_call(void)
 		return;
 	}
 
-	/*
-	 * During the idle period, stop measuring the disabled irqs
-	 * critical sections latencies
-	 */
-	stop_critical_timings();
-
 	/*
 	 * Tell the RCU framework we are entering an idle section,
 	 * so no more rcu read side critical sections and one more
@@ -198,7 +195,6 @@ static void cpuidle_idle_call(void)
 		local_irq_enable();
 
 	rcu_idle_exit();
-	start_critical_timings();
 }
 
 DEFINE_PER_CPU(bool, cpu_dead_idle);

commit 827a5aefc542b8fb17c00de06118e5cd0e3800f2
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sun May 10 01:18:46 2015 +0200

    sched / idle: Call default_idle_call() from cpuidle_enter_state()
    
    The check of the cpuidle_enter() return value against -EBUSY
    made in call_cpuidle() will not be necessary any more if
    cpuidle_enter_state() calls default_idle_call() directly when it
    is about to return -EBUSY, so make that happen and eliminate the
    check.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Tested-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 5d9f549fffa8..594275ed2620 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -76,12 +76,13 @@ void __weak arch_cpu_idle(void)
 	local_irq_enable();
 }
 
-static void default_idle_call(void)
+/**
+ * default_idle_call - Default CPU idle routine.
+ *
+ * To use when the cpuidle framework cannot be used.
+ */
+void default_idle_call(void)
 {
-	/*
-	 * We can't use the cpuidle framework, let's use the default idle
-	 * routine.
-	 */
 	if (current_clr_polling_and_test())
 		local_irq_enable();
 	else
@@ -91,8 +92,6 @@ static void default_idle_call(void)
 static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		      int next_state)
 {
-	int entered_state;
-
 	/* Fall back to the default arch idle method on errors. */
 	if (next_state < 0) {
 		default_idle_call();
@@ -114,12 +113,7 @@ static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 * This function will block until an interrupt occurs and will take
 	 * care of re-enabling the local interrupts
 	 */
-	entered_state = cpuidle_enter(drv, dev, next_state);
-
-	if (entered_state == -EBUSY)
-		default_idle_call();
-
-	return entered_state;
+	return cpuidle_enter(drv, dev, next_state);
 }
 
 /**

commit faad38492814112e3e7ce94d90123bbe301fff33
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Sun May 10 01:18:03 2015 +0200

    sched / idle: Call idle_set_state() from cpuidle_enter_state()
    
    Introduce a wrapper function around idle_set_state() called
    sched_idle_set_state() that will pass this_rq() to it as the
    first argument and make cpuidle_enter_state() call the new
    function before and after entering the target state.
    
    At the same time, remove direct invocations of idle_set_state()
    from call_cpuidle().
    
    This will allow the invocation of default_idle_call() to be
    moved from call_cpuidle() to cpuidle_enter_state() safely
    and call_cpuidle() to be simplified a bit as a result.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Tested-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Acked-by: Kevin Hilman <khilman@linaro.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 9c919b42f846..5d9f549fffa8 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -15,6 +15,15 @@
 
 #include "sched.h"
 
+/**
+ * sched_idle_set_state - Record idle state for the current CPU.
+ * @idle_state: State to record.
+ */
+void sched_idle_set_state(struct cpuidle_state *idle_state)
+{
+	idle_set_state(this_rq(), idle_state);
+}
+
 static int __read_mostly cpu_idle_force_poll;
 
 void cpu_idle_poll_ctrl(bool enable)
@@ -100,9 +109,6 @@ static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 		return -EBUSY;
 	}
 
-	/* Take note of the planned idle state. */
-	idle_set_state(this_rq(), &drv->states[next_state]);
-
 	/*
 	 * Enter the idle state previously returned by the governor decision.
 	 * This function will block until an interrupt occurs and will take
@@ -110,9 +116,6 @@ static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
 	 */
 	entered_state = cpuidle_enter(drv, dev, next_state);
 
-	/* The cpu is no longer idle or about to enter idle. */
-	idle_set_state(this_rq(), NULL);
-
 	if (entered_state == -EBUSY)
 		default_idle_call();
 

commit bcf6ad8a4a3d002e8bc8f6639cdc119168f4e87b
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon May 4 22:53:35 2015 +0200

    sched / idle: Eliminate the "reflect" check from cpuidle_idle_call()
    
    Since cpuidle_reflect() should only be called if the idle state
    to enter was selected by cpuidle_select(), there is the "reflect"
    variable in cpuidle_idle_call() whose value is used to determine
    whether or not that is the case.
    
    However, if the entire code run between the conditional setting
    "reflect" and the call to cpuidle_reflect() is moved to a separate
    function, it will be possible to call that new function in both
    branches of the conditional, in which case cpuidle_reflect() will
    only need to be called from one of them too and the "reflect"
    variable won't be necessary any more.
    
    This eliminates one check made by cpuidle_idle_call() on the majority
    of its invocations, so change the code as described.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index ae7c0be90d16..9c919b42f846 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -79,6 +79,46 @@ static void default_idle_call(void)
 		arch_cpu_idle();
 }
 
+static int call_cpuidle(struct cpuidle_driver *drv, struct cpuidle_device *dev,
+		      int next_state)
+{
+	int entered_state;
+
+	/* Fall back to the default arch idle method on errors. */
+	if (next_state < 0) {
+		default_idle_call();
+		return next_state;
+	}
+
+	/*
+	 * The idle task must be scheduled, it is pointless to go to idle, just
+	 * update no idle residency and return.
+	 */
+	if (current_clr_polling_and_test()) {
+		dev->last_residency = 0;
+		local_irq_enable();
+		return -EBUSY;
+	}
+
+	/* Take note of the planned idle state. */
+	idle_set_state(this_rq(), &drv->states[next_state]);
+
+	/*
+	 * Enter the idle state previously returned by the governor decision.
+	 * This function will block until an interrupt occurs and will take
+	 * care of re-enabling the local interrupts
+	 */
+	entered_state = cpuidle_enter(drv, dev, next_state);
+
+	/* The cpu is no longer idle or about to enter idle. */
+	idle_set_state(this_rq(), NULL);
+
+	if (entered_state == -EBUSY)
+		default_idle_call();
+
+	return entered_state;
+}
+
 /**
  * cpuidle_idle_call - the main idle function
  *
@@ -93,7 +133,6 @@ static void cpuidle_idle_call(void)
 	struct cpuidle_device *dev = __this_cpu_read(cpuidle_devices);
 	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
 	int next_state, entered_state;
-	bool reflect;
 
 	/*
 	 * Check if the idle task must be rescheduled. If it is the
@@ -138,56 +177,19 @@ static void cpuidle_idle_call(void)
 			goto exit_idle;
 		}
 
-		reflect = false;
 		next_state = cpuidle_find_deepest_state(drv, dev);
+		call_cpuidle(drv, dev, next_state);
 	} else {
-		reflect = true;
 		/*
 		 * Ask the cpuidle framework to choose a convenient idle state.
 		 */
 		next_state = cpuidle_select(drv, dev);
-	}
-	/* Fall back to the default arch idle method on errors. */
-	if (next_state < 0) {
-		default_idle_call();
-		goto exit_idle;
-	}
-
-	/*
-	 * The idle task must be scheduled, it is pointless to
-	 * go to idle, just update no idle residency and get
-	 * out of this function
-	 */
-	if (current_clr_polling_and_test()) {
-		dev->last_residency = 0;
-		entered_state = next_state;
-		local_irq_enable();
-		goto exit_idle;
-	}
-
-	/* Take note of the planned idle state. */
-	idle_set_state(this_rq(), &drv->states[next_state]);
-
-	/*
-	 * Enter the idle state previously returned by the governor decision.
-	 * This function will block until an interrupt occurs and will take
-	 * care of re-enabling the local interrupts
-	 */
-	entered_state = cpuidle_enter(drv, dev, next_state);
-
-	/* The cpu is no longer idle or about to enter idle. */
-	idle_set_state(this_rq(), NULL);
-
-	if (entered_state == -EBUSY) {
-		default_idle_call();
-		goto exit_idle;
-	}
-
-	/*
-	 * Give the governor an opportunity to reflect on the outcome
-	 */
-	if (reflect)
+		entered_state = call_cpuidle(drv, dev, next_state);
+		/*
+		 * Give the governor an opportunity to reflect on the outcome
+		 */
 		cpuidle_reflect(dev, entered_state);
+	}
 
 exit_idle:
 	__current_set_polling();

commit 82f663277d0db854e8978e5f89fd88f6df75a4a4
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon May 4 22:53:22 2015 +0200

    sched / idle: Move the default idle call code to a separate function
    
    Move the code under the "use_default" label in cpuidle_idle_call()
    into a separate (new) function.
    
    This just allows the subsequent changes to be more stratightforward.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Reviewed-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index fefcb1fa5160..ae7c0be90d16 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -67,6 +67,18 @@ void __weak arch_cpu_idle(void)
 	local_irq_enable();
 }
 
+static void default_idle_call(void)
+{
+	/*
+	 * We can't use the cpuidle framework, let's use the default idle
+	 * routine.
+	 */
+	if (current_clr_polling_and_test())
+		local_irq_enable();
+	else
+		arch_cpu_idle();
+}
+
 /**
  * cpuidle_idle_call - the main idle function
  *
@@ -105,8 +117,10 @@ static void cpuidle_idle_call(void)
 	 */
 	rcu_idle_enter();
 
-	if (cpuidle_not_available(drv, dev))
-		goto use_default;
+	if (cpuidle_not_available(drv, dev)) {
+		default_idle_call();
+		goto exit_idle;
+	}
 
 	/*
 	 * Suspend-to-idle ("freeze") is a system state in which all user space
@@ -134,8 +148,10 @@ static void cpuidle_idle_call(void)
 		next_state = cpuidle_select(drv, dev);
 	}
 	/* Fall back to the default arch idle method on errors. */
-	if (next_state < 0)
-		goto use_default;
+	if (next_state < 0) {
+		default_idle_call();
+		goto exit_idle;
+	}
 
 	/*
 	 * The idle task must be scheduled, it is pointless to
@@ -162,8 +178,10 @@ static void cpuidle_idle_call(void)
 	/* The cpu is no longer idle or about to enter idle. */
 	idle_set_state(this_rq(), NULL);
 
-	if (entered_state == -EBUSY)
-		goto use_default;
+	if (entered_state == -EBUSY) {
+		default_idle_call();
+		goto exit_idle;
+	}
 
 	/*
 	 * Give the governor an opportunity to reflect on the outcome
@@ -182,19 +200,6 @@ static void cpuidle_idle_call(void)
 
 	rcu_idle_exit();
 	start_critical_timings();
-	return;
-
-use_default:
-	/*
-	 * We can't use the cpuidle framework, let's use the default
-	 * idle routine.
-	 */
-	if (current_clr_polling_and_test())
-		local_irq_enable();
-	else
-		arch_cpu_idle();
-
-	goto exit_idle;
 }
 
 DEFINE_PER_CPU(bool, cpu_dead_idle);

commit df8d9eeadd0f7a216f2476351d5aee43c6550bf0
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Wed Apr 29 15:19:21 2015 +0200

    cpuidle: Run tick_broadcast_exit() with disabled interrupts
    
    Commit 335f49196fd6 (sched/idle: Use explicit broadcast oneshot
    control function) replaced clockevents_notify() invocations in
    cpuidle_idle_call() with direct calls to tick_broadcast_enter()
    and tick_broadcast_exit(), but it overlooked the fact that
    interrupts were already enabled before calling the latter which
    led to functional breakage on systems using idle states with the
    CPUIDLE_FLAG_TIMER_STOP flag set.
    
    Fix that by moving the invocations of tick_broadcast_enter()
    and tick_broadcast_exit() down into cpuidle_enter_state() where
    interrupts are still disabled when tick_broadcast_exit() is
    called.  Also ensure that interrupts will be disabled before
    running tick_broadcast_exit() even if they have been enabled by
    the idle state's ->enter callback.  Trigger a WARN_ON_ONCE() in
    that case, as we generally don't want that to happen for states
    with CPUIDLE_FLAG_TIMER_STOP set.
    
    Fixes: 335f49196fd6 (sched/idle: Use explicit broadcast oneshot control function)
    Reported-and-tested-by: Linus Walleij <linus.walleij@linaro.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Reported-and-tested-by: Sudeep Holla <sudeep.holla@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index deef1caa94c6..fefcb1fa5160 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -81,7 +81,6 @@ static void cpuidle_idle_call(void)
 	struct cpuidle_device *dev = __this_cpu_read(cpuidle_devices);
 	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
 	int next_state, entered_state;
-	unsigned int broadcast;
 	bool reflect;
 
 	/*
@@ -150,17 +149,6 @@ static void cpuidle_idle_call(void)
 		goto exit_idle;
 	}
 
-	broadcast = drv->states[next_state].flags & CPUIDLE_FLAG_TIMER_STOP;
-
-	/*
-	 * Tell the time framework to switch to a broadcast timer
-	 * because our local timer will be shutdown. If a local timer
-	 * is used from another cpu as a broadcast timer, this call may
-	 * fail if it is not available
-	 */
-	if (broadcast && tick_broadcast_enter())
-		goto use_default;
-
 	/* Take note of the planned idle state. */
 	idle_set_state(this_rq(), &drv->states[next_state]);
 
@@ -174,8 +162,8 @@ static void cpuidle_idle_call(void)
 	/* The cpu is no longer idle or about to enter idle. */
 	idle_set_state(this_rq(), NULL);
 
-	if (broadcast)
-		tick_broadcast_exit();
+	if (entered_state == -EBUSY)
+		goto use_default;
 
 	/*
 	 * Give the governor an opportunity to reflect on the outcome

commit 078838d56574694d0a4815d9c1b7f28e8844638b
Merge: eeee78cf77df 590ee7dbd569
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 13:36:04 2015 -0700

    Merge branch 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull RCU changes from Ingo Molnar:
     "The main changes in this cycle were:
    
       - changes permitting use of call_rcu() and friends very early in
         boot, for example, before rcu_init() is invoked.
    
       - add in-kernel API to enable and disable expediting of normal RCU
         grace periods.
    
       - improve RCU's handling of (hotplug-) outgoing CPUs.
    
       - NO_HZ_FULL_SYSIDLE fixes.
    
       - tiny-RCU updates to make it more tiny.
    
       - documentation updates.
    
       - miscellaneous fixes"
    
    * 'core-rcu-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (58 commits)
      cpu: Provide smpboot_thread_init() on !CONFIG_SMP kernels as well
      cpu: Defer smpboot kthread unparking until CPU known to scheduler
      rcu: Associate quiescent-state reports with grace period
      rcu: Yet another fix for preemption and CPU hotplug
      rcu: Add diagnostics to grace-period cleanup
      rcutorture: Default to grace-period-initialization delays
      rcu: Handle outgoing CPUs on exit from idle loop
      cpu: Make CPU-offline idle-loop transition point more precise
      rcu: Eliminate ->onoff_mutex from rcu_node structure
      rcu: Process offlining and onlining only at grace-period start
      rcu: Move rcu_report_unblock_qs_rnp() to common code
      rcu: Rework preemptible expedited bitmask handling
      rcu: Remove event tracing from rcu_cpu_notify(), used by offline CPUs
      rcutorture: Enable slow grace-period initializations
      rcu: Provide diagnostic option to slow down grace-period initialization
      rcu: Detect stalls caused by failure to propagate up rcu_node tree
      rcu: Eliminate empty HOTPLUG_CPU ifdef
      rcu: Simplify sync_rcu_preempt_exp_init()
      rcu: Put all orphan-callback-related code under same comment
      rcu: Consolidate offline-CPU callback initialization
      ...

commit 335f49196fd6011521f078cb44f445847e5aa183
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Apr 3 02:34:49 2015 +0200

    sched/idle: Use explicit broadcast oneshot control function
    
    Replace the clockevents_notify() call with an explicit function call.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/6422336.RMm7oUHcXh@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 80014a178342..4d207d2abcbd 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -158,8 +158,7 @@ static void cpuidle_idle_call(void)
 	 * is used from another cpu as a broadcast timer, this call may
 	 * fail if it is not available
 	 */
-	if (broadcast &&
-	    clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &dev->cpu))
+	if (broadcast && tick_broadcast_enter())
 		goto use_default;
 
 	/* Take note of the planned idle state. */
@@ -176,7 +175,7 @@ static void cpuidle_idle_call(void)
 	idle_set_state(this_rq(), NULL);
 
 	if (broadcast)
-		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &dev->cpu);
+		tick_broadcast_exit();
 
 	/*
 	 * Give the governor an opportunity to reflect on the outcome

commit 4bfe186dbe0a058680e4bfb0d673194f0ceaffd4
Merge: 3c435c1e472b 42528795ac1c
Author: Ingo Molnar <mingo@kernel.org>
Date:   Fri Mar 27 10:04:06 2015 +0100

    Merge branch 'for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
      - Documentation updates.
    
      - Changes permitting use of call_rcu() and friends very early in
        boot, for example, before rcu_init() is invoked.
    
      - Miscellaneous fixes.
    
      - Add in-kernel API to enable and disable expediting of normal RCU
        grace periods.
    
      - Improve RCU's handling of (hotplug-) outgoing CPUs.
    
        Note: ARM support is lagging a bit here, and these improved
        diagnostics might generate (harmless) splats.
    
      - NO_HZ_FULL_SYSIDLE fixes.
    
      - Tiny RCU updates to make it more tiny.
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 88428cc5c27c63a4313e213813bc39b9899224d5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 28 14:42:09 2015 -0800

    rcu: Handle outgoing CPUs on exit from idle loop
    
    This commit informs RCU of an outgoing CPU just before that CPU invokes
    arch_cpu_idle_dead() during its last pass through the idle loop (via a
    new CPU_DYING_IDLE notifier value).  This change means that RCU need not
    deal with outgoing CPUs passing through the scheduler after informing
    RCU that they are no longer online.  Note that removing the CPU from
    the rcu_node ->qsmaskinit bit masks is done at CPU_DYING_IDLE time,
    and orphaning callbacks is still done at CPU_DEAD time, the reason being
    that at CPU_DEAD time we have another CPU that can adopt them.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index e99e361ade20..b0090accfb5b 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -225,6 +225,8 @@ static void cpu_idle_loop(void)
 			rmb();
 
 			if (cpu_is_offline(smp_processor_id())) {
+				rcu_cpu_notify(NULL, CPU_DYING_IDLE,
+					       (void *)(long)smp_processor_id());
 				smp_mb(); /* all activity before dead. */
 				this_cpu_write(cpu_dead_idle, true);
 				arch_cpu_idle_dead();

commit 528a25b00e1f84eaba6c98e63f58ee0a8e472102
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed Jan 28 14:09:43 2015 -0800

    cpu: Make CPU-offline idle-loop transition point more precise
    
    This commit uses a per-CPU variable to make the CPU-offline code path
    through the idle loop more precise, so that the outgoing CPU is
    guaranteed to make it into the idle loop before it is powered off.
    This commit is in preparation for putting the RCU offline-handling
    code on this code path, which will eliminate the magic one-jiffy
    wait that RCU uses as the maximum time for an outgoing CPU to get
    all the way through the scheduler.
    
    The magic one-jiffy wait for incoming CPUs remains a separate issue.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 94b2d7b88a27..e99e361ade20 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -198,6 +198,8 @@ static void cpuidle_idle_call(void)
 	start_critical_timings();
 }
 
+DEFINE_PER_CPU(bool, cpu_dead_idle);
+
 /*
  * Generic idle loop implementation
  *
@@ -222,8 +224,11 @@ static void cpu_idle_loop(void)
 			check_pgt_cache();
 			rmb();
 
-			if (cpu_is_offline(smp_processor_id()))
+			if (cpu_is_offline(smp_processor_id())) {
+				smp_mb(); /* all activity before dead. */
+				this_cpu_write(cpu_dead_idle, true);
 				arch_cpu_idle_dead();
+			}
 
 			local_irq_disable();
 			arch_cpu_idle_enter();

commit ef2b22ac540c018bd574d1846ab95b9bfcf38702
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Mar 2 22:26:55 2015 +0100

    cpuidle / sleep: Use broadcast timer for states that stop local timer
    
    Commit 381063133246 (PM / sleep: Re-implement suspend-to-idle handling)
    overlooked the fact that entering some sufficiently deep idle states
    by CPUs may cause their local timers to stop and in those cases it
    is necessary to switch over to a broadcast timer prior to entering
    the idle state.  If the cpuidle driver in use does not provide
    the new ->enter_freeze callback for any of the idle states, that
    problem affects suspend-to-idle too, but it is not taken into account
    after the changes made by commit 381063133246.
    
    Fix that by changing the definition of cpuidle_enter_freeze() and
    re-arranging of the code in cpuidle_idle_call(), so the former does
    not call cpuidle_enter() any more and the fallback case is handled
    by cpuidle_idle_call() directly.
    
    Fixes: 381063133246 (PM / sleep: Re-implement suspend-to-idle handling)
    Reported-and-tested-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 84b93b68482a..80014a178342 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -82,6 +82,7 @@ static void cpuidle_idle_call(void)
 	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
 	int next_state, entered_state;
 	unsigned int broadcast;
+	bool reflect;
 
 	/*
 	 * Check if the idle task must be rescheduled. If it is the
@@ -105,6 +106,9 @@ static void cpuidle_idle_call(void)
 	 */
 	rcu_idle_enter();
 
+	if (cpuidle_not_available(drv, dev))
+		goto use_default;
+
 	/*
 	 * Suspend-to-idle ("freeze") is a system state in which all user space
 	 * has been frozen, all I/O devices have been suspended and the only
@@ -115,15 +119,22 @@ static void cpuidle_idle_call(void)
 	 * until a proper wakeup interrupt happens.
 	 */
 	if (idle_should_freeze()) {
-		cpuidle_enter_freeze();
-		goto exit_idle;
-	}
+		entered_state = cpuidle_enter_freeze(drv, dev);
+		if (entered_state >= 0) {
+			local_irq_enable();
+			goto exit_idle;
+		}
 
-	/*
-	 * Ask the cpuidle framework to choose a convenient idle state.
-	 * Fall back to the default arch idle method on errors.
-	 */
-	next_state = cpuidle_select(drv, dev);
+		reflect = false;
+		next_state = cpuidle_find_deepest_state(drv, dev);
+	} else {
+		reflect = true;
+		/*
+		 * Ask the cpuidle framework to choose a convenient idle state.
+		 */
+		next_state = cpuidle_select(drv, dev);
+	}
+	/* Fall back to the default arch idle method on errors. */
 	if (next_state < 0)
 		goto use_default;
 
@@ -170,7 +181,8 @@ static void cpuidle_idle_call(void)
 	/*
 	 * Give the governor an opportunity to reflect on the outcome
 	 */
-	cpuidle_reflect(dev, entered_state);
+	if (reflect)
+		cpuidle_reflect(dev, entered_state);
 
 exit_idle:
 	__current_set_polling();

commit dfcacc154fb38fdb2c243c3dbbdc1f26a64cedc8
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Mar 2 22:25:37 2015 +0100

    cpuidle: Clean up fallback handling in cpuidle_idle_call()
    
    Move the fallback code path in cpuidle_idle_call() to the end of the
    function to avoid jumping to a label in an if () branch.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index f59198bda1bf..84b93b68482a 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -124,20 +124,8 @@ static void cpuidle_idle_call(void)
 	 * Fall back to the default arch idle method on errors.
 	 */
 	next_state = cpuidle_select(drv, dev);
-	if (next_state < 0) {
-use_default:
-		/*
-		 * We can't use the cpuidle framework, let's use the default
-		 * idle routine.
-		 */
-		if (current_clr_polling_and_test())
-			local_irq_enable();
-		else
-			arch_cpu_idle();
-
-		goto exit_idle;
-	}
-
+	if (next_state < 0)
+		goto use_default;
 
 	/*
 	 * The idle task must be scheduled, it is pointless to
@@ -195,6 +183,19 @@ static void cpuidle_idle_call(void)
 
 	rcu_idle_exit();
 	start_critical_timings();
+	return;
+
+use_default:
+	/*
+	 * We can't use the cpuidle framework, let's use the default
+	 * idle routine.
+	 */
+	if (current_clr_polling_and_test())
+		local_irq_enable();
+	else
+		arch_cpu_idle();
+
+	goto exit_idle;
 }
 
 /*

commit 01e04f466e12e883907937eb04a9010533363f55
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Fri Feb 27 00:39:21 2015 +0100

    idle / sleep: Avoid excessive disabling and enabling interrupts
    
    Disabling interrupts at the end of cpuidle_enter_freeze() is not
    useful, because its caller, cpuidle_idle_call(), re-enables them
    right away after invoking it.
    
    To avoid that unnecessary back and forth dance with interrupts,
    make cpuidle_enter_freeze() enable interrupts after calling
    enter_freeze_proper() and drop the local_irq_disable() at its
    end, so that all of the code paths in it end up with interrupts
    enabled.  Then, cpuidle_idle_call() will not need to re-enable
    interrupts after calling cpuidle_enter_freeze() any more, because
    the latter will return with interrupts enabled, in analogy with
    cpuidle_enter().
    
    Reported-by: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 94b2d7b88a27..f59198bda1bf 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -116,7 +116,6 @@ static void cpuidle_idle_call(void)
 	 */
 	if (idle_should_freeze()) {
 		cpuidle_enter_freeze();
-		local_irq_enable();
 		goto exit_idle;
 	}
 

commit 3810631332465d967ba5e27ea2c7dff2c9afac6c
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu Feb 12 23:33:15 2015 +0100

    PM / sleep: Re-implement suspend-to-idle handling
    
    In preparation for adding support for quiescing timers in the final
    stage of suspend-to-idle transitions, rework the freeze_enter()
    function making the system wait on a wakeup event, the freeze_wake()
    function terminating the suspend-to-idle loop and the mechanism by
    which deep idle states are entered during suspend-to-idle.
    
    First of all, introduce a simple state machine for suspend-to-idle
    and make the code in question use it.
    
    Second, prevent freeze_enter() from losing wakeup events due to race
    conditions and ensure that the number of online CPUs won't change
    while it is being executed.  In addition to that, make it force
    all of the CPUs re-enter the idle loop in case they are in idle
    states already (so they can enter deeper idle states if possible).
    
    Next, drop cpuidle_use_deepest_state() and replace use_deepest_state
    checks in cpuidle_select() and cpuidle_reflect() with a single
    suspend-to-idle state check in cpuidle_idle_call().
    
    Finally, introduce cpuidle_enter_freeze() that will simply find the
    deepest idle state available to the given CPU and enter it using
    cpuidle_enter().
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index aaf1c1d5cf5d..94b2d7b88a27 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -7,6 +7,7 @@
 #include <linux/tick.h>
 #include <linux/mm.h>
 #include <linux/stackprotector.h>
+#include <linux/suspend.h>
 
 #include <asm/tlb.h>
 
@@ -104,6 +105,21 @@ static void cpuidle_idle_call(void)
 	 */
 	rcu_idle_enter();
 
+	/*
+	 * Suspend-to-idle ("freeze") is a system state in which all user space
+	 * has been frozen, all I/O devices have been suspended and the only
+	 * activity happens here and in iterrupts (if any).  In that case bypass
+	 * the cpuidle governor and go stratight for the deepest idle state
+	 * available.  Possibly also suspend the local tick and the entire
+	 * timekeeping to prevent timer interrupts from kicking us out of idle
+	 * until a proper wakeup interrupt happens.
+	 */
+	if (idle_should_freeze()) {
+		cpuidle_enter_freeze();
+		local_irq_enable();
+		goto exit_idle;
+	}
+
 	/*
 	 * Ask the cpuidle framework to choose a convenient idle state.
 	 * Fall back to the default arch idle method on errors.

commit ff6f2d29bd31cdfa1ac494a8b26d2af8ba887d59
Author: Preeti U Murthy <preeti@linux.vnet.ibm.com>
Date:   Wed Jan 21 16:27:25 2015 +0530

    sched/idle: Add missing checks to the exit condition of cpu_idle_poll()
    
    cpu_idle_poll() is entered into when either the cpu_idle_force_poll is set or
    tick_check_broadcast_expired() returns true. The exit condition from
    cpu_idle_poll() is tif_need_resched().
    
    However this does not take into account scenarios where cpu_idle_force_poll
    changes or tick_check_broadcast_expired() returns false, without setting
    the resched flag. So a cpu will be caught in cpu_idle_poll() needlessly,
    thereby wasting power. Add an explicit check on cpu_idle_force_poll and
    tick_check_broadcast_expired() to the exit condition of cpu_idle_poll()
    to avoid this.
    
    Signed-off-by: Preeti U Murthy <preeti@linux.vnet.ibm.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: linuxppc-dev@lists.ozlabs.org
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/20150121105655.15279.59626.stgit@preeti.in.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index c47fce75e666..aaf1c1d5cf5d 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -47,7 +47,8 @@ static inline int cpu_idle_poll(void)
 	rcu_idle_enter();
 	trace_cpu_idle_rcuidle(0, smp_processor_id());
 	local_irq_enable();
-	while (!tif_need_resched())
+	while (!tif_need_resched() &&
+		(cpu_idle_force_poll || tick_check_broadcast_expired()))
 		cpu_relax();
 	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
 	rcu_idle_exit();

commit 442bf3aaf55a91ebfec71da46a4ee10a3c905bcc
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Thu Sep 4 11:32:09 2014 -0400

    sched: Let the scheduler see CPU idle states
    
    When the cpu enters idle, it stores the cpuidle state pointer in its
    struct rq instance which in turn could be used to make a better decision
    when balancing tasks.
    
    As soon as the cpu exits its idle state, the struct rq reference is
    cleared.
    
    There are a couple of situations where the idle state pointer could be changed
    while it is being consulted:
    
    1. For x86/acpi with dynamic c-states, when a laptop switches from battery
       to AC that could result on removing the deeper idle state. The acpi driver
       triggers:
            'acpi_processor_cst_has_changed'
                    'cpuidle_pause_and_lock'
                            'cpuidle_uninstall_idle_handler'
                                    'kick_all_cpus_sync'.
    
    All cpus will exit their idle state and the pointed object will be set to
    NULL.
    
    2. The cpuidle driver is unloaded. Logically that could happen but not
    in practice because the drivers are always compiled in and 95% of them are
    not coded to unregister themselves.  In any case, the unloading code must
    call 'cpuidle_unregister_device', that calls 'cpuidle_pause_and_lock'
    leading to 'kick_all_cpus_sync' as mentioned above.
    
    A race can happen if we use the pointer and then one of these two scenarios
    occurs at the same moment.
    
    In order to be safe, the idle state pointer stored in the rq must be
    used inside a rcu_read_lock section where we are protected with the
    'rcu_barrier' in the 'cpuidle_uninstall_idle_handler' function. The
    idle_get_state() and idle_put_state() accessors should be used to that
    effect.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Rafael J. Wysocki" <rjw@rjwysocki.net>
    Cc: linux-pm@vger.kernel.org
    Cc: linaro-kernel@lists.linaro.org
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 11e7bc434f43..c47fce75e666 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -147,6 +147,9 @@ static void cpuidle_idle_call(void)
 	    clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &dev->cpu))
 		goto use_default;
 
+	/* Take note of the planned idle state. */
+	idle_set_state(this_rq(), &drv->states[next_state]);
+
 	/*
 	 * Enter the idle state previously returned by the governor decision.
 	 * This function will block until an interrupt occurs and will take
@@ -154,6 +157,9 @@ static void cpuidle_idle_call(void)
 	 */
 	entered_state = cpuidle_enter(drv, dev, next_state);
 
+	/* The cpu is no longer idle or about to enter idle. */
+	idle_set_state(this_rq(), NULL);
+
 	if (broadcast)
 		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &dev->cpu);
 

commit 7725131982477bffff8ffdea143434dcc69f5d90
Merge: 6b22df74f7af 7ef97e0e3a0f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Aug 6 20:34:19 2014 -0700

    Merge tag 'pm+acpi-3.17-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm
    
    Pull ACPI and power management updates from Rafael Wysocki:
     "Again, ACPICA leads the pack (47 commits), followed by cpufreq (18
      commits) and system suspend/hibernation (9 commits).
    
      From the new code perspective, the ACPICA update brings ACPI 5.1 to
      the table, including a new device configuration object called _DSD
      (Device Specific Data) that will hopefully help us to operate device
      properties like Device Trees do (at least to some extent) and changes
      related to supporting ACPI on ARM.
    
      Apart from that we have hibernation changes making it use radix trees
      to store memory bitmaps which should speed up some operations carried
      out by it quite significantly.  We also have some power management
      changes related to suspend-to-idle (the "freeze" sleep state) support
      and more preliminary changes needed to support ACPI on ARM (outside of
      ACPICA).
    
      The rest is fixes and cleanups pretty much everywhere.
    
      Specifics:
    
       - ACPICA update to upstream version 20140724.  That includes ACPI 5.1
         material (support for the _CCA and _DSD predefined names, changes
         related to the DMAR and PCCT tables and ARM support among other
         things) and cleanups related to using ACPICA's header files.  A
         major part of it is related to acpidump and the core code used by
         that utility.  Changes from Bob Moore, David E Box, Lv Zheng,
         Sascha Wildner, Tomasz Nowicki, Hanjun Guo.
    
       - Radix trees for memory bitmaps used by the hibernation core from
         Joerg Roedel.
    
       - Support for waking up the system from suspend-to-idle (also known
         as the "freeze" sleep state) using ACPI-based PCI wakeup signaling
         (Rafael J Wysocki).
    
       - Fixes for issues related to ACPI button events (Rafael J Wysocki).
    
       - New device ID for an ACPI-enumerated device included into the
         Wildcat Point PCH from Jie Yang.
    
       - ACPI video updates related to backlight handling from Hans de Goede
         and Linus Torvalds.
    
       - Preliminary changes needed to support ACPI on ARM from Hanjun Guo
         and Graeme Gregory.
    
       - ACPI PNP core cleanups from Arjun Sreedharan and Zhang Rui.
    
       - Cleanups related to ACPI_COMPANION() and ACPI_HANDLE() macros
         (Rafael J Wysocki).
    
       - ACPI-based device hotplug cleanups from Wei Yongjun and Rafael J
         Wysocki.
    
       - Cleanups and improvements related to system suspend from Lan
         Tianyu, Randy Dunlap and Rafael J Wysocki.
    
       - ACPI battery cleanup from Wei Yongjun.
    
       - cpufreq core fixes from Viresh Kumar.
    
       - Elimination of a deadband effect from the cpufreq ondemand governor
         and intel_pstate driver cleanups from Stratos Karafotis.
    
       - 350MHz CPU support for the powernow-k6 cpufreq driver from Mikulas
         Patocka.
    
       - Fix for the imx6 cpufreq driver from Anson Huang.
    
       - cpuidle core and governor cleanups from Daniel Lezcano, Sandeep
         Tripathy and Mohammad Merajul Islam Molla.
    
       - Build fix for the big_little cpuidle driver from Sachin Kamat.
    
       - Configuration fix for the Operation Performance Points (OPP)
         framework from Mark Brown.
    
       - APM cleanup from Jean Delvare.
    
       - cpupower utility fixes and cleanups from Peter Senna Tschudin,
         Andrey Utkin, Himangi Saraogi, Rickard Strandqvist, Thomas
         Renninger"
    
    * tag 'pm+acpi-3.17-rc1' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm: (118 commits)
      ACPI / LPSS: add LPSS device for Wildcat Point PCH
      ACPI / PNP: Replace faulty is_hex_digit() by isxdigit()
      ACPICA: Update version to 20140724.
      ACPICA: ACPI 5.1: Update for PCCT table changes.
      ACPICA/ARM: ACPI 5.1: Update for GTDT table changes.
      ACPICA/ARM: ACPI 5.1: Update for MADT changes.
      ACPICA/ARM: ACPI 5.1: Update for FADT changes.
      ACPICA: ACPI 5.1: Support for the _CCA predifined name.
      ACPICA: ACPI 5.1: New notify value for System Affinity Update.
      ACPICA: ACPI 5.1: Support for the _DSD predefined name.
      ACPICA: Debug object: Add current value of Timer() to debug line prefix.
      ACPICA: acpihelp: Add UUID support, restructure some existing files.
      ACPICA: Utilities: Fix local printf issue.
      ACPICA: Tables: Update for DMAR table changes.
      ACPICA: Remove some extraneous printf arguments.
      ACPICA: Update for comments/formatting. No functional changes.
      ACPICA: Disassembler: Add support for the ToUUID opererator (macro).
      ACPICA: Remove a redundant cast to acpi_size for ACPI_OFFSET() macro.
      ACPICA: Work around an ancient GCC bug.
      ACPI / processor: Make it possible to get local x2apic id via _MAT
      ...

commit 30fe6884021b9fa0124609e898a6341be188eb44
Author: Sandeep Tripathy <sandeep.tripathy@linaro.org>
Date:   Wed Jul 2 15:00:58 2014 +0530

    cpuidle: move idle traces to cpuidle_enter_state()
    
    idle_exit event is the first event after a core exits
    idle state. So this should be traced before local irq
    is ebabled. Likewise idle_entry is the last event before
    a core enters idle state. This will ease visualising the
    cpu idle state from kernel traces.
    
    Signed-off-by: Sandeep Tripathy <sandeep.tripathy@linaro.org>
    Acked-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    [rjw: Subject, rebase]
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index cf009fb0bc25..658a58dc30f4 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -147,8 +147,6 @@ static void cpuidle_idle_call(void)
 	    clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &dev->cpu))
 		goto use_default;
 
-	trace_cpu_idle_rcuidle(next_state, dev->cpu);
-
 	/*
 	 * Enter the idle state previously returned by the governor decision.
 	 * This function will block until an interrupt occurs and will take
@@ -156,8 +154,6 @@ static void cpuidle_idle_call(void)
 	 */
 	entered_state = cpuidle_enter(drv, dev, next_state);
 
-	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, dev->cpu);
-
 	if (broadcast)
 		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &dev->cpu);
 

commit 89abb5ad10ae8ac3405e635ac80815f781c8b8e9
Author: Viresh Kumar <viresh.kumar@linaro.org>
Date:   Tue Jun 24 10:01:01 2014 +0530

    sched/idle: Drop !! while calculating 'broadcast'
    
    We don't need 'broadcast' to be set to 'zero or one', but to 'zero or non-zero'
    and so the extra operation to convert it to 'zero or one' can be skipped.
    
    Also change type of 'broadcast' to unsigned int, i.e. type of
    drv->states[*].flags.
    
    Signed-off-by: Viresh Kumar <viresh.kumar@linaro.org>
    Cc: linaro-kernel@lists.linaro.org
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/0dfbe2976aa108c53e08d3477ea90f6360c1f54c.1403584026.git.viresh.kumar@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index cf009fb0bc25..9f1608f99819 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -79,7 +79,7 @@ static void cpuidle_idle_call(void)
 	struct cpuidle_device *dev = __this_cpu_read(cpuidle_devices);
 	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
 	int next_state, entered_state;
-	bool broadcast;
+	unsigned int broadcast;
 
 	/*
 	 * Check if the idle task must be rescheduled. If it is the
@@ -135,7 +135,7 @@ static void cpuidle_idle_call(void)
 		goto exit_idle;
 	}
 
-	broadcast = !!(drv->states[next_state].flags & CPUIDLE_FLAG_TIMER_STOP);
+	broadcast = drv->states[next_state].flags & CPUIDLE_FLAG_TIMER_STOP;
 
 	/*
 	 * Tell the time framework to switch to a broadcast timer

commit e3baac47f0e82c4be632f4f97215bb93bf16b342
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Jun 4 10:31:18 2014 -0700

    sched/idle: Optimize try-to-wake-up IPI
    
    [ This series reduces the number of IPIs on Andy's workload by something like
      99%. It's down from many hundreds per second to very few.
    
      The basic idea behind this series is to make TIF_POLLING_NRFLAG be a
      reliable indication that the idle task is polling.  Once that's done,
      the rest is reasonably straightforward. ]
    
    When enqueueing tasks on remote LLC domains, we send an IPI to do the
    work 'locally' and avoid bouncing all the cachelines over.
    
    However, when the remote CPU is idle (and polling, say x86 mwait), we
    don't need to send an IPI, we can simply kick the TIF word to wake it
    up and have the 'idle' loop do the work.
    
    So when _TIF_POLLING_NRFLAG is set, but _TIF_NEED_RESCHED is not (yet)
    set, set _TIF_NEED_RESCHED and avoid sending the IPI.
    
    Much-requested-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    [Edited by Andy Lutomirski, but this is mostly Peter Zijlstra's code.]
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Cc: nicolas.pitre@linaro.org
    Cc: daniel.lezcano@linaro.org
    Cc: Mike Galbraith <umgwanakikbuti@gmail.com>
    Cc: umgwanakikbuti@gmail.com
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/ce06f8b02e7e337be63e97597fc4b248d3aa6f9b.1401902905.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index fe4b24bf33ca..cf009fb0bc25 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -12,6 +12,8 @@
 
 #include <trace/events/power.h>
 
+#include "sched.h"
+
 static int __read_mostly cpu_idle_force_poll;
 
 void cpu_idle_poll_ctrl(bool enable)
@@ -237,12 +239,14 @@ static void cpu_idle_loop(void)
 		__current_clr_polling();
 
 		/*
-		 * We promise to reschedule if need_resched is set while
-		 * polling is set.  That means that clearing polling
-		 * needs to be visible before rescheduling.
+		 * We promise to call sched_ttwu_pending and reschedule
+		 * if need_resched is set while polling is set.  That
+		 * means that clearing polling needs to be visible
+		 * before doing these things.
 		 */
 		smp_mb__after_atomic();
 
+		sched_ttwu_pending();
 		schedule_preempt_disabled();
 	}
 }

commit 82c65d60d64401aedc1006d6572469bbfdf148de
Author: Andy Lutomirski <luto@amacapital.net>
Date:   Wed Jun 4 10:31:16 2014 -0700

    sched/idle: Clear polling before descheduling the idle thread
    
    Currently, the only real guarantee provided by the polling bit is
    that, if you hold rq->lock and the polling bit is set, then you can
    set need_resched to force a reschedule.
    
    The only reason the lock is needed is that the idle thread might not
    be running at all when setting its need_resched bit, and rq->lock
    keeps it pinned.
    
    This is easy to fix: just clear the polling bit before scheduling.
    Now the idle thread's polling bit is only ever set when
    rq->curr == rq->idle.
    
    Signed-off-by: Andy Lutomirski <luto@amacapital.net>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: nicolas.pitre@linaro.org
    Cc: daniel.lezcano@linaro.org
    Cc: umgwanakikbuti@gmail.com
    Cc: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: linux-kernel@vger.kernel.org
    Link: http://lkml.kernel.org/r/b2059fcb4c613d520cb503b6fad6e47033c7c203.1401902905.git.luto@amacapital.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 25b9423abce9..fe4b24bf33ca 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -67,6 +67,10 @@ void __weak arch_cpu_idle(void)
  * cpuidle_idle_call - the main idle function
  *
  * NOTE: no locks or semaphores should be used here
+ *
+ * On archs that support TIF_POLLING_NRFLAG, is called with polling
+ * set, and it returns with polling set.  If it ever stops polling, it
+ * must clear the polling bit.
  */
 static void cpuidle_idle_call(void)
 {
@@ -175,10 +179,22 @@ static void cpuidle_idle_call(void)
 
 /*
  * Generic idle loop implementation
+ *
+ * Called with polling cleared.
  */
 static void cpu_idle_loop(void)
 {
 	while (1) {
+		/*
+		 * If the arch has a polling bit, we maintain an invariant:
+		 *
+		 * Our polling bit is clear if we're not scheduled (i.e. if
+		 * rq->curr != rq->idle).  This means that, if rq->idle has
+		 * the polling bit set, then setting need_resched is
+		 * guaranteed to cause the cpu to reschedule.
+		 */
+
+		__current_set_polling();
 		tick_nohz_idle_enter();
 
 		while (!need_resched()) {
@@ -218,6 +234,15 @@ static void cpu_idle_loop(void)
 		 */
 		preempt_set_need_resched();
 		tick_nohz_idle_exit();
+		__current_clr_polling();
+
+		/*
+		 * We promise to reschedule if need_resched is set while
+		 * polling is set.  That means that clearing polling
+		 * needs to be visible before rescheduling.
+		 */
+		smp_mb__after_atomic();
+
 		schedule_preempt_disabled();
 	}
 }
@@ -239,7 +264,6 @@ void cpu_startup_entry(enum cpuhp_state state)
 	 */
 	boot_init_stack_canary();
 #endif
-	__current_set_polling();
 	arch_cpu_idle_prepare();
 	cpu_idle_loop();
 }

commit ec6e7f4082aa0c6c4334149e74673b6ed736fb63
Merge: 65c2ce70046c a6220fc19afc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu May 22 10:37:06 2014 +0200

    Merge branch 'pm-cpuidle' of git://git.kernel.org/pub/scm/linux/kernel/git/rafael/linux-pm into sched/core
    
    Pull scheduling related CPU idle updates from Rafael J. Wysocki.
    
    Conflicts:
            kernel/sched/idle.c
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 08c373e5123b4595588ae1a7aa7e00a046c61cc6
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Mon Apr 21 01:26:58 2014 +0200

    sched/idle: Make cpuidle_idle_call() void
    
    The only value ever returned by cpuidle_idle_call() is 0 and its
    only caller ignores that value anyway, so make it void.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/4717784.WmVEpDoliM@vostro.rjw.lan
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 88a6bc43738b..34083c9ac976 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -67,9 +67,8 @@ void __weak arch_cpu_idle(void)
  * cpuidle_idle_call - the main idle function
  *
  * NOTE: no locks or semaphores should be used here
- * return non-zero on failure
  */
-static int cpuidle_idle_call(void)
+static void cpuidle_idle_call(void)
 {
 	struct cpuidle_device *dev = __this_cpu_read(cpuidle_devices);
 	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
@@ -82,7 +81,7 @@ static int cpuidle_idle_call(void)
 	 */
 	if (need_resched()) {
 		local_irq_enable();
-		return 0;
+		return;
 	}
 
 	/*
@@ -177,8 +176,6 @@ static int cpuidle_idle_call(void)
 
 	rcu_idle_exit();
 	start_critical_timings();
-
-	return 0;
 }
 
 /*

commit 37352273ad48f2d177ed1b06ced32d5536b773fb
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 11 13:55:48 2014 +0200

    sched/idle: Reflow cpuidle_idle_call()
    
    Apply goto to reduce lines and nesting levels.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-cc6vb0snt3sr7op6rlbfeqfh@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index ed67f0cd2906..88a6bc43738b 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -73,7 +73,7 @@ static int cpuidle_idle_call(void)
 {
 	struct cpuidle_device *dev = __this_cpu_read(cpuidle_devices);
 	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
-	int next_state, entered_state, ret;
+	int next_state, entered_state;
 	bool broadcast;
 
 	/*
@@ -102,90 +102,75 @@ static int cpuidle_idle_call(void)
 	 * Check if the cpuidle framework is ready, otherwise fallback
 	 * to the default arch specific idle method
 	 */
-	ret = cpuidle_enabled(drv, dev);
-
-	if (!ret) {
-		/*
-		 * Ask the governor to choose an idle state it thinks
-		 * it is convenient to go to. There is *always* a
-		 * convenient idle state
-		 */
-		next_state = cpuidle_select(drv, dev);
-
+	if (cpuidle_enabled(drv, dev)) {
+use_default:
 		/*
-		 * The idle task must be scheduled, it is pointless to
-		 * go to idle, just update no idle residency and get
-		 * out of this function
+		 * We can't use the cpuidle framework, let's use the default
+		 * idle routine.
 		 */
-		if (current_clr_polling_and_test()) {
-			dev->last_residency = 0;
-			entered_state = next_state;
+		if (current_clr_polling_and_test())
 			local_irq_enable();
-		} else {
-			broadcast = !!(drv->states[next_state].flags &
-				       CPUIDLE_FLAG_TIMER_STOP);
-
-			if (broadcast) {
-				/*
-				 * Tell the time framework to switch
-				 * to a broadcast timer because our
-				 * local timer will be shutdown. If a
-				 * local timer is used from another
-				 * cpu as a broadcast timer, this call
-				 * may fail if it is not available
-				 */
-				ret = clockevents_notify(
-					CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
-					&dev->cpu);
-			}
-
-			if (!ret) {
-				trace_cpu_idle_rcuidle(next_state, dev->cpu);
-
-				/*
-				 * Enter the idle state previously
-				 * returned by the governor
-				 * decision. This function will block
-				 * until an interrupt occurs and will
-				 * take care of re-enabling the local
-				 * interrupts
-				 */
-				entered_state = cpuidle_enter(drv, dev,
-							      next_state);
-
-				trace_cpu_idle_rcuidle(PWR_EVENT_EXIT,
-						       dev->cpu);
-
-				if (broadcast)
-					clockevents_notify(
-						CLOCK_EVT_NOTIFY_BROADCAST_EXIT,
-						&dev->cpu);
-
-				/*
-				 * Give the governor an opportunity to reflect on the
-				 * outcome
-				 */
-				cpuidle_reflect(dev, entered_state);
-			}
-		}
+		else
+			arch_cpu_idle();
+
+		goto exit_idle;
 	}
 
 	/*
-	 * We can't use the cpuidle framework, let's use the default
-	 * idle routine
+	 * Ask the governor to choose an idle state it thinks
+	 * it is convenient to go to. There is *always* a
+	 * convenient idle state
 	 */
-	if (ret) {
-		if (!current_clr_polling_and_test())
-			arch_cpu_idle();
-		else
-			local_irq_enable();
+	next_state = cpuidle_select(drv, dev);
+
+	/*
+	 * The idle task must be scheduled, it is pointless to
+	 * go to idle, just update no idle residency and get
+	 * out of this function
+	 */
+	if (current_clr_polling_and_test()) {
+		dev->last_residency = 0;
+		entered_state = next_state;
+		local_irq_enable();
+		goto exit_idle;
 	}
 
+	broadcast = !!(drv->states[next_state].flags & CPUIDLE_FLAG_TIMER_STOP);
+
+	/*
+	 * Tell the time framework to switch to a broadcast timer
+	 * because our local timer will be shutdown. If a local timer
+	 * is used from another cpu as a broadcast timer, this call may
+	 * fail if it is not available
+	 */
+	if (broadcast &&
+	    clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &dev->cpu))
+		goto use_default;
+
+	trace_cpu_idle_rcuidle(next_state, dev->cpu);
+
+	/*
+	 * Enter the idle state previously returned by the governor decision.
+	 * This function will block until an interrupt occurs and will take
+	 * care of re-enabling the local interrupts
+	 */
+	entered_state = cpuidle_enter(drv, dev, next_state);
+
+	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, dev->cpu);
+
+	if (broadcast)
+		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &dev->cpu);
+
+	/*
+	 * Give the governor an opportunity to reflect on the outcome
+	 */
+	cpuidle_reflect(dev, entered_state);
+
+exit_idle:
 	__current_set_polling();
 
 	/*
-	 * It is up to the idle functions to enable back the local
-	 * interrupt
+	 * It is up to the idle functions to reenable local interrupts
 	 */
 	if (WARN_ON_ONCE(irqs_disabled()))
 		local_irq_enable();

commit c444117f0f39d59733ec23da67c44424df529230
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Apr 11 13:47:16 2014 +0200

    sched/idle: Delay clearing the polling bit
    
    With the generic idle functions assuming !polling we should only clear
    the polling bit at the very last opportunity in order to avoid
    spurious IPIs.
    
    Ideally we'd flip the default to polling, but that means auditing all
    arch idle functions.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-vq7719foqzf6z5h4j7eh7f9e@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 8f4390a079c7..ed67f0cd2906 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -78,12 +78,10 @@ static int cpuidle_idle_call(void)
 
 	/*
 	 * Check if the idle task must be rescheduled. If it is the
-	 * case, exit the function after re-enabling the local irq and
-	 * set again the polling flag
+	 * case, exit the function after re-enabling the local irq.
 	 */
-	if (current_clr_polling_and_test()) {
+	if (need_resched()) {
 		local_irq_enable();
-		__current_set_polling();
 		return 0;
 	}
 
@@ -127,7 +125,7 @@ static int cpuidle_idle_call(void)
 			broadcast = !!(drv->states[next_state].flags &
 				       CPUIDLE_FLAG_TIMER_STOP);
 
-			if (broadcast)
+			if (broadcast) {
 				/*
 				 * Tell the time framework to switch
 				 * to a broadcast timer because our
@@ -139,6 +137,7 @@ static int cpuidle_idle_call(void)
 				ret = clockevents_notify(
 					CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
 					&dev->cpu);
+			}
 
 			if (!ret) {
 				trace_cpu_idle_rcuidle(next_state, dev->cpu);
@@ -175,8 +174,12 @@ static int cpuidle_idle_call(void)
 	 * We can't use the cpuidle framework, let's use the default
 	 * idle routine
 	 */
-	if (ret)
-		arch_cpu_idle();
+	if (ret) {
+		if (!current_clr_polling_and_test())
+			arch_cpu_idle();
+		else
+			local_irq_enable();
+	}
 
 	__current_set_polling();
 

commit 52c324f8a87b336496d0f5e9d8dff1aa32bb08cd
Author: Rafael J. Wysocki <rafael.j.wysocki@intel.com>
Date:   Thu May 1 00:13:47 2014 +0200

    cpuidle: Combine cpuidle_enabled() with cpuidle_select()
    
    Since both cpuidle_enabled() and cpuidle_select() are only called by
    cpuidle_idle_call(), it is not really useful to keep them separate
    and combining them will help to avoid complicating cpuidle_idle_call()
    even further if governors are changed to return error codes sometimes.
    
    This code modification shouldn't lead to any functional changes.
    
    Signed-off-by: Rafael J. Wysocki <rafael.j.wysocki@intel.com>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 8f4390a079c7..a8f12247ce7c 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -101,19 +101,13 @@ static int cpuidle_idle_call(void)
 	rcu_idle_enter();
 
 	/*
-	 * Check if the cpuidle framework is ready, otherwise fallback
-	 * to the default arch specific idle method
+	 * Ask the cpuidle framework to choose a convenient idle state.
+	 * Fall back to the default arch specific idle method on errors.
 	 */
-	ret = cpuidle_enabled(drv, dev);
-
-	if (!ret) {
-		/*
-		 * Ask the governor to choose an idle state it thinks
-		 * it is convenient to go to. There is *always* a
-		 * convenient idle state
-		 */
-		next_state = cpuidle_select(drv, dev);
+	next_state = cpuidle_select(drv, dev);
 
+	ret = next_state;
+	if (ret >= 0) {
 		/*
 		 * The idle task must be scheduled, it is pointless to
 		 * go to idle, just update no idle residency and get
@@ -140,7 +134,7 @@ static int cpuidle_idle_call(void)
 					CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
 					&dev->cpu);
 
-			if (!ret) {
+			if (ret >= 0) {
 				trace_cpu_idle_rcuidle(next_state, dev->cpu);
 
 				/*
@@ -175,7 +169,7 @@ static int cpuidle_idle_call(void)
 	 * We can't use the cpuidle framework, let's use the default
 	 * idle routine
 	 */
-	if (ret)
+	if (ret < 0)
 		arch_cpu_idle();
 
 	__current_set_polling();

commit a1d028bd6d2b7789d15eddfd07c5bea2aaf36040
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Mar 3 08:48:54 2014 +0100

    sched/idle: Add more comments to the code
    
    The idle main function is a complex and a critical function. Added more
    comments to the code.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: tglx@linutronix.de
    Cc: rjw@rjwysocki.net
    Cc: preeti@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1393832934-11625-5-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index cc7a6f3801ff..8f4390a079c7 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -76,21 +76,49 @@ static int cpuidle_idle_call(void)
 	int next_state, entered_state, ret;
 	bool broadcast;
 
+	/*
+	 * Check if the idle task must be rescheduled. If it is the
+	 * case, exit the function after re-enabling the local irq and
+	 * set again the polling flag
+	 */
 	if (current_clr_polling_and_test()) {
 		local_irq_enable();
 		__current_set_polling();
 		return 0;
 	}
 
+	/*
+	 * During the idle period, stop measuring the disabled irqs
+	 * critical sections latencies
+	 */
 	stop_critical_timings();
+
+	/*
+	 * Tell the RCU framework we are entering an idle section,
+	 * so no more rcu read side critical sections and one more
+	 * step to the grace period
+	 */
 	rcu_idle_enter();
 
+	/*
+	 * Check if the cpuidle framework is ready, otherwise fallback
+	 * to the default arch specific idle method
+	 */
 	ret = cpuidle_enabled(drv, dev);
 
 	if (!ret) {
-		/* ask the governor for the next state */
+		/*
+		 * Ask the governor to choose an idle state it thinks
+		 * it is convenient to go to. There is *always* a
+		 * convenient idle state
+		 */
 		next_state = cpuidle_select(drv, dev);
 
+		/*
+		 * The idle task must be scheduled, it is pointless to
+		 * go to idle, just update no idle residency and get
+		 * out of this function
+		 */
 		if (current_clr_polling_and_test()) {
 			dev->last_residency = 0;
 			entered_state = next_state;
@@ -100,6 +128,14 @@ static int cpuidle_idle_call(void)
 				       CPUIDLE_FLAG_TIMER_STOP);
 
 			if (broadcast)
+				/*
+				 * Tell the time framework to switch
+				 * to a broadcast timer because our
+				 * local timer will be shutdown. If a
+				 * local timer is used from another
+				 * cpu as a broadcast timer, this call
+				 * may fail if it is not available
+				 */
 				ret = clockevents_notify(
 					CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
 					&dev->cpu);
@@ -107,6 +143,14 @@ static int cpuidle_idle_call(void)
 			if (!ret) {
 				trace_cpu_idle_rcuidle(next_state, dev->cpu);
 
+				/*
+				 * Enter the idle state previously
+				 * returned by the governor
+				 * decision. This function will block
+				 * until an interrupt occurs and will
+				 * take care of re-enabling the local
+				 * interrupts
+				 */
 				entered_state = cpuidle_enter(drv, dev,
 							      next_state);
 
@@ -118,17 +162,28 @@ static int cpuidle_idle_call(void)
 						CLOCK_EVT_NOTIFY_BROADCAST_EXIT,
 						&dev->cpu);
 
-				/* give the governor an opportunity to reflect on the outcome */
+				/*
+				 * Give the governor an opportunity to reflect on the
+				 * outcome
+				 */
 				cpuidle_reflect(dev, entered_state);
 			}
 		}
 	}
 
+	/*
+	 * We can't use the cpuidle framework, let's use the default
+	 * idle routine
+	 */
 	if (ret)
 		arch_cpu_idle();
 
 	__current_set_polling();
 
+	/*
+	 * It is up to the idle functions to enable back the local
+	 * interrupt
+	 */
 	if (WARN_ON_ONCE(irqs_disabled()))
 		local_irq_enable();
 

commit 8ca3c6424f4988fc19ed1067b121fbaf2e884d77
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Mar 3 08:48:53 2014 +0100

    sched/idle: Move idle conditions in cpuidle_idle main function
    
    This patch moves the condition before entering idle into the cpuidle main
    function located in idle.c. That simplify the idle mainloop functions and
    increase the readibility of the conditions to enter truly idle.
    
    This patch is code reorganization and does not change the behavior of the
    function.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: tglx@linutronix.de
    Cc: rjw@rjwysocki.net
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1393832934-11625-4-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index dc8a2466418f..cc7a6f3801ff 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -76,44 +76,59 @@ static int cpuidle_idle_call(void)
 	int next_state, entered_state, ret;
 	bool broadcast;
 
+	if (current_clr_polling_and_test()) {
+		local_irq_enable();
+		__current_set_polling();
+		return 0;
+	}
+
 	stop_critical_timings();
 	rcu_idle_enter();
 
 	ret = cpuidle_enabled(drv, dev);
-	if (ret < 0) {
-		arch_cpu_idle();
-		goto out;
-	}
 
-	/* ask the governor for the next state */
-	next_state = cpuidle_select(drv, dev);
+	if (!ret) {
+		/* ask the governor for the next state */
+		next_state = cpuidle_select(drv, dev);
 
-	if (need_resched()) {
-		dev->last_residency = 0;
-		/* give the governor an opportunity to reflect on the outcome */
-		cpuidle_reflect(dev, next_state);
-		local_irq_enable();
-		goto out;
-	}
+		if (current_clr_polling_and_test()) {
+			dev->last_residency = 0;
+			entered_state = next_state;
+			local_irq_enable();
+		} else {
+			broadcast = !!(drv->states[next_state].flags &
+				       CPUIDLE_FLAG_TIMER_STOP);
+
+			if (broadcast)
+				ret = clockevents_notify(
+					CLOCK_EVT_NOTIFY_BROADCAST_ENTER,
+					&dev->cpu);
 
-	broadcast = !!(drv->states[next_state].flags & CPUIDLE_FLAG_TIMER_STOP);
+			if (!ret) {
+				trace_cpu_idle_rcuidle(next_state, dev->cpu);
 
-	if (broadcast &&
-	    clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &dev->cpu))
-		return -EBUSY;
+				entered_state = cpuidle_enter(drv, dev,
+							      next_state);
 
-	trace_cpu_idle_rcuidle(next_state, dev->cpu);
+				trace_cpu_idle_rcuidle(PWR_EVENT_EXIT,
+						       dev->cpu);
 
-	entered_state = cpuidle_enter(drv, dev, next_state);
+				if (broadcast)
+					clockevents_notify(
+						CLOCK_EVT_NOTIFY_BROADCAST_EXIT,
+						&dev->cpu);
 
-	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, dev->cpu);
+				/* give the governor an opportunity to reflect on the outcome */
+				cpuidle_reflect(dev, entered_state);
+			}
+		}
+	}
+
+	if (ret)
+		arch_cpu_idle();
 
-	if (broadcast)
-		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &dev->cpu);
+	__current_set_polling();
 
-	/* give the governor an opportunity to reflect on the outcome */
-	cpuidle_reflect(dev, entered_state);
-out:
 	if (WARN_ON_ONCE(irqs_disabled()))
 		local_irq_enable();
 
@@ -150,16 +165,11 @@ static void cpu_idle_loop(void)
 			 * know that the IPI is going to arrive right
 			 * away
 			 */
-			if (cpu_idle_force_poll || tick_check_broadcast_expired()) {
+			if (cpu_idle_force_poll || tick_check_broadcast_expired())
 				cpu_idle_poll();
-			} else {
-				if (!current_clr_polling_and_test()) {
-					cpuidle_idle_call();
-				} else {
-					local_irq_enable();
-				}
-				__current_set_polling();
-			}
+			else
+				cpuidle_idle_call();
+
 			arch_cpu_idle_exit();
 		}
 

commit c8cc7d4de7a4f2fb1f8774ec2de5b49c46c42e64
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Mar 3 08:48:52 2014 +0100

    sched/idle: Reorganize the idle loop
    
    Now that we have the main cpuidle function in idle.c, move some code from
    the idle mainloop to this function for the sake of clarity.
    
    That removes if then else indentation difficult to follow when looking at the
    code. This patch does not change the current behavior.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: tglx@linutronix.de
    Cc: rjw@rjwysocki.net
    Cc: preeti@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1393832934-11625-3-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index d5aaf5eb4531..dc8a2466418f 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -63,7 +63,6 @@ void __weak arch_cpu_idle(void)
 	local_irq_enable();
 }
 
-#ifdef CONFIG_CPU_IDLE
 /**
  * cpuidle_idle_call - the main idle function
  *
@@ -77,9 +76,14 @@ static int cpuidle_idle_call(void)
 	int next_state, entered_state, ret;
 	bool broadcast;
 
+	stop_critical_timings();
+	rcu_idle_enter();
+
 	ret = cpuidle_enabled(drv, dev);
-	if (ret < 0)
-		return ret;
+	if (ret < 0) {
+		arch_cpu_idle();
+		goto out;
+	}
 
 	/* ask the governor for the next state */
 	next_state = cpuidle_select(drv, dev);
@@ -89,7 +93,7 @@ static int cpuidle_idle_call(void)
 		/* give the governor an opportunity to reflect on the outcome */
 		cpuidle_reflect(dev, next_state);
 		local_irq_enable();
-		return 0;
+		goto out;
 	}
 
 	broadcast = !!(drv->states[next_state].flags & CPUIDLE_FLAG_TIMER_STOP);
@@ -109,15 +113,15 @@ static int cpuidle_idle_call(void)
 
 	/* give the governor an opportunity to reflect on the outcome */
 	cpuidle_reflect(dev, entered_state);
+out:
+	if (WARN_ON_ONCE(irqs_disabled()))
+		local_irq_enable();
+
+	rcu_idle_exit();
+	start_critical_timings();
 
 	return 0;
 }
-#else
-static inline int cpuidle_idle_call(void)
-{
-	return -ENODEV;
-}
-#endif
 
 /*
  * Generic idle loop implementation
@@ -150,14 +154,7 @@ static void cpu_idle_loop(void)
 				cpu_idle_poll();
 			} else {
 				if (!current_clr_polling_and_test()) {
-					stop_critical_timings();
-					rcu_idle_enter();
-					if (cpuidle_idle_call())
-						arch_cpu_idle();
-					if (WARN_ON_ONCE(irqs_disabled()))
-						local_irq_enable();
-					rcu_idle_exit();
-					start_critical_timings();
+					cpuidle_idle_call();
 				} else {
 					local_irq_enable();
 				}

commit 30cdd69e2a266505ca8229c944d361ff350a6959
Author: Daniel Lezcano <daniel.lezcano@linaro.org>
Date:   Mon Mar 3 08:48:51 2014 +0100

    cpuidle/idle: Move the cpuidle_idle_call function to idle.c
    
    The cpuidle_idle_call does nothing more than calling the three individuals
    function and is no longer used by any arch specific code but only in the
    cpuidle framework code.
    
    We can move this function into the idle task code to ensure better
    proximity to the scheduler code.
    
    Signed-off-by: Daniel Lezcano <daniel.lezcano@linaro.org>
    Acked-by: Nicolas Pitre <nicolas.pitre@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: rjw@rjwysocki.net
    Cc: preeti@linux.vnet.ibm.com
    Link: http://lkml.kernel.org/r/1393832934-11625-2-git-send-email-daniel.lezcano@linaro.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index b7976a127178..d5aaf5eb4531 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -63,6 +63,62 @@ void __weak arch_cpu_idle(void)
 	local_irq_enable();
 }
 
+#ifdef CONFIG_CPU_IDLE
+/**
+ * cpuidle_idle_call - the main idle function
+ *
+ * NOTE: no locks or semaphores should be used here
+ * return non-zero on failure
+ */
+static int cpuidle_idle_call(void)
+{
+	struct cpuidle_device *dev = __this_cpu_read(cpuidle_devices);
+	struct cpuidle_driver *drv = cpuidle_get_cpu_driver(dev);
+	int next_state, entered_state, ret;
+	bool broadcast;
+
+	ret = cpuidle_enabled(drv, dev);
+	if (ret < 0)
+		return ret;
+
+	/* ask the governor for the next state */
+	next_state = cpuidle_select(drv, dev);
+
+	if (need_resched()) {
+		dev->last_residency = 0;
+		/* give the governor an opportunity to reflect on the outcome */
+		cpuidle_reflect(dev, next_state);
+		local_irq_enable();
+		return 0;
+	}
+
+	broadcast = !!(drv->states[next_state].flags & CPUIDLE_FLAG_TIMER_STOP);
+
+	if (broadcast &&
+	    clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &dev->cpu))
+		return -EBUSY;
+
+	trace_cpu_idle_rcuidle(next_state, dev->cpu);
+
+	entered_state = cpuidle_enter(drv, dev, next_state);
+
+	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, dev->cpu);
+
+	if (broadcast)
+		clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &dev->cpu);
+
+	/* give the governor an opportunity to reflect on the outcome */
+	cpuidle_reflect(dev, entered_state);
+
+	return 0;
+}
+#else
+static inline int cpuidle_idle_call(void)
+{
+	return -ENODEV;
+}
+#endif
+
 /*
  * Generic idle loop implementation
  */

commit 06d50c65b1043b166d102accc081093f79d8f7e5
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Feb 24 18:22:07 2014 +0100

    sched/idle: Remove stale old file
    
    Commit cf37b6b48428d ("sched/idle: Move cpu/idle.c to sched/idle.c")
    said to simply move a file; somehow it got mangled and created an old
    version of the file and forgot to remove the old file.
    
    Fix this fail; add the lost change and remove the now identical old
    file.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: rjw@rjwysocki.net
    Cc: nicolas.pitre@linaro.org
    Cc: preeti@linux.vnet.ibm.com
    Cc: Daniel Lezcano <daniel.lezcano@linaro.org>
    Link: http://lkml.kernel.org/r/20140224172207.GC9987@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
index 14ca43430aee..b7976a127178 100644
--- a/kernel/sched/idle.c
+++ b/kernel/sched/idle.c
@@ -108,14 +108,17 @@ static void cpu_idle_loop(void)
 				__current_set_polling();
 			}
 			arch_cpu_idle_exit();
-			/*
-			 * We need to test and propagate the TIF_NEED_RESCHED
-			 * bit here because we might not have send the
-			 * reschedule IPI to idle tasks.
-			 */
-			if (tif_need_resched())
-				set_preempt_need_resched();
 		}
+
+		/*
+		 * Since we fell out of the loop above, we know
+		 * TIF_NEED_RESCHED must be set, propagate it into
+		 * PREEMPT_NEED_RESCHED.
+		 *
+		 * This is required because for polling idle loops we will
+		 * not have had an IPI to fold the state for us.
+		 */
+		preempt_set_need_resched();
 		tick_nohz_idle_exit();
 		schedule_preempt_disabled();
 	}

commit cf37b6b48428d6be8f8762b3599d529c44644fb2
Author: Nicolas Pitre <nicolas.pitre@linaro.org>
Date:   Sun Jan 26 23:42:01 2014 -0500

    sched/idle: Move cpu/idle.c to sched/idle.c
    
    Integration of cpuidle with the scheduler requires that the idle loop be
    closely integrated with the scheduler proper. Moving cpu/idle.c into the
    sched directory will allow for a smoother integration, and eliminate a
    subdirectory which contained only one source file.
    
    Signed-off-by: Nicolas Pitre <nico@linaro.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/alpine.LFD.2.11.1401301102210.1652@knanqh.ubzr
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/idle.c b/kernel/sched/idle.c
new file mode 100644
index 000000000000..14ca43430aee
--- /dev/null
+++ b/kernel/sched/idle.c
@@ -0,0 +1,144 @@
+/*
+ * Generic entry point for the idle threads
+ */
+#include <linux/sched.h>
+#include <linux/cpu.h>
+#include <linux/cpuidle.h>
+#include <linux/tick.h>
+#include <linux/mm.h>
+#include <linux/stackprotector.h>
+
+#include <asm/tlb.h>
+
+#include <trace/events/power.h>
+
+static int __read_mostly cpu_idle_force_poll;
+
+void cpu_idle_poll_ctrl(bool enable)
+{
+	if (enable) {
+		cpu_idle_force_poll++;
+	} else {
+		cpu_idle_force_poll--;
+		WARN_ON_ONCE(cpu_idle_force_poll < 0);
+	}
+}
+
+#ifdef CONFIG_GENERIC_IDLE_POLL_SETUP
+static int __init cpu_idle_poll_setup(char *__unused)
+{
+	cpu_idle_force_poll = 1;
+	return 1;
+}
+__setup("nohlt", cpu_idle_poll_setup);
+
+static int __init cpu_idle_nopoll_setup(char *__unused)
+{
+	cpu_idle_force_poll = 0;
+	return 1;
+}
+__setup("hlt", cpu_idle_nopoll_setup);
+#endif
+
+static inline int cpu_idle_poll(void)
+{
+	rcu_idle_enter();
+	trace_cpu_idle_rcuidle(0, smp_processor_id());
+	local_irq_enable();
+	while (!tif_need_resched())
+		cpu_relax();
+	trace_cpu_idle_rcuidle(PWR_EVENT_EXIT, smp_processor_id());
+	rcu_idle_exit();
+	return 1;
+}
+
+/* Weak implementations for optional arch specific functions */
+void __weak arch_cpu_idle_prepare(void) { }
+void __weak arch_cpu_idle_enter(void) { }
+void __weak arch_cpu_idle_exit(void) { }
+void __weak arch_cpu_idle_dead(void) { }
+void __weak arch_cpu_idle(void)
+{
+	cpu_idle_force_poll = 1;
+	local_irq_enable();
+}
+
+/*
+ * Generic idle loop implementation
+ */
+static void cpu_idle_loop(void)
+{
+	while (1) {
+		tick_nohz_idle_enter();
+
+		while (!need_resched()) {
+			check_pgt_cache();
+			rmb();
+
+			if (cpu_is_offline(smp_processor_id()))
+				arch_cpu_idle_dead();
+
+			local_irq_disable();
+			arch_cpu_idle_enter();
+
+			/*
+			 * In poll mode we reenable interrupts and spin.
+			 *
+			 * Also if we detected in the wakeup from idle
+			 * path that the tick broadcast device expired
+			 * for us, we don't want to go deep idle as we
+			 * know that the IPI is going to arrive right
+			 * away
+			 */
+			if (cpu_idle_force_poll || tick_check_broadcast_expired()) {
+				cpu_idle_poll();
+			} else {
+				if (!current_clr_polling_and_test()) {
+					stop_critical_timings();
+					rcu_idle_enter();
+					if (cpuidle_idle_call())
+						arch_cpu_idle();
+					if (WARN_ON_ONCE(irqs_disabled()))
+						local_irq_enable();
+					rcu_idle_exit();
+					start_critical_timings();
+				} else {
+					local_irq_enable();
+				}
+				__current_set_polling();
+			}
+			arch_cpu_idle_exit();
+			/*
+			 * We need to test and propagate the TIF_NEED_RESCHED
+			 * bit here because we might not have send the
+			 * reschedule IPI to idle tasks.
+			 */
+			if (tif_need_resched())
+				set_preempt_need_resched();
+		}
+		tick_nohz_idle_exit();
+		schedule_preempt_disabled();
+	}
+}
+
+void cpu_startup_entry(enum cpuhp_state state)
+{
+	/*
+	 * This #ifdef needs to die, but it's too late in the cycle to
+	 * make this generic (arm and sh have never invoked the canary
+	 * init for the non boot cpus!). Will be fixed in 3.11
+	 */
+#ifdef CONFIG_X86
+	/*
+	 * If we're the non-boot CPU, nothing set the stack canary up
+	 * for us. The boot CPU already has it initialized but no harm
+	 * in doing it again. This is a good place for updating it, as
+	 * we wont ever return from this function (so the invalid
+	 * canaries already on the stack wont ever trigger).
+	 */
+	boot_init_stack_canary();
+#endif
+	__current_set_polling();
+	arch_cpu_idle_prepare();
+	cpu_idle_loop();
+}
