commit 8bf6c677ddb9c922423ea3bf494fe7c508bfbb8c
Author: Sebastian Siewior <bigeasy@linutronix.de>
Date:   Mon Mar 23 16:20:19 2020 +0100

    completion: Use lockdep_assert_RT_in_threaded_ctx() in complete_all()
    
    The warning was intended to spot complete_all() users from hardirq
    context on PREEMPT_RT. The warning as-is will also trigger in interrupt
    handlers, which are threaded on PREEMPT_RT, which was not intended.
    
    Use lockdep_assert_RT_in_threaded_ctx() which triggers in non-preemptive
    context on PREEMPT_RT.
    
    Fixes: a5c6234e1028 ("completion: Use simple wait queues")
    Reported-by: kernel test robot <rong.a.chen@intel.com>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: https://lkml.kernel.org/r/20200323152019.4qjwluldohuh3by5@linutronix.de

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index f15e96164ff1..a778554f9dad 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -58,7 +58,7 @@ void complete_all(struct completion *x)
 {
 	unsigned long flags;
 
-	WARN_ON(irqs_disabled());
+	lockdep_assert_RT_in_threaded_ctx();
 
 	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	x->done = UINT_MAX;

commit a5c6234e10280b3ec65e2410ce34904a2580e5f8
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sat Mar 21 12:26:00 2020 +0100

    completion: Use simple wait queues
    
    completion uses a wait_queue_head_t to enqueue waiters.
    
    wait_queue_head_t contains a spinlock_t to protect the list of waiters
    which excludes it from being used in truly atomic context on a PREEMPT_RT
    enabled kernel.
    
    The spinlock in the wait queue head cannot be replaced by a raw_spinlock
    because:
    
      - wait queues can have custom wakeup callbacks, which acquire other
        spinlock_t locks and have potentially long execution times
    
      - wake_up() walks an unbounded number of list entries during the wake up
        and may wake an unbounded number of waiters.
    
    For simplicity and performance reasons complete() should be usable on
    PREEMPT_RT enabled kernels.
    
    completions do not use custom wakeup callbacks and are usually single
    waiter, except for a few corner cases.
    
    Replace the wait queue in the completion with a simple wait queue (swait),
    which uses a raw_spinlock_t for protecting the waiter list and therefore is
    safe to use inside truly atomic regions on PREEMPT_RT.
    
    There is no semantical or functional change:
    
      - completions use the exclusive wait mode which is what swait provides
    
      - complete() wakes one exclusive waiter
    
      - complete_all() wakes all waiters while holding the lock which protects
        the wait queue against newly incoming waiters. The conversion to swait
        preserves this behaviour.
    
    complete_all() might cause unbound latencies with a large number of waiters
    being woken at once, but most complete_all() usage sites are either in
    testing or initialization code or have only a really small number of
    concurrent waiters which for now does not cause a latency problem. Keep it
    simple for now.
    
    The fixup of the warning check in the USB gadget driver is just a straight
    forward conversion of the lockless waiter check from one waitqueue type to
    the other.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Reviewed-by: Davidlohr Bueso <dbueso@suse.de>
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Link: https://lkml.kernel.org/r/20200321113242.317954042@linutronix.de

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index a1ad5b7d5521..f15e96164ff1 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -29,12 +29,12 @@ void complete(struct completion *x)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 
 	if (x->done != UINT_MAX)
 		x->done++;
-	__wake_up_locked(&x->wait, TASK_NORMAL, 1);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	swake_up_locked(&x->wait);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 }
 EXPORT_SYMBOL(complete);
 
@@ -58,10 +58,12 @@ void complete_all(struct completion *x)
 {
 	unsigned long flags;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	WARN_ON(irqs_disabled());
+
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	x->done = UINT_MAX;
-	__wake_up_locked(&x->wait, TASK_NORMAL, 0);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	swake_up_all_locked(&x->wait);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 }
 EXPORT_SYMBOL(complete_all);
 
@@ -70,20 +72,20 @@ do_wait_for_common(struct completion *x,
 		   long (*action)(long), long timeout, int state)
 {
 	if (!x->done) {
-		DECLARE_WAITQUEUE(wait, current);
+		DECLARE_SWAITQUEUE(wait);
 
-		__add_wait_queue_entry_tail_exclusive(&x->wait, &wait);
 		do {
 			if (signal_pending_state(state, current)) {
 				timeout = -ERESTARTSYS;
 				break;
 			}
+			__prepare_to_swait(&x->wait, &wait);
 			__set_current_state(state);
-			spin_unlock_irq(&x->wait.lock);
+			raw_spin_unlock_irq(&x->wait.lock);
 			timeout = action(timeout);
-			spin_lock_irq(&x->wait.lock);
+			raw_spin_lock_irq(&x->wait.lock);
 		} while (!x->done && timeout);
-		__remove_wait_queue(&x->wait, &wait);
+		__finish_swait(&x->wait, &wait);
 		if (!x->done)
 			return timeout;
 	}
@@ -100,9 +102,9 @@ __wait_for_common(struct completion *x,
 
 	complete_acquire(x);
 
-	spin_lock_irq(&x->wait.lock);
+	raw_spin_lock_irq(&x->wait.lock);
 	timeout = do_wait_for_common(x, action, timeout, state);
-	spin_unlock_irq(&x->wait.lock);
+	raw_spin_unlock_irq(&x->wait.lock);
 
 	complete_release(x);
 
@@ -291,12 +293,12 @@ bool try_wait_for_completion(struct completion *x)
 	if (!READ_ONCE(x->done))
 		return false;
 
-	spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
 	if (!x->done)
 		ret = false;
 	else if (x->done != UINT_MAX)
 		x->done--;
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 	return ret;
 }
 EXPORT_SYMBOL(try_wait_for_completion);
@@ -322,8 +324,8 @@ bool completion_done(struct completion *x)
 	 * otherwise we can end up freeing the completion before complete()
 	 * is done referencing it.
 	 */
-	spin_lock_irqsave(&x->wait.lock, flags);
-	spin_unlock_irqrestore(&x->wait.lock, flags);
+	raw_spin_lock_irqsave(&x->wait.lock, flags);
+	raw_spin_unlock_irqrestore(&x->wait.lock, flags);
 	return true;
 }
 EXPORT_SYMBOL(completion_done);

commit 7696f9910a9a40b8a952f57d3428515fabd2d889
Author: Andrea Parri <andrea.parri@amarulasolutions.com>
Date:   Mon Jul 16 11:06:03 2018 -0700

    sched/Documentation: Update wake_up() & co. memory-barrier guarantees
    
    Both the implementation and the users' expectation [1] for the various
    wakeup primitives have evolved over time, but the documentation has not
    kept up with these changes: brings it into 2018.
    
    [1] http://lkml.kernel.org/r/20180424091510.GB4064@hirez.programming.kicks-ass.net
    
    Also applied feedback from Alan Stern.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Andrea Parri <andrea.parri@amarulasolutions.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Akira Yokosawa <akiyks@gmail.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Boqun Feng <boqun.feng@gmail.com>
    Cc: Daniel Lustig <dlustig@nvidia.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Jade Alglave <j.alglave@ucl.ac.uk>
    Cc: Jonathan Corbet <corbet@lwn.net>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Luc Maranget <luc.maranget@inria.fr>
    Cc: Nicholas Piggin <npiggin@gmail.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: linux-arch@vger.kernel.org
    Cc: parri.andrea@gmail.com
    Link: http://lkml.kernel.org/r/20180716180605.16115-12-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index e426b0cb9ac6..a1ad5b7d5521 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -22,8 +22,8 @@
  *
  * See also complete_all(), wait_for_completion() and related routines.
  *
- * It may be assumed that this function implies a write memory barrier before
- * changing the task state if and only if any tasks are woken up.
+ * If this function wakes up a task, it executes a full memory barrier before
+ * accessing the task state.
  */
 void complete(struct completion *x)
 {
@@ -44,8 +44,8 @@ EXPORT_SYMBOL(complete);
  *
  * This will wake up all threads waiting on this particular completion event.
  *
- * It may be assumed that this function implies a write memory barrier before
- * changing the task state if and only if any tasks are woken up.
+ * If this function wakes up a task, it executes a full memory barrier before
+ * accessing the task state.
  *
  * Since complete_all() sets the completion of @x permanently to done
  * to allow multiple waiters to finish, a call to reinit_completion()

commit d17067e4487adc53bedb43681b3cb5a1714ff6ca
Author: gaurav jindal <gauravjindal1104@gmail.com>
Date:   Wed Feb 21 18:24:07 2018 +0530

    sched/completions: Use bool in try_wait_for_completion()
    
    Since the return type of the function is bool, the internal
    'ret' variable should be bool too.
    
    Signed-off-by: Gaurav Jindal<gauravjindal1104@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20180221125407.GA14292@gmail.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 5d2d56b0817a..e426b0cb9ac6 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -280,7 +280,7 @@ EXPORT_SYMBOL(wait_for_completion_killable_timeout);
 bool try_wait_for_completion(struct completion *x)
 {
 	unsigned long flags;
-	int ret = 1;
+	bool ret = true;
 
 	/*
 	 * Since x->done will need to be locked only
@@ -289,11 +289,11 @@ bool try_wait_for_completion(struct completion *x)
 	 * return early in the blocking case.
 	 */
 	if (!READ_ONCE(x->done))
-		return 0;
+		return false;
 
 	spin_lock_irqsave(&x->wait.lock, flags);
 	if (!x->done)
-		ret = 0;
+		ret = false;
 	else if (x->done != UINT_MAX)
 		x->done--;
 	spin_unlock_irqrestore(&x->wait.lock, flags);

commit 325ea10c0809406ce23f038602abbc454f3f761d
Author: Ingo Molnar <mingo@kernel.org>
Date:   Sat Mar 3 12:20:47 2018 +0100

    sched/headers: Simplify and clean up header usage in the scheduler
    
    Do the following cleanups and simplifications:
    
     - sched/sched.h already includes <asm/paravirt.h>, so no need to
       include it in sched/core.c again.
    
     - order the <linux/sched/*.h> headers alphabetically
    
     - add all <linux/sched/*.h> headers to kernel/sched/sched.h
    
     - remove all unnecessary includes from the .c files that
       are already included in kernel/sched/sched.h.
    
    Finally, make all scheduler .c files use a single common header:
    
      #include "sched.h"
    
    ... which now contains a union of the relied upon headers.
    
    This makes the various .c files easier to read and easier to handle.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 0926aef10dad..5d2d56b0817a 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -11,10 +11,7 @@
  * typically be used for exclusion which gives rise to priority inversion.
  * Waiting for completion is a typically sync point, but not an exclusion point.
  */
-
-#include <linux/sched/signal.h>
-#include <linux/sched/debug.h>
-#include <linux/completion.h>
+#include "sched.h"
 
 /**
  * complete: - signals a single thread waiting on this completion

commit 527187d28569e39c5d489d6306d3b79605cf85a6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Jan 8 17:27:19 2018 +0100

    locking/lockdep: Remove cross-release leftovers
    
    There's two cross-release leftover facilities:
    
     - the crossrelease_hist_*() irq-tracing callbacks (NOPs currently)
     - the complete_release_commit() callback (NOP as well)
    
    Remove them.
    
    Cc: David Sterba <dsterba@suse.com>
    Cc: Byungchul Park <byungchul.park@lge.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 2ddaec40956f..0926aef10dad 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -34,11 +34,6 @@ void complete(struct completion *x)
 
 	spin_lock_irqsave(&x->wait.lock, flags);
 
-	/*
-	 * Perform commit of crossrelease here.
-	 */
-	complete_release_commit(x);
-
 	if (x->done != UINT_MAX)
 		x->done++;
 	__wake_up_locked(&x->wait, TASK_NORMAL, 1);

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index cc873075c3bd..2ddaec40956f 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Generic wait-for-completion handler;
  *

commit 5f82e71a001d14824a7728ad9e49f6aea420f161
Merge: 6c51e67b64d1 edc2988c548d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 11:52:29 2017 -0700

    Merge branch 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull locking updates from Ingo Molnar:
    
     - Add 'cross-release' support to lockdep, which allows APIs like
       completions, where it's not the 'owner' who releases the lock, to be
       tracked. It's all activated automatically under
       CONFIG_PROVE_LOCKING=y.
    
     - Clean up (restructure) the x86 atomics op implementation to be more
       readable, in preparation of KASAN annotations. (Dmitry Vyukov)
    
     - Fix static keys (Paolo Bonzini)
    
     - Add killable versions of down_read() et al (Kirill Tkhai)
    
     - Rework and fix jump_label locking (Marc Zyngier, Paolo Bonzini)
    
     - Rework (and fix) tlb_flush_pending() barriers (Peter Zijlstra)
    
     - Remove smp_mb__before_spinlock() and convert its usages, introduce
       smp_mb__after_spinlock() (Peter Zijlstra)
    
    * 'locking-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (56 commits)
      locking/lockdep/selftests: Fix mixed read-write ABBA tests
      sched/completion: Avoid unnecessary stack allocation for COMPLETION_INITIALIZER_ONSTACK()
      acpi/nfit: Fix COMPLETION_INITIALIZER_ONSTACK() abuse
      locking/pvqspinlock: Relax cmpxchg's to improve performance on some architectures
      smp: Avoid using two cache lines for struct call_single_data
      locking/lockdep: Untangle xhlock history save/restore from task independence
      locking/refcounts, x86/asm: Disable CONFIG_ARCH_HAS_REFCOUNT for the time being
      futex: Remove duplicated code and fix undefined behaviour
      Documentation/locking/atomic: Finish the document...
      locking/lockdep: Fix workqueue crossrelease annotation
      workqueue/lockdep: 'Fix' flush_work() annotation
      locking/lockdep/selftests: Add mixed read-write ABBA tests
      mm, locking/barriers: Clarify tlb_flush_pending() barriers
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE and CONFIG_LOCKDEP_COMPLETIONS truly non-interactive
      locking/lockdep: Explicitly initialize wq_barrier::done::map
      locking/lockdep: Rename CONFIG_LOCKDEP_COMPLETE to CONFIG_LOCKDEP_COMPLETIONS
      locking/lockdep: Reword title of LOCKDEP_CROSSRELEASE config
      locking/lockdep: Make CONFIG_LOCKDEP_CROSSRELEASE part of CONFIG_PROVE_LOCKING
      locking/refcounts, x86/asm: Implement fast refcount overflow protection
      locking/lockdep: Fix the rollback and overwrite detection logic in crossrelease
      ...

commit f213a6c84c1b4b396a0713ee33cff0e02ba8235f
Merge: 621bee34f6ed bbdacdfed2f5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 4 09:10:24 2017 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
     "The main changes in this cycle were:
    
       - fix affine wakeups (Peter Zijlstra)
    
       - improve CPU onlining (and general bootup) scalability on systems
         with ridiculous number (thousands) of CPUs (Peter Zijlstra)
    
       - sched/numa updates (Rik van Riel)
    
       - sched/deadline updates (Byungchul Park)
    
       - sched/cpufreq enhancements and related cleanups (Viresh Kumar)
    
       - sched/debug enhancements (Xie XiuQi)
    
       - various fixes"
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (27 commits)
      sched/debug: Optimize sched_domain sysctl generation
      sched/topology: Avoid pointless rebuild
      sched/topology, cpuset: Avoid spurious/wrong domain rebuilds
      sched/topology: Improve comments
      sched/topology: Fix memory leak in __sdt_alloc()
      sched/completion: Document that reinit_completion() must be called after complete_all()
      sched/autogroup: Fix error reporting printk text in autogroup_create()
      sched/fair: Fix wake_affine() for !NUMA_BALANCING
      sched/debug: Intruduce task_state_to_char() helper function
      sched/debug: Show task state in /proc/sched_debug
      sched/debug: Use task_pid_nr_ns in /proc/$pid/sched
      sched/core: Remove unnecessary initialization init_idle_bootup_task()
      sched/deadline: Change return value of cpudl_find()
      sched/deadline: Make find_later_rq() choose a closer CPU in topology
      sched/numa: Scale scan period with tasks in group and shared/private
      sched/numa: Slow down scan rate if shared faults dominate
      sched/pelt: Fix false running accounting
      sched: Mark pick_next_task_dl() and build_sched_domain() as static
      sched/cpupri: Don't re-initialize 'struct cpupri'
      sched/deadline: Don't re-initialize 'struct cpudl'
      ...

commit dec13c42d21a96adc5d0d25510d0b59775dfd2ec
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 29 12:47:04 2017 -0700

    completion: Replace spin_unlock_wait() with lock/unlock pair
    
    There is no agreed-upon definition of spin_unlock_wait()'s semantics,
    and it appears that all callers could do just as well with a lock/unlock
    pair.  This commit therefore replaces the spin_unlock_wait() call in
    completion_done() with spin_lock() followed immediately by spin_unlock().
    This should be safe from a performance perspective because the lock
    will be held only the wakeup happens really quickly.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Andrea Parri <parri.andrea@gmail.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 13fc5ae9bf2f..c9524d2d9316 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -300,6 +300,8 @@ EXPORT_SYMBOL(try_wait_for_completion);
  */
 bool completion_done(struct completion *x)
 {
+	unsigned long flags;
+
 	if (!READ_ONCE(x->done))
 		return false;
 
@@ -307,14 +309,9 @@ bool completion_done(struct completion *x)
 	 * If ->done, we need to wait for complete() to release ->wait.lock
 	 * otherwise we can end up freeing the completion before complete()
 	 * is done referencing it.
-	 *
-	 * The RMB pairs with complete()'s RELEASE of ->wait.lock and orders
-	 * the loads of ->done and ->wait.lock such that we cannot observe
-	 * the lock before complete() acquires it while observing the ->done
-	 * after it's acquired the lock.
 	 */
-	smp_rmb();
-	spin_unlock_wait(&x->wait.lock);
+	spin_lock_irqsave(&x->wait.lock, flags);
+	spin_unlock_irqrestore(&x->wait.lock, flags);
 	return true;
 }
 EXPORT_SYMBOL(completion_done);

commit 9c8783201cb58e9af8ddeb0cc68f37b0a44ca16c
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Aug 16 13:12:02 2017 -0400

    sched/completion: Document that reinit_completion() must be called after complete_all()
    
    The complete_all() function modifies the completion's "done" variable to
    UINT_MAX, and no other caller (wait_for_completion(), etc) will modify
    it back to zero. That means that any call to complete_all() must have a
    reinit_completion() before that completion can be used again.
    
    Document this fact by the complete_all() function.
    
    Also document that completion_done() will always return true if
    complete_all() is called.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20170816131202.195c2f4b@gandalf.local.home
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 13fc5ae9bf2f..2950f446820d 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -47,6 +47,13 @@ EXPORT_SYMBOL(complete);
  *
  * It may be assumed that this function implies a write memory barrier before
  * changing the task state if and only if any tasks are woken up.
+ *
+ * Since complete_all() sets the completion of @x permanently to done
+ * to allow multiple waiters to finish, a call to reinit_completion()
+ * must be used on @x if @x is to be used again. The code must make
+ * sure that all waiters have woken and finished before reinitializing
+ * @x. Also note that the function completion_done() can not be used
+ * to know if there are still waiters after complete_all() has been called.
  */
 void complete_all(struct completion *x)
 {
@@ -297,6 +304,7 @@ EXPORT_SYMBOL(try_wait_for_completion);
  *	Return: 0 if there are waiters (wait_for_completion() in progress)
  *		 1 if there are no waiters.
  *
+ *	Note, this will always return true if complete_all() was called on @X.
  */
 bool completion_done(struct completion *x)
 {

commit cd8084f91c02c1afd256a39aa833bff737631304
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Aug 7 16:12:56 2017 +0900

    locking/lockdep: Apply crossrelease to completions
    
    Although wait_for_completion() and its family can cause deadlock, the
    lock correctness validator could not be applied to them until now,
    because things like complete() are usually called in a different context
    from the waiting context, which violates lockdep's assumption.
    
    Thanks to CONFIG_LOCKDEP_CROSSRELEASE, we can now apply the lockdep
    detector to those completion operations. Applied it.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: akpm@linux-foundation.org
    Cc: boqun.feng@gmail.com
    Cc: kernel-team@lge.com
    Cc: kirill@shutemov.name
    Cc: npiggin@gmail.com
    Cc: walken@google.com
    Cc: willy@infradead.org
    Link: http://lkml.kernel.org/r/1502089981-21272-10-git-send-email-byungchul.park@lge.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 13fc5ae9bf2f..566b6ec7b6fe 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -32,6 +32,12 @@ void complete(struct completion *x)
 	unsigned long flags;
 
 	spin_lock_irqsave(&x->wait.lock, flags);
+
+	/*
+	 * Perform commit of crossrelease here.
+	 */
+	complete_release_commit(x);
+
 	if (x->done != UINT_MAX)
 		x->done++;
 	__wake_up_locked(&x->wait, TASK_NORMAL, 1);
@@ -92,9 +98,14 @@ __wait_for_common(struct completion *x,
 {
 	might_sleep();
 
+	complete_acquire(x);
+
 	spin_lock_irq(&x->wait.lock);
 	timeout = do_wait_for_common(x, action, timeout, state);
 	spin_unlock_irq(&x->wait.lock);
+
+	complete_release(x);
+
 	return timeout;
 }
 

commit ac6424b981bce1c4bc55675c6ce11bfe1bbfa64f
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Jun 20 12:06:13 2017 +0200

    sched/wait: Rename wait_queue_t => wait_queue_entry_t
    
    Rename:
    
            wait_queue_t            =>      wait_queue_entry_t
    
    'wait_queue_t' was always a slight misnomer: its name implies that it's a "queue",
    but in reality it's a queue *entry*. The 'real' queue is the wait queue head,
    which had to carry the name.
    
    Start sorting this out by renaming it to 'wait_queue_entry_t'.
    
    This also allows the real structure name 'struct __wait_queue' to
    lose its double underscore and become 'struct wait_queue_entry',
    which is the more canonical nomenclature for such data types.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 53f9558fa925..13fc5ae9bf2f 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -66,7 +66,7 @@ do_wait_for_common(struct completion *x,
 	if (!x->done) {
 		DECLARE_WAITQUEUE(wait, current);
 
-		__add_wait_queue_tail_exclusive(&x->wait, &wait);
+		__add_wait_queue_entry_tail_exclusive(&x->wait, &wait);
 		do {
 			if (signal_pending_state(state, current)) {
 				timeout = -ERESTARTSYS;

commit b17b01533b719e9949e437abf66436a875739b40
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:35 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/debug.h>
    
    We are going to split <linux/sched/debug.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/debug.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index b294a8ee2842..53f9558fa925 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -12,6 +12,7 @@
  */
 
 #include <linux/sched/signal.h>
+#include <linux/sched/debug.h>
 #include <linux/completion.h>
 
 /**

commit 174cd4b1e5fbd0d74c68cf3a74f5bd4923485512
Author: Ingo Molnar <mingo@kernel.org>
Date:   Thu Feb 2 19:15:33 2017 +0100

    sched/headers: Prepare to move signal wakeup & sigpending methods from <linux/sched.h> into <linux/sched/signal.h>
    
    Fix up affected files that include this signal functionality via sched.h.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index f063a25d4449..b294a8ee2842 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -11,7 +11,7 @@
  * Waiting for completion is a typically sync point, but not an exclusion point.
  */
 
-#include <linux/sched.h>
+#include <linux/sched/signal.h>
 #include <linux/completion.h>
 
 /**

commit da9647e076d440190fff4b4ee0f4b829dd6e8c4f
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Dec 21 09:05:02 2016 +0100

    sched/completions: Fix complete_all() semantics
    
    Documentation/scheduler/completion.txt says this about complete_all():
    
      "calls complete_all() to signal all current and future waiters."
    
    Which doesn't strictly match the current semantics. Currently
    complete_all() is equivalent to UINT_MAX/2 complete() invocations,
    which is distinctly less than 'all current and future waiters'
    (enumerable vs innumerable), although it has worked in practice.
    
    However, Dmitry had a weird case where it might matter, so change
    completions to use saturation semantics for complete()/complete_all().
    Once done hits UINT_MAX (and complete_all() sets it there) it will
    never again be decremented.
    
    Requested-by: Dmitry Torokhov <dmitry.torokhov@gmail.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: der.herr@hofr.at
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 8d0f35debf35..f063a25d4449 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -31,7 +31,8 @@ void complete(struct completion *x)
 	unsigned long flags;
 
 	spin_lock_irqsave(&x->wait.lock, flags);
-	x->done++;
+	if (x->done != UINT_MAX)
+		x->done++;
 	__wake_up_locked(&x->wait, TASK_NORMAL, 1);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
@@ -51,7 +52,7 @@ void complete_all(struct completion *x)
 	unsigned long flags;
 
 	spin_lock_irqsave(&x->wait.lock, flags);
-	x->done += UINT_MAX/2;
+	x->done = UINT_MAX;
 	__wake_up_locked(&x->wait, TASK_NORMAL, 0);
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 }
@@ -79,7 +80,8 @@ do_wait_for_common(struct completion *x,
 		if (!x->done)
 			return timeout;
 	}
-	x->done--;
+	if (x->done != UINT_MAX)
+		x->done--;
 	return timeout ?: 1;
 }
 
@@ -280,7 +282,7 @@ bool try_wait_for_completion(struct completion *x)
 	spin_lock_irqsave(&x->wait.lock, flags);
 	if (!x->done)
 		ret = 0;
-	else
+	else if (x->done != UINT_MAX)
 		x->done--;
 	spin_unlock_irqrestore(&x->wait.lock, flags);
 	return ret;

commit bc9560155f4063bbc9be71bd69d6726d41b47653
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Feb 12 20:59:13 2015 +0100

    sched/completion: Serialize completion_done() with complete()
    
    Commit de30ec47302c "Remove unnecessary ->wait.lock serialization when
    reading completion state" was not correct, without lock/unlock the code
    like stop_machine_from_inactive_cpu()
    
            while (!completion_done())
                    cpu_relax();
    
    can return before complete() finishes its spin_unlock() which writes to
    this memory. And spin_unlock_wait().
    
    While at it, change try_wait_for_completion() to use READ_ONCE().
    
    Reported-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reported-by: Davidlohr Bueso <dave@stgolabs.net>
    Tested-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    [ Added a comment with the barrier. ]
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Nicholas Mc Guire <der.herr@hofr.at>
    Cc: raghavendra.kt@linux.vnet.ibm.com
    Cc: waiman.long@hp.com
    Fixes: de30ec47302c ("sched/completion: Remove unnecessary ->wait.lock serialization when reading completion state")
    Link: http://lkml.kernel.org/r/20150212195913.GA30430@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 7052d3fd4e7b..8d0f35debf35 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -274,7 +274,7 @@ bool try_wait_for_completion(struct completion *x)
 	 * first without taking the lock so we can
 	 * return early in the blocking case.
 	 */
-	if (!ACCESS_ONCE(x->done))
+	if (!READ_ONCE(x->done))
 		return 0;
 
 	spin_lock_irqsave(&x->wait.lock, flags);
@@ -297,6 +297,21 @@ EXPORT_SYMBOL(try_wait_for_completion);
  */
 bool completion_done(struct completion *x)
 {
-	return !!ACCESS_ONCE(x->done);
+	if (!READ_ONCE(x->done))
+		return false;
+
+	/*
+	 * If ->done, we need to wait for complete() to release ->wait.lock
+	 * otherwise we can end up freeing the completion before complete()
+	 * is done referencing it.
+	 *
+	 * The RMB pairs with complete()'s RELEASE of ->wait.lock and orders
+	 * the loads of ->done and ->wait.lock such that we cannot observe
+	 * the lock before complete() acquires it while observing the ->done
+	 * after it's acquired the lock.
+	 */
+	smp_rmb();
+	spin_unlock_wait(&x->wait.lock);
+	return true;
 }
 EXPORT_SYMBOL(completion_done);

commit 7c34e3180a01c800a40bc8535654d5735802fc1b
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Fri Jan 23 12:41:47 2015 +0100

    sched/completion: Add lock-free checking of the blocking case
    
    The "thread would block" case can be checked without grabbing ->wait.lock.
    
    [ If the check does not return early then grab the lock and recheck.
      A memory barrier is not needed as complete() and complete_all() imply
      a barrier.
    
      The ACCESS_ONCE() is needed for calls in a loop that, if inlined, could
      optimize out the re-fetching of x->done. ]
    
    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1422013307-13200-1-git-send-email-der.herr@hofr.at
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 9d1fe32da232..7052d3fd4e7b 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -268,6 +268,15 @@ bool try_wait_for_completion(struct completion *x)
 	unsigned long flags;
 	int ret = 1;
 
+	/*
+	 * Since x->done will need to be locked only
+	 * in the non-blocking case, we check x->done
+	 * first without taking the lock so we can
+	 * return early in the blocking case.
+	 */
+	if (!ACCESS_ONCE(x->done))
+		return 0;
+
 	spin_lock_irqsave(&x->wait.lock, flags);
 	if (!x->done)
 		ret = 0;

commit de30ec47302c101c7badc8fe687641fd75e596e7
Author: Nicholas Mc Guire <der.herr@hofr.at>
Date:   Sat Jan 17 05:05:34 2015 +0100

    sched/completion: Remove unnecessary ->wait.lock serialization when reading completion state
    
    Signed-off-by: Nicholas Mc Guire <der.herr@hofr.at>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1421467534-22834-1-git-send-email-der.herr@hofr.at
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index 607f852b4d04..9d1fe32da232 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -288,13 +288,6 @@ EXPORT_SYMBOL(try_wait_for_completion);
  */
 bool completion_done(struct completion *x)
 {
-	unsigned long flags;
-	int ret = 1;
-
-	spin_lock_irqsave(&x->wait.lock, flags);
-	if (!x->done)
-		ret = 0;
-	spin_unlock_irqrestore(&x->wait.lock, flags);
-	return ret;
+	return !!ACCESS_ONCE(x->done);
 }
 EXPORT_SYMBOL(completion_done);

commit a1bd5373357d3edc4b97548747d583c3efa484b3
Author: Wolfram Sang <wsa@the-dreams.de>
Date:   Tue Nov 4 12:01:41 2014 +0100

    sched/completion: Document when to use wait_for_completion_io_*()
    
    As discussed in [1], accounting IO is meant for blkio only. Document that
    so driver authors won't use them for device io.
    
     [1] http://thread.gmane.org/gmane.linux.drivers.i2c/20470
    
    Signed-off-by: Wolfram Sang <wsa@the-dreams.de>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: One Thousand Gnomes <gnomes@lxorguk.ukuu.org.uk>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Link: http://lkml.kernel.org/r/1415098901-2768-1-git-send-email-wsa@the-dreams.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
index a63f4dc27909..607f852b4d04 100644
--- a/kernel/sched/completion.c
+++ b/kernel/sched/completion.c
@@ -148,7 +148,7 @@ EXPORT_SYMBOL(wait_for_completion_timeout);
  *
  * This waits to be signaled for completion of a specific task. It is NOT
  * interruptible and there is no timeout. The caller is accounted as waiting
- * for IO.
+ * for IO (which traditionally means blkio only).
  */
 void __sched wait_for_completion_io(struct completion *x)
 {
@@ -163,7 +163,8 @@ EXPORT_SYMBOL(wait_for_completion_io);
  *
  * This waits for either a completion of a specific task to be signaled or for a
  * specified timeout to expire. The timeout is in jiffies. It is not
- * interruptible. The caller is accounted as waiting for IO.
+ * interruptible. The caller is accounted as waiting for IO (which traditionally
+ * means blkio only).
  *
  * Return: 0 if timed out, and positive (at least 1, or number of jiffies left
  * till timeout) if completed.

commit b8a216269ec0ce2e961d32e6d640d7010b8a818e
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Oct 4 22:06:53 2013 +0200

    sched: Move completion code from core.c to completion.c
    
    Completions already have their own header file: linux/completion.h
    Move the implementation out of kernel/sched/core.c and into its own
    file: kernel/sched/completion.c.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Link: http://lkml.kernel.org/n/tip-x2y49rmxu5dljt66ai2lcfuw@git.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/sched/completion.c b/kernel/sched/completion.c
new file mode 100644
index 000000000000..a63f4dc27909
--- /dev/null
+++ b/kernel/sched/completion.c
@@ -0,0 +1,299 @@
+/*
+ * Generic wait-for-completion handler;
+ *
+ * It differs from semaphores in that their default case is the opposite,
+ * wait_for_completion default blocks whereas semaphore default non-block. The
+ * interface also makes it easy to 'complete' multiple waiting threads,
+ * something which isn't entirely natural for semaphores.
+ *
+ * But more importantly, the primitive documents the usage. Semaphores would
+ * typically be used for exclusion which gives rise to priority inversion.
+ * Waiting for completion is a typically sync point, but not an exclusion point.
+ */
+
+#include <linux/sched.h>
+#include <linux/completion.h>
+
+/**
+ * complete: - signals a single thread waiting on this completion
+ * @x:  holds the state of this particular completion
+ *
+ * This will wake up a single thread waiting on this completion. Threads will be
+ * awakened in the same order in which they were queued.
+ *
+ * See also complete_all(), wait_for_completion() and related routines.
+ *
+ * It may be assumed that this function implies a write memory barrier before
+ * changing the task state if and only if any tasks are woken up.
+ */
+void complete(struct completion *x)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	x->done++;
+	__wake_up_locked(&x->wait, TASK_NORMAL, 1);
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+}
+EXPORT_SYMBOL(complete);
+
+/**
+ * complete_all: - signals all threads waiting on this completion
+ * @x:  holds the state of this particular completion
+ *
+ * This will wake up all threads waiting on this particular completion event.
+ *
+ * It may be assumed that this function implies a write memory barrier before
+ * changing the task state if and only if any tasks are woken up.
+ */
+void complete_all(struct completion *x)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	x->done += UINT_MAX/2;
+	__wake_up_locked(&x->wait, TASK_NORMAL, 0);
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+}
+EXPORT_SYMBOL(complete_all);
+
+static inline long __sched
+do_wait_for_common(struct completion *x,
+		   long (*action)(long), long timeout, int state)
+{
+	if (!x->done) {
+		DECLARE_WAITQUEUE(wait, current);
+
+		__add_wait_queue_tail_exclusive(&x->wait, &wait);
+		do {
+			if (signal_pending_state(state, current)) {
+				timeout = -ERESTARTSYS;
+				break;
+			}
+			__set_current_state(state);
+			spin_unlock_irq(&x->wait.lock);
+			timeout = action(timeout);
+			spin_lock_irq(&x->wait.lock);
+		} while (!x->done && timeout);
+		__remove_wait_queue(&x->wait, &wait);
+		if (!x->done)
+			return timeout;
+	}
+	x->done--;
+	return timeout ?: 1;
+}
+
+static inline long __sched
+__wait_for_common(struct completion *x,
+		  long (*action)(long), long timeout, int state)
+{
+	might_sleep();
+
+	spin_lock_irq(&x->wait.lock);
+	timeout = do_wait_for_common(x, action, timeout, state);
+	spin_unlock_irq(&x->wait.lock);
+	return timeout;
+}
+
+static long __sched
+wait_for_common(struct completion *x, long timeout, int state)
+{
+	return __wait_for_common(x, schedule_timeout, timeout, state);
+}
+
+static long __sched
+wait_for_common_io(struct completion *x, long timeout, int state)
+{
+	return __wait_for_common(x, io_schedule_timeout, timeout, state);
+}
+
+/**
+ * wait_for_completion: - waits for completion of a task
+ * @x:  holds the state of this particular completion
+ *
+ * This waits to be signaled for completion of a specific task. It is NOT
+ * interruptible and there is no timeout.
+ *
+ * See also similar routines (i.e. wait_for_completion_timeout()) with timeout
+ * and interrupt capability. Also see complete().
+ */
+void __sched wait_for_completion(struct completion *x)
+{
+	wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(wait_for_completion);
+
+/**
+ * wait_for_completion_timeout: - waits for completion of a task (w/timeout)
+ * @x:  holds the state of this particular completion
+ * @timeout:  timeout value in jiffies
+ *
+ * This waits for either a completion of a specific task to be signaled or for a
+ * specified timeout to expire. The timeout is in jiffies. It is not
+ * interruptible.
+ *
+ * Return: 0 if timed out, and positive (at least 1, or number of jiffies left
+ * till timeout) if completed.
+ */
+unsigned long __sched
+wait_for_completion_timeout(struct completion *x, unsigned long timeout)
+{
+	return wait_for_common(x, timeout, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(wait_for_completion_timeout);
+
+/**
+ * wait_for_completion_io: - waits for completion of a task
+ * @x:  holds the state of this particular completion
+ *
+ * This waits to be signaled for completion of a specific task. It is NOT
+ * interruptible and there is no timeout. The caller is accounted as waiting
+ * for IO.
+ */
+void __sched wait_for_completion_io(struct completion *x)
+{
+	wait_for_common_io(x, MAX_SCHEDULE_TIMEOUT, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(wait_for_completion_io);
+
+/**
+ * wait_for_completion_io_timeout: - waits for completion of a task (w/timeout)
+ * @x:  holds the state of this particular completion
+ * @timeout:  timeout value in jiffies
+ *
+ * This waits for either a completion of a specific task to be signaled or for a
+ * specified timeout to expire. The timeout is in jiffies. It is not
+ * interruptible. The caller is accounted as waiting for IO.
+ *
+ * Return: 0 if timed out, and positive (at least 1, or number of jiffies left
+ * till timeout) if completed.
+ */
+unsigned long __sched
+wait_for_completion_io_timeout(struct completion *x, unsigned long timeout)
+{
+	return wait_for_common_io(x, timeout, TASK_UNINTERRUPTIBLE);
+}
+EXPORT_SYMBOL(wait_for_completion_io_timeout);
+
+/**
+ * wait_for_completion_interruptible: - waits for completion of a task (w/intr)
+ * @x:  holds the state of this particular completion
+ *
+ * This waits for completion of a specific task to be signaled. It is
+ * interruptible.
+ *
+ * Return: -ERESTARTSYS if interrupted, 0 if completed.
+ */
+int __sched wait_for_completion_interruptible(struct completion *x)
+{
+	long t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_INTERRUPTIBLE);
+	if (t == -ERESTARTSYS)
+		return t;
+	return 0;
+}
+EXPORT_SYMBOL(wait_for_completion_interruptible);
+
+/**
+ * wait_for_completion_interruptible_timeout: - waits for completion (w/(to,intr))
+ * @x:  holds the state of this particular completion
+ * @timeout:  timeout value in jiffies
+ *
+ * This waits for either a completion of a specific task to be signaled or for a
+ * specified timeout to expire. It is interruptible. The timeout is in jiffies.
+ *
+ * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,
+ * or number of jiffies left till timeout) if completed.
+ */
+long __sched
+wait_for_completion_interruptible_timeout(struct completion *x,
+					  unsigned long timeout)
+{
+	return wait_for_common(x, timeout, TASK_INTERRUPTIBLE);
+}
+EXPORT_SYMBOL(wait_for_completion_interruptible_timeout);
+
+/**
+ * wait_for_completion_killable: - waits for completion of a task (killable)
+ * @x:  holds the state of this particular completion
+ *
+ * This waits to be signaled for completion of a specific task. It can be
+ * interrupted by a kill signal.
+ *
+ * Return: -ERESTARTSYS if interrupted, 0 if completed.
+ */
+int __sched wait_for_completion_killable(struct completion *x)
+{
+	long t = wait_for_common(x, MAX_SCHEDULE_TIMEOUT, TASK_KILLABLE);
+	if (t == -ERESTARTSYS)
+		return t;
+	return 0;
+}
+EXPORT_SYMBOL(wait_for_completion_killable);
+
+/**
+ * wait_for_completion_killable_timeout: - waits for completion of a task (w/(to,killable))
+ * @x:  holds the state of this particular completion
+ * @timeout:  timeout value in jiffies
+ *
+ * This waits for either a completion of a specific task to be
+ * signaled or for a specified timeout to expire. It can be
+ * interrupted by a kill signal. The timeout is in jiffies.
+ *
+ * Return: -ERESTARTSYS if interrupted, 0 if timed out, positive (at least 1,
+ * or number of jiffies left till timeout) if completed.
+ */
+long __sched
+wait_for_completion_killable_timeout(struct completion *x,
+				     unsigned long timeout)
+{
+	return wait_for_common(x, timeout, TASK_KILLABLE);
+}
+EXPORT_SYMBOL(wait_for_completion_killable_timeout);
+
+/**
+ *	try_wait_for_completion - try to decrement a completion without blocking
+ *	@x:	completion structure
+ *
+ *	Return: 0 if a decrement cannot be done without blocking
+ *		 1 if a decrement succeeded.
+ *
+ *	If a completion is being used as a counting completion,
+ *	attempt to decrement the counter without blocking. This
+ *	enables us to avoid waiting if the resource the completion
+ *	is protecting is not available.
+ */
+bool try_wait_for_completion(struct completion *x)
+{
+	unsigned long flags;
+	int ret = 1;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	if (!x->done)
+		ret = 0;
+	else
+		x->done--;
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+	return ret;
+}
+EXPORT_SYMBOL(try_wait_for_completion);
+
+/**
+ *	completion_done - Test to see if a completion has any waiters
+ *	@x:	completion structure
+ *
+ *	Return: 0 if there are waiters (wait_for_completion() in progress)
+ *		 1 if there are no waiters.
+ *
+ */
+bool completion_done(struct completion *x)
+{
+	unsigned long flags;
+	int ret = 1;
+
+	spin_lock_irqsave(&x->wait.lock, flags);
+	if (!x->done)
+		ret = 0;
+	spin_unlock_irqrestore(&x->wait.lock, flags);
+	return ret;
+}
+EXPORT_SYMBOL(completion_done);
