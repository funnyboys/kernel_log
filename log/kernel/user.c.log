commit de83dbd97f173650a602c5e356025b732173ecc4
Author: Jason Yan <yanaijie@huawei.com>
Date:   Thu Jun 4 16:49:58 2020 -0700

    user.c: make uidhash_table static
    
    Fix the following sparse warning:
    
      kernel/user.c:85:19: warning: symbol 'uidhash_table' was not declared.
      Should it be static?
    
    Reported-by: Hulk Robot <hulkci@huawei.com>
    Signed-off-by: Jason Yan <yanaijie@huawei.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20200413082146.22737-1-yanaijie@huawei.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 5235d7f49982..b1635d94a1f2 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -82,7 +82,7 @@ EXPORT_SYMBOL_GPL(init_user_ns);
 #define uidhashentry(uid)	(uidhash_table + __uidhashfn((__kuid_val(uid))))
 
 static struct kmem_cache *uid_cachep;
-struct hlist_head uidhash_table[UIDHASH_SZ];
+static struct hlist_head uidhash_table[UIDHASH_SZ];
 
 /*
  * The uidhash_lock is mostly taken from process context, but it is

commit c84ca912b07901be528e5184fd254fca1dddf2ac
Merge: c236b6dd48dc a58946c158a0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Jul 8 19:36:47 2019 -0700

    Merge tag 'keys-namespace-20190627' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs
    
    Pull keyring namespacing from David Howells:
     "These patches help make keys and keyrings more namespace aware.
    
      Firstly some miscellaneous patches to make the process easier:
    
       - Simplify key index_key handling so that the word-sized chunks
         assoc_array requires don't have to be shifted about, making it
         easier to add more bits into the key.
    
       - Cache the hash value in the key so that we don't have to calculate
         on every key we examine during a search (it involves a bunch of
         multiplications).
    
       - Allow keying_search() to search non-recursively.
    
      Then the main patches:
    
       - Make it so that keyring names are per-user_namespace from the point
         of view of KEYCTL_JOIN_SESSION_KEYRING so that they're not
         accessible cross-user_namespace.
    
         keyctl_capabilities() shows KEYCTL_CAPS1_NS_KEYRING_NAME for this.
    
       - Move the user and user-session keyrings to the user_namespace
         rather than the user_struct. This prevents them propagating
         directly across user_namespaces boundaries (ie. the KEY_SPEC_*
         flags will only pick from the current user_namespace).
    
       - Make it possible to include the target namespace in which the key
         shall operate in the index_key. This will allow the possibility of
         multiple keys with the same description, but different target
         domains to be held in the same keyring.
    
         keyctl_capabilities() shows KEYCTL_CAPS1_NS_KEY_TAG for this.
    
       - Make it so that keys are implicitly invalidated by removal of a
         domain tag, causing them to be garbage collected.
    
       - Institute a network namespace domain tag that allows keys to be
         differentiated by the network namespace in which they operate. New
         keys that are of a type marked 'KEY_TYPE_NET_DOMAIN' are assigned
         the network domain in force when they are created.
    
       - Make it so that the desired network namespace can be handed down
         into the request_key() mechanism. This allows AFS, NFS, etc. to
         request keys specific to the network namespace of the superblock.
    
         This also means that the keys in the DNS record cache are
         thenceforth namespaced, provided network filesystems pass the
         appropriate network namespace down into dns_query().
    
         For DNS, AFS and NFS are good, whilst CIFS and Ceph are not. Other
         cache keyrings, such as idmapper keyrings, also need to set the
         domain tag - for which they need access to the network namespace of
         the superblock"
    
    * tag 'keys-namespace-20190627' of git://git.kernel.org/pub/scm/linux/kernel/git/dhowells/linux-fs:
      keys: Pass the network namespace into request_key mechanism
      keys: Network namespace domain tag
      keys: Garbage collect keys for which the domain has been removed
      keys: Include target namespace in match criteria
      keys: Move the user and user-session keyrings to the user_namespace
      keys: Namespace keyring names
      keys: Add a 'recurse' flag for keyring searches
      keys: Cache the hash value to avoid lots of recalculation
      keys: Simplify key description management

commit 0f44e4d976f96c6439da0d6717238efa4b91196e
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jun 26 21:02:32 2019 +0100

    keys: Move the user and user-session keyrings to the user_namespace
    
    Move the user and user-session keyrings to the user_namespace struct rather
    than pinning them from the user_struct struct.  This prevents these
    keyrings from propagating across user-namespaces boundaries with regard to
    the KEY_SPEC_* flags, thereby making them more useful in a containerised
    environment.
    
    The issue is that a single user_struct may be represent UIDs in several
    different namespaces.
    
    The way the patch does this is by attaching a 'register keyring' in each
    user_namespace and then sticking the user and user-session keyrings into
    that.  It can then be searched to retrieve them.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Jann Horn <jannh@google.com>

diff --git a/kernel/user.c b/kernel/user.c
index 50979fd1b7aa..f8519b62cf9a 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -64,10 +64,7 @@ struct user_namespace init_user_ns = {
 	.flags = USERNS_INIT_FLAGS,
 #ifdef CONFIG_KEYS
 	.keyring_name_list = LIST_HEAD_INIT(init_user_ns.keyring_name_list),
-#endif
-#ifdef CONFIG_PERSISTENT_KEYRINGS
-	.persistent_keyring_register_sem =
-	__RWSEM_INITIALIZER(init_user_ns.persistent_keyring_register_sem),
+	.keyring_sem = __RWSEM_INITIALIZER(init_user_ns.keyring_sem),
 #endif
 };
 EXPORT_SYMBOL_GPL(init_user_ns);
@@ -143,8 +140,6 @@ static void free_user(struct user_struct *up, unsigned long flags)
 {
 	uid_hash_remove(up);
 	spin_unlock_irqrestore(&uidhash_lock, flags);
-	key_put(up->uid_keyring);
-	key_put(up->session_keyring);
 	kmem_cache_free(uid_cachep, up);
 }
 

commit b206f281d0ee14969878469816a69db22d5838e8
Author: David Howells <dhowells@redhat.com>
Date:   Wed Jun 26 21:02:32 2019 +0100

    keys: Namespace keyring names
    
    Keyring names are held in a single global list that any process can pick
    from by means of keyctl_join_session_keyring (provided the keyring grants
    Search permission).  This isn't very container friendly, however.
    
    Make the following changes:
    
     (1) Make default session, process and thread keyring names begin with a
         '.' instead of '_'.
    
     (2) Keyrings whose names begin with a '.' aren't added to the list.  Such
         keyrings are system specials.
    
     (3) Replace the global list with per-user_namespace lists.  A keyring adds
         its name to the list for the user_namespace that it is currently in.
    
     (4) When a user_namespace is deleted, it just removes itself from the
         keyring name list.
    
    The global keyring_name_lock is retained for accessing the name lists.
    This allows (4) to work.
    
    This can be tested by:
    
            # keyctl newring foo @s
            995906392
            # unshare -U
            $ keyctl show
            ...
             995906392 --alswrv  65534 65534   \_ keyring: foo
            ...
            $ keyctl session foo
            Joined session keyring: 935622349
    
    As can be seen, a new session keyring was created.
    
    The capability bit KEYCTL_CAPS1_NS_KEYRING_NAME is set if the kernel is
    employing this feature.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 88b834f0eebc..50979fd1b7aa 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -62,6 +62,9 @@ struct user_namespace init_user_ns = {
 	.ns.ops = &userns_operations,
 #endif
 	.flags = USERNS_INIT_FLAGS,
+#ifdef CONFIG_KEYS
+	.keyring_name_list = LIST_HEAD_INIT(init_user_ns.keyring_name_list),
+#endif
 #ifdef CONFIG_PERSISTENT_KEYRINGS
 	.persistent_keyring_register_sem =
 	__RWSEM_INITIALIZER(init_user_ns.persistent_keyring_register_sem),

commit 457c89965399115e5cd8bf38f9c597293405703d
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Sun May 19 13:08:55 2019 +0100

    treewide: Add SPDX license identifier for missed files
    
    Add SPDX license identifiers to all files which:
    
     - Have no license information of any form
    
     - Have EXPORT_.*_SYMBOL_GPL inside which was used in the
       initial scan/conversion to ignore the file
    
    These files fall under the project license, GPL v2 only. The resulting SPDX
    license identifier is:
    
      GPL-2.0-only
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 88b834f0eebc..78b17e36e705 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0-only
 /*
  * The "user cache".
  *

commit 6c4e121fda519e0da5c6755a60fdef8cd39634ae
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Tue May 14 15:42:37 2019 -0700

    kernel/user.c: clean up some leftover code
    
    The out_unlock label is misleading; no unlocking happens after it, so
    just return NULL directly.
    
    Also, nothing between the kmem_cache_zalloc() that creates new and the
    two key_put() can initialize new->uid_keyring or new->session_keyring,
    so those calls are no-ops.
    
    Link: http://lkml.kernel.org/r/20190424200404.9114-1-linux@rasmusvillemoes.dk
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: "Peter Zijlstra (Intel)" <peterz@infradead.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 0df9b1640b2a..88b834f0eebc 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -185,7 +185,7 @@ struct user_struct *alloc_uid(kuid_t uid)
 	if (!up) {
 		new = kmem_cache_zalloc(uid_cachep, GFP_KERNEL);
 		if (!new)
-			goto out_unlock;
+			return NULL;
 
 		new->uid = uid;
 		refcount_set(&new->__count, 1);
@@ -199,8 +199,6 @@ struct user_struct *alloc_uid(kuid_t uid)
 		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
-			key_put(new->uid_keyring);
-			key_put(new->session_keyring);
 			kmem_cache_free(uid_cachep, new);
 		} else {
 			uid_hash_insert(new, hashent);
@@ -210,9 +208,6 @@ struct user_struct *alloc_uid(kuid_t uid)
 	}
 
 	return up;
-
-out_unlock:
-	return NULL;
 }
 
 static int __init uid_cache_init(void)

commit ce0a568d327968367f294ea2425fe2c6f4f33ed2
Author: Anna-Maria Gleixner <anna-maria@linutronix.de>
Date:   Tue Aug 21 21:55:42 2018 -0700

    userns: use irqsave variant of refcount_dec_and_lock()
    
    The irqsave variant of refcount_dec_and_lock handles irqsave/restore when
    taking/releasing the spin lock.  With this variant the call of
    local_irq_save/restore is no longer required.
    
    [bigeasy@linutronix.de: s@atomic_dec_and_lock@refcount_dec_and_lock@g]
    Link: http://lkml.kernel.org/r/20180703200141.28415-7-bigeasy@linutronix.de
    Signed-off-by: Anna-Maria Gleixner <anna-maria@linutronix.de>
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 5f65ef195259..0df9b1640b2a 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -169,11 +169,8 @@ void free_uid(struct user_struct *up)
 	if (!up)
 		return;
 
-	local_irq_save(flags);
-	if (refcount_dec_and_lock(&up->__count, &uidhash_lock))
+	if (refcount_dec_and_lock_irqsave(&up->__count, &uidhash_lock, &flags))
 		free_user(up, flags);
-	else
-		local_irq_restore(flags);
 }
 
 struct user_struct *alloc_uid(kuid_t uid)

commit fc37191272a972d449bab3d8247cf52cadf5d4e6
Author: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
Date:   Tue Aug 21 21:55:38 2018 -0700

    userns: use refcount_t for reference counting instead atomic_t
    
    refcount_t type and corresponding API should be used instead of atomic_t
    wh en the variable is used as a reference counter.  This avoids accidental
    refcounter overflows that might lead to use-after-free situations.
    
    Link: http://lkml.kernel.org/r/20180703200141.28415-6-bigeasy@linutronix.de
    Signed-off-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Andrew Morton <akpm@linux-foundation.org>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 36288d840675..5f65ef195259 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -96,7 +96,7 @@ static DEFINE_SPINLOCK(uidhash_lock);
 
 /* root_user.__count is 1, for init task cred */
 struct user_struct root_user = {
-	.__count	= ATOMIC_INIT(1),
+	.__count	= REFCOUNT_INIT(1),
 	.processes	= ATOMIC_INIT(1),
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
@@ -123,7 +123,7 @@ static struct user_struct *uid_hash_find(kuid_t uid, struct hlist_head *hashent)
 
 	hlist_for_each_entry(user, hashent, uidhash_node) {
 		if (uid_eq(user->uid, uid)) {
-			atomic_inc(&user->__count);
+			refcount_inc(&user->__count);
 			return user;
 		}
 	}
@@ -170,7 +170,7 @@ void free_uid(struct user_struct *up)
 		return;
 
 	local_irq_save(flags);
-	if (atomic_dec_and_lock(&up->__count, &uidhash_lock))
+	if (refcount_dec_and_lock(&up->__count, &uidhash_lock))
 		free_user(up, flags);
 	else
 		local_irq_restore(flags);
@@ -191,7 +191,7 @@ struct user_struct *alloc_uid(kuid_t uid)
 			goto out_unlock;
 
 		new->uid = uid;
-		atomic_set(&new->__count, 1);
+		refcount_set(&new->__count, 1);
 		ratelimit_state_init(&new->ratelimit, HZ, 100);
 		ratelimit_set_flags(&new->ratelimit, RATELIMIT_MSG_ON_RELEASE);
 

commit bef3efbeb897b56867e271cdbc5f8adaacaeb9cd
Author: Luck, Tony <tony.luck@intel.com>
Date:   Thu Feb 22 09:15:06 2018 -0800

    efivarfs: Limit the rate for non-root to read files
    
    Each read from a file in efivarfs results in two calls to EFI
    (one to get the file size, another to get the actual data).
    
    On X86 these EFI calls result in broadcast system management
    interrupts (SMI) which affect performance of the whole system.
    A malicious user can loop performing reads from efivarfs bringing
    the system to its knees.
    
    Linus suggested per-user rate limit to solve this.
    
    So we add a ratelimit structure to "user_struct" and initialize
    it for the root user for no limit. When allocating user_struct for
    other users we set the limit to 100 per second. This could be used
    for other places that want to limit the rate of some detrimental
    user action.
    
    In efivarfs if the limit is exceeded when reading, we take an
    interruptible nap for 50ms and check the rate limit again.
    
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Acked-by: Ard Biesheuvel <ard.biesheuvel@linaro.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 9a20acce460d..36288d840675 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -101,6 +101,7 @@ struct user_struct root_user = {
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
 	.uid		= GLOBAL_ROOT_UID,
+	.ratelimit	= RATELIMIT_STATE_INIT(root_user.ratelimit, 0, 0),
 };
 
 /*
@@ -191,6 +192,8 @@ struct user_struct *alloc_uid(kuid_t uid)
 
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
+		ratelimit_state_init(&new->ratelimit, HZ, 100);
+		ratelimit_set_flags(&new->ratelimit, RATELIMIT_MSG_ON_RELEASE);
 
 		/*
 		 * Before adding this, check whether we raced

commit aa4bf44dc851c6bdd4f7b61b5f2c56c84dfe2ff0
Author: Christian Brauner <christian.brauner@ubuntu.com>
Date:   Wed Oct 25 00:04:40 2017 +0200

    userns: use union in {g,u}idmap struct
    
    - Add a struct containing two pointer to extents and wrap both the static extent
      array and the struct into a union. This is done in preparation for bumping the
      {g,u}idmap limits for user namespaces.
    - Add brackets around anonymous union when using designated initializers to
      initialize members in order to please gcc <= 4.4.
    
    Signed-off-by: Christian Brauner <christian.brauner@ubuntu.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 00281add65b2..9a20acce460d 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -26,26 +26,32 @@
 struct user_namespace init_user_ns = {
 	.uid_map = {
 		.nr_extents = 1,
-		.extent[0] = {
-			.first = 0,
-			.lower_first = 0,
-			.count = 4294967295U,
+		{
+			.extent[0] = {
+				.first = 0,
+				.lower_first = 0,
+				.count = 4294967295U,
+			},
 		},
 	},
 	.gid_map = {
 		.nr_extents = 1,
-		.extent[0] = {
-			.first = 0,
-			.lower_first = 0,
-			.count = 4294967295U,
+		{
+			.extent[0] = {
+				.first = 0,
+				.lower_first = 0,
+				.count = 4294967295U,
+			},
 		},
 	},
 	.projid_map = {
 		.nr_extents = 1,
-		.extent[0] = {
-			.first = 0,
-			.lower_first = 0,
-			.count = 4294967295U,
+		{
+			.extent[0] = {
+				.first = 0,
+				.lower_first = 0,
+				.count = 4294967295U,
+			},
 		},
 	},
 	.count = ATOMIC_INIT(3),

commit 8703e8a465b1e9cadc3680b4b1248f5987e54518
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:30 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/user.h>
    
    We are going to split <linux/sched/user.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/user.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/user.c b/kernel/user.c
index b069ccbfb0b0..00281add65b2 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -13,6 +13,7 @@
 #include <linux/slab.h>
 #include <linux/bitops.h>
 #include <linux/key.h>
+#include <linux/sched/user.h>
 #include <linux/interrupt.h>
 #include <linux/export.h>
 #include <linux/user_namespace.h>

commit 87c31b39abcb6fb6bd7d111200c9627a594bf6a9
Merge: f045bbb9fa1b db86da7cb76f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 17 12:31:40 2014 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace related fixes from Eric Biederman:
     "As these are bug fixes almost all of thes changes are marked for
      backporting to stable.
    
      The first change (implicitly adding MNT_NODEV on remount) addresses a
      regression that was created when security issues with unprivileged
      remount were closed.  I go on to update the remount test to make it
      easy to detect if this issue reoccurs.
    
      Then there are a handful of mount and umount related fixes.
    
      Then half of the changes deal with the a recently discovered design
      bug in the permission checks of gid_map.  Unix since the beginning has
      allowed setting group permissions on files to less than the user and
      other permissions (aka ---rwx---rwx).  As the unix permission checks
      stop as soon as a group matches, and setgroups allows setting groups
      that can not later be dropped, results in a situtation where it is
      possible to legitimately use a group to assign fewer privileges to a
      process.  Which means dropping a group can increase a processes
      privileges.
    
      The fix I have adopted is that gid_map is now no longer writable
      without privilege unless the new file /proc/self/setgroups has been
      set to permanently disable setgroups.
    
      The bulk of user namespace using applications even the applications
      using applications using user namespaces without privilege remain
      unaffected by this change.  Unfortunately this ix breaks a couple user
      space applications, that were relying on the problematic behavior (one
      of which was tools/selftests/mount/unprivileged-remount-test.c).
    
      To hopefully prevent needing a regression fix on top of my security
      fix I rounded folks who work with the container implementations mostly
      like to be affected and encouraged them to test the changes.
    
        > So far nothing broke on my libvirt-lxc test bed. :-)
        > Tested with openSUSE 13.2 and libvirt 1.2.9.
        > Tested-by: Richard Weinberger <richard@nod.at>
    
        > Tested on Fedora20 with libvirt 1.2.11, works fine.
        > Tested-by: Chen Hanxiao <chenhanxiao@cn.fujitsu.com>
    
        > Ok, thanks - yes, unprivileged lxc is working fine with your kernels.
        > Just to be sure I was testing the right thing I also tested using
        > my unprivileged nsexec testcases, and they failed on setgroup/setgid
        > as now expected, and succeeded there without your patches.
        > Tested-by: Serge Hallyn <serge.hallyn@ubuntu.com>
    
        > I tested this with Sandstorm.  It breaks as is and it works if I add
        > the setgroups thing.
        > Tested-by: Andy Lutomirski <luto@amacapital.net> # breaks things as designed :("
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace:
      userns: Unbreak the unprivileged remount tests
      userns; Correct the comment in map_write
      userns: Allow setting gid_maps without privilege when setgroups is disabled
      userns: Add a knob to disable setgroups on a per user namespace basis
      userns: Rename id_map_mutex to userns_state_mutex
      userns: Only allow the creator of the userns unprivileged mappings
      userns: Check euid no fsuid when establishing an unprivileged uid mapping
      userns: Don't allow unprivileged creation of gid mappings
      userns: Don't allow setgroups until a gid mapping has been setablished
      userns: Document what the invariant required for safe unprivileged mappings.
      groups: Consolidate the setgroups permission checks
      mnt: Clear mnt_expire during pivot_root
      mnt: Carefully set CL_UNPRIVILEGED in clone_mnt
      mnt: Move the clear of MNT_LOCKED from copy_tree to it's callers.
      umount: Do not allow unmounting rootfs.
      umount: Disallow unprivileged mount force
      mnt: Update unprivileged remount test
      mnt: Implicitly add MNT_NODEV on remount when it was implicitly added by mount

commit 9cc46516ddf497ea16e8d7cb986ae03a0f6b92f8
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Dec 2 12:27:26 2014 -0600

    userns: Add a knob to disable setgroups on a per user namespace basis
    
    - Expose the knob to user space through a proc file /proc/<pid>/setgroups
    
      A value of "deny" means the setgroups system call is disabled in the
      current processes user namespace and can not be enabled in the
      future in this user namespace.
    
      A value of "allow" means the segtoups system call is enabled.
    
    - Descendant user namespaces inherit the value of setgroups from
      their parents.
    
    - A proc file is used (instead of a sysctl) as sysctls currently do
      not allow checking the permissions at open time.
    
    - Writing to the proc file is restricted to before the gid_map
      for the user namespace is set.
    
      This ensures that disabling setgroups at a user namespace
      level will never remove the ability to call setgroups
      from a process that already has that ability.
    
      A process may opt in to the setgroups disable for itself by
      creating, entering and configuring a user namespace or by calling
      setns on an existing user namespace with setgroups disabled.
      Processes without privileges already can not call setgroups so this
      is a noop.  Prodcess with privilege become processes without
      privilege when entering a user namespace and as with any other path
      to dropping privilege they would not have the ability to call
      setgroups.  So this remains within the bounds of what is possible
      without a knob to disable setgroups permanently in a user namespace.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 4efa39350e44..2d09940c9632 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -51,6 +51,7 @@ struct user_namespace init_user_ns = {
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
 	.proc_inum = PROC_USER_INIT_INO,
+	.flags = USERNS_INIT_FLAGS,
 #ifdef CONFIG_PERSISTENT_KEYRINGS
 	.persistent_keyring_register_sem =
 	__RWSEM_INITIALIZER(init_user_ns.persistent_keyring_register_sem),

commit 33c429405a2c8d9e42afb9fee88a63cfb2de1e98
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sat Nov 1 02:32:53 2014 -0400

    copy address of proc_ns_ops into ns_common
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/user.c b/kernel/user.c
index a7ca84bad8e6..69b800aebf13 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -51,6 +51,9 @@ struct user_namespace init_user_ns = {
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
 	.ns.inum = PROC_USER_INIT_INO,
+#ifdef CONFIG_USER_NS
+	.ns.ops = &userns_operations,
+#endif
 #ifdef CONFIG_PERSISTENT_KEYRINGS
 	.persistent_keyring_register_sem =
 	__RWSEM_INITIALIZER(init_user_ns.persistent_keyring_register_sem),

commit 435d5f4bb2ccba3b791d9ef61d2590e30b8e806e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Fri Oct 31 22:56:04 2014 -0400

    common object embedded into various struct ....ns
    
    for now - just move corresponding ->proc_inum instances over there
    
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/user.c b/kernel/user.c
index 4efa39350e44..a7ca84bad8e6 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -50,7 +50,7 @@ struct user_namespace init_user_ns = {
 	.count = ATOMIC_INIT(3),
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
-	.proc_inum = PROC_USER_INIT_INO,
+	.ns.inum = PROC_USER_INIT_INO,
 #ifdef CONFIG_PERSISTENT_KEYRINGS
 	.persistent_keyring_register_sem =
 	__RWSEM_INITIALIZER(init_user_ns.persistent_keyring_register_sem),

commit b300a4ea665f7fa44f015616ac1874deca891c5e
Author: Kirill A. Shutemov <kirill@shutemov.name>
Date:   Wed Jun 4 16:11:27 2014 -0700

    kernel/user.c: drop unused field 'files' from user_struct
    
    Nobody seems uses it for a long time. Let's drop it.
    
    Signed-off-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 294fc6a94168..4efa39350e44 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -87,7 +87,6 @@ static DEFINE_SPINLOCK(uidhash_lock);
 struct user_struct root_user = {
 	.__count	= ATOMIC_INIT(1),
 	.processes	= ATOMIC_INIT(1),
-	.files		= ATOMIC_INIT(0),
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
 	.uid		= GLOBAL_ROOT_UID,

commit c96d6660dc65b0a90aea9834bfd8be1d5656da18
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Thu Apr 3 14:48:35 2014 -0700

    kernel: audit/fix non-modular users of module_init in core code
    
    Code that is obj-y (always built-in) or dependent on a bool Kconfig
    (built-in or absent) can never be modular.  So using module_init as an
    alias for __initcall can be somewhat misleading.
    
    Fix these up now, so that we can relocate module_init from init.h into
    module.h in the future.  If we don't do this, we'd have to add module.h
    to obviously non-modular code, and that would be a worse thing.
    
    The audit targets the following module_init users for change:
     kernel/user.c                  obj-y
     kernel/kexec.c                 bool KEXEC (one instance per arch)
     kernel/profile.c               bool PROFILING
     kernel/hung_task.c             bool DETECT_HUNG_TASK
     kernel/sched/stats.c           bool SCHEDSTATS
     kernel/user_namespace.c        bool USER_NS
    
    Note that direct use of __initcall is discouraged, vs.  one of the
    priority categorized subgroups.  As __initcall gets mapped onto
    device_initcall, our use of subsys_initcall (which makes sense for these
    files) will thus change this registration from level 6-device to level
    4-subsys (i.e.  slightly earlier).  However no observable impact of that
    difference has been observed during testing.
    
    Also, two instances of missing ";" at EOL are fixed in kexec.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Eric Biederman <ebiederm@xmission.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index c006131beb77..294fc6a94168 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -222,5 +222,4 @@ static int __init uid_cache_init(void)
 
 	return 0;
 }
-
-module_init(uid_cache_init);
+subsys_initcall(uid_cache_init);

commit 6bd364d82920be726c2d678e7ba9e27112686e11
Author: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
Date:   Fri Dec 13 15:00:32 2013 +0800

    KEYS: fix uninitialized persistent_keyring_register_sem
    
    We run into this bug:
    [ 2736.063245] Unable to handle kernel paging request for data at address 0x00000000
    [ 2736.063293] Faulting instruction address: 0xc00000000037efb0
    [ 2736.063300] Oops: Kernel access of bad area, sig: 11 [#1]
    [ 2736.063303] SMP NR_CPUS=2048 NUMA pSeries
    [ 2736.063310] Modules linked in: sg nfsv3 rpcsec_gss_krb5 nfsv4 dns_resolver nfs fscache nf_conntrack_netbios_ns nf_conntrack_broadcast ipt_MASQUERADE ip6table_mangle ip6table_security ip6table_raw ip6t_REJECT iptable_nat nf_nat_ipv4 iptable_mangle iptable_security iptable_raw ipt_REJECT nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack ebtable_filter ebtables ip6table_filter iptable_filter ip_tables ip6table_nat nf_conntrack_ipv6 nf_defrag_ipv6 nf_nat_ipv6 nf_nat nf_conntrack ip6_tables ibmveth pseries_rng nx_crypto nfsd auth_rpcgss nfs_acl lockd sunrpc binfmt_misc xfs libcrc32c dm_service_time sd_mod crc_t10dif crct10dif_common ibmvfc scsi_transport_fc scsi_tgt dm_mirror dm_region_hash dm_log dm_multipath dm_mod
    [ 2736.063383] CPU: 1 PID: 7128 Comm: ssh Not tainted 3.10.0-48.el7.ppc64 #1
    [ 2736.063389] task: c000000131930120 ti: c0000001319a0000 task.ti: c0000001319a0000
    [ 2736.063394] NIP: c00000000037efb0 LR: c0000000006c40f8 CTR: 0000000000000000
    [ 2736.063399] REGS: c0000001319a3870 TRAP: 0300   Not tainted  (3.10.0-48.el7.ppc64)
    [ 2736.063403] MSR: 8000000000009032 <SF,EE,ME,IR,DR,RI>  CR: 28824242  XER: 20000000
    [ 2736.063415] SOFTE: 0
    [ 2736.063418] CFAR: c00000000000908c
    [ 2736.063421] DAR: 0000000000000000, DSISR: 40000000
    [ 2736.063425]
    GPR00: c0000000006c40f8 c0000001319a3af0 c000000001074788 c0000001319a3bf0
    GPR04: 0000000000000000 0000000000000000 0000000000000020 000000000000000a
    GPR08: fffffffe00000002 00000000ffff0000 0000000080000001 c000000000924888
    GPR12: 0000000028824248 c000000007e00400 00001fffffa0f998 0000000000000000
    GPR16: 0000000000000022 00001fffffa0f998 0000010022e92470 0000000000000000
    GPR20: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
    GPR24: 0000000000000000 c000000000f4a828 00003ffffe527108 0000000000000000
    GPR28: c000000000f4a730 c000000000f4a828 0000000000000000 c0000001319a3bf0
    [ 2736.063498] NIP [c00000000037efb0] .__list_add+0x30/0x110
    [ 2736.063504] LR [c0000000006c40f8] .rwsem_down_write_failed+0x78/0x264
    [ 2736.063508] PACATMSCRATCH [800000000280f032]
    [ 2736.063511] Call Trace:
    [ 2736.063516] [c0000001319a3af0] [c0000001319a3b80] 0xc0000001319a3b80 (unreliable)
    [ 2736.063523] [c0000001319a3b80] [c0000000006c40f8] .rwsem_down_write_failed+0x78/0x264
    [ 2736.063530] [c0000001319a3c50] [c0000000006c1bb0] .down_write+0x70/0x78
    [ 2736.063536] [c0000001319a3cd0] [c0000000002e5ffc] .keyctl_get_persistent+0x20c/0x320
    [ 2736.063542] [c0000001319a3dc0] [c0000000002e2388] .SyS_keyctl+0x238/0x260
    [ 2736.063548] [c0000001319a3e30] [c000000000009e7c] syscall_exit+0x0/0x7c
    [ 2736.063553] Instruction dump:
    [ 2736.063556] 7c0802a6 fba1ffe8 fbc1fff0 fbe1fff8 7cbd2b78 7c9e2378 7c7f1b78 f8010010
    [ 2736.063566] f821ff71 e8a50008 7fa52040 40de00c0 <e8be0000> 7fbd2840 40de0094 7fbff040
    [ 2736.063579] ---[ end trace 2708241785538296 ]---
    
    It's caused by uninitialized persistent_keyring_register_sem.
    
    The bug was introduced by commit f36f8c75, two typos are in that commit:
    CONFIG_KEYS_KERBEROS_CACHE should be CONFIG_PERSISTENT_KEYRINGS and
    krb_cache_register_sem should be persistent_keyring_register_sem.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@linux.vnet.ibm.com>
    Signed-off-by: David Howells <dhowells@redhat.com>

diff --git a/kernel/user.c b/kernel/user.c
index a3a0dbfda329..c006131beb77 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -51,9 +51,9 @@ struct user_namespace init_user_ns = {
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
 	.proc_inum = PROC_USER_INIT_INO,
-#ifdef CONFIG_KEYS_KERBEROS_CACHE
-	.krb_cache_register_sem =
-	__RWSEM_INITIALIZER(init_user_ns.krb_cache_register_sem),
+#ifdef CONFIG_PERSISTENT_KEYRINGS
+	.persistent_keyring_register_sem =
+	__RWSEM_INITIALIZER(init_user_ns.persistent_keyring_register_sem),
 #endif
 };
 EXPORT_SYMBOL_GPL(init_user_ns);

commit f36f8c75ae2e7d4da34f4c908cebdb4aa42c977e
Author: David Howells <dhowells@redhat.com>
Date:   Tue Sep 24 10:35:19 2013 +0100

    KEYS: Add per-user_namespace registers for persistent per-UID kerberos caches
    
    Add support for per-user_namespace registers of persistent per-UID kerberos
    caches held within the kernel.
    
    This allows the kerberos cache to be retained beyond the life of all a user's
    processes so that the user's cron jobs can work.
    
    The kerberos cache is envisioned as a keyring/key tree looking something like:
    
            struct user_namespace
              \___ .krb_cache keyring               - The register
                    \___ _krb.0 keyring             - Root's Kerberos cache
                    \___ _krb.5000 keyring          - User 5000's Kerberos cache
                    \___ _krb.5001 keyring          - User 5001's Kerberos cache
                            \___ tkt785 big_key     - A ccache blob
                            \___ tkt12345 big_key   - Another ccache blob
    
    Or possibly:
    
            struct user_namespace
              \___ .krb_cache keyring               - The register
                    \___ _krb.0 keyring             - Root's Kerberos cache
                    \___ _krb.5000 keyring          - User 5000's Kerberos cache
                    \___ _krb.5001 keyring          - User 5001's Kerberos cache
                            \___ tkt785 keyring     - A ccache
                                    \___ krbtgt/REDHAT.COM@REDHAT.COM big_key
                                    \___ http/REDHAT.COM@REDHAT.COM user
                                    \___ afs/REDHAT.COM@REDHAT.COM user
                                    \___ nfs/REDHAT.COM@REDHAT.COM user
                                    \___ krbtgt/KERNEL.ORG@KERNEL.ORG big_key
                                    \___ http/KERNEL.ORG@KERNEL.ORG big_key
    
    What goes into a particular Kerberos cache is entirely up to userspace.  Kernel
    support is limited to giving you the Kerberos cache keyring that you want.
    
    The user asks for their Kerberos cache by:
    
            krb_cache = keyctl_get_krbcache(uid, dest_keyring);
    
    The uid is -1 or the user's own UID for the user's own cache or the uid of some
    other user's cache (requires CAP_SETUID).  This permits rpc.gssd or whatever to
    mess with the cache.
    
    The cache returned is a keyring named "_krb.<uid>" that the possessor can read,
    search, clear, invalidate, unlink from and add links to.  Active LSMs get a
    chance to rule on whether the caller is permitted to make a link.
    
    Each uid's cache keyring is created when it first accessed and is given a
    timeout that is extended each time this function is called so that the keyring
    goes away after a while.  The timeout is configurable by sysctl but defaults to
    three days.
    
    Each user_namespace struct gets a lazily-created keyring that serves as the
    register.  The cache keyrings are added to it.  This means that standard key
    search and garbage collection facilities are available.
    
    The user_namespace struct's register goes away when it does and anything left
    in it is then automatically gc'd.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Tested-by: Simo Sorce <simo@redhat.com>
    cc: Serge E. Hallyn <serge.hallyn@ubuntu.com>
    cc: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 5bbb91988e69..a3a0dbfda329 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -51,6 +51,10 @@ struct user_namespace init_user_ns = {
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
 	.proc_inum = PROC_USER_INIT_INO,
+#ifdef CONFIG_KEYS_KERBEROS_CACHE
+	.krb_cache_register_sem =
+	__RWSEM_INITIALIZER(init_user_ns.krb_cache_register_sem),
+#endif
 };
 EXPORT_SYMBOL_GPL(init_user_ns);
 

commit e51db73532955dc5eaba4235e62b74b460709d5b
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat Mar 30 19:57:41 2013 -0700

    userns: Better restrictions on when proc and sysfs can be mounted
    
    Rely on the fact that another flavor of the filesystem is already
    mounted and do not rely on state in the user namespace.
    
    Verify that the mounted filesystem is not covered in any significant
    way.  I would love to verify that the previously mounted filesystem
    has no mounts on top but there are at least the directories
    /proc/sys/fs/binfmt_misc and /sys/fs/cgroup/ that exist explicitly
    for other filesystems to mount on top of.
    
    Refactor the test into a function named fs_fully_visible and call that
    function from the mount routines of proc and sysfs.  This makes this
    test local to the filesystems involved and the results current of when
    the mounts take place, removing a weird threading of the user
    namespace, the mount namespace and the filesystems themselves.
    
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 69b4c3d48cde..5bbb91988e69 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -51,8 +51,6 @@ struct user_namespace init_user_ns = {
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
 	.proc_inum = PROC_USER_INIT_INO,
-	.may_mount_sysfs = true,
-	.may_mount_proc = true,
 };
 EXPORT_SYMBOL_GPL(init_user_ns);
 

commit 20b4fb485227404329e41ad15588afad3df23050
Merge: b9394d8a657c ac3e3c5b1164
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 1 17:51:54 2013 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull VFS updates from Al Viro,
    
    Misc cleanups all over the place, mainly wrt /proc interfaces (switch
    create_proc_entry to proc_create(), get rid of the deprecated
    create_proc_read_entry() in favor of using proc_create_data() and
    seq_file etc).
    
    7kloc removed.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (204 commits)
      don't bother with deferred freeing of fdtables
      proc: Move non-public stuff from linux/proc_fs.h to fs/proc/internal.h
      proc: Make the PROC_I() and PDE() macros internal to procfs
      proc: Supply a function to remove a proc entry by PDE
      take cgroup_open() and cpuset_open() to fs/proc/base.c
      ppc: Clean up scanlog
      ppc: Clean up rtas_flash driver somewhat
      hostap: proc: Use remove_proc_subtree()
      drm: proc: Use remove_proc_subtree()
      drm: proc: Use minor->index to label things, not PDE->name
      drm: Constify drm_proc_list[]
      zoran: Don't print proc_dir_entry data in debug
      reiserfs: Don't access the proc_dir_entry in r_open(), r_start() r_show()
      proc: Supply an accessor for getting the data from a PDE's parent
      airo: Use remove_proc_subtree()
      rtl8192u: Don't need to save device proc dir PDE
      rtl8187se: Use a dir under /proc/net/r8180/
      proc: Add proc_mkdir_data()
      proc: Move some bits from linux/proc_fs.h to linux/{of.h,signal.h,tty.h}
      proc: Move PDE_NET() to fs/proc/proc_net.c
      ...

commit 0bb80f240520c4148b623161e7856858c021696d
Author: David Howells <dhowells@redhat.com>
Date:   Fri Apr 12 01:50:06 2013 +0100

    proc: Split the namespace stuff out into linux/proc_ns.h
    
    Split the proc namespace stuff out into linux/proc_ns.h.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    cc: netdev@vger.kernel.org
    cc: Serge E. Hallyn <serge.hallyn@ubuntu.com>
    cc: Eric W. Biederman <ebiederm@xmission.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/user.c b/kernel/user.c
index e81978e8c03b..5bbb91988e69 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -16,7 +16,7 @@
 #include <linux/interrupt.h>
 #include <linux/export.h>
 #include <linux/user_namespace.h>
-#include <linux/proc_fs.h>
+#include <linux/proc_ns.h>
 
 /*
  * userns count is 1 for root user, 1 for init_uts_ns,

commit 87a8ebd637dafc255070f503909a053cf0d98d3f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sun Mar 24 14:28:27 2013 -0700

    userns: Restrict when proc and sysfs can be mounted
    
    Only allow unprivileged mounts of proc and sysfs if they are already
    mounted when the user namespace is created.
    
    proc and sysfs are interesting because they have content that is
    per namespace, and so fresh mounts are needed when new namespaces
    are created while at the same time proc and sysfs have content that
    is shared between every instance.
    
    Respect the policy of who may see the shared content of proc and sysfs
    by only allowing new mounts if there was an existing mount at the time
    the user namespace was created.
    
    In practice there are only two interesting cases: proc and sysfs are
    mounted at their usual places, proc and sysfs are not mounted at all
    (some form of mount namespace jail).
    
    Cc: stable@vger.kernel.org
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index e81978e8c03b..8e635a18ab52 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -51,6 +51,8 @@ struct user_namespace init_user_ns = {
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
 	.proc_inum = PROC_USER_INIT_INO,
+	.may_mount_sysfs = true,
+	.may_mount_proc = true,
 };
 EXPORT_SYMBOL_GPL(init_user_ns);
 

commit b67bfe0d42cac56c512dd5da4b1b347a23f4b70a
Author: Sasha Levin <sasha.levin@oracle.com>
Date:   Wed Feb 27 17:06:00 2013 -0800

    hlist: drop the node parameter from iterators
    
    I'm not sure why, but the hlist for each entry iterators were conceived
    
            list_for_each_entry(pos, head, member)
    
    The hlist ones were greedy and wanted an extra parameter:
    
            hlist_for_each_entry(tpos, pos, head, member)
    
    Why did they need an extra pos parameter? I'm not quite sure. Not only
    they don't really need it, it also prevents the iterator from looking
    exactly like the list iterator, which is unfortunate.
    
    Besides the semantic patch, there was some manual work required:
    
     - Fix up the actual hlist iterators in linux/list.h
     - Fix up the declaration of other iterators based on the hlist ones.
     - A very small amount of places were using the 'node' parameter, this
     was modified to use 'obj->member' instead.
     - Coccinelle didn't handle the hlist_for_each_entry_safe iterator
     properly, so those had to be fixed up manually.
    
    The semantic patch which is mostly the work of Peter Senna Tschudin is here:
    
    @@
    iterator name hlist_for_each_entry, hlist_for_each_entry_continue, hlist_for_each_entry_from, hlist_for_each_entry_rcu, hlist_for_each_entry_rcu_bh, hlist_for_each_entry_continue_rcu_bh, for_each_busy_worker, ax25_uid_for_each, ax25_for_each, inet_bind_bucket_for_each, sctp_for_each_hentry, sk_for_each, sk_for_each_rcu, sk_for_each_from, sk_for_each_safe, sk_for_each_bound, hlist_for_each_entry_safe, hlist_for_each_entry_continue_rcu, nr_neigh_for_each, nr_neigh_for_each_safe, nr_node_for_each, nr_node_for_each_safe, for_each_gfn_indirect_valid_sp, for_each_gfn_sp, for_each_host;
    
    type T;
    expression a,c,d,e;
    identifier b;
    statement S;
    @@
    
    -T b;
        <+... when != b
    (
    hlist_for_each_entry(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_continue(a,
    - b,
    c) S
    |
    hlist_for_each_entry_from(a,
    - b,
    c) S
    |
    hlist_for_each_entry_rcu(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_rcu_bh(a,
    - b,
    c, d) S
    |
    hlist_for_each_entry_continue_rcu_bh(a,
    - b,
    c) S
    |
    for_each_busy_worker(a, c,
    - b,
    d) S
    |
    ax25_uid_for_each(a,
    - b,
    c) S
    |
    ax25_for_each(a,
    - b,
    c) S
    |
    inet_bind_bucket_for_each(a,
    - b,
    c) S
    |
    sctp_for_each_hentry(a,
    - b,
    c) S
    |
    sk_for_each(a,
    - b,
    c) S
    |
    sk_for_each_rcu(a,
    - b,
    c) S
    |
    sk_for_each_from
    -(a, b)
    +(a)
    S
    + sk_for_each_from(a) S
    |
    sk_for_each_safe(a,
    - b,
    c, d) S
    |
    sk_for_each_bound(a,
    - b,
    c) S
    |
    hlist_for_each_entry_safe(a,
    - b,
    c, d, e) S
    |
    hlist_for_each_entry_continue_rcu(a,
    - b,
    c) S
    |
    nr_neigh_for_each(a,
    - b,
    c) S
    |
    nr_neigh_for_each_safe(a,
    - b,
    c, d) S
    |
    nr_node_for_each(a,
    - b,
    c) S
    |
    nr_node_for_each_safe(a,
    - b,
    c, d) S
    |
    - for_each_gfn_sp(a, c, d, b) S
    + for_each_gfn_sp(a, c, d) S
    |
    - for_each_gfn_indirect_valid_sp(a, c, d, b) S
    + for_each_gfn_indirect_valid_sp(a, c, d) S
    |
    for_each_host(a,
    - b,
    c) S
    |
    for_each_host_safe(a,
    - b,
    c, d) S
    |
    for_each_mesh_entry(a,
    - b,
    c, d) S
    )
        ...+>
    
    [akpm@linux-foundation.org: drop bogus change from net/ipv4/raw.c]
    [akpm@linux-foundation.org: drop bogus hunk from net/ipv6/raw.c]
    [akpm@linux-foundation.org: checkpatch fixes]
    [akpm@linux-foundation.org: fix warnings]
    [akpm@linux-foudnation.org: redo intrusive kvm changes]
    Tested-by: Peter Senna Tschudin <peter.senna@gmail.com>
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Sasha Levin <sasha.levin@oracle.com>
    Cc: Wu Fengguang <fengguang.wu@intel.com>
    Cc: Marcelo Tosatti <mtosatti@redhat.com>
    Cc: Gleb Natapov <gleb@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 57ebfd42023c..e81978e8c03b 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -105,9 +105,8 @@ static void uid_hash_remove(struct user_struct *up)
 static struct user_struct *uid_hash_find(kuid_t uid, struct hlist_head *hashent)
 {
 	struct user_struct *user;
-	struct hlist_node *h;
 
-	hlist_for_each_entry(user, h, hashent, uidhash_node) {
+	hlist_for_each_entry(user, hashent, uidhash_node) {
 		if (uid_eq(user->uid, uid)) {
 			atomic_inc(&user->__count);
 			return user;

commit c61a2810a2161986353705b44d9503e6bb079f4f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Fri Dec 28 18:58:39 2012 -0800

    userns: Avoid recursion in put_user_ns
    
    When freeing a deeply nested user namespace free_user_ns calls
    put_user_ns on it's parent which may in turn call free_user_ns again.
    When -fno-optimize-sibling-calls is passed to gcc one stack frame per
    user namespace is left on the stack, potentially overflowing the
    kernel stack.  CONFIG_FRAME_POINTER forces -fno-optimize-sibling-calls
    so we can't count on gcc to optimize this code.
    
    Remove struct kref and use a plain atomic_t.  Making the code more
    flexible and easier to comprehend.  Make the loop in free_user_ns
    explict to guarantee that the stack does not overflow with
    CONFIG_FRAME_POINTER enabled.
    
    I have tested this fix with a simple program that uses unshare to
    create a deeply nested user namespace structure and then calls exit.
    With 1000 nesteuser namespaces before this change running my test
    program causes the kernel to die a horrible death.  With 10,000,000
    nested user namespaces after this change my test program runs to
    completion and causes no harm.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Pointed-out-by: Vasily Kulikov <segoon@openwall.com>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 33acb5e53a5f..57ebfd42023c 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -47,9 +47,7 @@ struct user_namespace init_user_ns = {
 			.count = 4294967295U,
 		},
 	},
-	.kref = {
-		.refcount	= ATOMIC_INIT(3),
-	},
+	.count = ATOMIC_INIT(3),
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
 	.proc_inum = PROC_USER_INIT_INO,

commit 98f842e675f96ffac96e6c50315790912b2812be
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Jun 15 10:21:48 2011 -0700

    proc: Usable inode numbers for the namespace file descriptors.
    
    Assign a unique proc inode to each namespace, and use that
    inode number to ensure we only allocate at most one proc
    inode for every namespace in proc.
    
    A single proc inode per namespace allows userspace to test
    to see if two processes are in the same namespace.
    
    This has been a long requested feature and only blocked because
    a naive implementation would put the id in a global space and
    would ultimately require having a namespace for the names of
    namespaces, making migration and certain virtualization tricks
    impossible.
    
    We still don't have per superblock inode numbers for proc, which
    appears necessary for application unaware checkpoint/restart and
    migrations (if the application is using namespace file descriptors)
    but that is now allowd by the design if it becomes important.
    
    I have preallocated the ipc and uts initial proc inode numbers so
    their structures can be statically initialized.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 750acffbe9ec..33acb5e53a5f 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -16,6 +16,7 @@
 #include <linux/interrupt.h>
 #include <linux/export.h>
 #include <linux/user_namespace.h>
+#include <linux/proc_fs.h>
 
 /*
  * userns count is 1 for root user, 1 for init_uts_ns,
@@ -51,6 +52,7 @@ struct user_namespace init_user_ns = {
 	},
 	.owner = GLOBAL_ROOT_UID,
 	.group = GLOBAL_ROOT_GID,
+	.proc_inum = PROC_USER_INIT_INO,
 };
 EXPORT_SYMBOL_GPL(init_user_ns);
 

commit f76d207a66c3a53defea67e7d36c3eb1b7d6d61d
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Aug 30 01:24:05 2012 -0700

    userns: Add kprojid_t and associated infrastructure in projid.h
    
    Implement kprojid_t a cousin of the kuid_t and kgid_t.
    
    The per user namespace mapping of project id values can be set with
    /proc/<pid>/projid_map.
    
    A full compliment of helpers is provided: make_kprojid, from_kprojid,
    from_kprojid_munged, kporjid_has_mapping, projid_valid, projid_eq,
    projid_eq, projid_lt.
    
    Project identifiers are part of the generic disk quota interface,
    although it appears only xfs implements project identifiers currently.
    
    The xfs code allows anyone who has permission to set the project
    identifier on a file to use any project identifier so when
    setting up the user namespace project identifier mappings I do
    not require a capability.
    
    Cc: Dave Chinner <david@fromorbit.com>
    Cc: Jan Kara <jack@suse.cz>
    Signed-off-by: "Eric W. Biederman" <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index b815fefbe76f..750acffbe9ec 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -38,6 +38,14 @@ struct user_namespace init_user_ns = {
 			.count = 4294967295U,
 		},
 	},
+	.projid_map = {
+		.nr_extents = 1,
+		.extent[0] = {
+			.first = 0,
+			.lower_first = 0,
+			.count = 4294967295U,
+		},
+	},
 	.kref = {
 		.refcount	= ATOMIC_INIT(3),
 	},

commit 4b06a81f1daee668fbd6de85557bfb36dd36078f
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Sat May 19 15:44:06 2012 -0600

    userns:  Silence silly gcc warning.
    
    On 32bit builds gcc says:
    kernel/user.c:30:4: warning: this decimal constant is unsigned only in ISO C90 [enabled by default]
    kernel/user.c:38:4: warning: this decimal constant is unsigned only in ISO C90 [enabled by default]
    
    Silence gcc by changing the constant 4294967295 to 4294967295U.
    
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index f9e420e36699..b815fefbe76f 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -27,7 +27,7 @@ struct user_namespace init_user_ns = {
 		.extent[0] = {
 			.first = 0,
 			.lower_first = 0,
-			.count = 4294967295,
+			.count = 4294967295U,
 		},
 	},
 	.gid_map = {
@@ -35,7 +35,7 @@ struct user_namespace init_user_ns = {
 		.extent[0] = {
 			.first = 0,
 			.lower_first = 0,
-			.count = 4294967295,
+			.count = 4294967295U,
 		},
 	},
 	.kref = {

commit 22d917d80e842829d0ca0a561967d728eb1d6303
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Nov 17 00:11:58 2011 -0800

    userns: Rework the user_namespace adding uid/gid mapping support
    
    - Convert the old uid mapping functions into compatibility wrappers
    - Add a uid/gid mapping layer from user space uid and gids to kernel
      internal uids and gids that is extent based for simplicty and speed.
      * Working with number space after mapping uids/gids into their kernel
        internal version adds only mapping complexity over what we have today,
        leaving the kernel code easy to understand and test.
    - Add proc files /proc/self/uid_map /proc/self/gid_map
      These files display the mapping and allow a mapping to be added
      if a mapping does not exist.
    - Allow entering the user namespace without a uid or gid mapping.
      Since we are starting with an existing user our uids and gids
      still have global mappings so are still valid and useful they just don't
      have local mappings.  The requirement for things to work are global uid
      and gid so it is odd but perfectly fine not to have a local uid
      and gid mapping.
      Not requiring global uid and gid mappings greatly simplifies
      the logic of setting up the uid and gid mappings by allowing
      the mappings to be set after the namespace is created which makes the
      slight weirdness worth it.
    - Make the mappings in the initial user namespace to the global
      uid/gid space explicit.  Today it is an identity mapping
      but in the future we may want to twist this for debugging, similar
      to what we do with jiffies.
    - Document the memory ordering requirements of setting the uid and
      gid mappings.  We only allow the mappings to be set once
      and there are no pointers involved so the requirments are
      trivial but a little atypical.
    
    Performance:
    
    In this scheme for the permission checks the performance is expected to
    stay the same as the actuall machine instructions should remain the same.
    
    The worst case I could think of is ls -l on a large directory where
    all of the stat results need to be translated with from kuids and
    kgids to uids and gids.  So I benchmarked that case on my laptop
    with a dual core hyperthread Intel i5-2520M cpu with 3M of cpu cache.
    
    My benchmark consisted of going to single user mode where nothing else
    was running. On an ext4 filesystem opening 1,000,000 files and looping
    through all of the files 1000 times and calling fstat on the
    individuals files.  This was to ensure I was benchmarking stat times
    where the inodes were in the kernels cache, but the inode values were
    not in the processors cache.  My results:
    
    v3.4-rc1:         ~= 156ns (unmodified v3.4-rc1 with user namespace support disabled)
    v3.4-rc1-userns-: ~= 155ns (v3.4-rc1 with my user namespace patches and user namespace support disabled)
    v3.4-rc1-userns+: ~= 164ns (v3.4-rc1 with my user namespace patches and user namespace support enabled)
    
    All of the configurations ran in roughly 120ns when I performed tests
    that ran in the cpu cache.
    
    So in summary the performance impact is:
    1ns improvement in the worst case with user namespace support compiled out.
    8ns aka 5% slowdown in the worst case with user namespace support compiled in.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index cff385659175..f9e420e36699 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -22,6 +22,22 @@
  * and 1 for... ?
  */
 struct user_namespace init_user_ns = {
+	.uid_map = {
+		.nr_extents = 1,
+		.extent[0] = {
+			.first = 0,
+			.lower_first = 0,
+			.count = 4294967295,
+		},
+	},
+	.gid_map = {
+		.nr_extents = 1,
+		.extent[0] = {
+			.first = 0,
+			.lower_first = 0,
+			.count = 4294967295,
+		},
+	},
 	.kref = {
 		.refcount	= ATOMIC_INIT(3),
 	},

commit 783291e6900292521a3895583785e0c04a56c5b3
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Thu Nov 17 01:32:59 2011 -0800

    userns: Simplify the user_namespace by making userns->creator a kuid.
    
    - Transform userns->creator from a user_struct reference to a simple
      kuid_t, kgid_t pair.
    
      In cap_capable this allows the check to see if we are the creator of
      a namespace to become the classic suser style euid permission check.
    
      This allows us to remove the need for a struct cred in the mapping
      functions and still be able to dispaly the user namespace creators
      uid and gid as 0.
    
    - Remove the now unnecessary delayed_work in free_user_ns.
    
      All that is left for free_user_ns to do is to call kmem_cache_free
      and put_user_ns.  Those functions can be called in any context
      so call them directly from free_user_ns removing the need for delayed work.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 025077e54a7c..cff385659175 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -25,7 +25,8 @@ struct user_namespace init_user_ns = {
 	.kref = {
 		.refcount	= ATOMIC_INIT(3),
 	},
-	.creator = &root_user,
+	.owner = GLOBAL_ROOT_UID,
+	.group = GLOBAL_ROOT_GID,
 };
 EXPORT_SYMBOL_GPL(init_user_ns);
 
@@ -54,9 +55,9 @@ struct hlist_head uidhash_table[UIDHASH_SZ];
  */
 static DEFINE_SPINLOCK(uidhash_lock);
 
-/* root_user.__count is 2, 1 for init task cred, 1 for init_user_ns->user_ns */
+/* root_user.__count is 1, for init task cred */
 struct user_struct root_user = {
-	.__count	= ATOMIC_INIT(2),
+	.__count	= ATOMIC_INIT(1),
 	.processes	= ATOMIC_INIT(1),
 	.files		= ATOMIC_INIT(0),
 	.sigpending	= ATOMIC_INIT(0),

commit 7b44ab978b77a91b327058a0f4db7e6fcdb90b92
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Nov 16 23:20:58 2011 -0800

    userns: Disassociate user_struct from the user_namespace.
    
    Modify alloc_uid to take a kuid and make the user hash table global.
    Stop holding a reference to the user namespace in struct user_struct.
    
    This simplifies the code and makes the per user accounting not
    care about which user namespace a uid happens to appear in.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index d65fec0615a0..025077e54a7c 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -34,11 +34,14 @@ EXPORT_SYMBOL_GPL(init_user_ns);
  * when changing user ID's (ie setuid() and friends).
  */
 
+#define UIDHASH_BITS	(CONFIG_BASE_SMALL ? 3 : 7)
+#define UIDHASH_SZ	(1 << UIDHASH_BITS)
 #define UIDHASH_MASK		(UIDHASH_SZ - 1)
 #define __uidhashfn(uid)	(((uid >> UIDHASH_BITS) + uid) & UIDHASH_MASK)
-#define uidhashentry(ns, uid)	((ns)->uidhash_table + __uidhashfn((uid)))
+#define uidhashentry(uid)	(uidhash_table + __uidhashfn((__kuid_val(uid))))
 
 static struct kmem_cache *uid_cachep;
+struct hlist_head uidhash_table[UIDHASH_SZ];
 
 /*
  * The uidhash_lock is mostly taken from process context, but it is
@@ -58,7 +61,7 @@ struct user_struct root_user = {
 	.files		= ATOMIC_INIT(0),
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
-	._user_ns	= &init_user_ns,
+	.uid		= GLOBAL_ROOT_UID,
 };
 
 /*
@@ -72,16 +75,15 @@ static void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)
 static void uid_hash_remove(struct user_struct *up)
 {
 	hlist_del_init(&up->uidhash_node);
-	put_user_ns(up->_user_ns); /* It is safe to free the uid hash table now */
 }
 
-static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
+static struct user_struct *uid_hash_find(kuid_t uid, struct hlist_head *hashent)
 {
 	struct user_struct *user;
 	struct hlist_node *h;
 
 	hlist_for_each_entry(user, h, hashent, uidhash_node) {
-		if (user->uid == uid) {
+		if (uid_eq(user->uid, uid)) {
 			atomic_inc(&user->__count);
 			return user;
 		}
@@ -110,14 +112,13 @@ static void free_user(struct user_struct *up, unsigned long flags)
  *
  * If the user_struct could not be found, return NULL.
  */
-struct user_struct *find_user(uid_t uid)
+struct user_struct *find_user(kuid_t uid)
 {
 	struct user_struct *ret;
 	unsigned long flags;
-	struct user_namespace *ns = current_user_ns();
 
 	spin_lock_irqsave(&uidhash_lock, flags);
-	ret = uid_hash_find(uid, uidhashentry(ns, uid));
+	ret = uid_hash_find(uid, uidhashentry(uid));
 	spin_unlock_irqrestore(&uidhash_lock, flags);
 	return ret;
 }
@@ -136,9 +137,9 @@ void free_uid(struct user_struct *up)
 		local_irq_restore(flags);
 }
 
-struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
+struct user_struct *alloc_uid(kuid_t uid)
 {
-	struct hlist_head *hashent = uidhashentry(ns, uid);
+	struct hlist_head *hashent = uidhashentry(uid);
 	struct user_struct *up, *new;
 
 	spin_lock_irq(&uidhash_lock);
@@ -153,8 +154,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
 
-		new->_user_ns = get_user_ns(ns);
-
 		/*
 		 * Before adding this, check whether we raced
 		 * on adding the same user already..
@@ -162,7 +161,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
-			put_user_ns(ns);
 			key_put(new->uid_keyring);
 			key_put(new->session_keyring);
 			kmem_cache_free(uid_cachep, new);
@@ -187,11 +185,11 @@ static int __init uid_cache_init(void)
 			0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
 
 	for(n = 0; n < UIDHASH_SZ; ++n)
-		INIT_HLIST_HEAD(init_user_ns.uidhash_table + n);
+		INIT_HLIST_HEAD(uidhash_table + n);
 
 	/* Insert the root user immediately (init already runs as root) */
 	spin_lock_irq(&uidhash_lock);
-	uid_hash_insert(&root_user, uidhashentry(&init_user_ns, 0));
+	uid_hash_insert(&root_user, uidhashentry(GLOBAL_ROOT_UID));
 	spin_unlock_irq(&uidhash_lock);
 
 	return 0;

commit d0bd6594e286bd6145e04e19e8d3fa2e902cb800
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Wed Nov 16 23:20:58 2011 -0800

    userns: Deprecate and rename the user_namespace reference in the user_struct
    
    With a user_ns reference in struct cred the only user of the user namespace
    reference in struct user_struct is to keep the uid hash table alive.
    
    The user_namespace reference in struct user_struct will be going away soon, and
    I have removed all of the references.  Rename the field from user_ns to _user_ns
    so that the compiler can verify nothing follows the user struct to the user
    namespace anymore.
    
    Acked-by: Serge Hallyn <serge.hallyn@canonical.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/user.c b/kernel/user.c
index 71dd2363ab0f..d65fec0615a0 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -58,7 +58,7 @@ struct user_struct root_user = {
 	.files		= ATOMIC_INIT(0),
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
-	.user_ns	= &init_user_ns,
+	._user_ns	= &init_user_ns,
 };
 
 /*
@@ -72,7 +72,7 @@ static void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)
 static void uid_hash_remove(struct user_struct *up)
 {
 	hlist_del_init(&up->uidhash_node);
-	put_user_ns(up->user_ns);
+	put_user_ns(up->_user_ns); /* It is safe to free the uid hash table now */
 }
 
 static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
@@ -153,7 +153,7 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
 
-		new->user_ns = get_user_ns(ns);
+		new->_user_ns = get_user_ns(ns);
 
 		/*
 		 * Before adding this, check whether we raced

commit 9984de1a5a8a96275fcab818f7419af5a3c86e71
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Mon May 23 14:51:41 2011 -0400

    kernel: Map most files to use export.h instead of module.h
    
    The changed files were only including linux/module.h for the
    EXPORT_SYMBOL infrastructure, and nothing else.  Revector them
    onto the isolated export header for faster compile times.
    
    Nothing to see here but a whole lot of instances of:
    
      -#include <linux/module.h>
      +#include <linux/export.h>
    
    This commit is only changing the kernel dir; next targets
    will probably be mm, fs, the arch dirs, etc.
    
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>

diff --git a/kernel/user.c b/kernel/user.c
index 9e03e9c1df8d..71dd2363ab0f 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -14,7 +14,7 @@
 #include <linux/bitops.h>
 #include <linux/key.h>
 #include <linux/interrupt.h>
-#include <linux/module.h>
+#include <linux/export.h>
 #include <linux/user_namespace.h>
 
 /*

commit 59607db367c57f515183cb203642291bb14d9c40
Author: Serge E. Hallyn <serge@hallyn.com>
Date:   Wed Mar 23 16:43:16 2011 -0700

    userns: add a user_namespace as creator/owner of uts_namespace
    
    The expected course of development for user namespaces targeted
    capabilities is laid out at https://wiki.ubuntu.com/UserNamespace.
    
    Goals:
    
    - Make it safe for an unprivileged user to unshare namespaces.  They
      will be privileged with respect to the new namespace, but this should
      only include resources which the unprivileged user already owns.
    
    - Provide separate limits and accounting for userids in different
      namespaces.
    
    Status:
    
      Currently (as of 2.6.38) you can clone with the CLONE_NEWUSER flag to
      get a new user namespace if you have the CAP_SYS_ADMIN, CAP_SETUID, and
      CAP_SETGID capabilities.  What this gets you is a whole new set of
      userids, meaning that user 500 will have a different 'struct user' in
      your namespace than in other namespaces.  So any accounting information
      stored in struct user will be unique to your namespace.
    
      However, throughout the kernel there are checks which
    
      - simply check for a capability.  Since root in a child namespace
        has all capabilities, this means that a child namespace is not
        constrained.
    
      - simply compare uid1 == uid2.  Since these are the integer uids,
        uid 500 in namespace 1 will be said to be equal to uid 500 in
        namespace 2.
    
      As a result, the lxc implementation at lxc.sf.net does not use user
      namespaces.  This is actually helpful because it leaves us free to
      develop user namespaces in such a way that, for some time, user
      namespaces may be unuseful.
    
    Bugs aside, this patchset is supposed to not at all affect systems which
    are not actively using user namespaces, and only restrict what tasks in
    child user namespace can do.  They begin to limit privilege to a user
    namespace, so that root in a container cannot kill or ptrace tasks in the
    parent user namespace, and can only get world access rights to files.
    Since all files currently belong to the initila user namespace, that means
    that child user namespaces can only get world access rights to *all*
    files.  While this temporarily makes user namespaces bad for system
    containers, it starts to get useful for some sandboxing.
    
    I've run the 'runltplite.sh' with and without this patchset and found no
    difference.
    
    This patch:
    
    copy_process() handles CLONE_NEWUSER before the rest of the namespaces.
    So in the case of clone(CLONE_NEWUSER|CLONE_NEWUTS) the new uts namespace
    will have the new user namespace as its owner.  That is what we want,
    since we want root in that new userns to be able to have privilege over
    it.
    
    Changelog:
            Feb 15: don't set uts_ns->user_ns if we didn't create
                    a new uts_ns.
            Feb 23: Move extern init_user_ns declaration from
                    init/version.c to utsname.h.
    
    Signed-off-by: Serge E. Hallyn <serge.hallyn@canonical.com>
    Acked-by: "Eric W. Biederman" <ebiederm@xmission.com>
    Acked-by: Daniel Lezcano <daniel.lezcano@free.fr>
    Acked-by: David Howells <dhowells@redhat.com>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 5c598ca781df..9e03e9c1df8d 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -17,9 +17,13 @@
 #include <linux/module.h>
 #include <linux/user_namespace.h>
 
+/*
+ * userns count is 1 for root user, 1 for init_uts_ns,
+ * and 1 for... ?
+ */
 struct user_namespace init_user_ns = {
 	.kref = {
-		.refcount	= ATOMIC_INIT(2),
+		.refcount	= ATOMIC_INIT(3),
 	},
 	.creator = &root_user,
 };
@@ -47,7 +51,7 @@ static struct kmem_cache *uid_cachep;
  */
 static DEFINE_SPINLOCK(uidhash_lock);
 
-/* root_user.__count is 2, 1 for init task cred, 1 for init_user_ns->creator */
+/* root_user.__count is 2, 1 for init task cred, 1 for init_user_ns->user_ns */
 struct user_struct root_user = {
 	.__count	= ATOMIC_INIT(2),
 	.processes	= ATOMIC_INIT(1),

commit 4ef9e11d6867f88951e30db910fa015300e31871
Author: Hillf Danton <dhillf@gmail.com>
Date:   Wed Dec 29 21:55:28 2010 +0800

    fix freeing user_struct in user cache
    
    When racing on adding into user cache, the new allocated from mm slab
    is freed without putting user namespace.
    
    Since the user namespace is already operated by getting, putting has
    to be issued.
    
    Signed-off-by: Hillf Danton <dhillf@gmail.com>
    Acked-by: Serge Hallyn <serge@hallyn.com>
    Cc: stable@kernel.org
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 2c7d8d5914b1..5c598ca781df 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -158,6 +158,7 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
+			put_user_ns(ns);
 			key_put(new->uid_keyring);
 			key_put(new->session_keyring);
 			kmem_cache_free(uid_cachep, new);

commit 571428be550fbe37160596995e96ad398873fcbd
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Tue Oct 26 14:22:43 2010 -0700

    kernel/user.c: add lock release annotation on free_user()
    
    free_user() releases uidhash_lock but was missing annotation.  Add it.
    This removes following sparse warnings:
    
     include/linux/spinlock.h:339:9: warning: context imbalance in 'free_user' - unexpected unlock
     kernel/user.c:120:6: warning: context imbalance in 'free_uid' - wrong count at exit
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Dhaval Giani <dhaval.giani@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 7e72614b736d..2c7d8d5914b1 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -91,6 +91,7 @@ static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
  * upon function exit.
  */
 static void free_user(struct user_struct *up, unsigned long flags)
+	__releases(&uidhash_lock)
 {
 	uid_hash_remove(up);
 	spin_unlock_irqrestore(&uidhash_lock, flags);

commit af507ae8a0512a83728b17d8f8c5fa1561669f50
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon May 10 11:24:27 2010 +0800

    sched: Remove a stale comment
    
    This comment should have been removed together with uids_mutex
    when removing user sched.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Dhaval Giani <dhaval.giani@gmail.com>
    LKML-Reference: <4BE77C6B.5010402@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 8e1c8c0a496c..7e72614b736d 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -136,7 +136,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 	struct hlist_head *hashent = uidhashentry(ns, uid);
 	struct user_struct *up, *new;
 
-	/* Make uid_hash_find() + uid_hash_insert() atomic. */
 	spin_lock_irq(&uidhash_lock);
 	up = uid_hash_find(uid, hashent);
 	spin_unlock_irq(&uidhash_lock);

commit 32bd7eb5a7f4596c8440dd9440322fe9e686634d
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Mar 24 13:17:19 2010 +0800

    sched: Remove remaining USER_SCHED code
    
    This is left over from commit 7c9414385e ("sched: Remove USER_SCHED"")
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Dhaval Giani <dhaval.giani@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: David Howells <dhowells@redhat.com>
    LKML-Reference: <4BA9A05F.7010407@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index ec3b2229893b..8e1c8c0a496c 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -16,7 +16,6 @@
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/user_namespace.h>
-#include "cred-internals.h"
 
 struct user_namespace init_user_ns = {
 	.kref = {
@@ -137,9 +136,7 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 	struct hlist_head *hashent = uidhashentry(ns, uid);
 	struct user_struct *up, *new;
 
-	/* Make uid_hash_find() + uids_user_create() + uid_hash_insert()
-	 * atomic.
-	 */
+	/* Make uid_hash_find() + uid_hash_insert() atomic. */
 	spin_lock_irq(&uidhash_lock);
 	up = uid_hash_find(uid, hashent);
 	spin_unlock_irq(&uidhash_lock);
@@ -161,11 +158,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
-			/* This case is not possible when CONFIG_USER_SCHED
-			 * is defined, since we serialize alloc_uid() using
-			 * uids_mutex. Hence no need to call
-			 * sched_destroy_user() or remove_user_sysfs_dir().
-			 */
 			key_put(new->uid_keyring);
 			key_put(new->session_keyring);
 			kmem_cache_free(uid_cachep, new);

commit 6427462bfa50f50dc6c088c07037264fcc73eca1
Author: Dan Carpenter <error27@gmail.com>
Date:   Mon Mar 15 11:21:48 2010 +0300

    sched: Remove some dead code
    
    This was left over from "7c9414385e sched: Remove USER_SCHED"
    
    Signed-off-by: Dan Carpenter <error27@gmail.com>
    Acked-by: Dhaval Giani <dhaval.giani@gmail.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Cc: Greg Kroah-Hartman <gregkh@suse.de>
    LKML-Reference: <20100315082148.GD18181@bicker>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 766467b3bcb7..ec3b2229893b 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -178,8 +178,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 
 	return up;
 
-	put_user_ns(new->user_ns);
-	kmem_cache_free(uid_cachep, new);
 out_unlock:
 	return NULL;
 }

commit 7c9414385ebfdd87cc542d4e7e3bb0dbb2d3ce25
Author: Dhaval Giani <dhaval.giani@gmail.com>
Date:   Wed Jan 20 13:26:18 2010 +0100

    sched: Remove USER_SCHED
    
    Remove the USER_SCHED feature. It has been scheduled to be removed in
    2.6.34 as per http://marc.info/?l=linux-kernel&m=125728479022976&w=2
    
    Signed-off-by: Dhaval Giani <dhaval.giani@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1263990378.24844.3.camel@localhost>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 46d0165ca70c..766467b3bcb7 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -56,9 +56,6 @@ struct user_struct root_user = {
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
 	.user_ns	= &init_user_ns,
-#ifdef CONFIG_USER_SCHED
-	.tg		= &init_task_group,
-#endif
 };
 
 /*
@@ -75,268 +72,6 @@ static void uid_hash_remove(struct user_struct *up)
 	put_user_ns(up->user_ns);
 }
 
-#ifdef CONFIG_USER_SCHED
-
-static void sched_destroy_user(struct user_struct *up)
-{
-	sched_destroy_group(up->tg);
-}
-
-static int sched_create_user(struct user_struct *up)
-{
-	int rc = 0;
-
-	up->tg = sched_create_group(&root_task_group);
-	if (IS_ERR(up->tg))
-		rc = -ENOMEM;
-
-	set_tg_uid(up);
-
-	return rc;
-}
-
-#else	/* CONFIG_USER_SCHED */
-
-static void sched_destroy_user(struct user_struct *up) { }
-static int sched_create_user(struct user_struct *up) { return 0; }
-
-#endif	/* CONFIG_USER_SCHED */
-
-#if defined(CONFIG_USER_SCHED) && defined(CONFIG_SYSFS)
-
-static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
-{
-	struct user_struct *user;
-	struct hlist_node *h;
-
-	hlist_for_each_entry(user, h, hashent, uidhash_node) {
-		if (user->uid == uid) {
-			/* possibly resurrect an "almost deleted" object */
-			if (atomic_inc_return(&user->__count) == 1)
-				cancel_delayed_work(&user->work);
-			return user;
-		}
-	}
-
-	return NULL;
-}
-
-static struct kset *uids_kset; /* represents the /sys/kernel/uids/ directory */
-static DEFINE_MUTEX(uids_mutex);
-
-static inline void uids_mutex_lock(void)
-{
-	mutex_lock(&uids_mutex);
-}
-
-static inline void uids_mutex_unlock(void)
-{
-	mutex_unlock(&uids_mutex);
-}
-
-/* uid directory attributes */
-#ifdef CONFIG_FAIR_GROUP_SCHED
-static ssize_t cpu_shares_show(struct kobject *kobj,
-			       struct kobj_attribute *attr,
-			       char *buf)
-{
-	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
-
-	return sprintf(buf, "%lu\n", sched_group_shares(up->tg));
-}
-
-static ssize_t cpu_shares_store(struct kobject *kobj,
-				struct kobj_attribute *attr,
-				const char *buf, size_t size)
-{
-	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
-	unsigned long shares;
-	int rc;
-
-	sscanf(buf, "%lu", &shares);
-
-	rc = sched_group_set_shares(up->tg, shares);
-
-	return (rc ? rc : size);
-}
-
-static struct kobj_attribute cpu_share_attr =
-	__ATTR(cpu_share, 0644, cpu_shares_show, cpu_shares_store);
-#endif
-
-#ifdef CONFIG_RT_GROUP_SCHED
-static ssize_t cpu_rt_runtime_show(struct kobject *kobj,
-				   struct kobj_attribute *attr,
-				   char *buf)
-{
-	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
-
-	return sprintf(buf, "%ld\n", sched_group_rt_runtime(up->tg));
-}
-
-static ssize_t cpu_rt_runtime_store(struct kobject *kobj,
-				    struct kobj_attribute *attr,
-				    const char *buf, size_t size)
-{
-	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
-	unsigned long rt_runtime;
-	int rc;
-
-	sscanf(buf, "%ld", &rt_runtime);
-
-	rc = sched_group_set_rt_runtime(up->tg, rt_runtime);
-
-	return (rc ? rc : size);
-}
-
-static struct kobj_attribute cpu_rt_runtime_attr =
-	__ATTR(cpu_rt_runtime, 0644, cpu_rt_runtime_show, cpu_rt_runtime_store);
-
-static ssize_t cpu_rt_period_show(struct kobject *kobj,
-				   struct kobj_attribute *attr,
-				   char *buf)
-{
-	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
-
-	return sprintf(buf, "%lu\n", sched_group_rt_period(up->tg));
-}
-
-static ssize_t cpu_rt_period_store(struct kobject *kobj,
-				    struct kobj_attribute *attr,
-				    const char *buf, size_t size)
-{
-	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
-	unsigned long rt_period;
-	int rc;
-
-	sscanf(buf, "%lu", &rt_period);
-
-	rc = sched_group_set_rt_period(up->tg, rt_period);
-
-	return (rc ? rc : size);
-}
-
-static struct kobj_attribute cpu_rt_period_attr =
-	__ATTR(cpu_rt_period, 0644, cpu_rt_period_show, cpu_rt_period_store);
-#endif
-
-/* default attributes per uid directory */
-static struct attribute *uids_attributes[] = {
-#ifdef CONFIG_FAIR_GROUP_SCHED
-	&cpu_share_attr.attr,
-#endif
-#ifdef CONFIG_RT_GROUP_SCHED
-	&cpu_rt_runtime_attr.attr,
-	&cpu_rt_period_attr.attr,
-#endif
-	NULL
-};
-
-/* the lifetime of user_struct is not managed by the core (now) */
-static void uids_release(struct kobject *kobj)
-{
-	return;
-}
-
-static struct kobj_type uids_ktype = {
-	.sysfs_ops = &kobj_sysfs_ops,
-	.default_attrs = uids_attributes,
-	.release = uids_release,
-};
-
-/*
- * Create /sys/kernel/uids/<uid>/cpu_share file for this user
- * We do not create this file for users in a user namespace (until
- * sysfs tagging is implemented).
- *
- * See Documentation/scheduler/sched-design-CFS.txt for ramifications.
- */
-static int uids_user_create(struct user_struct *up)
-{
-	struct kobject *kobj = &up->kobj;
-	int error;
-
-	memset(kobj, 0, sizeof(struct kobject));
-	if (up->user_ns != &init_user_ns)
-		return 0;
-	kobj->kset = uids_kset;
-	error = kobject_init_and_add(kobj, &uids_ktype, NULL, "%d", up->uid);
-	if (error) {
-		kobject_put(kobj);
-		goto done;
-	}
-
-	kobject_uevent(kobj, KOBJ_ADD);
-done:
-	return error;
-}
-
-/* create these entries in sysfs:
- * 	"/sys/kernel/uids" directory
- * 	"/sys/kernel/uids/0" directory (for root user)
- * 	"/sys/kernel/uids/0/cpu_share" file (for root user)
- */
-int __init uids_sysfs_init(void)
-{
-	uids_kset = kset_create_and_add("uids", NULL, kernel_kobj);
-	if (!uids_kset)
-		return -ENOMEM;
-
-	return uids_user_create(&root_user);
-}
-
-/* delayed work function to remove sysfs directory for a user and free up
- * corresponding structures.
- */
-static void cleanup_user_struct(struct work_struct *w)
-{
-	struct user_struct *up = container_of(w, struct user_struct, work.work);
-	unsigned long flags;
-	int remove_user = 0;
-
-	/* Make uid_hash_remove() + sysfs_remove_file() + kobject_del()
-	 * atomic.
-	 */
-	uids_mutex_lock();
-
-	spin_lock_irqsave(&uidhash_lock, flags);
-	if (atomic_read(&up->__count) == 0) {
-		uid_hash_remove(up);
-		remove_user = 1;
-	}
-	spin_unlock_irqrestore(&uidhash_lock, flags);
-
-	if (!remove_user)
-		goto done;
-
-	if (up->user_ns == &init_user_ns) {
-		kobject_uevent(&up->kobj, KOBJ_REMOVE);
-		kobject_del(&up->kobj);
-		kobject_put(&up->kobj);
-	}
-
-	sched_destroy_user(up);
-	key_put(up->uid_keyring);
-	key_put(up->session_keyring);
-	kmem_cache_free(uid_cachep, up);
-
-done:
-	uids_mutex_unlock();
-}
-
-/* IRQs are disabled and uidhash_lock is held upon function entry.
- * IRQ state (as stored in flags) is restored and uidhash_lock released
- * upon function exit.
- */
-static void free_user(struct user_struct *up, unsigned long flags)
-{
-	INIT_DELAYED_WORK(&up->work, cleanup_user_struct);
-	schedule_delayed_work(&up->work, msecs_to_jiffies(1000));
-	spin_unlock_irqrestore(&uidhash_lock, flags);
-}
-
-#else	/* CONFIG_USER_SCHED && CONFIG_SYSFS */
-
 static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
 {
 	struct user_struct *user;
@@ -352,11 +87,6 @@ static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
 	return NULL;
 }
 
-int uids_sysfs_init(void) { return 0; }
-static inline int uids_user_create(struct user_struct *up) { return 0; }
-static inline void uids_mutex_lock(void) { }
-static inline void uids_mutex_unlock(void) { }
-
 /* IRQs are disabled and uidhash_lock is held upon function entry.
  * IRQ state (as stored in flags) is restored and uidhash_lock released
  * upon function exit.
@@ -365,32 +95,11 @@ static void free_user(struct user_struct *up, unsigned long flags)
 {
 	uid_hash_remove(up);
 	spin_unlock_irqrestore(&uidhash_lock, flags);
-	sched_destroy_user(up);
 	key_put(up->uid_keyring);
 	key_put(up->session_keyring);
 	kmem_cache_free(uid_cachep, up);
 }
 
-#endif
-
-#if defined(CONFIG_RT_GROUP_SCHED) && defined(CONFIG_USER_SCHED)
-/*
- * We need to check if a setuid can take place. This function should be called
- * before successfully completing the setuid.
- */
-int task_can_switch_user(struct user_struct *up, struct task_struct *tsk)
-{
-
-	return sched_rt_can_attach(up->tg, tsk);
-
-}
-#else
-int task_can_switch_user(struct user_struct *up, struct task_struct *tsk)
-{
-	return 1;
-}
-#endif
-
 /*
  * Locate the user_struct for the passed UID.  If found, take a ref on it.  The
  * caller must undo that ref with free_uid().
@@ -431,8 +140,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 	/* Make uid_hash_find() + uids_user_create() + uid_hash_insert()
 	 * atomic.
 	 */
-	uids_mutex_lock();
-
 	spin_lock_irq(&uidhash_lock);
 	up = uid_hash_find(uid, hashent);
 	spin_unlock_irq(&uidhash_lock);
@@ -445,14 +152,8 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
 
-		if (sched_create_user(new) < 0)
-			goto out_free_user;
-
 		new->user_ns = get_user_ns(ns);
 
-		if (uids_user_create(new))
-			goto out_destoy_sched;
-
 		/*
 		 * Before adding this, check whether we raced
 		 * on adding the same user already..
@@ -475,17 +176,11 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_unlock_irq(&uidhash_lock);
 	}
 
-	uids_mutex_unlock();
-
 	return up;
 
-out_destoy_sched:
-	sched_destroy_user(new);
 	put_user_ns(new->user_ns);
-out_free_user:
 	kmem_cache_free(uid_cachep, new);
 out_unlock:
-	uids_mutex_unlock();
 	return NULL;
 }
 

commit b00bc0b237055b4c45816325ee14f0bd83e6f590
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Nov 2 13:01:56 2009 +0100

    uids: Prevent tear down race
    
    Ingo triggered the following warning:
    
    WARNING: at lib/debugobjects.c:255 debug_print_object+0x42/0x50()
    Hardware name: System Product Name
    ODEBUG: init active object type: timer_list
    Modules linked in:
    Pid: 2619, comm: dmesg Tainted: G        W  2.6.32-rc5-tip+ #5298
    Call Trace:
     [<81035443>] warn_slowpath_common+0x6a/0x81
     [<8120e483>] ? debug_print_object+0x42/0x50
     [<81035498>] warn_slowpath_fmt+0x29/0x2c
     [<8120e483>] debug_print_object+0x42/0x50
     [<8120ec2a>] __debug_object_init+0x279/0x2d7
     [<8120ecb3>] debug_object_init+0x13/0x18
     [<810409d2>] init_timer_key+0x17/0x6f
     [<81041526>] free_uid+0x50/0x6c
     [<8104ed2d>] put_cred_rcu+0x61/0x72
     [<81067fac>] rcu_do_batch+0x70/0x121
    
    debugobjects warns about an enqueued timer being initialized. If
    CONFIG_USER_SCHED=y the user management code uses delayed work to
    remove the user from the hash table and tear down the sysfs objects.
    
    free_uid is called from RCU and initializes/schedules delayed work if
    the usage count of the user_struct is 0. The init/schedule happens
    outside of the uidhash_lock protected region which allows a concurrent
    caller of find_user() to reference the about to be destroyed
    user_struct w/o preventing the work from being scheduled. If the next
    free_uid call happens before the work timer expired then the active
    timer is initialized and the work scheduled again.
    
    The race was introduced in commit 5cb350ba (sched: group scheduling,
    sysfs tunables) and made more prominent by commit 3959214f (sched:
    delayed cleanup of user_struct)
    
    Move the init/schedule_delayed_work inside of the uidhash_lock
    protected region to prevent the race.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Cc: Paul E. McKenney <paulmck@us.ibm.com>
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Cc: stable@kernel.org

diff --git a/kernel/user.c b/kernel/user.c
index 2c000e7132ac..46d0165ca70c 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -330,9 +330,9 @@ static void cleanup_user_struct(struct work_struct *w)
  */
 static void free_user(struct user_struct *up, unsigned long flags)
 {
-	spin_unlock_irqrestore(&uidhash_lock, flags);
 	INIT_DELAYED_WORK(&up->work, cleanup_user_struct);
 	schedule_delayed_work(&up->work, msecs_to_jiffies(1000));
+	spin_unlock_irqrestore(&uidhash_lock, flags);
 }
 
 #else	/* CONFIG_USER_SCHED && CONFIG_SYSFS */

commit 3959214f971417f4162926ac52ad4cd042958caa
Author: Kay Sievers <kay.sievers@vrfy.org>
Date:   Tue Mar 24 15:43:30 2009 +0100

    sched: delayed cleanup of user_struct
    
    During bootup performance tracing we see repeated occurrences of
    /sys/kernel/uid/* events for the same uid, leading to a,
    in this case, rather pointless userspace processing for the
    same uid over and over.
    
    This is usually caused by tools which change their uid to "nobody",
    to run without privileges to read data supplied by untrusted users.
    
    This change delays the execution of the (already existing) scheduled
    work, to cleanup the uid after one second, so the allocated and announced
    uid can possibly be re-used by another process.
    
    This is the current behavior, where almost every invocation of a
    binary, which changes the uid, creates two events:
      $ read START < /sys/kernel/uevent_seqnum; \
      for i in `seq 100`; do su --shell=/bin/true bin; done; \
      read END < /sys/kernel/uevent_seqnum; \
      echo $(($END - $START))
      178
    
    With the delayed cleanup, we get only two events, and userspace finishes
    a bit faster too:
      $ read START < /sys/kernel/uevent_seqnum; \
      for i in `seq 100`; do su --shell=/bin/true bin; done; \
      read END < /sys/kernel/uevent_seqnum; \
      echo $(($END - $START))
      1
    
    Acked-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/kernel/user.c b/kernel/user.c
index 850e0ba41c1e..2c000e7132ac 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -75,21 +75,6 @@ static void uid_hash_remove(struct user_struct *up)
 	put_user_ns(up->user_ns);
 }
 
-static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
-{
-	struct user_struct *user;
-	struct hlist_node *h;
-
-	hlist_for_each_entry(user, h, hashent, uidhash_node) {
-		if (user->uid == uid) {
-			atomic_inc(&user->__count);
-			return user;
-		}
-	}
-
-	return NULL;
-}
-
 #ifdef CONFIG_USER_SCHED
 
 static void sched_destroy_user(struct user_struct *up)
@@ -119,6 +104,23 @@ static int sched_create_user(struct user_struct *up) { return 0; }
 
 #if defined(CONFIG_USER_SCHED) && defined(CONFIG_SYSFS)
 
+static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
+{
+	struct user_struct *user;
+	struct hlist_node *h;
+
+	hlist_for_each_entry(user, h, hashent, uidhash_node) {
+		if (user->uid == uid) {
+			/* possibly resurrect an "almost deleted" object */
+			if (atomic_inc_return(&user->__count) == 1)
+				cancel_delayed_work(&user->work);
+			return user;
+		}
+	}
+
+	return NULL;
+}
+
 static struct kset *uids_kset; /* represents the /sys/kernel/uids/ directory */
 static DEFINE_MUTEX(uids_mutex);
 
@@ -283,12 +285,12 @@ int __init uids_sysfs_init(void)
 	return uids_user_create(&root_user);
 }
 
-/* work function to remove sysfs directory for a user and free up
+/* delayed work function to remove sysfs directory for a user and free up
  * corresponding structures.
  */
 static void cleanup_user_struct(struct work_struct *w)
 {
-	struct user_struct *up = container_of(w, struct user_struct, work);
+	struct user_struct *up = container_of(w, struct user_struct, work.work);
 	unsigned long flags;
 	int remove_user = 0;
 
@@ -297,15 +299,12 @@ static void cleanup_user_struct(struct work_struct *w)
 	 */
 	uids_mutex_lock();
 
-	local_irq_save(flags);
-
-	if (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
+	spin_lock_irqsave(&uidhash_lock, flags);
+	if (atomic_read(&up->__count) == 0) {
 		uid_hash_remove(up);
 		remove_user = 1;
-		spin_unlock_irqrestore(&uidhash_lock, flags);
-	} else {
-		local_irq_restore(flags);
 	}
+	spin_unlock_irqrestore(&uidhash_lock, flags);
 
 	if (!remove_user)
 		goto done;
@@ -331,16 +330,28 @@ static void cleanup_user_struct(struct work_struct *w)
  */
 static void free_user(struct user_struct *up, unsigned long flags)
 {
-	/* restore back the count */
-	atomic_inc(&up->__count);
 	spin_unlock_irqrestore(&uidhash_lock, flags);
-
-	INIT_WORK(&up->work, cleanup_user_struct);
-	schedule_work(&up->work);
+	INIT_DELAYED_WORK(&up->work, cleanup_user_struct);
+	schedule_delayed_work(&up->work, msecs_to_jiffies(1000));
 }
 
 #else	/* CONFIG_USER_SCHED && CONFIG_SYSFS */
 
+static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
+{
+	struct user_struct *user;
+	struct hlist_node *h;
+
+	hlist_for_each_entry(user, h, hashent, uidhash_node) {
+		if (user->uid == uid) {
+			atomic_inc(&user->__count);
+			return user;
+		}
+	}
+
+	return NULL;
+}
+
 int uids_sysfs_init(void) { return 0; }
 static inline int uids_user_create(struct user_struct *up) { return 0; }
 static inline void uids_mutex_lock(void) { }

commit 703a3cd72817e99201cef84a8a7aecc60b2b3581
Merge: df7f54c012b9 8e0ee43bc2c3
Author: James Morris <jmorris@namei.org>
Date:   Tue Mar 24 10:52:46 2009 +1100

    Merge branch 'master' into next

commit be50b8342dead8cacf57d4839240106b225d31f5
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Tue Mar 10 12:55:56 2009 -0700

    kernel/user.c: fix a memory leak when freeing up non-init usernamespaces users
    
    We were returning early in the sysfs directory cleanup function if the
    user belonged to a non init usernamespace.  Due to this a lot of the
    cleanup was not done and we were left with a leak.  Fix the leak.
    
    Reported-by: Serge Hallyn <serue@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Tested-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 6a9b696128c8..fbb300e6191f 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -286,14 +286,12 @@ int __init uids_sysfs_init(void)
 /* work function to remove sysfs directory for a user and free up
  * corresponding structures.
  */
-static void remove_user_sysfs_dir(struct work_struct *w)
+static void cleanup_user_struct(struct work_struct *w)
 {
 	struct user_struct *up = container_of(w, struct user_struct, work);
 	unsigned long flags;
 	int remove_user = 0;
 
-	if (up->user_ns != &init_user_ns)
-		return;
 	/* Make uid_hash_remove() + sysfs_remove_file() + kobject_del()
 	 * atomic.
 	 */
@@ -312,9 +310,11 @@ static void remove_user_sysfs_dir(struct work_struct *w)
 	if (!remove_user)
 		goto done;
 
-	kobject_uevent(&up->kobj, KOBJ_REMOVE);
-	kobject_del(&up->kobj);
-	kobject_put(&up->kobj);
+	if (up->user_ns == &init_user_ns) {
+		kobject_uevent(&up->kobj, KOBJ_REMOVE);
+		kobject_del(&up->kobj);
+		kobject_put(&up->kobj);
+	}
 
 	sched_destroy_user(up);
 	key_put(up->uid_keyring);
@@ -335,7 +335,7 @@ static void free_user(struct user_struct *up, unsigned long flags)
 	atomic_inc(&up->__count);
 	spin_unlock_irqrestore(&uidhash_lock, flags);
 
-	INIT_WORK(&up->work, remove_user_sysfs_dir);
+	INIT_WORK(&up->work, cleanup_user_struct);
 	schedule_work(&up->work);
 }
 

commit 54e991242850edc8c53f71fa5aa3ba7a93ce38f5
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Fri Feb 27 15:13:54 2009 +0530

    sched: don't allow setuid to succeed if the user does not have rt bandwidth
    
    Impact: fix hung task with certain (non-default) rt-limit settings
    
    Corey Hickey reported that on using setuid to change the uid of a
    rt process, the process would be unkillable and not be running.
    This is because there was no rt runtime for that user group. Add
    in a check to see if a user can attach an rt task to its task group.
    On failure, return EINVAL, which is also returned in
    CONFIG_CGROUP_SCHED.
    
    Reported-by: Corey Hickey <bugfood-ml@fatooh.org>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 3551ac742395..6a9b696128c8 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -362,6 +362,24 @@ static void free_user(struct user_struct *up, unsigned long flags)
 
 #endif
 
+#if defined(CONFIG_RT_GROUP_SCHED) && defined(CONFIG_USER_SCHED)
+/*
+ * We need to check if a setuid can take place. This function should be called
+ * before successfully completing the setuid.
+ */
+int task_can_switch_user(struct user_struct *up, struct task_struct *tsk)
+{
+
+	return sched_rt_can_attach(up->tg, tsk);
+
+}
+#else
+int task_can_switch_user(struct user_struct *up, struct task_struct *tsk)
+{
+	return 1;
+}
+#endif
+
 /*
  * Locate the user_struct for the passed UID.  If found, take a ref on it.  The
  * caller must undo that ref with free_uid().

commit 1d1e97562e5e2ac60fb7b25437ba619f95f67fab
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Thu Feb 26 18:27:38 2009 -0600

    keys: distinguish per-uid keys in different namespaces
    
    per-uid keys were looked by uid only.  Use the user namespace
    to distinguish the same uid in different namespaces.
    
    This does not address key_permission.  So a task can for instance
    try to join a keyring owned by the same uid in another namespace.
    That will be handled by a separate patch.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/user.c b/kernel/user.c
index 477b6660f447..d8b332c3ae3a 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -20,7 +20,7 @@
 
 struct user_namespace init_user_ns = {
 	.kref = {
-		.refcount	= ATOMIC_INIT(1),
+		.refcount	= ATOMIC_INIT(2),
 	},
 	.creator = &root_user,
 };

commit fb5ae64fdde29236e1a15e0366946df7060f41f2
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Fri Feb 13 14:04:21 2009 +0000

    User namespaces: Only put the userns when we unhash the uid
    
    uids in namespaces other than init don't get a sysfs entry.
    
    For those in the init namespace, while we're waiting to remove
    the sysfs entry for the uid the uid is still hashed, and
    alloc_uid() may re-grab that uid without getting a new
    reference to the user_ns, which we've already put in free_user
    before scheduling remove_user_sysfs_dir().
    
    Reported-and-tested-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: David Howells <dhowells@redhat.com>
    Tested-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 477b6660f447..3551ac742395 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -72,6 +72,7 @@ static void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)
 static void uid_hash_remove(struct user_struct *up)
 {
 	hlist_del_init(&up->uidhash_node);
+	put_user_ns(up->user_ns);
 }
 
 static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
@@ -334,7 +335,6 @@ static void free_user(struct user_struct *up, unsigned long flags)
 	atomic_inc(&up->__count);
 	spin_unlock_irqrestore(&uidhash_lock, flags);
 
-	put_user_ns(up->user_ns);
 	INIT_WORK(&up->work, remove_user_sysfs_dir);
 	schedule_work(&up->work);
 }
@@ -357,7 +357,6 @@ static void free_user(struct user_struct *up, unsigned long flags)
 	sched_destroy_user(up);
 	key_put(up->uid_keyring);
 	key_put(up->session_keyring);
-	put_user_ns(up->user_ns);
 	kmem_cache_free(uid_cachep, up);
 }
 

commit a39b863342b8aba52390092be95db58f6ed56061
Merge: b0f4b285d7ed 4e202284e6ac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Dec 28 12:27:58 2008 -0800

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (31 commits)
      sched: fix warning in fs/proc/base.c
      schedstat: consolidate per-task cpu runtime stats
      sched: use RCU variant of list traversal in for_each_leaf_rt_rq()
      sched, cpuacct: export percpu cpuacct cgroup stats
      sched, cpuacct: refactoring cpuusage_read / cpuusage_write
      sched: optimize update_curr()
      sched: fix wakeup preemption clock
      sched: add missing arch_update_cpu_topology() call
      sched: let arch_update_cpu_topology indicate if topology changed
      sched: idle_balance() does not call load_balance_newidle()
      sched: fix sd_parent_degenerate on non-numa smp machine
      sched: add uid information to sched_debug for CONFIG_USER_SCHED
      sched: move double_unlock_balance() higher
      sched: update comment for move_task_off_dead_cpu
      sched: fix inconsistency when redistribute per-cpu tg->cfs_rq shares
      sched/rt: removed unneeded defintion
      sched: add hierarchical accounting to cpu accounting controller
      sched: include group statistics in /proc/sched_debug
      sched: rename SCHED_NO_NO_OMIT_FRAME_POINTER => SCHED_OMIT_FRAME_POINTER
      sched: clean up SCHED_CPUMASK_ALLOC
      ...

commit 94d6a5f7341ebaff53d4e41cc81fab37f0d9fbed
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Mon Dec 8 15:52:21 2008 -0600

    user namespaces: document CFS behavior
    
    Documented the currently bogus state of support for CFS user groups with
    user namespaces.  In particular, all users in a user namespace should be
    children of the user which created the user namespace.  This is yet to
    be implemented.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/user.c b/kernel/user.c
index 6c924bc48c08..6608a3d8ca61 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -239,7 +239,13 @@ static struct kobj_type uids_ktype = {
 	.release = uids_release,
 };
 
-/* create /sys/kernel/uids/<uid>/cpu_share file for this user */
+/*
+ * Create /sys/kernel/uids/<uid>/cpu_share file for this user
+ * We do not create this file for users in a user namespace (until
+ * sysfs tagging is implemented).
+ *
+ * See Documentation/scheduler/sched-design-CFS.txt for ramifications.
+ */
 static int uids_user_create(struct user_struct *up)
 {
 	struct kobject *kobj = &up->kobj;

commit c37bbb0fdcc01610fd55604eb6927210a1d20044
Author: Serge E. Hallyn <serue@us.ibm.com>
Date:   Wed Dec 3 13:17:06 2008 -0600

    user namespaces: let user_ns be cloned with fairsched
    
    (These two patches are in the next-unacked branch of
    git://git.kernel.org/pub/scm/linux/kernel/git/sergeh/userns-2.6.
    If they get some ACKs, then I hope to feed this into security-next.
    After these two, I think we're ready to tackle userns+capabilities)
    
    Fairsched creates a per-uid directory under /sys/kernel/uids/.
    So when you clone(CLONE_NEWUSER), it tries to create
    /sys/kernel/uids/0, which already exists, and you get back
    -ENOMEM.
    
    This was supposed to be fixed by sysfs tagging, but that
    was postponed (ok, rejected until sysfs locking is fixed).
    So, just as with network namespaces, we just don't create
    those directories for user namespaces other than the init.
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/user.c b/kernel/user.c
index 97202cb29adc..6c924bc48c08 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -246,6 +246,8 @@ static int uids_user_create(struct user_struct *up)
 	int error;
 
 	memset(kobj, 0, sizeof(struct kobject));
+	if (up->user_ns != &init_user_ns)
+		return 0;
 	kobj->kset = uids_kset;
 	error = kobject_init_and_add(kobj, &uids_ktype, NULL, "%d", up->uid);
 	if (error) {
@@ -281,6 +283,8 @@ static void remove_user_sysfs_dir(struct work_struct *w)
 	unsigned long flags;
 	int remove_user = 0;
 
+	if (up->user_ns != &init_user_ns)
+		return;
 	/* Make uid_hash_remove() + sysfs_remove_file() + kobject_del()
 	 * atomic.
 	 */

commit 6c415b9234a8c71f290e5d4fddc467f103f32719
Author: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
Date:   Mon Dec 1 20:49:05 2008 +0530

    sched: add uid information to sched_debug for CONFIG_USER_SCHED
    
    Impact: extend information in /proc/sched_debug
    
    This patch adds uid information in sched_debug for CONFIG_USER_SCHED
    
    Signed-off-by: Arun R Bharadwaj <arun@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 39d6159fae43..cec2224bc9f5 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -101,6 +101,8 @@ static int sched_create_user(struct user_struct *up)
 	if (IS_ERR(up->tg))
 		rc = -ENOMEM;
 
+	set_tg_uid(up);
+
 	return rc;
 }
 

commit 6ded6ab9be4f6164aef1c527407c1b94f0929799
Author: Serge Hallyn <serue@us.ibm.com>
Date:   Mon Nov 24 16:24:10 2008 -0500

    User namespaces: use the current_user_ns() macro
    
    Fix up the last current_user()->user_ns instance to use
    current_user_ns().
    
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>

diff --git a/kernel/user.c b/kernel/user.c
index c0ef3a464438..97202cb29adc 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -361,7 +361,7 @@ struct user_struct *find_user(uid_t uid)
 {
 	struct user_struct *ret;
 	unsigned long flags;
-	struct user_namespace *ns = current_user()->user_ns;
+	struct user_namespace *ns = current_user_ns();
 
 	spin_lock_irqsave(&uidhash_lock, flags);
 	ret = uid_hash_find(uid, uidhashentry(ns, uid));

commit 18b6e0414e42d95183f07d8177e3ff0241abd825
Author: Serge Hallyn <serue@us.ibm.com>
Date:   Wed Oct 15 16:38:45 2008 -0500

    User namespaces: set of cleanups (v2)
    
    The user_ns is moved from nsproxy to user_struct, so that a struct
    cred by itself is sufficient to determine access (which it otherwise
    would not be).  Corresponding ecryptfs fixes (by David Howells) are
    here as well.
    
    Fix refcounting.  The following rules now apply:
            1. The task pins the user struct.
            2. The user struct pins its user namespace.
            3. The user namespace pins the struct user which created it.
    
    User namespaces are cloned during copy_creds().  Unsharing a new user_ns
    is no longer possible.  (We could re-add that, but it'll cause code
    duplication and doesn't seem useful if PAM doesn't need to clone user
    namespaces).
    
    When a user namespace is created, its first user (uid 0) gets empty
    keyrings and a clean group_info.
    
    This incorporates a previous patch by David Howells.  Here
    is his original patch description:
    
    >I suggest adding the attached incremental patch.  It makes the following
    >changes:
    >
    > (1) Provides a current_user_ns() macro to wrap accesses to current's user
    >     namespace.
    >
    > (2) Fixes eCryptFS.
    >
    > (3) Renames create_new_userns() to create_user_ns() to be more consistent
    >     with the other associated functions and because the 'new' in the name is
    >     superfluous.
    >
    > (4) Moves the argument and permission checks made for CLONE_NEWUSER to the
    >     beginning of do_fork() so that they're done prior to making any attempts
    >     at allocation.
    >
    > (5) Calls create_user_ns() after prepare_creds(), and gives it the new creds
    >     to fill in rather than have it return the new root user.  I don't imagine
    >     the new root user being used for anything other than filling in a cred
    >     struct.
    >
    >     This also permits me to get rid of a get_uid() and a free_uid(), as the
    >     reference the creds were holding on the old user_struct can just be
    >     transferred to the new namespace's creator pointer.
    >
    > (6) Makes create_user_ns() reset the UIDs and GIDs of the creds under
    >     preparation rather than doing it in copy_creds().
    >
    >David
    
    >Signed-off-by: David Howells <dhowells@redhat.com>
    
    Changelog:
            Oct 20: integrate dhowells comments
                    1. leave thread_keyring alone
                    2. use current_user_ns() in set_user()
    
    Signed-off-by: Serge Hallyn <serue@us.ibm.com>

diff --git a/kernel/user.c b/kernel/user.c
index d476307dd4b0..c0ef3a464438 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -20,9 +20,9 @@
 
 struct user_namespace init_user_ns = {
 	.kref = {
-		.refcount	= ATOMIC_INIT(2),
+		.refcount	= ATOMIC_INIT(1),
 	},
-	.root_user = &root_user,
+	.creator = &root_user,
 };
 EXPORT_SYMBOL_GPL(init_user_ns);
 
@@ -48,12 +48,14 @@ static struct kmem_cache *uid_cachep;
  */
 static DEFINE_SPINLOCK(uidhash_lock);
 
+/* root_user.__count is 2, 1 for init task cred, 1 for init_user_ns->creator */
 struct user_struct root_user = {
-	.__count	= ATOMIC_INIT(1),
+	.__count	= ATOMIC_INIT(2),
 	.processes	= ATOMIC_INIT(1),
 	.files		= ATOMIC_INIT(0),
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
+	.user_ns	= &init_user_ns,
 #ifdef CONFIG_USER_SCHED
 	.tg		= &init_task_group,
 #endif
@@ -314,12 +316,13 @@ static void remove_user_sysfs_dir(struct work_struct *w)
  * IRQ state (as stored in flags) is restored and uidhash_lock released
  * upon function exit.
  */
-static inline void free_user(struct user_struct *up, unsigned long flags)
+static void free_user(struct user_struct *up, unsigned long flags)
 {
 	/* restore back the count */
 	atomic_inc(&up->__count);
 	spin_unlock_irqrestore(&uidhash_lock, flags);
 
+	put_user_ns(up->user_ns);
 	INIT_WORK(&up->work, remove_user_sysfs_dir);
 	schedule_work(&up->work);
 }
@@ -335,13 +338,14 @@ static inline void uids_mutex_unlock(void) { }
  * IRQ state (as stored in flags) is restored and uidhash_lock released
  * upon function exit.
  */
-static inline void free_user(struct user_struct *up, unsigned long flags)
+static void free_user(struct user_struct *up, unsigned long flags)
 {
 	uid_hash_remove(up);
 	spin_unlock_irqrestore(&uidhash_lock, flags);
 	sched_destroy_user(up);
 	key_put(up->uid_keyring);
 	key_put(up->session_keyring);
+	put_user_ns(up->user_ns);
 	kmem_cache_free(uid_cachep, up);
 }
 
@@ -357,7 +361,7 @@ struct user_struct *find_user(uid_t uid)
 {
 	struct user_struct *ret;
 	unsigned long flags;
-	struct user_namespace *ns = current->nsproxy->user_ns;
+	struct user_namespace *ns = current_user()->user_ns;
 
 	spin_lock_irqsave(&uidhash_lock, flags);
 	ret = uid_hash_find(uid, uidhashentry(ns, uid));
@@ -404,6 +408,8 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 		if (sched_create_user(new) < 0)
 			goto out_free_user;
 
+		new->user_ns = get_user_ns(ns);
+
 		if (uids_user_create(new))
 			goto out_destoy_sched;
 
@@ -427,7 +433,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 			up = new;
 		}
 		spin_unlock_irq(&uidhash_lock);
-
 	}
 
 	uids_mutex_unlock();
@@ -436,6 +441,7 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 
 out_destoy_sched:
 	sched_destroy_user(new);
+	put_user_ns(new->user_ns);
 out_free_user:
 	kmem_cache_free(uid_cachep, new);
 out_unlock:
@@ -443,33 +449,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 	return NULL;
 }
 
-#ifdef CONFIG_USER_NS
-void release_uids(struct user_namespace *ns)
-{
-	int i;
-	unsigned long flags;
-	struct hlist_head *head;
-	struct hlist_node *nd;
-
-	spin_lock_irqsave(&uidhash_lock, flags);
-	/*
-	 * collapse the chains so that the user_struct-s will
-	 * be still alive, but not in hashes. subsequent free_uid()
-	 * will free them.
-	 */
-	for (i = 0; i < UIDHASH_SZ; i++) {
-		head = ns->uidhash_table + i;
-		while (!hlist_empty(head)) {
-			nd = head->first;
-			hlist_del_init(nd);
-		}
-	}
-	spin_unlock_irqrestore(&uidhash_lock, flags);
-
-	free_uid(ns->root_user);
-}
-#endif
-
 static int __init uid_cache_init(void)
 {
 	int n;

commit d84f4f992cbd76e8f39c488cf0c5d123843923b1
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:23 2008 +1100

    CRED: Inaugurate COW credentials
    
    Inaugurate copy-on-write credentials management.  This uses RCU to manage the
    credentials pointer in the task_struct with respect to accesses by other tasks.
    A process may only modify its own credentials, and so does not need locking to
    access or modify its own credentials.
    
    A mutex (cred_replace_mutex) is added to the task_struct to control the effect
    of PTRACE_ATTACHED on credential calculations, particularly with respect to
    execve().
    
    With this patch, the contents of an active credentials struct may not be
    changed directly; rather a new set of credentials must be prepared, modified
    and committed using something like the following sequence of events:
    
            struct cred *new = prepare_creds();
            int ret = blah(new);
            if (ret < 0) {
                    abort_creds(new);
                    return ret;
            }
            return commit_creds(new);
    
    There are some exceptions to this rule: the keyrings pointed to by the active
    credentials may be instantiated - keyrings violate the COW rule as managing
    COW keyrings is tricky, given that it is possible for a task to directly alter
    the keys in a keyring in use by another task.
    
    To help enforce this, various pointers to sets of credentials, such as those in
    the task_struct, are declared const.  The purpose of this is compile-time
    discouragement of altering credentials through those pointers.  Once a set of
    credentials has been made public through one of these pointers, it may not be
    modified, except under special circumstances:
    
      (1) Its reference count may incremented and decremented.
    
      (2) The keyrings to which it points may be modified, but not replaced.
    
    The only safe way to modify anything else is to create a replacement and commit
    using the functions described in Documentation/credentials.txt (which will be
    added by a later patch).
    
    This patch and the preceding patches have been tested with the LTP SELinux
    testsuite.
    
    This patch makes several logical sets of alteration:
    
     (1) execve().
    
         This now prepares and commits credentials in various places in the
         security code rather than altering the current creds directly.
    
     (2) Temporary credential overrides.
    
         do_coredump() and sys_faccessat() now prepare their own credentials and
         temporarily override the ones currently on the acting thread, whilst
         preventing interference from other threads by holding cred_replace_mutex
         on the thread being dumped.
    
         This will be replaced in a future patch by something that hands down the
         credentials directly to the functions being called, rather than altering
         the task's objective credentials.
    
     (3) LSM interface.
    
         A number of functions have been changed, added or removed:
    
         (*) security_capset_check(), ->capset_check()
         (*) security_capset_set(), ->capset_set()
    
             Removed in favour of security_capset().
    
         (*) security_capset(), ->capset()
    
             New.  This is passed a pointer to the new creds, a pointer to the old
             creds and the proposed capability sets.  It should fill in the new
             creds or return an error.  All pointers, barring the pointer to the
             new creds, are now const.
    
         (*) security_bprm_apply_creds(), ->bprm_apply_creds()
    
             Changed; now returns a value, which will cause the process to be
             killed if it's an error.
    
         (*) security_task_alloc(), ->task_alloc_security()
    
             Removed in favour of security_prepare_creds().
    
         (*) security_cred_free(), ->cred_free()
    
             New.  Free security data attached to cred->security.
    
         (*) security_prepare_creds(), ->cred_prepare()
    
             New. Duplicate any security data attached to cred->security.
    
         (*) security_commit_creds(), ->cred_commit()
    
             New. Apply any security effects for the upcoming installation of new
             security by commit_creds().
    
         (*) security_task_post_setuid(), ->task_post_setuid()
    
             Removed in favour of security_task_fix_setuid().
    
         (*) security_task_fix_setuid(), ->task_fix_setuid()
    
             Fix up the proposed new credentials for setuid().  This is used by
             cap_set_fix_setuid() to implicitly adjust capabilities in line with
             setuid() changes.  Changes are made to the new credentials, rather
             than the task itself as in security_task_post_setuid().
    
         (*) security_task_reparent_to_init(), ->task_reparent_to_init()
    
             Removed.  Instead the task being reparented to init is referred
             directly to init's credentials.
    
             NOTE!  This results in the loss of some state: SELinux's osid no
             longer records the sid of the thread that forked it.
    
         (*) security_key_alloc(), ->key_alloc()
         (*) security_key_permission(), ->key_permission()
    
             Changed.  These now take cred pointers rather than task pointers to
             refer to the security context.
    
     (4) sys_capset().
    
         This has been simplified and uses less locking.  The LSM functions it
         calls have been merged.
    
     (5) reparent_to_kthreadd().
    
         This gives the current thread the same credentials as init by simply using
         commit_thread() to point that way.
    
     (6) __sigqueue_alloc() and switch_uid()
    
         __sigqueue_alloc() can't stop the target task from changing its creds
         beneath it, so this function gets a reference to the currently applicable
         user_struct which it then passes into the sigqueue struct it returns if
         successful.
    
         switch_uid() is now called from commit_creds(), and possibly should be
         folded into that.  commit_creds() should take care of protecting
         __sigqueue_alloc().
    
     (7) [sg]et[ug]id() and co and [sg]et_current_groups.
    
         The set functions now all use prepare_creds(), commit_creds() and
         abort_creds() to build and check a new set of credentials before applying
         it.
    
         security_task_set[ug]id() is called inside the prepared section.  This
         guarantees that nothing else will affect the creds until we've finished.
    
         The calling of set_dumpable() has been moved into commit_creds().
    
         Much of the functionality of set_user() has been moved into
         commit_creds().
    
         The get functions all simply access the data directly.
    
     (8) security_task_prctl() and cap_task_prctl().
    
         security_task_prctl() has been modified to return -ENOSYS if it doesn't
         want to handle a function, or otherwise return the return value directly
         rather than through an argument.
    
         Additionally, cap_task_prctl() now prepares a new set of credentials, even
         if it doesn't end up using it.
    
     (9) Keyrings.
    
         A number of changes have been made to the keyrings code:
    
         (a) switch_uid_keyring(), copy_keys(), exit_keys() and suid_keys() have
             all been dropped and built in to the credentials functions directly.
             They may want separating out again later.
    
         (b) key_alloc() and search_process_keyrings() now take a cred pointer
             rather than a task pointer to specify the security context.
    
         (c) copy_creds() gives a new thread within the same thread group a new
             thread keyring if its parent had one, otherwise it discards the thread
             keyring.
    
         (d) The authorisation key now points directly to the credentials to extend
             the search into rather pointing to the task that carries them.
    
         (e) Installing thread, process or session keyrings causes a new set of
             credentials to be created, even though it's not strictly necessary for
             process or session keyrings (they're shared).
    
    (10) Usermode helper.
    
         The usermode helper code now carries a cred struct pointer in its
         subprocess_info struct instead of a new session keyring pointer.  This set
         of credentials is derived from init_cred and installed on the new process
         after it has been cloned.
    
         call_usermodehelper_setup() allocates the new credentials and
         call_usermodehelper_freeinfo() discards them if they haven't been used.  A
         special cred function (prepare_usermodeinfo_creds()) is provided
         specifically for call_usermodehelper_setup() to call.
    
         call_usermodehelper_setkeys() adjusts the credentials to sport the
         supplied keyring as the new session keyring.
    
    (11) SELinux.
    
         SELinux has a number of changes, in addition to those to support the LSM
         interface changes mentioned above:
    
         (a) selinux_setprocattr() no longer does its check for whether the
             current ptracer can access processes with the new SID inside the lock
             that covers getting the ptracer's SID.  Whilst this lock ensures that
             the check is done with the ptracer pinned, the result is only valid
             until the lock is released, so there's no point doing it inside the
             lock.
    
    (12) is_single_threaded().
    
         This function has been extracted from selinux_setprocattr() and put into
         a file of its own in the lib/ directory as join_session_keyring() now
         wants to use it too.
    
         The code in SELinux just checked to see whether a task shared mm_structs
         with other tasks (CLONE_VM), but that isn't good enough.  We really want
         to know if they're part of the same thread group (CLONE_THREAD).
    
    (13) nfsd.
    
         The NFS server daemon now has to use the COW credentials to set the
         credentials it is going to use.  It really needs to pass the credentials
         down to the functions it calls, but it can't do that until other patches
         in this series have been applied.
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/user.c b/kernel/user.c
index 104d22ac84d5..d476307dd4b0 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -16,6 +16,7 @@
 #include <linux/interrupt.h>
 #include <linux/module.h>
 #include <linux/user_namespace.h>
+#include "cred-internals.h"
 
 struct user_namespace init_user_ns = {
 	.kref = {
@@ -104,16 +105,10 @@ static int sched_create_user(struct user_struct *up)
 	return rc;
 }
 
-static void sched_switch_user(struct task_struct *p)
-{
-	sched_move_task(p);
-}
-
 #else	/* CONFIG_USER_SCHED */
 
 static void sched_destroy_user(struct user_struct *up) { }
 static int sched_create_user(struct user_struct *up) { return 0; }
-static void sched_switch_user(struct task_struct *p) { }
 
 #endif	/* CONFIG_USER_SCHED */
 
@@ -448,36 +443,6 @@ struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 	return NULL;
 }
 
-void switch_uid(struct user_struct *new_user)
-{
-	struct user_struct *old_user;
-
-	/* What if a process setreuid()'s and this brings the
-	 * new uid over his NPROC rlimit?  We can check this now
-	 * cheaply with the new uid cache, so if it matters
-	 * we should be checking for it.  -DaveM
-	 */
-	old_user = current->cred->user;
-	atomic_inc(&new_user->processes);
-	atomic_dec(&old_user->processes);
-	switch_uid_keyring(new_user);
-	current->cred->user = new_user;
-	sched_switch_user(current);
-
-	/*
-	 * We need to synchronize with __sigqueue_alloc()
-	 * doing a get_uid(p->user).. If that saw the old
-	 * user value, we need to wait until it has exited
-	 * its critical region before we can free the old
-	 * structure.
-	 */
-	smp_mb();
-	spin_unlock_wait(&current->sighand->siglock);
-
-	free_uid(old_user);
-	suid_keys(current);
-}
-
 #ifdef CONFIG_USER_NS
 void release_uids(struct user_namespace *ns)
 {

commit b6dff3ec5e116e3af6f537d4caedcad6b9e5082a
Author: David Howells <dhowells@redhat.com>
Date:   Fri Nov 14 10:39:16 2008 +1100

    CRED: Separate task security context from task_struct
    
    Separate the task security context from task_struct.  At this point, the
    security data is temporarily embedded in the task_struct with two pointers
    pointing to it.
    
    Note that the Alpha arch is altered as it refers to (E)UID and (E)GID in
    entry.S via asm-offsets.
    
    With comment fixes Signed-off-by: Marc Dionne <marc.c.dionne@gmail.com>
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Acked-by: James Morris <jmorris@namei.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: James Morris <jmorris@namei.org>

diff --git a/kernel/user.c b/kernel/user.c
index 39d6159fae43..104d22ac84d5 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -457,11 +457,11 @@ void switch_uid(struct user_struct *new_user)
 	 * cheaply with the new uid cache, so if it matters
 	 * we should be checking for it.  -DaveM
 	 */
-	old_user = current->user;
+	old_user = current->cred->user;
 	atomic_inc(&new_user->processes);
 	atomic_dec(&old_user->processes);
 	switch_uid_keyring(new_user);
-	current->user = new_user;
+	current->cred->user = new_user;
 	sched_switch_user(current);
 
 	/*

commit af4491e51632d01fbc2b856ffa9ebcd4b38db68c
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Aug 19 12:33:02 2008 +0200

    sched: rt-bandwidth for user grouping interface
    
    rt_runtime is a signed value
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 865ecf57a096..39d6159fae43 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -169,7 +169,7 @@ static ssize_t cpu_rt_runtime_show(struct kobject *kobj,
 {
 	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
 
-	return sprintf(buf, "%lu\n", sched_group_rt_runtime(up->tg));
+	return sprintf(buf, "%ld\n", sched_group_rt_runtime(up->tg));
 }
 
 static ssize_t cpu_rt_runtime_store(struct kobject *kobj,
@@ -180,7 +180,7 @@ static ssize_t cpu_rt_runtime_store(struct kobject *kobj,
 	unsigned long rt_runtime;
 	int rc;
 
-	sscanf(buf, "%lu", &rt_runtime);
+	sscanf(buf, "%ld", &rt_runtime);
 
 	rc = sched_group_set_rt_runtime(up->tg, rt_runtime);
 

commit 354a1f4d99240f53980275416ca3e1ac2ee73d5d
Author: Andrew Morton <akpm@linux-foundation.org>
Date:   Wed Apr 30 00:54:54 2008 -0700

    alloc_uid: cleanup
    
    Use kmem_cache_zalloc(), remove large amounts of initialisation code and
    ifdeffery.
    
    Note: this assumes that memset(*atomic_t, 0) correctly initialises the
    atomic_t.  This is true for all present archtiectures and if it becomes false
    for a future architecture then we'll need to make large changes all over the
    place anyway.
    
    Acked-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index aefbbfa3159f..865ecf57a096 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -384,7 +384,7 @@ void free_uid(struct user_struct *up)
 		local_irq_restore(flags);
 }
 
-struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
+struct user_struct *alloc_uid(struct user_namespace *ns, uid_t uid)
 {
 	struct hlist_head *hashent = uidhashentry(ns, uid);
 	struct user_struct *up, *new;
@@ -399,26 +399,12 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 	spin_unlock_irq(&uidhash_lock);
 
 	if (!up) {
-		new = kmem_cache_alloc(uid_cachep, GFP_KERNEL);
+		new = kmem_cache_zalloc(uid_cachep, GFP_KERNEL);
 		if (!new)
 			goto out_unlock;
 
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
-		atomic_set(&new->processes, 0);
-		atomic_set(&new->files, 0);
-		atomic_set(&new->sigpending, 0);
-#ifdef CONFIG_INOTIFY_USER
-		atomic_set(&new->inotify_watches, 0);
-		atomic_set(&new->inotify_devs, 0);
-#endif
-#ifdef CONFIG_POSIX_MQUEUE
-		new->mq_bytes = 0;
-#endif
-		new->locked_shm = 0;
-#ifdef CONFIG_KEYS
-		new->uid_keyring = new->session_keyring = NULL;
-#endif
 
 		if (sched_create_user(new) < 0)
 			goto out_free_user;

commit 69664cf16af4f31cd54d77948a4baf9c7e0ca7b9
Author: David Howells <dhowells@redhat.com>
Date:   Tue Apr 29 01:01:31 2008 -0700

    keys: don't generate user and user session keyrings unless they're accessed
    
    Don't generate the per-UID user and user session keyrings unless they're
    explicitly accessed.  This solves a problem during a login process whereby
    set*uid() is called before the SELinux PAM module, resulting in the per-UID
    keyrings having the wrong security labels.
    
    This also cures the problem of multiple per-UID keyrings sometimes appearing
    due to PAM modules (including pam_keyinit) setuiding and causing user_structs
    to come into and go out of existence whilst the session keyring pins the user
    keyring.  This is achieved by first searching for extant per-UID keyrings
    before inventing new ones.
    
    The serial bound argument is also dropped from find_keyring_by_name() as it's
    not currently made use of (setting it to 0 disables the feature).
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Cc: <kwc@citi.umich.edu>
    Cc: <arunsr@cse.iitk.ac.in>
    Cc: <dwalsh@redhat.com>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index debce602bfdd..aefbbfa3159f 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -53,10 +53,6 @@ struct user_struct root_user = {
 	.files		= ATOMIC_INIT(0),
 	.sigpending	= ATOMIC_INIT(0),
 	.locked_shm     = 0,
-#ifdef CONFIG_KEYS
-	.uid_keyring	= &root_user_keyring,
-	.session_keyring = &root_session_keyring,
-#endif
 #ifdef CONFIG_USER_SCHED
 	.tg		= &init_task_group,
 #endif
@@ -420,12 +416,12 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 		new->mq_bytes = 0;
 #endif
 		new->locked_shm = 0;
-
-		if (alloc_uid_keyring(new, current) < 0)
-			goto out_free_user;
+#ifdef CONFIG_KEYS
+		new->uid_keyring = new->session_keyring = NULL;
+#endif
 
 		if (sched_create_user(new) < 0)
-			goto out_put_keys;
+			goto out_free_user;
 
 		if (uids_user_create(new))
 			goto out_destoy_sched;
@@ -459,9 +455,6 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 
 out_destoy_sched:
 	sched_destroy_user(new);
-out_put_keys:
-	key_put(new->uid_keyring);
-	key_put(new->session_keyring);
 out_free_user:
 	kmem_cache_free(uid_cachep, new);
 out_unlock:

commit eff766a65c60237bfa865160c3129de31fab591b
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:45:00 2008 +0200

    sched: fix the task_group hierarchy for UID grouping
    
    UID grouping doesn't actually have a task_group representing the root of
    the task_group tree. Add one.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index a28d9f992468..debce602bfdd 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -101,7 +101,7 @@ static int sched_create_user(struct user_struct *up)
 {
 	int rc = 0;
 
-	up->tg = sched_create_group(NULL);
+	up->tg = sched_create_group(&root_task_group);
 	if (IS_ERR(up->tg))
 		rc = -ENOMEM;
 

commit ec7dc8ac73e4a56ed03b673f026f08c0d547f597
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Sat Apr 19 19:44:59 2008 +0200

    sched: allow the group scheduler to have multiple levels
    
    This patch makes the group scheduler multi hierarchy aware.
    
    [a.p.zijlstra@chello.nl: rt-parts and assorted fixes]
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 5925c6887c10..a28d9f992468 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -101,7 +101,7 @@ static int sched_create_user(struct user_struct *up)
 {
 	int rc = 0;
 
-	up->tg = sched_create_group();
+	up->tg = sched_create_group(NULL);
 	if (IS_ERR(up->tg))
 		rc = -ENOMEM;
 

commit d0b27fa77854b149ad4af08b0fe47fe712a47ade
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Sat Apr 19 19:44:57 2008 +0200

    sched: rt-group: synchonised bandwidth period
    
    Various SMP balancing algorithms require that the bandwidth period
    run in sync.
    
    Possible improvements are moving the rt_bandwidth thing into root_domain
    and keeping a span per rt_bandwidth which marks throttled cpus.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 7132022a040c..5925c6887c10 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -193,6 +193,33 @@ static ssize_t cpu_rt_runtime_store(struct kobject *kobj,
 
 static struct kobj_attribute cpu_rt_runtime_attr =
 	__ATTR(cpu_rt_runtime, 0644, cpu_rt_runtime_show, cpu_rt_runtime_store);
+
+static ssize_t cpu_rt_period_show(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
+
+	return sprintf(buf, "%lu\n", sched_group_rt_period(up->tg));
+}
+
+static ssize_t cpu_rt_period_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buf, size_t size)
+{
+	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
+	unsigned long rt_period;
+	int rc;
+
+	sscanf(buf, "%lu", &rt_period);
+
+	rc = sched_group_set_rt_period(up->tg, rt_period);
+
+	return (rc ? rc : size);
+}
+
+static struct kobj_attribute cpu_rt_period_attr =
+	__ATTR(cpu_rt_period, 0644, cpu_rt_period_show, cpu_rt_period_store);
 #endif
 
 /* default attributes per uid directory */
@@ -202,6 +229,7 @@ static struct attribute *uids_attributes[] = {
 #endif
 #ifdef CONFIG_RT_GROUP_SCHED
 	&cpu_rt_runtime_attr.attr,
+	&cpu_rt_period_attr.attr,
 #endif
 	NULL
 };

commit 052f1dc7eb02300b05170ae341ccd03b76207778
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 13 15:45:40 2008 +0100

    sched: rt-group: make rt groups scheduling configurable
    
    Make the rt group scheduler compile time configurable.
    Keep it experimental for now.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 9f6d471bfd03..7132022a040c 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -57,7 +57,7 @@ struct user_struct root_user = {
 	.uid_keyring	= &root_user_keyring,
 	.session_keyring = &root_session_keyring,
 #endif
-#ifdef CONFIG_FAIR_USER_SCHED
+#ifdef CONFIG_USER_SCHED
 	.tg		= &init_task_group,
 #endif
 };
@@ -90,7 +90,7 @@ static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
 	return NULL;
 }
 
-#ifdef CONFIG_FAIR_USER_SCHED
+#ifdef CONFIG_USER_SCHED
 
 static void sched_destroy_user(struct user_struct *up)
 {
@@ -113,15 +113,15 @@ static void sched_switch_user(struct task_struct *p)
 	sched_move_task(p);
 }
 
-#else	/* CONFIG_FAIR_USER_SCHED */
+#else	/* CONFIG_USER_SCHED */
 
 static void sched_destroy_user(struct user_struct *up) { }
 static int sched_create_user(struct user_struct *up) { return 0; }
 static void sched_switch_user(struct task_struct *p) { }
 
-#endif	/* CONFIG_FAIR_USER_SCHED */
+#endif	/* CONFIG_USER_SCHED */
 
-#if defined(CONFIG_FAIR_USER_SCHED) && defined(CONFIG_SYSFS)
+#if defined(CONFIG_USER_SCHED) && defined(CONFIG_SYSFS)
 
 static struct kset *uids_kset; /* represents the /sys/kernel/uids/ directory */
 static DEFINE_MUTEX(uids_mutex);
@@ -137,6 +137,7 @@ static inline void uids_mutex_unlock(void)
 }
 
 /* uid directory attributes */
+#ifdef CONFIG_FAIR_GROUP_SCHED
 static ssize_t cpu_shares_show(struct kobject *kobj,
 			       struct kobj_attribute *attr,
 			       char *buf)
@@ -163,7 +164,9 @@ static ssize_t cpu_shares_store(struct kobject *kobj,
 
 static struct kobj_attribute cpu_share_attr =
 	__ATTR(cpu_share, 0644, cpu_shares_show, cpu_shares_store);
+#endif
 
+#ifdef CONFIG_RT_GROUP_SCHED
 static ssize_t cpu_rt_runtime_show(struct kobject *kobj,
 				   struct kobj_attribute *attr,
 				   char *buf)
@@ -190,11 +193,16 @@ static ssize_t cpu_rt_runtime_store(struct kobject *kobj,
 
 static struct kobj_attribute cpu_rt_runtime_attr =
 	__ATTR(cpu_rt_runtime, 0644, cpu_rt_runtime_show, cpu_rt_runtime_store);
+#endif
 
 /* default attributes per uid directory */
 static struct attribute *uids_attributes[] = {
+#ifdef CONFIG_FAIR_GROUP_SCHED
 	&cpu_share_attr.attr,
+#endif
+#ifdef CONFIG_RT_GROUP_SCHED
 	&cpu_rt_runtime_attr.attr,
+#endif
 	NULL
 };
 
@@ -297,7 +305,7 @@ static inline void free_user(struct user_struct *up, unsigned long flags)
 	schedule_work(&up->work);
 }
 
-#else	/* CONFIG_FAIR_USER_SCHED && CONFIG_SYSFS */
+#else	/* CONFIG_USER_SCHED && CONFIG_SYSFS */
 
 int uids_sysfs_init(void) { return 0; }
 static inline int uids_user_create(struct user_struct *up) { return 0; }
@@ -401,7 +409,7 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
-			/* This case is not possible when CONFIG_FAIR_USER_SCHED
+			/* This case is not possible when CONFIG_USER_SCHED
 			 * is defined, since we serialize alloc_uid() using
 			 * uids_mutex. Hence no need to call
 			 * sched_destroy_user() or remove_user_sysfs_dir().

commit 9f0c1e560c43327b70998e6c702b2f01321130d9
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Feb 13 15:45:39 2008 +0100

    sched: rt-group: interface
    
    Change the rt_ratio interface to rt_runtime_us, to match rt_period_us.
    This avoids picking a granularity for the ratio.
    
    Extend the /sys/kernel/uids/<uid>/ interface to allow setting
    the group's rt_runtime.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 7d7900c5a1fd..9f6d471bfd03 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -164,9 +164,37 @@ static ssize_t cpu_shares_store(struct kobject *kobj,
 static struct kobj_attribute cpu_share_attr =
 	__ATTR(cpu_share, 0644, cpu_shares_show, cpu_shares_store);
 
+static ssize_t cpu_rt_runtime_show(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   char *buf)
+{
+	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
+
+	return sprintf(buf, "%lu\n", sched_group_rt_runtime(up->tg));
+}
+
+static ssize_t cpu_rt_runtime_store(struct kobject *kobj,
+				    struct kobj_attribute *attr,
+				    const char *buf, size_t size)
+{
+	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
+	unsigned long rt_runtime;
+	int rc;
+
+	sscanf(buf, "%lu", &rt_runtime);
+
+	rc = sched_group_set_rt_runtime(up->tg, rt_runtime);
+
+	return (rc ? rc : size);
+}
+
+static struct kobj_attribute cpu_rt_runtime_attr =
+	__ATTR(cpu_rt_runtime, 0644, cpu_rt_runtime_show, cpu_rt_runtime_store);
+
 /* default attributes per uid directory */
 static struct attribute *uids_attributes[] = {
 	&cpu_share_attr.attr,
+	&cpu_rt_runtime_attr.attr,
 	NULL
 };
 

commit aee16ce73c71a241190cef3aaa265f6a3ab8e035
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Feb 8 04:18:23 2008 -0800

    namespaces: cleanup the code managed with the USER_NS option
    
    Make the user_namespace.o compilation depend on this option and move the
    init_user_ns into user.c file to make the kernel compile and work without the
    namespaces support.  This make the user namespace code be organized similar to
    other namespaces'.
    
    Also mask the USER_NS option as "depend on NAMESPACES".
    
    [akpm@linux-foundation.org: coding-style fixes]
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Cc: Cedric Le Goater <clg@fr.ibm.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Sukadev Bhattiprolu <sukadev@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index bc1c48d35cb3..7d7900c5a1fd 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -17,6 +17,14 @@
 #include <linux/module.h>
 #include <linux/user_namespace.h>
 
+struct user_namespace init_user_ns = {
+	.kref = {
+		.refcount	= ATOMIC_INIT(2),
+	},
+	.root_user = &root_user,
+};
+EXPORT_SYMBOL_GPL(init_user_ns);
+
 /*
  * UID task count cache, to get fast user lookup in "alloc_uid"
  * when changing user ID's (ie setuid() and friends).
@@ -427,6 +435,7 @@ void switch_uid(struct user_struct *new_user)
 	suid_keys(current);
 }
 
+#ifdef CONFIG_USER_NS
 void release_uids(struct user_namespace *ns)
 {
 	int i;
@@ -451,6 +460,7 @@ void release_uids(struct user_namespace *ns)
 
 	free_uid(ns->root_user);
 }
+#endif
 
 static int __init uid_cache_init(void)
 {

commit 8eb703e4f33488bf75829564d51d427e17f7cd4c
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Fri Jan 25 21:08:26 2008 +0100

    uids: merge multiple error paths in alloc_uid() into one
    
    There are already 4 error paths in alloc_uid() that do incremental rollbacks.
    I think it's time to merge them.  This costs us 8 lines of code :)
    
    Maybe it would be better to merge this patch with the previous one, but I
    remember that some time ago I sent a similar patch (fixing the error path and
    cleaning it), but I was told to make two patches in such cases.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index ab4fd706993b..bc1c48d35cb3 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -319,7 +319,7 @@ void free_uid(struct user_struct *up)
 struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 {
 	struct hlist_head *hashent = uidhashentry(ns, uid);
-	struct user_struct *up;
+	struct user_struct *up, *new;
 
 	/* Make uid_hash_find() + uids_user_create() + uid_hash_insert()
 	 * atomic.
@@ -331,13 +331,9 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 	spin_unlock_irq(&uidhash_lock);
 
 	if (!up) {
-		struct user_struct *new;
-
 		new = kmem_cache_alloc(uid_cachep, GFP_KERNEL);
-		if (!new) {
-			uids_mutex_unlock();
-			return NULL;
-		}
+		if (!new)
+			goto out_unlock;
 
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
@@ -353,28 +349,14 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 #endif
 		new->locked_shm = 0;
 
-		if (alloc_uid_keyring(new, current) < 0) {
-			kmem_cache_free(uid_cachep, new);
-			uids_mutex_unlock();
-			return NULL;
-		}
+		if (alloc_uid_keyring(new, current) < 0)
+			goto out_free_user;
 
-		if (sched_create_user(new) < 0) {
-			key_put(new->uid_keyring);
-			key_put(new->session_keyring);
-			kmem_cache_free(uid_cachep, new);
-			uids_mutex_unlock();
-			return NULL;
-		}
+		if (sched_create_user(new) < 0)
+			goto out_put_keys;
 
-		if (uids_user_create(new)) {
-			sched_destroy_user(new);
-			key_put(new->uid_keyring);
-			key_put(new->session_keyring);
-			kmem_cache_free(uid_cachep, new);
-			uids_mutex_unlock();
-			return NULL;
-		}
+		if (uids_user_create(new))
+			goto out_destoy_sched;
 
 		/*
 		 * Before adding this, check whether we raced
@@ -402,6 +384,17 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 	uids_mutex_unlock();
 
 	return up;
+
+out_destoy_sched:
+	sched_destroy_user(new);
+out_put_keys:
+	key_put(new->uid_keyring);
+	key_put(new->session_keyring);
+out_free_user:
+	kmem_cache_free(uid_cachep, new);
+out_unlock:
+	uids_mutex_unlock();
+	return NULL;
 }
 
 void switch_uid(struct user_struct *new_user)

commit cf15126b3d4511e06e5299781ab74922590900be
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Mon Dec 17 23:05:35 2007 -0700

    Kobject: convert kernel/user.c to use kobject_init/add_ng()
    
    This converts the code to use the new kobject functions, cleaning up the
    logic in doing so.
    
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/kernel/user.c b/kernel/user.c
index 7f17e6e8fd65..ab4fd706993b 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -181,13 +181,12 @@ static int uids_user_create(struct user_struct *up)
 	int error;
 
 	memset(kobj, 0, sizeof(struct kobject));
-	kobj->ktype = &uids_ktype;
 	kobj->kset = uids_kset;
-	kobject_init(kobj);
-	kobject_set_name(&up->kobj, "%d", up->uid);
-	error = kobject_add(kobj);
-	if (error)
+	error = kobject_init_and_add(kobj, &uids_ktype, NULL, "%d", up->uid);
+	if (error) {
+		kobject_put(kobj);
 		goto done;
+	}
 
 	kobject_uevent(kobj, KOBJ_ADD);
 done:

commit 0ff21e46630abce11fdaaffabd72bbd4eed5ac2c
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Tue Nov 6 10:36:58 2007 -0800

    kobject: convert kernel_kset to be a kobject
    
    kernel_kset does not need to be a kset, but a much simpler kobject now
    that we have kobj_attributes.
    
    We also rename kernel_kset to kernel_kobj to catch all users of this
    symbol with a build error instead of an easy-to-ignore build warning.
    
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/kernel/user.c b/kernel/user.c
index 5a106f3fdf05..7f17e6e8fd65 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -201,7 +201,7 @@ static int uids_user_create(struct user_struct *up)
  */
 int __init uids_sysfs_init(void)
 {
-	uids_kset = kset_create_and_add("uids", NULL, &kernel_kset->kobj);
+	uids_kset = kset_create_and_add("uids", NULL, kernel_kobj);
 	if (!uids_kset)
 		return -ENOMEM;
 

commit eb41d9465cdafee45e0cb30f3b7338646221908e
Author: Kay Sievers <kay.sievers@vrfy.org>
Date:   Fri Nov 2 13:47:53 2007 +0100

    fix struct user_info export's sysfs interaction
    
    Clean up the use of ksets and kobjects. Kobjects are instances of
    objects (like struct user_info), ksets are collections of objects of a
    similar type (like the uids directory containing the user_info directories).
    So, use kobjects for the user_info directories, and a kset for the "uids"
    directory.
    
    On object cleanup, the final kobject_put() was missing.
    
    Cc: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Cc: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/kernel/user.c b/kernel/user.c
index 80f1116b8fcd..5a106f3fdf05 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -115,7 +115,7 @@ static void sched_switch_user(struct task_struct *p) { }
 
 #if defined(CONFIG_FAIR_USER_SCHED) && defined(CONFIG_SYSFS)
 
-static struct kobject uids_kobject; /* represents /sys/kernel/uids directory */
+static struct kset *uids_kset; /* represents the /sys/kernel/uids/ directory */
 static DEFINE_MUTEX(uids_mutex);
 
 static inline void uids_mutex_lock(void)
@@ -128,86 +128,84 @@ static inline void uids_mutex_unlock(void)
 	mutex_unlock(&uids_mutex);
 }
 
-/* return cpu shares held by the user */
-static ssize_t cpu_shares_show(struct kset *kset, char *buffer)
+/* uid directory attributes */
+static ssize_t cpu_shares_show(struct kobject *kobj,
+			       struct kobj_attribute *attr,
+			       char *buf)
 {
-	struct user_struct *up = container_of(kset, struct user_struct, kset);
+	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
 
-	return sprintf(buffer, "%lu\n", sched_group_shares(up->tg));
+	return sprintf(buf, "%lu\n", sched_group_shares(up->tg));
 }
 
-/* modify cpu shares held by the user */
-static ssize_t cpu_shares_store(struct kset *kset, const char *buffer,
-				size_t size)
+static ssize_t cpu_shares_store(struct kobject *kobj,
+				struct kobj_attribute *attr,
+				const char *buf, size_t size)
 {
-	struct user_struct *up = container_of(kset, struct user_struct, kset);
+	struct user_struct *up = container_of(kobj, struct user_struct, kobj);
 	unsigned long shares;
 	int rc;
 
-	sscanf(buffer, "%lu", &shares);
+	sscanf(buf, "%lu", &shares);
 
 	rc = sched_group_set_shares(up->tg, shares);
 
 	return (rc ? rc : size);
 }
 
-static void user_attr_init(struct subsys_attribute *sa, char *name, int mode)
+static struct kobj_attribute cpu_share_attr =
+	__ATTR(cpu_share, 0644, cpu_shares_show, cpu_shares_store);
+
+/* default attributes per uid directory */
+static struct attribute *uids_attributes[] = {
+	&cpu_share_attr.attr,
+	NULL
+};
+
+/* the lifetime of user_struct is not managed by the core (now) */
+static void uids_release(struct kobject *kobj)
 {
-	sa->attr.name = name;
-	sa->attr.mode = mode;
-	sa->show = cpu_shares_show;
-	sa->store = cpu_shares_store;
+	return;
 }
 
-/* Create "/sys/kernel/uids/<uid>" directory and
- *  "/sys/kernel/uids/<uid>/cpu_share" file for this user.
- */
-static int user_kobject_create(struct user_struct *up)
+static struct kobj_type uids_ktype = {
+	.sysfs_ops = &kobj_sysfs_ops,
+	.default_attrs = uids_attributes,
+	.release = uids_release,
+};
+
+/* create /sys/kernel/uids/<uid>/cpu_share file for this user */
+static int uids_user_create(struct user_struct *up)
 {
-	struct kset *kset = &up->kset;
-	struct kobject *kobj = &kset->kobj;
+	struct kobject *kobj = &up->kobj;
 	int error;
 
-	memset(kset, 0, sizeof(struct kset));
-	kobj->parent = &uids_kobject;	/* create under /sys/kernel/uids dir */
-	kobject_set_name(kobj, "%d", up->uid);
-	kset_init(kset);
-	user_attr_init(&up->user_attr, "cpu_share", 0644);
-
+	memset(kobj, 0, sizeof(struct kobject));
+	kobj->ktype = &uids_ktype;
+	kobj->kset = uids_kset;
+	kobject_init(kobj);
+	kobject_set_name(&up->kobj, "%d", up->uid);
 	error = kobject_add(kobj);
 	if (error)
 		goto done;
 
-	error = sysfs_create_file(kobj, &up->user_attr.attr);
-	if (error)
-		kobject_del(kobj);
-
 	kobject_uevent(kobj, KOBJ_ADD);
-
 done:
 	return error;
 }
 
-/* create these in sysfs filesystem:
+/* create these entries in sysfs:
  * 	"/sys/kernel/uids" directory
  * 	"/sys/kernel/uids/0" directory (for root user)
  * 	"/sys/kernel/uids/0/cpu_share" file (for root user)
  */
-int __init uids_kobject_init(void)
+int __init uids_sysfs_init(void)
 {
-	int error;
+	uids_kset = kset_create_and_add("uids", NULL, &kernel_kset->kobj);
+	if (!uids_kset)
+		return -ENOMEM;
 
-	/* create under /sys/kernel dir */
-	uids_kobject.parent = &kernel_kset->kobj;
-	uids_kobject.kset = kernel_kset;
-	kobject_set_name(&uids_kobject, "uids");
-	kobject_init(&uids_kobject);
-
-	error = kobject_add(&uids_kobject);
-	if (!error)
-		error = user_kobject_create(&root_user);
-
-	return error;
+	return uids_user_create(&root_user);
 }
 
 /* work function to remove sysfs directory for a user and free up
@@ -216,7 +214,6 @@ int __init uids_kobject_init(void)
 static void remove_user_sysfs_dir(struct work_struct *w)
 {
 	struct user_struct *up = container_of(w, struct user_struct, work);
-	struct kobject *kobj = &up->kset.kobj;
 	unsigned long flags;
 	int remove_user = 0;
 
@@ -238,9 +235,9 @@ static void remove_user_sysfs_dir(struct work_struct *w)
 	if (!remove_user)
 		goto done;
 
-	sysfs_remove_file(kobj, &up->user_attr.attr);
-	kobject_uevent(kobj, KOBJ_REMOVE);
-	kobject_del(kobj);
+	kobject_uevent(&up->kobj, KOBJ_REMOVE);
+	kobject_del(&up->kobj);
+	kobject_put(&up->kobj);
 
 	sched_destroy_user(up);
 	key_put(up->uid_keyring);
@@ -267,7 +264,8 @@ static inline void free_user(struct user_struct *up, unsigned long flags)
 
 #else	/* CONFIG_FAIR_USER_SCHED && CONFIG_SYSFS */
 
-static inline int user_kobject_create(struct user_struct *up) { return 0; }
+int uids_sysfs_init(void) { return 0; }
+static inline int uids_user_create(struct user_struct *up) { return 0; }
 static inline void uids_mutex_lock(void) { }
 static inline void uids_mutex_unlock(void) { }
 
@@ -324,7 +322,7 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 	struct hlist_head *hashent = uidhashentry(ns, uid);
 	struct user_struct *up;
 
-	/* Make uid_hash_find() + user_kobject_create() + uid_hash_insert()
+	/* Make uid_hash_find() + uids_user_create() + uid_hash_insert()
 	 * atomic.
 	 */
 	uids_mutex_lock();
@@ -370,7 +368,7 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 			return NULL;
 		}
 
-		if (user_kobject_create(new)) {
+		if (uids_user_create(new)) {
 			sched_destroy_user(new);
 			key_put(new->uid_keyring);
 			key_put(new->session_keyring);

commit bd35b93d8049ab47b5bfaf6b10ba39badf21d1c3
Author: Greg Kroah-Hartman <gregkh@suse.de>
Date:   Mon Oct 29 20:13:17 2007 +0100

    kset: convert kernel_subsys to use kset_create
    
    Dynamically create the kset instead of declaring it statically.  We also
    rename kernel_subsys to kernel_kset to catch all users of this symbol
    with a build error instead of an easy-to-ignore build warning.
    
    Cc: Kay Sievers <kay.sievers@vrfy.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/kernel/user.c b/kernel/user.c
index 8320a87f3e5a..80f1116b8fcd 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -198,8 +198,8 @@ int __init uids_kobject_init(void)
 	int error;
 
 	/* create under /sys/kernel dir */
-	uids_kobject.parent = &kernel_subsys.kobj;
-	uids_kobject.kset = &kernel_subsys;
+	uids_kobject.parent = &kernel_kset->kobj;
+	uids_kobject.kset = kernel_kset;
 	kobject_set_name(&uids_kobject, "uids");
 	kobject_init(&uids_kobject);
 

commit 5e8869bb699d50be5c0733edfc71cfcd5b43e10a
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Mon Nov 26 21:21:49 2007 +0100

    sched: don't forget to unlock uids_mutex on error paths
    
    The commit
    
     commit 5cb350baf580017da38199625b7365b1763d7180
     Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
     Date:   Mon Oct 15 17:00:14 2007 +0200
    
        sched: group scheduling, sysfs tunables
    
    introduced the uids_mutex and the helpers to lock/unlock it.
    Unfortunately, the error paths of alloc_uid() were not patched
    to unlock it.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Acked-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 0f3aa0234107..8320a87f3e5a 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -337,8 +337,11 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 		struct user_struct *new;
 
 		new = kmem_cache_alloc(uid_cachep, GFP_KERNEL);
-		if (!new)
+		if (!new) {
+			uids_mutex_unlock();
 			return NULL;
+		}
+
 		new->uid = uid;
 		atomic_set(&new->__count, 1);
 		atomic_set(&new->processes, 0);
@@ -355,6 +358,7 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 
 		if (alloc_uid_keyring(new, current) < 0) {
 			kmem_cache_free(uid_cachep, new);
+			uids_mutex_unlock();
 			return NULL;
 		}
 
@@ -362,6 +366,7 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 			key_put(new->uid_keyring);
 			key_put(new->session_keyring);
 			kmem_cache_free(uid_cachep, new);
+			uids_mutex_unlock();
 			return NULL;
 		}
 

commit a0f846aa76c3e03d54c1700a87cab3a46ccd71e2
Author: Adrian Bunk <bunk@kernel.org>
Date:   Wed Oct 24 18:23:50 2007 +0200

    sched: make cpu_shares_{show,store}() static
    
    cpu_shares_{show,store}() can become static.
    
    Signed-off-by: Adrian Bunk <bunk@kernel.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index e91331c457e2..0f3aa0234107 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -129,7 +129,7 @@ static inline void uids_mutex_unlock(void)
 }
 
 /* return cpu shares held by the user */
-ssize_t cpu_shares_show(struct kset *kset, char *buffer)
+static ssize_t cpu_shares_show(struct kset *kset, char *buffer)
 {
 	struct user_struct *up = container_of(kset, struct user_struct, kset);
 
@@ -137,7 +137,8 @@ ssize_t cpu_shares_show(struct kset *kset, char *buffer)
 }
 
 /* modify cpu shares held by the user */
-ssize_t cpu_shares_store(struct kset *kset, const char *buffer, size_t size)
+static ssize_t cpu_shares_store(struct kset *kset, const char *buffer,
+				size_t size)
 {
 	struct user_struct *up = container_of(kset, struct user_struct, kset);
 	unsigned long shares;

commit e6d5a11dad44b8ae18ca8fc4ecb72ccccfa0a2d2
Merge: b6257a9036f0 b9dca1e0fcb6
Author: Linus Torvalds <torvalds@woody.linux-foundation.org>
Date:   Wed Oct 17 09:11:18 2007 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/mingo/linux-2.6-sched:
      sched: fix new task startup crash
      sched: fix !SYSFS build breakage
      sched: fix improper load balance across sched domain
      sched: more robust sd-sysctl entry freeing

commit 970a8645ca051225a32401e4c80b50fc0a49c081
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Tue Oct 16 23:30:09 2007 -0700

    user.c: #ifdef ->mq_bytes
    
    For those who deselect POSIX message queues.
    
    Reduces SLAB size of user_struct from 64 to 32 bytes here, SLUB size -- from
    40 bytes to 32 bytes.
    
    [akpm@linux-foundation.org: fix build]
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index f42a68e008fd..9cb6f6403561 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -44,7 +44,6 @@ struct user_struct root_user = {
 	.processes	= ATOMIC_INIT(1),
 	.files		= ATOMIC_INIT(0),
 	.sigpending	= ATOMIC_INIT(0),
-	.mq_bytes	= 0,
 	.locked_shm     = 0,
 #ifdef CONFIG_KEYS
 	.uid_keyring	= &root_user_keyring,
@@ -341,8 +340,9 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 		atomic_set(&new->inotify_watches, 0);
 		atomic_set(&new->inotify_devs, 0);
 #endif
-
+#ifdef CONFIG_POSIX_MQUEUE
 		new->mq_bytes = 0;
+#endif
 		new->locked_shm = 0;
 
 		if (alloc_uid_keyring(new, current) < 0) {

commit 40aeb400f68090f92010920ddf284b672a1461b8
Author: Alexey Dobriyan <adobriyan@sw.ru>
Date:   Tue Oct 16 23:30:09 2007 -0700

    user.c: deinline
    
    Save some space because uid_hash_find() has 3 callsites.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@sw.ru>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index f0e561e6d085..f42a68e008fd 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -58,19 +58,17 @@ struct user_struct root_user = {
 /*
  * These routines must be called with the uidhash spinlock held!
  */
-static inline void uid_hash_insert(struct user_struct *up,
-						struct hlist_head *hashent)
+static void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)
 {
 	hlist_add_head(&up->uidhash_node, hashent);
 }
 
-static inline void uid_hash_remove(struct user_struct *up)
+static void uid_hash_remove(struct user_struct *up)
 {
 	hlist_del_init(&up->uidhash_node);
 }
 
-static inline struct user_struct *uid_hash_find(uid_t uid,
-						struct hlist_head *hashent)
+static struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
 {
 	struct user_struct *user;
 	struct hlist_node *h;

commit b1a8c172c318534b96d0f0f1aecdad3898118b98
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Wed Oct 17 16:55:11 2007 +0200

    sched: fix !SYSFS build breakage
    
    When CONFIG_SYSFS is not set, CONFIG_FAIR_USER_SCHED fails to build
    with
    
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1488): undefined reference to `kernel_subsys'
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1490): undefined reference to `kernel_subsys'
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1480): undefined reference to `kernel_subsys'
    kernel/built-in.o: In function `uids_kobject_init':
    (.init.text+0x1494): undefined reference to `kernel_subsys'
    
    This patch fixes this build error.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index f0e561e6d085..7e8215d87b40 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -87,9 +87,6 @@ static inline struct user_struct *uid_hash_find(uid_t uid,
 
 #ifdef CONFIG_FAIR_USER_SCHED
 
-static struct kobject uids_kobject; /* represents /sys/kernel/uids directory */
-static DEFINE_MUTEX(uids_mutex);
-
 static void sched_destroy_user(struct user_struct *up)
 {
 	sched_destroy_group(up->tg);
@@ -111,6 +108,19 @@ static void sched_switch_user(struct task_struct *p)
 	sched_move_task(p);
 }
 
+#else	/* CONFIG_FAIR_USER_SCHED */
+
+static void sched_destroy_user(struct user_struct *up) { }
+static int sched_create_user(struct user_struct *up) { return 0; }
+static void sched_switch_user(struct task_struct *p) { }
+
+#endif	/* CONFIG_FAIR_USER_SCHED */
+
+#if defined(CONFIG_FAIR_USER_SCHED) && defined(CONFIG_SYSFS)
+
+static struct kobject uids_kobject; /* represents /sys/kernel/uids directory */
+static DEFINE_MUTEX(uids_mutex);
+
 static inline void uids_mutex_lock(void)
 {
 	mutex_lock(&uids_mutex);
@@ -257,11 +267,8 @@ static inline void free_user(struct user_struct *up, unsigned long flags)
 	schedule_work(&up->work);
 }
 
-#else	/* CONFIG_FAIR_USER_SCHED */
+#else	/* CONFIG_FAIR_USER_SCHED && CONFIG_SYSFS */
 
-static void sched_destroy_user(struct user_struct *up) { }
-static int sched_create_user(struct user_struct *up) { return 0; }
-static void sched_switch_user(struct task_struct *p) { }
 static inline int user_kobject_create(struct user_struct *up) { return 0; }
 static inline void uids_mutex_lock(void) { }
 static inline void uids_mutex_unlock(void) { }
@@ -280,7 +287,7 @@ static inline void free_user(struct user_struct *up, unsigned long flags)
 	kmem_cache_free(uid_cachep, up);
 }
 
-#endif	/* CONFIG_FAIR_USER_SCHED */
+#endif
 
 /*
  * Locate the user_struct for the passed UID.  If found, take a ref on it.  The

commit fb7dde37ece82e13de383afd7042c45df67a9d17
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Mon Oct 15 17:00:18 2007 +0200

    sched: generate uevents for user creation/destruction
    
    Generate uevents when a user is being created/destroyed. These events
    can be used to configure cpu share of a new user.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 74cadea8466f..f0e561e6d085 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -174,6 +174,8 @@ static int user_kobject_create(struct user_struct *up)
 	if (error)
 		kobject_del(kobj);
 
+	kobject_uevent(kobj, KOBJ_ADD);
+
 done:
 	return error;
 }
@@ -189,6 +191,7 @@ int __init uids_kobject_init(void)
 
 	/* create under /sys/kernel dir */
 	uids_kobject.parent = &kernel_subsys.kobj;
+	uids_kobject.kset = &kernel_subsys;
 	kobject_set_name(&uids_kobject, "uids");
 	kobject_init(&uids_kobject);
 
@@ -228,6 +231,7 @@ static void remove_user_sysfs_dir(struct work_struct *w)
 		goto done;
 
 	sysfs_remove_file(kobj, &up->user_attr.attr);
+	kobject_uevent(kobj, KOBJ_REMOVE);
 	kobject_del(kobj);
 
 	sched_destroy_user(up);

commit 5cb350baf580017da38199625b7365b1763d7180
Author: Dhaval Giani <dhaval@linux.vnet.ibm.com>
Date:   Mon Oct 15 17:00:14 2007 +0200

    sched: group scheduling, sysfs tunables
    
    Add tunables in sysfs to modify a user's cpu share.
    
    A directory is created in sysfs for each new user in the system.
    
            /sys/kernel/uids/<uid>/cpu_share
    
    Reading this file returns the cpu shares granted for the user.
    Writing into this file modifies the cpu share for the user. Only an
    administrator is allowed to modify a user's cpu share.
    
    Ex:
            # cd /sys/kernel/uids/
            # cat 512/cpu_share
            1024
            # echo 2048 > 512/cpu_share
            # cat 512/cpu_share
            2048
            #
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index 0c9a7870d08f..74cadea8466f 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -55,7 +55,41 @@ struct user_struct root_user = {
 #endif
 };
 
+/*
+ * These routines must be called with the uidhash spinlock held!
+ */
+static inline void uid_hash_insert(struct user_struct *up,
+						struct hlist_head *hashent)
+{
+	hlist_add_head(&up->uidhash_node, hashent);
+}
+
+static inline void uid_hash_remove(struct user_struct *up)
+{
+	hlist_del_init(&up->uidhash_node);
+}
+
+static inline struct user_struct *uid_hash_find(uid_t uid,
+						struct hlist_head *hashent)
+{
+	struct user_struct *user;
+	struct hlist_node *h;
+
+	hlist_for_each_entry(user, h, hashent, uidhash_node) {
+		if (user->uid == uid) {
+			atomic_inc(&user->__count);
+			return user;
+		}
+	}
+
+	return NULL;
+}
+
 #ifdef CONFIG_FAIR_USER_SCHED
+
+static struct kobject uids_kobject; /* represents /sys/kernel/uids directory */
+static DEFINE_MUTEX(uids_mutex);
+
 static void sched_destroy_user(struct user_struct *up)
 {
 	sched_destroy_group(up->tg);
@@ -77,42 +111,173 @@ static void sched_switch_user(struct task_struct *p)
 	sched_move_task(p);
 }
 
-#else	/* CONFIG_FAIR_USER_SCHED */
+static inline void uids_mutex_lock(void)
+{
+	mutex_lock(&uids_mutex);
+}
 
-static void sched_destroy_user(struct user_struct *up) { }
-static int sched_create_user(struct user_struct *up) { return 0; }
-static void sched_switch_user(struct task_struct *p) { }
+static inline void uids_mutex_unlock(void)
+{
+	mutex_unlock(&uids_mutex);
+}
 
-#endif	/* CONFIG_FAIR_USER_SCHED */
+/* return cpu shares held by the user */
+ssize_t cpu_shares_show(struct kset *kset, char *buffer)
+{
+	struct user_struct *up = container_of(kset, struct user_struct, kset);
 
-/*
- * These routines must be called with the uidhash spinlock held!
+	return sprintf(buffer, "%lu\n", sched_group_shares(up->tg));
+}
+
+/* modify cpu shares held by the user */
+ssize_t cpu_shares_store(struct kset *kset, const char *buffer, size_t size)
+{
+	struct user_struct *up = container_of(kset, struct user_struct, kset);
+	unsigned long shares;
+	int rc;
+
+	sscanf(buffer, "%lu", &shares);
+
+	rc = sched_group_set_shares(up->tg, shares);
+
+	return (rc ? rc : size);
+}
+
+static void user_attr_init(struct subsys_attribute *sa, char *name, int mode)
+{
+	sa->attr.name = name;
+	sa->attr.mode = mode;
+	sa->show = cpu_shares_show;
+	sa->store = cpu_shares_store;
+}
+
+/* Create "/sys/kernel/uids/<uid>" directory and
+ *  "/sys/kernel/uids/<uid>/cpu_share" file for this user.
  */
-static inline void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)
+static int user_kobject_create(struct user_struct *up)
 {
-	hlist_add_head(&up->uidhash_node, hashent);
+	struct kset *kset = &up->kset;
+	struct kobject *kobj = &kset->kobj;
+	int error;
+
+	memset(kset, 0, sizeof(struct kset));
+	kobj->parent = &uids_kobject;	/* create under /sys/kernel/uids dir */
+	kobject_set_name(kobj, "%d", up->uid);
+	kset_init(kset);
+	user_attr_init(&up->user_attr, "cpu_share", 0644);
+
+	error = kobject_add(kobj);
+	if (error)
+		goto done;
+
+	error = sysfs_create_file(kobj, &up->user_attr.attr);
+	if (error)
+		kobject_del(kobj);
+
+done:
+	return error;
 }
 
-static inline void uid_hash_remove(struct user_struct *up)
+/* create these in sysfs filesystem:
+ * 	"/sys/kernel/uids" directory
+ * 	"/sys/kernel/uids/0" directory (for root user)
+ * 	"/sys/kernel/uids/0/cpu_share" file (for root user)
+ */
+int __init uids_kobject_init(void)
 {
-	hlist_del_init(&up->uidhash_node);
+	int error;
+
+	/* create under /sys/kernel dir */
+	uids_kobject.parent = &kernel_subsys.kobj;
+	kobject_set_name(&uids_kobject, "uids");
+	kobject_init(&uids_kobject);
+
+	error = kobject_add(&uids_kobject);
+	if (!error)
+		error = user_kobject_create(&root_user);
+
+	return error;
 }
 
-static inline struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
+/* work function to remove sysfs directory for a user and free up
+ * corresponding structures.
+ */
+static void remove_user_sysfs_dir(struct work_struct *w)
 {
-	struct user_struct *user;
-	struct hlist_node *h;
+	struct user_struct *up = container_of(w, struct user_struct, work);
+	struct kobject *kobj = &up->kset.kobj;
+	unsigned long flags;
+	int remove_user = 0;
 
-	hlist_for_each_entry(user, h, hashent, uidhash_node) {
-		if(user->uid == uid) {
-			atomic_inc(&user->__count);
-			return user;
-		}
+	/* Make uid_hash_remove() + sysfs_remove_file() + kobject_del()
+	 * atomic.
+	 */
+	uids_mutex_lock();
+
+	local_irq_save(flags);
+
+	if (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
+		uid_hash_remove(up);
+		remove_user = 1;
+		spin_unlock_irqrestore(&uidhash_lock, flags);
+	} else {
+		local_irq_restore(flags);
 	}
 
-	return NULL;
+	if (!remove_user)
+		goto done;
+
+	sysfs_remove_file(kobj, &up->user_attr.attr);
+	kobject_del(kobj);
+
+	sched_destroy_user(up);
+	key_put(up->uid_keyring);
+	key_put(up->session_keyring);
+	kmem_cache_free(uid_cachep, up);
+
+done:
+	uids_mutex_unlock();
+}
+
+/* IRQs are disabled and uidhash_lock is held upon function entry.
+ * IRQ state (as stored in flags) is restored and uidhash_lock released
+ * upon function exit.
+ */
+static inline void free_user(struct user_struct *up, unsigned long flags)
+{
+	/* restore back the count */
+	atomic_inc(&up->__count);
+	spin_unlock_irqrestore(&uidhash_lock, flags);
+
+	INIT_WORK(&up->work, remove_user_sysfs_dir);
+	schedule_work(&up->work);
 }
 
+#else	/* CONFIG_FAIR_USER_SCHED */
+
+static void sched_destroy_user(struct user_struct *up) { }
+static int sched_create_user(struct user_struct *up) { return 0; }
+static void sched_switch_user(struct task_struct *p) { }
+static inline int user_kobject_create(struct user_struct *up) { return 0; }
+static inline void uids_mutex_lock(void) { }
+static inline void uids_mutex_unlock(void) { }
+
+/* IRQs are disabled and uidhash_lock is held upon function entry.
+ * IRQ state (as stored in flags) is restored and uidhash_lock released
+ * upon function exit.
+ */
+static inline void free_user(struct user_struct *up, unsigned long flags)
+{
+	uid_hash_remove(up);
+	spin_unlock_irqrestore(&uidhash_lock, flags);
+	sched_destroy_user(up);
+	key_put(up->uid_keyring);
+	key_put(up->session_keyring);
+	kmem_cache_free(uid_cachep, up);
+}
+
+#endif	/* CONFIG_FAIR_USER_SCHED */
+
 /*
  * Locate the user_struct for the passed UID.  If found, take a ref on it.  The
  * caller must undo that ref with free_uid().
@@ -139,16 +304,10 @@ void free_uid(struct user_struct *up)
 		return;
 
 	local_irq_save(flags);
-	if (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
-		uid_hash_remove(up);
-		spin_unlock_irqrestore(&uidhash_lock, flags);
-		sched_destroy_user(up);
-		key_put(up->uid_keyring);
-		key_put(up->session_keyring);
-		kmem_cache_free(uid_cachep, up);
-	} else {
+	if (atomic_dec_and_lock(&up->__count, &uidhash_lock))
+		free_user(up, flags);
+	else
 		local_irq_restore(flags);
-	}
 }
 
 struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
@@ -156,6 +315,11 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 	struct hlist_head *hashent = uidhashentry(ns, uid);
 	struct user_struct *up;
 
+	/* Make uid_hash_find() + user_kobject_create() + uid_hash_insert()
+	 * atomic.
+	 */
+	uids_mutex_lock();
+
 	spin_lock_irq(&uidhash_lock);
 	up = uid_hash_find(uid, hashent);
 	spin_unlock_irq(&uidhash_lock);
@@ -191,6 +355,15 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 			return NULL;
 		}
 
+		if (user_kobject_create(new)) {
+			sched_destroy_user(new);
+			key_put(new->uid_keyring);
+			key_put(new->session_keyring);
+			kmem_cache_free(uid_cachep, new);
+			uids_mutex_unlock();
+			return NULL;
+		}
+
 		/*
 		 * Before adding this, check whether we raced
 		 * on adding the same user already..
@@ -198,7 +371,11 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
-			sched_destroy_user(new);
+			/* This case is not possible when CONFIG_FAIR_USER_SCHED
+			 * is defined, since we serialize alloc_uid() using
+			 * uids_mutex. Hence no need to call
+			 * sched_destroy_user() or remove_user_sysfs_dir().
+			 */
 			key_put(new->uid_keyring);
 			key_put(new->session_keyring);
 			kmem_cache_free(uid_cachep, new);
@@ -209,6 +386,9 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_unlock_irq(&uidhash_lock);
 
 	}
+
+	uids_mutex_unlock();
+
 	return up;
 }
 

commit 4cf86d77f5942336e7cd9de874b38b3c83b54d5e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Oct 15 17:00:14 2007 +0200

    sched: cleanup: rename task_grp to task_group
    
    cleanup: rename task_grp to task_group. No need to save two characters
    and 'grp' is annoying to read.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/user.c b/kernel/user.c
index c6387fac932d..0c9a7870d08f 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -51,7 +51,7 @@ struct user_struct root_user = {
 	.session_keyring = &root_session_keyring,
 #endif
 #ifdef CONFIG_FAIR_USER_SCHED
-	.tg		= &init_task_grp,
+	.tg		= &init_task_group,
 #endif
 };
 

commit 24e377a83220ef05c9b5bec7e01d65eed6609aa6
Author: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
Date:   Mon Oct 15 17:00:09 2007 +0200

    sched: add fair-user scheduler
    
    Enable user-id based fair group scheduling. This is useful for anyone
    who wants to test the group scheduler w/o having to enable
    CONFIG_CGROUPS.
    
    A separate scheduling group (i.e struct task_grp) is automatically created for
    every new user added to the system. Upon uid change for a task, it is made to
    move to the corresponding scheduling group.
    
    A /proc tunable (/proc/root_user_share) is also provided to tune root
    user's quota of cpu bandwidth.
    
    Signed-off-by: Srivatsa Vaddagiri <vatsa@linux.vnet.ibm.com>
    Signed-off-by: Dhaval Giani <dhaval@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/user.c b/kernel/user.c
index 9ca2848fc356..c6387fac932d 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -50,8 +50,41 @@ struct user_struct root_user = {
 	.uid_keyring	= &root_user_keyring,
 	.session_keyring = &root_session_keyring,
 #endif
+#ifdef CONFIG_FAIR_USER_SCHED
+	.tg		= &init_task_grp,
+#endif
 };
 
+#ifdef CONFIG_FAIR_USER_SCHED
+static void sched_destroy_user(struct user_struct *up)
+{
+	sched_destroy_group(up->tg);
+}
+
+static int sched_create_user(struct user_struct *up)
+{
+	int rc = 0;
+
+	up->tg = sched_create_group();
+	if (IS_ERR(up->tg))
+		rc = -ENOMEM;
+
+	return rc;
+}
+
+static void sched_switch_user(struct task_struct *p)
+{
+	sched_move_task(p);
+}
+
+#else	/* CONFIG_FAIR_USER_SCHED */
+
+static void sched_destroy_user(struct user_struct *up) { }
+static int sched_create_user(struct user_struct *up) { return 0; }
+static void sched_switch_user(struct task_struct *p) { }
+
+#endif	/* CONFIG_FAIR_USER_SCHED */
+
 /*
  * These routines must be called with the uidhash spinlock held!
  */
@@ -109,6 +142,7 @@ void free_uid(struct user_struct *up)
 	if (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
 		uid_hash_remove(up);
 		spin_unlock_irqrestore(&uidhash_lock, flags);
+		sched_destroy_user(up);
 		key_put(up->uid_keyring);
 		key_put(up->session_keyring);
 		kmem_cache_free(uid_cachep, up);
@@ -150,6 +184,13 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 			return NULL;
 		}
 
+		if (sched_create_user(new) < 0) {
+			key_put(new->uid_keyring);
+			key_put(new->session_keyring);
+			kmem_cache_free(uid_cachep, new);
+			return NULL;
+		}
+
 		/*
 		 * Before adding this, check whether we raced
 		 * on adding the same user already..
@@ -157,6 +198,7 @@ struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
+			sched_destroy_user(new);
 			key_put(new->uid_keyring);
 			key_put(new->session_keyring);
 			kmem_cache_free(uid_cachep, new);
@@ -184,6 +226,7 @@ void switch_uid(struct user_struct *new_user)
 	atomic_dec(&old_user->processes);
 	switch_uid_keyring(new_user);
 	current->user = new_user;
+	sched_switch_user(current);
 
 	/*
 	 * We need to synchronize with __sigqueue_alloc()

commit 28f300d23674fa01ae747c66ce861d4ee6aebe8c
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Tue Sep 18 22:46:45 2007 -0700

    Fix user namespace exiting OOPs
    
    It turned out, that the user namespace is released during the do_exit() in
    exit_task_namespaces(), but the struct user_struct is released only during the
    put_task_struct(), i.e.  MUCH later.
    
    On debug kernels with poisoned slabs this will cause the oops in
    uid_hash_remove() because the head of the chain, which resides inside the
    struct user_namespace, will be already freed and poisoned.
    
    Since the uid hash itself is required only when someone can search it, i.e.
    when the namespace is alive, we can safely unhash all the user_struct-s from
    it during the namespace exiting.  The subsequent free_uid() will complete the
    user_struct destruction.
    
    For example simple program
    
       #include <sched.h>
    
       char stack[2 * 1024 * 1024];
    
       int f(void *foo)
       {
            return 0;
       }
    
       int main(void)
       {
            clone(f, stack + 1 * 1024 * 1024, 0x10000000, 0);
            return 0;
       }
    
    run on kernel with CONFIG_USER_NS turned on will oops the
    kernel immediately.
    
    This was spotted during OpenVZ kernel testing.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Alexey Dobriyan <adobriyan@openvz.org>
    Acked-by: "Serge E. Hallyn" <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index add57c7e4c07..9ca2848fc356 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -62,7 +62,7 @@ static inline void uid_hash_insert(struct user_struct *up, struct hlist_head *ha
 
 static inline void uid_hash_remove(struct user_struct *up)
 {
-	hlist_del(&up->uidhash_node);
+	hlist_del_init(&up->uidhash_node);
 }
 
 static inline struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
@@ -199,6 +199,30 @@ void switch_uid(struct user_struct *new_user)
 	suid_keys(current);
 }
 
+void release_uids(struct user_namespace *ns)
+{
+	int i;
+	unsigned long flags;
+	struct hlist_head *head;
+	struct hlist_node *nd;
+
+	spin_lock_irqsave(&uidhash_lock, flags);
+	/*
+	 * collapse the chains so that the user_struct-s will
+	 * be still alive, but not in hashes. subsequent free_uid()
+	 * will free them.
+	 */
+	for (i = 0; i < UIDHASH_SZ; i++) {
+		head = ns->uidhash_table + i;
+		while (!hlist_empty(head)) {
+			nd = head->first;
+			hlist_del_init(nd);
+		}
+	}
+	spin_unlock_irqrestore(&uidhash_lock, flags);
+
+	free_uid(ns->root_user);
+}
 
 static int __init uid_cache_init(void)
 {

commit 735de2230f09741077a645a913de0a04b10208bf
Author: Pavel Emelyanov <xemul@openvz.org>
Date:   Tue Sep 18 22:46:44 2007 -0700

    Convert uid hash to hlist
    
    Surprisingly, but (spotted by Alexey Dobriyan) the uid hash still uses
    list_heads, thus occupying twice as much place as it could.  Convert it to
    hlist_heads.
    
    Signed-off-by: Pavel Emelyanov <xemul@openvz.org>
    Signed-off-by: Alexey Dobriyan <adobriyan@openvz.org>
    Acked-by: Serge Hallyn <serue@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index e080ba863ae3..add57c7e4c07 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -55,21 +55,22 @@ struct user_struct root_user = {
 /*
  * These routines must be called with the uidhash spinlock held!
  */
-static inline void uid_hash_insert(struct user_struct *up, struct list_head *hashent)
+static inline void uid_hash_insert(struct user_struct *up, struct hlist_head *hashent)
 {
-	list_add(&up->uidhash_list, hashent);
+	hlist_add_head(&up->uidhash_node, hashent);
 }
 
 static inline void uid_hash_remove(struct user_struct *up)
 {
-	list_del(&up->uidhash_list);
+	hlist_del(&up->uidhash_node);
 }
 
-static inline struct user_struct *uid_hash_find(uid_t uid, struct list_head *hashent)
+static inline struct user_struct *uid_hash_find(uid_t uid, struct hlist_head *hashent)
 {
 	struct user_struct *user;
+	struct hlist_node *h;
 
-	list_for_each_entry(user, hashent, uidhash_list) {
+	hlist_for_each_entry(user, h, hashent, uidhash_node) {
 		if(user->uid == uid) {
 			atomic_inc(&user->__count);
 			return user;
@@ -118,7 +119,7 @@ void free_uid(struct user_struct *up)
 
 struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 {
-	struct list_head *hashent = uidhashentry(ns, uid);
+	struct hlist_head *hashent = uidhashentry(ns, uid);
 	struct user_struct *up;
 
 	spin_lock_irq(&uidhash_lock);
@@ -207,7 +208,7 @@ static int __init uid_cache_init(void)
 			0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
 
 	for(n = 0; n < UIDHASH_SZ; ++n)
-		INIT_LIST_HEAD(init_user_ns.uidhash_table + n);
+		INIT_HLIST_HEAD(init_user_ns.uidhash_table + n);
 
 	/* Insert the root user immediately (init already runs as root) */
 	spin_lock_irq(&uidhash_lock);

commit d8a4821dca693867a7953104c1e3cc830eb9191f
Author: Matthias Kaehlcke <matthias.kaehlcke@gmail.com>
Date:   Tue Sep 18 22:46:43 2007 -0700

    kernel/user.c: Use list_for_each_entry instead of list_for_each
    
    kernel/user.c: Convert list_for_each to list_for_each_entry in
    uid_hash_find()
    
    Signed-off-by: Matthias Kaehlcke <matthias.kaehlcke@gmail.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index e7d11cef6998..e080ba863ae3 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -67,13 +67,9 @@ static inline void uid_hash_remove(struct user_struct *up)
 
 static inline struct user_struct *uid_hash_find(uid_t uid, struct list_head *hashent)
 {
-	struct list_head *up;
-
-	list_for_each(up, hashent) {
-		struct user_struct *user;
-
-		user = list_entry(up, struct user_struct, uidhash_list);
+	struct user_struct *user;
 
+	list_for_each_entry(user, hashent, uidhash_list) {
 		if(user->uid == uid) {
 			atomic_inc(&user->__count);
 			return user;

commit 20c2df83d25c6a95affe6157a4c9cac4cf5ffaac
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Fri Jul 20 10:11:58 2007 +0900

    mm: Remove slab destructors from kmem_cache_create().
    
    Slab destructors were no longer supported after Christoph's
    c59def9f222d44bb7e2f0a559f2906191a0862d7 change. They've been
    BUGs for both slab and slub, and slob never supported them
    either.
    
    This rips out support for the dtor pointer from kmem_cache_create()
    completely and fixes up every single callsite in the kernel (there were
    about 224, not including the slab allocator definitions themselves,
    or the documentation references).
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>

diff --git a/kernel/user.c b/kernel/user.c
index 98b82507797a..e7d11cef6998 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -208,7 +208,7 @@ static int __init uid_cache_init(void)
 	int n;
 
 	uid_cachep = kmem_cache_create("uid_cache", sizeof(struct user_struct),
-			0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+			0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL);
 
 	for(n = 0; n < UIDHASH_SZ; ++n)
 		INIT_LIST_HEAD(init_user_ns.uidhash_table + n);

commit acce292c82d4d82d35553b928df2b0597c3a9c78
Author: Cedric Le Goater <clg@fr.ibm.com>
Date:   Sun Jul 15 23:40:59 2007 -0700

    user namespace: add the framework
    
    Basically, it will allow a process to unshare its user_struct table,
    resetting at the same time its own user_struct and all the associated
    accounting.
    
    A new root user (uid == 0) is added to the user namespace upon creation.
    Such root users have full privileges and it seems that theses privileges
    should be controlled through some means (process capabilities ?)
    
    The unshare is not included in this patch.
    
    Changes since [try #4]:
            - Updated get_user_ns and put_user_ns to accept NULL, and
              get_user_ns to return the namespace.
    
    Changes since [try #3]:
            - moved struct user_namespace to files user_namespace.{c,h}
    
    Changes since [try #2]:
            - removed struct user_namespace* argument from find_user()
    
    Changes since [try #1]:
            - removed struct user_namespace* argument from find_user()
            - added a root_user per user namespace
    
    Signed-off-by: Cedric Le Goater <clg@fr.ibm.com>
    Signed-off-by: Serge E. Hallyn <serue@us.ibm.com>
    Acked-by: Pavel Emelianov <xemul@openvz.org>
    Cc: Herbert Poetzl <herbert@13thfloor.at>
    Cc: Kirill Korotaev <dev@sw.ru>
    Cc: Eric W. Biederman <ebiederm@xmission.com>
    Cc: Chris Wright <chrisw@sous-sol.org>
    Cc: Stephen Smalley <sds@tycho.nsa.gov>
    Cc: James Morris <jmorris@namei.org>
    Cc: Andrew Morgan <agm@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/user.c b/kernel/user.c
index 4869563080e9..98b82507797a 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -14,20 +14,19 @@
 #include <linux/bitops.h>
 #include <linux/key.h>
 #include <linux/interrupt.h>
+#include <linux/module.h>
+#include <linux/user_namespace.h>
 
 /*
  * UID task count cache, to get fast user lookup in "alloc_uid"
  * when changing user ID's (ie setuid() and friends).
  */
 
-#define UIDHASH_BITS (CONFIG_BASE_SMALL ? 3 : 8)
-#define UIDHASH_SZ		(1 << UIDHASH_BITS)
 #define UIDHASH_MASK		(UIDHASH_SZ - 1)
 #define __uidhashfn(uid)	(((uid >> UIDHASH_BITS) + uid) & UIDHASH_MASK)
-#define uidhashentry(uid)	(uidhash_table + __uidhashfn((uid)))
+#define uidhashentry(ns, uid)	((ns)->uidhash_table + __uidhashfn((uid)))
 
 static struct kmem_cache *uid_cachep;
-static struct list_head uidhash_table[UIDHASH_SZ];
 
 /*
  * The uidhash_lock is mostly taken from process context, but it is
@@ -94,9 +93,10 @@ struct user_struct *find_user(uid_t uid)
 {
 	struct user_struct *ret;
 	unsigned long flags;
+	struct user_namespace *ns = current->nsproxy->user_ns;
 
 	spin_lock_irqsave(&uidhash_lock, flags);
-	ret = uid_hash_find(uid, uidhashentry(uid));
+	ret = uid_hash_find(uid, uidhashentry(ns, uid));
 	spin_unlock_irqrestore(&uidhash_lock, flags);
 	return ret;
 }
@@ -120,9 +120,9 @@ void free_uid(struct user_struct *up)
 	}
 }
 
-struct user_struct * alloc_uid(uid_t uid)
+struct user_struct * alloc_uid(struct user_namespace *ns, uid_t uid)
 {
-	struct list_head *hashent = uidhashentry(uid);
+	struct list_head *hashent = uidhashentry(ns, uid);
 	struct user_struct *up;
 
 	spin_lock_irq(&uidhash_lock);
@@ -211,11 +211,11 @@ static int __init uid_cache_init(void)
 			0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
 
 	for(n = 0; n < UIDHASH_SZ; ++n)
-		INIT_LIST_HEAD(uidhash_table + n);
+		INIT_LIST_HEAD(init_user_ns.uidhash_table + n);
 
 	/* Insert the root user immediately (init already runs as root) */
 	spin_lock_irq(&uidhash_lock);
-	uid_hash_insert(&root_user, uidhashentry(0));
+	uid_hash_insert(&root_user, uidhashentry(&init_user_ns, 0));
 	spin_unlock_irq(&uidhash_lock);
 
 	return 0;

commit e18b890bb0881bbab6f4f1a6cd20d9c60d66b003
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:20 2006 -0800

    [PATCH] slab: remove kmem_cache_t
    
    Replace all uses of kmem_cache_t with struct kmem_cache.
    
    The patch was generated using the following script:
    
            #!/bin/sh
            #
            # Replace one string by another in all the kernel sources.
            #
    
            set -e
    
            for file in `find * -name "*.c" -o -name "*.h"|xargs grep -l $1`; do
                    quilt add $file
                    sed -e "1,\$s/$1/$2/g" $file >/tmp/$$
                    mv /tmp/$$ $file
                    quilt refresh
            done
    
    The script was run like this
    
            sh replace kmem_cache_t "struct kmem_cache"
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index c1f93c164c93..4869563080e9 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -26,7 +26,7 @@
 #define __uidhashfn(uid)	(((uid >> UIDHASH_BITS) + uid) & UIDHASH_MASK)
 #define uidhashentry(uid)	(uidhash_table + __uidhashfn((uid)))
 
-static kmem_cache_t *uid_cachep;
+static struct kmem_cache *uid_cachep;
 static struct list_head uidhash_table[UIDHASH_SZ];
 
 /*

commit e94b1766097d53e6f3ccfb36c8baa562ffeda3fc
Author: Christoph Lameter <clameter@sgi.com>
Date:   Wed Dec 6 20:33:17 2006 -0800

    [PATCH] slab: remove SLAB_KERNEL
    
    SLAB_KERNEL is an alias of GFP_KERNEL.
    
    Signed-off-by: Christoph Lameter <clameter@sgi.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index 220e586127a0..c1f93c164c93 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -132,7 +132,7 @@ struct user_struct * alloc_uid(uid_t uid)
 	if (!up) {
 		struct user_struct *new;
 
-		new = kmem_cache_alloc(uid_cachep, SLAB_KERNEL);
+		new = kmem_cache_alloc(uid_cachep, GFP_KERNEL);
 		if (!new)
 			return NULL;
 		new->uid = uid;

commit 45c18b0bb579b5c1b89f8c99f1b6ffa4c586ba08
Author: Linus Torvalds <torvalds@g5.osdl.org>
Date:   Sat Nov 4 10:06:02 2006 -0800

    Fix unlikely (but possible) race condition on task->user access
    
    There's a possible race condition when doing a "switch_uid()" from one
    user to another, which could race with another thread doing a signal
    allocation and looking at the old thread ->user pointer as it is freed.
    
    This explains an oops reported by Lukasz Trabinski:
            http://permalink.gmane.org/gmane.linux.kernel/462241
    
    We fix this by delaying the (reference-counted) freeing of the user
    structure until the thread signal handler lock has been released, so
    that we know that the signal allocation has either seen the new value or
    has properly incremented the reference count of the old one.
    
    Race identified by Oleg Nesterov.
    
    Cc: Lukasz Trabinski <lukasz@wsisiz.edu.pl>
    Cc: Oleg Nesterov <oleg@tv-sign.ru>
    Cc: Andrew Morton <akpm@osdl.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index 6408c0424291..220e586127a0 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -187,6 +187,17 @@ void switch_uid(struct user_struct *new_user)
 	atomic_dec(&old_user->processes);
 	switch_uid_keyring(new_user);
 	current->user = new_user;
+
+	/*
+	 * We need to synchronize with __sigqueue_alloc()
+	 * doing a get_uid(p->user).. If that saw the old
+	 * user value, we need to wait until it has exited
+	 * its critical region before we can free the old
+	 * structure.
+	 */
+	smp_mb();
+	spin_unlock_wait(&current->sighand->siglock);
+
 	free_uid(old_user);
 	suid_keys(current);
 }

commit d720024e94de4e8b7f10ee83c532926f3ad5d708
Author: Michael LeMay <mdlemay@epoch.ncsc.mil>
Date:   Thu Jun 22 14:47:17 2006 -0700

    [PATCH] selinux: add hooks for key subsystem
    
    Introduce SELinux hooks to support the access key retention subsystem
    within the kernel.  Incorporate new flask headers from a modified version
    of the SELinux reference policy, with support for the new security class
    representing retained keys.  Extend the "key_alloc" security hook with a
    task parameter representing the intended ownership context for the key
    being allocated.  Attach security information to root's default keyrings
    within the SELinux initialization routine.
    
    Has passed David's testsuite.
    
    Signed-off-by: Michael LeMay <mdlemay@epoch.ncsc.mil>
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: James Morris <jmorris@namei.org>
    Acked-by: Chris Wright <chrisw@sous-sol.org>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index 4b1eb745afa1..6408c0424291 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -148,7 +148,7 @@ struct user_struct * alloc_uid(uid_t uid)
 		new->mq_bytes = 0;
 		new->locked_shm = 0;
 
-		if (alloc_uid_keyring(new) < 0) {
+		if (alloc_uid_keyring(new, current) < 0) {
 			kmem_cache_free(uid_cachep, new);
 			return NULL;
 		}

commit 2d9048e201bfb67ba21f05e647b1286b8a4a5667
Author: Amy Griffis <amy.griffis@hp.com>
Date:   Thu Jun 1 13:10:59 2006 -0700

    [PATCH] inotify (1/5): split kernel API from userspace support
    
    The following series of patches introduces a kernel API for inotify,
    making it possible for kernel modules to benefit from inotify's
    mechanism for watching inodes.  With these patches, inotify will
    maintain for each caller a list of watches (via an embedded struct
    inotify_watch), where each inotify_watch is associated with a
    corresponding struct inode.  The caller registers an event handler and
    specifies for which filesystem events their event handler should be
    called per inotify_watch.
    
    Signed-off-by: Amy Griffis <amy.griffis@hp.com>
    Acked-by: Robert Love <rml@novell.com>
    Acked-by: John McCutchan <john@johnmccutchan.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/user.c b/kernel/user.c
index 2116642f42c6..4b1eb745afa1 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -140,7 +140,7 @@ struct user_struct * alloc_uid(uid_t uid)
 		atomic_set(&new->processes, 0);
 		atomic_set(&new->files, 0);
 		atomic_set(&new->sigpending, 0);
-#ifdef CONFIG_INOTIFY
+#ifdef CONFIG_INOTIFY_USER
 		atomic_set(&new->inotify_watches, 0);
 		atomic_set(&new->inotify_devs, 0);
 #endif

commit 36f574135e36b86bb6ae794bf1d0fce3efa5601f
Author: Andrew Morton <akpm@osdl.org>
Date:   Fri Mar 24 03:15:47 2006 -0800

    [PATCH] free_uid() locking improvement
    
    Reduce lock hold times in free_uid().
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "Paul E. McKenney" <paulmck@us.ibm.com>
    Cc: David Howells <dhowells@redhat.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index d9deae43a9ab..2116642f42c6 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -105,15 +105,19 @@ void free_uid(struct user_struct *up)
 {
 	unsigned long flags;
 
+	if (!up)
+		return;
+
 	local_irq_save(flags);
-	if (up && atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
+	if (atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
 		uid_hash_remove(up);
+		spin_unlock_irqrestore(&uidhash_lock, flags);
 		key_put(up->uid_keyring);
 		key_put(up->session_keyring);
 		kmem_cache_free(uid_cachep, up);
-		spin_unlock(&uidhash_lock);
+	} else {
+		local_irq_restore(flags);
 	}
-	local_irq_restore(flags);
 }
 
 struct user_struct * alloc_uid(uid_t uid)

commit 3fa97c9db4f6f93f41f7a40d08872dbfd8dc907e
Author: Andrew Morton <akpm@osdl.org>
Date:   Tue Jan 31 16:34:26 2006 -0800

    [PATCH] "Fix uidhash_lock <-> RXU deadlock" fix
    
    I get storms of warnings from local_bh_enable().  Better-tested patches,
    please.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "Paul E. McKenney" <paulmck@us.ibm.com>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index d1ae2349347e..d9deae43a9ab 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -33,6 +33,10 @@ static struct list_head uidhash_table[UIDHASH_SZ];
  * The uidhash_lock is mostly taken from process context, but it is
  * occasionally also taken from softirq/tasklet context, when
  * task-structs get RCU-freed. Hence all locking must be softirq-safe.
+ * But free_uid() is also called with local interrupts disabled, and running
+ * local_bh_enable() with local interrupts disabled is an error - we'll run
+ * softirq callbacks, and they can unconditionally enable interrupts, and
+ * the caller of free_uid() didn't expect that..
  */
 static DEFINE_SPINLOCK(uidhash_lock);
 
@@ -89,16 +93,19 @@ static inline struct user_struct *uid_hash_find(uid_t uid, struct list_head *has
 struct user_struct *find_user(uid_t uid)
 {
 	struct user_struct *ret;
+	unsigned long flags;
 
-	spin_lock_bh(&uidhash_lock);
+	spin_lock_irqsave(&uidhash_lock, flags);
 	ret = uid_hash_find(uid, uidhashentry(uid));
-	spin_unlock_bh(&uidhash_lock);
+	spin_unlock_irqrestore(&uidhash_lock, flags);
 	return ret;
 }
 
 void free_uid(struct user_struct *up)
 {
-	local_bh_disable();
+	unsigned long flags;
+
+	local_irq_save(flags);
 	if (up && atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
 		uid_hash_remove(up);
 		key_put(up->uid_keyring);
@@ -106,7 +113,7 @@ void free_uid(struct user_struct *up)
 		kmem_cache_free(uid_cachep, up);
 		spin_unlock(&uidhash_lock);
 	}
-	local_bh_enable();
+	local_irq_restore(flags);
 }
 
 struct user_struct * alloc_uid(uid_t uid)
@@ -114,9 +121,9 @@ struct user_struct * alloc_uid(uid_t uid)
 	struct list_head *hashent = uidhashentry(uid);
 	struct user_struct *up;
 
-	spin_lock_bh(&uidhash_lock);
+	spin_lock_irq(&uidhash_lock);
 	up = uid_hash_find(uid, hashent);
-	spin_unlock_bh(&uidhash_lock);
+	spin_unlock_irq(&uidhash_lock);
 
 	if (!up) {
 		struct user_struct *new;
@@ -146,7 +153,7 @@ struct user_struct * alloc_uid(uid_t uid)
 		 * Before adding this, check whether we raced
 		 * on adding the same user already..
 		 */
-		spin_lock_bh(&uidhash_lock);
+		spin_lock_irq(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
 			key_put(new->uid_keyring);
@@ -156,7 +163,7 @@ struct user_struct * alloc_uid(uid_t uid)
 			uid_hash_insert(new, hashent);
 			up = new;
 		}
-		spin_unlock_bh(&uidhash_lock);
+		spin_unlock_irq(&uidhash_lock);
 
 	}
 	return up;
@@ -192,9 +199,9 @@ static int __init uid_cache_init(void)
 		INIT_LIST_HEAD(uidhash_table + n);
 
 	/* Insert the root user immediately (init already runs as root) */
-	spin_lock_bh(&uidhash_lock);
+	spin_lock_irq(&uidhash_lock);
 	uid_hash_insert(&root_user, uidhashentry(0));
-	spin_unlock_bh(&uidhash_lock);
+	spin_unlock_irq(&uidhash_lock);
 
 	return 0;
 }

commit 4021cb279a532728c3208a16b9b09b0ca8016850
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jan 25 15:23:07 2006 +0100

    [PATCH] fix uidhash_lock <-> RCU deadlock
    
    RCU task-struct freeing can call free_uid(), which is taking
    uidhash_lock - while other users of uidhash_lock are softirq-unsafe.
    
    The fix is to always take the uidhash_spinlock in a softirq-safe manner.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Acked-by: Paul E. McKenney <paulmck@us.ibm.com>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index 89e562feb1b1..d1ae2349347e 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -13,6 +13,7 @@
 #include <linux/slab.h>
 #include <linux/bitops.h>
 #include <linux/key.h>
+#include <linux/interrupt.h>
 
 /*
  * UID task count cache, to get fast user lookup in "alloc_uid"
@@ -27,6 +28,12 @@
 
 static kmem_cache_t *uid_cachep;
 static struct list_head uidhash_table[UIDHASH_SZ];
+
+/*
+ * The uidhash_lock is mostly taken from process context, but it is
+ * occasionally also taken from softirq/tasklet context, when
+ * task-structs get RCU-freed. Hence all locking must be softirq-safe.
+ */
 static DEFINE_SPINLOCK(uidhash_lock);
 
 struct user_struct root_user = {
@@ -83,14 +90,15 @@ struct user_struct *find_user(uid_t uid)
 {
 	struct user_struct *ret;
 
-	spin_lock(&uidhash_lock);
+	spin_lock_bh(&uidhash_lock);
 	ret = uid_hash_find(uid, uidhashentry(uid));
-	spin_unlock(&uidhash_lock);
+	spin_unlock_bh(&uidhash_lock);
 	return ret;
 }
 
 void free_uid(struct user_struct *up)
 {
+	local_bh_disable();
 	if (up && atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
 		uid_hash_remove(up);
 		key_put(up->uid_keyring);
@@ -98,6 +106,7 @@ void free_uid(struct user_struct *up)
 		kmem_cache_free(uid_cachep, up);
 		spin_unlock(&uidhash_lock);
 	}
+	local_bh_enable();
 }
 
 struct user_struct * alloc_uid(uid_t uid)
@@ -105,9 +114,9 @@ struct user_struct * alloc_uid(uid_t uid)
 	struct list_head *hashent = uidhashentry(uid);
 	struct user_struct *up;
 
-	spin_lock(&uidhash_lock);
+	spin_lock_bh(&uidhash_lock);
 	up = uid_hash_find(uid, hashent);
-	spin_unlock(&uidhash_lock);
+	spin_unlock_bh(&uidhash_lock);
 
 	if (!up) {
 		struct user_struct *new;
@@ -137,7 +146,7 @@ struct user_struct * alloc_uid(uid_t uid)
 		 * Before adding this, check whether we raced
 		 * on adding the same user already..
 		 */
-		spin_lock(&uidhash_lock);
+		spin_lock_bh(&uidhash_lock);
 		up = uid_hash_find(uid, hashent);
 		if (up) {
 			key_put(new->uid_keyring);
@@ -147,7 +156,7 @@ struct user_struct * alloc_uid(uid_t uid)
 			uid_hash_insert(new, hashent);
 			up = new;
 		}
-		spin_unlock(&uidhash_lock);
+		spin_unlock_bh(&uidhash_lock);
 
 	}
 	return up;
@@ -183,9 +192,9 @@ static int __init uid_cache_init(void)
 		INIT_LIST_HEAD(uidhash_table + n);
 
 	/* Insert the root user immediately (init already runs as root) */
-	spin_lock(&uidhash_lock);
+	spin_lock_bh(&uidhash_lock);
 	uid_hash_insert(&root_user, uidhashentry(0));
-	spin_unlock(&uidhash_lock);
+	spin_unlock_bh(&uidhash_lock);
 
 	return 0;
 }

commit 0eeca28300df110bd6ed54b31193c83b87921443
Author: Robert Love <rml@novell.com>
Date:   Tue Jul 12 17:06:03 2005 -0400

    [PATCH] inotify
    
    inotify is intended to correct the deficiencies of dnotify, particularly
    its inability to scale and its terrible user interface:
    
            * dnotify requires the opening of one fd per each directory
              that you intend to watch. This quickly results in too many
              open files and pins removable media, preventing unmount.
            * dnotify is directory-based. You only learn about changes to
              directories. Sure, a change to a file in a directory affects
              the directory, but you are then forced to keep a cache of
              stat structures.
            * dnotify's interface to user-space is awful.  Signals?
    
    inotify provides a more usable, simple, powerful solution to file change
    notification:
    
            * inotify's interface is a system call that returns a fd, not SIGIO.
              You get a single fd, which is select()-able.
            * inotify has an event that says "the filesystem that the item
              you were watching is on was unmounted."
            * inotify can watch directories or files.
    
    Inotify is currently used by Beagle (a desktop search infrastructure),
    Gamin (a FAM replacement), and other projects.
    
    See Documentation/filesystems/inotify.txt.
    
    Signed-off-by: Robert Love <rml@novell.com>
    Cc: John McCutchan <ttb@tentacle.dhs.org>
    Cc: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Andrew Morton <akpm@osdl.org>
    Signed-off-by: Linus Torvalds <torvalds@osdl.org>

diff --git a/kernel/user.c b/kernel/user.c
index 734575d55769..89e562feb1b1 100644
--- a/kernel/user.c
+++ b/kernel/user.c
@@ -120,6 +120,10 @@ struct user_struct * alloc_uid(uid_t uid)
 		atomic_set(&new->processes, 0);
 		atomic_set(&new->files, 0);
 		atomic_set(&new->sigpending, 0);
+#ifdef CONFIG_INOTIFY
+		atomic_set(&new->inotify_watches, 0);
+		atomic_set(&new->inotify_devs, 0);
+#endif
 
 		new->mq_bytes = 0;
 		new->locked_shm = 0;

commit 1da177e4c3f41524e886b7f1b8a0c1fc7321cac2
Author: Linus Torvalds <torvalds@ppc970.osdl.org>
Date:   Sat Apr 16 15:20:36 2005 -0700

    Linux-2.6.12-rc2
    
    Initial git repository build. I'm not bothering with the full history,
    even though we have it. We can create a separate "historical" git
    archive of that later if we want to, and in the meantime it's about
    3.2GB when imported into git - space that would just make the early
    git days unnecessarily complicated, when we don't have a lot of good
    infrastructure for it.
    
    Let it rip!

diff --git a/kernel/user.c b/kernel/user.c
new file mode 100644
index 000000000000..734575d55769
--- /dev/null
+++ b/kernel/user.c
@@ -0,0 +1,189 @@
+/*
+ * The "user cache".
+ *
+ * (C) Copyright 1991-2000 Linus Torvalds
+ *
+ * We have a per-user structure to keep track of how many
+ * processes, files etc the user has claimed, in order to be
+ * able to have per-user limits for system resources. 
+ */
+
+#include <linux/init.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/bitops.h>
+#include <linux/key.h>
+
+/*
+ * UID task count cache, to get fast user lookup in "alloc_uid"
+ * when changing user ID's (ie setuid() and friends).
+ */
+
+#define UIDHASH_BITS (CONFIG_BASE_SMALL ? 3 : 8)
+#define UIDHASH_SZ		(1 << UIDHASH_BITS)
+#define UIDHASH_MASK		(UIDHASH_SZ - 1)
+#define __uidhashfn(uid)	(((uid >> UIDHASH_BITS) + uid) & UIDHASH_MASK)
+#define uidhashentry(uid)	(uidhash_table + __uidhashfn((uid)))
+
+static kmem_cache_t *uid_cachep;
+static struct list_head uidhash_table[UIDHASH_SZ];
+static DEFINE_SPINLOCK(uidhash_lock);
+
+struct user_struct root_user = {
+	.__count	= ATOMIC_INIT(1),
+	.processes	= ATOMIC_INIT(1),
+	.files		= ATOMIC_INIT(0),
+	.sigpending	= ATOMIC_INIT(0),
+	.mq_bytes	= 0,
+	.locked_shm     = 0,
+#ifdef CONFIG_KEYS
+	.uid_keyring	= &root_user_keyring,
+	.session_keyring = &root_session_keyring,
+#endif
+};
+
+/*
+ * These routines must be called with the uidhash spinlock held!
+ */
+static inline void uid_hash_insert(struct user_struct *up, struct list_head *hashent)
+{
+	list_add(&up->uidhash_list, hashent);
+}
+
+static inline void uid_hash_remove(struct user_struct *up)
+{
+	list_del(&up->uidhash_list);
+}
+
+static inline struct user_struct *uid_hash_find(uid_t uid, struct list_head *hashent)
+{
+	struct list_head *up;
+
+	list_for_each(up, hashent) {
+		struct user_struct *user;
+
+		user = list_entry(up, struct user_struct, uidhash_list);
+
+		if(user->uid == uid) {
+			atomic_inc(&user->__count);
+			return user;
+		}
+	}
+
+	return NULL;
+}
+
+/*
+ * Locate the user_struct for the passed UID.  If found, take a ref on it.  The
+ * caller must undo that ref with free_uid().
+ *
+ * If the user_struct could not be found, return NULL.
+ */
+struct user_struct *find_user(uid_t uid)
+{
+	struct user_struct *ret;
+
+	spin_lock(&uidhash_lock);
+	ret = uid_hash_find(uid, uidhashentry(uid));
+	spin_unlock(&uidhash_lock);
+	return ret;
+}
+
+void free_uid(struct user_struct *up)
+{
+	if (up && atomic_dec_and_lock(&up->__count, &uidhash_lock)) {
+		uid_hash_remove(up);
+		key_put(up->uid_keyring);
+		key_put(up->session_keyring);
+		kmem_cache_free(uid_cachep, up);
+		spin_unlock(&uidhash_lock);
+	}
+}
+
+struct user_struct * alloc_uid(uid_t uid)
+{
+	struct list_head *hashent = uidhashentry(uid);
+	struct user_struct *up;
+
+	spin_lock(&uidhash_lock);
+	up = uid_hash_find(uid, hashent);
+	spin_unlock(&uidhash_lock);
+
+	if (!up) {
+		struct user_struct *new;
+
+		new = kmem_cache_alloc(uid_cachep, SLAB_KERNEL);
+		if (!new)
+			return NULL;
+		new->uid = uid;
+		atomic_set(&new->__count, 1);
+		atomic_set(&new->processes, 0);
+		atomic_set(&new->files, 0);
+		atomic_set(&new->sigpending, 0);
+
+		new->mq_bytes = 0;
+		new->locked_shm = 0;
+
+		if (alloc_uid_keyring(new) < 0) {
+			kmem_cache_free(uid_cachep, new);
+			return NULL;
+		}
+
+		/*
+		 * Before adding this, check whether we raced
+		 * on adding the same user already..
+		 */
+		spin_lock(&uidhash_lock);
+		up = uid_hash_find(uid, hashent);
+		if (up) {
+			key_put(new->uid_keyring);
+			key_put(new->session_keyring);
+			kmem_cache_free(uid_cachep, new);
+		} else {
+			uid_hash_insert(new, hashent);
+			up = new;
+		}
+		spin_unlock(&uidhash_lock);
+
+	}
+	return up;
+}
+
+void switch_uid(struct user_struct *new_user)
+{
+	struct user_struct *old_user;
+
+	/* What if a process setreuid()'s and this brings the
+	 * new uid over his NPROC rlimit?  We can check this now
+	 * cheaply with the new uid cache, so if it matters
+	 * we should be checking for it.  -DaveM
+	 */
+	old_user = current->user;
+	atomic_inc(&new_user->processes);
+	atomic_dec(&old_user->processes);
+	switch_uid_keyring(new_user);
+	current->user = new_user;
+	free_uid(old_user);
+	suid_keys(current);
+}
+
+
+static int __init uid_cache_init(void)
+{
+	int n;
+
+	uid_cachep = kmem_cache_create("uid_cache", sizeof(struct user_struct),
+			0, SLAB_HWCACHE_ALIGN|SLAB_PANIC, NULL, NULL);
+
+	for(n = 0; n < UIDHASH_SZ; ++n)
+		INIT_LIST_HEAD(uidhash_table + n);
+
+	/* Insert the root user immediately (init already runs as root) */
+	spin_lock(&uidhash_lock);
+	uid_hash_insert(&root_user, uidhashentry(0));
+	spin_unlock(&uidhash_lock);
+
+	return 0;
+}
+
+module_init(uid_cache_init);
