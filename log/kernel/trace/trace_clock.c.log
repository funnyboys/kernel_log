commit bcea3f96e11cf2f0232d851e0fdb854f5ada425a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 16 11:23:53 2018 -0400

    tracing: Add SPDX License format tags to tracing files
    
    Add the SPDX License header to ease license compliance management.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index d8a188e0418a..aaf6793ededa 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * tracing clocks
  *

commit f7a1570da91558fb85b61e53243fe3fa79e2bbae
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 4 14:50:15 2018 -0400

    tracing: Hide global trace clock from lockdep
    
    Function tracing can trace in NMIs and such. If the TSC is determined
    to be unstable, the tracing clock will switch to the global clock on
    boot up, unless "trace_clock" is specified on the kernel command line.
    
    The global clock disables interrupts to access sched_clock_cpu(), and in
    doing so can be done within lockdep internals (because of function
    tracing and NMIs). This can trigger false lockdep splats.
    
    The trace_clock_global() is special, best not to trace the irq logic
    within it.
    
    Link: http://lkml.kernel.org/r/20180404145015.77bde42d@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 5fdc779f411d..d8a188e0418a 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -96,7 +96,7 @@ u64 notrace trace_clock_global(void)
 	int this_cpu;
 	u64 now;
 
-	local_irq_save(flags);
+	raw_local_irq_save(flags);
 
 	this_cpu = raw_smp_processor_id();
 	now = sched_clock_cpu(this_cpu);
@@ -122,7 +122,7 @@ u64 notrace trace_clock_global(void)
 	arch_spin_unlock(&trace_clock_struct.lock);
 
  out:
-	local_irq_restore(flags);
+	raw_local_irq_restore(flags);
 
 	return now;
 }

commit e601757102cfd3eeae068f53b3bc1234f3a2b2e9
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 16:36:40 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/clock.h>
    
    We are going to split <linux/sched/clock.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and .c files.
    
    Create a trivial placeholder <linux/sched/clock.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 0f06532a755b..5fdc779f411d 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -18,6 +18,7 @@
 #include <linux/module.h>
 #include <linux/percpu.h>
 #include <linux/sched.h>
+#include <linux/sched/clock.h>
 #include <linux/ktime.h>
 #include <linux/trace_clock.h>
 

commit 7e255d346c12888f7cce4b89a03a5fe5e9196ab1
Author: Jerry Snitselaar <jsnitsel@redhat.com>
Date:   Thu Apr 30 08:10:24 2015 -0700

    tracing: Export tracing clock functions
    
    Critical tracepoint hooks should never call anything that takes a lock,
    so they are unable to call getrawmonotonic() or ktime_get().
    
    Export the rest of the tracing clock functions so can be used in
    tracepoint hooks.
    
    Background: We have a customer that adds their own module and registers
    a tracepoint hook to sched_wakeup. They were using ktime_get() for a
    time source, but it grabs a seq lock and caused a deadlock to occur.
    
    Link: http://lkml.kernel.org/r/1430406624-22609-1-git-send-email-jsnitsel@redhat.com
    
    Signed-off-by: Jerry Snitselaar <jsnitsel@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 57b67b1f24d1..0f06532a755b 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -56,6 +56,7 @@ u64 notrace trace_clock(void)
 {
 	return local_clock();
 }
+EXPORT_SYMBOL_GPL(trace_clock);
 
 /*
  * trace_jiffy_clock(): Simply use jiffies as a clock counter.
@@ -68,6 +69,7 @@ u64 notrace trace_clock_jiffies(void)
 {
 	return jiffies_64_to_clock_t(jiffies_64 - INITIAL_JIFFIES);
 }
+EXPORT_SYMBOL_GPL(trace_clock_jiffies);
 
 /*
  * trace_clock_global(): special globally coherent trace clock
@@ -123,6 +125,7 @@ u64 notrace trace_clock_global(void)
 
 	return now;
 }
+EXPORT_SYMBOL_GPL(trace_clock_global);
 
 static atomic64_t trace_counter;
 

commit 58d4e21e50ff3cc57910a8abc20d7e14375d2f61
Author: Tony Luck <tony.luck@intel.com>
Date:   Fri Jul 18 11:43:01 2014 -0700

    tracing: Fix wraparound problems in "uptime" trace clock
    
    The "uptime" trace clock added in:
    
        commit 8aacf017b065a805d27467843490c976835eb4a5
        tracing: Add "uptime" trace clock that uses jiffies
    
    has wraparound problems when the system has been up more
    than 1 hour 11 minutes and 34 seconds. It converts jiffies
    to nanoseconds using:
            (u64)jiffies_to_usecs(jiffy) * 1000ULL
    but since jiffies_to_usecs() only returns a 32-bit value, it
    truncates at 2^32 microseconds.  An additional problem on 32-bit
    systems is that the argument is "unsigned long", so fixing the
    return value only helps until 2^32 jiffies (49.7 days on a HZ=1000
    system).
    
    Avoid these problems by using jiffies_64 as our basis, and
    not converting to nanoseconds (we do convert to clock_t because
    user facing API must not be dependent on internal kernel
    HZ values).
    
    Link: http://lkml.kernel.org/p/99d63c5bfe9b320a3b428d773825a37095bf6a51.1405708254.git.tony.luck@intel.com
    
    Cc: stable@vger.kernel.org # 3.10+
    Fixes: 8aacf017b065 "tracing: Add "uptime" trace clock that uses jiffies"
    Signed-off-by: Tony Luck <tony.luck@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 26dc348332b7..57b67b1f24d1 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -59,13 +59,14 @@ u64 notrace trace_clock(void)
 
 /*
  * trace_jiffy_clock(): Simply use jiffies as a clock counter.
+ * Note that this use of jiffies_64 is not completely safe on
+ * 32-bit systems. But the window is tiny, and the effect if
+ * we are affected is that we will have an obviously bogus
+ * timestamp on a trace event - i.e. not life threatening.
  */
 u64 notrace trace_clock_jiffies(void)
 {
-	u64 jiffy = jiffies - INITIAL_JIFFIES;
-
-	/* Return nsecs */
-	return (u64)jiffies_to_usecs(jiffy) * 1000ULL;
+	return jiffies_64_to_clock_t(jiffies_64 - INITIAL_JIFFIES);
 }
 
 /*

commit 8aacf017b065a805d27467843490c976835eb4a5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Mar 14 13:13:45 2013 -0400

    tracing: Add "uptime" trace clock that uses jiffies
    
    Add a simple trace clock called "uptime" for those that are
    interested in the uptime of the trace. It uses jiffies as that's
    the safest method, as other uptime clocks grab seq locks, which could
    cause a deadlock if taken from an event or function tracer.
    
    Requested-by: Mauro Carvalho Chehab <mchehab@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index aa8f5f48dae6..26dc348332b7 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -57,6 +57,16 @@ u64 notrace trace_clock(void)
 	return local_clock();
 }
 
+/*
+ * trace_jiffy_clock(): Simply use jiffies as a clock counter.
+ */
+u64 notrace trace_clock_jiffies(void)
+{
+	u64 jiffy = jiffies - INITIAL_JIFFIES;
+
+	/* Return nsecs */
+	return (u64)jiffies_to_usecs(jiffy) * 1000ULL;
+}
 
 /*
  * trace_clock_global(): special globally coherent trace clock

commit 8f55cea410dbc56114bb71a3742032070c8108d0
Merge: b7133a9a1036 e259514eef76
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Feb 19 17:49:41 2013 -0800

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf changes from Ingo Molnar:
     "There are lots of improvements, the biggest changes are:
    
      Main kernel side changes:
    
       - Improve uprobes performance by adding 'pre-filtering' support, by
         Oleg Nesterov.
    
       - Make some POWER7 events available in sysfs, equivalent to what was
         done on x86, from Sukadev Bhattiprolu.
    
       - tracing updates by Steve Rostedt - mostly misc fixes and smaller
         improvements.
    
       - Use perf/event tracing to report PCI Express advanced errors, by
         Tony Luck.
    
       - Enable northbridge performance counters on AMD family 15h, by Jacob
         Shin.
    
       - This tracing commit:
    
            tracing: Remove the extra 4 bytes of padding in events
    
         changes the ABI.  All involved parties (PowerTop in particular)
         seem to agree that it's safe to do now with the introduction of
         libtraceevent, but the devil is in the details ...
    
      Main tooling side changes:
    
       - Add 'event group view', from Namyung Kim:
    
         To use it, 'perf record' should group events when recording.  And
         then perf report parses the saved group relation from file header
         and prints them together if --group option is provided.  You can
         use the 'perf evlist' command to see event group information:
    
            $ perf record -e '{ref-cycles,cycles}' noploop 1
            [ perf record: Woken up 2 times to write data ]
            [ perf record: Captured and wrote 0.385 MB perf.data (~16807 samples) ]
    
            $ perf evlist --group
            {ref-cycles,cycles}
    
         With this example, default perf report will show you each event
         separately.
    
         You can use --group option to enable event group view:
    
            $ perf report --group
            ...
            # group: {ref-cycles,cycles}
            # ========
            # Samples: 7K of event 'anon group { ref-cycles, cycles }'
            # Event count (approx.): 6876107743
            #
            #         Overhead  Command      Shared Object                      Symbol
            # ................  .......  .................  ..........................
                99.84%  99.76%  noploop  noploop            [.] main
                 0.07%   0.00%  noploop  ld-2.15.so         [.] strcmp
                 0.03%   0.00%  noploop  [kernel.kallsyms]  [k] timerqueue_del
                 0.03%   0.03%  noploop  [kernel.kallsyms]  [k] sched_clock_cpu
                 0.02%   0.00%  noploop  [kernel.kallsyms]  [k] account_user_time
                 0.01%   0.00%  noploop  [kernel.kallsyms]  [k] __alloc_pages_nodemask
                 0.00%   0.00%  noploop  [kernel.kallsyms]  [k] native_write_msr_safe
                 0.00%   0.11%  noploop  [kernel.kallsyms]  [k] _raw_spin_lock
                 0.00%   0.06%  noploop  [kernel.kallsyms]  [k] find_get_page
                 0.00%   0.02%  noploop  [kernel.kallsyms]  [k] rcu_check_callbacks
                 0.00%   0.02%  noploop  [kernel.kallsyms]  [k] __current_kernel_time
    
         As you can see the Overhead column now contains both of ref-cycles
         and cycles and header line shows group information also - 'anon
         group { ref-cycles, cycles }'.  The output is sorted by period of
         group leader first.
    
       - Initial GTK+ annotate browser, from Namhyung Kim.
    
       - Add option for runtime switching perf data file in perf report,
         just press 's' and a menu with the valid files found in the current
         directory will be presented, from Feng Tang.
    
       - Add support to display whole group data for raw columns, from Jiri
         Olsa.
    
       - Add per processor socket count aggregation in perf stat, from
         Stephane Eranian.
    
       - Add interval printing in 'perf stat', from Stephane Eranian.
    
       - 'perf test' improvements
    
       - Add support for wildcards in tracepoint system name, from Jiri
         Olsa.
    
       - Add anonymous huge page recognition, from Joshua Zhu.
    
       - perf build-id cache now can show DSOs present in a perf.data file
         that are not in the cache, to integrate with build-id servers being
         put in place by organizations such as Fedora.
    
       - perf top now shares more of the evsel config/creation routines with
         'record', paving the way for further integration like 'top'
         snapshots, etc.
    
       - perf top now supports DWARF callchains.
    
       - Fix mmap limitations on 32-bit, fix from David Miller.
    
       - 'perf bench numa mem' NUMA performance measurement suite
    
       - ... and lots of fixes, performance improvements, cleanups and other
         improvements I failed to list - see the shortlog and git log for
         details."
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (270 commits)
      perf/x86/amd: Enable northbridge performance counters on AMD family 15h
      perf/hwbp: Fix cleanup in case of kzalloc failure
      perf tools: Fix build with bison 2.3 and older.
      perf tools: Limit unwind support to x86 archs
      perf annotate: Make it to be able to skip unannotatable symbols
      perf gtk/annotate: Fail early if it can't annotate
      perf gtk/annotate: Show source lines with gray color
      perf gtk/annotate: Support multiple event annotation
      perf ui/gtk: Implement basic GTK2 annotation browser
      perf annotate: Fix warning message on a missing vmlinux
      perf buildid-cache: Add --update option
      uprobes/perf: Avoid uprobe_apply() whenever possible
      uprobes/perf: Teach trace_uprobe/perf code to use UPROBE_HANDLER_REMOVE
      uprobes/perf: Teach trace_uprobe/perf code to pre-filter
      uprobes/perf: Teach trace_uprobe/perf code to track the active perf_event's
      uprobes: Introduce uprobe_apply()
      perf: Introduce hw_perf_event->tp_target and ->tp_list
      uprobes/perf: Always increment trace_uprobe->nhit
      uprobes/tracing: Kill uprobe_trace_consumer, embed uprobe_consumer into trace_uprobe
      uprobes/tracing: Introduce is_trace_uprobe_enabled()
      ...

commit 5e67b51e3fb22ad43faf9589e9019ad9c6a00413
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Thu Dec 27 11:49:45 2012 +0900

    tracing: Use sched_clock_cpu for trace_clock_global
    
    For systems with an unstable sched_clock, all cpu_clock() does is enable/
    disable local irq during the call to sched_clock_cpu().  And for stable
    systems they are same.
    
    trace_clock_global() already disables interrupts, so it can call
    sched_clock_cpu() directly.
    
    Link: http://lkml.kernel.org/r/1356576585-28782-2-git-send-email-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 22b638b28e48..24bf48eabfcc 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -84,7 +84,7 @@ u64 notrace trace_clock_global(void)
 	local_irq_save(flags);
 
 	this_cpu = raw_smp_processor_id();
-	now = cpu_clock(this_cpu);
+	now = sched_clock_cpu(this_cpu);
 	/*
 	 * If in an NMI context then dont risk lockups and return the
 	 * cpu_clock() time:

commit 0a71e4c6d749d06f52e75a406fc9046924fcfcc1
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jan 22 12:06:56 2013 -0500

    tracing: Remove trace.h header from trace_clock.c
    
    As trace_clock is used by other things besides tracing, and it
    does not require anything from trace.h, it is best not to include
    the header file in trace_clock.c.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 394783531cbb..22b638b28e48 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -21,8 +21,6 @@
 #include <linux/ktime.h>
 #include <linux/trace_clock.h>
 
-#include "trace.h"
-
 /*
  * trace_clock_local(): the simplest and least coherent tracing clock.
  *

commit dc975e94f322e60fa8fcc44dec1820fde4de174c
Author: Paul E. McKenney <paul.mckenney@linaro.org>
Date:   Thu Nov 15 11:27:26 2012 -0800

    tracing: Export trace_clock_local()
    
    The rcutorture tests need to be able to trace the time of the
    beginning of an RCU read-side critical section, and thus need access
    to trace_clock_local().  This commit therefore adds a the needed
    EXPORT_SYMBOL_GPL().
    
    Signed-off-by: Paul E. McKenney <paul.mckenney@linaro.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 394783531cbb..1bbb1b200cec 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -44,6 +44,7 @@ u64 notrace trace_clock_local(void)
 
 	return clock;
 }
+EXPORT_SYMBOL_GPL(trace_clock_local);
 
 /*
  * trace_clock(): 'between' trace clock. Not completely serialized,

commit 6249687f76b69cc0b2ad34636f4a18d693ef3262
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Sep 19 11:35:58 2011 -0400

    tracing: Add a counter clock for those that do not trust clocks
    
    When debugging tight race conditions, it can be helpful to have a
    synchronized tracing method. Although in most cases the global clock
    provides this functionality, if timings is not the issue, it is more
    comforting to know that the order of events really happened in a precise
    order.
    
    Instead of using a clock, add a "counter" that is simply an incrementing
    atomic 64bit counter that orders the events as they are perceived to
    happen.
    
    The trace_clock_counter() is added from the attempt by Peter Zijlstra
    trying to convert the trace_clock_global() to it. I took Peter's counter
    code and made trace_clock_counter() instead, and added it to the choice
    of clocks. Just echo counter > /debug/tracing/trace_clock to activate
    it.
    
    Requested-by: Thomas Gleixner <tglx@linutronix.de>
    Requested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Reviewed-By: Valdis Kletnieks <valdis.kletnieks@vt.edu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 6302747a1398..394783531cbb 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -113,3 +113,15 @@ u64 notrace trace_clock_global(void)
 
 	return now;
 }
+
+static atomic64_t trace_counter;
+
+/*
+ * trace_clock_counter(): simply an atomic counter.
+ * Use the trace_counter "counter" for cases where you do not care
+ * about timings, but are interested in strict ordering.
+ */
+u64 notrace trace_clock_counter(void)
+{
+	return atomic64_add_return(1, &trace_counter);
+}

commit 25985edcedea6396277003854657b5f3cb31a628
Author: Lucas De Marchi <lucas.demarchi@profusion.mobi>
Date:   Wed Mar 30 22:57:33 2011 -0300

    Fix common misspellings
    
    Fixes generated by 'codespell' and manually reviewed.
    
    Signed-off-by: Lucas De Marchi <lucas.demarchi@profusion.mobi>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 685a67d55db0..6302747a1398 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -46,7 +46,7 @@ u64 notrace trace_clock_local(void)
 }
 
 /*
- * trace_clock(): 'inbetween' trace clock. Not completely serialized,
+ * trace_clock(): 'between' trace clock. Not completely serialized,
  * but not completely incorrect when crossing CPUs either.
  *
  * This is based on cpu_clock(), which will allow at most ~1 jiffy of

commit c4efd6b569b2646e1346a08a4c40286f8bcb5f11
Merge: 4aed2fd8e318 0bcfe7580794
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 09:39:22 2010 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (27 commits)
      sched: Use correct macro to display sched_child_runs_first in /proc/sched_debug
      sched: No need for bootmem special cases
      sched: Revert nohz_ratelimit() for now
      sched: Reduce update_group_power() calls
      sched: Update rq->clock for nohz balanced cpus
      sched: Fix spelling of sibling
      sched, cpuset: Drop __cpuexit from cpu hotplug callbacks
      sched: Fix the racy usage of thread_group_cputimer() in fastpath_timer_check()
      sched: run_posix_cpu_timers: Don't check ->exit_state, use lock_task_sighand()
      sched: thread_group_cputime: Simplify, document the "alive" check
      sched: Remove the obsolete exit_state/signal hacks
      sched: task_tick_rt: Remove the obsolete ->signal != NULL check
      sched: __sched_setscheduler: Read the RLIMIT_RTPRIO value lockless
      sched: Fix comments to make them DocBook happy
      sched: Fix fix_small_capacity
      powerpc: Exclude arch_sd_sibiling_asym_packing() on UP
      powerpc: Enable asymmetric SMT scheduling on POWER7
      sched: Add asymmetric group packing option for sibling domain
      sched: Fix capacity calculations for SMT4
      sched: Change nohz idle load balancing logic to push model
      ...

commit c676329abb2b8359d9a5d734dec0c81779823fd6
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 25 10:48:51 2010 +0200

    sched_clock: Add local_clock() API and improve documentation
    
    For people who otherwise get to write: cpu_clock(smp_processor_id()),
    there is now: local_clock().
    
    Also, as per suggestion from Andrew, provide some documentation on
    the various clock interfaces, and minimize the unsigned long long vs
    u64 mess.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Jens Axboe <jaxboe@fusionio.com>
    LKML-Reference: <1275052414.1645.52.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 9d589d8dcd1a..1723e2b8c589 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -56,7 +56,7 @@ u64 notrace trace_clock_local(void)
  */
 u64 notrace trace_clock(void)
 {
-	return cpu_clock(raw_smp_processor_id());
+	return local_clock();
 }
 
 

commit 5168ae50a66e3ff7184c2b16d661bd6d70367e50
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 3 09:36:50 2010 -0400

    tracing: Remove ftrace_preempt_disable/enable
    
    The ftrace_preempt_disable/enable functions were to address a
    recursive race caused by the function tracer. The function tracer
    traces all functions which makes it easily susceptible to recursion.
    One area was preempt_enable(). This would call the scheduler and
    the schedulre would call the function tracer and loop.
    (So was it thought).
    
    The ftrace_preempt_disable/enable was made to protect against recursion
    inside the scheduler by storing the NEED_RESCHED flag. If it was
    set before the ftrace_preempt_disable() it would not call schedule
    on ftrace_preempt_enable(), thinking that if it was set before then
    it would have already scheduled unless it was already in the scheduler.
    
    This worked fine except in the case of SMP, where another task would set
    the NEED_RESCHED flag for a task on another CPU, and then kick off an
    IPI to trigger it. This could cause the NEED_RESCHED to be saved at
    ftrace_preempt_disable() but the IPI to arrive in the the preempt
    disabled section. The ftrace_preempt_enable() would not call the scheduler
    because the flag was already set before entring the section.
    
    This bug would cause a missed preemption check and cause lower latencies.
    
    Investigating further, I found that the recusion caused by the function
    tracer was not due to schedule(), but due to preempt_schedule(). Now
    that preempt_schedule is completely annotated with notrace, the recusion
    no longer is an issue.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 9d589d8dcd1a..52fda6c04ac3 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -32,16 +32,15 @@
 u64 notrace trace_clock_local(void)
 {
 	u64 clock;
-	int resched;
 
 	/*
 	 * sched_clock() is an architecture implemented, fast, scalable,
 	 * lockless clock. It is not guaranteed to be coherent across
 	 * CPUs, nor across CPU idle events.
 	 */
-	resched = ftrace_preempt_disable();
+	preempt_disable_notrace();
 	clock = sched_clock();
-	ftrace_preempt_enable(resched);
+	preempt_enable_notrace();
 
 	return clock;
 }

commit e36673ec5126f15a8cddf6049aede7bdcf484c26
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Mar 24 10:57:37 2010 +0800

    tracing: Fix lockdep warning in global_clock()
    
    # echo 1 > events/enable
     # echo global > trace_clock
    
    ------------[ cut here ]------------
    WARNING: at kernel/lockdep.c:3162 check_flags+0xb2/0x190()
    ...
    ---[ end trace 3f86734a89416623 ]---
    possible reason: unannotated irqs-on.
    ...
    
    There's no reason to use the raw_local_irq_save() in trace_clock_global.
    The local_irq_save() version is fine, and does not cause the bug in lockdep.
    
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4BA97FA1.7030606@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 6fbfb8f417b9..9d589d8dcd1a 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -84,7 +84,7 @@ u64 notrace trace_clock_global(void)
 	int this_cpu;
 	u64 now;
 
-	raw_local_irq_save(flags);
+	local_irq_save(flags);
 
 	this_cpu = raw_smp_processor_id();
 	now = cpu_clock(this_cpu);
@@ -110,7 +110,7 @@ u64 notrace trace_clock_global(void)
 	arch_spin_unlock(&trace_clock_struct.lock);
 
  out:
-	raw_local_irq_restore(flags);
+	local_irq_restore(flags);
 
 	return now;
 }

commit ae1f30384baef4056438d81b305a6a5199b0d16c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Feb 28 19:42:38 2010 +0100

    tracing: Include irqflags headers from trace clock
    
    trace_clock.c includes spinlock.h, which ends up including
    asm/system.h, which in turn includes linux/irqflags.h in x86.
    
    So the definition of raw_local_irq_save is luckily covered there,
    but this is not the case in parisc:
    
       tip/kernel/trace/trace_clock.c:86: error: implicit declaration of function 'raw_local_irq_save'
       tip/kernel/trace/trace_clock.c:112: error: implicit declaration of function 'raw_local_irq_restore'
    
    We need to include linux/irqflags.h directly from trace_clock.c
    to avoid such build error.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 84a3a7ba072a..6fbfb8f417b9 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -13,6 +13,7 @@
  * Tracer plugins will chose a default from these clocks.
  */
 #include <linux/spinlock.h>
+#include <linux/irqflags.h>
 #include <linux/hardirq.h>
 #include <linux/module.h>
 #include <linux/percpu.h>

commit 0199c4e68d1f02894bdefe4b5d9e9ee4aedd8d62
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 2 20:01:25 2009 +0100

    locking: Convert __raw_spin* functions to arch_spin*
    
    Name space cleanup. No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: linux-arch@vger.kernel.org

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 433e2eda2d01..84a3a7ba072a 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -94,7 +94,7 @@ u64 notrace trace_clock_global(void)
 	if (unlikely(in_nmi()))
 		goto out;
 
-	__raw_spin_lock(&trace_clock_struct.lock);
+	arch_spin_lock(&trace_clock_struct.lock);
 
 	/*
 	 * TODO: if this happens often then maybe we should reset
@@ -106,7 +106,7 @@ u64 notrace trace_clock_global(void)
 
 	trace_clock_struct.prev_time = now;
 
-	__raw_spin_unlock(&trace_clock_struct.lock);
+	arch_spin_unlock(&trace_clock_struct.lock);
 
  out:
 	raw_local_irq_restore(flags);

commit edc35bd72e2079b25f99c5da7d7a65dbbffc4a26
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 3 12:38:57 2009 +0100

    locking: Rename __RAW_SPIN_LOCK_UNLOCKED to __ARCH_SPIN_LOCK_UNLOCKED
    
    Further name space cleanup. No functional change
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: linux-arch@vger.kernel.org

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 206ec3d4b3c2..433e2eda2d01 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -74,7 +74,7 @@ static struct {
 	arch_spinlock_t lock;
 } trace_clock_struct ____cacheline_aligned_in_smp =
 	{
-		.lock = (arch_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED,
+		.lock = (arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED,
 	};
 
 u64 notrace trace_clock_global(void)

commit 445c89514be242b1b0080056d50bdc1b72adeb5c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 2 19:49:50 2009 +0100

    locking: Convert raw_spinlock to arch_spinlock
    
    The raw_spin* namespace was taken by lockdep for the architecture
    specific implementations. raw_spin_* would be the ideal name space for
    the spinlocks which are not converted to sleeping locks in preempt-rt.
    
    Linus suggested to convert the raw_ to arch_ locks and cleanup the
    name space instead of using an artifical name like core_spin,
    atomic_spin or whatever
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: linux-arch@vger.kernel.org

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 878c03f386ba..206ec3d4b3c2 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -71,10 +71,10 @@ u64 notrace trace_clock(void)
 /* keep prev_time and lock in the same cacheline. */
 static struct {
 	u64 prev_time;
-	raw_spinlock_t lock;
+	arch_spinlock_t lock;
 } trace_clock_struct ____cacheline_aligned_in_smp =
 	{
-		.lock = (raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED,
+		.lock = (arch_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED,
 	};
 
 u64 notrace trace_clock_global(void)

commit 8b2a5dac7859dd1954095fce8b6445c3ceb36ef6
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 11 19:36:03 2009 -0500

    tracing: do not disable interrupts for trace_clock_local
    
    Disabling interrupts in trace_clock_local takes quite a performance
    hit to the recording of traces. Using perf top we see:
    
    ------------------------------------------------------------------------------
       PerfTop:     244 irqs/sec  kernel:100.0% [1000Hz cpu-clock-msecs],  (all, 4 CPUs)
    ------------------------------------------------------------------------------
    
                 samples    pcnt   kernel function
                 _______   _____   _______________
    
                 2842.00 - 40.4% : trace_clock_local
                 1043.00 - 14.8% : rb_reserve_next_event
                  784.00 - 11.1% : ring_buffer_lock_reserve
                  600.00 -  8.5% : __rb_reserve_next
                  579.00 -  8.2% : rb_end_commit
                  440.00 -  6.3% : ring_buffer_unlock_commit
                  290.00 -  4.1% : ring_buffer_producer_thread      [ring_buffer_benchmark]
                  155.00 -  2.2% : debug_smp_processor_id
                  117.00 -  1.7% : trace_recursive_unlock
                  103.00 -  1.5% : ring_buffer_event_data
                   28.00 -  0.4% : do_gettimeofday
                   22.00 -  0.3% : _spin_unlock_irq
                   14.00 -  0.2% : native_read_tsc
                   11.00 -  0.2% : getnstimeofday
    
    Where trace_clock_local is 40% of the tracing, and the time for recording
    a trace according to ring_buffer_benchmark is 210ns. After converting
    the interrupts to preemption disabling we have from perf top:
    
    ------------------------------------------------------------------------------
       PerfTop:    1084 irqs/sec  kernel:99.9% [1000Hz cpu-clock-msecs],  (all, 4 CPUs)
    ------------------------------------------------------------------------------
    
                 samples    pcnt   kernel function
                 _______   _____   _______________
    
                 1277.00 - 16.8% : native_read_tsc
                 1148.00 - 15.1% : rb_reserve_next_event
                  896.00 - 11.8% : ring_buffer_lock_reserve
                  688.00 -  9.1% : __rb_reserve_next
                  664.00 -  8.8% : rb_end_commit
                  563.00 -  7.4% : ring_buffer_unlock_commit
                  508.00 -  6.7% : _spin_unlock_irq
                  365.00 -  4.8% : debug_smp_processor_id
                  321.00 -  4.2% : trace_clock_local
                  303.00 -  4.0% : ring_buffer_producer_thread      [ring_buffer_benchmark]
                  273.00 -  3.6% : native_sched_clock
                  122.00 -  1.6% : trace_recursive_unlock
                  113.00 -  1.5% : sched_clock
                  101.00 -  1.3% : ring_buffer_event_data
                   53.00 -  0.7% : tick_nohz_stop_sched_tick
    
    Where trace_clock_local drops from 40% to only taking 4% of the total time.
    The trace time also goes from 210ns down to 179ns (31ns).
    
    I talked with Peter Zijlstra about the impact that sched_clock may have
    without having interrupts disabled, and he told me that if a timer interrupt
    comes in, sched_clock may report a wrong time.
    
    Balancing a seldom incorrect timestamp with a 15% performance boost, I'll
    take the performance boost.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 20c5f92e28a8..878c03f386ba 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -20,6 +20,8 @@
 #include <linux/ktime.h>
 #include <linux/trace_clock.h>
 
+#include "trace.h"
+
 /*
  * trace_clock_local(): the simplest and least coherent tracing clock.
  *
@@ -28,17 +30,17 @@
  */
 u64 notrace trace_clock_local(void)
 {
-	unsigned long flags;
 	u64 clock;
+	int resched;
 
 	/*
 	 * sched_clock() is an architecture implemented, fast, scalable,
 	 * lockless clock. It is not guaranteed to be coherent across
 	 * CPUs, nor across CPU idle events.
 	 */
-	raw_local_irq_save(flags);
+	resched = ftrace_preempt_disable();
 	clock = sched_clock();
-	raw_local_irq_restore(flags);
+	ftrace_preempt_enable(resched);
 
 	return clock;
 }

commit 6ca6cca31ddc7cc8b1dc38b12d7593d2667defe8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Sep 15 12:24:22 2009 -0400

    tracing: optimize global_trace_clock cachelines
    
    The prev_trace_clock_time is only read or written to when the
    trace_clock_lock is taken. For better perfomance, they
    should share the same cache line.
    
    Reported-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index b588fd81f7f9..20c5f92e28a8 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -66,10 +66,14 @@ u64 notrace trace_clock(void)
  * Used by plugins that need globally coherent timestamps.
  */
 
-static u64 prev_trace_clock_time;
-
-static raw_spinlock_t trace_clock_lock ____cacheline_aligned_in_smp =
-	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+/* keep prev_time and lock in the same cacheline. */
+static struct {
+	u64 prev_time;
+	raw_spinlock_t lock;
+} trace_clock_struct ____cacheline_aligned_in_smp =
+	{
+		.lock = (raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED,
+	};
 
 u64 notrace trace_clock_global(void)
 {
@@ -88,19 +92,19 @@ u64 notrace trace_clock_global(void)
 	if (unlikely(in_nmi()))
 		goto out;
 
-	__raw_spin_lock(&trace_clock_lock);
+	__raw_spin_lock(&trace_clock_struct.lock);
 
 	/*
 	 * TODO: if this happens often then maybe we should reset
-	 * my_scd->clock to prev_trace_clock_time+1, to make sure
+	 * my_scd->clock to prev_time+1, to make sure
 	 * we start ticking with the local clock from now on?
 	 */
-	if ((s64)(now - prev_trace_clock_time) < 0)
-		now = prev_trace_clock_time + 1;
+	if ((s64)(now - trace_clock_struct.prev_time) < 0)
+		now = trace_clock_struct.prev_time + 1;
 
-	prev_trace_clock_time = now;
+	trace_clock_struct.prev_time = now;
 
-	__raw_spin_unlock(&trace_clock_lock);
+	__raw_spin_unlock(&trace_clock_struct.lock);
 
  out:
 	raw_local_irq_restore(flags);

commit b8b94265337f83b7db9c5f429b1769d463d7da8c
Author: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
Date:   Sun Mar 22 19:11:11 2009 +0200

    tracing: fix four sparse warnings
    
    Impact: cleanup.
    
    This patch fixes the following sparse warnings:
    
     kernel/trace/trace.c:385:9: warning: symbol 'trace_seq_to_buffer' was
     not declared. Should it be static?
    
     kernel/trace/trace_clock.c:29:13: warning: symbol 'trace_clock_local'
     was not declared. Should it be static?
    
     kernel/trace/trace_clock.c:54:13: warning: symbol 'trace_clock' was not
     declared. Should it be static?
    
     kernel/trace/trace_clock.c:74:13: warning: symbol 'trace_clock_global'
     was not declared. Should it be static?
    
    Signed-off-by: Dmitri Vorobiev <dmitri.vorobiev@movial.com>
    LKML-Reference: <1237741871-5827-4-git-send-email-dmitri.vorobiev@movial.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 05b176abfd30..b588fd81f7f9 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -18,6 +18,7 @@
 #include <linux/percpu.h>
 #include <linux/sched.h>
 #include <linux/ktime.h>
+#include <linux/trace_clock.h>
 
 /*
  * trace_clock_local(): the simplest and least coherent tracing clock.

commit 6cc3c6e12bb039047974ad2e7e2d46d15a1b762f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue Mar 10 19:03:43 2009 +0100

    trace_clock: fix preemption bug
    
    Using the function_graph tracer in recent kernels generates a spew of
    preemption BUGs. Fix this by not requiring trace_clock_local() users
    to disable preemption themselves.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
index 2d4953f93560..05b176abfd30 100644
--- a/kernel/trace/trace_clock.c
+++ b/kernel/trace/trace_clock.c
@@ -27,12 +27,19 @@
  */
 u64 notrace trace_clock_local(void)
 {
+	unsigned long flags;
+	u64 clock;
+
 	/*
 	 * sched_clock() is an architecture implemented, fast, scalable,
 	 * lockless clock. It is not guaranteed to be coherent across
 	 * CPUs, nor across CPU idle events.
 	 */
-	return sched_clock();
+	raw_local_irq_save(flags);
+	clock = sched_clock();
+	raw_local_irq_restore(flags);
+
+	return clock;
 }
 
 /*

commit 14131f2f98ac350ee9e73faed916d2238a8b6a0d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Feb 26 18:47:11 2009 +0100

    tracing: implement trace_clock_*() APIs
    
    Impact: implement new tracing timestamp APIs
    
    Add three trace clock variants, with differing scalability/precision
    tradeoffs:
    
     -   local: CPU-local trace clock
     -  medium: scalable global clock with some jitter
     -  global: globally monotonic, serialized clock
    
    Make the ring-buffer use the local trace clock internally.
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_clock.c b/kernel/trace/trace_clock.c
new file mode 100644
index 000000000000..2d4953f93560
--- /dev/null
+++ b/kernel/trace/trace_clock.c
@@ -0,0 +1,101 @@
+/*
+ * tracing clocks
+ *
+ *  Copyright (C) 2009 Red Hat, Inc., Ingo Molnar <mingo@redhat.com>
+ *
+ * Implements 3 trace clock variants, with differing scalability/precision
+ * tradeoffs:
+ *
+ *  -   local: CPU-local trace clock
+ *  -  medium: scalable global clock with some jitter
+ *  -  global: globally monotonic, serialized clock
+ *
+ * Tracer plugins will chose a default from these clocks.
+ */
+#include <linux/spinlock.h>
+#include <linux/hardirq.h>
+#include <linux/module.h>
+#include <linux/percpu.h>
+#include <linux/sched.h>
+#include <linux/ktime.h>
+
+/*
+ * trace_clock_local(): the simplest and least coherent tracing clock.
+ *
+ * Useful for tracing that does not cross to other CPUs nor
+ * does it go through idle events.
+ */
+u64 notrace trace_clock_local(void)
+{
+	/*
+	 * sched_clock() is an architecture implemented, fast, scalable,
+	 * lockless clock. It is not guaranteed to be coherent across
+	 * CPUs, nor across CPU idle events.
+	 */
+	return sched_clock();
+}
+
+/*
+ * trace_clock(): 'inbetween' trace clock. Not completely serialized,
+ * but not completely incorrect when crossing CPUs either.
+ *
+ * This is based on cpu_clock(), which will allow at most ~1 jiffy of
+ * jitter between CPUs. So it's a pretty scalable clock, but there
+ * can be offsets in the trace data.
+ */
+u64 notrace trace_clock(void)
+{
+	return cpu_clock(raw_smp_processor_id());
+}
+
+
+/*
+ * trace_clock_global(): special globally coherent trace clock
+ *
+ * It has higher overhead than the other trace clocks but is still
+ * an order of magnitude faster than GTOD derived hardware clocks.
+ *
+ * Used by plugins that need globally coherent timestamps.
+ */
+
+static u64 prev_trace_clock_time;
+
+static raw_spinlock_t trace_clock_lock ____cacheline_aligned_in_smp =
+	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+
+u64 notrace trace_clock_global(void)
+{
+	unsigned long flags;
+	int this_cpu;
+	u64 now;
+
+	raw_local_irq_save(flags);
+
+	this_cpu = raw_smp_processor_id();
+	now = cpu_clock(this_cpu);
+	/*
+	 * If in an NMI context then dont risk lockups and return the
+	 * cpu_clock() time:
+	 */
+	if (unlikely(in_nmi()))
+		goto out;
+
+	__raw_spin_lock(&trace_clock_lock);
+
+	/*
+	 * TODO: if this happens often then maybe we should reset
+	 * my_scd->clock to prev_trace_clock_time+1, to make sure
+	 * we start ticking with the local clock from now on?
+	 */
+	if ((s64)(now - prev_trace_clock_time) < 0)
+		now = prev_trace_clock_time + 1;
+
+	prev_trace_clock_time = now;
+
+	__raw_spin_unlock(&trace_clock_lock);
+
+ out:
+	raw_local_irq_restore(flags);
+
+	return now;
+}
