commit 7ff0d4490ebadae8037a079a70c5cac13385a808
Author: Christoph Hellwig <hch@lst.de>
Date:   Wed Jun 3 07:52:37 2020 +0200

    trace: fix an incorrect __user annotation on stack_trace_sysctl
    
    No user pointers for sysctls anymore.
    
    Fixes: 32927393dc1c ("sysctl: pass kernel pointers to ->proc_handler")
    Reported-by: build test robot <lkp@intel.com>
    Signed-off-by: Christoph Hellwig <hch@lst.de>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index c557f42a9397..98bba4764c52 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -515,9 +515,8 @@ static const struct file_operations stack_trace_filter_fops = {
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 int
-stack_trace_sysctl(struct ctl_table *table, int write,
-		   void __user *buffer, size_t *lenp,
-		   loff_t *ppos)
+stack_trace_sysctl(struct ctl_table *table, int write, void *buffer,
+		   size_t *lenp, loff_t *ppos)
 {
 	int was_enabled;
 	int ret;

commit b8299d362d0837ae39e87e9019ebe6b736e0f035
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jan 2 22:02:41 2020 -0500

    tracing: Have stack tracer compile when MCOUNT_INSN_SIZE is not defined
    
    On some archs with some configurations, MCOUNT_INSN_SIZE is not defined, and
    this makes the stack tracer fail to compile. Just define it to zero in this
    case.
    
    Link: https://lore.kernel.org/r/202001020219.zvE3vsty%lkp@intel.com
    
    Cc: stable@vger.kernel.org
    Fixes: 4df297129f622 ("tracing: Remove most or all of stack tracer stack size from stack_max_size")
    Reported-by: kbuild test robot <lkp@intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4df9a209f7ca..c557f42a9397 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -283,6 +283,11 @@ static void check_stack(unsigned long ip, unsigned long *stack)
 	local_irq_restore(flags);
 }
 
+/* Some archs may not define MCOUNT_INSN_SIZE */
+#ifndef MCOUNT_INSN_SIZE
+# define MCOUNT_INSN_SIZE 0
+#endif
+
 static void
 stack_trace_call(unsigned long ip, unsigned long parent_ip,
 		 struct ftrace_ops *op, struct pt_regs *pt_regs)

commit 17911ff38aa58d3c95c07589dbf5d3564c4cf3c5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Oct 11 17:22:50 2019 -0400

    tracing: Add locked_down checks to the open calls of files created for tracefs
    
    Added various checks on open tracefs calls to see if tracefs is in lockdown
    mode, and if so, to return -EPERM.
    
    Note, the event format files (which are basically standard on all machines)
    as well as the enabled_functions file (which shows what is currently being
    traced) are not lockde down. Perhaps they should be, but it seems counter
    intuitive to lockdown information to help you know if the system has been
    modified.
    
    Link: http://lkml.kernel.org/r/CAHk-=wj7fGPKUspr579Cii-w_y60PtRaiDgKuxVtBAMK0VNNkA@mail.gmail.com
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index ec9a34a97129..4df9a209f7ca 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -5,6 +5,7 @@
  */
 #include <linux/sched/task_stack.h>
 #include <linux/stacktrace.h>
+#include <linux/security.h>
 #include <linux/kallsyms.h>
 #include <linux/seq_file.h>
 #include <linux/spinlock.h>
@@ -470,6 +471,12 @@ static const struct seq_operations stack_trace_seq_ops = {
 
 static int stack_trace_open(struct inode *inode, struct file *file)
 {
+	int ret;
+
+	ret = security_locked_down(LOCKDOWN_TRACEFS);
+	if (ret)
+		return ret;
+
 	return seq_open(file, &stack_trace_seq_ops);
 }
 
@@ -487,6 +494,7 @@ stack_trace_filter_open(struct inode *inode, struct file *file)
 {
 	struct ftrace_ops *ops = inode->i_private;
 
+	/* Checks for tracefs lockdown */
 	return ftrace_regex_open(ops, FTRACE_ITER_FILTER,
 				 inode, file);
 }

commit 58fe7a87db51ea00596187765dabfc2c4ea2b436
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Aug 7 12:27:30 2019 -0400

    tracing: Document the stack trace algorithm in the comments
    
    As the max stack tracer algorithm is not that easy to understand from the
    code, add comments that explain the algorithm and mentions how
    ARCH_FTRACE_SHIFT_STACK_TRACER affects it.
    
    Link: http://lkml.kernel.org/r/20190806123455.487ac02b@gandalf.local.home
    
    Suggested-by: Joel Fernandes <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 642a850af81a..ec9a34a97129 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -53,6 +53,104 @@ static void print_max_stack(void)
 	}
 }
 
+/*
+ * The stack tracer looks for a maximum stack at each call from a function. It
+ * registers a callback from ftrace, and in that callback it examines the stack
+ * size. It determines the stack size from the variable passed in, which is the
+ * address of a local variable in the stack_trace_call() callback function.
+ * The stack size is calculated by the address of the local variable to the top
+ * of the current stack. If that size is smaller than the currently saved max
+ * stack size, nothing more is done.
+ *
+ * If the size of the stack is greater than the maximum recorded size, then the
+ * following algorithm takes place.
+ *
+ * For architectures (like x86) that store the function's return address before
+ * saving the function's local variables, the stack will look something like
+ * this:
+ *
+ *   [ top of stack ]
+ *    0: sys call entry frame
+ *   10: return addr to entry code
+ *   11: start of sys_foo frame
+ *   20: return addr to sys_foo
+ *   21: start of kernel_func_bar frame
+ *   30: return addr to kernel_func_bar
+ *   31: [ do trace stack here ]
+ *
+ * The save_stack_trace() is called returning all the functions it finds in the
+ * current stack. Which would be (from the bottom of the stack to the top):
+ *
+ *   return addr to kernel_func_bar
+ *   return addr to sys_foo
+ *   return addr to entry code
+ *
+ * Now to figure out how much each of these functions' local variable size is,
+ * a search of the stack is made to find these values. When a match is made, it
+ * is added to the stack_dump_trace[] array. The offset into the stack is saved
+ * in the stack_trace_index[] array. The above example would show:
+ *
+ *        stack_dump_trace[]        |   stack_trace_index[]
+ *        ------------------        +   -------------------
+ *  return addr to kernel_func_bar  |          30
+ *  return addr to sys_foo          |          20
+ *  return addr to entry            |          10
+ *
+ * The print_max_stack() function above, uses these values to print the size of
+ * each function's portion of the stack.
+ *
+ *  for (i = 0; i < nr_entries; i++) {
+ *     size = i == nr_entries - 1 ? stack_trace_index[i] :
+ *                    stack_trace_index[i] - stack_trace_index[i+1]
+ *     print "%d %d %d %s\n", i, stack_trace_index[i], size, stack_dump_trace[i]);
+ *  }
+ *
+ * The above shows
+ *
+ *     depth size  location
+ *     ----- ----  --------
+ *  0    30   10   kernel_func_bar
+ *  1    20   10   sys_foo
+ *  2    10   10   entry code
+ *
+ * Now for architectures that might save the return address after the functions
+ * local variables (saving the link register before calling nested functions),
+ * this will cause the stack to look a little different:
+ *
+ * [ top of stack ]
+ *  0: sys call entry frame
+ * 10: start of sys_foo_frame
+ * 19: return addr to entry code << lr saved before calling kernel_func_bar
+ * 20: start of kernel_func_bar frame
+ * 29: return addr to sys_foo_frame << lr saved before calling next function
+ * 30: [ do trace stack here ]
+ *
+ * Although the functions returned by save_stack_trace() may be the same, the
+ * placement in the stack will be different. Using the same algorithm as above
+ * would yield:
+ *
+ *        stack_dump_trace[]        |   stack_trace_index[]
+ *        ------------------        +   -------------------
+ *  return addr to kernel_func_bar  |          30
+ *  return addr to sys_foo          |          29
+ *  return addr to entry            |          19
+ *
+ * Where the mapping is off by one:
+ *
+ *   kernel_func_bar stack frame size is 29 - 19 not 30 - 29!
+ *
+ * To fix this, if the architecture sets ARCH_RET_ADDR_AFTER_LOCAL_VARS the
+ * values in stack_trace_index[] are shifted by one to and the number of
+ * stack trace entries is decremented by one.
+ *
+ *        stack_dump_trace[]        |   stack_trace_index[]
+ *        ------------------        +   -------------------
+ *  return addr to kernel_func_bar  |          29
+ *  return addr to sys_foo          |          19
+ *
+ * Although the entry function is not displayed, the first function (sys_foo)
+ * will still include the stack size of it.
+ */
 static void check_stack(unsigned long ip, unsigned long *stack)
 {
 	unsigned long this_size, flags; unsigned long *p, *top, *start;

commit f7edb451fa51e44e62177347ea7850aa0e901ea5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Aug 7 11:28:59 2019 -0400

    tracing/arm64: Have max stack tracer handle the case of return address after data
    
    Most archs (well at least x86) store the function call return address on the
    stack before storing the local variables for the function. The max stack
    tracer depends on this in its algorithm to display the stack size of each
    function it finds in the back trace.
    
    Some archs (arm64), may store the return address (from its link register)
    just before calling a nested function. There's no reason to save the link
    register on leaf functions, as it wont be updated. This breaks the algorithm
    of the max stack tracer.
    
    Add a new define ARCH_FTRACE_SHIFT_STACK_TRACER that an architecture may set
    if it stores the return address (link register) after it stores the
    function's local variables, and have the stack trace shift the values of the
    mapped stack size to the appropriate functions.
    
    Link: 20190802094103.163576-1-jiping.ma2@windriver.com
    
    Reported-by: Jiping Ma <jiping.ma2@windriver.com>
    Acked-by: Will Deacon <will@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 5d16f73898db..642a850af81a 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -158,6 +158,20 @@ static void check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
+#ifdef ARCH_FTRACE_SHIFT_STACK_TRACER
+	/*
+	 * Some archs will store the link register before calling
+	 * nested functions. This means the saved return address
+	 * comes after the local storage, and we need to shift
+	 * for that.
+	 */
+	if (x > 1) {
+		memmove(&stack_trace_index[0], &stack_trace_index[1],
+			sizeof(stack_trace_index[0]) * (x - 1));
+		x--;
+	}
+#endif
+
 	stack_trace_nr_entries = x;
 
 	if (task_stack_end_corrupted(current)) {

commit 9f50c91b1195dfffd183d5d8505e45af86623532
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:17 2019 +0200

    tracing: Remove the last struct stack_trace usage
    
    Simplify the stack retrieval code by using the storage array based
    interface.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190425094803.340000461@linutronix.de

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4efda5f75a0f..5d16f73898db 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -23,11 +23,7 @@
 static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES];
 static unsigned stack_trace_index[STACK_TRACE_ENTRIES];
 
-struct stack_trace stack_trace_max = {
-	.max_entries		= STACK_TRACE_ENTRIES,
-	.entries		= &stack_dump_trace[0],
-};
-
+static unsigned int stack_trace_nr_entries;
 static unsigned long stack_trace_max_size;
 static arch_spinlock_t stack_trace_max_lock =
 	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
@@ -44,10 +40,10 @@ static void print_max_stack(void)
 
 	pr_emerg("        Depth    Size   Location    (%d entries)\n"
 			   "        -----    ----   --------\n",
-			   stack_trace_max.nr_entries);
+			   stack_trace_nr_entries);
 
-	for (i = 0; i < stack_trace_max.nr_entries; i++) {
-		if (i + 1 == stack_trace_max.nr_entries)
+	for (i = 0; i < stack_trace_nr_entries; i++) {
+		if (i + 1 == stack_trace_nr_entries)
 			size = stack_trace_index[i];
 		else
 			size = stack_trace_index[i] - stack_trace_index[i+1];
@@ -93,13 +89,12 @@ static void check_stack(unsigned long ip, unsigned long *stack)
 
 	stack_trace_max_size = this_size;
 
-	stack_trace_max.nr_entries = 0;
-	stack_trace_max.skip = 0;
-
-	save_stack_trace(&stack_trace_max);
+	stack_trace_nr_entries = stack_trace_save(stack_dump_trace,
+					       ARRAY_SIZE(stack_dump_trace) - 1,
+					       0);
 
 	/* Skip over the overhead of the stack tracer itself */
-	for (i = 0; i < stack_trace_max.nr_entries; i++) {
+	for (i = 0; i < stack_trace_nr_entries; i++) {
 		if (stack_dump_trace[i] == ip)
 			break;
 	}
@@ -108,7 +103,7 @@ static void check_stack(unsigned long ip, unsigned long *stack)
 	 * Some archs may not have the passed in ip in the dump.
 	 * If that happens, we need to show everything.
 	 */
-	if (i == stack_trace_max.nr_entries)
+	if (i == stack_trace_nr_entries)
 		i = 0;
 
 	/*
@@ -126,13 +121,13 @@ static void check_stack(unsigned long ip, unsigned long *stack)
 	 * loop will only happen once. This code only takes place
 	 * on a new max, so it is far from a fast path.
 	 */
-	while (i < stack_trace_max.nr_entries) {
+	while (i < stack_trace_nr_entries) {
 		int found = 0;
 
 		stack_trace_index[x] = this_size;
 		p = start;
 
-		for (; p < top && i < stack_trace_max.nr_entries; p++) {
+		for (; p < top && i < stack_trace_nr_entries; p++) {
 			/*
 			 * The READ_ONCE_NOCHECK is used to let KASAN know that
 			 * this is not a stack-out-of-bounds error.
@@ -163,7 +158,7 @@ static void check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
-	stack_trace_max.nr_entries = x;
+	stack_trace_nr_entries = x;
 
 	if (task_stack_end_corrupted(current)) {
 		print_max_stack();
@@ -265,7 +260,7 @@ __next(struct seq_file *m, loff_t *pos)
 {
 	long n = *pos - 1;
 
-	if (n >= stack_trace_max.nr_entries)
+	if (n >= stack_trace_nr_entries)
 		return NULL;
 
 	m->private = (void *)n;
@@ -329,7 +324,7 @@ static int t_show(struct seq_file *m, void *v)
 		seq_printf(m, "        Depth    Size   Location"
 			   "    (%d entries)\n"
 			   "        -----    ----   --------\n",
-			   stack_trace_max.nr_entries);
+			   stack_trace_nr_entries);
 
 		if (!stack_tracer_enabled && !stack_trace_max_size)
 			print_disabled(m);
@@ -339,10 +334,10 @@ static int t_show(struct seq_file *m, void *v)
 
 	i = *(long *)v;
 
-	if (i >= stack_trace_max.nr_entries)
+	if (i >= stack_trace_nr_entries)
 		return 0;
 
-	if (i + 1 == stack_trace_max.nr_entries)
+	if (i + 1 == stack_trace_nr_entries)
 		size = stack_trace_index[i];
 	else
 		size = stack_trace_index[i] - stack_trace_index[i+1];

commit 3d9a8072915366b5932beeed97f158f8d4955768
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:44:54 2019 +0200

    tracing: Cleanup stack trace code
    
    - Remove the extra array member of stack_dump_trace[] along with the
      ARRAY_SIZE - 1 initialization for struct stack_trace :: max_entries.
    
      Both are historical leftovers of no value. The stack tracer never exceeds
      the array and there is no extra storage requirement either.
    
    - Make variables which are only used in trace_stack.c static.
    
    - Simplify the enable/disable logic.
    
    - Rename stack_trace_print() as it's using the stack_trace_ namespace. Free
      the name up for stack trace related functions.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190425094801.230654524@linutronix.de

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index c6e54ff25cae..4efda5f75a0f 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -18,30 +18,26 @@
 
 #include "trace.h"
 
-static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES + 1];
-unsigned stack_trace_index[STACK_TRACE_ENTRIES];
+#define STACK_TRACE_ENTRIES 500
+
+static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES];
+static unsigned stack_trace_index[STACK_TRACE_ENTRIES];
 
-/*
- * Reserve one entry for the passed in ip. This will allow
- * us to remove most or all of the stack size overhead
- * added by the stack tracer itself.
- */
 struct stack_trace stack_trace_max = {
-	.max_entries		= STACK_TRACE_ENTRIES - 1,
+	.max_entries		= STACK_TRACE_ENTRIES,
 	.entries		= &stack_dump_trace[0],
 };
 
-unsigned long stack_trace_max_size;
-arch_spinlock_t stack_trace_max_lock =
+static unsigned long stack_trace_max_size;
+static arch_spinlock_t stack_trace_max_lock =
 	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
 
 DEFINE_PER_CPU(int, disable_stack_tracer);
 static DEFINE_MUTEX(stack_sysctl_mutex);
 
 int stack_tracer_enabled;
-static int last_stack_tracer_enabled;
 
-void stack_trace_print(void)
+static void print_max_stack(void)
 {
 	long i;
 	int size;
@@ -61,16 +57,7 @@ void stack_trace_print(void)
 	}
 }
 
-/*
- * When arch-specific code overrides this function, the following
- * data should be filled up, assuming stack_trace_max_lock is held to
- * prevent concurrent updates.
- *     stack_trace_index[]
- *     stack_trace_max
- *     stack_trace_max_size
- */
-void __weak
-check_stack(unsigned long ip, unsigned long *stack)
+static void check_stack(unsigned long ip, unsigned long *stack)
 {
 	unsigned long this_size, flags; unsigned long *p, *top, *start;
 	static int tracer_frame;
@@ -179,7 +166,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 	stack_trace_max.nr_entries = x;
 
 	if (task_stack_end_corrupted(current)) {
-		stack_trace_print();
+		print_max_stack();
 		BUG();
 	}
 
@@ -412,23 +399,21 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 		   void __user *buffer, size_t *lenp,
 		   loff_t *ppos)
 {
+	int was_enabled;
 	int ret;
 
 	mutex_lock(&stack_sysctl_mutex);
+	was_enabled = !!stack_tracer_enabled;
 
 	ret = proc_dointvec(table, write, buffer, lenp, ppos);
 
-	if (ret || !write ||
-	    (last_stack_tracer_enabled == !!stack_tracer_enabled))
+	if (ret || !write || (was_enabled == !!stack_tracer_enabled))
 		goto out;
 
-	last_stack_tracer_enabled = !!stack_tracer_enabled;
-
 	if (stack_tracer_enabled)
 		register_ftrace_function(&trace_ops);
 	else
 		unregister_ftrace_function(&trace_ops);
-
  out:
 	mutex_unlock(&stack_sysctl_mutex);
 	return ret;
@@ -444,7 +429,6 @@ static __init int enable_stacktrace(char *str)
 		strncpy(stack_trace_filter_buf, str + len, COMMAND_LINE_SIZE);
 
 	stack_tracer_enabled = 1;
-	last_stack_tracer_enabled = 1;
 	return 1;
 }
 __setup("stacktrace", enable_stacktrace);

commit 4285f2fcef8001ead0f1c9315ba50302cab68cda
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Apr 10 12:28:10 2019 +0200

    tracing: Remove the ULONG_MAX stack trace hackery
    
    No architecture terminates the stack trace with ULONG_MAX anymore. As the
    code checks the number of entries stored anyway there is no point in
    keeping all that ULONG_MAX magic around.
    
    The histogram code zeroes the storage before saving the stack, so if the
    trace is shorter than the maximum number of entries it can terminate the
    print loop if a zero entry is detected.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexander Potapenko <glider@google.com>
    Link: https://lkml.kernel.org/r/20190410103645.048761764@linutronix.de

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index eec648a0d673..c6e54ff25cae 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -18,8 +18,7 @@
 
 #include "trace.h"
 
-static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES+1] =
-	 { [0 ... (STACK_TRACE_ENTRIES)] = ULONG_MAX };
+static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES + 1];
 unsigned stack_trace_index[STACK_TRACE_ENTRIES];
 
 /*
@@ -52,10 +51,7 @@ void stack_trace_print(void)
 			   stack_trace_max.nr_entries);
 
 	for (i = 0; i < stack_trace_max.nr_entries; i++) {
-		if (stack_dump_trace[i] == ULONG_MAX)
-			break;
-		if (i+1 == stack_trace_max.nr_entries ||
-				stack_dump_trace[i+1] == ULONG_MAX)
+		if (i + 1 == stack_trace_max.nr_entries)
 			size = stack_trace_index[i];
 		else
 			size = stack_trace_index[i] - stack_trace_index[i+1];
@@ -150,8 +146,6 @@ check_stack(unsigned long ip, unsigned long *stack)
 		p = start;
 
 		for (; p < top && i < stack_trace_max.nr_entries; p++) {
-			if (stack_dump_trace[i] == ULONG_MAX)
-				break;
 			/*
 			 * The READ_ONCE_NOCHECK is used to let KASAN know that
 			 * this is not a stack-out-of-bounds error.
@@ -183,8 +177,6 @@ check_stack(unsigned long ip, unsigned long *stack)
 	}
 
 	stack_trace_max.nr_entries = x;
-	for (; x < i; x++)
-		stack_dump_trace[x] = ULONG_MAX;
 
 	if (task_stack_end_corrupted(current)) {
 		stack_trace_print();
@@ -286,7 +278,7 @@ __next(struct seq_file *m, loff_t *pos)
 {
 	long n = *pos - 1;
 
-	if (n >= stack_trace_max.nr_entries || stack_dump_trace[n] == ULONG_MAX)
+	if (n >= stack_trace_max.nr_entries)
 		return NULL;
 
 	m->private = (void *)n;
@@ -360,12 +352,10 @@ static int t_show(struct seq_file *m, void *v)
 
 	i = *(long *)v;
 
-	if (i >= stack_trace_max.nr_entries ||
-	    stack_dump_trace[i] == ULONG_MAX)
+	if (i >= stack_trace_max.nr_entries)
 		return 0;
 
-	if (i+1 == stack_trace_max.nr_entries ||
-	    stack_dump_trace[i+1] == ULONG_MAX)
+	if (i + 1 == stack_trace_max.nr_entries)
 		size = stack_trace_index[i];
 	else
 		size = stack_trace_index[i] - stack_trace_index[i+1];

commit 3d739c1f6156c70eb0548aa288dcfbac9e0bd162
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 21 23:10:26 2018 -0500

    tracing: Use the return of str_has_prefix() to remove open coded numbers
    
    There are several locations that compare constants to the beginning of
    string variables to determine what commands should be done, then the
    constant length is used to index into the string. This is error prone as the
    hard coded numbers have to match the size of the constants. Instead, use the
    len returned from str_has_prefix() and remove the open coded string length
    sizes.
    
    Cc: Joe Perches <joe@perches.com>
    Acked-by: Masami  Hiramatsu <mhiramat@kernel.org> (for trace_probe part)
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 3641f28c343f..eec648a0d673 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -448,8 +448,10 @@ static char stack_trace_filter_buf[COMMAND_LINE_SIZE+1] __initdata;
 
 static __init int enable_stacktrace(char *str)
 {
-	if (str_has_prefix(str, "_filter="))
-		strncpy(stack_trace_filter_buf, str+8, COMMAND_LINE_SIZE);
+	int len;
+
+	if ((len = str_has_prefix(str, "_filter=")))
+		strncpy(stack_trace_filter_buf, str + len, COMMAND_LINE_SIZE);
 
 	stack_tracer_enabled = 1;
 	last_stack_tracer_enabled = 1;

commit b6b2735514bcd70ad1556a33892a636b20ece671
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Dec 20 13:20:07 2018 -0500

    tracing: Use str_has_prefix() instead of using fixed sizes
    
    There are several instances of strncmp(str, "const", 123), where 123 is the
    strlen of the const string to check if "const" is the prefix of str. But
    this can be error prone. Use str_has_prefix() instead.
    
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index e2a153fc1afc..3641f28c343f 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -448,7 +448,7 @@ static char stack_trace_filter_buf[COMMAND_LINE_SIZE+1] __initdata;
 
 static __init int enable_stacktrace(char *str)
 {
-	if (strncmp(str, "_filter=", 8) == 0)
+	if (str_has_prefix(str, "_filter="))
 		strncpy(stack_trace_filter_buf, str+8, COMMAND_LINE_SIZE);
 
 	stack_tracer_enabled = 1;

commit ca16b0fbb05242f18da9d810c07d3882ffed831c
Author: Dan Carpenter <dan.carpenter@oracle.com>
Date:   Wed Jun 20 14:08:00 2018 +0300

    tracing: Have trace_stack nr_entries compare not be so subtle
    
    Dan Carpenter reviewed the trace_stack.c code and figured he found an off by
    one bug.
    
     "From reviewing the code, it seems possible for
      stack_trace_max.nr_entries to be set to .max_entries and in that case we
      would be reading one element beyond the end of the stack_dump_trace[]
      array.  If it's not set to .max_entries then the bug doesn't affect
      runtime."
    
    Although it looks to be the case, it is not. Because we have:
    
     static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES+1] =
             { [0 ... (STACK_TRACE_ENTRIES)] = ULONG_MAX };
    
     struct stack_trace stack_trace_max = {
            .max_entries            = STACK_TRACE_ENTRIES - 1,
            .entries                = &stack_dump_trace[0],
     };
    
    And:
    
            stack_trace_max.nr_entries = x;
            for (; x < i; x++)
                    stack_dump_trace[x] = ULONG_MAX;
    
    Even if nr_entries equals max_entries, indexing with it into the
    stack_dump_trace[] array will not overflow the array. But if it is the case,
    the second part of the conditional that tests stack_dump_trace[nr_entries]
    to ULONG_MAX will always be true.
    
    By applying Dan's patch, it removes the subtle aspect of it and makes the if
    conditional slightly more efficient.
    
    Link: http://lkml.kernel.org/r/20180620110758.crunhd5bfep7zuiz@kili.mountain
    
    Signed-off-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 2b0d1ee3241c..e2a153fc1afc 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -286,7 +286,7 @@ __next(struct seq_file *m, loff_t *pos)
 {
 	long n = *pos - 1;
 
-	if (n > stack_trace_max.nr_entries || stack_dump_trace[n] == ULONG_MAX)
+	if (n >= stack_trace_max.nr_entries || stack_dump_trace[n] == ULONG_MAX)
 		return NULL;
 
 	m->private = (void *)n;

commit a2acce536921bd793bae13fa344fcea157638e72
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Oct 15 23:14:28 2018 -0400

    tracing: Have stack tracer trace full stack
    
    The stack tracer traces every function call checking the current stack (in
    non interrupt context), looking for the deepest stack, and saving it when it
    finds a new max depth. The problem is that it calls save_stack_trace(), and
    with the new ORC unwinder, it can skip too much. As it looks at the ip of
    the function call in the backtrace to find where it should start, it doesn't
    need to skip anything.
    
    The stack trace selftest would fail when the kernel was complied with the
    ORC UNDWINDER enabled. Without skipping functions when doing the stack
    trace, it now passes again.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4237eba4ef20..2b0d1ee3241c 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -111,7 +111,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 	stack_trace_max_size = this_size;
 
 	stack_trace_max.nr_entries = 0;
-	stack_trace_max.skip = 3;
+	stack_trace_max.skip = 0;
 
 	save_stack_trace(&stack_trace_max);
 

commit 0c5a9acc8b4e878e761f735e144d4a7e4477d4e6
Author: Zhengyuan Liu <liuzhengyuan@kylinos.cn>
Date:   Thu Feb 8 09:41:53 2018 +0800

    tracing: Fix the file mode of stack tracer
    
    It looks weird that the stack_trace_filter file can be written by root
    but shows that it does not have write permission by ll command.
    
    Link: http://lkml.kernel.org/r/1518054113-28096-1-git-send-email-liuzhengyuan@kylinos.cn
    
    Signed-off-by: Zhengyuan Liu <liuzhengyuan@kylinos.cn>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 3c7bfc4bf5e9..4237eba4ef20 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -472,7 +472,7 @@ static __init int stack_trace_init(void)
 			NULL, &stack_trace_fops);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-	trace_create_file("stack_trace_filter", 0444, d_tracer,
+	trace_create_file("stack_trace_filter", 0644, d_tracer,
 			  &trace_ops, &stack_trace_filter_fops);
 #endif
 

commit b00d607bb188e187c7b60074d2fa91a6f1985029
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Dec 5 04:41:51 2017 -0500

    tracing: Have stack trace not record if RCU is not watching
    
    The stack tracer records a stack dump whenever it sees a stack usage that is
    more than what it ever saw before. This can happen at any function that is
    being traced. If it happens when the CPU is going idle (or other strange
    locations), RCU may not be watching, and in this case, the recording of the
    stack trace will trigger a warning. There's been lots of efforts to make
    hacks to allow stack tracing to proceed even if RCU is not watching, but
    this only causes more issues to appear. Simply do not trace a stack if RCU
    is not watching. It probably isn't a bad stack anyway.
    
    Acked-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 734accc02418..3c7bfc4bf5e9 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -209,6 +209,10 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	if (__this_cpu_read(disable_stack_tracer) != 1)
 		goto out;
 
+	/* If rcu is not watching, then save stack trace can fail */
+	if (!rcu_is_watching())
+		goto out;
+
 	ip += MCOUNT_INSN_SIZE;
 
 	check_stack(ip, &stack);

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 49cb41412eec..719a52a4064a 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>
  *

commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 23 14:07:29 2017 -0700

    locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()
    
    Please do not apply this to mainline directly, instead please re-run the
    coccinelle script shown below and apply its output.
    
    For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
    preference to ACCESS_ONCE(), and new code is expected to use one of the
    former. So far, there's been no reason to change most existing uses of
    ACCESS_ONCE(), as these aren't harmful, and changing them results in
    churn.
    
    However, for some features, the read/write distinction is critical to
    correct operation. To distinguish these cases, separate read/write
    accessors must be used. This patch migrates (most) remaining
    ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
    coccinelle script:
    
    ----
    // Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
    // WRITE_ONCE()
    
    // $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch
    
    virtual patch
    
    @ depends on patch @
    expression E1, E2;
    @@
    
    - ACCESS_ONCE(E1) = E2
    + WRITE_ONCE(E1, E2)
    
    @ depends on patch @
    expression E;
    @@
    
    - ACCESS_ONCE(E)
    + READ_ONCE(E)
    ----
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: davem@davemloft.net
    Cc: linux-arch@vger.kernel.org
    Cc: mpe@ellerman.id.au
    Cc: shuah@kernel.org
    Cc: snitzer@redhat.com
    Cc: thor.thayer@linux.intel.com
    Cc: tj@kernel.org
    Cc: viro@zeniv.linux.org.uk
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 49cb41412eec..780262210c9a 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -77,7 +77,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 {
 	unsigned long this_size, flags; unsigned long *p, *top, *start;
 	static int tracer_frame;
-	int frame_size = ACCESS_ONCE(tracer_frame);
+	int frame_size = READ_ONCE(tracer_frame);
 	int i, x;
 
 	this_size = ((unsigned long)stack) & (THREAD_SIZE-1);

commit 15516c89acce948debc4c598e03c3fee53045797
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Sep 21 13:00:21 2017 -0400

    tracing: Remove RCU work arounds from stack tracer
    
    Currently the stack tracer calls rcu_irq_enter() to make sure RCU
    is watching when it records a stack trace. But if the stack tracer
    is triggered while tracing inside of a rcu_irq_enter(), calling
    rcu_irq_enter() unconditionally can be problematic.
    
    The reason for having rcu_irq_enter() in the first place has been
    fixed from within the saving of the stack trace code, and there's no
    reason for doing it in the stack tracer itself. Just remove it.
    
    Cc: stable@vger.kernel.org
    Fixes: 0be964be0 ("module: Sanitize RCU usage and locking")
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Suggested-by: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index a4df67cbc711..49cb41412eec 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -96,23 +96,9 @@ check_stack(unsigned long ip, unsigned long *stack)
 	if (in_nmi())
 		return;
 
-	/*
-	 * There's a slight chance that we are tracing inside the
-	 * RCU infrastructure, and rcu_irq_enter() will not work
-	 * as expected.
-	 */
-	if (unlikely(rcu_irq_enter_disabled()))
-		return;
-
 	local_irq_save(flags);
 	arch_spin_lock(&stack_trace_max_lock);
 
-	/*
-	 * RCU may not be watching, make it see us.
-	 * The stack trace code uses rcu_sched.
-	 */
-	rcu_irq_enter();
-
 	/* In case another CPU set the tracer_frame on us */
 	if (unlikely(!frame_size))
 		this_size -= tracer_frame;
@@ -205,7 +191,6 @@ check_stack(unsigned long ip, unsigned long *stack)
 	}
 
  out:
-	rcu_irq_exit();
 	arch_spin_unlock(&stack_trace_max_lock);
 	local_irq_restore(flags);
 }

commit bbd1d27d863d5c0acee65ecd0c2e34035e1df5ea
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Jul 11 19:21:04 2017 -0400

    tracing: Do note expose stack_trace_filter without DYNAMIC_FTRACE
    
    The "stack_trace_filter" file only makes sense if DYNAMIC_FTRACE is
    configured in. If it is not, then the user can not filter any functions.
    
    Not only that, the open function causes warnings when DYNAMIC_FTRACE is not
    set.
    
    Link: http://lkml.kernel.org/r/20170710110521.600806-1-arnd@arndb.de
    
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index b4a751e8f9d6..a4df67cbc711 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -406,6 +406,8 @@ static const struct file_operations stack_trace_fops = {
 	.release	= seq_release,
 };
 
+#ifdef CONFIG_DYNAMIC_FTRACE
+
 static int
 stack_trace_filter_open(struct inode *inode, struct file *file)
 {
@@ -423,6 +425,8 @@ static const struct file_operations stack_trace_filter_fops = {
 	.release = ftrace_regex_release,
 };
 
+#endif /* CONFIG_DYNAMIC_FTRACE */
+
 int
 stack_trace_sysctl(struct ctl_table *table, int write,
 		   void __user *buffer, size_t *lenp,
@@ -477,8 +481,10 @@ static __init int stack_trace_init(void)
 	trace_create_file("stack_trace", 0444, d_tracer,
 			NULL, &stack_trace_fops);
 
+#ifdef CONFIG_DYNAMIC_FTRACE
 	trace_create_file("stack_trace_filter", 0444, d_tracer,
 			  &trace_ops, &stack_trace_filter_fops);
+#endif
 
 	if (stack_trace_filter_buf[0])
 		ftrace_set_early_filter(&trace_ops, stack_trace_filter_buf, 1);

commit 0f17976568b3f72e676450af0c0db6f8752253d6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 29 10:05:45 2017 -0400

    ftrace: Fix regression with module command in stack_trace_filter
    
    When doing the following command:
    
     # echo ":mod:kvm_intel" > /sys/kernel/tracing/stack_trace_filter
    
    it triggered a crash.
    
    This happened with the clean up of probes. It required all callers to the
    regex function (doing ftrace filtering) to have ops->private be a pointer to
    a trace_array. But for the stack tracer, that is not the case.
    
    Allow for the ops->private to be NULL, and change the function command
    callbacks to handle the trace_array pointer being NULL as well.
    
    Fixes: d2afd57a4b96 ("tracing/ftrace: Allow instances to have their own function probes")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 76aa04d4c925..b4a751e8f9d6 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -409,7 +409,9 @@ static const struct file_operations stack_trace_fops = {
 static int
 stack_trace_filter_open(struct inode *inode, struct file *file)
 {
-	return ftrace_regex_open(&trace_ops, FTRACE_ITER_FILTER,
+	struct ftrace_ops *ops = inode->i_private;
+
+	return ftrace_regex_open(ops, FTRACE_ITER_FILTER,
 				 inode, file);
 }
 
@@ -476,7 +478,7 @@ static __init int stack_trace_init(void)
 			NULL, &stack_trace_fops);
 
 	trace_create_file("stack_trace_filter", 0444, d_tracer,
-			NULL, &stack_trace_filter_fops);
+			  &trace_ops, &stack_trace_filter_fops);
 
 	if (stack_trace_filter_buf[0])
 		ftrace_set_early_filter(&trace_ops, stack_trace_filter_buf, 1);

commit 03ecd3f48e57f2e6154584e0ee7450d7a05e2d3b
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Apr 7 12:20:36 2017 -0400

    rcu/tracing: Add rcu_disabled to denote when rcu_irq_enter() will not work
    
    Tracing uses rcu_irq_enter() as a way to make sure that RCU is watching when
    it needs to use rcu_read_lock() and friends. This is because tracing can
    happen as RCU is about to enter user space, or about to go idle, and RCU
    does not watch for RCU read side critical sections as it makes the
    transition.
    
    There is a small location within the RCU infrastructure that rcu_irq_enter()
    itself will not work. If tracing were to occur in that section it will break
    if it tries to use rcu_irq_enter().
    
    Originally, this happens with the stack_tracer, because it will call
    save_stack_trace when it encounters stack usage that is greater than any
    stack usage it had encountered previously. There was a case where that
    happened in the RCU section where rcu_irq_enter() did not work, and lockdep
    complained loudly about it. To fix it, stack tracing added a call to be
    disabled and RCU would disable stack tracing during the critical section
    that rcu_irq_enter() was inoperable. This solution worked, but there are
    other cases that use rcu_irq_enter() and it would be a good idea to let RCU
    give a way to let others know that rcu_irq_enter() will not work. For
    example, in trace events.
    
    Another helpful aspect of this change is that it also moves the per cpu
    variable called in the RCU critical section into a cache locale along with
    other RCU per cpu variables used in that same location.
    
    I'm keeping the stack_trace_disable() code, as that still could be used in
    the future by places that really need to disable it. And since it's only a
    static inline, it wont take up any kernel text if it is not used.
    
    Link: http://lkml.kernel.org/r/20170405093207.404f8deb@gandalf.local.home
    
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index f2f02ff350d4..76aa04d4c925 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -96,6 +96,14 @@ check_stack(unsigned long ip, unsigned long *stack)
 	if (in_nmi())
 		return;
 
+	/*
+	 * There's a slight chance that we are tracing inside the
+	 * RCU infrastructure, and rcu_irq_enter() will not work
+	 * as expected.
+	 */
+	if (unlikely(rcu_irq_enter_disabled()))
+		return;
+
 	local_irq_save(flags);
 	arch_spin_lock(&stack_trace_max_lock);
 

commit 8aaf1ee70e19ac74cbbb81098edfa328d1ab4bd7
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 6 15:47:32 2017 -0400

    tracing: Rename trace_active to disable_stack_tracer and inline its modification
    
    In order to eliminate a function call, make "trace_active" into
    "disable_stack_tracer" and convert stack_tracer_disable() and friends into
    static inline functions.
    
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 21e536cf66e4..f2f02ff350d4 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -35,44 +35,12 @@ unsigned long stack_trace_max_size;
 arch_spinlock_t stack_trace_max_lock =
 	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
 
-static DEFINE_PER_CPU(int, trace_active);
+DEFINE_PER_CPU(int, disable_stack_tracer);
 static DEFINE_MUTEX(stack_sysctl_mutex);
 
 int stack_tracer_enabled;
 static int last_stack_tracer_enabled;
 
-/**
- * stack_tracer_disable - temporarily disable the stack tracer
- *
- * There's a few locations (namely in RCU) where stack tracing
- * cannot be executed. This function is used to disable stack
- * tracing during those critical sections.
- *
- * This function must be called with preemption or interrupts
- * disabled and stack_tracer_enable() must be called shortly after
- * while preemption or interrupts are still disabled.
- */
-void stack_tracer_disable(void)
-{
-	/* Preemption or interupts must be disabled */
-	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
-		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
-	this_cpu_inc(trace_active);
-}
-
-/**
- * stack_tracer_enable - re-enable the stack tracer
- *
- * After stack_tracer_disable() is called, stack_tracer_enable()
- * must be called shortly afterward.
- */
-void stack_tracer_enable(void)
-{
-	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
-		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
-	this_cpu_dec(trace_active);
-}
-
 void stack_trace_print(void)
 {
 	long i;
@@ -243,8 +211,8 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	preempt_disable_notrace();
 
 	/* no atomic needed, we only modify this variable by this cpu */
-	__this_cpu_inc(trace_active);
-	if (__this_cpu_read(trace_active) != 1)
+	__this_cpu_inc(disable_stack_tracer);
+	if (__this_cpu_read(disable_stack_tracer) != 1)
 		goto out;
 
 	ip += MCOUNT_INSN_SIZE;
@@ -252,7 +220,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	check_stack(ip, &stack);
 
  out:
-	__this_cpu_dec(trace_active);
+	__this_cpu_dec(disable_stack_tracer);
 	/* prevent recursion in schedule */
 	preempt_enable_notrace();
 }
@@ -294,15 +262,15 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	/*
 	 * In case we trace inside arch_spin_lock() or after (NMI),
 	 * we will cause circular lock, so we also need to increase
-	 * the percpu trace_active here.
+	 * the percpu disable_stack_tracer here.
 	 */
-	__this_cpu_inc(trace_active);
+	__this_cpu_inc(disable_stack_tracer);
 
 	arch_spin_lock(&stack_trace_max_lock);
 	*ptr = val;
 	arch_spin_unlock(&stack_trace_max_lock);
 
-	__this_cpu_dec(trace_active);
+	__this_cpu_dec(disable_stack_tracer);
 	local_irq_restore(flags);
 
 	return count;
@@ -338,7 +306,7 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 {
 	local_irq_disable();
 
-	__this_cpu_inc(trace_active);
+	__this_cpu_inc(disable_stack_tracer);
 
 	arch_spin_lock(&stack_trace_max_lock);
 
@@ -352,7 +320,7 @@ static void t_stop(struct seq_file *m, void *p)
 {
 	arch_spin_unlock(&stack_trace_max_lock);
 
-	__this_cpu_dec(trace_active);
+	__this_cpu_dec(disable_stack_tracer);
 
 	local_irq_enable();
 }

commit 5367278cb7ba74537bcad1470d75f30d95b09c14
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 6 12:26:20 2017 -0400

    tracing: Add stack_tracer_disable/enable() functions
    
    There are certain parts of the kernel that cannot let stack tracing
    proceed (namely in RCU), because the stack tracer uses RCU, and parts of RCU
    internals cannot handle having RCU read side locks taken.
    
    Add stack_tracer_disable() and stack_tracer_enable() functions to let RCU
    stop stack tracing on the current CPU when it is in those critical sections.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 338d076a06da..21e536cf66e4 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -41,6 +41,38 @@ static DEFINE_MUTEX(stack_sysctl_mutex);
 int stack_tracer_enabled;
 static int last_stack_tracer_enabled;
 
+/**
+ * stack_tracer_disable - temporarily disable the stack tracer
+ *
+ * There's a few locations (namely in RCU) where stack tracing
+ * cannot be executed. This function is used to disable stack
+ * tracing during those critical sections.
+ *
+ * This function must be called with preemption or interrupts
+ * disabled and stack_tracer_enable() must be called shortly after
+ * while preemption or interrupts are still disabled.
+ */
+void stack_tracer_disable(void)
+{
+	/* Preemption or interupts must be disabled */
+	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
+		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
+	this_cpu_inc(trace_active);
+}
+
+/**
+ * stack_tracer_enable - re-enable the stack tracer
+ *
+ * After stack_tracer_disable() is called, stack_tracer_enable()
+ * must be called shortly afterward.
+ */
+void stack_tracer_enable(void)
+{
+	if (IS_ENABLED(CONFIG_PREEMPT_DEBUG))
+		WARN_ON_ONCE(!preempt_count() || !irqs_disabled());
+	this_cpu_dec(trace_active);
+}
+
 void stack_trace_print(void)
 {
 	long i;

commit 252babcd52aabe37aaad03685e7d6ad454edb9f9
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 6 12:11:36 2017 -0400

    tracing: Replace the per_cpu() with __this_cpu*() in trace_stack.c
    
    The updates to the trace_active per cpu variable can be updated with the
    __this_cpu_*() functions as it only gets updated on the CPU that the variable
    is on.
    
    Thanks to Paul McKenney for suggesting __this_cpu_* instead of this_cpu_*.
    
    Acked-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 5fb1f2c87e6b..338d076a06da 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -207,13 +207,12 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 		 struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
 	unsigned long stack;
-	int cpu;
 
 	preempt_disable_notrace();
 
-	cpu = raw_smp_processor_id();
 	/* no atomic needed, we only modify this variable by this cpu */
-	if (per_cpu(trace_active, cpu)++ != 0)
+	__this_cpu_inc(trace_active);
+	if (__this_cpu_read(trace_active) != 1)
 		goto out;
 
 	ip += MCOUNT_INSN_SIZE;
@@ -221,7 +220,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	check_stack(ip, &stack);
 
  out:
-	per_cpu(trace_active, cpu)--;
+	__this_cpu_dec(trace_active);
 	/* prevent recursion in schedule */
 	preempt_enable_notrace();
 }
@@ -253,7 +252,6 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	long *ptr = filp->private_data;
 	unsigned long val, flags;
 	int ret;
-	int cpu;
 
 	ret = kstrtoul_from_user(ubuf, count, 10, &val);
 	if (ret)
@@ -266,14 +264,13 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	 * we will cause circular lock, so we also need to increase
 	 * the percpu trace_active here.
 	 */
-	cpu = smp_processor_id();
-	per_cpu(trace_active, cpu)++;
+	__this_cpu_inc(trace_active);
 
 	arch_spin_lock(&stack_trace_max_lock);
 	*ptr = val;
 	arch_spin_unlock(&stack_trace_max_lock);
 
-	per_cpu(trace_active, cpu)--;
+	__this_cpu_dec(trace_active);
 	local_irq_restore(flags);
 
 	return count;
@@ -307,12 +304,9 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
-	int cpu;
-
 	local_irq_disable();
 
-	cpu = smp_processor_id();
-	per_cpu(trace_active, cpu)++;
+	__this_cpu_inc(trace_active);
 
 	arch_spin_lock(&stack_trace_max_lock);
 
@@ -324,12 +318,9 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 
 static void t_stop(struct seq_file *m, void *p)
 {
-	int cpu;
-
 	arch_spin_unlock(&stack_trace_max_lock);
 
-	cpu = smp_processor_id();
-	per_cpu(trace_active, cpu)--;
+	__this_cpu_dec(trace_active);
 
 	local_irq_enable();
 }

commit 505d3085d7120a9f4cd0d6ffaa876968854b3baa
Author: Masahiro Yamada <yamada.masahiro@socionext.com>
Date:   Thu Mar 9 16:16:33 2017 -0800

    scripts/spelling.txt: add "overide" pattern and fix typo instances
    
    Fix typos and add the following to the scripts/spelling.txt:
    
      overide||override
    
    While we are here, fix the doubled "address" in the touched line
    Documentation/devicetree/bindings/regulator/ti-abb-regulator.txt.
    
    Also, fix the comment block style in the touched hunks in
    drivers/media/dvb-frontends/drx39xyj/drx_driver.h.
    
    Link: http://lkml.kernel.org/r/1481573103-11329-21-git-send-email-yamada.masahiro@socionext.com
    Signed-off-by: Masahiro Yamada <yamada.masahiro@socionext.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 1d68b5b7ad41..5fb1f2c87e6b 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -65,7 +65,7 @@ void stack_trace_print(void)
 }
 
 /*
- * When arch-specific code overides this function, the following
+ * When arch-specific code overrides this function, the following
  * data should be filled up, assuming stack_trace_max_lock is held to
  * prevent concurrent updates.
  *     stack_trace_index[]

commit 68db0cf10678630d286f4bbbbdfa102951a35faa
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 8 18:51:37 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <linux/sched/task_stack.h>
    
    We are going to split <linux/sched/task_stack.h> out of <linux/sched.h>, which
    will have to be picked up from other headers and a couple of .c files.
    
    Create a trivial placeholder <linux/sched/task_stack.h> file that just
    maps to <linux/sched.h> to make this patch obviously correct and
    bisectable.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 2a1abbaca10e..1d68b5b7ad41 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -2,6 +2,7 @@
  * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>
  *
  */
+#include <linux/sched/task_stack.h>
 #include <linux/stacktrace.h>
 #include <linux/kallsyms.h>
 #include <linux/seq_file.h>

commit 6e22c8366416251a3d88ba6c92d13d595089f0ed
Author: Yang Shi <yang.shi@linaro.org>
Date:   Fri Feb 12 12:46:00 2016 -0800

    tracing, kasan: Silence Kasan warning in check_stack of stack_tracer
    
    When enabling stack trace via "echo 1 > /proc/sys/kernel/stack_tracer_enabled",
    the below KASAN warning is triggered:
    
    BUG: KASAN: stack-out-of-bounds in check_stack+0x344/0x848 at addr ffffffc0689ebab8
    Read of size 8 by task ksoftirqd/4/29
    page:ffffffbdc3a27ac0 count:0 mapcount:0 mapping:          (null) index:0x0
    flags: 0x0()
    page dumped because: kasan: bad access detected
    CPU: 4 PID: 29 Comm: ksoftirqd/4 Not tainted 4.5.0-rc1 #129
    Hardware name: Freescale Layerscape 2085a RDB Board (DT)
    Call trace:
    [<ffffffc000091300>] dump_backtrace+0x0/0x3a0
    [<ffffffc0000916c4>] show_stack+0x24/0x30
    [<ffffffc0009bbd78>] dump_stack+0xd8/0x168
    [<ffffffc000420bb0>] kasan_report_error+0x6a0/0x920
    [<ffffffc000421688>] kasan_report+0x70/0xb8
    [<ffffffc00041f7f0>] __asan_load8+0x60/0x78
    [<ffffffc0002e05c4>] check_stack+0x344/0x848
    [<ffffffc0002e0c8c>] stack_trace_call+0x1c4/0x370
    [<ffffffc0002af558>] ftrace_ops_no_ops+0x2c0/0x590
    [<ffffffc00009f25c>] ftrace_graph_call+0x0/0x14
    [<ffffffc0000881bc>] fpsimd_thread_switch+0x24/0x1e8
    [<ffffffc000089864>] __switch_to+0x34/0x218
    [<ffffffc0011e089c>] __schedule+0x3ac/0x15b8
    [<ffffffc0011e1f6c>] schedule+0x5c/0x178
    [<ffffffc0001632a8>] smpboot_thread_fn+0x350/0x960
    [<ffffffc00015b518>] kthread+0x1d8/0x2b0
    [<ffffffc0000874d0>] ret_from_fork+0x10/0x40
    Memory state around the buggy address:
     ffffffc0689eb980: 00 00 00 00 00 00 00 00 f1 f1 f1 f1 00 f4 f4 f4
     ffffffc0689eba00: f3 f3 f3 f3 00 00 00 00 00 00 00 00 00 00 00 00
    >ffffffc0689eba80: 00 00 f1 f1 f1 f1 00 f4 f4 f4 f3 f3 f3 f3 00 00
                                            ^
     ffffffc0689ebb00: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
     ffffffc0689ebb80: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
    
    The stacker tracer traverses the whole kernel stack when saving the max stack
    trace. It may touch the stack red zones to cause the warning. So, just disable
    the instrumentation to silence the warning.
    
    Link: http://lkml.kernel.org/r/1455309960-18930-1-git-send-email-yang.shi@linaro.org
    
    Signed-off-by: Yang Shi <yang.shi@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 202df6cffcca..2a1abbaca10e 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -156,7 +156,11 @@ check_stack(unsigned long ip, unsigned long *stack)
 		for (; p < top && i < stack_trace_max.nr_entries; p++) {
 			if (stack_dump_trace[i] == ULONG_MAX)
 				break;
-			if (*p == stack_dump_trace[i]) {
+			/*
+			 * The READ_ONCE_NOCHECK is used to let KASAN know that
+			 * this is not a stack-out-of-bounds error.
+			 */
+			if ((READ_ONCE_NOCHECK(*p)) == stack_dump_trace[i]) {
 				stack_dump_trace[x] = stack_dump_trace[i++];
 				this_size = stack_trace_index[x++] =
 					(top - p) * sizeof(unsigned long);

commit 6ccd83714a009ee301b50c15f6c3a5dc1f30164c
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Jan 29 10:22:41 2016 -0500

    tracing/stacktrace: Show entire trace if passed in function not found
    
    When a max stack trace is discovered, the stack dump is saved. In order to
    not record the overhead of the stack tracer, the ip of the traced function
    is looked for within the dump. The trace is started from the location of
    that function. But if for some reason the ip is not found, the entire stack
    trace is then truncated. That's not very useful. Instead, print everything
    if the ip of the traced function is not found within the trace.
    
    This issue showed up on s390.
    
    Link: http://lkml.kernel.org/r/20160129102241.1b3c9c04@gandalf.local.home
    
    Fixes: 72ac426a5bb0 ("tracing: Clean up stack tracing and fix fentry updates")
    Cc: stable@vger.kernel.org # v4.3+
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index dda9e6742950..202df6cffcca 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -125,6 +125,13 @@ check_stack(unsigned long ip, unsigned long *stack)
 			break;
 	}
 
+	/*
+	 * Some archs may not have the passed in ip in the dump.
+	 * If that happens, we need to show everything.
+	 */
+	if (i == stack_trace_max.nr_entries)
+		i = 0;
+
 	/*
 	 * Now find where in the stack these are.
 	 */

commit 22402cd0af685c1a5d067c87db3051db7fff7709
Merge: 7c623cac4939 d227c3ae4e94
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 6 13:30:20 2015 -0800

    Merge tag 'trace-v4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracking updates from Steven Rostedt:
     "Most of the changes are clean ups and small fixes.  Some of them have
      stable tags to them.  I searched through my INBOX just as the merge
      window opened and found lots of patches to pull.  I ran them through
      all my tests and they were in linux-next for a few days.
    
      Features added this release:
      ----------------------------
    
       - Module globbing.  You can now filter function tracing to several
         modules.  # echo '*:mod:*snd*' > set_ftrace_filter (Dmitry Safonov)
    
       - Tracer specific options are now visible even when the tracer is not
         active.  It was rather annoying that you can only see and modify
         tracer options after enabling the tracer.  Now they are in the
         options/ directory even when the tracer is not active.  Although
         they are still only visible when the tracer is active in the
         trace_options file.
    
       - Trace options are now per instance (although some of the tracer
         specific options are global)
    
       - New tracefs file: set_event_pid.  If any pid is added to this file,
         then all events in the instance will filter out events that are not
         part of this pid.  sched_switch and sched_wakeup events handle next
         and the wakee pids"
    
    * tag 'trace-v4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (68 commits)
      tracefs: Fix refcount imbalance in start_creating()
      tracing: Put back comma for empty fields in boot string parsing
      tracing: Apply tracer specific options from kernel command line.
      tracing: Add some documentation about set_event_pid
      ring_buffer: Remove unneeded smp_wmb() before wakeup of reader benchmark
      tracing: Allow dumping traces without tracking trace started cpus
      ring_buffer: Fix more races when terminating the producer in the benchmark
      ring_buffer: Do no not complete benchmark reader too early
      tracing: Remove redundant TP_ARGS redefining
      tracing: Rename max_stack_lock to stack_trace_max_lock
      tracing: Allow arch-specific stack tracer
      recordmcount: arm64: Replace the ignored mcount call into nop
      recordmcount: Fix endianness handling bug for nop_mcount
      tracepoints: Fix documentation of RCU lockdep checks
      tracing: ftrace_event_is_function() can return boolean
      tracing: is_legal_op() can return boolean
      ring-buffer: rb_event_is_commit() can return boolean
      ring-buffer: rb_per_cpu_empty() can return boolean
      ring_buffer: ring_buffer_empty{cpu}() can return boolean
      ring-buffer: rb_is_reader_page() can return boolean
      ...

commit d332736df0c277905de06311ae084e2c76580a3f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 3 14:50:15 2015 -0500

    tracing: Rename max_stack_lock to stack_trace_max_lock
    
    Now that max_stack_lock is a global variable, it requires a naming
    convention that is unlikely to collide. Rename it to the same naming
    convention that the other stack_trace variables have.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 50945a7939f4..0bd212af406c 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -31,7 +31,7 @@ struct stack_trace stack_trace_max = {
 };
 
 unsigned long stack_trace_max_size;
-arch_spinlock_t max_stack_lock =
+arch_spinlock_t stack_trace_max_lock =
 	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
 
 static DEFINE_PER_CPU(int, trace_active);
@@ -65,7 +65,7 @@ void stack_trace_print(void)
 
 /*
  * When arch-specific code overides this function, the following
- * data should be filled up, assuming max_stack_lock is held to
+ * data should be filled up, assuming stack_trace_max_lock is held to
  * prevent concurrent updates.
  *     stack_trace_index[]
  *     stack_trace_max
@@ -92,7 +92,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 		return;
 
 	local_irq_save(flags);
-	arch_spin_lock(&max_stack_lock);
+	arch_spin_lock(&stack_trace_max_lock);
 
 	/* In case another CPU set the tracer_frame on us */
 	if (unlikely(!frame_size))
@@ -175,7 +175,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 	}
 
  out:
-	arch_spin_unlock(&max_stack_lock);
+	arch_spin_unlock(&stack_trace_max_lock);
 	local_irq_restore(flags);
 }
 
@@ -246,9 +246,9 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	cpu = smp_processor_id();
 	per_cpu(trace_active, cpu)++;
 
-	arch_spin_lock(&max_stack_lock);
+	arch_spin_lock(&stack_trace_max_lock);
 	*ptr = val;
-	arch_spin_unlock(&max_stack_lock);
+	arch_spin_unlock(&stack_trace_max_lock);
 
 	per_cpu(trace_active, cpu)--;
 	local_irq_restore(flags);
@@ -291,7 +291,7 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 	cpu = smp_processor_id();
 	per_cpu(trace_active, cpu)++;
 
-	arch_spin_lock(&max_stack_lock);
+	arch_spin_lock(&stack_trace_max_lock);
 
 	if (*pos == 0)
 		return SEQ_START_TOKEN;
@@ -303,7 +303,7 @@ static void t_stop(struct seq_file *m, void *p)
 {
 	int cpu;
 
-	arch_spin_unlock(&max_stack_lock);
+	arch_spin_unlock(&stack_trace_max_lock);
 
 	cpu = smp_processor_id();
 	per_cpu(trace_active, cpu)--;

commit bb99d8ccec7f83a2730a29d1ae7eee5ffa446a9e
Author: AKASHI Takahiro <takahiro.akashi@linaro.org>
Date:   Fri Oct 30 14:25:39 2015 +0900

    tracing: Allow arch-specific stack tracer
    
    A stack frame may be used in a different way depending on cpu architecture.
    Thus it is not always appropriate to slurp the stack contents, as current
    check_stack() does, in order to calcurate a stack index (height) at a given
    function call. At least not on arm64.
    In addition, there is a possibility that we will mistakenly detect a stale
    stack frame which has not been overwritten.
    
    This patch makes check_stack() a weak function so as to later implement
    arch-specific version.
    
    Link: http://lkml.kernel.org/r/1446182741-31019-5-git-send-email-takahiro.akashi@linaro.org
    
    Signed-off-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index b746399ab59c..50945a7939f4 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -16,24 +16,22 @@
 
 #include "trace.h"
 
-#define STACK_TRACE_ENTRIES 500
-
 static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES+1] =
 	 { [0 ... (STACK_TRACE_ENTRIES)] = ULONG_MAX };
-static unsigned stack_dump_index[STACK_TRACE_ENTRIES];
+unsigned stack_trace_index[STACK_TRACE_ENTRIES];
 
 /*
  * Reserve one entry for the passed in ip. This will allow
  * us to remove most or all of the stack size overhead
  * added by the stack tracer itself.
  */
-static struct stack_trace max_stack_trace = {
+struct stack_trace stack_trace_max = {
 	.max_entries		= STACK_TRACE_ENTRIES - 1,
 	.entries		= &stack_dump_trace[0],
 };
 
-static unsigned long max_stack_size;
-static arch_spinlock_t max_stack_lock =
+unsigned long stack_trace_max_size;
+arch_spinlock_t max_stack_lock =
 	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
 
 static DEFINE_PER_CPU(int, trace_active);
@@ -42,30 +40,38 @@ static DEFINE_MUTEX(stack_sysctl_mutex);
 int stack_tracer_enabled;
 static int last_stack_tracer_enabled;
 
-static inline void print_max_stack(void)
+void stack_trace_print(void)
 {
 	long i;
 	int size;
 
 	pr_emerg("        Depth    Size   Location    (%d entries)\n"
 			   "        -----    ----   --------\n",
-			   max_stack_trace.nr_entries);
+			   stack_trace_max.nr_entries);
 
-	for (i = 0; i < max_stack_trace.nr_entries; i++) {
+	for (i = 0; i < stack_trace_max.nr_entries; i++) {
 		if (stack_dump_trace[i] == ULONG_MAX)
 			break;
-		if (i+1 == max_stack_trace.nr_entries ||
+		if (i+1 == stack_trace_max.nr_entries ||
 				stack_dump_trace[i+1] == ULONG_MAX)
-			size = stack_dump_index[i];
+			size = stack_trace_index[i];
 		else
-			size = stack_dump_index[i] - stack_dump_index[i+1];
+			size = stack_trace_index[i] - stack_trace_index[i+1];
 
-		pr_emerg("%3ld) %8d   %5d   %pS\n", i, stack_dump_index[i],
+		pr_emerg("%3ld) %8d   %5d   %pS\n", i, stack_trace_index[i],
 				size, (void *)stack_dump_trace[i]);
 	}
 }
 
-static inline void
+/*
+ * When arch-specific code overides this function, the following
+ * data should be filled up, assuming max_stack_lock is held to
+ * prevent concurrent updates.
+ *     stack_trace_index[]
+ *     stack_trace_max
+ *     stack_trace_max_size
+ */
+void __weak
 check_stack(unsigned long ip, unsigned long *stack)
 {
 	unsigned long this_size, flags; unsigned long *p, *top, *start;
@@ -78,7 +84,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 	/* Remove the frame of the tracer */
 	this_size -= frame_size;
 
-	if (this_size <= max_stack_size)
+	if (this_size <= stack_trace_max_size)
 		return;
 
 	/* we do not handle interrupt stacks yet */
@@ -93,18 +99,18 @@ check_stack(unsigned long ip, unsigned long *stack)
 		this_size -= tracer_frame;
 
 	/* a race could have already updated it */
-	if (this_size <= max_stack_size)
+	if (this_size <= stack_trace_max_size)
 		goto out;
 
-	max_stack_size = this_size;
+	stack_trace_max_size = this_size;
 
-	max_stack_trace.nr_entries = 0;
-	max_stack_trace.skip = 3;
+	stack_trace_max.nr_entries = 0;
+	stack_trace_max.skip = 3;
 
-	save_stack_trace(&max_stack_trace);
+	save_stack_trace(&stack_trace_max);
 
 	/* Skip over the overhead of the stack tracer itself */
-	for (i = 0; i < max_stack_trace.nr_entries; i++) {
+	for (i = 0; i < stack_trace_max.nr_entries; i++) {
 		if (stack_dump_trace[i] == ip)
 			break;
 	}
@@ -124,18 +130,18 @@ check_stack(unsigned long ip, unsigned long *stack)
 	 * loop will only happen once. This code only takes place
 	 * on a new max, so it is far from a fast path.
 	 */
-	while (i < max_stack_trace.nr_entries) {
+	while (i < stack_trace_max.nr_entries) {
 		int found = 0;
 
-		stack_dump_index[x] = this_size;
+		stack_trace_index[x] = this_size;
 		p = start;
 
-		for (; p < top && i < max_stack_trace.nr_entries; p++) {
+		for (; p < top && i < stack_trace_max.nr_entries; p++) {
 			if (stack_dump_trace[i] == ULONG_MAX)
 				break;
 			if (*p == stack_dump_trace[i]) {
 				stack_dump_trace[x] = stack_dump_trace[i++];
-				this_size = stack_dump_index[x++] =
+				this_size = stack_trace_index[x++] =
 					(top - p) * sizeof(unsigned long);
 				found = 1;
 				/* Start the search from here */
@@ -150,7 +156,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 				if (unlikely(!tracer_frame)) {
 					tracer_frame = (p - stack) *
 						sizeof(unsigned long);
-					max_stack_size -= tracer_frame;
+					stack_trace_max_size -= tracer_frame;
 				}
 			}
 		}
@@ -159,12 +165,12 @@ check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
-	max_stack_trace.nr_entries = x;
+	stack_trace_max.nr_entries = x;
 	for (; x < i; x++)
 		stack_dump_trace[x] = ULONG_MAX;
 
 	if (task_stack_end_corrupted(current)) {
-		print_max_stack();
+		stack_trace_print();
 		BUG();
 	}
 
@@ -262,7 +268,7 @@ __next(struct seq_file *m, loff_t *pos)
 {
 	long n = *pos - 1;
 
-	if (n > max_stack_trace.nr_entries || stack_dump_trace[n] == ULONG_MAX)
+	if (n > stack_trace_max.nr_entries || stack_dump_trace[n] == ULONG_MAX)
 		return NULL;
 
 	m->private = (void *)n;
@@ -332,9 +338,9 @@ static int t_show(struct seq_file *m, void *v)
 		seq_printf(m, "        Depth    Size   Location"
 			   "    (%d entries)\n"
 			   "        -----    ----   --------\n",
-			   max_stack_trace.nr_entries);
+			   stack_trace_max.nr_entries);
 
-		if (!stack_tracer_enabled && !max_stack_size)
+		if (!stack_tracer_enabled && !stack_trace_max_size)
 			print_disabled(m);
 
 		return 0;
@@ -342,17 +348,17 @@ static int t_show(struct seq_file *m, void *v)
 
 	i = *(long *)v;
 
-	if (i >= max_stack_trace.nr_entries ||
+	if (i >= stack_trace_max.nr_entries ||
 	    stack_dump_trace[i] == ULONG_MAX)
 		return 0;
 
-	if (i+1 == max_stack_trace.nr_entries ||
+	if (i+1 == stack_trace_max.nr_entries ||
 	    stack_dump_trace[i+1] == ULONG_MAX)
-		size = stack_dump_index[i];
+		size = stack_trace_index[i];
 	else
-		size = stack_dump_index[i] - stack_dump_index[i+1];
+		size = stack_trace_index[i] - stack_trace_index[i+1];
 
-	seq_printf(m, "%3ld) %8d   %5d   ", i, stack_dump_index[i], size);
+	seq_printf(m, "%3ld) %8d   %5d   ", i, stack_trace_index[i], size);
 
 	trace_lookup_stack(m, i);
 
@@ -442,7 +448,7 @@ static __init int stack_trace_init(void)
 		return 0;
 
 	trace_create_file("stack_max_size", 0644, d_tracer,
-			&max_stack_size, &stack_max_size_fops);
+			&stack_trace_max_size, &stack_max_size_fops);
 
 	trace_create_file("stack_trace", 0444, d_tracer,
 			NULL, &stack_trace_fops);

commit 1904be1b6bb92058c8e00063dd59df2df294e258
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Oct 20 21:48:02 2015 -0400

    tracing: Do not allow stack_tracer to record stack in NMI
    
    The code in stack tracer should not be executed within an NMI as it grabs
    spinlocks and stack tracing an NMI gives the possibility of causing a
    deadlock. Although this is safe on x86_64, because it does not perform stack
    traces when the task struct stack is not in use (interrupts and NMIs), it
    may be an issue for NMIs on i386 and other archs that use the same stack as
    the NMI.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 5f29402bff0f..8abf1ba18085 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -85,6 +85,10 @@ check_stack(unsigned long ip, unsigned long *stack)
 	if (!object_is_on_stack(stack))
 		return;
 
+	/* Can't do this from NMI context (can cause deadlocks) */
+	if (in_nmi())
+		return;
+
 	local_irq_save(flags);
 	arch_spin_lock(&max_stack_lock);
 

commit a2d7629048322ae62bff57f34f5f995e25ed234c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Oct 20 11:38:08 2015 -0400

    tracing: Have stack tracer force RCU to be watching
    
    The stack tracer was triggering the WARN_ON() in module.c:
    
     static void module_assert_mutex_or_preempt(void)
     {
     #ifdef CONFIG_LOCKDEP
            if (unlikely(!debug_locks))
                    return;
    
            WARN_ON(!rcu_read_lock_sched_held() &&
                    !lockdep_is_held(&module_mutex));
     #endif
     }
    
    The reason is that the stack tracer traces all function calls, and some of
    those calls happen while exiting or entering user space and idle. Some of
    these functions are called after RCU had already stopped watching, as RCU
    does not watch userspace or idle CPUs.
    
    If a max stack is hit, then the save_stack_trace() is called, which will
    check module addresses and call module_assert_mutex_or_preempt(), and then
    trigger the warning. Sad part is, the warning itself will also do a stack
    trace and tigger the same warning. That probably should be fixed.
    
    The warning was added by 0be964be0d45 "module: Sanitize RCU usage and
    locking" but this bug has probably been around longer. But it's unlikely to
    cause much harm, but the new warning causes the system to lock up.
    
    Cc: stable@vger.kernel.org # 4.2+
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc:"Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index b746399ab59c..5f29402bff0f 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -88,6 +88,12 @@ check_stack(unsigned long ip, unsigned long *stack)
 	local_irq_save(flags);
 	arch_spin_lock(&max_stack_lock);
 
+	/*
+	 * RCU may not be watching, make it see us.
+	 * The stack trace code uses rcu_sched.
+	 */
+	rcu_irq_enter();
+
 	/* In case another CPU set the tracer_frame on us */
 	if (unlikely(!frame_size))
 		this_size -= tracer_frame;
@@ -169,6 +175,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 	}
 
  out:
+	rcu_irq_exit();
 	arch_spin_unlock(&max_stack_lock);
 	local_irq_restore(flags);
 }

commit 72ac426a5bb0cec572d26b4456f8c1e14601694e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jul 16 13:24:54 2015 -0400

    tracing: Clean up stack tracing and fix fentry updates
    
    Akashi Takahiro was porting the stack tracer to arm64 and found some
    issues with it. One was that it repeats the top function, due to the
    stack frame added by the mcount caller and added by itself. This
    was added when fentry came in, and before fentry created its own stack
    frame. But x86's fentry now creates its own stack frame, and there's
    no need to insert the function again.
    
    This also cleans up the code a bit, where it doesn't need to do something
    special for fentry, and doesn't include insertion of a duplicate
    entry for the called function being traced.
    
    Link: http://lkml.kernel.org/r/55A646EE.6030402@linaro.org
    
    Some-suggestions-by: Jungseok Lee <jungseoklee85@gmail.com>
    Some-suggestions-by: Mark Rutland <mark.rutland@arm.com>
    Reported-by: AKASHI Takahiro <takahiro.akashi@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 3f34496244e9..b746399ab59c 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -18,12 +18,6 @@
 
 #define STACK_TRACE_ENTRIES 500
 
-#ifdef CC_USING_FENTRY
-# define fentry		1
-#else
-# define fentry		0
-#endif
-
 static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES+1] =
 	 { [0 ... (STACK_TRACE_ENTRIES)] = ULONG_MAX };
 static unsigned stack_dump_index[STACK_TRACE_ENTRIES];
@@ -35,7 +29,7 @@ static unsigned stack_dump_index[STACK_TRACE_ENTRIES];
  */
 static struct stack_trace max_stack_trace = {
 	.max_entries		= STACK_TRACE_ENTRIES - 1,
-	.entries		= &stack_dump_trace[1],
+	.entries		= &stack_dump_trace[0],
 };
 
 static unsigned long max_stack_size;
@@ -55,7 +49,7 @@ static inline void print_max_stack(void)
 
 	pr_emerg("        Depth    Size   Location    (%d entries)\n"
 			   "        -----    ----   --------\n",
-			   max_stack_trace.nr_entries - 1);
+			   max_stack_trace.nr_entries);
 
 	for (i = 0; i < max_stack_trace.nr_entries; i++) {
 		if (stack_dump_trace[i] == ULONG_MAX)
@@ -77,7 +71,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 	unsigned long this_size, flags; unsigned long *p, *top, *start;
 	static int tracer_frame;
 	int frame_size = ACCESS_ONCE(tracer_frame);
-	int i;
+	int i, x;
 
 	this_size = ((unsigned long)stack) & (THREAD_SIZE-1);
 	this_size = THREAD_SIZE - this_size;
@@ -105,26 +99,20 @@ check_stack(unsigned long ip, unsigned long *stack)
 	max_stack_size = this_size;
 
 	max_stack_trace.nr_entries = 0;
-
-	if (using_ftrace_ops_list_func())
-		max_stack_trace.skip = 4;
-	else
-		max_stack_trace.skip = 3;
+	max_stack_trace.skip = 3;
 
 	save_stack_trace(&max_stack_trace);
 
-	/*
-	 * Add the passed in ip from the function tracer.
-	 * Searching for this on the stack will skip over
-	 * most of the overhead from the stack tracer itself.
-	 */
-	stack_dump_trace[0] = ip;
-	max_stack_trace.nr_entries++;
+	/* Skip over the overhead of the stack tracer itself */
+	for (i = 0; i < max_stack_trace.nr_entries; i++) {
+		if (stack_dump_trace[i] == ip)
+			break;
+	}
 
 	/*
 	 * Now find where in the stack these are.
 	 */
-	i = 0;
+	x = 0;
 	start = stack;
 	top = (unsigned long *)
 		(((unsigned long)start & ~(THREAD_SIZE-1)) + THREAD_SIZE);
@@ -139,12 +127,15 @@ check_stack(unsigned long ip, unsigned long *stack)
 	while (i < max_stack_trace.nr_entries) {
 		int found = 0;
 
-		stack_dump_index[i] = this_size;
+		stack_dump_index[x] = this_size;
 		p = start;
 
 		for (; p < top && i < max_stack_trace.nr_entries; p++) {
+			if (stack_dump_trace[i] == ULONG_MAX)
+				break;
 			if (*p == stack_dump_trace[i]) {
-				this_size = stack_dump_index[i++] =
+				stack_dump_trace[x] = stack_dump_trace[i++];
+				this_size = stack_dump_index[x++] =
 					(top - p) * sizeof(unsigned long);
 				found = 1;
 				/* Start the search from here */
@@ -156,7 +147,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 				 * out what that is, then figure it out
 				 * now.
 				 */
-				if (unlikely(!tracer_frame) && i == 1) {
+				if (unlikely(!tracer_frame)) {
 					tracer_frame = (p - stack) *
 						sizeof(unsigned long);
 					max_stack_size -= tracer_frame;
@@ -168,6 +159,10 @@ check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
+	max_stack_trace.nr_entries = x;
+	for (; x < i; x++)
+		stack_dump_trace[x] = ULONG_MAX;
+
 	if (task_stack_end_corrupted(current)) {
 		print_max_stack();
 		BUG();
@@ -192,24 +187,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	if (per_cpu(trace_active, cpu)++ != 0)
 		goto out;
 
-	/*
-	 * When fentry is used, the traced function does not get
-	 * its stack frame set up, and we lose the parent.
-	 * The ip is pretty useless because the function tracer
-	 * was called before that function set up its stack frame.
-	 * In this case, we use the parent ip.
-	 *
-	 * By adding the return address of either the parent ip
-	 * or the current ip we can disregard most of the stack usage
-	 * caused by the stack tracer itself.
-	 *
-	 * The function tracer always reports the address of where the
-	 * mcount call was, but the stack will hold the return address.
-	 */
-	if (fentry)
-		ip = parent_ip;
-	else
-		ip += MCOUNT_INSN_SIZE;
+	ip += MCOUNT_INSN_SIZE;
 
 	check_stack(ip, &stack);
 
@@ -284,7 +262,7 @@ __next(struct seq_file *m, loff_t *pos)
 {
 	long n = *pos - 1;
 
-	if (n >= max_stack_trace.nr_entries || stack_dump_trace[n] == ULONG_MAX)
+	if (n > max_stack_trace.nr_entries || stack_dump_trace[n] == ULONG_MAX)
 		return NULL;
 
 	m->private = (void *)n;
@@ -354,7 +332,7 @@ static int t_show(struct seq_file *m, void *v)
 		seq_printf(m, "        Depth    Size   Location"
 			   "    (%d entries)\n"
 			   "        -----    ----   --------\n",
-			   max_stack_trace.nr_entries - 1);
+			   max_stack_trace.nr_entries);
 
 		if (!stack_tracer_enabled && !max_stack_size)
 			print_disabled(m);

commit 962e3707d9fb16bcf66ec5e5ebcea5248b9c2ab3
Author: Joe Perches <joe@perches.com>
Date:   Wed Apr 15 16:18:22 2015 -0700

    tracing: remove use of seq_printf return value
    
    The seq_printf return value, because it's frequently misused,
    will eventually be converted to void.
    
    See: commit 1f33c41c03da ("seq_file: Rename seq_overflow() to
         seq_has_overflowed() and make public")
    
    Miscellanea:
    
    o Remove unused return value from trace_lookup_stack
    
    Signed-off-by: Joe Perches <joe@perches.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index c3e4fcfddd45..3f34496244e9 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -327,11 +327,11 @@ static void t_stop(struct seq_file *m, void *p)
 	local_irq_enable();
 }
 
-static int trace_lookup_stack(struct seq_file *m, long i)
+static void trace_lookup_stack(struct seq_file *m, long i)
 {
 	unsigned long addr = stack_dump_trace[i];
 
-	return seq_printf(m, "%pS\n", (void *)addr);
+	seq_printf(m, "%pS\n", (void *)addr);
 }
 
 static void print_disabled(struct seq_file *m)

commit 14a5ae40f0def33a422a45b2ed09198adb7bf11c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 20 11:14:16 2015 -0500

    tracing: Use IS_ERR() check for return value of tracing_init_dentry()
    
    tracing_init_dentry() will soon return NULL as a valid pointer for the
    top level tracing directroy. NULL can not be used as an error value.
    Instead, switch to ERR_PTR() and check the return status with
    IS_ERR().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index e80927b88eb0..c3e4fcfddd45 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -460,7 +460,7 @@ static __init int stack_trace_init(void)
 	struct dentry *d_tracer;
 
 	d_tracer = tracing_init_dentry();
-	if (!d_tracer)
+	if (IS_ERR(d_tracer))
 		return 0;
 
 	trace_create_file("stack_max_size", 0644, d_tracer,

commit 3efb5f21a36fbddd524cffe36426a84622ce580e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 20 11:28:28 2015 -0500

    tracing: Remove unneeded includes of debugfs.h and fs.h
    
    The creation of tracing files and directories is for the most part
    encapsulated in helper functions in trace.c. Other files do not need to
    include debugfs.h or fs.h, as they may have needed to in the past.
    
    Remove them from the files that do not need them.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 16eddb308c33..e80927b88eb0 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -7,12 +7,10 @@
 #include <linux/seq_file.h>
 #include <linux/spinlock.h>
 #include <linux/uaccess.h>
-#include <linux/debugfs.h>
 #include <linux/ftrace.h>
 #include <linux/module.h>
 #include <linux/sysctl.h>
 #include <linux/init.h>
-#include <linux/fs.h>
 
 #include <asm/setup.h>
 

commit a70857e46dd13e87ae06bf0e64cb6a2d4f436265
Author: Aaron Tomlin <atomlin@redhat.com>
Date:   Fri Sep 12 14:16:18 2014 +0100

    sched: Add helper for task stack page overrun checking
    
    This facility is used in a few places so let's introduce
    a helper function to improve code readability.
    
    Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: aneesh.kumar@linux.vnet.ibm.com
    Cc: dzickus@redhat.com
    Cc: bmr@redhat.com
    Cc: jcastillo@redhat.com
    Cc: oleg@redhat.com
    Cc: riel@redhat.com
    Cc: prarit@redhat.com
    Cc: jgh@redhat.com
    Cc: minchan@kernel.org
    Cc: mpe@ellerman.id.au
    Cc: tglx@linutronix.de
    Cc: hannes@cmpxchg.org
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1410527779-8133-3-git-send-email-atomlin@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 1636e41828c2..16eddb308c33 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -170,7 +170,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
-	if (*end_of_stack(current) != STACK_END_MAGIC) {
+	if (task_stack_end_corrupted(current)) {
 		print_max_stack();
 		BUG();
 	}

commit d4311ff1a8da48d609db9500f121c15580dfeeb7
Author: Aaron Tomlin <atomlin@redhat.com>
Date:   Fri Sep 12 14:16:17 2014 +0100

    init/main.c: Give init_task a canary
    
    Tasks get their end of stack set to STACK_END_MAGIC with the
    aim to catch stack overruns. Currently this feature does not
    apply to init_task. This patch removes this restriction.
    
    Note that a similar patch was posted by Prarit Bhargava
    some time ago but was never merged:
    
      http://marc.info/?l=linux-kernel&m=127144305403241&w=2
    
    Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Michael Ellerman <mpe@ellerman.id.au>
    Cc: aneesh.kumar@linux.vnet.ibm.com
    Cc: dzickus@redhat.com
    Cc: bmr@redhat.com
    Cc: jcastillo@redhat.com
    Cc: jgh@redhat.com
    Cc: minchan@kernel.org
    Cc: tglx@linutronix.de
    Cc: hannes@cmpxchg.org
    Cc: Alex Thorlton <athorlton@sgi.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Benjamin Herrenschmidt <benh@kernel.crashing.org>
    Cc: Daeseok Youn <daeseok.youn@gmail.com>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Fabian Frederick <fabf@skynet.be>
    Cc: Geert Uytterhoeven <geert@linux-m68k.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Michael Opdenacker <michael.opdenacker@free-electrons.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Prarit Bhargava <prarit@redhat.com>
    Cc: Rik van Riel <riel@redhat.com>
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Cc: Seiji Aguchi <seiji.aguchi@hds.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Vladimir Davydov <vdavydov@parallels.com>
    Cc: Yasuaki Ishimatsu <isimatu.yasuaki@jp.fujitsu.com>
    Cc: linuxppc-dev@lists.ozlabs.org
    Link: http://lkml.kernel.org/r/1410527779-8133-2-git-send-email-atomlin@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 8a4e5cb66a4c..1636e41828c2 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -13,7 +13,6 @@
 #include <linux/sysctl.h>
 #include <linux/init.h>
 #include <linux/fs.h>
-#include <linux/magic.h>
 
 #include <asm/setup.h>
 
@@ -171,8 +170,7 @@ check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
-	if ((current != &init_task &&
-		*(end_of_stack(current)) != STACK_END_MAGIC)) {
+	if (*end_of_stack(current) != STACK_END_MAGIC) {
 		print_max_stack();
 		BUG();
 	}

commit e3172181946fadd3bd0e4c362931094ba87e5718
Author: Minchan Kim <minchan@kernel.org>
Date:   Mon Jun 2 13:33:12 2014 +0900

    tracing: Print max callstack on stacktrace bug
    
    While I played with my own feature(ex, something on the way to reclaim),
    the kernel would easily oops. I guessed that the reason had to do with
    stack overflow and wanted to prove it.
    
    I discovered the stack tracer which proved to be very useful for me but
    the kernel would oops before my user program gather the information via
    "watch cat /sys/kernel/debug/tracing/stack_trace" so I couldn't get any
    message from that. What I needed was to have the stack tracer emit the
    kernel stack usage before it does the oops so I could find what was
    hogging the stack.
    
    This patch shows the callstack of max stack usage right before an oops so
    we can find a culprit.
    
    So, the result is as follows.
    
    [ 1116.522206] init: lightdm main process (1246) terminated with status 1
    [ 1119.922916] init: failsafe-x main process (1272) terminated with status 1
    [ 3887.728131] kworker/u24:1 (6637) used greatest stack depth: 256 bytes left
    [ 6397.629227] cc1 (9554) used greatest stack depth: 128 bytes left
    [ 7174.467392]         Depth    Size   Location    (47 entries)
    [ 7174.467392]         -----    ----   --------
    [ 7174.467785]   0)     7248     256   get_page_from_freelist+0xa7/0x920
    [ 7174.468506]   1)     6992     352   __alloc_pages_nodemask+0x1cd/0xb20
    [ 7174.469224]   2)     6640       8   alloc_pages_current+0x10f/0x1f0
    [ 7174.469413]   3)     6632     168   new_slab+0x2c5/0x370
    [ 7174.469413]   4)     6464       8   __slab_alloc+0x3a9/0x501
    [ 7174.469413]   5)     6456      80   __kmalloc+0x1cb/0x200
    [ 7174.469413]   6)     6376     376   vring_add_indirect+0x36/0x200
    [ 7174.469413]   7)     6000     144   virtqueue_add_sgs+0x2e2/0x320
    [ 7174.469413]   8)     5856     288   __virtblk_add_req+0xda/0x1b0
    [ 7174.469413]   9)     5568      96   virtio_queue_rq+0xd3/0x1d0
    [ 7174.469413]  10)     5472     128   __blk_mq_run_hw_queue+0x1ef/0x440
    [ 7174.469413]  11)     5344      16   blk_mq_run_hw_queue+0x35/0x40
    [ 7174.469413]  12)     5328      96   blk_mq_insert_requests+0xdb/0x160
    [ 7174.469413]  13)     5232     112   blk_mq_flush_plug_list+0x12b/0x140
    [ 7174.469413]  14)     5120     112   blk_flush_plug_list+0xc7/0x220
    [ 7174.469413]  15)     5008      64   io_schedule_timeout+0x88/0x100
    [ 7174.469413]  16)     4944     128   mempool_alloc+0x145/0x170
    [ 7174.469413]  17)     4816      96   bio_alloc_bioset+0x10b/0x1d0
    [ 7174.469413]  18)     4720      48   get_swap_bio+0x30/0x90
    [ 7174.469413]  19)     4672     160   __swap_writepage+0x150/0x230
    [ 7174.469413]  20)     4512      32   swap_writepage+0x42/0x90
    [ 7174.469413]  21)     4480     320   shrink_page_list+0x676/0xa80
    [ 7174.469413]  22)     4160     208   shrink_inactive_list+0x262/0x4e0
    [ 7174.469413]  23)     3952     304   shrink_lruvec+0x3e1/0x6a0
    [ 7174.469413]  24)     3648      80   shrink_zone+0x3f/0x110
    [ 7174.469413]  25)     3568     128   do_try_to_free_pages+0x156/0x4c0
    [ 7174.469413]  26)     3440     208   try_to_free_pages+0xf7/0x1e0
    [ 7174.469413]  27)     3232     352   __alloc_pages_nodemask+0x783/0xb20
    [ 7174.469413]  28)     2880       8   alloc_pages_current+0x10f/0x1f0
    [ 7174.469413]  29)     2872     200   __page_cache_alloc+0x13f/0x160
    [ 7174.469413]  30)     2672      80   find_or_create_page+0x4c/0xb0
    [ 7174.469413]  31)     2592      80   ext4_mb_load_buddy+0x1e9/0x370
    [ 7174.469413]  32)     2512     176   ext4_mb_regular_allocator+0x1b7/0x460
    [ 7174.469413]  33)     2336     128   ext4_mb_new_blocks+0x458/0x5f0
    [ 7174.469413]  34)     2208     256   ext4_ext_map_blocks+0x70b/0x1010
    [ 7174.469413]  35)     1952     160   ext4_map_blocks+0x325/0x530
    [ 7174.469413]  36)     1792     384   ext4_writepages+0x6d1/0xce0
    [ 7174.469413]  37)     1408      16   do_writepages+0x23/0x40
    [ 7174.469413]  38)     1392      96   __writeback_single_inode+0x45/0x2e0
    [ 7174.469413]  39)     1296     176   writeback_sb_inodes+0x2ad/0x500
    [ 7174.469413]  40)     1120      80   __writeback_inodes_wb+0x9e/0xd0
    [ 7174.469413]  41)     1040     160   wb_writeback+0x29b/0x350
    [ 7174.469413]  42)      880     208   bdi_writeback_workfn+0x11c/0x480
    [ 7174.469413]  43)      672     144   process_one_work+0x1d2/0x570
    [ 7174.469413]  44)      528     112   worker_thread+0x116/0x370
    [ 7174.469413]  45)      416     240   kthread+0xf3/0x110
    [ 7174.469413]  46)      176     176   ret_from_fork+0x7c/0xb0
    [ 7174.469413] ------------[ cut here ]------------
    [ 7174.469413] kernel BUG at kernel/trace/trace_stack.c:174!
    [ 7174.469413] invalid opcode: 0000 [#1] SMP DEBUG_PAGEALLOC
    [ 7174.469413] Dumping ftrace buffer:
    [ 7174.469413]    (ftrace buffer empty)
    [ 7174.469413] Modules linked in:
    [ 7174.469413] CPU: 0 PID: 440 Comm: kworker/u24:0 Not tainted 3.14.0+ #212
    [ 7174.469413] Hardware name: Bochs Bochs, BIOS Bochs 01/01/2011
    [ 7174.469413] Workqueue: writeback bdi_writeback_workfn (flush-253:0)
    [ 7174.469413] task: ffff880034170000 ti: ffff880029518000 task.ti: ffff880029518000
    [ 7174.469413] RIP: 0010:[<ffffffff8112336e>]  [<ffffffff8112336e>] stack_trace_call+0x2de/0x340
    [ 7174.469413] RSP: 0000:ffff880029518290  EFLAGS: 00010046
    [ 7174.469413] RAX: 0000000000000030 RBX: 000000000000002f RCX: 0000000000000000
    [ 7174.469413] RDX: 0000000000000000 RSI: 000000000000002f RDI: ffffffff810b7159
    [ 7174.469413] RBP: ffff8800295182f0 R08: ffffffffffffffff R09: 0000000000000000
    [ 7174.469413] R10: 0000000000000001 R11: 0000000000000001 R12: ffffffff82768dfc
    [ 7174.469413] R13: 000000000000f2e8 R14: ffff8800295182b8 R15: 00000000000000f8
    [ 7174.469413] FS:  0000000000000000(0000) GS:ffff880037c00000(0000) knlGS:0000000000000000
    [ 7174.469413] CS:  0010 DS: 0000 ES: 0000 CR0: 000000008005003b
    [ 7174.469413] CR2: 00002acd0b994000 CR3: 0000000001c0b000 CR4: 00000000000006f0
    [ 7174.469413] Stack:
    [ 7174.469413]  0000000000000000 ffffffff8114fdb7 0000000000000087 0000000000001c50
    [ 7174.469413]  0000000000000000 0000000000000000 0000000000000000 0000000000000000
    [ 7174.469413]  0000000000000002 ffff880034170000 ffff880034171028 0000000000000000
    [ 7174.469413] Call Trace:
    [ 7174.469413]  [<ffffffff8114fdb7>] ? get_page_from_freelist+0xa7/0x920
    [ 7174.469413]  [<ffffffff816eee3f>] ftrace_call+0x5/0x2f
    [ 7174.469413]  [<ffffffff81165065>] ? next_zones_zonelist+0x5/0x70
    [ 7174.469413]  [<ffffffff810a23fa>] ? __bfs+0x11a/0x270
    [ 7174.469413]  [<ffffffff81165065>] ? next_zones_zonelist+0x5/0x70
    [ 7174.469413]  [<ffffffff8114fdb7>] ? get_page_from_freelist+0xa7/0x920
    [ 7174.469413]  [<ffffffff8119092f>] ? alloc_pages_current+0x10f/0x1f0
    [ 7174.469413]  [<ffffffff811507fd>] __alloc_pages_nodemask+0x1cd/0xb20
    [ 7174.469413]  [<ffffffff810a4de6>] ? check_irq_usage+0x96/0xe0
    [ 7174.469413]  [<ffffffff816eee3f>] ? ftrace_call+0x5/0x2f
    [ 7174.469413]  [<ffffffff8119092f>] alloc_pages_current+0x10f/0x1f0
    [ 7174.469413]  [<ffffffff81199cd5>] ? new_slab+0x2c5/0x370
    [ 7174.469413]  [<ffffffff81199cd5>] new_slab+0x2c5/0x370
    [ 7174.469413]  [<ffffffff816eee3f>] ? ftrace_call+0x5/0x2f
    [ 7174.469413]  [<ffffffff816db002>] __slab_alloc+0x3a9/0x501
    [ 7174.469413]  [<ffffffff8119af8b>] ? __kmalloc+0x1cb/0x200
    [ 7174.469413]  [<ffffffff8141dc46>] ? vring_add_indirect+0x36/0x200
    [ 7174.469413]  [<ffffffff8141dc46>] ? vring_add_indirect+0x36/0x200
    [ 7174.469413]  [<ffffffff8141dc46>] ? vring_add_indirect+0x36/0x200
    [ 7174.469413]  [<ffffffff8119af8b>] __kmalloc+0x1cb/0x200
    [ 7174.469413]  [<ffffffff8141de10>] ? vring_add_indirect+0x200/0x200
    [ 7174.469413]  [<ffffffff8141dc46>] vring_add_indirect+0x36/0x200
    [ 7174.469413]  [<ffffffff8141e402>] virtqueue_add_sgs+0x2e2/0x320
    [ 7174.469413]  [<ffffffff8148e35a>] __virtblk_add_req+0xda/0x1b0
    [ 7174.469413]  [<ffffffff8148e503>] virtio_queue_rq+0xd3/0x1d0
    [ 7174.469413]  [<ffffffff8134aa0f>] __blk_mq_run_hw_queue+0x1ef/0x440
    [ 7174.469413]  [<ffffffff8134b0d5>] blk_mq_run_hw_queue+0x35/0x40
    [ 7174.469413]  [<ffffffff8134b7bb>] blk_mq_insert_requests+0xdb/0x160
    [ 7174.469413]  [<ffffffff8134be5b>] blk_mq_flush_plug_list+0x12b/0x140
    [ 7174.469413]  [<ffffffff81342237>] blk_flush_plug_list+0xc7/0x220
    [ 7174.469413]  [<ffffffff816e60ef>] ? _raw_spin_unlock_irqrestore+0x3f/0x70
    [ 7174.469413]  [<ffffffff816e16e8>] io_schedule_timeout+0x88/0x100
    [ 7174.469413]  [<ffffffff816e1665>] ? io_schedule_timeout+0x5/0x100
    [ 7174.469413]  [<ffffffff81149415>] mempool_alloc+0x145/0x170
    [ 7174.469413]  [<ffffffff8109baf0>] ? __init_waitqueue_head+0x60/0x60
    [ 7174.469413]  [<ffffffff811e246b>] bio_alloc_bioset+0x10b/0x1d0
    [ 7174.469413]  [<ffffffff81184230>] ? end_swap_bio_read+0xc0/0xc0
    [ 7174.469413]  [<ffffffff81184230>] ? end_swap_bio_read+0xc0/0xc0
    [ 7174.469413]  [<ffffffff81184110>] get_swap_bio+0x30/0x90
    [ 7174.469413]  [<ffffffff81184230>] ? end_swap_bio_read+0xc0/0xc0
    [ 7174.469413]  [<ffffffff81184660>] __swap_writepage+0x150/0x230
    [ 7174.469413]  [<ffffffff810ab405>] ? do_raw_spin_unlock+0x5/0xa0
    [ 7174.469413]  [<ffffffff81184230>] ? end_swap_bio_read+0xc0/0xc0
    [ 7174.469413]  [<ffffffff81184515>] ? __swap_writepage+0x5/0x230
    [ 7174.469413]  [<ffffffff81184782>] swap_writepage+0x42/0x90
    [ 7174.469413]  [<ffffffff8115ae96>] shrink_page_list+0x676/0xa80
    [ 7174.469413]  [<ffffffff816eee3f>] ? ftrace_call+0x5/0x2f
    [ 7174.469413]  [<ffffffff8115b872>] shrink_inactive_list+0x262/0x4e0
    [ 7174.469413]  [<ffffffff8115c1c1>] shrink_lruvec+0x3e1/0x6a0
    [ 7174.469413]  [<ffffffff8115c4bf>] shrink_zone+0x3f/0x110
    [ 7174.469413]  [<ffffffff816eee3f>] ? ftrace_call+0x5/0x2f
    [ 7174.469413]  [<ffffffff8115c9e6>] do_try_to_free_pages+0x156/0x4c0
    [ 7174.469413]  [<ffffffff8115cf47>] try_to_free_pages+0xf7/0x1e0
    [ 7174.469413]  [<ffffffff81150db3>] __alloc_pages_nodemask+0x783/0xb20
    [ 7174.469413]  [<ffffffff8119092f>] alloc_pages_current+0x10f/0x1f0
    [ 7174.469413]  [<ffffffff81145c0f>] ? __page_cache_alloc+0x13f/0x160
    [ 7174.469413]  [<ffffffff81145c0f>] __page_cache_alloc+0x13f/0x160
    [ 7174.469413]  [<ffffffff81146c6c>] find_or_create_page+0x4c/0xb0
    [ 7174.469413]  [<ffffffff811463e5>] ? find_get_page+0x5/0x130
    [ 7174.469413]  [<ffffffff812837b9>] ext4_mb_load_buddy+0x1e9/0x370
    [ 7174.469413]  [<ffffffff81284c07>] ext4_mb_regular_allocator+0x1b7/0x460
    [ 7174.469413]  [<ffffffff81281070>] ? ext4_mb_use_preallocated+0x40/0x360
    [ 7174.469413]  [<ffffffff816eee3f>] ? ftrace_call+0x5/0x2f
    [ 7174.469413]  [<ffffffff81287eb8>] ext4_mb_new_blocks+0x458/0x5f0
    [ 7174.469413]  [<ffffffff8127d83b>] ext4_ext_map_blocks+0x70b/0x1010
    [ 7174.469413]  [<ffffffff8124e6d5>] ext4_map_blocks+0x325/0x530
    [ 7174.469413]  [<ffffffff81253871>] ext4_writepages+0x6d1/0xce0
    [ 7174.469413]  [<ffffffff812531a0>] ? ext4_journalled_write_end+0x330/0x330
    [ 7174.469413]  [<ffffffff811539b3>] do_writepages+0x23/0x40
    [ 7174.469413]  [<ffffffff811d2365>] __writeback_single_inode+0x45/0x2e0
    [ 7174.469413]  [<ffffffff811d36ed>] writeback_sb_inodes+0x2ad/0x500
    [ 7174.469413]  [<ffffffff811d39de>] __writeback_inodes_wb+0x9e/0xd0
    [ 7174.469413]  [<ffffffff811d40bb>] wb_writeback+0x29b/0x350
    [ 7174.469413]  [<ffffffff81057c3d>] ? __local_bh_enable_ip+0x6d/0xd0
    [ 7174.469413]  [<ffffffff811d6e9c>] bdi_writeback_workfn+0x11c/0x480
    [ 7174.469413]  [<ffffffff81070610>] ? process_one_work+0x170/0x570
    [ 7174.469413]  [<ffffffff81070672>] process_one_work+0x1d2/0x570
    [ 7174.469413]  [<ffffffff81070610>] ? process_one_work+0x170/0x570
    [ 7174.469413]  [<ffffffff81071bb6>] worker_thread+0x116/0x370
    [ 7174.469413]  [<ffffffff81071aa0>] ? manage_workers.isra.19+0x2e0/0x2e0
    [ 7174.469413]  [<ffffffff81078e53>] kthread+0xf3/0x110
    [ 7174.469413]  [<ffffffff81078d60>] ? flush_kthread_worker+0x150/0x150
    [ 7174.469413]  [<ffffffff816ef0ec>] ret_from_fork+0x7c/0xb0
    [ 7174.469413]  [<ffffffff81078d60>] ? flush_kthread_worker+0x150/0x150
    [ 7174.469413] Code: c0 49 bc fc 8d 76 82 ff ff ff ff e8 44 5a 5b 00 31 f6 8b 05 95 2b b3 00 48 39 c6 7d 0e 4c 8b 04 f5 20 5f c5 81 49 83 f8 ff 75 11 <0f> 0b 48 63 05 71 5a 64 01 48 29 c3 e9 d0 fd ff ff 48 8d 5e 01
    [ 7174.469413] RIP  [<ffffffff8112336e>] stack_trace_call+0x2de/0x340
    [ 7174.469413]  RSP <ffff880029518290>
    [ 7174.469413] ---[ end trace c97d325b36b718f3 ]---
    
    Link: http://lkml.kernel.org/p/1401683592-1651-1-git-send-email-minchan@kernel.org
    
    Signed-off-by: Minchan Kim <minchan@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 5aa9a5b9b6e2..8a4e5cb66a4c 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -51,11 +51,33 @@ static DEFINE_MUTEX(stack_sysctl_mutex);
 int stack_tracer_enabled;
 static int last_stack_tracer_enabled;
 
+static inline void print_max_stack(void)
+{
+	long i;
+	int size;
+
+	pr_emerg("        Depth    Size   Location    (%d entries)\n"
+			   "        -----    ----   --------\n",
+			   max_stack_trace.nr_entries - 1);
+
+	for (i = 0; i < max_stack_trace.nr_entries; i++) {
+		if (stack_dump_trace[i] == ULONG_MAX)
+			break;
+		if (i+1 == max_stack_trace.nr_entries ||
+				stack_dump_trace[i+1] == ULONG_MAX)
+			size = stack_dump_index[i];
+		else
+			size = stack_dump_index[i] - stack_dump_index[i+1];
+
+		pr_emerg("%3ld) %8d   %5d   %pS\n", i, stack_dump_index[i],
+				size, (void *)stack_dump_trace[i]);
+	}
+}
+
 static inline void
 check_stack(unsigned long ip, unsigned long *stack)
 {
-	unsigned long this_size, flags;
-	unsigned long *p, *top, *start;
+	unsigned long this_size, flags; unsigned long *p, *top, *start;
 	static int tracer_frame;
 	int frame_size = ACCESS_ONCE(tracer_frame);
 	int i;
@@ -149,8 +171,12 @@ check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
-	BUG_ON(current != &init_task &&
-		*(end_of_stack(current)) != STACK_END_MAGIC);
+	if ((current != &init_task &&
+		*(end_of_stack(current)) != STACK_END_MAGIC)) {
+		print_max_stack();
+		BUG();
+	}
+
  out:
 	arch_spin_unlock(&max_stack_lock);
 	local_irq_restore(flags);

commit 7eea4fce0246fe3a15ad7f3bb8d0a56d1f9440e6
Author: Jiaxing Wang <wangjiaxing@insigma.com.cn>
Date:   Sun Apr 20 23:10:43 2014 +0800

    tracing/stack_trace: Skip 4 instead of 3 when using ftrace_ops_list_func
    
    When using ftrace_ops_list_func, we should skip 4 instead of 3,
    to avoid ftrace_call+0x5/0xb appearing in the stack trace:
    
            Depth    Size   Location    (110 entries)
            -----    ----   --------
      0)     2956       0   update_curr+0xe/0x1e0
      1)     2956      68   ftrace_call+0x5/0xb
      2)     2888      92   enqueue_entity+0x53/0xe80
      3)     2796      80   enqueue_task_fair+0x47/0x7e0
      4)     2716      28   enqueue_task+0x45/0x70
      5)     2688      12   activate_task+0x22/0x30
    
    Add a function using_ftrace_ops_list_func() to test for this while keeping
    ftrace_ops_list_func to remain static.
    
    Link: http://lkml.kernel.org/p/1398006644-5935-2-git-send-email-wangjiaxing@insigma.com.cn
    
    Signed-off-by: Jiaxing Wang <wangjiaxing@insigma.com.cn>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 21b320e5d163..5aa9a5b9b6e2 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -85,8 +85,12 @@ check_stack(unsigned long ip, unsigned long *stack)
 
 	max_stack_size = this_size;
 
-	max_stack_trace.nr_entries	= 0;
-	max_stack_trace.skip		= 3;
+	max_stack_trace.nr_entries = 0;
+
+	if (using_ftrace_ops_list_func())
+		max_stack_trace.skip = 4;
+	else
+		max_stack_trace.skip = 3;
 
 	save_stack_trace(&max_stack_trace);
 

commit 3862807880acc0adaef6749738d210c9f45c3049
Author: Aaron Tomlin <atomlin@redhat.com>
Date:   Mon Mar 24 14:03:57 2014 +0000

    tracing: Add BUG_ON when stack end location is over written
    
    It is difficult to detect a stack overrun when it
    actually occurs.
    
    We have observed that this type of corruption is often
    silent and can go unnoticed. Once the corrupted region
    is examined, the outcome is undefined and often
    results in sporadic system crashes.
    
    When the stack tracing feature is enabled, let's check
    for this condition and take appropriate action.
    
    Note: init_task doesn't get its stack end location
    set to STACK_END_MAGIC.
    
    Link: http://lkml.kernel.org/r/1395669837-30209-1-git-send-email-atomlin@redhat.com
    
    Signed-off-by: Aaron Tomlin <atomlin@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index e6be585cf06a..21b320e5d163 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -13,6 +13,7 @@
 #include <linux/sysctl.h>
 #include <linux/init.h>
 #include <linux/fs.h>
+#include <linux/magic.h>
 
 #include <asm/setup.h>
 
@@ -144,6 +145,8 @@ check_stack(unsigned long ip, unsigned long *stack)
 			i++;
 	}
 
+	BUG_ON(current != &init_task &&
+		*(end_of_stack(current)) != STACK_END_MAGIC);
  out:
 	arch_spin_unlock(&max_stack_lock);
 	local_irq_restore(flags);

commit 098c879e1f2d6ee7afbfe959f6b04070065cec90
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Dec 21 17:39:40 2013 -0500

    tracing: Add generic tracing_lseek() function
    
    Trace event triggers added a lseek that uses the ftrace_filter_lseek()
    function. Unfortunately, when function tracing is not configured in
    that function is not defined and the kernel fails to build.
    
    This is the second time that function was added to a file ops and
    it broke the build due to requiring special config dependencies.
    
    Make a generic tracing_lseek() that all the tracing utilities may
    use.
    
    Also, modify the old ftrace_filter_lseek() to return 0 instead of
    1 on WRONLY. Not sure why it was a 1 as that does not make sense.
    
    This also changes the old tracing_seek() to modify the file pos
    pointer on WRONLY as well.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Tested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Acked-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index b20428c5efe2..e6be585cf06a 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -382,7 +382,7 @@ static const struct file_operations stack_trace_filter_fops = {
 	.open = stack_trace_filter_open,
 	.read = seq_read,
 	.write = ftrace_filter_write,
-	.llseek = ftrace_filter_lseek,
+	.llseek = tracing_lseek,
 	.release = ftrace_regex_release,
 };
 

commit 9e8529afc4518f4e5d610001545ebc97e1333c79
Merge: ec25e246b94a 4c69e6ea415a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Apr 29 13:55:38 2013 -0700

    Merge tag 'trace-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Along with the usual minor fixes and clean ups there are a few major
      changes with this pull request.
    
       1) Multiple buffers for the ftrace facility
    
      This feature has been requested by many people over the last few
      years.  I even heard that Google was about to implement it themselves.
      I finally had time and cleaned up the code such that you can now
      create multiple instances of the ftrace buffer and have different
      events go to different buffers.  This way, a low frequency event will
      not be lost in the noise of a high frequency event.
    
      Note, currently only events can go to different buffers, the tracers
      (ie function, function_graph and the latency tracers) still can only
      be written to the main buffer.
    
       2) The function tracer triggers have now been extended.
    
      The function tracer had two triggers.  One to enable tracing when a
      function is hit, and one to disable tracing.  Now you can record a
      stack trace on a single (or many) function(s), take a snapshot of the
      buffer (copy it to the snapshot buffer), and you can enable or disable
      an event to be traced when a function is hit.
    
       3) A perf clock has been added.
    
      A "perf" clock can be chosen to be used when tracing.  This will cause
      ftrace to use the same clock as perf uses, and hopefully this will
      make it easier to interleave the perf and ftrace data for analysis."
    
    * tag 'trace-3.10' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (82 commits)
      tracepoints: Prevent null probe from being added
      tracing: Compare to 1 instead of zero for is_signed_type()
      tracing: Remove obsolete macro guard _TRACE_PROFILE_INIT
      ftrace: Get rid of ftrace_profile_bits
      tracing: Check return value of tracing_init_dentry()
      tracing: Get rid of unneeded key calculation in ftrace_hash_move()
      tracing: Reset ftrace_graph_filter_enabled if count is zero
      tracing: Fix off-by-one on allocating stat->pages
      kernel: tracing: Use strlcpy instead of strncpy
      tracing: Update debugfs README file
      tracing: Fix ftrace_dump()
      tracing: Rename trace_event_mutex to trace_event_sem
      tracing: Fix comment about prefix in arch_syscall_match_sym_name()
      tracing: Convert trace_destroy_fields() to static
      tracing: Move find_event_field() into trace_events.c
      tracing: Use TRACE_MAX_PRINT instead of constant
      tracing: Use pr_warn_once instead of open coded implementation
      ring-buffer: Add ring buffer startup selftest
      tracing: Bring Documentation/trace/ftrace.txt up to date
      tracing: Add "perf" trace_clock
      ...
    
    Conflicts:
            kernel/trace/ftrace.c
            kernel/trace/trace.c

commit ed6f1c996bfe4b6e520cf7a74b51cd6988d84420
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Wed Apr 10 09:18:12 2013 +0900

    tracing: Check return value of tracing_init_dentry()
    
    Check return value and bail out if it's NULL.
    
    Link: http://lkml.kernel.org/r/1365553093-10180-2-git-send-email-namhyung@kernel.org
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index aab277b67fa9..8c3f37e2dc43 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -431,6 +431,8 @@ static __init int stack_trace_init(void)
 	struct dentry *d_tracer;
 
 	d_tracer = tracing_init_dentry();
+	if (!d_tracer)
+		return 0;
 
 	trace_create_file("stack_max_size", 0644, d_tracer,
 			&max_stack_size, &stack_max_size_fops);

commit 6a76f8c0ab19f215af2a3442870eeb5f0e81998d
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Thu Apr 11 15:55:01 2013 +0900

    tracing: Fix possible NULL pointer dereferences
    
    Currently set_ftrace_pid and set_graph_function files use seq_lseek
    for their fops.  However seq_open() is called only for FMODE_READ in
    the fops->open() so that if an user tries to seek one of those file
    when she open it for writing, it sees NULL seq_file and then panic.
    
    It can be easily reproduced with following command:
    
      $ cd /sys/kernel/debug/tracing
      $ echo 1234 | sudo tee -a set_ftrace_pid
    
    In this example, GNU coreutils' tee opens the file with fopen(, "a")
    and then the fopen() internally calls lseek().
    
    Link: http://lkml.kernel.org/r/1365663302-2170-1-git-send-email-namhyung@kernel.org
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 42ca822fc701..83a8b5b7bd35 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -322,7 +322,7 @@ static const struct file_operations stack_trace_filter_fops = {
 	.open = stack_trace_filter_open,
 	.read = seq_read,
 	.write = ftrace_filter_write,
-	.llseek = ftrace_regex_lseek,
+	.llseek = ftrace_filter_lseek,
 	.release = ftrace_regex_release,
 };
 

commit 4df297129f622bdc18935c856f42b9ddd18f9f28
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Mar 13 23:34:22 2013 -0400

    tracing: Remove most or all of stack tracer stack size from stack_max_size
    
    Currently, the depth reported in the stack tracer stack_trace file
    does not match the stack_max_size file. This is because the stack_max_size
    includes the overhead of stack tracer itself while the depth does not.
    
    The first time a max is triggered, a calculation is not performed that
    figures out the overhead of the stack tracer and subtracts it from
    the stack_max_size variable. The overhead is stored and is subtracted
    from the reported stack size for comparing for a new max.
    
    Now the stack_max_size corresponds to the reported depth:
    
     # cat stack_max_size
    4640
    
     # cat stack_trace
            Depth    Size   Location    (48 entries)
            -----    ----   --------
      0)     4640      32   _raw_spin_lock+0x18/0x24
      1)     4608     112   ____cache_alloc+0xb7/0x22d
      2)     4496      80   kmem_cache_alloc+0x63/0x12f
      3)     4416      16   mempool_alloc_slab+0x15/0x17
    [...]
    
    While testing against and older gcc on x86 that uses mcount instead
    of fentry, I found that pasing in ip + MCOUNT_INSN_SIZE let the
    stack trace show one more function deep which was missing before.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index ea28e4b0ed58..aab277b67fa9 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -20,27 +20,24 @@
 
 #define STACK_TRACE_ENTRIES 500
 
-/*
- * If fentry is used, then the function being traced will
- * jump to fentry directly before it sets up its stack frame.
- * We need to ignore that one and record the parent. Since
- * the stack frame for the traced function wasn't set up yet,
- * the stack_trace wont see the parent. That needs to be added
- * manually to stack_dump_trace[] as the first element.
- */
 #ifdef CC_USING_FENTRY
-# define add_func	1
+# define fentry		1
 #else
-# define add_func	0
+# define fentry		0
 #endif
 
 static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES+1] =
 	 { [0 ... (STACK_TRACE_ENTRIES)] = ULONG_MAX };
 static unsigned stack_dump_index[STACK_TRACE_ENTRIES];
 
+/*
+ * Reserve one entry for the passed in ip. This will allow
+ * us to remove most or all of the stack size overhead
+ * added by the stack tracer itself.
+ */
 static struct stack_trace max_stack_trace = {
-	.max_entries		= STACK_TRACE_ENTRIES - add_func,
-	.entries		= &stack_dump_trace[add_func],
+	.max_entries		= STACK_TRACE_ENTRIES - 1,
+	.entries		= &stack_dump_trace[1],
 };
 
 static unsigned long max_stack_size;
@@ -58,10 +55,14 @@ check_stack(unsigned long ip, unsigned long *stack)
 {
 	unsigned long this_size, flags;
 	unsigned long *p, *top, *start;
+	static int tracer_frame;
+	int frame_size = ACCESS_ONCE(tracer_frame);
 	int i;
 
 	this_size = ((unsigned long)stack) & (THREAD_SIZE-1);
 	this_size = THREAD_SIZE - this_size;
+	/* Remove the frame of the tracer */
+	this_size -= frame_size;
 
 	if (this_size <= max_stack_size)
 		return;
@@ -73,6 +74,10 @@ check_stack(unsigned long ip, unsigned long *stack)
 	local_irq_save(flags);
 	arch_spin_lock(&max_stack_lock);
 
+	/* In case another CPU set the tracer_frame on us */
+	if (unlikely(!frame_size))
+		this_size -= tracer_frame;
+
 	/* a race could have already updated it */
 	if (this_size <= max_stack_size)
 		goto out;
@@ -85,15 +90,12 @@ check_stack(unsigned long ip, unsigned long *stack)
 	save_stack_trace(&max_stack_trace);
 
 	/*
-	 * When fentry is used, the traced function does not get
-	 * its stack frame set up, and we lose the parent.
-	 * Add that one in manally. We set up save_stack_trace()
-	 * to not touch the first element in this case.
+	 * Add the passed in ip from the function tracer.
+	 * Searching for this on the stack will skip over
+	 * most of the overhead from the stack tracer itself.
 	 */
-	if (add_func) {
-		stack_dump_trace[0] = ip;
-		max_stack_trace.nr_entries++;
-	}
+	stack_dump_trace[0] = ip;
+	max_stack_trace.nr_entries++;
 
 	/*
 	 * Now find where in the stack these are.
@@ -123,6 +125,18 @@ check_stack(unsigned long ip, unsigned long *stack)
 				found = 1;
 				/* Start the search from here */
 				start = p + 1;
+				/*
+				 * We do not want to show the overhead
+				 * of the stack tracer stack in the
+				 * max stack. If we haven't figured
+				 * out what that is, then figure it out
+				 * now.
+				 */
+				if (unlikely(!tracer_frame) && i == 1) {
+					tracer_frame = (p - stack) *
+						sizeof(unsigned long);
+					max_stack_size -= tracer_frame;
+				}
 			}
 		}
 
@@ -149,7 +163,26 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	if (per_cpu(trace_active, cpu)++ != 0)
 		goto out;
 
-	check_stack(parent_ip, &stack);
+	/*
+	 * When fentry is used, the traced function does not get
+	 * its stack frame set up, and we lose the parent.
+	 * The ip is pretty useless because the function tracer
+	 * was called before that function set up its stack frame.
+	 * In this case, we use the parent ip.
+	 *
+	 * By adding the return address of either the parent ip
+	 * or the current ip we can disregard most of the stack usage
+	 * caused by the stack tracer itself.
+	 *
+	 * The function tracer always reports the address of where the
+	 * mcount call was, but the stack will hold the return address.
+	 */
+	if (fentry)
+		ip = parent_ip;
+	else
+		ip += MCOUNT_INSN_SIZE;
+
+	check_stack(ip, &stack);
 
  out:
 	per_cpu(trace_active, cpu)--;

commit d4ecbfc49b4b1d4b597fb5ba9e4fa25d62f105c5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Mar 13 21:25:35 2013 -0400

    tracing: Fix stack tracer with fentry use
    
    When gcc 4.6 on x86 is used, the function tracer will use the new
    option -mfentry which does a call to "fentry" at every function
    instead of "mcount". The significance of this is that fentry is
    called as the first operation of the function instead of the mcount
    usage of being called after the stack.
    
    This causes the stack tracer to show some bogus results for the size
    of the last function traced, as well as showing "ftrace_call" instead
    of the function. This is due to the stack frame not being set up
    by the function that is about to be traced.
    
     # cat stack_trace
            Depth    Size   Location    (48 entries)
            -----    ----   --------
      0)     4824     216   ftrace_call+0x5/0x2f
      1)     4608     112   ____cache_alloc+0xb7/0x22d
      2)     4496      80   kmem_cache_alloc+0x63/0x12f
    
    The 216 size for ftrace_call includes both the ftrace_call stack
    (which includes the saving of registers it does), as well as the
    stack size of the parent.
    
    To fix this, if CC_USING_FENTRY is defined, then the stack_tracer
    will reserve the first item in stack_dump_trace[] array when
    calling save_stack_trace(), and it will fill it in with the parent ip.
    Then the code will look for the parent pointer on the stack and
    give the real size of the parent's stack pointer:
    
     # cat stack_trace
            Depth    Size   Location    (14 entries)
            -----    ----   --------
      0)     2640      48   update_group_power+0x26/0x187
      1)     2592     224   update_sd_lb_stats+0x2a5/0x4ac
      2)     2368     160   find_busiest_group+0x31/0x1f1
      3)     2208     256   load_balance+0xd9/0x662
    
    I'm Cc'ing stable, although it's not urgent, as it only shows bogus
    size for item #0, the rest of the trace is legit. It should still be
    corrected in previous stable releases.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index dc02e29d8255..ea28e4b0ed58 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -20,13 +20,27 @@
 
 #define STACK_TRACE_ENTRIES 500
 
+/*
+ * If fentry is used, then the function being traced will
+ * jump to fentry directly before it sets up its stack frame.
+ * We need to ignore that one and record the parent. Since
+ * the stack frame for the traced function wasn't set up yet,
+ * the stack_trace wont see the parent. That needs to be added
+ * manually to stack_dump_trace[] as the first element.
+ */
+#ifdef CC_USING_FENTRY
+# define add_func	1
+#else
+# define add_func	0
+#endif
+
 static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES+1] =
 	 { [0 ... (STACK_TRACE_ENTRIES)] = ULONG_MAX };
 static unsigned stack_dump_index[STACK_TRACE_ENTRIES];
 
 static struct stack_trace max_stack_trace = {
-	.max_entries		= STACK_TRACE_ENTRIES,
-	.entries		= stack_dump_trace,
+	.max_entries		= STACK_TRACE_ENTRIES - add_func,
+	.entries		= &stack_dump_trace[add_func],
 };
 
 static unsigned long max_stack_size;
@@ -40,7 +54,7 @@ int stack_tracer_enabled;
 static int last_stack_tracer_enabled;
 
 static inline void
-check_stack(unsigned long *stack)
+check_stack(unsigned long ip, unsigned long *stack)
 {
 	unsigned long this_size, flags;
 	unsigned long *p, *top, *start;
@@ -70,6 +84,17 @@ check_stack(unsigned long *stack)
 
 	save_stack_trace(&max_stack_trace);
 
+	/*
+	 * When fentry is used, the traced function does not get
+	 * its stack frame set up, and we lose the parent.
+	 * Add that one in manally. We set up save_stack_trace()
+	 * to not touch the first element in this case.
+	 */
+	if (add_func) {
+		stack_dump_trace[0] = ip;
+		max_stack_trace.nr_entries++;
+	}
+
 	/*
 	 * Now find where in the stack these are.
 	 */
@@ -124,7 +149,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	if (per_cpu(trace_active, cpu)++ != 0)
 		goto out;
 
-	check_stack(&stack);
+	check_stack(parent_ip, &stack);
 
  out:
 	per_cpu(trace_active, cpu)--;

commit 87889501d0adfae10e3b0f0e6f2d7536eed9ae84
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Mar 13 20:43:57 2013 -0400

    tracing: Use stack of calling function for stack tracer
    
    Use the stack of stack_trace_call() instead of check_stack() as
    the test pointer for max stack size. It makes it a bit cleaner
    and a little more accurate.
    
    Adding stable, as a later fix depends on this patch.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 42ca822fc701..dc02e29d8255 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -39,20 +39,21 @@ static DEFINE_MUTEX(stack_sysctl_mutex);
 int stack_tracer_enabled;
 static int last_stack_tracer_enabled;
 
-static inline void check_stack(void)
+static inline void
+check_stack(unsigned long *stack)
 {
 	unsigned long this_size, flags;
 	unsigned long *p, *top, *start;
 	int i;
 
-	this_size = ((unsigned long)&this_size) & (THREAD_SIZE-1);
+	this_size = ((unsigned long)stack) & (THREAD_SIZE-1);
 	this_size = THREAD_SIZE - this_size;
 
 	if (this_size <= max_stack_size)
 		return;
 
 	/* we do not handle interrupt stacks yet */
-	if (!object_is_on_stack(&this_size))
+	if (!object_is_on_stack(stack))
 		return;
 
 	local_irq_save(flags);
@@ -73,7 +74,7 @@ static inline void check_stack(void)
 	 * Now find where in the stack these are.
 	 */
 	i = 0;
-	start = &this_size;
+	start = stack;
 	top = (unsigned long *)
 		(((unsigned long)start & ~(THREAD_SIZE-1)) + THREAD_SIZE);
 
@@ -113,6 +114,7 @@ static void
 stack_trace_call(unsigned long ip, unsigned long parent_ip,
 		 struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
+	unsigned long stack;
 	int cpu;
 
 	preempt_disable_notrace();
@@ -122,7 +124,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	if (per_cpu(trace_active, cpu)++ != 0)
 		goto out;
 
-	check_stack();
+	check_stack(&stack);
 
  out:
 	per_cpu(trace_active, cpu)--;

commit 717a9ef7f355480686cdbac3f32d6075437a923e
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Wed Jul 18 11:56:01 2012 -0700

    tracing: Remove unneeded checks from the stack tracer
    
    It seems that 'ftrace_enabled' flag should not be used inside the tracer
    functions. The ftrace core is using this flag for internal purposes, and
    the flag wasn't meant to be used in tracers' runtime checks.
    
    stack tracer is the only tracer that abusing the flag. So stop it from
    serving as a bad example.
    
    Also, there is a local 'stack_trace_disabled' flag in the stack tracer,
    which is never updated; so it can be removed as well.
    
    Link: http://lkml.kernel.org/r/1342637761-9655-1-git-send-email-anton.vorontsov@linaro.org
    
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 0c1b165778e5..42ca822fc701 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -33,7 +33,6 @@ static unsigned long max_stack_size;
 static arch_spinlock_t max_stack_lock =
 	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
 
-static int stack_trace_disabled __read_mostly;
 static DEFINE_PER_CPU(int, trace_active);
 static DEFINE_MUTEX(stack_sysctl_mutex);
 
@@ -116,9 +115,6 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 {
 	int cpu;
 
-	if (unlikely(!ftrace_enabled || stack_trace_disabled))
-		return;
-
 	preempt_disable_notrace();
 
 	cpu = raw_smp_processor_id();

commit 4740974a6844156c14d741b0080b59d275679a23
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jul 20 11:04:44 2012 -0400

    ftrace: Add default recursion protection for function tracing
    
    As more users of the function tracer utility are being added, they do
    not always add the necessary recursion protection. To protect from
    function recursion due to tracing, if the callback ftrace_ops does not
    specifically specify that it protects against recursion (by setting
    the FTRACE_OPS_FL_RECURSION_SAFE flag), the list operation will be
    called by the mcount trampoline which adds recursion protection.
    
    If the flag is set, then the function will be called directly with no
    extra protection.
    
    Note, the list operation is called if more than one function callback
    is registered, or if the arch does not support all of the function
    tracer features.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 2fa5328e8893..0c1b165778e5 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -137,6 +137,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip,
 static struct ftrace_ops trace_ops __read_mostly =
 {
 	.func = stack_trace_call,
+	.flags = FTRACE_OPS_FL_RECURSION_SAFE,
 };
 
 static ssize_t

commit a1e2e31d175a1349274eba3465d17616c6725f8c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 9 12:50:46 2011 -0400

    ftrace: Return pt_regs to function trace callback
    
    Return as the 4th paramater to the function tracer callback the pt_regs.
    
    Later patches that implement regs passing for the architectures will require
    having the ftrace_ops set the SAVE_REGS flag, which will tell the arch
    to take the time to pass a full set of pt_regs to the ftrace_ops callback
    function. If the arch does not support it then it should pass NULL.
    
    If an arch can pass full regs, then it should define:
     ARCH_SUPPORTS_FTRACE_SAVE_REGS to 1
    
    Link: http://lkml.kernel.org/r/20120702201821.019966811@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index e20006d5fb6a..2fa5328e8893 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -111,7 +111,8 @@ static inline void check_stack(void)
 }
 
 static void
-stack_trace_call(unsigned long ip, unsigned long parent_ip, struct ftrace_ops *op)
+stack_trace_call(unsigned long ip, unsigned long parent_ip,
+		 struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
 	int cpu;
 

commit 2f5f6ad9390c1ebbf738d130dbfe80b60eaa167e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 8 16:57:47 2011 -0400

    ftrace: Pass ftrace_ops as third parameter to function trace callback
    
    Currently the function trace callback receives only the ip and parent_ip
    of the function that it traced. It would be more powerful to also return
    the ops that registered the function as well. This allows the same function
    to act differently depending on what ftrace_ops registered it.
    
    Link: http://lkml.kernel.org/r/20120612225424.267254552@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index d4545f49242e..e20006d5fb6a 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -111,7 +111,7 @@ static inline void check_stack(void)
 }
 
 static void
-stack_trace_call(unsigned long ip, unsigned long parent_ip)
+stack_trace_call(unsigned long ip, unsigned long parent_ip, struct ftrace_ops *op)
 {
 	int cpu;
 

commit 762e1207889b3451c50d365b741af6f9ce958886
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Dec 19 22:01:00 2011 -0500

    tracing: Have stack tracing set filtered functions at boot
    
    Add stacktrace_filter= to the kernel command line that lets
    the user pick specific functions to check the stack on.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 0398b7c7afd6..d4545f49242e 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -13,6 +13,9 @@
 #include <linux/sysctl.h>
 #include <linux/init.h>
 #include <linux/fs.h>
+
+#include <asm/setup.h>
+
 #include "trace.h"
 
 #define STACK_TRACE_ENTRIES 500
@@ -352,8 +355,13 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 	return ret;
 }
 
+static char stack_trace_filter_buf[COMMAND_LINE_SIZE+1] __initdata;
+
 static __init int enable_stacktrace(char *str)
 {
+	if (strncmp(str, "_filter=", 8) == 0)
+		strncpy(stack_trace_filter_buf, str+8, COMMAND_LINE_SIZE);
+
 	stack_tracer_enabled = 1;
 	last_stack_tracer_enabled = 1;
 	return 1;
@@ -375,6 +383,9 @@ static __init int stack_trace_init(void)
 	trace_create_file("stack_trace_filter", 0444, d_tracer,
 			NULL, &stack_trace_filter_fops);
 
+	if (stack_trace_filter_buf[0])
+		ftrace_set_early_filter(&trace_ops, stack_trace_filter_buf, 1);
+
 	if (stack_tracer_enabled)
 		register_ftrace_function(&trace_ops);
 

commit d2d45c7a03a2b1a14159cbb665e9dd60991a7d4f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Dec 19 14:44:09 2011 -0500

    tracing: Have stack_tracer use a separate list of functions
    
    The stack_tracer is used to look at every function and check
    if the current stack is bigger than the last recorded max stack size.
    When a new max is found, then it saves that stack off.
    
    Currently the stack tracer is limited by the global_ops of
    the function tracer. As the stack tracer has nothing to do with
    the ftrace function tracer, except that it uses it as its internal
    engine, the stack tracer should have its own list.
    
    A new file is added to the tracing debugfs directory called:
    
      stack_trace_filter
    
    that can be used to select which functions you want to check the stack
    on.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 77575b386d97..0398b7c7afd6 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -133,7 +133,6 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip)
 static struct ftrace_ops trace_ops __read_mostly =
 {
 	.func = stack_trace_call,
-	.flags = FTRACE_OPS_FL_GLOBAL,
 };
 
 static ssize_t
@@ -311,6 +310,21 @@ static const struct file_operations stack_trace_fops = {
 	.release	= seq_release,
 };
 
+static int
+stack_trace_filter_open(struct inode *inode, struct file *file)
+{
+	return ftrace_regex_open(&trace_ops, FTRACE_ITER_FILTER,
+				 inode, file);
+}
+
+static const struct file_operations stack_trace_filter_fops = {
+	.open = stack_trace_filter_open,
+	.read = seq_read,
+	.write = ftrace_filter_write,
+	.llseek = ftrace_regex_lseek,
+	.release = ftrace_regex_release,
+};
+
 int
 stack_trace_sysctl(struct ctl_table *table, int write,
 		   void __user *buffer, size_t *lenp,
@@ -358,6 +372,9 @@ static __init int stack_trace_init(void)
 	trace_create_file("stack_trace", 0444, d_tracer,
 			NULL, &stack_trace_fops);
 
+	trace_create_file("stack_trace_filter", 0444, d_tracer,
+			NULL, &stack_trace_filter_fops);
+
 	if (stack_tracer_enabled)
 		register_ftrace_function(&trace_ops);
 

commit 22fe9b54d859e53bfbbbdc1a0a77a82bc453927c
Author: Peter Huewe <peterhuewe@gmx.de>
Date:   Tue Jun 7 21:58:27 2011 +0200

    tracing: Convert to kstrtoul_from_user
    
    This patch replaces the code for getting an unsigned long from a
    userspace buffer by a simple call to kstroul_from_user.
    This makes it easier to read and less error prone.
    
    Signed-off-by: Peter Huewe <peterhuewe@gmx.de>
    Link: http://lkml.kernel.org/r/1307476707-14762-1-git-send-email-peterhuewe@gmx.de
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index b0b53b8e4c25..77575b386d97 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -156,20 +156,11 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 {
 	long *ptr = filp->private_data;
 	unsigned long val, flags;
-	char buf[64];
 	int ret;
 	int cpu;
 
-	if (count >= sizeof(buf))
-		return -EINVAL;
-
-	if (copy_from_user(&buf, ubuf, count))
-		return -EFAULT;
-
-	buf[count] = 0;
-
-	ret = strict_strtoul(buf, 10, &val);
-	if (ret < 0)
+	ret = kstrtoul_from_user(ubuf, count, 10, &val);
+	if (ret)
 		return ret;
 
 	local_irq_save(flags);

commit b848914ce39589d89ee0078a6d1ef452b464729e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 4 09:27:52 2011 -0400

    ftrace: Implement separate user function filtering
    
    ftrace_ops that are registered to trace functions can now be
    agnostic to each other in respect to what functions they trace.
    Each ops has their own hash of the functions they want to trace
    and a hash to what they do not want to trace. A empty hash for
    the functions they want to trace denotes all functions should
    be traced that are not in the notrace hash.
    
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4c5dead0c239..b0b53b8e4c25 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -133,6 +133,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip)
 static struct ftrace_ops trace_ops __read_mostly =
 {
 	.func = stack_trace_call,
+	.flags = FTRACE_OPS_FL_GLOBAL,
 };
 
 static ssize_t

commit 6038f373a3dc1f1c26496e60b6c40b164716f07e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sun Aug 15 18:52:59 2010 +0200

    llseek: automatically add .llseek fop
    
    All file_operations should get a .llseek operation so we can make
    nonseekable_open the default for future file operations without a
    .llseek pointer.
    
    The three cases that we can automatically detect are no_llseek, seq_lseek
    and default_llseek. For cases where we can we can automatically prove that
    the file offset is always ignored, we use noop_llseek, which maintains
    the current behavior of not returning an error from a seek.
    
    New drivers should normally not use noop_llseek but instead use no_llseek
    and call nonseekable_open at open time.  Existing drivers can be converted
    to do the same when the maintainer knows for certain that no user code
    relies on calling seek on the device file.
    
    The generated code is often incorrectly indented and right now contains
    comments that clarify for each added line why a specific variant was
    chosen. In the version that gets submitted upstream, the comments will
    be gone and I will manually fix the indentation, because there does not
    seem to be a way to do that using coccinelle.
    
    Some amount of new code is currently sitting in linux-next that should get
    the same modifications, which I will do at the end of the merge window.
    
    Many thanks to Julia Lawall for helping me learn to write a semantic
    patch that does all this.
    
    ===== begin semantic patch =====
    // This adds an llseek= method to all file operations,
    // as a preparation for making no_llseek the default.
    //
    // The rules are
    // - use no_llseek explicitly if we do nonseekable_open
    // - use seq_lseek for sequential files
    // - use default_llseek if we know we access f_pos
    // - use noop_llseek if we know we don't access f_pos,
    //   but we still want to allow users to call lseek
    //
    @ open1 exists @
    identifier nested_open;
    @@
    nested_open(...)
    {
    <+...
    nonseekable_open(...)
    ...+>
    }
    
    @ open exists@
    identifier open_f;
    identifier i, f;
    identifier open1.nested_open;
    @@
    int open_f(struct inode *i, struct file *f)
    {
    <+...
    (
    nonseekable_open(...)
    |
    nested_open(...)
    )
    ...+>
    }
    
    @ read disable optional_qualifier exists @
    identifier read_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    expression E;
    identifier func;
    @@
    ssize_t read_f(struct file *f, char *p, size_t s, loff_t *off)
    {
    <+...
    (
       *off = E
    |
       *off += E
    |
       func(..., off, ...)
    |
       E = *off
    )
    ...+>
    }
    
    @ read_no_fpos disable optional_qualifier exists @
    identifier read_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    @@
    ssize_t read_f(struct file *f, char *p, size_t s, loff_t *off)
    {
    ... when != off
    }
    
    @ write @
    identifier write_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    expression E;
    identifier func;
    @@
    ssize_t write_f(struct file *f, const char *p, size_t s, loff_t *off)
    {
    <+...
    (
      *off = E
    |
      *off += E
    |
      func(..., off, ...)
    |
      E = *off
    )
    ...+>
    }
    
    @ write_no_fpos @
    identifier write_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    @@
    ssize_t write_f(struct file *f, const char *p, size_t s, loff_t *off)
    {
    ... when != off
    }
    
    @ fops0 @
    identifier fops;
    @@
    struct file_operations fops = {
     ...
    };
    
    @ has_llseek depends on fops0 @
    identifier fops0.fops;
    identifier llseek_f;
    @@
    struct file_operations fops = {
    ...
     .llseek = llseek_f,
    ...
    };
    
    @ has_read depends on fops0 @
    identifier fops0.fops;
    identifier read_f;
    @@
    struct file_operations fops = {
    ...
     .read = read_f,
    ...
    };
    
    @ has_write depends on fops0 @
    identifier fops0.fops;
    identifier write_f;
    @@
    struct file_operations fops = {
    ...
     .write = write_f,
    ...
    };
    
    @ has_open depends on fops0 @
    identifier fops0.fops;
    identifier open_f;
    @@
    struct file_operations fops = {
    ...
     .open = open_f,
    ...
    };
    
    // use no_llseek if we call nonseekable_open
    ////////////////////////////////////////////
    @ nonseekable1 depends on !has_llseek && has_open @
    identifier fops0.fops;
    identifier nso ~= "nonseekable_open";
    @@
    struct file_operations fops = {
    ...  .open = nso, ...
    +.llseek = no_llseek, /* nonseekable */
    };
    
    @ nonseekable2 depends on !has_llseek @
    identifier fops0.fops;
    identifier open.open_f;
    @@
    struct file_operations fops = {
    ...  .open = open_f, ...
    +.llseek = no_llseek, /* open uses nonseekable */
    };
    
    // use seq_lseek for sequential files
    /////////////////////////////////////
    @ seq depends on !has_llseek @
    identifier fops0.fops;
    identifier sr ~= "seq_read";
    @@
    struct file_operations fops = {
    ...  .read = sr, ...
    +.llseek = seq_lseek, /* we have seq_read */
    };
    
    // use default_llseek if there is a readdir
    ///////////////////////////////////////////
    @ fops1 depends on !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier readdir_e;
    @@
    // any other fop is used that changes pos
    struct file_operations fops = {
    ... .readdir = readdir_e, ...
    +.llseek = default_llseek, /* readdir is present */
    };
    
    // use default_llseek if at least one of read/write touches f_pos
    /////////////////////////////////////////////////////////////////
    @ fops2 depends on !fops1 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read.read_f;
    @@
    // read fops use offset
    struct file_operations fops = {
    ... .read = read_f, ...
    +.llseek = default_llseek, /* read accesses f_pos */
    };
    
    @ fops3 depends on !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier write.write_f;
    @@
    // write fops use offset
    struct file_operations fops = {
    ... .write = write_f, ...
    +       .llseek = default_llseek, /* write accesses f_pos */
    };
    
    // Use noop_llseek if neither read nor write accesses f_pos
    ///////////////////////////////////////////////////////////
    
    @ fops4 depends on !fops1 && !fops2 && !fops3 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read_no_fpos.read_f;
    identifier write_no_fpos.write_f;
    @@
    // write fops use offset
    struct file_operations fops = {
    ...
     .write = write_f,
     .read = read_f,
    ...
    +.llseek = noop_llseek, /* read and write both use no f_pos */
    };
    
    @ depends on has_write && !has_read && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier write_no_fpos.write_f;
    @@
    struct file_operations fops = {
    ... .write = write_f, ...
    +.llseek = noop_llseek, /* write uses no f_pos */
    };
    
    @ depends on has_read && !has_write && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read_no_fpos.read_f;
    @@
    struct file_operations fops = {
    ... .read = read_f, ...
    +.llseek = noop_llseek, /* read uses no f_pos */
    };
    
    @ depends on !has_read && !has_write && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    @@
    struct file_operations fops = {
    ...
    +.llseek = noop_llseek, /* no read or write fn */
    };
    ===== End semantic patch =====
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Julia Lawall <julia@diku.dk>
    Cc: Christoph Hellwig <hch@infradead.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index a6b7e0e0f3eb..4c5dead0c239 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -195,6 +195,7 @@ static const struct file_operations stack_max_size_fops = {
 	.open		= tracing_open_generic,
 	.read		= stack_max_size_read,
 	.write		= stack_max_size_write,
+	.llseek		= default_llseek,
 };
 
 static void *

commit 151772dbfad4dbe81721e40f9b3d588ea77bb7aa
Author: Anton Blanchard <anton@samba.org>
Date:   Wed Aug 25 11:32:38 2010 +1000

    tracing/trace_stack: Fix stack trace on ppc64
    
    save_stack_trace() stores the instruction pointer, not the
    function descriptor. On ppc64 the trace stack code currently
    dereferences the instruction pointer and shows 8 bytes of
    instructions in our backtraces:
    
     # cat /sys/kernel/debug/tracing/stack_trace
            Depth    Size   Location    (26 entries)
            -----    ----   --------
      0)     5424     112   0x6000000048000004
      1)     5312     160   0x60000000ebad01b0
      2)     5152     160   0x2c23000041c20030
      3)     4992     240   0x600000007c781b79
      4)     4752     160   0xe84100284800000c
      5)     4592     192   0x600000002fa30000
      6)     4400     256   0x7f1800347b7407e0
      7)     4144     208   0xe89f0108f87f0070
      8)     3936     272   0xe84100282fa30000
    
    Since we aren't dealing with function descriptors, use %pS
    instead of %pF to fix it:
    
     # cat /sys/kernel/debug/tracing/stack_trace
            Depth    Size   Location    (26 entries)
            -----    ----   --------
      0)     5424     112   ftrace_call+0x4/0x8
      1)     5312     160   .current_io_context+0x28/0x74
      2)     5152     160   .get_io_context+0x48/0xa0
      3)     4992     240   .cfq_set_request+0x94/0x4c4
      4)     4752     160   .elv_set_request+0x60/0x84
      5)     4592     192   .get_request+0x2d4/0x468
      6)     4400     256   .get_request_wait+0x7c/0x258
      7)     4144     208   .__make_request+0x49c/0x610
      8)     3936     272   .generic_make_request+0x390/0x434
    
    Signed-off-by: Anton Blanchard <anton@samba.org>
    Cc: rostedt@goodmis.org
    Cc: fweisbec@gmail.com
    LKML-Reference: <20100825013238.GE28360@kryten>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 056468eae7cf..a6b7e0e0f3eb 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -249,7 +249,7 @@ static int trace_lookup_stack(struct seq_file *m, long i)
 {
 	unsigned long addr = stack_dump_trace[i];
 
-	return seq_printf(m, "%pF\n", (void *)addr);
+	return seq_printf(m, "%pS\n", (void *)addr);
 }
 
 static void print_disabled(struct seq_file *m)

commit 5168ae50a66e3ff7184c2b16d661bd6d70367e50
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 3 09:36:50 2010 -0400

    tracing: Remove ftrace_preempt_disable/enable
    
    The ftrace_preempt_disable/enable functions were to address a
    recursive race caused by the function tracer. The function tracer
    traces all functions which makes it easily susceptible to recursion.
    One area was preempt_enable(). This would call the scheduler and
    the schedulre would call the function tracer and loop.
    (So was it thought).
    
    The ftrace_preempt_disable/enable was made to protect against recursion
    inside the scheduler by storing the NEED_RESCHED flag. If it was
    set before the ftrace_preempt_disable() it would not call schedule
    on ftrace_preempt_enable(), thinking that if it was set before then
    it would have already scheduled unless it was already in the scheduler.
    
    This worked fine except in the case of SMP, where another task would set
    the NEED_RESCHED flag for a task on another CPU, and then kick off an
    IPI to trigger it. This could cause the NEED_RESCHED to be saved at
    ftrace_preempt_disable() but the IPI to arrive in the the preempt
    disabled section. The ftrace_preempt_enable() would not call the scheduler
    because the flag was already set before entring the section.
    
    This bug would cause a missed preemption check and cause lower latencies.
    
    Investigating further, I found that the recusion caused by the function
    tracer was not due to schedule(), but due to preempt_schedule(). Now
    that preempt_schedule is completely annotated with notrace, the recusion
    no longer is an issue.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index f4bc9b27de5f..056468eae7cf 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -110,12 +110,12 @@ static inline void check_stack(void)
 static void
 stack_trace_call(unsigned long ip, unsigned long parent_ip)
 {
-	int cpu, resched;
+	int cpu;
 
 	if (unlikely(!ftrace_enabled || stack_trace_disabled))
 		return;
 
-	resched = ftrace_preempt_disable();
+	preempt_disable_notrace();
 
 	cpu = raw_smp_processor_id();
 	/* no atomic needed, we only modify this variable by this cpu */
@@ -127,7 +127,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip)
  out:
 	per_cpu(trace_active, cpu)--;
 	/* prevent recursion in schedule */
-	ftrace_preempt_enable(resched);
+	preempt_enable_notrace();
 }
 
 static struct ftrace_ops trace_ops __read_mostly =

commit 4f48f8b7fd18c44f8478174f9925cc3c059c6ce4
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Feb 2 15:32:09 2010 +0800

    tracing: Fix circular dead lock in stack trace
    
    When we cat <debugfs>/tracing/stack_trace, we may cause circular lock:
    sys_read()
      t_start()
         arch_spin_lock(&max_stack_lock);
    
      t_show()
         seq_printf(), vsnprintf() .... /* they are all trace-able,
           when they are traced, max_stack_lock may be required again. */
    
    The following script can trigger this circular dead lock very easy:
    #!/bin/bash
    
    echo 1 > /proc/sys/kernel/stack_tracer_enabled
    
    mount -t debugfs xxx /mnt > /dev/null 2>&1
    
    (
    # make check_stack() zealous to require max_stack_lock
    for ((; ;))
    {
            echo 1 > /mnt/tracing/stack_max_size
    }
    ) &
    
    for ((; ;))
    {
            cat /mnt/tracing/stack_trace > /dev/null
    }
    
    To fix this bug, we increase the percpu trace_active before
    require the lock.
    
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <4B67D4F9.9080905@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 678a5120ee30..f4bc9b27de5f 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -157,6 +157,7 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	unsigned long val, flags;
 	char buf[64];
 	int ret;
+	int cpu;
 
 	if (count >= sizeof(buf))
 		return -EINVAL;
@@ -171,9 +172,20 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 		return ret;
 
 	local_irq_save(flags);
+
+	/*
+	 * In case we trace inside arch_spin_lock() or after (NMI),
+	 * we will cause circular lock, so we also need to increase
+	 * the percpu trace_active here.
+	 */
+	cpu = smp_processor_id();
+	per_cpu(trace_active, cpu)++;
+
 	arch_spin_lock(&max_stack_lock);
 	*ptr = val;
 	arch_spin_unlock(&max_stack_lock);
+
+	per_cpu(trace_active, cpu)--;
 	local_irq_restore(flags);
 
 	return count;
@@ -206,7 +218,13 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
+	int cpu;
+
 	local_irq_disable();
+
+	cpu = smp_processor_id();
+	per_cpu(trace_active, cpu)++;
+
 	arch_spin_lock(&max_stack_lock);
 
 	if (*pos == 0)
@@ -217,7 +235,13 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 
 static void t_stop(struct seq_file *m, void *p)
 {
+	int cpu;
+
 	arch_spin_unlock(&max_stack_lock);
+
+	cpu = smp_processor_id();
+	per_cpu(trace_active, cpu)--;
+
 	local_irq_enable();
 }
 

commit 0199c4e68d1f02894bdefe4b5d9e9ee4aedd8d62
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 2 20:01:25 2009 +0100

    locking: Convert __raw_spin* functions to arch_spin*
    
    Name space cleanup. No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: linux-arch@vger.kernel.org

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 728c35221483..678a5120ee30 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -54,7 +54,7 @@ static inline void check_stack(void)
 		return;
 
 	local_irq_save(flags);
-	__raw_spin_lock(&max_stack_lock);
+	arch_spin_lock(&max_stack_lock);
 
 	/* a race could have already updated it */
 	if (this_size <= max_stack_size)
@@ -103,7 +103,7 @@ static inline void check_stack(void)
 	}
 
  out:
-	__raw_spin_unlock(&max_stack_lock);
+	arch_spin_unlock(&max_stack_lock);
 	local_irq_restore(flags);
 }
 
@@ -171,9 +171,9 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 		return ret;
 
 	local_irq_save(flags);
-	__raw_spin_lock(&max_stack_lock);
+	arch_spin_lock(&max_stack_lock);
 	*ptr = val;
-	__raw_spin_unlock(&max_stack_lock);
+	arch_spin_unlock(&max_stack_lock);
 	local_irq_restore(flags);
 
 	return count;
@@ -207,7 +207,7 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
 	local_irq_disable();
-	__raw_spin_lock(&max_stack_lock);
+	arch_spin_lock(&max_stack_lock);
 
 	if (*pos == 0)
 		return SEQ_START_TOKEN;
@@ -217,7 +217,7 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 
 static void t_stop(struct seq_file *m, void *p)
 {
-	__raw_spin_unlock(&max_stack_lock);
+	arch_spin_unlock(&max_stack_lock);
 	local_irq_enable();
 }
 

commit edc35bd72e2079b25f99c5da7d7a65dbbffc4a26
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Dec 3 12:38:57 2009 +0100

    locking: Rename __RAW_SPIN_LOCK_UNLOCKED to __ARCH_SPIN_LOCK_UNLOCKED
    
    Further name space cleanup. No functional change
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: linux-arch@vger.kernel.org

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 9a82d568fdec..728c35221483 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -28,7 +28,7 @@ static struct stack_trace max_stack_trace = {
 
 static unsigned long max_stack_size;
 static arch_spinlock_t max_stack_lock =
-	(arch_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+	(arch_spinlock_t)__ARCH_SPIN_LOCK_UNLOCKED;
 
 static int stack_trace_disabled __read_mostly;
 static DEFINE_PER_CPU(int, trace_active);

commit 445c89514be242b1b0080056d50bdc1b72adeb5c
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 2 19:49:50 2009 +0100

    locking: Convert raw_spinlock to arch_spinlock
    
    The raw_spin* namespace was taken by lockdep for the architecture
    specific implementations. raw_spin_* would be the ideal name space for
    the spinlocks which are not converted to sleeping locks in preempt-rt.
    
    Linus suggested to convert the raw_ to arch_ locks and cleanup the
    name space instead of using an artifical name like core_spin,
    atomic_spin or whatever
    
    No functional change.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: linux-arch@vger.kernel.org

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 8504ac71e4e8..9a82d568fdec 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -27,8 +27,8 @@ static struct stack_trace max_stack_trace = {
 };
 
 static unsigned long max_stack_size;
-static raw_spinlock_t max_stack_lock =
-	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+static arch_spinlock_t max_stack_lock =
+	(arch_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
 
 static int stack_trace_disabled __read_mostly;
 static DEFINE_PER_CPU(int, trace_active);

commit 8d65af789f3e2cf4cfbdbf71a0f7a61ebcd41d38
Author: Alexey Dobriyan <adobriyan@gmail.com>
Date:   Wed Sep 23 15:57:19 2009 -0700

    sysctl: remove "struct file *" argument of ->proc_handler
    
    It's unused.
    
    It isn't needed -- read or write flag is already passed and sysctl
    shouldn't care about the rest.
    
    It _was_ used in two places at arch/frv for some reason.
    
    Signed-off-by: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: David Howells <dhowells@redhat.com>
    Cc: "Eric W. Biederman" <ebiederm@xmission.com>
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Cc: Ralf Baechle <ralf@linux-mips.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: "David S. Miller" <davem@davemloft.net>
    Cc: James Morris <jmorris@namei.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 0f6facb050a1..8504ac71e4e8 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -296,14 +296,14 @@ static const struct file_operations stack_trace_fops = {
 
 int
 stack_trace_sysctl(struct ctl_table *table, int write,
-		   struct file *file, void __user *buffer, size_t *lenp,
+		   void __user *buffer, size_t *lenp,
 		   loff_t *ppos)
 {
 	int ret;
 
 	mutex_lock(&stack_sysctl_mutex);
 
-	ret = proc_dointvec(table, write, file, buffer, lenp, ppos);
+	ret = proc_dointvec(table, write, buffer, lenp, ppos);
 
 	if (ret || !write ||
 	    (last_stack_tracer_enabled == !!stack_tracer_enabled))

commit 2fc5f0cff4cf1c4cd336d0f61f11bca6eeee1d84
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Aug 17 16:53:37 2009 +0800

    trace_stack: Simplify seqfile code
    
    Extract duplicate code in t_start() and t_next().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A891A91.4030602@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 0da1cff08d67..0f6facb050a1 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -186,43 +186,33 @@ static const struct file_operations stack_max_size_fops = {
 };
 
 static void *
-t_next(struct seq_file *m, void *v, loff_t *pos)
+__next(struct seq_file *m, loff_t *pos)
 {
-	long i;
+	long n = *pos - 1;
 
-	(*pos)++;
-
-	if (v == SEQ_START_TOKEN)
-		i = 0;
-	else {
-		i = *(long *)v;
-		i++;
-	}
-
-	if (i >= max_stack_trace.nr_entries ||
-	    stack_dump_trace[i] == ULONG_MAX)
+	if (n >= max_stack_trace.nr_entries || stack_dump_trace[n] == ULONG_MAX)
 		return NULL;
 
-	m->private = (void *)i;
-
+	m->private = (void *)n;
 	return &m->private;
 }
 
-static void *t_start(struct seq_file *m, loff_t *pos)
+static void *
+t_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	void *t = SEQ_START_TOKEN;
-	loff_t l = 0;
+	(*pos)++;
+	return __next(m, pos);
+}
 
+static void *t_start(struct seq_file *m, loff_t *pos)
+{
 	local_irq_disable();
 	__raw_spin_lock(&max_stack_lock);
 
 	if (*pos == 0)
 		return SEQ_START_TOKEN;
 
-	for (; t && l < *pos; t = t_next(m, t, &l))
-		;
-
-	return t;
+	return __next(m, pos);
 }
 
 static void t_stop(struct seq_file *m, void *p)

commit 89034bc2c7b839702c00a704e79d112737f98be0
Merge: fb82ad719831 85dfd81dc57e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Aug 11 14:19:09 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Conflicts:
            kernel/trace/trace_events_filter.c
    
    We use the tracing/core version.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit d8cc1ab793993c886c62abf77c93287df33ffd8b
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Jul 23 11:28:40 2009 +0800

    trace_stack: Fix seqfile memory leak
    
    Every time we cat stack_trace, we leak memory allocated by seq_open().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4A67D8E8.3020500@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index e644af910124..6a2a9d484cd6 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -301,17 +301,14 @@ static const struct seq_operations stack_trace_seq_ops = {
 
 static int stack_trace_open(struct inode *inode, struct file *file)
 {
-	int ret;
-
-	ret = seq_open(file, &stack_trace_seq_ops);
-
-	return ret;
+	return seq_open(file, &stack_trace_seq_ops);
 }
 
 static const struct file_operations stack_trace_fops = {
 	.open		= stack_trace_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
+	.release	= seq_release,
 };
 
 int

commit 79173bf556417a737e9d2e096e0788452ec30a61
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Jul 16 14:17:11 2009 +0800

    tracing/trace_stack: Cleanup for trace_lookup_stack()
    
    We can directly use %pF input format instead of sprint_symbol()
    and %s input format.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index e644af910124..a4dc8d9ad1b1 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -234,15 +234,8 @@ static void t_stop(struct seq_file *m, void *p)
 static int trace_lookup_stack(struct seq_file *m, long i)
 {
 	unsigned long addr = stack_dump_trace[i];
-#ifdef CONFIG_KALLSYMS
-	char str[KSYM_SYMBOL_LEN];
 
-	sprint_symbol(str, addr);
-
-	return seq_printf(m, "%s\n", str);
-#else
-	return seq_printf(m, "%p\n", (void*)addr);
-#endif
+	return seq_printf(m, "%pF\n", (void *)addr);
 }
 
 static void print_disabled(struct seq_file *m)

commit a32c7765e2796395aec49f699bd25c407155e9c5
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Jun 26 16:55:51 2009 +0800

    tracing: Fix stack tracer sysctl handling
    
    This made my machine completely frozen:
    
      # echo 1 > /proc/sys/kernel/stack_tracer_enabled
      # echo 2 > /proc/sys/kernel/stack_tracer_enabled
    
    The cause is register_ftrace_function() was called twice.
    
    Also fix ftrace_enabled sysctl, though seems nothing bad happened
    as I tested it.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A448D17.9010305@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 2d7aebd71dbd..e644af910124 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -326,10 +326,10 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 	ret = proc_dointvec(table, write, file, buffer, lenp, ppos);
 
 	if (ret || !write ||
-	    (last_stack_tracer_enabled == stack_tracer_enabled))
+	    (last_stack_tracer_enabled == !!stack_tracer_enabled))
 		goto out;
 
-	last_stack_tracer_enabled = stack_tracer_enabled;
+	last_stack_tracer_enabled = !!stack_tracer_enabled;
 
 	if (stack_tracer_enabled)
 		register_ftrace_function(&trace_ops);

commit 083a63b48e4dd0a6a2d44216720076dc81ebb255
Author: walimis <walimisdev@gmail.com>
Date:   Wed Jun 3 16:01:28 2009 +0800

    tracing/trace_stack: fix the number of entries in the header
    
    The last entry in the stack_dump_trace is ULONG_MAX, which is not
    a valid entry, but max_stack_trace.nr_entries has accounted for it.
    So when printing the header, we should decrease it by one.
    Before fix, print as following, for example:
    
            Depth    Size   Location    (53 entries)        <--- should be 52
            -----    ----   --------
      0)     3264     108   update_wall_time+0x4d5/0x9a0
      ...
     51)       80      80   syscall_call+0x7/0xb
     ^^^
       it's correct.
    
    Signed-off-by: walimis <walimisdev@gmail.com>
    LKML-Reference: <1244016090-7814-1-git-send-email-walimisdev@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 1796f00524e1..2d7aebd71dbd 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -265,7 +265,7 @@ static int t_show(struct seq_file *m, void *v)
 		seq_printf(m, "        Depth    Size   Location"
 			   "    (%d entries)\n"
 			   "        -----    ----   --------\n",
-			   max_stack_trace.nr_entries);
+			   max_stack_trace.nr_entries - 1);
 
 		if (!stack_tracer_enabled && !max_stack_size)
 			print_disabled(m);

commit 5452af664f6fba26b80eb2c8c4ceae2999d5cf56
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 27 00:25:38 2009 +0100

    tracing/ftrace: factorize the tracing files creation
    
    Impact: cleanup
    
    Most of the tracing files creation follow the same pattern:
    
    ret = debugfs_create_file(...)
    if (!ret)
            pr_warning("Couldn't create ... entry\n")
    
    Unify it!
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1238109938-11840-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index c750f65f9661..1796f00524e1 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -352,19 +352,14 @@ __setup("stacktrace", enable_stacktrace);
 static __init int stack_trace_init(void)
 {
 	struct dentry *d_tracer;
-	struct dentry *entry;
 
 	d_tracer = tracing_init_dentry();
 
-	entry = debugfs_create_file("stack_max_size", 0644, d_tracer,
-				    &max_stack_size, &stack_max_size_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs 'stack_max_size' entry\n");
+	trace_create_file("stack_max_size", 0644, d_tracer,
+			&max_stack_size, &stack_max_size_fops);
 
-	entry = debugfs_create_file("stack_trace", 0444, d_tracer,
-				    NULL, &stack_trace_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs 'stack_trace' entry\n");
+	trace_create_file("stack_trace", 0444, d_tracer,
+			NULL, &stack_trace_fops);
 
 	if (stack_tracer_enabled)
 		register_ftrace_function(&trace_ops);

commit eb1871f34358024acfa3523ef375ef14b7527173
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Mar 13 00:00:58 2009 -0400

    tracing: left align location header in stack_trace
    
    Ingo Molnar suggested, instead of:
    
            Depth    Size      Location    (27 entries)
            -----    ----      --------
      0)     2880      48   lock_timer_base+0x2b/0x4f
      1)     2832      80   __mod_timer+0x33/0xe0
      2)     2752      16   __ide_set_handler+0x63/0x65
    
    To have it be:
    
            Depth    Size   Location    (27 entries)
            -----    ----   --------
      0)     2880      48   lock_timer_base+0x2b/0x4f
      1)     2832      80   __mod_timer+0x33/0xe0
      2)     2752      16   __ide_set_handler+0x63/0x65
    
    Requested-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 91ccbf396c9a..c750f65f9661 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -262,9 +262,9 @@ static int t_show(struct seq_file *m, void *v)
 	int size;
 
 	if (v == SEQ_START_TOKEN) {
-		seq_printf(m, "        Depth    Size      Location"
+		seq_printf(m, "        Depth    Size   Location"
 			   "    (%d entries)\n"
-			   "        -----    ----      --------\n",
+			   "        -----    ----   --------\n",
 			   max_stack_trace.nr_entries);
 
 		if (!stack_tracer_enabled && !max_stack_size)

commit e447e1df2e568cd43d1918963c9f09fae85aea57
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 12 19:42:29 2009 -0400

    tracing: explain why stack tracer is empty
    
    If the stack tracing is disabled (by default) the stack_trace file
    will only contain the header:
    
     # cat /debug/tracing/stack_trace
            Depth    Size      Location    (0 entries)
            -----    ----      --------
    
    This can be frustrating to a developer that does not realize that the
    stack tracer is disabled. This patch adds the following text:
    
      # cat /debug/tracing/stack_trace
            Depth    Size      Location    (0 entries)
            -----    ----      --------
     #
     #  Stack tracer disabled
     #
     # To enable the stack tracer, either add 'stacktrace' to the
     # kernel command line
     # or 'echo 1 > /proc/sys/kernel/stack_tracer_enabled'
     #
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4564fd94b0cd..91ccbf396c9a 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -245,6 +245,17 @@ static int trace_lookup_stack(struct seq_file *m, long i)
 #endif
 }
 
+static void print_disabled(struct seq_file *m)
+{
+	seq_puts(m, "#\n"
+		 "#  Stack tracer disabled\n"
+		 "#\n"
+		 "# To enable the stack tracer, either add 'stacktrace' to the\n"
+		 "# kernel command line\n"
+		 "# or 'echo 1 > /proc/sys/kernel/stack_tracer_enabled'\n"
+		 "#\n");
+}
+
 static int t_show(struct seq_file *m, void *v)
 {
 	long i;
@@ -255,6 +266,10 @@ static int t_show(struct seq_file *m, void *v)
 			   "    (%d entries)\n"
 			   "        -----    ----      --------\n",
 			   max_stack_trace.nr_entries);
+
+		if (!stack_tracer_enabled && !max_stack_size)
+			print_disabled(m);
+
 		return 0;
 	}
 

commit 2da03ecee6308ea174e8a02b92a3c4ec92e886c8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 12 18:57:51 2009 -0400

    tracing: fix stack tracer header
    
    The stack tracer use to look like this:
    
     # cat /debug/tracing/stack_trace
             Depth  Size      Location    (57 entries)
             -----  ----      --------
      0)     5088      16   mempool_alloc_slab+0x16/0x18
      1)     5072     144   mempool_alloc+0x4d/0xfe
      2)     4928      16   scsi_sg_alloc+0x48/0x4a [scsi_mod]
    
    Now it looks like this:
    
     # cat /debug/tracing/stack_trace
    
            Depth    Size      Location    (57 entries)
            -----    ----      --------
      0)     5088      16   mempool_alloc_slab+0x16/0x18
      1)     5072     144   mempool_alloc+0x4d/0xfe
      2)     4928      16   scsi_sg_alloc+0x48/0x4a [scsi_mod]
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index d0871bc0aca5..4564fd94b0cd 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -251,9 +251,9 @@ static int t_show(struct seq_file *m, void *v)
 	int size;
 
 	if (v == SEQ_START_TOKEN) {
-		seq_printf(m, "        Depth   Size      Location"
+		seq_printf(m, "        Depth    Size      Location"
 			   "    (%d entries)\n"
-			   "        -----   ----      --------\n",
+			   "        -----    ----      --------\n",
 			   max_stack_trace.nr_entries);
 		return 0;
 	}

commit e05a43b744fb9518cbf8539a7ef33164ac60a70f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Dec 17 09:43:00 2008 -0500

    trace: better use of stack_trace_enabled for boot up code
    
    Impact: clean up
    
    Andrew Morton suggested to use the stack_tracer_enabled variable
    to decide whether or not to start stack tracing on bootup.
    This lets us remove the start_stack_trace variable.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4842c969c785..d0871bc0aca5 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -308,7 +308,7 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 
 	mutex_lock(&stack_sysctl_mutex);
 
-	ret  = proc_dointvec(table, write, file, buffer, lenp, ppos);
+	ret = proc_dointvec(table, write, file, buffer, lenp, ppos);
 
 	if (ret || !write ||
 	    (last_stack_tracer_enabled == stack_tracer_enabled))
@@ -326,11 +326,10 @@ stack_trace_sysctl(struct ctl_table *table, int write,
 	return ret;
 }
 
-static int start_stack_trace __initdata;
-
 static __init int enable_stacktrace(char *str)
 {
-	start_stack_trace = 1;
+	stack_tracer_enabled = 1;
+	last_stack_tracer_enabled = 1;
 	return 1;
 }
 __setup("stacktrace", enable_stacktrace);
@@ -352,10 +351,8 @@ static __init int stack_trace_init(void)
 	if (!entry)
 		pr_warning("Could not create debugfs 'stack_trace' entry\n");
 
-	if (start_stack_trace) {
+	if (stack_tracer_enabled)
 		register_ftrace_function(&trace_ops);
-		stack_tracer_enabled = 1;
-	}
 
 	return 0;
 }

commit f38f1d2aa5a3520cf05da7cd6bd12fe2b0c509b7
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 16 23:06:40 2008 -0500

    trace: add a way to enable or disable the stack tracer
    
    Impact: enhancement to stack tracer
    
    The stack tracer currently is either on when configured in or
    off when it is not. It can not be disabled when it is configured on.
    (besides disabling the function tracer that it uses)
    
    This patch adds a way to enable or disable the stack tracer at
    run time. It defaults off on bootup, but a kernel parameter 'stacktrace'
    has been added to enable it on bootup.
    
    A new sysctl has been added "kernel.stack_tracer_enabled" to let
    the user enable or disable the stack tracer at run time.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 0b863f2cbc8e..4842c969c785 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -10,6 +10,7 @@
 #include <linux/debugfs.h>
 #include <linux/ftrace.h>
 #include <linux/module.h>
+#include <linux/sysctl.h>
 #include <linux/init.h>
 #include <linux/fs.h>
 #include "trace.h"
@@ -31,6 +32,10 @@ static raw_spinlock_t max_stack_lock =
 
 static int stack_trace_disabled __read_mostly;
 static DEFINE_PER_CPU(int, trace_active);
+static DEFINE_MUTEX(stack_sysctl_mutex);
+
+int stack_tracer_enabled;
+static int last_stack_tracer_enabled;
 
 static inline void check_stack(void)
 {
@@ -174,7 +179,7 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	return count;
 }
 
-static struct file_operations stack_max_size_fops = {
+static const struct file_operations stack_max_size_fops = {
 	.open		= tracing_open_generic,
 	.read		= stack_max_size_read,
 	.write		= stack_max_size_write,
@@ -272,7 +277,7 @@ static int t_show(struct seq_file *m, void *v)
 	return 0;
 }
 
-static struct seq_operations stack_trace_seq_ops = {
+static const struct seq_operations stack_trace_seq_ops = {
 	.start		= t_start,
 	.next		= t_next,
 	.stop		= t_stop,
@@ -288,12 +293,48 @@ static int stack_trace_open(struct inode *inode, struct file *file)
 	return ret;
 }
 
-static struct file_operations stack_trace_fops = {
+static const struct file_operations stack_trace_fops = {
 	.open		= stack_trace_open,
 	.read		= seq_read,
 	.llseek		= seq_lseek,
 };
 
+int
+stack_trace_sysctl(struct ctl_table *table, int write,
+		   struct file *file, void __user *buffer, size_t *lenp,
+		   loff_t *ppos)
+{
+	int ret;
+
+	mutex_lock(&stack_sysctl_mutex);
+
+	ret  = proc_dointvec(table, write, file, buffer, lenp, ppos);
+
+	if (ret || !write ||
+	    (last_stack_tracer_enabled == stack_tracer_enabled))
+		goto out;
+
+	last_stack_tracer_enabled = stack_tracer_enabled;
+
+	if (stack_tracer_enabled)
+		register_ftrace_function(&trace_ops);
+	else
+		unregister_ftrace_function(&trace_ops);
+
+ out:
+	mutex_unlock(&stack_sysctl_mutex);
+	return ret;
+}
+
+static int start_stack_trace __initdata;
+
+static __init int enable_stacktrace(char *str)
+{
+	start_stack_trace = 1;
+	return 1;
+}
+__setup("stacktrace", enable_stacktrace);
+
 static __init int stack_trace_init(void)
 {
 	struct dentry *d_tracer;
@@ -311,7 +352,10 @@ static __init int stack_trace_init(void)
 	if (!entry)
 		pr_warning("Could not create debugfs 'stack_trace' entry\n");
 
-	register_ftrace_function(&trace_ops);
+	if (start_stack_trace) {
+		register_ftrace_function(&trace_ops);
+		stack_tracer_enabled = 1;
+	}
 
 	return 0;
 }

commit 0a37119d963e876ca86912497346ec50dea2541b
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Dec 3 11:04:50 2008 -0500

    trace: fix output of stack trace
    
    Impact: fix to output of stack trace
    
    If a function is not found in the stack of the stack tracer, the
    number printed is quite strange. This fixes the algorithm to handle
    missing functions better.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 06a16115be0f..0b863f2cbc8e 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -78,6 +78,7 @@ static inline void check_stack(void)
 	 * on a new max, so it is far from a fast path.
 	 */
 	while (i < max_stack_trace.nr_entries) {
+		int found = 0;
 
 		stack_dump_index[i] = this_size;
 		p = start;
@@ -86,12 +87,14 @@ static inline void check_stack(void)
 			if (*p == stack_dump_trace[i]) {
 				this_size = stack_dump_index[i++] =
 					(top - p) * sizeof(unsigned long);
+				found = 1;
 				/* Start the search from here */
 				start = p + 1;
 			}
 		}
 
-		i++;
+		if (!found)
+			i++;
 	}
 
  out:

commit a5e25883a445dce94a087ca479b21a5959cd5c18
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 2 15:34:05 2008 -0500

    ftrace: replace raw_local_irq_save with local_irq_save
    
    Impact: fix for lockdep and ftrace
    
    The raw_local_irq_save/restore confuses lockdep. This patch
    converts them to the local_irq_save/restore variants.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index fde3be15c642..06a16115be0f 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -48,7 +48,7 @@ static inline void check_stack(void)
 	if (!object_is_on_stack(&this_size))
 		return;
 
-	raw_local_irq_save(flags);
+	local_irq_save(flags);
 	__raw_spin_lock(&max_stack_lock);
 
 	/* a race could have already updated it */
@@ -96,7 +96,7 @@ static inline void check_stack(void)
 
  out:
 	__raw_spin_unlock(&max_stack_lock);
-	raw_local_irq_restore(flags);
+	local_irq_restore(flags);
 }
 
 static void
@@ -162,11 +162,11 @@ stack_max_size_write(struct file *filp, const char __user *ubuf,
 	if (ret < 0)
 		return ret;
 
-	raw_local_irq_save(flags);
+	local_irq_save(flags);
 	__raw_spin_lock(&max_stack_lock);
 	*ptr = val;
 	__raw_spin_unlock(&max_stack_lock);
-	raw_local_irq_restore(flags);
+	local_irq_restore(flags);
 
 	return count;
 }

commit a0a70c735ef714fe1b6777b571630c3d50c7b008
Merge: 9676e73a9e0c 60a515132086 0231022cc32d 522a110b42b3
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Nov 23 09:10:32 2008 +0100

    Merge branches 'tracing/profiling', 'tracing/options' and 'tracing/urgent' into tracing/core

commit 522a110b42b306d696cf84e34c677ed0e7080194
Author: Liming Wang <liming.wang@windriver.com>
Date:   Fri Nov 21 11:00:18 2008 +0800

    function tracing: fix wrong position computing of stack_trace
    
    Impact: make output of stack_trace complete if buffer overruns
    
    When read buffer overruns, the output of stack_trace isn't complete.
    
    When printing records with seq_printf in t_show, if the read buffer
    has overruned by the current record, then this record won't be
    printed to user space through read buffer, it will just be dropped in
    this printing.
    
    When next printing, t_start should return the "*pos"th record, which
    is the one dropped by previous printing, but it just returns
    (m->private + *pos)th record.
    
    Here we use a more sane method to implement seq_operations which can
    be found in kernel code. Thus we needn't initialize m->private.
    
    About testing, it's not easy to overrun read buffer, but we can use
    seq_printf to print more padding bytes in t_show, then it's easy to
    check whether or not records are lost.
    
    This commit has been tested on both condition of overrun and non
    overrun.
    
    Signed-off-by: Liming Wang <liming.wang@windriver.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index be682b62fe58..3bdb44bde4b7 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -184,11 +184,16 @@ static struct file_operations stack_max_size_fops = {
 static void *
 t_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	long i = (long)m->private;
+	long i;
 
 	(*pos)++;
 
-	i++;
+	if (v == SEQ_START_TOKEN)
+		i = 0;
+	else {
+		i = *(long *)v;
+		i++;
+	}
 
 	if (i >= max_stack_trace.nr_entries ||
 	    stack_dump_trace[i] == ULONG_MAX)
@@ -201,12 +206,15 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
-	void *t = &m->private;
+	void *t = SEQ_START_TOKEN;
 	loff_t l = 0;
 
 	local_irq_disable();
 	__raw_spin_lock(&max_stack_lock);
 
+	if (*pos == 0)
+		return SEQ_START_TOKEN;
+
 	for (; t && l < *pos; t = t_next(m, t, &l))
 		;
 
@@ -235,10 +243,10 @@ static int trace_lookup_stack(struct seq_file *m, long i)
 
 static int t_show(struct seq_file *m, void *v)
 {
-	long i = *(long *)v;
+	long i;
 	int size;
 
-	if (i < 0) {
+	if (v == SEQ_START_TOKEN) {
 		seq_printf(m, "        Depth   Size      Location"
 			   "    (%d entries)\n"
 			   "        -----   ----      --------\n",
@@ -246,6 +254,8 @@ static int t_show(struct seq_file *m, void *v)
 		return 0;
 	}
 
+	i = *(long *)v;
+
 	if (i >= max_stack_trace.nr_entries ||
 	    stack_dump_trace[i] == ULONG_MAX)
 		return 0;
@@ -275,10 +285,6 @@ static int stack_trace_open(struct inode *inode, struct file *file)
 	int ret;
 
 	ret = seq_open(file, &stack_trace_seq_ops);
-	if (!ret) {
-		struct seq_file *m = file->private_data;
-		m->private = (void *)-1;
-	}
 
 	return ret;
 }

commit 182e9f5f704ed6b9175142fe8da33c9ce0c52b52
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Nov 3 23:15:56 2008 -0500

    ftrace: insert in the ftrace_preempt_disable()/enable() functions
    
    Impact: use new, consolidated APIs in ftrace plugins
    
    This patch replaces the schedule safe preempt disable code with the
    ftrace_preempt_disable() and ftrace_preempt_enable() safe functions.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index be682b62fe58..d39e8b7de6a2 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -107,8 +107,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip)
 	if (unlikely(!ftrace_enabled || stack_trace_disabled))
 		return;
 
-	resched = need_resched();
-	preempt_disable_notrace();
+	resched = ftrace_preempt_disable();
 
 	cpu = raw_smp_processor_id();
 	/* no atomic needed, we only modify this variable by this cpu */
@@ -120,10 +119,7 @@ stack_trace_call(unsigned long ip, unsigned long parent_ip)
  out:
 	per_cpu(trace_active, cpu)--;
 	/* prevent recursion in schedule */
-	if (resched)
-		preempt_enable_no_resched_notrace();
-	else
-		preempt_enable_notrace();
+	ftrace_preempt_enable(resched);
 }
 
 static struct ftrace_ops trace_ops __read_mostly =

commit 81520a1b0649d0701205b818714a8c1e1cfbbb5b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Oct 6 21:24:18 2008 -0400

    ftrace: stack tracer only record when on stack
    
    The stack trace API does not record if the stack is not on the current
    task's stack. That is, if the stack is the interrupt stack or NMI stack,
    the output does not show. Also, the size of those stacks are not
    consistent with the size of the thread stack, this makes the calculation
    of the stack size usually bogus.
    
    This all confuses the stack tracer. I unfortunately do not have time to
    fix all these problems, but this patch does record the worst stack when
    the stack pointer is on the tasks stack (instead of bogus numbers).
    
    The patch simply returns if the stack pointer is not on the task's stack.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 74c5d9a3afae..be682b62fe58 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -44,6 +44,10 @@ static inline void check_stack(void)
 	if (this_size <= max_stack_size)
 		return;
 
+	/* we do not handle interrupt stacks yet */
+	if (!object_is_on_stack(&this_size))
+		return;
+
 	raw_local_irq_save(flags);
 	__raw_spin_lock(&max_stack_lock);
 

commit 1b6cced6ec9677fa65471e890dfdcb4bf5387643
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Aug 29 16:51:43 2008 -0400

    ftrace: stack trace add indexes
    
    This patch adds indexes into the stack that the functions in the
    stack dump were found at. As an added bonus, I also added a diff
    to show which function is the most notorious consumer of the stack.
    
    The output now looks like this:
    
    # cat /debug/tracing/stack_trace
            Depth   Size      Location    (48 entries)
            -----   ----      --------
      0)     2476     212   blk_recount_segments+0x39/0x59
      1)     2264      12   bio_phys_segments+0x16/0x1d
      2)     2252      20   blk_rq_bio_prep+0x23/0xaf
      3)     2232      12   init_request_from_bio+0x74/0x77
      4)     2220      56   __make_request+0x294/0x331
      5)     2164     136   generic_make_request+0x34f/0x37d
      6)     2028      56   submit_bio+0xe7/0xef
      7)     1972      28   submit_bh+0xd1/0xf0
      8)     1944     112   block_read_full_page+0x299/0x2a9
      9)     1832       8   blkdev_readpage+0x14/0x16
     10)     1824      28   read_cache_page_async+0x7e/0x109
     11)     1796      16   read_cache_page+0x11/0x49
     12)     1780      32   read_dev_sector+0x3c/0x72
     13)     1748      48   read_lba+0x4d/0xaa
     14)     1700     168   efi_partition+0x85/0x61b
     15)     1532      72   rescan_partitions+0x10e/0x266
     16)     1460      40   do_open+0x1c7/0x24e
     17)     1420     292   __blkdev_get+0x79/0x84
     18)     1128      12   blkdev_get+0x12/0x14
     19)     1116      20   register_disk+0xd1/0x11e
     20)     1096      28   add_disk+0x34/0x90
     21)     1068      52   sd_probe+0x2b1/0x366
     22)     1016      20   driver_probe_device+0xa5/0x120
     23)      996       8   __device_attach+0xd/0xf
     24)      988      32   bus_for_each_drv+0x3e/0x68
     25)      956      24   device_attach+0x56/0x6c
     26)      932      16   bus_attach_device+0x26/0x4d
     27)      916      64   device_add+0x380/0x4b4
     28)      852      28   scsi_sysfs_add_sdev+0xa1/0x1c9
     29)      824     160   scsi_probe_and_add_lun+0x919/0xa2a
     30)      664      36   __scsi_add_device+0x88/0xae
     31)      628      44   ata_scsi_scan_host+0x9e/0x21c
     32)      584      28   ata_host_register+0x1cb/0x1db
     33)      556      24   ata_host_activate+0x98/0xb5
     34)      532     192   ahci_init_one+0x9bd/0x9e9
     35)      340      20   pci_device_probe+0x3e/0x5e
     36)      320      20   driver_probe_device+0xa5/0x120
     37)      300      20   __driver_attach+0x3f/0x5e
     38)      280      36   bus_for_each_dev+0x40/0x62
     39)      244      12   driver_attach+0x19/0x1b
     40)      232      28   bus_add_driver+0x9c/0x1af
     41)      204      28   driver_register+0x76/0xd2
     42)      176      20   __pci_register_driver+0x44/0x71
     43)      156       8   ahci_init+0x14/0x16
     44)      148     100   _stext+0x42/0x122
     45)       48      20   kernel_init+0x175/0x1dc
     46)       28      28   kernel_thread_helper+0x7/0x10
    
    The first column is simply an index starting from the inner most function
    and counting down to the outer most.
    
    The next column is the location that the function was found on the stack.
    
    The next column is the size of the stack for that function.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
index 4d1e522e3fe8..74c5d9a3afae 100644
--- a/kernel/trace/trace_stack.c
+++ b/kernel/trace/trace_stack.c
@@ -16,8 +16,10 @@
 
 #define STACK_TRACE_ENTRIES 500
 
-static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES] =
-	{ [0 ... (STACK_TRACE_ENTRIES-1)] = ULONG_MAX };
+static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES+1] =
+	 { [0 ... (STACK_TRACE_ENTRIES)] = ULONG_MAX };
+static unsigned stack_dump_index[STACK_TRACE_ENTRIES];
+
 static struct stack_trace max_stack_trace = {
 	.max_entries		= STACK_TRACE_ENTRIES,
 	.entries		= stack_dump_trace,
@@ -32,8 +34,9 @@ static DEFINE_PER_CPU(int, trace_active);
 
 static inline void check_stack(void)
 {
-	unsigned long this_size;
-	unsigned long flags;
+	unsigned long this_size, flags;
+	unsigned long *p, *top, *start;
+	int i;
 
 	this_size = ((unsigned long)&this_size) & (THREAD_SIZE-1);
 	this_size = THREAD_SIZE - this_size;
@@ -51,10 +54,42 @@ static inline void check_stack(void)
 	max_stack_size = this_size;
 
 	max_stack_trace.nr_entries	= 0;
-	max_stack_trace.skip		= 1;
+	max_stack_trace.skip		= 3;
 
 	save_stack_trace(&max_stack_trace);
 
+	/*
+	 * Now find where in the stack these are.
+	 */
+	i = 0;
+	start = &this_size;
+	top = (unsigned long *)
+		(((unsigned long)start & ~(THREAD_SIZE-1)) + THREAD_SIZE);
+
+	/*
+	 * Loop through all the entries. One of the entries may
+	 * for some reason be missed on the stack, so we may
+	 * have to account for them. If they are all there, this
+	 * loop will only happen once. This code only takes place
+	 * on a new max, so it is far from a fast path.
+	 */
+	while (i < max_stack_trace.nr_entries) {
+
+		stack_dump_index[i] = this_size;
+		p = start;
+
+		for (; p < top && i < max_stack_trace.nr_entries; p++) {
+			if (*p == stack_dump_trace[i]) {
+				this_size = stack_dump_index[i++] =
+					(top - p) * sizeof(unsigned long);
+				/* Start the search from here */
+				start = p + 1;
+			}
+		}
+
+		i++;
+	}
+
  out:
 	__raw_spin_unlock(&max_stack_lock);
 	raw_local_irq_restore(flags);
@@ -145,22 +180,24 @@ static struct file_operations stack_max_size_fops = {
 static void *
 t_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	unsigned long *t = m->private;
+	long i = (long)m->private;
 
 	(*pos)++;
 
-	if (!t || *t == ULONG_MAX)
+	i++;
+
+	if (i >= max_stack_trace.nr_entries ||
+	    stack_dump_trace[i] == ULONG_MAX)
 		return NULL;
 
-	t++;
-	m->private = t;
+	m->private = (void *)i;
 
-	return t;
+	return &m->private;
 }
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
-	unsigned long *t = m->private;
+	void *t = &m->private;
 	loff_t l = 0;
 
 	local_irq_disable();
@@ -178,14 +215,15 @@ static void t_stop(struct seq_file *m, void *p)
 	local_irq_enable();
 }
 
-static int trace_lookup_stack(struct seq_file *m, unsigned long addr)
+static int trace_lookup_stack(struct seq_file *m, long i)
 {
+	unsigned long addr = stack_dump_trace[i];
 #ifdef CONFIG_KALLSYMS
 	char str[KSYM_SYMBOL_LEN];
 
 	sprint_symbol(str, addr);
 
-	return seq_printf(m, "[<%p>] %s\n", (void*)addr, str);
+	return seq_printf(m, "%s\n", str);
 #else
 	return seq_printf(m, "%p\n", (void*)addr);
 #endif
@@ -193,12 +231,30 @@ static int trace_lookup_stack(struct seq_file *m, unsigned long addr)
 
 static int t_show(struct seq_file *m, void *v)
 {
-	unsigned long *t = v;
+	long i = *(long *)v;
+	int size;
+
+	if (i < 0) {
+		seq_printf(m, "        Depth   Size      Location"
+			   "    (%d entries)\n"
+			   "        -----   ----      --------\n",
+			   max_stack_trace.nr_entries);
+		return 0;
+	}
 
-	if (!t || *t == ULONG_MAX)
+	if (i >= max_stack_trace.nr_entries ||
+	    stack_dump_trace[i] == ULONG_MAX)
 		return 0;
 
-	trace_lookup_stack(m, *t);
+	if (i+1 == max_stack_trace.nr_entries ||
+	    stack_dump_trace[i+1] == ULONG_MAX)
+		size = stack_dump_index[i];
+	else
+		size = stack_dump_index[i] - stack_dump_index[i+1];
+
+	seq_printf(m, "%3ld) %8d   %5d   ", i, stack_dump_index[i], size);
+
+	trace_lookup_stack(m, i);
 
 	return 0;
 }
@@ -217,7 +273,7 @@ static int stack_trace_open(struct inode *inode, struct file *file)
 	ret = seq_open(file, &stack_trace_seq_ops);
 	if (!ret) {
 		struct seq_file *m = file->private_data;
-		m->private = stack_dump_trace;
+		m->private = (void *)-1;
 	}
 
 	return ret;

commit e5a81b629ea8feb9e7530cfac35cfb41c45facf3
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Aug 27 23:31:01 2008 -0400

    ftrace: add stack tracer
    
    This is another tracer using the ftrace infrastructure, that examines
    at each function call the size of the stack. If the stack use is greater
    than the previous max it is recorded.
    
    You can always see (and set) the max stack size seen. By setting it
    to zero will start the recording again. The backtrace is also available.
    
    For example:
    
    # cat /debug/tracing/stack_max_size
    1856
    
    # cat /debug/tracing/stack_trace
    [<c027764d>] stack_trace_call+0x8f/0x101
    [<c021b966>] ftrace_call+0x5/0x8
    [<c02553cc>] clocksource_get_next+0x12/0x48
    [<c02542a5>] update_wall_time+0x538/0x6d1
    [<c0245913>] do_timer+0x23/0xb0
    [<c0257657>] tick_do_update_jiffies64+0xd9/0xf1
    [<c02576b9>] tick_sched_timer+0x4a/0xad
    [<c0250fe6>] __run_hrtimer+0x3e/0x75
    [<c02518ed>] hrtimer_interrupt+0xf1/0x154
    [<c022c870>] smp_apic_timer_interrupt+0x71/0x84
    [<c021b7e9>] apic_timer_interrupt+0x2d/0x34
    [<c0238597>] finish_task_switch+0x29/0xa0
    [<c05abd13>] schedule+0x765/0x7be
    [<c05abfca>] schedule_timeout+0x1b/0x90
    [<c05ab4d4>] wait_for_common+0xab/0x101
    [<c05ab5ac>] wait_for_completion+0x12/0x14
    [<c033cfc3>] blk_execute_rq+0x84/0x99
    [<c0402470>] scsi_execute+0xc2/0x105
    [<c040250a>] scsi_execute_req+0x57/0x7f
    [<c043afe0>] sr_test_unit_ready+0x3e/0x97
    [<c043bbd6>] sr_media_change+0x43/0x205
    [<c046b59f>] media_changed+0x48/0x77
    [<c046b5ff>] cdrom_media_changed+0x31/0x37
    [<c043b091>] sr_block_media_changed+0x16/0x18
    [<c02b9e69>] check_disk_change+0x1b/0x63
    [<c046f4c3>] cdrom_open+0x7a1/0x806
    [<c043b148>] sr_block_open+0x78/0x8d
    [<c02ba4c0>] do_open+0x90/0x257
    [<c02ba869>] blkdev_open+0x2d/0x56
    [<c0296a1f>] __dentry_open+0x14d/0x23c
    [<c0296b32>] nameidata_to_filp+0x24/0x38
    [<c02a1c68>] do_filp_open+0x347/0x626
    [<c02967ef>] do_sys_open+0x47/0xbc
    [<c02968b0>] sys_open+0x23/0x2b
    [<c021aadd>] sysenter_do_call+0x12/0x26
    
    I've tested this on both x86_64 and i386.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_stack.c b/kernel/trace/trace_stack.c
new file mode 100644
index 000000000000..4d1e522e3fe8
--- /dev/null
+++ b/kernel/trace/trace_stack.c
@@ -0,0 +1,254 @@
+/*
+ * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>
+ *
+ */
+#include <linux/stacktrace.h>
+#include <linux/kallsyms.h>
+#include <linux/seq_file.h>
+#include <linux/spinlock.h>
+#include <linux/uaccess.h>
+#include <linux/debugfs.h>
+#include <linux/ftrace.h>
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/fs.h>
+#include "trace.h"
+
+#define STACK_TRACE_ENTRIES 500
+
+static unsigned long stack_dump_trace[STACK_TRACE_ENTRIES] =
+	{ [0 ... (STACK_TRACE_ENTRIES-1)] = ULONG_MAX };
+static struct stack_trace max_stack_trace = {
+	.max_entries		= STACK_TRACE_ENTRIES,
+	.entries		= stack_dump_trace,
+};
+
+static unsigned long max_stack_size;
+static raw_spinlock_t max_stack_lock =
+	(raw_spinlock_t)__RAW_SPIN_LOCK_UNLOCKED;
+
+static int stack_trace_disabled __read_mostly;
+static DEFINE_PER_CPU(int, trace_active);
+
+static inline void check_stack(void)
+{
+	unsigned long this_size;
+	unsigned long flags;
+
+	this_size = ((unsigned long)&this_size) & (THREAD_SIZE-1);
+	this_size = THREAD_SIZE - this_size;
+
+	if (this_size <= max_stack_size)
+		return;
+
+	raw_local_irq_save(flags);
+	__raw_spin_lock(&max_stack_lock);
+
+	/* a race could have already updated it */
+	if (this_size <= max_stack_size)
+		goto out;
+
+	max_stack_size = this_size;
+
+	max_stack_trace.nr_entries	= 0;
+	max_stack_trace.skip		= 1;
+
+	save_stack_trace(&max_stack_trace);
+
+ out:
+	__raw_spin_unlock(&max_stack_lock);
+	raw_local_irq_restore(flags);
+}
+
+static void
+stack_trace_call(unsigned long ip, unsigned long parent_ip)
+{
+	int cpu, resched;
+
+	if (unlikely(!ftrace_enabled || stack_trace_disabled))
+		return;
+
+	resched = need_resched();
+	preempt_disable_notrace();
+
+	cpu = raw_smp_processor_id();
+	/* no atomic needed, we only modify this variable by this cpu */
+	if (per_cpu(trace_active, cpu)++ != 0)
+		goto out;
+
+	check_stack();
+
+ out:
+	per_cpu(trace_active, cpu)--;
+	/* prevent recursion in schedule */
+	if (resched)
+		preempt_enable_no_resched_notrace();
+	else
+		preempt_enable_notrace();
+}
+
+static struct ftrace_ops trace_ops __read_mostly =
+{
+	.func = stack_trace_call,
+};
+
+static ssize_t
+stack_max_size_read(struct file *filp, char __user *ubuf,
+		    size_t count, loff_t *ppos)
+{
+	unsigned long *ptr = filp->private_data;
+	char buf[64];
+	int r;
+
+	r = snprintf(buf, sizeof(buf), "%ld\n", *ptr);
+	if (r > sizeof(buf))
+		r = sizeof(buf);
+	return simple_read_from_buffer(ubuf, count, ppos, buf, r);
+}
+
+static ssize_t
+stack_max_size_write(struct file *filp, const char __user *ubuf,
+		     size_t count, loff_t *ppos)
+{
+	long *ptr = filp->private_data;
+	unsigned long val, flags;
+	char buf[64];
+	int ret;
+
+	if (count >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, count))
+		return -EFAULT;
+
+	buf[count] = 0;
+
+	ret = strict_strtoul(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	raw_local_irq_save(flags);
+	__raw_spin_lock(&max_stack_lock);
+	*ptr = val;
+	__raw_spin_unlock(&max_stack_lock);
+	raw_local_irq_restore(flags);
+
+	return count;
+}
+
+static struct file_operations stack_max_size_fops = {
+	.open		= tracing_open_generic,
+	.read		= stack_max_size_read,
+	.write		= stack_max_size_write,
+};
+
+static void *
+t_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	unsigned long *t = m->private;
+
+	(*pos)++;
+
+	if (!t || *t == ULONG_MAX)
+		return NULL;
+
+	t++;
+	m->private = t;
+
+	return t;
+}
+
+static void *t_start(struct seq_file *m, loff_t *pos)
+{
+	unsigned long *t = m->private;
+	loff_t l = 0;
+
+	local_irq_disable();
+	__raw_spin_lock(&max_stack_lock);
+
+	for (; t && l < *pos; t = t_next(m, t, &l))
+		;
+
+	return t;
+}
+
+static void t_stop(struct seq_file *m, void *p)
+{
+	__raw_spin_unlock(&max_stack_lock);
+	local_irq_enable();
+}
+
+static int trace_lookup_stack(struct seq_file *m, unsigned long addr)
+{
+#ifdef CONFIG_KALLSYMS
+	char str[KSYM_SYMBOL_LEN];
+
+	sprint_symbol(str, addr);
+
+	return seq_printf(m, "[<%p>] %s\n", (void*)addr, str);
+#else
+	return seq_printf(m, "%p\n", (void*)addr);
+#endif
+}
+
+static int t_show(struct seq_file *m, void *v)
+{
+	unsigned long *t = v;
+
+	if (!t || *t == ULONG_MAX)
+		return 0;
+
+	trace_lookup_stack(m, *t);
+
+	return 0;
+}
+
+static struct seq_operations stack_trace_seq_ops = {
+	.start		= t_start,
+	.next		= t_next,
+	.stop		= t_stop,
+	.show		= t_show,
+};
+
+static int stack_trace_open(struct inode *inode, struct file *file)
+{
+	int ret;
+
+	ret = seq_open(file, &stack_trace_seq_ops);
+	if (!ret) {
+		struct seq_file *m = file->private_data;
+		m->private = stack_dump_trace;
+	}
+
+	return ret;
+}
+
+static struct file_operations stack_trace_fops = {
+	.open		= stack_trace_open,
+	.read		= seq_read,
+	.llseek		= seq_lseek,
+};
+
+static __init int stack_trace_init(void)
+{
+	struct dentry *d_tracer;
+	struct dentry *entry;
+
+	d_tracer = tracing_init_dentry();
+
+	entry = debugfs_create_file("stack_max_size", 0644, d_tracer,
+				    &max_stack_size, &stack_max_size_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs 'stack_max_size' entry\n");
+
+	entry = debugfs_create_file("stack_trace", 0444, d_tracer,
+				    NULL, &stack_trace_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs 'stack_trace' entry\n");
+
+	register_ftrace_function(&trace_ops);
+
+	return 0;
+}
+
+device_initcall(stack_trace_init);
