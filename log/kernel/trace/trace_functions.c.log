commit 48a42f5d138435242529726b8802076a24b6db17
Author: Wei Yang <richard.weiyang@linux.alibaba.com>
Date:   Wed Jun 10 11:32:51 2020 +0800

    trace: Fix typo in allocate_ftrace_ops()'s comment
    
    No functional change, just correct the word.
    
    Link: https://lkml.kernel.org/r/20200610033251.31713-1-richard.weiyang@linux.alibaba.com
    
    Signed-off-by: Wei Yang <richard.weiyang@linux.alibaba.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 8a4c8d5c2c98..dd4dff71d89a 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -42,7 +42,7 @@ static int allocate_ftrace_ops(struct trace_array *tr)
 	if (!ops)
 		return -ENOMEM;
 
-	/* Currently only the non stack verision is supported */
+	/* Currently only the non stack version is supported */
 	ops->func = function_trace_call;
 	ops->flags = FTRACE_OPS_FL_RECURSION_SAFE | FTRACE_OPS_FL_PID;
 

commit 1c5eb4481e0151d579f738175497f998840f7bbc
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jan 9 18:53:48 2020 -0500

    tracing: Rename trace_buffer to array_buffer
    
    As we are working to remove the generic "ring_buffer" name that is used by
    both tracing and perf, the ring_buffer name for tracing will be renamed to
    trace_buffer, and perf's ring buffer will be renamed to perf_buffer.
    
    As there already exists a trace_buffer that is used by the trace_arrays, it
    needs to be first renamed to array_buffer.
    
    Link: https://lore.kernel.org/r/20191213153553.GE20583@krava
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b611cd36e22d..8a4c8d5c2c98 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -101,7 +101,7 @@ static int function_trace_init(struct trace_array *tr)
 
 	ftrace_init_array_ops(tr, func);
 
-	tr->trace_buffer.cpu = get_cpu();
+	tr->array_buffer.cpu = get_cpu();
 	put_cpu();
 
 	tracing_start_cmdline_record();
@@ -118,7 +118,7 @@ static void function_trace_reset(struct trace_array *tr)
 
 static void function_trace_start(struct trace_array *tr)
 {
-	tracing_reset_online_cpus(&tr->trace_buffer);
+	tracing_reset_online_cpus(&tr->array_buffer);
 }
 
 static void
@@ -143,7 +143,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 		goto out;
 
 	cpu = smp_processor_id();
-	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	if (!atomic_read(&data->disabled)) {
 		local_save_flags(flags);
 		trace_function(tr, ip, parent_ip, flags, pc);
@@ -192,7 +192,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	 */
 	local_irq_save(flags);
 	cpu = raw_smp_processor_id();
-	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
+	data = per_cpu_ptr(tr->array_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1)) {

commit 2ee5b92a2598d9e403337185fdf88f661dee8616
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Jan 23 13:25:04 2018 -0500

    tracing: Update stack trace skipping for ORC unwinder
    
    With the addition of ORC unwinder and FRAME POINTER unwinder, the stack
    trace skipping requirements have changed.
    
    I went through the tracing stack trace dumps with ORC and with frame
    pointers and recalculated the proper values.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 27f7ad12c4b1..b611cd36e22d 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -154,6 +154,24 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 	preempt_enable_notrace();
 }
 
+#ifdef CONFIG_UNWINDER_ORC
+/*
+ * Skip 2:
+ *
+ *   function_stack_trace_call()
+ *   ftrace_call()
+ */
+#define STACK_SKIP 2
+#else
+/*
+ * Skip 3:
+ *   __trace_stack()
+ *   function_stack_trace_call()
+ *   ftrace_call()
+ */
+#define STACK_SKIP 3
+#endif
+
 static void
 function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *op, struct pt_regs *pt_regs)
@@ -180,15 +198,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	if (likely(disabled == 1)) {
 		pc = preempt_count();
 		trace_function(tr, ip, parent_ip, flags, pc);
-		/*
-		 * skip over 5 funcs:
-		 *    __ftrace_trace_stack,
-		 *    __trace_stack,
-		 *    function_stack_trace_call
-		 *    ftrace_list_func
-		 *    ftrace_call
-		 */
-		__trace_stack(tr, flags, 5, pc);
+		__trace_stack(tr, flags, STACK_SKIP, pc);
 	}
 
 	atomic_dec(&data->disabled);
@@ -367,14 +377,27 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
 	tracer_tracing_off(tr);
 }
 
+#ifdef CONFIG_UNWINDER_ORC
 /*
- * Skip 4:
+ * Skip 3:
+ *
+ *   function_trace_probe_call()
+ *   ftrace_ops_assist_func()
+ *   ftrace_call()
+ */
+#define FTRACE_STACK_SKIP 3
+#else
+/*
+ * Skip 5:
+ *
+ *   __trace_stack()
  *   ftrace_stacktrace()
  *   function_trace_probe_call()
- *   ftrace_ops_list_func()
+ *   ftrace_ops_assist_func()
  *   ftrace_call()
  */
-#define STACK_SKIP 4
+#define FTRACE_STACK_SKIP 5
+#endif
 
 static __always_inline void trace_stack(struct trace_array *tr)
 {
@@ -384,7 +407,7 @@ static __always_inline void trace_stack(struct trace_array *tr)
 	local_save_flags(flags);
 	pc = preempt_count();
 
-	__trace_stack(tr, flags, STACK_SKIP, pc);
+	__trace_stack(tr, flags, FTRACE_STACK_SKIP, pc);
 }
 
 static void

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index a0910c0cdf2e..27f7ad12c4b1 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * ring buffer based function tracer
  *

commit 0f17976568b3f72e676450af0c0db6f8752253d6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jun 29 10:05:45 2017 -0400

    ftrace: Fix regression with module command in stack_trace_filter
    
    When doing the following command:
    
     # echo ":mod:kvm_intel" > /sys/kernel/tracing/stack_trace_filter
    
    it triggered a crash.
    
    This happened with the clean up of probes. It required all callers to the
    regex function (doing ftrace filtering) to have ops->private be a pointer to
    a trace_array. But for the stack tracer, that is not the case.
    
    Allow for the ops->private to be NULL, and change the function command
    callbacks to handle the trace_array pointer being NULL as well.
    
    Fixes: d2afd57a4b96 ("tracing/ftrace: Allow instances to have their own function probes")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index a3bddbfd0874..a0910c0cdf2e 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -654,6 +654,9 @@ ftrace_trace_onoff_callback(struct trace_array *tr, struct ftrace_hash *hash,
 {
 	struct ftrace_probe_ops *ops;
 
+	if (!tr)
+		return -ENODEV;
+
 	/* we register both traceon and traceoff to this callback */
 	if (strcmp(cmd, "traceon") == 0)
 		ops = param ? &traceon_count_probe_ops : &traceon_probe_ops;
@@ -670,6 +673,9 @@ ftrace_stacktrace_callback(struct trace_array *tr, struct ftrace_hash *hash,
 {
 	struct ftrace_probe_ops *ops;
 
+	if (!tr)
+		return -ENODEV;
+
 	ops = param ? &stacktrace_count_probe_ops : &stacktrace_probe_ops;
 
 	return ftrace_trace_probe_callback(tr, ops, hash, glob, cmd,
@@ -682,6 +688,9 @@ ftrace_dump_callback(struct trace_array *tr, struct ftrace_hash *hash,
 {
 	struct ftrace_probe_ops *ops;
 
+	if (!tr)
+		return -ENODEV;
+
 	ops = &dump_probe_ops;
 
 	/* Only dump once. */
@@ -695,6 +704,9 @@ ftrace_cpudump_callback(struct trace_array *tr, struct ftrace_hash *hash,
 {
 	struct ftrace_probe_ops *ops;
 
+	if (!tr)
+		return -ENODEV;
+
 	ops = &cpudump_probe_ops;
 
 	/* Only dump once. */

commit dcc19d28091a86d5baf78e3fbb32e3fc3de524be
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 20 11:59:18 2017 -0400

    tracing/ftrace: Allow for instances to trigger their own stacktrace probes
    
    Have the stacktrace function trigger probe trigger stack traces within the
    instance that they were added to in the set_ftrace_filter.
    
     ># cd /sys/kernel/debug/tracing
     ># mkdir instances/foo
     ># cd instances/foo
     ># echo schedule:stacktrace:1 > set_ftrace_filter
     ># cat trace
     # tracer: nop
     #
     # entries-in-buffer/entries-written: 1/1   #P:4
     #
     #                              _-----=> irqs-off
     #                             / _----=> need-resched
     #                            | / _---=> hardirq/softirq
     #                            || / _--=> preempt-depth
     #                            ||| /     delay
     #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
     #              | |       |   ||||       |         |
               <idle>-0     [001] .N.2   202.585010: <stack trace>
      =>
      => schedule
      => schedule_preempt_disabled
      => do_idle
      => cpu_startup_entry
      => start_secondary
      => verify_cpu
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 8c30ca733a5c..a3bddbfd0874 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -375,12 +375,23 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
  */
 #define STACK_SKIP 4
 
+static __always_inline void trace_stack(struct trace_array *tr)
+{
+	unsigned long flags;
+	int pc;
+
+	local_save_flags(flags);
+	pc = preempt_count();
+
+	__trace_stack(tr, flags, STACK_SKIP, pc);
+}
+
 static void
 ftrace_stacktrace(unsigned long ip, unsigned long parent_ip,
 		  struct trace_array *tr, struct ftrace_probe_ops *ops,
 		  void *data)
 {
-	trace_dump_stack(STACK_SKIP);
+	trace_stack(tr);
 }
 
 static void
@@ -398,7 +409,7 @@ ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip,
 
 	/* unlimited? */
 	if (!mapper) {
-		trace_dump_stack(STACK_SKIP);
+		trace_stack(tr);
 		return;
 	}
 
@@ -417,7 +428,7 @@ ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip,
 		new_count = old_count - 1;
 		new_count = cmpxchg(count, old_count, new_count);
 		if (new_count == old_count)
-			trace_dump_stack(STACK_SKIP);
+			trace_stack(tr);
 
 		if (!tracing_is_on())
 			return;

commit 2290f2c589285d0031e3b7445afff8949f3fdbb6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 20 11:46:03 2017 -0400

    tracing/ftrace: Allow for the traceonoff probe be unique to instances
    
    Have the traceon/off function probe triggers affect only the instance they
    are set in. This required making the trace_on/off accessible for other files
    in the tracing directory.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 7775e1ca5bad..8c30ca733a5c 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -268,7 +268,8 @@ static struct tracer function_trace __tracer_data =
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 static void update_traceon_count(struct ftrace_probe_ops *ops,
-				 unsigned long ip, bool on,
+				 unsigned long ip,
+				 struct trace_array *tr, bool on,
 				 void *data)
 {
 	struct ftrace_func_mapper *mapper = data;
@@ -313,13 +314,13 @@ static void update_traceon_count(struct ftrace_probe_ops *ops,
 	/* Make sure we see count before checking tracing state */
 	smp_rmb();
 
-	if (on == !!tracing_is_on())
+	if (on == !!tracer_tracing_is_on(tr))
 		return;
 
 	if (on)
-		tracing_on();
+		tracer_tracing_on(tr);
 	else
-		tracing_off();
+		tracer_tracing_off(tr);
 
 	/* Make sure tracing state is visible before updating count */
 	smp_wmb();
@@ -332,7 +333,7 @@ ftrace_traceon_count(unsigned long ip, unsigned long parent_ip,
 		     struct trace_array *tr, struct ftrace_probe_ops *ops,
 		     void *data)
 {
-	update_traceon_count(ops, ip, 1, data);
+	update_traceon_count(ops, ip, tr, 1, data);
 }
 
 static void
@@ -340,7 +341,7 @@ ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip,
 		      struct trace_array *tr, struct ftrace_probe_ops *ops,
 		      void *data)
 {
-	update_traceon_count(ops, ip, 0, data);
+	update_traceon_count(ops, ip, tr, 0, data);
 }
 
 static void
@@ -348,10 +349,10 @@ ftrace_traceon(unsigned long ip, unsigned long parent_ip,
 	       struct trace_array *tr, struct ftrace_probe_ops *ops,
 	       void *data)
 {
-	if (tracing_is_on())
+	if (tracer_tracing_is_on(tr))
 		return;
 
-	tracing_on();
+	tracer_tracing_on(tr);
 }
 
 static void
@@ -359,10 +360,10 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
 		struct trace_array *tr, struct ftrace_probe_ops *ops,
 		void *data)
 {
-	if (!tracing_is_on())
+	if (!tracer_tracing_is_on(tr))
 		return;
 
-	tracing_off();
+	tracer_tracing_off(tr);
 }
 
 /*

commit 6e4443199e5354255e8a4c1e8e5cfc8ef064c3ce
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 19 22:39:44 2017 -0400

    tracing/ftrace: Add a better way to pass data via the probe functions
    
    With the redesign of the registration and execution of the function probes
    (triggers), data can now be passed from the setup of the probe to the probe
    callers that are specific to the trace_array it is on. Although, all probes
    still only affect the toplevel trace array, this change will allow for
    instances to have their own probes separated from other instances and the
    top array.
    
    That is, something like the stacktrace probe can be set to trace only in an
    instance and not the toplevel trace array. This isn't implement yet, but
    this change sets the ground work for the change.
    
    When a probe callback is triggered (someone writes the probe format into
    set_ftrace_filter), it calls register_ftrace_function_probe() passing in
    init_data that will be used to initialize the probe. Then for every matching
    function, register_ftrace_function_probe() will call the probe_ops->init()
    function with the init data that was passed to it, as well as an address to
    a place holder that is associated with the probe and the instance. The first
    occurrence will have a NULL in the pointer. The init() function will then
    initialize it. If other probes are added, or more functions are part of the
    probe, the place holder will be passed to the init() function with the place
    holder data that it was initialized to the last time.
    
    Then this place_holder is passed to each of the other probe_ops functions,
    where it can be used in the function callback. When the probe_ops free()
    function is called, it can be called either with the rip of the function
    that is being removed from the probe, or zero, indicating that there are no
    more functions attached to the probe, and the place holder is about to be
    freed. This gives the probe_ops a way to free the data it assigned to the
    place holder if it was allocade during the first init call.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b95f56ba9744..7775e1ca5bad 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -268,9 +268,10 @@ static struct tracer function_trace __tracer_data =
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 static void update_traceon_count(struct ftrace_probe_ops *ops,
-				 unsigned long ip, bool on)
+				 unsigned long ip, bool on,
+				 void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct ftrace_func_mapper *mapper = data;
 	long *count;
 	long old_count;
 
@@ -329,23 +330,23 @@ static void update_traceon_count(struct ftrace_probe_ops *ops,
 static void
 ftrace_traceon_count(unsigned long ip, unsigned long parent_ip,
 		     struct trace_array *tr, struct ftrace_probe_ops *ops,
-		     void **data)
+		     void *data)
 {
-	update_traceon_count(ops, ip, 1);
+	update_traceon_count(ops, ip, 1, data);
 }
 
 static void
 ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip,
 		      struct trace_array *tr, struct ftrace_probe_ops *ops,
-		      void **data)
+		      void *data)
 {
-	update_traceon_count(ops, ip, 0);
+	update_traceon_count(ops, ip, 0, data);
 }
 
 static void
 ftrace_traceon(unsigned long ip, unsigned long parent_ip,
 	       struct trace_array *tr, struct ftrace_probe_ops *ops,
-	       void **data)
+	       void *data)
 {
 	if (tracing_is_on())
 		return;
@@ -356,7 +357,7 @@ ftrace_traceon(unsigned long ip, unsigned long parent_ip,
 static void
 ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
 		struct trace_array *tr, struct ftrace_probe_ops *ops,
-		void **data)
+		void *data)
 {
 	if (!tracing_is_on())
 		return;
@@ -376,7 +377,7 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
 static void
 ftrace_stacktrace(unsigned long ip, unsigned long parent_ip,
 		  struct trace_array *tr, struct ftrace_probe_ops *ops,
-		  void **data)
+		  void *data)
 {
 	trace_dump_stack(STACK_SKIP);
 }
@@ -384,9 +385,9 @@ ftrace_stacktrace(unsigned long ip, unsigned long parent_ip,
 static void
 ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip,
 			struct trace_array *tr, struct ftrace_probe_ops *ops,
-			void **data)
+			void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct ftrace_func_mapper *mapper = data;
 	long *count;
 	long old_count;
 	long new_count;
@@ -423,9 +424,10 @@ ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip,
 	} while (new_count != old_count);
 }
 
-static int update_count(struct ftrace_probe_ops *ops, unsigned long ip)
+static int update_count(struct ftrace_probe_ops *ops, unsigned long ip,
+			void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct ftrace_func_mapper *mapper = data;
 	long *count = NULL;
 
 	if (mapper)
@@ -443,9 +445,9 @@ static int update_count(struct ftrace_probe_ops *ops, unsigned long ip)
 static void
 ftrace_dump_probe(unsigned long ip, unsigned long parent_ip,
 		  struct trace_array *tr, struct ftrace_probe_ops *ops,
-		  void **data)
+		  void *data)
 {
-	if (update_count(ops, ip))
+	if (update_count(ops, ip, data))
 		ftrace_dump(DUMP_ALL);
 }
 
@@ -453,17 +455,18 @@ ftrace_dump_probe(unsigned long ip, unsigned long parent_ip,
 static void
 ftrace_cpudump_probe(unsigned long ip, unsigned long parent_ip,
 		     struct trace_array *tr, struct ftrace_probe_ops *ops,
-		     void **data)
+		     void *data)
 {
-	if (update_count(ops, ip))
+	if (update_count(ops, ip, data))
 		ftrace_dump(DUMP_ORIG);
 }
 
 static int
 ftrace_probe_print(const char *name, struct seq_file *m,
-		   unsigned long ip, struct ftrace_probe_ops *ops)
+		   unsigned long ip, struct ftrace_probe_ops *ops,
+		   void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct ftrace_func_mapper *mapper = data;
 	long *count = NULL;
 
 	seq_printf(m, "%ps:%s", (void *)ip, name);
@@ -484,52 +487,64 @@ ftrace_traceon_print(struct seq_file *m, unsigned long ip,
 		     struct ftrace_probe_ops *ops,
 		     void *data)
 {
-	return ftrace_probe_print("traceon", m, ip, ops);
+	return ftrace_probe_print("traceon", m, ip, ops, data);
 }
 
 static int
 ftrace_traceoff_print(struct seq_file *m, unsigned long ip,
 			 struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("traceoff", m, ip, ops);
+	return ftrace_probe_print("traceoff", m, ip, ops, data);
 }
 
 static int
 ftrace_stacktrace_print(struct seq_file *m, unsigned long ip,
 			struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("stacktrace", m, ip, ops);
+	return ftrace_probe_print("stacktrace", m, ip, ops, data);
 }
 
 static int
 ftrace_dump_print(struct seq_file *m, unsigned long ip,
 			struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("dump", m, ip, ops);
+	return ftrace_probe_print("dump", m, ip, ops, data);
 }
 
 static int
 ftrace_cpudump_print(struct seq_file *m, unsigned long ip,
 			struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("cpudump", m, ip, ops);
+	return ftrace_probe_print("cpudump", m, ip, ops, data);
 }
 
 
 static int
 ftrace_count_init(struct ftrace_probe_ops *ops, struct trace_array *tr,
-		  unsigned long ip, void *data)
+		  unsigned long ip, void *init_data, void **data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct ftrace_func_mapper *mapper = *data;
+
+	if (!mapper) {
+		mapper = allocate_ftrace_func_mapper();
+		if (!mapper)
+			return -ENOMEM;
+		*data = mapper;
+	}
 
-	return ftrace_func_mapper_add_ip(mapper, ip, data);
+	return ftrace_func_mapper_add_ip(mapper, ip, init_data);
 }
 
 static void
 ftrace_count_free(struct ftrace_probe_ops *ops, struct trace_array *tr,
-		  unsigned long ip, void **_data)
+		  unsigned long ip, void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct ftrace_func_mapper *mapper = data;
+
+	if (!ip) {
+		free_ftrace_func_mapper(mapper, NULL);
+		return;
+	}
 
 	ftrace_func_mapper_remove_ip(mapper, ip);
 }
@@ -607,12 +622,6 @@ ftrace_trace_probe_callback(struct trace_array *tr,
 	if (!strlen(number))
 		goto out_reg;
 
-	if (!ops->private_data) {
-		ops->private_data = allocate_ftrace_func_mapper();
-		if (!ops->private_data)
-			return -ENOMEM;
-	}
-
 	/*
 	 * We use the callback data field (which is a pointer)
 	 * as our counter.

commit 7b60f3d8761561d95d7e962522d6338143fc2329
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 18 14:50:39 2017 -0400

    ftrace: Dynamically create the probe ftrace_ops for the trace_array
    
    In order to eventually have each trace_array instance have its own unique
    set of function probes (triggers), the trace array needs to hold the ops and
    the filters for the probes.
    
    This is the first step to accomplish this. Instead of having the private
    data of the probe ops point to the trace_array, create a separate list that
    the trace_array holds. There's only one private_data for a probe, we need
    one per trace_array. The probe ftrace_ops will be dynamically created for
    each instance, instead of being static.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 797f087183c5..b95f56ba9744 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -597,7 +597,7 @@ ftrace_trace_probe_callback(struct trace_array *tr,
 		return -EINVAL;
 
 	if (glob[0] == '!')
-		return unregister_ftrace_function_probe_func(glob+1, ops);
+		return unregister_ftrace_function_probe_func(glob+1, tr, ops);
 
 	if (!param)
 		goto out_reg;

commit b5f081b563a6cdcb85a543df8c851951a8978275
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 10 22:30:05 2017 -0400

    tracing: Pass the trace_array into ftrace_probe_ops functions
    
    Pass the trace_array associated to a ftrace_probe_ops into the probe_ops
    func(), init() and free() functions. The trace_array is the descriptor that
    describes a tracing instance. This will help create the infrastructure that
    will allow having function probes unique to tracing instances.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 2c8961b35401..797f087183c5 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -328,21 +328,24 @@ static void update_traceon_count(struct ftrace_probe_ops *ops,
 
 static void
 ftrace_traceon_count(unsigned long ip, unsigned long parent_ip,
-		     struct ftrace_probe_ops *ops, void **data)
+		     struct trace_array *tr, struct ftrace_probe_ops *ops,
+		     void **data)
 {
 	update_traceon_count(ops, ip, 1);
 }
 
 static void
 ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip,
-		      struct ftrace_probe_ops *ops, void **data)
+		      struct trace_array *tr, struct ftrace_probe_ops *ops,
+		      void **data)
 {
 	update_traceon_count(ops, ip, 0);
 }
 
 static void
 ftrace_traceon(unsigned long ip, unsigned long parent_ip,
-	       struct ftrace_probe_ops *ops, void **data)
+	       struct trace_array *tr, struct ftrace_probe_ops *ops,
+	       void **data)
 {
 	if (tracing_is_on())
 		return;
@@ -352,7 +355,8 @@ ftrace_traceon(unsigned long ip, unsigned long parent_ip,
 
 static void
 ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
-		struct ftrace_probe_ops *ops, void **data)
+		struct trace_array *tr, struct ftrace_probe_ops *ops,
+		void **data)
 {
 	if (!tracing_is_on())
 		return;
@@ -371,14 +375,16 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
 
 static void
 ftrace_stacktrace(unsigned long ip, unsigned long parent_ip,
-		  struct ftrace_probe_ops *ops, void **data)
+		  struct trace_array *tr, struct ftrace_probe_ops *ops,
+		  void **data)
 {
 	trace_dump_stack(STACK_SKIP);
 }
 
 static void
 ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip,
-			struct ftrace_probe_ops *ops, void **data)
+			struct trace_array *tr, struct ftrace_probe_ops *ops,
+			void **data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 	long *count;
@@ -436,7 +442,8 @@ static int update_count(struct ftrace_probe_ops *ops, unsigned long ip)
 
 static void
 ftrace_dump_probe(unsigned long ip, unsigned long parent_ip,
-	struct ftrace_probe_ops *ops, void **data)
+		  struct trace_array *tr, struct ftrace_probe_ops *ops,
+		  void **data)
 {
 	if (update_count(ops, ip))
 		ftrace_dump(DUMP_ALL);
@@ -445,7 +452,8 @@ ftrace_dump_probe(unsigned long ip, unsigned long parent_ip,
 /* Only dump the current CPU buffer. */
 static void
 ftrace_cpudump_probe(unsigned long ip, unsigned long parent_ip,
-	struct ftrace_probe_ops *ops, void **data)
+		     struct trace_array *tr, struct ftrace_probe_ops *ops,
+		     void **data)
 {
 	if (update_count(ops, ip))
 		ftrace_dump(DUMP_ORIG);
@@ -473,7 +481,8 @@ ftrace_probe_print(const char *name, struct seq_file *m,
 
 static int
 ftrace_traceon_print(struct seq_file *m, unsigned long ip,
-			 struct ftrace_probe_ops *ops, void *data)
+		     struct ftrace_probe_ops *ops,
+		     void *data)
 {
 	return ftrace_probe_print("traceon", m, ip, ops);
 }
@@ -508,8 +517,8 @@ ftrace_cpudump_print(struct seq_file *m, unsigned long ip,
 
 
 static int
-ftrace_count_init(struct ftrace_probe_ops *ops, unsigned long ip,
-		     void *data)
+ftrace_count_init(struct ftrace_probe_ops *ops, struct trace_array *tr,
+		  unsigned long ip, void *data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 
@@ -517,8 +526,8 @@ ftrace_count_init(struct ftrace_probe_ops *ops, unsigned long ip,
 }
 
 static void
-ftrace_count_free(struct ftrace_probe_ops *ops, unsigned long ip,
-		  void **_data)
+ftrace_count_free(struct ftrace_probe_ops *ops, struct trace_array *tr,
+		  unsigned long ip, void **_data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 

commit 04ec7bb642b77374b53731b795b5654b5aff1c00
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 5 13:12:55 2017 -0400

    tracing: Have the trace_array hold the list of registered func probes
    
    Add a link list to the trace_array to hold func probes that are registered.
    Currently, all function probes are the same for all instances as it was
    before, that is, only the top level trace_array holds the function probes.
    But this lays the ground work to have function probes be attached to
    individual instances, and having the event trigger only affect events in the
    given instance. But that work is still to be done.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index dcb4d37ed4bd..2c8961b35401 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -574,7 +574,8 @@ static struct ftrace_probe_ops stacktrace_probe_ops = {
 };
 
 static int
-ftrace_trace_probe_callback(struct ftrace_probe_ops *ops,
+ftrace_trace_probe_callback(struct trace_array *tr,
+			    struct ftrace_probe_ops *ops,
 			    struct ftrace_hash *hash, char *glob,
 			    char *cmd, char *param, int enable)
 {
@@ -612,13 +613,13 @@ ftrace_trace_probe_callback(struct ftrace_probe_ops *ops,
 		return ret;
 
  out_reg:
-	ret = register_ftrace_function_probe(glob, ops, count);
+	ret = register_ftrace_function_probe(glob, tr, ops, count);
 
 	return ret < 0 ? ret : 0;
 }
 
 static int
-ftrace_trace_onoff_callback(struct ftrace_hash *hash,
+ftrace_trace_onoff_callback(struct trace_array *tr, struct ftrace_hash *hash,
 			    char *glob, char *cmd, char *param, int enable)
 {
 	struct ftrace_probe_ops *ops;
@@ -629,24 +630,24 @@ ftrace_trace_onoff_callback(struct ftrace_hash *hash,
 	else
 		ops = param ? &traceoff_count_probe_ops : &traceoff_probe_ops;
 
-	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+	return ftrace_trace_probe_callback(tr, ops, hash, glob, cmd,
 					   param, enable);
 }
 
 static int
-ftrace_stacktrace_callback(struct ftrace_hash *hash,
+ftrace_stacktrace_callback(struct trace_array *tr, struct ftrace_hash *hash,
 			   char *glob, char *cmd, char *param, int enable)
 {
 	struct ftrace_probe_ops *ops;
 
 	ops = param ? &stacktrace_count_probe_ops : &stacktrace_probe_ops;
 
-	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+	return ftrace_trace_probe_callback(tr, ops, hash, glob, cmd,
 					   param, enable);
 }
 
 static int
-ftrace_dump_callback(struct ftrace_hash *hash,
+ftrace_dump_callback(struct trace_array *tr, struct ftrace_hash *hash,
 			   char *glob, char *cmd, char *param, int enable)
 {
 	struct ftrace_probe_ops *ops;
@@ -654,12 +655,12 @@ ftrace_dump_callback(struct ftrace_hash *hash,
 	ops = &dump_probe_ops;
 
 	/* Only dump once. */
-	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+	return ftrace_trace_probe_callback(tr, ops, hash, glob, cmd,
 					   "1", enable);
 }
 
 static int
-ftrace_cpudump_callback(struct ftrace_hash *hash,
+ftrace_cpudump_callback(struct trace_array *tr, struct ftrace_hash *hash,
 			   char *glob, char *cmd, char *param, int enable)
 {
 	struct ftrace_probe_ops *ops;
@@ -667,7 +668,7 @@ ftrace_cpudump_callback(struct ftrace_hash *hash,
 	ops = &cpudump_probe_ops;
 
 	/* Only dump once. */
-	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+	return ftrace_trace_probe_callback(tr, ops, hash, glob, cmd,
 					   "1", enable);
 }
 

commit d3d532d798c5720055ab02a10bf7829a33c3645a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 16:44:43 2017 -0400

    ftrace: Have unregister_ftrace_function_probe_func() return a value
    
    Currently unregister_ftrace_function_probe_func() is a void function. It
    does not give any feedback if an error occurred or no item was found to
    remove and nothing was done.
    
    Change it to return status and success if it removed something. Also update
    the callers to return that feedback to the user.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 56d0fe1e4ea1..dcb4d37ed4bd 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -586,10 +586,8 @@ ftrace_trace_probe_callback(struct ftrace_probe_ops *ops,
 	if (!enable)
 		return -EINVAL;
 
-	if (glob[0] == '!') {
-		unregister_ftrace_function_probe_func(glob+1, ops);
-		return 0;
-	}
+	if (glob[0] == '!')
+		return unregister_ftrace_function_probe_func(glob+1, ops);
 
 	if (!param)
 		goto out_reg;

commit 1a48df0041c2756194e700affb0e2ff084092e28
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 10:27:51 2017 -0400

    ftrace: Remove data field from ftrace_func_probe structure
    
    No users of the function probes uses the data field anymore. Remove it, and
    change the init function to take a void *data parameter instead of a
    void **data, because the init will just get the data that the registering
    function was received, and there's no state after it is called.
    
    The other functions for ftrace_probe_ops still take the data parameter, but
    it will currently only be passed NULL. It will stay as a parameter for
    future data to be passed to these functions.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index d9cbde8575a8..56d0fe1e4ea1 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -509,11 +509,11 @@ ftrace_cpudump_print(struct seq_file *m, unsigned long ip,
 
 static int
 ftrace_count_init(struct ftrace_probe_ops *ops, unsigned long ip,
-		     void **data)
+		     void *data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 
-	return ftrace_func_mapper_add_ip(mapper, ip, *data);
+	return ftrace_func_mapper_add_ip(mapper, ip, data);
 }
 
 static void

commit fe014e24b6adfecdf8b3d0de1ef4d55a3eaf2b50
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 3 23:22:41 2017 -0400

    ftrace: Convert the rest of the function trigger over to the mapping functions
    
    As the data pointer for individual ips will soon be removed and no longer
    passed to the callback function probe handlers, convert the rest of the function
    trigger counters over to the new ftrace_func_mapper helper functions.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b99f6231281e..d9cbde8575a8 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -267,10 +267,12 @@ static struct tracer function_trace __tracer_data =
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-static void update_traceon_count(void **data, bool on)
+static void update_traceon_count(struct ftrace_probe_ops *ops,
+				 unsigned long ip, bool on)
 {
-	long *count = (long *)data;
-	long old_count = *count;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	long *count;
+	long old_count;
 
 	/*
 	 * Tracing gets disabled (or enabled) once per count.
@@ -301,7 +303,10 @@ static void update_traceon_count(void **data, bool on)
 	 * setting the tracing_on file. But we currently don't care
 	 * about that.
 	 */
-	if (!old_count)
+	count = (long *)ftrace_func_mapper_find_ip(mapper, ip);
+	old_count = *count;
+
+	if (old_count <= 0)
 		return;
 
 	/* Make sure we see count before checking tracing state */
@@ -315,10 +320,6 @@ static void update_traceon_count(void **data, bool on)
 	else
 		tracing_off();
 
-	/* unlimited? */
-	if (old_count == -1)
-		return;
-
 	/* Make sure tracing state is visible before updating count */
 	smp_wmb();
 
@@ -329,14 +330,14 @@ static void
 ftrace_traceon_count(unsigned long ip, unsigned long parent_ip,
 		     struct ftrace_probe_ops *ops, void **data)
 {
-	update_traceon_count(data, 1);
+	update_traceon_count(ops, ip, 1);
 }
 
 static void
 ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip,
 		      struct ftrace_probe_ops *ops, void **data)
 {
-	update_traceon_count(data, 0);
+	update_traceon_count(ops, ip, 0);
 }
 
 static void
@@ -379,47 +380,56 @@ static void
 ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip,
 			struct ftrace_probe_ops *ops, void **data)
 {
-	long *count = (long *)data;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	long *count;
 	long old_count;
 	long new_count;
 
+	if (!tracing_is_on())
+		return;
+
+	/* unlimited? */
+	if (!mapper) {
+		trace_dump_stack(STACK_SKIP);
+		return;
+	}
+
+	count = (long *)ftrace_func_mapper_find_ip(mapper, ip);
+
 	/*
 	 * Stack traces should only execute the number of times the
 	 * user specified in the counter.
 	 */
 	do {
-
-		if (!tracing_is_on())
-			return;
-
 		old_count = *count;
 
 		if (!old_count)
 			return;
 
-		/* unlimited? */
-		if (old_count == -1) {
-			trace_dump_stack(STACK_SKIP);
-			return;
-		}
-
 		new_count = old_count - 1;
 		new_count = cmpxchg(count, old_count, new_count);
 		if (new_count == old_count)
 			trace_dump_stack(STACK_SKIP);
 
+		if (!tracing_is_on())
+			return;
+
 	} while (new_count != old_count);
 }
 
-static int update_count(void **data)
+static int update_count(struct ftrace_probe_ops *ops, unsigned long ip)
 {
-	unsigned long *count = (long *)data;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	long *count = NULL;
 
-	if (!*count)
-		return 0;
+	if (mapper)
+		count = (long *)ftrace_func_mapper_find_ip(mapper, ip);
 
-	if (*count != -1)
+	if (count) {
+		if (*count <= 0)
+			return 0;
 		(*count)--;
+	}
 
 	return 1;
 }
@@ -428,7 +438,7 @@ static void
 ftrace_dump_probe(unsigned long ip, unsigned long parent_ip,
 	struct ftrace_probe_ops *ops, void **data)
 {
-	if (update_count(data))
+	if (update_count(ops, ip))
 		ftrace_dump(DUMP_ALL);
 }
 
@@ -437,22 +447,26 @@ static void
 ftrace_cpudump_probe(unsigned long ip, unsigned long parent_ip,
 	struct ftrace_probe_ops *ops, void **data)
 {
-	if (update_count(data))
+	if (update_count(ops, ip))
 		ftrace_dump(DUMP_ORIG);
 }
 
 static int
 ftrace_probe_print(const char *name, struct seq_file *m,
-		   unsigned long ip, void *data)
+		   unsigned long ip, struct ftrace_probe_ops *ops)
 {
-	long count = (long)data;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	long *count = NULL;
 
 	seq_printf(m, "%ps:%s", (void *)ip, name);
 
-	if (count == -1)
-		seq_puts(m, ":unlimited\n");
+	if (mapper)
+		count = (long *)ftrace_func_mapper_find_ip(mapper, ip);
+
+	if (count)
+		seq_printf(m, ":count=%ld\n", *count);
 	else
-		seq_printf(m, ":count=%ld\n", count);
+		seq_puts(m, ":unlimited\n");
 
 	return 0;
 }
@@ -461,55 +475,82 @@ static int
 ftrace_traceon_print(struct seq_file *m, unsigned long ip,
 			 struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("traceon", m, ip, data);
+	return ftrace_probe_print("traceon", m, ip, ops);
 }
 
 static int
 ftrace_traceoff_print(struct seq_file *m, unsigned long ip,
 			 struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("traceoff", m, ip, data);
+	return ftrace_probe_print("traceoff", m, ip, ops);
 }
 
 static int
 ftrace_stacktrace_print(struct seq_file *m, unsigned long ip,
 			struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("stacktrace", m, ip, data);
+	return ftrace_probe_print("stacktrace", m, ip, ops);
 }
 
 static int
 ftrace_dump_print(struct seq_file *m, unsigned long ip,
 			struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("dump", m, ip, data);
+	return ftrace_probe_print("dump", m, ip, ops);
 }
 
 static int
 ftrace_cpudump_print(struct seq_file *m, unsigned long ip,
 			struct ftrace_probe_ops *ops, void *data)
 {
-	return ftrace_probe_print("cpudump", m, ip, data);
+	return ftrace_probe_print("cpudump", m, ip, ops);
+}
+
+
+static int
+ftrace_count_init(struct ftrace_probe_ops *ops, unsigned long ip,
+		     void **data)
+{
+	struct ftrace_func_mapper *mapper = ops->private_data;
+
+	return ftrace_func_mapper_add_ip(mapper, ip, *data);
+}
+
+static void
+ftrace_count_free(struct ftrace_probe_ops *ops, unsigned long ip,
+		  void **_data)
+{
+	struct ftrace_func_mapper *mapper = ops->private_data;
+
+	ftrace_func_mapper_remove_ip(mapper, ip);
 }
 
 static struct ftrace_probe_ops traceon_count_probe_ops = {
 	.func			= ftrace_traceon_count,
 	.print			= ftrace_traceon_print,
+	.init			= ftrace_count_init,
+	.free			= ftrace_count_free,
 };
 
 static struct ftrace_probe_ops traceoff_count_probe_ops = {
 	.func			= ftrace_traceoff_count,
 	.print			= ftrace_traceoff_print,
+	.init			= ftrace_count_init,
+	.free			= ftrace_count_free,
 };
 
 static struct ftrace_probe_ops stacktrace_count_probe_ops = {
 	.func			= ftrace_stacktrace_count,
 	.print			= ftrace_stacktrace_print,
+	.init			= ftrace_count_init,
+	.free			= ftrace_count_free,
 };
 
 static struct ftrace_probe_ops dump_probe_ops = {
 	.func			= ftrace_dump_probe,
 	.print			= ftrace_dump_print,
+	.init			= ftrace_count_init,
+	.free			= ftrace_count_free,
 };
 
 static struct ftrace_probe_ops cpudump_probe_ops = {
@@ -558,6 +599,12 @@ ftrace_trace_probe_callback(struct ftrace_probe_ops *ops,
 	if (!strlen(number))
 		goto out_reg;
 
+	if (!ops->private_data) {
+		ops->private_data = allocate_ftrace_func_mapper();
+		if (!ops->private_data)
+			return -ENOMEM;
+	}
+
 	/*
 	 * We use the callback data field (which is a pointer)
 	 * as our counter.

commit bca6c8d0480a8aa5c86f8f416db96c71f6b79e29
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 3 18:18:47 2017 -0400

    ftrace: Pass probe ops to probe function
    
    In preparation to cleaning up the probe function registration code, the
    "data" parameter will eventually be removed from the probe->func() call.
    Instead it will receive its own "ops" function, in which it can set up its
    own data that it needs to map.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 4199ca61b0e5..b99f6231281e 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -326,19 +326,22 @@ static void update_traceon_count(void **data, bool on)
 }
 
 static void
-ftrace_traceon_count(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_traceon_count(unsigned long ip, unsigned long parent_ip,
+		     struct ftrace_probe_ops *ops, void **data)
 {
 	update_traceon_count(data, 1);
 }
 
 static void
-ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip,
+		      struct ftrace_probe_ops *ops, void **data)
 {
 	update_traceon_count(data, 0);
 }
 
 static void
-ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_traceon(unsigned long ip, unsigned long parent_ip,
+	       struct ftrace_probe_ops *ops, void **data)
 {
 	if (tracing_is_on())
 		return;
@@ -347,7 +350,8 @@ ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
 }
 
 static void
-ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_traceoff(unsigned long ip, unsigned long parent_ip,
+		struct ftrace_probe_ops *ops, void **data)
 {
 	if (!tracing_is_on())
 		return;
@@ -365,13 +369,15 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
 #define STACK_SKIP 4
 
 static void
-ftrace_stacktrace(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_stacktrace(unsigned long ip, unsigned long parent_ip,
+		  struct ftrace_probe_ops *ops, void **data)
 {
 	trace_dump_stack(STACK_SKIP);
 }
 
 static void
-ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip,
+			struct ftrace_probe_ops *ops, void **data)
 {
 	long *count = (long *)data;
 	long old_count;
@@ -419,7 +425,8 @@ static int update_count(void **data)
 }
 
 static void
-ftrace_dump_probe(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_dump_probe(unsigned long ip, unsigned long parent_ip,
+	struct ftrace_probe_ops *ops, void **data)
 {
 	if (update_count(data))
 		ftrace_dump(DUMP_ALL);
@@ -427,7 +434,8 @@ ftrace_dump_probe(unsigned long ip, unsigned long parent_ip, void **data)
 
 /* Only dump the current CPU buffer. */
 static void
-ftrace_cpudump_probe(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_cpudump_probe(unsigned long ip, unsigned long parent_ip,
+	struct ftrace_probe_ops *ops, void **data)
 {
 	if (update_count(data))
 		ftrace_dump(DUMP_ORIG);

commit dbeafd0d6131d0f6ae8cd7551f5f4bf8c54aa49a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 3 13:48:42 2017 -0500

    ftrace: Have function tracing start in early boot up
    
    Register the function tracer right after the tracing buffers are initialized
    in early boot up. This will allow function tracing to begin early if it is
    enabled via the kernel command line.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 0efa00d80623..4199ca61b0e5 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -687,9 +687,8 @@ static inline int init_func_cmd_traceon(void)
 }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
-static __init int init_function_trace(void)
+__init int init_function_trace(void)
 {
 	init_func_cmd_traceon();
 	return register_tracer(&function_trace);
 }
-core_initcall(init_function_trace);

commit 345ddcc882d8896dcbdcb3e0ee4a415fc23ec8b0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 22 18:11:33 2016 -0400

    ftrace: Have set_ftrace_pid use the bitmap like events do
    
    Convert set_ftrace_pid to use the bitmap like set_event_pid does. This
    allows for instances to use the pid filtering as well, and will allow for
    function-fork option to set if the children of a traced function should be
    traced or not.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 5a095c2e4b69..0efa00d80623 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -43,7 +43,7 @@ static int allocate_ftrace_ops(struct trace_array *tr)
 
 	/* Currently only the non stack verision is supported */
 	ops->func = function_trace_call;
-	ops->flags = FTRACE_OPS_FL_RECURSION_SAFE;
+	ops->flags = FTRACE_OPS_FL_RECURSION_SAFE | FTRACE_OPS_FL_PID;
 
 	tr->ops = ops;
 	ops->private = tr;

commit d39cdd2036a63eef17a14efbd969405ca5612886
Author: Chunyu Hu <chuhu@redhat.com>
Date:   Tue Mar 8 21:37:01 2016 +0800

    tracing: Make tracer_flags use the right set_flag callback
    
    When I was updating the ftrace_stress test of ltp. I encountered
    a strange phenomemon, excute following steps:
    
    echo nop > /sys/kernel/debug/tracing/current_tracer
    echo 0 > /sys/kernel/debug/tracing/options/funcgraph-cpu
    bash: echo: write error: Invalid argument
    
    check dmesg:
    [ 1024.903855] nop_test_refuse flag set to 0: we refuse.Now cat trace_options to see the result
    
    The reason is that the trace option test will randomly setup trace
    option under tracing/options no matter what the current_tracer is.
    but the set_tracer_option is always using the set_flag callback
    from the current_tracer. This patch adds a pointer to tracer_flags
    and make it point to the tracer it belongs to. When the option is
    setup, the set_flag of the right tracer will be used no matter
    what the the current_tracer is.
    
    And the old dummy_tracer_flags is used for all the tracers which
    doesn't have a tracer_flags, having issue to use it to save the
    pointer of a tracer. So remove it and use dynamic dummy tracer_flags
    for tracers needing a dummy tracer_flags, as a result, there are no
    tracers sharing tracer_flags, so remove the check code.
    
    And save the current tracer to trace_option_dentry seems not good as
    it may waste mem space when mount the debug/trace fs more than one time.
    
    Link: http://lkml.kernel.org/r/1457444222-8654-1-git-send-email-chuhu@redhat.com
    
    Signed-off-by: Chunyu Hu <chuhu@redhat.com>
    [ Fixed up function tracer options to work with the change ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index fcd41a166405..5a095c2e4b69 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -219,6 +219,8 @@ static void tracing_stop_function_trace(struct trace_array *tr)
 	unregister_ftrace_function(tr->ops);
 }
 
+static struct tracer function_trace;
+
 static int
 func_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)
 {
@@ -228,6 +230,10 @@ func_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)
 		if (!!set == !!(func_flags.val & TRACE_FUNC_OPT_STACK))
 			break;
 
+		/* We can change this flag when not running. */
+		if (tr->current_trace != &function_trace)
+			break;
+
 		unregister_ftrace_function(tr->ops);
 
 		if (set) {

commit 0af26492d5f5c00a08d52e9f3f3831faead90246
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Nov 20 10:05:36 2014 -0500

    tracing/trivial: Fix typos and make an int into a bool
    
    Fix up a few typos in comments and convert an int into a bool in
    update_traceon_count().
    
    Link: http://lkml.kernel.org/r/546DD445.5080108@hitachi.com
    
    Suggested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 973db52eb070..fcd41a166405 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -261,14 +261,14 @@ static struct tracer function_trace __tracer_data =
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-static void update_traceon_count(void **data, int on)
+static void update_traceon_count(void **data, bool on)
 {
 	long *count = (long *)data;
 	long old_count = *count;
 
 	/*
 	 * Tracing gets disabled (or enabled) once per count.
-	 * This function can be called at the same time on mulitple CPUs.
+	 * This function can be called at the same time on multiple CPUs.
 	 * It is fine if both disable (or enable) tracing, as disabling
 	 * (or enabling) the second time doesn't do anything as the
 	 * state of the tracer is already disabled (or enabled).
@@ -288,7 +288,7 @@ static void update_traceon_count(void **data, int on)
 	 * the new state is visible before changing the counter by
 	 * one minus the old counter. This guarantees that another CPU
 	 * executing this code will see the new state before seeing
-	 * the new counter value, and would not do anthing if the new
+	 * the new counter value, and would not do anything if the new
 	 * counter is seen.
 	 *
 	 * Note, there is no synchronization between this and a user

commit a9ce7c36aa4256019180c590d60e2fad7431c749
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 17 23:08:24 2014 -0500

    tracing: Fix race of function probes counting
    
    The function probe counting for traceon and traceoff suffered a race
    condition where if the probe was executing on two or more CPUs at the
    same time, it could decrement the counter by more than one when
    disabling (or enabling) the tracer only once.
    
    The way the traceon and traceoff probes are suppose to work is that
    they disable (or enable) tracing once per count. If a user were to
    echo 'schedule:traceoff:3' into set_ftrace_filter, then when the
    schedule function was called, it would disable tracing. But the count
    should only be decremented once (to 2). Then if the user enabled tracing
    again (via tracing_on file), the next call to schedule would disable
    tracing again and the count would be decremented to 1.
    
    But if multiple CPUS called schedule at the same time, it is possible
    that the count would be decremented more than once because of the
    simple "count--" used.
    
    By reading the count into a local variable and using memory barriers
    we can guarantee that the count would only be decremented once per
    disable (or enable).
    
    The stack trace probe had a similar race, but here the stack trace will
    decrement for each time it is called. But this had the read-modify-
    write race, where it could stack trace more than the number of times
    that was specified. This case we use a cmpxchg to stack trace only the
    number of times specified.
    
    The dump probes can still use the old "update_count()" function as
    they only run once, and that is controlled by the dump logic
    itself.
    
    Link: http://lkml.kernel.org/r/20141118134643.4b550ee4@gandalf.local.home
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index a8e0c7666164..973db52eb070 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -261,37 +261,74 @@ static struct tracer function_trace __tracer_data =
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-static int update_count(void **data)
+static void update_traceon_count(void **data, int on)
 {
-	unsigned long *count = (long *)data;
+	long *count = (long *)data;
+	long old_count = *count;
 
-	if (!*count)
-		return 0;
+	/*
+	 * Tracing gets disabled (or enabled) once per count.
+	 * This function can be called at the same time on mulitple CPUs.
+	 * It is fine if both disable (or enable) tracing, as disabling
+	 * (or enabling) the second time doesn't do anything as the
+	 * state of the tracer is already disabled (or enabled).
+	 * What needs to be synchronized in this case is that the count
+	 * only gets decremented once, even if the tracer is disabled
+	 * (or enabled) twice, as the second one is really a nop.
+	 *
+	 * The memory barriers guarantee that we only decrement the
+	 * counter once. First the count is read to a local variable
+	 * and a read barrier is used to make sure that it is loaded
+	 * before checking if the tracer is in the state we want.
+	 * If the tracer is not in the state we want, then the count
+	 * is guaranteed to be the old count.
+	 *
+	 * Next the tracer is set to the state we want (disabled or enabled)
+	 * then a write memory barrier is used to make sure that
+	 * the new state is visible before changing the counter by
+	 * one minus the old counter. This guarantees that another CPU
+	 * executing this code will see the new state before seeing
+	 * the new counter value, and would not do anthing if the new
+	 * counter is seen.
+	 *
+	 * Note, there is no synchronization between this and a user
+	 * setting the tracing_on file. But we currently don't care
+	 * about that.
+	 */
+	if (!old_count)
+		return;
 
-	if (*count != -1)
-		(*count)--;
+	/* Make sure we see count before checking tracing state */
+	smp_rmb();
 
-	return 1;
+	if (on == !!tracing_is_on())
+		return;
+
+	if (on)
+		tracing_on();
+	else
+		tracing_off();
+
+	/* unlimited? */
+	if (old_count == -1)
+		return;
+
+	/* Make sure tracing state is visible before updating count */
+	smp_wmb();
+
+	*count = old_count - 1;
 }
 
 static void
 ftrace_traceon_count(unsigned long ip, unsigned long parent_ip, void **data)
 {
-	if (tracing_is_on())
-		return;
-
-	if (update_count(data))
-		tracing_on();
+	update_traceon_count(data, 1);
 }
 
 static void
 ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip, void **data)
 {
-	if (!tracing_is_on())
-		return;
-
-	if (update_count(data))
-		tracing_off();
+	update_traceon_count(data, 0);
 }
 
 static void
@@ -330,11 +367,49 @@ ftrace_stacktrace(unsigned long ip, unsigned long parent_ip, void **data)
 static void
 ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip, void **data)
 {
-	if (!tracing_is_on())
-		return;
+	long *count = (long *)data;
+	long old_count;
+	long new_count;
 
-	if (update_count(data))
-		trace_dump_stack(STACK_SKIP);
+	/*
+	 * Stack traces should only execute the number of times the
+	 * user specified in the counter.
+	 */
+	do {
+
+		if (!tracing_is_on())
+			return;
+
+		old_count = *count;
+
+		if (!old_count)
+			return;
+
+		/* unlimited? */
+		if (old_count == -1) {
+			trace_dump_stack(STACK_SKIP);
+			return;
+		}
+
+		new_count = old_count - 1;
+		new_count = cmpxchg(count, old_count, new_count);
+		if (new_count == old_count)
+			trace_dump_stack(STACK_SKIP);
+
+	} while (new_count != old_count);
+}
+
+static int update_count(void **data)
+{
+	unsigned long *count = (long *)data;
+
+	if (!*count)
+		return 0;
+
+	if (*count != -1)
+		(*count)--;
+
+	return 1;
 }
 
 static void

commit fa6f0cc751d377af3f4f1484bceb47dc10163753
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Sat Nov 8 21:42:10 2014 +0100

    tracing: Replace seq_printf by simpler equivalents
    
    Using seq_printf to print a simple string or a single character is a
    lot more expensive than it needs to be, since seq_puts and seq_putc
    exist.
    
    These patches do
    
      seq_printf(m, s) -> seq_puts(m, s)
      seq_printf(m, "%s", s) -> seq_puts(m, s)
      seq_printf(m, "%c", c) -> seq_putc(m, c)
    
    Subsequent patches will simplify further.
    
    Link: http://lkml.kernel.org/r/1415479332-25944-2-git-send-email-linux@rasmusvillemoes.dk
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 57f0ec962d2c..a8e0c7666164 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -361,7 +361,7 @@ ftrace_probe_print(const char *name, struct seq_file *m,
 	seq_printf(m, "%ps:%s", (void *)ip, name);
 
 	if (count == -1)
-		seq_printf(m, ":unlimited\n");
+		seq_puts(m, ":unlimited\n");
 	else
 		seq_printf(m, ":count=%ld\n", count);
 

commit b1169cc69ba96b124df820904a6d3eb775491d7f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Apr 29 17:54:37 2014 -0400

    tracing: Remove mock up poll wait function
    
    Now that the ring buffer has a built in way to wake up readers
    when there's data, using irq_work such that it is safe to do it
    in any context. But it was still using the old "poor man's"
    wait polling that checks every 1/10 of a second to see if it
    should wake up a waiter. This makes the latency for a wake up
    excruciatingly long. No need to do that anymore.
    
    Completely remove the different wait_poll types from the tracers
    and have them all use the default one now.
    
    Reported-by: Johannes Berg <johannes@sipsolutions.net>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 2d9482b8f26a..57f0ec962d2c 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -252,7 +252,6 @@ static struct tracer function_trace __tracer_data =
 	.init		= function_trace_init,
 	.reset		= function_trace_reset,
 	.start		= function_trace_start,
-	.wait_pipe	= poll_wait_pipe,
 	.flags		= &func_flags,
 	.set_flag	= func_set_flag,
 	.allow_instances = true,

commit 4104d326b670c2b66f575d2004daa28b2d1b4c8d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 17:01:58 2014 -0500

    ftrace: Remove global function list and call function directly
    
    Instead of having a list of global functions that are called,
    as only one global function is allow to be enabled at a time, there's
    no reason to have a list.
    
    Instead, simply have all the users of the global ops, use the global ops
    directly, instead of registering their own ftrace_ops. Just switch what
    function is used before enabling the function tracer.
    
    This removes a lot of code as well as the complexity involved with it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index ffd56351b521..2d9482b8f26a 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -26,8 +26,6 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 static void
 function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *op, struct pt_regs *pt_regs);
-static struct ftrace_ops trace_ops;
-static struct ftrace_ops trace_stack_ops;
 static struct tracer_flags func_flags;
 
 /* Our option */
@@ -83,28 +81,24 @@ void ftrace_destroy_function_files(struct trace_array *tr)
 
 static int function_trace_init(struct trace_array *tr)
 {
-	struct ftrace_ops *ops;
-
-	if (tr->flags & TRACE_ARRAY_FL_GLOBAL) {
-		/* There's only one global tr */
-		if (!trace_ops.private) {
-			trace_ops.private = tr;
-			trace_stack_ops.private = tr;
-		}
+	ftrace_func_t func;
 
-		if (func_flags.val & TRACE_FUNC_OPT_STACK)
-			ops = &trace_stack_ops;
-		else
-			ops = &trace_ops;
-		tr->ops = ops;
-	} else if (!tr->ops) {
-		/*
-		 * Instance trace_arrays get their ops allocated
-		 * at instance creation. Unless it failed
-		 * the allocation.
-		 */
+	/*
+	 * Instance trace_arrays get their ops allocated
+	 * at instance creation. Unless it failed
+	 * the allocation.
+	 */
+	if (!tr->ops)
 		return -ENOMEM;
-	}
+
+	/* Currently only the global instance can do stack tracing */
+	if (tr->flags & TRACE_ARRAY_FL_GLOBAL &&
+	    func_flags.val & TRACE_FUNC_OPT_STACK)
+		func = function_stack_trace_call;
+	else
+		func = function_trace_call;
+
+	ftrace_init_array_ops(tr, func);
 
 	tr->trace_buffer.cpu = get_cpu();
 	put_cpu();
@@ -118,6 +112,7 @@ static void function_trace_reset(struct trace_array *tr)
 {
 	tracing_stop_function_trace(tr);
 	tracing_stop_cmdline_record();
+	ftrace_reset_array_ops(tr);
 }
 
 static void function_trace_start(struct trace_array *tr)
@@ -199,18 +194,6 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	local_irq_restore(flags);
 }
 
-static struct ftrace_ops trace_ops __read_mostly =
-{
-	.func = function_trace_call,
-	.flags = FTRACE_OPS_FL_GLOBAL | FTRACE_OPS_FL_RECURSION_SAFE,
-};
-
-static struct ftrace_ops trace_stack_ops __read_mostly =
-{
-	.func = function_stack_trace_call,
-	.flags = FTRACE_OPS_FL_GLOBAL | FTRACE_OPS_FL_RECURSION_SAFE,
-};
-
 static struct tracer_opt func_opts[] = {
 #ifdef CONFIG_STACKTRACE
 	{ TRACER_OPT(func_stack_trace, TRACE_FUNC_OPT_STACK) },
@@ -248,10 +231,10 @@ func_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)
 		unregister_ftrace_function(tr->ops);
 
 		if (set) {
-			tr->ops = &trace_stack_ops;
+			tr->ops->func = function_stack_trace_call;
 			register_ftrace_function(tr->ops);
 		} else {
-			tr->ops = &trace_ops;
+			tr->ops->func = function_trace_call;
 			register_ftrace_function(tr->ops);
 		}
 

commit 5d6c97c55984b3b991400692f9e8568a702b93c0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 16 19:21:53 2014 -0400

    tracing: Do not try to recreated toplevel set_ftrace_* files
    
    With the restructing of the function tracer working with instances, the
    "top level" buffer is a bit special, as the function tracing is mapped
    to the same set of filters. This is done by using a "global_ops" descriptor
    and having the "set_ftrace_filter" and "set_ftrace_notrace" map to it.
    
    When an instance is created, it creates the same files but its for the
    local instance and not the global_ops.
    
    The issues is that the local instance creation shares some code with
    the global instance one and we end up trying to create th top level
    "set_ftrace_*" files twice, and on boot up, we get an error like this:
    
     Could not create debugfs 'set_ftrace_filter' entry
     Could not create debugfs 'set_ftrace_notrace' entry
    
    The reason they failed to be created was because they were created
    twice, and the second time gives this error as you can not create the
    same file twice.
    
    Reported-by: Borislav Petkov <bp@alien8.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 5b781d2be383..ffd56351b521 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -58,12 +58,16 @@ int ftrace_create_function_files(struct trace_array *tr,
 {
 	int ret;
 
-	/* The top level array uses the "global_ops". */
-	if (!(tr->flags & TRACE_ARRAY_FL_GLOBAL)) {
-		ret = allocate_ftrace_ops(tr);
-		if (ret)
-			return ret;
-	}
+	/*
+	 * The top level array uses the "global_ops", and the files are
+	 * created on boot up.
+	 */
+	if (tr->flags & TRACE_ARRAY_FL_GLOBAL)
+		return 0;
+
+	ret = allocate_ftrace_ops(tr);
+	if (ret)
+		return ret;
 
 	ftrace_create_filter_files(tr->ops, parent);
 

commit 591dffdade9f07692a7dd3ed16830ec24e901ece
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 16:17:45 2014 -0500

    ftrace: Allow for function tracing instance to filter functions
    
    Create a "set_ftrace_filter" and "set_ftrace_notrace" files in the instance
    directories to let users filter of functions to trace for the given instance.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 3f8dc1ce8b9c..5b781d2be383 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -52,10 +52,34 @@ static int allocate_ftrace_ops(struct trace_array *tr)
 	return 0;
 }
 
+
+int ftrace_create_function_files(struct trace_array *tr,
+				 struct dentry *parent)
+{
+	int ret;
+
+	/* The top level array uses the "global_ops". */
+	if (!(tr->flags & TRACE_ARRAY_FL_GLOBAL)) {
+		ret = allocate_ftrace_ops(tr);
+		if (ret)
+			return ret;
+	}
+
+	ftrace_create_filter_files(tr->ops, parent);
+
+	return 0;
+}
+
+void ftrace_destroy_function_files(struct trace_array *tr)
+{
+	ftrace_destroy_filter_files(tr->ops);
+	kfree(tr->ops);
+	tr->ops = NULL;
+}
+
 static int function_trace_init(struct trace_array *tr)
 {
 	struct ftrace_ops *ops;
-	int ret;
 
 	if (tr->flags & TRACE_ARRAY_FL_GLOBAL) {
 		/* There's only one global tr */
@@ -69,10 +93,13 @@ static int function_trace_init(struct trace_array *tr)
 		else
 			ops = &trace_ops;
 		tr->ops = ops;
-	} else {
-		ret = allocate_ftrace_ops(tr);
-		if (ret)
-			return ret;
+	} else if (!tr->ops) {
+		/*
+		 * Instance trace_arrays get their ops allocated
+		 * at instance creation. Unless it failed
+		 * the allocation.
+		 */
+		return -ENOMEM;
 	}
 
 	tr->trace_buffer.cpu = get_cpu();
@@ -87,9 +114,6 @@ static void function_trace_reset(struct trace_array *tr)
 {
 	tracing_stop_function_trace(tr);
 	tracing_stop_cmdline_record();
-	if (!(tr->flags & TRACE_ARRAY_FL_GLOBAL))
-		kfree(tr->ops);
-	tr->ops = NULL;
 }
 
 static void function_trace_start(struct trace_array *tr)

commit f20a580627f43e73e4e57cb37e3864080ca06088
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Nov 7 20:08:58 2013 -0500

    ftrace: Allow instances to use function tracing
    
    Allow instances (sub-buffers) to enable function tracing.
    Each instance will have its own function tracing capability.
    For now, instances will not have function stack tracing, or will
    they be able to pick and choose what functions they can trace.
    
    Picking and choosing their own functions will come later.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 85e517e84f50..3f8dc1ce8b9c 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -13,33 +13,83 @@
 #include <linux/debugfs.h>
 #include <linux/uaccess.h>
 #include <linux/ftrace.h>
+#include <linux/slab.h>
 #include <linux/fs.h>
 
 #include "trace.h"
 
-/* function tracing enabled */
-static int			ftrace_function_enabled;
+static void tracing_start_function_trace(struct trace_array *tr);
+static void tracing_stop_function_trace(struct trace_array *tr);
+static void
+function_trace_call(unsigned long ip, unsigned long parent_ip,
+		    struct ftrace_ops *op, struct pt_regs *pt_regs);
+static void
+function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
+			  struct ftrace_ops *op, struct pt_regs *pt_regs);
+static struct ftrace_ops trace_ops;
+static struct ftrace_ops trace_stack_ops;
+static struct tracer_flags func_flags;
+
+/* Our option */
+enum {
+	TRACE_FUNC_OPT_STACK	= 0x1,
+};
+
+static int allocate_ftrace_ops(struct trace_array *tr)
+{
+	struct ftrace_ops *ops;
 
-static struct trace_array	*func_trace;
+	ops = kzalloc(sizeof(*ops), GFP_KERNEL);
+	if (!ops)
+		return -ENOMEM;
 
-static void tracing_start_function_trace(void);
-static void tracing_stop_function_trace(void);
+	/* Currently only the non stack verision is supported */
+	ops->func = function_trace_call;
+	ops->flags = FTRACE_OPS_FL_RECURSION_SAFE;
+
+	tr->ops = ops;
+	ops->private = tr;
+	return 0;
+}
 
 static int function_trace_init(struct trace_array *tr)
 {
-	func_trace = tr;
+	struct ftrace_ops *ops;
+	int ret;
+
+	if (tr->flags & TRACE_ARRAY_FL_GLOBAL) {
+		/* There's only one global tr */
+		if (!trace_ops.private) {
+			trace_ops.private = tr;
+			trace_stack_ops.private = tr;
+		}
+
+		if (func_flags.val & TRACE_FUNC_OPT_STACK)
+			ops = &trace_stack_ops;
+		else
+			ops = &trace_ops;
+		tr->ops = ops;
+	} else {
+		ret = allocate_ftrace_ops(tr);
+		if (ret)
+			return ret;
+	}
+
 	tr->trace_buffer.cpu = get_cpu();
 	put_cpu();
 
 	tracing_start_cmdline_record();
-	tracing_start_function_trace();
+	tracing_start_function_trace(tr);
 	return 0;
 }
 
 static void function_trace_reset(struct trace_array *tr)
 {
-	tracing_stop_function_trace();
+	tracing_stop_function_trace(tr);
 	tracing_stop_cmdline_record();
+	if (!(tr->flags & TRACE_ARRAY_FL_GLOBAL))
+		kfree(tr->ops);
+	tr->ops = NULL;
 }
 
 static void function_trace_start(struct trace_array *tr)
@@ -47,25 +97,18 @@ static void function_trace_start(struct trace_array *tr)
 	tracing_reset_online_cpus(&tr->trace_buffer);
 }
 
-/* Our option */
-enum {
-	TRACE_FUNC_OPT_STACK	= 0x1,
-};
-
-static struct tracer_flags func_flags;
-
 static void
 function_trace_call(unsigned long ip, unsigned long parent_ip,
 		    struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
-	struct trace_array *tr = func_trace;
+	struct trace_array *tr = op->private;
 	struct trace_array_cpu *data;
 	unsigned long flags;
 	int bit;
 	int cpu;
 	int pc;
 
-	if (unlikely(!ftrace_function_enabled))
+	if (unlikely(!tr->function_enabled))
 		return;
 
 	pc = preempt_count();
@@ -91,14 +134,14 @@ static void
 function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
-	struct trace_array *tr = func_trace;
+	struct trace_array *tr = op->private;
 	struct trace_array_cpu *data;
 	unsigned long flags;
 	long disabled;
 	int cpu;
 	int pc;
 
-	if (unlikely(!ftrace_function_enabled))
+	if (unlikely(!tr->function_enabled))
 		return;
 
 	/*
@@ -128,7 +171,6 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	local_irq_restore(flags);
 }
 
-
 static struct ftrace_ops trace_ops __read_mostly =
 {
 	.func = function_trace_call,
@@ -153,26 +195,17 @@ static struct tracer_flags func_flags = {
 	.opts = func_opts
 };
 
-static void tracing_start_function_trace(void)
+static void tracing_start_function_trace(struct trace_array *tr)
 {
-	ftrace_function_enabled = 0;
-
-	if (func_flags.val & TRACE_FUNC_OPT_STACK)
-		register_ftrace_function(&trace_stack_ops);
-	else
-		register_ftrace_function(&trace_ops);
-
-	ftrace_function_enabled = 1;
+	tr->function_enabled = 0;
+	register_ftrace_function(tr->ops);
+	tr->function_enabled = 1;
 }
 
-static void tracing_stop_function_trace(void)
+static void tracing_stop_function_trace(struct trace_array *tr)
 {
-	ftrace_function_enabled = 0;
-
-	if (func_flags.val & TRACE_FUNC_OPT_STACK)
-		unregister_ftrace_function(&trace_stack_ops);
-	else
-		unregister_ftrace_function(&trace_ops);
+	tr->function_enabled = 0;
+	unregister_ftrace_function(tr->ops);
 }
 
 static int
@@ -184,12 +217,14 @@ func_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)
 		if (!!set == !!(func_flags.val & TRACE_FUNC_OPT_STACK))
 			break;
 
+		unregister_ftrace_function(tr->ops);
+
 		if (set) {
-			unregister_ftrace_function(&trace_ops);
-			register_ftrace_function(&trace_stack_ops);
+			tr->ops = &trace_stack_ops;
+			register_ftrace_function(tr->ops);
 		} else {
-			unregister_ftrace_function(&trace_stack_ops);
-			register_ftrace_function(&trace_ops);
+			tr->ops = &trace_ops;
+			register_ftrace_function(tr->ops);
 		}
 
 		break;
@@ -209,6 +244,7 @@ static struct tracer function_trace __tracer_data =
 	.wait_pipe	= poll_wait_pipe,
 	.flags		= &func_flags,
 	.set_flag	= func_set_flag,
+	.allow_instances = true,
 #ifdef CONFIG_FTRACE_SELFTEST
 	.selftest	= trace_selftest_startup_function,
 #endif

commit 8c1a49aedb73fb2f15aaa32ad9e2e1c4289f45cb
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 11:13:54 2014 -0500

    tracing: Pass trace_array to set_flag callback
    
    As options (flags) may affect instances instead of being global
    the set_flag() callbacks need to receive the trace_array descriptor
    of the instance they will be modifying.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 38fe1483c508..85e517e84f50 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -175,7 +175,8 @@ static void tracing_stop_function_trace(void)
 		unregister_ftrace_function(&trace_ops);
 }
 
-static int func_set_flag(u32 old_flags, u32 bit, int set)
+static int
+func_set_flag(struct trace_array *tr, u32 old_flags, u32 bit, int set)
 {
 	switch (bit) {
 	case TRACE_FUNC_OPT_STACK:

commit 8f768993394a8c0d3801033c11fd86ce8c88dcac
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jul 18 14:41:51 2013 -0400

    tracing: Add ref_data to function and fgraph tracer structs
    
    The selftest for function and function graph tracers are defined as
    __init, as they are only executed at boot up. The "tracer" structs
    that are associated to those tracers are not setup as __init as they
    are used after boot. To stop mismatch warnings, those structures
    need to be annotated with __ref_data.
    
    Currently, the tracer structures are defined to __read_mostly, as they
    do not really change. But in the future they should be converted to
    consts, but that will take a little work because they have a "next"
    pointer that gets updated when they are registered. That will have to
    wait till the next major release.
    
    Link: http://lkml.kernel.org/r/1373596735.17876.84.camel@gandalf.local.home
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Reported-by: Chen Gang <gang.chen@asianux.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b863f93b30f3..38fe1483c508 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -199,7 +199,7 @@ static int func_set_flag(u32 old_flags, u32 bit, int set)
 	return 0;
 }
 
-static struct tracer function_trace __read_mostly =
+static struct tracer function_trace __tracer_data =
 {
 	.name		= "function",
 	.init		= function_trace_init,

commit 90e3c03c3a09a7b176b3fe59d78f5d9755ac8e37
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Apr 30 19:00:46 2013 -0400

    tracing: Add function probe to trigger a ftrace dump of current CPU trace
    
    Add the "cpudump" command to have the current CPU ftrace buffer dumped
    to console if a function is hit. This is useful when debugging a
    tripple fault, where you have an idea of a function that is called
    just before the tripple fault occurs, and can tell ftrace to dump its
    content out to the console before it continues.
    
    This differs from the "dump" command as it only dumps the content of
    the ring buffer for the currently executing CPU, and does not show
    the contents of the other CPUs.
    
    Format is:
    
      <function>:cpudump
    
    echo 'bad_address:cpudump' > /debug/tracing/set_ftrace_filter
    
    To remove this:
    
    echo '!bad_address:cpudump' > /debug/tracing/set_ftrace_filter
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index d7c8719734b8..b863f93b30f3 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -297,6 +297,14 @@ ftrace_dump_probe(unsigned long ip, unsigned long parent_ip, void **data)
 		ftrace_dump(DUMP_ALL);
 }
 
+/* Only dump the current CPU buffer. */
+static void
+ftrace_cpudump_probe(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	if (update_count(data))
+		ftrace_dump(DUMP_ORIG);
+}
+
 static int
 ftrace_probe_print(const char *name, struct seq_file *m,
 		   unsigned long ip, void *data)
@@ -341,6 +349,13 @@ ftrace_dump_print(struct seq_file *m, unsigned long ip,
 	return ftrace_probe_print("dump", m, ip, data);
 }
 
+static int
+ftrace_cpudump_print(struct seq_file *m, unsigned long ip,
+			struct ftrace_probe_ops *ops, void *data)
+{
+	return ftrace_probe_print("cpudump", m, ip, data);
+}
+
 static struct ftrace_probe_ops traceon_count_probe_ops = {
 	.func			= ftrace_traceon_count,
 	.print			= ftrace_traceon_print,
@@ -361,6 +376,11 @@ static struct ftrace_probe_ops dump_probe_ops = {
 	.print			= ftrace_dump_print,
 };
 
+static struct ftrace_probe_ops cpudump_probe_ops = {
+	.func			= ftrace_cpudump_probe,
+	.print			= ftrace_cpudump_print,
+};
+
 static struct ftrace_probe_ops traceon_probe_ops = {
 	.func			= ftrace_traceon,
 	.print			= ftrace_traceon_print,
@@ -457,6 +477,19 @@ ftrace_dump_callback(struct ftrace_hash *hash,
 					   "1", enable);
 }
 
+static int
+ftrace_cpudump_callback(struct ftrace_hash *hash,
+			   char *glob, char *cmd, char *param, int enable)
+{
+	struct ftrace_probe_ops *ops;
+
+	ops = &cpudump_probe_ops;
+
+	/* Only dump once. */
+	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+					   "1", enable);
+}
+
 static struct ftrace_func_command ftrace_traceon_cmd = {
 	.name			= "traceon",
 	.func			= ftrace_trace_onoff_callback,
@@ -477,6 +510,11 @@ static struct ftrace_func_command ftrace_dump_cmd = {
 	.func			= ftrace_dump_callback,
 };
 
+static struct ftrace_func_command ftrace_cpudump_cmd = {
+	.name			= "cpudump",
+	.func			= ftrace_cpudump_callback,
+};
+
 static int __init init_func_cmd_traceon(void)
 {
 	int ret;
@@ -497,8 +535,14 @@ static int __init init_func_cmd_traceon(void)
 	if (ret)
 		goto out_free_stacktrace;
 
+	ret = register_ftrace_command(&ftrace_cpudump_cmd);
+	if (ret)
+		goto out_free_dump;
+
 	return 0;
 
+ out_free_dump:
+	unregister_ftrace_command(&ftrace_dump_cmd);
  out_free_stacktrace:
 	unregister_ftrace_command(&ftrace_stacktrace_cmd);
  out_free_traceon:

commit ad71d889b88055e61e3970a6744a271a51a94f42
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Apr 30 15:46:14 2013 -0400

    tracing: Add function probe to trigger a ftrace dump to console
    
    Add the "dump" command to have the ftrace buffer dumped to console if
    a function is hit. This is useful when debugging a tripple fault,
    where you have an idea of a function that is called just before the
    tripple fault occurs, and can tell ftrace to dump its content out
    to the console before it continues.
    
    Format is:
    
      <function>:dump
    
    echo 'bad_address:dump' > /debug/tracing/set_ftrace_filter
    
    To remove this:
    
    echo '!bad_address:dump' > /debug/tracing/set_ftrace_filter
    
    Requested-by: Luis Claudio R. Goncalves <lclaudio@uudg.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index c4d6d7191988..d7c8719734b8 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -290,6 +290,13 @@ ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip, void **data)
 		trace_dump_stack(STACK_SKIP);
 }
 
+static void
+ftrace_dump_probe(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	if (update_count(data))
+		ftrace_dump(DUMP_ALL);
+}
+
 static int
 ftrace_probe_print(const char *name, struct seq_file *m,
 		   unsigned long ip, void *data)
@@ -327,6 +334,13 @@ ftrace_stacktrace_print(struct seq_file *m, unsigned long ip,
 	return ftrace_probe_print("stacktrace", m, ip, data);
 }
 
+static int
+ftrace_dump_print(struct seq_file *m, unsigned long ip,
+			struct ftrace_probe_ops *ops, void *data)
+{
+	return ftrace_probe_print("dump", m, ip, data);
+}
+
 static struct ftrace_probe_ops traceon_count_probe_ops = {
 	.func			= ftrace_traceon_count,
 	.print			= ftrace_traceon_print,
@@ -342,6 +356,11 @@ static struct ftrace_probe_ops stacktrace_count_probe_ops = {
 	.print			= ftrace_stacktrace_print,
 };
 
+static struct ftrace_probe_ops dump_probe_ops = {
+	.func			= ftrace_dump_probe,
+	.print			= ftrace_dump_print,
+};
+
 static struct ftrace_probe_ops traceon_probe_ops = {
 	.func			= ftrace_traceon,
 	.print			= ftrace_traceon_print,
@@ -425,6 +444,19 @@ ftrace_stacktrace_callback(struct ftrace_hash *hash,
 					   param, enable);
 }
 
+static int
+ftrace_dump_callback(struct ftrace_hash *hash,
+			   char *glob, char *cmd, char *param, int enable)
+{
+	struct ftrace_probe_ops *ops;
+
+	ops = &dump_probe_ops;
+
+	/* Only dump once. */
+	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+					   "1", enable);
+}
+
 static struct ftrace_func_command ftrace_traceon_cmd = {
 	.name			= "traceon",
 	.func			= ftrace_trace_onoff_callback,
@@ -440,6 +472,11 @@ static struct ftrace_func_command ftrace_stacktrace_cmd = {
 	.func			= ftrace_stacktrace_callback,
 };
 
+static struct ftrace_func_command ftrace_dump_cmd = {
+	.name			= "dump",
+	.func			= ftrace_dump_callback,
+};
+
 static int __init init_func_cmd_traceon(void)
 {
 	int ret;
@@ -450,13 +487,25 @@ static int __init init_func_cmd_traceon(void)
 
 	ret = register_ftrace_command(&ftrace_traceon_cmd);
 	if (ret)
-		unregister_ftrace_command(&ftrace_traceoff_cmd);
+		goto out_free_traceoff;
 
 	ret = register_ftrace_command(&ftrace_stacktrace_cmd);
-	if (ret) {
-		unregister_ftrace_command(&ftrace_traceoff_cmd);
-		unregister_ftrace_command(&ftrace_traceon_cmd);
-	}
+	if (ret)
+		goto out_free_traceon;
+
+	ret = register_ftrace_command(&ftrace_dump_cmd);
+	if (ret)
+		goto out_free_stacktrace;
+
+	return 0;
+
+ out_free_stacktrace:
+	unregister_ftrace_command(&ftrace_stacktrace_cmd);
+ out_free_traceon:
+	unregister_ftrace_command(&ftrace_traceon_cmd);
+ out_free_traceoff:
+	unregister_ftrace_command(&ftrace_traceoff_cmd);
+
 	return ret;
 }
 #else

commit dd42cd3ea96d687f15525c4f14fa582702db223f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Mar 13 10:17:50 2013 -0400

    tracing: Add function probe to trigger stack traces
    
    Add a function probe that will cause a stack trace to be traced in
    the ring buffer when the given function(s) are called.
    
    format is:
    
     <function>:stacktrace[:<count>]
    
     echo 'schedule:stacktrace' > /debug/tracing/set_ftrace_filter
     cat /debug/tracing/trace_pipe
         kworker/2:0-4329  [002] ...2  2933.558007: <stack trace>
     => kthread
     => ret_from_fork
              <idle>-0     [000] .N.2  2933.558019: <stack trace>
     => rest_init
     => start_kernel
     => x86_64_start_reservations
     => x86_64_start_kernel
         kworker/2:0-4329  [002] ...2  2933.558109: <stack trace>
     => kthread
     => ret_from_fork
    [...]
    
    This can be set to only trace a specific amount of times:
    
     echo 'schedule:stacktrace:3' > /debug/tracing/set_ftrace_filter
     cat /debug/tracing/trace_pipe
               <...>-58    [003] ...2   841.801694: <stack trace>
     => kthread
     => ret_from_fork
              <idle>-0     [001] .N.2   841.801697: <stack trace>
     => start_secondary
               <...>-2059  [001] ...2   841.801736: <stack trace>
     => wait_for_common
     => wait_for_completion
     => flush_work
     => tty_flush_to_ldisc
     => input_available_p
     => n_tty_poll
     => tty_poll
     => do_select
     => core_sys_select
     => sys_select
     => system_call_fastpath
    
    To remove these:
    
     echo '!schedule:stacktrace' > /debug/tracing/set_ftrace_filter
     echo '!schedule:stacktrace:0' > /debug/tracing/set_ftrace_filter
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 043b2425ae73..c4d6d7191988 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -265,56 +265,103 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
 	tracing_off();
 }
 
+/*
+ * Skip 4:
+ *   ftrace_stacktrace()
+ *   function_trace_probe_call()
+ *   ftrace_ops_list_func()
+ *   ftrace_call()
+ */
+#define STACK_SKIP 4
+
+static void
+ftrace_stacktrace(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	trace_dump_stack(STACK_SKIP);
+}
+
+static void
+ftrace_stacktrace_count(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	if (!tracing_is_on())
+		return;
+
+	if (update_count(data))
+		trace_dump_stack(STACK_SKIP);
+}
+
 static int
-ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
-			 struct ftrace_probe_ops *ops, void *data);
+ftrace_probe_print(const char *name, struct seq_file *m,
+		   unsigned long ip, void *data)
+{
+	long count = (long)data;
+
+	seq_printf(m, "%ps:%s", (void *)ip, name);
+
+	if (count == -1)
+		seq_printf(m, ":unlimited\n");
+	else
+		seq_printf(m, ":count=%ld\n", count);
+
+	return 0;
+}
+
+static int
+ftrace_traceon_print(struct seq_file *m, unsigned long ip,
+			 struct ftrace_probe_ops *ops, void *data)
+{
+	return ftrace_probe_print("traceon", m, ip, data);
+}
+
+static int
+ftrace_traceoff_print(struct seq_file *m, unsigned long ip,
+			 struct ftrace_probe_ops *ops, void *data)
+{
+	return ftrace_probe_print("traceoff", m, ip, data);
+}
+
+static int
+ftrace_stacktrace_print(struct seq_file *m, unsigned long ip,
+			struct ftrace_probe_ops *ops, void *data)
+{
+	return ftrace_probe_print("stacktrace", m, ip, data);
+}
 
 static struct ftrace_probe_ops traceon_count_probe_ops = {
 	.func			= ftrace_traceon_count,
-	.print			= ftrace_trace_onoff_print,
+	.print			= ftrace_traceon_print,
 };
 
 static struct ftrace_probe_ops traceoff_count_probe_ops = {
 	.func			= ftrace_traceoff_count,
-	.print			= ftrace_trace_onoff_print,
+	.print			= ftrace_traceoff_print,
+};
+
+static struct ftrace_probe_ops stacktrace_count_probe_ops = {
+	.func			= ftrace_stacktrace_count,
+	.print			= ftrace_stacktrace_print,
 };
 
 static struct ftrace_probe_ops traceon_probe_ops = {
 	.func			= ftrace_traceon,
-	.print			= ftrace_trace_onoff_print,
+	.print			= ftrace_traceon_print,
 };
 
 static struct ftrace_probe_ops traceoff_probe_ops = {
 	.func			= ftrace_traceoff,
-	.print			= ftrace_trace_onoff_print,
+	.print			= ftrace_traceoff_print,
 };
 
-static int
-ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
-			 struct ftrace_probe_ops *ops, void *data)
-{
-	long count = (long)data;
-
-	seq_printf(m, "%ps:", (void *)ip);
-
-	if (ops == &traceon_probe_ops || ops == &traceon_count_probe_ops)
-		seq_printf(m, "traceon");
-	else
-		seq_printf(m, "traceoff");
-
-	if (count == -1)
-		seq_printf(m, ":unlimited\n");
-	else
-		seq_printf(m, ":count=%ld\n", count);
-
-	return 0;
-}
+static struct ftrace_probe_ops stacktrace_probe_ops = {
+	.func			= ftrace_stacktrace,
+	.print			= ftrace_stacktrace_print,
+};
 
 static int
-ftrace_trace_onoff_callback(struct ftrace_hash *hash,
-			    char *glob, char *cmd, char *param, int enable)
+ftrace_trace_probe_callback(struct ftrace_probe_ops *ops,
+			    struct ftrace_hash *hash, char *glob,
+			    char *cmd, char *param, int enable)
 {
-	struct ftrace_probe_ops *ops;
 	void *count = (void *)-1;
 	char *number;
 	int ret;
@@ -323,12 +370,6 @@ ftrace_trace_onoff_callback(struct ftrace_hash *hash,
 	if (!enable)
 		return -EINVAL;
 
-	/* we register both traceon and traceoff to this callback */
-	if (strcmp(cmd, "traceon") == 0)
-		ops = param ? &traceon_count_probe_ops : &traceon_probe_ops;
-	else
-		ops = param ? &traceoff_count_probe_ops : &traceoff_probe_ops;
-
 	if (glob[0] == '!') {
 		unregister_ftrace_function_probe_func(glob+1, ops);
 		return 0;
@@ -356,6 +397,34 @@ ftrace_trace_onoff_callback(struct ftrace_hash *hash,
 	return ret < 0 ? ret : 0;
 }
 
+static int
+ftrace_trace_onoff_callback(struct ftrace_hash *hash,
+			    char *glob, char *cmd, char *param, int enable)
+{
+	struct ftrace_probe_ops *ops;
+
+	/* we register both traceon and traceoff to this callback */
+	if (strcmp(cmd, "traceon") == 0)
+		ops = param ? &traceon_count_probe_ops : &traceon_probe_ops;
+	else
+		ops = param ? &traceoff_count_probe_ops : &traceoff_probe_ops;
+
+	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+					   param, enable);
+}
+
+static int
+ftrace_stacktrace_callback(struct ftrace_hash *hash,
+			   char *glob, char *cmd, char *param, int enable)
+{
+	struct ftrace_probe_ops *ops;
+
+	ops = param ? &stacktrace_count_probe_ops : &stacktrace_probe_ops;
+
+	return ftrace_trace_probe_callback(ops, hash, glob, cmd,
+					   param, enable);
+}
+
 static struct ftrace_func_command ftrace_traceon_cmd = {
 	.name			= "traceon",
 	.func			= ftrace_trace_onoff_callback,
@@ -366,6 +435,11 @@ static struct ftrace_func_command ftrace_traceoff_cmd = {
 	.func			= ftrace_trace_onoff_callback,
 };
 
+static struct ftrace_func_command ftrace_stacktrace_cmd = {
+	.name			= "stacktrace",
+	.func			= ftrace_stacktrace_callback,
+};
+
 static int __init init_func_cmd_traceon(void)
 {
 	int ret;
@@ -377,6 +451,12 @@ static int __init init_func_cmd_traceon(void)
 	ret = register_ftrace_command(&ftrace_traceon_cmd);
 	if (ret)
 		unregister_ftrace_command(&ftrace_traceoff_cmd);
+
+	ret = register_ftrace_command(&ftrace_stacktrace_cmd);
+	if (ret) {
+		unregister_ftrace_command(&ftrace_traceoff_cmd);
+		unregister_ftrace_command(&ftrace_traceon_cmd);
+	}
 	return ret;
 }
 #else

commit 8380d24860e9d1659ab22896b86d7fe591c424fa
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Mar 9 08:56:43 2013 -0500

    ftrace: Separate unlimited probes from count limited probes
    
    The function tracing probes that trigger traceon or traceoff can be
    set to unlimited, or given a count of # of times to execute.
    
    By separating these two types of probes, we can then use the dynamic
    ftrace function filtering directly, and remove the brute force
    "check if this function called is my probe" routines in ftrace.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index a88a3e0b0cc2..043b2425ae73 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -228,7 +228,7 @@ static int update_count(void **data)
 }
 
 static void
-ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_traceon_count(unsigned long ip, unsigned long parent_ip, void **data)
 {
 	if (tracing_is_on())
 		return;
@@ -238,7 +238,7 @@ ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
 }
 
 static void
-ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_traceoff_count(unsigned long ip, unsigned long parent_ip, void **data)
 {
 	if (!tracing_is_on())
 		return;
@@ -247,10 +247,38 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
 		tracing_off();
 }
 
+static void
+ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	if (tracing_is_on())
+		return;
+
+	tracing_on();
+}
+
+static void
+ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	if (!tracing_is_on())
+		return;
+
+	tracing_off();
+}
+
 static int
 ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 			 struct ftrace_probe_ops *ops, void *data);
 
+static struct ftrace_probe_ops traceon_count_probe_ops = {
+	.func			= ftrace_traceon_count,
+	.print			= ftrace_trace_onoff_print,
+};
+
+static struct ftrace_probe_ops traceoff_count_probe_ops = {
+	.func			= ftrace_traceoff_count,
+	.print			= ftrace_trace_onoff_print,
+};
+
 static struct ftrace_probe_ops traceon_probe_ops = {
 	.func			= ftrace_traceon,
 	.print			= ftrace_trace_onoff_print,
@@ -269,7 +297,7 @@ ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 
 	seq_printf(m, "%ps:", (void *)ip);
 
-	if (ops == &traceon_probe_ops)
+	if (ops == &traceon_probe_ops || ops == &traceon_count_probe_ops)
 		seq_printf(m, "traceon");
 	else
 		seq_printf(m, "traceoff");
@@ -297,9 +325,9 @@ ftrace_trace_onoff_callback(struct ftrace_hash *hash,
 
 	/* we register both traceon and traceoff to this callback */
 	if (strcmp(cmd, "traceon") == 0)
-		ops = &traceon_probe_ops;
+		ops = param ? &traceon_count_probe_ops : &traceon_probe_ops;
 	else
-		ops = &traceoff_probe_ops;
+		ops = param ? &traceoff_count_probe_ops : &traceoff_probe_ops;
 
 	if (glob[0] == '!') {
 		unregister_ftrace_function_probe_func(glob+1, ops);

commit 8b8fa62c60e03a53c46324075a8dc25821741daa
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Mar 12 09:25:00 2013 -0400

    tracing: Consolidate ftrace_trace_onoff_unreg() into callback
    
    The only thing ftrace_trace_onoff_unreg() does is to do a strcmp()
    against the cmd parameter to determine what op to unregister. But
    this compare is also done after the location that this function is
    called (and returns). By moving the check for '!' to unregister after
    the strcmp(), the callback function itself can just do the unregister
    and we can get rid of the helper function.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 38cfb290ecd9..a88a3e0b0cc2 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -282,22 +282,6 @@ ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 	return 0;
 }
 
-static int
-ftrace_trace_onoff_unreg(char *glob, char *cmd, char *param)
-{
-	struct ftrace_probe_ops *ops;
-
-	/* we register both traceon and traceoff to this callback */
-	if (strcmp(cmd, "traceon") == 0)
-		ops = &traceon_probe_ops;
-	else
-		ops = &traceoff_probe_ops;
-
-	unregister_ftrace_function_probe_func(glob, ops);
-
-	return 0;
-}
-
 static int
 ftrace_trace_onoff_callback(struct ftrace_hash *hash,
 			    char *glob, char *cmd, char *param, int enable)
@@ -311,15 +295,17 @@ ftrace_trace_onoff_callback(struct ftrace_hash *hash,
 	if (!enable)
 		return -EINVAL;
 
-	if (glob[0] == '!')
-		return ftrace_trace_onoff_unreg(glob+1, cmd, param);
-
 	/* we register both traceon and traceoff to this callback */
 	if (strcmp(cmd, "traceon") == 0)
 		ops = &traceon_probe_ops;
 	else
 		ops = &traceoff_probe_ops;
 
+	if (glob[0] == '!') {
+		unregister_ftrace_function_probe_func(glob+1, ops);
+		return 0;
+	}
+
 	if (!param)
 		goto out_reg;
 

commit 1c31714328be90764e46716f31fb0bd6da44c305
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Mar 9 08:36:53 2013 -0500

    tracing: Consolidate updating of count for traceon/off
    
    Remove some duplicate code and replace it with a helper function.
    This makes the code a it cleaner.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index e467c0c7bdd5..38cfb290ecd9 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -214,38 +214,37 @@ static struct tracer function_trace __read_mostly =
 };
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-static void
-ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
+static int update_count(void **data)
 {
-	long *count = (long *)data;
-
-	if (tracing_is_on())
-		return;
+	unsigned long *count = (long *)data;
 
 	if (!*count)
-		return;
+		return 0;
 
 	if (*count != -1)
 		(*count)--;
 
-	tracing_on();
+	return 1;
 }
 
 static void
-ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
+ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
 {
-	long *count = (long *)data;
-
-	if (!tracing_is_on())
+	if (tracing_is_on())
 		return;
 
-	if (!*count)
-		return;
+	if (update_count(data))
+		tracing_on();
+}
 
-	if (*count != -1)
-		(*count)--;
+static void
+ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	if (!tracing_is_on())
+		return;
 
-	tracing_off();
+	if (update_count(data))
+		tracing_off();
 }
 
 static int

commit 12883efb670c28dff57dcd7f4f995a1ffe153b2d
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Tue Mar 5 09:24:35 2013 -0500

    tracing: Consolidate max_tr into main trace_array structure
    
    Currently, the way the latency tracers and snapshot feature works
    is to have a separate trace_array called "max_tr" that holds the
    snapshot buffer. For latency tracers, this snapshot buffer is used
    to swap the running buffer with this buffer to save the current max
    latency.
    
    The only items needed for the max_tr is really just a copy of the buffer
    itself, the per_cpu data pointers, the time_start timestamp that states
    when the max latency was triggered, and the cpu that the max latency
    was triggered on. All other fields in trace_array are unused by the
    max_tr, making the max_tr mostly bloat.
    
    This change removes the max_tr completely, and adds a new structure
    called trace_buffer, that holds the buffer pointer, the per_cpu data
    pointers, the time_start timestamp, and the cpu where the latency occurred.
    
    The trace_array, now has two trace_buffers, one for the normal trace and
    one for the max trace or snapshot. By doing this, not only do we remove
    the bloat from the max_trace but the instances of traces can now use
    their own snapshot feature and not have just the top level global_trace have
    the snapshot feature and latency tracers for itself.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 9d73861efc6a..e467c0c7bdd5 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -28,7 +28,7 @@ static void tracing_stop_function_trace(void);
 static int function_trace_init(struct trace_array *tr)
 {
 	func_trace = tr;
-	tr->cpu = get_cpu();
+	tr->trace_buffer.cpu = get_cpu();
 	put_cpu();
 
 	tracing_start_cmdline_record();
@@ -44,7 +44,7 @@ static void function_trace_reset(struct trace_array *tr)
 
 static void function_trace_start(struct trace_array *tr)
 {
-	tracing_reset_online_cpus(tr);
+	tracing_reset_online_cpus(&tr->trace_buffer);
 }
 
 /* Our option */
@@ -76,7 +76,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 		goto out;
 
 	cpu = smp_processor_id();
-	data = per_cpu_ptr(tr->data, cpu);
+	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
 	if (!atomic_read(&data->disabled)) {
 		local_save_flags(flags);
 		trace_function(tr, ip, parent_ip, flags, pc);
@@ -107,7 +107,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	 */
 	local_irq_save(flags);
 	cpu = raw_smp_processor_id();
-	data = per_cpu_ptr(tr->data, cpu);
+	data = per_cpu_ptr(tr->trace_buffer.data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1)) {

commit a7603ff4b5f7e26e67af82a4c3d05eeeb8d7b160
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 6 16:24:11 2012 -0400

    tracing: Replace the static global per_cpu arrays with allocated per_cpu
    
    The global and max-tr currently use static per_cpu arrays for the CPU data
    descriptors. But in order to get new allocated trace_arrays, they need to
    be allocated per_cpu arrays. Instead of using the static arrays, switch
    the global and max-tr to use allocated data.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 601152523326..9d73861efc6a 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -76,7 +76,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 		goto out;
 
 	cpu = smp_processor_id();
-	data = tr->data[cpu];
+	data = per_cpu_ptr(tr->data, cpu);
 	if (!atomic_read(&data->disabled)) {
 		local_save_flags(flags);
 		trace_function(tr, ip, parent_ip, flags, pc);
@@ -107,7 +107,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 	 */
 	local_irq_save(flags);
 	cpu = raw_smp_processor_id();
-	data = tr->data[cpu];
+	data = per_cpu_ptr(tr->data, cpu);
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1)) {

commit d41032a83b4683481cadff84bbf8e0eafeaba830
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 24 07:52:34 2013 -0500

    tracing: Fix unsigned int compare of zero in recursion check
    
    Dan's smatch found a compare bug with the result of the
    trace_test_and_set_recursion() and comparing to less than
    zero. If the function fails, it returns -1, but was saved in
    an unsigned int, which will never be less than zero and will
    ignore the result of the test if a recursion did happen.
    
    Luckily this is the last of the recursion tests, as the
    infrastructure of ftrace would catch recursions before it
    got here, except for some few exceptions.
    
    Reported-by: Dan Carpenter <dan.carpenter@oracle.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 1c327ef13a9a..601152523326 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -61,7 +61,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;
 	unsigned long flags;
-	unsigned int bit;
+	int bit;
 	int cpu;
 	int pc;
 

commit 897f68a48b1f8fb6cb7493e1ee37e3ed7f879937
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 2 17:52:35 2012 -0400

    ftrace: Use only the preempt version of function tracing
    
    The function tracer had two different versions of function tracing.
    
    The disabling of irqs version and the preempt disable version.
    
    As function tracing in very intrusive and can cause nasty recursion
    issues, it has its own recursion protection. But the old method to
    do this was a flat layer. If it detected that a recursion was happening
    then it would just return without recording.
    
    This made the preempt version (much faster than the irq disabling one)
    not very useful, because if an interrupt were to occur after the
    recursion flag was set, the interrupt would not be traced at all,
    because every function that was traced would think it recursed on
    itself (due to the context it preempted setting the recursive flag).
    
    Now that we have a recursion flag for every context level, we
    no longer need to worry about that. We can disable preemption,
    set the current context recursion check bit, and go on. If an
    interrupt were to come along, it would check its own context bit
    and happily continue to trace.
    
    As the preempt version is faster than the irq disable version,
    there's no more reason to keep the preempt version around.
    And the irq disable version still had an issue with missing
    out on tracing NMI code.
    
    Remove the irq disable function tracer version and have the
    preempt disable version be the default (and only version).
    
    Before this patch we had from running:
    
     # echo function > /debug/tracing/current_tracer
     # for i in `seq 10`; do ./hackbench 50; done
    Time: 12.028
    Time: 11.945
    Time: 11.925
    Time: 11.964
    Time: 12.002
    Time: 11.910
    Time: 11.944
    Time: 11.929
    Time: 11.941
    Time: 11.924
    
    (average: 11.9512)
    
    Now we have:
    
     # echo function > /debug/tracing/current_tracer
     # for i in `seq 10`; do ./hackbench 50; done
    Time: 10.285
    Time: 10.407
    Time: 10.243
    Time: 10.372
    Time: 10.380
    Time: 10.198
    Time: 10.272
    Time: 10.354
    Time: 10.248
    Time: 10.253
    
    (average: 10.3012)
    
     a 13.8% savings!
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 8e3ad8082ab7..1c327ef13a9a 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -47,34 +47,6 @@ static void function_trace_start(struct trace_array *tr)
 	tracing_reset_online_cpus(tr);
 }
 
-static void
-function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip,
-				 struct ftrace_ops *op, struct pt_regs *pt_regs)
-{
-	struct trace_array *tr = func_trace;
-	struct trace_array_cpu *data;
-	unsigned long flags;
-	long disabled;
-	int cpu;
-	int pc;
-
-	if (unlikely(!ftrace_function_enabled))
-		return;
-
-	pc = preempt_count();
-	preempt_disable_notrace();
-	local_save_flags(flags);
-	cpu = raw_smp_processor_id();
-	data = tr->data[cpu];
-	disabled = atomic_inc_return(&data->disabled);
-
-	if (likely(disabled == 1))
-		trace_function(tr, ip, parent_ip, flags, pc);
-
-	atomic_dec(&data->disabled);
-	preempt_enable_notrace();
-}
-
 /* Our option */
 enum {
 	TRACE_FUNC_OPT_STACK	= 0x1,
@@ -85,34 +57,34 @@ static struct tracer_flags func_flags;
 static void
 function_trace_call(unsigned long ip, unsigned long parent_ip,
 		    struct ftrace_ops *op, struct pt_regs *pt_regs)
-
 {
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;
 	unsigned long flags;
-	long disabled;
+	unsigned int bit;
 	int cpu;
 	int pc;
 
 	if (unlikely(!ftrace_function_enabled))
 		return;
 
-	/*
-	 * Need to use raw, since this must be called before the
-	 * recursive protection is performed.
-	 */
-	local_irq_save(flags);
-	cpu = raw_smp_processor_id();
-	data = tr->data[cpu];
-	disabled = atomic_inc_return(&data->disabled);
+	pc = preempt_count();
+	preempt_disable_notrace();
 
-	if (likely(disabled == 1)) {
-		pc = preempt_count();
+	bit = trace_test_and_set_recursion(TRACE_FTRACE_START, TRACE_FTRACE_MAX);
+	if (bit < 0)
+		goto out;
+
+	cpu = smp_processor_id();
+	data = tr->data[cpu];
+	if (!atomic_read(&data->disabled)) {
+		local_save_flags(flags);
 		trace_function(tr, ip, parent_ip, flags, pc);
 	}
+	trace_clear_recursion(bit);
 
-	atomic_dec(&data->disabled);
-	local_irq_restore(flags);
+ out:
+	preempt_enable_notrace();
 }
 
 static void
@@ -185,11 +157,6 @@ static void tracing_start_function_trace(void)
 {
 	ftrace_function_enabled = 0;
 
-	if (trace_flags & TRACE_ITER_PREEMPTONLY)
-		trace_ops.func = function_trace_call_preempt_only;
-	else
-		trace_ops.func = function_trace_call;
-
 	if (func_flags.val & TRACE_FUNC_OPT_STACK)
 		register_ftrace_function(&trace_stack_ops);
 	else

commit a2013a13e68354e0c8f3696b69701803e13fb737
Merge: dadfab487325 106f9d9337f6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Dec 13 12:00:02 2012 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    Pull trivial branch from Jiri Kosina:
     "Usual stuff -- comment/printk typo fixes, documentation updates, dead
      code elimination."
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (39 commits)
      HOWTO: fix double words typo
      x86 mtrr: fix comment typo in mtrr_bp_init
      propagate name change to comments in kernel source
      doc: Update the name of profiling based on sysfs
      treewide: Fix typos in various drivers
      treewide: Fix typos in various Kconfig
      wireless: mwifiex: Fix typo in wireless/mwifiex driver
      messages: i2o: Fix typo in messages/i2o
      scripts/kernel-doc: check that non-void fcts describe their return value
      Kernel-doc: Convention: Use a "Return" section to describe return values
      radeon: Fix typo and copy/paste error in comments
      doc: Remove unnecessary declarations from Documentation/accounting/getdelays.c
      various: Fix spelling of "asynchronous" in comments.
      Fix misspellings of "whether" in comments.
      eisa: Fix spelling of "asynchronous".
      various: Fix spelling of "registered" in comments.
      doc: fix quite a few typos within Documentation
      target: iscsi: fix comment typos in target/iscsi drivers
      treewide: fix typo of "suport" in various comments and Kconfig
      treewide: fix typo of "suppport" in various comments
      ...

commit 6d49e352ae9aed3f599041b0c0389aa924815f14
Author: Nadia Yvette Chambers <nyc@holomorphy.com>
Date:   Thu Dec 6 10:39:54 2012 +0100

    propagate name change to comments in kernel source
    
    I've legally changed my name with New York State, the US Social Security
    Administration, et al. This patch propagates the name change and change
    in initials and login to comments in the kernel source as well.
    
    Signed-off-by: Nadia Yvette Chambers <nyc@holomorphy.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 507a7a9630bf..c7b83d03ff68 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -7,7 +7,7 @@
  * Based on code from the latency_tracer, that is:
  *
  *  Copyright (C) 2004-2006 Ingo Molnar
- *  Copyright (C) 2004 William Lee Irwin III
+ *  Copyright (C) 2004 Nadia Yvette Chambers
  */
 #include <linux/ring_buffer.h>
 #include <linux/debugfs.h>

commit 6f4156723c084bfc0c0f72205c541fafb8ad3ded
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Oct 5 12:13:07 2012 -0400

    tracing: Allow tracers to start at core initcall
    
    There's times during debugging that it is helpful to see traces of early
    boot functions. But the tracers are initialized at device_initcall()
    which is quite late during the boot process. Setting the kernel command
    line parameter ftrace=function will not show anything until the function
    tracer is initialized. This prevents being able to trace functions before
    device_initcall().
    
    There's no reason that the tracers need to be initialized so late in the
    boot process. Move them up to core_initcall() as they still need to come
    after early_initcall() which initializes the tracing buffers.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 618dcf8bdb87..bb227e380cb5 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -411,5 +411,4 @@ static __init int init_function_trace(void)
 	init_func_cmd_traceon();
 	return register_tracer(&function_trace);
 }
-device_initcall(init_function_trace);
-
+core_initcall(init_function_trace);

commit bcd83ea6cbfee54e33d1527b87538dc99ca2137b
Author: Daniel Walter <sahne@0x90.at>
Date:   Wed Sep 26 22:08:38 2012 +0200

    tracing: Replace strict_strto* with kstrto*
    
     * remove old string conversions with kstrto*
    
    Link: http://lkml.kernel.org/r/20120926200838.GC1244@0x90.at
    
    Signed-off-by: Daniel Walter <sahne@0x90.at>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 507a7a9630bf..618dcf8bdb87 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -366,7 +366,7 @@ ftrace_trace_onoff_callback(struct ftrace_hash *hash,
 	 * We use the callback data field (which is a pointer)
 	 * as our counter.
 	 */
-	ret = strict_strtoul(number, 0, (unsigned long *)&count);
+	ret = kstrtoul(number, 0, (unsigned long *)&count);
 	if (ret)
 		return ret;
 

commit 7f60ba388f5b9dd8b0da463b394412dace3ab814
Merge: e665faa424a4 80c9d03c22f1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Oct 7 17:30:50 2012 +0900

    Merge tag 'for-v3.7' of git://git.infradead.org/users/cbou/linux-pstore
    
    Pull pstore changes from Anton Vorontsov:
    
     1) We no longer ad-hoc to the function tracer "high level"
        infrastructure and no longer use its debugfs knobs.  The change
        slightly touches kernel/trace directory, but it got the needed ack
        from Steven Rostedt:
    
          http://lkml.org/lkml/2012/8/21/688
    
     2) Added maintainers entry;
    
     3) A bunch of fixes, nothing special.
    
    * tag 'for-v3.7' of git://git.infradead.org/users/cbou/linux-pstore:
      pstore: Avoid recursive spinlocks in the oops_in_progress case
      pstore/ftrace: Convert to its own enable/disable debugfs knob
      pstore/ram: Add missing platform_device_unregister
      MAINTAINERS: Add pstore maintainers
      pstore/ram: Mark ramoops_pstore_write_buf() as notrace
      pstore/ram: Fix printk format warning
      pstore/ram: Fix possible NULL dereference

commit 65f8c95e46a1827ae8bbc52a817ea308dd7d65ae
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Tue Jul 17 14:26:15 2012 -0700

    pstore/ftrace: Convert to its own enable/disable debugfs knob
    
    With this patch we no longer reuse function tracer infrastructure, now
    we register our own tracer back-end via a debugfs knob.
    
    It's a bit more code, but that is the only downside. On the bright side we
    have:
    
    - Ability to make persistent_ram module removable (when needed, we can
      move ftrace_ops struct into a module). Note that persistent_ram is still
      not removable for other reasons, but with this patch it's just one
      thing less to worry about;
    
    - Pstore part is more isolated from the generic function tracer. We tried
      it already by registering our own tracer in available_tracers, but that
      way we're loosing ability to see the traces while we record them to
      pstore. This solution is somewhere in the middle: we only register
      "internal ftracer" back-end, but not the "front-end";
    
    - When there is only pstore tracing enabled, the kernel will only write
      to the pstore buffer, omitting function tracer buffer (which, of course,
      still can be enabled via 'echo function > current_tracer').
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index a426f410c060..0ad83e3929d1 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -13,7 +13,6 @@
 #include <linux/debugfs.h>
 #include <linux/uaccess.h>
 #include <linux/ftrace.h>
-#include <linux/pstore.h>
 #include <linux/fs.h>
 
 #include "trace.h"
@@ -75,10 +74,9 @@ function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
 	preempt_enable_notrace();
 }
 
-/* Our two options */
+/* Our option */
 enum {
 	TRACE_FUNC_OPT_STACK	= 0x1,
-	TRACE_FUNC_OPT_PSTORE	= 0x2,
 };
 
 static struct tracer_flags func_flags;
@@ -106,12 +104,6 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1)) {
-		/*
-		 * So far tracing doesn't support multiple buffers, so
-		 * we make an explicit call for now.
-		 */
-		if (unlikely(func_flags.val & TRACE_FUNC_OPT_PSTORE))
-			pstore_ftrace_call(ip, parent_ip);
 		pc = preempt_count();
 		trace_function(tr, ip, parent_ip, flags, pc);
 	}
@@ -176,9 +168,6 @@ static struct ftrace_ops trace_stack_ops __read_mostly =
 static struct tracer_opt func_opts[] = {
 #ifdef CONFIG_STACKTRACE
 	{ TRACER_OPT(func_stack_trace, TRACE_FUNC_OPT_STACK) },
-#endif
-#ifdef CONFIG_PSTORE_FTRACE
-	{ TRACER_OPT(func_pstore, TRACE_FUNC_OPT_PSTORE) },
 #endif
 	{ } /* Always set a last empty entry */
 };
@@ -231,8 +220,6 @@ static int func_set_flag(u32 old_flags, u32 bit, int set)
 			register_ftrace_function(&trace_ops);
 		}
 
-		break;
-	case TRACE_FUNC_OPT_PSTORE:
 		break;
 	default:
 		return -EINVAL;

commit bcada3d4b8c96b8792c2306f363992ca5ab9da42
Merge: 26198c21d1b2 000078bc3ee6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Aug 21 11:27:00 2012 +0200

    Merge tag 'perf-core-for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/acme/linux into perf/core
    
    Pull perf/core improvements and fixes from Arnaldo Carvalho de Melo:
    
     * Fix include order for bison/flex-generated C files, from Ben Hutchings
    
     * Build fixes and documentation corrections from David Ahern
    
     * Group parsing support, from Jiri Olsa
    
     * UI/gtk refactorings and improvements from Namhyung Kim
    
     * NULL deref fix for perf script, from Namhyung Kim
    
     * Assorted cleanups from Robert Richter
    
     * Let O= makes handle relative paths, from Steven Rostedt
    
     * perf script python fixes, from Feng Tang.
    
     * Improve 'perf lock' error message when the needed tracepoints
       are not present, from David Ahern.
    
     * Initial bash completion support, from Frederic Weisbecker
    
     * Allow building without libelf, from Namhyung Kim.
    
     * Support DWARF CFI based unwind to have callchains when %bp
       based unwinding is not possible, from Jiri Olsa.
    
     * Symbol resolution fixes, while fixing support PPC64 files with an .opt ELF
       section was the end goal, several fixes for code that handles all
       architectures and cleanups are included, from Cody Schafer.
    
     * Add a description for the JIT interface, from Andi Kleen.
    
     * Assorted fixes for Documentation and build in 32 bit, from Robert Richter
    
     * Add support for non-tracepoint events in perf script python, from Feng Tang
    
     * Cache the libtraceevent event_format associated to each evsel early, so that we
       avoid relookups, i.e. calling pevent_find_event repeatedly when processing
       tracepoint events.
    
       [ This is to reduce the surface contact with libtraceevents and make clear what
         is that the perf tools needs from that lib: so far parsing the common and per
         event fields. ]
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 4740974a6844156c14d741b0080b59d275679a23
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jul 20 11:04:44 2012 -0400

    ftrace: Add default recursion protection for function tracing
    
    As more users of the function tracer utility are being added, they do
    not always add the necessary recursion protection. To protect from
    function recursion due to tracing, if the callback ftrace_ops does not
    specifically specify that it protects against recursion (by setting
    the FTRACE_OPS_FL_RECURSION_SAFE flag), the list operation will be
    called by the mcount trampoline which adds recursion protection.
    
    If the flag is set, then the function will be called directly with no
    extra protection.
    
    Note, the list operation is called if more than one function callback
    is registered, or if the arch does not support all of the function
    tracer features.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 5675ebd541f0..fdff65dff1bb 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -153,13 +153,13 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
 static struct ftrace_ops trace_ops __read_mostly =
 {
 	.func = function_trace_call,
-	.flags = FTRACE_OPS_FL_GLOBAL,
+	.flags = FTRACE_OPS_FL_GLOBAL | FTRACE_OPS_FL_RECURSION_SAFE,
 };
 
 static struct ftrace_ops trace_stack_ops __read_mostly =
 {
 	.func = function_stack_trace_call,
-	.flags = FTRACE_OPS_FL_GLOBAL,
+	.flags = FTRACE_OPS_FL_GLOBAL | FTRACE_OPS_FL_RECURSION_SAFE,
 };
 
 /* Our two options */

commit a1e2e31d175a1349274eba3465d17616c6725f8c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 9 12:50:46 2011 -0400

    ftrace: Return pt_regs to function trace callback
    
    Return as the 4th paramater to the function tracer callback the pt_regs.
    
    Later patches that implement regs passing for the architectures will require
    having the ftrace_ops set the SAVE_REGS flag, which will tell the arch
    to take the time to pass a full set of pt_regs to the ftrace_ops callback
    function. If the arch does not support it then it should pass NULL.
    
    If an arch can pass full regs, then it should define:
     ARCH_SUPPORTS_FTRACE_SAVE_REGS to 1
    
    Link: http://lkml.kernel.org/r/20120702201821.019966811@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index fceb7a9aa06d..5675ebd541f0 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -49,7 +49,7 @@ static void function_trace_start(struct trace_array *tr)
 
 static void
 function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip,
-				 struct ftrace_ops *op)
+				 struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;
@@ -77,7 +77,8 @@ function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip,
 
 static void
 function_trace_call(unsigned long ip, unsigned long parent_ip,
-		    struct ftrace_ops *op)
+		    struct ftrace_ops *op, struct pt_regs *pt_regs)
+
 {
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;
@@ -109,7 +110,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip,
 
 static void
 function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
-			  struct ftrace_ops *op)
+			  struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;

commit 2f5f6ad9390c1ebbf738d130dbfe80b60eaa167e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 8 16:57:47 2011 -0400

    ftrace: Pass ftrace_ops as third parameter to function trace callback
    
    Currently the function trace callback receives only the ip and parent_ip
    of the function that it traced. It would be more powerful to also return
    the ops that registered the function as well. This allows the same function
    to act differently depending on what ftrace_ops registered it.
    
    Link: http://lkml.kernel.org/r/20120612225424.267254552@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index c7b0c6a7db09..fceb7a9aa06d 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -48,7 +48,8 @@ static void function_trace_start(struct trace_array *tr)
 }
 
 static void
-function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
+function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip,
+				 struct ftrace_ops *op)
 {
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;
@@ -75,7 +76,8 @@ function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
 }
 
 static void
-function_trace_call(unsigned long ip, unsigned long parent_ip)
+function_trace_call(unsigned long ip, unsigned long parent_ip,
+		    struct ftrace_ops *op)
 {
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;
@@ -106,7 +108,8 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 }
 
 static void
-function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
+function_stack_trace_call(unsigned long ip, unsigned long parent_ip,
+			  struct ftrace_ops *op)
 {
 	struct trace_array *tr = func_trace;
 	struct trace_array_cpu *data;

commit f555f1231a69846d57099760f9c361982600ffa2
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Mon Jul 9 17:10:46 2012 -0700

    tracing/function: Convert func_set_flag() to a switch statement
    
    Since the function accepts just one bit, we can use the switch
    construction instead of if/else if/...
    
    Just a cosmetic change, there should be no functional changes.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 13770abd7a12..a426f410c060 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -217,10 +217,11 @@ static void tracing_stop_function_trace(void)
 
 static int func_set_flag(u32 old_flags, u32 bit, int set)
 {
-	if (bit == TRACE_FUNC_OPT_STACK) {
+	switch (bit) {
+	case TRACE_FUNC_OPT_STACK:
 		/* do nothing if already set */
 		if (!!set == !!(func_flags.val & TRACE_FUNC_OPT_STACK))
-			return 0;
+			break;
 
 		if (set) {
 			unregister_ftrace_function(&trace_ops);
@@ -230,12 +231,14 @@ static int func_set_flag(u32 old_flags, u32 bit, int set)
 			register_ftrace_function(&trace_ops);
 		}
 
-		return 0;
-	} else if (bit == TRACE_FUNC_OPT_PSTORE) {
-		return 0;
+		break;
+	case TRACE_FUNC_OPT_PSTORE:
+		break;
+	default:
+		return -EINVAL;
 	}
 
-	return -EINVAL;
+	return 0;
 }
 
 static struct tracer function_trace __read_mostly =

commit 21f679404a0c28bd5b1b3aff2a7218bbff4cb43d
Author: Anton Vorontsov <anton.vorontsov@linaro.org>
Date:   Mon Jul 9 17:10:42 2012 -0700

    tracing/function: Introduce persistent trace option
    
    This patch introduces 'func_ptrace' option, now available in
    /sys/kernel/debug/tracing/options when function tracer
    is selected.
    
    The patch also adds some tiny code that calls back to pstore
    to record the trace. The callback is no-op when PSTORE=n.
    
    Signed-off-by: Anton Vorontsov <anton.vorontsov@linaro.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index c7b0c6a7db09..13770abd7a12 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -13,6 +13,7 @@
 #include <linux/debugfs.h>
 #include <linux/uaccess.h>
 #include <linux/ftrace.h>
+#include <linux/pstore.h>
 #include <linux/fs.h>
 
 #include "trace.h"
@@ -74,6 +75,14 @@ function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
 	preempt_enable_notrace();
 }
 
+/* Our two options */
+enum {
+	TRACE_FUNC_OPT_STACK	= 0x1,
+	TRACE_FUNC_OPT_PSTORE	= 0x2,
+};
+
+static struct tracer_flags func_flags;
+
 static void
 function_trace_call(unsigned long ip, unsigned long parent_ip)
 {
@@ -97,6 +106,12 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1)) {
+		/*
+		 * So far tracing doesn't support multiple buffers, so
+		 * we make an explicit call for now.
+		 */
+		if (unlikely(func_flags.val & TRACE_FUNC_OPT_PSTORE))
+			pstore_ftrace_call(ip, parent_ip);
 		pc = preempt_count();
 		trace_function(tr, ip, parent_ip, flags, pc);
 	}
@@ -158,14 +173,12 @@ static struct ftrace_ops trace_stack_ops __read_mostly =
 	.flags = FTRACE_OPS_FL_GLOBAL,
 };
 
-/* Our two options */
-enum {
-	TRACE_FUNC_OPT_STACK = 0x1,
-};
-
 static struct tracer_opt func_opts[] = {
 #ifdef CONFIG_STACKTRACE
 	{ TRACER_OPT(func_stack_trace, TRACE_FUNC_OPT_STACK) },
+#endif
+#ifdef CONFIG_PSTORE_FTRACE
+	{ TRACER_OPT(func_pstore, TRACE_FUNC_OPT_PSTORE) },
 #endif
 	{ } /* Always set a last empty entry */
 };
@@ -217,6 +230,8 @@ static int func_set_flag(u32 old_flags, u32 bit, int set)
 			register_ftrace_function(&trace_ops);
 		}
 
+		return 0;
+	} else if (bit == TRACE_FUNC_OPT_PSTORE) {
 		return 0;
 	}
 

commit 43dd61c9a09bd413e837df829e6bfb42159be52a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jul 7 11:09:22 2011 -0400

    ftrace: Fix regression of :mod:module function enabling
    
    The new code that allows different utilities to pick and choose
    what functions they trace broke the :mod: hook that allows users
    to trace only functions of a particular module.
    
    The reason is that the :mod: hook bypasses the hash that is setup
    to allow individual users to trace their own functions and uses
    the global hash directly. But if the global hash has not been
    set up, it will cause a bug:
    
    echo '*:mod:radeon' > /sys/kernel/debug/set_ftrace_filter
    
    produces:
    
     [drm:drm_mode_getfb] *ERROR* invalid framebuffer id
     [drm:radeon_crtc_page_flip] *ERROR* failed to reserve new rbo buffer before flip
     BUG: unable to handle kernel paging request at ffffffff8160ec90
     IP: [<ffffffff810d9136>] add_hash_entry+0x66/0xd0
     PGD 1a05067 PUD 1a09063 PMD 80000000016001e1
     Oops: 0003 [#1] SMP Jul  7 04:02:28 phyllis kernel: [55303.858604] CPU 1
     Modules linked in: cryptd aes_x86_64 aes_generic binfmt_misc rfcomm bnep ip6table_filter hid radeon r8169 ahci libahci mii ttm drm_kms_helper drm video i2c_algo_bit intel_agp intel_gtt
    
     Pid: 10344, comm: bash Tainted: G        WC  3.0.0-rc5 #1 Dell Inc. Inspiron N5010/0YXXJJ
     RIP: 0010:[<ffffffff810d9136>]  [<ffffffff810d9136>] add_hash_entry+0x66/0xd0
     RSP: 0018:ffff88003a96bda8  EFLAGS: 00010246
     RAX: ffff8801301735c0 RBX: ffffffff8160ec80 RCX: 0000000000306ee0
     RDX: 0000000000000000 RSI: 0000000000000000 RDI: ffff880137c92940
     RBP: ffff88003a96bdb8 R08: ffff880137c95680 R09: 0000000000000000
     R10: 0000000000000001 R11: 0000000000000000 R12: ffffffff81c9df78
     R13: ffff8801153d1000 R14: 0000000000000000 R15: 0000000000000000
     FS: 00007f329c18a700(0000) GS:ffff880137c80000(0000) knlGS:0000000000000000
     CS:  0010 DS: 0000 ES: 0000 CR0: 0000000080050033
     CR2: ffffffff8160ec90 CR3: 000000003002b000 CR4: 00000000000006e0
     DR0: 0000000000000000 DR1: 0000000000000000 DR2: 0000000000000000
     DR3: 0000000000000000 DR6: 00000000ffff0ff0 DR7: 0000000000000400
     Process bash (pid: 10344, threadinfo ffff88003a96a000, task ffff88012fcfc470)
     Stack:
      0000000000000fd0 00000000000000fc ffff88003a96be38 ffffffff810d92f5
      ffff88011c4c4e00 ffff880000000000 000000000b69f4d0 ffffffff8160ec80
      ffff8800300e6f06 0000000081130295 0000000000000282 ffff8800300e6f00
     Call Trace:
      [<ffffffff810d92f5>] match_records+0x155/0x1b0
      [<ffffffff810d940c>] ftrace_mod_callback+0xbc/0x100
      [<ffffffff810dafdf>] ftrace_regex_write+0x16f/0x210
      [<ffffffff810db09f>] ftrace_filter_write+0xf/0x20
      [<ffffffff81166e48>] vfs_write+0xc8/0x190
      [<ffffffff81167001>] sys_write+0x51/0x90
      [<ffffffff815c7e02>] system_call_fastpath+0x16/0x1b
     Code: 48 8b 33 31 d2 48 85 f6 75 33 49 89 d4 4c 03 63 08 49 8b 14 24 48 85 d2 48 89 10 74 04 48 89 42 08 49 89 04 24 4c 89 60 08 31 d2
     RIP [<ffffffff810d9136>] add_hash_entry+0x66/0xd0
      RSP <ffff88003a96bda8>
     CR2: ffffffff8160ec90
     ---[ end trace a5d031828efdd88e ]---
    
    Reported-by: Brian Marete <marete@toshnix.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 8d0e1cc4e974..c7b0c6a7db09 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -324,7 +324,8 @@ ftrace_trace_onoff_unreg(char *glob, char *cmd, char *param)
 }
 
 static int
-ftrace_trace_onoff_callback(char *glob, char *cmd, char *param, int enable)
+ftrace_trace_onoff_callback(struct ftrace_hash *hash,
+			    char *glob, char *cmd, char *param, int enable)
 {
 	struct ftrace_probe_ops *ops;
 	void *count = (void *)-1;

commit b848914ce39589d89ee0078a6d1ef452b464729e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 4 09:27:52 2011 -0400

    ftrace: Implement separate user function filtering
    
    ftrace_ops that are registered to trace functions can now be
    agnostic to each other in respect to what functions they trace.
    Each ops has their own hash of the functions they want to trace
    and a hash to what they do not want to trace. A empty hash for
    the functions they want to trace denotes all functions should
    be traced that are not in the notrace hash.
    
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 16aee4d44e8f..8d0e1cc4e974 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -149,11 +149,13 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 static struct ftrace_ops trace_ops __read_mostly =
 {
 	.func = function_trace_call,
+	.flags = FTRACE_OPS_FL_GLOBAL,
 };
 
 static struct ftrace_ops trace_stack_ops __read_mostly =
 {
 	.func = function_stack_trace_call,
+	.flags = FTRACE_OPS_FL_GLOBAL,
 };
 
 /* Our two options */

commit 5168ae50a66e3ff7184c2b16d661bd6d70367e50
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 3 09:36:50 2010 -0400

    tracing: Remove ftrace_preempt_disable/enable
    
    The ftrace_preempt_disable/enable functions were to address a
    recursive race caused by the function tracer. The function tracer
    traces all functions which makes it easily susceptible to recursion.
    One area was preempt_enable(). This would call the scheduler and
    the schedulre would call the function tracer and loop.
    (So was it thought).
    
    The ftrace_preempt_disable/enable was made to protect against recursion
    inside the scheduler by storing the NEED_RESCHED flag. If it was
    set before the ftrace_preempt_disable() it would not call schedule
    on ftrace_preempt_enable(), thinking that if it was set before then
    it would have already scheduled unless it was already in the scheduler.
    
    This worked fine except in the case of SMP, where another task would set
    the NEED_RESCHED flag for a task on another CPU, and then kick off an
    IPI to trigger it. This could cause the NEED_RESCHED to be saved at
    ftrace_preempt_disable() but the IPI to arrive in the the preempt
    disabled section. The ftrace_preempt_enable() would not call the scheduler
    because the flag was already set before entring the section.
    
    This bug would cause a missed preemption check and cause lower latencies.
    
    Investigating further, I found that the recusion caused by the function
    tracer was not due to schedule(), but due to preempt_schedule(). Now
    that preempt_schedule is completely annotated with notrace, the recusion
    no longer is an issue.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b3f3776b0cd6..16aee4d44e8f 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -54,14 +54,14 @@ function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
 	struct trace_array_cpu *data;
 	unsigned long flags;
 	long disabled;
-	int cpu, resched;
+	int cpu;
 	int pc;
 
 	if (unlikely(!ftrace_function_enabled))
 		return;
 
 	pc = preempt_count();
-	resched = ftrace_preempt_disable();
+	preempt_disable_notrace();
 	local_save_flags(flags);
 	cpu = raw_smp_processor_id();
 	data = tr->data[cpu];
@@ -71,7 +71,7 @@ function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
 		trace_function(tr, ip, parent_ip, flags, pc);
 
 	atomic_dec(&data->disabled);
-	ftrace_preempt_enable(resched);
+	preempt_enable_notrace();
 }
 
 static void

commit b375a11a239e9e1cac40c7f3ff28b343d9f7ac51
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Sep 17 00:05:58 2009 -0400

    tracing: switch function prints from %pf to %ps
    
    For direct function pointers (like what mcount provides) PowerPC64
    requires the use of %ps, otherwise nothing is printed.
    
    This patch converts all prints of functions retrieved through mcount
    to use the %ps format from the %pf.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 5b01b94518fc..b3f3776b0cd6 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -290,7 +290,7 @@ ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 {
 	long count = (long)data;
 
-	seq_printf(m, "%pf:", (void *)ip);
+	seq_printf(m, "%ps:", (void *)ip);
 
 	if (ops == &traceon_probe_ops)
 		seq_printf(m, "traceon");

commit 45bceffc3013bda7d2ebc7802e9b153b674e2d44
Merge: 6f2f3cf00ee3 78af08d90b8f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Jul 18 12:19:57 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: tracing/core was on an older, pre-rc1 base.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 6f2f3cf00ee32f75ba007a46bab88a54d68a5deb
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Jul 16 14:21:08 2009 +0800

    tracing/function: Cleanup for function tracer
    
    We can directly use %pf input format instead of kallsyms_lookup()
    and %s input format
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 7402144bff21..b53dc994dfb6 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -288,11 +288,9 @@ static int
 ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 			 struct ftrace_probe_ops *ops, void *data)
 {
-	char str[KSYM_SYMBOL_LEN];
 	long count = (long)data;
 
-	kallsyms_lookup(ip, NULL, NULL, NULL, str);
-	seq_printf(m, "%s:", str);
+	seq_printf(m, "%pf:", (void *)ip);
 
 	if (ops == &traceon_probe_ops)
 		seq_printf(m, "traceon");

commit 04aef32d39cc4ef80087c0ce8ed113c6d64f1a6b
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Wed Jul 15 12:29:06 2009 +0800

    tracing/function: Fix the return value of ftrace_trace_onoff_callback()
    
    ftrace_trace_onoff_callback() will return an error even if we do the
    right operation, for example:
    
     # echo _spin_*:traceon:10 > set_ftrace_filter
     -bash: echo: write error: Invalid argument
     # cat set_ftrace_filter
     #### all functions enabled ####
     _spin_trylock_bh:traceon:count=10
     _spin_unlock_irq:traceon:count=10
     _spin_unlock_bh:traceon:count=10
     _spin_lock_irq:traceon:count=10
     _spin_unlock:traceon:count=10
     _spin_trylock:traceon:count=10
     _spin_unlock_irqrestore:traceon:count=10
     _spin_lock_irqsave:traceon:count=10
     _spin_lock_bh:traceon:count=10
     _spin_lock:traceon:count=10
    
    We want to set _spin_*:traceon:10 to set_ftrace_filter, it complains
    with "Invalid argument", but the operation is successful.
    
    This is because ftrace_process_regex() returns the number of functions that
    matched the pattern. If the number is not 0, this value is returned
    by ftrace_regex_write() whereas we want to return the number of bytes
    virtually written.
    Also the file offset pointer is not updated in this case.
    
    If the number of matched functions is lower than the number of bytes written
    by the user, this results to a reprocessing of the string given by the user with
    a lower size, leading to a malformed ftrace regex and then a -EINVAL returned.
    
    So, this patch fixes it by returning 0 if no error occured.
    The fix also applies on 2.6.30
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: stable@kernel.org
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 7402144bff21..75ef000613c3 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -363,7 +363,7 @@ ftrace_trace_onoff_callback(char *glob, char *cmd, char *param, int enable)
  out_reg:
 	ret = register_ftrace_function_probe(glob, ops, count);
 
-	return ret;
+	return ret < 0 ? ret : 0;
 }
 
 static struct ftrace_func_command ftrace_traceon_cmd = {

commit 00e54d087afb3867b0b461aef6c1ff433d0df564
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Jun 25 14:05:27 2009 +0800

    ftrace: Remove duplicate newline
    
    Before:
      # echo 'sys_open:traceon:' > set_ftrace_filter
      # echo 'sys_close:traceoff:5' > set_ftrace_filter
      # cat set_ftrace_filter
      #### all functions enabled ####
      sys_open:traceon:unlimited
    
      sys_close:traceoff:count=0
    
    After:
      # cat set_ftrace_filter
      #### all functions enabled ####
      sys_open:traceon:unlimited
      sys_close:traceoff:count=0
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A4313A7.7030105@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 90f134764837..7402144bff21 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -302,8 +302,7 @@ ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 	if (count == -1)
 		seq_printf(m, ":unlimited\n");
 	else
-		seq_printf(m, ":count=%ld", count);
-	seq_putc(m, '\n');
+		seq_printf(m, ":count=%ld\n", count);
 
 	return 0;
 }

commit c85a17e22695969aa24a7ffa40cf26d6e6fcfd50
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Jun 20 05:45:14 2009 +0200

    tracing/urgent: fix unbalanced ftrace_start_up
    
    Perfcounter reports the following stats for a wide system
    profiling:
    
     #
     # (2364 samples)
     #
     # Overhead  Symbol
     # ........  ......
     #
        15.40%  [k] mwait_idle_with_hints
         8.29%  [k] read_hpet
         5.75%  [k] ftrace_caller
         3.60%  [k] ftrace_call
         [...]
    
    This snapshot has been taken while neither the function tracer nor
    the function graph tracer was running.
    With dynamic ftrace, such results show a wrong ftrace behaviour
    because all calls to ftrace_caller or ftrace_graph_caller (the patched
    calls to mcount) are supposed to be patched into nop if none of those
    tracers are running.
    
    The problem occurs after the first run of the function tracer. Once we
    launch it a second time, the callsites will never be nopped back,
    unless you set custom filters.
    For example it happens during the self tests at boot time.
    The function tracer selftest runs, and then the dynamic tracing is
    tested too. After that, the callsites are left un-nopped.
    
    This is because the reset callback of the function tracer tries to
    unregister two ftrace callbacks in once: the common function tracer
    and the function tracer with stack backtrace, regardless of which
    one is currently in use.
    It then creates an unbalance on ftrace_start_up value which is expected
    to be zero when the last ftrace callback is unregistered. When it
    reaches zero, the FTRACE_DISABLE_CALLS is set on the next ftrace
    command, triggering the patching into nop. But since it becomes
    unbalanced, ie becomes lower than zero, if the kernel functions
    are patched again (as in every further function tracer runs), they
    won't ever be nopped back.
    
    Note that ftrace_call and ftrace_graph_call are still patched back
    to ftrace_stub in the off case, but not the callers of ftrace_call
    and ftrace_graph_caller. It means that the tracing is well deactivated
    but we waste a useless call into every kernel function.
    
    This patch just unregisters the right ftrace_ops for the function
    tracer on its reset callback and ignores the other one which is
    not registered, fixing the unbalance. The problem also happens
    is .30
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: stable@kernel.org

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index c9a0b7df44ff..90f134764837 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -193,9 +193,11 @@ static void tracing_start_function_trace(void)
 static void tracing_stop_function_trace(void)
 {
 	ftrace_function_enabled = 0;
-	/* OK if they are not registered */
-	unregister_ftrace_function(&trace_stack_ops);
-	unregister_ftrace_function(&trace_ops);
+
+	if (func_flags.val & TRACE_FUNC_OPT_STACK)
+		unregister_ftrace_function(&trace_stack_ops);
+	else
+		unregister_ftrace_function(&trace_ops);
 }
 
 static int func_set_flag(u32 old_flags, u32 bit, int set)

commit 6eaaa5d57e76c454479833fc8594cd7c3b75c789
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Feb 11 02:25:00 2009 +0100

    tracing/core: use appropriate waiting on trace_pipe
    
    Impact: api and pipe waiting change
    
    Currently, the waiting used in tracing_read_pipe() is done through a
    100 msecs schedule_timeout() loop which periodically check if there
    are traces on the buffer.
    
    This can cause small latencies for programs which are reading the incoming
    events.
    
    This patch makes the reader waiting for the trace_wait waitqueue except
    for few tracers such as the sched and functions tracers which might be
    already hold the runqueue lock while waking up the reader.
    
    This is performed through a new callback wait_pipe() on struct tracer.
    If none is implemented on a specific tracer, the default waiting for
    trace_wait queue is attached.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 4c113a8c466f..c9a0b7df44ff 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -225,6 +225,7 @@ static struct tracer function_trace __read_mostly =
 	.init		= function_trace_init,
 	.reset		= function_trace_reset,
 	.start		= function_trace_start,
+	.wait_pipe	= poll_wait_pipe,
 	.flags		= &func_flags,
 	.set_flag	= func_set_flag,
 #ifdef CONFIG_FTRACE_SELFTEST

commit 35ebf1caa4854ad5ba25f3a72967acc064147994
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 17 13:12:12 2009 -0500

    ftrace: show unlimited when traceon or traceoff has no counter
    
    Impact: clean up
    
    The traceon and traceoff function probes are confusing to developers
    to what happens when a counter is not specified. This should help
    clear things up.
    
     # echo "*:traceoff" > set_ftrace_filter
     # cat /debug/tracing/set_ftrace_filter
    
      #### all functions enabled ####
      do_fork:traceoff:unlimited
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 6ea73ed03bfa..4c113a8c466f 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -296,7 +296,9 @@ ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 	else
 		seq_printf(m, "traceoff");
 
-	if (count != -1)
+	if (count == -1)
+		seq_printf(m, ":unlimited\n");
+	else
 		seq_printf(m, ":count=%ld", count);
 	seq_putc(m, '\n');
 

commit b6887d7916e44c1d8913084fb6aa5004d9473f1a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 17 12:32:04 2009 -0500

    ftrace: rename _hook to _probe
    
    Impact: clean up
    
    Ingo Molnar did not like the _hook naming convention used by the
    select function tracer. Luis Claudio R. Goncalves suggested using
    the "_probe" extension. This patch implements the change of
    calling the functions and variables "_hook" and replacing them
    with "_probe".
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 021a574c5988..6ea73ed03bfa 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -269,21 +269,21 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
 
 static int
 ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
-			 struct ftrace_hook_ops *ops, void *data);
+			 struct ftrace_probe_ops *ops, void *data);
 
-static struct ftrace_hook_ops traceon_hook_ops = {
+static struct ftrace_probe_ops traceon_probe_ops = {
 	.func			= ftrace_traceon,
 	.print			= ftrace_trace_onoff_print,
 };
 
-static struct ftrace_hook_ops traceoff_hook_ops = {
+static struct ftrace_probe_ops traceoff_probe_ops = {
 	.func			= ftrace_traceoff,
 	.print			= ftrace_trace_onoff_print,
 };
 
 static int
 ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
-			 struct ftrace_hook_ops *ops, void *data)
+			 struct ftrace_probe_ops *ops, void *data)
 {
 	char str[KSYM_SYMBOL_LEN];
 	long count = (long)data;
@@ -291,7 +291,7 @@ ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 	kallsyms_lookup(ip, NULL, NULL, NULL, str);
 	seq_printf(m, "%s:", str);
 
-	if (ops == &traceon_hook_ops)
+	if (ops == &traceon_probe_ops)
 		seq_printf(m, "traceon");
 	else
 		seq_printf(m, "traceoff");
@@ -306,15 +306,15 @@ ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
 static int
 ftrace_trace_onoff_unreg(char *glob, char *cmd, char *param)
 {
-	struct ftrace_hook_ops *ops;
+	struct ftrace_probe_ops *ops;
 
 	/* we register both traceon and traceoff to this callback */
 	if (strcmp(cmd, "traceon") == 0)
-		ops = &traceon_hook_ops;
+		ops = &traceon_probe_ops;
 	else
-		ops = &traceoff_hook_ops;
+		ops = &traceoff_probe_ops;
 
-	unregister_ftrace_function_hook_func(glob, ops);
+	unregister_ftrace_function_probe_func(glob, ops);
 
 	return 0;
 }
@@ -322,7 +322,7 @@ ftrace_trace_onoff_unreg(char *glob, char *cmd, char *param)
 static int
 ftrace_trace_onoff_callback(char *glob, char *cmd, char *param, int enable)
 {
-	struct ftrace_hook_ops *ops;
+	struct ftrace_probe_ops *ops;
 	void *count = (void *)-1;
 	char *number;
 	int ret;
@@ -336,9 +336,9 @@ ftrace_trace_onoff_callback(char *glob, char *cmd, char *param, int enable)
 
 	/* we register both traceon and traceoff to this callback */
 	if (strcmp(cmd, "traceon") == 0)
-		ops = &traceon_hook_ops;
+		ops = &traceon_probe_ops;
 	else
-		ops = &traceoff_hook_ops;
+		ops = &traceoff_probe_ops;
 
 	if (!param)
 		goto out_reg;
@@ -357,7 +357,7 @@ ftrace_trace_onoff_callback(char *glob, char *cmd, char *param, int enable)
 		return ret;
 
  out_reg:
-	ret = register_ftrace_function_hook(glob, ops, count);
+	ret = register_ftrace_function_probe(glob, ops, count);
 
 	return ret;
 }

commit 6a24a244cd3a02d5b290293c32fcf2c6e92b4235
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 17 11:20:26 2009 -0500

    ftrace: clean up coding style
    
    Ingo Molnar pointed out some coding style issues with the recent ftrace
    updates. This patch cleans them up.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index f520aa419dff..021a574c5988 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -397,6 +397,5 @@ static __init int init_function_trace(void)
 	init_func_cmd_traceon();
 	return register_tracer(&function_trace);
 }
-
 device_initcall(init_function_trace);
 

commit e110e3d1eaa0f9628918be67ddd32e8ad65a2871
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Feb 16 23:38:13 2009 -0500

    ftrace: add pretty print function for traceon and traceoff hooks
    
    This patch adds a pretty print version of traceon and traceoff
    output for set_ftrace_filter.
    
      # echo 'sys_open:traceon:4' > set_ftrace_filter
      # cat set_ftrace_filter
    
     #### all functions enabled ####
     sys_open:traceon:count=4
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 5c95708b9dc3..f520aa419dff 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -267,14 +267,42 @@ ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
 	tracing_off();
 }
 
+static int
+ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
+			 struct ftrace_hook_ops *ops, void *data);
+
 static struct ftrace_hook_ops traceon_hook_ops = {
 	.func			= ftrace_traceon,
+	.print			= ftrace_trace_onoff_print,
 };
 
 static struct ftrace_hook_ops traceoff_hook_ops = {
 	.func			= ftrace_traceoff,
+	.print			= ftrace_trace_onoff_print,
 };
 
+static int
+ftrace_trace_onoff_print(struct seq_file *m, unsigned long ip,
+			 struct ftrace_hook_ops *ops, void *data)
+{
+	char str[KSYM_SYMBOL_LEN];
+	long count = (long)data;
+
+	kallsyms_lookup(ip, NULL, NULL, NULL, str);
+	seq_printf(m, "%s:", str);
+
+	if (ops == &traceon_hook_ops)
+		seq_printf(m, "traceon");
+	else
+		seq_printf(m, "traceoff");
+
+	if (count != -1)
+		seq_printf(m, ":count=%ld", count);
+	seq_putc(m, '\n');
+
+	return 0;
+}
+
 static int
 ftrace_trace_onoff_unreg(char *glob, char *cmd, char *param)
 {

commit 23b4ff3aa479c9e3bb23cb6b2d0a97878399784a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Feb 14 19:04:24 2009 -0500

    ftrace: add traceon traceoff commands to enable/disable the buffers
    
    This patch adds the new function selection commands traceon and
    traceoff. traceon sets the function to enable the ring buffers
    while traceoff disables the ring buffers.  You can pass in the
    number of times you want the command to be executed when the function
    is hit. It will only execute if the state of the buffers are not
    already in that state.
    
    Example:
    
     # echo do_fork:traceon:4
    
    Will enable the ring buffers if they are disabled every time it
    hits do_fork, up to 4 times.
    
     # echo sys_close:traceoff
    
    This will disable the ring buffers every time (unlimited) when
    sys_close is called.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 36bf9568ccd9..5c95708b9dc3 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -9,6 +9,7 @@
  *  Copyright (C) 2004-2006 Ingo Molnar
  *  Copyright (C) 2004 William Lee Irwin III
  */
+#include <linux/ring_buffer.h>
 #include <linux/debugfs.h>
 #include <linux/uaccess.h>
 #include <linux/ftrace.h>
@@ -231,9 +232,143 @@ static struct tracer function_trace __read_mostly =
 #endif
 };
 
+#ifdef CONFIG_DYNAMIC_FTRACE
+static void
+ftrace_traceon(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	long *count = (long *)data;
+
+	if (tracing_is_on())
+		return;
+
+	if (!*count)
+		return;
+
+	if (*count != -1)
+		(*count)--;
+
+	tracing_on();
+}
+
+static void
+ftrace_traceoff(unsigned long ip, unsigned long parent_ip, void **data)
+{
+	long *count = (long *)data;
+
+	if (!tracing_is_on())
+		return;
+
+	if (!*count)
+		return;
+
+	if (*count != -1)
+		(*count)--;
+
+	tracing_off();
+}
+
+static struct ftrace_hook_ops traceon_hook_ops = {
+	.func			= ftrace_traceon,
+};
+
+static struct ftrace_hook_ops traceoff_hook_ops = {
+	.func			= ftrace_traceoff,
+};
+
+static int
+ftrace_trace_onoff_unreg(char *glob, char *cmd, char *param)
+{
+	struct ftrace_hook_ops *ops;
+
+	/* we register both traceon and traceoff to this callback */
+	if (strcmp(cmd, "traceon") == 0)
+		ops = &traceon_hook_ops;
+	else
+		ops = &traceoff_hook_ops;
+
+	unregister_ftrace_function_hook_func(glob, ops);
+
+	return 0;
+}
+
+static int
+ftrace_trace_onoff_callback(char *glob, char *cmd, char *param, int enable)
+{
+	struct ftrace_hook_ops *ops;
+	void *count = (void *)-1;
+	char *number;
+	int ret;
+
+	/* hash funcs only work with set_ftrace_filter */
+	if (!enable)
+		return -EINVAL;
+
+	if (glob[0] == '!')
+		return ftrace_trace_onoff_unreg(glob+1, cmd, param);
+
+	/* we register both traceon and traceoff to this callback */
+	if (strcmp(cmd, "traceon") == 0)
+		ops = &traceon_hook_ops;
+	else
+		ops = &traceoff_hook_ops;
+
+	if (!param)
+		goto out_reg;
+
+	number = strsep(&param, ":");
+
+	if (!strlen(number))
+		goto out_reg;
+
+	/*
+	 * We use the callback data field (which is a pointer)
+	 * as our counter.
+	 */
+	ret = strict_strtoul(number, 0, (unsigned long *)&count);
+	if (ret)
+		return ret;
+
+ out_reg:
+	ret = register_ftrace_function_hook(glob, ops, count);
+
+	return ret;
+}
+
+static struct ftrace_func_command ftrace_traceon_cmd = {
+	.name			= "traceon",
+	.func			= ftrace_trace_onoff_callback,
+};
+
+static struct ftrace_func_command ftrace_traceoff_cmd = {
+	.name			= "traceoff",
+	.func			= ftrace_trace_onoff_callback,
+};
+
+static int __init init_func_cmd_traceon(void)
+{
+	int ret;
+
+	ret = register_ftrace_command(&ftrace_traceoff_cmd);
+	if (ret)
+		return ret;
+
+	ret = register_ftrace_command(&ftrace_traceon_cmd);
+	if (ret)
+		unregister_ftrace_command(&ftrace_traceoff_cmd);
+	return ret;
+}
+#else
+static inline int init_func_cmd_traceon(void)
+{
+	return 0;
+}
+#endif /* CONFIG_DYNAMIC_FTRACE */
+
 static __init int init_function_trace(void)
 {
+	init_func_cmd_traceon();
 	return register_tracer(&function_trace);
 }
 
 device_initcall(init_function_trace);
+

commit b6f11df26fdc28324cf9c9e3b77f2dc985c1bb13
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Feb 5 18:02:00 2009 -0200

    trace: Call tracing_reset_online_cpus before tracer->init()
    
    Impact: cleanup
    
    To make it easy for ftrace plugin writers, as this was open coded in
    the existing plugins
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Frdric Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index d067cea2ccc3..36bf9568ccd9 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -24,32 +24,21 @@ static struct trace_array	*func_trace;
 static void tracing_start_function_trace(void);
 static void tracing_stop_function_trace(void);
 
-static void start_function_trace(struct trace_array *tr)
+static int function_trace_init(struct trace_array *tr)
 {
 	func_trace = tr;
 	tr->cpu = get_cpu();
-	tracing_reset_online_cpus(tr);
 	put_cpu();
 
 	tracing_start_cmdline_record();
 	tracing_start_function_trace();
-}
-
-static void stop_function_trace(struct trace_array *tr)
-{
-	tracing_stop_function_trace();
-	tracing_stop_cmdline_record();
-}
-
-static int function_trace_init(struct trace_array *tr)
-{
-	start_function_trace(tr);
 	return 0;
 }
 
 static void function_trace_reset(struct trace_array *tr)
 {
-	stop_function_trace(tr);
+	tracing_stop_function_trace();
+	tracing_stop_cmdline_record();
 }
 
 static void function_trace_start(struct trace_array *tr)

commit 7be421510b91491d5aa5a29fa1005712039b95af
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Feb 5 01:13:37 2009 -0500

    trace: Remove unused trace_array_cpu parameter
    
    Impact: cleanup
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index b3a320f8aba7..d067cea2ccc3 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -78,7 +78,7 @@ function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1))
-		trace_function(tr, data, ip, parent_ip, flags, pc);
+		trace_function(tr, ip, parent_ip, flags, pc);
 
 	atomic_dec(&data->disabled);
 	ftrace_preempt_enable(resched);
@@ -108,7 +108,7 @@ function_trace_call(unsigned long ip, unsigned long parent_ip)
 
 	if (likely(disabled == 1)) {
 		pc = preempt_count();
-		trace_function(tr, data, ip, parent_ip, flags, pc);
+		trace_function(tr, ip, parent_ip, flags, pc);
 	}
 
 	atomic_dec(&data->disabled);
@@ -139,7 +139,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 
 	if (likely(disabled == 1)) {
 		pc = preempt_count();
-		trace_function(tr, data, ip, parent_ip, flags, pc);
+		trace_function(tr, ip, parent_ip, flags, pc);
 		/*
 		 * skip over 5 funcs:
 		 *    __ftrace_trace_stack,
@@ -148,7 +148,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 		 *    ftrace_list_func
 		 *    ftrace_call
 		 */
-		__trace_stack(tr, data, flags, 5, pc);
+		__trace_stack(tr, flags, 5, pc);
 	}
 
 	atomic_dec(&data->disabled);

commit a225cdd263f340c864febb1992802fb5b08bc328
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 15 23:06:03 2009 -0500

    ftrace: remove static from function tracer functions
    
    Impact: clean up
    
    After reorganizing the functions in trace.c and trace_function.c,
    they no longer need to be in global context. This patch makes the
    functions and one variable into static.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 61d0b73dabf5..b3a320f8aba7 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -16,8 +16,14 @@
 
 #include "trace.h"
 
+/* function tracing enabled */
+static int			ftrace_function_enabled;
+
 static struct trace_array	*func_trace;
 
+static void tracing_start_function_trace(void);
+static void tracing_stop_function_trace(void);
+
 static void start_function_trace(struct trace_array *tr)
 {
 	func_trace = tr;
@@ -177,7 +183,7 @@ static struct tracer_flags func_flags = {
 	.opts = func_opts
 };
 
-void tracing_start_function_trace(void)
+static void tracing_start_function_trace(void)
 {
 	ftrace_function_enabled = 0;
 
@@ -194,7 +200,7 @@ void tracing_start_function_trace(void)
 	ftrace_function_enabled = 1;
 }
 
-void tracing_stop_function_trace(void)
+static void tracing_stop_function_trace(void)
 {
 	ftrace_function_enabled = 0;
 	/* OK if they are not registered */

commit 3eb36aa05329a47cbe201c151fd0024a4a3649cd
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 15 22:21:43 2009 -0500

    ftrace: combine stack trace in function call
    
    Impact: less likely to interleave function and stack traces
    
    This patch does replaces the separate stack trace on function with
    a record function and stack trace together. This will switch between
    the function only recording to a function and stack recording.
    
    Also some whitespace fix ups as well.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 2dce3c7370d1..61d0b73dabf5 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -133,6 +133,7 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 
 	if (likely(disabled == 1)) {
 		pc = preempt_count();
+		trace_function(tr, data, ip, parent_ip, flags, pc);
 		/*
 		 * skip over 5 funcs:
 		 *    __ftrace_trace_stack,
@@ -154,24 +155,6 @@ static struct ftrace_ops trace_ops __read_mostly =
 	.func = function_trace_call,
 };
 
-void tracing_start_function_trace(void)
-{
-	ftrace_function_enabled = 0;
-
-	if (trace_flags & TRACE_ITER_PREEMPTONLY)
-		trace_ops.func = function_trace_call_preempt_only;
-	else
-		trace_ops.func = function_trace_call;
-
-	register_ftrace_function(&trace_ops);
-	ftrace_function_enabled = 1;
-}
-
-void tracing_stop_function_trace(void)
-{
-	ftrace_function_enabled = 0;
-	unregister_ftrace_function(&trace_ops);
-}
 static struct ftrace_ops trace_stack_ops __read_mostly =
 {
 	.func = function_stack_trace_call,
@@ -194,6 +177,31 @@ static struct tracer_flags func_flags = {
 	.opts = func_opts
 };
 
+void tracing_start_function_trace(void)
+{
+	ftrace_function_enabled = 0;
+
+	if (trace_flags & TRACE_ITER_PREEMPTONLY)
+		trace_ops.func = function_trace_call_preempt_only;
+	else
+		trace_ops.func = function_trace_call;
+
+	if (func_flags.val & TRACE_FUNC_OPT_STACK)
+		register_ftrace_function(&trace_stack_ops);
+	else
+		register_ftrace_function(&trace_ops);
+
+	ftrace_function_enabled = 1;
+}
+
+void tracing_stop_function_trace(void)
+{
+	ftrace_function_enabled = 0;
+	/* OK if they are not registered */
+	unregister_ftrace_function(&trace_stack_ops);
+	unregister_ftrace_function(&trace_ops);
+}
+
 static int func_set_flag(u32 old_flags, u32 bit, int set)
 {
 	if (bit == TRACE_FUNC_OPT_STACK) {
@@ -201,10 +209,13 @@ static int func_set_flag(u32 old_flags, u32 bit, int set)
 		if (!!set == !!(func_flags.val & TRACE_FUNC_OPT_STACK))
 			return 0;
 
-		if (set)
+		if (set) {
+			unregister_ftrace_function(&trace_ops);
 			register_ftrace_function(&trace_stack_ops);
-		else
+		} else {
 			unregister_ftrace_function(&trace_stack_ops);
+			register_ftrace_function(&trace_ops);
+		}
 
 		return 0;
 	}
@@ -214,14 +225,14 @@ static int func_set_flag(u32 old_flags, u32 bit, int set)
 
 static struct tracer function_trace __read_mostly =
 {
-	.name	     = "function",
-	.init	     = function_trace_init,
-	.reset	     = function_trace_reset,
-	.start	     = function_trace_start,
+	.name		= "function",
+	.init		= function_trace_init,
+	.reset		= function_trace_reset,
+	.start		= function_trace_start,
 	.flags		= &func_flags,
 	.set_flag	= func_set_flag,
 #ifdef CONFIG_FTRACE_SELFTEST
-	.selftest    = trace_selftest_startup_function,
+	.selftest	= trace_selftest_startup_function,
 #endif
 };
 

commit bb3c3c95f330f7bf16e33b002e48882616089db1
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 15 20:40:23 2009 -0500

    ftrace: move function tracer functions out of trace.c
    
    Impact: clean up of trace.c
    
    The function tracer functions were put in trace.c because it needed
    to share static variables that were in trace.c.  Since then, those
    variables have become global for various reasons. This patch moves
    the function tracer functions into trace_function.c where they belong.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 3a5fa08cedb0..2dce3c7370d1 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -20,6 +20,7 @@ static struct trace_array	*func_trace;
 
 static void start_function_trace(struct trace_array *tr)
 {
+	func_trace = tr;
 	tr->cpu = get_cpu();
 	tracing_reset_online_cpus(tr);
 	put_cpu();
@@ -36,7 +37,6 @@ static void stop_function_trace(struct trace_array *tr)
 
 static int function_trace_init(struct trace_array *tr)
 {
-	func_trace = tr;
 	start_function_trace(tr);
 	return 0;
 }
@@ -51,6 +51,64 @@ static void function_trace_start(struct trace_array *tr)
 	tracing_reset_online_cpus(tr);
 }
 
+static void
+function_trace_call_preempt_only(unsigned long ip, unsigned long parent_ip)
+{
+	struct trace_array *tr = func_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disabled;
+	int cpu, resched;
+	int pc;
+
+	if (unlikely(!ftrace_function_enabled))
+		return;
+
+	pc = preempt_count();
+	resched = ftrace_preempt_disable();
+	local_save_flags(flags);
+	cpu = raw_smp_processor_id();
+	data = tr->data[cpu];
+	disabled = atomic_inc_return(&data->disabled);
+
+	if (likely(disabled == 1))
+		trace_function(tr, data, ip, parent_ip, flags, pc);
+
+	atomic_dec(&data->disabled);
+	ftrace_preempt_enable(resched);
+}
+
+static void
+function_trace_call(unsigned long ip, unsigned long parent_ip)
+{
+	struct trace_array *tr = func_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disabled;
+	int cpu;
+	int pc;
+
+	if (unlikely(!ftrace_function_enabled))
+		return;
+
+	/*
+	 * Need to use raw, since this must be called before the
+	 * recursive protection is performed.
+	 */
+	local_irq_save(flags);
+	cpu = raw_smp_processor_id();
+	data = tr->data[cpu];
+	disabled = atomic_inc_return(&data->disabled);
+
+	if (likely(disabled == 1)) {
+		pc = preempt_count();
+		trace_function(tr, data, ip, parent_ip, flags, pc);
+	}
+
+	atomic_dec(&data->disabled);
+	local_irq_restore(flags);
+}
+
 static void
 function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 {
@@ -90,6 +148,30 @@ function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
 	local_irq_restore(flags);
 }
 
+
+static struct ftrace_ops trace_ops __read_mostly =
+{
+	.func = function_trace_call,
+};
+
+void tracing_start_function_trace(void)
+{
+	ftrace_function_enabled = 0;
+
+	if (trace_flags & TRACE_ITER_PREEMPTONLY)
+		trace_ops.func = function_trace_call_preempt_only;
+	else
+		trace_ops.func = function_trace_call;
+
+	register_ftrace_function(&trace_ops);
+	ftrace_function_enabled = 1;
+}
+
+void tracing_stop_function_trace(void)
+{
+	ftrace_function_enabled = 0;
+	unregister_ftrace_function(&trace_ops);
+}
 static struct ftrace_ops trace_stack_ops __read_mostly =
 {
 	.func = function_stack_trace_call,

commit 5361499101306cfb776c3cfa0f69d0479bc63868
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 15 19:12:40 2009 -0500

    ftrace: add stack trace to function tracer
    
    Impact: new feature to stack trace any function
    
    Chris Mason asked about being able to pick and choose a function
    and get a stack trace from it. This feature enables his request.
    
     # echo io_schedule > /debug/tracing/set_ftrace_filter
     # echo function > /debug/tracing/current_tracer
     # echo func_stack_trace > /debug/tracing/trace_options
    
    Produces the following in /debug/tracing/trace:
    
           kjournald-702   [001]   135.673060: io_schedule <-sync_buffer
           kjournald-702   [002]   135.673671:
     <= sync_buffer
     <= __wait_on_bit
     <= out_of_line_wait_on_bit
     <= __wait_on_buffer
     <= sync_dirty_buffer
     <= journal_commit_transaction
     <= kjournald
    
    Note, be careful about turning this on without filtering the functions.
    You may find that you have a 10 second lag between typing and seeing
    what you typed. This is why the stack trace for the function tracer
    does not use the same stack_trace flag as the other tracers use.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 9236d7e25a16..3a5fa08cedb0 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -16,6 +16,8 @@
 
 #include "trace.h"
 
+static struct trace_array	*func_trace;
+
 static void start_function_trace(struct trace_array *tr)
 {
 	tr->cpu = get_cpu();
@@ -34,6 +36,7 @@ static void stop_function_trace(struct trace_array *tr)
 
 static int function_trace_init(struct trace_array *tr)
 {
+	func_trace = tr;
 	start_function_trace(tr);
 	return 0;
 }
@@ -48,12 +51,93 @@ static void function_trace_start(struct trace_array *tr)
 	tracing_reset_online_cpus(tr);
 }
 
+static void
+function_stack_trace_call(unsigned long ip, unsigned long parent_ip)
+{
+	struct trace_array *tr = func_trace;
+	struct trace_array_cpu *data;
+	unsigned long flags;
+	long disabled;
+	int cpu;
+	int pc;
+
+	if (unlikely(!ftrace_function_enabled))
+		return;
+
+	/*
+	 * Need to use raw, since this must be called before the
+	 * recursive protection is performed.
+	 */
+	local_irq_save(flags);
+	cpu = raw_smp_processor_id();
+	data = tr->data[cpu];
+	disabled = atomic_inc_return(&data->disabled);
+
+	if (likely(disabled == 1)) {
+		pc = preempt_count();
+		/*
+		 * skip over 5 funcs:
+		 *    __ftrace_trace_stack,
+		 *    __trace_stack,
+		 *    function_stack_trace_call
+		 *    ftrace_list_func
+		 *    ftrace_call
+		 */
+		__trace_stack(tr, data, flags, 5, pc);
+	}
+
+	atomic_dec(&data->disabled);
+	local_irq_restore(flags);
+}
+
+static struct ftrace_ops trace_stack_ops __read_mostly =
+{
+	.func = function_stack_trace_call,
+};
+
+/* Our two options */
+enum {
+	TRACE_FUNC_OPT_STACK = 0x1,
+};
+
+static struct tracer_opt func_opts[] = {
+#ifdef CONFIG_STACKTRACE
+	{ TRACER_OPT(func_stack_trace, TRACE_FUNC_OPT_STACK) },
+#endif
+	{ } /* Always set a last empty entry */
+};
+
+static struct tracer_flags func_flags = {
+	.val = 0, /* By default: all flags disabled */
+	.opts = func_opts
+};
+
+static int func_set_flag(u32 old_flags, u32 bit, int set)
+{
+	if (bit == TRACE_FUNC_OPT_STACK) {
+		/* do nothing if already set */
+		if (!!set == !!(func_flags.val & TRACE_FUNC_OPT_STACK))
+			return 0;
+
+		if (set)
+			register_ftrace_function(&trace_stack_ops);
+		else
+			unregister_ftrace_function(&trace_stack_ops);
+
+		return 0;
+	}
+
+	return -EINVAL;
+}
+
 static struct tracer function_trace __read_mostly =
 {
 	.name	     = "function",
 	.init	     = function_trace_init,
 	.reset	     = function_trace_reset,
 	.start	     = function_trace_start,
+	.flags		= &func_flags,
+	.set_flag	= func_set_flag,
 #ifdef CONFIG_FTRACE_SELFTEST
 	.selftest    = trace_selftest_startup_function,
 #endif

commit 213cc060797378059a28ebc5c539f3e9a80160bd
Author: Pekka J Enberg <penberg@cs.helsinki.fi>
Date:   Fri Dec 19 12:08:39 2008 +0200

    ftrace: introduce tracing_reset_online_cpus() helper
    
    Impact: cleanup
    
    This patch factors out common code from multiple tracers into a
    tracing_reset_online_cpus() function and converts the tracers to use it.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index e74f6d0a3216..9236d7e25a16 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -16,20 +16,10 @@
 
 #include "trace.h"
 
-static void function_reset(struct trace_array *tr)
-{
-	int cpu;
-
-	tr->time_start = ftrace_now(tr->cpu);
-
-	for_each_online_cpu(cpu)
-		tracing_reset(tr, cpu);
-}
-
 static void start_function_trace(struct trace_array *tr)
 {
 	tr->cpu = get_cpu();
-	function_reset(tr);
+	tracing_reset_online_cpus(tr);
 	put_cpu();
 
 	tracing_start_cmdline_record();
@@ -55,7 +45,7 @@ static void function_trace_reset(struct trace_array *tr)
 
 static void function_trace_start(struct trace_array *tr)
 {
-	function_reset(tr);
+	tracing_reset_online_cpus(tr);
 }
 
 static struct tracer function_trace __read_mostly =

commit 1c80025a49855b12fa09bb6db71820e3367b1369
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 16 05:57:26 2008 +0100

    tracing/ftrace: change the type of the init() callback
    
    Impact: extend the ->init() method with the ability to fail
    
    This bring a way to know if the initialization of a tracer successed.
    A tracer must return 0 on success and a traditional error (ie:
    -ENOMEM) if it fails.
    
    If a tracer fails to init, it is free to print a detailed warn. The
    tracing api will not and switch to a new tracer will just return the
    error from the init callback.
    
    Note: this will be used for the return tracer.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 8693b7a0a5b2..e74f6d0a3216 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -42,9 +42,10 @@ static void stop_function_trace(struct trace_array *tr)
 	tracing_stop_cmdline_record();
 }
 
-static void function_trace_init(struct trace_array *tr)
+static int function_trace_init(struct trace_array *tr)
 {
 	start_function_trace(tr);
+	return 0;
 }
 
 static void function_trace_reset(struct trace_array *tr)

commit c76f06945be50564f925799ddfb6235ee4c26aa0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: remove trace array ctrl
    
    Impact: remove obsolete variable in trace_array structure
    
    With the new start / stop method of ftrace, the ctrl variable
    in the trace_array structure is now obsolete. Remove it.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index e980b872bef5..8693b7a0a5b2 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -44,14 +44,12 @@ static void stop_function_trace(struct trace_array *tr)
 
 static void function_trace_init(struct trace_array *tr)
 {
-	if (tr->ctrl)
-		start_function_trace(tr);
+	start_function_trace(tr);
 }
 
 static void function_trace_reset(struct trace_array *tr)
 {
-	if (tr->ctrl)
-		stop_function_trace(tr);
+	stop_function_trace(tr);
 }
 
 static void function_trace_start(struct trace_array *tr)

commit bbf5b1a0cecb56de6236db8b01c5bfb7ab8ba8b2
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: remove ctrl_update method
    
    Impact: Remove the ctrl_update tracer method
    
    With the new quick start/stop method of tracing, the ctrl_update
    method is out of date.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 9f1b0de71284..e980b872bef5 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -54,14 +54,6 @@ static void function_trace_reset(struct trace_array *tr)
 		stop_function_trace(tr);
 }
 
-static void function_trace_ctrl_update(struct trace_array *tr)
-{
-	if (tr->ctrl)
-		start_function_trace(tr);
-	else
-		stop_function_trace(tr);
-}
-
 static void function_trace_start(struct trace_array *tr)
 {
 	function_reset(tr);
@@ -73,7 +65,6 @@ static struct tracer function_trace __read_mostly =
 	.init	     = function_trace_init,
 	.reset	     = function_trace_reset,
 	.start	     = function_trace_start,
-	.ctrl_update = function_trace_ctrl_update,
 #ifdef CONFIG_FTRACE_SELFTEST
 	.selftest    = trace_selftest_startup_function,
 #endif

commit 9036990d462e09366f7297a2d1da6582c3e6b1d3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 5 16:05:44 2008 -0500

    ftrace: restructure tracing start/stop infrastructure
    
    Impact: change where tracing is started up and stopped
    
    Currently, when a new tracer is selected via echo'ing a tracer name into
    the current_tracer file, the startup is only done if tracing_enabled is
    set to one. If tracing_enabled is changed to zero (by echo'ing 0 into
    the tracing_enabled file) a full shutdown is performed.
    
    The full startup and shutdown of a tracer can be expensive and the
    user can lose out traces when echo'ing in 0 to the tracing_enabled file,
    because the process takes too long. There can also be places that
    the user would like to start and stop the tracer several times and
    doing the full startup and shutdown of a tracer might be too expensive.
    
    This patch performs the full startup and shutdown when a tracer is
    selected. It also adds a way to do a quick start or stop of a tracer.
    The quick version is just a flag that prevents the tracing from
    taking place, but the overhead of the code is still there.
    
    For example, the startup of a tracer may enable tracepoints, or enable
    the function tracer.  The stop and start will just set a flag to
    have the tracer ignore the calls when the tracepoint or function trace
    is called.  The overhead of the tracer may still be present when
    the tracer is stopped, but no tracing will occur. Setting the tracer
    to the 'nop' tracer (or any other tracer) will perform the shutdown
    of the tracer which will disable the tracepoint or disable the
    function tracer.
    
    The tracing_enabled file will simply start or stop tracing.
    
    This change is all internal. The end result for the user should be the same
    as before. If tracing_enabled is not set, no trace will happen.
    If tracing_enabled is set, then the trace will happen. The tracing_enabled
    variable is static between tracers. Enabling  tracing_enabled and
    going to another tracer will keep tracing_enabled enabled. Same
    is true with disabling tracing_enabled.
    
    This patch will now provide a fast start/stop method to the users
    for enabling or disabling tracing.
    
    Note: There were two methods to the struct tracer that were never
     used: The methods start and stop. These were to be used as a hook
     to the reading of the trace output, but ended up not being
     necessary. These two methods are now used to enable the start
     and stop of each tracer, in case the tracer needs to do more than
     just not write into the buffer. For example, the irqsoff tracer
     must stop recording max latencies when tracing is stopped.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 0f85a64003d3..9f1b0de71284 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -62,11 +62,17 @@ static void function_trace_ctrl_update(struct trace_array *tr)
 		stop_function_trace(tr);
 }
 
+static void function_trace_start(struct trace_array *tr)
+{
+	function_reset(tr);
+}
+
 static struct tracer function_trace __read_mostly =
 {
 	.name	     = "function",
 	.init	     = function_trace_init,
 	.reset	     = function_trace_reset,
+	.start	     = function_trace_start,
 	.ctrl_update = function_trace_ctrl_update,
 #ifdef CONFIG_FTRACE_SELFTEST
 	.selftest    = trace_selftest_startup_function,

commit 3ce83aea86bf46fd1bff59d2e6d16f48fdce22fc
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Oct 6 19:06:13 2008 -0400

    ftrace: rename the ftrace tracer to function
    
    To avoid further confusion between the ftrace infrastructure and the
    function tracer. This patch renames the "ftrace" function tracer
    to "function".
    
    Now in available_tracers, instead of "ftrace" there will be "function".
    
    This makes more sense, since people will not know exactly what the
    "ftrace" tracer does.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index e90eb0c2c56c..0f85a64003d3 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -64,7 +64,7 @@ static void function_trace_ctrl_update(struct trace_array *tr)
 
 static struct tracer function_trace __read_mostly =
 {
-	.name	     = "ftrace",
+	.name	     = "function",
 	.init	     = function_trace_init,
 	.reset	     = function_trace_reset,
 	.ctrl_update = function_trace_ctrl_update,

commit 3928a8a2d98081d1bc3c0a84a2d70e29b90ecf1c
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Sep 29 23:02:41 2008 -0400

    ftrace: make work with new ring buffer
    
    This patch ports ftrace over to the new ring buffer.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 312144897970..e90eb0c2c56c 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -23,7 +23,7 @@ static void function_reset(struct trace_array *tr)
 	tr->time_start = ftrace_now(tr->cpu);
 
 	for_each_online_cpu(cpu)
-		tracing_reset(tr->data[cpu]);
+		tracing_reset(tr, cpu);
 }
 
 static void start_function_trace(struct trace_array *tr)

commit 26bc83f4cb911a0b4dabfe23b700aaf3235f2955
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Jul 10 20:58:14 2008 -0400

    ftrace: use current CPU for function startup
    
    This is more of a clean up. Currently the function tracer initializes the
    tracer with which ever CPU was last used for tracing. This value isn't
    realy useful for function tracing, but at least it should be something other
    than a random number.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 7ee7dcd76b7d..312144897970 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -28,7 +28,10 @@ static void function_reset(struct trace_array *tr)
 
 static void start_function_trace(struct trace_array *tr)
 {
+	tr->cpu = get_cpu();
 	function_reset(tr);
+	put_cpu();
+
 	tracing_start_cmdline_record();
 	tracing_start_function_trace();
 }

commit 41bc8144d02028133bcd1d545023c6f49e8b2411
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu May 22 11:49:22 2008 -0400

    ftrace: fix up cmdline recording
    
    The new work with converting the trace hooks over to markers broke the
    command line recording of ftrace. This patch fixes it again.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 0a084656d7cf..7ee7dcd76b7d 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -29,14 +29,14 @@ static void function_reset(struct trace_array *tr)
 static void start_function_trace(struct trace_array *tr)
 {
 	function_reset(tr);
-	atomic_inc(&trace_record_cmdline_enabled);
+	tracing_start_cmdline_record();
 	tracing_start_function_trace();
 }
 
 static void stop_function_trace(struct trace_array *tr)
 {
 	tracing_stop_function_trace();
-	atomic_dec(&trace_record_cmdline_enabled);
+	tracing_stop_cmdline_record();
 }
 
 static void function_trace_init(struct trace_array *tr)

commit 25b0b44a1c732ccfc58095cdd8438955a0a19fff
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:21:00 2008 +0200

    ftrace: fix comm on function trace output
    
    In cleaning up of the sched_switch code, the function trace recording
    of task comms was removed. This patch adds back the recording of comms
    for function trace. The output of ftrace now has the task comm instead
    of <...>.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 4165d34bd28a..0a084656d7cf 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -29,12 +29,14 @@ static void function_reset(struct trace_array *tr)
 static void start_function_trace(struct trace_array *tr)
 {
 	function_reset(tr);
+	atomic_inc(&trace_record_cmdline_enabled);
 	tracing_start_function_trace();
 }
 
 static void stop_function_trace(struct trace_array *tr)
 {
 	tracing_stop_function_trace();
+	atomic_dec(&trace_record_cmdline_enabled);
 }
 
 static void function_trace_init(struct trace_array *tr)

commit e309b41dd65aa953f86765eeeecc941d8e1e8b8f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:51 2008 +0200

    ftrace: remove notrace
    
    now that we have a kbuild method for notrace, no need to pollute the
    C code with the annotations.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 69a0eb00a0a5..4165d34bd28a 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -16,7 +16,7 @@
 
 #include "trace.h"
 
-static notrace void function_reset(struct trace_array *tr)
+static void function_reset(struct trace_array *tr)
 {
 	int cpu;
 
@@ -26,30 +26,30 @@ static notrace void function_reset(struct trace_array *tr)
 		tracing_reset(tr->data[cpu]);
 }
 
-static notrace void start_function_trace(struct trace_array *tr)
+static void start_function_trace(struct trace_array *tr)
 {
 	function_reset(tr);
 	tracing_start_function_trace();
 }
 
-static notrace void stop_function_trace(struct trace_array *tr)
+static void stop_function_trace(struct trace_array *tr)
 {
 	tracing_stop_function_trace();
 }
 
-static notrace void function_trace_init(struct trace_array *tr)
+static void function_trace_init(struct trace_array *tr)
 {
 	if (tr->ctrl)
 		start_function_trace(tr);
 }
 
-static notrace void function_trace_reset(struct trace_array *tr)
+static void function_trace_reset(struct trace_array *tr)
 {
 	if (tr->ctrl)
 		stop_function_trace(tr);
 }
 
-static notrace void function_trace_ctrl_update(struct trace_array *tr)
+static void function_trace_ctrl_update(struct trace_array *tr)
 {
 	if (tr->ctrl)
 		start_function_trace(tr);

commit 2e0f57618529a2739a5e1570e6c445c9c966b595
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:49 2008 +0200

    ftrace: build fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index e5d34b78fc99..69a0eb00a0a5 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -9,10 +9,10 @@
  *  Copyright (C) 2004-2006 Ingo Molnar
  *  Copyright (C) 2004 William Lee Irwin III
  */
-#include <linux/fs.h>
 #include <linux/debugfs.h>
 #include <linux/uaccess.h>
 #include <linux/ftrace.h>
+#include <linux/fs.h>
 
 #include "trace.h"
 

commit 750ed1a40783432d0dcb0e6c2e813a12615d7664
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:46 2008 +0200

    ftrace: timestamp syncing, prepare
    
    rename and uninline now() to ftrace_now().
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 5d8ad7a09605..e5d34b78fc99 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -20,7 +20,7 @@ static notrace void function_reset(struct trace_array *tr)
 {
 	int cpu;
 
-	tr->time_start = now(tr->cpu);
+	tr->time_start = ftrace_now(tr->cpu);
 
 	for_each_online_cpu(cpu)
 		tracing_reset(tr->data[cpu]);

commit 60a11774b38fef1ab90b18c5353bd1c7c4d311c8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:44 2008 +0200

    ftrace: add self-tests
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
index 82988c5336e0..5d8ad7a09605 100644
--- a/kernel/trace/trace_functions.c
+++ b/kernel/trace/trace_functions.c
@@ -63,6 +63,9 @@ static struct tracer function_trace __read_mostly =
 	.init	     = function_trace_init,
 	.reset	     = function_trace_reset,
 	.ctrl_update = function_trace_ctrl_update,
+#ifdef CONFIG_FTRACE_SELFTEST
+	.selftest    = trace_selftest_startup_function,
+#endif
 };
 
 static __init int init_function_trace(void)

commit 1b29b01887e6032dcaf818c14999c7a39593b4e7
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: function tracer
    
    This is a simple trace that uses the ftrace infrastructure. It is
    designed to be fast and small, and easy to use. It is useful to
    record things that happen over a very short period of time, and
    not to analyze the system in general.
    
     Updates:
    
      available_tracers
         "function" is added to this file.
    
      current_tracer
        To enable the function tracer:
    
          echo function > /debugfs/tracing/current_tracer
    
         To disable the tracer:
    
           echo disable > /debugfs/tracing/current_tracer
    
    The output of the function_trace file is as follows
    
      "echo noverbose > /debugfs/tracing/iter_ctrl"
    
    preemption latency trace v1.1.5 on 2.6.24-rc7-tst
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    --------------------------------------------------------------------
     latency: 0 us, #419428/4361791, CPU#1 | (M:desktop VP:0, KP:0, SP:0 HP:0 #P:4)
        -----------------
        | task: -0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
    
                     _------=> CPU#
                    / _-----=> irqs-off
                   | / _----=> need-resched
                   || / _---=> hardirq/softirq
                   ||| / _--=> preempt-depth
                   |||| /
                   |||||     delay
       cmd     pid ||||| time  |   caller
          \   /    |||||   \   |   /
     swapper-0     0d.h. 1595128us+: set_normalized_timespec+0x8/0x2d <c043841d> (ktime_get_ts+0x4a/0x4e <c04499d4>)
     swapper-0     0d.h. 1595131us+: _spin_lock+0x8/0x18 <c0630690> (hrtimer_interrupt+0x6e/0x1b0 <c0449c56>)
    
    Or with verbose turned on:
    
      "echo verbose > /debugfs/tracing/iter_ctrl"
    
    preemption latency trace v1.1.5 on 2.6.24-rc7-tst
    --------------------------------------------------------------------
     latency: 0 us, #419428/4361791, CPU#1 | (M:desktop VP:0, KP:0, SP:0 HP:0 #P:4)
        -----------------
        | task: -0 (uid:0 nice:0 policy:0 rt_prio:0)
        -----------------
    
             swapper     0 0 9 00000000 00000000 [f3675f41] 1595.128ms (+0.003ms): set_normalized_timespec+0x8/0x2d <c043841d> (ktime_get_ts+0x4a/0x4e <c04499d4>)
             swapper     0 0 9 00000000 00000001 [f3675f45] 1595.131ms (+0.003ms): _spin_lock+0x8/0x18 <c0630690> (hrtimer_interrupt+0x6e/0x1b0 <c0449c56>)
             swapper     0 0 9 00000000 00000002 [f3675f48] 1595.135ms (+0.003ms): _spin_lock+0x8/0x18 <c0630690> (hrtimer_interrupt+0x6e/0x1b0 <c0449c56>)
    
    The "trace" file is not affected by the verbose mode, but is by the symonly.
    
     echo "nosymonly" > /debugfs/tracing/iter_ctrl
    
    tracer:
    [   81.479967] CPU 0: bash:3154 register_ftrace_function+0x5f/0x66 <ffffffff80337a4d> <-- _spin_unlock_irqrestore+0xe/0x5a <ffffffff8048cc8f>
    [   81.479967] CPU 0: bash:3154 _spin_unlock_irqrestore+0x3e/0x5a <ffffffff8048ccbf> <-- sub_preempt_count+0xc/0x7a <ffffffff80233d7b>
    [   81.479968] CPU 0: bash:3154 sub_preempt_count+0x30/0x7a <ffffffff80233d9f> <-- in_lock_functions+0x9/0x24 <ffffffff8025a75d>
    [   81.479968] CPU 0: bash:3154 vfs_write+0x11d/0x155 <ffffffff8029a043> <-- dnotify_parent+0x12/0x78 <ffffffff802d54fb>
    [   81.479968] CPU 0: bash:3154 dnotify_parent+0x2d/0x78 <ffffffff802d5516> <-- _spin_lock+0xe/0x70 <ffffffff8048c910>
    [   81.479969] CPU 0: bash:3154 _spin_lock+0x1b/0x70 <ffffffff8048c91d> <-- add_preempt_count+0xe/0x77 <ffffffff80233df7>
    [   81.479969] CPU 0: bash:3154 add_preempt_count+0x3e/0x77 <ffffffff80233e27> <-- in_lock_functions+0x9/0x24 <ffffffff8025a75d>
    
     echo "symonly" > /debugfs/tracing/iter_ctrl
    
    tracer:
    [   81.479913] CPU 0: bash:3154 register_ftrace_function+0x5f/0x66 <-- _spin_unlock_irqrestore+0xe/0x5a
    [   81.479913] CPU 0: bash:3154 _spin_unlock_irqrestore+0x3e/0x5a <-- sub_preempt_count+0xc/0x7a
    [   81.479913] CPU 0: bash:3154 sub_preempt_count+0x30/0x7a <-- in_lock_functions+0x9/0x24
    [   81.479914] CPU 0: bash:3154 vfs_write+0x11d/0x155 <-- dnotify_parent+0x12/0x78
    [   81.479914] CPU 0: bash:3154 dnotify_parent+0x2d/0x78 <-- _spin_lock+0xe/0x70
    [   81.479914] CPU 0: bash:3154 _spin_lock+0x1b/0x70 <-- add_preempt_count+0xe/0x77
    [   81.479914] CPU 0: bash:3154 add_preempt_count+0x3e/0x77 <-- in_lock_functions+0x9/0x24
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace_functions.c b/kernel/trace/trace_functions.c
new file mode 100644
index 000000000000..82988c5336e0
--- /dev/null
+++ b/kernel/trace/trace_functions.c
@@ -0,0 +1,73 @@
+/*
+ * ring buffer based function tracer
+ *
+ * Copyright (C) 2007-2008 Steven Rostedt <srostedt@redhat.com>
+ * Copyright (C) 2008 Ingo Molnar <mingo@redhat.com>
+ *
+ * Based on code from the latency_tracer, that is:
+ *
+ *  Copyright (C) 2004-2006 Ingo Molnar
+ *  Copyright (C) 2004 William Lee Irwin III
+ */
+#include <linux/fs.h>
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+#include <linux/ftrace.h>
+
+#include "trace.h"
+
+static notrace void function_reset(struct trace_array *tr)
+{
+	int cpu;
+
+	tr->time_start = now(tr->cpu);
+
+	for_each_online_cpu(cpu)
+		tracing_reset(tr->data[cpu]);
+}
+
+static notrace void start_function_trace(struct trace_array *tr)
+{
+	function_reset(tr);
+	tracing_start_function_trace();
+}
+
+static notrace void stop_function_trace(struct trace_array *tr)
+{
+	tracing_stop_function_trace();
+}
+
+static notrace void function_trace_init(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		start_function_trace(tr);
+}
+
+static notrace void function_trace_reset(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		stop_function_trace(tr);
+}
+
+static notrace void function_trace_ctrl_update(struct trace_array *tr)
+{
+	if (tr->ctrl)
+		start_function_trace(tr);
+	else
+		stop_function_trace(tr);
+}
+
+static struct tracer function_trace __read_mostly =
+{
+	.name	     = "ftrace",
+	.init	     = function_trace_init,
+	.reset	     = function_trace_reset,
+	.ctrl_update = function_trace_ctrl_update,
+};
+
+static __init int init_function_trace(void)
+{
+	return register_tracer(&function_trace);
+}
+
+device_initcall(init_function_trace);
