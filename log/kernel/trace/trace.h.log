commit 4649079b9de1ad86be9f4c989373adb8235a8485
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Jun 9 22:00:41 2020 -0400

    tracing: Make ftrace packed events have align of 1
    
    When using trace-cmd on 5.6-rt for the function graph tracer, the output was
    corrupted. It gave output like this:
    
     funcgraph_entry:       func=0xffffffff depth=38982
     funcgraph_entry:       func=0x1ffffffff depth=16044
     funcgraph_exit:        func=0xffffffff overrun=0x92539aaf00000000 calltime=0x92539c9900000072 rettime=0x100000072 depth=11084
     funcgraph_exit:        func=0xffffffff overrun=0x9253946e00000000 calltime=0x92539e2100000072 rettime=0x72 depth=26033702
     funcgraph_entry:       func=0xffffffff depth=85798
     funcgraph_entry:       func=0x1ffffffff depth=12044
    
    The reason was because the tracefs/events/ftrace/funcgraph_entry/exit format
    file was incorrect. The -rt kernel adds more common fields to the trace
    events. Namely, common_migrate_disable and common_preempt_lazy_count. Each
    is one byte in size. This changes the alignment of the normal payload. Most
    events are aligned normally, but the function and function graph events are
    defined with a "PACKED" macro, that packs their payload. As the offsets
    displayed in the format files are now calculated by an aligned field, the
    aligned field for function and function graph events should be 1, not their
    normal alignment.
    
    With aligning of the funcgraph_entry event, the format file has:
    
            field:unsigned short common_type;       offset:0;       size:2; signed:0;
            field:unsigned char common_flags;       offset:2;       size:1; signed:0;
            field:unsigned char common_preempt_count;       offset:3;       size:1; signed:0;
            field:int common_pid;   offset:4;       size:4; signed:1;
            field:unsigned char common_migrate_disable;     offset:8;       size:1; signed:0;
            field:unsigned char common_preempt_lazy_count;  offset:9;       size:1; signed:0;
    
            field:unsigned long func;       offset:16;      size:8; signed:0;
            field:int depth;        offset:24;      size:4; signed:1;
    
    But the actual alignment is:
    
            field:unsigned short common_type;       offset:0;       size:2; signed:0;
            field:unsigned char common_flags;       offset:2;       size:1; signed:0;
            field:unsigned char common_preempt_count;       offset:3;       size:1; signed:0;
            field:int common_pid;   offset:4;       size:4; signed:1;
            field:unsigned char common_migrate_disable;     offset:8;       size:1; signed:0;
            field:unsigned char common_preempt_lazy_count;  offset:9;       size:1; signed:0;
    
            field:unsigned long func;       offset:12;      size:8; signed:0;
            field:int depth;        offset:20;      size:4; signed:1;
    
    Link: https://lkml.kernel.org/r/20200609220041.2a3b527f@oasis.local.home
    
    Cc: stable@vger.kernel.org
    Fixes: 04ae87a52074e ("ftrace: Rework event_create_dir()")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index def769df5bf1..13db4000af3f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -61,6 +61,9 @@ enum trace_type {
 #undef __field_desc
 #define __field_desc(type, container, item)
 
+#undef __field_packed
+#define __field_packed(type, container, item)
+
 #undef __array
 #define __array(type, item, size)	type	item[size];
 

commit 2d19bd79ae6509858582a9cade739c2e9a4fdca8
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Fri Apr 3 14:31:21 2020 -0500

    tracing: Add hist_debug trace event files for histogram debugging
    
    Add a new "hist_debug" file for each trace event, which when read will
    dump out a bunch of internal details about the hist triggers defined
    on that event.
    
    This is normally off but can be enabled by saying 'y' to the new
    CONFIG_HIST_TRIGGERS_DEBUG config option.
    
    This is in support of the new Documentation file describing histogram
    internals, Documentation/trace/histogram-design.rst, which was
    requested by developers trying to understand the internals when
    extending or making use of the hist triggers for higher-level tools.
    
    The histogram-design.rst documentation refers to the hist_debug files
    and demonstrates their use with output in the test examples.
    
    Link: http://lkml.kernel.org/r/77914c22b0ba493d9783c53bbfbc6087d6a7e1b1.1585941485.git.zanussi@kernel.org
    
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4eb1d004d5f2..def769df5bf1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1661,6 +1661,7 @@ extern struct list_head ftrace_events;
 
 extern const struct file_operations event_trigger_fops;
 extern const struct file_operations event_hist_fops;
+extern const struct file_operations event_hist_debug_fops;
 extern const struct file_operations event_inject_fops;
 
 #ifdef CONFIG_HIST_TRIGGERS

commit 2768362603018da2be44ae4d01f22406152db05a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Mar 25 19:51:19 2020 -0400

    tracing: Create set_event_notrace_pid to not trace tasks
    
    There's currently a way to select a task that should only have its events
    traced, but there's no way to select a task not to have itsevents traced.
    Add a set_event_notrace_pid file that acts the same as set_event_pid (and is
    also affected by event-fork), but the task pids in this file will not be
    traced even if they are listed in the set_event_pid file. This makes it easy
    for tools like trace-cmd to "hide" itself from beint traced by events when
    it is recording other tasks.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6b5ff5adb4ad..4eb1d004d5f2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -207,6 +207,30 @@ struct trace_pid_list {
 	unsigned long			*pids;
 };
 
+enum {
+	TRACE_PIDS		= BIT(0),
+	TRACE_NO_PIDS		= BIT(1),
+};
+
+static inline bool pid_type_enabled(int type, struct trace_pid_list *pid_list,
+				    struct trace_pid_list *no_pid_list)
+{
+	/* Return true if the pid list in type has pids */
+	return ((type & TRACE_PIDS) && pid_list) ||
+		((type & TRACE_NO_PIDS) && no_pid_list);
+}
+
+static inline bool still_need_pid_events(int type, struct trace_pid_list *pid_list,
+					 struct trace_pid_list *no_pid_list)
+{
+	/*
+	 * Turning off what is in @type, return true if the "other"
+	 * pid list, still has pids in it.
+	 */
+	return (!(type & TRACE_PIDS) && pid_list) ||
+		(!(type & TRACE_NO_PIDS) && no_pid_list);
+}
+
 typedef bool (*cond_update_fn_t)(struct trace_array *tr, void *cond_data);
 
 /**
@@ -285,6 +309,7 @@ struct trace_array {
 #endif
 #endif
 	struct trace_pid_list	__rcu *filtered_pids;
+	struct trace_pid_list	__rcu *filtered_no_pids;
 	/*
 	 * max_lock is used to protect the swapping of buffers
 	 * when taking a max snapshot. The buffers themselves are

commit b3b1e6ededa4337940adba6cf06e8351056e3097
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Mar 19 23:19:06 2020 -0400

    ftrace: Create set_ftrace_notrace_pid to not trace tasks
    
    There's currently a way to select a task that should only be traced by
    functions, but there's no way to select a task not to be traced by the
    function tracer. Add a set_ftrace_notrace_pid file that acts the same as
    set_ftrace_pid (and is also affected by function-fork), but the task pids in
    this file will not be traced even if they are listed in the set_ftrace_pid
    file. This makes it easy for tools like trace-cmd to "hide" itself from the
    function tracer when it is recording other tasks.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fdc72f5f0bb0..6b5ff5adb4ad 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -331,6 +331,7 @@ struct trace_array {
 #ifdef CONFIG_FUNCTION_TRACER
 	struct ftrace_ops	*ops;
 	struct trace_pid_list	__rcu *function_pids;
+	struct trace_pid_list	__rcu *function_no_pids;
 #ifdef CONFIG_DYNAMIC_FTRACE
 	/* All of these are protected by the ftrace_lock */
 	struct list_head	func_probes;
@@ -782,6 +783,7 @@ extern int pid_max;
 bool trace_find_filtered_pid(struct trace_pid_list *filtered_pids,
 			     pid_t search_pid);
 bool trace_ignore_this_task(struct trace_pid_list *filtered_pids,
+			    struct trace_pid_list *filtered_no_pids,
 			    struct task_struct *task);
 void trace_filter_add_remove_task(struct trace_pid_list *pid_list,
 				  struct task_struct *self,

commit 717e3f5ebc823e5ecfdd15155b0fd81af9fc58d6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Mar 19 23:40:40 2020 -0400

    ftrace: Make function trace pid filtering a bit more exact
    
    The set_ftrace_pid file is used to filter function tracing to only trace
    tasks that are listed in that file. Instead of testing the pids listed in
    that file (it's a bitmask) at each function trace event, the logic is done
    via a sched_switch hook. A flag is set when the next task to run is in the
    list of pids in the set_ftrace_pid file. But the sched_switch hook is not at
    the exact location of when the task switches, and the flag gets set before
    the task to be traced actually runs. This leaves a residue of traced
    functions that do not belong to the pid that should be filtered on.
    
    By changing the logic slightly, where instead of having  a boolean flag to
    test, record the pid that should be traced, with special values for not to
    trace and always trace. Then at each function call, a check will be made to
    see if the function should be ignored, or if the current pid matches the
    function that should be traced, and only trace if it matches (or if it has
    the special value to always trace).
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f37e05135986..fdc72f5f0bb0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -178,10 +178,10 @@ struct trace_array_cpu {
 	kuid_t			uid;
 	char			comm[TASK_COMM_LEN];
 
-	bool			ignore_pid;
 #ifdef CONFIG_FUNCTION_TRACER
-	bool			ftrace_ignore_pid;
+	int			ftrace_ignore_pid;
 #endif
+	bool			ignore_pid;
 };
 
 struct tracer;

commit 06e0a548bad0f43a21e036db018e4dadb501ce8b
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Mar 17 17:32:31 2020 -0400

    tracing: Do not disable tracing when reading the trace file
    
    When opening the "trace" file, it is no longer necessary to disable tracing.
    
    Note, a new option is created called "pause-on-trace", when set, will cause
    the trace file to emulate its original behavior.
    
    Link: http://lkml.kernel.org/r/20200317213416.903351225@goodmis.org
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c61e1b1c85a6..f37e05135986 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1302,6 +1302,7 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(IRQ_INFO,		"irq-info"),		\
 		C(MARKERS,		"markers"),		\
 		C(EVENT_FORK,		"event-fork"),		\
+		C(PAUSE_ON_TRACE,	"pause-on-trace"),	\
 		FUNCTION_FLAGS					\
 		FGRAPH_FLAGS					\
 		STACK_FLAGS					\

commit 5412e0b763e0c46165fa353f695fa3b68a66ac91
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Feb 14 16:20:04 2020 -0500

    tracing: Remove unused TRACE_BUFFER bits
    
    Commit 567cd4da54ff ("ring-buffer: User context bit recursion checking")
    added the TRACE_BUFFER bits to be used in the current task's trace_recursion
    field. But the final submission of the logic removed the use of those bits,
    but never removed the bits themselves (they were never used in upstream
    Linux). These can be safely removed.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 99372dd7d168..c61e1b1c85a6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -557,12 +557,7 @@ struct tracer {
  * caller, and we can skip the current check.
  */
 enum {
-	TRACE_BUFFER_BIT,
-	TRACE_BUFFER_NMI_BIT,
-	TRACE_BUFFER_IRQ_BIT,
-	TRACE_BUFFER_SIRQ_BIT,
-
-	/* Start of function recursion bits */
+	/* Function recursion bits */
 	TRACE_FTRACE_BIT,
 	TRACE_FTRACE_NMI_BIT,
 	TRACE_FTRACE_IRQ_BIT,

commit e310396bb8d7db977a0e10ef7b5040e98b89c34c
Merge: c1ef57a3a3f5 a00574036c26
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 6 07:12:11 2020 +0000

    Merge tag 'trace-v5.6-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
    
     - Added new "bootconfig".
    
       This looks for a file appended to initrd to add boot config options,
       and has been discussed thoroughly at Linux Plumbers.
    
       Very useful for adding kprobes at bootup.
    
       Only enabled if "bootconfig" is on the real kernel command line.
    
     - Created dynamic event creation.
    
       Merges common code between creating synthetic events and kprobe
       events.
    
     - Rename perf "ring_buffer" structure to "perf_buffer"
    
     - Rename ftrace "ring_buffer" structure to "trace_buffer"
    
       Had to rename existing "trace_buffer" to "array_buffer"
    
     - Allow trace_printk() to work withing (some) tracing code.
    
     - Sort of tracing configs to be a little better organized
    
     - Fixed bug where ftrace_graph hash was not being protected properly
    
     - Various other small fixes and clean ups
    
    * tag 'trace-v5.6-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (88 commits)
      bootconfig: Show the number of nodes on boot message
      tools/bootconfig: Show the number of bootconfig nodes
      bootconfig: Add more parse error messages
      bootconfig: Use bootconfig instead of boot config
      ftrace: Protect ftrace_graph_hash with ftrace_sync
      ftrace: Add comment to why rcu_dereference_sched() is open coded
      tracing: Annotate ftrace_graph_notrace_hash pointer with __rcu
      tracing: Annotate ftrace_graph_hash pointer with __rcu
      bootconfig: Only load bootconfig if "bootconfig" is on the kernel cmdline
      tracing: Use seq_buf for building dynevent_cmd string
      tracing: Remove useless code in dynevent_arg_pair_add()
      tracing: Remove check_arg() callbacks from dynevent args
      tracing: Consolidate some synth_event_trace code
      tracing: Fix now invalid var_ref_vals assumption in trace action
      tracing: Change trace_boot to use synth_event interface
      tracing: Move tracing selftests to bottom of menu
      tracing: Move mmio tracer config up with the other tracers
      tracing: Move tracing test module configs together
      tracing: Move all function tracing configs together
      tracing: Documentation for in-kernel synthetic event API
      ...

commit 54a16ff6f2e50775145b210bcd94d62c3c2af117
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Feb 5 09:20:32 2020 -0500

    ftrace: Protect ftrace_graph_hash with ftrace_sync
    
    As function_graph tracer can run when RCU is not "watching", it can not be
    protected by synchronize_rcu() it requires running a task on each CPU before
    it can be freed. Calling schedule_on_each_cpu(ftrace_sync) needs to be used.
    
    Link: https://lore.kernel.org/r/20200205131110.GT2935@paulmck-ThinkPad-P72
    
    Cc: stable@vger.kernel.org
    Fixes: b9b0c831bed26 ("ftrace: Convert graph filter to use hash tables")
    Reported-by: "Paul E. McKenney" <paulmck@kernel.org>
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8c52f5de9384..3c75d29bd861 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -979,6 +979,7 @@ static inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)
 	 * Have to open code "rcu_dereference_sched()" because the
 	 * function graph tracer can be called when RCU is not
 	 * "watching".
+	 * Protected with schedule_on_each_cpu(ftrace_sync)
 	 */
 	hash = rcu_dereference_protected(ftrace_graph_hash, !preemptible());
 
@@ -1031,6 +1032,7 @@ static inline int ftrace_graph_notrace_addr(unsigned long addr)
 	 * Have to open code "rcu_dereference_sched()" because the
 	 * function graph tracer can be called when RCU is not
 	 * "watching".
+	 * Protected with schedule_on_each_cpu(ftrace_sync)
 	 */
 	notrace_hash = rcu_dereference_protected(ftrace_graph_notrace_hash,
 						 !preemptible());

commit 16052dd5bdfa16dbe18d8c1d4cde2ddab9d23177
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Feb 5 02:17:57 2020 -0500

    ftrace: Add comment to why rcu_dereference_sched() is open coded
    
    Because the function graph tracer can execute in sections where RCU is not
    "watching", the rcu_dereference_sched() for the has needs to be open coded.
    This is fine because the RCU "flavor" of the ftrace hash is protected by
    its own RCU handling (it does its own little synchronization on every CPU
    and does not rely on RCU sched).
    
    Acked-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 022def96d307..8c52f5de9384 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -975,6 +975,11 @@ static inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)
 
 	preempt_disable_notrace();
 
+	/*
+	 * Have to open code "rcu_dereference_sched()" because the
+	 * function graph tracer can be called when RCU is not
+	 * "watching".
+	 */
 	hash = rcu_dereference_protected(ftrace_graph_hash, !preemptible());
 
 	if (ftrace_hash_empty(hash)) {
@@ -1022,6 +1027,11 @@ static inline int ftrace_graph_notrace_addr(unsigned long addr)
 
 	preempt_disable_notrace();
 
+	/*
+	 * Have to open code "rcu_dereference_sched()" because the
+	 * function graph tracer can be called when RCU is not
+	 * "watching".
+	 */
 	notrace_hash = rcu_dereference_protected(ftrace_graph_notrace_hash,
 						 !preemptible());
 

commit fd0e6852c407dd9aefc594f54ddcc21d84803d3b
Author: Amol Grover <frextrite@gmail.com>
Date:   Wed Feb 5 11:27:02 2020 +0530

    tracing: Annotate ftrace_graph_notrace_hash pointer with __rcu
    
    Fix following instances of sparse error
    kernel/trace/ftrace.c:5667:29: error: incompatible types in comparison
    kernel/trace/ftrace.c:5813:21: error: incompatible types in comparison
    kernel/trace/ftrace.c:5868:36: error: incompatible types in comparison
    kernel/trace/ftrace.c:5870:25: error: incompatible types in comparison
    
    Use rcu_dereference_protected to dereference the newly annotated pointer.
    
    Link: http://lkml.kernel.org/r/20200205055701.30195-1-frextrite@gmail.com
    
    Signed-off-by: Amol Grover <frextrite@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 18ceab59a5ba..022def96d307 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -965,7 +965,7 @@ extern void __trace_graph_return(struct trace_array *tr,
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern struct ftrace_hash __rcu *ftrace_graph_hash;
-extern struct ftrace_hash *ftrace_graph_notrace_hash;
+extern struct ftrace_hash __rcu *ftrace_graph_notrace_hash;
 
 static inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)
 {
@@ -1018,10 +1018,14 @@ static inline void ftrace_graph_addr_finish(struct ftrace_graph_ret *trace)
 static inline int ftrace_graph_notrace_addr(unsigned long addr)
 {
 	int ret = 0;
+	struct ftrace_hash *notrace_hash;
 
 	preempt_disable_notrace();
 
-	if (ftrace_lookup_ip(ftrace_graph_notrace_hash, addr))
+	notrace_hash = rcu_dereference_protected(ftrace_graph_notrace_hash,
+						 !preemptible());
+
+	if (ftrace_lookup_ip(notrace_hash, addr))
 		ret = 1;
 
 	preempt_enable_notrace();

commit 24a9729f831462b1d9d61dc85ecc91c59037243f
Author: Amol Grover <frextrite@gmail.com>
Date:   Sat Feb 1 12:57:04 2020 +0530

    tracing: Annotate ftrace_graph_hash pointer with __rcu
    
    Fix following instances of sparse error
    kernel/trace/ftrace.c:5664:29: error: incompatible types in comparison
    kernel/trace/ftrace.c:5785:21: error: incompatible types in comparison
    kernel/trace/ftrace.c:5864:36: error: incompatible types in comparison
    kernel/trace/ftrace.c:5866:25: error: incompatible types in comparison
    
    Use rcu_dereference_protected to access the __rcu annotated pointer.
    
    Link: http://lkml.kernel.org/r/20200201072703.17330-1-frextrite@gmail.com
    
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Amol Grover <frextrite@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f5480a2aa334..18ceab59a5ba 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -964,22 +964,25 @@ extern void __trace_graph_return(struct trace_array *tr,
 				 unsigned long flags, int pc);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-extern struct ftrace_hash *ftrace_graph_hash;
+extern struct ftrace_hash __rcu *ftrace_graph_hash;
 extern struct ftrace_hash *ftrace_graph_notrace_hash;
 
 static inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)
 {
 	unsigned long addr = trace->func;
 	int ret = 0;
+	struct ftrace_hash *hash;
 
 	preempt_disable_notrace();
 
-	if (ftrace_hash_empty(ftrace_graph_hash)) {
+	hash = rcu_dereference_protected(ftrace_graph_hash, !preemptible());
+
+	if (ftrace_hash_empty(hash)) {
 		ret = 1;
 		goto out;
 	}
 
-	if (ftrace_lookup_ip(ftrace_graph_hash, addr)) {
+	if (ftrace_lookup_ip(hash, addr)) {
 
 		/*
 		 * This needs to be cleared on the return functions

commit 89c95fcef1942415e0f20d8c82e6e36ff8eeca9c
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:21 2020 -0600

    tracing: Add trace_array_find/_get() to find instance trace arrays
    
    Add a new trace_array_find() function that can be used to find a trace
    array given the instance name, and replace existing code that does the
    same thing with it.  Also add trace_array_find_get() which does the
    same but returns the trace array after upping its refcount.
    
    Also make both available for use outside of trace.c.
    
    Link: http://lkml.kernel.org/r/cb68528c975eba95bee4561ac67dd1499423b2e5.1580323897.git.zanussi@kernel.org
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b3075b637d14..f5480a2aa334 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -358,6 +358,8 @@ extern struct mutex trace_types_lock;
 
 extern int trace_array_get(struct trace_array *tr);
 extern int tracing_check_open_get_tr(struct trace_array *tr);
+extern struct trace_array *trace_array_find(const char *instance);
+extern struct trace_array *trace_array_find_get(const char *instance);
 
 extern int tracing_set_time_stamp_abs(struct trace_array *tr, bool abs);
 extern int tracing_set_clock(struct trace_array *tr, const char *clockstr);

commit 5c3469cb899abe998299aafb8f16f325d62d2d68
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Wed Jan 29 18:36:44 2020 +0900

    tracing/boot: Move external function declarations to kernel/trace/trace.h
    
    Move external function declarations into kernel/trace/trace.h
    from trace_boot.c for tracing subsystem internal use.
    
    Link: http://lkml.kernel.org/r/158029060405.12381.11944554430359702545.stgit@devnote2
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6bb64d89c321..b3075b637d14 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1157,6 +1157,11 @@ int unregister_ftrace_command(struct ftrace_func_command *cmd);
 void ftrace_create_filter_files(struct ftrace_ops *ops,
 				struct dentry *parent);
 void ftrace_destroy_filter_files(struct ftrace_ops *ops);
+
+extern int ftrace_set_filter(struct ftrace_ops *ops, unsigned char *buf,
+			     int len, int reset);
+extern int ftrace_set_notrace(struct ftrace_ops *ops, unsigned char *buf,
+			      int len, int reset);
 #else
 struct ftrace_func_command;
 
@@ -1905,6 +1910,15 @@ void trace_printk_start_comm(void);
 int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);
 int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 
+/* Used from boot time tracer */
+extern int trace_set_options(struct trace_array *tr, char *option);
+extern int tracing_set_tracer(struct trace_array *tr, const char *buf);
+extern ssize_t tracing_resize_ring_buffer(struct trace_array *tr,
+					  unsigned long size, int cpu_id);
+extern int tracing_set_cpumask(struct trace_array *tr,
+				cpumask_var_t tracing_cpumask_new);
+
+
 #define MAX_EVENT_NAME_LEN	64
 
 extern int trace_run_command(const char *buf, int (*createfn)(int, char**));
@@ -1964,6 +1978,9 @@ static inline const char *get_syscall_name(int syscall)
 #ifdef CONFIG_EVENT_TRACING
 void trace_event_init(void);
 void trace_event_eval_update(struct trace_eval_map **map, int len);
+/* Used from boot time tracer */
+extern int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set);
+extern int trigger_process_regex(struct trace_event_file *file, char *buff);
 #else
 static inline void __init trace_event_init(void) { }
 static inline void trace_event_eval_update(struct trace_eval_map **map, int len) { }

commit 24589e3a20876dc07c62f45c8f8f8266dd39ba38
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Sat Jan 25 10:52:30 2020 -0500

    tracing: Use pr_err() instead of WARN() for memory failures
    
    As warnings can trigger panics, especially when "panic_on_warn" is set,
    memory failure warnings can cause panics and fail fuzz testers that are
    stressing memory.
    
    Create a MEM_FAIL() macro to use instead of WARN() in the tracing code
    (perhaps this should be a kernel wide macro?), and use that for memory
    failure issues. This should stop failing fuzz tests due to warnings.
    
    Link: https://lore.kernel.org/r/CACT4Y+ZP-7np20GVRu3p+eZys9GPtbu+JpfV+HtsufAzvTgJrg@mail.gmail.com
    
    Suggested-by: Dmitry Vyukov <dvyukov@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4812a36affac..6bb64d89c321 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -94,6 +94,18 @@ enum trace_type {
 
 #include "trace_entries.h"
 
+/* Use this for memory failure errors */
+#define MEM_FAIL(condition, fmt, ...) ({			\
+	static bool __section(.data.once) __warned;		\
+	int __ret_warn_once = !!(condition);			\
+								\
+	if (unlikely(__ret_warn_once && !__warned)) {		\
+		__warned = true;				\
+		pr_err("ERROR: " fmt, ##__VA_ARGS__);		\
+	}							\
+	unlikely(__ret_warn_once);				\
+})
+
 /*
  * syscalls are special, and need special handling, this is why
  * they are not included in trace_entries.h

commit 13292494379f92f532de71b31a54018336adc589
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 13 13:58:57 2019 -0500

    tracing: Make struct ring_buffer less ambiguous
    
    As there's two struct ring_buffers in the kernel, it causes some confusion.
    The other one being the perf ring buffer. It was agreed upon that as neither
    of the ring buffers are generic enough to be used globally, they should be
    renamed as:
    
       perf's ring_buffer -> perf_buffer
       ftrace's ring_buffer -> trace_buffer
    
    This implements the changes to the ring buffer that ftrace uses.
    
    Link: https://lore.kernel.org/r/20191213140531.116b3200@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fd679fe92c1f..4812a36affac 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -178,7 +178,7 @@ struct trace_option_dentry;
 
 struct array_buffer {
 	struct trace_array		*tr;
-	struct ring_buffer		*buffer;
+	struct trace_buffer		*buffer;
 	struct trace_array_cpu __percpu	*data;
 	u64				time_start;
 	int				cpu;
@@ -705,7 +705,7 @@ struct dentry *tracing_init_dentry(void);
 struct ring_buffer_event;
 
 struct ring_buffer_event *
-trace_buffer_lock_reserve(struct ring_buffer *buffer,
+trace_buffer_lock_reserve(struct trace_buffer *buffer,
 			  int type,
 			  unsigned long len,
 			  unsigned long flags,
@@ -717,7 +717,7 @@ struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 struct trace_entry *trace_find_next_entry(struct trace_iterator *iter,
 					  int *ent_cpu, u64 *ent_ts);
 
-void trace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,
+void trace_buffer_unlock_commit_nostack(struct trace_buffer *buffer,
 					struct ring_buffer_event *event);
 
 int trace_empty(struct trace_iterator *iter);
@@ -873,7 +873,7 @@ trace_vprintk(unsigned long ip, const char *fmt, va_list args);
 extern int
 trace_array_vprintk(struct trace_array *tr,
 		    unsigned long ip, const char *fmt, va_list args);
-int trace_array_printk_buf(struct ring_buffer *buffer,
+int trace_array_printk_buf(struct trace_buffer *buffer,
 			   unsigned long ip, const char *fmt, ...);
 void trace_printk_seq(struct trace_seq *s);
 enum print_line_t print_trace_line(struct trace_iterator *iter);
@@ -1367,17 +1367,17 @@ struct trace_subsystem_dir {
 };
 
 extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
-				     struct ring_buffer *buffer,
+				     struct trace_buffer *buffer,
 				     struct ring_buffer_event *event);
 
 void trace_buffer_unlock_commit_regs(struct trace_array *tr,
-				     struct ring_buffer *buffer,
+				     struct trace_buffer *buffer,
 				     struct ring_buffer_event *event,
 				     unsigned long flags, int pc,
 				     struct pt_regs *regs);
 
 static inline void trace_buffer_unlock_commit(struct trace_array *tr,
-					      struct ring_buffer *buffer,
+					      struct trace_buffer *buffer,
 					      struct ring_buffer_event *event,
 					      unsigned long flags, int pc)
 {
@@ -1390,7 +1390,7 @@ void trace_buffered_event_disable(void);
 void trace_buffered_event_enable(void);
 
 static inline void
-__trace_event_discard_commit(struct ring_buffer *buffer,
+__trace_event_discard_commit(struct trace_buffer *buffer,
 			     struct ring_buffer_event *event)
 {
 	if (this_cpu_read(trace_buffered_event) == event) {
@@ -1416,7 +1416,7 @@ __trace_event_discard_commit(struct ring_buffer *buffer,
  */
 static inline bool
 __event_trigger_test_discard(struct trace_event_file *file,
-			     struct ring_buffer *buffer,
+			     struct trace_buffer *buffer,
 			     struct ring_buffer_event *event,
 			     void *entry,
 			     enum event_trigger_type *tt)
@@ -1451,7 +1451,7 @@ __event_trigger_test_discard(struct trace_event_file *file,
  */
 static inline void
 event_trigger_unlock_commit(struct trace_event_file *file,
-			    struct ring_buffer *buffer,
+			    struct trace_buffer *buffer,
 			    struct ring_buffer_event *event,
 			    void *entry, unsigned long irq_flags, int pc)
 {
@@ -1482,7 +1482,7 @@ event_trigger_unlock_commit(struct trace_event_file *file,
  */
 static inline void
 event_trigger_unlock_commit_regs(struct trace_event_file *file,
-				 struct ring_buffer *buffer,
+				 struct trace_buffer *buffer,
 				 struct ring_buffer_event *event,
 				 void *entry, unsigned long irq_flags, int pc,
 				 struct pt_regs *regs)

commit 1c5eb4481e0151d579f738175497f998840f7bbc
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jan 9 18:53:48 2020 -0500

    tracing: Rename trace_buffer to array_buffer
    
    As we are working to remove the generic "ring_buffer" name that is used by
    both tracing and perf, the ring_buffer name for tracing will be renamed to
    trace_buffer, and perf's ring buffer will be renamed to perf_buffer.
    
    As there already exists a trace_buffer that is used by the trace_arrays, it
    needs to be first renamed to array_buffer.
    
    Link: https://lore.kernel.org/r/20191213153553.GE20583@krava
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 63bf60f79398..fd679fe92c1f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -176,7 +176,7 @@ struct trace_array_cpu {
 struct tracer;
 struct trace_option_dentry;
 
-struct trace_buffer {
+struct array_buffer {
 	struct trace_array		*tr;
 	struct ring_buffer		*buffer;
 	struct trace_array_cpu __percpu	*data;
@@ -249,7 +249,7 @@ struct cond_snapshot {
 struct trace_array {
 	struct list_head	list;
 	char			*name;
-	struct trace_buffer	trace_buffer;
+	struct array_buffer	array_buffer;
 #ifdef CONFIG_TRACER_MAX_TRACE
 	/*
 	 * The max_buffer is used to snapshot the trace when a maximum
@@ -257,12 +257,12 @@ struct trace_array {
 	 * Some tracers will use this to store a maximum trace while
 	 * it continues examining live traces.
 	 *
-	 * The buffers for the max_buffer are set up the same as the trace_buffer
+	 * The buffers for the max_buffer are set up the same as the array_buffer
 	 * When a snapshot is taken, the buffer of the max_buffer is swapped
-	 * with the buffer of the trace_buffer and the buffers are reset for
-	 * the trace_buffer so the tracing can continue.
+	 * with the buffer of the array_buffer and the buffers are reset for
+	 * the array_buffer so the tracing can continue.
 	 */
-	struct trace_buffer	max_buffer;
+	struct array_buffer	max_buffer;
 	bool			allocated_snapshot;
 #endif
 #if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)
@@ -685,7 +685,7 @@ trace_buffer_iter(struct trace_iterator *iter, int cpu)
 
 int tracer_init(struct tracer *t, struct trace_array *tr);
 int tracing_is_enabled(void);
-void tracing_reset_online_cpus(struct trace_buffer *buf);
+void tracing_reset_online_cpus(struct array_buffer *buf);
 void tracing_reset_current(int cpu);
 void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
@@ -1057,7 +1057,7 @@ struct ftrace_func_command {
 extern bool ftrace_filter_param __initdata;
 static inline int ftrace_trace_task(struct trace_array *tr)
 {
-	return !this_cpu_read(tr->trace_buffer.data->ftrace_ignore_pid);
+	return !this_cpu_read(tr->array_buffer.data->ftrace_ignore_pid);
 }
 extern int ftrace_is_dead(void);
 int ftrace_create_function_files(struct trace_array *tr,

commit 2040cf9f59037aa8aec749363e69ead165b67b43
Merge: f66c0447cca1 e42617b825f8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 10 10:11:00 2019 +0100

    Merge tag 'v5.5-rc1' into core/kprobes, to resolve conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6c3edaf9fd6a3be7fb5bc6931897c24cd3848f84
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Fri Nov 29 20:52:18 2019 -0800

    tracing: Introduce trace event injection
    
    We have been trying to use rasdaemon to monitor hardware errors like
    correctable memory errors. rasdaemon uses trace events to monitor
    various hardware errors. In order to test it, we have to inject some
    hardware errors, unfortunately not all of them provide error
    injections. MCE does provide a way to inject MCE errors, but errors
    like PCI error and devlink error don't, it is not easy to add error
    injection to each of them. Instead, it is relatively easier to just
    allow users to inject trace events in a generic way so that all trace
    events can be injected.
    
    This patch introduces trace event injection, where a new 'inject' is
    added to each tracepoint directory. Users could write into this file
    with key=value pairs to specify the value of each fields of the trace
    event, all unspecified fields are set to zero values by default.
    
    For example, for the net/net_dev_queue tracepoint, we can inject:
    
      INJECT=/sys/kernel/debug/tracing/events/net/net_dev_queue/inject
      echo "" > $INJECT
      echo "name='test'" > $INJECT
      echo "name='test' len=1024" > $INJECT
      cat /sys/kernel/debug/tracing/trace
      ...
       <...>-614   [000] ....    36.571483: net_dev_queue: dev= skbaddr=00000000fbf338c2 len=0
       <...>-614   [001] ....   136.588252: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=0
       <...>-614   [001] .N..   208.431878: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=1024
    
    Triggers could be triggered as usual too:
    
      echo "stacktrace if len == 1025" > /sys/kernel/debug/tracing/events/net/net_dev_queue/trigger
      echo "len=1025" > $INJECT
      cat /sys/kernel/debug/tracing/trace
      ...
          bash-614   [000] ....    36.571483: net_dev_queue: dev= skbaddr=00000000fbf338c2 len=0
          bash-614   [001] ....   136.588252: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=0
          bash-614   [001] .N..   208.431878: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=1024
          bash-614   [001] .N.1   284.236349: <stack trace>
     => event_inject_write
     => vfs_write
     => ksys_write
     => do_syscall_64
     => entry_SYSCALL_64_after_hwframe
    
    The only thing that can't be injected is string pointers as they
    require constant string pointers, this can't be done at run time.
    
    Link: http://lkml.kernel.org/r/20191130045218.18979-1-xiyou.wangcong@gmail.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ca7fccafbcbb..63bf60f79398 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1601,6 +1601,7 @@ extern struct list_head ftrace_events;
 
 extern const struct file_operations event_trigger_fops;
 extern const struct file_operations event_hist_fops;
+extern const struct file_operations event_inject_fops;
 
 #ifdef CONFIG_HIST_TRIGGERS
 extern int register_trigger_hist_cmd(void);

commit 04ae87a52074e2d448fc66143f1bd2c7d694d2b9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 24 22:26:59 2019 +0200

    ftrace: Rework event_create_dir()
    
    Rework event_create_dir() to use an array of static data instead of
    function pointers where possible.
    
    The problem is that it would call the function pointer on module load
    before parse_args(), possibly even before jump_labels were initialized.
    Luckily the generated functions don't use jump_labels but it still seems
    fragile. It also gets in the way of changing when we make the module map
    executable.
    
    The generated function are basically calling trace_define_field() with a
    bunch of static arguments. So instead of a function, capture these
    arguments in a static array, avoiding the function call.
    
    Now there are a number of cases where the fields are dynamic (syscall
    arguments, kprobes and uprobes), in which case a static array does not
    work, for these we preserve the function call. Luckily all these cases
    are not related to modules and so we can retain the function call for
    them.
    
    Also fix up all broken tracepoint definitions that now generate a
    compile error.
    
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191111132458.342979914@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d685c61085c0..298a7cacf146 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -49,6 +49,9 @@ enum trace_type {
 #undef __field
 #define __field(type, item)		type	item;
 
+#undef __field_fn
+#define __field_fn(type, item)		type	item;
+
 #undef __field_struct
 #define __field_struct(type, item)	__field(type, item)
 
@@ -68,26 +71,22 @@ enum trace_type {
 #define F_STRUCT(args...)		args
 
 #undef FTRACE_ENTRY
-#define FTRACE_ENTRY(name, struct_name, id, tstruct, print, filter)	\
+#define FTRACE_ENTRY(name, struct_name, id, tstruct, print)		\
 	struct struct_name {						\
 		struct trace_entry	ent;				\
 		tstruct							\
 	}
 
 #undef FTRACE_ENTRY_DUP
-#define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk, filter)
+#define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk)
 
 #undef FTRACE_ENTRY_REG
-#define FTRACE_ENTRY_REG(name, struct_name, id, tstruct, print,	\
-			 filter, regfn) \
-	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print), \
-		     filter)
+#define FTRACE_ENTRY_REG(name, struct_name, id, tstruct, print,	regfn)	\
+	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print))
 
 #undef FTRACE_ENTRY_PACKED
-#define FTRACE_ENTRY_PACKED(name, struct_name, id, tstruct, print,	\
-			    filter)					\
-	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print), \
-		     filter) __packed
+#define FTRACE_ENTRY_PACKED(name, struct_name, id, tstruct, print)	\
+	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print)) __packed
 
 #include "trace_entries.h"
 
@@ -1899,17 +1898,15 @@ extern void tracing_log_err(struct trace_array *tr,
 #define internal_trace_puts(str) __trace_puts(_THIS_IP_, str, strlen(str))
 
 #undef FTRACE_ENTRY
-#define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\
+#define FTRACE_ENTRY(call, struct_name, id, tstruct, print)	\
 	extern struct trace_event_call					\
 	__aligned(4) event_##call;
 #undef FTRACE_ENTRY_DUP
-#define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print, filter)	\
-	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print), \
-		     filter)
+#define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print)	\
+	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))
 #undef FTRACE_ENTRY_PACKED
-#define FTRACE_ENTRY_PACKED(call, struct_name, id, tstruct, print, filter) \
-	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print), \
-		     filter)
+#define FTRACE_ENTRY_PACKED(call, struct_name, id, tstruct, print) \
+	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))
 
 #include "trace_entries.h"
 

commit 28879787147358e8ffcae397f11748de3dd26577
Author: Divya Indi <divya.indi@oracle.com>
Date:   Wed Nov 20 11:08:38 2019 -0800

    tracing: Adding new functions for kernel access to Ftrace instances
    
    Adding 2 new functions -
    1) struct trace_array *trace_array_get_by_name(const char *name);
    
    Return pointer to a trace array with given name. If it does not exist,
    create and return pointer to the new trace array.
    
    2) int trace_array_set_clr_event(struct trace_array *tr,
    const char *system ,const char *event, bool enable);
    
    Enable/Disable events to this trace array.
    
    Additionally,
    - To handle reference counters, export trace_array_put()
    - Due to introduction of the above 2 new functions, we no longer need to
      export - ftrace_set_clr_event & trace_array_create APIs.
    
    Link: http://lkml.kernel.org/r/1574276919-11119-2-git-send-email-divya.indi@oracle.com
    
    Signed-off-by: Divya Indi <divya.indi@oracle.com>
    Reviewed-by: Aruna Ramakrishna <aruna.ramakrishna@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2df8aed6a8f0..ca7fccafbcbb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -345,7 +345,6 @@ extern struct list_head ftrace_trace_arrays;
 extern struct mutex trace_types_lock;
 
 extern int trace_array_get(struct trace_array *tr);
-extern void trace_array_put(struct trace_array *tr);
 extern int tracing_check_open_get_tr(struct trace_array *tr);
 
 extern int tracing_set_time_stamp_abs(struct trace_array *tr, bool abs);

commit 36b3615dc3b625c8b587f34e413a600f7ac16403
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Nov 14 22:43:58 2019 -0500

    tracing: Add missing "inline" in stub function of latency_fsnotify()
    
    The latency_fsnotify() stub when the function is not defined, was missing
    the "inline".
    
    Link: https://lore.kernel.org/r/20191115140213.74c5efe7@canb.auug.org.au
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 90cba68c8b50..2df8aed6a8f0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -801,7 +801,7 @@ void latency_fsnotify(struct trace_array *tr);
 
 #else
 
-static void latency_fsnotify(struct trace_array *tr) { }
+static inline void latency_fsnotify(struct trace_array *tr) { }
 
 #endif
 

commit 2d6425af61166e026e7476db64f70f1266127b1d
Author: Divya Indi <divya.indi@oracle.com>
Date:   Wed Aug 14 10:55:23 2019 -0700

    tracing: Declare newly exported APIs in include/linux/trace.h
    
    Declare the newly introduced and exported APIs in the header file -
    include/linux/trace.h. Moving previous declarations from
    kernel/trace/trace.h to include/linux/trace.h.
    
    Link: http://lkml.kernel.org/r/1565805327-579-2-git-send-email-divya.indi@oracle.com
    
    Signed-off-by: Divya Indi <divya.indi@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 718eb998c13e..90cba68c8b50 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -11,6 +11,7 @@
 #include <linux/mmiotrace.h>
 #include <linux/tracepoint.h>
 #include <linux/ftrace.h>
+#include <linux/trace.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/trace_seq.h>
 #include <linux/trace_events.h>
@@ -873,8 +874,6 @@ trace_vprintk(unsigned long ip, const char *fmt, va_list args);
 extern int
 trace_array_vprintk(struct trace_array *tr,
 		    unsigned long ip, const char *fmt, va_list args);
-int trace_array_printk(struct trace_array *tr,
-		       unsigned long ip, const char *fmt, ...);
 int trace_array_printk_buf(struct ring_buffer *buffer,
 			   unsigned long ip, const char *fmt, ...);
 void trace_printk_seq(struct trace_seq *s);
@@ -1890,7 +1889,6 @@ extern const char *__start___tracepoint_str[];
 extern const char *__stop___tracepoint_str[];
 
 void trace_printk_control(bool enabled);
-void trace_printk_init_buffers(void);
 void trace_printk_start_comm(void);
 int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);
 int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);

commit 91edde2e6ae1dd5e33812f076f3fe4cb7ccbfdd0
Author: Viktor Rosendahl (BMW) <viktor.rosendahl@gmail.com>
Date:   Wed Oct 9 00:08:21 2019 +0200

    ftrace: Implement fs notification for tracing_max_latency
    
    This patch implements the feature that the tracing_max_latency file,
    e.g. /sys/kernel/debug/tracing/tracing_max_latency will receive
    notifications through the fsnotify framework when a new latency is
    available.
    
    One particularly interesting use of this facility is when enabling
    threshold tracing, through /sys/kernel/debug/tracing/tracing_thresh,
    together with the preempt/irqsoff tracers. This makes it possible to
    implement a user space program that can, with equal probability,
    obtain traces of latencies that occur immediately after each other in
    spite of the fact that the preempt/irqsoff tracers operate in overwrite
    mode.
    
    This facility works with the hwlat, preempt/irqsoff, and wakeup
    tracers.
    
    The tracers may call the latency_fsnotify() from places such as
    __schedule() or do_idle(); this makes it impossible to call
    queue_work() directly without risking a deadlock. The same would
    happen with a softirq,  kernel thread or tasklet. For this reason we
    use the irq_work mechanism to call queue_work().
    
    This patch creates a new workqueue. The reason for doing this is that
    I wanted to use the WQ_UNBOUND and WQ_HIGHPRI flags.  My thinking was
    that WQ_UNBOUND might help with the latency in some important cases.
    
    If we use:
    
    queue_work(system_highpri_wq, &tr->fsnotify_work);
    
    then the work will (almost) always execute on the same CPU but if we are
    unlucky that CPU could be too busy while there could be another CPU in
    the system that would be able to process the work soon enough.
    
    queue_work_on() could be used to queue the work on another CPU but it
    seems difficult to select the right CPU.
    
    Link: http://lkml.kernel.org/r/20191008220824.7911-2-viktor.rosendahl@gmail.com
    
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Viktor Rosendahl (BMW) <viktor.rosendahl@gmail.com>
    [ Added max() to have one compare for max latency ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8b590f10bc72..718eb998c13e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -16,6 +16,8 @@
 #include <linux/trace_events.h>
 #include <linux/compiler.h>
 #include <linux/glob.h>
+#include <linux/irq_work.h>
+#include <linux/workqueue.h>
 
 #ifdef CONFIG_FTRACE_SYSCALLS
 #include <asm/unistd.h>		/* For NR_SYSCALLS	     */
@@ -264,6 +266,11 @@ struct trace_array {
 #endif
 #if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)
 	unsigned long		max_latency;
+#ifdef CONFIG_FSNOTIFY
+	struct dentry		*d_max_latency;
+	struct work_struct	fsnotify_work;
+	struct irq_work		fsnotify_irqwork;
+#endif
 #endif
 	struct trace_pid_list	__rcu *filtered_pids;
 	/*
@@ -786,6 +793,17 @@ void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
 #endif /* CONFIG_TRACER_MAX_TRACE */
 
+#if (defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)) && \
+	defined(CONFIG_FSNOTIFY)
+
+void latency_fsnotify(struct trace_array *tr);
+
+#else
+
+static void latency_fsnotify(struct trace_array *tr) { }
+
+#endif
+
 #ifdef CONFIG_STACKTRACE
 void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
 		   int pc);

commit da537f0aef1372c5204356a7df06be8769467b7b
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Oct 1 14:38:07 2019 -0400

    ftrace: Add information on number of page groups allocated
    
    Looking for ways to shrink the size of the dyn_ftrace structure, knowing the
    information about how many pages and the number of groups of those pages, is
    useful in working out the best ways to save on memory.
    
    This adds one info print on how many groups of pages were used to allocate
    the ftrace dyn_ftrace structures, and also shows the number of pages and
    groups in the dyn_ftrace_total_info (which is used for debugging).
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d685c61085c0..8b590f10bc72 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -804,6 +804,8 @@ extern void trace_event_follow_fork(struct trace_array *tr, bool enable);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern unsigned long ftrace_update_tot_cnt;
+extern unsigned long ftrace_number_of_pages;
+extern unsigned long ftrace_number_of_groups;
 void ftrace_init_trace_array(struct trace_array *tr);
 #else
 static inline void ftrace_init_trace_array(struct trace_array *tr) { }

commit 8530dec63e7b486e3761cc3d74a22de301845ff5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Oct 11 17:39:57 2019 -0400

    tracing: Add tracing_check_open_get_tr()
    
    Currently, most files in the tracefs directory test if tracing_disabled is
    set. If so, it should return -ENODEV. The tracing_disabled is called when
    tracing is found to be broken. Originally it was done in case the ring
    buffer was found to be corrupted, and we wanted to prevent reading it from
    crashing the kernel. But it's also called if a tracing selftest fails on
    boot. It's a one way switch. That is, once it is triggered, tracing is
    disabled until reboot.
    
    As most tracefs files can also be used by instances in the tracefs
    directory, they need to be carefully done. Each instance has a trace_array
    associated to it, and when the instance is removed, the trace_array is
    freed. But if an instance is opened with a reference to the trace_array,
    then it requires looking up the trace_array to get its ref counter (as there
    could be a race with it being deleted and the open itself). Once it is
    found, a reference is added to prevent the instance from being removed (and
    the trace_array associated with it freed).
    
    Combine the two checks (tracing_disabled and trace_array_get()) into a
    single helper function. This will also make it easier to add lockdown to
    tracefs later.
    
    Link: http://lkml.kernel.org/r/20191011135458.7399da44@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 854dbf4050f8..d685c61085c0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -338,6 +338,7 @@ extern struct mutex trace_types_lock;
 
 extern int trace_array_get(struct trace_array *tr);
 extern void trace_array_put(struct trace_array *tr);
+extern int tracing_check_open_get_tr(struct trace_array *tr);
 
 extern int tracing_set_time_stamp_abs(struct trace_array *tr, bool abs);
 extern int tracing_set_clock(struct trace_array *tr, const char *clockstr);

commit aa07d71f1bc7ea20e442e812b5de9d632b7f84c6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Oct 11 19:12:21 2019 -0400

    tracing: Have trace events system open call tracing_open_generic_tr()
    
    Instead of having the trace events system open call open code the taking of
    the trace_array descriptor (with trace_array_get()) and then calling
    trace_open_generic(), have it use the tracing_open_generic_tr() that does
    the combination of the two. This requires making tracing_open_generic_tr()
    global.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f801d154ff6a..854dbf4050f8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -681,6 +681,7 @@ void tracing_reset_online_cpus(struct trace_buffer *buf);
 void tracing_reset_current(int cpu);
 void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
+int tracing_open_generic_tr(struct inode *inode, struct file *filp);
 bool tracing_is_disabled(void);
 bool tracer_tracing_is_on(struct trace_array *tr);
 void tracer_tracing_on(struct trace_array *tr);

commit 968e5170939662341242812b9c82ef51cf140a33
Author: Nathan Chancellor <natechancellor@gmail.com>
Date:   Thu Sep 26 09:22:59 2019 -0700

    tracing: Fix clang -Wint-in-bool-context warnings in IF_ASSIGN macro
    
    After r372664 in clang, the IF_ASSIGN macro causes a couple hundred
    warnings along the lines of:
    
    kernel/trace/trace_output.c:1331:2: warning: converting the enum
    constant to a boolean [-Wint-in-bool-context]
    kernel/trace/trace.h:409:3: note: expanded from macro
    'trace_assign_type'
                    IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,
                    ^
    kernel/trace/trace.h:371:14: note: expanded from macro 'IF_ASSIGN'
                    WARN_ON(id && (entry)->type != id);     \
                               ^
    264 warnings generated.
    
    This warning can catch issues with constructs like:
    
        if (state == A || B)
    
    where the developer really meant:
    
        if (state == A || state == B)
    
    This is currently the only occurrence of the warning in the kernel
    tree across defconfig, allyesconfig, allmodconfig for arm32, arm64,
    and x86_64. Add the implicit '!= 0' to the WARN_ON statement to fix
    the warnings and find potential issues in the future.
    
    Link: https://github.com/llvm/llvm-project/commit/28b38c277a2941e9e891b2db30652cfd962f070b
    Link: https://github.com/ClangBuiltLinux/linux/issues/686
    Link: http://lkml.kernel.org/r/20190926162258.466321-1-natechancellor@gmail.com
    
    Reviewed-by: Nick Desaulniers <ndesaulniers@google.com>
    Signed-off-by: Nathan Chancellor <natechancellor@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 26b0a08f3c7d..f801d154ff6a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -365,11 +365,11 @@ static inline struct trace_array *top_trace_array(void)
 	__builtin_types_compatible_p(typeof(var), type *)
 
 #undef IF_ASSIGN
-#define IF_ASSIGN(var, entry, etype, id)		\
-	if (FTRACE_CMP_TYPE(var, etype)) {		\
-		var = (typeof(var))(entry);		\
-		WARN_ON(id && (entry)->type != id);	\
-		break;					\
+#define IF_ASSIGN(var, entry, etype, id)			\
+	if (FTRACE_CMP_TYPE(var, etype)) {			\
+		var = (typeof(var))(entry);			\
+		WARN_ON(id != 0 && (entry)->type != id);	\
+		break;						\
 	}
 
 /* Will cause compile errors if type is not found. */

commit a47b53e95accfd2814efe39dfca06dbd45cd857a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Aug 13 12:14:35 2019 -0400

    tracing: Rename tracing_reset() to tracing_reset_cpu()
    
    The name tracing_reset() was a misnomer, as it really only reset a single
    CPU buffer. Rename it to tracing_reset_cpu() and also make it static and
    remove the prototype from trace.h, as it is only used in a single function.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 005f08629b8b..26b0a08f3c7d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -677,7 +677,6 @@ trace_buffer_iter(struct trace_iterator *iter, int cpu)
 
 int tracer_init(struct tracer *t, struct trace_array *tr);
 int tracing_is_enabled(void);
-void tracing_reset(struct trace_buffer *buf, int cpu);
 void tracing_reset_online_cpus(struct trace_buffer *buf);
 void tracing_reset_current(int cpu);
 void tracing_reset_all_online_cpus(void);

commit 0c97bf863efce63d6ab7971dad811601e6171d2f
Author: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
Date:   Thu May 23 14:45:35 2019 +0200

    tracing: Silence GCC 9 array bounds warning
    
    Starting with GCC 9, -Warray-bounds detects cases when memset is called
    starting on a member of a struct but the size to be cleared ends up
    writing over further members.
    
    Such a call happens in the trace code to clear, at once, all members
    after and including `seq` on struct trace_iterator:
    
        In function 'memset',
            inlined from 'ftrace_dump' at kernel/trace/trace.c:8914:3:
        ./include/linux/string.h:344:9: warning: '__builtin_memset' offset
        [8505, 8560] from the object at 'iter' is out of the bounds of
        referenced subobject 'seq' with type 'struct trace_seq' at offset
        4368 [-Warray-bounds]
          344 |  return __builtin_memset(p, c, size);
              |         ^~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    In order to avoid GCC complaining about it, we compute the address
    ourselves by adding the offsetof distance instead of referring
    directly to the member.
    
    Since there are two places doing this clear (trace.c and trace_kdb.c),
    take the chance to move the workaround into a single place in
    the internal header.
    
    Link: http://lkml.kernel.org/r/20190523124535.GA12931@gmail.com
    
    Signed-off-by: Miguel Ojeda <miguel.ojeda.sandonis@gmail.com>
    [ Removed unnecessary parenthesis around "iter" ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 82c70b63d375..005f08629b8b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1966,4 +1966,22 @@ static inline void tracer_hardirqs_off(unsigned long a0, unsigned long a1) { }
 
 extern struct trace_iterator *tracepoint_print_iter;
 
+/*
+ * Reset the state of the trace_iterator so that it can read consumed data.
+ * Normally, the trace_iterator is used for reading the data when it is not
+ * consumed, and must retain state.
+ */
+static __always_inline void trace_iterator_reset(struct trace_iterator *iter)
+{
+	const size_t offset = offsetof(struct trace_iterator, seq);
+
+	/*
+	 * Keep gcc from complaining about overwriting more than just one
+	 * member in the structure.
+	 */
+	memset((char *)iter + offset, 0, sizeof(struct trace_iterator) - offset);
+
+	iter->pos = -1;
+}
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 4eebe38a37f9397ffecd4bd3afbdf36838a97969
Author: Jagadeesh Pagadala <jagdsh.linux@gmail.com>
Date:   Thu Mar 28 03:49:46 2019 +0530

    kernel/trace/trace.h: Remove duplicate header of trace_seq.h
    
    Remove duplicate header which is included twice.
    
    Link: http://lkml.kernel.org/r/1553725186-41442-1-git-send-email-jagdsh.linux@gmail.com
    
    Reviewed-by: Mukesh Ojha <mojha@codeaurora.org>
    Signed-off-by: Jagadeesh Pagadala <jagdsh.linux@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1974ce818ddb..82c70b63d375 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -15,7 +15,6 @@
 #include <linux/trace_seq.h>
 #include <linux/trace_events.h>
 #include <linux/compiler.h>
-#include <linux/trace_seq.h>
 #include <linux/glob.h>
 
 #ifdef CONFIG_FTRACE_SYSCALLS

commit d2d8b146043ae7e250aef1fb312971f6f479d487
Merge: 2bbacd1a9278 693713cbdb3a
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 15 16:05:47 2019 -0700

    Merge tag 'trace-v5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "The major changes in this tracing update includes:
    
       - Removal of non-DYNAMIC_FTRACE from 32bit x86
    
       - Removal of mcount support from x86
    
       - Emulating a call from int3 on x86_64, fixes live kernel patching
    
       - Consolidated Tracing Error logs file
    
      Minor updates:
    
       - Removal of klp_check_compiler_support()
    
       - kdb ftrace dumping output changes
    
       - Accessing and creating ftrace instances from inside the kernel
    
       - Clean up of #define if macro
    
       - Introduction of TRACE_EVENT_NOP() to disable trace events based on
         config options
    
      And other minor fixes and clean ups"
    
    * tag 'trace-v5.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (44 commits)
      x86: Hide the int3_emulate_call/jmp functions from UML
      livepatch: Remove klp_check_compiler_support()
      ftrace/x86: Remove mcount support
      ftrace/x86_32: Remove support for non DYNAMIC_FTRACE
      tracing: Simplify "if" macro code
      tracing: Fix documentation about disabling options using trace_options
      tracing: Replace kzalloc with kcalloc
      tracing: Fix partial reading of trace event's id file
      tracing: Allow RCU to run between postponed startup tests
      tracing: Fix white space issues in parse_pred() function
      tracing: Eliminate const char[] auto variables
      ring-buffer: Fix mispelling of Calculate
      tracing: probeevent: Fix to make the type of $comm string
      tracing: probeevent: Do not accumulate on ret variable
      tracing: uprobes: Re-enable $comm support for uprobe events
      ftrace/x86_64: Emulate call function while updating in breakpoint handler
      x86_64: Allow breakpoints to emulate call instructions
      x86_64: Add gap to int3 to allow for call emulation
      tracing: kdb: Allow ftdump to skip all but the last few entries
      tracing: Add trace_total_entries() / trace_total_entries_cpu()
      ...

commit ecffc8a8c7301f6f3c731ba23e38cd049a046416
Author: Douglas Anderson <dianders@chromium.org>
Date:   Tue Mar 19 10:12:05 2019 -0700

    tracing: Add trace_total_entries() / trace_total_entries_cpu()
    
    These two new exported functions will be used in a future patch by
    kdb_ftdump() to quickly skip all but the last few trace entries.
    
    Link: http://lkml.kernel.org/r/20190319171206.97107-2-dianders@chromium.org
    
    Acked-by: Daniel Thompson <daniel.thompson@linaro.org>
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index da00a3d508c1..33f14b9e78b7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -721,6 +721,9 @@ void trace_init_global_iter(struct trace_iterator *iter);
 
 void tracing_iter_reset(struct trace_iterator *iter, int cpu);
 
+unsigned long trace_total_entries_cpu(struct trace_array *tr, int cpu);
+unsigned long trace_total_entries(struct trace_array *tr);
+
 void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,

commit c438f140cc16d47fac808d893f5017f6d641cb46
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Thu Apr 25 11:45:15 2019 +0200

    tracing: Make ftrace_trace_userstack() static and conditional
    
    It's only used in trace.c and there is absolutely no point in compiling it
    in when user space stack traces are not supported.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Steven Rostedt <rostedt@goodmis.org>
    Reviewed-by: Josh Poimboeuf <jpoimboe@redhat.com>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Alexander Potapenko <glider@google.com>
    Cc: Alexey Dobriyan <adobriyan@gmail.com>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Christoph Lameter <cl@linux.com>
    Cc: Pekka Enberg <penberg@kernel.org>
    Cc: linux-mm@kvack.org
    Cc: David Rientjes <rientjes@google.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Dmitry Vyukov <dvyukov@google.com>
    Cc: Andrey Ryabinin <aryabinin@virtuozzo.com>
    Cc: kasan-dev@googlegroups.com
    Cc: Mike Rapoport <rppt@linux.vnet.ibm.com>
    Cc: Akinobu Mita <akinobu.mita@gmail.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: iommu@lists.linux-foundation.org
    Cc: Robin Murphy <robin.murphy@arm.com>
    Cc: Marek Szyprowski <m.szyprowski@samsung.com>
    Cc: Johannes Thumshirn <jthumshirn@suse.de>
    Cc: David Sterba <dsterba@suse.com>
    Cc: Chris Mason <clm@fb.com>
    Cc: Josef Bacik <josef@toxicpanda.com>
    Cc: linux-btrfs@vger.kernel.org
    Cc: dm-devel@redhat.com
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Daniel Vetter <daniel@ffwll.ch>
    Cc: intel-gfx@lists.freedesktop.org
    Cc: Joonas Lahtinen <joonas.lahtinen@linux.intel.com>
    Cc: Maarten Lankhorst <maarten.lankhorst@linux.intel.com>
    Cc: dri-devel@lists.freedesktop.org
    Cc: David Airlie <airlied@linux.ie>
    Cc: Jani Nikula <jani.nikula@linux.intel.com>
    Cc: Rodrigo Vivi <rodrigo.vivi@intel.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Miroslav Benes <mbenes@suse.cz>
    Cc: linux-arch@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190425094803.162400595@linutronix.de

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d80cee49e0eb..639047b259d7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -782,17 +782,9 @@ void update_max_tr_single(struct trace_array *tr,
 #endif /* CONFIG_TRACER_MAX_TRACE */
 
 #ifdef CONFIG_STACKTRACE
-void ftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags,
-			    int pc);
-
 void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
 		   int pc);
 #else
-static inline void ftrace_trace_userstack(struct ring_buffer *buffer,
-					  unsigned long flags, int pc)
-{
-}
-
 static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
 				 int skip, int pc)
 {

commit 2f754e771b1a6feba670782e82c45555984ac43b
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 1 22:52:21 2019 -0400

    tracing: Have the error logs show up in the proper instances
    
    As each instance has their own error_log file, it makes more sense that the
    instances show the errors of their own instead of all error_logs having the
    same data. Make it that the errors show up in the instance error_log file
    that the error happens in. If no instance trace_array is available, then
    NULL can be passed in which will create the error in the top level instance
    (the one at the top of the tracefs directory).
    
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 809c5d7f0064..da00a3d508c1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -293,11 +293,13 @@ struct trace_array {
 	int			nr_topts;
 	bool			clear_trace;
 	int			buffer_percent;
+	unsigned int		n_err_log_entries;
 	struct tracer		*current_trace;
 	unsigned int		trace_flags;
 	unsigned char		trace_flags_index[TRACE_FLAGS_MAX_SIZE];
 	unsigned int		flags;
 	raw_spinlock_t		start_lock;
+	struct list_head	err_log;
 	struct dentry		*dir;
 	struct dentry		*options;
 	struct dentry		*percpu_dir;
@@ -1886,7 +1888,8 @@ extern ssize_t trace_parse_run_command(struct file *file,
 		int (*createfn)(int, char**));
 
 extern unsigned int err_pos(char *cmd, const char *str);
-extern void tracing_log_err(const char *loc, const char *cmd,
+extern void tracing_log_err(struct trace_array *tr,
+			    const char *loc, const char *cmd,
 			    const char **errs, u8 type, u8 pos);
 
 /*

commit 1e144d73f7295f766568c357448a11eb12868e29
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 1 16:07:48 2019 -0400

    tracing: Add trace_array parameter to create_event_filter()
    
    Pass in the trace_array that represents the instance the filter being
    changed is in to create_event_filter(). This will allow for error messages
    that happen when writing to the filter can be displayed in the proper
    instance "error_log" file.
    
    Note, for calls to create_filter() (that was also modified to support
    create_event_filter()), that changes filters that do not exist in a instance
    (for perf for example), NULL may be passed in, which means that there will
    not be any message to log for that filter.
    
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b711edbef7e7..809c5d7f0064 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1553,7 +1553,8 @@ extern int apply_subsystem_event_filter(struct trace_subsystem_dir *dir,
 extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);
 extern int filter_assign_type(const char *type);
-extern int create_event_filter(struct trace_event_call *call,
+extern int create_event_filter(struct trace_array *tr,
+			       struct trace_event_call *call,
 			       char *filter_str, bool set_str,
 			       struct event_filter **filterp);
 extern void free_event_filter(struct event_filter *filter);

commit 8a062902be725f647dc8da532b04d836546a369a
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Sun Mar 31 18:48:15 2019 -0500

    tracing: Add tracing error log
    
    Introduce a new ftrace file, tracing/error_log, for ftrace commands to
    log errors.  This is useful for allowing more complex commands such as
    hist trigger and kprobe_event commands to point out specifically where
    something may have gone wrong without forcing them to resort to more
    ad hoc methods such as tacking error messages onto existing output
    files.
    
    To log a tracing error, call the event_log_err() function, passing it
    a location string describing where it came from e.g. kprobe_events or
    system:event, the command that caused the error, an array of static
    error strings describing errors and an index within that array which
    describes the specific error, along with the position to place the
    error caret.
    
    Reading the log displays the last (currently) 8 errors logged in the
    following format:
    
      [timestamp] <loc>: error: <static error text>
        Command: <command that caused the error>
                          ^
    
    Memory for the error log isn't allocated unless there has been a trace
    event error, and the error log can be cleared and have its memory
    freed by writing the empty string in truncation mode to it:
    
      # echo > tracing/error_log.
    
    Link: http://lkml.kernel.org/r/0c2c82571fd38c5f3a88ca823627edff250e9416.1554072478.git.tom.zanussi@linux.intel.com
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Suggested-by: Masami Hiramatsu <mhiramat@kernel.org>
    Improvements-suggested-by: Steve Rostedt <rostedt@goodmis.org>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d80cee49e0eb..b711edbef7e7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1884,6 +1884,10 @@ extern ssize_t trace_parse_run_command(struct file *file,
 		const char __user *buffer, size_t count, loff_t *ppos,
 		int (*createfn)(int, char**));
 
+extern unsigned int err_pos(char *cmd, const char *str);
+extern void tracing_log_err(const char *loc, const char *cmd,
+			    const char **errs, u8 type, u8 pos);
+
 /*
  * Normal trace_printk() and friends allocates special buffers
  * to do the manipulation, as well as saves the print formats

commit a35873a0993b4d38b40871f10fa4356c088c7140
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Wed Feb 13 17:42:45 2019 -0600

    tracing: Add conditional snapshot
    
    Currently, tracing snapshots are context-free - they capture the ring
    buffer contents at the time the tracing_snapshot() function was
    invoked, and nothing else.  Additionally, they're always taken
    unconditionally - the calling code can decide whether or not to take a
    snapshot, but the data used to make that decision is kept separately
    from the snapshot itself.
    
    This change adds the ability to associate with each trace instance
    some user data, along with an 'update' function that can use that data
    to determine whether or not to actually take a snapshot.  The update
    function can then update that data along with any other state (as part
    of the data presumably), if warranted.
    
    Because snapshots are 'global' per-instance, only one user can enable
    and use a conditional snapshot for any given trace instance.  To
    enable a conditional snapshot (see details in the function and data
    structure comments), the user calls tracing_snapshot_cond_enable().
    Similarly, to disable a conditional snapshot and free it up for other
    users, tracing_snapshot_cond_disable() should be called.
    
    To actually initiate a conditional snapshot, tracing_snapshot_cond()
    should be called.  tracing_snapshot_cond() will invoke the update()
    callback, allowing the user to decide whether or not to actually take
    the snapshot and update the user-defined data associated with the
    snapshot.  If the callback returns 'true', tracing_snapshot_cond()
    will then actually take the snapshot and return.
    
    This scheme allows for flexibility in snapshot implementations - for
    example, by implementing slightly different update() callbacks,
    snapshots can be taken in situations where the user is only interested
    in taking a snapshot when a new maximum in hit versus when a value
    changes in any way at all.  Future patches will demonstrate both
    cases.
    
    Link: http://lkml.kernel.org/r/1bea07828d5fd6864a585f83b1eed47ce097eb45.1550100284.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ae7df090b93e..d80cee49e0eb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -194,6 +194,51 @@ struct trace_pid_list {
 	unsigned long			*pids;
 };
 
+typedef bool (*cond_update_fn_t)(struct trace_array *tr, void *cond_data);
+
+/**
+ * struct cond_snapshot - conditional snapshot data and callback
+ *
+ * The cond_snapshot structure encapsulates a callback function and
+ * data associated with the snapshot for a given tracing instance.
+ *
+ * When a snapshot is taken conditionally, by invoking
+ * tracing_snapshot_cond(tr, cond_data), the cond_data passed in is
+ * passed in turn to the cond_snapshot.update() function.  That data
+ * can be compared by the update() implementation with the cond_data
+ * contained wihin the struct cond_snapshot instance associated with
+ * the trace_array.  Because the tr->max_lock is held throughout the
+ * update() call, the update() function can directly retrieve the
+ * cond_snapshot and cond_data associated with the per-instance
+ * snapshot associated with the trace_array.
+ *
+ * The cond_snapshot.update() implementation can save data to be
+ * associated with the snapshot if it decides to, and returns 'true'
+ * in that case, or it returns 'false' if the conditional snapshot
+ * shouldn't be taken.
+ *
+ * The cond_snapshot instance is created and associated with the
+ * user-defined cond_data by tracing_cond_snapshot_enable().
+ * Likewise, the cond_snapshot instance is destroyed and is no longer
+ * associated with the trace instance by
+ * tracing_cond_snapshot_disable().
+ *
+ * The method below is required.
+ *
+ * @update: When a conditional snapshot is invoked, the update()
+ *	callback function is invoked with the tr->max_lock held.  The
+ *	update() implementation signals whether or not to actually
+ *	take the snapshot, by returning 'true' if so, 'false' if no
+ *	snapshot should be taken.  Because the max_lock is held for
+ *	the duration of update(), the implementation is safe to
+ *	directly retrieven and save any implementation data it needs
+ *	to in association with the snapshot.
+ */
+struct cond_snapshot {
+	void				*cond_data;
+	cond_update_fn_t		update;
+};
+
 /*
  * The trace array - an array of per-CPU trace arrays. This is the
  * highest level data structure that individual tracers deal with.
@@ -277,6 +322,9 @@ struct trace_array {
 #endif
 	int			time_stamp_abs_ref;
 	struct list_head	hist_vars;
+#ifdef CONFIG_TRACER_SNAPSHOT
+	struct cond_snapshot	*cond_snapshot;
+#endif
 };
 
 enum {
@@ -727,7 +775,8 @@ int trace_pid_write(struct trace_pid_list *filtered_pids,
 		    const char __user *ubuf, size_t cnt);
 
 #ifdef CONFIG_TRACER_MAX_TRACE
-void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
+void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu,
+		   void *cond_data);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
 #endif /* CONFIG_TRACER_MAX_TRACE */
@@ -1810,6 +1859,11 @@ static inline bool event_command_needs_rec(struct event_command *cmd_ops)
 extern int trace_event_enable_disable(struct trace_event_file *file,
 				      int enable, int soft_disable);
 extern int tracing_alloc_snapshot(void);
+extern void tracing_snapshot_cond(struct trace_array *tr, void *cond_data);
+extern int tracing_snapshot_cond_enable(struct trace_array *tr, void *cond_data, cond_update_fn_t update);
+
+extern int tracing_snapshot_cond_disable(struct trace_array *tr);
+extern void *tracing_cond_snapshot_data(struct trace_array *tr);
 
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];

commit f79b3f338564e7674dbe6375bcf685c2ba483efe
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Feb 11 15:00:48 2019 -0500

    ftrace: Allow enabling of filters via index of available_filter_functions
    
    Enabling of large number of functions by echoing in a large subset of the
    functions in available_filter_functions can take a very long time. The
    process requires testing all functions registered by the function tracer
    (which is in the 10s of thousands), and doing a kallsyms lookup to convert
    the ip address into a name, then comparing that name with the string passed
    in.
    
    When a function causes the function tracer to crash the system, a binary
    bisect of the available_filter_functions can be done to find the culprit.
    But this requires passing in half of the functions in
    available_filter_functions over and over again, which makes it basically a
    O(n^2) operation. With 40,000 functions, that ends up bing 1,600,000,000
    opertions! And enabling this can take over 20 minutes.
    
    As a quick speed up, if a number is passed into one of the filter files,
    instead of doing a search, it just enables the function at the corresponding
    line of the available_filter_functions file. That is:
    
     # echo 50 > set_ftrace_filter
     # cat set_ftrace_filter
     x86_pmu_commit_txn
    
     # head -50 available_filter_functions | tail -1
     x86_pmu_commit_txn
    
    This allows setting of half the available_filter_functions to take place in
    less than a second!
    
     # time seq 20000 > set_ftrace_filter
     real    0m0.042s
     user    0m0.005s
     sys     0m0.015s
    
     # wc -l set_ftrace_filter
     20000 set_ftrace_filter
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a34fa5e76abb..ae7df090b93e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1459,6 +1459,7 @@ enum regex_type {
 	MATCH_MIDDLE_ONLY,
 	MATCH_END_ONLY,
 	MATCH_GLOB,
+	MATCH_INDEX,
 };
 
 struct regex {

commit 9acd8de69d107537a68d010c9149fa9d9aba91f4
Author: Changbin Du <changbin.du@gmail.com>
Date:   Tue Jan 1 23:46:10 2019 +0800

    function_graph: Support displaying relative timestamp
    
    When function_graph is used for latency tracers, relative timestamp
    is more straightforward than absolute timestamp as function trace
    does. This change adds relative timestamp support to function_graph
    and applies to latency tracers (wakeup and irqsoff).
    
    Instead of:
    
     # tracer: irqsoff
     #
     # irqsoff latency trace v1.1.5 on 5.0.0-rc1-test
     # --------------------------------------------------------------------
     # latency: 521 us, #1125/1125, CPU#2 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:8)
     #    -----------------
     #    | task: swapper/2-0 (uid:0 nice:0 policy:0 rt_prio:0)
     #    -----------------
     #  => started at: __schedule
     #  => ended at:   _raw_spin_unlock_irq
     #
     #
     #                                       _-----=> irqs-off
     #                                      / _----=> need-resched
     #                                     | / _---=> hardirq/softirq
     #                                     || / _--=> preempt-depth
     #                                     ||| /
     #     TIME        CPU  TASK/PID       ||||  DURATION                  FUNCTION CALLS
     #      |          |     |    |        ||||   |   |                     |   |   |   |
       124.974306 |   2)  systemd-693   |  d..1  0.000 us    |  __schedule();
       124.974307 |   2)  systemd-693   |  d..1              |    rcu_note_context_switch() {
       124.974308 |   2)  systemd-693   |  d..1  0.487 us    |      rcu_preempt_deferred_qs();
       124.974309 |   2)  systemd-693   |  d..1  0.451 us    |      rcu_qs();
       124.974310 |   2)  systemd-693   |  d..1  2.301 us    |    }
    [..]
       124.974826 |   2)    <idle>-0    |  d..2              |  finish_task_switch() {
       124.974826 |   2)    <idle>-0    |  d..2              |    _raw_spin_unlock_irq() {
       124.974827 |   2)    <idle>-0    |  d..2  0.000 us    |  _raw_spin_unlock_irq();
       124.974828 |   2)    <idle>-0    |  d..2  0.000 us    |  tracer_hardirqs_on();
       <idle>-0       2d..2  552us : <stack trace>
      => __schedule
      => schedule_idle
      => do_idle
      => cpu_startup_entry
      => start_secondary
      => secondary_startup_64
    
    Show:
    
     # tracer: irqsoff
     #
     # irqsoff latency trace v1.1.5 on 5.0.0-rc1-test+
     # --------------------------------------------------------------------
     # latency: 511 us, #1053/1053, CPU#7 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:8)
     #    -----------------
     #    | task: swapper/7-0 (uid:0 nice:0 policy:0 rt_prio:0)
     #    -----------------
     #  => started at: __schedule
     #  => ended at:   _raw_spin_unlock_irq
     #
     #
     #                                       _-----=> irqs-off
     #                                      / _----=> need-resched
     #                                     | / _---=> hardirq/softirq
     #                                     || / _--=> preempt-depth
     #                                     ||| /
     #   REL TIME      CPU  TASK/PID       ||||  DURATION                  FUNCTION CALLS
     #      |          |     |    |        ||||   |   |                     |   |   |   |
             0 us |   7)   sshd-1704    |  d..1  0.000 us    |  __schedule();
             1 us |   7)   sshd-1704    |  d..1              |    rcu_note_context_switch() {
             1 us |   7)   sshd-1704    |  d..1  0.611 us    |      rcu_preempt_deferred_qs();
             2 us |   7)   sshd-1704    |  d..1  0.484 us    |      rcu_qs();
             3 us |   7)   sshd-1704    |  d..1  2.599 us    |    }
    [..]
           509 us |   7)    <idle>-0    |  d..2              |  finish_task_switch() {
           510 us |   7)    <idle>-0    |  d..2              |    _raw_spin_unlock_irq() {
           510 us |   7)    <idle>-0    |  d..2  0.000 us    |  _raw_spin_unlock_irq();
           512 us |   7)    <idle>-0    |  d..2  0.000 us    |  tracer_hardirqs_on();
       <idle>-0       7d..2  543us : <stack trace>
      => __schedule
      => schedule_idle
      => do_idle
      => cpu_startup_entry
      => start_secondary
      => secondary_startup_64
    
    Link: http://lkml.kernel.org/r/20190101154614.8887-2-changbin.du@gmail.com
    
    Signed-off-by: Changbin Du <changbin.du@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 08900828d282..a34fa5e76abb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -855,10 +855,11 @@ static __always_inline bool ftrace_hash_empty(struct ftrace_hash *hash)
 #define TRACE_GRAPH_PRINT_PROC          0x8
 #define TRACE_GRAPH_PRINT_DURATION      0x10
 #define TRACE_GRAPH_PRINT_ABS_TIME      0x20
-#define TRACE_GRAPH_PRINT_IRQS          0x40
-#define TRACE_GRAPH_PRINT_TAIL          0x80
-#define TRACE_GRAPH_SLEEP_TIME		0x100
-#define TRACE_GRAPH_GRAPH_TIME		0x200
+#define TRACE_GRAPH_PRINT_REL_TIME      0x40
+#define TRACE_GRAPH_PRINT_IRQS          0x80
+#define TRACE_GRAPH_PRINT_TAIL          0x100
+#define TRACE_GRAPH_SLEEP_TIME          0x200
+#define TRACE_GRAPH_GRAPH_TIME          0x400
 #define TRACE_GRAPH_PRINT_FILL_SHIFT	28
 #define TRACE_GRAPH_PRINT_FILL_MASK	(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)
 

commit 03329f9939781041424edd29487b9603a704fcd9
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Nov 29 21:38:42 2018 -0500

    tracing: Add tracefs file buffer_percentage
    
    Add a "buffer_percentage" file, that allows users to specify how much of the
    buffer (percentage of pages) need to be filled before waking up a task
    blocked on a per cpu trace_pipe_raw file.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ab16eca76e59..08900828d282 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -247,6 +247,7 @@ struct trace_array {
 	int			clock_id;
 	int			nr_topts;
 	bool			clear_trace;
+	int			buffer_percent;
 	struct tracer		*current_trace;
 	unsigned int		trace_flags;
 	unsigned char		trace_flags_index[TRACE_FLAGS_MAX_SIZE];

commit c8dd0f45874547e6e77bab03d71feb16c4cb98a8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Nov 23 13:06:07 2018 -0500

    function_graph: Do not expose the graph_time option when profiler is not configured
    
    When the function profiler is not configured, the "graph_time" option is
    meaningless, as the function profiler is the only thing that makes use of
    it. Do not expose it if the profiler is not configured.
    
    Link: http://lkml.kernel.org/r/20181123061133.GA195223@google.com
    
    Reported-by: Joel Fernandes <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f67060a75f38..ab16eca76e59 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -862,7 +862,12 @@ static __always_inline bool ftrace_hash_empty(struct ftrace_hash *hash)
 #define TRACE_GRAPH_PRINT_FILL_MASK	(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)
 
 extern void ftrace_graph_sleep_time_control(bool enable);
+
+#ifdef CONFIG_FUNCTION_PROFILER
 extern void ftrace_graph_graph_time_control(bool enable);
+#else
+static inline void ftrace_graph_graph_time_control(bool enable) { }
+#endif
 
 extern enum print_line_t
 print_graph_function_flags(struct trace_iterator *iter, u32 flags);

commit 9cd2992f2d6c8df54c5b937d5d1f8a23b684cc1d
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Nov 14 13:14:58 2018 -0500

    fgraph: Have set_graph_notrace only affect function_graph tracer
    
    In order to make the function graph infrastructure more generic, there can
    not be code specific for the function_graph tracer in the generic code. This
    includes the set_graph_notrace logic, that stops all graph calls when a
    function in the set_graph_notrace is hit.
    
    By using the trace_recursion mask, we can use a bit in the current
    task_struct to implement the notrace code, and move the logic out of
    fgraph.c and into trace_functions_graph.c and keeps it affecting only the
    tracer and not all call graph callbacks.
    
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 447bd96ee658..f67060a75f38 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -534,6 +534,13 @@ enum {
 
 	TRACE_GRAPH_DEPTH_START_BIT,
 	TRACE_GRAPH_DEPTH_END_BIT,
+
+	/*
+	 * To implement set_graph_notrace, if this bit is set, we ignore
+	 * function graph tracing of called functions, until the return
+	 * function is called to clear it.
+	 */
+	TRACE_GRAPH_NOTRACE_BIT,
 };
 
 #define trace_recursion_set(bit)	do { (current)->trace_recursion |= (1<<(bit)); } while (0)

commit 5cf99a0f3161bc3ae2391269d134d6bf7e26f00e
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Nov 29 08:50:27 2018 -0500

    tracing/fgraph: Fix set_graph_function from showing interrupts
    
    The tracefs file set_graph_function is used to only function graph functions
    that are listed in that file (or all functions if the file is empty). The
    way this is implemented is that the function graph tracer looks at every
    function, and if the current depth is zero and the function matches
    something in the file then it will trace that function. When other functions
    are called, the depth will be greater than zero (because the original
    function will be at depth zero), and all functions will be traced where the
    depth is greater than zero.
    
    The issue is that when a function is first entered, and the handler that
    checks this logic is called, the depth is set to zero. If an interrupt comes
    in and a function in the interrupt handler is traced, its depth will be
    greater than zero and it will automatically be traced, even if the original
    function was not. But because the logic only looks at depth it may trace
    interrupts when it should not be.
    
    The recent design change of the function graph tracer to fix other bugs
    caused the depth to be zero while the function graph callback handler is
    being called for a longer time, widening the race of this happening. This
    bug was actually there for a longer time, but because the race window was so
    small it seldom happened. The Fixes tag below is for the commit that widen
    the race window, because that commit belongs to a series that will also help
    fix the original bug.
    
    Cc: stable@kernel.org
    Fixes: 39eb456dacb5 ("function_graph: Use new curr_ret_depth to manage depth instead of curr_ret_stack")
    Reported-by: Joe Lawrence <joe.lawrence@redhat.com>
    Tested-by: Joe Lawrence <joe.lawrence@redhat.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3b8c0e24ab30..447bd96ee658 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -512,12 +512,44 @@ enum {
  * can only be modified by current, we can reuse trace_recursion.
  */
 	TRACE_IRQ_BIT,
+
+	/* Set if the function is in the set_graph_function file */
+	TRACE_GRAPH_BIT,
+
+	/*
+	 * In the very unlikely case that an interrupt came in
+	 * at a start of graph tracing, and we want to trace
+	 * the function in that interrupt, the depth can be greater
+	 * than zero, because of the preempted start of a previous
+	 * trace. In an even more unlikely case, depth could be 2
+	 * if a softirq interrupted the start of graph tracing,
+	 * followed by an interrupt preempting a start of graph
+	 * tracing in the softirq, and depth can even be 3
+	 * if an NMI came in at the start of an interrupt function
+	 * that preempted a softirq start of a function that
+	 * preempted normal context!!!! Luckily, it can't be
+	 * greater than 3, so the next two bits are a mask
+	 * of what the depth is when we set TRACE_GRAPH_BIT
+	 */
+
+	TRACE_GRAPH_DEPTH_START_BIT,
+	TRACE_GRAPH_DEPTH_END_BIT,
 };
 
 #define trace_recursion_set(bit)	do { (current)->trace_recursion |= (1<<(bit)); } while (0)
 #define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(1<<(bit)); } while (0)
 #define trace_recursion_test(bit)	((current)->trace_recursion & (1<<(bit)))
 
+#define trace_recursion_depth() \
+	(((current)->trace_recursion >> TRACE_GRAPH_DEPTH_START_BIT) & 3)
+#define trace_recursion_set_depth(depth) \
+	do {								\
+		current->trace_recursion &=				\
+			~(3 << TRACE_GRAPH_DEPTH_START_BIT);		\
+		current->trace_recursion |=				\
+			((depth) & 3) << TRACE_GRAPH_DEPTH_START_BIT;	\
+	} while (0)
+
 #define TRACE_CONTEXT_BITS	4
 
 #define TRACE_FTRACE_START	TRACE_FTRACE_BIT
@@ -843,8 +875,9 @@ extern void __trace_graph_return(struct trace_array *tr,
 extern struct ftrace_hash *ftrace_graph_hash;
 extern struct ftrace_hash *ftrace_graph_notrace_hash;
 
-static inline int ftrace_graph_addr(unsigned long addr)
+static inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)
 {
+	unsigned long addr = trace->func;
 	int ret = 0;
 
 	preempt_disable_notrace();
@@ -855,6 +888,14 @@ static inline int ftrace_graph_addr(unsigned long addr)
 	}
 
 	if (ftrace_lookup_ip(ftrace_graph_hash, addr)) {
+
+		/*
+		 * This needs to be cleared on the return functions
+		 * when the depth is zero.
+		 */
+		trace_recursion_set(TRACE_GRAPH_BIT);
+		trace_recursion_set_depth(trace->depth);
+
 		/*
 		 * If no irqs are to be traced, but a set_graph_function
 		 * is set, and called by an interrupt handler, we still
@@ -872,6 +913,13 @@ static inline int ftrace_graph_addr(unsigned long addr)
 	return ret;
 }
 
+static inline void ftrace_graph_addr_finish(struct ftrace_graph_ret *trace)
+{
+	if (trace_recursion_test(TRACE_GRAPH_BIT) &&
+	    trace->depth == trace_recursion_depth())
+		trace_recursion_clear(TRACE_GRAPH_BIT);
+}
+
 static inline int ftrace_graph_notrace_addr(unsigned long addr)
 {
 	int ret = 0;
@@ -885,7 +933,7 @@ static inline int ftrace_graph_notrace_addr(unsigned long addr)
 	return ret;
 }
 #else
-static inline int ftrace_graph_addr(unsigned long addr)
+static inline int ftrace_graph_addr(struct ftrace_graph_ent *trace)
 {
 	return 1;
 }
@@ -894,6 +942,8 @@ static inline int ftrace_graph_notrace_addr(unsigned long addr)
 {
 	return 0;
 }
+static inline void ftrace_graph_addr_finish(struct ftrace_graph_ret *trace)
+{ }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 extern unsigned int fgraph_max_depth;
@@ -901,7 +951,8 @@ extern unsigned int fgraph_max_depth;
 static inline bool ftrace_graph_ignore_func(struct ftrace_graph_ent *trace)
 {
 	/* trace it when it is-nested-in or is a function enabled. */
-	return !(trace->depth || ftrace_graph_addr(trace->func)) ||
+	return !(trace_recursion_test(TRACE_GRAPH_BIT) ||
+		 ftrace_graph_addr(trace)) ||
 		(trace->depth < 0) ||
 		(fgraph_max_depth && trace->depth >= fgraph_max_depth);
 }

commit bb730b5833b5bddf5cb226865e5f4496770d00b0
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 16 11:26:55 2018 -0400

    tracing: Fix SPDX format headers to use C++ style comments
    
    The Linux kernel adopted the SPDX License format headers to ease license
    compliance management, and uses the C++ '//' style comments for the SPDX
    header tags. Some files in the tracing directory used the C style /* */
    comments for them. To be consistent across all files, replace the /* */
    C style SPDX tags with the C++ // SPDX tags.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a62b678731e3..3b8c0e24ab30 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1,4 +1,4 @@
-/* SPDX-License-Identifier: GPL-2.0 */
+// SPDX-License-Identifier: GPL-2.0
 
 #ifndef _LINUX_KERNEL_TRACE_H
 #define _LINUX_KERNEL_TRACE_H

commit 3f1756dc210e5abb37121da3e7c10d65920f6ec0
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Aug 8 21:28:05 2018 -0400

    tracing: More reverting of "tracing: Centralize preemptirq tracepoints and unify their usage"
    
    Joel Fernandes created a nice patch that cleaned up the duplicate hooks used
    by lockdep and irqsoff latency tracer. It made both use tracepoints. But the
    latency tracer is triggering warnings when using tracepoints to call into
    the latency tracer's routines. Mainly, they can be called from NMI context.
    If that happens, then the SRCU may not work properly because on some
    architectures, SRCU is not safe to be called in both NMI and non-NMI
    context.
    
    This is a partial revert of the clean up patch c3bc8fd637a9 ("tracing:
    Centralize preemptirq tracepoints and unify their usage") that adds back the
    direct calls into the latency tracer. It also only calls the trace events
    when not in NMI.
    
    Link: http://lkml.kernel.org/r/20180809210654.622445925@goodmis.org
    Reviewed-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Fixes: c3bc8fd637a9 ("tracing: Centralize preemptirq tracepoints and unify their usage")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d88cd9bb72f4..a62b678731e3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1827,6 +1827,21 @@ static inline int tracing_alloc_snapshot_instance(struct trace_array *tr)
 }
 #endif
 
+#ifdef CONFIG_PREEMPT_TRACER
+void tracer_preempt_on(unsigned long a0, unsigned long a1);
+void tracer_preempt_off(unsigned long a0, unsigned long a1);
+#else
+static inline void tracer_preempt_on(unsigned long a0, unsigned long a1) { }
+static inline void tracer_preempt_off(unsigned long a0, unsigned long a1) { }
+#endif
+#ifdef CONFIG_IRQSOFF_TRACER
+void tracer_hardirqs_on(unsigned long a0, unsigned long a1);
+void tracer_hardirqs_off(unsigned long a0, unsigned long a1);
+#else
+static inline void tracer_hardirqs_on(unsigned long a0, unsigned long a1) { }
+static inline void tracer_hardirqs_off(unsigned long a0, unsigned long a1) { }
+#endif
+
 extern struct trace_iterator *tracepoint_print_iter;
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit ec57350883cd7fccd867b0d2260bac3a9bf6442d
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Aug 1 16:08:57 2018 -0400

    tracing: Make tracer_tracing_is_on() return bool
    
    There's code that expects tracer_tracing_is_on() to be either true or false,
    not some random number. Currently, it should only return one or zero, but
    just in case, change its return value to bool, to enforce it.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e1c8a1d6f240..d88cd9bb72f4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -594,7 +594,7 @@ void tracing_reset_current(int cpu);
 void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 bool tracing_is_disabled(void);
-int tracer_tracing_is_on(struct trace_array *tr);
+bool tracer_tracing_is_on(struct trace_array *tr);
 void tracer_tracing_on(struct trace_array *tr);
 void tracer_tracing_off(struct trace_array *tr);
 struct dentry *trace_create_file(const char *name,

commit 7b144b6c795a380beae6f7b40dcfb21014c4afb8
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Thu Jul 26 21:44:04 2018 +0900

    tracing: Remove orphaned function using_ftrace_ops_list_func().
    
    Remove using_ftrace_ops_list_func() since it is no longer used.
    
    Using ftrace_ops_list_func() has been introduced by commit 7eea4fce0246
    ("tracing/stack_trace: Skip 4 instead of 3 when using ftrace_ops_list_func")
    as a helper function, but its caller has been removed by commit 72ac426a5bb0
    ("tracing: Clean up stack tracing and fix fentry updates").  So it is not
    called anymore.
    
    Link: http://lkml.kernel.org/r/153260904427.12474.9952096317439329851.stgit@devbox
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3f9d9acc69e6..e1c8a1d6f240 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -937,7 +937,6 @@ void ftrace_destroy_function_files(struct trace_array *tr);
 void ftrace_init_global_array_ops(struct trace_array *tr);
 void ftrace_init_array_ops(struct trace_array *tr, ftrace_func_t func);
 void ftrace_reset_array_ops(struct trace_array *tr);
-int using_ftrace_ops_list_func(void);
 void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer);
 void ftrace_init_tracefs_toplevel(struct trace_array *tr,
 				  struct dentry *d_tracer);

commit f6b7425cfb92cbf0d04de816bec1bd8cb9a79d0f
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Jul 24 18:55:58 2018 -0400

    tracing: Make unregister_trigger() static
    
    Nothing uses unregister_trigger() outside of trace_events_trigger.c file,
    thus it should be static. Not sure why this was ever converted, because
    its counter part, register_trigger(), was always static.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f8f86231ad90..3f9d9acc69e6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1533,9 +1533,6 @@ extern int event_trigger_init(struct event_trigger_ops *ops,
 extern int trace_event_trigger_enable_disable(struct trace_event_file *file,
 					      int trigger_enable);
 extern void update_cond_flag(struct trace_event_file *file);
-extern void unregister_trigger(char *glob, struct event_trigger_ops *ops,
-			       struct event_trigger_data *test,
-			       struct trace_event_file *file);
 extern int set_trigger_filter(char *filter_str,
 			      struct event_trigger_data *trigger_data,
 			      struct trace_event_file *file);

commit f26808ba7227a921e0e8549c7d3c52332b920085
Author: yuan linyu <Linyu.Yuan@alcatel-sbell.com.cn>
Date:   Sun Apr 8 19:36:31 2018 +0800

    tracing: Optimize trace_buffer_iter() logic
    
    Simplify and optimize the logic in trace_buffer_iter() to use a conditional
    operation instead of an if conditional.
    
    Link: http://lkml.kernel.org/r/20180408113631.3947-1-cugyly@163.com
    
    Signed-off-by: yuan linyu <Linyu.Yuan@alcatel-sbell.com.cn>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 630c5a24b2b2..f8f86231ad90 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -583,9 +583,7 @@ static __always_inline void trace_clear_recursion(int bit)
 static inline struct ring_buffer_iter *
 trace_buffer_iter(struct trace_iterator *iter, int cpu)
 {
-	if (iter->buffer_iter && iter->buffer_iter[cpu])
-		return iter->buffer_iter[cpu];
-	return NULL;
+	return iter->buffer_iter ? iter->buffer_iter[cpu] : NULL;
 }
 
 int tracer_init(struct tracer *t, struct trace_array *tr);

commit 3dd8095368475a9538895ce757b63dd311e58fe8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed May 9 14:17:48 2018 -0400

    tracing: Add trigger file for trace_markers tracefs/ftrace/print
    
    Allow writing to the trace_markers file initiate triggers defined in
    tracefs/ftrace/print/trigger file. This will allow of user space to trigger
    the same type of triggers (including histograms) that the trace events use.
    
    Had to create a ftrace_event_register() function that will become the
    trace_marker print event's reg() function. This is required because of how
    triggers are enabled:
    
      event_trigger_write() {
        event_trigger_regex_write() {
          trigger_process_regex() {
            for p in trigger_commands {
              p->func(); /* trigger_snapshot_cmd->func */
                event_trigger_callback() {
                  cmd_ops->reg() /* register_trigger() */ {
                    trace_event_trigger_enable_disable() {
                      trace_event_enable_disable() {
                        call->class->reg();
    
    Without the reg() function, the trigger code will call a NULL pointer and
    crash the system.
    
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: Karim Yaghmour <karim.yaghmour@opersys.com>
    Cc: Brendan Gregg <bgregg@netflix.com>
    Suggested-by: Joel Fernandes <joelaf@google.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d0319cbacf11..630c5a24b2b2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -259,6 +259,7 @@ struct trace_array {
 	struct trace_options	*topts;
 	struct list_head	systems;
 	struct list_head	events;
+	struct trace_event_file *trace_marker_file;
 	cpumask_var_t		tracing_cpumask; /* only trace on set CPUs */
 	int			ref;
 #ifdef CONFIG_FUNCTION_TRACER

commit 58b9254757e026102a68cb44a0a15ba63787d0c0
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue May 8 15:09:27 2018 -0400

    tracing: Have event_trace_init() called by trace_init_tracefs()
    
    Instead of having both trace_init_tracefs() and event_trace_init() be called
    by fs_initcall() routines, have event_trace_init() called directly by
    trace_init_tracefs(). This will guarantee order of how the events are
    created with respect to the rest of the ftrace infrastructure. This is
    needed to be able to assoctiate event files with ftrace internal events,
    such as the trace_marker.
    
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 07c43960a704..d0319cbacf11 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1451,6 +1451,7 @@ trace_find_event_field(struct trace_event_call *call, char *name);
 extern void trace_event_enable_cmd_record(bool enable);
 extern void trace_event_enable_tgid_record(bool enable);
 
+extern int event_trace_init(void);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 extern int event_trace_del_tracer(struct trace_array *tr);
 

commit 3c96529c0739959e2aa235d44e47f5c68c1e40de
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue May 8 15:06:38 2018 -0400

    tracing: Add __find_event_file() to find event files without restrictions
    
    By adding the function __find_event_file() that can search for files without
    restrictions, such as if the event associated with the file has a reg
    function, or if it has the "ignore" flag set, the files that are associated
    to ftrace internal events (like trace_marker and function events) can be
    found and used.
    
    find_event_file() still returns a "filtered" file, as most callers need a
    valid trace event file. One created by the trace_events.h macros and not one
    created for parsing ftrace specific events.
    
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6bfc2467479c..07c43960a704 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1454,6 +1454,9 @@ extern void trace_event_enable_tgid_record(bool enable);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 extern int event_trace_del_tracer(struct trace_array *tr);
 
+extern struct trace_event_file *__find_event_file(struct trace_array *tr,
+						  const char *system,
+						  const char *event);
 extern struct trace_event_file *find_event_file(struct trace_array *tr,
 						const char *system,
 						const char *event);

commit c94e45bc38ae484ee989e6e3b2496a52776da9e4
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 7 16:02:14 2018 -0400

    tracing: Do not reference event data in post call triggers
    
    Trace event triggers can be called before or after the event has been
    committed. If it has been called after the commit, there's a possibility
    that the event no longer exists. Currently, the two post callers is the
    trigger to disable tracing (traceoff) and the one that will record a stack
    dump (stacktrace). Neither of them reference the trace event entry record,
    as that would lead to a race condition that could pass in corrupted data.
    
    To prevent any other users of the post data triggers from using the trace
    event record, pass in NULL to the post call trigger functions for the event
    record, as they should never need to use them in the first place.
    
    This does not fix any bug, but prevents bugs from happening by new post call
    trigger users.
    
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 507954b4e058..6bfc2467479c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1334,7 +1334,7 @@ event_trigger_unlock_commit(struct trace_event_file *file,
 		trace_buffer_unlock_commit(file->tr, buffer, event, irq_flags, pc);
 
 	if (tt)
-		event_triggers_post_call(file, tt, entry, event);
+		event_triggers_post_call(file, tt);
 }
 
 /**
@@ -1367,7 +1367,7 @@ event_trigger_unlock_commit_regs(struct trace_event_file *file,
 						irq_flags, pc, regs);
 
 	if (tt)
-		event_triggers_post_call(file, tt, entry, event);
+		event_triggers_post_call(file, tt);
 }
 
 #define FILTER_PRED_INVALID	((unsigned short)-1)

commit 2824f5033248600673e3e126a4d135363cbfd9ac
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 28 10:56:36 2018 -0400

    tracing: Make the snapshot trigger work with instances
    
    The snapshot trigger currently only affects the main ring buffer, even when
    it is used by the instances. This can be confusing as the snapshot trigger
    is listed in the instance.
    
     > # cd /sys/kernel/tracing
     > # mkdir instances/foo
     > # echo snapshot > instances/foo/events/syscalls/sys_enter_fchownat/trigger
     > # echo top buffer > trace_marker
     > # echo foo buffer > instances/foo/trace_marker
     > # touch /tmp/bar
     > # chown rostedt /tmp/bar
     > # cat instances/foo/snapshot
     # tracer: nop
     #
     #
     # * Snapshot is freed *
     #
     # Snapshot commands:
     # echo 0 > snapshot : Clears and frees snapshot buffer
     # echo 1 > snapshot : Allocates snapshot buffer, if not already allocated.
     #                      Takes a snapshot of the main buffer.
     # echo 2 > snapshot : Clears snapshot buffer (but does not allocate or free)
     #                      (Doesn't have to be '2' works with any number that
     #                       is not a '0' or '1')
    
     > # cat snapshot
     # tracer: nop
     #
     #                              _-----=> irqs-off
     #                             / _----=> need-resched
     #                            | / _---=> hardirq/softirq
     #                            || / _--=> preempt-depth
     #                            ||| /     delay
     #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
     #              | |       |   ||||       |         |
                 bash-1189  [000] ....   111.488323: tracing_mark_write: top buffer
    
    Not only did the snapshot occur in the top level buffer, but the instance
    snapshot buffer should have been allocated, and it is still free.
    
    Cc: stable@vger.kernel.org
    Fixes: 85f2b08268c01 ("tracing: Add basic event trigger framework")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6fb46a06c9dc..507954b4e058 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1817,6 +1817,17 @@ static inline void __init trace_event_init(void) { }
 static inline void trace_event_eval_update(struct trace_eval_map **map, int len) { }
 #endif
 
+#ifdef CONFIG_TRACER_SNAPSHOT
+void tracing_snapshot_instance(struct trace_array *tr);
+int tracing_alloc_snapshot_instance(struct trace_array *tr);
+#else
+static inline void tracing_snapshot_instance(struct trace_array *tr) { }
+static inline int tracing_alloc_snapshot_instance(struct trace_array *tr)
+{
+	return 0;
+}
+#endif
+
 extern struct trace_iterator *tracepoint_print_iter;
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 80765597bc587feae8dbc8ce97a0f32e12a6e625
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 9 13:19:28 2018 -0500

    tracing: Rewrite filter logic to be simpler and faster
    
    Al Viro reviewed the filter logic of ftrace trace events and found it to be
    very troubling. It creates a binary tree based on the logic operators and
    walks it during tracing. He sent myself and Tom Zanussi a long explanation
    (and formal proof) of how to do the string parsing better and end up with a
    program array that can be simply iterated to come up with the correct
    results.
    
    I took his ideas and his pseudo code and rewrote the filter logic based on
    them. In doing so, I was able to remove a lot of code, and have a much more
    condensed filter logic in the process. I wrote a very long comment
    describing the methadology that Al proposed in my own words. For more info
    on how this works, read the comment above predicate_parse().
    
    Suggested-by: Al Viro <viro@ZenIV.linux.org.uk>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9de3e2a2f042..6fb46a06c9dc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1216,12 +1216,11 @@ struct ftrace_event_field {
 	int			is_signed;
 };
 
+struct prog_entry;
+
 struct event_filter {
-	int			n_preds;	/* Number assigned */
-	int			a_preds;	/* allocated */
-	struct filter_pred __rcu	*preds;
-	struct filter_pred __rcu	*root;
-	char				*filter_string;
+	struct prog_entry __rcu	*prog;
+	char			*filter_string;
 };
 
 struct event_subsystem {
@@ -1413,12 +1412,8 @@ struct filter_pred {
 	unsigned short		*ops;
 	struct ftrace_event_field *field;
 	int 			offset;
-	int 			not;
+	int			not;
 	int 			op;
-	unsigned short		index;
-	unsigned short		parent;
-	unsigned short		left;
-	unsigned short		right;
 };
 
 static inline bool is_string_field(struct ftrace_event_field *field)

commit d71bd34d78bb78b9e6f8a0be3952d5fa470a260a
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Mon Jan 15 20:52:07 2018 -0600

    tracing: Make tracing_set_clock() non-static
    
    Allow tracing code outside of trace.c to access tracing_set_clock().
    
    Some applications may require a particular clock in order to function
    properly, such as latency calculations.
    
    Also, add an accessor returning the current clock string.
    
    Link: http://lkml.kernel.org/r/6d1c53e9ee2163f54e1849f5376573f54f0e6009.1516069914.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 99b7ee7ed127..9de3e2a2f042 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -289,6 +289,7 @@ extern int trace_array_get(struct trace_array *tr);
 extern void trace_array_put(struct trace_array *tr);
 
 extern int tracing_set_time_stamp_abs(struct trace_array *tr, bool abs);
+extern int tracing_set_clock(struct trace_array *tr, const char *clockstr);
 
 extern bool trace_clock_in_ns(struct trace_array *tr);
 

commit 067fe038e70f6e64960d26a79c4df5f1413d0f13
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Mon Jan 15 20:51:56 2018 -0600

    tracing: Add variable reference handling to hist triggers
    
    Add the necessary infrastructure to allow the variables defined on one
    event to be referenced in another.  This allows variables set by a
    previous event to be referenced and used in expressions combining the
    variable values saved by that previous event and the event fields of
    the current event.  For example, here's how a latency can be
    calculated and saved into yet another variable named 'wakeup_lat':
    
        # echo 'hist:keys=pid,prio:ts0=common_timestamp ...
        # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp-$ts0 ...
    
    In the first event, the event's timetamp is saved into the variable
    ts0.  In the next line, ts0 is subtracted from the second event's
    timestamp to produce the latency.
    
    Further users of variable references will be described in subsequent
    patches, such as for instance how the 'wakeup_lat' variable above can
    be displayed in a latency histogram.
    
    Link: http://lkml.kernel.org/r/b1d3e6975374e34d501ff417c20189c3f9b2c7b8.1516069914.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 89771b4f16df..99b7ee7ed127 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -274,6 +274,7 @@ struct trace_array {
 	int			function_enabled;
 #endif
 	int			time_stamp_abs_ref;
+	struct list_head	hist_vars;
 };
 
 enum {
@@ -1548,6 +1549,8 @@ extern void pause_named_trigger(struct event_trigger_data *data);
 extern void unpause_named_trigger(struct event_trigger_data *data);
 extern void set_named_trigger_data(struct event_trigger_data *data,
 				   struct event_trigger_data *named_data);
+extern struct event_trigger_data *
+get_named_trigger_data(struct event_trigger_data *data);
 extern int register_event_command(struct event_command *cmd);
 extern int unregister_event_command(struct event_command *cmd);
 extern int register_trigger_hist_enable_disable_cmds(void);

commit 860f9f6b02e9e846c4cfb3505efed331a910d0b7
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Mon Jan 15 20:51:48 2018 -0600

    tracing: Add usecs modifier for hist trigger timestamps
    
    Appending .usecs onto a common_timestamp field will cause the
    timestamp value to be in microseconds instead of the default
    nanoseconds.  A typical latency histogram using usecs would look like
    this:
    
       # echo 'hist:keys=pid,prio:ts0=common_timestamp.usecs ...
       # echo 'hist:keys=next_pid:wakeup_lat=common_timestamp.usecs-$ts0 ...
    
    This also adds an external trace_clock_in_ns() to trace.c for the
    timestamp conversion.
    
    Link: http://lkml.kernel.org/r/4e813705a170b3e13e97dc3135047362fb1a39f3.1516069914.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 99060f7eebbd..89771b4f16df 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -289,6 +289,8 @@ extern void trace_array_put(struct trace_array *tr);
 
 extern int tracing_set_time_stamp_abs(struct trace_array *tr, bool abs);
 
+extern bool trace_clock_in_ns(struct trace_array *tr);
+
 /*
  * The global tracer (top) should be the first trace array added,
  * but we check the flag anyway.

commit 1ac4f51c0eb518e04ff3455f0c7d17ad9187eb27
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Mon Jan 15 20:51:42 2018 -0600

    tracing: Give event triggers access to ring_buffer_event
    
    The ring_buffer event can provide a timestamp that may be useful to
    various triggers - pass it into the handlers for that purpose.
    
    Link: http://lkml.kernel.org/r/6de592683b59fa70ffa5d43d0109896623fc1367.1516069914.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 477341710ebf..99060f7eebbd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1294,7 +1294,7 @@ __event_trigger_test_discard(struct trace_event_file *file,
 	unsigned long eflags = file->flags;
 
 	if (eflags & EVENT_FILE_FL_TRIGGER_COND)
-		*tt = event_triggers_call(file, entry);
+		*tt = event_triggers_call(file, entry, event);
 
 	if (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags) ||
 	    (unlikely(file->flags & EVENT_FILE_FL_FILTERED) &&
@@ -1331,7 +1331,7 @@ event_trigger_unlock_commit(struct trace_event_file *file,
 		trace_buffer_unlock_commit(file->tr, buffer, event, irq_flags, pc);
 
 	if (tt)
-		event_triggers_post_call(file, tt, entry);
+		event_triggers_post_call(file, tt, entry, event);
 }
 
 /**
@@ -1364,7 +1364,7 @@ event_trigger_unlock_commit_regs(struct trace_event_file *file,
 						irq_flags, pc, regs);
 
 	if (tt)
-		event_triggers_post_call(file, tt, entry);
+		event_triggers_post_call(file, tt, entry, event);
 }
 
 #define FILTER_PRED_INVALID	((unsigned short)-1)
@@ -1589,7 +1589,8 @@ extern int register_trigger_hist_enable_disable_cmds(void);
  */
 struct event_trigger_ops {
 	void			(*func)(struct event_trigger_data *data,
-					void *rec);
+					void *rec,
+					struct ring_buffer_event *rbe);
 	int			(*init)(struct event_trigger_ops *ops,
 					struct event_trigger_data *data);
 	void			(*free)(struct event_trigger_ops *ops,

commit 00b4145298aeb05a2d110117ed18148cb21ebd14
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Mon Jan 15 20:51:39 2018 -0600

    ring-buffer: Add interface for setting absolute time stamps
    
    Define a new function, tracing_set_time_stamp_abs(), which can be used
    to enable or disable the use of absolute timestamps rather than time
    deltas for a trace array.
    
    Only the interface is added here; a subsequent patch will add the
    underlying implementation.
    
    Link: http://lkml.kernel.org/r/ce96119de44c7fe0ee44786d15254e9b493040d3.1516069914.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Baohong Liu <baohong.liu@intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2a6d0325a761..477341710ebf 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -273,6 +273,7 @@ struct trace_array {
 	/* function tracing enabled */
 	int			function_enabled;
 #endif
+	int			time_stamp_abs_ref;
 };
 
 enum {
@@ -286,6 +287,8 @@ extern struct mutex trace_types_lock;
 extern int trace_array_get(struct trace_array *tr);
 extern void trace_array_put(struct trace_array *tr);
 
+extern int tracing_set_time_stamp_abs(struct trace_array *tr, bool abs);
+
 /*
  * The global tracer (top) should be the first trace array added,
  * but we check the flag anyway.

commit 2dcd9c71c1ffa9a036e09047f60e08383bb0abb6
Merge: b1c2a344cc19 a96a5037ed0f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 17 14:58:01 2017 -0800

    Merge tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from
    
     - allow module init functions to be traced
    
     - clean up some unused or not used by config events (saves space)
    
     - clean up of trace histogram code
    
     - add support for preempt and interrupt enabled/disable events
    
     - other various clean ups
    
    * tag 'trace-v4.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (30 commits)
      tracing, thermal: Hide cpu cooling trace events when not in use
      tracing, thermal: Hide devfreq trace events when not in use
      ftrace: Kill FTRACE_OPS_FL_PER_CPU
      perf/ftrace: Small cleanup
      perf/ftrace: Fix function trace events
      perf/ftrace: Revert ("perf/ftrace: Fix double traces of perf on ftrace:function")
      tracing, dma-buf: Remove unused trace event dma_fence_annotate_wait_on
      tracing, memcg, vmscan: Hide trace events when not in use
      tracing/xen: Hide events that are not used when X86_PAE is not defined
      tracing: mark trace_test_buffer as __maybe_unused
      printk: Remove superfluous memory barriers from printk_safe
      ftrace: Clear hashes of stale ips of init memory
      tracing: Add support for preempt and irq enable/disable events
      tracing: Prepare to add preempt and irq trace events
      ftrace/kallsyms: Have /proc/kallsyms show saved mod init functions
      ftrace: Add freeing algorithm to free ftrace_mod_maps
      ftrace: Save module init functions kallsyms symbols for tracing
      ftrace: Allow module init functions to be traced
      ftrace: Add a ftrace_free_mem() function for modules to use
      tracing: Reimplement log2
      ...

commit 8c5db92a705d9e2c986adec475980d1120fa07b4
Merge: ca5d376e1707 e4880bc5dfb1
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Nov 7 10:32:44 2017 +0100

    Merge branch 'linus' into locking/core, to resolve conflicts
    
    Conflicts:
            include/linux/compiler-clang.h
            include/linux/compiler-gcc.h
            include/linux/compiler-intel.h
            include/uapi/linux/stddef.h
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit b24413180f5600bcb3bb70fbed5cf186b60864bd
Author: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
Date:   Wed Nov 1 15:07:57 2017 +0100

    License cleanup: add SPDX GPL-2.0 license identifier to files with no license
    
    Many source files in the tree are missing licensing information, which
    makes it harder for compliance tools to determine the correct license.
    
    By default all files without license information are under the default
    license of the kernel, which is GPL version 2.
    
    Update the files which contain no license information with the 'GPL-2.0'
    SPDX license identifier.  The SPDX identifier is a legally binding
    shorthand, which can be used instead of the full boiler plate text.
    
    This patch is based on work done by Thomas Gleixner and Kate Stewart and
    Philippe Ombredanne.
    
    How this work was done:
    
    Patches were generated and checked against linux-4.14-rc6 for a subset of
    the use cases:
     - file had no licensing information it it.
     - file was a */uapi/* one with no licensing information in it,
     - file was a */uapi/* one with existing licensing information,
    
    Further patches will be generated in subsequent months to fix up cases
    where non-standard license headers were used, and references to license
    had to be inferred by heuristics based on keywords.
    
    The analysis to determine which SPDX License Identifier to be applied to
    a file was done in a spreadsheet of side by side results from of the
    output of two independent scanners (ScanCode & Windriver) producing SPDX
    tag:value files created by Philippe Ombredanne.  Philippe prepared the
    base worksheet, and did an initial spot review of a few 1000 files.
    
    The 4.13 kernel was the starting point of the analysis with 60,537 files
    assessed.  Kate Stewart did a file by file comparison of the scanner
    results in the spreadsheet to determine which SPDX license identifier(s)
    to be applied to the file. She confirmed any determination that was not
    immediately clear with lawyers working with the Linux Foundation.
    
    Criteria used to select files for SPDX license identifier tagging was:
     - Files considered eligible had to be source code files.
     - Make and config files were included as candidates if they contained >5
       lines of source
     - File already had some variant of a license header in it (even if <5
       lines).
    
    All documentation files were explicitly excluded.
    
    The following heuristics were used to determine which SPDX license
    identifiers to apply.
    
     - when both scanners couldn't find any license traces, file was
       considered to have no license information in it, and the top level
       COPYING file license applied.
    
       For non */uapi/* files that summary was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0                                              11139
    
       and resulted in the first patch in this series.
    
       If that file was a */uapi/* path one, it was "GPL-2.0 WITH
       Linux-syscall-note" otherwise it was "GPL-2.0".  Results of that was:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|-------
       GPL-2.0 WITH Linux-syscall-note                        930
    
       and resulted in the second patch in this series.
    
     - if a file had some form of licensing information in it, and was one
       of the */uapi/* ones, it was denoted with the Linux-syscall-note if
       any GPL family license was found in the file or had no licensing in
       it (per prior point).  Results summary:
    
       SPDX license identifier                            # files
       ---------------------------------------------------|------
       GPL-2.0 WITH Linux-syscall-note                       270
       GPL-2.0+ WITH Linux-syscall-note                      169
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-2-Clause)    21
       ((GPL-2.0 WITH Linux-syscall-note) OR BSD-3-Clause)    17
       LGPL-2.1+ WITH Linux-syscall-note                      15
       GPL-1.0+ WITH Linux-syscall-note                       14
       ((GPL-2.0+ WITH Linux-syscall-note) OR BSD-3-Clause)    5
       LGPL-2.0+ WITH Linux-syscall-note                       4
       LGPL-2.1 WITH Linux-syscall-note                        3
       ((GPL-2.0 WITH Linux-syscall-note) OR MIT)              3
       ((GPL-2.0 WITH Linux-syscall-note) AND MIT)             1
    
       and that resulted in the third patch in this series.
    
     - when the two scanners agreed on the detected license(s), that became
       the concluded license(s).
    
     - when there was disagreement between the two scanners (one detected a
       license but the other didn't, or they both detected different
       licenses) a manual inspection of the file occurred.
    
     - In most cases a manual inspection of the information in the file
       resulted in a clear resolution of the license that should apply (and
       which scanner probably needed to revisit its heuristics).
    
     - When it was not immediately clear, the license identifier was
       confirmed with lawyers working with the Linux Foundation.
    
     - If there was any question as to the appropriate license identifier,
       the file was flagged for further research and to be revisited later
       in time.
    
    In total, over 70 hours of logged manual review was done on the
    spreadsheet to determine the SPDX license identifiers to apply to the
    source files by Kate, Philippe, Thomas and, in some cases, confirmation
    by lawyers working with the Linux Foundation.
    
    Kate also obtained a third independent scan of the 4.13 code base from
    FOSSology, and compared selected files where the other two scanners
    disagreed against that SPDX file, to see if there was new insights.  The
    Windriver scanner is based on an older version of FOSSology in part, so
    they are related.
    
    Thomas did random spot checks in about 500 files from the spreadsheets
    for the uapi headers and agreed with SPDX license identifier in the
    files he inspected. For the non-uapi files Thomas did random spot checks
    in about 15000 files.
    
    In initial set of patches against 4.14-rc6, 3 files were found to have
    copy/paste license identifier errors, and have been fixed to reflect the
    correct identifier.
    
    Additionally Philippe spent 10 hours this week doing a detailed manual
    inspection and review of the 12,461 patched files from the initial patch
    version early this week with:
     - a full scancode scan run, collecting the matched texts, detected
       license ids and scores
     - reviewing anything where there was a license detected (about 500+
       files) to ensure that the applied SPDX license was correct
     - reviewing anything where there was no detection but the patch license
       was not GPL-2.0 WITH Linux-syscall-note to ensure that the applied
       SPDX license was correct
    
    This produced a worksheet with 20 files needing minor correction.  This
    worksheet was then exported into 3 different .csv files for the
    different types of files to be modified.
    
    These .csv files were then reviewed by Greg.  Thomas wrote a script to
    parse the csv files and add the proper SPDX tag to the file, in the
    format that the file expected.  This script was further refined by Greg
    based on the output to detect more types of files automatically and to
    distinguish between header and source .c files (which need different
    comment types.)  Finally Greg ran the script using the .csv files to
    generate the patches.
    
    Reviewed-by: Kate Stewart <kstewart@linuxfoundation.org>
    Reviewed-by: Philippe Ombredanne <pombredanne@nexb.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 652c682707cd..401b0639116f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1,3 +1,4 @@
+/* SPDX-License-Identifier: GPL-2.0 */
 
 #ifndef _LINUX_KERNEL_TRACE_H
 #define _LINUX_KERNEL_TRACE_H

commit 6aa7de059173a986114ac43b8f50b297a86f09a8
Author: Mark Rutland <mark.rutland@arm.com>
Date:   Mon Oct 23 14:07:29 2017 -0700

    locking/atomics: COCCINELLE/treewide: Convert trivial ACCESS_ONCE() patterns to READ_ONCE()/WRITE_ONCE()
    
    Please do not apply this to mainline directly, instead please re-run the
    coccinelle script shown below and apply its output.
    
    For several reasons, it is desirable to use {READ,WRITE}_ONCE() in
    preference to ACCESS_ONCE(), and new code is expected to use one of the
    former. So far, there's been no reason to change most existing uses of
    ACCESS_ONCE(), as these aren't harmful, and changing them results in
    churn.
    
    However, for some features, the read/write distinction is critical to
    correct operation. To distinguish these cases, separate read/write
    accessors must be used. This patch migrates (most) remaining
    ACCESS_ONCE() instances to {READ,WRITE}_ONCE(), using the following
    coccinelle script:
    
    ----
    // Convert trivial ACCESS_ONCE() uses to equivalent READ_ONCE() and
    // WRITE_ONCE()
    
    // $ make coccicheck COCCI=/home/mark/once.cocci SPFLAGS="--include-headers" MODE=patch
    
    virtual patch
    
    @ depends on patch @
    expression E1, E2;
    @@
    
    - ACCESS_ONCE(E1) = E2
    + WRITE_ONCE(E1, E2)
    
    @ depends on patch @
    expression E;
    @@
    
    - ACCESS_ONCE(E)
    + READ_ONCE(E)
    ----
    
    Signed-off-by: Mark Rutland <mark.rutland@arm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: davem@davemloft.net
    Cc: linux-arch@vger.kernel.org
    Cc: mpe@ellerman.id.au
    Cc: shuah@kernel.org
    Cc: snitzer@redhat.com
    Cc: thor.thayer@linux.intel.com
    Cc: tj@kernel.org
    Cc: viro@zeniv.linux.org.uk
    Cc: will.deacon@arm.com
    Link: http://lkml.kernel.org/r/1508792849-3115-19-git-send-email-paulmck@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 652c682707cd..9050c8b3ccde 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1459,7 +1459,7 @@ extern struct trace_event_file *find_event_file(struct trace_array *tr,
 
 static inline void *event_file_data(struct file *filp)
 {
-	return ACCESS_ONCE(file_inode(filp)->i_private);
+	return READ_ONCE(file_inode(filp)->i_private);
 }
 
 extern struct mutex event_mutex;

commit 7e465baa80293ed5f87fdf6405391d6f02110d4e
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Fri Sep 22 14:58:20 2017 -0500

    tracing: Make traceprobe parsing code reusable
    
    traceprobe_probes_write() and traceprobe_command() actually contain
    nothing that ties them to kprobes - the code is generically useful for
    similar types of parsing elsewhere, so separate it out and move it to
    trace.c/trace.h.
    
    Other than moving it, the only change is in naming:
    traceprobe_probes_write() becomes trace_parse_run_command() and
    traceprobe_command() becomes trace_run_command().
    
    Link: http://lkml.kernel.org/r/ae5c26ea40c196a8986854d921eb6e713ede7e3f.1506105045.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3c078e2ad777..f8343eb3c1f9 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1752,6 +1752,13 @@ void trace_printk_start_comm(void);
 int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);
 int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 
+#define MAX_EVENT_NAME_LEN	64
+
+extern int trace_run_command(const char *buf, int (*createfn)(int, char**));
+extern ssize_t trace_parse_run_command(struct file *file,
+		const char __user *buffer, size_t count, loff_t *ppos,
+		int (*createfn)(int, char**));
+
 /*
  * Normal trace_printk() and friends allocates special buffers
  * to do the manipulation, as well as saves the print formats

commit d8c4deee6dc6876ae3b7d09a5b58138a57ee45f6
Author: Joel Fernandes <joelaf@google.com>
Date:   Fri Sep 8 23:55:17 2017 -0700

    tracing: Remove obsolete sched_switch tracer selftest
    
    Since commit 87d80de2800d087ea833cb79bc13f85ff34ed49f ("tracing: Remove
    obsolete sched_switch tracer"), the sched_switch tracer selftest is no longer
    used.  This patch removes the same.
    
    Link: http://lkml.kernel.org/r/20170909065517.22262-1-joelaf@google.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: kernel-team@android.com
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 652c682707cd..3c078e2ad777 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -738,8 +738,6 @@ extern int trace_selftest_startup_wakeup(struct tracer *trace,
 					 struct trace_array *tr);
 extern int trace_selftest_startup_nop(struct tracer *trace,
 					 struct trace_array *tr);
-extern int trace_selftest_startup_sched_switch(struct tracer *trace,
-					       struct trace_array *tr);
 extern int trace_selftest_startup_branch(struct tracer *trace,
 					 struct trace_array *tr);
 /*

commit c7b3ae0bd2ca658c7a71c49901d08c590294fac9
Author: Ziqian SUN (Zamir) <zsun@redhat.com>
Date:   Mon Sep 11 14:26:35 2017 +0800

    tracing: Ignore mmiotrace from kernel commandline
    
    The mmiotrace tracer cannot be enabled with ftrace=mmiotrace in kernel
    commandline. With this patch, noboot is added to the tracer struct,
    and when system boot with a tracer that has noboot=true, it will print
    out a warning message and continue booting.
    
    Link: http://lkml.kernel.org/r/1505111195-31942-1-git-send-email-zsun@redhat.com
    
    Signed-off-by: Ziqian SUN (Zamir) <zsun@redhat.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fb5d54d0d1b3..652c682707cd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -444,6 +444,8 @@ struct tracer {
 #ifdef CONFIG_TRACER_MAX_TRACE
 	bool			use_max_tr;
 #endif
+	/* True if tracer cannot be enabled in kernel param */
+	bool			noboot;
 };
 
 

commit 065e63f951432068ba89a844fcbff68ea16ee186
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 31 17:03:47 2017 -0400

    tracing: Only have rmmod clear buffers that its events were active in
    
    Currently, when a module event is enabled, when that module is removed, it
    clears all ring buffers. This is to prevent another module from being loaded
    and having one of its trace event IDs from reusing a trace event ID of the
    removed module. This could cause undesirable effects as the trace event of
    the new module would be using its own processing algorithms to process raw
    data of another event. To prevent this, when a module is loaded, if any of
    its events have been used (signified by the WAS_ENABLED event call flag,
    which is never cleared), all ring buffers are cleared, just in case any one
    of them contains event data of the removed event.
    
    The problem is, there's no reason to clear all ring buffers if only one (or
    less than all of them) uses one of the events. Instead, only clear the ring
    buffers that recorded the events of a module that is being removed.
    
    To do this, instead of keeping the WAS_ENABLED flag with the trace event
    call, move it to the per instance (per ring buffer) event file descriptor.
    The event file descriptor maps each event to a separate ring buffer
    instance. Then when the module is removed, only the ring buffers that
    activated one of the module's events get cleared. The rest are not touched.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 490ba229931d..fb5d54d0d1b3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -245,6 +245,7 @@ struct trace_array {
 	int			stop_count;
 	int			clock_id;
 	int			nr_topts;
+	bool			clear_trace;
 	struct tracer		*current_trace;
 	unsigned int		trace_flags;
 	unsigned char		trace_flags_index[TRACE_FLAGS_MAX_SIZE];

commit f86f418059b94aa01f9342611a272ca60c583e89
Author: Chunyan Zhang <zhang.chunyan@linaro.org>
Date:   Wed Jun 7 16:12:51 2017 +0800

    trace: fix the errors caused by incompatible type of RCU variables
    
    The variables which are processed by RCU functions should be annotated
    as RCU, otherwise sparse will report the errors like below:
    
    "error: incompatible types in comparison expression (different
    address spaces)"
    
    Link: http://lkml.kernel.org/r/1496823171-7758-1-git-send-email-zhang.chunyan@linaro.org
    
    Signed-off-by: Chunyan Zhang <zhang.chunyan@linaro.org>
    [ Updated to not be 100% 80 column strict ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6ade1c55cc3a..490ba229931d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1210,9 +1210,9 @@ struct ftrace_event_field {
 struct event_filter {
 	int			n_preds;	/* Number assigned */
 	int			a_preds;	/* allocated */
-	struct filter_pred	*preds;
-	struct filter_pred	*root;
-	char			*filter_string;
+	struct filter_pred __rcu	*preds;
+	struct filter_pred __rcu	*root;
+	char				*filter_string;
 };
 
 struct event_subsystem {

commit d914ba37d7145acb9fd3bb23075c2d56e5a44eb6
Author: Joel Fernandes <joelaf@google.com>
Date:   Mon Jun 26 19:01:55 2017 -0700

    tracing: Add support for recording tgid of tasks
    
    Inorder to support recording of tgid, the following changes are made:
    
    * Introduce a new API (tracing_record_taskinfo) to additionally record the tgid
      along with the task's comm at the same time. This has has the benefit of not
      setting trace_cmdline_save before all the information for a task is saved.
    * Add a new API tracing_record_taskinfo_sched_switch to record task information
      for 2 tasks at a time (previous and next) and use it from sched_switch probe.
    * Preserve the old API (tracing_record_cmdline) and create it as a wrapper
      around the new one so that existing callers aren't affected.
    * Reuse the existing sched_switch and sched_wakeup probes to record tgid
      information and add a new option 'record-tgid' to enable recording of tgid
    
    When record-tgid option isn't enabled to being with, we take care to make sure
    that there's isn't memory or runtime overhead.
    
    Link: http://lkml.kernel.org/r/20170627020155.5139-1-joelaf@google.com
    
    Cc: kernel-team@android.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Tested-by: Michael Sartain <mikesart@gmail.com>
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 13823951e42b..6ade1c55cc3a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -640,6 +640,9 @@ void set_graph_array(struct trace_array *tr);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
+void tracing_start_tgid_record(void);
+void tracing_stop_tgid_record(void);
+
 int register_tracer(struct tracer *type);
 int is_tracing_stopped(void);
 
@@ -700,6 +703,7 @@ static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
 extern u64 ftrace_now(int cpu);
 
 extern void trace_find_cmdline(int pid, char comm[]);
+extern int trace_find_tgid(int pid);
 extern void trace_event_follow_fork(struct trace_array *tr, bool enable);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
@@ -1124,6 +1128,7 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(CONTEXT_INFO,		"context-info"),   /* Print pid/cpu/time */ \
 		C(LATENCY_FMT,		"latency-format"),	\
 		C(RECORD_CMD,		"record-cmd"),		\
+		C(RECORD_TGID,		"record-tgid"),		\
 		C(OVERWRITE,		"overwrite"),		\
 		C(STOP_ON_FREE,		"disable_on_free"),	\
 		C(IRQ_INFO,		"irq-info"),		\
@@ -1440,6 +1445,8 @@ struct ftrace_event_field *
 trace_find_event_field(struct trace_event_call *call, char *name);
 
 extern void trace_event_enable_cmd_record(bool enable);
+extern void trace_event_enable_tgid_record(bool enable);
+
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 extern int event_trace_del_tracer(struct trace_array *tr);
 

commit 8c08f0d5c6fb10ff93ffb1cbf416f4f1c3a52a80
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Jun 26 11:47:31 2017 -0400

    ftrace: Have cached module filters be an active filter
    
    When a module filter is added to set_ftrace_filter, if the module is not
    loaded, it is cached. This should be considered an active filter, and
    function tracing should be filtered by this. That is, if a cached module
    filter is the only filter set, then no function tracing should be happening,
    as all the functions available will be filtered out.
    
    This makes sense, as the reason to add a cached module filter, is to trace
    the module when you load it. There shouldn't be any other tracing happening
    until then.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d63550cdbdfa..13823951e42b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -773,10 +773,15 @@ struct ftrace_mod_load {
 	int			 enable;
 };
 
+enum {
+	FTRACE_HASH_FL_MOD	= (1 << 0),
+};
+
 struct ftrace_hash {
 	unsigned long		size_bits;
 	struct hlist_head	*buckets;
 	unsigned long		count;
+	unsigned long		flags;
 	struct rcu_head		rcu;
 };
 
@@ -785,7 +790,7 @@ ftrace_lookup_ip(struct ftrace_hash *hash, unsigned long ip);
 
 static __always_inline bool ftrace_hash_empty(struct ftrace_hash *hash)
 {
-	return !hash || !hash->count;
+	return !hash || !(hash->count || (hash->flags & FTRACE_HASH_FL_MOD));
 }
 
 /* Standard output formatting function used for function return traces */

commit 673feb9d76ab3eddde7acfd94b206e321cfc90b9
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Jun 23 15:26:26 2017 -0400

    ftrace: Add :mod: caching infrastructure to trace_array
    
    This is the start of the infrastructure work to allow for tracing module
    functions before it is loaded.
    
    Currently the following command:
    
      # echo :mod:some-mod > set_ftrace_filter
    
    will enable tracing of all functions within the module "some-mod" if it is
    loaded. What we want, is if the module is not loaded, that line will be
    saved. When the module is loaded, then the "some-mod" will have that line
    executed on it, so that the functions within it starts being traced.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 69a3ab3ee4f5..d63550cdbdfa 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -263,7 +263,10 @@ struct trace_array {
 	struct ftrace_ops	*ops;
 	struct trace_pid_list	__rcu *function_pids;
 #ifdef CONFIG_DYNAMIC_FTRACE
+	/* All of these are protected by the ftrace_lock */
 	struct list_head	func_probes;
+	struct list_head	mod_trace;
+	struct list_head	mod_notrace;
 #endif
 	/* function tracing enabled */
 	int			function_enabled;
@@ -761,6 +764,15 @@ enum print_line_t print_trace_line(struct trace_iterator *iter);
 
 extern char trace_find_mark(unsigned long long duration);
 
+struct ftrace_hash;
+
+struct ftrace_mod_load {
+	struct list_head	list;
+	char			*func;
+	char			*module;
+	int			 enable;
+};
+
 struct ftrace_hash {
 	unsigned long		size_bits;
 	struct hlist_head	*buckets;

commit f57a41434fc51732dd5e35e0e1aa9e607f1a05d6
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed May 31 16:56:48 2017 -0500

    trace: rename enum_map functions
    
    Rename the core trace enum routines to use eval, to
    reflect their use by more than just enum to value mapping.
    
    Link: http://lkml.kernel.org/r/20170531215653.3240-8-jeremy.linton@arm.com
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a9667297ae49..69a3ab3ee4f5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1773,10 +1773,10 @@ static inline const char *get_syscall_name(int syscall)
 
 #ifdef CONFIG_EVENT_TRACING
 void trace_event_init(void);
-void trace_event_enum_update(struct trace_eval_map **map, int len);
+void trace_event_eval_update(struct trace_eval_map **map, int len);
 #else
 static inline void __init trace_event_init(void) { }
-static inline void trace_event_enum_update(struct trace_eval_map **map, int len) { }
+static inline void trace_event_eval_update(struct trace_eval_map **map, int len) { }
 #endif
 
 extern struct trace_iterator *tracepoint_print_iter;

commit 00f4b652b6f1dbfd4e1d5419d7f1cc23b1374da8
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed May 31 16:56:43 2017 -0500

    trace: rename trace_enum_map to trace_eval_map
    
    Each enum is loaded into the trace_enum_map, as we
    are now using this for more than enums rename it.
    
    Link: http://lkml.kernel.org/r/20170531215653.3240-3-jeremy.linton@arm.com
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 39fd77330aab..a9667297ae49 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1773,10 +1773,10 @@ static inline const char *get_syscall_name(int syscall)
 
 #ifdef CONFIG_EVENT_TRACING
 void trace_event_init(void);
-void trace_event_enum_update(struct trace_enum_map **map, int len);
+void trace_event_enum_update(struct trace_eval_map **map, int len);
 #else
 static inline void __init trace_event_init(void) { }
-static inline void trace_event_enum_update(struct trace_enum_map **map, int len) { }
+static inline void trace_event_enum_update(struct trace_eval_map **map, int len) { }
 #endif
 
 extern struct trace_iterator *tracepoint_print_iter;

commit 8a49f3e03c8ac52fe1b706fffb13142295fa0c47
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed May 17 21:53:32 2017 -0400

    ftrace: Remove #ifdef from code and add clear_ftrace_function_probes() stub
    
    No need to add ugly #ifdefs in the code. Having a standard stub file is much
    prettier.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 98e0845f7235..39fd77330aab 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -999,6 +999,10 @@ static inline __init int unregister_ftrace_command(char *cmd_name)
 {
 	return -EINVAL;
 }
+static inline void clear_ftrace_function_probes(struct trace_array *tr)
+{
+}
+
 /*
  * The ops parameter passed in is usually undefined.
  * This must be a macro.

commit a0e6369e4bac8844825ae1a66ccd122b290dcc86
Author: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
Date:   Tue May 16 23:21:26 2017 +0530

    ftrace/instances: Clear function triggers when removing instances
    
    If instance directories are deleted while there are registered function
    triggers:
    
      # cd /sys/kernel/debug/tracing/instances
      # mkdir test
      # echo "schedule:enable_event:sched:sched_switch" > test/set_ftrace_filter
      # rmdir test
      Unable to handle kernel paging request for data at address 0x00000008
      Unable to handle kernel paging request for data at address 0x00000008
      Faulting instruction address: 0xc0000000021edde8
      Oops: Kernel access of bad area, sig: 11 [#1]
      SMP NR_CPUS=2048
      NUMA
      pSeries
      Modules linked in: iptable_mangle ipt_MASQUERADE nf_nat_masquerade_ipv4 iptable_nat nf_nat_ipv4 nf_nat nf_conntrack_ipv4 nf_defrag_ipv4 xt_conntrack nf_conntrack ipt_REJECT nf_reject_ipv4 xt_tcpudp tun bridge stp llc kvm iptable_filter fuse binfmt_misc pseries_rng rng_core vmx_crypto ib_iser rdma_cm iw_cm ib_cm ib_core libiscsi scsi_transport_iscsi ip_tables x_tables autofs4 btrfs raid10 raid456 async_raid6_recov async_memcpy async_pq async_xor async_tx xor raid6_pq libcrc32c multipath virtio_net virtio_blk virtio_pci crc32c_vpmsum virtio_ring virtio
      CPU: 8 PID: 8694 Comm: rmdir Not tainted 4.11.0-nnr+ #113
      task: c0000000bab52800 task.stack: c0000000baba0000
      NIP: c0000000021edde8 LR: c0000000021f0590 CTR: c000000002119620
      REGS: c0000000baba3870 TRAP: 0300   Not tainted  (4.11.0-nnr+)
      MSR: 8000000000009033 <SF,EE,ME,IR,DR,RI,LE>
        CR: 22002422  XER: 20000000
      CFAR: 00007fffabb725a8 DAR: 0000000000000008 DSISR: 40000000 SOFTE: 0
      GPR00: c00000000220f750 c0000000baba3af0 c000000003157e00 0000000000000000
      GPR04: 0000000000000040 00000000000000eb 0000000000000040 0000000000000000
      GPR08: 0000000000000000 0000000000000113 0000000000000000 c00000000305db98
      GPR12: c000000002119620 c00000000fd42c00 0000000000000000 0000000000000000
      GPR16: 0000000000000000 0000000000000000 0000000000000000 0000000000000000
      GPR20: 0000000000000000 0000000000000000 c0000000bab52e90 0000000000000000
      GPR24: 0000000000000000 00000000000000eb 0000000000000040 c0000000baba3bb0
      GPR28: c00000009cb06eb0 c0000000bab52800 c00000009cb06eb0 c0000000baba3bb0
      NIP [c0000000021edde8] ring_buffer_lock_reserve+0x8/0x4e0
      LR [c0000000021f0590] trace_event_buffer_lock_reserve+0xe0/0x1a0
      Call Trace:
      [c0000000baba3af0] [c0000000021f96c8] trace_event_buffer_commit+0x1b8/0x280 (unreliable)
      [c0000000baba3b60] [c00000000220f750] trace_event_buffer_reserve+0x80/0xd0
      [c0000000baba3b90] [c0000000021196b8] trace_event_raw_event_sched_switch+0x98/0x180
      [c0000000baba3c10] [c0000000029d9980] __schedule+0x6e0/0xab0
      [c0000000baba3ce0] [c000000002122230] do_task_dead+0x70/0xc0
      [c0000000baba3d10] [c0000000020ea9c8] do_exit+0x828/0xd00
      [c0000000baba3dd0] [c0000000020eaf70] do_group_exit+0x60/0x100
      [c0000000baba3e10] [c0000000020eb034] SyS_exit_group+0x24/0x30
      [c0000000baba3e30] [c00000000200bcec] system_call+0x38/0x54
      Instruction dump:
      60000000 60420000 7d244b78 7f63db78 4bffaa09 393efff8 793e0020 39200000
      4bfffecc 60420000 3c4c00f7 3842a020 <81230008> 2f890000 409e02f0 a14d0008
      ---[ end trace b917b8985d0e650b ]---
      Unable to handle kernel paging request for data at address 0x00000008
      Faulting instruction address: 0xc0000000021edde8
      Unable to handle kernel paging request for data at address 0x00000008
      Faulting instruction address: 0xc0000000021edde8
      Faulting instruction address: 0xc0000000021edde8
    
    To address this, let's clear all registered function probes before
    deleting the ftrace instance.
    
    Link: http://lkml.kernel.org/r/c5f1ca624043690bd94642bb6bffd3f2fc504035.1494956770.git.naveen.n.rao@linux.vnet.ibm.com
    
    Reported-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Naveen N. Rao <naveen.n.rao@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 291a1bca5748..98e0845f7235 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -980,6 +980,7 @@ register_ftrace_function_probe(char *glob, struct trace_array *tr,
 extern int
 unregister_ftrace_function_probe_func(char *glob, struct trace_array *tr,
 				      struct ftrace_probe_ops *ops);
+extern void clear_ftrace_function_probes(struct trace_array *tr);
 
 int register_ftrace_command(struct ftrace_func_command *cmd);
 int unregister_ftrace_command(struct ftrace_func_command *cmd);

commit 4c174688ee92805aa5df6e06e5b625a3286e415c
Merge: 9c35baf6cee9 73a757e63114
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 3 18:41:21 2017 -0700

    Merge tag 'trace-v4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "New features for this release:
    
       - Pretty much a full rewrite of the processing of function plugins.
         i.e. echo do_IRQ:stacktrace > set_ftrace_filter
    
       - The rewrite was needed to add plugins to be unique to tracing
         instances. i.e. mkdir instance/foo; cd instances/foo; echo
         do_IRQ:stacktrace > set_ftrace_filter The old way was written very
         hacky. This removes a lot of those hacks.
    
       - New "function-fork" tracing option. When set, pids in the
         set_ftrace_pid will have their children added when the processes
         with their pids listed in the set_ftrace_pid file forks.
    
       - Exposure of "maxactive" for kretprobe in kprobe_events
    
       - Allow for builtin init functions to be traced by the function
         tracer (via the kernel command line). Module init function tracing
         will come in the next release.
    
       - Added more selftests, and have selftests also test in an instance"
    
    * tag 'trace-v4.12' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (60 commits)
      ring-buffer: Return reader page back into existing ring buffer
      selftests: ftrace: Allow some event trigger tests to run in an instance
      selftests: ftrace: Have some basic tests run in a tracing instance too
      selftests: ftrace: Have event tests also run in an tracing instance
      selftests: ftrace: Make func_event_triggers and func_traceonoff_triggers tests do instances
      selftests: ftrace: Allow some tests to be run in a tracing instance
      tracing/ftrace: Allow for instances to trigger their own stacktrace probes
      tracing/ftrace: Allow for the traceonoff probe be unique to instances
      tracing/ftrace: Enable snapshot function trigger to work with instances
      tracing/ftrace: Allow instances to have their own function probes
      tracing/ftrace: Add a better way to pass data via the probe functions
      ftrace: Dynamically create the probe ftrace_ops for the trace_array
      tracing: Pass the trace_array into ftrace_probe_ops functions
      tracing: Have the trace_array hold the list of registered func probes
      ftrace: If the hash for a probe fails to update then free what was initialized
      ftrace: Have the function probes call their own function
      ftrace: Have each function probe use its own ftrace_ops
      ftrace: Have unregister_ftrace_function_probe_func() return a value
      ftrace: Add helper function ftrace_hash_move_and_update_ops()
      ftrace: Remove data field from ftrace_func_probe structure
      ...

commit 2290f2c589285d0031e3b7445afff8949f3fdbb6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Apr 20 11:46:03 2017 -0400

    tracing/ftrace: Allow for the traceonoff probe be unique to instances
    
    Have the traceon/off function probe triggers affect only the instance they
    are set in. This required making the trace_on/off accessible for other files
    in the tracing directory.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8f6754fba778..bc011c1f3d71 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -582,6 +582,8 @@ void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 bool tracing_is_disabled(void);
 int tracer_tracing_is_on(struct trace_array *tr);
+void tracer_tracing_on(struct trace_array *tr);
+void tracer_tracing_off(struct trace_array *tr);
 struct dentry *trace_create_file(const char *name,
 				 umode_t mode,
 				 struct dentry *parent,

commit 6e4443199e5354255e8a4c1e8e5cfc8ef064c3ce
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 19 22:39:44 2017 -0400

    tracing/ftrace: Add a better way to pass data via the probe functions
    
    With the redesign of the registration and execution of the function probes
    (triggers), data can now be passed from the setup of the probe to the probe
    callers that are specific to the trace_array it is on. Although, all probes
    still only affect the toplevel trace array, this change will allow for
    instances to have their own probes separated from other instances and the
    top array.
    
    That is, something like the stacktrace probe can be set to trace only in an
    instance and not the toplevel trace array. This isn't implement yet, but
    this change sets the ground work for the change.
    
    When a probe callback is triggered (someone writes the probe format into
    set_ftrace_filter), it calls register_ftrace_function_probe() passing in
    init_data that will be used to initialize the probe. Then for every matching
    function, register_ftrace_function_probe() will call the probe_ops->init()
    function with the init data that was passed to it, as well as an address to
    a place holder that is associated with the probe and the instance. The first
    occurrence will have a NULL in the pointer. The init() function will then
    initialize it. If other probes are added, or more functions are part of the
    probe, the place holder will be passed to the init() function with the place
    holder data that it was initialized to the last time.
    
    Then this place_holder is passed to each of the other probe_ops functions,
    where it can be used in the function callback. When the probe_ops free()
    function is called, it can be called either with the rip of the function
    that is being removed from the probe, or zero, indicating that there are no
    more functions attached to the probe, and the place holder is about to be
    freed. This gives the probe_ops a way to free the data it assigned to the
    place holder if it was allocade during the first init call.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e978ecd257b8..8f6754fba778 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -943,18 +943,18 @@ struct ftrace_probe_ops {
 					unsigned long parent_ip,
 					struct trace_array *tr,
 					struct ftrace_probe_ops *ops,
-					void **data);
+					void *data);
 	int			(*init)(struct ftrace_probe_ops *ops,
 					struct trace_array *tr,
-					unsigned long ip, void *data);
+					unsigned long ip, void *init_data,
+					void **data);
 	void			(*free)(struct ftrace_probe_ops *ops,
 					struct trace_array *tr,
-					unsigned long ip, void **data);
+					unsigned long ip, void *data);
 	int			(*print)(struct seq_file *m,
 					 unsigned long ip,
 					 struct ftrace_probe_ops *ops,
 					 void *data);
-	void			*private_data;
 };
 
 struct ftrace_func_mapper;

commit 7b60f3d8761561d95d7e962522d6338143fc2329
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 18 14:50:39 2017 -0400

    ftrace: Dynamically create the probe ftrace_ops for the trace_array
    
    In order to eventually have each trace_array instance have its own unique
    set of function probes (triggers), the trace array needs to hold the ops and
    the filters for the probes.
    
    This is the first step to accomplish this. Instead of having the private
    data of the probe ops point to the trace_array, create a separate list that
    the trace_array holds. There's only one private_data for a probe, we need
    one per trace_array. The probe ftrace_ops will be dynamically created for
    each instance, instead of being static.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 390761804886..e978ecd257b8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -939,8 +939,6 @@ static inline void ftrace_pid_follow_fork(struct trace_array *tr, bool enable) {
 #if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_DYNAMIC_FTRACE)
 
 struct ftrace_probe_ops {
-	struct ftrace_ops	ops;
-	struct list_head	list;
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
 					struct trace_array *tr,
@@ -976,7 +974,8 @@ extern int
 register_ftrace_function_probe(char *glob, struct trace_array *tr,
 			       struct ftrace_probe_ops *ops, void *data);
 extern int
-unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
+unregister_ftrace_function_probe_func(char *glob, struct trace_array *tr,
+				      struct ftrace_probe_ops *ops);
 
 int register_ftrace_command(struct ftrace_func_command *cmd);
 int unregister_ftrace_command(struct ftrace_func_command *cmd);

commit b5f081b563a6cdcb85a543df8c851951a8978275
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 10 22:30:05 2017 -0400

    tracing: Pass the trace_array into ftrace_probe_ops functions
    
    Pass the trace_array associated to a ftrace_probe_ops into the probe_ops
    func(), init() and free() functions. The trace_array is the descriptor that
    describes a tracing instance. This will help create the infrastructure that
    will allow having function probes unique to tracing instances.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 68ff25e4cb19..390761804886 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -943,11 +943,14 @@ struct ftrace_probe_ops {
 	struct list_head	list;
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
+					struct trace_array *tr,
 					struct ftrace_probe_ops *ops,
 					void **data);
 	int			(*init)(struct ftrace_probe_ops *ops,
+					struct trace_array *tr,
 					unsigned long ip, void *data);
 	void			(*free)(struct ftrace_probe_ops *ops,
+					struct trace_array *tr,
 					unsigned long ip, void **data);
 	int			(*print)(struct seq_file *m,
 					 unsigned long ip,

commit 04ec7bb642b77374b53731b795b5654b5aff1c00
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 5 13:12:55 2017 -0400

    tracing: Have the trace_array hold the list of registered func probes
    
    Add a link list to the trace_array to hold func probes that are registered.
    Currently, all function probes are the same for all instances as it was
    before, that is, only the top level trace_array holds the function probes.
    But this lays the ground work to have function probes be attached to
    individual instances, and having the event trigger only affect events in the
    given instance. But that work is still to be done.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d457addcc224..68ff25e4cb19 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -262,6 +262,9 @@ struct trace_array {
 #ifdef CONFIG_FUNCTION_TRACER
 	struct ftrace_ops	*ops;
 	struct trace_pid_list	__rcu *function_pids;
+#ifdef CONFIG_DYNAMIC_FTRACE
+	struct list_head	func_probes;
+#endif
 	/* function tracing enabled */
 	int			function_enabled;
 #endif
@@ -696,6 +699,9 @@ extern void trace_event_follow_fork(struct trace_array *tr, bool enable);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern unsigned long ftrace_update_tot_cnt;
+void ftrace_init_trace_array(struct trace_array *tr);
+#else
+static inline void ftrace_init_trace_array(struct trace_array *tr) { }
 #endif
 #define DYN_FTRACE_TEST_NAME trace_selftest_dynamic_test_func
 extern int DYN_FTRACE_TEST_NAME(void);
@@ -883,7 +889,8 @@ extern struct list_head ftrace_pids;
 struct ftrace_func_command {
 	struct list_head	list;
 	char			*name;
-	int			(*func)(struct ftrace_hash *hash,
+	int			(*func)(struct trace_array *tr,
+					struct ftrace_hash *hash,
 					char *func, char *cmd,
 					char *params, int enable);
 };
@@ -963,8 +970,8 @@ void free_ftrace_func_mapper(struct ftrace_func_mapper *mapper,
 			     ftrace_mapper_func free_func);
 
 extern int
-register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
-			      void *data);
+register_ftrace_function_probe(char *glob, struct trace_array *tr,
+			       struct ftrace_probe_ops *ops, void *data);
 extern int
 unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
 

commit eee8ded131f15e0f5b1897c9c4a7687fabd28822
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 21:31:28 2017 -0400

    ftrace: Have the function probes call their own function
    
    Now that the function probes have their own ftrace_ops, there's no reason to
    continue using the ftrace_func_hash to find which probe to call in the
    function callback. The ops that is passed in to the function callback is
    part of the probe_ops to call.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e16c67c49de4..d457addcc224 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -933,6 +933,7 @@ static inline void ftrace_pid_follow_fork(struct trace_array *tr, bool enable) {
 
 struct ftrace_probe_ops {
 	struct ftrace_ops	ops;
+	struct list_head	list;
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
 					struct ftrace_probe_ops *ops,

commit 1ec3a81a0cf4236b644282794932c4eda9c1714a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 18:16:29 2017 -0400

    ftrace: Have each function probe use its own ftrace_ops
    
    Have the function probes have their own ftrace_ops, and remove the
    trace_probe_ops. This simplifies some of the ftrace infrastructure code.
    
    Individual entries for each function is still allocated for the use of the
    output for set_ftrace_filter, but they will be removed soon too.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 31d80bff9ee6..e16c67c49de4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -932,6 +932,7 @@ static inline void ftrace_pid_follow_fork(struct trace_array *tr, bool enable) {
 #if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_DYNAMIC_FTRACE)
 
 struct ftrace_probe_ops {
+	struct ftrace_ops	ops;
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
 					struct ftrace_probe_ops *ops,

commit d3d532d798c5720055ab02a10bf7829a33c3645a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 16:44:43 2017 -0400

    ftrace: Have unregister_ftrace_function_probe_func() return a value
    
    Currently unregister_ftrace_function_probe_func() is a void function. It
    does not give any feedback if an error occurred or no item was found to
    remove and nothing was done.
    
    Change it to return status and success if it removed something. Also update
    the callers to return that feedback to the user.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 86aa5a2222ba..31d80bff9ee6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -963,7 +963,7 @@ void free_ftrace_func_mapper(struct ftrace_func_mapper *mapper,
 extern int
 register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
 			      void *data);
-extern void
+extern int
 unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
 
 int register_ftrace_command(struct ftrace_func_command *cmd);

commit 1a48df0041c2756194e700affb0e2ff084092e28
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 10:27:51 2017 -0400

    ftrace: Remove data field from ftrace_func_probe structure
    
    No users of the function probes uses the data field anymore. Remove it, and
    change the init function to take a void *data parameter instead of a
    void **data, because the init will just get the data that the registering
    function was received, and there's no state after it is called.
    
    The other functions for ftrace_probe_ops still take the data parameter, but
    it will currently only be passed NULL. It will stay as a parameter for
    future data to be passed to these functions.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 376d5a798489..86aa5a2222ba 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -937,7 +937,7 @@ struct ftrace_probe_ops {
 					struct ftrace_probe_ops *ops,
 					void **data);
 	int			(*init)(struct ftrace_probe_ops *ops,
-					unsigned long ip, void **data);
+					unsigned long ip, void *data);
 	void			(*free)(struct ftrace_probe_ops *ops,
 					unsigned long ip, void **data);
 	int			(*print)(struct seq_file *m,

commit 78f78e07d51e440d01e6b1aef172883821193771
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 13:39:59 2017 -0400

    ftrace: Remove unused unregister_ftrace_function_probe_all() function
    
    There are no users of unregister_ftrace_function_probe_all(). The only probe
    function that is used is unregister_ftrace_function_probe_func(). Rename the
    internal static function __unregister_ftrace_function_probe() to
    unregister_ftrace_function_probe_func() and make it global.
    
    Also remove the PROBE_TEST_FUNC as it would be always set.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 507a62e9192e..376d5a798489 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -965,7 +965,6 @@ register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
 			      void *data);
 extern void
 unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
-extern void unregister_ftrace_function_probe_all(char *glob);
 
 int register_ftrace_command(struct ftrace_func_command *cmd);
 int unregister_ftrace_command(struct ftrace_func_command *cmd);

commit 0fe7e7e3f8391d4c9260b41cdb15c7917cb2e5b3
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 09:56:26 2017 -0400

    ftrace: Remove unused unregister_ftrace_function_probe() function
    
    Nothing calls unregister_ftrace_function_probe(). Remove it as well as the
    flag PROBE_TEST_DATA, as this function was the only one to set it.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dbbdee21bcc4..507a62e9192e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -964,9 +964,6 @@ extern int
 register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
 			      void *data);
 extern void
-unregister_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
-				void *data);
-extern void
 unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
 extern void unregister_ftrace_function_probe_all(char *glob);
 

commit 41794f190780c28784fa62b22001691e5876d149
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 3 20:58:35 2017 -0400

    ftrace: Added ftrace_func_mapper for function probe triggers
    
    In order to move the ops to the function probes directly, they need a way to
    map function ips to their own data without depending on the infrastructure
    of the function probes, as the data field will be going away.
    
    New helper functions are added that are based on the ftrace_hash code.
    ftrace_func_mapper functions are there to let the probes map ips to their
    data. These can be allocated by the probe ops, and referenced in the
    function callbacks.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0f915c264c19..dbbdee21bcc4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -944,8 +944,22 @@ struct ftrace_probe_ops {
 					 unsigned long ip,
 					 struct ftrace_probe_ops *ops,
 					 void *data);
+	void			*private_data;
 };
 
+struct ftrace_func_mapper;
+typedef int (*ftrace_mapper_func)(void *data);
+
+struct ftrace_func_mapper *allocate_ftrace_func_mapper(void);
+void **ftrace_func_mapper_find_ip(struct ftrace_func_mapper *mapper,
+					   unsigned long ip);
+int ftrace_func_mapper_add_ip(struct ftrace_func_mapper *mapper,
+			       unsigned long ip, void *data);
+void *ftrace_func_mapper_remove_ip(struct ftrace_func_mapper *mapper,
+				   unsigned long ip);
+void free_ftrace_func_mapper(struct ftrace_func_mapper *mapper,
+			     ftrace_mapper_func free_func);
+
 extern int
 register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
 			      void *data);

commit bca6c8d0480a8aa5c86f8f416db96c71f6b79e29
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 3 18:18:47 2017 -0400

    ftrace: Pass probe ops to probe function
    
    In preparation to cleaning up the probe function registration code, the
    "data" parameter will eventually be removed from the probe->func() call.
    Instead it will receive its own "ops" function, in which it can set up its
    own data that it needs to map.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a63411c53c5e..0f915c264c19 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -934,6 +934,7 @@ static inline void ftrace_pid_follow_fork(struct trace_array *tr, bool enable) {
 struct ftrace_probe_ops {
 	void			(*func)(unsigned long ip,
 					unsigned long parent_ip,
+					struct ftrace_probe_ops *ops,
 					void **data);
 	int			(*init)(struct ftrace_probe_ops *ops,
 					unsigned long ip, void **data);

commit 92a68fa047ca5b8e1991af2d50b23ad9452613cd
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 31 19:21:41 2017 -0400

    ftrace: Move the function commands into the tracing directory
    
    As nothing outside the tracing directory uses the function command mechanism,
    I'm moving the prototypes out of the include/linux/ftrace.h and into the
    local kernel/trace/trace.h header. I plan on making them hook to the
    trace_array structure which is local to kernel/trace, and I do not want to
    expose it to the rest of the kernel. This requires that the command functions
    must also be local to tracing. But luckily nothing else uses them.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2ff6d49fa5ca..a63411c53c5e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -880,6 +880,13 @@ print_graph_function_flags(struct trace_iterator *iter, u32 flags)
 extern struct list_head ftrace_pids;
 
 #ifdef CONFIG_FUNCTION_TRACER
+struct ftrace_func_command {
+	struct list_head	list;
+	char			*name;
+	int			(*func)(struct ftrace_hash *hash,
+					char *func, char *cmd,
+					char *params, int enable);
+};
 extern bool ftrace_filter_param __initdata;
 static inline int ftrace_trace_task(struct trace_array *tr)
 {
@@ -948,10 +955,23 @@ extern void
 unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
 extern void unregister_ftrace_function_probe_all(char *glob);
 
+int register_ftrace_command(struct ftrace_func_command *cmd);
+int unregister_ftrace_command(struct ftrace_func_command *cmd);
+
 void ftrace_create_filter_files(struct ftrace_ops *ops,
 				struct dentry *parent);
 void ftrace_destroy_filter_files(struct ftrace_ops *ops);
 #else
+struct ftrace_func_command;
+
+static inline __init int register_ftrace_command(struct ftrace_func_command *cmd)
+{
+	return -EINVAL;
+}
+static inline __init int unregister_ftrace_command(char *cmd_name)
+{
+	return -EINVAL;
+}
 /*
  * The ops parameter passed in is usually undefined.
  * This must be a macro.

commit ec19b85913486993d7d6f747beed1a711afd47d8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 31 19:01:14 2017 -0400

    ftrace: Move the probe function into the tracing directory
    
    As nothing outside the tracing directory uses the function probes mechanism,
    I'm moving the prototypes out of the include/linux/ftrace.h and into the
    local kernel/trace/trace.h header. I plan on making them hook to the
    trace_array structure which is local to kernel/trace, and I do not want to
    expose it to the rest of the kernel. This requires that the probe functions
    must also be local to tracing. But luckily nothing else uses them.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 31a4997b67c6..2ff6d49fa5ca 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -923,6 +923,31 @@ static inline void ftrace_pid_follow_fork(struct trace_array *tr, bool enable) {
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_DYNAMIC_FTRACE)
+
+struct ftrace_probe_ops {
+	void			(*func)(unsigned long ip,
+					unsigned long parent_ip,
+					void **data);
+	int			(*init)(struct ftrace_probe_ops *ops,
+					unsigned long ip, void **data);
+	void			(*free)(struct ftrace_probe_ops *ops,
+					unsigned long ip, void **data);
+	int			(*print)(struct seq_file *m,
+					 unsigned long ip,
+					 struct ftrace_probe_ops *ops,
+					 void *data);
+};
+
+extern int
+register_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
+			      void *data);
+extern void
+unregister_ftrace_function_probe(char *glob, struct ftrace_probe_ops *ops,
+				void *data);
+extern void
+unregister_ftrace_function_probe_func(char *glob, struct ftrace_probe_ops *ops);
+extern void unregister_ftrace_function_probe_all(char *glob);
+
 void ftrace_create_filter_files(struct ftrace_ops *ops,
 				struct dentry *parent);
 void ftrace_destroy_filter_files(struct ftrace_ops *ops);

commit 1e10486ffee0a5b060c58b9c8c712422f7b88b3b
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Mon Apr 17 11:44:28 2017 +0900

    ftrace: Add 'function-fork' trace option
    
    The function-fork option is same as event-fork that it tracks task
    fork/exit and set the pid filter properly.  This can be useful if user
    wants to trace selected tasks including their children only.
    
    Link: http://lkml.kernel.org/r/20170417024430.21194-3-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 571acee52a32..31a4997b67c6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -897,6 +897,7 @@ void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer);
 void ftrace_init_tracefs_toplevel(struct trace_array *tr,
 				  struct dentry *d_tracer);
 int init_function_trace(void);
+void ftrace_pid_follow_fork(struct trace_array *tr, bool enable);
 #else
 static inline int ftrace_trace_task(struct trace_array *tr)
 {
@@ -916,6 +917,7 @@ static inline void ftrace_reset_array_ops(struct trace_array *tr) { }
 static inline void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d) { }
 static inline void ftrace_init_tracefs_toplevel(struct trace_array *tr, struct dentry *d) { }
 static inline int init_function_trace(void) { return 0; }
+static inline void ftrace_pid_follow_fork(struct trace_array *tr, bool enable) { }
 /* ftace_func_t type is not defined, use macro instead of static inline */
 #define ftrace_init_array_ops(tr, func) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER */
@@ -989,11 +991,13 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 
 #ifdef CONFIG_FUNCTION_TRACER
 # define FUNCTION_FLAGS						\
-		C(FUNCTION,		"function-trace"),
+		C(FUNCTION,		"function-trace"),	\
+		C(FUNC_FORK,		"function-fork"),
 # define FUNCTION_DEFAULT_FLAGS		TRACE_ITER_FUNCTION
 #else
 # define FUNCTION_FLAGS
 # define FUNCTION_DEFAULT_FLAGS		0UL
+# define TRACE_ITER_FUNC_FORK		0UL
 #endif
 
 #ifdef CONFIG_STACKTRACE

commit d879d0b8c183aabeb9a65eba91f3f9e3c7e7b905
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Mon Apr 17 11:44:27 2017 +0900

    ftrace: Fix function pid filter on instances
    
    When function tracer has a pid filter, it adds a probe to sched_switch
    to track if current task can be ignored.  The probe checks the
    ftrace_ignore_pid from current tr to filter tasks.  But it misses to
    delete the probe when removing an instance so that it can cause a crash
    due to the invalid tr pointer (use-after-free).
    
    This is easily reproducible with the following:
    
      # cd /sys/kernel/debug/tracing
      # mkdir instances/buggy
      # echo $$ > instances/buggy/set_ftrace_pid
      # rmdir instances/buggy
    
      ============================================================================
      BUG: KASAN: use-after-free in ftrace_filter_pid_sched_switch_probe+0x3d/0x90
      Read of size 8 by task kworker/0:1/17
      CPU: 0 PID: 17 Comm: kworker/0:1 Tainted: G    B           4.11.0-rc3  #198
      Call Trace:
       dump_stack+0x68/0x9f
       kasan_object_err+0x21/0x70
       kasan_report.part.1+0x22b/0x500
       ? ftrace_filter_pid_sched_switch_probe+0x3d/0x90
       kasan_report+0x25/0x30
       __asan_load8+0x5e/0x70
       ftrace_filter_pid_sched_switch_probe+0x3d/0x90
       ? fpid_start+0x130/0x130
       __schedule+0x571/0xce0
       ...
    
    To fix it, use ftrace_clear_pids() to unregister the probe.  As
    instance_rmdir() already updated ftrace codes, it can just free the
    filter safely.
    
    Link: http://lkml.kernel.org/r/20170417024430.21194-2-namhyung@kernel.org
    
    Fixes: 0c8916c34203 ("tracing: Add rmdir to remove multibuffer instances")
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ae1cce91fead..d19d52d600d6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -896,6 +896,7 @@ int using_ftrace_ops_list_func(void);
 void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer);
 void ftrace_init_tracefs_toplevel(struct trace_array *tr,
 				  struct dentry *d_tracer);
+void ftrace_clear_pids(struct trace_array *tr);
 #else
 static inline int ftrace_trace_task(struct trace_array *tr)
 {
@@ -914,6 +915,7 @@ ftrace_init_global_array_ops(struct trace_array *tr) { }
 static inline void ftrace_reset_array_ops(struct trace_array *tr) { }
 static inline void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d) { }
 static inline void ftrace_init_tracefs_toplevel(struct trace_array *tr, struct dentry *d) { }
+static inline void ftrace_clear_pids(struct trace_array *tr) { }
 /* ftace_func_t type is not defined, use macro instead of static inline */
 #define ftrace_init_array_ops(tr, func) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER */

commit dbeafd0d6131d0f6ae8cd7551f5f4bf8c54aa49a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Mar 3 13:48:42 2017 -0500

    ftrace: Have function tracing start in early boot up
    
    Register the function tracer right after the tracing buffers are initialized
    in early boot up. This will allow function tracing to begin early if it is
    enabled via the kernel command line.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ae1cce91fead..571acee52a32 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -896,6 +896,7 @@ int using_ftrace_ops_list_func(void);
 void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer);
 void ftrace_init_tracefs_toplevel(struct trace_array *tr,
 				  struct dentry *d_tracer);
+int init_function_trace(void);
 #else
 static inline int ftrace_trace_task(struct trace_array *tr)
 {
@@ -914,6 +915,7 @@ ftrace_init_global_array_ops(struct trace_array *tr) { }
 static inline void ftrace_reset_array_ops(struct trace_array *tr) { }
 static inline void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d) { }
 static inline void ftrace_init_tracefs_toplevel(struct trace_array *tr, struct dentry *d) { }
+static inline int init_function_trace(void) { return 0; }
 /* ftace_func_t type is not defined, use macro instead of static inline */
 #define ftrace_init_array_ops(tr, func) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER */

commit eb583cd484f9a76c3716c0539bd06340943eeff9
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Mon Jan 23 13:24:45 2017 +0100

    tracing: Use modern function declaration
    
    We get a lot of harmless warnings about this header file at W=1 level
    because of an unusual function declaration:
    
    kernel/trace/trace.h:766:1: error: 'inline' is not at beginning of declaration [-Werror=old-style-declaration]
    
    This moves the inline statement where it normally belongs, avoiding the
    warning.
    
    Link: http://lkml.kernel.org/r/20170123122521.3389010-1-arnd@arndb.de
    
    Fixes: 4046bf023b06 ("ftrace: Expose ftrace_hash_empty and ftrace_lookup_ip")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d2d068b36341..ae1cce91fead 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -763,7 +763,7 @@ struct ftrace_hash {
 struct ftrace_func_entry *
 ftrace_lookup_ip(struct ftrace_hash *hash, unsigned long ip);
 
-static bool __always_inline ftrace_hash_empty(struct ftrace_hash *hash)
+static __always_inline bool ftrace_hash_empty(struct ftrace_hash *hash)
 {
 	return !hash || !hash->count;
 }

commit 4c7384131c8d343c0bf79abac3b3e78596d85b10
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Feb 8 13:36:37 2017 -0500

    tracing: Have COMM event filter key be treated as a string
    
    The GLOB operation "~" should be able to work with the COMM filter key in
    order to trace programs with a glob. For example
    
      echo 'COMM ~ "systemd*"' > events/syscalls/filter
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index afbec961eab1..d2d068b36341 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1312,7 +1312,8 @@ static inline bool is_string_field(struct ftrace_event_field *field)
 {
 	return field->filter_type == FILTER_DYN_STRING ||
 	       field->filter_type == FILTER_STATIC_STRING ||
-	       field->filter_type == FILTER_PTR_STRING;
+	       field->filter_type == FILTER_PTR_STRING ||
+	       field->filter_type == FILTER_COMM;
 }
 
 static inline bool is_function_field(struct ftrace_event_field *field)

commit b9b0c831bed2682c2e3e9f5420fb6985549ef020
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Fri Jan 20 11:44:47 2017 +0900

    ftrace: Convert graph filter to use hash tables
    
    Use ftrace_hash instead of a static array of a fixed size.  This is
    useful when a graph filter pattern matches to a large number of
    functions.  Now hash lookup is done with preemption disabled to protect
    from the hash being changed/freed.
    
    Link: http://lkml.kernel.org/r/20170120024447.26097-3-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9001460291bb..afbec961eab1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -803,51 +803,49 @@ extern void __trace_graph_return(struct trace_array *tr,
 				 unsigned long flags, int pc);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
-/* TODO: make this variable */
-#define FTRACE_GRAPH_MAX_FUNCS		32
-extern int ftrace_graph_count;
-extern unsigned long ftrace_graph_funcs[FTRACE_GRAPH_MAX_FUNCS];
-extern int ftrace_graph_notrace_count;
-extern unsigned long ftrace_graph_notrace_funcs[FTRACE_GRAPH_MAX_FUNCS];
+extern struct ftrace_hash *ftrace_graph_hash;
+extern struct ftrace_hash *ftrace_graph_notrace_hash;
 
 static inline int ftrace_graph_addr(unsigned long addr)
 {
-	int i;
-
-	if (!ftrace_graph_count)
-		return 1;
-
-	for (i = 0; i < ftrace_graph_count; i++) {
-		if (addr == ftrace_graph_funcs[i]) {
-			/*
-			 * If no irqs are to be traced, but a set_graph_function
-			 * is set, and called by an interrupt handler, we still
-			 * want to trace it.
-			 */
-			if (in_irq())
-				trace_recursion_set(TRACE_IRQ_BIT);
-			else
-				trace_recursion_clear(TRACE_IRQ_BIT);
-			return 1;
-		}
+	int ret = 0;
+
+	preempt_disable_notrace();
+
+	if (ftrace_hash_empty(ftrace_graph_hash)) {
+		ret = 1;
+		goto out;
 	}
 
-	return 0;
+	if (ftrace_lookup_ip(ftrace_graph_hash, addr)) {
+		/*
+		 * If no irqs are to be traced, but a set_graph_function
+		 * is set, and called by an interrupt handler, we still
+		 * want to trace it.
+		 */
+		if (in_irq())
+			trace_recursion_set(TRACE_IRQ_BIT);
+		else
+			trace_recursion_clear(TRACE_IRQ_BIT);
+		ret = 1;
+	}
+
+out:
+	preempt_enable_notrace();
+	return ret;
 }
 
 static inline int ftrace_graph_notrace_addr(unsigned long addr)
 {
-	int i;
+	int ret = 0;
 
-	if (!ftrace_graph_notrace_count)
-		return 0;
+	preempt_disable_notrace();
 
-	for (i = 0; i < ftrace_graph_notrace_count; i++) {
-		if (addr == ftrace_graph_notrace_funcs[i])
-			return 1;
-	}
+	if (ftrace_lookup_ip(ftrace_graph_notrace_hash, addr))
+		ret = 1;
 
-	return 0;
+	preempt_enable_notrace();
+	return ret;
 }
 #else
 static inline int ftrace_graph_addr(unsigned long addr)

commit 4046bf023b0647d09704a32d9fe8aecbcee3e4c3
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Fri Jan 20 11:44:46 2017 +0900

    ftrace: Expose ftrace_hash_empty and ftrace_lookup_ip
    
    It will be used when checking graph filter hashes later.
    
    Link: http://lkml.kernel.org/r/20170120024447.26097-2-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    [ Moved ftrace_hash dec and functions outside of FUNCTION_GRAPH define ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1ea51ab53edf..9001460291bb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -753,6 +753,21 @@ enum print_line_t print_trace_line(struct trace_iterator *iter);
 
 extern char trace_find_mark(unsigned long long duration);
 
+struct ftrace_hash {
+	unsigned long		size_bits;
+	struct hlist_head	*buckets;
+	unsigned long		count;
+	struct rcu_head		rcu;
+};
+
+struct ftrace_func_entry *
+ftrace_lookup_ip(struct ftrace_hash *hash, unsigned long ip);
+
+static bool __always_inline ftrace_hash_empty(struct ftrace_hash *hash)
+{
+	return !hash || !hash->count;
+}
+
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 
@@ -787,7 +802,6 @@ extern void __trace_graph_return(struct trace_array *tr,
 				 struct ftrace_graph_ret *trace,
 				 unsigned long flags, int pc);
 
-
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* TODO: make this variable */
 #define FTRACE_GRAPH_MAX_FUNCS		32

commit a5a1d1c2914b5316924c7893eb683a5420ebd3be
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Wed Dec 21 20:32:01 2016 +0100

    clocksource: Use a plain u64 instead of cycle_t
    
    There is no point in having an extra type for extra confusion. u64 is
    unambiguous.
    
    Conversion was done with the following coccinelle script:
    
    @rem@
    @@
    -typedef u64 cycle_t;
    
    @fix@
    typedef cycle_t;
    @@
    -cycle_t
    +u64
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: John Stultz <john.stultz@linaro.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c2234494f40c..1ea51ab53edf 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -159,7 +159,7 @@ struct trace_array_cpu {
 	unsigned long		policy;
 	unsigned long		rt_priority;
 	unsigned long		skipped_entries;
-	cycle_t			preempt_timestamp;
+	u64			preempt_timestamp;
 	pid_t			pid;
 	kuid_t			uid;
 	char			comm[TASK_COMM_LEN];
@@ -177,7 +177,7 @@ struct trace_buffer {
 	struct trace_array		*tr;
 	struct ring_buffer		*buffer;
 	struct trace_array_cpu __percpu	*data;
-	cycle_t				time_start;
+	u64				time_start;
 	int				cpu;
 };
 
@@ -689,7 +689,7 @@ static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
 }
 #endif /* CONFIG_STACKTRACE */
 
-extern cycle_t ftrace_now(int cpu);
+extern u64 ftrace_now(int cpu);
 
 extern void trace_find_cmdline(int pid, char comm[]);
 extern void trace_event_follow_fork(struct trace_array *tr, bool enable);
@@ -736,7 +736,7 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);
-extern unsigned long long ns2usecs(cycle_t nsec);
+extern unsigned long long ns2usecs(u64 nsec);
 extern int
 trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
 extern int

commit 1a41442864e35bff859582fe9c5d051d0b1040ba
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Dec 8 19:28:28 2016 -0500

    tracing/fgraph: Have wakeup and irqsoff tracers ignore graph functions too
    
    Currently both the wakeup and irqsoff traces do not handle set_graph_notrace
    well. The ftrace infrastructure will ignore the return paths of all
    functions leaving them hanging without an end:
    
      # echo '*spin*' > set_graph_notrace
      # cat trace
      [...]
              _raw_spin_lock() {
                preempt_count_add() {
                do_raw_spin_lock() {
              update_rq_clock();
    
    Where the '*spin*' functions should have looked like this:
    
              _raw_spin_lock() {
                preempt_count_add();
                do_raw_spin_lock();
              }
              update_rq_clock();
    
    Instead, have the wakeup and irqsoff tracers ignore the functions that are
    set by the set_graph_notrace like the function_graph tracer does. Move
    the logic in the function_graph tracer into a header to allow wakeup and
    irqsoff tracers to use it as well.
    
    Cc: Namhyung Kim <namhyung.kim@lge.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 37602e722336..c2234494f40c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -846,6 +846,17 @@ static inline int ftrace_graph_notrace_addr(unsigned long addr)
 	return 0;
 }
 #endif /* CONFIG_DYNAMIC_FTRACE */
+
+extern unsigned int fgraph_max_depth;
+
+static inline bool ftrace_graph_ignore_func(struct ftrace_graph_ent *trace)
+{
+	/* trace it when it is-nested-in or is a function enabled. */
+	return !(trace->depth || ftrace_graph_addr(trace->func)) ||
+		(trace->depth < 0) ||
+		(fgraph_max_depth && trace->depth >= fgraph_max_depth);
+}
+
 #else /* CONFIG_FUNCTION_GRAPH_TRACER */
 static inline enum print_line_t
 print_graph_function_flags(struct trace_iterator *iter, u32 flags)

commit 52ffabe3848a1ebd944cdf7801a77247b1cb46d5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 23 20:28:38 2016 -0500

    tracing: Make __buffer_unlock_commit() always_inline
    
    The function __buffer_unlock_commit() is called in a few places outside of
    trace.c. But for the most part, it should really be inlined, as it is in the
    hot path of the trace_events. For the callers outside of trace.c, create a
    new function trace_buffer_unlock_commit_nostack(), as the reason it was used
    was to avoid the stack tracing that trace_buffer_unlock_commit() could do.
    
    Link: http://lkml.kernel.org/r/20161121183700.GW26852@two.firstfloor.org
    
    Reported-by: Andi Kleen <andi@firstfloor.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9294f8606ade..37602e722336 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -602,8 +602,8 @@ struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 struct trace_entry *trace_find_next_entry(struct trace_iterator *iter,
 					  int *ent_cpu, u64 *ent_ts);
 
-void __buffer_unlock_commit(struct ring_buffer *buffer,
-			    struct ring_buffer_event *event);
+void trace_buffer_unlock_commit_nostack(struct ring_buffer *buffer,
+					struct ring_buffer_event *event);
 
 int trace_empty(struct trace_iterator *iter);
 

commit fa32e8557b470f5ff90babc6cbacc61535a81a0f
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Jul 6 15:25:08 2016 -0400

    tracing: Add new trace_marker_raw
    
    A new file is created:
    
     /sys/kernel/debug/tracing/trace_marker_raw
    
    This allows for appications to create data structures and write the binary
    data directly into it, and then read the trace data out from trace_pipe_raw
    into the same type of data structure. This saves on converting numbers into
    ASCII that would be required by trace_marker.
    
    Suggested-by: Olof Johansson <olof@lixom.net>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4b7918902ab8..9294f8606ade 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -40,6 +40,7 @@ enum trace_type {
 	TRACE_BLK,
 	TRACE_BPUTS,
 	TRACE_HWLAT,
+	TRACE_RAW_DATA,
 
 	__TRACE_LAST_TYPE,
 };
@@ -331,6 +332,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct bprint_entry, TRACE_BPRINT);	\
 		IF_ASSIGN(var, ent, struct bputs_entry, TRACE_BPUTS);	\
 		IF_ASSIGN(var, ent, struct hwlat_entry, TRACE_HWLAT);	\
+		IF_ASSIGN(var, ent, struct raw_data_entry, TRACE_RAW_DATA);\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
 			  TRACE_MMIO_RW);				\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\

commit 60f1d5e3bac44b598f67d36062da96c095d2b700
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Wed Oct 5 20:58:15 2016 +0900

    ftrace: Support full glob matching
    
    Use glob_match() to support flexible glob wildcards (*,?)
    and character classes ([) for ftrace.
    Since the full glob matching is slower than the current
    partial matching routines(*pat, pat*, *pat*), this leaves
    those routines and just add MATCH_GLOB for complex glob
    expression.
    
    e.g.
    ----
    [root@localhost tracing]# echo 'sched*group' > set_ftrace_filter
    [root@localhost tracing]# cat set_ftrace_filter
    sched_free_group
    sched_change_group
    sched_create_group
    sched_online_group
    sched_destroy_group
    sched_offline_group
    [root@localhost tracing]# echo '[Ss]y[Ss]_*' > set_ftrace_filter
    [root@localhost tracing]# head set_ftrace_filter
    sys_arch_prctl
    sys_rt_sigreturn
    sys_ioperm
    SyS_iopl
    sys_modify_ldt
    SyS_mmap
    SyS_set_thread_area
    SyS_get_thread_area
    SyS_set_tid_address
    sys_fork
    ----
    
    Link: http://lkml.kernel.org/r/147566869501.29136.6462645009894738056.stgit@devbox
    
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fd24b1f9ac43..4b7918902ab8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -15,6 +15,7 @@
 #include <linux/trace_events.h>
 #include <linux/compiler.h>
 #include <linux/trace_seq.h>
+#include <linux/glob.h>
 
 #ifdef CONFIG_FTRACE_SYSCALLS
 #include <asm/unistd.h>		/* For NR_SYSCALLS	     */
@@ -1257,6 +1258,7 @@ enum regex_type {
 	MATCH_FRONT_ONLY,
 	MATCH_MIDDLE_ONLY,
 	MATCH_END_ONLY,
+	MATCH_GLOB,
 };
 
 struct regex {

commit f971cc9aabc287120bbe7f3f1abe70c13e61ee94
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Sep 7 12:45:09 2016 -0400

    tracing: Have max_latency be defined for HWLAT_TRACER as well
    
    The hwlat tracer uses tr->max_latency, and if it's the only tracer enabled
    that uses it, the build will fail. Add max_latency and its file when the
    hwlat tracer is enabled.
    
    Link: http://lkml.kernel.org/r/d6c3b7eb-ba95-1ffa-0453-464e1e24262a@infradead.org
    
    Reported-by: Randy Dunlap <rdunlap@infradead.org>
    Tested-by: Randy Dunlap <rdunlap@infradead.org>
    Acked-by: Randy Dunlap <rdunlap@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1d866b0c1567..fd24b1f9ac43 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -214,6 +214,8 @@ struct trace_array {
 	 */
 	struct trace_buffer	max_buffer;
 	bool			allocated_snapshot;
+#endif
+#if defined(CONFIG_TRACER_MAX_TRACE) || defined(CONFIG_HWLAT_TRACER)
 	unsigned long		max_latency;
 #endif
 	struct trace_pid_list	__rcu *filtered_pids;

commit e7c15cd8a113335cf7154f027c9c8da1a92238ee
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 23 12:45:36 2016 -0400

    tracing: Added hardware latency tracer
    
    The hardware latency tracer has been in the PREEMPT_RT patch for some time.
    It is used to detect possible SMIs or any other hardware interruptions that
    the kernel is unaware of. Note, NMIs may also be detected, but that may be
    good to note as well.
    
    The logic is pretty simple. It simply creates a thread that spins on a
    single CPU for a specified amount of time (width) within a periodic window
    (window). These numbers may be adjusted by their cooresponding names in
    
       /sys/kernel/tracing/hwlat_detector/
    
    The defaults are window = 1000000 us (1 second)
                     width  =  500000 us (1/2 second)
    
    The loop consists of:
    
            t1 = trace_clock_local();
            t2 = trace_clock_local();
    
    Where trace_clock_local() is a variant of sched_clock().
    
    The difference of t2 - t1 is recorded as the "inner" timestamp and also the
    timestamp  t1 - prev_t2 is recorded as the "outer" timestamp. If either of
    these differences are greater than the time denoted in
    /sys/kernel/tracing/tracing_thresh then it records the event.
    
    When this tracer is started, and tracing_thresh is zero, it changes to the
    default threshold of 10 us.
    
    The hwlat tracer in the PREEMPT_RT patch was originally written by
    Jon Masters. I have modified it quite a bit and turned it into a
    tracer.
    
    Based-on-code-by: Jon Masters <jcm@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f783df416726..1d866b0c1567 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -38,6 +38,7 @@ enum trace_type {
 	TRACE_USER_STACK,
 	TRACE_BLK,
 	TRACE_BPUTS,
+	TRACE_HWLAT,
 
 	__TRACE_LAST_TYPE,
 };
@@ -326,6 +327,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
 		IF_ASSIGN(var, ent, struct bprint_entry, TRACE_BPRINT);	\
 		IF_ASSIGN(var, ent, struct bputs_entry, TRACE_BPUTS);	\
+		IF_ASSIGN(var, ent, struct hwlat_entry, TRACE_HWLAT);	\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
 			  TRACE_MMIO_RW);				\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\
@@ -571,6 +573,7 @@ void tracing_reset_current(int cpu);
 void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 bool tracing_is_disabled(void);
+int tracer_tracing_is_on(struct trace_array *tr);
 struct dentry *trace_create_file(const char *name,
 				 umode_t mode,
 				 struct dentry *parent,

commit a4a551b8f1d4c4ebffd0f49dfef44df3128546f8
Author: Namhyung Kim <namhyung@kernel.org>
Date:   Wed Jun 29 19:56:48 2016 +0900

    ftrace: Reduce size of function graph entries
    
    Currently ftrace_graph_ent{,_entry} and ftrace_graph_ret{,_entry} struct
    can have padding bytes at the end due to alignment in 64-bit data type.
    As these data are recorded so frequently, those paddings waste
    non-negligible space.  As the ring buffer maintains alignment properly
    for each architecture, just to remove the extra padding using 'packed'
    attribute.
    
      ftrace_graph_ent_entry:  24 -> 20
      ftrace_graph_ret_entry:  48 -> 44
    
    Also I moved the 'overrun' field in struct ftrace_graph_ret to minimize
    the padding in the middle.
    
    Tested on x86_64 only.
    
    Link: http://lkml.kernel.org/r/1467197808-13578-1-git-send-email-namhyung@kernel.org
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: linux-arch@vger.kernel.org
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c1de3f493cd3..f783df416726 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -80,6 +80,12 @@ enum trace_type {
 	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print), \
 		     filter)
 
+#undef FTRACE_ENTRY_PACKED
+#define FTRACE_ENTRY_PACKED(name, struct_name, id, tstruct, print,	\
+			    filter)					\
+	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print), \
+		     filter) __packed
+
 #include "trace_entries.h"
 
 /*
@@ -1625,6 +1631,11 @@ int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 #define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print, filter)	\
 	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print), \
 		     filter)
+#undef FTRACE_ENTRY_PACKED
+#define FTRACE_ENTRY_PACKED(call, struct_name, id, tstruct, print, filter) \
+	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print), \
+		     filter)
+
 #include "trace_entries.h"
 
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_FUNCTION_TRACER)

commit 501c2375253c0795048f48368e0b3e8b2f6646dc
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jul 5 10:04:34 2016 -0400

    ftrace: Move toplevel init out of ftrace_init_tracefs()
    
    Commit 345ddcc882d8 ("ftrace: Have set_ftrace_pid use the bitmap like events
    do") placed ftrace_init_tracefs into the instance creation, and encapsulated
    the top level updating with an if conditional, as the top level only gets
    updated at boot up. Unfortunately, this triggers section mismatch errors as
    the init functions are called from a function that can be called later, and
    the section mismatch logic is unaware of the if conditional that would
    prevent it from happening at run time.
    
    To make everyone happy, create a separate ftrace_init_tracefs_toplevel()
    routine that only gets called by init functions, and this will be what calls
    other init functions for the toplevel directory.
    
    Link: http://lkml.kernel.org/r/20160704102139.19cbc0d9@gandalf.local.home
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Reported-by: Arnd Bergmann <arnd@arndb.de>
    Fixes: 345ddcc882d8 ("ftrace: Have set_ftrace_pid use the bitmap like events do")
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index eaee458755a4..c1de3f493cd3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -857,6 +857,8 @@ void ftrace_init_array_ops(struct trace_array *tr, ftrace_func_t func);
 void ftrace_reset_array_ops(struct trace_array *tr);
 int using_ftrace_ops_list_func(void);
 void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer);
+void ftrace_init_tracefs_toplevel(struct trace_array *tr,
+				  struct dentry *d_tracer);
 #else
 static inline int ftrace_trace_task(struct trace_array *tr)
 {
@@ -874,6 +876,7 @@ static inline __init void
 ftrace_init_global_array_ops(struct trace_array *tr) { }
 static inline void ftrace_reset_array_ops(struct trace_array *tr) { }
 static inline void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d) { }
+static inline void ftrace_init_tracefs_toplevel(struct trace_array *tr, struct dentry *d) { }
 /* ftace_func_t type is not defined, use macro instead of static inline */
 #define ftrace_init_array_ops(tr, func) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER */

commit 345ddcc882d8896dcbdcb3e0ee4a415fc23ec8b0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 22 18:11:33 2016 -0400

    ftrace: Have set_ftrace_pid use the bitmap like events do
    
    Convert set_ftrace_pid to use the bitmap like set_event_pid does. This
    allows for instances to use the pid filtering as well, and will allow for
    function-fork option to set if the children of a traced function should be
    traced or not.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a4dce1ef9e03..eaee458755a4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -156,6 +156,9 @@ struct trace_array_cpu {
 	char			comm[TASK_COMM_LEN];
 
 	bool			ignore_pid;
+#ifdef CONFIG_FUNCTION_TRACER
+	bool			ftrace_ignore_pid;
+#endif
 };
 
 struct tracer;
@@ -247,6 +250,7 @@ struct trace_array {
 	int			ref;
 #ifdef CONFIG_FUNCTION_TRACER
 	struct ftrace_ops	*ops;
+	struct trace_pid_list	__rcu *function_pids;
 	/* function tracing enabled */
 	int			function_enabled;
 #endif
@@ -840,12 +844,9 @@ extern struct list_head ftrace_pids;
 
 #ifdef CONFIG_FUNCTION_TRACER
 extern bool ftrace_filter_param __initdata;
-static inline int ftrace_trace_task(struct task_struct *task)
+static inline int ftrace_trace_task(struct trace_array *tr)
 {
-	if (list_empty(&ftrace_pids))
-		return 1;
-
-	return test_tsk_trace_trace(task);
+	return !this_cpu_read(tr->trace_buffer.data->ftrace_ignore_pid);
 }
 extern int ftrace_is_dead(void);
 int ftrace_create_function_files(struct trace_array *tr,
@@ -855,8 +856,9 @@ void ftrace_init_global_array_ops(struct trace_array *tr);
 void ftrace_init_array_ops(struct trace_array *tr, ftrace_func_t func);
 void ftrace_reset_array_ops(struct trace_array *tr);
 int using_ftrace_ops_list_func(void);
+void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d_tracer);
 #else
-static inline int ftrace_trace_task(struct task_struct *task)
+static inline int ftrace_trace_task(struct trace_array *tr)
 {
 	return 1;
 }
@@ -871,6 +873,7 @@ static inline void ftrace_destroy_function_files(struct trace_array *tr) { }
 static inline __init void
 ftrace_init_global_array_ops(struct trace_array *tr) { }
 static inline void ftrace_reset_array_ops(struct trace_array *tr) { }
+static inline void ftrace_init_tracefs(struct trace_array *tr, struct dentry *d) { }
 /* ftace_func_t type is not defined, use macro instead of static inline */
 #define ftrace_init_array_ops(tr, func) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER */

commit 76c813e26606d35ea9d8d6f96e646b3944c730a9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Apr 21 11:35:30 2016 -0400

    tracing: Move pid_list write processing into its own function
    
    The addition of PIDs into a pid_list via the write operation of
    set_event_pid is a bit complex. The same operation will be needed for
    function tracing pids. Move the code into its own generic function in
    trace.c, so that we can avoid duplication of this code.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 45442d5842f2..a4dce1ef9e03 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -629,6 +629,9 @@ extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 extern unsigned long tracing_thresh;
 
 /* PID filtering */
+
+extern int pid_max;
+
 bool trace_find_filtered_pid(struct trace_pid_list *filtered_pids,
 			     pid_t search_pid);
 bool trace_ignore_this_task(struct trace_pid_list *filtered_pids,
@@ -639,6 +642,10 @@ void trace_filter_add_remove_task(struct trace_pid_list *pid_list,
 void *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos);
 void *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos);
 int trace_pid_show(struct seq_file *m, void *v);
+void trace_free_pid_list(struct trace_pid_list *pid_list);
+int trace_pid_write(struct trace_pid_list *filtered_pids,
+		    struct trace_pid_list **new_pid_list,
+		    const char __user *ubuf, size_t cnt);
 
 #ifdef CONFIG_TRACER_MAX_TRACE
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);

commit 5cc8976bd52153678ca37cc1e3000833b20276f3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 20 15:19:54 2016 -0400

    tracing: Move the pid_list seq_file functions to be global
    
    To allow other aspects of ftrace to use the pid_list logic, we need to reuse
    the seq_file functions. Making the generic part into functions that can be
    called by other files will help in this regard.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 172330891c6d..45442d5842f2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -636,6 +636,9 @@ bool trace_ignore_this_task(struct trace_pid_list *filtered_pids,
 void trace_filter_add_remove_task(struct trace_pid_list *pid_list,
 				  struct task_struct *self,
 				  struct task_struct *task);
+void *trace_pid_next(struct trace_pid_list *pid_list, void *v, loff_t *pos);
+void *trace_pid_start(struct trace_pid_list *pid_list, loff_t *pos);
+int trace_pid_show(struct seq_file *m, void *v);
 
 #ifdef CONFIG_TRACER_MAX_TRACE
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);

commit 4e267db135c44d0b18e553899fe7df32b89211a5
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Apr 14 07:38:13 2016 -0400

    tracing: Make the pid filtering helper functions global
    
    Make the functions used for pid filtering global for tracing, such that the
    function tracer can use the pid code as well.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5167c366d6b7..172330891c6d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -628,6 +628,15 @@ extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 
 extern unsigned long tracing_thresh;
 
+/* PID filtering */
+bool trace_find_filtered_pid(struct trace_pid_list *filtered_pids,
+			     pid_t search_pid);
+bool trace_ignore_this_task(struct trace_pid_list *filtered_pids,
+			    struct task_struct *task);
+void trace_filter_add_remove_task(struct trace_pid_list *pid_list,
+				  struct task_struct *self,
+				  struct task_struct *task);
+
 #ifdef CONFIG_TRACER_MAX_TRACE
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,

commit 0fc1b09ff1ff404ddf753f5ffa5cd0adc8fdcdc9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 3 17:15:43 2016 -0400

    tracing: Use temp buffer when filtering events
    
    Filtering of events requires the data to be written to the ring buffer
    before it can be decided to filter or not. This is because the parameters of
    the filter are based on the result that is written to the ring buffer and
    not on the parameters that are passed into the trace functions.
    
    The ftrace ring buffer is optimized for writing into the ring buffer and
    committing. The discard procedure used when filtering decides the event
    should be discarded is much more heavy weight. Thus, using a temporary
    filter when filtering events can speed things up drastically.
    
    Without a temp buffer we have:
    
     # trace-cmd start -p nop
     # perf stat -r 10 hackbench 50
           0.790706626 seconds time elapsed ( +-  0.71% )
    
     # trace-cmd start -e all
     # perf stat -r 10 hackbench 50
           1.566904059 seconds time elapsed ( +-  0.27% )
    
     # trace-cmd start -e all -f 'common_preempt_count==20'
     # perf stat -r 10 hackbench 50
           1.690598511 seconds time elapsed ( +-  0.19% )
    
     # trace-cmd start -e all -f 'common_preempt_count!=20'
     # perf stat -r 10 hackbench 50
           1.707486364 seconds time elapsed ( +-  0.30% )
    
    The first run above is without any tracing, just to get a based figure.
    hackbench takes ~0.79 seconds to run on the system.
    
    The second run enables tracing all events where nothing is filtered. This
    increases the time by 100% and hackbench takes 1.57 seconds to run.
    
    The third run filters all events where the preempt count will equal "20"
    (this should never happen) thus all events are discarded. This takes 1.69
    seconds to run. This is 10% slower than just committing the events!
    
    The last run enables all events and filters where the filter will commit all
    events, and this takes 1.70 seconds to run. The filtering overhead is
    approximately 10%. Thus, the discard and commit of an event from the ring
    buffer may be about the same time.
    
    With this patch, the numbers change:
    
     # trace-cmd start -p nop
     # perf stat -r 10 hackbench 50
           0.778233033 seconds time elapsed ( +-  0.38% )
    
     # trace-cmd start -e all
     # perf stat -r 10 hackbench 50
           1.582102692 seconds time elapsed ( +-  0.28% )
    
     # trace-cmd start -e all -f 'common_preempt_count==20'
     # perf stat -r 10 hackbench 50
           1.309230710 seconds time elapsed ( +-  0.22% )
    
     # trace-cmd start -e all -f 'common_preempt_count!=20'
     # perf stat -r 10 hackbench 50
           1.786001924 seconds time elapsed ( +-  0.20% )
    
    The first run is again the base with no tracing.
    
    The second run is all tracing with no filtering. It is a little slower, but
    that may be well within the noise.
    
    The third run shows that discarding all events only took 1.3 seconds. This
    is a speed up of 23%! The discard is much faster than even the commit.
    
    The one downside is shown in the last run. Events that are not discarded by
    the filter will take longer to add, this is due to the extra copy of the
    event.
    
    Cc: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 10156a09103f..5167c366d6b7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1083,6 +1083,23 @@ static inline void trace_buffer_unlock_commit(struct trace_array *tr,
 	trace_buffer_unlock_commit_regs(tr, buffer, event, flags, pc, NULL);
 }
 
+DECLARE_PER_CPU(struct ring_buffer_event *, trace_buffered_event);
+DECLARE_PER_CPU(int, trace_buffered_event_cnt);
+void trace_buffered_event_disable(void);
+void trace_buffered_event_enable(void);
+
+static inline void
+__trace_event_discard_commit(struct ring_buffer *buffer,
+			     struct ring_buffer_event *event)
+{
+	if (this_cpu_read(trace_buffered_event) == event) {
+		/* Simply release the temp buffer */
+		this_cpu_dec(trace_buffered_event_cnt);
+		return;
+	}
+	ring_buffer_discard_commit(buffer, event);
+}
+
 /*
  * Helper function for event_trigger_unlock_commit{_regs}().
  * If there are event triggers attached to this event that requires
@@ -1111,7 +1128,7 @@ __event_trigger_test_discard(struct trace_event_file *file,
 	if (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags) ||
 	    (unlikely(file->flags & EVENT_FILE_FL_FILTERED) &&
 	     !filter_match_preds(file->filter, entry))) {
-		ring_buffer_discard_commit(buffer, event);
+		__trace_event_discard_commit(buffer, event);
 		return true;
 	}
 

commit 33fddff24d05d71f97722cb7deec4964d39d10dc
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 29 17:44:01 2016 -0400

    tracing: Have trace_buffer_unlock_commit() call the _regs version with NULL
    
    There's no real difference between trace_buffer_unlock_commit() and
    trace_buffer_unlock_commit_regs() except that the former passes NULL to
    ftrace_stack_trace() instead of regs. Have the former be a static inline of
    the latter which passes NULL for regs.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index bd5ae56dec7a..10156a09103f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1069,15 +1069,20 @@ extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
 				     struct ring_buffer *buffer,
 				     struct ring_buffer_event *event);
 
-void trace_buffer_unlock_commit(struct trace_array *tr,
-				struct ring_buffer *buffer,
-				struct ring_buffer_event *event,
-				unsigned long flags, int pc);
 void trace_buffer_unlock_commit_regs(struct trace_array *tr,
 				     struct ring_buffer *buffer,
 				     struct ring_buffer_event *event,
 				     unsigned long flags, int pc,
 				     struct pt_regs *regs);
+
+static inline void trace_buffer_unlock_commit(struct trace_array *tr,
+					      struct ring_buffer *buffer,
+					      struct ring_buffer_event *event,
+					      unsigned long flags, int pc)
+{
+	trace_buffer_unlock_commit_regs(tr, buffer, event, flags, pc, NULL);
+}
+
 /*
  * Helper function for event_trigger_unlock_commit{_regs}().
  * If there are event triggers attached to this event that requires

commit fa66ddb870ca022342fe6d1312ef76d2f7233a1d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Apr 28 12:04:13 2016 -0400

    tracing: Move trace_buffer_unlock_commit{_regs}() to local header
    
    The functions trace_buffer_unlock_commit() and the _regs() version are only
    used within the kernel/trace directory. Move them to the local header and
    remove the export as well.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0862e7559548..bd5ae56dec7a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1068,6 +1068,16 @@ struct trace_subsystem_dir {
 extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
 				     struct ring_buffer *buffer,
 				     struct ring_buffer_event *event);
+
+void trace_buffer_unlock_commit(struct trace_array *tr,
+				struct ring_buffer *buffer,
+				struct ring_buffer_event *event,
+				unsigned long flags, int pc);
+void trace_buffer_unlock_commit_regs(struct trace_array *tr,
+				     struct ring_buffer *buffer,
+				     struct ring_buffer_event *event,
+				     unsigned long flags, int pc,
+				     struct pt_regs *regs);
 /*
  * Helper function for event_trigger_unlock_commit{_regs}().
  * If there are event triggers attached to this event that requires

commit 9cbb1506ab2db987c160e7fc50665bf47b5b6fa1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 27 11:09:42 2016 -0400

    tracing: Fold filter_check_discard() into its only user
    
    The function filter_check_discard() is small and only called by one user,
    its code can be folded into that one caller and make the code a bit less
    comlplex.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ee8691c66bfe..0862e7559548 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1065,9 +1065,6 @@ struct trace_subsystem_dir {
 	int				nr_events;
 };
 
-extern int filter_check_discard(struct trace_event_file *file, void *rec,
-				struct ring_buffer *buffer,
-				struct ring_buffer_event *event);
 extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
 				     struct ring_buffer *buffer,
 				     struct ring_buffer_event *event);
@@ -1096,12 +1093,14 @@ __event_trigger_test_discard(struct trace_event_file *file,
 	if (eflags & EVENT_FILE_FL_TRIGGER_COND)
 		*tt = event_triggers_call(file, entry);
 
-	if (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags))
+	if (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags) ||
+	    (unlikely(file->flags & EVENT_FILE_FL_FILTERED) &&
+	     !filter_match_preds(file->filter, entry))) {
 		ring_buffer_discard_commit(buffer, event);
-	else if (!filter_check_discard(file, entry, buffer, event))
-		return false;
+		return true;
+	}
 
-	return true;
+	return false;
 }
 
 /**

commit 65da9a0a3bf2202c2432f42d41eb908f2fa30579
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 27 10:13:46 2016 -0400

    tracing: Make filter_check_discard() local
    
    Nothing outside of the tracing directory calls filter_check_discard() or
    check_filter_check_discard(). They should not be called by modules. Move
    their prototypes into the local tracing header and remove their
    EXPORT_SYMBOL() macros.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c0eac7b1e5a6..ee8691c66bfe 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1065,6 +1065,12 @@ struct trace_subsystem_dir {
 	int				nr_events;
 };
 
+extern int filter_check_discard(struct trace_event_file *file, void *rec,
+				struct ring_buffer *buffer,
+				struct ring_buffer_event *event);
+extern int call_filter_check_discard(struct trace_event_call *call, void *rec,
+				     struct ring_buffer *buffer,
+				     struct ring_buffer_event *event);
 /*
  * Helper function for event_trigger_unlock_commit{_regs}().
  * If there are event triggers attached to this event that requires

commit dad56ee742a3abbb5d9e8108f8537d412bff3f57
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Apr 26 21:22:22 2016 -0400

    tracing: Move event_trigger_unlock_commit{_regs}() to local header
    
    The functions event_trigger_unlock_commit() and
    event_trigger_unlock_commit_regs() are no longer used outside the tracing
    system. Move them out of the generic headers and into the local one.
    
    Along with __event_trigger_test_discard() that is only used by them.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 727a3d28bce5..c0eac7b1e5a6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1065,6 +1065,100 @@ struct trace_subsystem_dir {
 	int				nr_events;
 };
 
+/*
+ * Helper function for event_trigger_unlock_commit{_regs}().
+ * If there are event triggers attached to this event that requires
+ * filtering against its fields, then they wil be called as the
+ * entry already holds the field information of the current event.
+ *
+ * It also checks if the event should be discarded or not.
+ * It is to be discarded if the event is soft disabled and the
+ * event was only recorded to process triggers, or if the event
+ * filter is active and this event did not match the filters.
+ *
+ * Returns true if the event is discarded, false otherwise.
+ */
+static inline bool
+__event_trigger_test_discard(struct trace_event_file *file,
+			     struct ring_buffer *buffer,
+			     struct ring_buffer_event *event,
+			     void *entry,
+			     enum event_trigger_type *tt)
+{
+	unsigned long eflags = file->flags;
+
+	if (eflags & EVENT_FILE_FL_TRIGGER_COND)
+		*tt = event_triggers_call(file, entry);
+
+	if (test_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags))
+		ring_buffer_discard_commit(buffer, event);
+	else if (!filter_check_discard(file, entry, buffer, event))
+		return false;
+
+	return true;
+}
+
+/**
+ * event_trigger_unlock_commit - handle triggers and finish event commit
+ * @file: The file pointer assoctiated to the event
+ * @buffer: The ring buffer that the event is being written to
+ * @event: The event meta data in the ring buffer
+ * @entry: The event itself
+ * @irq_flags: The state of the interrupts at the start of the event
+ * @pc: The state of the preempt count at the start of the event.
+ *
+ * This is a helper function to handle triggers that require data
+ * from the event itself. It also tests the event against filters and
+ * if the event is soft disabled and should be discarded.
+ */
+static inline void
+event_trigger_unlock_commit(struct trace_event_file *file,
+			    struct ring_buffer *buffer,
+			    struct ring_buffer_event *event,
+			    void *entry, unsigned long irq_flags, int pc)
+{
+	enum event_trigger_type tt = ETT_NONE;
+
+	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
+		trace_buffer_unlock_commit(file->tr, buffer, event, irq_flags, pc);
+
+	if (tt)
+		event_triggers_post_call(file, tt, entry);
+}
+
+/**
+ * event_trigger_unlock_commit_regs - handle triggers and finish event commit
+ * @file: The file pointer assoctiated to the event
+ * @buffer: The ring buffer that the event is being written to
+ * @event: The event meta data in the ring buffer
+ * @entry: The event itself
+ * @irq_flags: The state of the interrupts at the start of the event
+ * @pc: The state of the preempt count at the start of the event.
+ *
+ * This is a helper function to handle triggers that require data
+ * from the event itself. It also tests the event against filters and
+ * if the event is soft disabled and should be discarded.
+ *
+ * Same as event_trigger_unlock_commit() but calls
+ * trace_buffer_unlock_commit_regs() instead of trace_buffer_unlock_commit().
+ */
+static inline void
+event_trigger_unlock_commit_regs(struct trace_event_file *file,
+				 struct ring_buffer *buffer,
+				 struct ring_buffer_event *event,
+				 void *entry, unsigned long irq_flags, int pc,
+				 struct pt_regs *regs)
+{
+	enum event_trigger_type tt = ETT_NONE;
+
+	if (!__event_trigger_test_discard(file, buffer, event, entry, &tt))
+		trace_buffer_unlock_commit_regs(file->tr, buffer, event,
+						irq_flags, pc, regs);
+
+	if (tt)
+		event_triggers_post_call(file, tt, entry);
+}
+
 #define FILTER_PRED_INVALID	((unsigned short)-1)
 #define FILTER_PRED_IS_RIGHT	(1 << 15)
 #define FILTER_PRED_FOLD	(1 << 15)

commit db1388b4ffa9e31e9ff0abacc3bdb121bec8c688
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Mar 3 12:54:58 2016 -0600

    tracing: Add support for named triggers
    
    Named triggers are sets of triggers that share a common set of trigger
    data.  An example of functionality that could benefit from this type
    of capability would be a set of inlined probes that would each
    contribute event counts, for example, to a shared counter data
    structure.
    
    The first named trigger registered with a given name owns the common
    trigger data that the others subsequently registered with the same
    name will reference.  The functions defined here allow users to add,
    delete, and find named triggers.
    
    It also adds functions to pause and unpause named triggers; since
    named triggers act upon common data, they should also be paused and
    unpaused as a group.
    
    Link: http://lkml.kernel.org/r/c09ff648360f65b10a3e321eddafe18060b4a04f.1457029949.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cab1f4bfe85b..727a3d28bce5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1184,7 +1184,11 @@ struct event_trigger_data {
 	char				*filter_str;
 	void				*private_data;
 	bool				paused;
+	bool				paused_tmp;
 	struct list_head		list;
+	char				*name;
+	struct list_head		named_list;
+	struct event_trigger_data	*named_data;
 };
 
 /* Avoid typos */
@@ -1227,6 +1231,15 @@ extern void unregister_trigger(char *glob, struct event_trigger_ops *ops,
 extern int set_trigger_filter(char *filter_str,
 			      struct event_trigger_data *trigger_data,
 			      struct trace_event_file *file);
+extern struct event_trigger_data *find_named_trigger(const char *name);
+extern bool is_named_trigger(struct event_trigger_data *test);
+extern int save_named_trigger(const char *name,
+			      struct event_trigger_data *data);
+extern void del_named_trigger(struct event_trigger_data *data);
+extern void pause_named_trigger(struct event_trigger_data *data);
+extern void unpause_named_trigger(struct event_trigger_data *data);
+extern void set_named_trigger_data(struct event_trigger_data *data,
+				   struct event_trigger_data *named_data);
 extern int register_event_command(struct event_command *cmd);
 extern int unregister_event_command(struct event_command *cmd);
 extern int register_trigger_hist_enable_disable_cmds(void);

commit d0bad49bb0a094a1beb06640785f95cb256b7272
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Mar 3 12:54:55 2016 -0600

    tracing: Add enable_hist/disable_hist triggers
    
    Similar to enable_event/disable_event triggers, these triggers enable
    and disable the aggregation of events into maps rather than enabling
    and disabling their writing into the trace buffer.
    
    They can be used to automatically start and stop hist triggers based
    on a matching filter condition.
    
    If there's a paused hist trigger on system:event, the following would
    start it when the filter condition was hit:
    
      # echo enable_hist:system:event [ if filter] > event/trigger
    
    And the following would disable a running system:event hist trigger:
    
      # echo disable_hist:system:event [ if filter] > event/trigger
    
    See Documentation/trace/events.txt for real examples.
    
    Link: http://lkml.kernel.org/r/f812f086e52c8b7c8ad5443487375e03c96a601f.1457029949.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 505f8a45f426..cab1f4bfe85b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1166,8 +1166,10 @@ extern const struct file_operations event_hist_fops;
 
 #ifdef CONFIG_HIST_TRIGGERS
 extern int register_trigger_hist_cmd(void);
+extern int register_trigger_hist_enable_disable_cmds(void);
 #else
 static inline int register_trigger_hist_cmd(void) { return 0; }
+static inline int register_trigger_hist_enable_disable_cmds(void) { return 0; }
 #endif
 
 extern int register_trigger_cmds(void);
@@ -1185,6 +1187,34 @@ struct event_trigger_data {
 	struct list_head		list;
 };
 
+/* Avoid typos */
+#define ENABLE_EVENT_STR	"enable_event"
+#define DISABLE_EVENT_STR	"disable_event"
+#define ENABLE_HIST_STR		"enable_hist"
+#define DISABLE_HIST_STR	"disable_hist"
+
+struct enable_trigger_data {
+	struct trace_event_file		*file;
+	bool				enable;
+	bool				hist;
+};
+
+extern int event_enable_trigger_print(struct seq_file *m,
+				      struct event_trigger_ops *ops,
+				      struct event_trigger_data *data);
+extern void event_enable_trigger_free(struct event_trigger_ops *ops,
+				      struct event_trigger_data *data);
+extern int event_enable_trigger_func(struct event_command *cmd_ops,
+				     struct trace_event_file *file,
+				     char *glob, char *cmd, char *param);
+extern int event_enable_register_trigger(char *glob,
+					 struct event_trigger_ops *ops,
+					 struct event_trigger_data *data,
+					 struct trace_event_file *file);
+extern void event_enable_unregister_trigger(char *glob,
+					    struct event_trigger_ops *ops,
+					    struct event_trigger_data *test,
+					    struct trace_event_file *file);
 extern void trigger_data_free(struct event_trigger_data *data);
 extern int event_trigger_init(struct event_trigger_ops *ops,
 			      struct event_trigger_data *data);
@@ -1198,6 +1228,8 @@ extern int set_trigger_filter(char *filter_str,
 			      struct event_trigger_data *trigger_data,
 			      struct trace_event_file *file);
 extern int register_event_command(struct event_command *cmd);
+extern int unregister_event_command(struct event_command *cmd);
+extern int register_trigger_hist_enable_disable_cmds(void);
 
 /**
  * struct event_trigger_ops - callbacks for trace event triggers

commit 7ef224d1d0e3a1ade02d02c01ce1dcffb736d2c3
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Mar 3 12:54:42 2016 -0600

    tracing: Add 'hist' event trigger command
    
    'hist' triggers allow users to continually aggregate trace events,
    which can then be viewed afterwards by simply reading a 'hist' file
    containing the aggregation in a human-readable format.
    
    The basic idea is very simple and boils down to a mechanism whereby
    trace events, rather than being exhaustively dumped in raw form and
    viewed directly, are automatically 'compressed' into meaningful tables
    completely defined by the user.
    
    This is done strictly via single-line command-line commands and
    without the aid of any kind of programming language or interpreter.
    
    A surprising number of typical use cases can be accomplished by users
    via this simple mechanism.  In fact, a large number of the tasks that
    users typically do using the more complicated script-based tracing
    tools, at least during the initial stages of an investigation, can be
    accomplished by simply specifying a set of keys and values to be used
    in the creation of a hash table.
    
    The Linux kernel trace event subsystem happens to provide an extensive
    list of keys and values ready-made for such a purpose in the form of
    the event format files associated with each trace event.  By simply
    consulting the format file for field names of interest and by plugging
    them into the hist trigger command, users can create an endless number
    of useful aggregations to help with investigating various properties
    of the system.  See Documentation/trace/events.txt for examples.
    
    hist triggers are implemented on top of the existing event trigger
    infrastructure, and as such are consistent with the existing triggers
    from a user's perspective as well.
    
    The basic syntax follows the existing trigger syntax.  Users start an
    aggregation by writing a 'hist' trigger to the event of interest's
    trigger file:
    
      # echo hist:keys=xxx [ if filter] > event/trigger
    
    Once a hist trigger has been set up, by default it continually
    aggregates every matching event into a hash table using the event key
    and a value field named 'hitcount'.
    
    To view the aggregation at any point in time, simply read the 'hist'
    file in the same directory as the 'trigger' file:
    
      # cat event/hist
    
    The detailed syntax provides additional options for user control, and
    is described exhaustively in Documentation/trace/events.txt and in the
    virtual tracing/README file in the tracing subsystem.
    
    Link: http://lkml.kernel.org/r/72d263b5e1853fe9c314953b65833c3aa75479f2.1457029949.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2525042760e6..505f8a45f426 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1162,6 +1162,13 @@ extern struct mutex event_mutex;
 extern struct list_head ftrace_events;
 
 extern const struct file_operations event_trigger_fops;
+extern const struct file_operations event_hist_fops;
+
+#ifdef CONFIG_HIST_TRIGGERS
+extern int register_trigger_hist_cmd(void);
+#else
+static inline int register_trigger_hist_cmd(void) { return 0; }
+#endif
 
 extern int register_trigger_cmds(void);
 extern void clear_event_triggers(struct trace_array *tr);

commit c37775d57830a36382a9774bb84eca4ce3d019cc
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Apr 13 16:59:18 2016 -0400

    tracing: Add infrastructure to allow set_event_pid to follow children
    
    Add the infrastructure needed to have the PIDs in set_event_pid to
    automatically add PIDs of the children of the tasks that have their PIDs in
    set_event_pid. This will also remove PIDs from set_event_pid when a task
    exits
    
    This is implemented by adding hooks into the fork and exit tracepoints. On
    fork, the PIDs are added to the list, and on exit, they are removed.
    
    Add a new option called event_fork that when set, PIDs in set_event_pid will
    automatically get their children PIDs added when they fork, as well as any
    task that exits will have its PID removed from set_event_pid.
    
    This works for instances as well.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 68cbb8e10aea..2525042760e6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -655,6 +655,7 @@ static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
 extern cycle_t ftrace_now(int cpu);
 
 extern void trace_find_cmdline(int pid, char comm[]);
+extern void trace_event_follow_fork(struct trace_array *tr, bool enable);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern unsigned long ftrace_update_tot_cnt;
@@ -966,6 +967,7 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(STOP_ON_FREE,		"disable_on_free"),	\
 		C(IRQ_INFO,		"irq-info"),		\
 		C(MARKERS,		"markers"),		\
+		C(EVENT_FORK,		"event-fork"),		\
 		FUNCTION_FLAGS					\
 		FGRAPH_FLAGS					\
 		STACK_FLAGS					\

commit f4d34a87e9c10f0ffd03d3548db6bfb200d06cdf
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Apr 13 16:27:49 2016 -0400

    tracing: Use pid bitmap instead of a pid array for set_event_pid
    
    In order to add the ability to let tasks that are filtered by the events
    have their children also be traced on fork (and then not traced on exit),
    convert the array into a pid bitmask. Most of the time the number of pids is
    only 32768 pids or a 4k bitmask, which is the same size as the default list
    currently is, and that list could grow if more pids are listed.
    
    This also greatly simplifies the code.
    
    Suggested-by: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3fff4adfd431..68cbb8e10aea 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -177,9 +177,8 @@ struct trace_options {
 };
 
 struct trace_pid_list {
-	unsigned int			nr_pids;
-	int				order;
-	pid_t				*pids;
+	int				pid_max;
+	unsigned long			*pids;
 };
 
 /*

commit 7e6867bf831c71fe0e47438831ae3a94d4c7ab3c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Mar 18 16:28:04 2016 +0100

    tracing: Record and show NMI state
    
    The latency tracer format has a nice column to indicate IRQ state, but
    this is not able to tell us about NMI state.
    
    When tracing perf interrupt handlers (which often run in NMI context)
    it is very useful to see how the events nest.
    
    Link: http://lkml.kernel.org/r/20160318153022.105068893@infradead.org
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 39588c23dd8b..3fff4adfd431 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -125,6 +125,7 @@ enum trace_flag_type {
 	TRACE_FLAG_HARDIRQ		= 0x08,
 	TRACE_FLAG_SOFTIRQ		= 0x10,
 	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
+	TRACE_FLAG_NMI			= 0x40,
 };
 
 #define TRACE_BUF_SIZE		1024

commit 353206f5ca05eb65704b2b3ec9a331b4fdfd3257
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Feb 22 15:55:09 2016 -0500

    tracing: Use flags instead of bool in trigger structure
    
    gcc isn't known for handling bool in structures. Instead of using bool, use
    an integer mask and use bit flags instead.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f4dd0adf71df..39588c23dd8b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1273,30 +1273,7 @@ struct event_trigger_ops {
  *	values are defined by adding new values to the trigger_type
  *	enum in include/linux/trace_events.h.
  *
- * @post_trigger: A flag that says whether or not this command needs
- *	to have its action delayed until after the current event has
- *	been closed.  Some triggers need to avoid being invoked while
- *	an event is currently in the process of being logged, since
- *	the trigger may itself log data into the trace buffer.  Thus
- *	we make sure the current event is committed before invoking
- *	those triggers.  To do that, the trigger invocation is split
- *	in two - the first part checks the filter using the current
- *	trace record; if a command has the @post_trigger flag set, it
- *	sets a bit for itself in the return value, otherwise it
- *	directly invokes the trigger.  Once all commands have been
- *	either invoked or set their return flag, the current record is
- *	either committed or discarded.  At that point, if any commands
- *	have deferred their triggers, those commands are finally
- *	invoked following the close of the current event.  In other
- *	words, if the event_trigger_ops @func() probe implementation
- *	itself logs to the trace buffer, this flag should be set,
- *	otherwise it can be left unspecified.
- *
- * @needs_rec: A flag that says whether or not this command needs
- *	access to the trace record in order to perform its function,
- *	regardless of whether or not it has a filter associated with
- *	it (filters make a trigger require access to the trace record
- *	but are not always present).
+ * @flags: See the enum event_command_flags below.
  *
  * All the methods below, except for @set_filter() and @unreg_all(),
  * must be implemented.
@@ -1341,8 +1318,7 @@ struct event_command {
 	struct list_head	list;
 	char			*name;
 	enum event_trigger_type	trigger_type;
-	bool			post_trigger;
-	bool			needs_rec;
+	int			flags;
 	int			(*func)(struct event_command *cmd_ops,
 					struct trace_event_file *file,
 					char *glob, char *cmd, char *params);
@@ -1361,6 +1337,49 @@ struct event_command {
 	struct event_trigger_ops *(*get_trigger_ops)(char *cmd, char *param);
 };
 
+/**
+ * enum event_command_flags - flags for struct event_command
+ *
+ * @POST_TRIGGER: A flag that says whether or not this command needs
+ *	to have its action delayed until after the current event has
+ *	been closed.  Some triggers need to avoid being invoked while
+ *	an event is currently in the process of being logged, since
+ *	the trigger may itself log data into the trace buffer.  Thus
+ *	we make sure the current event is committed before invoking
+ *	those triggers.  To do that, the trigger invocation is split
+ *	in two - the first part checks the filter using the current
+ *	trace record; if a command has the @post_trigger flag set, it
+ *	sets a bit for itself in the return value, otherwise it
+ *	directly invokes the trigger.  Once all commands have been
+ *	either invoked or set their return flag, the current record is
+ *	either committed or discarded.  At that point, if any commands
+ *	have deferred their triggers, those commands are finally
+ *	invoked following the close of the current event.  In other
+ *	words, if the event_trigger_ops @func() probe implementation
+ *	itself logs to the trace buffer, this flag should be set,
+ *	otherwise it can be left unspecified.
+ *
+ * @NEEDS_REC: A flag that says whether or not this command needs
+ *	access to the trace record in order to perform its function,
+ *	regardless of whether or not it has a filter associated with
+ *	it (filters make a trigger require access to the trace record
+ *	but are not always present).
+ */
+enum event_command_flags {
+	EVENT_CMD_FL_POST_TRIGGER	= 1,
+	EVENT_CMD_FL_NEEDS_REC		= 2,
+};
+
+static inline bool event_command_post_trigger(struct event_command *cmd_ops)
+{
+	return cmd_ops->flags & EVENT_CMD_FL_POST_TRIGGER;
+}
+
+static inline bool event_command_needs_rec(struct event_command *cmd_ops)
+{
+	return cmd_ops->flags & EVENT_CMD_FL_NEEDS_REC;
+}
+
 extern int trace_event_enable_disable(struct trace_event_file *file,
 				      int enable, int soft_disable);
 extern int tracing_alloc_snapshot(void);

commit a88e1cfb1d3081ffb34864d9cf8a5c289630f48e
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:49 2015 -0600

    tracing: Add an unreg_all() callback to trigger commands
    
    Add a new unreg_all() callback that can be used to remove all
    command-specific triggers from an event and arrange to have it called
    whenever a trigger file is opened with O_TRUNC set.
    
    Commands that don't want truncate semantics, or existing commands that
    don't implement this function simply do nothing and their triggers
    remain intact.
    
    Link: http://lkml.kernel.org/r/2b7d62854d01f28c19185e1bbb8f826f385edfba.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8c6aefbb24d2..f4dd0adf71df 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1298,8 +1298,8 @@ struct event_trigger_ops {
  *	it (filters make a trigger require access to the trace record
  *	but are not always present).
  *
- * All the methods below, except for @set_filter(), must be
- * implemented.
+ * All the methods below, except for @set_filter() and @unreg_all(),
+ * must be implemented.
  *
  * @func: The callback function responsible for parsing and
  *	registering the trigger written to the 'trigger' file by the
@@ -1324,6 +1324,10 @@ struct event_trigger_ops {
  *	This is usually implemented by the generic utility function
  *	@unregister_trigger() (see trace_event_triggers.c).
  *
+ * @unreg_all: An optional function called to remove all the triggers
+ *	from the list of triggers associated with the event.  Called
+ *	when a trigger file is opened in truncate mode.
+ *
  * @set_filter: An optional function called to parse and set a filter
  *	for the trigger.  If no @set_filter() method is set for the
  *	event command, filters set by the user for the command will be
@@ -1350,6 +1354,7 @@ struct event_command {
 					 struct event_trigger_ops *ops,
 					 struct event_trigger_data *data,
 					 struct trace_event_file *file);
+	void			(*unreg_all)(struct trace_event_file *file);
 	int			(*set_filter)(char *filter_str,
 					      struct event_trigger_data *data,
 					      struct trace_event_file *file);

commit a5863dae84e2da83a1e5de485a7f150d0c28f08e
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:48 2015 -0600

    tracing: Add needs_rec flag to event triggers
    
    Add a new needs_rec flag for triggers that require unconditional
    access to trace records in order to function.
    
    Normally a trigger requires access to the contents of a trace record
    only if it has a filter associated with it (since filters need the
    contents of a record in order to make a filtering decision).  Some
    types of triggers, such as 'hist' triggers, require access to trace
    record contents independent of the presence of filters, so add a new
    flag for those triggers.
    
    Link: http://lkml.kernel.org/r/7be8fa38f9b90fdb6c47ca0f98d20a07b9fd512b.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f1868677f856..8c6aefbb24d2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1292,6 +1292,12 @@ struct event_trigger_ops {
  *	itself logs to the trace buffer, this flag should be set,
  *	otherwise it can be left unspecified.
  *
+ * @needs_rec: A flag that says whether or not this command needs
+ *	access to the trace record in order to perform its function,
+ *	regardless of whether or not it has a filter associated with
+ *	it (filters make a trigger require access to the trace record
+ *	but are not always present).
+ *
  * All the methods below, except for @set_filter(), must be
  * implemented.
  *
@@ -1332,6 +1338,7 @@ struct event_command {
 	char			*name;
 	enum event_trigger_type	trigger_type;
 	bool			post_trigger;
+	bool			needs_rec;
 	int			(*func)(struct event_command *cmd_ops,
 					struct trace_event_file *file,
 					char *glob, char *cmd, char *params);

commit 104f281044a9c2ac86b851bbebbf74500172b625
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:47 2015 -0600

    tracing: Add a per-event-trigger 'paused' field
    
    Add a simple per-trigger 'paused' flag, allowing individual triggers
    to pause.  We could leave it to individual triggers that need this
    functionality to do it themselves, but we also want to allow other
    events to control pausing, so add it to the trigger data.
    
    Link: http://lkml.kernel.org/r/fed37e4879684d7dcc57fe00ce0cbf170032b06d.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0044b91d5469..f1868677f856 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1172,6 +1172,7 @@ struct event_trigger_data {
 	struct event_filter __rcu	*filter;
 	char				*filter_str;
 	void				*private_data;
+	bool				paused;
 	struct list_head		list;
 };
 

commit dbfeaa7abae4f105afdf8ed4f85b5879cff136ea
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:46 2015 -0600

    tracing: Add get_syscall_name()
    
    Add a utility function to grab the syscall name from the syscall
    metadata, given a syscall id.
    
    Link: http://lkml.kernel.org/r/be26a8dfe3f15e16a837799f1c1e2b4d62742843.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c10456e72106..0044b91d5469 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1394,8 +1394,13 @@ int perf_ftrace_event_register(struct trace_event_call *call,
 
 #ifdef CONFIG_FTRACE_SYSCALLS
 void init_ftrace_syscalls(void);
+const char *get_syscall_name(int syscall);
 #else
 static inline void init_ftrace_syscalls(void) { }
+static inline const char *get_syscall_name(int syscall)
+{
+	return NULL;
+}
 #endif
 
 #ifdef CONFIG_EVENT_TRACING

commit c4a5923055c9e0c87dfc0387f7cda5ee2bbac3c1
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:45 2015 -0600

    tracing: Add event record param to trigger_ops.func()
    
    Some triggers may need access to the trace event, so pass it in.  Also
    fix up the existing trigger funcs and their callers.
    
    Link: http://lkml.kernel.org/r/543e31e9fc445ef61077421ab219033401c39846.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b2bc956e2b0d..c10456e72106 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1201,7 +1201,8 @@ extern int register_event_command(struct event_command *cmd);
  * @func: The trigger 'probe' function called when the triggering
  *	event occurs.  The data passed into this callback is the data
  *	that was supplied to the event_command @reg() function that
- *	registered the trigger (see struct event_command).
+ *	registered the trigger (see struct event_command) along with
+ *	the trace record, rec.
  *
  * @init: An optional initialization function called for the trigger
  *	when the trigger is registered (via the event_command reg()
@@ -1226,7 +1227,8 @@ extern int register_event_command(struct event_command *cmd);
  *	(see trace_event_triggers.c).
  */
 struct event_trigger_ops {
-	void			(*func)(struct event_trigger_data *data);
+	void			(*func)(struct event_trigger_data *data,
+					void *rec);
 	int			(*init)(struct event_trigger_ops *ops,
 					struct event_trigger_data *data);
 	void			(*free)(struct event_trigger_ops *ops,

commit ab4bf008928e8fc73fe1cbaa9249792d36845345
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:44 2015 -0600

    tracing: Make event trigger functions available
    
    Make various event trigger utility functions available outside of
    trace_events_trigger.c so that new triggers can be defined outside of
    that file.
    
    Link: http://lkml.kernel.org/r/4a40c1695dd43cac6cd475d72e13ffe30ba84bff.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 81a8359e9c29..b2bc956e2b0d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1175,6 +1175,20 @@ struct event_trigger_data {
 	struct list_head		list;
 };
 
+extern void trigger_data_free(struct event_trigger_data *data);
+extern int event_trigger_init(struct event_trigger_ops *ops,
+			      struct event_trigger_data *data);
+extern int trace_event_trigger_enable_disable(struct trace_event_file *file,
+					      int trigger_enable);
+extern void update_cond_flag(struct trace_event_file *file);
+extern void unregister_trigger(char *glob, struct event_trigger_ops *ops,
+			       struct event_trigger_data *test,
+			       struct trace_event_file *file);
+extern int set_trigger_filter(char *filter_str,
+			      struct event_trigger_data *trigger_data,
+			      struct trace_event_file *file);
+extern int register_event_command(struct event_command *cmd);
+
 /**
  * struct event_trigger_ops - callbacks for trace event triggers
  *

commit 4ef56902fba4d9949918c6266e67ba7d05fba7a4
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Dec 10 12:50:43 2015 -0600

    tracing: Make ftrace_event_field checking functions available
    
    Make is_string_field() and is_function_field() accessible outside of
    trace_event_filters.c for other users of ftrace_event_fields.
    
    Link: http://lkml.kernel.org/r/2d3f00d3311702e556e82eed7754bae6f017939f.1449767187.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b4cae47f283e..81a8359e9c29 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1112,6 +1112,18 @@ struct filter_pred {
 	unsigned short		right;
 };
 
+static inline bool is_string_field(struct ftrace_event_field *field)
+{
+	return field->filter_type == FILTER_DYN_STRING ||
+	       field->filter_type == FILTER_STATIC_STRING ||
+	       field->filter_type == FILTER_PTR_STRING;
+}
+
+static inline bool is_function_field(struct ftrace_event_field *field)
+{
+	return field->filter_type == FILTER_TRACE_FN;
+}
+
 extern enum regex_type
 filter_parse_regex(char *buff, int len, char **search, int *not);
 extern void print_event_filter(struct trace_event_file *file,

commit d39cdd2036a63eef17a14efbd969405ca5612886
Author: Chunyu Hu <chuhu@redhat.com>
Date:   Tue Mar 8 21:37:01 2016 +0800

    tracing: Make tracer_flags use the right set_flag callback
    
    When I was updating the ftrace_stress test of ltp. I encountered
    a strange phenomemon, excute following steps:
    
    echo nop > /sys/kernel/debug/tracing/current_tracer
    echo 0 > /sys/kernel/debug/tracing/options/funcgraph-cpu
    bash: echo: write error: Invalid argument
    
    check dmesg:
    [ 1024.903855] nop_test_refuse flag set to 0: we refuse.Now cat trace_options to see the result
    
    The reason is that the trace option test will randomly setup trace
    option under tracing/options no matter what the current_tracer is.
    but the set_tracer_option is always using the set_flag callback
    from the current_tracer. This patch adds a pointer to tracer_flags
    and make it point to the tracer it belongs to. When the option is
    setup, the set_flag of the right tracer will be used no matter
    what the the current_tracer is.
    
    And the old dummy_tracer_flags is used for all the tracers which
    doesn't have a tracer_flags, having issue to use it to save the
    pointer of a tracer. So remove it and use dynamic dummy tracer_flags
    for tracers needing a dummy tracer_flags, as a result, there are no
    tracers sharing tracer_flags, so remove the check code.
    
    And save the current tracer to trace_option_dentry seems not good as
    it may waste mem space when mount the debug/trace fs more than one time.
    
    Link: http://lkml.kernel.org/r/1457444222-8654-1-git-send-email-chuhu@redhat.com
    
    Signed-off-by: Chunyu Hu <chuhu@redhat.com>
    [ Fixed up function tracer options to work with the change ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8414fa40bf27..b4cae47f283e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -345,6 +345,7 @@ struct tracer_opt {
 struct tracer_flags {
 	u32			val;
 	struct tracer_opt	*opts;
+	struct tracer		*trace;
 };
 
 /* Makes more easy to define a tracer opt */

commit 05a724bd44a68cd5c60bc4b949a946b57d4c9e55
Author: Chuyu Hu <chuhu@redhat.com>
Date:   Tue Dec 22 09:44:33 2015 -0500

    tracing: Fix comment to use tracing_on over tracing_enable
    
    The file tracing_enable is obsolete and does not exist anymore. Replace
    the comment that references it with the proper tracing_on file.
    
    Link: http://lkml.kernel.org/r/1450787141-45544-1-git-send-email-chuhu@redhat.com
    
    Signed-off-by: Chuyu Hu <chuhu@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d3980b87bf04..8414fa40bf27 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -363,8 +363,8 @@ struct trace_option_dentry {
  * @name: the name chosen to select it on the available_tracers file
  * @init: called when one switches to this tracer (echo name > current_tracer)
  * @reset: called when one switches to another tracer
- * @start: called when tracing is unpaused (echo 1 > tracing_enabled)
- * @stop: called when tracing is paused (echo 0 > tracing_enabled)
+ * @start: called when tracing is unpaused (echo 1 > tracing_on)
+ * @stop: called when tracing is paused (echo 0 > tracing_on)
  * @update_thresh: called when tracing_thresh is updated
  * @open: called when the trace file is opened
  * @pipe_open: called when the trace_pipe file is opened

commit ba27f2bc731135a0396f3968bdddb54f3bc72e64
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 30 17:23:39 2015 -0500

    ftrace: Remove use of control list and ops
    
    Currently perf has its own list function within the ftrace infrastructure
    that seems to be used only to allow for it to have per-cpu disabling as well
    as a check to make sure that it's not called while RCU is not watching. It
    uses something called the "control_ops" which is used to iterate over ops
    under it with the control_list_func().
    
    The problem is that this control_ops and control_list_func unnecessarily
    complicates the code. By replacing FTRACE_OPS_FL_CONTROL with two new flags
    (FTRACE_OPS_FL_RCU and FTRACE_OPS_FL_PER_CPU) we can remove all the code
    that is special with the control ops and add the needed checks within the
    generic ftrace_list_func().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 919d9d07686f..d3980b87bf04 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -467,8 +467,6 @@ enum {
 	TRACE_INTERNAL_IRQ_BIT,
 	TRACE_INTERNAL_SIRQ_BIT,
 
-	TRACE_CONTROL_BIT,
-
 	TRACE_BRANCH_BIT,
 /*
  * Abuse of the trace_recursion.

commit 03e88ae6b369da2a26a6e09ad165e57d210789cd
Author: Dmitry Safonov <0x7f454c46@gmail.com>
Date:   Fri Nov 6 22:07:26 2015 +0300

    tracing: Remove unused ftrace_cpu_disabled per cpu variable
    
    Since the ring buffer is lockless, there is no need to disable ftrace on
    CPU. And no one doing so: after commit 68179686ac67cb ("tracing: Remove
    ftrace_disable/enable_cpu()") ftrace_cpu_disabled stays the same after
    initialization, nothing changes it.
    ftrace_cpu_disabled shouldn't be used by any external module since it
    disables only function and graph_function tracers but not any other
    tracer.
    
    Link: http://lkml.kernel.org/r/1446836846-22239-1-git-send-email-0x7f454c46@gmail.com
    
    Signed-off-by: Dmitry Safonov <0x7f454c46@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dd7620802e72..919d9d07686f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -667,7 +667,6 @@ extern int DYN_FTRACE_TEST_NAME2(void);
 
 extern bool ring_buffer_expanded;
 extern bool tracing_selftest_disabled;
-DECLARE_PER_CPU(int, ftrace_cpu_disabled);
 
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 extern int trace_selftest_startup_function(struct tracer *trace,

commit fb8c2293e1a3c4a35a571b82cc2efae0c9e59b2b
Author: Dmitry Safonov <0x7f454c46@gmail.com>
Date:   Tue Nov 3 21:49:20 2015 +0300

    tracing: Remove redundant TP_ARGS redefining
    
    TP_ARGS is not used anywhere in trace.h nor trace_entries.h
    Firstly, I left just #undef TP_ARGS and had no errors - remove it.
    
    Link: http://lkml.kernel.org/r/1446576560-14085-1-git-send-email-0x7f454c46@gmail.com
    
    Signed-off-by: Dmitry Safonov <0x7f454c46@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index be2126f5215d..dd7620802e72 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -71,9 +71,6 @@ enum trace_type {
 		tstruct							\
 	}
 
-#undef TP_ARGS
-#define TP_ARGS(args...)	args
-
 #undef FTRACE_ENTRY_DUP
 #define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk, filter)
 

commit c6650b2e57725abaa2e36e620d06fa576d26c21c
Author: Yaowei Bai <bywxiaobai@163.com>
Date:   Tue Sep 29 22:43:36 2015 +0800

    tracing: ftrace_event_is_function() can return boolean
    
    Make ftrace_event_is_function() return bool to improve readability
    due to this particular function only using either one or zero as its
    return value.
    
    No functional change.
    
    Link: http://lkml.kernel.org/r/1443537816-5788-9-git-send-email-bywxiaobai@163.com
    
    Signed-off-by: Yaowei Bai <bywxiaobai@163.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 89ffdaf3e371..be2126f5215d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -873,7 +873,7 @@ void ftrace_destroy_filter_files(struct ftrace_ops *ops);
 #define ftrace_destroy_filter_files(ops) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER && CONFIG_DYNAMIC_FTRACE */
 
-int ftrace_event_is_function(struct trace_event_call *call);
+bool ftrace_event_is_function(struct trace_event_call *call);
 
 /*
  * struct trace_parser - servers for reading the user input separated by spaces

commit 3fdaf80f4a836911c0eda1cee92f8aa625f90197
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 25 12:58:44 2015 -0400

    tracing: Implement event pid filtering
    
    Add the necessary hooks to use the pids loaded in set_event_pid to filter
    all the events enabled in the tracing instance that match the pids listed.
    
    Two probes are added to both sched_switch and sched_wakeup tracepoints to be
    called before other probes are called and after the other probes are called.
    The first is used to set the necessary flags to let the probes know to test
    if they should be traced or not.
    
    The sched_switch pre probe will set the "ignore_pid" flag if neither the
    previous or next task has a matching pid.
    
    The sched_switch probe will set the "ignore_pid" flag if the next task
    does not match the matching pid.
    
    The pre probe allows for probes tracing sched_switch to be traced if
    necessary.
    
    The sched_wakeup pre probe will set the "ignore_pid" flag if neither the
    current task nor the wakee task has a matching pid.
    
    The sched_wakeup post probe will set the "ignore_pid" flag if the current
    task does not have a matching pid.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 250481043bb5..89ffdaf3e371 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -156,6 +156,8 @@ struct trace_array_cpu {
 	pid_t			pid;
 	kuid_t			uid;
 	char			comm[TASK_COMM_LEN];
+
+	bool			ignore_pid;
 };
 
 struct tracer;

commit 4909010788640b7101bf50cddb7c5e60172b4433
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Sep 24 11:33:26 2015 -0400

    tracing: Add set_event_pid directory for future use
    
    Create a tracing directory called set_event_pid, which currently has no
    function, but will be used to filter all events for the tracing instance or
    the pids that are added to the file.
    
    The reason no functionality is added with this commit is that this commit
    focuses on the creation and removal of the pids in a safe manner. And tests
    can be made against this change to make sure things are correct before
    hooking features to the list of pids.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fb8a61c710ea..250481043bb5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -176,6 +176,12 @@ struct trace_options {
 	struct trace_option_dentry	*topts;
 };
 
+struct trace_pid_list {
+	unsigned int			nr_pids;
+	int				order;
+	pid_t				*pids;
+};
+
 /*
  * The trace array - an array of per-CPU trace arrays. This is the
  * highest level data structure that individual tracers deal with.
@@ -201,6 +207,7 @@ struct trace_array {
 	bool			allocated_snapshot;
 	unsigned long		max_latency;
 #endif
+	struct trace_pid_list	__rcu *filtered_pids;
 	/*
 	 * max_lock is used to protect the swapping of buffers
 	 * when taking a max snapshot. The buffers themselves are

commit 37aea98b84c0ce2ac638510fefeed9f8f920bd34
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Sep 30 14:27:31 2015 -0400

    tracing: Add trace options for tracer options to instances
    
    Add the tracer options to instances options directory as well. Only add the
    options for tracers that are allowed to be enabled by an instance. But note,
    that tracer options are global. That is, tracer options enabled in an
    instance, also take affect at the top level and in other instances.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 423cb48a1d6d..fb8a61c710ea 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -159,6 +159,7 @@ struct trace_array_cpu {
 };
 
 struct tracer;
+struct trace_option_dentry;
 
 struct trace_buffer {
 	struct trace_array		*tr;
@@ -170,6 +171,11 @@ struct trace_buffer {
 
 #define TRACE_FLAGS_MAX_SIZE		32
 
+struct trace_options {
+	struct tracer			*tracer;
+	struct trace_option_dentry	*topts;
+};
+
 /*
  * The trace array - an array of per-CPU trace arrays. This is the
  * highest level data structure that individual tracers deal with.
@@ -218,6 +224,7 @@ struct trace_array {
 #endif
 	int			stop_count;
 	int			clock_id;
+	int			nr_topts;
 	struct tracer		*current_trace;
 	unsigned int		trace_flags;
 	unsigned char		trace_flags_index[TRACE_FLAGS_MAX_SIZE];
@@ -227,6 +234,7 @@ struct trace_array {
 	struct dentry		*options;
 	struct dentry		*percpu_dir;
 	struct dentry		*event_dir;
+	struct trace_options	*topts;
 	struct list_head	systems;
 	struct list_head	events;
 	cpumask_var_t		tracing_cpumask; /* only trace on set CPUs */
@@ -398,7 +406,6 @@ struct tracer {
 						u32 mask, int set);
 	struct tracer		*next;
 	struct tracer_flags	*flags;
-	struct trace_option_dentry *topts;
 	int			enabled;
 	int			ref;
 	bool			print_max;

commit 9a38a8856f41f90cc7e57798c544e3fe77033196
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Sep 30 11:11:15 2015 -0400

    tracing: Add a method to pass in trace_array descriptor to option files
    
    In preparation of having the multi buffer instances having their own trace
    option flags, the trace option files needs a way to not only pass in the
    flag they represent, but also the trace_array descriptor.
    
    A new field is added to the trace_array descriptor called trace_flags_index,
    which is a 32 byte character array representing a bit. This array is simply
    filled with the index of the array, where
    
      index_array[n] = n;
    
    Then the address of this array is passed to the file callbacks instead of
    the index of the flag index. Then to retrieve both the flag index and the
    trace_array descriptor:
    
      data is the passed in argument.
    
      index = *(unsigned char *)data;
    
      data -= index;
    
      /* Now data points to the address of the array in the trace_array */
    
      tr = container_of(data, struct trace_array, trace_flags_index);
    
    Suggested-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index eda4e6f8159b..423cb48a1d6d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -168,6 +168,8 @@ struct trace_buffer {
 	int				cpu;
 };
 
+#define TRACE_FLAGS_MAX_SIZE		32
+
 /*
  * The trace array - an array of per-CPU trace arrays. This is the
  * highest level data structure that individual tracers deal with.
@@ -218,6 +220,7 @@ struct trace_array {
 	int			clock_id;
 	struct tracer		*current_trace;
 	unsigned int		trace_flags;
+	unsigned char		trace_flags_index[TRACE_FLAGS_MAX_SIZE];
 	unsigned int		flags;
 	raw_spinlock_t		start_lock;
 	struct dentry		*dir;

commit 983f938ae69585213bbb779d841b90e75f93f545
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Sep 30 09:42:05 2015 -0400

    tracing: Move trace_flags from global to a trace_array field
    
    In preparation to make trace options per instance, the global trace_flags
    needs to be moved from being a global variable to a field within the trace
    instance trace_array structure.
    
    There's still more work to do, as there's some functions that use
    trace_flags without passing in a way to get to the current_trace array. For
    those, the global_trace is used directly (from trace.c). This includes
    setting and clearing the trace_flags. This means that when a new instance is
    created, it just gets the trace_flags of the global_trace and will not be
    able to modify them. Depending on the functions that have access to the
    trace_array, the flags of an instance may not affect parts of its trace,
    where the global_trace is used. These will be fixed in future changes.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5219bf5f708a..eda4e6f8159b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -217,6 +217,7 @@ struct trace_array {
 	int			stop_count;
 	int			clock_id;
 	struct tracer		*current_trace;
+	unsigned int		trace_flags;
 	unsigned int		flags;
 	raw_spinlock_t		start_lock;
 	struct dentry		*dir;
@@ -698,8 +699,6 @@ int trace_array_printk_buf(struct ring_buffer *buffer,
 void trace_printk_seq(struct trace_seq *s);
 enum print_line_t print_trace_line(struct trace_iterator *iter);
 
-extern unsigned long trace_flags;
-
 extern char trace_find_mark(unsigned long long duration);
 
 /* Standard output formatting function used for function return traces */
@@ -994,7 +993,7 @@ extern int enable_branch_tracing(struct trace_array *tr);
 extern void disable_branch_tracing(void);
 static inline int trace_branch_enable(struct trace_array *tr)
 {
-	if (trace_flags & TRACE_ITER_BRANCH)
+	if (tr->trace_flags & TRACE_ITER_BRANCH)
 		return enable_branch_tracing(tr);
 	return 0;
 }

commit 55577204154c7a95c6bce4cb185366d638b238b5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 19:06:50 2015 -0400

    tracing: Move sleep-time and graph-time options out of the core trace_flags
    
    The sleep-time and graph-time options are only for the function graph tracer
    and are not used by anything else. As tracer options are now visible when
    the tracer is not activated, its better to move the function graph specific
    tracer options into the function graph tracer.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 33d1e5384481..5219bf5f708a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -714,9 +714,14 @@ extern char trace_find_mark(unsigned long long duration);
 #define TRACE_GRAPH_PRINT_ABS_TIME      0x20
 #define TRACE_GRAPH_PRINT_IRQS          0x40
 #define TRACE_GRAPH_PRINT_TAIL          0x80
+#define TRACE_GRAPH_SLEEP_TIME		0x100
+#define TRACE_GRAPH_GRAPH_TIME		0x200
 #define TRACE_GRAPH_PRINT_FILL_SHIFT	28
 #define TRACE_GRAPH_PRINT_FILL_MASK	(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)
 
+extern void ftrace_graph_sleep_time_control(bool enable);
+extern void ftrace_graph_graph_time_control(bool enable);
+
 extern enum print_line_t
 print_graph_function_flags(struct trace_iterator *iter, u32 flags);
 extern void print_graph_headers_flags(struct seq_file *s, u32 flags);
@@ -892,15 +897,9 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
  */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 # define FGRAPH_FLAGS						\
-		C(SLEEP_TIME,		"sleep-time"),		\
-		C(GRAPH_TIME,		"graph-time"),		\
 		C(DISPLAY_GRAPH,	"display-graph"),
-/* Initially set for trace_flags */
-# define FUNCTION_GRAPH_DEFAULT_FLAGS \
-	(TRACE_ITER_SLEEP_TIME | TRACE_ITER_GRAPH_TIME)
 #else
 # define FGRAPH_FLAGS
-# define FUNCTION_GRAPH_DEFAULT_FLAGS  0UL
 #endif
 
 #ifdef CONFIG_BRANCH_TRACER

commit b9f9108cad3998a4c8fd26051c37a451f1dff1f1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 18:21:35 2015 -0400

    tracing: Remove access to trace_flags in trace_printk.c
    
    In the effort to move the global trace_flags to the tracing instances, the
    direct access to trace_flags must be removed from trace_printk.c
    
    Instead, add a new trace_printk_enabled boolean that is set by a new access
    function trace_printk_control(), that will enable or disable trace_printk.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 07155652254d..33d1e5384481 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1318,6 +1318,7 @@ extern const char *__stop___trace_bprintk_fmt[];
 extern const char *__start___tracepoint_str[];
 extern const char *__stop___tracepoint_str[];
 
+void trace_printk_control(bool enabled);
 void trace_printk_init_buffers(void);
 void trace_printk_start_comm(void);
 int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);

commit b5e87c0581319481399b6d8e8d6972b5523c18e6
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 18:13:33 2015 -0400

    tracing: Add build bug if we have more trace_flags than bits
    
    Add a enum that denotes the last bit of the trace_flags and have a
    BUILD_BUG_ON(last_bit > 32).
    
    If we add more bits than we have in trace_flags, the kernel wont build.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8ed97872b65b..07155652254d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -966,7 +966,11 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 #undef C
 #define C(a, b) TRACE_ITER_##a##_BIT
 
-enum trace_iterator_bits { TRACE_FLAGS };
+enum trace_iterator_bits {
+	TRACE_FLAGS
+	/* Make sure we don't go more than we have bits for */
+	TRACE_ITER_LAST_BIT
+};
 
 /*
  * By redefining C, we can make TRACE_FLAGS a list of masks that

commit 41d9c0beccbb92397bea8b04a6afd1253c064a1a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 17:31:55 2015 -0400

    tracing: Always show all tracer options in the options directory
    
    There are options that are unique to a specific tracer (like function and
    function graph). Currently, these options are only visible in the options
    directory when the tracer is enabled.
    
    This has been a pain, especially for something like the func_stack_trace
    option that if used inappropriately, could bring the system to a crawl. But
    the only way to see it, is to enable the function tracer.
    
    For example, if one had done:
    
     # cd /sys/kernel/tracing
     # echo __schedule > set_ftrace_filter
     # echo 1 > options/func_stack_trace
     # echo function > current_tracer
    
    The __schedule call will be traced and a stack trace will also be recorded
    there. Now when you were done, you may do...
    
     # echo nop > current_tracer
     # echo > set_ftrace_filter
    
    But you forgot to disable the func_stack_trace. The only way to disable it
    is to re-enable function tracing first. If you do not add a filter to
    set_ftrace_filter and just do:
    
     # echo function > current_tracer
    
    Now you would be performing a stack trace on *every* function! On some
    systems, that causes a live lock. Others may take a few minutes to fix your
    mistake.
    
    Having the func_stack_trace option visible allows you to check it and
    disable it before enabling the funtion tracer.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index af34e1822dad..8ed97872b65b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -333,6 +333,13 @@ struct tracer_flags {
 #define TRACER_OPT(s, b)	.name = #s, .bit = b
 
 
+struct trace_option_dentry {
+	struct tracer_opt		*opt;
+	struct tracer_flags		*flags;
+	struct trace_array		*tr;
+	struct dentry			*entry;
+};
+
 /**
  * struct tracer - a specific tracer and its callbacks to interact with tracefs
  * @name: the name chosen to select it on the available_tracers file
@@ -387,6 +394,7 @@ struct tracer {
 						u32 mask, int set);
 	struct tracer		*next;
 	struct tracer_flags	*flags;
+	struct trace_option_dentry *topts;
 	int			enabled;
 	int			ref;
 	bool			print_max;

commit 73dddbb57bb08d465dd0ecab93db0c5209e50cfe
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 15:38:55 2015 -0400

    tracing: Only create stacktrace option when STACKTRACE is configured
    
    Only create the stacktrace trace option when CONFIG_STACKTRACE is
    configured.
    
    Cleaned up the ftrace_trace_stack() function call a little to allow better
    encapsulation of the stacktrace trace flag.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b389d409b952..af34e1822dad 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -911,6 +911,13 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 # define FUNCTION_DEFAULT_FLAGS		0UL
 #endif
 
+#ifdef CONFIG_STACKTRACE
+# define STACK_FLAGS				\
+		C(STACKTRACE,		"stacktrace"),
+#else
+# define STACK_FLAGS
+#endif
+
 /*
  * trace_iterator_flags is an enumeration that defines bit
  * positions into trace_flags that controls the output.
@@ -927,7 +934,6 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(HEX,			"hex"),			\
 		C(BIN,			"bin"),			\
 		C(BLOCK,		"block"),		\
-		C(STACKTRACE,		"stacktrace"),		\
 		C(PRINTK,		"trace_printk"),	\
 		C(ANNOTATE,		"annotate"),		\
 		C(USERSTACKTRACE,	"userstacktrace"),	\
@@ -942,6 +948,7 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(MARKERS,		"markers"),		\
 		FUNCTION_FLAGS					\
 		FGRAPH_FLAGS					\
+		STACK_FLAGS					\
 		BRANCH_FLAGS
 
 /*

commit 8179e8a15b76eaec1e757da7a0f96de9f0c466c6
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 10:24:56 2015 -0400

    tracing: Do not create function tracer options when not compiled in
    
    When the function tracer is not compiled in, do not create the option files
    for it.
    
    Fix up both the sched_wakeup and irqsoff tracers to handle the change.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3f1cc45b7007..b389d409b952 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -902,6 +902,15 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 # define BRANCH_FLAGS
 #endif
 
+#ifdef CONFIG_FUNCTION_TRACER
+# define FUNCTION_FLAGS						\
+		C(FUNCTION,		"function-trace"),
+# define FUNCTION_DEFAULT_FLAGS		TRACE_ITER_FUNCTION
+#else
+# define FUNCTION_FLAGS
+# define FUNCTION_DEFAULT_FLAGS		0UL
+#endif
+
 /*
  * trace_iterator_flags is an enumeration that defines bit
  * positions into trace_flags that controls the output.
@@ -931,7 +940,7 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(STOP_ON_FREE,		"disable_on_free"),	\
 		C(IRQ_INFO,		"irq-info"),		\
 		C(MARKERS,		"markers"),		\
-		C(FUNCTION,		"function-trace"),	\
+		FUNCTION_FLAGS					\
 		FGRAPH_FLAGS					\
 		BRANCH_FLAGS
 

commit 4ee4301c4bab22c84df20ce694cc6932dd812be5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 10:19:35 2015 -0400

    tracing: Only create branch tracer options when compiled in
    
    When the branch tracer is not compiled in, do not create the option files
    associated to it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 33cd09799ceb..3f1cc45b7007 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -895,6 +895,13 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 # define FUNCTION_GRAPH_DEFAULT_FLAGS  0UL
 #endif
 
+#ifdef CONFIG_BRANCH_TRACER
+# define BRANCH_FLAGS					\
+		C(BRANCH,		"branch"),
+#else
+# define BRANCH_FLAGS
+#endif
+
 /*
  * trace_iterator_flags is an enumeration that defines bit
  * positions into trace_flags that controls the output.
@@ -913,7 +920,6 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(BLOCK,		"block"),		\
 		C(STACKTRACE,		"stacktrace"),		\
 		C(PRINTK,		"trace_printk"),	\
-		C(BRANCH,		"branch"),		\
 		C(ANNOTATE,		"annotate"),		\
 		C(USERSTACKTRACE,	"userstacktrace"),	\
 		C(SYM_USEROBJ,		"sym-userobj"),		\
@@ -926,7 +932,8 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(IRQ_INFO,		"irq-info"),		\
 		C(MARKERS,		"markers"),		\
 		C(FUNCTION,		"function-trace"),	\
-		FGRAPH_FLAGS
+		FGRAPH_FLAGS					\
+		BRANCH_FLAGS
 
 /*
  * By defining C, we can make TRACE_FLAGS a list of bit names

commit 729358da95a1b3850ef892e9384f58932da1dc69
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 10:15:10 2015 -0400

    tracing: Only create function graph options when it is compiled in
    
    Do not create fuction graph tracer options when function graph tracer is not
    even compiled in.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d164845edddd..33cd09799ceb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -879,6 +879,22 @@ extern void trace_parser_put(struct trace_parser *parser);
 extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 	size_t cnt, loff_t *ppos);
 
+/*
+ * Only create function graph options if function graph is configured.
+ */
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+# define FGRAPH_FLAGS						\
+		C(SLEEP_TIME,		"sleep-time"),		\
+		C(GRAPH_TIME,		"graph-time"),		\
+		C(DISPLAY_GRAPH,	"display-graph"),
+/* Initially set for trace_flags */
+# define FUNCTION_GRAPH_DEFAULT_FLAGS \
+	(TRACE_ITER_SLEEP_TIME | TRACE_ITER_GRAPH_TIME)
+#else
+# define FGRAPH_FLAGS
+# define FUNCTION_GRAPH_DEFAULT_FLAGS  0UL
+#endif
+
 /*
  * trace_iterator_flags is an enumeration that defines bit
  * positions into trace_flags that controls the output.
@@ -904,15 +920,13 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
 		C(PRINTK_MSGONLY,	"printk-msg-only"),	\
 		C(CONTEXT_INFO,		"context-info"),   /* Print pid/cpu/time */ \
 		C(LATENCY_FMT,		"latency-format"),	\
-		C(SLEEP_TIME,		"sleep-time"),		\
-		C(GRAPH_TIME,		"graph-time"),		\
 		C(RECORD_CMD,		"record-cmd"),		\
 		C(OVERWRITE,		"overwrite"),		\
 		C(STOP_ON_FREE,		"disable_on_free"),	\
 		C(IRQ_INFO,		"irq-info"),		\
 		C(MARKERS,		"markers"),		\
 		C(FUNCTION,		"function-trace"),	\
-		C(DISPLAY_GRAPH,	"display-graph"),
+		FGRAPH_FLAGS
 
 /*
  * By defining C, we can make TRACE_FLAGS a list of bit names

commit a3418a364ec3c8f0c29bf3f4cfc71dc6f240150e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 09:43:30 2015 -0400

    tracing: Use TRACE_FLAGS macro to keep enums and strings matched
    
    Use a cute little macro trick to keep the names of the trace flags file
    guaranteed to match the corresponding masks.
    
    The macro TRACE_FLAGS is defined as a serious of enum names followed by
    the string name of the file that matches it. For example:
    
     #define TRACE_FLAGS                                            \
                    C(PRINT_PARENT,         "print-parent"),        \
                    C(SYM_OFFSET,           "sym-offset"),          \
                    C(SYM_ADDR,             "sym-addr"),            \
                    C(VERBOSE,              "verbose"),
    
    Now we can define the following:
    
     #undef C
     #define C(a, b) TRACE_ITER_##a##_BIT
     enum trace_iterator_bits { TRACE_FLAGS };
    
    The above creates:
    
     enum trace_iterator_bits {
            TRACE_ITER_PRINT_PARENT_BIT,
            TRACE_ITER_SYM_OFFSET_BIT,
            TRACE_ITER_SYM_ADDR_BIT,
            TRACE_ITER_VERBOSE_BIT,
     };
    
    Then we can redefine C as:
    
     #undef C
     #define C(a, b) TRACE_ITER_##a = (1 << TRACE_ITER_##a##_BIT)
     enum trace_iterator_flags { TRACE_FLAGS };
    
    Which creates:
    
     enum trace_iterator_flags {
            TRACE_ITER_PRINT_PARENT = (1 << TRACE_ITER_PRINT_PARENT_BIT),
            TRACE_ITER_SYM_OFFSET   = (1 << TRACE_ITER_SYM_OFFSET_BIT),
            TRACE_ITER_SYM_ADDR     = (1 << TRACE_ITER_SYM_ADDR_BIT),
            TRACE_ITER_VERBOSE      = (1 << TRACE_ITER_VERBOSE_BIT),
     };
    
    Then finally we can create the list of file names:
    
     #undef C
     #define C(a, b) b
     static const char *trace_options[] = {
            TRACE_FLAGS
            NULL
     };
    
    Which creates:
     static const char *trace_options[] = {
            "print-parent",
            "sym-offset",
            "sym-addr",
            "verbose",
            NULL
     };
    
    The importance of this is that the strings match the bit index.
    
            trace_options[TRACE_ITER_SYM_ADDR_BIT] == "sym-addr"
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 31d8395c8dc5..d164845edddd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -884,65 +884,53 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
  * positions into trace_flags that controls the output.
  *
  * NOTE: These bits must match the trace_options array in
- *       trace.c.
+ *       trace.c (this macro guarantees it).
  */
-enum trace_iterator_bits {
-	TRACE_ITER_PRINT_PARENT_BIT	= 0,
-	TRACE_ITER_SYM_OFFSET_BIT,
-	TRACE_ITER_SYM_ADDR_BIT,
-	TRACE_ITER_VERBOSE_BIT,
-	TRACE_ITER_RAW_BIT,
-	TRACE_ITER_HEX_BIT,
-	TRACE_ITER_BIN_BIT,
-	TRACE_ITER_BLOCK_BIT,
-	TRACE_ITER_STACKTRACE_BIT,
-	TRACE_ITER_PRINTK_BIT,
-	TRACE_ITER_BRANCH_BIT,
-	TRACE_ITER_ANNOTATE_BIT,
-	TRACE_ITER_USERSTACKTRACE_BIT,
-	TRACE_ITER_SYM_USEROBJ_BIT,
-	TRACE_ITER_PRINTK_MSGONLY_BIT,
-	TRACE_ITER_CONTEXT_INFO_BIT,	/* Print pid/cpu/time */
-	TRACE_ITER_LATENCY_FMT_BIT,
-	TRACE_ITER_SLEEP_TIME_BIT,
-	TRACE_ITER_GRAPH_TIME_BIT,
-	TRACE_ITER_RECORD_CMD_BIT,
-	TRACE_ITER_OVERWRITE_BIT,
-	TRACE_ITER_STOP_ON_FREE_BIT,
-	TRACE_ITER_IRQ_INFO_BIT,
-	TRACE_ITER_MARKERS_BIT,
-	TRACE_ITER_FUNCTION_BIT,
-	TRACE_ITER_DISPLAY_GRAPH_BIT,
-};
+#define TRACE_FLAGS						\
+		C(PRINT_PARENT,		"print-parent"),	\
+		C(SYM_OFFSET,		"sym-offset"),		\
+		C(SYM_ADDR,		"sym-addr"),		\
+		C(VERBOSE,		"verbose"),		\
+		C(RAW,			"raw"),			\
+		C(HEX,			"hex"),			\
+		C(BIN,			"bin"),			\
+		C(BLOCK,		"block"),		\
+		C(STACKTRACE,		"stacktrace"),		\
+		C(PRINTK,		"trace_printk"),	\
+		C(BRANCH,		"branch"),		\
+		C(ANNOTATE,		"annotate"),		\
+		C(USERSTACKTRACE,	"userstacktrace"),	\
+		C(SYM_USEROBJ,		"sym-userobj"),		\
+		C(PRINTK_MSGONLY,	"printk-msg-only"),	\
+		C(CONTEXT_INFO,		"context-info"),   /* Print pid/cpu/time */ \
+		C(LATENCY_FMT,		"latency-format"),	\
+		C(SLEEP_TIME,		"sleep-time"),		\
+		C(GRAPH_TIME,		"graph-time"),		\
+		C(RECORD_CMD,		"record-cmd"),		\
+		C(OVERWRITE,		"overwrite"),		\
+		C(STOP_ON_FREE,		"disable_on_free"),	\
+		C(IRQ_INFO,		"irq-info"),		\
+		C(MARKERS,		"markers"),		\
+		C(FUNCTION,		"function-trace"),	\
+		C(DISPLAY_GRAPH,	"display-graph"),
 
-enum trace_iterator_flags {
-	TRACE_ITER_PRINT_PARENT		= (1 << TRACE_ITER_PRINT_PARENT_BIT),
-	TRACE_ITER_SYM_OFFSET		= (1 << TRACE_ITER_SYM_OFFSET_BIT),
-	TRACE_ITER_SYM_ADDR		= (1 << TRACE_ITER_SYM_ADDR_BIT),
-	TRACE_ITER_VERBOSE		= (1 << TRACE_ITER_VERBOSE_BIT),
-	TRACE_ITER_RAW			= (1 << TRACE_ITER_RAW_BIT),
-	TRACE_ITER_HEX			= (1 << TRACE_ITER_HEX_BIT),
-	TRACE_ITER_BIN			= (1 << TRACE_ITER_BIN_BIT),
-	TRACE_ITER_BLOCK		= (1 << TRACE_ITER_BLOCK_BIT),
-	TRACE_ITER_STACKTRACE		= (1 << TRACE_ITER_STACKTRACE_BIT),
-	TRACE_ITER_PRINTK		= (1 << TRACE_ITER_PRINTK_BIT),
-	TRACE_ITER_BRANCH		= (1 << TRACE_ITER_BRANCH_BIT),
-	TRACE_ITER_ANNOTATE		= (1 << TRACE_ITER_ANNOTATE_BIT),
-	TRACE_ITER_USERSTACKTRACE       = (1 << TRACE_ITER_USERSTACKTRACE_BIT),
-	TRACE_ITER_SYM_USEROBJ          = (1 << TRACE_ITER_SYM_USEROBJ_BIT),
-	TRACE_ITER_PRINTK_MSGONLY	= (1 << TRACE_ITER_PRINTK_MSGONLY_BIT),
-	TRACE_ITER_CONTEXT_INFO		= (1 << TRACE_ITER_CONTEXT_INFO_BIT),
-	TRACE_ITER_LATENCY_FMT		= (1 << TRACE_ITER_LATENCY_FMT_BIT),
-	TRACE_ITER_SLEEP_TIME		= (1 << TRACE_ITER_SLEEP_TIME_BIT),
-	TRACE_ITER_GRAPH_TIME		= (1 << TRACE_ITER_GRAPH_TIME_BIT),
-	TRACE_ITER_RECORD_CMD		= (1 << TRACE_ITER_RECORD_CMD_BIT),
-	TRACE_ITER_OVERWRITE		= (1 << TRACE_ITER_OVERWRITE_BIT),
-	TRACE_ITER_STOP_ON_FREE		= (1 << TRACE_ITER_STOP_ON_FREE_BIT),
-	TRACE_ITER_IRQ_INFO		= (1 << TRACE_ITER_IRQ_INFO_BIT),
-	TRACE_ITER_MARKERS		= (1 << TRACE_ITER_MARKERS_BIT),
-	TRACE_ITER_FUNCTION		= (1 << TRACE_ITER_FUNCTION_BIT),
-	TRACE_ITER_DISPLAY_GRAPH	= (1 << TRACE_ITER_DISPLAY_GRAPH_BIT),
-};
+/*
+ * By defining C, we can make TRACE_FLAGS a list of bit names
+ * that will define the bits for the flag masks.
+ */
+#undef C
+#define C(a, b) TRACE_ITER_##a##_BIT
+
+enum trace_iterator_bits { TRACE_FLAGS };
+
+/*
+ * By redefining C, we can make TRACE_FLAGS a list of masks that
+ * use the bits as defined above.
+ */
+#undef C
+#define C(a, b) TRACE_ITER_##a = (1 << TRACE_ITER_##a##_BIT)
+
+enum trace_iterator_flags { TRACE_FLAGS };
 
 /*
  * TRACE_ITER_SYM_MASK masks the options in trace_flags that

commit ce3fed628ecc86d81fdb2be5a5c336c636960bfe
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Sep 29 09:22:05 2015 -0400

    tracing: Use enums instead of hard coded bitmasks for TRACE_ITER flags
    
    Using enums with FLAG_BIT and then defining a FLAG = (1 << FLAG_BIT), is a
    bit more robust as we require that there are no bits out of order or skipped
    to match the file names that represent the bits.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 19d5c411d4ec..31d8395c8dc5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -886,33 +886,62 @@ extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
  * NOTE: These bits must match the trace_options array in
  *       trace.c.
  */
+enum trace_iterator_bits {
+	TRACE_ITER_PRINT_PARENT_BIT	= 0,
+	TRACE_ITER_SYM_OFFSET_BIT,
+	TRACE_ITER_SYM_ADDR_BIT,
+	TRACE_ITER_VERBOSE_BIT,
+	TRACE_ITER_RAW_BIT,
+	TRACE_ITER_HEX_BIT,
+	TRACE_ITER_BIN_BIT,
+	TRACE_ITER_BLOCK_BIT,
+	TRACE_ITER_STACKTRACE_BIT,
+	TRACE_ITER_PRINTK_BIT,
+	TRACE_ITER_BRANCH_BIT,
+	TRACE_ITER_ANNOTATE_BIT,
+	TRACE_ITER_USERSTACKTRACE_BIT,
+	TRACE_ITER_SYM_USEROBJ_BIT,
+	TRACE_ITER_PRINTK_MSGONLY_BIT,
+	TRACE_ITER_CONTEXT_INFO_BIT,	/* Print pid/cpu/time */
+	TRACE_ITER_LATENCY_FMT_BIT,
+	TRACE_ITER_SLEEP_TIME_BIT,
+	TRACE_ITER_GRAPH_TIME_BIT,
+	TRACE_ITER_RECORD_CMD_BIT,
+	TRACE_ITER_OVERWRITE_BIT,
+	TRACE_ITER_STOP_ON_FREE_BIT,
+	TRACE_ITER_IRQ_INFO_BIT,
+	TRACE_ITER_MARKERS_BIT,
+	TRACE_ITER_FUNCTION_BIT,
+	TRACE_ITER_DISPLAY_GRAPH_BIT,
+};
+
 enum trace_iterator_flags {
-	TRACE_ITER_PRINT_PARENT		= 0x01,
-	TRACE_ITER_SYM_OFFSET		= 0x02,
-	TRACE_ITER_SYM_ADDR		= 0x04,
-	TRACE_ITER_VERBOSE		= 0x08,
-	TRACE_ITER_RAW			= 0x10,
-	TRACE_ITER_HEX			= 0x20,
-	TRACE_ITER_BIN			= 0x40,
-	TRACE_ITER_BLOCK		= 0x80,
-	TRACE_ITER_STACKTRACE		= 0x100,
-	TRACE_ITER_PRINTK		= 0x200,
-	TRACE_ITER_BRANCH		= 0x400,
-	TRACE_ITER_ANNOTATE		= 0x800,
-	TRACE_ITER_USERSTACKTRACE       = 0x1000,
-	TRACE_ITER_SYM_USEROBJ          = 0x2000,
-	TRACE_ITER_PRINTK_MSGONLY	= 0x4000,
-	TRACE_ITER_CONTEXT_INFO		= 0x8000, /* Print pid/cpu/time */
-	TRACE_ITER_LATENCY_FMT		= 0x10000,
-	TRACE_ITER_SLEEP_TIME		= 0x20000,
-	TRACE_ITER_GRAPH_TIME		= 0x40000,
-	TRACE_ITER_RECORD_CMD		= 0x80000,
-	TRACE_ITER_OVERWRITE		= 0x100000,
-	TRACE_ITER_STOP_ON_FREE		= 0x200000,
-	TRACE_ITER_IRQ_INFO		= 0x400000,
-	TRACE_ITER_MARKERS		= 0x800000,
-	TRACE_ITER_FUNCTION		= 0x1000000,
-	TRACE_ITER_DISPLAY_GRAPH	= 0x2000000,
+	TRACE_ITER_PRINT_PARENT		= (1 << TRACE_ITER_PRINT_PARENT_BIT),
+	TRACE_ITER_SYM_OFFSET		= (1 << TRACE_ITER_SYM_OFFSET_BIT),
+	TRACE_ITER_SYM_ADDR		= (1 << TRACE_ITER_SYM_ADDR_BIT),
+	TRACE_ITER_VERBOSE		= (1 << TRACE_ITER_VERBOSE_BIT),
+	TRACE_ITER_RAW			= (1 << TRACE_ITER_RAW_BIT),
+	TRACE_ITER_HEX			= (1 << TRACE_ITER_HEX_BIT),
+	TRACE_ITER_BIN			= (1 << TRACE_ITER_BIN_BIT),
+	TRACE_ITER_BLOCK		= (1 << TRACE_ITER_BLOCK_BIT),
+	TRACE_ITER_STACKTRACE		= (1 << TRACE_ITER_STACKTRACE_BIT),
+	TRACE_ITER_PRINTK		= (1 << TRACE_ITER_PRINTK_BIT),
+	TRACE_ITER_BRANCH		= (1 << TRACE_ITER_BRANCH_BIT),
+	TRACE_ITER_ANNOTATE		= (1 << TRACE_ITER_ANNOTATE_BIT),
+	TRACE_ITER_USERSTACKTRACE       = (1 << TRACE_ITER_USERSTACKTRACE_BIT),
+	TRACE_ITER_SYM_USEROBJ          = (1 << TRACE_ITER_SYM_USEROBJ_BIT),
+	TRACE_ITER_PRINTK_MSGONLY	= (1 << TRACE_ITER_PRINTK_MSGONLY_BIT),
+	TRACE_ITER_CONTEXT_INFO		= (1 << TRACE_ITER_CONTEXT_INFO_BIT),
+	TRACE_ITER_LATENCY_FMT		= (1 << TRACE_ITER_LATENCY_FMT_BIT),
+	TRACE_ITER_SLEEP_TIME		= (1 << TRACE_ITER_SLEEP_TIME_BIT),
+	TRACE_ITER_GRAPH_TIME		= (1 << TRACE_ITER_GRAPH_TIME_BIT),
+	TRACE_ITER_RECORD_CMD		= (1 << TRACE_ITER_RECORD_CMD_BIT),
+	TRACE_ITER_OVERWRITE		= (1 << TRACE_ITER_OVERWRITE_BIT),
+	TRACE_ITER_STOP_ON_FREE		= (1 << TRACE_ITER_STOP_ON_FREE_BIT),
+	TRACE_ITER_IRQ_INFO		= (1 << TRACE_ITER_IRQ_INFO_BIT),
+	TRACE_ITER_MARKERS		= (1 << TRACE_ITER_MARKERS_BIT),
+	TRACE_ITER_FUNCTION		= (1 << TRACE_ITER_FUNCTION_BIT),
+	TRACE_ITER_DISPLAY_GRAPH	= (1 << TRACE_ITER_DISPLAY_GRAPH_BIT),
 };
 
 /*

commit 938db5f569247d13910d4542666709623c4253b0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Sep 28 16:21:55 2015 -0400

    tracing: Remove unused tracing option "ftrace_preempt"
    
    There was a time where the function tracing would disable interrupts unless
    specifically told not to, where it would only disable preemption. With the
    new lockless code, the function tracing never disalbes interrupts and just
    uses disabling of preemption. Remove the option "ftrace_preempt" as it does
    nothing anyway.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dfa3cd2feb22..19d5c411d4ec 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -897,23 +897,22 @@ enum trace_iterator_flags {
 	TRACE_ITER_BLOCK		= 0x80,
 	TRACE_ITER_STACKTRACE		= 0x100,
 	TRACE_ITER_PRINTK		= 0x200,
-	TRACE_ITER_PREEMPTONLY		= 0x400,
-	TRACE_ITER_BRANCH		= 0x800,
-	TRACE_ITER_ANNOTATE		= 0x1000,
-	TRACE_ITER_USERSTACKTRACE       = 0x2000,
-	TRACE_ITER_SYM_USEROBJ          = 0x4000,
-	TRACE_ITER_PRINTK_MSGONLY	= 0x8000,
-	TRACE_ITER_CONTEXT_INFO		= 0x10000, /* Print pid/cpu/time */
-	TRACE_ITER_LATENCY_FMT		= 0x20000,
-	TRACE_ITER_SLEEP_TIME		= 0x40000,
-	TRACE_ITER_GRAPH_TIME		= 0x80000,
-	TRACE_ITER_RECORD_CMD		= 0x100000,
-	TRACE_ITER_OVERWRITE		= 0x200000,
-	TRACE_ITER_STOP_ON_FREE		= 0x400000,
-	TRACE_ITER_IRQ_INFO		= 0x800000,
-	TRACE_ITER_MARKERS		= 0x1000000,
-	TRACE_ITER_FUNCTION		= 0x2000000,
-	TRACE_ITER_DISPLAY_GRAPH	= 0x4000000,
+	TRACE_ITER_BRANCH		= 0x400,
+	TRACE_ITER_ANNOTATE		= 0x800,
+	TRACE_ITER_USERSTACKTRACE       = 0x1000,
+	TRACE_ITER_SYM_USEROBJ          = 0x2000,
+	TRACE_ITER_PRINTK_MSGONLY	= 0x4000,
+	TRACE_ITER_CONTEXT_INFO		= 0x8000, /* Print pid/cpu/time */
+	TRACE_ITER_LATENCY_FMT		= 0x10000,
+	TRACE_ITER_SLEEP_TIME		= 0x20000,
+	TRACE_ITER_GRAPH_TIME		= 0x40000,
+	TRACE_ITER_RECORD_CMD		= 0x80000,
+	TRACE_ITER_OVERWRITE		= 0x100000,
+	TRACE_ITER_STOP_ON_FREE		= 0x200000,
+	TRACE_ITER_IRQ_INFO		= 0x400000,
+	TRACE_ITER_MARKERS		= 0x800000,
+	TRACE_ITER_FUNCTION		= 0x1000000,
+	TRACE_ITER_DISPLAY_GRAPH	= 0x2000000,
 };
 
 /*

commit 03905582fd093940cf609956adf6feb494e45346
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Sep 28 15:37:49 2015 -0400

    tracing: Move "display-graph" option to main options
    
    In order to facilitate making all tracer options visible even when the
    tracer is not active, we need to get rid of duplicate options. Any option
    that is shared between multiple tracers really should be a main option.
    
    As the wakeup and irqsoff tracers both use the "display-graph" option, and
    use it exactly the same way, move that option from the tracer options to the
    main options and consolidate them.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 107cf081ac27..dfa3cd2feb22 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -913,6 +913,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_IRQ_INFO		= 0x800000,
 	TRACE_ITER_MARKERS		= 0x1000000,
 	TRACE_ITER_FUNCTION		= 0x2000000,
+	TRACE_ITER_DISPLAY_GRAPH	= 0x4000000,
 };
 
 /*

commit ca475e831fd59e131bccd60de43c4104d82d02f5
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Sep 28 09:41:11 2015 -0400

    tracing: Make ftrace_trace_stack() static
    
    ftrace_trace_stack() is not called outside of trace.c. Make it a static
    function.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3b2a950a6291..107cf081ac27 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -611,20 +611,12 @@ void update_max_tr_single(struct trace_array *tr,
 #endif /* CONFIG_TRACER_MAX_TRACE */
 
 #ifdef CONFIG_STACKTRACE
-void ftrace_trace_stack(struct ring_buffer *buffer, unsigned long flags,
-			int skip, int pc);
-
 void ftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags,
 			    int pc);
 
 void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
 		   int pc);
 #else
-static inline void ftrace_trace_stack(struct ring_buffer *buffer,
-				      unsigned long flags, int skip, int pc)
-{
-}
-
 static inline void ftrace_trace_userstack(struct ring_buffer *buffer,
 					  unsigned long flags, int pc)
 {

commit d78a461427d752bc1cd5b87515167453a18de7e3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 25 13:30:47 2015 -0400

    tracing: Remove ftrace_trace_stack_regs()
    
    ftrace_trace_stack_regs() is used in only one place, and because that is
    such a simple function, just move its code into the location that it was
    used in (trace_buffer_unlock_commit_regs()).
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 74bde81601a9..3b2a950a6291 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -614,9 +614,6 @@ void update_max_tr_single(struct trace_array *tr,
 void ftrace_trace_stack(struct ring_buffer *buffer, unsigned long flags,
 			int skip, int pc);
 
-void ftrace_trace_stack_regs(struct ring_buffer *buffer, unsigned long flags,
-			     int skip, int pc, struct pt_regs *regs);
-
 void ftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags,
 			    int pc);
 
@@ -628,12 +625,6 @@ static inline void ftrace_trace_stack(struct ring_buffer *buffer,
 {
 }
 
-static inline void ftrace_trace_stack_regs(struct ring_buffer *buffer,
-					   unsigned long flags, int skip,
-					   int pc, struct pt_regs *regs)
-{
-}
-
 static inline void ftrace_trace_userstack(struct ring_buffer *buffer,
 					  unsigned long flags, int pc)
 {

commit 6224beb12e190ff11f3c7d4bf50cb2922878f600
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jul 7 15:05:03 2015 -0400

    tracing: Have branch tracer use recursive field of task struct
    
    Fengguang Wu's tests triggered a bug in the branch tracer's start up
    test when CONFIG_DEBUG_PREEMPT set. This was because that config
    adds some debug logic in the per cpu field, which calls back into
    the branch tracer.
    
    The branch tracer has its own recursive checks, but uses a per cpu
    variable to implement it. If retrieving the per cpu variable calls
    back into the branch tracer, you can see how things will break.
    
    Instead of using a per cpu variable, use the trace_recursion field
    of the current task struct. Simply set a bit when entering the
    branch tracing and clear it when leaving. If the bit is set on
    entry, just don't do the tracing.
    
    There's also the case with lockdep, as the local_irq_save() called
    before the recursion can also trigger code that can call back into
    the function. Changing that to a raw_local_irq_save() will protect
    that as well.
    
    This prevents the recursion and the inevitable crash that follows.
    
    Link: http://lkml.kernel.org/r/20150630141803.GA28071@wfg-t540p.sh.intel.com
    
    Cc: stable@vger.kernel.org # 3.10+
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Tested-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f060716b02ae..74bde81601a9 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -444,6 +444,7 @@ enum {
 
 	TRACE_CONTROL_BIT,
 
+	TRACE_BRANCH_BIT,
 /*
  * Abuse of the trace_recursion.
  * As we need a way to maintain state if we are tracing the function

commit e382608254e06c8109f40044f5e693f2e04f3899
Merge: fcbc1777ce8b b44754d8262d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 14:02:43 2015 -0700

    Merge tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This patch series contains several clean ups and even a new trace
      clock "monitonic raw".  Also some enhancements to make the ring buffer
      even faster.  But the biggest and most noticeable change is the
      renaming of the ftrace* files, structures and variables that have to
      deal with trace events.
    
      Over the years I've had several developers tell me about their
      confusion with what ftrace is compared to events.  Technically,
      "ftrace" is the infrastructure to do the function hooks, which include
      tracing and also helps with live kernel patching.  But the trace
      events are a separate entity altogether, and the files that affect the
      trace events should not be named "ftrace".  These include:
    
        include/trace/ftrace.h         ->    include/trace/trace_events.h
        include/linux/ftrace_event.h   ->    include/linux/trace_events.h
    
      Also, functions that are specific for trace events have also been renamed:
    
        ftrace_print_*()               ->    trace_print_*()
        (un)register_ftrace_event()    ->    (un)register_trace_event()
        ftrace_event_name()            ->    trace_event_name()
        ftrace_trigger_soft_disabled() ->    trace_trigger_soft_disabled()
        ftrace_define_fields_##call()  ->    trace_define_fields_##call()
        ftrace_get_offsets_##call()    ->    trace_get_offsets_##call()
    
      Structures have been renamed:
    
        ftrace_event_file              ->    trace_event_file
        ftrace_event_{call,class}      ->    trace_event_{call,class}
        ftrace_event_buffer            ->    trace_event_buffer
        ftrace_subsystem_dir           ->    trace_subsystem_dir
        ftrace_event_raw_##call        ->    trace_event_raw_##call
        ftrace_event_data_offset_##call->    trace_event_data_offset_##call
        ftrace_event_type_funcs_##call ->    trace_event_type_funcs_##call
    
      And a few various variables and flags have also been updated.
    
      This has been sitting in linux-next for some time, and I have not
      heard a single complaint about this rename breaking anything.  Mostly
      because these functions, variables and structures are mostly internal
      to the tracing system and are seldom (if ever) used by anything
      external to that"
    
    * tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (33 commits)
      ring_buffer: Allow to exit the ring buffer benchmark immediately
      ring-buffer-benchmark: Fix the wrong type
      ring-buffer-benchmark: Fix the wrong param in module_param
      ring-buffer: Add enum names for the context levels
      ring-buffer: Remove useless unused tracing_off_permanent()
      ring-buffer: Give NMIs a chance to lock the reader_lock
      ring-buffer: Add trace_recursive checks to ring_buffer_write()
      ring-buffer: Allways do the trace_recursive checks
      ring-buffer: Move recursive check to per_cpu descriptor
      ring-buffer: Add unlikelys to make fast path the default
      tracing: Rename ftrace_get_offsets_##call() to trace_event_get_offsets_##call()
      tracing: Rename ftrace_define_fields_##call() to trace_event_define_fields_##call()
      tracing: Rename ftrace_event_type_funcs_##call to trace_event_type_funcs_##call
      tracing: Rename ftrace_data_offset_##call to trace_event_data_offset_##call
      tracing: Rename ftrace_raw_##call event structures to trace_event_raw_##call
      tracing: Rename ftrace_trigger_soft_disabled() to trace_trigger_soft_disabled()
      tracing: Rename FTRACE_EVENT_FL_* flags to EVENT_FILE_FL_*
      tracing: Rename struct ftrace_subsystem_dir to trace_subsystem_dir
      tracing: Rename ftrace_event_name() to trace_event_name()
      tracing: Rename FTRACE_MAX_EVENT to TRACE_EVENT_TYPE_MAX
      ...

commit cc9e4bde03f2b4cfba52406c021364cbd2a4a0f3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 25 18:19:37 2015 -0400

    tracing: Fix typo from "static inlin" to "static inline"
    
    The trace.h header when called without CONFIG_EVENT_TRACING enabled
    (seldom done), will not compile because of a typo in the protocol
    of trace_event_enum_update().
    
    Cc: stable@vger.kernel.org # 4.1+
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d2612016de94..3d2ad5f83e94 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1312,7 +1312,7 @@ void trace_event_init(void);
 void trace_event_enum_update(struct trace_enum_map **map, int len);
 #else
 static inline void __init trace_event_init(void) { }
-static inlin void trace_event_enum_update(struct trace_enum_map **map, int len) { }
+static inline void trace_event_enum_update(struct trace_enum_map **map, int len) { }
 #endif
 
 extern struct trace_iterator *tracepoint_print_iter;

commit 7967b3e0c40ff72fb2cf44d3b50e2cb388ef6c67
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 14:59:40 2015 -0400

    tracing: Rename struct ftrace_subsystem_dir to trace_subsystem_dir
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structure ftrace_subsystem_dir holds
    the information about trace event subsystems. It should not be named
    ftrace, rename it to trace_subsystem_dir.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 64de3837c383..4c41fcda83ed 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -992,7 +992,7 @@ struct event_subsystem {
 	int			ref_count;
 };
 
-struct ftrace_subsystem_dir {
+struct trace_subsystem_dir {
 	struct list_head		list;
 	struct event_subsystem		*subsystem;
 	struct trace_array		*tr;
@@ -1056,7 +1056,7 @@ extern void print_event_filter(struct trace_event_file *file,
 			       struct trace_seq *s);
 extern int apply_event_filter(struct trace_event_file *file,
 			      char *filter_string);
-extern int apply_subsystem_event_filter(struct ftrace_subsystem_dir *dir,
+extern int apply_subsystem_event_filter(struct trace_subsystem_dir *dir,
 					char *filter_string);
 extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);

commit 2425bcb9240f8c97d793cb31c8e8d8d0a843fa29
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 11:45:27 2015 -0400

    tracing: Rename ftrace_event_{call,class} to trace_event_{call,class}
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structures ftrace_event_call and
    ftrace_event_class have nothing to do with the function hooks, and are
    really trace_event structures. Rename ftrace_event_* to trace_event_*.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4e1715e55b38..64de3837c383 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -858,7 +858,7 @@ void ftrace_destroy_filter_files(struct ftrace_ops *ops);
 #define ftrace_destroy_filter_files(ops) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER && CONFIG_DYNAMIC_FTRACE */
 
-int ftrace_event_is_function(struct ftrace_event_call *call);
+int ftrace_event_is_function(struct trace_event_call *call);
 
 /*
  * struct trace_parser - servers for reading the user input separated by spaces
@@ -1061,13 +1061,13 @@ extern int apply_subsystem_event_filter(struct ftrace_subsystem_dir *dir,
 extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);
 extern int filter_assign_type(const char *type);
-extern int create_event_filter(struct ftrace_event_call *call,
+extern int create_event_filter(struct trace_event_call *call,
 			       char *filter_str, bool set_str,
 			       struct event_filter **filterp);
 extern void free_event_filter(struct event_filter *filter);
 
 struct ftrace_event_field *
-trace_find_event_field(struct ftrace_event_call *call, char *name);
+trace_find_event_field(struct trace_event_call *call, char *name);
 
 extern void trace_event_enable_cmd_record(bool enable);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
@@ -1286,7 +1286,7 @@ int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\
-	extern struct ftrace_event_call					\
+	extern struct trace_event_call					\
 	__aligned(4) event_##call;
 #undef FTRACE_ENTRY_DUP
 #define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print, filter)	\
@@ -1295,7 +1295,7 @@ int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 #include "trace_entries.h"
 
 #if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_FUNCTION_TRACER)
-int perf_ftrace_event_register(struct ftrace_event_call *call,
+int perf_ftrace_event_register(struct trace_event_call *call,
 			       enum trace_reg type, void *data);
 #else
 #define perf_ftrace_event_register NULL

commit 7f1d2f8210195c8c309d424a77dbf06a6d2186f4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 10:09:53 2015 -0400

    tracing: Rename ftrace_event_file to trace_event_file
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structure ftrace_event_file is really
    about trace events and not "ftrace". Rename it to trace_event_file.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c09ecfed57db..4e1715e55b38 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -211,8 +211,8 @@ struct trace_array {
 #ifdef CONFIG_FTRACE_SYSCALLS
 	int			sys_refcount_enter;
 	int			sys_refcount_exit;
-	struct ftrace_event_file __rcu *enter_syscall_files[NR_syscalls];
-	struct ftrace_event_file __rcu *exit_syscall_files[NR_syscalls];
+	struct trace_event_file __rcu *enter_syscall_files[NR_syscalls];
+	struct trace_event_file __rcu *exit_syscall_files[NR_syscalls];
 #endif
 	int			stop_count;
 	int			clock_id;
@@ -1052,9 +1052,9 @@ struct filter_pred {
 
 extern enum regex_type
 filter_parse_regex(char *buff, int len, char **search, int *not);
-extern void print_event_filter(struct ftrace_event_file *file,
+extern void print_event_filter(struct trace_event_file *file,
 			       struct trace_seq *s);
-extern int apply_event_filter(struct ftrace_event_file *file,
+extern int apply_event_filter(struct trace_event_file *file,
 			      char *filter_string);
 extern int apply_subsystem_event_filter(struct ftrace_subsystem_dir *dir,
 					char *filter_string);
@@ -1073,9 +1073,9 @@ extern void trace_event_enable_cmd_record(bool enable);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 extern int event_trace_del_tracer(struct trace_array *tr);
 
-extern struct ftrace_event_file *find_event_file(struct trace_array *tr,
-						 const char *system,
-						 const char *event);
+extern struct trace_event_file *find_event_file(struct trace_array *tr,
+						const char *system,
+						const char *event);
 
 static inline void *event_file_data(struct file *filp)
 {
@@ -1242,23 +1242,23 @@ struct event_command {
 	enum event_trigger_type	trigger_type;
 	bool			post_trigger;
 	int			(*func)(struct event_command *cmd_ops,
-					struct ftrace_event_file *file,
+					struct trace_event_file *file,
 					char *glob, char *cmd, char *params);
 	int			(*reg)(char *glob,
 				       struct event_trigger_ops *ops,
 				       struct event_trigger_data *data,
-				       struct ftrace_event_file *file);
+				       struct trace_event_file *file);
 	void			(*unreg)(char *glob,
 					 struct event_trigger_ops *ops,
 					 struct event_trigger_data *data,
-					 struct ftrace_event_file *file);
+					 struct trace_event_file *file);
 	int			(*set_filter)(char *filter_str,
 					      struct event_trigger_data *data,
-					      struct ftrace_event_file *file);
+					      struct trace_event_file *file);
 	struct event_trigger_ops *(*get_trigger_ops)(char *cmd, char *param);
 };
 
-extern int trace_event_enable_disable(struct ftrace_event_file *file,
+extern int trace_event_enable_disable(struct trace_event_file *file,
 				      int enable, int soft_disable);
 extern int tracing_alloc_snapshot(void);
 

commit af658dca221207174fc0a7bcdcd4cff7c589fdd8
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 29 14:36:05 2015 -0400

    tracing: Rename ftrace_event.h to trace_events.h
    
    The term "ftrace" is really the infrastructure of the function hooks,
    and not the trace events. Rename ftrace_event.h to trace_events.h to
    represent the trace_event infrastructure and decouple the term ftrace
    from it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d2612016de94..c09ecfed57db 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -12,7 +12,7 @@
 #include <linux/ftrace.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/trace_seq.h>
-#include <linux/ftrace_event.h>
+#include <linux/trace_events.h>
 #include <linux/compiler.h>
 #include <linux/trace_seq.h>
 
@@ -1180,7 +1180,7 @@ struct event_trigger_ops {
  *	commands need to do this if they themselves log to the trace
  *	buffer (see the @post_trigger() member below).  @trigger_type
  *	values are defined by adding new values to the trigger_type
- *	enum in include/linux/ftrace_event.h.
+ *	enum in include/linux/trace_events.h.
  *
  * @post_trigger: A flag that says whether or not this command needs
  *	to have its action delayed until after the current event has

commit eeee78cf77df0450ca285a7cd6d73842181e825c
Merge: 3f3c73de77b5 9828413d4715
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 10:49:03 2015 -0700

    Merge tag 'trace-v4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Some clean ups and small fixes, but the biggest change is the addition
      of the TRACE_DEFINE_ENUM() macro that can be used by tracepoints.
    
      Tracepoints have helper functions for the TP_printk() called
      __print_symbolic() and __print_flags() that lets a numeric number be
      displayed as a a human comprehensible text.  What is placed in the
      TP_printk() is also shown in the tracepoint format file such that user
      space tools like perf and trace-cmd can parse the binary data and
      express the values too.  Unfortunately, the way the TRACE_EVENT()
      macro works, anything placed in the TP_printk() will be shown pretty
      much exactly as is.  The problem arises when enums are used.  That's
      because unlike macros, enums will not be changed into their values by
      the C pre-processor.  Thus, the enum string is exported to the format
      file, and this makes it useless for user space tools.
    
      The TRACE_DEFINE_ENUM() solves this by converting the enum strings in
      the TP_printk() format into their number, and that is what is shown to
      user space.  For example, the tracepoint tlb_flush currently has this
      in its format file:
    
         __print_symbolic(REC->reason,
            { TLB_FLUSH_ON_TASK_SWITCH, "flush on task switch" },
            { TLB_REMOTE_SHOOTDOWN, "remote shootdown" },
            { TLB_LOCAL_SHOOTDOWN, "local shootdown" },
            { TLB_LOCAL_MM_SHOOTDOWN, "local mm shootdown" })
    
      After adding:
    
         TRACE_DEFINE_ENUM(TLB_FLUSH_ON_TASK_SWITCH);
         TRACE_DEFINE_ENUM(TLB_REMOTE_SHOOTDOWN);
         TRACE_DEFINE_ENUM(TLB_LOCAL_SHOOTDOWN);
         TRACE_DEFINE_ENUM(TLB_LOCAL_MM_SHOOTDOWN);
    
      Its format file will contain this:
    
         __print_symbolic(REC->reason,
            { 0, "flush on task switch" },
            { 1, "remote shootdown" },
            { 2, "local shootdown" },
            { 3, "local mm shootdown" })"
    
    * tag 'trace-v4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (27 commits)
      tracing: Add enum_map file to show enums that have been mapped
      writeback: Export enums used by tracepoint to user space
      v4l: Export enums used by tracepoints to user space
      SUNRPC: Export enums in tracepoints to user space
      mm: tracing: Export enums in tracepoints to user space
      irq/tracing: Export enums in tracepoints to user space
      f2fs: Export the enums in the tracepoints to userspace
      net/9p/tracing: Export enums in tracepoints to userspace
      x86/tlb/trace: Export enums in used by tlb_flush tracepoint
      tracing/samples: Update the trace-event-sample.h with TRACE_DEFINE_ENUM()
      tracing: Allow for modules to convert their enums to values
      tracing: Add TRACE_DEFINE_ENUM() macro to map enums to their values
      tracing: Update trace-event-sample with TRACE_SYSTEM_VAR documentation
      tracing: Give system name a pointer
      brcmsmac: Move each system tracepoints to their own header
      iwlwifi: Move each system tracepoints to their own header
      mac80211: Move message tracepoints to their own header
      tracing: Add TRACE_SYSTEM_VAR to xhci-hcd
      tracing: Add TRACE_SYSTEM_VAR to kvm-s390
      tracing: Add TRACE_SYSTEM_VAR to intel-sst
      ...

commit 0c564a538aa934ad15b2145aaf8b64f3feb0be63
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Mar 24 17:58:09 2015 -0400

    tracing: Add TRACE_DEFINE_ENUM() macro to map enums to their values
    
    Several tracepoints use the helper functions __print_symbolic() or
    __print_flags() and pass in enums that do the mapping between the
    binary data stored and the value to print. This works well for reading
    the ASCII trace files, but when the data is read via userspace tools
    such as perf and trace-cmd, the conversion of the binary value to a
    human string format is lost if an enum is used, as userspace does not
    have access to what the ENUM is.
    
    For example, the tracepoint trace_tlb_flush() has:
    
     __print_symbolic(REC->reason,
        { TLB_FLUSH_ON_TASK_SWITCH, "flush on task switch" },
        { TLB_REMOTE_SHOOTDOWN, "remote shootdown" },
        { TLB_LOCAL_SHOOTDOWN, "local shootdown" },
        { TLB_LOCAL_MM_SHOOTDOWN, "local mm shootdown" })
    
    Which maps the enum values to the strings they represent. But perf and
    trace-cmd do no know what value TLB_LOCAL_MM_SHOOTDOWN is, and would
    not be able to map it.
    
    With TRACE_DEFINE_ENUM(), developers can place these in the event header
    files and ftrace will convert the enums to their values:
    
    By adding:
    
     TRACE_DEFINE_ENUM(TLB_FLUSH_ON_TASK_SWITCH);
     TRACE_DEFINE_ENUM(TLB_REMOTE_SHOOTDOWN);
     TRACE_DEFINE_ENUM(TLB_LOCAL_SHOOTDOWN);
     TRACE_DEFINE_ENUM(TLB_LOCAL_MM_SHOOTDOWN);
    
     $ cat /sys/kernel/debug/tracing/events/tlb/tlb_flush/format
    [...]
     __print_symbolic(REC->reason,
        { 0, "flush on task switch" },
        { 1, "remote shootdown" },
        { 2, "local shootdown" },
        { 3, "local mm shootdown" })
    
    The above is what userspace expects to see, and tools do not need to
    be modified to parse them.
    
    Link: http://lkml.kernel.org/r/20150403013802.220157513@goodmis.org
    
    Cc: Guilherme Cox <cox@computer.org>
    Cc: Tony Luck <tony.luck@gmail.com>
    Cc: Xie XiuQi <xiexiuqi@huawei.com>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dd8205a35760..b48d4b08f691 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1309,8 +1309,10 @@ static inline void init_ftrace_syscalls(void) { }
 
 #ifdef CONFIG_EVENT_TRACING
 void trace_event_init(void);
+void trace_event_enum_update(struct trace_enum_map **map, int len);
 #else
 static inline void __init trace_event_init(void) { }
+static inlin void trace_event_enum_update(struct trace_enum_map **map, int len) { }
 #endif
 
 extern struct trace_iterator *tracepoint_print_iter;

commit 8434dc9340cd2e117fc944cf7526263bf490a52a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 20 12:13:40 2015 -0500

    tracing: Convert the tracing facility over to use tracefs
    
    debugfs was fine for the tracing facility as a quick way to get
    an interface. Now that tracing has matured, it should separate itself
    from debugfs such that it can be mounted separately without needing
    to mount all of debugfs with it. That is, users resist using tracing
    because it requires mounting debugfs. Having tracing have its own file
    system lets users get the features of tracing without needing to bring
    in the rest of the kernel's debug infrastructure.
    
    Another reason for tracefs is that debubfs does not support mkdir.
    Currently, to create instances, one does a mkdir in the tracing/instance
    directory. This is implemented via a hack that forces debugfs to do
    something it is not intended on doing. By converting over to tracefs, this
    hack can be removed and mkdir can be properly implemented. This patch does
    not address this yet, but it lays the ground work for that to be done.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dd8205a35760..d951deddec89 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -334,7 +334,7 @@ struct tracer_flags {
 
 
 /**
- * struct tracer - a specific tracer and its callbacks to interact with debugfs
+ * struct tracer - a specific tracer and its callbacks to interact with tracefs
  * @name: the name chosen to select it on the available_tracers file
  * @init: called when one switches to this tracer (echo name > current_tracer)
  * @reset: called when one switches to another tracer

commit c602894814adc93589dde028182101818c5f938b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Jan 26 20:38:39 2015 -0500

    tracing: Make tracing_init_dentry_tr() static
    
    tracing_init_dentry_tr() is not used outside of trace.c, it should
    be static.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0eddfeb05fee..dd8205a35760 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -542,7 +542,6 @@ struct dentry *trace_create_file(const char *name,
 				 void *data,
 				 const struct file_operations *fops);
 
-struct dentry *tracing_init_dentry_tr(struct trace_array *tr);
 struct dentry *tracing_init_dentry(void);
 
 struct ring_buffer_event;

commit cf6ab6d9143b157786bf29bca5c32e55234bb07d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Dec 15 20:13:31 2014 -0500

    tracing: Add ref count to tracer for when they are being read by pipe
    
    When one of the trace pipe files are being read (by either the trace_pipe
    or trace_pipe_raw), do not allow the current_trace to change. By adding
    a ref count that is incremented when the pipe files are opened, will
    prevent the current_trace from being changed.
    
    This will allow for the removal of the global trace_types_lock from
    reading the pipe buffers (which is currently a bottle neck).
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8de48bac1ce2..0eddfeb05fee 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -388,6 +388,7 @@ struct tracer {
 	struct tracer		*next;
 	struct tracer_flags	*flags;
 	int			enabled;
+	int			ref;
 	bool			print_max;
 	bool			allow_instances;
 #ifdef CONFIG_TRACER_MAX_TRACE

commit 0daa2302968c13b657118d6ac92471f8fd2f3f28
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 12 22:27:10 2014 -0500

    tracing: Add tp_printk cmdline to have tracepoints go to printk()
    
    Add the kernel command line tp_printk option that will have tracepoints
    that are active sent to printk() as well as to the trace buffer.
    
    Passing "tp_printk" will activate this. To turn it off, the sysctl
    /proc/sys/kernel/tracepoint_printk can have '0' echoed into it. Note,
    this only works if the cmdline option is used. Echoing 1 into the sysctl
    file without the cmdline option will have no affect.
    
    Note, this is a dangerous option. Having high frequency tracepoints send
    their data to printk() can possibly cause a live lock. This is another
    reason why this is only active if the command line option is used.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1412121539300.16494@nanos
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c138c149d6ef..8de48bac1ce2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1313,5 +1313,6 @@ void trace_event_init(void);
 static inline void __init trace_event_init(void) { }
 #endif
 
+extern struct trace_iterator *tracepoint_print_iter;
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 5f893b2639b21ffe6834b1aebba392c37d2b83f9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 12 20:05:10 2014 -0500

    tracing: Move enabling tracepoints to just after rcu_init()
    
    Enabling tracepoints at boot up can be very useful. The tracepoint
    can be initialized right after RCU has been. There's no need to
    wait for the early_initcall() to be called. That's too late for some
    things that can use tracepoints for debugging. Move the logic to
    enable tracepoints out of the initcalls and into init/main.c to
    right after rcu_init().
    
    This also allows trace_printk() to be used early too.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1412121539300.16494@nanos
    Link: http://lkml.kernel.org/r/20141214164104.307127356@goodmis.org
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3255dfb054a0..c138c149d6ef 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1301,4 +1301,17 @@ int perf_ftrace_event_register(struct ftrace_event_call *call,
 #define perf_ftrace_event_register NULL
 #endif
 
+#ifdef CONFIG_FTRACE_SYSCALLS
+void init_ftrace_syscalls(void);
+#else
+static inline void init_ftrace_syscalls(void) { }
+#endif
+
+#ifdef CONFIG_EVENT_TRACING
+void trace_event_init(void);
+#else
+static inline void __init trace_event_init(void) { }
+#endif
+
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 8e1e1df29d837c589c8b4d7b49864481ff7795b8
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Mon Nov 24 09:34:19 2014 +0900

    tracing: Add additional marks to signal very large time deltas
    
    Currently, function graph tracer prints "!" or "+" just before
    function execution time to signal a function overhead, depending
    on the time. And some tracers tracing latency also print "!" or
    "+" just after time to signal overhead, depending on the interval
    between events. Even it is usually enough to do that, we sometimes
    need to signal for bigger execution time than 100 micro seconds.
    
    For example, I used function graph tracer to detect if there is
    any case that exit_mm() takes too much time. I did following steps
    in /sys/kernel/debug/tracing. It was easier to detect very large
    excution time with patched kernel than with original kernel.
    
    $ echo exit_mm > set_graph_function
    $ echo function_graph > current_tracer
    $ echo > trace
    $ cat trace_pipe > $LOGFILE
     ... (do something and terminate logging)
    $ grep "\\$" $LOGFILE
     3) $ 22082032 us |                      } /* kernel_map_pages */
     3) $ 22082040 us |                    } /* free_pages_prepare */
     3) $ 22082113 us |                  } /* free_hot_cold_page */
     3) $ 22083455 us |                } /* free_hot_cold_page_list */
     3) $ 22083895 us |              } /* release_pages */
     3) $ 22177873 us |            } /* free_pages_and_swap_cache */
     3) $ 22178929 us |          } /* unmap_single_vma */
     3) $ 22198885 us |        } /* unmap_vmas */
     3) $ 22206949 us |      } /* exit_mmap */
     3) $ 22207659 us |    } /* mmput */
     3) $ 22207793 us |  } /* exit_mm */
    
    And then, it was easy to find out that a schedule-out occured by
    sub_preempt_count() within kernel_map_pages().
    
    To detect very large function exection time caused by either problematic
    function implementation or scheduling issues, this patch can be useful.
    
    Link: http://lkml.kernel.org/r/1416789259-24038-1-git-send-email-byungchul.park@lge.com
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c3a37e55ec8b..3255dfb054a0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -708,6 +708,8 @@ enum print_line_t print_trace_line(struct trace_iterator *iter);
 
 extern unsigned long trace_flags;
 
+extern char trace_find_mark(unsigned long long duration);
+
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 

commit 9d9add34ec7b2cdd438b0b26481f8d1861bde45c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 12 14:57:38 2014 -0500

    tracing: Have function_graph use trace_seq_has_overflowed()
    
    Instead of doing individual checks all over the place that makes the code
    very messy. Just check trace_seq_has_overflowed() at the end or in
    strategic places.
    
    This makes the code much cleaner and also helps with getting closer
    to removing the return values of trace_seq_printf() and friends.
    
    Link: http://lkml.kernel.org/r/20141114011410.987913836@goodmis.org
    
    Reviewed-by: Petr Mladek <pmladek@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 19418221b302..c3a37e55ec8b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -726,7 +726,7 @@ extern unsigned long trace_flags;
 extern enum print_line_t
 print_graph_function_flags(struct trace_iterator *iter, u32 flags);
 extern void print_graph_headers_flags(struct seq_file *s, u32 flags);
-extern enum print_line_t
+extern void
 trace_print_graph_duration(unsigned long long duration, struct trace_seq *s);
 extern void graph_trace_open(struct trace_iterator *iter);
 extern void graph_trace_close(struct trace_iterator *iter);

commit 19a7fe206232cc875a3083211e0a21c08edd756e
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 12 10:29:54 2014 -0500

    tracing: Add trace_seq_has_overflowed() and trace_handle_return()
    
    Adding a trace_seq_has_overflowed() which returns true if the trace_seq
    had too much written into it allows us to simplify the code.
    
    Instead of checking the return value of every call to trace_seq_printf()
    and friends, they can all be called normally, and at the end we can
    return !trace_seq_has_overflowed() instead.
    
    Several functions also return TRACE_TYPE_PARTIAL_LINE when the trace_seq
    overflowed and TRACE_TYPE_HANDLED otherwise. Another helper function
    was created called trace_handle_return() which takes a trace_seq and
    returns these enums. Using this helper function also simplifies the
    code.
    
    This change also makes it possible to remove the return values of
    trace_seq_printf() and friends. They should instead just be
    void functions.
    
    Link: http://lkml.kernel.org/r/20141114011410.365183157@goodmis.org
    
    Reviewed-by: Petr Mladek <pmladek@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3376de623ea0..19418221b302 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -14,6 +14,7 @@
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
 #include <linux/compiler.h>
+#include <linux/trace_seq.h>
 
 #ifdef CONFIG_FTRACE_SYSCALLS
 #include <asm/unistd.h>		/* For NR_SYSCALLS	     */

commit 243f7610a68a606eb1787c09450a440bf30bebe0
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Oct 30 20:44:53 2014 -0400

    tracing: Move tracing_sched_{switch,wakeup}() into wakeup tracer
    
    The only code that references tracing_sched_switch_trace() and
    tracing_sched_wakeup_trace() is the wakeup latency tracer. Those
    two functions use to belong to the sched_switch tracer which has
    long been removed. These functions were left behind because the
    wakeup latency tracer used them. But since the wakeup latency tracer
    is the only one to use them, they should be static functions inside
    that code.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a3a82d5f25dc..3376de623ea0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -569,15 +569,6 @@ void trace_init_global_iter(struct trace_iterator *iter);
 
 void tracing_iter_reset(struct trace_iterator *iter, int cpu);
 
-void tracing_sched_switch_trace(struct trace_array *tr,
-				struct task_struct *prev,
-				struct task_struct *next,
-				unsigned long flags, int pc);
-
-void tracing_sched_wakeup_trace(struct trace_array *tr,
-				struct task_struct *wakee,
-				struct task_struct *cur,
-				unsigned long flags, int pc);
 void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,

commit 632537256e9f969a188cc4d0159e0027a459d3e7
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 23 21:35:01 2014 +0200

    tracing: Kill tracing_{start,stop}_sched_switch_record() and tracing_sched_switch_assign_trace()
    
    tracing_{start,stop}_sched_switch_record() have no callers since
    87d80de2800d "tracing: Remove obsolete sched_switch tracer".
    
    The last caller of tracing_sched_switch_assign_trace() was removed
    by 30dbb20e68e6 "tracing: Remove boot tracer".
    
    Link: http://lkml.kernel.org/p/20140723193501.GA30214@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 385391fb1d3b..a3a82d5f25dc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -597,9 +597,6 @@ void set_graph_array(struct trace_array *tr);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
-void tracing_sched_switch_assign_trace(struct trace_array *tr);
-void tracing_stop_sched_switch_record(void);
-void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 int is_tracing_stopped(void);
 

commit 6508fa761c330a1d2b4ae36199d08dbcb70e3ddb
Author: Stanislav Fomichev <stfomichev@yandex-team.ru>
Date:   Fri Jul 18 15:17:27 2014 +0400

    tracing: let user specify tracing_thresh after selecting function_graph
    
    Currently, tracing_thresh works only if we specify it before selecting
    function_graph tracer. If we do the opposite, tracing_thresh will change
    it's value, but it will not be applied.
    To fix it, we add update_thresh callback which is called whenever
    tracing_thresh is updated and for function_graph tracer we register
    handler which reinitializes tracer depending on tracing_thresh.
    
    Link: http://lkml.kernel.org/p/20140718111727.GA3206@stfomichev-desktop.yandex.net
    
    Signed-off-by: Stanislav Fomichev <stfomichev@yandex-team.ru>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9258f5a815db..385391fb1d3b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -339,6 +339,7 @@ struct tracer_flags {
  * @reset: called when one switches to another tracer
  * @start: called when tracing is unpaused (echo 1 > tracing_enabled)
  * @stop: called when tracing is paused (echo 0 > tracing_enabled)
+ * @update_thresh: called when tracing_thresh is updated
  * @open: called when the trace file is opened
  * @pipe_open: called when the trace_pipe file is opened
  * @close: called when the trace file is released
@@ -357,6 +358,7 @@ struct tracer {
 	void			(*reset)(struct trace_array *tr);
 	void			(*start)(struct trace_array *tr);
 	void			(*stop)(struct trace_array *tr);
+	int			(*update_thresh)(struct trace_array *tr);
 	void			(*open)(struct trace_iterator *iter);
 	void			(*pipe_open)(struct trace_iterator *iter);
 	void			(*close)(struct trace_iterator *iter);

commit da9c3413a27be5ba6f996e90495c836dd30b8841
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jun 10 13:53:50 2014 -0400

    tracing: Fix check of ftrace_trace_arrays list_empty() check
    
    The check that tests if ftrace_trace_arrays is empty in
    top_trace_array(), uses the .prev pointer:
    
      if (list_empty(ftrace_trace_arrays.prev))
    
    instead of testing the variable itself:
    
      if (list_empty(&ftrace_trace_arrays))
    
    Although it is technically correct, it is awkward and confusing.
    Use the proper method.
    
    Link: http://lkml.kernel.org/r/87oay1bas8.fsf@sejong.aot.lge.com
    
    Reported-by: Namhyung Kim <namhyung@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9e82551dd566..9258f5a815db 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -252,7 +252,7 @@ static inline struct trace_array *top_trace_array(void)
 {
 	struct trace_array *tr;
 
-	if (list_empty(ftrace_trace_arrays.prev))
+	if (list_empty(&ftrace_trace_arrays))
 		return NULL;
 
 	tr = list_entry(ftrace_trace_arrays.prev,

commit dc81e5e3abb9f98a3cb6f269c0bee595b2c1235d
Author: Yoshihiro YUNOMAE <yoshihiro.yunomae.ez@hitachi.com>
Date:   Fri Jun 6 07:35:17 2014 +0900

    tracing: Return error if ftrace_trace_arrays list is empty
    
    ftrace_trace_arrays links global_trace.list. However, global_trace
    is not added to ftrace_trace_arrays if trace_alloc_buffers() failed.
    As the result, ftrace_trace_arrays becomes an empty list. If
    ftrace_trace_arrays is an empty list, current top_trace_array() returns
    an invalid pointer. As the result, the kernel can induce memory corruption
    or panic.
    
    Current implementation does not check whether ftrace_trace_arrays is empty
    list or not. So, in this patch, if ftrace_trace_arrays is empty list,
    top_trace_array() returns NULL. Moreover, this patch makes all functions
    calling top_trace_array() handle it appropriately.
    
    Link: http://lkml.kernel.org/p/20140605223517.32311.99233.stgit@yunodevel
    
    Signed-off-by: Yoshihiro YUNOMAE <yoshihiro.yunomae.ez@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 217207ad60b3..9e82551dd566 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -252,6 +252,9 @@ static inline struct trace_array *top_trace_array(void)
 {
 	struct trace_array *tr;
 
+	if (list_empty(ftrace_trace_arrays.prev))
+		return NULL;
+
 	tr = list_entry(ftrace_trace_arrays.prev,
 			typeof(*tr), list);
 	WARN_ON(!(tr->flags & TRACE_ARRAY_FL_GLOBAL));

commit 607e3a29203633eaec7334f2f58f9653df788a06
Author: Robert Elliott <elliott@hp.com>
Date:   Tue May 20 17:10:51 2014 -0500

    tracing: Add funcgraph_tail option to print function name after closing braces
    
    In the function-graph tracer, add a funcgraph_tail option
    to print the function name on all } lines, not just
    functions whose first line is no longer in the trace
    buffer.
    
    If a function calls other traced functions, its total
    time appears on its } line.  This change allows grep
    to be used to determine the function for which the
    line corresponds.
    
    Update Documentation/trace/ftrace.txt to describe
    this new option.
    
    Link: http://lkml.kernel.org/p/20140520221041.8359.6782.stgit@beardog.cce.hp.com
    
    Signed-off-by: Robert Elliott <elliott@hp.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 68050633255e..217207ad60b3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -725,6 +725,7 @@ extern unsigned long trace_flags;
 #define TRACE_GRAPH_PRINT_DURATION      0x10
 #define TRACE_GRAPH_PRINT_ABS_TIME      0x20
 #define TRACE_GRAPH_PRINT_IRQS          0x40
+#define TRACE_GRAPH_PRINT_TAIL          0x80
 #define TRACE_GRAPH_PRINT_FILL_SHIFT	28
 #define TRACE_GRAPH_PRINT_FILL_MASK	(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)
 

commit ccdb594653b1c0d5b0f3e6f8bf764c544ba0606d
Author: Robert Elliott <elliott@hp.com>
Date:   Tue May 20 17:10:36 2014 -0500

    tracing: Eliminate duplicate TRACE_GRAPH_PRINT_xx defines
    
    Eliminate duplicate TRACE_GRAPH_PRINT_xx defines
    in trace_functions_graph.c that are already in
    trace.h.
    
    Add TRACE_GRAPH_PRINT_IRQS to trace.h, which is
    the only one that is missing.
    
    Link: http://lkml.kernel.org/p/20140520221031.8359.24733.stgit@beardog.cce.hp.com
    
    Signed-off-by: Robert Elliott <elliott@hp.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3b3e09e61f33..68050633255e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -724,6 +724,7 @@ extern unsigned long trace_flags;
 #define TRACE_GRAPH_PRINT_PROC          0x8
 #define TRACE_GRAPH_PRINT_DURATION      0x10
 #define TRACE_GRAPH_PRINT_ABS_TIME      0x20
+#define TRACE_GRAPH_PRINT_IRQS          0x40
 #define TRACE_GRAPH_PRINT_FILL_SHIFT	28
 #define TRACE_GRAPH_PRINT_FILL_MASK	(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)
 

commit b1169cc69ba96b124df820904a6d3eb775491d7f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Apr 29 17:54:37 2014 -0400

    tracing: Remove mock up poll wait function
    
    Now that the ring buffer has a built in way to wake up readers
    when there's data, using irq_work such that it is safe to do it
    in any context. But it was still using the old "poor man's"
    wait polling that checks every 1/10 of a second to see if it
    should wake up a waiter. This makes the latency for a wake up
    excruciatingly long. No need to do that anymore.
    
    Completely remove the different wait_poll types from the tracers
    and have them all use the default one now.
    
    Reported-by: Johannes Berg <johannes@sipsolutions.net>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8624b5041466..3b3e09e61f33 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -338,7 +338,6 @@ struct tracer_flags {
  * @stop: called when tracing is paused (echo 0 > tracing_enabled)
  * @open: called when the trace file is opened
  * @pipe_open: called when the trace_pipe file is opened
- * @wait_pipe: override how the user waits for traces on trace_pipe
  * @close: called when the trace file is released
  * @pipe_close: called when the trace_pipe file is released
  * @read: override the default read callback on trace_pipe
@@ -357,7 +356,6 @@ struct tracer {
 	void			(*stop)(struct trace_array *tr);
 	void			(*open)(struct trace_iterator *iter);
 	void			(*pipe_open)(struct trace_iterator *iter);
-	void			(*wait_pipe)(struct trace_iterator *iter);
 	void			(*close)(struct trace_iterator *iter);
 	void			(*pipe_close)(struct trace_iterator *iter);
 	ssize_t			(*read)(struct trace_iterator *iter,
@@ -566,8 +564,6 @@ void trace_init_global_iter(struct trace_iterator *iter);
 
 void tracing_iter_reset(struct trace_iterator *iter, int cpu);
 
-void poll_wait_pipe(struct trace_iterator *iter);
-
 void tracing_sched_switch_trace(struct trace_array *tr,
 				struct task_struct *prev,
 				struct task_struct *next,

commit 7eea4fce0246fe3a15ad7f3bb8d0a56d1f9440e6
Author: Jiaxing Wang <wangjiaxing@insigma.com.cn>
Date:   Sun Apr 20 23:10:43 2014 +0800

    tracing/stack_trace: Skip 4 instead of 3 when using ftrace_ops_list_func
    
    When using ftrace_ops_list_func, we should skip 4 instead of 3,
    to avoid ftrace_call+0x5/0xb appearing in the stack trace:
    
            Depth    Size   Location    (110 entries)
            -----    ----   --------
      0)     2956       0   update_curr+0xe/0x1e0
      1)     2956      68   ftrace_call+0x5/0xb
      2)     2888      92   enqueue_entity+0x53/0xe80
      3)     2796      80   enqueue_task_fair+0x47/0x7e0
      4)     2716      28   enqueue_task+0x45/0x70
      5)     2688      12   activate_task+0x22/0x30
    
    Add a function using_ftrace_ops_list_func() to test for this while keeping
    ftrace_ops_list_func to remain static.
    
    Link: http://lkml.kernel.org/p/1398006644-5935-2-git-send-email-wangjiaxing@insigma.com.cn
    
    Signed-off-by: Jiaxing Wang <wangjiaxing@insigma.com.cn>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5d2f07d6746c..8624b5041466 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -830,6 +830,7 @@ void ftrace_destroy_function_files(struct trace_array *tr);
 void ftrace_init_global_array_ops(struct trace_array *tr);
 void ftrace_init_array_ops(struct trace_array *tr, ftrace_func_t func);
 void ftrace_reset_array_ops(struct trace_array *tr);
+int using_ftrace_ops_list_func(void);
 #else
 static inline int ftrace_trace_task(struct task_struct *task)
 {

commit 0b9b12c1b884eb34773312f15c194220025e0416
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 14 10:04:59 2014 -0500

    tracing: Move ftrace_max_lock into trace_array
    
    In preparation for having tracers enabled in instances, the max_lock
    should be unique as updating the max for one tracer is a separate
    operation than updating it for another tracer using a different max.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 644a8b533e1d..5d2f07d6746c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -192,6 +192,20 @@ struct trace_array {
 	bool			allocated_snapshot;
 	unsigned long		max_latency;
 #endif
+	/*
+	 * max_lock is used to protect the swapping of buffers
+	 * when taking a max snapshot. The buffers themselves are
+	 * protected by per_cpu spinlocks. But the action of the swap
+	 * needs its own lock.
+	 *
+	 * This is defined as a arch_spinlock_t in order to help
+	 * with performance when lockdep debugging is enabled.
+	 *
+	 * It is also used in other places outside the update_max_tr
+	 * so it needs to be defined outside of the
+	 * CONFIG_TRACER_MAX_TRACE.
+	 */
+	arch_spinlock_t		max_lock;
 	int			buffer_disabled;
 #ifdef CONFIG_FTRACE_SYSCALLS
 	int			sys_refcount_enter;

commit 6d9b3fa5e7f663bbfb9d2d80d46136f75319cb28
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 14 11:28:38 2014 -0500

    tracing: Move tracing_max_latency into trace_array
    
    In preparation for letting the latency tracers be used by instances,
    remove the global tracing_max_latency variable and add a max_latency
    field to the trace_array that the latency tracers will now use.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index df5256be64cd..644a8b533e1d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -190,6 +190,7 @@ struct trace_array {
 	 */
 	struct trace_buffer	max_buffer;
 	bool			allocated_snapshot;
+	unsigned long		max_latency;
 #endif
 	int			buffer_disabled;
 #ifdef CONFIG_FTRACE_SYSCALLS
@@ -599,8 +600,6 @@ extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 extern unsigned long tracing_thresh;
 
 #ifdef CONFIG_TRACER_MAX_TRACE
-extern unsigned long tracing_max_latency;
-
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);

commit 4104d326b670c2b66f575d2004daa28b2d1b4c8d
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 17:01:58 2014 -0500

    ftrace: Remove global function list and call function directly
    
    Instead of having a list of global functions that are called,
    as only one global function is allow to be enabled at a time, there's
    no reason to have a list.
    
    Instead, simply have all the users of the global ops, use the global ops
    directly, instead of registering their own ftrace_ops. Just switch what
    function is used before enabling the function tracer.
    
    This removes a lot of code as well as the complexity involved with it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2e29d7ba5a52..df5256be64cd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -416,13 +416,7 @@ enum {
 	TRACE_FTRACE_IRQ_BIT,
 	TRACE_FTRACE_SIRQ_BIT,
 
-	/* GLOBAL_BITs must be greater than FTRACE_BITs */
-	TRACE_GLOBAL_BIT,
-	TRACE_GLOBAL_NMI_BIT,
-	TRACE_GLOBAL_IRQ_BIT,
-	TRACE_GLOBAL_SIRQ_BIT,
-
-	/* INTERNAL_BITs must be greater than GLOBAL_BITs */
+	/* INTERNAL_BITs must be greater than FTRACE_BITs */
 	TRACE_INTERNAL_BIT,
 	TRACE_INTERNAL_NMI_BIT,
 	TRACE_INTERNAL_IRQ_BIT,
@@ -449,9 +443,6 @@ enum {
 #define TRACE_FTRACE_START	TRACE_FTRACE_BIT
 #define TRACE_FTRACE_MAX	((1 << (TRACE_FTRACE_START + TRACE_CONTEXT_BITS)) - 1)
 
-#define TRACE_GLOBAL_START	TRACE_GLOBAL_BIT
-#define TRACE_GLOBAL_MAX	((1 << (TRACE_GLOBAL_START + TRACE_CONTEXT_BITS)) - 1)
-
 #define TRACE_LIST_START	TRACE_INTERNAL_BIT
 #define TRACE_LIST_MAX		((1 << (TRACE_LIST_START + TRACE_CONTEXT_BITS)) - 1)
 
@@ -823,6 +814,9 @@ extern int ftrace_is_dead(void);
 int ftrace_create_function_files(struct trace_array *tr,
 				 struct dentry *parent);
 void ftrace_destroy_function_files(struct trace_array *tr);
+void ftrace_init_global_array_ops(struct trace_array *tr);
+void ftrace_init_array_ops(struct trace_array *tr, ftrace_func_t func);
+void ftrace_reset_array_ops(struct trace_array *tr);
 #else
 static inline int ftrace_trace_task(struct task_struct *task)
 {
@@ -836,6 +830,11 @@ ftrace_create_function_files(struct trace_array *tr,
 	return 0;
 }
 static inline void ftrace_destroy_function_files(struct trace_array *tr) { }
+static inline __init void
+ftrace_init_global_array_ops(struct trace_array *tr) { }
+static inline void ftrace_reset_array_ops(struct trace_array *tr) { }
+/* ftace_func_t type is not defined, use macro instead of static inline */
+#define ftrace_init_array_ops(tr, func) do { } while (0)
 #endif /* CONFIG_FUNCTION_TRACER */
 
 #if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_DYNAMIC_FTRACE)

commit 52f5684c8e1ec7463192aba8e2916df49807511a
Author: Gideon Israel Dsouza <gidisrael@gmail.com>
Date:   Mon Apr 7 15:39:20 2014 -0700

    kernel: use macros from compiler.h instead of __attribute__((...))
    
    To increase compiler portability there is <linux/compiler.h> which
    provides convenience macros for various gcc constructs.  Eg: __weak for
    __attribute__((weak)).  I've replaced all instances of gcc attributes
    with the right macro in the kernel subsystem.
    
    Signed-off-by: Gideon Israel Dsouza <gidisrael@gmail.com>
    Cc: "Rafael J. Wysocki" <rjw@sisk.pl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ffc314b7e92b..2e29d7ba5a52 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -13,6 +13,7 @@
 #include <linux/hw_breakpoint.h>
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
+#include <linux/compiler.h>
 
 #ifdef CONFIG_FTRACE_SYSCALLS
 #include <asm/unistd.h>		/* For NR_SYSCALLS	     */
@@ -1279,7 +1280,7 @@ int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\
 	extern struct ftrace_event_call					\
-	__attribute__((__aligned__(4))) event_##call;
+	__aligned(4) event_##call;
 #undef FTRACE_ENTRY_DUP
 #define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print, filter)	\
 	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print), \

commit 591dffdade9f07692a7dd3ed16830ec24e901ece
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 16:17:45 2014 -0500

    ftrace: Allow for function tracing instance to filter functions
    
    Create a "set_ftrace_filter" and "set_ftrace_notrace" files in the instance
    directories to let users filter of functions to trace for the given instance.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 35cca055da0f..ffc314b7e92b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -819,13 +819,36 @@ static inline int ftrace_trace_task(struct task_struct *task)
 	return test_tsk_trace_trace(task);
 }
 extern int ftrace_is_dead(void);
+int ftrace_create_function_files(struct trace_array *tr,
+				 struct dentry *parent);
+void ftrace_destroy_function_files(struct trace_array *tr);
 #else
 static inline int ftrace_trace_task(struct task_struct *task)
 {
 	return 1;
 }
 static inline int ftrace_is_dead(void) { return 0; }
-#endif
+static inline int
+ftrace_create_function_files(struct trace_array *tr,
+			     struct dentry *parent)
+{
+	return 0;
+}
+static inline void ftrace_destroy_function_files(struct trace_array *tr) { }
+#endif /* CONFIG_FUNCTION_TRACER */
+
+#if defined(CONFIG_FUNCTION_TRACER) && defined(CONFIG_DYNAMIC_FTRACE)
+void ftrace_create_filter_files(struct ftrace_ops *ops,
+				struct dentry *parent);
+void ftrace_destroy_filter_files(struct ftrace_ops *ops);
+#else
+/*
+ * The ops parameter passed in is usually undefined.
+ * This must be a macro.
+ */
+#define ftrace_create_filter_files(ops, parent) do { } while (0)
+#define ftrace_destroy_filter_files(ops) do { } while (0)
+#endif /* CONFIG_FUNCTION_TRACER && CONFIG_DYNAMIC_FTRACE */
 
 int ftrace_event_is_function(struct ftrace_event_call *call);
 

commit f20a580627f43e73e4e57cb37e3864080ca06088
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Nov 7 20:08:58 2013 -0500

    ftrace: Allow instances to use function tracing
    
    Allow instances (sub-buffers) to enable function tracing.
    Each instance will have its own function tracing capability.
    For now, instances will not have function stack tracing, or will
    they be able to pick and choose what functions they can trace.
    
    Picking and choosing their own functions will come later.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 86915b220bbe..35cca055da0f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -210,6 +210,11 @@ struct trace_array {
 	struct list_head	events;
 	cpumask_var_t		tracing_cpumask; /* only trace on set CPUs */
 	int			ref;
+#ifdef CONFIG_FUNCTION_TRACER
+	struct ftrace_ops	*ops;
+	/* function tracing enabled */
+	int			function_enabled;
+#endif
 };
 
 enum {

commit 50512ab576e1ce29953c9259e1f36ce16f350f20
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 14 08:52:35 2014 -0500

    tracing: Convert tracer->enabled to counter
    
    As tracers will soon be used by instances, the tracer enabled field
    needs to be converted to a counter instead of a boolean.
    This counter is protected by the trace_types_lock mutex.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ea51bb2004d2..86915b220bbe 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -362,8 +362,8 @@ struct tracer {
 						u32 mask, int set);
 	struct tracer		*next;
 	struct tracer_flags	*flags;
+	int			enabled;
 	bool			print_max;
-	bool			enabled;
 	bool			allow_instances;
 #ifdef CONFIG_TRACER_MAX_TRACE
 	bool			use_max_tr;

commit 607e2ea167e56db84387f3ab97e59a862e101cab
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 6 22:42:48 2013 -0500

    tracing: Set up infrastructure to allow tracers for instances
    
    Currently the tracers (function, function_graph, irqsoff, etc) can only
    be used by the top level tracing directory (not for instances).
    
    This sets up the infrastructure to allow instances to be able to
    run a separate tracer apart from the what the top level tracing is
    doing.
    
    As tracers need to adapt for being used by instances, the tracers
    must flag if they can be used by instances or not. Currently only the
    'nop' tracer can be used by all instances.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 36e44732c650..ea51bb2004d2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -364,6 +364,7 @@ struct tracer {
 	struct tracer_flags	*flags;
 	bool			print_max;
 	bool			enabled;
+	bool			allow_instances;
 #ifdef CONFIG_TRACER_MAX_TRACE
 	bool			use_max_tr;
 #endif

commit bf6065b5c7014ab30383405718c7a6b96d2cbdb2
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 17:51:01 2014 -0500

    tracing: Pass trace_array to flag_changed callback
    
    As options (flags) may affect instances instead of being global
    the flag_changed() callbacks need to receive the trace_array descriptor
    of the instance they will be modifying.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 649a23d421c1..36e44732c650 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -358,7 +358,7 @@ struct tracer {
 	int			(*set_flag)(struct trace_array *tr,
 					    u32 old_flags, u32 bit, int set);
 	/* Return 0 if OK with change, else return non-zero */
-	int			(*flag_changed)(struct tracer *tracer,
+	int			(*flag_changed)(struct trace_array *tr,
 						u32 mask, int set);
 	struct tracer		*next;
 	struct tracer_flags	*flags;

commit 8c1a49aedb73fb2f15aaa32ad9e2e1c4289f45cb
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jan 10 11:13:54 2014 -0500

    tracing: Pass trace_array to set_flag callback
    
    As options (flags) may affect instances instead of being global
    the set_flag() callbacks need to receive the trace_array descriptor
    of the instance they will be modifying.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 02b592f2d4b7..649a23d421c1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -355,7 +355,8 @@ struct tracer {
 	void			(*print_header)(struct seq_file *m);
 	enum print_line_t	(*print_line)(struct trace_iterator *iter);
 	/* If you handled the flag setting, return 0 */
-	int			(*set_flag)(u32 old_flags, u32 bit, int set);
+	int			(*set_flag)(struct trace_array *tr,
+					    u32 old_flags, u32 bit, int set);
 	/* Return 0 if OK with change, else return non-zero */
 	int			(*flag_changed)(struct tracer *tracer,
 						u32 mask, int set);

commit d8a30f20347a60a796a5221e07711c0d30d42dc3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Dec 21 21:55:17 2013 -0500

    tracing: Fix rcu handling of event_trigger_data filter field
    
    The filter field of the event_trigger_data structure is protected under
    RCU sched locks. It was not annotated as such, and after doing so,
    sparse pointed out several locations that required fix ups.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Tested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Acked-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0380cab0af9b..02b592f2d4b7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1057,7 +1057,7 @@ struct event_trigger_data {
 	int				ref;
 	struct event_trigger_ops	*ops;
 	struct event_command		*cmd_ops;
-	struct event_filter		*filter;
+	struct event_filter __rcu	*filter;
 	char				*filter_str;
 	void				*private_data;
 	struct list_head		list;

commit 098c879e1f2d6ee7afbfe959f6b04070065cec90
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Dec 21 17:39:40 2013 -0500

    tracing: Add generic tracing_lseek() function
    
    Trace event triggers added a lseek that uses the ftrace_filter_lseek()
    function. Unfortunately, when function tracing is not configured in
    that function is not defined and the kernel fails to build.
    
    This is the second time that function was added to a file ops and
    it broke the build due to requiring special config dependencies.
    
    Make a generic tracing_lseek() that all the tracing utilities may
    use.
    
    Also, modify the old ftrace_filter_lseek() to return 0 instead of
    1 on WRONLY. Not sure why it was a 1 as that does not make sense.
    
    This also changes the old tracing_seek() to modify the file pos
    pointer on WRONLY as well.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Tested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Acked-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 433bfc5dd576..0380cab0af9b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -588,6 +588,8 @@ void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 int is_tracing_stopped(void);
 
+loff_t tracing_lseek(struct file *file, loff_t offset, int whence);
+
 extern cpumask_var_t __read_mostly tracing_buffer_mask;
 
 #define for_each_tracing_cpu(cpu)	\

commit bac5fb97a173aeef8296b3efdb552e3489d55179
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:59:29 2013 -0500

    tracing: Add and use generic set_trigger_filter() implementation
    
    Add a generic event_command.set_trigger_filter() op implementation and
    have the current set of trigger commands use it - this essentially
    gives them all support for filters.
    
    Syntactically, filters are supported by adding 'if <filter>' just
    after the command, in which case only events matching the filter will
    invoke the trigger.  For example, to add a filter to an
    enable/disable_event command:
    
        echo 'enable_event:system:event if common_pid == 999' > \
                  .../othersys/otherevent/trigger
    
    The above command will only enable the system:event event if the
    common_pid field in the othersys:otherevent event is 999.
    
    As another example, to add a filter to a stacktrace command:
    
        echo 'stacktrace if common_pid == 999' > \
                       .../somesys/someevent/trigger
    
    The above command will only trigger a stacktrace if the common_pid
    field in the event is 999.
    
    The filter syntax is the same as that described in the 'Event
    filtering' section of Documentation/trace/events.txt.
    
    Because triggers can now use filters, the trigger-invoking logic needs
    to be moved in those cases - e.g. for ftrace_raw_event_calls, if a
    trigger has a filter associated with it, the trigger invocation now
    needs to happen after the { assign; } part of the call, in order for
    the trigger condition to be tested.
    
    There's still a SOFT_DISABLED-only check at the top of e.g. the
    ftrace_raw_events function, so when an event is soft disabled but not
    because of the presence of a trigger, the original SOFT_DISABLED
    behavior remains unchanged.
    
    There's also a bit of trickiness in that some triggers need to avoid
    being invoked while an event is currently in the process of being
    logged, since the trigger may itself log data into the trace buffer.
    Thus we make sure the current event is committed before invoking those
    triggers.  To do that, we split the trigger invocation in two - the
    first part (event_triggers_call()) checks the filter using the current
    trace record; if a command has the post_trigger flag set, it sets a
    bit for itself in the return value, otherwise it directly invoks the
    trigger.  Once all commands have been either invoked or set their
    return flag, event_triggers_call() returns.  The current record is
    then either committed or discarded; if any commands have deferred
    their triggers, those commands are finally invoked following the close
    of the current event by event_triggers_post_call().
    
    To simplify the above and make it more efficient, the TRIGGER_COND bit
    is introduced, which is set only if a soft-disabled trigger needs to
    use the log record for filter testing or needs to wait until the
    current log record is closed.
    
    The syscall event invocation code is also changed in analogous ways.
    
    Because event triggers need to be able to create and free filters,
    this also adds a couple external wrappers for the existing
    create_filter and free_filter functions, which are too generic to be
    made extern functions themselves.
    
    Link: http://lkml.kernel.org/r/7164930759d8719ef460357f143d995406e4eead.1382622043.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ccbd8104cf99..433bfc5dd576 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1,3 +1,4 @@
+
 #ifndef _LINUX_KERNEL_TRACE_H
 #define _LINUX_KERNEL_TRACE_H
 
@@ -1020,6 +1021,10 @@ extern int apply_subsystem_event_filter(struct ftrace_subsystem_dir *dir,
 extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);
 extern int filter_assign_type(const char *type);
+extern int create_event_filter(struct ftrace_event_call *call,
+			       char *filter_str, bool set_str,
+			       struct event_filter **filterp);
+extern void free_event_filter(struct event_filter *filter);
 
 struct ftrace_event_field *
 trace_find_event_field(struct ftrace_event_call *call, char *name);

commit 7862ad1846e994574cb47dc503cc2b1646ea6593
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:59:28 2013 -0500

    tracing: Add 'enable_event' and 'disable_event' event trigger commands
    
    Add 'enable_event' and 'disable_event' event_command commands.
    
    enable_event and disable_event event triggers are added by the user
    via these commands in a similar way and using practically the same
    syntax as the analagous 'enable_event' and 'disable_event' ftrace
    function commands, but instead of writing to the set_ftrace_filter
    file, the enable_event and disable_event triggers are written to the
    per-event 'trigger' files:
    
        echo 'enable_event:system:event' > .../othersys/otherevent/trigger
        echo 'disable_event:system:event' > .../othersys/otherevent/trigger
    
    The above commands will enable or disable the 'system:event' trace
    events whenever the othersys:otherevent events are hit.
    
    This also adds a 'count' version that limits the number of times the
    command will be invoked:
    
        echo 'enable_event:system:event:N' > .../othersys/otherevent/trigger
        echo 'disable_event:system:event:N' > .../othersys/otherevent/trigger
    
    Where N is the number of times the command will be invoked.
    
    The above commands will will enable or disable the 'system:event'
    trace events whenever the othersys:otherevent events are hit, but only
    N times.
    
    This also makes the find_event_file() helper function extern, since
    it's useful to use from other places, such as the event triggers code,
    so make it accessible.
    
    Link: http://lkml.kernel.org/r/f825f3048c3f6b026ee37ae5825f9fc373451828.1382622043.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 50723e5e2b3c..ccbd8104cf99 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1028,6 +1028,10 @@ extern void trace_event_enable_cmd_record(bool enable);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 extern int event_trace_del_tracer(struct trace_array *tr);
 
+extern struct ftrace_event_file *find_event_file(struct trace_array *tr,
+						 const char *system,
+						 const char *event);
+
 static inline void *event_file_data(struct file *filp)
 {
 	return ACCESS_ONCE(file_inode(filp)->i_private);

commit 93e31ffbf417a84fbae518fb46b3ea3f0d8fa6e1
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:59:26 2013 -0500

    tracing: Add 'snapshot' event trigger command
    
    Add 'snapshot' event_command.  snapshot event triggers are added by
    the user via this command in a similar way and using practically the
    same syntax as the analogous 'snapshot' ftrace function command, but
    instead of writing to the set_ftrace_filter file, the snapshot event
    trigger is written to the per-event 'trigger' files:
    
        echo 'snapshot' > .../somesys/someevent/trigger
    
    The above command will turn on snapshots for someevent i.e. whenever
    someevent is hit, a snapshot will be done.
    
    This also adds a 'count' version that limits the number of times the
    command will be invoked:
    
        echo 'snapshot:N' > .../somesys/someevent/trigger
    
    Where N is the number of times the command will be invoked.
    
    The above command will snapshot N times for someevent i.e. whenever
    someevent is hit N times, a snapshot will be done.
    
    Also adds a new tracing_alloc_snapshot() function - the existing
    tracing_snapshot_alloc() function is a special version of
    tracing_snapshot() that also does the snapshot allocation - the
    snapshot triggers would like to be able to do just the allocation but
    not take a snapshot; the existing tracing_snapshot_alloc() in turn now
    also calls tracing_alloc_snapshot() underneath to do that allocation.
    
    Link: http://lkml.kernel.org/r/c9524dd07ce01f9dcbd59011290e0a8d5b47d7ad.1382622043.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    [ fix up from kbuild test robot <fengguang.wu@intel.com report ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9775e518fe77..50723e5e2b3c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1211,6 +1211,7 @@ struct event_command {
 
 extern int trace_event_enable_disable(struct ftrace_event_file *file,
 				      int enable, int soft_disable);
+extern int tracing_alloc_snapshot(void);
 
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];

commit 85f2b08268c014e290b600ba49fa85530600eaa1
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:59:24 2013 -0500

    tracing: Add basic event trigger framework
    
    Add a 'trigger' file for each trace event, enabling 'trace event
    triggers' to be set for trace events.
    
    'trace event triggers' are patterned after the existing 'ftrace
    function triggers' implementation except that triggers are written to
    per-event 'trigger' files instead of to a single file such as the
    'set_ftrace_filter' used for ftrace function triggers.
    
    The implementation is meant to be entirely separate from ftrace
    function triggers, in order to keep the respective implementations
    relatively simple and to allow them to diverge.
    
    The event trigger functionality is built on top of SOFT_DISABLE
    functionality.  It adds a TRIGGER_MODE bit to the ftrace_event_file
    flags which is checked when any trace event fires.  Triggers set for a
    particular event need to be checked regardless of whether that event
    is actually enabled or not - getting an event to fire even if it's not
    enabled is what's already implemented by SOFT_DISABLE mode, so trigger
    mode directly reuses that.  Event trigger essentially inherit the soft
    disable logic in __ftrace_event_enable_disable() while adding a bit of
    logic and trigger reference counting via tm_ref on top of that in a
    new trace_event_trigger_enable_disable() function.  Because the base
    __ftrace_event_enable_disable() code now needs to be invoked from
    outside trace_events.c, a wrapper is also added for those usages.
    
    The triggers for an event are actually invoked via a new function,
    event_triggers_call(), and code is also added to invoke them for
    ftrace_raw_event calls as well as syscall events.
    
    The main part of the patch creates a new trace_events_trigger.c file
    to contain the trace event triggers implementation.
    
    The standard open, read, and release file operations are implemented
    here.
    
    The open() implementation sets up for the various open modes of the
    'trigger' file.  It creates and attaches the trigger iterator and sets
    up the command parser.  If opened for reading set up the trigger
    seq_ops.
    
    The read() implementation parses the event trigger written to the
    'trigger' file, looks up the trigger command, and passes it along to
    that event_command's func() implementation for command-specific
    processing.
    
    The release() implementation does whatever cleanup is needed to
    release the 'trigger' file, like releasing the parser and trigger
    iterator, etc.
    
    A couple of functions for event command registration and
    unregistration are added, along with a list to add them to and a mutex
    to protect them, as well as an (initially empty) registration function
    to add the set of commands that will be added by future commits, and
    call to it from the trace event initialization code.
    
    also added are a couple trigger-specific data structures needed for
    these implementations such as a trigger iterator and a struct for
    trigger-specific data.
    
    A couple structs consisting mostly of function meant to be implemented
    in command-specific ways, event_command and event_trigger_ops, are
    used by the generic event trigger command implementations.  They're
    being put into trace.h alongside the other trace_event data structures
    and functions, in the expectation that they'll be needed in several
    trace_event-related files such as trace_events_trigger.c and
    trace_events.c.
    
    The event_command.func() function is meant to be called by the trigger
    parsing code in order to add a trigger instance to the corresponding
    event.  It essentially coordinates adding a live trigger instance to
    the event, and arming the triggering the event.
    
    Every event_command func() implementation essentially does the
    same thing for any command:
    
       - choose ops - use the value of param to choose either a number or
         count version of event_trigger_ops specific to the command
       - do the register or unregister of those ops
       - associate a filter, if specified, with the triggering event
    
    The reg() and unreg() ops allow command-specific implementations for
    event_trigger_op registration and unregistration, and the
    get_trigger_ops() op allows command-specific event_trigger_ops
    selection to be parameterized.  When a trigger instance is added, the
    reg() op essentially adds that trigger to the triggering event and
    arms it, while unreg() does the opposite.  The set_filter() function
    is used to associate a filter with the trigger - if the command
    doesn't specify a set_filter() implementation, the command will ignore
    filters.
    
    Each command has an associated trigger_type, which serves double duty,
    both as a unique identifier for the command as well as a value that
    can be used for setting a trigger mode bit during trigger invocation.
    
    The signature of func() adds a pointer to the event_command struct,
    used to invoke those functions, along with a command_data param that
    can be passed to the reg/unreg functions.  This allows func()
    implementations to use command-specific blobs and supports code
    re-use.
    
    The event_trigger_ops.func() command corrsponds to the trigger 'probe'
    function that gets called when the triggering event is actually
    invoked.  The other functions are used to list the trigger when
    needed, along with a couple mundane book-keeping functions.
    
    This also moves event_file_data() into trace.h so it can be used
    outside of trace_events.c.
    
    Link: http://lkml.kernel.org/r/316d95061accdee070aac8e5750afba0192fa5b9.1382622043.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Idea-by: Steve Rostedt <rostedt@goodmis.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ea189e027b80..9775e518fe77 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1028,9 +1028,190 @@ extern void trace_event_enable_cmd_record(bool enable);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 extern int event_trace_del_tracer(struct trace_array *tr);
 
+static inline void *event_file_data(struct file *filp)
+{
+	return ACCESS_ONCE(file_inode(filp)->i_private);
+}
+
 extern struct mutex event_mutex;
 extern struct list_head ftrace_events;
 
+extern const struct file_operations event_trigger_fops;
+
+extern int register_trigger_cmds(void);
+extern void clear_event_triggers(struct trace_array *tr);
+
+struct event_trigger_data {
+	unsigned long			count;
+	int				ref;
+	struct event_trigger_ops	*ops;
+	struct event_command		*cmd_ops;
+	struct event_filter		*filter;
+	char				*filter_str;
+	void				*private_data;
+	struct list_head		list;
+};
+
+/**
+ * struct event_trigger_ops - callbacks for trace event triggers
+ *
+ * The methods in this structure provide per-event trigger hooks for
+ * various trigger operations.
+ *
+ * All the methods below, except for @init() and @free(), must be
+ * implemented.
+ *
+ * @func: The trigger 'probe' function called when the triggering
+ *	event occurs.  The data passed into this callback is the data
+ *	that was supplied to the event_command @reg() function that
+ *	registered the trigger (see struct event_command).
+ *
+ * @init: An optional initialization function called for the trigger
+ *	when the trigger is registered (via the event_command reg()
+ *	function).  This can be used to perform per-trigger
+ *	initialization such as incrementing a per-trigger reference
+ *	count, for instance.  This is usually implemented by the
+ *	generic utility function @event_trigger_init() (see
+ *	trace_event_triggers.c).
+ *
+ * @free: An optional de-initialization function called for the
+ *	trigger when the trigger is unregistered (via the
+ *	event_command @reg() function).  This can be used to perform
+ *	per-trigger de-initialization such as decrementing a
+ *	per-trigger reference count and freeing corresponding trigger
+ *	data, for instance.  This is usually implemented by the
+ *	generic utility function @event_trigger_free() (see
+ *	trace_event_triggers.c).
+ *
+ * @print: The callback function invoked to have the trigger print
+ *	itself.  This is usually implemented by a wrapper function
+ *	that calls the generic utility function @event_trigger_print()
+ *	(see trace_event_triggers.c).
+ */
+struct event_trigger_ops {
+	void			(*func)(struct event_trigger_data *data);
+	int			(*init)(struct event_trigger_ops *ops,
+					struct event_trigger_data *data);
+	void			(*free)(struct event_trigger_ops *ops,
+					struct event_trigger_data *data);
+	int			(*print)(struct seq_file *m,
+					 struct event_trigger_ops *ops,
+					 struct event_trigger_data *data);
+};
+
+/**
+ * struct event_command - callbacks and data members for event commands
+ *
+ * Event commands are invoked by users by writing the command name
+ * into the 'trigger' file associated with a trace event.  The
+ * parameters associated with a specific invocation of an event
+ * command are used to create an event trigger instance, which is
+ * added to the list of trigger instances associated with that trace
+ * event.  When the event is hit, the set of triggers associated with
+ * that event is invoked.
+ *
+ * The data members in this structure provide per-event command data
+ * for various event commands.
+ *
+ * All the data members below, except for @post_trigger, must be set
+ * for each event command.
+ *
+ * @name: The unique name that identifies the event command.  This is
+ *	the name used when setting triggers via trigger files.
+ *
+ * @trigger_type: A unique id that identifies the event command
+ *	'type'.  This value has two purposes, the first to ensure that
+ *	only one trigger of the same type can be set at a given time
+ *	for a particular event e.g. it doesn't make sense to have both
+ *	a traceon and traceoff trigger attached to a single event at
+ *	the same time, so traceon and traceoff have the same type
+ *	though they have different names.  The @trigger_type value is
+ *	also used as a bit value for deferring the actual trigger
+ *	action until after the current event is finished.  Some
+ *	commands need to do this if they themselves log to the trace
+ *	buffer (see the @post_trigger() member below).  @trigger_type
+ *	values are defined by adding new values to the trigger_type
+ *	enum in include/linux/ftrace_event.h.
+ *
+ * @post_trigger: A flag that says whether or not this command needs
+ *	to have its action delayed until after the current event has
+ *	been closed.  Some triggers need to avoid being invoked while
+ *	an event is currently in the process of being logged, since
+ *	the trigger may itself log data into the trace buffer.  Thus
+ *	we make sure the current event is committed before invoking
+ *	those triggers.  To do that, the trigger invocation is split
+ *	in two - the first part checks the filter using the current
+ *	trace record; if a command has the @post_trigger flag set, it
+ *	sets a bit for itself in the return value, otherwise it
+ *	directly invokes the trigger.  Once all commands have been
+ *	either invoked or set their return flag, the current record is
+ *	either committed or discarded.  At that point, if any commands
+ *	have deferred their triggers, those commands are finally
+ *	invoked following the close of the current event.  In other
+ *	words, if the event_trigger_ops @func() probe implementation
+ *	itself logs to the trace buffer, this flag should be set,
+ *	otherwise it can be left unspecified.
+ *
+ * All the methods below, except for @set_filter(), must be
+ * implemented.
+ *
+ * @func: The callback function responsible for parsing and
+ *	registering the trigger written to the 'trigger' file by the
+ *	user.  It allocates the trigger instance and registers it with
+ *	the appropriate trace event.  It makes use of the other
+ *	event_command callback functions to orchestrate this, and is
+ *	usually implemented by the generic utility function
+ *	@event_trigger_callback() (see trace_event_triggers.c).
+ *
+ * @reg: Adds the trigger to the list of triggers associated with the
+ *	event, and enables the event trigger itself, after
+ *	initializing it (via the event_trigger_ops @init() function).
+ *	This is also where commands can use the @trigger_type value to
+ *	make the decision as to whether or not multiple instances of
+ *	the trigger should be allowed.  This is usually implemented by
+ *	the generic utility function @register_trigger() (see
+ *	trace_event_triggers.c).
+ *
+ * @unreg: Removes the trigger from the list of triggers associated
+ *	with the event, and disables the event trigger itself, after
+ *	initializing it (via the event_trigger_ops @free() function).
+ *	This is usually implemented by the generic utility function
+ *	@unregister_trigger() (see trace_event_triggers.c).
+ *
+ * @set_filter: An optional function called to parse and set a filter
+ *	for the trigger.  If no @set_filter() method is set for the
+ *	event command, filters set by the user for the command will be
+ *	ignored.  This is usually implemented by the generic utility
+ *	function @set_trigger_filter() (see trace_event_triggers.c).
+ *
+ * @get_trigger_ops: The callback function invoked to retrieve the
+ *	event_trigger_ops implementation associated with the command.
+ */
+struct event_command {
+	struct list_head	list;
+	char			*name;
+	enum event_trigger_type	trigger_type;
+	bool			post_trigger;
+	int			(*func)(struct event_command *cmd_ops,
+					struct ftrace_event_file *file,
+					char *glob, char *cmd, char *params);
+	int			(*reg)(char *glob,
+				       struct event_trigger_ops *ops,
+				       struct event_trigger_data *data,
+				       struct ftrace_event_file *file);
+	void			(*unreg)(char *glob,
+					 struct event_trigger_ops *ops,
+					 struct event_trigger_data *data,
+					 struct ftrace_event_file *file);
+	int			(*set_filter)(char *filter_str,
+					      struct event_trigger_data *data,
+					      struct ftrace_event_file *file);
+	struct event_trigger_ops *(*get_trigger_ops)(char *cmd, char *param);
+};
+
+extern int trace_event_enable_disable(struct ftrace_event_file *file,
+				      int enable, int soft_disable);
+
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 

commit b29c8306a368cf65782669eba079f81dc861c54d
Merge: 0bde7294e2ad 3a81a5210b7d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 16 12:23:18 2013 -0800

    Merge tag 'trace-3.13' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing update from Steven Rostedt:
     "This batch of changes is mostly clean ups and small bug fixes.  The
      only real feature that was added this release is from Namhyung Kim,
      who introduced "set_graph_notrace" filter that lets you run the
      function graph tracer and not trace particular functions and their
      call chain.
    
      Tom Zanussi added some updates to the ftrace multibuffer tracing that
      made it more consistent with the top level tracing.
    
      One of the fixes for perf function tracing required an API change in
      RCU; the addition of "rcu_is_watching()".  As Paul McKenney is pushing
      that change in this release too, he gave me a branch that included all
      the changes to get that working, and I pulled that into my tree in
      order to complete the perf function tracing fix"
    
    * tag 'trace-3.13' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Add rcu annotation for syscall trace descriptors
      tracing: Do not use signed enums with unsigned long long in fgragh output
      tracing: Remove unused function ftrace_off_permanent()
      tracing: Do not assign filp->private_data to freed memory
      tracing: Add helper function tracing_is_disabled()
      tracing: Open tracer when ftrace_dump_on_oops is used
      tracing: Add support for SOFT_DISABLE to syscall events
      tracing: Make register/unregister_ftrace_command __init
      tracing: Update event filters for multibuffer
      recordmcount.pl: Add support for __fentry__
      ftrace: Have control op function callback only trace when RCU is watching
      rcu: Do not trace rcu_is_watching() functions
      ftrace/x86: skip over the breakpoint for ftrace caller
      trace/trace_stat: use rbtree postorder iteration helper instead of opencoding
      ftrace: Add set_graph_notrace filter
      ftrace: Narrow down the protected area of graph_lock
      ftrace: Introduce struct ftrace_graph_data
      ftrace: Get rid of ftrace_graph_filter_enabled
      tracing: Fix potential out-of-bounds in trace_get_user()
      tracing: Show more exact help information about snapshot

commit 3a81a5210b7d33bb6d836b4c4952a54166a336f3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 11 11:47:06 2013 -0500

    tracing: Add rcu annotation for syscall trace descriptors
    
    sparse complains about the enter/exit_sysycall_files[] variables being
    dereferenced with rcu_dereference_sched(). The fields need to be
    annotated with __rcu.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 11a04d6eaa23..7ca1993c33e4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -192,8 +192,8 @@ struct trace_array {
 #ifdef CONFIG_FTRACE_SYSCALLS
 	int			sys_refcount_enter;
 	int			sys_refcount_exit;
-	struct ftrace_event_file *enter_syscall_files[NR_syscalls];
-	struct ftrace_event_file *exit_syscall_files[NR_syscalls];
+	struct ftrace_event_file __rcu *enter_syscall_files[NR_syscalls];
+	struct ftrace_event_file __rcu *exit_syscall_files[NR_syscalls];
 #endif
 	int			stop_count;
 	int			clock_id;

commit e5137b50a0640009fd63a3e65c14bc6e1be8796a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Fri Oct 4 17:28:26 2013 +0200

    ftrace, sched: Add TRACE_FLAG_PREEMPT_RESCHED
    
    Since the introduction of PREEMPT_NEED_RESCHED in:
    
      f27dde8deef3 ("sched: Add NEED_RESCHED to the preempt_count")
    
    we need to be able to look at both TIF_NEED_RESCHED and
    PREEMPT_NEED_RESCHED to understand the full preemption behaviour.
    
    Add it to the trace output.
    
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Fengguang Wu <fengguang.wu@intel.com>
    Cc: Huang Ying <ying.huang@intel.com>
    Cc: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Link: http://lkml.kernel.org/r/20131004152826.GP3081@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 10c86fb7a2b4..73d08aa25b55 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -124,6 +124,7 @@ enum trace_flag_type {
 	TRACE_FLAG_NEED_RESCHED		= 0x04,
 	TRACE_FLAG_HARDIRQ		= 0x08,
 	TRACE_FLAG_SOFTIRQ		= 0x10,
+	TRACE_FLAG_PREEMPT_RESCHED	= 0x20,
 };
 
 #define TRACE_BUF_SIZE		1024

commit 6fc84ea70eae478099c866ace022ecfdef998032
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 6 14:50:06 2013 -0500

    tracing: Do not use signed enums with unsigned long long in fgragh output
    
    The duration field of print_graph_duration() can also be used
    to do the space filling by passing an enum in it:
    
      DURATION_FILL_FULL
      DURATION_FILL_START
      DURATION_FILL_END
    
    The problem is that these are enums and defined as negative,
    but the duration field is unsigned long long. Most archs are
    fine with this but blackfin fails to compile because of it:
    
    kernel/built-in.o: In function `print_graph_duration':
    kernel/trace/trace_functions_graph.c:782: undefined reference to `__ucmpdi2'
    
    Overloading a unsigned long long with an signed enum is just
    bad in principle. We can accomplish the same thing by using
    part of the flags field instead.
    
    Cc: Mike Frysinger <vapier@gentoo.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4388e16484f1..11a04d6eaa23 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -712,6 +712,8 @@ extern unsigned long trace_flags;
 #define TRACE_GRAPH_PRINT_PROC          0x8
 #define TRACE_GRAPH_PRINT_DURATION      0x10
 #define TRACE_GRAPH_PRINT_ABS_TIME      0x20
+#define TRACE_GRAPH_PRINT_FILL_SHIFT	28
+#define TRACE_GRAPH_PRINT_FILL_MASK	(0x3 << TRACE_GRAPH_PRINT_FILL_SHIFT)
 
 extern enum print_line_t
 print_graph_function_flags(struct trace_iterator *iter, u32 flags);

commit 2e86421debc2cf4d1513c9b73fcd34c5ce431ae3
Author: Geyslan G. Bem <geyslan@gmail.com>
Date:   Fri Oct 18 21:15:54 2013 -0300

    tracing: Add helper function tracing_is_disabled()
    
    This patch creates the function 'tracing_is_disabled', which
    can be used outside of trace.c.
    
    Link: http://lkml.kernel.org/r/1382141754-12155-1-git-send-email-geyslan@gmail.com
    
    Signed-off-by: Geyslan G. Bem <geyslan@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9c27cdadd71f..4388e16484f1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -514,6 +514,7 @@ void tracing_reset_online_cpus(struct trace_buffer *buf);
 void tracing_reset_current(int cpu);
 void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
+bool tracing_is_disabled(void);
 struct dentry *trace_create_file(const char *name,
 				 umode_t mode,
 				 struct dentry *parent,

commit d562aff93bfb530b0992141500a402d17081189d
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:34:19 2013 -0500

    tracing: Add support for SOFT_DISABLE to syscall events
    
    The original SOFT_DISABLE patches didn't add support for soft disable
    of syscall events; this adds it.
    
    Add an array of ftrace_event_file pointers indexed by syscall number
    to the trace array and remove the existing enabled bitmaps, which as a
    result are now redundant.  The ftrace_event_file structs in turn
    contain the soft disable flags we need for per-syscall soft disable
    accounting.
    
    Adding ftrace_event_files also means we can remove the USE_CALL_FILTER
    bit, thus enabling multibuffer filter support for syscall events.
    
    Link: http://lkml.kernel.org/r/6e72b566e85d8df8042f133efbc6c30e21fb017e.1382620672.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 12d1a612a73e..9c27cdadd71f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -192,8 +192,8 @@ struct trace_array {
 #ifdef CONFIG_FTRACE_SYSCALLS
 	int			sys_refcount_enter;
 	int			sys_refcount_exit;
-	DECLARE_BITMAP(enabled_enter_syscalls, NR_syscalls);
-	DECLARE_BITMAP(enabled_exit_syscalls, NR_syscalls);
+	struct ftrace_event_file *enter_syscall_files[NR_syscalls];
+	struct ftrace_event_file *exit_syscall_files[NR_syscalls];
 #endif
 	int			stop_count;
 	int			clock_id;

commit f306cc82a93d6b19f01634b80c580b9755c8b7cc
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:34:17 2013 -0500

    tracing: Update event filters for multibuffer
    
    The trace event filters are still tied to event calls rather than
    event files, which means you don't get what you'd expect when using
    filters in the multibuffer case:
    
    Before:
    
      # echo 'bytes_alloc > 8192' > /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 8192
      # mkdir /sys/kernel/debug/tracing/instances/test1
      # echo 'bytes_alloc > 2048' > /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 2048
      # cat /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      bytes_alloc > 2048
    
    Setting the filter in tracing/instances/test1/events shouldn't affect
    the same event in tracing/events as it does above.
    
    After:
    
      # echo 'bytes_alloc > 8192' > /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 8192
      # mkdir /sys/kernel/debug/tracing/instances/test1
      # echo 'bytes_alloc > 2048' > /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 8192
      # cat /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      bytes_alloc > 2048
    
    We'd like to just move the filter directly from ftrace_event_call to
    ftrace_event_file, but there are a couple cases that don't yet have
    multibuffer support and therefore have to continue using the current
    event_call-based filters.  For those cases, a new USE_CALL_FILTER bit
    is added to the event_call flags, whose main purpose is to keep the
    old behavior for those cases until they can be updated with
    multibuffer support; at that point, the USE_CALL_FILTER flag (and the
    new associated call_filter_check_discard() function) can go away.
    
    The multibuffer support also made filter_current_check_discard()
    redundant, so this change removes that function as well and replaces
    it with filter_check_discard() (or call_filter_check_discard() as
    appropriate).
    
    Link: http://lkml.kernel.org/r/f16e9ce4270c62f46b2e966119225e1c3cca7e60.1382620672.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d1cf5159bec0..12d1a612a73e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1007,9 +1007,9 @@ struct filter_pred {
 
 extern enum regex_type
 filter_parse_regex(char *buff, int len, char **search, int *not);
-extern void print_event_filter(struct ftrace_event_call *call,
+extern void print_event_filter(struct ftrace_event_file *file,
 			       struct trace_seq *s);
-extern int apply_event_filter(struct ftrace_event_call *call,
+extern int apply_event_filter(struct ftrace_event_file *file,
 			      char *filter_string);
 extern int apply_subsystem_event_filter(struct ftrace_subsystem_dir *dir,
 					char *filter_string);
@@ -1020,20 +1020,6 @@ extern int filter_assign_type(const char *type);
 struct ftrace_event_field *
 trace_find_event_field(struct ftrace_event_call *call, char *name);
 
-static inline int
-filter_check_discard(struct ftrace_event_call *call, void *rec,
-		     struct ring_buffer *buffer,
-		     struct ring_buffer_event *event)
-{
-	if (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&
-	    !filter_match_preds(call->filter, rec)) {
-		ring_buffer_discard_commit(buffer, event);
-		return 1;
-	}
-
-	return 0;
-}
-
 extern void trace_event_enable_cmd_record(bool enable);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 extern int event_trace_del_tracer(struct trace_array *tr);

commit 29ad23b00474c34e3b5040dda508c78d33a1a3eb
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Mon Oct 14 17:24:26 2013 +0900

    ftrace: Add set_graph_notrace filter
    
    The set_graph_notrace filter is analogous to set_ftrace_notrace and
    can be used for eliminating uninteresting part of function graph trace
    output.  It also works with set_graph_function nicely.
    
      # cd /sys/kernel/debug/tracing/
      # echo do_page_fault > set_graph_function
      # perf ftrace live true
       2)               |  do_page_fault() {
       2)               |    __do_page_fault() {
       2)   0.381 us    |      down_read_trylock();
       2)   0.055 us    |      __might_sleep();
       2)   0.696 us    |      find_vma();
       2)               |      handle_mm_fault() {
       2)               |        handle_pte_fault() {
       2)               |          __do_fault() {
       2)               |            filemap_fault() {
       2)               |              find_get_page() {
       2)   0.033 us    |                __rcu_read_lock();
       2)   0.035 us    |                __rcu_read_unlock();
       2)   1.696 us    |              }
       2)   0.031 us    |              __might_sleep();
       2)   2.831 us    |            }
       2)               |            _raw_spin_lock() {
       2)   0.046 us    |              add_preempt_count();
       2)   0.841 us    |            }
       2)   0.033 us    |            page_add_file_rmap();
       2)               |            _raw_spin_unlock() {
       2)   0.057 us    |              sub_preempt_count();
       2)   0.568 us    |            }
       2)               |            unlock_page() {
       2)   0.084 us    |              page_waitqueue();
       2)   0.126 us    |              __wake_up_bit();
       2)   1.117 us    |            }
       2)   7.729 us    |          }
       2)   8.397 us    |        }
       2)   8.956 us    |      }
       2)   0.085 us    |      up_read();
       2) + 12.745 us   |    }
       2) + 13.401 us   |  }
      ...
    
      # echo handle_mm_fault > set_graph_notrace
      # perf ftrace live true
       1)               |  do_page_fault() {
       1)               |    __do_page_fault() {
       1)   0.205 us    |      down_read_trylock();
       1)   0.041 us    |      __might_sleep();
       1)   0.344 us    |      find_vma();
       1)   0.069 us    |      up_read();
       1)   4.692 us    |    }
       1)   5.311 us    |  }
      ...
    
    Link: http://lkml.kernel.org/r/1381739066-7531-5-git-send-email-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 40211cef2796..d1cf5159bec0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -732,6 +732,8 @@ extern void __trace_graph_return(struct trace_array *tr,
 #define FTRACE_GRAPH_MAX_FUNCS		32
 extern int ftrace_graph_count;
 extern unsigned long ftrace_graph_funcs[FTRACE_GRAPH_MAX_FUNCS];
+extern int ftrace_graph_notrace_count;
+extern unsigned long ftrace_graph_notrace_funcs[FTRACE_GRAPH_MAX_FUNCS];
 
 static inline int ftrace_graph_addr(unsigned long addr)
 {
@@ -757,11 +759,31 @@ static inline int ftrace_graph_addr(unsigned long addr)
 
 	return 0;
 }
+
+static inline int ftrace_graph_notrace_addr(unsigned long addr)
+{
+	int i;
+
+	if (!ftrace_graph_notrace_count)
+		return 0;
+
+	for (i = 0; i < ftrace_graph_notrace_count; i++) {
+		if (addr == ftrace_graph_notrace_funcs[i])
+			return 1;
+	}
+
+	return 0;
+}
 #else
 static inline int ftrace_graph_addr(unsigned long addr)
 {
 	return 1;
 }
+
+static inline int ftrace_graph_notrace_addr(unsigned long addr)
+{
+	return 0;
+}
 #endif /* CONFIG_DYNAMIC_FTRACE */
 #else /* CONFIG_FUNCTION_GRAPH_TRACER */
 static inline enum print_line_t

commit 9aa72b4bf823b7b439fbba95fa84abee3b9d6d79
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Mon Oct 14 17:24:23 2013 +0900

    ftrace: Get rid of ftrace_graph_filter_enabled
    
    The ftrace_graph_filter_enabled means that user sets function filter
    and it always has same meaning of ftrace_graph_count > 0.
    
    Link: http://lkml.kernel.org/r/1381739066-7531-2-git-send-email-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 10c86fb7a2b4..40211cef2796 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -730,7 +730,6 @@ extern void __trace_graph_return(struct trace_array *tr,
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* TODO: make this variable */
 #define FTRACE_GRAPH_MAX_FUNCS		32
-extern int ftrace_graph_filter_enabled;
 extern int ftrace_graph_count;
 extern unsigned long ftrace_graph_funcs[FTRACE_GRAPH_MAX_FUNCS];
 
@@ -738,7 +737,7 @@ static inline int ftrace_graph_addr(unsigned long addr)
 {
 	int i;
 
-	if (!ftrace_graph_filter_enabled)
+	if (!ftrace_graph_count)
 		return 1;
 
 	for (i = 0; i < ftrace_graph_count; i++) {

commit 7eb69529cbaf4229baf5559a400a7a46352c6e52
Merge: 300893b08f3b a0a5a0561f63
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 9 14:42:15 2013 -0700

    Merge tag 'trace-3.12' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Not much changes for the 3.12 merge window.  The major tracing changes
      are still in flux, and will have to wait for 3.13.
    
      The changes for 3.12 are mostly clean ups and minor fixes.
    
      H Peter Anvin added a check to x86_32 static function tracing that
      helps a small segment of the kernel community.
    
      Oleg Nesterov had a few changes from 3.11, but were mostly clean ups
      and not worth pushing in the -rc time frame.
    
      Li Zefan had small clean up with annotating a raw_init with __init.
    
      I fixed a slight race in updating function callbacks, but the race is
      so small and the bug that happens when it occurs is so minor it's not
      even worth pushing to stable.
    
      The only real enhancement is from Alexander Z Lam that made the
      tracing_cpumask work for trace buffer instances, instead of them all
      sharing a global cpumask"
    
    * tag 'trace-3.12' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      ftrace/rcu: Do not trace debug_lockdep_rcu_enabled()
      x86-32, ftrace: Fix static ftrace when early microcode is enabled
      ftrace: Fix a slight race in modifying what function callback gets traced
      tracing: Make tracing_cpumask available for all instances
      tracing: Kill the !CONFIG_MODULES code in trace_events.c
      tracing: Don't pass file_operations array to event_create_dir()
      tracing: Kill trace_create_file_ops() and friends
      tracing/syscalls: Annotate raw_init function with __init

commit 7d992feb7694a21ee81f22894b455dadd5d1c110
Merge: 6e4664525b1d 25f27ce4a6a4
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Sep 3 07:41:11 2013 +0200

    Merge branch 'rcu/next' of git://git.kernel.org/pub/scm/linux/kernel/git/paulmck/linux-rcu into core/rcu
    
    Pull RCU updates from Paul E. McKenney:
    
    "
     * Update RCU documentation.  These were posted to LKML at
       https://lkml.org/lkml/2013/8/19/611.
    
     * Miscellaneous fixes.  These were posted to LKML at
       https://lkml.org/lkml/2013/8/19/619.
    
     * Full-system idle detection.  This is for use by Frederic
       Weisbecker's adaptive-ticks mechanism.  Its purpose is
       to allow the timekeeping CPU to shut off its tick when
       all other CPUs are idle.  These were posted to LKML at
       https://lkml.org/lkml/2013/8/19/648.
    
     * Improve rcutorture test coverage.  These were posted to LKML at
       https://lkml.org/lkml/2013/8/19/675.
    "
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit ccfe9e42e451232dd17a230d1b4e979c3d15311e
Author: Alexander Z Lam <azl@google.com>
Date:   Thu Aug 8 09:47:45 2013 -0700

    tracing: Make tracing_cpumask available for all instances
    
    Allow tracer instances to disable tracing by cpu by moving
    the static global tracing_cpumask into trace_array.
    
    Link: http://lkml.kernel.org/r/921622317f239bfc2283cac2242647801ef584f2.1375980149.git.azl@google.com
    
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Cc: David Sharp <dhsharp@google.com>
    Cc: Alexander Z Lam <lambchop468@gmail.com>
    Signed-off-by: Alexander Z Lam <azl@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index afaae41b0a02..502fed770751 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -206,6 +206,7 @@ struct trace_array {
 	struct dentry		*event_dir;
 	struct list_head	systems;
 	struct list_head	events;
+	cpumask_var_t		tracing_cpumask; /* only trace on set CPUs */
 	int			ref;
 };
 

commit 102c9323c35a83789ad5ebd3c45fa8fb389add88
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jul 12 17:07:27 2013 -0400

    tracing: Add __tracepoint_string() to export string pointers
    
    There are several tracepoints (mostly in RCU), that reference a string
    pointer and uses the print format of "%s" to display the string that
    exists in the kernel, instead of copying the actual string to the
    ring buffer (saves time and ring buffer space).
    
    But this has an issue with userspace tools that read the binary buffers
    that has the address of the string but has no access to what the string
    itself is. The end result is just output that looks like:
    
     rcu_dyntick:          ffffffff818adeaa 1 0
     rcu_dyntick:          ffffffff818adeb5 0 140000000000000
     rcu_dyntick:          ffffffff818adeb5 0 140000000000000
     rcu_utilization:      ffffffff8184333b
     rcu_utilization:      ffffffff8184333b
    
    The above is pretty useless when read by the userspace tools. Ideally
    we would want something that looks like this:
    
     rcu_dyntick:          Start 1 0
     rcu_dyntick:          End 0 140000000000000
     rcu_dyntick:          Start 140000000000000 0
     rcu_callback:         rcu_preempt rhp=0xffff880037aff710 func=put_cred_rcu 0/4
     rcu_callback:         rcu_preempt rhp=0xffff880078961980 func=file_free_rcu 0/5
     rcu_dyntick:          End 0 1
    
    The trace_printk() which also only stores the address of the string
    format instead of recording the string into the buffer itself, exports
    the mapping of kernel addresses to format strings via the printk_format
    file in the debugfs tracing directory.
    
    The tracepoint strings can use this same method and output the format
    to the same file and the userspace tools will be able to decipher
    the address without any modification.
    
    The tracepoint strings need its own section to save the strings because
    the trace_printk section will cause the trace_printk() buffers to be
    allocated if anything exists within the section. trace_printk() is only
    used for debugging and should never exist in the kernel, we can not use
    the trace_printk sections.
    
    Add a new tracepoint_str section that will also be examined by the output
    of the printk_format file.
    
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4a4f6e1828b6..ba321f12df8c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1022,6 +1022,9 @@ extern struct list_head ftrace_events;
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 
+extern const char *__start___tracepoint_str[];
+extern const char *__stop___tracepoint_str[];
+
 void trace_printk_init_buffers(void);
 void trace_printk_start_comm(void);
 int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);

commit 9c01fe4593db123c5a72dc36f0400f776e92c954
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jul 23 17:26:13 2013 +0200

    tracing: Kill trace_cpu struct/members
    
    After the previous changes trace_array_cpu->trace_cpu and
    trace_array->trace_cpu becomes write-only. Remove these members
    and kill "struct trace_cpu" as well.
    
    As a side effect this also removes memset(per_cpu_memory, 0).
    It was not needed, alloc_percpu() returns zero-filled memory.
    
    Link: http://lkml.kernel.org/r/20130723152613.GA23741@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e7d643b8a907..afaae41b0a02 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -130,19 +130,12 @@ enum trace_flag_type {
 
 struct trace_array;
 
-struct trace_cpu {
-	struct trace_array	*tr;
-	struct dentry		*dir;
-	int			cpu;
-};
-
 /*
  * The CPU trace array - it consists of thousands of trace entries
  * plus some other descriptor data: (for example which task started
  * the trace, etc.)
  */
 struct trace_array_cpu {
-	struct trace_cpu	trace_cpu;
 	atomic_t		disabled;
 	void			*buffer_page;	/* ring buffer spare */
 
@@ -196,7 +189,6 @@ struct trace_array {
 	bool			allocated_snapshot;
 #endif
 	int			buffer_disabled;
-	struct trace_cpu	trace_cpu;	/* place holder */
 #ifdef CONFIG_FTRACE_SYSCALLS
 	int			sys_refcount_enter;
 	int			sys_refcount_exit;

commit a644a7e9587802eabb2e229177606f6a74a60fc1
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 19 16:20:36 2013 +0200

    tracing: Kill trace_array->waiter
    
    Trivial. trace_array->waiter has no users since 6eaaa5d5
    "tracing/core: use appropriate waiting on trace_pipe".
    
    Link: http://lkml.kernel.org/r/20130719142036.GA1594@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 57b7bb0d39b7..e7d643b8a907 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -214,7 +214,6 @@ struct trace_array {
 	struct dentry		*event_dir;
 	struct list_head	systems;
 	struct list_head	events;
-	struct task_struct	*waiter;
 	int			ref;
 };
 

commit 8f768993394a8c0d3801033c11fd86ce8c88dcac
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jul 18 14:41:51 2013 -0400

    tracing: Add ref_data to function and fgraph tracer structs
    
    The selftest for function and function graph tracers are defined as
    __init, as they are only executed at boot up. The "tracer" structs
    that are associated to those tracers are not setup as __init as they
    are used after boot. To stop mismatch warnings, those structures
    need to be annotated with __ref_data.
    
    Currently, the tracer structures are defined to __read_mostly, as they
    do not really change. But in the future they should be converted to
    consts, but that will take a little work because they have a "next"
    pointer that gets updated when they are registered. That will have to
    wait till the next major release.
    
    Link: http://lkml.kernel.org/r/1373596735.17876.84.camel@gandalf.local.home
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Reported-by: Chen Gang <gang.chen@asianux.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4a4f6e1828b6..57b7bb0d39b7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -680,6 +680,15 @@ extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 					       struct trace_array *tr);
 extern int trace_selftest_startup_branch(struct tracer *trace,
 					 struct trace_array *tr);
+/*
+ * Tracer data references selftest functions that only occur
+ * on boot up. These can be __init functions. Thus, when selftests
+ * are enabled, then the tracers need to reference __init functions.
+ */
+#define __tracer_data		__refdata
+#else
+/* Tracers are seldom changed. Optimize when selftests are disabled. */
+#define __tracer_data		__read_mostly
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);

commit c72bb316916b1a6cf35e1d5238566ef27b0b7f80
Merge: 6d128e1e72bf dcc302232c1f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jul 11 09:02:09 2013 -0700

    Merge tag 'trace-3.11' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing changes from Steven Rostedt:
     "The majority of the changes here are cleanups for the large changes
      that were added to 3.10, which includes several bug fixes that have
      been marked for stable.
    
      As for new features, there were a few, but nothing to write to LWN
      about.  These include:
    
      New function trigger called "dump" and "cpudump" that will cause
      ftrace to dump its buffer to the console when the function is called.
      The difference between "dump" and "cpudump" is that "dump" will dump
      the entire contents of the ftrace buffer, where as "cpudump" will only
      dump the contents of the ftrace buffer for the CPU that called the
      function.
    
      Another small enhancement is a new sysctl switch called
      "traceoff_on_warning" which, when enabled, will disable tracing if any
      WARN_ON() is triggered.  This is useful if you want to debug what
      caused a warning and do not want to risk losing your trace data by the
      ring buffer overwriting the data before you can disable it.  There's
      also a kernel command line option that will make this enabled at boot
      up called the same thing"
    
    * tag 'trace-3.11' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (34 commits)
      tracing: Make tracing_open_generic_{tr,tc}() static
      tracing: Remove ftrace() function
      tracing: Remove TRACE_EVENT_TYPE enum definition
      tracing: Make tracer_tracing_{off,on,is_on}() static
      tracing: Fix irqs-off tag display in syscall tracing
      uprobes: Fix return value in error handling path
      tracing: Fix race between deleting buffer and setting events
      tracing: Add trace_array_get/put() to event handling
      tracing: Get trace_array ref counts when accessing trace files
      tracing: Add trace_array_get/put() to handle instance refs better
      tracing: Protect ftrace_trace_arrays list in trace_events.c
      tracing: Make trace_marker use the correct per-instance buffer
      ftrace: Do not run selftest if command line parameter is set
      tracing/kprobes: Don't pass addr=ip to perf_trace_buf_submit()
      tracing: Use flag buffer_disabled for irqsoff tracer
      tracing/kprobes: Turn trace_probe->files into list_head
      tracing: Fix disabling of soft disable
      tracing: Add missing syscall_metadata comment
      tracing: Simplify code for showing of soft disabled flag
      tracing/kprobes: Kill probe_enable_lock
      ...

commit 8de1eb02778b64f8b292db531cf39a429f84315f
Author: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
Date:   Wed Apr 10 11:26:30 2013 +0800

    tracing: Remove ftrace() function
    
    The only caller of function ftrace(...) was removed a long time ago,
    so remove the function body as well.
    
    Link: http://lkml.kernel.org/r/1365564393-10972-10-git-send-email-jovi.zhangwei@huawei.com
    
    Signed-off-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1cbba04976b4..a4ed382dea2f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -559,11 +559,6 @@ void tracing_iter_reset(struct trace_iterator *iter, int cpu);
 
 void poll_wait_pipe(struct trace_iterator *iter);
 
-void ftrace(struct trace_array *tr,
-			    struct trace_array_cpu *data,
-			    unsigned long ip,
-			    unsigned long parent_ip,
-			    unsigned long flags, int pc);
 void tracing_sched_switch_trace(struct trace_array *tr,
 				struct task_struct *prev,
 				struct task_struct *next,

commit 4480361c3c592fcbce3ef74e030719f0715e3a7e
Author: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
Date:   Wed Apr 10 11:26:28 2013 +0800

    tracing: Remove TRACE_EVENT_TYPE enum definition
    
    TRACE_EVENT_TYPE enum is not used at present, remove it.
    
    Link: http://lkml.kernel.org/r/1365564393-10972-8-git-send-email-jovi.zhangwei@huawei.com
    
    Signed-off-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c7fbf93f1b7c..1cbba04976b4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -907,12 +907,6 @@ static inline void trace_branch_disable(void)
 /* set ring buffers to default size if not already done so */
 int tracing_update_buffers(void);
 
-/* trace event type bit fields, not numeric */
-enum {
-	TRACE_EVENT_TYPE_PRINTF		= 1,
-	TRACE_EVENT_TYPE_RAW		= 2,
-};
-
 struct ftrace_event_field {
 	struct list_head	link;
 	const char		*name;

commit 8e2e2fa47129532a30cff6c25a47078dc97d9260
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jul 2 15:30:53 2013 -0400

    tracing: Add trace_array_get/put() to event handling
    
    Commit a695cb58162 "tracing: Prevent deleting instances when they are being read"
    tried to fix a race between deleting a trace instance and reading contents
    of a trace file. But it wasn't good enough. The following could crash the kernel:
    
     # cd /sys/kernel/debug/tracing/instances
     # ( while :; do mkdir foo; rmdir foo; done ) &
     # ( while :; do echo 1 > foo/events/sched/sched_switch 2> /dev/null; done ) &
    
    Luckily this can only be done by root user, but it should be fixed regardless.
    
    The problem is that a delete of the file can happen after the write to the event
    is opened, but before the enabling happens.
    
    The solution is to make sure the trace_array is available before succeeding in
    opening for write, and incerment the ref counter while opened.
    
    Now the instance can be deleted when the events are writing to the buffer,
    but the deletion of the instance will disable all events before the instance
    is actually deleted.
    
    Cc: stable@vger.kernel.org # 3.10
    Reported-by: Alexander Lam <azl@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2c3cba59552d..c7fbf93f1b7c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -226,6 +226,9 @@ extern struct list_head ftrace_trace_arrays;
 
 extern struct mutex trace_types_lock;
 
+extern int trace_array_get(struct trace_array *tr);
+extern void trace_array_put(struct trace_array *tr);
+
 /*
  * The global tracer (top) should be the first trace array added,
  * but we check the flag anyway.

commit a82274151af2b075163e3c42c828529dee311487
Author: Alexander Z Lam <azl@google.com>
Date:   Mon Jul 1 19:37:54 2013 -0700

    tracing: Protect ftrace_trace_arrays list in trace_events.c
    
    There are multiple places where the ftrace_trace_arrays list is accessed in
    trace_events.c without the trace_types_lock held.
    
    Link: http://lkml.kernel.org/r/1372732674-22726-1-git-send-email-azl@google.com
    
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Cc: David Sharp <dhsharp@google.com>
    Cc: Alexander Z Lam <lambchop468@gmail.com>
    Cc: stable@vger.kernel.org # 3.10
    Signed-off-by: Alexander Z Lam <azl@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a88939e666b7..2c3cba59552d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -224,6 +224,8 @@ enum {
 
 extern struct list_head ftrace_trace_arrays;
 
+extern struct mutex trace_types_lock;
+
 /*
  * The global tracer (top) should be the first trace array added,
  * but we check the flag anyway.

commit f1ed7c741fcd0c3d7d318e7c19813d89934b9296
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Jun 27 22:18:06 2013 -0400

    ftrace: Do not run selftest if command line parameter is set
    
    If the kernel command line ftrace filter parameters are set
    (ftrace_filter or ftrace_notrace), force the function self test to
    pass, with a warning why it was forced.
    
    If the user adds a filter to the kernel command line, it is assumed
    that they know what they are doing, and the self test should just not
    run instead of failing (which disables function tracing) or clearing
    the filter, as that will probably annoy the user.
    
    If the user wants the selftest to run, the message will tell them why
    it did not.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 711ca7d3e7f1..a88939e666b7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -776,6 +776,7 @@ print_graph_function_flags(struct trace_iterator *iter, u32 flags)
 extern struct list_head ftrace_pids;
 
 #ifdef CONFIG_FUNCTION_TRACER
+extern bool ftrace_filter_param __initdata;
 static inline int ftrace_trace_task(struct task_struct *task)
 {
 	if (list_empty(&ftrace_pids))

commit 58e8eedf18577c7eac722d5d1f190507ea263d1b
Author: Yoshihiro YUNOMAE <yoshihiro.yunomae.ez@hitachi.com>
Date:   Tue Apr 23 10:32:39 2013 +0900

    tracing: Fix outputting formats of x86-tsc and counter when use trace_clock
    
    Outputting formats of x86-tsc and counter should be a raw format, but after
    applying the patch(2b6080f28c7cc3efc8625ab71495aae89aeb63a0), the format was
    changed to nanosec. This is because the global variable trace_clock_id was used.
    When we use multiple buffers, clock_id of each sub-buffer should be used. Then,
    this patch uses tr->clock_id instead of the global variable trace_clock_id.
    
    [ Basically, this fixes a regression where the multibuffer code changed the
      trace_clock file to update tr->clock_id but the traces still use the old
      global trace_clock_id variable, negating the file's effect. The global
      trace_clock_id variable is obsolete and removed. - SR ]
    
    Link: http://lkml.kernel.org/r/20130423013239.22334.7394.stgit@yunodevel
    
    Signed-off-by: Yoshihiro YUNOMAE <yoshihiro.yunomae.ez@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 711ca7d3e7f1..20572ed88c5c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -700,8 +700,6 @@ enum print_line_t print_trace_line(struct trace_iterator *iter);
 
 extern unsigned long trace_flags;
 
-extern int trace_clock_id;
-
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 

commit e0972916e8fe943f342b0dd1c9d43dbf5bc261c2
Merge: 1f889ec62c3f 5ac2b5c27215
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 30 07:41:01 2013 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull perf updates from Ingo Molnar:
     "Features:
    
       - Add "uretprobes" - an optimization to uprobes, like kretprobes are
         an optimization to kprobes.  "perf probe -x file sym%return" now
         works like kretprobes.  By Oleg Nesterov.
    
       - Introduce per core aggregation in 'perf stat', from Stephane
         Eranian.
    
       - Add memory profiling via PEBS, from Stephane Eranian.
    
       - Event group view for 'annotate' in --stdio, --tui and --gtk, from
         Namhyung Kim.
    
       - Add support for AMD NB and L2I "uncore" counters, by Jacob Shin.
    
       - Add Ivy Bridge-EP uncore support, by Zheng Yan
    
       - IBM zEnterprise EC12 oprofile support patchlet from Robert Richter.
    
       - Add perf test entries for checking breakpoint overflow signal
         handler issues, from Jiri Olsa.
    
       - Add perf test entry for for checking number of EXIT events, from
         Namhyung Kim.
    
       - Add perf test entries for checking --cpu in record and stat, from
         Jiri Olsa.
    
       - Introduce perf stat --repeat forever, from Frederik Deweerdt.
    
       - Add --no-demangle to report/top, from Namhyung Kim.
    
       - PowerPC fixes plus a couple of cleanups/optimizations in uprobes
         and trace_uprobes, by Oleg Nesterov.
    
      Various fixes and refactorings:
    
       - Fix dependency of the python binding wrt libtraceevent, from
         Naohiro Aota.
    
       - Simplify some perf_evlist methods and to allow 'stat' to share code
         with 'record' and 'trace', by Arnaldo Carvalho de Melo.
    
       - Remove dead code in related to libtraceevent integration, from
         Namhyung Kim.
    
       - Revert "perf sched: Handle PERF_RECORD_EXIT events" to get 'perf
         sched lat' back working, by Arnaldo Carvalho de Melo
    
       - We don't use Newt anymore, just plain libslang, by Arnaldo Carvalho
         de Melo.
    
       - Kill a bunch of die() calls, from Namhyung Kim.
    
       - Fix build on non-glibc systems due to libio.h absence, from Cody P
         Schafer.
    
       - Remove some perf_session and tracing dead code, from David Ahern.
    
       - Honor parallel jobs, fix from Borislav Petkov
    
       - Introduce tools/lib/lk library, initially just removing duplication
         among tools/perf and tools/vm.  from Borislav Petkov
    
      ... and many more I missed to list, see the shortlog and git log for
      more details."
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (136 commits)
      perf/x86/intel/P4: Robistify P4 PMU types
      perf/x86/amd: Fix AMD NB and L2I "uncore" support
      perf/x86/amd: Remove old-style NB counter support from perf_event_amd.c
      perf/x86: Check all MSRs before passing hw check
      perf/x86/amd: Add support for AMD NB and L2I "uncore" counters
      perf/x86/intel: Add Ivy Bridge-EP uncore support
      perf/x86/intel: Fix SNB-EP CBO and PCU uncore PMU filter management
      perf/x86: Avoid kfree() in CPU_{STARTING,DYING}
      uprobes/perf: Avoid perf_trace_buf_prepare/submit if ->perf_events is empty
      uprobes/tracing: Don't pass addr=ip to perf_trace_buf_submit()
      uprobes/tracing: Change create_trace_uprobe() to support uretprobes
      uprobes/tracing: Make seq_printf() code uretprobe-friendly
      uprobes/tracing: Make register_uprobe_event() paths uretprobe-friendly
      uprobes/tracing: Make uprobe_{trace,perf}_print() uretprobe-friendly
      uprobes/tracing: Introduce is_ret_probe() and uretprobe_dispatcher()
      uprobes/tracing: Introduce uprobe_{trace,perf}_print() helpers
      uprobes/tracing: Generalize struct uprobe_trace_entry_head
      uprobes/tracing: Kill the pointless local_save_flags/preempt_count calls
      uprobes/tracing: Kill the pointless seq_print_ip_sym() call
      uprobes/tracing: Kill the pointless task_pt_regs() calls
      ...

commit 457d1772f1c1bcf37b2ae7fc8f1d6f303d1d5cf9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Mar 29 18:26:51 2013 +0100

    uprobes/tracing: Generalize struct uprobe_trace_entry_head
    
    struct uprobe_trace_entry_head has a single member for reporting,
    "unsigned long ip". If we want to support uretprobes we need to
    create another struct which has "func" and "ret_ip" and duplicate
    a lot of functions, like trace_kprobe.c does.
    
    To avoid this copy-and-paste horror we turn ->ip into ->vaddr[]
    and add couple of trivial helpers to calculate sizeof/data. This
    uglifies the code a bit, but this allows us to avoid a lot more
    complications later, when we add the support for ret-probes.
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Tested-by: Anton Arapov <anton@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2081971367ea..8bed1dfcb938 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -103,11 +103,6 @@ struct kretprobe_trace_entry_head {
 	unsigned long		ret_ip;
 };
 
-struct uprobe_trace_entry_head {
-	struct trace_entry	ent;
-	unsigned long		ip;
-};
-
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:

commit b3a8c6fd7bb61c910bd4f80ae1d75056e8f98c19
Author: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
Date:   Mon Mar 11 15:13:42 2013 +0800

    tracing: Move find_event_field() into trace_events.c
    
    By moving find_event_field() and trace_find_field() into trace_events.c,
    the ftrace_common_fields list and trace_get_fields() can become local to
    the trace_events.c file.
    
    find_event_field() is renamed to trace_find_event_field() to conform to
    the tracing global function names.
    
    Link: http://lkml.kernel.org/r/513D8426.9070109@huawei.com
    
    Signed-off-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    [ rostedt: Modified trace_find_field() to trace_find_event_field() ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5cc52361bc9f..9e014582e763 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -995,8 +995,6 @@ struct filter_pred {
 	unsigned short		right;
 };
 
-extern struct list_head ftrace_common_fields;
-
 extern enum regex_type
 filter_parse_regex(char *buff, int len, char **search, int *not);
 extern void print_event_filter(struct ftrace_event_call *call,
@@ -1009,8 +1007,8 @@ extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);
 extern int filter_assign_type(const char *type);
 
-struct list_head *
-trace_get_fields(struct ftrace_event_call *event_call);
+struct ftrace_event_field *
+trace_find_event_field(struct ftrace_event_call *call, char *name);
 
 static inline int
 filter_check_discard(struct ftrace_event_call *call, void *rec,

commit 328df4759c03e2c3e7429cc6cb0e180c38f32063
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Mar 14 12:10:40 2013 -0400

    tracing: Add function-trace option to disable function tracing of latency tracers
    
    Currently, the only way to stop the latency tracers from doing function
    tracing is to fully disable the function tracer from the proc file
    system:
    
      echo 0 > /proc/sys/kernel/ftrace_enabled
    
    This is a big hammer approach as it disables function tracing for
    all users. This includes kprobes, perf, stack tracer, etc.
    
    Instead, create a function-trace option that the latency tracers can
    check to determine if it should enable function tracing or not.
    This option can be set or cleared even while the tracer is active
    and the tracers will disable or enable function tracing depending
    on how the option was set.
    
    Instead of using the proc file, disable latency function tracing with
    
      echo 0 > /debug/tracing/options/function-trace
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Clark Williams <williams@redhat.com>
    Cc: John Kacur <jkacur@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0e430b401ab6..5cc52361bc9f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -867,6 +867,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_STOP_ON_FREE		= 0x400000,
 	TRACE_ITER_IRQ_INFO		= 0x800000,
 	TRACE_ITER_MARKERS		= 0x1000000,
+	TRACE_ITER_FUNCTION		= 0x2000000,
 };
 
 /*

commit ca268da6e415448a43138e1abc5d5f057af319d7
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Sat Mar 9 00:40:58 2013 -0500

    tracing: Add internal ftrace trace_puts() for ftrace to use
    
    There's a few places that ftrace uses trace_printk() for internal
    use, but this requires context (normal, softirq, irq, NMI) buffers
    to keep things lockless. But the trace_puts() does not, as it can
    write the string directly into the ring buffer. Make a internal helper
    for trace_puts() and have the internal functions use that.
    
    This way the extra context buffers are not used.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d5764a8532e2..0e430b401ab6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1040,6 +1040,17 @@ void trace_printk_start_comm(void);
 int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);
 int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 
+/*
+ * Normal trace_printk() and friends allocates special buffers
+ * to do the manipulation, as well as saves the print formats
+ * into sections to display. But the trace infrastructure wants
+ * to use these without the added overhead at the price of being
+ * a bit slower (used mainly for warnings, where we don't care
+ * about performance). The internal_trace_puts() is for such
+ * a purpose.
+ */
+#define internal_trace_puts(str) __trace_puts(_THIS_IP_, str, strlen(str))
+
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\
 	extern struct ftrace_event_call					\

commit 09ae72348eccb60e304cf8ce94653f4a78fcd407
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Mar 8 21:02:34 2013 -0500

    tracing: Add trace_puts() for even faster trace_printk() tracing
    
    The trace_printk() is extremely fast and is very handy as it can be
    used in any context (including NMIs!). But it still requires scanning
    the fmt string for parsing the args. Even the trace_bprintk() requires
    a scan to know what args will be saved, although it doesn't copy the
    format string itself.
    
    Several times trace_printk() has no args, and wastes cpu cycles scanning
    the fmt string.
    
    Adding trace_puts() allows the developer to use an even faster
    tracing method that only saves the pointer to the string in the
    ring buffer without doing any format parsing at all. This will
    help remove even more of the "Heisenbug" effect, when debugging.
    
    Also fixed up the F_printk()s for the ftrace internal bprint and print events.
    
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 26bc71834041..d5764a8532e2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -34,6 +34,7 @@ enum trace_type {
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
 	TRACE_BLK,
+	TRACE_BPUTS,
 
 	__TRACE_LAST_TYPE,
 };
@@ -277,6 +278,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
 		IF_ASSIGN(var, ent, struct bprint_entry, TRACE_BPRINT);	\
+		IF_ASSIGN(var, ent, struct bputs_entry, TRACE_BPUTS);	\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
 			  TRACE_MMIO_RW);				\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\

commit 55034cd6e648155393b0d665eef76b38d49ad6bf
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Thu Mar 7 22:48:09 2013 -0500

    tracing: Add alloc_snapshot kernel command line parameter
    
    If debugging the kernel, and the developer wants to use
    tracing_snapshot() in places where tracing_snapshot_alloc() may
    be difficult (or more likely, the developer is lazy and doesn't
    want to bother with tracing_snapshot_alloc() at all), then adding
    
      alloc_snapshot
    
    to the kernel command line parameter will tell ftrace to allocate
    the snapshot buffer (if configured) when it allocates the main
    tracing buffer.
    
    I also noticed that ring_buffer_expanded and tracing_selftest_disabled
    had inconsistent use of boolean "true" and "false" with "0" and "1".
    I cleaned that up too.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f4931821a966..26bc71834041 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -660,7 +660,7 @@ extern int DYN_FTRACE_TEST_NAME(void);
 #define DYN_FTRACE_TEST_NAME2 trace_selftest_dynamic_test_func2
 extern int DYN_FTRACE_TEST_NAME2(void);
 
-extern int ring_buffer_expanded;
+extern bool ring_buffer_expanded;
 extern bool tracing_selftest_disabled;
 DECLARE_PER_CPU(int, ftrace_cpu_disabled);
 

commit a695cb5816228f86576f5f5c6809fdf8ed382ece
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Wed Mar 6 15:27:24 2013 -0500

    tracing: Prevent deleting instances when they are being read
    
    Add a ref count to the trace_array structure and prevent removal
    of instances that have open descriptors.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1a456c291a07..f4931821a966 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -219,6 +219,7 @@ struct trace_array {
 	struct list_head	systems;
 	struct list_head	events;
 	struct task_struct	*waiter;
+	int			ref;
 };
 
 enum {

commit 45ad21ca5530efdca6a19e4a5ac5e7bd6e24f996
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Tue Mar 5 18:25:02 2013 -0500

    tracing: Have trace_array keep track if snapshot buffer is allocated
    
    The snapshot buffer belongs to the trace array not the tracer that is
    running. The trace array should be the data structure that keeps track
    of whether or not the snapshot buffer is allocated, not the tracer
    desciptor. Having the trace array keep track of it makes modifications
    so much easier.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 986834f1f4dd..1a456c291a07 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -197,6 +197,7 @@ struct trace_array {
 	 * the trace_buffer so the tracing can continue.
 	 */
 	struct trace_buffer	max_buffer;
+	bool			allocated_snapshot;
 #endif
 	int			buffer_disabled;
 	struct trace_cpu	trace_cpu;	/* place holder */
@@ -367,7 +368,6 @@ struct tracer {
 	bool			enabled;
 #ifdef CONFIG_TRACER_MAX_TRACE
 	bool			use_max_tr;
-	bool			allocated_snapshot;
 #endif
 };
 

commit 12883efb670c28dff57dcd7f4f995a1ffe153b2d
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Tue Mar 5 09:24:35 2013 -0500

    tracing: Consolidate max_tr into main trace_array structure
    
    Currently, the way the latency tracers and snapshot feature works
    is to have a separate trace_array called "max_tr" that holds the
    snapshot buffer. For latency tracers, this snapshot buffer is used
    to swap the running buffer with this buffer to save the current max
    latency.
    
    The only items needed for the max_tr is really just a copy of the buffer
    itself, the per_cpu data pointers, the time_start timestamp that states
    when the max latency was triggered, and the cpu that the max latency
    was triggered on. All other fields in trace_array are unused by the
    max_tr, making the max_tr mostly bloat.
    
    This change removes the max_tr completely, and adds a new structure
    called trace_buffer, that holds the buffer pointer, the per_cpu data
    pointers, the time_start timestamp, and the cpu where the latency occurred.
    
    The trace_array, now has two trace_buffers, one for the normal trace and
    one for the max trace or snapshot. By doing this, not only do we remove
    the bloat from the max_trace but the instances of traces can now use
    their own snapshot feature and not have just the top level global_trace have
    the snapshot feature and latency tracers for itself.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fa60b2977524..986834f1f4dd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -167,16 +167,37 @@ struct trace_array_cpu {
 
 struct tracer;
 
+struct trace_buffer {
+	struct trace_array		*tr;
+	struct ring_buffer		*buffer;
+	struct trace_array_cpu __percpu	*data;
+	cycle_t				time_start;
+	int				cpu;
+};
+
 /*
  * The trace array - an array of per-CPU trace arrays. This is the
  * highest level data structure that individual tracers deal with.
  * They have on/off state as well:
  */
 struct trace_array {
-	struct ring_buffer	*buffer;
 	struct list_head	list;
 	char			*name;
-	int			cpu;
+	struct trace_buffer	trace_buffer;
+#ifdef CONFIG_TRACER_MAX_TRACE
+	/*
+	 * The max_buffer is used to snapshot the trace when a maximum
+	 * latency is reached, or when the user initiates a snapshot.
+	 * Some tracers will use this to store a maximum trace while
+	 * it continues examining live traces.
+	 *
+	 * The buffers for the max_buffer are set up the same as the trace_buffer
+	 * When a snapshot is taken, the buffer of the max_buffer is swapped
+	 * with the buffer of the trace_buffer and the buffers are reset for
+	 * the trace_buffer so the tracing can continue.
+	 */
+	struct trace_buffer	max_buffer;
+#endif
 	int			buffer_disabled;
 	struct trace_cpu	trace_cpu;	/* place holder */
 #ifdef CONFIG_FTRACE_SYSCALLS
@@ -189,7 +210,6 @@ struct trace_array {
 	int			clock_id;
 	struct tracer		*current_trace;
 	unsigned int		flags;
-	cycle_t			time_start;
 	raw_spinlock_t		start_lock;
 	struct dentry		*dir;
 	struct dentry		*options;
@@ -198,7 +218,6 @@ struct trace_array {
 	struct list_head	systems;
 	struct list_head	events;
 	struct task_struct	*waiter;
-	struct trace_array_cpu __percpu	*data;
 };
 
 enum {
@@ -345,9 +364,11 @@ struct tracer {
 	struct tracer		*next;
 	struct tracer_flags	*flags;
 	bool			print_max;
+	bool			enabled;
+#ifdef CONFIG_TRACER_MAX_TRACE
 	bool			use_max_tr;
 	bool			allocated_snapshot;
-	bool			enabled;
+#endif
 };
 
 
@@ -493,8 +514,8 @@ trace_buffer_iter(struct trace_iterator *iter, int cpu)
 
 int tracer_init(struct tracer *t, struct trace_array *tr);
 int tracing_is_enabled(void);
-void tracing_reset(struct trace_array *tr, int cpu);
-void tracing_reset_online_cpus(struct trace_array *tr);
+void tracing_reset(struct trace_buffer *buf, int cpu);
+void tracing_reset_online_cpus(struct trace_buffer *buf);
 void tracing_reset_current(int cpu);
 void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
@@ -674,6 +695,8 @@ trace_array_vprintk(struct trace_array *tr,
 		    unsigned long ip, const char *fmt, va_list args);
 int trace_array_printk(struct trace_array *tr,
 		       unsigned long ip, const char *fmt, ...);
+int trace_array_printk_buf(struct ring_buffer *buffer,
+			   unsigned long ip, const char *fmt, ...);
 void trace_printk_seq(struct trace_seq *s);
 enum print_line_t print_trace_line(struct trace_iterator *iter);
 

commit 873c642f5964b260480850040dec21e42d0ae4e4
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Mon Mar 4 23:26:06 2013 -0500

    tracing: Clear all trace buffers when unloaded module event was used
    
    Currently we do not know what buffer a module event was enabled in.
    On unload, it is safest to clear all buffer instances, not just the
    top level buffer.
    
    Todo: Clear only the buffer that the event was used in. The
    infrastructure is there to do this, but it makes the code a bit
    more complex. Lets get the current code vetted before we add that.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6728a249e817..fa60b2977524 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -496,7 +496,7 @@ int tracing_is_enabled(void);
 void tracing_reset(struct trace_array *tr, int cpu);
 void tracing_reset_online_cpus(struct trace_array *tr);
 void tracing_reset_current(int cpu);
-void tracing_reset_current_online_cpus(void);
+void tracing_reset_all_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *trace_create_file(const char *name,
 				 umode_t mode,

commit 34ef61b1fa6172e994e441f1f0241dc53a75bd5f
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Sat Mar 2 16:49:10 2013 -0500

    tracing: Add __per_cpu annotation to trace array percpu data pointer
    
    With the conversion of the data array to per cpu, sparse now complains
    about the use of per_cpu_ptr() on the variable. But The variable is
    allocated with alloc_percpu() and is fine to use. But since the structure
    that contains the data variable does not annotate it as such, sparse
    gives out a lot of false warnings.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e420f2a230de..6728a249e817 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -198,7 +198,7 @@ struct trace_array {
 	struct list_head	systems;
 	struct list_head	events;
 	struct task_struct	*waiter;
-	struct trace_array_cpu	*data;
+	struct trace_array_cpu __percpu	*data;
 };
 
 enum {

commit 92edca073c374f66b8eee20ec6426fb0cdb6c4d5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Feb 27 20:41:37 2013 -0500

    tracing: Use direct field, type and system names
    
    The names used to display the field and type in the event format
    files are copied, as well as the system name that is displayed.
    
    All these names are created by constant values passed in.
    If one of theses values were to be removed by a module, the module
    would also be required to remove any event it created.
    
    By using the strings directly, we can save over 100K of memory.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b825ea2d8c64..e420f2a230de 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -887,8 +887,8 @@ enum {
 
 struct ftrace_event_field {
 	struct list_head	link;
-	char			*name;
-	char			*type;
+	const char		*name;
+	const char		*type;
 	int			filter_type;
 	int			offset;
 	int			size;

commit 0c8916c34203734d3b05953ebace52d7c2969f16
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 7 16:14:16 2012 -0400

    tracing: Add rmdir to remove multibuffer instances
    
    Add a method to the hijacked dentry descriptor of the
    "instances" directory to allow for rmdir to remove an
    instance of a multibuffer.
    
    Example:
    
      cd /debug/tracing/instances
      mkdir hello
      ls
    hello/
      rmdir hello
      ls
    
    Like the mkdir method, the i_mutex is dropped for the instances
    directory. The instances directory is created at boot up and can
    not be renamed or removed. The trace_types_lock mutex is used to
    synchronize adding and removing of instances.
    
    I've run several stress tests with different threads trying to
    create and delete directories of the same name, and it has stood
    up fine.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 883fe0b62f0a..b825ea2d8c64 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -1001,6 +1001,7 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 
 extern void trace_event_enable_cmd_record(bool enable);
 extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
+extern int event_trace_del_tracer(struct trace_array *tr);
 
 extern struct mutex event_mutex;
 extern struct list_head ftrace_events;

commit 277ba04461c2746cf935353474c0961161951b68
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Aug 3 16:10:49 2012 -0400

    tracing: Add interface to allow multiple trace buffers
    
    Add the interface ("instances" directory) to add multiple buffers
    to ftrace. To create a new instance, simply do a mkdir in the
    instances directory:
    
    This will create a directory with the following:
    
     # cd instances
     # mkdir foo
     # ls foo
    buffer_size_kb        free_buffer  trace_clock    trace_pipe
    buffer_total_size_kb  set_event    trace_marker   tracing_enabled
    events/               trace        trace_options  tracing_on
    
    Currently only events are able to be set, and there isn't a way
    to delete a buffer when one is created (yet).
    
    Note, the i_mutex lock is dropped from the parent "instances"
    directory during the mkdir operation. As the "instances" directory
    can not be renamed or deleted (created on boot), I do not see
    any harm in dropping the lock. The creation of the sub directories
    is protected by trace_types_lock mutex, which only lets one
    instance get into the code path at a time. If two tasks try to
    create or delete directories of the same name, only one will occur
    and the other will fail with -EEXIST.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 68cad7a9e089..883fe0b62f0a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -175,6 +175,7 @@ struct tracer;
 struct trace_array {
 	struct ring_buffer	*buffer;
 	struct list_head	list;
+	char			*name;
 	int			cpu;
 	int			buffer_disabled;
 	struct trace_cpu	trace_cpu;	/* place holder */
@@ -999,6 +1000,7 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 }
 
 extern void trace_event_enable_cmd_record(bool enable);
+extern int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr);
 
 extern struct mutex event_mutex;
 extern struct list_head ftrace_events;

commit 12ab74ee00d154bc05ea2fc659b7ce6519e5d5a6
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Aug 8 14:48:20 2012 -0400

    tracing: Make syscall events suitable for multiple buffers
    
    Currently the syscall events record into the global buffer. But if
    multiple buffers are in place, then we need to have syscall events
    record in the proper buffers.
    
    By adding descriptors to pass to the syscall event functions, the
    syscall events can now record into the buffers that have been assigned
    to them (one event may be applied to mulitple buffers).
    
    This will allow tracing high volume syscalls along with seldom occurring
    syscalls without losing the seldom syscall events.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 15ccd7cd1560..68cad7a9e089 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -13,6 +13,11 @@
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
 
+#ifdef CONFIG_FTRACE_SYSCALLS
+#include <asm/unistd.h>		/* For NR_SYSCALLS	     */
+#include <asm/syscall.h>	/* some archs define it here */
+#endif
+
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
 
@@ -173,6 +178,12 @@ struct trace_array {
 	int			cpu;
 	int			buffer_disabled;
 	struct trace_cpu	trace_cpu;	/* place holder */
+#ifdef CONFIG_FTRACE_SYSCALLS
+	int			sys_refcount_enter;
+	int			sys_refcount_exit;
+	DECLARE_BITMAP(enabled_enter_syscalls, NR_syscalls);
+	DECLARE_BITMAP(enabled_exit_syscalls, NR_syscalls);
+#endif
 	int			stop_count;
 	int			clock_id;
 	struct tracer		*current_trace;

commit a7603ff4b5f7e26e67af82a4c3d05eeeb8d7b160
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 6 16:24:11 2012 -0400

    tracing: Replace the static global per_cpu arrays with allocated per_cpu
    
    The global and max-tr currently use static per_cpu arrays for the CPU data
    descriptors. But in order to get new allocated trace_arrays, they need to
    be allocated per_cpu arrays. Instead of using the static arrays, switch
    the global and max-tr to use allocated data.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b80fbcf70af4..15ccd7cd1560 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -186,7 +186,7 @@ struct trace_array {
 	struct list_head	systems;
 	struct list_head	events;
 	struct task_struct	*waiter;
-	struct trace_array_cpu	*data[NR_CPUS];
+	struct trace_array_cpu	*data;
 };
 
 enum {

commit 2b6080f28c7cc3efc8625ab71495aae89aeb63a0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 11 13:29:49 2012 -0400

    tracing: Encapsulate global_trace and remove dependencies on global vars
    
    The global_trace variable in kernel/trace/trace.c has been kept 'static' and
    local to that file so that it would not be used too much outside of that
    file. This has paid off, even though there were lots of changes to make
    the trace_array structure more generic (not depending on global_trace).
    
    Removal of a lot of direct usages of global_trace is needed to be able to
    create more trace_arrays such that we can add multiple buffers.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index da09a037abcd..b80fbcf70af4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -127,12 +127,21 @@ enum trace_flag_type {
 
 #define TRACE_BUF_SIZE		1024
 
+struct trace_array;
+
+struct trace_cpu {
+	struct trace_array	*tr;
+	struct dentry		*dir;
+	int			cpu;
+};
+
 /*
  * The CPU trace array - it consists of thousands of trace entries
  * plus some other descriptor data: (for example which task started
  * the trace, etc.)
  */
 struct trace_array_cpu {
+	struct trace_cpu	trace_cpu;
 	atomic_t		disabled;
 	void			*buffer_page;	/* ring buffer spare */
 
@@ -151,6 +160,8 @@ struct trace_array_cpu {
 	char			comm[TASK_COMM_LEN];
 };
 
+struct tracer;
+
 /*
  * The trace array - an array of per-CPU trace arrays. This is the
  * highest level data structure that individual tracers deal with.
@@ -161,9 +172,16 @@ struct trace_array {
 	struct list_head	list;
 	int			cpu;
 	int			buffer_disabled;
+	struct trace_cpu	trace_cpu;	/* place holder */
+	int			stop_count;
+	int			clock_id;
+	struct tracer		*current_trace;
 	unsigned int		flags;
 	cycle_t			time_start;
+	raw_spinlock_t		start_lock;
 	struct dentry		*dir;
+	struct dentry		*options;
+	struct dentry		*percpu_dir;
 	struct dentry		*event_dir;
 	struct list_head	systems;
 	struct list_head	events;
@@ -474,6 +492,7 @@ struct dentry *trace_create_file(const char *name,
 				 void *data,
 				 const struct file_operations *fops);
 
+struct dentry *tracing_init_dentry_tr(struct trace_array *tr);
 struct dentry *tracing_init_dentry(void);
 
 struct ring_buffer_event;
@@ -979,7 +998,7 @@ extern const char *__stop___trace_bprintk_fmt[];
 void trace_printk_init_buffers(void);
 void trace_printk_start_comm(void);
 int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);
-int set_tracer_flag(unsigned int mask, int enabled);
+int set_tracer_flag(struct trace_array *tr, unsigned int mask, int enabled);
 
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\

commit ae3b5093ad6004b52e2825f3db1ad8200a2724d8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Jan 23 15:22:59 2013 -0500

    tracing: Use RING_BUFFER_ALL_CPUS for TRACE_PIPE_ALL_CPU
    
    Both RING_BUFFER_ALL_CPUS and TRACE_PIPE_ALL_CPU are defined as
    -1 and used to say that all the ring buffers are to be modified
    or read (instead of just a single cpu, which would be >= 0).
    
    There's no reason to keep TRACE_PIPE_ALL_CPU as it is also started
    to be used for more than what it was created for, and now that
    the ring buffer code added a generic RING_BUFFER_ALL_CPUS define,
    we can clean up the trace code to use that instead and remove
    the TRACE_PIPE_ALL_CPU macro.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 037f7eb03d69..da09a037abcd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -453,8 +453,6 @@ static __always_inline void trace_clear_recursion(int bit)
 	current->trace_recursion = val;
 }
 
-#define TRACE_PIPE_ALL_CPU	-1
-
 static inline struct ring_buffer_iter *
 trace_buffer_iter(struct trace_iterator *iter, int cpu)
 {

commit ae63b31e4d0e2ec09c569306ea46f664508ef717
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 3 23:09:03 2012 -0400

    tracing: Separate out trace events from global variables
    
    The trace events for ftrace are all defined via global variables.
    The arrays of events and event systems are linked to a global list.
    This prevents multiple users of the event system (what to enable and
    what not to).
    
    By adding descriptors to represent the event/file relation, as well
    as to which trace_array descriptor they are associated with, allows
    for more than one set of events to be defined. Once the trace events
    files have a link between the trace event and the trace_array they
    are associated with, we can create multiple trace_arrays that can
    record separate events in separate buffers.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2081971367ea..037f7eb03d69 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -158,13 +158,39 @@ struct trace_array_cpu {
  */
 struct trace_array {
 	struct ring_buffer	*buffer;
+	struct list_head	list;
 	int			cpu;
 	int			buffer_disabled;
+	unsigned int		flags;
 	cycle_t			time_start;
+	struct dentry		*dir;
+	struct dentry		*event_dir;
+	struct list_head	systems;
+	struct list_head	events;
 	struct task_struct	*waiter;
 	struct trace_array_cpu	*data[NR_CPUS];
 };
 
+enum {
+	TRACE_ARRAY_FL_GLOBAL	= (1 << 0)
+};
+
+extern struct list_head ftrace_trace_arrays;
+
+/*
+ * The global tracer (top) should be the first trace array added,
+ * but we check the flag anyway.
+ */
+static inline struct trace_array *top_trace_array(void)
+{
+	struct trace_array *tr;
+
+	tr = list_entry(ftrace_trace_arrays.prev,
+			typeof(*tr), list);
+	WARN_ON(!(tr->flags & TRACE_ARRAY_FL_GLOBAL));
+	return tr;
+}
+
 #define FTRACE_CMP_TYPE(var, type) \
 	__builtin_types_compatible_p(typeof(var), type *)
 
@@ -851,12 +877,19 @@ struct event_filter {
 struct event_subsystem {
 	struct list_head	list;
 	const char		*name;
-	struct dentry		*entry;
 	struct event_filter	*filter;
-	int			nr_events;
 	int			ref_count;
 };
 
+struct ftrace_subsystem_dir {
+	struct list_head		list;
+	struct event_subsystem		*subsystem;
+	struct trace_array		*tr;
+	struct dentry			*entry;
+	int				ref_count;
+	int				nr_events;
+};
+
 #define FILTER_PRED_INVALID	((unsigned short)-1)
 #define FILTER_PRED_IS_RIGHT	(1 << 15)
 #define FILTER_PRED_FOLD	(1 << 15)
@@ -914,7 +947,7 @@ extern void print_event_filter(struct ftrace_event_call *call,
 			       struct trace_seq *s);
 extern int apply_event_filter(struct ftrace_event_call *call,
 			      char *filter_string);
-extern int apply_subsystem_event_filter(struct event_subsystem *system,
+extern int apply_subsystem_event_filter(struct ftrace_subsystem_dir *dir,
 					char *filter_string);
 extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);

commit 613f04a0f51e6e68ac6fe571ab79da3c0a5eb4da
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Mar 14 15:03:53 2013 -0400

    tracing: Prevent buffer overwrite disabled for latency tracers
    
    The latency tracers require the buffers to be in overwrite mode,
    otherwise they get screwed up. Force the buffers to stay in overwrite
    mode when latency tracers are enabled.
    
    Added a flag_changed() method to the tracer structure to allow
    the tracers to see what flags are being changed, and also be able
    to prevent the change from happing.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 57d7e5397d56..2081971367ea 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -283,11 +283,15 @@ struct tracer {
 	enum print_line_t	(*print_line)(struct trace_iterator *iter);
 	/* If you handled the flag setting, return 0 */
 	int			(*set_flag)(u32 old_flags, u32 bit, int set);
+	/* Return 0 if OK with change, else return non-zero */
+	int			(*flag_changed)(struct tracer *tracer,
+						u32 mask, int set);
 	struct tracer		*next;
 	struct tracer_flags	*flags;
 	bool			print_max;
 	bool			use_max_tr;
 	bool			allocated_snapshot;
+	bool			enabled;
 };
 
 
@@ -943,6 +947,8 @@ extern const char *__stop___trace_bprintk_fmt[];
 
 void trace_printk_init_buffers(void);
 void trace_printk_start_comm(void);
+int trace_keep_overwrite(struct tracer *tracer, u32 mask, int set);
+int set_tracer_flag(unsigned int mask, int enabled);
 
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\

commit debdd57f5145f3c6a4b3f8d0126abd1a2def7fc6
Author: Hiraku Toyooka <hiraku.toyooka.gu@hitachi.com>
Date:   Wed Dec 26 11:53:00 2012 +0900

    tracing: Make a snapshot feature available from userspace
    
    Ftrace has a snapshot feature available from kernel space and
    latency tracers (e.g. irqsoff) are using it. This patch enables
    user applictions to take a snapshot via debugfs.
    
    Add "snapshot" debugfs file in "tracing" directory.
    
      snapshot:
        This is used to take a snapshot and to read the output of the
        snapshot.
    
         # echo 1 > snapshot
    
        This will allocate the spare buffer for snapshot (if it is
        not allocated), and take a snapshot.
    
         # cat snapshot
    
        This will show contents of the snapshot.
    
         # echo 0 > snapshot
    
        This will free the snapshot if it is allocated.
    
        Any other positive values will clear the snapshot contents if
        the snapshot is allocated, or return EINVAL if it is not allocated.
    
    Link: http://lkml.kernel.org/r/20121226025300.3252.86850.stgit@liselsia
    
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: David Sharp <dhsharp@google.com>
    Signed-off-by: Hiraku Toyooka <hiraku.toyooka.gu@hitachi.com>
    [
       Fixed irqsoff selftest and also a conflict with a change
       that fixes the update_max_tr.
    ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 04a2c7ab1735..57d7e5397d56 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -287,6 +287,7 @@ struct tracer {
 	struct tracer_flags	*flags;
 	bool			print_max;
 	bool			use_max_tr;
+	bool			allocated_snapshot;
 };
 
 

commit 567cd4da54ff45513d2ca1f0e3cb9ba45b66d6cf
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 2 18:33:05 2012 -0400

    ring-buffer: User context bit recursion checking
    
    Using context bit recursion checking, we can help increase the
    performance of the ring buffer.
    
    Before this patch:
    
     # echo function > /debug/tracing/current_tracer
     # for i in `seq 10`; do ./hackbench 50; done
    Time: 10.285
    Time: 10.407
    Time: 10.243
    Time: 10.372
    Time: 10.380
    Time: 10.198
    Time: 10.272
    Time: 10.354
    Time: 10.248
    Time: 10.253
    
    (average: 10.3012)
    
    Now we have:
    
     # echo function > /debug/tracing/current_tracer
     # for i in `seq 10`; do ./hackbench 50; done
    Time: 9.712
    Time: 9.824
    Time: 9.861
    Time: 9.827
    Time: 9.962
    Time: 9.905
    Time: 9.886
    Time: 10.088
    Time: 9.861
    Time: 9.834
    
    (average: 9.876)
    
     a 4% savings!
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c203a51dd412..04a2c7ab1735 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -291,11 +291,6 @@ struct tracer {
 
 
 /* Only current can touch trace_recursion */
-#define trace_recursion_inc() do { (current)->trace_recursion++; } while (0)
-#define trace_recursion_dec() do { (current)->trace_recursion--; } while (0)
-
-/* Ring buffer has the 10 LSB bits to count */
-#define trace_recursion_buffer() ((current)->trace_recursion & 0x3ff)
 
 /*
  * For function tracing recursion:
@@ -323,7 +318,13 @@ struct tracer {
  * caller, and we can skip the current check.
  */
 enum {
-	TRACE_FTRACE_BIT = 11,
+	TRACE_BUFFER_BIT,
+	TRACE_BUFFER_NMI_BIT,
+	TRACE_BUFFER_IRQ_BIT,
+	TRACE_BUFFER_SIRQ_BIT,
+
+	/* Start of function recursion bits */
+	TRACE_FTRACE_BIT,
 	TRACE_FTRACE_NMI_BIT,
 	TRACE_FTRACE_IRQ_BIT,
 	TRACE_FTRACE_SIRQ_BIT,

commit edc15cafcbfa3d73f819cae99885a2e35e4cbce5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 2 17:47:21 2012 -0400

    tracing: Avoid unnecessary multiple recursion checks
    
    When function tracing occurs, the following steps are made:
      If arch does not support a ftrace feature:
       call internal function (uses INTERNAL bits) which calls...
      If callback is registered to the "global" list, the list
       function is called and recursion checks the GLOBAL bits.
       then this function calls...
      The function callback, which can use the FTRACE bits to
       check for recursion.
    
    Now if the arch does not suppport a feature, and it calls
    the global list function which calls the ftrace callback
    all three of these steps will do a recursion protection.
    There's no reason to do one if the previous caller already
    did. The recursion that we are protecting against will
    go through the same steps again.
    
    To prevent the multiple recursion checks, if a recursion
    bit is set that is higher than the MAX bit of the current
    check, then we know that the check was made by the previous
    caller, and we can skip the current check.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5a095d6f088d..c203a51dd412 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -297,18 +297,49 @@ struct tracer {
 /* Ring buffer has the 10 LSB bits to count */
 #define trace_recursion_buffer() ((current)->trace_recursion & 0x3ff)
 
-/* for function tracing recursion */
+/*
+ * For function tracing recursion:
+ *  The order of these bits are important.
+ *
+ *  When function tracing occurs, the following steps are made:
+ *   If arch does not support a ftrace feature:
+ *    call internal function (uses INTERNAL bits) which calls...
+ *   If callback is registered to the "global" list, the list
+ *    function is called and recursion checks the GLOBAL bits.
+ *    then this function calls...
+ *   The function callback, which can use the FTRACE bits to
+ *    check for recursion.
+ *
+ * Now if the arch does not suppport a feature, and it calls
+ * the global list function which calls the ftrace callback
+ * all three of these steps will do a recursion protection.
+ * There's no reason to do one if the previous caller already
+ * did. The recursion that we are protecting against will
+ * go through the same steps again.
+ *
+ * To prevent the multiple recursion checks, if a recursion
+ * bit is set that is higher than the MAX bit of the current
+ * check, then we know that the check was made by the previous
+ * caller, and we can skip the current check.
+ */
 enum {
-	TRACE_INTERNAL_BIT = 11,
-	TRACE_INTERNAL_NMI_BIT,
-	TRACE_INTERNAL_IRQ_BIT,
-	TRACE_INTERNAL_SIRQ_BIT,
+	TRACE_FTRACE_BIT = 11,
+	TRACE_FTRACE_NMI_BIT,
+	TRACE_FTRACE_IRQ_BIT,
+	TRACE_FTRACE_SIRQ_BIT,
 
+	/* GLOBAL_BITs must be greater than FTRACE_BITs */
 	TRACE_GLOBAL_BIT,
 	TRACE_GLOBAL_NMI_BIT,
 	TRACE_GLOBAL_IRQ_BIT,
 	TRACE_GLOBAL_SIRQ_BIT,
 
+	/* INTERNAL_BITs must be greater than GLOBAL_BITs */
+	TRACE_INTERNAL_BIT,
+	TRACE_INTERNAL_NMI_BIT,
+	TRACE_INTERNAL_IRQ_BIT,
+	TRACE_INTERNAL_SIRQ_BIT,
+
 	TRACE_CONTROL_BIT,
 
 /*
@@ -325,6 +356,71 @@ enum {
 #define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(1<<(bit)); } while (0)
 #define trace_recursion_test(bit)	((current)->trace_recursion & (1<<(bit)))
 
+#define TRACE_CONTEXT_BITS	4
+
+#define TRACE_FTRACE_START	TRACE_FTRACE_BIT
+#define TRACE_FTRACE_MAX	((1 << (TRACE_FTRACE_START + TRACE_CONTEXT_BITS)) - 1)
+
+#define TRACE_GLOBAL_START	TRACE_GLOBAL_BIT
+#define TRACE_GLOBAL_MAX	((1 << (TRACE_GLOBAL_START + TRACE_CONTEXT_BITS)) - 1)
+
+#define TRACE_LIST_START	TRACE_INTERNAL_BIT
+#define TRACE_LIST_MAX		((1 << (TRACE_LIST_START + TRACE_CONTEXT_BITS)) - 1)
+
+#define TRACE_CONTEXT_MASK	TRACE_LIST_MAX
+
+static __always_inline int trace_get_context_bit(void)
+{
+	int bit;
+
+	if (in_interrupt()) {
+		if (in_nmi())
+			bit = 0;
+
+		else if (in_irq())
+			bit = 1;
+		else
+			bit = 2;
+	} else
+		bit = 3;
+
+	return bit;
+}
+
+static __always_inline int trace_test_and_set_recursion(int start, int max)
+{
+	unsigned int val = current->trace_recursion;
+	int bit;
+
+	/* A previous recursion check was made */
+	if ((val & TRACE_CONTEXT_MASK) > max)
+		return 0;
+
+	bit = trace_get_context_bit() + start;
+	if (unlikely(val & (1 << bit)))
+		return -1;
+
+	val |= 1 << bit;
+	current->trace_recursion = val;
+	barrier();
+
+	return bit;
+}
+
+static __always_inline void trace_clear_recursion(int bit)
+{
+	unsigned int val = current->trace_recursion;
+
+	if (!bit)
+		return;
+
+	bit = 1 << bit;
+	val &= ~bit;
+
+	barrier();
+	current->trace_recursion = val;
+}
+
 #define TRACE_PIPE_ALL_CPU	-1
 
 static inline struct ring_buffer_iter *

commit e46cbf75c621725964fe1f6e7013e8bcd86a0e3d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 2 17:32:25 2012 -0400

    tracing: Make the trace recursion bits into enums
    
    Convert the bits into enums which makes the code a little easier
    to maintain.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fe6ccff9cc70..5a095d6f088d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -298,15 +298,18 @@ struct tracer {
 #define trace_recursion_buffer() ((current)->trace_recursion & 0x3ff)
 
 /* for function tracing recursion */
-#define TRACE_INTERNAL_BIT		(1<<11)
-#define TRACE_INTERNAL_NMI_BIT		(1<<12)
-#define TRACE_INTERNAL_IRQ_BIT		(1<<13)
-#define TRACE_INTERNAL_SIRQ_BIT		(1<<14)
-#define TRACE_GLOBAL_BIT		(1<<15)
-#define TRACE_GLOBAL_NMI_BIT		(1<<16)
-#define TRACE_GLOBAL_IRQ_BIT		(1<<17)
-#define TRACE_GLOBAL_SIRQ_BIT		(1<<18)
-#define TRACE_CONTROL_BIT		(1<<19)
+enum {
+	TRACE_INTERNAL_BIT = 11,
+	TRACE_INTERNAL_NMI_BIT,
+	TRACE_INTERNAL_IRQ_BIT,
+	TRACE_INTERNAL_SIRQ_BIT,
+
+	TRACE_GLOBAL_BIT,
+	TRACE_GLOBAL_NMI_BIT,
+	TRACE_GLOBAL_IRQ_BIT,
+	TRACE_GLOBAL_SIRQ_BIT,
+
+	TRACE_CONTROL_BIT,
 
 /*
  * Abuse of the trace_recursion.
@@ -315,11 +318,12 @@ struct tracer {
  * was called in irq context but we have irq tracing off. Since this
  * can only be modified by current, we can reuse trace_recursion.
  */
-#define TRACE_IRQ_BIT			(1<<20)
+	TRACE_IRQ_BIT,
+};
 
-#define trace_recursion_set(bit)	do { (current)->trace_recursion |= (bit); } while (0)
-#define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(bit); } while (0)
-#define trace_recursion_test(bit)	((current)->trace_recursion & (bit))
+#define trace_recursion_set(bit)	do { (current)->trace_recursion |= (1<<(bit)); } while (0)
+#define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(1<<(bit)); } while (0)
+#define trace_recursion_test(bit)	((current)->trace_recursion & (1<<(bit)))
 
 #define TRACE_PIPE_ALL_CPU	-1
 

commit c29f122cd7fc178b72b1335b1fce0dff2e5c0f5d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 2 17:17:59 2012 -0400

    ftrace: Add context level recursion bit checking
    
    Currently for recursion checking in the function tracer, ftrace
    tests a task_struct bit to determine if the function tracer had
    recursed or not. If it has, then it will will return without going
    further.
    
    But this leads to races. If an interrupt came in after the bit
    was set, the functions being traced would see that bit set and
    think that the function tracer recursed on itself, and would return.
    
    Instead add a bit for each context (normal, softirq, irq and nmi).
    
    A check of which context the task is in is made before testing the
    associated bit. Now if an interrupt preempts the function tracer
    after the previous context has been set, the interrupt functions
    can still be traced.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c75d7988902c..fe6ccff9cc70 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -299,8 +299,14 @@ struct tracer {
 
 /* for function tracing recursion */
 #define TRACE_INTERNAL_BIT		(1<<11)
-#define TRACE_GLOBAL_BIT		(1<<12)
-#define TRACE_CONTROL_BIT		(1<<13)
+#define TRACE_INTERNAL_NMI_BIT		(1<<12)
+#define TRACE_INTERNAL_IRQ_BIT		(1<<13)
+#define TRACE_INTERNAL_SIRQ_BIT		(1<<14)
+#define TRACE_GLOBAL_BIT		(1<<15)
+#define TRACE_GLOBAL_NMI_BIT		(1<<16)
+#define TRACE_GLOBAL_IRQ_BIT		(1<<17)
+#define TRACE_GLOBAL_SIRQ_BIT		(1<<18)
+#define TRACE_CONTROL_BIT		(1<<19)
 
 /*
  * Abuse of the trace_recursion.
@@ -309,7 +315,7 @@ struct tracer {
  * was called in irq context but we have irq tracing off. Since this
  * can only be modified by current, we can reuse trace_recursion.
  */
-#define TRACE_IRQ_BIT			(1<<13)
+#define TRACE_IRQ_BIT			(1<<20)
 
 #define trace_recursion_set(bit)	do { (current)->trace_recursion |= (bit); } while (0)
 #define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(bit); } while (0)

commit 8be0709f10e3dd5d7d07933ad61a9f18c4b93ca5
Author: David Sharp <dhsharp@google.com>
Date:   Tue Nov 13 12:18:22 2012 -0800

    tracing: Format non-nanosec times from tsc clock without a decimal point.
    
    With the addition of the "tsc" clock, formatting timestamps to look like
    fractional seconds is misleading. Mark clocks as either in nanoseconds or
    not, and format non-nanosecond timestamps as decimal integers.
    
    Tested:
    $ cd /sys/kernel/debug/tracing/
    $ cat trace_clock
    [local] global tsc
    $ echo sched_switch > set_event
    $ echo 1 > tracing_on ; sleep 0.0005 ; echo 0 > tracing_on
    $ cat trace
              <idle>-0     [000]  6330.555552: sched_switch: prev_comm=swapper prev_pid=0 prev_prio=120 prev_state=R ==> next_comm=bash next_pid=29964 next_prio=120
               sleep-29964 [000]  6330.555628: sched_switch: prev_comm=bash prev_pid=29964 prev_prio=120 prev_state=S ==> next_comm=swapper next_pid=0 next_prio=120
      ...
    $ echo 1 > options/latency-format
    $ cat trace
      <idle>-0       0 4104553247us+: sched_switch: prev_comm=swapper prev_pid=0 prev_prio=120 prev_state=R ==> next_comm=bash next_pid=29964 next_prio=120
       sleep-29964   0 4104553322us+: sched_switch: prev_comm=bash prev_pid=29964 prev_prio=120 prev_state=S ==> next_comm=swapper next_pid=0 next_prio=120
      ...
    $ echo tsc > trace_clock
    $ cat trace
    $ echo 1 > tracing_on ; sleep 0.0005 ; echo 0 > tracing_on
    $ echo 0 > options/latency-format
    $ cat trace
              <idle>-0     [000] 16490053398357: sched_switch: prev_comm=swapper prev_pid=0 prev_prio=120 prev_state=R ==> next_comm=bash next_pid=31128 next_prio=120
               sleep-31128 [000] 16490053588518: sched_switch: prev_comm=bash prev_pid=31128 prev_prio=120 prev_state=S ==> next_comm=swapper next_pid=0 next_prio=120
      ...
    echo 1 > options/latency-format
    $ cat trace
      <idle>-0       0 91557653238+: sched_switch: prev_comm=swapper prev_pid=0 prev_prio=120 prev_state=R ==> next_comm=bash next_pid=31128 next_prio=120
       sleep-31128   0 91557843399+: sched_switch: prev_comm=bash prev_pid=31128 prev_prio=120 prev_state=S ==> next_comm=swapper next_pid=0 next_prio=120
      ...
    
    v2:
    Move arch-specific bits out of generic code.
    v4:
    Fix x86_32 build due to 64-bit division.
    
    Google-Bug-Id: 6980623
    Link: http://lkml.kernel.org/r/1352837903-32191-2-git-send-email-dhsharp@google.com
    
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: David Sharp <dhsharp@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 55010ed175f0..c75d7988902c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -406,10 +406,6 @@ void tracing_stop_sched_switch_record(void);
 void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 int is_tracing_stopped(void);
-enum trace_file_type {
-	TRACE_FILE_LAT_FMT	= 1,
-	TRACE_FILE_ANNOTATE	= 2,
-};
 
 extern cpumask_var_t __read_mostly tracing_buffer_mask;
 

commit 0d5c6e1c19bab82fad4837108c2902f557d62a04
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Nov 1 20:54:21 2012 -0400

    tracing: Use irq_work for wake ups and remove *_nowake_*() functions
    
    Have the ring buffer commit function use the irq_work infrastructure to
    wake up any waiters waiting on the ring buffer for new data. The irq_work
    was created for such a purpose, where doing the actual wake up at the
    time of adding data is too dangerous, as an event or function trace may
    be in the midst of the work queue locks and cause deadlocks. The irq_work
    will either delay the action to the next timer interrupt, or trigger an IPI
    to itself forcing an interrupt to do the work (in a safe location).
    
    With irq_work, all ring buffer commits can safely do wakeups, removing
    the need for the ring buffer commit "nowake" variants, which were used
    by events and function tracing. All commits can now safely use the
    normal commit, and the "nowake" variants can be removed.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3e8a176f64e6..55010ed175f0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -327,7 +327,6 @@ trace_buffer_iter(struct trace_iterator *iter, int cpu)
 
 int tracer_init(struct tracer *t, struct trace_array *tr);
 int tracing_is_enabled(void);
-void trace_wake_up(void);
 void tracing_reset(struct trace_array *tr, int cpu);
 void tracing_reset_online_cpus(struct trace_array *tr);
 void tracing_reset_current(int cpu);
@@ -349,9 +348,6 @@ trace_buffer_lock_reserve(struct ring_buffer *buffer,
 			  unsigned long len,
 			  unsigned long flags,
 			  int pc);
-void trace_buffer_unlock_commit(struct ring_buffer *buffer,
-				struct ring_buffer_event *event,
-				unsigned long flags, int pc);
 
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);
@@ -370,7 +366,6 @@ void trace_init_global_iter(struct trace_iterator *iter);
 
 void tracing_iter_reset(struct trace_iterator *iter, int cpu);
 
-void default_wait_pipe(struct trace_iterator *iter);
 void poll_wait_pipe(struct trace_iterator *iter);
 
 void ftrace(struct trace_array *tr,

commit c7b84ecada9a8b7fe3e6c081e70801703897ed5d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 11 20:54:53 2012 -0400

    tracing: Remove unused function unregister_tracer()
    
    The function register_tracer() is only used by kernel core code,
    that never needs to remove the tracer. As trace_events have become
    the main way to add new tracing to the kernel, the need to
    unregister a tracer has diminished. Remove the unused function
    unregister_tracer(). If a need arises where we need it, then we
    can always add it back.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 839ae003a053..3e8a176f64e6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -410,7 +410,6 @@ void tracing_sched_switch_assign_trace(struct trace_array *tr);
 void tracing_stop_sched_switch_record(void);
 void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
-void unregister_tracer(struct tracer *type);
 int is_tracing_stopped(void);
 enum trace_file_type {
 	TRACE_FILE_LAT_FMT	= 1,

commit 7ffbd48d5cab22bcd1120eb2349db1319e2d827a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Oct 11 12:14:25 2012 -0400

    tracing: Cache comms only after an event occurred
    
    Whenever an event is registered, the comm of tasks are saved at
    every task switch instead of saving them at every event. But if
    an event isn't executed much, the comm cache will be filled up
    by tasks that did not record the event and you lose out on the comms
    that did.
    
    Here's an example, if you enable the following events:
    
    echo 1 > /debug/tracing/events/kvm/kvm_cr/enable
    echo 1 > /debug/tracing/events/net/net_dev_xmit/enable
    
    Note, there's no kvm running on this machine so the first event will
    never be triggered, but because it is enabled, the storing of comms
    will continue. If we now disable the network event:
    
    echo 0 > /debug/tracing/events/net/net_dev_xmit/enable
    
    and look at the trace:
    
    cat /debug/tracing/trace
                sshd-2672  [001] ..s2   375.731616: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s1   375.731617: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s2   375.859356: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s1   375.859357: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s2   375.947351: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s1   375.947352: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s2   376.035383: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s1   376.035383: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
                sshd-2672  [001] ..s2   377.563806: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=226 rc=0
                sshd-2672  [001] ..s1   377.563807: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=226 rc=0
                sshd-2672  [001] ..s2   377.563834: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6be0 len=114 rc=0
                sshd-2672  [001] ..s1   377.563842: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6be0 len=114 rc=0
    
    We see that process 2672 which triggered the events has the comm "sshd".
    But if we run hackbench for a bit and look again:
    
    cat /debug/tracing/trace
               <...>-2672  [001] ..s2   375.731616: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s1   375.731617: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s2   375.859356: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s1   375.859357: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s2   375.947351: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s1   375.947352: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s2   376.035383: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s1   376.035383: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=242 rc=0
               <...>-2672  [001] ..s2   377.563806: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6de0 len=226 rc=0
               <...>-2672  [001] ..s1   377.563807: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6de0 len=226 rc=0
               <...>-2672  [001] ..s2   377.563834: net_dev_xmit: dev=eth0 skbaddr=ffff88005cbb6be0 len=114 rc=0
               <...>-2672  [001] ..s1   377.563842: net_dev_xmit: dev=br0 skbaddr=ffff88005cbb6be0 len=114 rc=0
    
    The stored "sshd" comm has been flushed out and we get a useless "<...>".
    
    But by only storing comms after a trace event occurred, we can run
    hackbench all day and still get the same output.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7824a55bd3fc..839ae003a053 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -359,6 +359,9 @@ struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 struct trace_entry *trace_find_next_entry(struct trace_iterator *iter,
 					  int *ent_cpu, u64 *ent_ts);
 
+void __buffer_unlock_commit(struct ring_buffer *buffer,
+			    struct ring_buffer_event *event);
+
 int trace_empty(struct trace_iterator *iter);
 
 void *trace_find_next_entry_inc(struct trace_iterator *iter);

commit 81698831bc462ff16f76bc11249a1e492424da4c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Oct 11 10:15:05 2012 -0400

    tracing: Enable comm recording if trace_printk() is used
    
    If comm recording is not enabled when trace_printk() is used then
    you just get this type of output:
    
    [ adding trace_printk("hello! %d", irq); in do_IRQ ]
    
               <...>-2843  [001] d.h.    80.812300: do_IRQ: hello! 14
               <...>-2734  [002] d.h2    80.824664: do_IRQ: hello! 14
               <...>-2713  [003] d.h.    80.829971: do_IRQ: hello! 14
               <...>-2814  [000] d.h.    80.833026: do_IRQ: hello! 14
    
    By enabling the comm recorder when trace_printk is enabled:
    
           hackbench-6715  [001] d.h.   193.233776: do_IRQ: hello! 21
                sshd-2659  [001] d.h.   193.665862: do_IRQ: hello! 21
              <idle>-0     [001] d.h1   193.665996: do_IRQ: hello! 21
    
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c56a233c006e..7824a55bd3fc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -841,6 +841,7 @@ extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 
 void trace_printk_init_buffers(void);
+void trace_printk_start_comm(void);
 
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\

commit f43c738bfa8608424610e4fc1aef4d4644e2ce11
Author: Hiraku Toyooka <hiraku.toyooka.gu@hitachi.com>
Date:   Tue Oct 2 17:27:10 2012 +0900

    tracing: Change tracer's integer flags to bool
    
    print_max and use_max_tr in struct tracer are "int" variables and
    used like flags. This is wasteful, so change the type to "bool".
    
    Link: http://lkml.kernel.org/r/20121002082710.9807.86393.stgit@falsita
    
    Signed-off-by: Hiraku Toyooka <hiraku.toyooka.gu@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c15f528c1af4..c56a233c006e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -285,8 +285,8 @@ struct tracer {
 	int			(*set_flag)(u32 old_flags, u32 bit, int set);
 	struct tracer		*next;
 	struct tracer_flags	*flags;
-	int			print_max;
-	int			use_max_tr;
+	bool			print_max;
+	bool			use_max_tr;
 };
 
 

commit 437589a74b6a590d175f86cf9f7b2efcee7765e7
Merge: 68d47a137c3b 72235465864d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 2 11:11:09 2012 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace
    
    Pull user namespace changes from Eric Biederman:
     "This is a mostly modest set of changes to enable basic user namespace
      support.  This allows the code to code to compile with user namespaces
      enabled and removes the assumption there is only the initial user
      namespace.  Everything is converted except for the most complex of the
      filesystems: autofs4, 9p, afs, ceph, cifs, coda, fuse, gfs2, ncpfs,
      nfs, ocfs2 and xfs as those patches need a bit more review.
    
      The strategy is to push kuid_t and kgid_t values are far down into
      subsystems and filesystems as reasonable.  Leaving the make_kuid and
      from_kuid operations to happen at the edge of userspace, as the values
      come off the disk, and as the values come in from the network.
      Letting compile type incompatible compile errors (present when user
      namespaces are enabled) guide me to find the issues.
    
      The most tricky areas have been the places where we had an implicit
      union of uid and gid values and were storing them in an unsigned int.
      Those places were converted into explicit unions.  I made certain to
      handle those places with simple trivial patches.
    
      Out of that work I discovered we have generic interfaces for storing
      quota by projid.  I had never heard of the project identifiers before.
      Adding full user namespace support for project identifiers accounts
      for most of the code size growth in my git tree.
    
      Ultimately there will be work to relax privlige checks from
      "capable(FOO)" to "ns_capable(user_ns, FOO)" where it is safe allowing
      root in a user names to do those things that today we only forbid to
      non-root users because it will confuse suid root applications.
    
      While I was pushing kuid_t and kgid_t changes deep into the audit code
      I made a few other cleanups.  I capitalized on the fact we process
      netlink messages in the context of the message sender.  I removed
      usage of NETLINK_CRED, and started directly using current->tty.
    
      Some of these patches have also made it into maintainer trees, with no
      problems from identical code from different trees showing up in
      linux-next.
    
      After reading through all of this code I feel like I might be able to
      win a game of kernel trivial pursuit."
    
    Fix up some fairly trivial conflicts in netfilter uid/git logging code.
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/ebiederm/user-namespace: (107 commits)
      userns: Convert the ufs filesystem to use kuid/kgid where appropriate
      userns: Convert the udf filesystem to use kuid/kgid where appropriate
      userns: Convert ubifs to use kuid/kgid
      userns: Convert squashfs to use kuid/kgid where appropriate
      userns: Convert reiserfs to use kuid and kgid where appropriate
      userns: Convert jfs to use kuid/kgid where appropriate
      userns: Convert jffs2 to use kuid and kgid where appropriate
      userns: Convert hpfs to use kuid and kgid where appropriate
      userns: Convert btrfs to use kuid/kgid where appropriate
      userns: Convert bfs to use kuid/kgid where appropriate
      userns: Convert affs to use kuid/kgid wherwe appropriate
      userns: On alpha modify linux_to_osf_stat to use convert from kuids and kgids
      userns: On ia64 deal with current_uid and current_gid being kuid and kgid
      userns: On ppc convert current_uid from a kuid before printing.
      userns: Convert s390 getting uid and gid system calls to use kuid and kgid
      userns: Convert s390 hypfs to use kuid and kgid where appropriate
      userns: Convert binder ipc to use kuids
      userns: Teach security_path_chown to take kuids and kgids
      userns: Add user namespace support to IMA
      userns: Convert EVM to deal with kuids and kgids in it's hmac computation
      ...

commit 5224c3a31549f1c056039545b289e1b01ed02f12
Author: Mandeep Singh Baines <mandeep.baines@gmail.com>
Date:   Fri Sep 7 18:12:19 2012 -0700

    tracing: Add an option for disabling markers
    
    In our application, we have trace markers spread through user-space.
    We have markers in GL, X, etc. These are super handy for Chrome's
    about:tracing feature (Chrome + system + kernel trace view), but
    can be very distracting when you're trying to debug a kernel issue.
    
    I normally, use "grep -v tracing_mark_write" but it would be nice
    if I could just temporarily disable markers all together.
    
    Link: http://lkml.kernel.org/r/1347066739-26285-1-git-send-email-msb@chromium.org
    
    CC: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Mandeep Singh Baines <msb@chromium.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 593debefc4e9..63a2da0b9a6e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -680,6 +680,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_OVERWRITE		= 0x200000,
 	TRACE_ITER_STOP_ON_FREE		= 0x400000,
 	TRACE_ITER_IRQ_INFO		= 0x800000,
+	TRACE_ITER_MARKERS		= 0x1000000,
 };
 
 /*

commit d20b92ab668cc44fc84bba0001839c5a8013a5cd
Author: Eric W. Biederman <ebiederm@xmission.com>
Date:   Tue Mar 13 16:02:19 2012 -0700

    userns: Teach trace to use from_kuid
    
    - When tracing capture the kuid.
    - When displaying the data to user space convert the kuid into the
      user namespace of the process that opened the report file.
    
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Eric W. Biederman <ebiederm@xmission.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 55e1f7f0db12..40a6f30c985f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -147,7 +147,7 @@ struct trace_array_cpu {
 	unsigned long		skipped_entries;
 	cycle_t			preempt_timestamp;
 	pid_t			pid;
-	uid_t			uid;
+	kuid_t			uid;
 	char			comm[TASK_COMM_LEN];
 };
 

commit ad97772ad82f57c83968079d0880c71ab126ab04
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jul 20 13:45:59 2012 -0400

    ftrace: Add selftest to test function save-regs support
    
    Add selftests to test the save-regs functionality of ftrace.
    
    If the arch supports saving regs, then it will make sure that regs is
    at least not NULL in the callback.
    
    If the arch does not support saving regs, it makes sure that the
    registering of the ftrace_ops that requests saving regs fails.
    It then tests the registering of the ftrace_ops succeeds if the
    'IF_SUPPORTED' flag is set. Then it makes sure that the regs passed to
    the function is NULL.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 55e1f7f0db12..593debefc4e9 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -472,11 +472,11 @@ extern void trace_find_cmdline(int pid, char comm[]);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern unsigned long ftrace_update_tot_cnt;
+#endif
 #define DYN_FTRACE_TEST_NAME trace_selftest_dynamic_test_func
 extern int DYN_FTRACE_TEST_NAME(void);
 #define DYN_FTRACE_TEST_NAME2 trace_selftest_dynamic_test_func2
 extern int DYN_FTRACE_TEST_NAME2(void);
-#endif
 
 extern int ring_buffer_expanded;
 extern bool tracing_selftest_disabled;

commit 6d158a813efcd09661c23f16ddf7e2ff834cb20c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Jun 27 20:46:14 2012 -0400

    tracing: Remove NR_CPUS array from trace_iterator
    
    Replace the NR_CPUS array of buffer_iter from the trace_iterator
    with an allocated array. This will just create an array of
    possible CPUS instead of the max number specified.
    
    The use of NR_CPUS in that array caused allocation failures for
    machines that were tight on memory. This did not cause any failures
    to the system itself (no crashes), but caused unnecessary failures
    for reading the trace files.
    
    Added a helper function called 'trace_buffer_iter()' that returns
    the buffer_iter item or NULL if it is not defined or the array was
    not allocated. Some routines do not require the array
    (tracing_open_pipe() for one).
    
    Reported-by: Dave Jones <davej@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5aec220d2de0..55e1f7f0db12 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -317,6 +317,14 @@ struct tracer {
 
 #define TRACE_PIPE_ALL_CPU	-1
 
+static inline struct ring_buffer_iter *
+trace_buffer_iter(struct trace_iterator *iter, int cpu)
+{
+	if (iter->buffer_iter && iter->buffer_iter[cpu])
+		return iter->buffer_iter[cpu];
+	return NULL;
+}
+
 int tracer_init(struct tracer *t, struct trace_array *tr);
 int tracing_is_enabled(void);
 void trace_wake_up(void);

commit 654443e20dfc0617231f28a07c96a979ee1a0239
Merge: 2c01e7bc46f1 9cba26e66d09
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu May 24 11:39:34 2012 -0700

    Merge branch 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull user-space probe instrumentation from Ingo Molnar:
     "The uprobes code originates from SystemTap and has been used for years
      in Fedora and RHEL kernels.  This version is much rewritten, reviews
      from PeterZ, Oleg and myself shaped the end result.
    
      This tree includes uprobes support in 'perf probe' - but SystemTap
      (and other tools) can take advantage of user probe points as well.
    
      Sample usage of uprobes via perf, for example to profile malloc()
      calls without modifying user-space binaries.
    
      First boot a new kernel with CONFIG_UPROBE_EVENT=y enabled.
    
      If you don't know which function you want to probe you can pick one
      from 'perf top' or can get a list all functions that can be probed
      within libc (binaries can be specified as well):
    
            $ perf probe -F -x /lib/libc.so.6
    
      To probe libc's malloc():
    
            $ perf probe -x /lib64/libc.so.6 malloc
            Added new event:
            probe_libc:malloc    (on 0x7eac0)
    
      You can now use it in all perf tools, such as:
    
            perf record -e probe_libc:malloc -aR sleep 1
    
      Make use of it to create a call graph (as the flat profile is going to
      look very boring):
    
            $ perf record -e probe_libc:malloc -gR make
            [ perf record: Woken up 173 times to write data ]
            [ perf record: Captured and wrote 44.190 MB perf.data (~1930712
    
            $ perf report | less
    
              32.03%            git  libc-2.15.so   [.] malloc
                                |
                                --- malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                                |
                                --- malloc
                                   |
                                   |--0.95%-- 0x208eb1000000000
                                   |
                                   |--0.63%-- htab_traverse_noresize
    
              11.04%             as  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               7.15%             ld  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
    
               5.07%             sh  libc-2.15.so   [.] malloc
                                 |
                                 --- malloc
                                    |
               4.99%  python-config  libc-2.15.so   [.] malloc
                      |
                      --- malloc
                         |
               4.54%           make  libc-2.15.so   [.] malloc
                               |
                               --- malloc
                                  |
                                  |--7.34%-- glob
                                  |          |
                                  |          |--93.18%-- 0x41588f
                                  |          |
                                  |           --6.82%-- glob
                                  |                     0x41588f
    
               ...
    
      Or:
    
            $ perf report -g flat | less
    
            # Overhead        Command  Shared Object      Symbol
            # ........  .............  .............  ..........
            #
              32.03%            git  libc-2.15.so   [.] malloc
                      27.19%
                          malloc
    
              29.49%            cc1  libc-2.15.so   [.] malloc
                      24.77%
                          malloc
    
              11.04%             as  libc-2.15.so   [.] malloc
                      11.02%
                          malloc
    
               7.15%             ld  libc-2.15.so   [.] malloc
                       6.57%
                          malloc
    
             ...
    
      The core uprobes design is fairly straightforward: uprobes probe
      points register themselves at (inode:offset) addresses of
      libraries/binaries, after which all existing (or new) vmas that map
      that address will have a software breakpoint injected at that address.
      vmas are COW-ed to preserve original content.  The probe points are
      kept in an rbtree.
    
      If user-space executes the probed inode:offset instruction address
      then an event is generated which can be recovered from the regular
      perf event channels and mmap-ed ring-buffer.
    
      Multiple probes at the same address are supported, they create a
      dynamic callback list of event consumers.
    
      The basic model is further complicated by the XOL speedup: the
      original instruction that is probed is copied (in an architecture
      specific fashion) and executed out of line when the probe triggers.
      The XOL area is a single vma per process, with a fixed number of
      entries (which limits probe execution parallelism).
    
      The API: uprobes are installed/removed via
      /sys/kernel/debug/tracing/uprobe_events, the API is integrated to
      align with the kprobes interface as much as possible, but is separate
      to it.
    
      Injecting a probe point is privileged operation, which can be relaxed
      by setting perf_paranoid to -1.
    
      You can use multiple probes as well and mix them with kprobes and
      regular PMU events or tracepoints, when instrumenting a task."
    
    Fix up trivial conflicts in mm/memory.c due to previous cleanup of
    unmap_single_vma().
    
    * 'perf-uprobes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (21 commits)
      perf probe: Detect probe target when m/x options are absent
      perf probe: Provide perf interface for uprobes
      tracing: Fix kconfig warning due to a typo
      tracing: Provide trace events interface for uprobes
      tracing: Extract out common code for kprobes/uprobes trace events
      tracing: Modify is_delete, is_return from int to bool
      uprobes/core: Decrement uprobe count before the pages are unmapped
      uprobes/core: Make background page replacement logic account for rss_stat counters
      uprobes/core: Optimize probe hits with the help of a counter
      uprobes/core: Allocate XOL slots for uprobes use
      uprobes/core: Handle breakpoint and singlestep exceptions
      uprobes/core: Rename bkpt to swbp
      uprobes/core: Make order of function parameters consistent across functions
      uprobes/core: Make macro names consistent
      uprobes: Update copyright notices
      uprobes/core: Move insn to arch specific structure
      uprobes/core: Remove uprobe_opcode_sz
      uprobes/core: Make instruction tables volatile
      uprobes: Move to kernel/events/
      uprobes/core: Clean up, refactor and improve the code
      ...

commit 16ee6576e25b83806d26eb771138249fcfb5eddc
Merge: 16fa7e8200fb 9b63776fa3ca
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri May 18 13:13:33 2012 -0300

    Merge remote-tracking branch 'tip/perf/urgent' into perf/core
    
    Merge reason: We are going to queue up a dependent patch:
    
    "perf tools: Move parse event automated tests to separated object"
    
    That depends on:
    
    commit e7c72d8
    perf tools: Add 'G' and 'H' modifiers to event parsing
    
    Conflicts:
            tools/perf/builtin-stat.c
    
    Conflicted with the recent 'perf_target' patches when checking the
    result of perf_evsel open routines to see if a retry is needed to cope
    with older kernels where the exclude guest/host perf_event_attr bits
    were not used.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

commit 9cba26e66d09bf394ae5a739627a1dc8b7cae6f4
Merge: ec83db0f78cd 73eff9f56e15
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon May 14 14:43:40 2012 +0200

    Merge branch 'perf/uprobes' of git://git.kernel.org/pub/scm/linux/kernel/git/acme/linux into perf/uprobes

commit f3f096cfedf8113380c56fc855275cc75cd8cf55
Author: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
Date:   Wed Apr 11 16:00:43 2012 +0530

    tracing: Provide trace events interface for uprobes
    
    Implements trace_event support for uprobes. In its current form
    it can be used to put probes at a specified offset in a file and
    dump the required registers when the code flow reaches the
    probed address.
    
    The following example shows how to dump the instruction pointer
    and %ax a register at the probed text address.  Here we are
    trying to probe zfree in /bin/zsh:
    
     # cd /sys/kernel/debug/tracing/
     # cat /proc/`pgrep  zsh`/maps | grep /bin/zsh | grep r-xp
     00400000-0048a000 r-xp 00000000 08:03 130904 /bin/zsh
     # objdump -T /bin/zsh | grep -w zfree
     0000000000446420 g    DF .text  0000000000000012  Base
     zfree # echo 'p /bin/zsh:0x46420 %ip %ax' > uprobe_events
     # cat uprobe_events
     p:uprobes/p_zsh_0x46420 /bin/zsh:0x0000000000046420
     # echo 1 > events/uprobes/enable
     # sleep 20
     # echo 0 > events/uprobes/enable
     # cat trace
     # tracer: nop
     #
     #           TASK-PID    CPU#    TIMESTAMP  FUNCTION
     #              | |       |          |         |
                  zsh-24842 [006] 258544.995456: p_zsh_0x46420: (0x446420) arg1=446421 arg2=79
                  zsh-24842 [007] 258545.000270: p_zsh_0x46420: (0x446420) arg1=446421 arg2=79
                  zsh-24842 [002] 258545.043929: p_zsh_0x46420: (0x446420) arg1=446421 arg2=79
                  zsh-24842 [004] 258547.046129: p_zsh_0x46420: (0x446420) arg1=446421 arg2=79
    
    Signed-off-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Jim Keniston <jkenisto@linux.vnet.ibm.com>
    Cc: Linux-mm <linux-mm@kvack.org>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@infradead.org>
    Cc: Anton Arapov <anton@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/20120411103043.GB29437@linux.vnet.ibm.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 95059f091a24..1bcdbec95a11 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -103,6 +103,11 @@ struct kretprobe_trace_entry_head {
 	unsigned long		ret_ip;
 };
 
+struct uprobe_trace_entry_head {
+	struct trace_entry	ent;
+	unsigned long		ip;
+};
+
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:

commit 438ced1720b584000a9e8a4349d1f6bb7ee3ad6d
Author: Vaibhav Nagarnaik <vnagarnaik@google.com>
Date:   Thu Feb 2 12:00:41 2012 -0800

    ring-buffer: Add per_cpu ring buffer control files
    
    Add a debugfs entry under per_cpu/ folder for each cpu called
    buffer_size_kb to control the ring buffer size for each CPU
    independently.
    
    If the global file buffer_size_kb is used to set size, the individual
    ring buffers will be adjusted to the given size. The buffer_size_kb will
    report the common size to maintain backward compatibility.
    
    If the buffer_size_kb file under the per_cpu/ directory is used to
    change buffer size for a specific CPU, only the size of the respective
    ring buffer is updated. When tracing/buffer_size_kb is read, it reports
    'X' to indicate that sizes of per_cpu ring buffers are not equivalent.
    
    Link: http://lkml.kernel.org/r/1328212844-11889-1-git-send-email-vnagarnaik@google.com
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Michael Rubin <mrubin@google.com>
    Cc: David Sharp <dhsharp@google.com>
    Cc: Justin Teravest <teravest@google.com>
    Signed-off-by: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f9d85504f04b..1c8b7c6f7b3b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -131,6 +131,7 @@ struct trace_array_cpu {
 	atomic_t		disabled;
 	void			*buffer_page;	/* ring buffer spare */
 
+	unsigned long		entries;
 	unsigned long		saved_latency;
 	unsigned long		critical_start;
 	unsigned long		critical_end;
@@ -152,7 +153,6 @@ struct trace_array_cpu {
  */
 struct trace_array {
 	struct ring_buffer	*buffer;
-	unsigned long		entries;
 	int			cpu;
 	int			buffer_disabled;
 	cycle_t			time_start;

commit 07d777fe8c3985bc83428c2866713c2d1b3d4129
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Sep 22 14:01:55 2011 -0400

    tracing: Add percpu buffers for trace_printk()
    
    Currently, trace_printk() uses a single buffer to write into
    to calculate the size and format needed to save the trace. To
    do this safely in an SMP environment, a spin_lock() is taken
    to only allow one writer at a time to the buffer. But this could
    also affect what is being traced, and add synchronization that
    would not be there otherwise.
    
    Ideally, using percpu buffers would be useful, but since trace_printk()
    is only used in development, having per cpu buffers for something
    never used is a waste of space. Thus, the use of the trace_bprintk()
    format section is changed to be used for static fmts as well as dynamic ones.
    Then at boot up, we can check if the section that holds the trace_printk
    formats is non-empty, and if it does contain something, then we
    know a trace_printk() has been added to the kernel. At this time
    the trace_printk per cpu buffers are allocated. A check is also
    done at module load time in case a module is added that contains a
    trace_printk().
    
    Once the buffers are allocated, they are never freed. If you use
    a trace_printk() then you should know what you are doing.
    
    A buffer is made for each type of context:
    
      normal
      softirq
      irq
      nmi
    
    The context is checked and the appropriate buffer is used.
    This allows for totally lockless usage of trace_printk(),
    and they no longer even disable interrupts.
    
    Requested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 95059f091a24..f9d85504f04b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -826,6 +826,8 @@ extern struct list_head ftrace_events;
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 
+void trace_printk_init_buffers(void);
+
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\
 	extern struct ftrace_event_call					\

commit 6e48b550d1f5f1919e6500547ae14a73fbf66c7b
Author: Mark Brown <broonie@opensource.wolfsonmicro.com>
Date:   Fri Apr 13 09:52:59 2012 +0100

    tracing: Fix build breakage without CONFIG_PERF_EVENTS (again)
    
    Today's -next fails to link for me:
    
    kernel/built-in.o:(.data+0x178e50): undefined reference to `perf_ftrace_event_register'
    
    It looks like multiple fixes have been merged for the issue fixed by
    commit fa73dc9 (tracing: Fix build breakage without CONFIG_PERF_EVENTS)
    though I can't identify the other changes that have gone in at the
    minute, it's possible that the changes which caused the breakage fixed
    by the previous commit got dropped but the fix made it in.
    
    Link: http://lkml.kernel.org/r/1334307179-21255-1-git-send-email-broonie@opensource.wolfsonmicro.com
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 95059f091a24..f95d65da6db8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -836,11 +836,11 @@ extern const char *__stop___trace_bprintk_fmt[];
 		     filter)
 #include "trace_entries.h"
 
-#ifdef CONFIG_FUNCTION_TRACER
+#if defined(CONFIG_PERF_EVENTS) && defined(CONFIG_FUNCTION_TRACER)
 int perf_ftrace_event_register(struct ftrace_event_call *call,
 			       enum trace_reg type, void *data);
 #else
 #define perf_ftrace_event_register NULL
-#endif /* CONFIG_FUNCTION_TRACER */
+#endif
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit fa73dc9400516945bcbae8d98c23393bcefe1440
Author: Mark Brown <broonie@opensource.wolfsonmicro.com>
Date:   Tue Feb 28 11:02:46 2012 +0000

    tracing: Fix build breakage without CONFIG_PERF_EVENTS
    
    Today's -next fails to build for me:
    
      CC      kernel/trace/trace_export.o
    In file included from kernel/trace/trace_export.c:197: kernel/trace/trace_entries.h:58: error: 'perf_ftrace_event_register' undeclared here (not in a function)
    make[2]: *** [kernel/trace/trace_export.o] Error 1
    make[1]: *** [kernel/trace] Error 2
    make: *** [kernel] Error 2
    
    because as of ced390 (ftrace, perf: Add support to use function
    tracepoint in perf) perf_trace_event_register() is declared in trace.h
    only if CONFIG_PERF_EVENTS is enabled but I don't have that set.
    
    Ensure that we always have a definition of perf_trace_event_register()
    by making the definition unconditional.
    
    Link: http://lkml.kernel.org/r/1330426967-17067-1-git-send-email-broonie@opensource.wolfsonmicro.com
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Mark Brown <broonie@opensource.wolfsonmicro.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ce887c0eca56..95059f091a24 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -836,13 +836,11 @@ extern const char *__stop___trace_bprintk_fmt[];
 		     filter)
 #include "trace_entries.h"
 
-#ifdef CONFIG_PERF_EVENTS
 #ifdef CONFIG_FUNCTION_TRACER
 int perf_ftrace_event_register(struct ftrace_event_call *call,
 			       enum trace_reg type, void *data);
 #else
 #define perf_ftrace_event_register NULL
 #endif /* CONFIG_FUNCTION_TRACER */
-#endif /* CONFIG_PERF_EVENTS */
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 499e547057f5bba5cd6f87ebe59b05d0c59da905
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Feb 22 15:50:28 2012 -0500

    tracing/ring-buffer: Only have tracing_on disable tracing buffers
    
    As the ring-buffer code is being used by other facilities in the
    kernel, having tracing_on file disable *all* buffers is not a desired
    affect. It should only disable the ftrace buffers that are being used.
    
    Move the code into the trace.c file and use the buffer disabling
    for tracing_on() and tracing_off(). This way only the ftrace buffers
    will be affected by them and other kernel utilities will not be
    confused to why their output suddenly stopped.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 54faec790bc1..ce887c0eca56 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -154,6 +154,7 @@ struct trace_array {
 	struct ring_buffer	*buffer;
 	unsigned long		entries;
 	int			cpu;
+	int			buffer_disabled;
 	cycle_t			time_start;
 	struct task_struct	*waiter;
 	struct trace_array_cpu	*data[NR_CPUS];

commit 5500fa51199aee770ce53718853732600543619e
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:54 2012 +0100

    ftrace, perf: Add filter support for function trace event
    
    Adding support to filter function trace event via perf
    interface. It is now possible to use filter interface
    in the perf tool like:
    
      perf record -e ftrace:function --filter="(ip == mm_*)" ls
    
    The filter syntax is restricted to the the 'ip' field only,
    and following operators are accepted '==' '!=' '||', ending
    up with the filter strings like:
    
      ip == f1[, ]f2 ... || ip != f3[, ]f4 ...
    
    with comma ',' or space ' ' as a function separator. If the
    space ' ' is used as a separator, the right side of the
    assignment needs to be enclosed in double quotes '"', e.g.:
    
      perf record -e ftrace:function --filter '(ip == do_execve,sys_*,ext*)' ls
      perf record -e ftrace:function --filter '(ip == "do_execve,sys_*,ext*")' ls
      perf record -e ftrace:function --filter '(ip == "do_execve sys_* ext*")' ls
    
    The '==' operator adds trace filter with same effect as would
    be added via set_ftrace_filter file.
    
    The '!=' operator adds trace filter with same effect as would
    be added via set_ftrace_notrace file.
    
    The right side of the '!=', '==' operators is list of functions
    or regexp. to be added to filter separated by space.
    
    The '||' operator is used for connecting multiple filter definitions
    together. It is possible to have more than one '==' and '!='
    operators within one filter string.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-8-git-send-email-jolsa@redhat.com
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 29f93cd434a5..54faec790bc1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -776,9 +776,7 @@ struct filter_pred {
 	u64 			val;
 	struct regex		regex;
 	unsigned short		*ops;
-#ifdef CONFIG_FTRACE_STARTUP_TEST
 	struct ftrace_event_field *field;
-#endif
 	int 			offset;
 	int 			not;
 	int 			op;

commit 02aa3162edaa166a01d193f80ccde890be8b55da
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:53 2012 +0100

    ftrace: Allow to specify filter field type for ftrace events
    
    Adding FILTER_TRACE_FN event field type for function tracepoint
    event, so it can be properly recognized within filtering code.
    
    Currently all fields of ftrace subsystem events share the common
    field type FILTER_OTHER. Since the function trace fields need
    special care within the filtering code we need to recognize it
    properly, hence adding the FILTER_TRACE_FN event type.
    
    Adding filter parameter to the FTRACE_ENTRY macro, to specify the
    filter field type for the event.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-7-git-send-email-jolsa@redhat.com
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 76a1c5094bbf..29f93cd434a5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -56,21 +56,23 @@ enum trace_type {
 #define F_STRUCT(args...)		args
 
 #undef FTRACE_ENTRY
-#define FTRACE_ENTRY(name, struct_name, id, tstruct, print)	\
-	struct struct_name {					\
-		struct trace_entry	ent;			\
-		tstruct						\
+#define FTRACE_ENTRY(name, struct_name, id, tstruct, print, filter)	\
+	struct struct_name {						\
+		struct trace_entry	ent;				\
+		tstruct							\
 	}
 
 #undef TP_ARGS
 #define TP_ARGS(args...)	args
 
 #undef FTRACE_ENTRY_DUP
-#define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk)
+#define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk, filter)
 
 #undef FTRACE_ENTRY_REG
-#define FTRACE_ENTRY_REG(name, struct_name, id, tstruct, print, regfn) \
-	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print))
+#define FTRACE_ENTRY_REG(name, struct_name, id, tstruct, print,	\
+			 filter, regfn) \
+	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print), \
+		     filter)
 
 #include "trace_entries.h"
 
@@ -826,12 +828,13 @@ extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 
 #undef FTRACE_ENTRY
-#define FTRACE_ENTRY(call, struct_name, id, tstruct, print)		\
+#define FTRACE_ENTRY(call, struct_name, id, tstruct, print, filter)	\
 	extern struct ftrace_event_call					\
 	__attribute__((__aligned__(4))) event_##call;
 #undef FTRACE_ENTRY_DUP
-#define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print)		\
-	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))
+#define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print, filter)	\
+	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print), \
+		     filter)
 #include "trace_entries.h"
 
 #ifdef CONFIG_PERF_EVENTS

commit ced39002f5ea736b716ae233fb68b26d59783912
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:52 2012 +0100

    ftrace, perf: Add support to use function tracepoint in perf
    
    Adding perf registration support for the ftrace function event,
    so it is now possible to register it via perf interface.
    
    The perf_event struct statically contains ftrace_ops as a handle
    for function tracer. The function tracer is registered/unregistered
    in open/close actions.
    
    To be efficient, we enable/disable ftrace_ops each time the traced
    process is scheduled in/out (via TRACE_REG_PERF_(ADD|DELL) handlers).
    This way tracing is enabled only when the process is running.
    Intentionally using this way instead of the event's hw state
    PERF_HES_STOPPED, which would not disable the ftrace_ops.
    
    It is now possible to use function trace within perf commands
    like:
    
      perf record -e ftrace:function ls
      perf stat -e ftrace:function ls
    
    Allowed only for root.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-6-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 638476af8d7b..76a1c5094bbf 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -595,6 +595,8 @@ static inline int ftrace_trace_task(struct task_struct *task)
 static inline int ftrace_is_dead(void) { return 0; }
 #endif
 
+int ftrace_event_is_function(struct ftrace_event_call *call);
+
 /*
  * struct trace_parser - servers for reading the user input separated by spaces
  * @cont: set if the input is not complete - no final space char was found
@@ -832,4 +834,13 @@ extern const char *__stop___trace_bprintk_fmt[];
 	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))
 #include "trace_entries.h"
 
+#ifdef CONFIG_PERF_EVENTS
+#ifdef CONFIG_FUNCTION_TRACER
+int perf_ftrace_event_register(struct ftrace_event_call *call,
+			       enum trace_reg type, void *data);
+#else
+#define perf_ftrace_event_register NULL
+#endif /* CONFIG_FUNCTION_TRACER */
+#endif /* CONFIG_PERF_EVENTS */
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit e59a0bff3ecf389951e3c9378ddfd00f6448bfaa
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:51 2012 +0100

    ftrace: Add FTRACE_ENTRY_REG macro to allow event registration
    
    Adding FTRACE_ENTRY_REG macro so particular ftrace entries
    could specify registration function and thus become accesible
    via perf.
    
    This will be used in upcomming patch for function trace.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-5-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 55c6ea00f28a..638476af8d7b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -68,6 +68,10 @@ enum trace_type {
 #undef FTRACE_ENTRY_DUP
 #define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk)
 
+#undef FTRACE_ENTRY_REG
+#define FTRACE_ENTRY_REG(name, struct_name, id, tstruct, print, regfn) \
+	FTRACE_ENTRY(name, struct_name, id, PARAMS(tstruct), PARAMS(print))
+
 #include "trace_entries.h"
 
 /*

commit e248491ac283b516958ca9ab62c8e74b6718bca8
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:48 2012 +0100

    ftrace: Add enable/disable ftrace_ops control interface
    
    Adding a way to temporarily enable/disable ftrace_ops. The change
    follows the same way as 'global' ftrace_ops are done.
    
    Introducing 2 global ftrace_ops - control_ops and ftrace_control_list
    which take over all ftrace_ops registered with FTRACE_OPS_FL_CONTROL
    flag. In addition new per cpu flag called 'disabled' is also added to
    ftrace_ops to provide the control information for each cpu.
    
    When ftrace_ops with FTRACE_OPS_FL_CONTROL is registered, it is
    set as disabled for all cpus.
    
    The ftrace_control_list contains all the registered 'control' ftrace_ops.
    The control_ops provides function which iterates ftrace_control_list
    and does the check for 'disabled' flag on current cpu.
    
    Adding 3 inline functions:
      ftrace_function_local_disable/ftrace_function_local_enable
      - enable/disable the ftrace_ops on current cpu
      ftrace_function_local_disabled
      - get disabled ftrace_ops::disabled value for current cpu
    
    Link: http://lkml.kernel.org/r/1329317514-8131-2-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b93ecbadad6d..55c6ea00f28a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -288,6 +288,8 @@ struct tracer {
 /* for function tracing recursion */
 #define TRACE_INTERNAL_BIT		(1<<11)
 #define TRACE_GLOBAL_BIT		(1<<12)
+#define TRACE_CONTROL_BIT		(1<<13)
+
 /*
  * Abuse of the trace_recursion.
  * As we need a way to maintain state if we are tracing the function

commit 972b2c719990f91eb3b2310d44ef8a2d38955a14
Merge: 02550d61f492 c3aa077648e1
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Jan 8 12:19:57 2012 -0800

    Merge branch 'for-linus2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    * 'for-linus2' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs: (165 commits)
      reiserfs: Properly display mount options in /proc/mounts
      vfs: prevent remount read-only if pending removes
      vfs: count unlinked inodes
      vfs: protect remounting superblock read-only
      vfs: keep list of mounts for each superblock
      vfs: switch ->show_options() to struct dentry *
      vfs: switch ->show_path() to struct dentry *
      vfs: switch ->show_devname() to struct dentry *
      vfs: switch ->show_stats to struct dentry *
      switch security_path_chmod() to struct path *
      vfs: prefer ->dentry->d_sb to ->mnt->mnt_sb
      vfs: trim includes a bit
      switch mnt_namespace ->root to struct mount
      vfs: take /proc/*/mounts and friends to fs/proc_namespace.c
      vfs: opencode mntget() mnt_set_mountpoint()
      vfs: spread struct mount - remaining argument of next_mnt()
      vfs: move fsnotify junk to struct mount
      vfs: move mnt_devname
      vfs: move mnt_list to struct mount
      vfs: switch pnode.h macros to struct mount *
      ...

commit f4ae40a6a50a98ac23d4b285f739455e926a473e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Jul 24 04:33:43 2011 -0400

    switch debugfs to umode_t
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 092e1f8d18dc..0154c0b850de 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -312,7 +312,7 @@ void tracing_reset_current(int cpu);
 void tracing_reset_current_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *trace_create_file(const char *name,
-				 mode_t mode,
+				 umode_t mode,
 				 struct dentry *parent,
 				 void *data,
 				 const struct file_operations *fops);

commit 77271ce4b2c0df0a76ad1cbb6a95b07e1f88c1ea
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Nov 17 09:34:33 2011 -0500

    tracing: Add irq, preempt-count and need resched info to default trace output
    
    People keep asking how to get the preempt count, irq, and need resched info
    and we keep telling them to enable the latency format. Some developers think
    that traces without this info is completely useless, and for a lot of tasks
    it is useless.
    
    The first option was to enable the latency trace as the default format, but
    the header for the latency format is pretty useless for most tracers and
    it also does the timestamp in straight microseconds from the time the trace
    started. This is sometimes more difficult to read as the default trace is
    seconds from the start of boot up.
    
    Latency format:
    
     # tracer: nop
     #
     # nop latency trace v1.1.5 on 3.2.0-rc1-test+
     # --------------------------------------------------------------------
     # latency: 0 us, #159771/64234230, CPU#1 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:4)
     #    -----------------
     #    | task: -0 (uid:0 nice:0 policy:0 rt_prio:0)
     #    -----------------
     #
     #                  _------=> CPU#
     #                 / _-----=> irqs-off
     #                | / _----=> need-resched
     #                || / _---=> hardirq/softirq
     #                ||| / _--=> preempt-depth
     #                |||| /     delay
     #  cmd     pid   ||||| time  |   caller
     #     \   /      |||||  \    |   /
     migratio-6       0...2 41778231us+: rcu_note_context_switch <-__schedule
     migratio-6       0...2 41778233us : trace_rcu_utilization <-rcu_note_context_switch
     migratio-6       0...2 41778235us+: rcu_sched_qs <-rcu_note_context_switch
     migratio-6       0d..2 41778236us+: rcu_preempt_qs <-rcu_note_context_switch
     migratio-6       0...2 41778238us : trace_rcu_utilization <-rcu_note_context_switch
     migratio-6       0...2 41778239us+: debug_lockdep_rcu_enabled <-__schedule
    
    default format:
    
     # tracer: nop
     #
     #           TASK-PID    CPU#    TIMESTAMP  FUNCTION
     #              | |       |          |         |
          migration/0-6     [000]    50.025810: rcu_note_context_switch <-__schedule
          migration/0-6     [000]    50.025812: trace_rcu_utilization <-rcu_note_context_switch
          migration/0-6     [000]    50.025813: rcu_sched_qs <-rcu_note_context_switch
          migration/0-6     [000]    50.025815: rcu_preempt_qs <-rcu_note_context_switch
          migration/0-6     [000]    50.025817: trace_rcu_utilization <-rcu_note_context_switch
          migration/0-6     [000]    50.025818: debug_lockdep_rcu_enabled <-__schedule
          migration/0-6     [000]    50.025820: debug_lockdep_rcu_enabled <-__schedule
    
    The latency format header has latency information that is pretty meaningless
    for most tracers. Although some of the header is useful, and we can add that
    later to the default format as well.
    
    What is really useful with the latency format is the irqs-off, need-resched
    hard/softirq context and the preempt count.
    
    This commit adds the option irq-info which is on by default that adds this
    information:
    
     # tracer: nop
     #
     #                              _-----=> irqs-off
     #                             / _----=> need-resched
     #                            | / _---=> hardirq/softirq
     #                            || / _--=> preempt-depth
     #                            ||| /     delay
     #           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION
     #              | |       |   ||||       |         |
               <idle>-0     [000] d..2    49.309305: cpuidle_get_driver <-cpuidle_idle_call
               <idle>-0     [000] d..2    49.309307: mwait_idle <-cpu_idle
               <idle>-0     [000] d..2    49.309309: need_resched <-mwait_idle
               <idle>-0     [000] d..2    49.309310: test_ti_thread_flag <-need_resched
               <idle>-0     [000] d..2    49.309312: trace_power_start.constprop.13 <-mwait_idle
               <idle>-0     [000] d..2    49.309313: trace_cpu_idle <-mwait_idle
               <idle>-0     [000] d..2    49.309315: need_resched <-mwait_idle
    
    If a user wants the old format, they can disable the 'irq-info' option:
    
     # tracer: nop
     #
     #           TASK-PID   CPU#      TIMESTAMP  FUNCTION
     #              | |       |          |         |
               <idle>-0     [000]     49.309305: cpuidle_get_driver <-cpuidle_idle_call
               <idle>-0     [000]     49.309307: mwait_idle <-cpu_idle
               <idle>-0     [000]     49.309309: need_resched <-mwait_idle
               <idle>-0     [000]     49.309310: test_ti_thread_flag <-need_resched
               <idle>-0     [000]     49.309312: trace_power_start.constprop.13 <-mwait_idle
               <idle>-0     [000]     49.309313: trace_cpu_idle <-mwait_idle
               <idle>-0     [000]     49.309315: need_resched <-mwait_idle
    
    Requested-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f8ec2291b522..2c2657462ac3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -655,6 +655,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_RECORD_CMD		= 0x100000,
 	TRACE_ITER_OVERWRITE		= 0x200000,
 	TRACE_ITER_STOP_ON_FREE		= 0x400000,
+	TRACE_ITER_IRQ_INFO		= 0x800000,
 };
 
 /*

commit 7e9a49ef542610609144d1afcd516dc3fafac4d6
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Mon Nov 7 16:08:49 2011 +0100

    tracing/latency: Fix header output for latency tracers
    
    In case the the graph tracer (CONFIG_FUNCTION_GRAPH_TRACER) or even the
    function tracer (CONFIG_FUNCTION_TRACER) are not set, the latency tracers
    do not display proper latency header.
    
    The involved/fixed latency tracers are:
            wakeup_rt
            wakeup
            preemptirqsoff
            preemptoff
            irqsoff
    
    The patch adds proper handling of tracer configuration options for latency
    tracers, and displaying correct header info accordingly.
    
    * The current output (for wakeup tracer) with both graph and function
      tracers disabled is:
    
      # tracer: wakeup
      #
        <idle>-0       0d.h5    1us+:      0:120:R   + [000]     7:  0:R watchdog/0
        <idle>-0       0d.h5    3us+: ttwu_do_activate.clone.1 <-try_to_wake_up
        ...
    
    * The fixed output is:
    
      # tracer: wakeup
      #
      # wakeup latency trace v1.1.5 on 3.1.0-tip+
      # --------------------------------------------------------------------
      # latency: 55 us, #4/4, CPU#0 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:2)
      #    -----------------
      #    | task: migration/0-6 (uid:0 nice:0 policy:1 rt_prio:99)
      #    -----------------
      #
      #                  _------=> CPU#
      #                 / _-----=> irqs-off
      #                | / _----=> need-resched
      #                || / _---=> hardirq/softirq
      #                ||| / _--=> preempt-depth
      #                |||| /     delay
      #  cmd     pid   ||||| time  |   caller
      #     \   /      |||||  \    |   /
           cat-1129    0d..4    1us :   1129:120:R   + [000]     6:  0:R migration/0
           cat-1129    0d..4    2us+: ttwu_do_activate.clone.1 <-try_to_wake_up
    
    * The current output (for wakeup tracer) with only function
      tracer enabled is:
    
      # tracer: wakeup
      #
           cat-1140    0d..4    1us+:   1140:120:R   + [000]     6:  0:R migration/0
           cat-1140    0d..4    2us : ttwu_do_activate.clone.1 <-try_to_wake_up
    
    * The fixed output is:
      # tracer: wakeup
      #
      # wakeup latency trace v1.1.5 on 3.1.0-tip+
      # --------------------------------------------------------------------
      # latency: 207 us, #109/109, CPU#1 | (M:preempt VP:0, KP:0, SP:0 HP:0 #P:2)
      #    -----------------
      #    | task: watchdog/1-12 (uid:0 nice:0 policy:1 rt_prio:99)
      #    -----------------
      #
      #                  _------=> CPU#
      #                 / _-----=> irqs-off
      #                | / _----=> need-resched
      #                || / _---=> hardirq/softirq
      #                ||| / _--=> preempt-depth
      #                |||| /     delay
      #  cmd     pid   ||||| time  |   caller
      #     \   /      |||||  \    |   /
        <idle>-0       1d.h5    1us+:      0:120:R   + [001]    12:  0:R watchdog/1
        <idle>-0       1d.h5    3us : ttwu_do_activate.clone.1 <-try_to_wake_up
    
    Link: http://lkml.kernel.org/r/20111107150849.GE1807@m.brq.redhat.com
    
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 092e1f8d18dc..f8ec2291b522 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -370,6 +370,7 @@ void trace_graph_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
 		    unsigned long flags, int pc);
+void trace_latency_header(struct seq_file *m);
 void trace_default_header(struct seq_file *m);
 void print_trace_header(struct seq_file *m, struct trace_iterator *iter);
 int trace_empty(struct trace_iterator *iter);

commit e0a413f619ef8bc366dafc6f8221674993b8d85f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Sep 29 21:26:16 2011 -0400

    tracing: Warn on output if the function tracer was found corrupted
    
    As the function tracer is very intrusive, lots of self checks are
    performed on the tracer and if something is found to be strange
    it will shut itself down keeping it from corrupting the rest of the
    kernel. This shutdown may still allow functions to be traced, as the
    tracing only stops new modifications from happening. Trying to stop
    the function tracer itself can cause more harm as it requires code
    modification.
    
    Although a WARN_ON() is executed, a user may not notice it. To help
    the user see that something isn't right with the tracing of the system
    a big warning is added to the output of the tracer that lets the user
    know that their data may be incomplete.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4c7540ad5dcb..092e1f8d18dc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -579,11 +579,13 @@ static inline int ftrace_trace_task(struct task_struct *task)
 
 	return test_tsk_trace_trace(task);
 }
+extern int ftrace_is_dead(void);
 #else
 static inline int ftrace_trace_task(struct task_struct *task)
 {
 	return 1;
 }
+static inline int ftrace_is_dead(void) { return 0; }
 #endif
 
 /*

commit 1d0e78e380cd2802aa603a50e08220dfc681141c
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Thu Aug 11 16:25:54 2011 +0200

    tracing/filter: Add startup tests for events filter
    
    Adding automated tests running as late_initcall. Tests are
    compiled in with CONFIG_FTRACE_STARTUP_TEST option.
    
    Adding test event "ftrace_test_filter" used to simulate
    filter processing during event occurance.
    
    String filters are compiled and tested against several
    test events with different values.
    
    Also testing that evaluation of explicit predicates is ommited
    due to the lazy filter evaluation.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/1313072754-4620-11-git-send-email-jolsa@redhat.com
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2eb3cf6d37bc..4c7540ad5dcb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -762,6 +762,9 @@ struct filter_pred {
 	u64 			val;
 	struct regex		regex;
 	unsigned short		*ops;
+#ifdef CONFIG_FTRACE_STARTUP_TEST
+	struct ftrace_event_field *field;
+#endif
 	int 			offset;
 	int 			not;
 	int 			op;

commit 61aaef55300088e12d7f853adeea65d1aa1562db
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Thu Aug 11 16:25:47 2011 +0200

    tracing/filter: Remove field_name from filter_pred struct
    
    The field_name was used just for finding event's fields. This way we
    don't need to care about field_name allocation/free.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Link: http://lkml.kernel.org/r/1313072754-4620-4-git-send-email-jolsa@redhat.com
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 616846bcfee5..2eb3cf6d37bc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -761,16 +761,7 @@ struct filter_pred {
 	filter_pred_fn_t 	fn;
 	u64 			val;
 	struct regex		regex;
-	/*
-	 * Leaf nodes use field_name, ops is used by AND and OR
-	 * nodes. The field_name is always freed when freeing a pred.
-	 * We can overload field_name for ops and have it freed
-	 * as well.
-	 */
-	union {
-		char		*field_name;
-		unsigned short	*ops;
-	};
+	unsigned short		*ops;
 	int 			offset;
 	int 			not;
 	int 			op;

commit 60063497a95e716c9a689af3be2687d261f115b4
Author: Arun Sharma <asharma@fb.com>
Date:   Tue Jul 26 16:09:06 2011 -0700

    atomic: use <linux/atomic.h>
    
    This allows us to move duplicated code in <asm/atomic.h>
    (atomic_inc_not_zero() for now) to <linux/atomic.h>
    
    Signed-off-by: Arun Sharma <asharma@fb.com>
    Reviewed-by: Eric Dumazet <eric.dumazet@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Eric Dumazet <eric.dumazet@gmail.com>
    Acked-by: Mike Frysinger <vapier@gentoo.org>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3f381d0b20a8..616846bcfee5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -2,7 +2,7 @@
 #define _LINUX_KERNEL_TRACE_H
 
 #include <linux/fs.h>
-#include <asm/atomic.h>
+#include <linux/atomic.h>
 #include <linux/sched.h>
 #include <linux/clocksource.h>
 #include <linux/ring_buffer.h>

commit 40bcea7bbe8fe452a2d272e2ffd3dea281eec9ff
Merge: 492f73a303b4 14a8fd7ceea6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 21 09:32:40 2011 +0200

    Merge branch 'tip/perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into perf/core

commit 492f73a303b488ffd67097b2351d54aa6e6c7c73
Merge: e08fbb78f03f f7bc8b61f657
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 21 09:29:14 2011 +0200

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: pick up the latest fixes - they won't make v3.0.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e4a3f541f0b67fdad98b326c851dfe7f4b6b6dad
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 14 19:02:29 2011 -0400

    tracing: Still trace filtered irq functions when irq trace is disabled
    
    If a function is set to be traced by the set_graph_function, but the
    option funcgraph-irqs is zero, and the traced function happens to be
    called from a interrupt, it will not be traced.
    
    The point of funcgraph-irqs is to not trace interrupts when we are
    preempted by an irq, not to not trace functions we want to trace that
    happen to be *in* a irq.
    
    Luckily the current->trace_recursion element is perfect to add a flag
    to help us be able to trace functions within an interrupt even when
    we are not tracing interrupts that preempt the trace.
    
    Reported-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Tested-by: Heiko Carstens <heiko.carstens@de.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a3e2db708072..651f35be372a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -278,6 +278,29 @@ struct tracer {
 };
 
 
+/* Only current can touch trace_recursion */
+#define trace_recursion_inc() do { (current)->trace_recursion++; } while (0)
+#define trace_recursion_dec() do { (current)->trace_recursion--; } while (0)
+
+/* Ring buffer has the 10 LSB bits to count */
+#define trace_recursion_buffer() ((current)->trace_recursion & 0x3ff)
+
+/* for function tracing recursion */
+#define TRACE_INTERNAL_BIT		(1<<11)
+#define TRACE_GLOBAL_BIT		(1<<12)
+/*
+ * Abuse of the trace_recursion.
+ * As we need a way to maintain state if we are tracing the function
+ * graph in irq because we want to trace a particular function that
+ * was called in irq context but we have irq tracing off. Since this
+ * can only be modified by current, we can reuse trace_recursion.
+ */
+#define TRACE_IRQ_BIT			(1<<13)
+
+#define trace_recursion_set(bit)	do { (current)->trace_recursion |= (bit); } while (0)
+#define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(bit); } while (0)
+#define trace_recursion_test(bit)	((current)->trace_recursion & (bit))
+
 #define TRACE_PIPE_ALL_CPU	-1
 
 int tracer_init(struct tracer *t, struct trace_array *tr);
@@ -516,8 +539,18 @@ static inline int ftrace_graph_addr(unsigned long addr)
 		return 1;
 
 	for (i = 0; i < ftrace_graph_count; i++) {
-		if (addr == ftrace_graph_funcs[i])
+		if (addr == ftrace_graph_funcs[i]) {
+			/*
+			 * If no irqs are to be traced, but a set_graph_function
+			 * is set, and called by an interrupt handler, we still
+			 * want to trace it.
+			 */
+			if (in_irq())
+				trace_recursion_set(TRACE_IRQ_BIT);
+			else
+				trace_recursion_clear(TRACE_IRQ_BIT);
 			return 1;
+		}
 	}
 
 	return 0;
@@ -794,19 +827,4 @@ extern const char *__stop___trace_bprintk_fmt[];
 	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))
 #include "trace_entries.h"
 
-/* Only current can touch trace_recursion */
-#define trace_recursion_inc() do { (current)->trace_recursion++; } while (0)
-#define trace_recursion_dec() do { (current)->trace_recursion--; } while (0)
-
-/* Ring buffer has the 10 LSB bits to count */
-#define trace_recursion_buffer() ((current)->trace_recursion & 0x3ff)
-
-/* for function tracing recursion */
-#define TRACE_INTERNAL_BIT		(1<<11)
-#define TRACE_GLOBAL_BIT		(1<<12)
-
-#define trace_recursion_set(bit)	do { (current)->trace_recursion |= (bit); } while (0)
-#define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(bit); } while (0)
-#define trace_recursion_test(bit)	((current)->trace_recursion & (bit))
-
 #endif /* _LINUX_KERNEL_TRACE_H */

commit e9dbfae53eeb9fc3d4bb7da3df87fa9875f5da02
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jul 5 11:36:06 2011 -0400

    tracing: Fix bug when reading system filters on module removal
    
    The event system is freed when its nr_events is set to zero. This happens
    when a module created an event system and then later the module is
    removed. Modules may share systems, so the system is allocated when
    it is created and freed when the modules are unloaded and all the
    events under the system are removed (nr_events set to zero).
    
    The problem arises when a task opened the "filter" file for the
    system. If the module is unloaded and it removed the last event for
    that system, the system structure is freed. If the task that opened
    the filter file accesses the "filter" file after the system has
    been freed, the system will access an invalid pointer.
    
    By adding a ref_count, and using it to keep track of what
    is using the event system, we can free it after all users
    are finished with the event system.
    
    Cc: <stable@kernel.org>
    Reported-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 229f8591f61d..f8074072d111 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -677,6 +677,7 @@ struct event_subsystem {
 	struct dentry		*entry;
 	struct event_filter	*filter;
 	int			nr_events;
+	int			ref_count;
 };
 
 #define FILTER_PRED_INVALID	((unsigned short)-1)

commit 1fd8df2c3970c9e7e4e262354154ee39e58bdd7c
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Wed Jun 8 16:09:34 2011 +0900

    tracing/kprobes: Fix kprobe-tracer to support stack trace
    
    Fix to support kernel stack trace correctly on kprobe-tracer.
    Since the execution path of kprobe-based dynamic events is different
    from other tracepoint-based events, normal ftrace_trace_stack() doesn't
    work correctly. To fix that, this introduces ftrace_trace_stack_regs()
    which traces stack via pt_regs instead of current stack register.
    
    e.g.
    
     # echo p schedule+4 > /sys/kernel/debug/tracing/kprobe_events
     # echo 1 > /sys/kernel/debug/tracing/options/stacktrace
     # echo 1 > /sys/kernel/debug/tracing/events/kprobes/enable
     # head -n 20 /sys/kernel/debug/tracing/trace
                bash-2968  [000] 10297.050245: p_schedule_4: (schedule+0x4/0x4ca)
                bash-2968  [000] 10297.050247: <stack trace>
     => schedule_timeout
     => n_tty_read
     => tty_read
     => vfs_read
     => sys_read
     => system_call_fastpath
         kworker/0:1-2940  [000] 10297.050265: p_schedule_4: (schedule+0x4/0x4ca)
         kworker/0:1-2940  [000] 10297.050266: <stack trace>
     => worker_thread
     => kthread
     => kernel_thread_helper
                sshd-1132  [000] 10297.050365: p_schedule_4: (schedule+0x4/0x4ca)
                sshd-1132  [000] 10297.050365: <stack trace>
     => sysret_careful
    
    Note: Even with this fix, the first entry will be skipped
    if the probe is put on the function entry area before
    the frame pointer is set up (usually, that is 4 bytes
     (push %bp; mov %sp %bp) on x86), because stack unwinder
    depends on the frame pointer.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: yrl.pp-manager.tt@hitachi.com
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Namhyung Kim <namhyung@gmail.com>
    Link: http://lkml.kernel.org/r/20110608070934.17777.17116.stgit@fedora15
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 742f545ae185..a3e2db708072 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -389,6 +389,9 @@ void update_max_tr_single(struct trace_array *tr,
 void ftrace_trace_stack(struct ring_buffer *buffer, unsigned long flags,
 			int skip, int pc);
 
+void ftrace_trace_stack_regs(struct ring_buffer *buffer, unsigned long flags,
+			     int skip, int pc, struct pt_regs *regs);
+
 void ftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags,
 			    int pc);
 
@@ -400,6 +403,12 @@ static inline void ftrace_trace_stack(struct ring_buffer *buffer,
 {
 }
 
+static inline void ftrace_trace_stack_regs(struct ring_buffer *buffer,
+					   unsigned long flags, int skip,
+					   int pc, struct pt_regs *regs)
+{
+}
+
 static inline void ftrace_trace_userstack(struct ring_buffer *buffer,
 					  unsigned long flags, int pc)
 {

commit cf30cf67d6c7592c670ec946d89fc15ee0deb0eb
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 14 22:44:07 2011 -0400

    tracing: Add disable_on_free option
    
    Add a trace option to disable tracing on free. When this option is
    set, a write into the free_buffer file will not only shrink the
    ring buffer down to zero, but it will also disable tracing.
    
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 229f8591f61d..742f545ae185 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -609,6 +609,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_GRAPH_TIME		= 0x80000,
 	TRACE_ITER_RECORD_CMD		= 0x100000,
 	TRACE_ITER_OVERWRITE		= 0x200000,
+	TRACE_ITER_STOP_ON_FREE		= 0x400000,
 };
 
 /*

commit b1cff0ad1062621ae63cb6c5dc4165191fe2e9f1
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed May 25 14:27:43 2011 -0400

    ftrace: Add internal recursive checks
    
    Witold reported a reboot caused by the selftests of the dynamic function
    tracer. He sent me a config and I used ktest to do a config_bisect on it
    (as my config did not cause the crash). It pointed out that the problem
    config was CONFIG_PROVE_RCU.
    
    What happened was that if multiple callbacks are attached to the
    function tracer, we iterate a list of callbacks. Because the list is
    managed by synchronize_sched() and preempt_disable, the access to the
    pointers uses rcu_dereference_raw().
    
    When PROVE_RCU is enabled, the rcu_dereference_raw() calls some
    debugging functions, which happen to be traced. The tracing of the debug
    function would then call rcu_dereference_raw() which would then call the
    debug function and then... well you get the idea.
    
    I first wrote two different patches to solve this bug.
    
    1) add a __rcu_dereference_raw() that would not do any checks.
    2) add notrace to the offending debug functions.
    
    Both of these patches worked.
    
    Talking with Paul McKenney on IRC, he suggested to add recursion
    detection instead. This seemed to be a better solution, so I decided to
    implement it. As the task_struct already has a trace_recursion to detect
    recursion in the ring buffer, and that has a very small number it
    allows, I decided to use that same variable to add flags that can detect
    the recursion inside the infrastructure of the function tracer.
    
    I plan to change it so that the task struct bit can be checked in
    mcount, but as that requires changes to all archs, I will hold that off
    to the next merge window.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Link: http://lkml.kernel.org/r/1306348063.1465.116.camel@gandalf.stny.rr.com
    Reported-by: Witold Baryluk <baryluk@smp.if.uj.edu.pl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6b69c4bd306f..229f8591f61d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -784,4 +784,19 @@ extern const char *__stop___trace_bprintk_fmt[];
 	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))
 #include "trace_entries.h"
 
+/* Only current can touch trace_recursion */
+#define trace_recursion_inc() do { (current)->trace_recursion++; } while (0)
+#define trace_recursion_dec() do { (current)->trace_recursion--; } while (0)
+
+/* Ring buffer has the 10 LSB bits to count */
+#define trace_recursion_buffer() ((current)->trace_recursion & 0x3ff)
+
+/* for function tracing recursion */
+#define TRACE_INTERNAL_BIT		(1<<11)
+#define TRACE_GLOBAL_BIT		(1<<12)
+
+#define trace_recursion_set(bit)	do { (current)->trace_recursion |= (bit); } while (0)
+#define trace_recursion_clear(bit)	do { (current)->trace_recursion &= ~(bit); } while (0)
+#define trace_recursion_test(bit)	((current)->trace_recursion & (bit))
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 95950c2ecb31314ef827428e43ff771cf3b037e5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 6 00:08:51 2011 -0400

    ftrace: Add self-tests for multiple function trace users
    
    Add some basic sanity tests for multiple users of the function
    tracer at startup.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5e9dfc6286dd..6b69c4bd306f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -419,6 +419,8 @@ extern void trace_find_cmdline(int pid, char comm[]);
 extern unsigned long ftrace_update_tot_cnt;
 #define DYN_FTRACE_TEST_NAME trace_selftest_dynamic_test_func
 extern int DYN_FTRACE_TEST_NAME(void);
+#define DYN_FTRACE_TEST_NAME2 trace_selftest_dynamic_test_func2
+extern int DYN_FTRACE_TEST_NAME2(void);
 #endif
 
 extern int ring_buffer_expanded;

commit 9a24470b2826e4665b1484836c7ae6aba1ddea32
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Mar 9 14:53:38 2011 -0500

    tracing: Align 4 byte ints together in struct tracer
    
    Move elements in struct tracer for better alignment.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 951d0b7e7062..5e9dfc6286dd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -272,8 +272,8 @@ struct tracer {
 	/* If you handled the flag setting, return 0 */
 	int			(*set_flag)(u32 old_flags, u32 bit, int set);
 	struct tracer		*next;
-	int			print_max;
 	struct tracer_flags	*flags;
+	int			print_max;
 	int			use_max_tr;
 };
 

commit 750912fa366312e9c5bc83eab352898a26750401
Author: David Sharp <dhsharp@google.com>
Date:   Wed Dec 8 13:46:47 2010 -0800

    tracing: Add an 'overwrite' trace_option.
    
    Add an "overwrite" trace_option for ftrace to control whether the buffer should
    be overwritten on overflow or not. The default remains to overwrite old events
    when the buffer is full. This patch adds the option to instead discard newest
    events when the buffer is full. This is useful to get a snapshot of traces just
    after enabling traces. Dropping the current event is also a simpler code path.
    
    Signed-off-by: David Sharp <dhsharp@google.com>
    LKML-Reference: <1291844807-15481-1-git-send-email-dhsharp@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 856e73cc1d3f..951d0b7e7062 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -606,6 +606,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_SLEEP_TIME		= 0x40000,
 	TRACE_ITER_GRAPH_TIME		= 0x80000,
 	TRACE_ITER_RECORD_CMD		= 0x100000,
+	TRACE_ITER_OVERWRITE		= 0x200000,
 };
 
 /*

commit bf93f9ed3a2cb89eb7e58851139d3be375b98027
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 23:21:34 2011 -0500

    tracing/filter: Increase the max preds to 2^14
    
    Now that the filter logic does not require to save the pred results
    on the stack, we can increase the max number of preds we allow.
    As the preds are index by a short value, and we use the MSBs as flags
    we can increase the max preds to 2^14 (16384) which should be way
    more than enough.
    
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fbff872f8db1..856e73cc1d3f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -680,7 +680,14 @@ struct event_subsystem {
 #define FILTER_PRED_IS_RIGHT	(1 << 15)
 #define FILTER_PRED_FOLD	(1 << 15)
 
-#define MAX_FILTER_PRED		32
+/*
+ * The max preds is the size of unsigned short with
+ * two flags at the MSBs. One bit is used for both the IS_RIGHT
+ * and FOLD flags. The other is reserved.
+ *
+ * 2^14 preds is way more than enough.
+ */
+#define MAX_FILTER_PRED		16384
 
 struct filter_pred;
 struct regex;

commit 4a3d27e98a7f2682e96d6f863752e0424b00d691
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 23:19:49 2011 -0500

    tracing/filter: Move MAX_FILTER_PRED to local tracing directory
    
    The MAX_FILTER_PRED is only needed by the kernel/trace/*.c files.
    Move it to kernel/trace/trace.h.
    
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d754330934bb..fbff872f8db1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -680,6 +680,8 @@ struct event_subsystem {
 #define FILTER_PRED_IS_RIGHT	(1 << 15)
 #define FILTER_PRED_FOLD	(1 << 15)
 
+#define MAX_FILTER_PRED		32
+
 struct filter_pred;
 struct regex;
 

commit 43cd414552d8137157e926e46361678ea867e476
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 23:16:51 2011 -0500

    tracing/filter: Optimize filter by folding the tree
    
    There are many cases that a filter will contain multiple ORs or
    ANDs together near the leafs. Walking up and down the tree to get
    to the next compare can be a waste.
    
    If there are several ORs or ANDs together, fold them into a single
    pred and allocate an array of the conditions that they check.
    This will speed up the filter by linearly walking an array
    and can still break out if a short circuit condition is met.
    
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index bba34a72c780..d754330934bb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -678,6 +678,7 @@ struct event_subsystem {
 
 #define FILTER_PRED_INVALID	((unsigned short)-1)
 #define FILTER_PRED_IS_RIGHT	(1 << 15)
+#define FILTER_PRED_FOLD	(1 << 15)
 
 struct filter_pred;
 struct regex;
@@ -704,7 +705,16 @@ struct filter_pred {
 	filter_pred_fn_t 	fn;
 	u64 			val;
 	struct regex		regex;
-	char 			*field_name;
+	/*
+	 * Leaf nodes use field_name, ops is used by AND and OR
+	 * nodes. The field_name is always freed when freeing a pred.
+	 * We can overload field_name for ops and have it freed
+	 * as well.
+	 */
+	union {
+		char		*field_name;
+		unsigned short	*ops;
+	};
 	int 			offset;
 	int 			not;
 	int 			op;

commit 61e9dea20e1ada886cc49a9ec6fe3c6ac0de7324
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 22:54:33 2011 -0500

    tracing/filter: Use a tree instead of stack for filter_match_preds()
    
    Currently the filter_match_preds() requires a stack to push
    and pop the preds to determine if the filter matches the record or not.
    This has two drawbacks:
    
    1) It requires a stack to store state information. As this is done
       in fast paths we can't allocate the storage for this stack, and
       we can't use a global as it must be re-entrant. The stack is stored
       on the kernel stack and this greatly limits how many preds we
       may allow.
    
    2) All conditions are calculated even when a short circuit exists.
       a || b  will always calculate a and b even though a was determined
       to be true.
    
    Using a tree we can walk a constant structure that will save
    the state as we go. The algorithm is simply:
    
      pred = root;
      do {
            switch (move) {
            case MOVE_DOWN:
                    if (OR or AND) {
                            pred = left;
                            continue;
                    }
                    if (pred == root)
                            break;
                    match = pred->fn();
                    pred = pred->parent;
                    move = left child ? MOVE_UP_FROM_LEFT : MOVE_UP_FROM_RIGHT;
                    continue;
    
            case MOVE_UP_FROM_LEFT:
                    /* Only OR or AND can be a parent */
                    if (match && OR || !match && AND) {
                            /* short circuit */
                            if (pred == root)
                                    break;
                            pred = pred->parent;
                            move = left child ?
                                    MOVE_UP_FROM_LEFT :
                                    MOVE_UP_FROM_RIGHT;
                            continue;
                    }
                    pred = pred->right;
                    move = MOVE_DOWN;
                    continue;
    
            case MOVE_UP_FROM_RIGHT:
                    if (pred == root)
                            break;
                    pred = pred->parent;
                    move = left child ? MOVE_UP_FROM_LEFT : MOVE_UP_FROM_RIGHT;
                    continue;
            }
            done = 1;
      } while (!done);
    
    This way there's no strict limit to how many preds we allow
    and it also will short circuit the logical operations when possible.
    
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 254d04a84ec3..bba34a72c780 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -664,6 +664,7 @@ struct event_filter {
 	int			n_preds;	/* Number assigned */
 	int			a_preds;	/* allocated */
 	struct filter_pred	*preds;
+	struct filter_pred	*root;
 	char			*filter_string;
 };
 
@@ -675,6 +676,9 @@ struct event_subsystem {
 	int			nr_events;
 };
 
+#define FILTER_PRED_INVALID	((unsigned short)-1)
+#define FILTER_PRED_IS_RIGHT	(1 << 15)
+
 struct filter_pred;
 struct regex;
 
@@ -704,7 +708,10 @@ struct filter_pred {
 	int 			offset;
 	int 			not;
 	int 			op;
-	int 			pop_n;
+	unsigned short		index;
+	unsigned short		parent;
+	unsigned short		left;
+	unsigned short		right;
 };
 
 extern struct list_head ftrace_common_fields;

commit 74e9e58c350a24139e268dd6857bbaa55c5aafcf
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 22:49:48 2011 -0500

    tracing/filter: Allocate the preds in an array
    
    Currently we allocate an array of pointers to filter_preds, and then
    allocate a separate filter_pred for each item in the array.
    This adds slight overhead in the filters as it needs to derefernce
    twice to get to the op condition.
    
    Allocating the preds themselves in a single array removes a dereference
    as well as helps on the cache footprint.
    
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 441fc1bc85d6..254d04a84ec3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -663,7 +663,7 @@ struct ftrace_event_field {
 struct event_filter {
 	int			n_preds;	/* Number assigned */
 	int			a_preds;	/* allocated */
-	struct filter_pred	**preds;
+	struct filter_pred	*preds;
 	char			*filter_string;
 };
 

commit c9c53ca03d6f97fdd9832d5ed3f15b30ee5cdb86
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 22:42:43 2011 -0500

    tracing/filter: Dynamically allocate preds
    
    For every filter that is made, we create predicates to hold every
    operation within the filter. We have a max of 32 predicates that we
    can hold. Currently, we allocate all 32 even if we only need to
    use one.
    
    Part of the reason we do this is that the filter can be used at
    any moment by any event. Fortunately, the filter is only used
    with preemption disabled. By reseting the count of preds used "n_preds"
    to zero, then performing a synchronize_sched(), we can safely
    free and reallocate a new array of preds.
    
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1597bc0749c1..441fc1bc85d6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -661,7 +661,8 @@ struct ftrace_event_field {
 };
 
 struct event_filter {
-	int			n_preds;
+	int			n_preds;	/* Number assigned */
+	int			a_preds;	/* allocated */
 	struct filter_pred	**preds;
 	char			*filter_string;
 };

commit 58d9a597c4275d830a819625e7d437cd6fb23fa5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 22:37:09 2011 -0500

    tracing/filter: Move OR and AND logic out of fn() method
    
    The ops OR and AND act different from the other ops, as they
    are the only ones to take other ops as their arguements.
    These ops als change the logic of the filter_match_preds.
    
    By removing the OR and AND fn's we can also remove the val1 and val2
    that is passed to all other fn's and are unused.
    
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9021f8c0c0c3..1597bc0749c1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -677,8 +677,7 @@ struct event_subsystem {
 struct filter_pred;
 struct regex;
 
-typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event,
-				 int val1, int val2);
+typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event);
 
 typedef int (*regex_match_func)(char *str, struct regex *r, int len);
 

commit 0a772620a2e21fb55a02f70fe38d4b5c3a5fbbbf
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Thu Sep 23 14:00:52 2010 +0200

    tracing: Make graph related irqs/preemptsoff functions global
    
    Move trace_graph_function() and print_graph_headers_flags() functions
    to the trace_function_graph.c to be globaly available.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    LKML-Reference: <1285243253-7372-3-git-send-email-jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d39b3c5454a5..9021f8c0c0c3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -343,6 +343,10 @@ void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
 		    unsigned long flags, int pc);
+void trace_graph_function(struct trace_array *tr,
+		    unsigned long ip,
+		    unsigned long parent_ip,
+		    unsigned long flags, int pc);
 void trace_default_header(struct seq_file *m);
 void print_trace_header(struct seq_file *m, struct trace_iterator *iter);
 int trace_empty(struct trace_iterator *iter);

commit 4aed2fd8e3181fea7c09ba79cf64e7e3f4413bf9
Merge: 3a3527b6461b fc9ea5a1e53e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Aug 6 09:30:52 2010 -0700

    Merge branch 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (162 commits)
      tracing/kprobes: unregister_trace_probe needs to be called under mutex
      perf: expose event__process function
      perf events: Fix mmap offset determination
      perf, powerpc: fsl_emb: Restore setting perf_sample_data.period
      perf, powerpc: Convert the FSL driver to use local64_t
      perf tools: Don't keep unreferenced maps when unmaps are detected
      perf session: Invalidate last_match when removing threads from rb_tree
      perf session: Free the ref_reloc_sym memory at the right place
      x86,mmiotrace: Add support for tracing STOS instruction
      perf, sched migration: Librarize task states and event headers helpers
      perf, sched migration: Librarize the GUI class
      perf, sched migration: Make the GUI class client agnostic
      perf, sched migration: Make it vertically scrollable
      perf, sched migration: Parameterize cpu height and spacing
      perf, sched migration: Fix key bindings
      perf, sched migration: Ignore unhandled task states
      perf, sched migration: Handle ignored migrate out events
      perf: New migration tool overview
      tracing: Drop cpparg() macro
      perf: Use tracepoint_synchronize_unregister() to flush any pending tracepoint call
      ...
    
    Fix up trivial conflicts in Makefile and drivers/cpufreq/cpufreq.c

commit 955b61e597984745fb7d34c75708f6503b6aaeab
Author: Jason Wessel <jason.wessel@windriver.com>
Date:   Thu Aug 5 09:22:23 2010 -0500

    ftrace,kdb: Extend kdb to be able to dump the ftrace buffer
    
    Add in a helper function to allow the kdb shell to dump the ftrace
    buffer.
    
    Modify trace.c to expose the capability to iterate over the ftrace
    buffer in a read only capacity.
    
    Signed-off-by: Jason Wessel <jason.wessel@windriver.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    CC: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2cd96399463f..0605fc00c176 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -338,6 +338,14 @@ struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 struct trace_entry *trace_find_next_entry(struct trace_iterator *iter,
 					  int *ent_cpu, u64 *ent_ts);
 
+int trace_empty(struct trace_iterator *iter);
+
+void *trace_find_next_entry_inc(struct trace_iterator *iter);
+
+void trace_init_global_iter(struct trace_iterator *iter);
+
+void tracing_iter_reset(struct trace_iterator *iter, int cpu);
+
 void default_wait_pipe(struct trace_iterator *iter);
 void poll_wait_pipe(struct trace_iterator *iter);
 
@@ -380,6 +388,15 @@ void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 int is_tracing_stopped(void);
+enum trace_file_type {
+	TRACE_FILE_LAT_FMT	= 1,
+	TRACE_FILE_ANNOTATE	= 2,
+};
+
+extern cpumask_var_t __read_mostly tracing_buffer_mask;
+
+#define for_each_tracing_cpu(cpu)	\
+	for_each_cpu(cpu, tracing_buffer_mask)
 
 extern int process_new_ksym_entry(char *ksymname, int op, unsigned long addr);
 
@@ -471,6 +488,8 @@ trace_array_vprintk(struct trace_array *tr,
 		    unsigned long ip, const char *fmt, va_list args);
 int trace_array_printk(struct trace_array *tr,
 		       unsigned long ip, const char *fmt, ...);
+void trace_printk_seq(struct trace_seq *s);
+enum print_line_t print_trace_line(struct trace_iterator *iter);
 
 extern unsigned long trace_flags;
 

commit 3a01736e70a7d629140695ba46a901266b4460cc
Merge: 4c21adf26f8f 24a461d537f4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Jul 23 09:10:29 2010 +0200

    Merge branch 'tip/perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into perf/core

commit ef710e100c1068d3dd5774d2b34c5485219e06ce
Author: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
Date:   Thu Jul 1 14:34:35 2010 +0900

    tracing: Shrink max latency ringbuffer if unnecessary
    
    Documentation/trace/ftrace.txt says
    
      buffer_size_kb:
    
            This sets or displays the number of kilobytes each CPU
            buffer can hold. The tracer buffers are the same size
            for each CPU. The displayed number is the size of the
            CPU buffer and not total size of all buffers. The
            trace buffers are allocated in pages (blocks of memory
            that the kernel uses for allocation, usually 4 KB in size).
            If the last page allocated has room for more bytes
            than requested, the rest of the page will be used,
            making the actual allocation bigger than requested.
            ( Note, the size may not be a multiple of the page size
              due to buffer management overhead. )
    
            This can only be updated when the current_tracer
            is set to "nop".
    
    But it's incorrect. currently total memory consumption is
    'buffer_size_kb x CPUs x 2'.
    
    Why two times difference is there? because ftrace implicitly allocate
    the buffer for max latency too.
    
    That makes sad result when admin want to use large buffer. (If admin
    want full logging and makes detail analysis). example, If admin
    have 24 CPUs machine and write 200MB to buffer_size_kb, the system
    consume ~10GB memory (200MB x 24 x 2). umm.. 5GB memory waste is
    usually unacceptable.
    
    Fortunatelly, almost all users don't use max latency feature.
    The max latency buffer can be disabled easily.
    
    This patch shrink buffer size of the max latency buffer if
    unnecessary.
    
    Signed-off-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    LKML-Reference: <20100701104554.DA2D.A69D9226@jp.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7778f067fc8b..cb629b3b108c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -276,6 +276,7 @@ struct tracer {
 	struct tracer		*next;
 	int			print_max;
 	struct tracer_flags	*flags;
+	int			use_max_tr;
 };
 
 

commit e870e9a1240bcef1157ffaaf71dac63362e71904
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Jul 2 11:07:32 2010 +0800

    tracing: Allow to disable cmdline recording
    
    We found that even enabling a single trace event that will rarely be
    triggered can add big overhead to context switch.
    
    (lmbench context switch test)
     -------------------------------------------------
     2p/0K 2p/16K 2p/64K 8p/16K 8p/64K 16p/16K 16p/64K
     ctxsw  ctxsw  ctxsw ctxsw  ctxsw   ctxsw   ctxsw
    ------ ------ ------ ------ ------ ------- -------
      2.19   2.3   2.21   2.56   2.13     2.54    2.07
      2.39   2.51  2.35   2.75   2.27     2.81    2.24
    
    The overhead is 6% ~ 11%.
    
    It's because when a trace event is enabled 3 tracepoints (sched_switch,
    sched_wakeup, sched_wakeup_new) will be activated to map pid to cmdname.
    
    We'd like to avoid this overhead, so add a trace option '(no)record-cmd'
    to allow to disable cmdline recording.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4C2D57F4.2050204@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 84d3f123e86f..7778f067fc8b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -591,6 +591,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_LATENCY_FMT		= 0x20000,
 	TRACE_ITER_SLEEP_TIME		= 0x40000,
 	TRACE_ITER_GRAPH_TIME		= 0x80000,
+	TRACE_ITER_RECORD_CMD		= 0x100000,
 };
 
 /*
@@ -723,6 +724,8 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 	return 0;
 }
 
+extern void trace_event_enable_cmd_record(bool enable);
+
 extern struct mutex event_mutex;
 extern struct list_head ftrace_events;
 

commit eb7beb5c09af75494234ea6acd09d0a647cf7338
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 16 00:50:03 2010 +0200

    tracing: Remove special traces
    
    Special traces type was only used by sysprof. Lets remove it now
    that sysprof ftrace plugin has been dropped.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Soeren Sandmann <sandmann@daimi.au.dk>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2114b4c1150f..638a5887e2ec 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -22,7 +22,6 @@ enum trace_type {
 	TRACE_STACK,
 	TRACE_PRINT,
 	TRACE_BPRINT,
-	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
 	TRACE_BRANCH,
@@ -189,7 +188,6 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
 		IF_ASSIGN(var, ent, struct bprint_entry, TRACE_BPRINT);	\
-		IF_ASSIGN(var, ent, struct special_entry, 0);		\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
 			  TRACE_MMIO_RW);				\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\
@@ -332,11 +330,6 @@ void tracing_sched_wakeup_trace(struct trace_array *tr,
 				struct task_struct *wakee,
 				struct task_struct *cur,
 				unsigned long flags, int pc);
-void trace_special(struct trace_array *tr,
-		   struct trace_array_cpu *data,
-		   unsigned long arg1,
-		   unsigned long arg2,
-		   unsigned long arg3, int pc);
 void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,

commit f376bf5ffbad863d4bc3b2586b7e34cdf756ad17
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Jul 16 00:26:26 2010 +0200

    tracing: Remove sysprof ftrace plugin
    
    The sysprof ftrace plugin doesn't seem to be seriously used
    somewhere. There is a branch in the sysprof tree that makes
    an interface to it, but the real sysprof tool uses either its
    own module or perf events.
    
    Drop the sysprof ftrace plugin then, as it's mostly useless.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Soeren Sandmann <sandmann@daimi.au.dk>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 84d3f123e86f..2114b4c1150f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -296,7 +296,6 @@ struct dentry *trace_create_file(const char *name,
 				 const struct file_operations *fops);
 
 struct dentry *tracing_init_dentry(void);
-void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 
 struct ring_buffer_event;
 
@@ -428,8 +427,6 @@ extern int trace_selftest_startup_nop(struct tracer *trace,
 					 struct trace_array *tr);
 extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 					       struct trace_array *tr);
-extern int trace_selftest_startup_sysprof(struct tracer *trace,
-					       struct trace_array *tr);
 extern int trace_selftest_startup_branch(struct tracer *trace,
 					 struct trace_array *tr);
 #endif /* CONFIG_FTRACE_STARTUP_TEST */

commit 5d550467b9770042e9699690907babc32104a8d4
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jul 15 23:27:35 2010 +0200

    tracing: Remove ksym tracer
    
    The ksym (breakpoint) ftrace plugin has been superseded by perf
    tools that are much more poweful to use the cpu breakpoints.
    This tracer doesn't bring more feature. It has been deprecated
    for a while now, lets remove it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cc90ccdad469..84d3f123e86f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -30,7 +30,6 @@ enum trace_type {
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
 	TRACE_BLK,
-	TRACE_KSYM,
 
 	__TRACE_LAST_TYPE,
 };
@@ -200,7 +199,6 @@ extern void __ftrace_bad_type(void);
 			  TRACE_GRAPH_ENT);		\
 		IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,	\
 			  TRACE_GRAPH_RET);		\
-		IF_ASSIGN(var, ent, struct ksym_trace_entry, TRACE_KSYM);\
 		__ftrace_bad_type();					\
 	} while (0)
 
@@ -361,8 +359,6 @@ int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 int is_tracing_stopped(void);
 
-extern int process_new_ksym_entry(char *ksymname, int op, unsigned long addr);
-
 extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 
 extern unsigned long tracing_thresh;
@@ -436,8 +432,6 @@ extern int trace_selftest_startup_sysprof(struct tracer *trace,
 					       struct trace_array *tr);
 extern int trace_selftest_startup_branch(struct tracer *trace,
 					 struct trace_array *tr);
-extern int trace_selftest_startup_ksym(struct tracer *trace,
-					 struct trace_array *tr);
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);

commit 8728fe501ed562c1b46dde3c195eadec77bca033
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon May 24 16:22:49 2010 +0800

    tracing: Don't allocate common fields for every trace events
    
    Every event has the same common fields, so it's a big waste of
    memory to have a copy of those fields for every event.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4BFA3759.30105@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 01ce088c1cdf..cc90ccdad469 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -698,6 +698,8 @@ struct filter_pred {
 	int 			pop_n;
 };
 
+extern struct list_head ftrace_common_fields;
+
 extern enum regex_type
 filter_parse_regex(char *buff, int len, char **search, int *not);
 extern void print_event_filter(struct ftrace_event_call *call,

commit c726b61c6a5acc54c55ed7a0e7638cc4c5a100a8
Merge: 7be7923633a1 018378c55b03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 9 18:55:20 2010 +0200

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/random-tracing into perf/core

commit 039ca4e74a1cf60bd7487324a564ecf5c981f254
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed May 26 17:22:17 2010 +0800

    tracing: Remove kmemtrace ftrace plugin
    
    We have been resisting new ftrace plugins and removing existing
    ones, and kmemtrace has been superseded by kmem trace events
    and perf-kmem, so we remove it.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Acked-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    [ remove kmemtrace from the makefile, handle slob too ]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 75a5e800a737..075cd2ea84a2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -9,7 +9,6 @@
 #include <linux/mmiotrace.h>
 #include <linux/tracepoint.h>
 #include <linux/ftrace.h>
-#include <linux/kmemtrace.h>
 #include <linux/hw_breakpoint.h>
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
@@ -30,19 +29,12 @@ enum trace_type {
 	TRACE_GRAPH_RET,
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
-	TRACE_KMEM_ALLOC,
-	TRACE_KMEM_FREE,
 	TRACE_BLK,
 	TRACE_KSYM,
 
 	__TRACE_LAST_TYPE,
 };
 
-enum kmemtrace_type_id {
-	KMEMTRACE_TYPE_KMALLOC = 0,	/* kmalloc() or kfree(). */
-	KMEMTRACE_TYPE_CACHE,		/* kmem_cache_*(). */
-	KMEMTRACE_TYPE_PAGES,		/* __get_free_pages() and friends. */
-};
 
 #undef __field
 #define __field(type, item)		type	item;
@@ -208,10 +200,6 @@ extern void __ftrace_bad_type(void);
 			  TRACE_GRAPH_ENT);		\
 		IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,	\
 			  TRACE_GRAPH_RET);		\
-		IF_ASSIGN(var, ent, struct kmemtrace_alloc_entry,	\
-			  TRACE_KMEM_ALLOC);	\
-		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
-			  TRACE_KMEM_FREE);	\
 		IF_ASSIGN(var, ent, struct ksym_trace_entry, TRACE_KSYM);\
 		__ftrace_bad_type();					\
 	} while (0)

commit 30dbb20e68e6f7df974b77d2350ebad5eb6f6c9e
Author: Américo Wang <xiyou.wangcong@gmail.com>
Date:   Wed May 26 18:57:53 2010 +0800

    tracing: Remove boot tracer
    
    The boot tracer is useless. It simply logs the initcalls
    but in fact these initcalls are also logged through printk
    while using the initcall_debug kernel parameter.
    
    Nobody seem to be using it so far. Then just remove it.
    
    Signed-off-by: WANG Cong <xiyou.wangcong@gmail.com>
    Cc: Chase Douglas <chase.douglas@canonical.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <20100526105753.GA5677@cr0.nay.redhat.com>
    [ remove the hooks in main.c, and the headers ]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2cd96399463f..75a5e800a737 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -9,10 +9,8 @@
 #include <linux/mmiotrace.h>
 #include <linux/tracepoint.h>
 #include <linux/ftrace.h>
-#include <trace/boot.h>
 #include <linux/kmemtrace.h>
 #include <linux/hw_breakpoint.h>
-
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
 
@@ -29,8 +27,6 @@ enum trace_type {
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
 	TRACE_BRANCH,
-	TRACE_BOOT_CALL,
-	TRACE_BOOT_RET,
 	TRACE_GRAPH_RET,
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
@@ -48,8 +44,6 @@ enum kmemtrace_type_id {
 	KMEMTRACE_TYPE_PAGES,		/* __get_free_pages() and friends. */
 };
 
-extern struct tracer boot_tracer;
-
 #undef __field
 #define __field(type, item)		type	item;
 
@@ -209,8 +203,6 @@ extern void __ftrace_bad_type(void);
 			  TRACE_MMIO_RW);				\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\
 			  TRACE_MMIO_MAP);				\
-		IF_ASSIGN(var, ent, struct trace_boot_call, TRACE_BOOT_CALL);\
-		IF_ASSIGN(var, ent, struct trace_boot_ret, TRACE_BOOT_RET);\
 		IF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \
 		IF_ASSIGN(var, ent, struct ftrace_graph_ent_entry,	\
 			  TRACE_GRAPH_ENT);		\

commit 5168ae50a66e3ff7184c2b16d661bd6d70367e50
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 3 09:36:50 2010 -0400

    tracing: Remove ftrace_preempt_disable/enable
    
    The ftrace_preempt_disable/enable functions were to address a
    recursive race caused by the function tracer. The function tracer
    traces all functions which makes it easily susceptible to recursion.
    One area was preempt_enable(). This would call the scheduler and
    the schedulre would call the function tracer and loop.
    (So was it thought).
    
    The ftrace_preempt_disable/enable was made to protect against recursion
    inside the scheduler by storing the NEED_RESCHED flag. If it was
    set before the ftrace_preempt_disable() it would not call schedule
    on ftrace_preempt_enable(), thinking that if it was set before then
    it would have already scheduled unless it was already in the scheduler.
    
    This worked fine except in the case of SMP, where another task would set
    the NEED_RESCHED flag for a task on another CPU, and then kick off an
    IPI to trigger it. This could cause the NEED_RESCHED to be saved at
    ftrace_preempt_disable() but the IPI to arrive in the the preempt
    disabled section. The ftrace_preempt_enable() would not call the scheduler
    because the flag was already set before entring the section.
    
    This bug would cause a missed preemption check and cause lower latencies.
    
    Investigating further, I found that the recusion caused by the function
    tracer was not due to schedule(), but due to preempt_schedule(). Now
    that preempt_schedule is completely annotated with notrace, the recusion
    no longer is an issue.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2cd96399463f..6c45e55097ce 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -628,54 +628,6 @@ enum trace_iterator_flags {
 
 extern struct tracer nop_trace;
 
-/**
- * ftrace_preempt_disable - disable preemption scheduler safe
- *
- * When tracing can happen inside the scheduler, there exists
- * cases that the tracing might happen before the need_resched
- * flag is checked. If this happens and the tracer calls
- * preempt_enable (after a disable), a schedule might take place
- * causing an infinite recursion.
- *
- * To prevent this, we read the need_resched flag before
- * disabling preemption. When we want to enable preemption we
- * check the flag, if it is set, then we call preempt_enable_no_resched.
- * Otherwise, we call preempt_enable.
- *
- * The rational for doing the above is that if need_resched is set
- * and we have yet to reschedule, we are either in an atomic location
- * (where we do not need to check for scheduling) or we are inside
- * the scheduler and do not want to resched.
- */
-static inline int ftrace_preempt_disable(void)
-{
-	int resched;
-
-	resched = need_resched();
-	preempt_disable_notrace();
-
-	return resched;
-}
-
-/**
- * ftrace_preempt_enable - enable preemption scheduler safe
- * @resched: the return value from ftrace_preempt_disable
- *
- * This is a scheduler safe way to enable preemption and not miss
- * any preemption checks. The disabled saved the state of preemption.
- * If resched is set, then we are either inside an atomic or
- * are inside the scheduler (we would have already scheduled
- * otherwise). In this case, we do not want to call normal
- * preempt_enable, but preempt_enable_no_resched instead.
- */
-static inline void ftrace_preempt_enable(int resched)
-{
-	if (resched)
-		preempt_enable_no_resched_notrace();
-	else
-		preempt_enable_notrace();
-}
-
 #ifdef CONFIG_BRANCH_TRACER
 extern int enable_branch_tracing(struct trace_array *tr);
 extern void disable_branch_tracing(void);

commit f0218b3e9974f06014b61be8987159f4a20e011e
Merge: 1eaa4787a774 9d192e118a09
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 17 22:26:53 2010 -0400

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip into trace/tip/tracing/core-6
    
    Conflicts:
            include/trace/ftrace.h
            kernel/trace/trace_kprobe.c
    
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit e1f7992e015ae1373d66c8068d0a45e4111a0aed
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon May 10 11:23:00 2010 +0800

    tracing: Fix function declarations if !CONFIG_STACKTRACE
    
    ftrace_trace_stack() and frace_trace_userstacke() take a
    struct ring_buffer argument, not struct trace_array. Commit
    e77405ad("tracing: pass around ring buffer instead of tracer")
    made this change.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4BE77C14.5010806@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 63562595f2b2..40cd1718fb1b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -419,12 +419,12 @@ void ftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags,
 void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
 		   int pc);
 #else
-static inline void ftrace_trace_stack(struct trace_array *tr,
+static inline void ftrace_trace_stack(struct ring_buffer *buffer,
 				      unsigned long flags, int skip, int pc)
 {
 }
 
-static inline void ftrace_trace_userstack(struct trace_array *tr,
+static inline void ftrace_trace_userstack(struct ring_buffer *buffer,
 					  unsigned long flags, int pc)
 {
 }

commit 553552ce1796c32cf4e3d4f45cd5b537de91dd1d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 23 11:12:36 2010 -0400

    tracing: Combine event filter_active and enable into single flags field
    
    The filter_active and enable both use an int (4 bytes each) to
    set a single flag. We can save 4 bytes per event by combining the
    two into a single integer.
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4894944 1018052  861512 6774508  675eec vmlinux.id
    4894871 1012292  861512 6768675  674823 vmlinux.flags
    
    This gives us another 5K in savings.
    
    The modification of both the enable and filter fields are done
    under the event_mutex, so it is still safe to combine the two.
    
    Note: Although Mathieu gave his Acked-by, he would like it documented
     that the reads of flags are not protected by the mutex. The way the
     code works, these reads will not break anything, but will have a
     residual effect. Since this behavior is the same even before this
     patch, describing this situation is left to another patch, as this
     patch does not change the behavior, but just brought it to Mathieu's
     attention.
    
    v2: Updated the event trace self test to for this change.
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c88c563a59a5..63562595f2b2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -802,7 +802,7 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 		     struct ring_buffer *buffer,
 		     struct ring_buffer_event *event)
 {
-	if (unlikely(call->filter_active) &&
+	if (unlikely(call->flags & TRACE_EVENT_FL_FILTERED) &&
 	    !filter_match_preds(call->filter, rec)) {
 		ring_buffer_discard_commit(buffer, event);
 		return 1;

commit 2e33af029556cb8bd22bf4f86f42d540249177ea
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 22 10:35:55 2010 -0400

    tracing: Move fields from event to class structure
    
    Move the defined fields from the event to the class structure.
    Since the fields of the event are defined by the class they belong
    to, it makes sense to have the class hold the information instead
    of the individual events. The events of the same class would just
    hold duplicate information.
    
    After this change the size of the kernel dropped another 3K:
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4900252 1057412  861512 6819176  680d68 vmlinux.regs
    4900375 1053380  861512 6815267  67fe23 vmlinux.fields
    
    Although the text increased, this was mainly due to the C files
    having to adapt to the change. This is a constant increase, where
    new tracepoints will not increase the Text. But the big drop is
    in the data size (as well as needed allocations to hold the fields).
    This will give even more savings as more tracepoints are created.
    
    Note, if just TRACE_EVENT()s are used and not DECLARE_EVENT_CLASS()
    with several DEFINE_EVENT()s, then the savings will be lost. But
    we are pushing developers to consolidate events with DEFINE_EVENT()
    so this should not be an issue.
    
    The kprobes define a unique class to every new event, but are dynamic
    so it should not be a issue.
    
    The syscalls however have a single class but the fields for the individual
    events are different. The syscalls use a metadata to define the
    fields. I moved the fields list from the event to the metadata and
    added a "get_fields()" function to the class. This function is used
    to find the fields. For normal events and kprobes, get_fields() just
    returns a pointer to the fields list_head in the class. For syscall
    events, it returns the fields list_head in the metadata for the event.
    
    v2:  Fixed the syscall fields. The syscall metadata needs a list
         of fields for both enter and exit.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 911e9864e94a..c88c563a59a5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -794,6 +794,9 @@ extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);
 extern int filter_assign_type(const char *type);
 
+struct list_head *
+trace_get_fields(struct ftrace_event_call *event_call);
+
 static inline int
 filter_check_discard(struct ftrace_event_call *call, void *rec,
 		     struct ring_buffer *buffer,

commit 62b915f1060996a8e1f69be50e3b8e9e43b710cb
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Fri Apr 2 19:01:22 2010 +0200

    tracing: Add graph output support for irqsoff tracer
    
    Add function graph output to irqsoff tracer.
    
    The graph output is enabled by setting new 'display-graph' trace option.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    LKML-Reference: <1270227683-14631-4-git-send-email-jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 970004c5fa79..911e9864e94a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -378,6 +378,9 @@ void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
 		    unsigned long flags, int pc);
+void trace_default_header(struct seq_file *m);
+void print_trace_header(struct seq_file *m, struct trace_iterator *iter);
+int trace_empty(struct trace_iterator *iter);
 
 void trace_graph_return(struct ftrace_graph_ret *trace);
 int trace_graph_entry(struct ftrace_graph_ent *trace);
@@ -491,11 +494,29 @@ extern int trace_clock_id;
 
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
+
+/* Flag options */
+#define TRACE_GRAPH_PRINT_OVERRUN       0x1
+#define TRACE_GRAPH_PRINT_CPU           0x2
+#define TRACE_GRAPH_PRINT_OVERHEAD      0x4
+#define TRACE_GRAPH_PRINT_PROC          0x8
+#define TRACE_GRAPH_PRINT_DURATION      0x10
+#define TRACE_GRAPH_PRINT_ABS_TIME      0x20
+
 extern enum print_line_t
 print_graph_function_flags(struct trace_iterator *iter, u32 flags);
 extern void print_graph_headers_flags(struct seq_file *s, u32 flags);
 extern enum print_line_t
 trace_print_graph_duration(unsigned long long duration, struct trace_seq *s);
+extern void graph_trace_open(struct trace_iterator *iter);
+extern void graph_trace_close(struct trace_iterator *iter);
+extern int __trace_graph_entry(struct trace_array *tr,
+			       struct ftrace_graph_ent *trace,
+			       unsigned long flags, int pc);
+extern void __trace_graph_return(struct trace_array *tr,
+				 struct ftrace_graph_ret *trace,
+				 unsigned long flags, int pc);
+
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* TODO: make this variable */

commit d7a8d9e907cc294ec7a4a7046d1886375fbcc82e
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Fri Apr 2 19:01:21 2010 +0200

    tracing: Have graph flags passed in to ouput functions
    
    Let the function graph tracer have custom flags passed to its
    output functions.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    LKML-Reference: <1270227683-14631-3-git-send-email-jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2825ef2c0b15..970004c5fa79 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -491,7 +491,9 @@ extern int trace_clock_id;
 
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
-extern enum print_line_t print_graph_function(struct trace_iterator *iter);
+extern enum print_line_t
+print_graph_function_flags(struct trace_iterator *iter, u32 flags);
+extern void print_graph_headers_flags(struct seq_file *s, u32 flags);
 extern enum print_line_t
 trace_print_graph_duration(unsigned long long duration, struct trace_seq *s);
 
@@ -524,7 +526,7 @@ static inline int ftrace_graph_addr(unsigned long addr)
 #endif /* CONFIG_DYNAMIC_FTRACE */
 #else /* CONFIG_FUNCTION_GRAPH_TRACER */
 static inline enum print_line_t
-print_graph_function(struct trace_iterator *iter)
+print_graph_function_flags(struct trace_iterator *iter, u32 flags)
 {
 	return TRACE_TYPE_UNHANDLED;
 }

commit 93ccae7a2227466a0d071fe52c51319f2f34c365
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Apr 12 13:17:08 2010 -0400

    tracing/kprobes: Support basic types on dynamic events
    
    Support basic types of integer (u8, u16, u32, u64, s8, s16, s32, s64) in
    kprobe tracer. With this patch, users can specify above basic types on
    each arguments after ':'. If omitted, the argument type is set as
    unsigned long (u32 or u64, arch-dependent).
    
     e.g.
      echo 'p account_system_time+0 hardirq_offset=%si:s32' > kprobe_events
    
      adds a probe recording hardirq_offset in signed-32bits value on the
      entry of account_system_time.
    
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20100412171708.3790.18599.stgit@localhost6.localdomain6>
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index bec2c973ff0c..3ebdb6bd2362 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -102,29 +102,17 @@ struct syscall_trace_exit {
 	long			ret;
 };
 
-struct kprobe_trace_entry {
+struct kprobe_trace_entry_head {
 	struct trace_entry	ent;
 	unsigned long		ip;
-	int			nargs;
-	unsigned long		args[];
 };
 
-#define SIZEOF_KPROBE_TRACE_ENTRY(n)			\
-	(offsetof(struct kprobe_trace_entry, args) +	\
-	(sizeof(unsigned long) * (n)))
-
-struct kretprobe_trace_entry {
+struct kretprobe_trace_entry_head {
 	struct trace_entry	ent;
 	unsigned long		func;
 	unsigned long		ret_ip;
-	int			nargs;
-	unsigned long		args[];
 };
 
-#define SIZEOF_KRETPROBE_TRACE_ENTRY(n)			\
-	(offsetof(struct kretprobe_trace_entry, args) +	\
-	(sizeof(unsigned long) * (n)))
-
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:

commit faa4602e47690fb11221e00f9b9697c8dc0d4b19
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 25 14:51:50 2010 +0100

    x86, perf, bts, mm: Delete the never used BTS-ptrace code
    
    Support for the PMU's BTS features has been upstreamed in
    v2.6.32, but we still have the old and disabled ptrace-BTS,
    as Linus noticed it not so long ago.
    
    It's buggy: TIF_DEBUGCTLMSR is trampling all over that MSR without
    regard for other uses (perf) and doesn't provide the flexibility
    needed for perf either.
    
    Its users are ptrace-block-step and ptrace-bts, since ptrace-bts
    was never used and ptrace-block-step can be implemented using a
    much simpler approach.
    
    So axe all 3000 lines of it. That includes the *locked_memory*()
    APIs in mm/mlock.c as well.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Markus Metzger <markus.t.metzger@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    LKML-Reference: <20100325135413.938004390@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2825ef2c0b15..bec2c973ff0c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -34,7 +34,6 @@ enum trace_type {
 	TRACE_GRAPH_RET,
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
-	TRACE_HW_BRANCHES,
 	TRACE_KMEM_ALLOC,
 	TRACE_KMEM_FREE,
 	TRACE_BLK,
@@ -229,7 +228,6 @@ extern void __ftrace_bad_type(void);
 			  TRACE_GRAPH_ENT);		\
 		IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,	\
 			  TRACE_GRAPH_RET);		\
-		IF_ASSIGN(var, ent, struct hw_branch_entry, TRACE_HW_BRANCHES);\
 		IF_ASSIGN(var, ent, struct kmemtrace_alloc_entry,	\
 			  TRACE_KMEM_ALLOC);	\
 		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
@@ -467,8 +465,6 @@ extern int trace_selftest_startup_sysprof(struct tracer *trace,
 					       struct trace_array *tr);
 extern int trace_selftest_startup_branch(struct tracer *trace,
 					 struct trace_array *tr);
-extern int trace_selftest_startup_hw_branches(struct tracer *trace,
-					      struct trace_array *tr);
 extern int trace_selftest_startup_ksym(struct tracer *trace,
 					 struct trace_array *tr);
 #endif /* CONFIG_FTRACE_STARTUP_TEST */

commit 8655e7e3ddec60603c4f6c14cdf642e2ba198df8
Merge: 461d208cfbd1 b6345879ccbd
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 13 14:40:50 2010 -0800

    Merge branch 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      tracing: Do not record user stack trace from NMI context
      tracing: Disable buffer switching when starting or stopping trace
      tracing: Use same local variable when resetting the ring buffer
      function-graph: Init curr_ret_stack with ret_stack
      ring-buffer: Move disabled check into preempt disable section
      function-graph: Add tracing_thresh support to function_graph tracer
      tracing: Update the comm field in the right variable in update_max_tr
      function-graph: Use comment notation for func names of dangling '}'
      function-graph: Fix unused reference to ftrace_set_func()
      tracing: Fix warning in s_next of trace file ops
      tracing: Include irqflags headers from trace clock

commit 318ae2edc3b29216abd8a2510f3f80b764f06858
Merge: 25cf84cf377c 3e58974027b0
Author: Jiri Kosina <jkosina@suse.cz>
Date:   Mon Mar 8 16:55:37 2010 +0100

    Merge branch 'for-next' into for-linus
    
    Conflicts:
            Documentation/filesystems/proc.txt
            arch/arm/mach-u300/include/mach/debug-macro.S
            drivers/net/qlge/qlge_ethtool.c
            drivers/net/qlge/qlge_main.c
            drivers/net/typhoon.c

commit 0e95017355dcf43031da6d0e360a748717e56df1
Author: Tim Bird <tim.bird@am.sony.com>
Date:   Thu Feb 25 15:36:43 2010 -0800

    function-graph: Add tracing_thresh support to function_graph tracer
    
    Add support for tracing_thresh to the function_graph tracer.  This
    version of this feature isolates the checks into new entry and
    return functions, to avoid adding more conditional code into the
    main function_graph paths.
    
    When the tracing_thresh is set and the function graph tracer is
    enabled, only the functions that took longer than the time in
    microseconds that was set in tracing_thresh are recorded. To do this
    efficiently, only the function exits are recorded:
    
     [tracing]# echo 100 > tracing_thresh
     [tracing]# echo function_graph > current_tracer
     [tracing]# cat trace
     # tracer: function_graph
     #
     # CPU  DURATION                  FUNCTION CALLS
     # |     |   |                     |   |   |   |
      1) ! 119.214 us  |  } /* smp_apic_timer_interrupt */
      1)   <========== |
      0) ! 101.527 us  |              } /* __rcu_process_callbacks */
      0) ! 126.461 us  |            } /* rcu_process_callbacks */
      0) ! 145.111 us  |          } /* __do_softirq */
      0) ! 149.667 us  |        } /* do_softirq */
      0) ! 168.817 us  |      } /* irq_exit */
      0) ! 248.254 us  |    } /* smp_apic_timer_interrupt */
    
    Also, add support for specifying tracing_thresh on the kernel
    command line.  When used like so: "tracing_thresh=200 ftrace=function_graph"
    this can be used to analyse system startup.  It is important to disable
    tracing soon after boot, in order to avoid losing the trace data.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Tim Bird <tim.bird@am.sony.com>
    LKML-Reference: <4B87098B.4040308@am.sony.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fd05bcaf91b0..1bc8cd1431d7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -396,9 +396,10 @@ extern int process_new_ksym_entry(char *ksymname, int op, unsigned long addr);
 
 extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 
+extern unsigned long tracing_thresh;
+
 #ifdef CONFIG_TRACER_MAX_TRACE
 extern unsigned long tracing_max_latency;
-extern unsigned long tracing_thresh;
 
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,

commit 86c38a31aa7f2dd6e74a262710bf8ebf7455acc5
Author: Jeff Mahoney <jeffm@suse.com>
Date:   Wed Feb 24 13:59:23 2010 -0500

    tracing: Fix ftrace_event_call alignment for use with gcc 4.5
    
    GCC 4.5 introduces behavior that forces the alignment of structures to
     use the largest possible value. The default value is 32 bytes, so if
     some structures are defined with a 4-byte alignment and others aren't
     declared with an alignment constraint at all - it will align at 32-bytes.
    
     For things like the ftrace events, this results in a non-standard array.
     When initializing the ftrace subsystem, we traverse the _ftrace_events
     section and call the initialization callback for each event. When the
     structures are misaligned, we could be treating another part of the
     structure (or the zeroed out space between them) as a function pointer.
    
     This patch forces the alignment for all the ftrace_event_call structures
     to 4 bytes.
    
     Without this patch, the kernel fails to boot very early when built with
     gcc 4.5.
    
     It's trivial to check the alignment of the members of the array, so it
     might be worthwhile to add something to the build system to do that
     automatically. Unfortunately, that only covers this case. I've asked one
     of the gcc developers about adding a warning when this condition is seen.
    
    Cc: stable@kernel.org
    Signed-off-by: Jeff Mahoney <jeffm@suse.com>
    LKML-Reference: <4B85770B.6010901@suse.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b477fce41edf..fd05bcaf91b0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -792,7 +792,8 @@ extern const char *__stop___trace_bprintk_fmt[];
 
 #undef FTRACE_ENTRY
 #define FTRACE_ENTRY(call, struct_name, id, tstruct, print)		\
-	extern struct ftrace_event_call event_##call;
+	extern struct ftrace_event_call					\
+	__attribute__((__aligned__(4))) event_##call;
 #undef FTRACE_ENTRY_DUP
 #define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print)		\
 	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))

commit c7c6b1fe9f942c1a30585ec2210a09dfff238506
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Feb 10 15:43:04 2010 +0800

    ftrace: Allow to remove a single function from function graph filter
    
    I don't see why we can only clear all functions from the filter.
    
    After patching:
    
      # echo sys_open > set_graph_function
      # echo sys_close >> set_graph_function
      # cat set_graph_function
      sys_open
      sys_close
      # echo '!sys_close' >> set_graph_function
      # cat set_graph_function
      sys_open
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4B726388.2000408@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ce077fbbf552..b477fce41edf 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -497,6 +497,7 @@ trace_print_graph_duration(unsigned long long duration, struct trace_seq *s);
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* TODO: make this variable */
 #define FTRACE_GRAPH_MAX_FUNCS		32
+extern int ftrace_graph_filter_enabled;
 extern int ftrace_graph_count;
 extern unsigned long ftrace_graph_funcs[FTRACE_GRAPH_MAX_FUNCS];
 
@@ -504,7 +505,7 @@ static inline int ftrace_graph_addr(unsigned long addr)
 {
 	int i;
 
-	if (!ftrace_graph_count)
+	if (!ftrace_graph_filter_enabled)
 		return 1;
 
 	for (i = 0; i < ftrace_graph_count; i++) {

commit 1537a3638cbf741d3826c1002026cce487a6bee0
Author: Daniel Mack <daniel@caiaq.de>
Date:   Fri Jan 29 15:57:49 2010 +0800

    tree-wide: fix 'lenght' typo in comments and code
    
    Some misspelled occurences of 'octet' and some comments were also fixed
    as I was on it.
    
    Signed-off-by: Daniel Mack <daniel@caiaq.de>
    Cc: Jiri Kosina <trivial@kernel.org>
    Cc: Joe Perches <joe@perches.com>
    Cc: Junio C Hamano <gitster@pobox.com>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4df6a77eb196..e4b32c8aa85f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -549,7 +549,7 @@ static inline int ftrace_trace_task(struct task_struct *task)
  * struct trace_parser - servers for reading the user input separated by spaces
  * @cont: set if the input is not complete - no final space char was found
  * @buffer: holds the parsed user input
- * @idx: user input lenght
+ * @idx: user input length
  * @size: buffer size
  */
 struct trace_parser {

commit ea2c68a08fedb5053ba312d661e47df9f4d72411
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Wed Jan 13 19:38:30 2010 +0800

    tracing: Simplify test for function_graph tracing start point
    
    In the function graph tracer, a calling function is to be traced
    only when it is enabled through the set_graph_function file,
    or when it is nested in an enabled function.
    
    Current code uses TSK_TRACE_FL_GRAPH to test whether it is nested
    or not. Looking at the code, we can get this:
    (trace->depth > 0) <==> (TSK_TRACE_FL_GRAPH is set)
    
    trace->depth is more explicit to tell that it is nested.
    So we use trace->depth directly and simplify the code.
    
    No functionality is changed.
    TSK_TRACE_FL_GRAPH is not removed yet, it is left for future usage.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <4B4DB0B6.7040607@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4df6a77eb196..ce077fbbf552 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -504,7 +504,7 @@ static inline int ftrace_graph_addr(unsigned long addr)
 {
 	int i;
 
-	if (!ftrace_graph_count || test_tsk_trace_graph(current))
+	if (!ftrace_graph_count)
 		return 1;
 
 	for (i = 0; i < ftrace_graph_count; i++) {

commit da184a8064efe2a78d8542877970f7c6bb62775a
Merge: 525995d77ca0 e36c54582c6f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 16 12:02:25 2009 -0800

    Merge branch 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      tracing: Fix return of trace_dump_stack()
      ksym_tracer: Fix bad cast
      tracing/power: Remove two exports
      tracing: Change event->profile_count to be int type
      tracing: Simplify trace_option_write()
      tracing: Remove useless trace option
      tracing: Use seq file for trace_clock
      tracing: Use seq file for trace_options
      function-graph: Allow writing the same val to set_graph_function
      ftrace: Call trace_parser_clear() properly
      ftrace: Return EINVAL when writing invalid val to set_ftrace_filter
      tracing: Move a printk out of ftrace_raw_reg_event_foo()
      tracing: Pull up calls to trace_define_common_fields()
      tracing: Extract duplicate ftrace_raw_init_event_foo()
      ftrace.h: Use common pr_info fmt string
      tracing: Add stack trace to irqsoff tracer
      tracing: Add trace_dump_stack()
      ring-buffer: Move resize integrity check under reader lock
      ring-buffer: Use sync sched protection on ring buffer resizing
      tracing: Fix wrong usage of strstrip in trace_ksyms

commit d0316554d3586cbea60592a41391b5def2553d6f
Merge: fb0bbb92d42d 51e99be00ce2
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 14 09:58:24 2009 -0800

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (34 commits)
      m68k: rename global variable vmalloc_end to m68k_vmalloc_end
      percpu: add missing per_cpu_ptr_to_phys() definition for UP
      percpu: Fix kdump failure if booted with percpu_alloc=page
      percpu: make misc percpu symbols unique
      percpu: make percpu symbols in ia64 unique
      percpu: make percpu symbols in powerpc unique
      percpu: make percpu symbols in x86 unique
      percpu: make percpu symbols in xen unique
      percpu: make percpu symbols in cpufreq unique
      percpu: make percpu symbols in oprofile unique
      percpu: make percpu symbols in tracer unique
      percpu: make percpu symbols under kernel/ and mm/ unique
      percpu: remove some sparse warnings
      percpu: make alloc_percpu() handle array types
      vmalloc: fix use of non-existent percpu variable in put_cpu_var()
      this_cpu: Use this_cpu_xx in trace_functions_graph.c
      this_cpu: Use this_cpu_xx for ftrace
      this_cpu: Use this_cpu_xx in nmi handling
      this_cpu: Use this_cpu operations in RCU
      this_cpu: Use this_cpu ops for VM statistics
      ...
    
    Fix up trivial (famous last words) global per-cpu naming conflicts in
            arch/x86/kvm/svm.c
            mm/slab.c

commit 2cbafd68b826f8e0471875cf33cdfb8a1478aef1
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Dec 8 11:16:26 2009 +0800

    tracing: Remove useless trace option
    
    Since commit 4d9493c90f8e6e1b164aede3814010a290161abb
    ("ftrace: remove add-hoc code"), option "sched-tree"
    has become useless.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <4B1DC50A.7040402@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7fa33cab6962..1b18cb240c16 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -597,18 +597,17 @@ enum trace_iterator_flags {
 	TRACE_ITER_BIN			= 0x40,
 	TRACE_ITER_BLOCK		= 0x80,
 	TRACE_ITER_STACKTRACE		= 0x100,
-	TRACE_ITER_SCHED_TREE		= 0x200,
-	TRACE_ITER_PRINTK		= 0x400,
-	TRACE_ITER_PREEMPTONLY		= 0x800,
-	TRACE_ITER_BRANCH		= 0x1000,
-	TRACE_ITER_ANNOTATE		= 0x2000,
-	TRACE_ITER_USERSTACKTRACE       = 0x4000,
-	TRACE_ITER_SYM_USEROBJ          = 0x8000,
-	TRACE_ITER_PRINTK_MSGONLY	= 0x10000,
-	TRACE_ITER_CONTEXT_INFO		= 0x20000, /* Print pid/cpu/time */
-	TRACE_ITER_LATENCY_FMT		= 0x40000,
-	TRACE_ITER_SLEEP_TIME		= 0x80000,
-	TRACE_ITER_GRAPH_TIME		= 0x100000,
+	TRACE_ITER_PRINTK		= 0x200,
+	TRACE_ITER_PREEMPTONLY		= 0x400,
+	TRACE_ITER_BRANCH		= 0x800,
+	TRACE_ITER_ANNOTATE		= 0x1000,
+	TRACE_ITER_USERSTACKTRACE       = 0x2000,
+	TRACE_ITER_SYM_USEROBJ          = 0x4000,
+	TRACE_ITER_PRINTK_MSGONLY	= 0x8000,
+	TRACE_ITER_CONTEXT_INFO		= 0x10000, /* Print pid/cpu/time */
+	TRACE_ITER_LATENCY_FMT		= 0x20000,
+	TRACE_ITER_SLEEP_TIME		= 0x40000,
+	TRACE_ITER_GRAPH_TIME		= 0x80000,
 };
 
 /*

commit c521efd1700a8c0f7ce26f011f5eaecca17fabfa
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Dec 7 09:06:24 2009 -0500

    tracing: Add pipe_close interface
    
    An ftrace plugin can add a pipe_open interface when the user opens
    trace_pipe. But if the plugin allocates something within the pipe_open
    it can not free it because there exists no pipe_close. The hook to
    the trace file open has a corresponding close. The closing of the
    trace_pipe file should also have a corresponding close.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1d7f4830a80d..7fa33cab6962 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -272,6 +272,7 @@ struct tracer_flags {
  * @pipe_open: called when the trace_pipe file is opened
  * @wait_pipe: override how the user waits for traces on trace_pipe
  * @close: called when the trace file is released
+ * @pipe_close: called when the trace_pipe file is released
  * @read: override the default read callback on trace_pipe
  * @splice_read: override the default splice_read callback on trace_pipe
  * @selftest: selftest to run on boot (see trace_selftest.c)
@@ -290,6 +291,7 @@ struct tracer {
 	void			(*pipe_open)(struct trace_iterator *iter);
 	void			(*wait_pipe)(struct trace_iterator *iter);
 	void			(*close)(struct trace_iterator *iter);
+	void			(*pipe_close)(struct trace_iterator *iter);
 	ssize_t			(*read)(struct trace_iterator *iter,
 					struct file *filp, char __user *ubuf,
 					size_t cnt, loff_t *ppos);

commit 99df5a6a215f026e62287083de2b44b22edd3623
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Wed Nov 25 01:14:59 2009 -0600

    trace/syscalls: Change ret param in struct syscall_trace_exit to long
    
    Commit ee949a86b3aef15845ea677aa60231008de62672 ("tracing/syscalls:
    Use long for syscall ret format and field definitions") changed the
    syscall exit return type to long, but forgot to change it in the
    struct.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1259133299-23594-3-git-send-email-tzanussi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4da6ede74401..1d7f4830a80d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -100,7 +100,7 @@ struct syscall_trace_enter {
 struct syscall_trace_exit {
 	struct trace_entry	ent;
 	int			nr;
-	unsigned long		ret;
+	long			ret;
 };
 
 struct kprobe_trace_entry {

commit 96200591a34f8ecb98481c626125df43a2463b55
Merge: 7031281e02bf 68efa37df779
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Nov 21 14:07:23 2009 +0100

    Merge branch 'tracing/hw-breakpoints' into perf/core
    
    Conflicts:
            arch/x86/kernel/kprobes.c
            kernel/trace/Makefile
    
    Merge reason: hw-breakpoints perf integration is looking
                  good in testing and in reviews, plus conflicts
                  are mounting up - so merge & resolve.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 30ff21e31fe5c8b7b1b7d30cc41e32bc4ee9f175
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Sep 10 09:35:20 2009 +0800

    ksym_tracer: Remove KSYM_SELFTEST_ENTRY
    
    The macro used to be used in both trace_selftest.c and
    trace_ksym.c, but no longer, so remove it from header file.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d72f06ff263f..ee00475742eb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -367,7 +367,6 @@ int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 int is_tracing_stopped(void);
 
-#define KSYM_SELFTEST_ENTRY "ksym_selftest_dummy"
 extern int process_new_ksym_entry(char *ksymname, int op, unsigned long addr);
 
 extern unsigned long nsecs_to_usecs(unsigned long nsecs);

commit 24f1e32c60c45c89a997c73395b69c8af6f0a84e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 9 19:22:48 2009 +0200

    hw-breakpoints: Rewrite the hw-breakpoints layer on top of perf events
    
    This patch rebase the implementation of the breakpoints API on top of
    perf events instances.
    
    Each breakpoints are now perf events that handle the
    register scheduling, thread/cpu attachment, etc..
    
    The new layering is now made as follows:
    
           ptrace       kgdb      ftrace   perf syscall
              \          |          /         /
               \         |         /         /
                                            /
                Core breakpoint API        /
                                          /
                         |               /
                         |              /
    
                  Breakpoints perf events
    
                         |
                         |
    
                   Breakpoints PMU ---- Debug Register constraints handling
                                        (Part of core breakpoint API)
                         |
                         |
    
                 Hardware debug registers
    
    Reasons of this rewrite:
    
    - Use the centralized/optimized pmu registers scheduling,
      implying an easier arch integration
    - More powerful register handling: perf attributes (pinned/flexible
      events, exclusive/non-exclusive, tunable period, etc...)
    
    Impact:
    
    - New perf ABI: the hardware breakpoints counters
    - Ptrace breakpoints setting remains tricky and still needs some per
      thread breakpoints references.
    
    Todo (in the order):
    
    - Support breakpoints perf counter events for perf tools (ie: implement
      perf_bpcounter_event())
    - Support from perf tools
    
    Changes in v2:
    
    - Follow the perf "event " rename
    - The ptrace regression have been fixed (ptrace breakpoint perf events
      weren't released when a task ended)
    - Drop the struct hw_breakpoint and store generic fields in
      perf_event_attr.
    - Separate core and arch specific headers, drop
      asm-generic/hw_breakpoint.h and create linux/hw_breakpoint.h
    - Use new generic len/type for breakpoint
    - Handle off case: when breakpoints api is not supported by an arch
    
    Changes in v3:
    
    - Fix broken CONFIG_KVM, we need to propagate the breakpoint api
      changes to kvm when we exit the guest and restore the bp registers
      to the host.
    
    Changes in v4:
    
    - Drop the hw_breakpoint_restore() stub as it is only used by KVM
    - EXPORT_SYMBOL_GPL hw_breakpoint_restore() as KVM can be built as a
      module
    - Restore the breakpoints unconditionally on kvm guest exit:
      TIF_DEBUG_THREAD doesn't anymore cover every cases of running
      breakpoints and vcpu->arch.switch_db_regs might not always be
      set when the guest used debug registers.
      (Waiting for a reliable optimization)
    
    Changes in v5:
    
    - Split-up the asm-generic/hw-breakpoint.h moving to
      linux/hw_breakpoint.h into a separate patch
    - Optimize the breakpoints restoring while switching from kvm guest
      to host. We only want to restore the state if we have active
      breakpoints to the host, otherwise we don't care about messed-up
      address registers.
    - Add asm/hw_breakpoint.h to Kbuild
    - Fix bad breakpoint type in trace_selftest.c
    
    Changes in v6:
    
    - Fix wrong header inclusion in trace.h (triggered a build
      error with CONFIG_FTRACE_SELFTEST
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Prasad <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jan Kiszka <jan.kiszka@web.de>
    Cc: Jiri Slaby <jirislaby@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Paul Mundt <lethal@linux-sh.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 91c3d0e9a5a1..d72f06ff263f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -11,14 +11,11 @@
 #include <linux/ftrace.h>
 #include <trace/boot.h>
 #include <linux/kmemtrace.h>
+#include <linux/hw_breakpoint.h>
 
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
 
-#ifdef CONFIG_KSYM_TRACER
-#include <asm/hw_breakpoint.h>
-#endif
-
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
 

commit 43315956509ca6913764861ac7dec128b91eb1ec
Merge: 9bf4e7fba800 6beba7adbe09
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 23 08:23:20 2009 +0200

    Merge branch 'perf/core' into perf/probes
    
    Conflicts:
            tools/perf/Makefile
    
    Merge reason:
    
     - fix the conflict
     - pick up the pr_*() infrastructure to queue up dependent patch
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 0f8f86c7bdd1c954fbe153af437a0d91a6c5721a
Merge: dca2d6ac09d9 f39cdf25bf77
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Oct 18 01:09:09 2009 +0200

    Merge commit 'perf/core' into perf/hw-breakpoint
    
    Conflicts:
            kernel/Makefile
            kernel/trace/Makefile
            kernel/trace/trace.h
            samples/Makefile
    
    Merge reason: We need to be uptodate with the perf events development
    branch because we plan to rewrite the breakpoints API on top of
    perf events.

commit 6fb2915df7f0747d9044da9dbff5b46dc2e20830
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Oct 15 11:21:42 2009 +0800

    tracing/profile: Add filter support
    
    - Add an ioctl to allocate a filter for a perf event.
    
    - Free the filter when the associated perf event is to be freed.
    
    - Do the filtering in perf_swevent_match().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <4AD69546.8050401@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ffe53ddbe67a..4959ada9e0bb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -743,7 +743,8 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 		     struct ring_buffer *buffer,
 		     struct ring_buffer_event *event)
 {
-	if (unlikely(call->filter_active) && !filter_match_preds(call, rec)) {
+	if (unlikely(call->filter_active) &&
+	    !filter_match_preds(call->filter, rec)) {
 		ring_buffer_discard_commit(buffer, event);
 		return 1;
 	}

commit b0f1a59a98d7ac2102e7e4f22904c26d564a5628
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Oct 15 11:21:12 2009 +0800

    tracing/filters: Use a different op for glob match
    
    "==" will always do a full match, and "~" will do a glob match.
    
    In the future, we may add "=~" for regex match.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <4AD69528.3050309@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 628614532d16..ffe53ddbe67a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -702,7 +702,7 @@ typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event,
 typedef int (*regex_match_func)(char *str, struct regex *r, int len);
 
 enum regex_type {
-	MATCH_FULL,
+	MATCH_FULL = 0,
 	MATCH_FRONT_ONLY,
 	MATCH_MIDDLE_ONLY,
 	MATCH_END_ONLY,

commit fce29d15b59245597f7f320db4a9f2be0f5fb512
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Oct 15 11:20:34 2009 +0800

    tracing/filters: Refactor subsystem filter code
    
    Change:
            for_each_pred
                    for_each_subsystem
    To:
            for_each_subsystem
                    for_each_pred
    
    This change also prepares for later patches.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <4AD69502.8060903@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index acef8b4636f0..628614532d16 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -683,7 +683,6 @@ struct event_filter {
 	int			n_preds;
 	struct filter_pred	**preds;
 	char			*filter_string;
-	bool			no_reset;
 };
 
 struct event_subsystem {

commit 756d17ee7ee4fbc8238bdf97100af63e6ac441ef
Author: jolsa@redhat.com <jolsa@redhat.com>
Date:   Tue Oct 13 16:33:52 2009 -0400

    tracing: Support multiple pids in set_pid_ftrace file
    
    Adding the possibility to set more than 1 pid in the set_pid_ftrace
    file, thus allowing to trace more than 1 independent processes.
    
    Usage:
    
     sh-4.0# echo 284 > ./set_ftrace_pid
     sh-4.0# cat ./set_ftrace_pid
     284
     sh-4.0# echo 1 >> ./set_ftrace_pid
     sh-4.0# echo 0 >> ./set_ftrace_pid
     sh-4.0# cat ./set_ftrace_pid
     swapper tasks
     1
     284
     sh-4.0# echo 4 > ./set_ftrace_pid
     sh-4.0# cat ./set_ftrace_pid
     4
     sh-4.0# echo > ./set_ftrace_pid
     sh-4.0# cat ./set_ftrace_pid
     no pid
     sh-4.0#
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20091013203425.565454612@goodmis.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f22a7ac32380..acef8b4636f0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -496,12 +496,12 @@ print_graph_function(struct trace_iterator *iter)
 }
 #endif /* CONFIG_FUNCTION_GRAPH_TRACER */
 
-extern struct pid *ftrace_pid_trace;
+extern struct list_head ftrace_pids;
 
 #ifdef CONFIG_FUNCTION_TRACER
 static inline int ftrace_trace_task(struct task_struct *task)
 {
-	if (!ftrace_pid_trace)
+	if (list_empty(&ftrace_pids))
 		return 1;
 
 	return test_tsk_trace_trace(task);

commit bf7c5b43a12614847b83f507fb169ad30640e406
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Oct 12 22:31:32 2009 +0200

    tracing: Remove unused ftrace_trace_addr helper
    
    Remove the ftrace_trace_addr() function as only its off-case is
    implemented and there are no users of it currently.
    
    But we keep ftrace_graph_addr() off-case, in case someone come to use
    the function graph tracer to profit from top-level callers filtering.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 365fb19d9e11..f22a7ac32380 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -483,10 +483,6 @@ static inline int ftrace_graph_addr(unsigned long addr)
 	return 0;
 }
 #else
-static inline int ftrace_trace_addr(unsigned long addr)
-{
-	return 1;
-}
 static inline int ftrace_graph_addr(unsigned long addr)
 {
 	return 1;

commit 9288f99aa52d90a5b82573c4b769c97c55af2f56
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Wed Oct 7 19:17:45 2009 -0400

    this_cpu: Use this_cpu_xx for ftrace
    
    this_cpu_xx can reduce the instruction count here and also
    avoid address arithmetic.
    
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 405cb850b75d..542f45554883 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -413,7 +413,7 @@ extern int DYN_FTRACE_TEST_NAME(void);
 
 extern int ring_buffer_expanded;
 extern bool tracing_selftest_disabled;
-DECLARE_PER_CPU(local_t, ftrace_cpu_disabled);
+DECLARE_PER_CPU(int, ftrace_cpu_disabled);
 
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 extern int trace_selftest_startup_function(struct tracer *trace,

commit 0aa73ba1c4e1ad1d51a29e0df95ccd9f746918b6
Merge: 925936ebf35a 33974093c024
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Oct 1 11:20:33 2009 +0200

    Merge branch 'tracing/urgent' into tracing/core
    
    Merge reason: Pick up latest fixes and update to latest upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 3f6fe06dbf67b46d36fedec502300e04dffeb67a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 24 21:31:51 2009 +0200

    tracing/filters: Unify the regex parsing helpers
    
    The filter code has stolen the regex parsing function from ftrace to
    get the regex support.
    We have duplicated this code, so factorize it in the filter area and
    make it generally available, as the filter code is the most suited to
    host this feature.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8d0db6018fe4..db6b83edd49b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -709,6 +709,13 @@ typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event,
 
 typedef int (*regex_match_func)(char *str, struct regex *r, int len);
 
+enum regex_type {
+	MATCH_FULL,
+	MATCH_FRONT_ONLY,
+	MATCH_MIDDLE_ONLY,
+	MATCH_END_ONLY,
+};
+
 struct regex {
 	char			pattern[MAX_FILTER_STR_VAL];
 	int			len;
@@ -727,6 +734,8 @@ struct filter_pred {
 	int 			pop_n;
 };
 
+extern enum regex_type
+filter_parse_regex(char *buff, int len, char **search, int *not);
 extern void print_event_filter(struct ftrace_event_call *call,
 			       struct trace_seq *s);
 extern int apply_event_filter(struct ftrace_event_call *call,

commit 1889d20922d14a97b2099fa4d47587217c0ba48b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 24 21:10:44 2009 +0200

    tracing/filters: Provide basic regex support
    
    This patch provides basic support for regular expressions in filters.
    
    It supports the following types of regexp:
    
    - *match_beginning
    - *match_middle*
    - match_end*
    - !don't match
    
    Example:
            cd /debug/tracing/events/bkl/lock_kernel
            echo 'file == "*reiserfs*"' > filter
            echo 1 > enable
    
               gedit-4941  [000]   457.735437: lock_kernel: depth: 0, fs/reiserfs/namei.c:334 reiserfs_lookup()
         sync_supers-227   [001]   461.379985: lock_kernel: depth: 0, fs/reiserfs/super.c:69 reiserfs_sync_fs()
         sync_supers-227   [000]   461.383096: lock_kernel: depth: 0, fs/reiserfs/journal.c:1069 flush_commit_list()
          reiserfs/1-1369  [001]   461.479885: lock_kernel: depth: 0, fs/reiserfs/journal.c:3509 flush_async_commits()
    
    Every string is now handled as a regexp in the filter framework, which
    helps to factorize the code for handling both simple strings and
    regexp comparisons.
    
    (The regexp parsing code has been wildly cherry picked from ftrace.c
    written by Steve.)
    
    v2: Simplify the whole and drop the filter_regex file
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 86bcff94791a..8d0db6018fe4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -702,20 +702,29 @@ struct event_subsystem {
 };
 
 struct filter_pred;
+struct regex;
 
 typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event,
 				 int val1, int val2);
 
+typedef int (*regex_match_func)(char *str, struct regex *r, int len);
+
+struct regex {
+	char			pattern[MAX_FILTER_STR_VAL];
+	int			len;
+	int			field_len;
+	regex_match_func	match;
+};
+
 struct filter_pred {
-	filter_pred_fn_t fn;
-	u64 val;
-	char str_val[MAX_FILTER_STR_VAL];
-	int str_len;
-	char *field_name;
-	int offset;
-	int not;
-	int op;
-	int pop_n;
+	filter_pred_fn_t 	fn;
+	u64 			val;
+	struct regex		regex;
+	char 			*field_name;
+	int 			offset;
+	int 			not;
+	int 			op;
+	int 			pop_n;
 };
 
 extern void print_event_filter(struct ftrace_event_call *call,

commit d7a4b414eed51f1653bb05ebe84122bf9a7ae18b
Merge: 1f0ab4097646 a724eada8c2a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 23 23:08:43 2009 +0200

    Merge commit 'linus/master' into tracing/kprobes
    
    Conflicts:
            kernel/trace/Makefile
            kernel/trace/trace.h
            kernel/trace/trace_event_types.h
            kernel/trace/trace_export.c
    
    Merge reason:
            Sync with latest significant tracing core changes.

commit 6161352142d5fed4cd753b32e5ccde66e705b14e
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Thu Sep 17 16:11:28 2009 +0200

    tracing, perf: Convert the power tracer into an event tracer
    
    This patch converts the existing power tracer into an event tracer,
    so that power events (C states and frequency changes) can be
    tracked via "perf".
    
    This also removes the perl script that was used to demo the tracer;
    its functionality is being replaced entirely with timechart.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20090912130542.6d314860@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 86bcff94791a..405cb850b75d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -11,7 +11,6 @@
 #include <linux/ftrace.h>
 #include <trace/boot.h>
 #include <linux/kmemtrace.h>
-#include <trace/power.h>
 
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
@@ -37,7 +36,6 @@ enum trace_type {
 	TRACE_HW_BRANCHES,
 	TRACE_KMEM_ALLOC,
 	TRACE_KMEM_FREE,
-	TRACE_POWER,
 	TRACE_BLK,
 
 	__TRACE_LAST_TYPE,
@@ -207,7 +205,6 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,	\
 			  TRACE_GRAPH_RET);		\
 		IF_ASSIGN(var, ent, struct hw_branch_entry, TRACE_HW_BRANCHES);\
-		IF_ASSIGN(var, ent, struct trace_power, TRACE_POWER); \
 		IF_ASSIGN(var, ent, struct kmemtrace_alloc_entry,	\
 			  TRACE_KMEM_ALLOC);	\
 		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\

commit 4e5292ea1ac0c2939e815e6c44fad3d8696ea281
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Sep 12 19:26:21 2009 -0400

    tracing: use the new trace_entries.h to create format files
    
    This patch changes the way the format files in
    
      debugfs/tracing/events/ftrace/*/format
    
    are created. It uses the new trace_entries.h file to automate the
    creation of the format files to ensure that they are always in sync
    with the actual structures. This is the same methodology used to
    create the format files for the TRACE_EVENT macro.
    
    This also updates the filter creation that was built on the creation
    of the format files.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b0d287d49a6d..86bcff94791a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -7,6 +7,7 @@
 #include <linux/clocksource.h>
 #include <linux/ring_buffer.h>
 #include <linux/mmiotrace.h>
+#include <linux/tracepoint.h>
 #include <linux/ftrace.h>
 #include <trace/boot.h>
 #include <linux/kmemtrace.h>
@@ -746,11 +747,12 @@ extern struct list_head ftrace_events;
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 
-#undef TRACE_EVENT_FORMAT
-#define TRACE_EVENT_FORMAT(call, proto, args, fmt, tstruct, tpfmt)	\
+#undef FTRACE_ENTRY
+#define FTRACE_ENTRY(call, struct_name, id, tstruct, print)		\
 	extern struct ftrace_event_call event_##call;
-#undef TRACE_EVENT_FORMAT_NOFILTER
-#define TRACE_EVENT_FORMAT_NOFILTER(call, proto, args, fmt, tstruct, tpfmt)
-#include "trace_event_types.h"
+#undef FTRACE_ENTRY_DUP
+#define FTRACE_ENTRY_DUP(call, struct_name, id, tstruct, print)		\
+	FTRACE_ENTRY(call, struct_name, id, PARAMS(tstruct), PARAMS(print))
+#include "trace_entries.h"
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit d73150943cf47b6cabcb4f4e52dd25975e820ae2
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Sep 12 19:22:23 2009 -0400

    tracing: show details of structures within the ftrace structures
    
    Some of the internal ftrace structures use structures within. The
    output of a field saying it is just a structure is useless for a format
    file. A binary reader of the ring buffer needs to know more about
    how the fields are broken up.
    
    This patch adds to the ftrace structure macros new fields to
    describe the structures inside a structure.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d308195d40aa..b0d287d49a6d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -53,9 +53,18 @@ extern struct tracer boot_tracer;
 #undef __field
 #define __field(type, item)		type	item;
 
+#undef __field_struct
+#define __field_struct(type, item)	__field(type, item)
+
+#undef __field_desc
+#define __field_desc(type, container, item)
+
 #undef __array
 #define __array(type, item, size)	type	item[size];
 
+#undef __array_desc
+#define __array_desc(type, container, item, size)
+
 #undef __dynamic_array
 #define __dynamic_array(type, item)	type	item[];
 

commit 0a1c49db8d91c538f104f8d70e560c6fdd589bd4
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Sep 12 19:17:15 2009 -0400

    tracing: use macros to create internal ftrace entry ring buffer structures
    
    The entries used by ftrace internal code (plugins) currently have their
    formats manually exported to userspace. That is, the format files in
    debugfs/tracing/events/ftrace/*/format are currently created by hand.
    This is a maintenance nightmare, and can easily become out of sync
    with what is actually shown.
    
    This patch uses the methodology of the TRACE_EVENT macros to build
    the structures so that their formats can be automated and this
    will keep the structures in sync with what users can see.
    
    This patch only changes the way the structures are created. Further
    patches will build off of this to automate the format files.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4ad4e1ddcb9b..d308195d40aa 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -42,150 +42,45 @@ enum trace_type {
 	__TRACE_LAST_TYPE,
 };
 
-/*
- * Function trace entry - function address and parent function addres:
- */
-struct ftrace_entry {
-	struct trace_entry	ent;
-	unsigned long		ip;
-	unsigned long		parent_ip;
-};
-
-/* Function call entry */
-struct ftrace_graph_ent_entry {
-	struct trace_entry		ent;
-	struct ftrace_graph_ent		graph_ent;
+enum kmemtrace_type_id {
+	KMEMTRACE_TYPE_KMALLOC = 0,	/* kmalloc() or kfree(). */
+	KMEMTRACE_TYPE_CACHE,		/* kmem_cache_*(). */
+	KMEMTRACE_TYPE_PAGES,		/* __get_free_pages() and friends. */
 };
 
-/* Function return entry */
-struct ftrace_graph_ret_entry {
-	struct trace_entry		ent;
-	struct ftrace_graph_ret		ret;
-};
 extern struct tracer boot_tracer;
 
-/*
- * Context switch trace entry - which task (and prio) we switched from/to:
- */
-struct ctx_switch_entry {
-	struct trace_entry	ent;
-	unsigned int		prev_pid;
-	unsigned char		prev_prio;
-	unsigned char		prev_state;
-	unsigned int		next_pid;
-	unsigned char		next_prio;
-	unsigned char		next_state;
-	unsigned int		next_cpu;
-};
-
-/*
- * Special (free-form) trace entry:
- */
-struct special_entry {
-	struct trace_entry	ent;
-	unsigned long		arg1;
-	unsigned long		arg2;
-	unsigned long		arg3;
-};
-
-/*
- * Stack-trace entry:
- */
+#undef __field
+#define __field(type, item)		type	item;
 
-#define FTRACE_STACK_ENTRIES	8
+#undef __array
+#define __array(type, item, size)	type	item[size];
 
-struct stack_entry {
-	struct trace_entry	ent;
-	unsigned long		caller[FTRACE_STACK_ENTRIES];
-};
+#undef __dynamic_array
+#define __dynamic_array(type, item)	type	item[];
 
-struct userstack_entry {
-	struct trace_entry	ent;
-	unsigned int		tgid;
-	unsigned long		caller[FTRACE_STACK_ENTRIES];
-};
-
-/*
- * trace_printk entry:
- */
-struct bprint_entry {
-	struct trace_entry	ent;
-	unsigned long		ip;
-	const char		*fmt;
-	u32			buf[];
-};
-
-struct print_entry {
-	struct trace_entry	ent;
-	unsigned long		ip;
-	char			buf[];
-};
+#undef F_STRUCT
+#define F_STRUCT(args...)		args
 
-struct trace_mmiotrace_rw {
-	struct trace_entry	ent;
-	struct mmiotrace_rw	rw;
-};
-
-struct trace_mmiotrace_map {
-	struct trace_entry	ent;
-	struct mmiotrace_map	map;
-};
-
-struct trace_boot_call {
-	struct trace_entry	ent;
-	struct boot_trace_call boot_call;
-};
-
-struct trace_boot_ret {
-	struct trace_entry	ent;
-	struct boot_trace_ret boot_ret;
-};
-
-#define TRACE_FUNC_SIZE 30
-#define TRACE_FILE_SIZE 20
-struct trace_branch {
-	struct trace_entry	ent;
-	unsigned	        line;
-	char			func[TRACE_FUNC_SIZE+1];
-	char			file[TRACE_FILE_SIZE+1];
-	char			correct;
-};
-
-struct hw_branch_entry {
-	struct trace_entry	ent;
-	u64			from;
-	u64			to;
-};
-
-struct trace_power {
-	struct trace_entry	ent;
-	struct power_trace	state_data;
-};
+#undef FTRACE_ENTRY
+#define FTRACE_ENTRY(name, struct_name, id, tstruct, print)	\
+	struct struct_name {					\
+		struct trace_entry	ent;			\
+		tstruct						\
+	}
 
-enum kmemtrace_type_id {
-	KMEMTRACE_TYPE_KMALLOC = 0,	/* kmalloc() or kfree(). */
-	KMEMTRACE_TYPE_CACHE,		/* kmem_cache_*(). */
-	KMEMTRACE_TYPE_PAGES,		/* __get_free_pages() and friends. */
-};
+#undef TP_ARGS
+#define TP_ARGS(args...)	args
 
-struct kmemtrace_alloc_entry {
-	struct trace_entry	ent;
-	enum kmemtrace_type_id	type_id;
-	unsigned long		call_site;
-	const void		*ptr;
-	size_t			bytes_req;
-	size_t			bytes_alloc;
-	gfp_t			gfp_flags;
-	int			node;
-};
+#undef FTRACE_ENTRY_DUP
+#define FTRACE_ENTRY_DUP(name, name_struct, id, tstruct, printk)
 
-struct kmemtrace_free_entry {
-	struct trace_entry	ent;
-	enum kmemtrace_type_id	type_id;
-	unsigned long		call_site;
-	const void		*ptr;
-};
+#include "trace_entries.h"
 
+/*
+ * syscalls are special, and need special handling, this is why
+ * they are not included in trace_entries.h
+ */
 struct syscall_trace_enter {
 	struct trace_entry	ent;
 	int			nr;
@@ -198,7 +93,6 @@ struct syscall_trace_exit {
 	unsigned long		ret;
 };
 
-
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:

commit b5130b1e7d3717d03ab1916b198bf0d49fa0a619
Author: Carsten Emde <Carsten.Emde@osadl.org>
Date:   Sun Sep 13 01:43:07 2009 +0200

    tracing: do not update tracing_max_latency when tracer is stopped
    
    The state of the function pair tracing_stop()/tracing_start() is
    correctly considered when tracer data are updated. However, the global
    and externally accessible variable tracing_max_latency is always updated
    - even when tracing is stopped.
    
    The update should only occur, if tracing was not stopped.
    
    Signed-off-by: Carsten Emde <C.Emde@osadl.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 28247cecd955..4ad4e1ddcb9b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -461,6 +461,7 @@ void tracing_stop_sched_switch_record(void);
 void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
+int is_tracing_stopped(void);
 
 extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 

commit b63f39ea50330f836e301ddda21c6a93dcf0d6a3
Author: jolsa@redhat.com <jolsa@redhat.com>
Date:   Fri Sep 11 17:29:27 2009 +0200

    tracing: create generic trace parser
    
    Create a "trace_parser" that can parse the user space input for
    separate words.
    
    struct trace_parser is the descriptor.
    
    Generic "trace_get_user" function that can be a helper to read multiple
    words passed in by user space.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    LKML-Reference: <1252682969-3366-2-git-send-email-jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b69697b4b846..28247cecd955 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -615,6 +615,41 @@ static inline int ftrace_trace_task(struct task_struct *task)
 }
 #endif
 
+/*
+ * struct trace_parser - servers for reading the user input separated by spaces
+ * @cont: set if the input is not complete - no final space char was found
+ * @buffer: holds the parsed user input
+ * @idx: user input lenght
+ * @size: buffer size
+ */
+struct trace_parser {
+	bool		cont;
+	char		*buffer;
+	unsigned	idx;
+	unsigned	size;
+};
+
+static inline bool trace_parser_loaded(struct trace_parser *parser)
+{
+	return (parser->idx != 0);
+}
+
+static inline bool trace_parser_cont(struct trace_parser *parser)
+{
+	return parser->cont;
+}
+
+static inline void trace_parser_clear(struct trace_parser *parser)
+{
+	parser->cont = false;
+	parser->idx = 0;
+}
+
+extern int trace_parser_get_init(struct trace_parser *parser, int size);
+extern void trace_parser_put(struct trace_parser *parser);
+extern int trace_get_user(struct trace_parser *parser, const char __user *ubuf,
+	size_t cnt, loff_t *ppos);
+
 /*
  * trace_iterator_flags is an enumeration that defines bit
  * positions into trace_flags that controls the output.

commit 48659d31195bb76d688e99dabd816c5472fb1656
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Sep 11 11:36:23 2009 -0400

    tracing: move tgid out of generic entry and into userstack
    
    The userstack trace required the recording of the tgid entry.
    Unfortunately, it was added to the generic entry where it wasted
    4 bytes of every entry and was only used by one entry.
    
    This patch moves it out of the generic field and moves it into the
    only user (userstack_entry).
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index acaa68060ebc..b69697b4b846 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -101,6 +101,7 @@ struct stack_entry {
 
 struct userstack_entry {
 	struct trace_entry	ent;
+	unsigned int		tgid;
 	unsigned long		caller[FTRACE_STACK_ENTRIES];
 };
 

commit 8f8ffe2485bcaa890800681451d380779cea06af
Merge: 70069577323e d28daf923ac5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Sep 11 01:09:23 2009 +0200

    Merge commit 'tracing/core' into tracing/kprobes
    
    Conflicts:
            kernel/trace/trace_export.c
            kernel/trace/trace_kprobe.c
    
    Merge reason: This topic branch lacks an important
    build fix in tracing/core:
    
            0dd7b74787eaf7858c6c573353a83c3e2766e674:
            tracing: Fix double CPP substitution in TRACE_EVENT_FN
    
    that prevents from multiple tracepoint headers inclusion crashes.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit 197e2eabc90c203d1086916b7f66694ba5fbb937
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Sep 10 09:34:19 2009 +0800

    tracing: move PRED macros to trace_events_filter.c
    
    Move DEFINE_COMPARISON_PRED() and DEFINE_EQUALITY_PRED()
      to kernel/trace/trace_events_filter.c
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4AA8579B.4020706@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2163d185fe28..acaa68060ebc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -800,47 +800,6 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 	return 0;
 }
 
-#define DEFINE_COMPARISON_PRED(type)					\
-static int filter_pred_##type(struct filter_pred *pred, void *event,	\
-			      int val1, int val2)			\
-{									\
-	type *addr = (type *)(event + pred->offset);			\
-	type val = (type)pred->val;					\
-	int match = 0;							\
-									\
-	switch (pred->op) {						\
-	case OP_LT:							\
-		match = (*addr < val);					\
-		break;							\
-	case OP_LE:							\
-		match = (*addr <= val);					\
-		break;							\
-	case OP_GT:							\
-		match = (*addr > val);					\
-		break;							\
-	case OP_GE:							\
-		match = (*addr >= val);					\
-		break;							\
-	default:							\
-		break;							\
-	}								\
-									\
-	return match;							\
-}
-
-#define DEFINE_EQUALITY_PRED(size)					\
-static int filter_pred_##size(struct filter_pred *pred, void *event,	\
-			      int val1, int val2)			\
-{									\
-	u##size *addr = (u##size *)(event + pred->offset);		\
-	u##size val = (u##size)pred->val;				\
-	int match;							\
-									\
-	match = (val == *addr) ^ pred->not;				\
-									\
-	return match;							\
-}
-
 extern struct mutex event_mutex;
 extern struct list_head ftrace_events;
 

commit a5921c6c37d51ee2079ca3c69ea6f7b7384f5d87
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Sep 10 09:34:19 2009 +0800

    tracing: remove stats from struct tracer
    
    Remove unused field @stats from struct tracer.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4AA8579B.4020706@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 86a0523b8e59..2163d185fe28 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -382,7 +382,6 @@ struct tracer {
 	struct tracer		*next;
 	int			print_max;
 	struct tracer_flags	*flags;
-	struct tracer_stat	*stats;
 };
 
 

commit bd9cfca9cb71200dd82b320bba12540dc078f4e0
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Sep 10 09:34:19 2009 +0800

    tracing: format clean ups
    
    Fix white-space formatting.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4AA8579B.4020706@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 536ae1d83629..86a0523b8e59 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -169,20 +169,20 @@ enum kmemtrace_type_id {
 
 struct kmemtrace_alloc_entry {
 	struct trace_entry	ent;
-	enum kmemtrace_type_id type_id;
-	unsigned long call_site;
-	const void *ptr;
-	size_t bytes_req;
-	size_t bytes_alloc;
-	gfp_t gfp_flags;
-	int node;
+	enum kmemtrace_type_id	type_id;
+	unsigned long		call_site;
+	const void		*ptr;
+	size_t			bytes_req;
+	size_t			bytes_alloc;
+	gfp_t			gfp_flags;
+	int			node;
 };
 
 struct kmemtrace_free_entry {
 	struct trace_entry	ent;
-	enum kmemtrace_type_id type_id;
-	unsigned long call_site;
-	const void *ptr;
+	enum kmemtrace_type_id	type_id;
+	unsigned long		call_site;
+	const void		*ptr;
 };
 
 struct syscall_trace_enter {
@@ -203,7 +203,7 @@ struct syscall_trace_exit {
  * states when a trace occurs. These are:
  *  IRQS_OFF		- interrupts were disabled
  *  IRQS_NOSUPPORT	- arch does not support irqs_disabled_flags
- *  NEED_RESCED		- reschedule is requested
+ *  NEED_RESCHED	- reschedule is requested
  *  HARDIRQ		- inside an interrupt handler
  *  SOFTIRQ		- inside a softirq handler
  */

commit e0ab5f2daee1c7a6a387591bf37f0bad4e407112
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Thu Sep 10 09:34:19 2009 +0800

    tracing: remove dead code
    
    Removes unreachable code.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4AA8579B.4020706@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fa1dccb579d5..536ae1d83629 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -120,14 +120,6 @@ struct print_entry {
 	char			buf[];
 };
 
-#define TRACE_OLD_SIZE		88
-
-struct trace_field_cont {
-	unsigned char		type;
-	/* Temporary till we get rid of this completely */
-	char			buf[TRACE_OLD_SIZE - 1];
-};
-
 struct trace_mmiotrace_rw {
 	struct trace_entry	ent;
 	struct mmiotrace_rw	rw;
@@ -509,20 +501,6 @@ static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
 
 extern cycle_t ftrace_now(int cpu);
 
-#ifdef CONFIG_CONTEXT_SWITCH_TRACER
-typedef void
-(*tracer_switch_func_t)(void *private,
-			void *__rq,
-			struct task_struct *prev,
-			struct task_struct *next);
-
-struct tracer_switch_ops {
-	tracer_switch_func_t		func;
-	void				*private;
-	struct tracer_switch_ops	*next;
-};
-#endif /* CONFIG_CONTEXT_SWITCH_TRACER */
-
 extern void trace_find_cmdline(int pid, char comm[]);
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit a1922ed661ab2c1637d0b10cde933bd9cd33d965
Merge: 75e33751ca8b d28daf923ac5
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Sep 7 08:19:51 2009 +0200

    Merge branch 'tracing/core' into tracing/hw-breakpoints
    
    Conflicts:
            arch/Kconfig
            kernel/trace/trace.h
    
    Merge reason: resolve the conflicts, plus adopt to the new
                  ring-buffer APIs.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 659372d3e42a3e17a2e042d38a8bcdb94bfbe797
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Sep 3 19:11:07 2009 -0400

    tracing: add trace_array_printk for internal tracers to use
    
    This patch adds a trace_array_printk to allow a tracer to use the
    trace_printk on its own trace array.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 4d30414fe19a..fa1dccb579d5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -566,6 +566,11 @@ extern int
 trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
 extern int
 trace_vprintk(unsigned long ip, const char *fmt, va_list args);
+extern int
+trace_array_vprintk(struct trace_array *tr,
+		    unsigned long ip, const char *fmt, va_list args);
+int trace_array_printk(struct trace_array *tr,
+		       unsigned long ip, const char *fmt, ...);
 
 extern unsigned long trace_flags;
 

commit e77405ad80f53966524b5c31244e13fbbbecbd84
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Sep 2 14:17:06 2009 -0400

    tracing: pass around ring buffer instead of tracer
    
    The latency tracers (irqsoff and wakeup) can swap trace buffers
    on the fly. If an event is happening and has reserved data on one of
    the buffers, and the latency tracer swaps the global buffer with the
    max buffer, the result is that the event may commit the data to the
    wrong buffer.
    
    This patch changes the API to the trace recording to be recieve the
    buffer that was used to reserve a commit. Then this buffer can be passed
    in to the commit.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ca070de36227..4d30414fe19a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -415,12 +415,13 @@ void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 
 struct ring_buffer_event;
 
-struct ring_buffer_event *trace_buffer_lock_reserve(struct trace_array *tr,
-						    int type,
-						    unsigned long len,
-						    unsigned long flags,
-						    int pc);
-void trace_buffer_unlock_commit(struct trace_array *tr,
+struct ring_buffer_event *
+trace_buffer_lock_reserve(struct ring_buffer *buffer,
+			  int type,
+			  unsigned long len,
+			  unsigned long flags,
+			  int pc);
+void trace_buffer_unlock_commit(struct ring_buffer *buffer,
 				struct ring_buffer_event *event,
 				unsigned long flags, int pc);
 
@@ -481,10 +482,10 @@ void update_max_tr_single(struct trace_array *tr,
 #endif /* CONFIG_TRACER_MAX_TRACE */
 
 #ifdef CONFIG_STACKTRACE
-void ftrace_trace_stack(struct trace_array *tr, unsigned long flags,
+void ftrace_trace_stack(struct ring_buffer *buffer, unsigned long flags,
 			int skip, int pc);
 
-void ftrace_trace_userstack(struct trace_array *tr, unsigned long flags,
+void ftrace_trace_userstack(struct ring_buffer *buffer, unsigned long flags,
 			    int pc);
 
 void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,

commit 2f26ebd549b9ab55ac756b836ec759c11fe93f81
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Sep 1 11:06:29 2009 -0400

    tracing: use timestamp to determine start of latency traces
    
    Currently the latency tracers reset the ring buffer. Unfortunately
    if a commit is in process (due to a trace event), this can corrupt
    the ring buffer. When this happens, the ring buffer will detect
    the corruption and then permanently disable the ring buffer.
    
    The bug does not crash the system, but it does prevent further tracing
    after the bug is hit.
    
    Instead of reseting the trace buffers, the timestamp of the start of
    the trace is used instead. The buffers will still contain the previous
    data, but the output will not count any data that is before the
    timestamp of the trace.
    
    Note, this only affects the static trace output (trace) and not the
    runtime trace output (trace_pipe). The runtime trace output does not
    make sense for the latency tracers anyway.
    
    Reported-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f2af713a8bcc..ca070de36227 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -241,6 +241,7 @@ struct trace_array_cpu {
 	unsigned long		nice;
 	unsigned long		policy;
 	unsigned long		rt_priority;
+	unsigned long		skipped_entries;
 	cycle_t			preempt_timestamp;
 	pid_t			pid;
 	uid_t			uid;

commit 8248ac052dfd1eb41819fbc0ca5c7a1667e7e70c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Sep 2 12:27:41 2009 -0400

    tracing: print out start and stop in latency traces
    
    During development of the tracer, we would copy information from
    the live tracer to the max tracer with one memcpy. Since then we
    added a generic ring buffer and we handle the copies differently now.
    Unfortunately, we never copied the critical section information, and
    we lost the output:
    
     #  => started at: kmem_cache_alloc
     #  => ended at:   kmem_cache_alloc
    
    This patch adds back the critical start and end copying as well as
    removes the unused "trace_idx" and "overrun" fields of the
    trace_array_cpu structure.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e2c06b21dd82..f2af713a8bcc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -234,9 +234,6 @@ struct trace_array_cpu {
 	atomic_t		disabled;
 	void			*buffer_page;	/* ring buffer spare */
 
-	/* these fields get copied into max-trace: */
-	unsigned long		trace_idx;
-	unsigned long		overrun;
 	unsigned long		saved_latency;
 	unsigned long		critical_start;
 	unsigned long		critical_end;

commit 5d4a9dba2d7fbab69f00dedd430d1788834a055a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Aug 27 16:52:21 2009 -0400

    tracing: only show tracing_max_latency when latency tracer configured
    
    The tracing_max_latency file should only be present when one of the
    latency tracers ({preempt|irqs}off, wakeup*) are enabled.
    
    This patch also removes tracing_thresh when latency tracers are not
    enabled, as well as compiles out code that is only used for latency
    tracers.
    
    Reported-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 654fd657bd03..e2c06b21dd82 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -473,12 +473,14 @@ void unregister_tracer(struct tracer *type);
 
 extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 
+#ifdef CONFIG_TRACER_MAX_TRACE
 extern unsigned long tracing_max_latency;
 extern unsigned long tracing_thresh;
 
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
+#endif /* CONFIG_TRACER_MAX_TRACE */
 
 #ifdef CONFIG_STACKTRACE
 void ftrace_trace_stack(struct trace_array *tr, unsigned long flags,

commit ff50d99136c3315513ef3b2921e77f35ab04d081
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Aug 13 16:35:34 2009 -0400

    tracing: Kprobe tracer assigns new event ids for each event
    
    Assign new event ids for each kprobes event. This doesn't clear
    ring_buffer when unregistering each kprobe event. Thus, if you mind
    'Unknown event' messages, clear the buffer manually after changing
    kprobe events.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Przemysław Pawełczyk <przemyslaw@pawelczyk.it>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    LKML-Reference: <20090813203534.31965.49105.stgit@localhost.localdomain>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 667f832d16b7..f5362a0529eb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -38,8 +38,6 @@ enum trace_type {
 	TRACE_KMEM_FREE,
 	TRACE_POWER,
 	TRACE_BLK,
-	TRACE_KPROBE,
-	TRACE_KRETPROBE,
 
 	__TRACE_LAST_TYPE,
 };
@@ -343,10 +341,6 @@ extern void __ftrace_bad_type(void);
 			  TRACE_KMEM_ALLOC);	\
 		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
 			  TRACE_KMEM_FREE);	\
-		IF_ASSIGN(var, ent, struct kprobe_trace_entry,		\
-			  TRACE_KPROBE);				\
-		IF_ASSIGN(var, ent, struct kretprobe_trace_entry,	\
-			  TRACE_KRETPROBE);				\
 		__ftrace_bad_type();					\
 	} while (0)
 

commit 413d37d1eb69c1765b9ace0a612dac9b6c990e66
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Aug 13 16:35:11 2009 -0400

    tracing: Add kprobe-based event tracer
    
    Add kprobes-based event tracer on ftrace.
    
    This tracer is similar to the events tracer which is based on Tracepoint
    infrastructure. Instead of Tracepoint, this tracer is based on kprobes
    (kprobe and kretprobe). It probes anywhere where kprobes can probe(this
     means, all functions body except for __kprobes functions).
    
    Similar to the events tracer, this tracer doesn't need to be activated
    via current_tracer, instead of that, just set probe points via
    /sys/kernel/debug/tracing/kprobe_events. And you can set filters on each
    probe events via /sys/kernel/debug/tracing/events/kprobes/<EVENT>/filter.
    
    This tracer supports following probe arguments for each probe.
    
      %REG  : Fetch register REG
      sN    : Fetch Nth entry of stack (N >= 0)
      sa    : Fetch stack address.
      @ADDR : Fetch memory at ADDR (ADDR should be in kernel)
      @SYM[+|-offs] : Fetch memory at SYM +|- offs (SYM should be a data symbol)
      aN    : Fetch function argument. (N >= 0)
      rv    : Fetch return value.
      ra    : Fetch return address.
      +|-offs(FETCHARG) : fetch memory at FETCHARG +|- offs address.
    
    See Documentation/trace/kprobetrace.txt in the next patch for details.
    
    Changes from v13:
     - Support 'sa' for stack address.
     - Use call->data instead of container_of() macro.
    
    [fweisbec@gmail.com: Fixed conflict against latest tracing/core]
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Przemysław Pawełczyk <przemyslaw@pawelczyk.it>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    LKML-Reference: <20090813203510.31965.29123.stgit@localhost.localdomain>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 654fd657bd03..667f832d16b7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -38,6 +38,8 @@ enum trace_type {
 	TRACE_KMEM_FREE,
 	TRACE_POWER,
 	TRACE_BLK,
+	TRACE_KPROBE,
+	TRACE_KRETPROBE,
 
 	__TRACE_LAST_TYPE,
 };
@@ -205,6 +207,30 @@ struct syscall_trace_exit {
 	unsigned long		ret;
 };
 
+struct kprobe_trace_entry {
+	struct trace_entry	ent;
+	unsigned long		ip;
+	int			nargs;
+	unsigned long		args[];
+};
+
+#define SIZEOF_KPROBE_TRACE_ENTRY(n)			\
+	(offsetof(struct kprobe_trace_entry, args) +	\
+	(sizeof(unsigned long) * (n)))
+
+struct kretprobe_trace_entry {
+	struct trace_entry	ent;
+	unsigned long		func;
+	unsigned long		ret_ip;
+	int			nargs;
+	unsigned long		args[];
+};
+
+#define SIZEOF_KRETPROBE_TRACE_ENTRY(n)			\
+	(offsetof(struct kretprobe_trace_entry, args) +	\
+	(sizeof(unsigned long) * (n)))
+
+
 
 /*
  * trace_flag_type is an enumeration that holds different
@@ -317,6 +343,10 @@ extern void __ftrace_bad_type(void);
 			  TRACE_KMEM_ALLOC);	\
 		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
 			  TRACE_KMEM_FREE);	\
+		IF_ASSIGN(var, ent, struct kprobe_trace_entry,		\
+			  TRACE_KPROBE);				\
+		IF_ASSIGN(var, ent, struct kretprobe_trace_entry,	\
+			  TRACE_KRETPROBE);				\
 		__ftrace_bad_type();					\
 	} while (0)
 

commit 5079f3261ffd7fe4a537679af695f2328943a245
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Tue Aug 25 16:12:56 2009 +0800

    ftrace: Move setting of clock-source out of options
    
    There are many clock sources for the tracing system but we can only
    enable/disable one at a time with the trace/options file.
    We can move the setting of clock-source out of options and add a separate
    file for it:
     # cat trace_clock
     [local] global
     # echo global > trace_clock
     # cat trace_clock
     local [global]
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    LKML-Reference: <4A939D08.6050604@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 64dda5709cb9..654fd657bd03 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -568,6 +568,8 @@ trace_vprintk(unsigned long ip, const char *fmt, va_list args);
 
 extern unsigned long trace_flags;
 
+extern int trace_clock_id;
+
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 extern enum print_line_t print_graph_function(struct trace_iterator *iter);
@@ -656,9 +658,8 @@ enum trace_iterator_flags {
 	TRACE_ITER_PRINTK_MSGONLY	= 0x10000,
 	TRACE_ITER_CONTEXT_INFO		= 0x20000, /* Print pid/cpu/time */
 	TRACE_ITER_LATENCY_FMT		= 0x40000,
-	TRACE_ITER_GLOBAL_CLK		= 0x80000,
-	TRACE_ITER_SLEEP_TIME		= 0x100000,
-	TRACE_ITER_GRAPH_TIME		= 0x200000,
+	TRACE_ITER_SLEEP_TIME		= 0x80000,
+	TRACE_ITER_GRAPH_TIME		= 0x100000,
 };
 
 /*

commit aa38e9fc3ea804290efd3a39316d7f7e6c945800
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Aug 7 10:33:02 2009 +0800

    tracing/filters: Add filter_type to struct ftrace_event_field
    
    The type of a field is stored as a string in @type, and here
    we add @filter_type which is an enum value.
    
    This prepares for later patches, so we can specifically assign
    different @filter_type for the same @type.
    
    For example normally a "char *" field is treated as a ptr,
    but we may want it to be treated as a string when doing filting.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4A7B925E.9030605@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 300ef788c976..64dda5709cb9 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -755,6 +755,7 @@ struct ftrace_event_field {
 	struct list_head	link;
 	char			*name;
 	char			*type;
+	int			filter_type;
 	int			offset;
 	int			size;
 	int			is_signed;
@@ -800,6 +801,7 @@ extern int apply_subsystem_event_filter(struct event_subsystem *system,
 					char *filter_string);
 extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);
+extern int filter_assign_type(const char *type);
 
 static inline int
 filter_check_discard(struct ftrace_event_call *call, void *rec,

commit 64c12e0444fcc6b75eb49144ba46d43dbdc6bc8f
Author: Jason Baron <jbaron@redhat.com>
Date:   Mon Aug 10 16:52:53 2009 -0400

    tracing: Add individual syscalls tracepoint id support
    
    The current state of syscalls tracepoints generates only one event id
    for every syscall events.
    
    This patch associates an id with each syscall trace event, so that we
    can identify each syscall trace event using the 'perf' tool.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d682357e4b1f..300ef788c976 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -34,8 +34,6 @@ enum trace_type {
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
 	TRACE_HW_BRANCHES,
-	TRACE_SYSCALL_ENTER,
-	TRACE_SYSCALL_EXIT,
 	TRACE_KMEM_ALLOC,
 	TRACE_KMEM_FREE,
 	TRACE_POWER,
@@ -319,10 +317,6 @@ extern void __ftrace_bad_type(void);
 			  TRACE_KMEM_ALLOC);	\
 		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
 			  TRACE_KMEM_FREE);	\
-		IF_ASSIGN(var, ent, struct syscall_trace_enter,		\
-			  TRACE_SYSCALL_ENTER);				\
-		IF_ASSIGN(var, ent, struct syscall_trace_exit,		\
-			  TRACE_SYSCALL_EXIT);				\
 		__ftrace_bad_type();					\
 	} while (0)
 

commit 89034bc2c7b839702c00a704e79d112737f98be0
Merge: fb82ad719831 85dfd81dc57e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Aug 11 14:19:09 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Conflicts:
            kernel/trace/trace_events_filter.c
    
    We use the tracing/core version.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f413cdb80ce00ec1a4d0ab949b5d96c81cae7f75
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Aug 7 01:25:54 2009 +0200

    perf_counter: Fix/complete ftrace event records sampling
    
    This patch implements the kernel side support for ftrace event
    record sampling.
    
    A new counter sampling attribute is added:
    
       PERF_SAMPLE_TP_RECORD
    
    which requests ftrace events record sampling. In this case
    if a PERF_TYPE_TRACEPOINT counter is active and a tracepoint
    fires, we emit the tracepoint binary record to the
    perfcounter event buffer, as a sample.
    
    Result, after setting PERF_SAMPLE_TP_RECORD attribute from perf
    record:
    
     perf record -f -F 1 -a -e workqueue:workqueue_execution
     perf report -D
    
     0x21e18 [0x48]: event: 9
     .
     . ... raw event: size 72 bytes
     .  0000:  09 00 00 00 01 00 48 00 d0 c7 00 81 ff ff ff ff  ......H........
     .  0010:  0a 00 00 00 0a 00 00 00 21 00 00 00 00 00 00 00  ........!......
     .  0020:  2b 00 01 02 0a 00 00 00 0a 00 00 00 65 76 65 6e  +...........eve
     .  0030:  74 73 2f 31 00 00 00 00 00 00 00 00 0a 00 00 00  ts/1...........
     .  0040:  e0 b1 31 81 ff ff ff ff                          .......
    .
    0x21e18 [0x48]: PERF_EVENT_SAMPLE (IP, 1): 10: 0xffffffff8100c7d0 period: 33
    
    The raw ftrace binary record starts at offset 0020.
    
    Translation:
    
     struct trace_entry {
            type            = 0x2b = 43;
            flags           = 1;
            preempt_count   = 2;
            pid             = 0xa = 10;
            tgid            = 0xa = 10;
     }
    
     thread_comm = "events/1"
     thread_pid  = 0xa = 10;
     func       = 0xffffffff8131b1e0 = flush_to_ldisc()
    
    What will come next?
    
     - Userspace support ('perf trace'), 'flight data recorder' mode
       for perf trace, etc.
    
     - The unconditional copy from the profiling callback brings
       some costs however if someone wants no such sampling to
       occur, and needs to be fixed in the future. For that we need
       to have an instant access to the perf counter attribute.
       This is a matter of a flag to add in the struct ftrace_event.
    
     - Take care of the events recursivity! Don't ever try to record
       a lock event for example, it seems some locking is used in
       the profiling fast path and lead to a tracing recursivity.
       That will be fixed using raw spinlock or recursivity
       protection.
    
     - [...]
    
     - Profit! :-)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3548ae5cc780..8b9f4f6e9559 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -438,10 +438,6 @@ struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 struct trace_entry *trace_find_next_entry(struct trace_iterator *iter,
 					  int *ent_cpu, u64 *ent_ts);
 
-void tracing_generic_entry_update(struct trace_entry *entry,
-				  unsigned long flags,
-				  int pc);
-
 void default_wait_pipe(struct trace_iterator *iter);
 void poll_wait_pipe(struct trace_iterator *iter);
 

commit 1a0799a8fef5acc6503f9c5e79b2cd003317826c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 29 18:59:58 2009 +0200

    tracing/function-graph-tracer: Move graph event insertion helpers in the graph tracer file
    
    The function graph events helpers which insert the function entry and
    return events into the ring buffer currently reside in trace.c
    But this file is quite overloaded and the right place for these helpers
    is in the function graph tracer file.
    
    Then move them to trace_functions_graph.c
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 116524d62366..9301f1263c5c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -471,6 +471,7 @@ void trace_function(struct trace_array *tr,
 
 void trace_graph_return(struct ftrace_graph_ret *trace);
 int trace_graph_entry(struct ftrace_graph_ent *trace);
+void set_graph_array(struct trace_array *tr);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);

commit c0a0d0d3f65284c71115a9bb1ed801ee33eeb552
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 29 17:51:13 2009 +0200

    tracing/core: Make the stack entry helpers global
    
    Make the stacktrace event insertion helpers globals.
    This has two effects:
    
    - Prepare for moving the sched events insertion helpers to
      the sched switch tracer file.
    - Move some ifdef outside function definitions
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c7e92732982d..116524d62366 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -489,9 +489,31 @@ void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
 
-void __trace_stack(struct trace_array *tr,
-		   unsigned long flags,
-		   int skip, int pc);
+#ifdef CONFIG_STACKTRACE
+void ftrace_trace_stack(struct trace_array *tr, unsigned long flags,
+			int skip, int pc);
+
+void ftrace_trace_userstack(struct trace_array *tr, unsigned long flags,
+			    int pc);
+
+void __trace_stack(struct trace_array *tr, unsigned long flags, int skip,
+		   int pc);
+#else
+static inline void ftrace_trace_stack(struct trace_array *tr,
+				      unsigned long flags, int skip, int pc)
+{
+}
+
+static inline void ftrace_trace_userstack(struct trace_array *tr,
+					  unsigned long flags, int pc)
+{
+}
+
+static inline void __trace_stack(struct trace_array *tr, unsigned long flags,
+				 int skip, int pc)
+{
+}
+#endif /* CONFIG_STACKTRACE */
 
 extern cycle_t ftrace_now(int cpu);
 

commit 5e5bf483986ad86ad25f25eec5299c86eb2d1c57
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Jul 29 17:11:12 2009 +0200

    tracing/core: Turn ftrace_cpu_disabled into a global var
    
    In order to prepare the moving of the function graph tracer insertion
    helpers from trace.c to trace_functions_graph.c, we need to export the
    ftrace_cpu_disabled variable.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 758b0dbed552..c7e92732982d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -519,6 +519,7 @@ extern int DYN_FTRACE_TEST_NAME(void);
 
 extern int ring_buffer_expanded;
 extern bool tracing_selftest_disabled;
+DECLARE_PER_CPU(local_t, ftrace_cpu_disabled);
 
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 extern int trace_selftest_startup_function(struct tracer *trace,

commit 1f9963cbb0280e0cd554161e00f1a0eeddbf1ae1
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon Jul 20 10:20:53 2009 +0800

    tracing/filters: improve subsystem filter
    
    Currently a subsystem filter should be applicable to all events
    under the subsystem, and if it failed, all the event filters
    will be cleared. Those behaviors make subsys filter much less
    useful:
    
      # echo 'vec == 1' > irq/softirq_entry/filter
      # echo 'irq == 5' > irq/filter
      bash: echo: write error: Invalid argument
      # cat irq/softirq_entry/filter
      none
    
    I'd expect it set the filter for irq_handler_entry/exit, and
    not touch softirq_entry/exit.
    
    The basic idea is, try to see if the filter can be applied
    to which events, and then just apply to the those events:
    
      # echo 'vec == 1' > softirq_entry/filter
      # echo 'irq == 5' > filter
      # cat irq_handler_entry/filter
      irq == 5
      # cat softirq_entry/filter
      vec == 1
    
    Changelog for v2:
    - do some cleanups to address Frederic's comments.
    
    Inspired-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A63D485.7030703@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 94305c7bc11c..758b0dbed552 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -750,13 +750,14 @@ struct event_filter {
 	int			n_preds;
 	struct filter_pred	**preds;
 	char			*filter_string;
+	bool			no_reset;
 };
 
 struct event_subsystem {
 	struct list_head	list;
 	const char		*name;
 	struct dentry		*entry;
-	void			*filter;
+	struct event_filter	*filter;
 	int			nr_events;
 };
 

commit db59504d89db1462a5281fb55b1d962cb74a398f
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Jul 7 13:52:36 2009 +0800

    ksym_tracer: Extract trace entry from struct trace_ksym
    
    struct trace_ksym is used as an entry in hbp list, and is also
    used as trace_entry stored in ring buffer.
    
    This is not necessary and is a waste of memory in ring buffer.
    
    There is also a bug that dereferencing field->ksym_hbp in
    ksym_trace_output() can be invalid.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: "K.Prasad" <prasad@linux.vnet.ibm.com>
    Cc: Alan Stern <stern@rowland.harvard.edu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <4A52E2A4.4050007@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7d5cc37b8fca..ff1ef411a176 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -215,17 +215,12 @@ struct syscall_trace_exit {
 #define KSYM_SELFTEST_ENTRY "ksym_selftest_dummy"
 extern int process_new_ksym_entry(char *ksymname, int op, unsigned long addr);
 
-struct trace_ksym {
+struct ksym_trace_entry {
 	struct trace_entry	ent;
-	struct hw_breakpoint	*ksym_hbp;
-	unsigned long		ksym_addr;
 	unsigned long		ip;
-#ifdef CONFIG_PROFILE_KSYM_TRACER
-	unsigned long		counter;
-#endif
-	struct hlist_node	ksym_hlist;
+	unsigned char		type;
 	char			ksym_name[KSYM_NAME_LEN];
-	char			p_name[TASK_COMM_LEN];
+	char			cmd[TASK_COMM_LEN];
 };
 
 /*
@@ -343,7 +338,7 @@ extern void __ftrace_bad_type(void);
 			  TRACE_SYSCALL_ENTER);				\
 		IF_ASSIGN(var, ent, struct syscall_trace_exit,		\
 			  TRACE_SYSCALL_EXIT);				\
-		IF_ASSIGN(var, ent, struct trace_ksym, TRACE_KSYM);	\
+		IF_ASSIGN(var, ent, struct ksym_trace_entry, TRACE_KSYM);\
 		__ftrace_bad_type();					\
 	} while (0)
 

commit dc82ec98a4727fd51b77e92d05fe7d2db3dcc11c
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Jul 9 16:22:22 2009 +0800

    tracing/filter: Remove empty subsystem and its directory
    
    Remove empty subsystem and its directory when module unload.
    
    Before patch:
     # rmmod trace-events-sample.ko
     # ls sample
     enable  filter
    
    After patch:
     # rmmod trace-events-sample.ko
     # ls sample
     ls: cannot access sample: No such file or directory
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Acked-by: Tom Zanussi <tzanussi@gmail.com>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <4A55A8BE.9010707@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 52eb0d8dcd75..94305c7bc11c 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -757,6 +757,7 @@ struct event_subsystem {
 	const char		*name;
 	struct dentry		*entry;
 	void			*filter;
+	int			nr_events;
 };
 
 struct filter_pred;

commit 020e5f85cb087a40572c8b8b2dd06292a14fa212
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Jul 1 10:47:05 2009 +0800

    tracing/events: Add trace_event boot option
    
    We already have ftrace= boot option, and this adds a similar
    boot option for trace events, so allow trace events to be
    enabled at boot, for boot debugging purpose.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A4ACE29.3010407@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3548ae5cc780..52eb0d8dcd75 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -517,6 +517,9 @@ extern unsigned long ftrace_update_tot_cnt;
 extern int DYN_FTRACE_TEST_NAME(void);
 #endif
 
+extern int ring_buffer_expanded;
+extern bool tracing_selftest_disabled;
+
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 extern int trace_selftest_startup_function(struct tracer *trace,
 					   struct trace_array *tr);

commit 1155de47cd66d0c496d5a6fb2223e980ef1285b2
Author: Paul Mundt <lethal@linux-sh.org>
Date:   Thu Jun 25 14:30:12 2009 +0900

    ring-buffer: Make it generally available
    
    In hunting down the cause for the hwlat_detector ring buffer spew in
    my failed -next builds it became obvious that folks are now treating
    ring_buffer as something that is generic independent of tracing and thus,
    suitable for public driver consumption.
    
    Given that there are only a few minor areas in ring_buffer that have any
    reliance on CONFIG_TRACING or CONFIG_FUNCTION_TRACER, provide stubs for
    those and make it generally available.
    
    Signed-off-by: Paul Mundt <lethal@linux-sh.org>
    Cc: Jon Masters <jcm@jonmasters.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20090625053012.GB19944@linux-sh.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6e735d4771f8..3548ae5cc780 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -597,6 +597,7 @@ print_graph_function(struct trace_iterator *iter)
 
 extern struct pid *ftrace_pid_trace;
 
+#ifdef CONFIG_FUNCTION_TRACER
 static inline int ftrace_trace_task(struct task_struct *task)
 {
 	if (!ftrace_pid_trace)
@@ -604,6 +605,12 @@ static inline int ftrace_trace_task(struct task_struct *task)
 
 	return test_tsk_trace_trace(task);
 }
+#else
+static inline int ftrace_trace_task(struct task_struct *task)
+{
+	return 1;
+}
+#endif
 
 /*
  * trace_iterator_flags is an enumeration that defines bit

commit 0722db015c246204044299eae3b02d18d3ca4faf
Author: K.Prasad <prasad@linux.vnet.ibm.com>
Date:   Mon Jun 1 23:46:40 2009 +0530

    hw-breakpoints: ftrace plugin for kernel symbol tracing using HW Breakpoint interfaces
    
    This patch adds an ftrace plugin to detect and profile memory access over kernel
    variables. It uses HW Breakpoint interfaces to 'watch memory addresses.
    
    Signed-off-by: K.Prasad <prasad@linux.vnet.ibm.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6e735d4771f8..7d5cc37b8fca 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -15,6 +15,10 @@
 #include <linux/trace_seq.h>
 #include <linux/ftrace_event.h>
 
+#ifdef CONFIG_KSYM_TRACER
+#include <asm/hw_breakpoint.h>
+#endif
+
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
 
@@ -40,6 +44,7 @@ enum trace_type {
 	TRACE_KMEM_FREE,
 	TRACE_POWER,
 	TRACE_BLK,
+	TRACE_KSYM,
 
 	__TRACE_LAST_TYPE,
 };
@@ -207,6 +212,21 @@ struct syscall_trace_exit {
 	unsigned long		ret;
 };
 
+#define KSYM_SELFTEST_ENTRY "ksym_selftest_dummy"
+extern int process_new_ksym_entry(char *ksymname, int op, unsigned long addr);
+
+struct trace_ksym {
+	struct trace_entry	ent;
+	struct hw_breakpoint	*ksym_hbp;
+	unsigned long		ksym_addr;
+	unsigned long		ip;
+#ifdef CONFIG_PROFILE_KSYM_TRACER
+	unsigned long		counter;
+#endif
+	struct hlist_node	ksym_hlist;
+	char			ksym_name[KSYM_NAME_LEN];
+	char			p_name[TASK_COMM_LEN];
+};
 
 /*
  * trace_flag_type is an enumeration that holds different
@@ -323,6 +343,7 @@ extern void __ftrace_bad_type(void);
 			  TRACE_SYSCALL_ENTER);				\
 		IF_ASSIGN(var, ent, struct syscall_trace_exit,		\
 			  TRACE_SYSCALL_EXIT);				\
+		IF_ASSIGN(var, ent, struct trace_ksym, TRACE_KSYM);	\
 		__ftrace_bad_type();					\
 	} while (0)
 
@@ -540,6 +561,8 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 					 struct trace_array *tr);
 extern int trace_selftest_startup_hw_branches(struct tracer *trace,
 					      struct trace_array *tr);
+extern int trace_selftest_startup_ksym(struct tracer *trace,
+					 struct trace_array *tr);
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);

commit 0ad5d703c6c0fcd385d956555460df95dff7eb7e
Merge: 44347d947f62 1cb81b143fa8
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu May 7 11:18:34 2009 +0200

    Merge branch 'tracing/hw-branch-tracing' into tracing/core
    
    Merge reason: this topic is ready for upstream now. It passed
                  Oleg's review and Andrew had no further mm/*
                  objections/observations either.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9456f0fa6d3cb944d3b9fc31c9a244e0362c26ea
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 6 21:54:09 2009 -0400

    tracing: reset ring buffer when removing modules with events
    
    Li Zefan found that there's a race using the event ids of events and
    modules. When a module is loaded, an event id is incremented. We only
    have 16 bits for event ids (65536) and there is a possible (but highly
    unlikely) race that we could load and unload a module that registers
    events so many times that the event id counter overflows.
    
    When it overflows, it then restarts and goes looking for available
    ids. An id is available if it was added by a module and released.
    
    The race is if you have one module add an id, and then is removed.
    Another module loaded can use that same event id. But if the old module
    still had events in the ring buffer, the new module's call back would
    get bogus data.  At best (and most likely) the output would just be
    garbage. But if the module for some reason used pointers (not recommended)
    then this could potentially crash.
    
    The safest thing to do is just reset the ring buffer if a module that
    registered events is removed.
    
    [ Impact: prevent unpredictable results of event id overflows ]
    
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <49FEAFD0.30106@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 777c6c3a0cde..ba25793ffe67 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -409,6 +409,8 @@ int tracing_is_enabled(void);
 void trace_wake_up(void);
 void tracing_reset(struct trace_array *tr, int cpu);
 void tracing_reset_online_cpus(struct trace_array *tr);
+void tracing_reset_current(int cpu);
+void tracing_reset_current_online_cpus(void);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *trace_create_file(const char *name,
 				 mode_t mode,

commit 20c8928abe70e204bd077ab6cfe23002d7788983
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed May 6 10:33:45 2009 +0800

    tracing/events: fix concurrent access to ftrace_events list
    
    A module will add/remove its trace events when it gets loaded/unloaded, so
    the ftrace_events list is not "const", and concurrent access needs to be
    protected.
    
    This patch thus fixes races between loading/unloding modules and read
    'available_events' or read/write 'set_event', etc.
    
    Below shows how to reproduce the race:
    
     # for ((; ;)) { cat /mnt/tracing/available_events; } > /dev/null &
     # for ((; ;)) { insmod trace-events-sample.ko; rmmod sample; } &
    
    After a while:
    
    BUG: unable to handle kernel paging request at 0010011c
    IP: [<c1080f27>] t_next+0x1b/0x2d
    ...
    Call Trace:
     [<c10c90e6>] ? seq_read+0x217/0x30d
     [<c10c8ecf>] ? seq_read+0x0/0x30d
     [<c10b4c19>] ? vfs_read+0x8f/0x136
     [<c10b4fc3>] ? sys_read+0x40/0x65
     [<c1002a68>] ? sysenter_do_call+0x12/0x36
    
    [ Impact: fix races when concurrent accessing ftrace_events list ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4A00F709.3080800@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7736fe8c1b76..777c6c3a0cde 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -825,6 +825,7 @@ static int filter_pred_##size(struct filter_pred *pred, void *event,	\
 	return match;							\
 }
 
+extern struct mutex event_mutex;
 extern struct list_head ftrace_events;
 
 extern const char *__start___trace_bprintk_fmt[];

commit 8b3725621074040d380664964ffbc40610aef8c6
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Apr 28 03:04:59 2009 -0500

    tracing/filters: a better event parser
    
    Replace the current event parser hack with a better one.  Filters are
    no longer specified predicate by predicate, but all at once and can
    use parens and any of the following operators:
    
    numeric fields:
    
    ==, !=, <, <=, >, >=
    
    string fields:
    
    ==, !=
    
    predicates can be combined with the logical operators:
    
    &&, ||
    
    examples:
    
    "common_preempt_count > 4" > filter
    
    "((sig >= 10 && sig < 15) || sig == 17) && comm != bash" > filter
    
    If there was an error, the erroneous string along with an error
    message can be seen by looking at the filter e.g.:
    
    ((sig >= 10 && sig < 15) || dsig == 17) && comm != bash
    ^
    parse_error: Field not found
    
    Currently the caret for an error always appears at the beginning of
    the filter; a real position should be used, but the error message
    should be useful even without it.
    
    To clear a filter, '0' can be written to the filter file.
    
    Filters can also be set or cleared for a complete subsystem by writing
    the same filter as would be written to an individual event to the
    filter file at the root of the subsytem.  Note however, that if any
    event in the subsystem lacks a field specified in the filter being
    set, the set will fail and all filters in the subsytem are
    automatically cleared.  This change from the previous version was made
    because using only the fields that happen to exist for a given event
    would most likely result in a meaningless filter.
    
    Because the logical operators are now implemented as predicates, the
    maximum number of predicates in a filter was increased from 8 to 16.
    
    [ Impact: add new, extended trace-filter implementation ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <1240905899.6416.121.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 866d0108fd2f..7736fe8c1b76 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -735,6 +735,7 @@ struct ftrace_event_field {
 struct event_filter {
 	int			n_preds;
 	struct filter_pred	**preds;
+	char			*filter_string;
 };
 
 struct event_subsystem {
@@ -746,7 +747,8 @@ struct event_subsystem {
 
 struct filter_pred;
 
-typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event);
+typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event,
+				 int val1, int val2);
 
 struct filter_pred {
 	filter_pred_fn_t fn;
@@ -756,23 +758,18 @@ struct filter_pred {
 	char *field_name;
 	int offset;
 	int not;
-	int or;
-	int compound;
-	int clear;
+	int op;
+	int pop_n;
 };
 
-extern void filter_free_pred(struct filter_pred *pred);
-extern void filter_print_preds(struct ftrace_event_call *call,
+extern void print_event_filter(struct ftrace_event_call *call,
 			       struct trace_seq *s);
-extern int filter_parse(char **pbuf, struct filter_pred *pred);
-extern int filter_add_pred(struct ftrace_event_call *call,
-			   struct filter_pred *pred);
-extern void filter_disable_preds(struct ftrace_event_call *call);
-extern void filter_free_subsystem_preds(struct event_subsystem *system);
-extern void filter_print_subsystem_preds(struct event_subsystem *system,
+extern int apply_event_filter(struct ftrace_event_call *call,
+			      char *filter_string);
+extern int apply_subsystem_event_filter(struct event_subsystem *system,
+					char *filter_string);
+extern void print_subsystem_event_filter(struct event_subsystem *system,
 					 struct trace_seq *s);
-extern int filter_add_subsystem_pred(struct event_subsystem *system,
-				     struct filter_pred *pred);
 
 static inline int
 filter_check_discard(struct ftrace_event_call *call, void *rec,
@@ -787,6 +784,47 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 	return 0;
 }
 
+#define DEFINE_COMPARISON_PRED(type)					\
+static int filter_pred_##type(struct filter_pred *pred, void *event,	\
+			      int val1, int val2)			\
+{									\
+	type *addr = (type *)(event + pred->offset);			\
+	type val = (type)pred->val;					\
+	int match = 0;							\
+									\
+	switch (pred->op) {						\
+	case OP_LT:							\
+		match = (*addr < val);					\
+		break;							\
+	case OP_LE:							\
+		match = (*addr <= val);					\
+		break;							\
+	case OP_GT:							\
+		match = (*addr > val);					\
+		break;							\
+	case OP_GE:							\
+		match = (*addr >= val);					\
+		break;							\
+	default:							\
+		break;							\
+	}								\
+									\
+	return match;							\
+}
+
+#define DEFINE_EQUALITY_PRED(size)					\
+static int filter_pred_##size(struct filter_pred *pred, void *event,	\
+			      int val1, int val2)			\
+{									\
+	u##size *addr = (u##size *)(event + pred->offset);		\
+	u##size val = (u##size)pred->val;				\
+	int match;							\
+									\
+	match = (val == *addr) ^ pred->not;				\
+									\
+	return match;							\
+}
+
 extern struct list_head ftrace_events;
 
 extern const char *__start___trace_bprintk_fmt[];

commit a118e4d1402f1349fe3d953493e4168a300a752d
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Apr 28 03:04:53 2009 -0500

    tracing/filters: distinguish between signed and unsigned fields
    
    The new filter comparison ops need to be able to distinguish between
    signed and unsigned field types, so add an is_signed flag/param to the
    event field struct/trace_define_fields().  Also define a simple macro,
    is_signed_type() to determine the signedness at compile time, used in the
    trace macros.  If the is_signed_type() macro won't work with a specific
    type, a new slightly modified version of TRACE_FIELD() called
    TRACE_FIELD_SIGN(), allows the signedness to be set explicitly.
    
    [ Impact: extend trace-filter code for new feature ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <1240905893.6416.120.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1fb7d6ccadf4..866d0108fd2f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -729,6 +729,7 @@ struct ftrace_event_field {
 	char			*type;
 	int			offset;
 	int			size;
+	int			is_signed;
 };
 
 struct event_filter {

commit 30e673b230f9d556eb81ef68a7b1a08c8b3b142c
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Apr 28 03:04:47 2009 -0500

    tracing/filters: move preds into event_filter object
    
    Create a new event_filter object, and move the pred-related members
    out of the call and subsystem objects and into the filter object - the
    details of the filter implementation don't need to be exposed in the
    call and subsystem in any case, and it will also help make the new
    parser implementation a little cleaner.
    
    [ Impact: refactor trace-filter code to prepare for new features ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <1240905887.6416.119.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7d55bcf50e49..1fb7d6ccadf4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -731,12 +731,16 @@ struct ftrace_event_field {
 	int			size;
 };
 
+struct event_filter {
+	int			n_preds;
+	struct filter_pred	**preds;
+};
+
 struct event_subsystem {
 	struct list_head	list;
 	const char		*name;
 	struct dentry		*entry;
-	int			n_preds;
-	struct filter_pred	**preds;
+	void			*filter;
 };
 
 struct filter_pred;
@@ -774,7 +778,7 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 		     struct ring_buffer *buffer,
 		     struct ring_buffer_event *event)
 {
-	if (unlikely(call->n_preds) && !filter_match_preds(call, rec)) {
+	if (unlikely(call->filter_active) && !filter_match_preds(call, rec)) {
 		ring_buffer_discard_commit(buffer, event);
 		return 1;
 	}

commit 416dfdcdb894432547ead4fcb9fa6a36b396059e
Merge: 56449f437add 091069740304
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Apr 24 10:11:18 2009 +0200

    Merge commit 'v2.6.30-rc3' into tracing/hw-branch-tracing
    
    Conflicts:
            arch/x86/kernel/ptrace.c
    
    Merge reason: fix the conflict above, and also pick up the CONFIG_BROKEN
                  dependency change from upstream so that we can remove it
                  here.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 7a4f453b6d7379a7c380825949977c5a838aa012
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Apr 22 16:53:34 2009 +0800

    tracing/events: make struct trace_entry->type to be int type
    
    struct trace_entry->type is unsigned char, while trace event's id is
    int type, thus for a event with id >= 256, it's entry->type is cast
    to (id % 256), and then we can't see the trace output of this event.
    
     # insmod trace-events-sample.ko
     # echo foo_bar > /mnt/tracing/set_event
     # cat /debug/tracing/events/trace-events-sample/foo_bar/id
     256
     # cat /mnt/tracing/trace_pipe
               <...>-3548  [001]   215.091142: Unknown type 0
               <...>-3548  [001]   216.089207: Unknown type 0
               <...>-3548  [001]   217.087271: Unknown type 0
               <...>-3548  [001]   218.085332: Unknown type 0
    
    [ Impact: fix output for trace events with id >= 256 ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <49EEDB0E.5070207@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 247948e81b08..7d55bcf50e49 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -422,7 +422,7 @@ void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 struct ring_buffer_event;
 
 struct ring_buffer_event *trace_buffer_lock_reserve(struct trace_array *tr,
-						    unsigned char type,
+						    int type,
 						    unsigned long len,
 						    unsigned long flags,
 						    int pc);

commit ac1adc55fc71c7515caa2eb0e63e49b3d1c6a47c
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Fri Apr 17 00:27:08 2009 -0500

    tracing/filters: add filter_mutex to protect filter predicates
    
    This patch adds a filter_mutex to prevent the filter predicates from
    being accessed concurrently by various external functions.
    
    It's based on a previous patch by Li Zefan:
            "[PATCH 7/7] tracing/filters: make filter preds RCU safe"
    
    v2 changes:
    
    - fixed wrong value returned in a add_subsystem_pred() failure case
      noticed by Li Zefan.
    
    [ Impact: fix trace filter corruption/crashes on parallel access ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Tested-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: paulmck@linux.vnet.ibm.com
    LKML-Reference: <1239946028.6639.13.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8817c18ef97a..247948e81b08 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -757,13 +757,15 @@ struct filter_pred {
 };
 
 extern void filter_free_pred(struct filter_pred *pred);
-extern void filter_print_preds(struct filter_pred **preds, int n_preds,
+extern void filter_print_preds(struct ftrace_event_call *call,
 			       struct trace_seq *s);
 extern int filter_parse(char **pbuf, struct filter_pred *pred);
 extern int filter_add_pred(struct ftrace_event_call *call,
 			   struct filter_pred *pred);
 extern void filter_disable_preds(struct ftrace_event_call *call);
 extern void filter_free_subsystem_preds(struct event_subsystem *system);
+extern void filter_print_subsystem_preds(struct event_subsystem *system,
+					 struct trace_seq *s);
 extern int filter_add_subsystem_pred(struct event_subsystem *system,
 				     struct filter_pred *pred);
 

commit a59fd6027218bd7c994e39d14afe0242f895144f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 13:52:20 2009 -0400

    tracing/events: convert event call sites to use a link list
    
    Impact: makes it possible to define events in modules
    
    The events are created by reading down the section that they are linked
    in by the macros. But this is not scalable to modules. This patch converts
    the manipulations to use a global link list, and on boot up it adds
    the items in the section to the list.
    
    This change will allow modules to add their tracing events to the list as
    well.
    
    Note, this change alone does not permit modules to use the TRACE_EVENT macros,
    but the change is needed for them to eventually do so.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6bcdf4af9b2d..8817c18ef97a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -739,11 +739,6 @@ struct event_subsystem {
 	struct filter_pred	**preds;
 };
 
-#define events_for_each(event)						\
-	for (event = __start_ftrace_events;				\
-	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
-	     event++)
-
 struct filter_pred;
 
 typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event);
@@ -785,13 +780,7 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 	return 0;
 }
 
-extern struct ftrace_event_call __start_ftrace_events[];
-extern struct ftrace_event_call __stop_ftrace_events[];
-
-#define for_each_event(event)						\
-	for (event = __start_ftrace_events;				\
-	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
-	     event++)
+extern struct list_head ftrace_events;
 
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];

commit 97f2025153499faa17267a0d4e18c7afaf73f39d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Apr 13 11:20:49 2009 -0400

    tracing/events: move declarations from trace directory to core include
    
    In preparation to allowing trace events to happen in modules, we need
    to move some of the local declarations in the kernel/trace directory
    into include/linux.
    
    This patch simply moves the declarations and performs no context changes.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1882846b7389..6bcdf4af9b2d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -13,6 +13,7 @@
 #include <trace/power.h>
 
 #include <linux/trace_seq.h>
+#include <linux/ftrace_event.h>
 
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
@@ -43,20 +44,6 @@ enum trace_type {
 	__TRACE_LAST_TYPE,
 };
 
-/*
- * The trace entry - the most basic unit of tracing. This is what
- * is printed in the end as a single line in the trace output, such as:
- *
- *     bash-15816 [01]   235.197585: idle_cpu <- irq_enter
- */
-struct trace_entry {
-	unsigned char		type;
-	unsigned char		flags;
-	unsigned char		preempt_count;
-	int			pid;
-	int			tgid;
-};
-
 /*
  * Function trace entry - function address and parent function addres:
  */
@@ -265,8 +252,6 @@ struct trace_array_cpu {
 	char			comm[TASK_COMM_LEN];
 };
 
-struct trace_iterator;
-
 /*
  * The trace array - an array of per-CPU trace arrays. This is the
  * highest level data structure that individual tracers deal with.
@@ -341,15 +326,6 @@ extern void __ftrace_bad_type(void);
 		__ftrace_bad_type();					\
 	} while (0)
 
-/* Return values for print_line callback */
-enum print_line_t {
-	TRACE_TYPE_PARTIAL_LINE	= 0,	/* Retry after flushing the seq */
-	TRACE_TYPE_HANDLED	= 1,
-	TRACE_TYPE_UNHANDLED	= 2,	/* Relay to other output functions */
-	TRACE_TYPE_NO_CONSUME	= 3	/* Handled but ask to not consume */
-};
-
-
 /*
  * An option specific to a tracer. This is a boolean value.
  * The bit is the bit index that sets its value on the
@@ -428,31 +404,6 @@ struct tracer {
 
 #define TRACE_PIPE_ALL_CPU	-1
 
-/*
- * Trace iterator - used by printout routines who present trace
- * results to users and which routines might sleep, etc:
- */
-struct trace_iterator {
-	struct trace_array	*tr;
-	struct tracer		*trace;
-	void			*private;
-	int			cpu_file;
-	struct mutex		mutex;
-	struct ring_buffer_iter	*buffer_iter[NR_CPUS];
-
-	/* The below is zeroed out in pipe_read */
-	struct trace_seq	seq;
-	struct trace_entry	*ent;
-	int			cpu;
-	u64			ts;
-
-	unsigned long		iter_flags;
-	loff_t			pos;
-	long			idx;
-
-	cpumask_var_t		started;
-};
-
 int tracer_init(struct tracer *t, struct trace_array *tr);
 int tracing_is_enabled(void);
 void trace_wake_up(void);
@@ -479,15 +430,6 @@ void trace_buffer_unlock_commit(struct trace_array *tr,
 				struct ring_buffer_event *event,
 				unsigned long flags, int pc);
 
-struct ring_buffer_event *
-trace_current_buffer_lock_reserve(unsigned char type, unsigned long len,
-				  unsigned long flags, int pc);
-void trace_current_buffer_unlock_commit(struct ring_buffer_event *event,
-					unsigned long flags, int pc);
-void trace_nowake_buffer_unlock_commit(struct ring_buffer_event *event,
-					unsigned long flags, int pc);
-void trace_current_buffer_discard_commit(struct ring_buffer_event *event);
-
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);
 
@@ -510,7 +452,6 @@ void tracing_sched_switch_trace(struct trace_array *tr,
 				struct task_struct *prev,
 				struct task_struct *next,
 				unsigned long flags, int pc);
-void tracing_record_cmdline(struct task_struct *tsk);
 
 void tracing_sched_wakeup_trace(struct trace_array *tr,
 				struct task_struct *wakee,
@@ -790,28 +731,6 @@ struct ftrace_event_field {
 	int			size;
 };
 
-struct ftrace_event_call {
-	char			*name;
-	char			*system;
-	struct dentry		*dir;
-	int			enabled;
-	int			(*regfunc)(void);
-	void			(*unregfunc)(void);
-	int			id;
-	int			(*raw_init)(void);
-	int			(*show_format)(struct trace_seq *s);
-	int			(*define_fields)(void);
-	struct list_head	fields;
-	int			n_preds;
-	struct filter_pred	**preds;
-
-#ifdef CONFIG_EVENT_PROFILE
-	atomic_t	profile_count;
-	int		(*profile_enable)(struct ftrace_event_call *);
-	void		(*profile_disable)(struct ftrace_event_call *);
-#endif
-};
-
 struct event_subsystem {
 	struct list_head	list;
 	const char		*name;
@@ -825,9 +744,6 @@ struct event_subsystem {
 	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
 	     event++)
 
-#define MAX_FILTER_PRED		8
-#define MAX_FILTER_STR_VAL	128
-
 struct filter_pred;
 
 typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event);
@@ -845,9 +761,6 @@ struct filter_pred {
 	int clear;
 };
 
-int trace_define_field(struct ftrace_event_call *call, char *type,
-		       char *name, int offset, int size);
-extern int init_preds(struct ftrace_event_call *call);
 extern void filter_free_pred(struct filter_pred *pred);
 extern void filter_print_preds(struct filter_pred **preds, int n_preds,
 			       struct trace_seq *s);
@@ -855,13 +768,9 @@ extern int filter_parse(char **pbuf, struct filter_pred *pred);
 extern int filter_add_pred(struct ftrace_event_call *call,
 			   struct filter_pred *pred);
 extern void filter_disable_preds(struct ftrace_event_call *call);
-extern int filter_match_preds(struct ftrace_event_call *call, void *rec);
 extern void filter_free_subsystem_preds(struct event_subsystem *system);
 extern int filter_add_subsystem_pred(struct event_subsystem *system,
 				     struct filter_pred *pred);
-extern int filter_current_check_discard(struct ftrace_event_call *call,
-					void *rec,
-					struct ring_buffer_event *event);
 
 static inline int
 filter_check_discard(struct ftrace_event_call *call, void *rec,
@@ -876,14 +785,6 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 	return 0;
 }
 
-#define __common_field(type, item)					\
-	ret = trace_define_field(event_call, #type, "common_" #item,	\
-				 offsetof(typeof(field.ent), item),	\
-				 sizeof(field.ent.item));		\
-	if (ret)							\
-		return ret;
-
-void event_trace_printk(unsigned long ip, const char *fmt, ...);
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];
 
@@ -895,25 +796,6 @@ extern struct ftrace_event_call __stop_ftrace_events[];
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 
-/*
- * The double __builtin_constant_p is because gcc will give us an error
- * if we try to allocate the static variable to fmt if it is not a
- * constant. Even with the outer if statement optimizing out.
- */
-#define event_trace_printk(ip, fmt, args...)				\
-do {									\
-	__trace_printk_check_format(fmt, ##args);			\
-	tracing_record_cmdline(current);				\
-	if (__builtin_constant_p(fmt)) {				\
-		static const char *trace_printk_fmt			\
-		  __attribute__((section("__trace_printk_fmt"))) =	\
-			__builtin_constant_p(fmt) ? fmt : NULL;		\
-									\
-		__trace_bprintk(ip, trace_printk_fmt, ##args);		\
-	} else								\
-		__trace_printk(ip, fmt, ##args);			\
-} while (0)
-
 #undef TRACE_EVENT_FORMAT
 #define TRACE_EVENT_FORMAT(call, proto, args, fmt, tstruct, tpfmt)	\
 	extern struct ftrace_event_call event_##call;

commit 9504504cbab29ecb694186b1c5b15d3579c43c51
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Apr 11 12:59:57 2009 -0400

    tracing: make trace_seq operations available for core kernel
    
    In the process to make TRACE_EVENT macro work for modules, the trace_seq
    operations must be available for core kernel code.
    
    These operations are quite useful and can be used for other implementations.
    
    The main idea is that we create a trace_seq handle that acts very much
    like the seq_file handle.
    
            struct trace_seq *s = kmalloc(sizeof(*s, GFP_KERNEL);
    
            trace_seq_init(s);
            trace_seq_printf(s, "some data %d\n", variable);
    
            printk("%s", s->buffer);
    
    The main use is to allow a top level function call several other functions
    that may store printf like data into the buffer. Then at the end, the top
    level function can process all the data with any method it would like to.
    It could be passed to userspace, output via printk or even use seq_file:
    
            trace_seq_to_user(s, ubuf, cnt);
            seq_puts(m, s->buffer);
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b05b6ac982a1..1882846b7389 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -12,6 +12,8 @@
 #include <linux/kmemtrace.h>
 #include <trace/power.h>
 
+#include <linux/trace_seq.h>
+
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
 
@@ -423,19 +425,6 @@ struct tracer {
 	struct tracer_stat	*stats;
 };
 
-struct trace_seq {
-	unsigned char		buffer[PAGE_SIZE];
-	unsigned int		len;
-	unsigned int		readpos;
-};
-
-static inline void
-trace_seq_init(struct trace_seq *s)
-{
-	s->len = 0;
-	s->readpos = 0;
-}
-
 
 #define TRACE_PIPE_ALL_CPU	-1
 

commit 0a19e53c1514ad8e9c3cbab40c6c3f52c86f403d
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Mon Apr 13 03:17:50 2009 -0500

    tracing/filters: allow on-the-fly filter switching
    
    This patch allows event filters to be safely removed or switched
    on-the-fly while avoiding the use of rcu or the suspension of tracing of
    previous versions.
    
    It does it by adding a new filter_pred_none() predicate function which
    does nothing and by never deallocating either the predicates or any of
    the filter_pred members used in matching; the predicate lists are
    allocated and initialized during ftrace_event_calls initialization.
    
    Whenever a filter is removed or replaced, the filter_pred_* functions
    currently in use by the affected ftrace_event_call are immediately
    switched over to to the filter_pred_none() function, while the rest of
    the filter_pred members are left intact, allowing any currently
    executing filter_pred_* functions to finish up, using the values they're
    currently using.
    
    In the case of filter replacement, the new predicate values are copied
    into the old predicates after the above step, and the filter_pred_none()
    functions are replaced by the filter_pred_* functions for the new
    filter.  In this case, it is possible though very unlikely that a
    previous filter_pred_* is still running even after the
    filter_pred_none() switch and the switch to the new filter_pred_*.  In
    that case, however, because nothing has been deallocated in the
    filter_pred, the worst that can happen is that the old filter_pred_*
    function sees the new values and as a result produces either a false
    positive or a false negative, depending on the values it finds.
    
    So one downside to this method is that rarely, it can produce a bad
    match during the filter switch, but it should be possible to live with
    that, IMHO.
    
    The other downside is that at least in this patch the predicate lists
    are always pre-allocated, taking up memory from the start.  They could
    probably be allocated on first-use, and de-allocated when tracing is
    completely stopped - if this patch makes sense, I could create another
    one to do that later on.
    
    Oh, and it also places a restriction on the size of __arrays in events,
    currently set to 128, since they can't be larger than the now embedded
    str_val arrays in the filter_pred struct.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: paulmck@linux.vnet.ibm.com
    LKML-Reference: <1239610670.6660.49.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9729d14767d8..b05b6ac982a1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -813,6 +813,7 @@ struct ftrace_event_call {
 	int			(*show_format)(struct trace_seq *s);
 	int			(*define_fields)(void);
 	struct list_head	fields;
+	int			n_preds;
 	struct filter_pred	**preds;
 
 #ifdef CONFIG_EVENT_PROFILE
@@ -826,6 +827,7 @@ struct event_subsystem {
 	struct list_head	list;
 	const char		*name;
 	struct dentry		*entry;
+	int			n_preds;
 	struct filter_pred	**preds;
 };
 
@@ -834,7 +836,8 @@ struct event_subsystem {
 	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
 	     event++)
 
-#define MAX_FILTER_PRED 8
+#define MAX_FILTER_PRED		8
+#define MAX_FILTER_STR_VAL	128
 
 struct filter_pred;
 
@@ -843,7 +846,7 @@ typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event);
 struct filter_pred {
 	filter_pred_fn_t fn;
 	u64 val;
-	char *str_val;
+	char str_val[MAX_FILTER_STR_VAL];
 	int str_len;
 	char *field_name;
 	int offset;
@@ -855,13 +858,14 @@ struct filter_pred {
 
 int trace_define_field(struct ftrace_event_call *call, char *type,
 		       char *name, int offset, int size);
+extern int init_preds(struct ftrace_event_call *call);
 extern void filter_free_pred(struct filter_pred *pred);
-extern void filter_print_preds(struct filter_pred **preds,
+extern void filter_print_preds(struct filter_pred **preds, int n_preds,
 			       struct trace_seq *s);
 extern int filter_parse(char **pbuf, struct filter_pred *pred);
 extern int filter_add_pred(struct ftrace_event_call *call,
 			   struct filter_pred *pred);
-extern void filter_free_preds(struct ftrace_event_call *call);
+extern void filter_disable_preds(struct ftrace_event_call *call);
 extern int filter_match_preds(struct ftrace_event_call *call, void *rec);
 extern void filter_free_subsystem_preds(struct event_subsystem *system);
 extern int filter_add_subsystem_pred(struct event_subsystem *system,
@@ -875,7 +879,7 @@ filter_check_discard(struct ftrace_event_call *call, void *rec,
 		     struct ring_buffer *buffer,
 		     struct ring_buffer_event *event)
 {
-	if (unlikely(call->preds) && !filter_match_preds(call, rec)) {
+	if (unlikely(call->n_preds) && !filter_match_preds(call, rec)) {
 		ring_buffer_discard_commit(buffer, event);
 		return 1;
 	}

commit eb02ce017dd83985041a7e54c6449f92d53b026f
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Wed Apr 8 03:15:54 2009 -0500

    tracing/filters: use ring_buffer_discard_commit() in filter_check_discard()
    
    This patch changes filter_check_discard() to make use of the new
    ring_buffer_discard_commit() function and modifies the current users to
    call the old commit function in the non-discard case.
    
    It also introduces a version of filter_check_discard() that uses the
    global trace buffer (filter_current_check_discard()) for those cases.
    
    v2 changes:
    
    - fix compile error noticed by Ingo Molnar
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    LKML-Reference: <1239178554.10295.36.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dfefffd7ae39..9729d14767d8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -866,13 +866,21 @@ extern int filter_match_preds(struct ftrace_event_call *call, void *rec);
 extern void filter_free_subsystem_preds(struct event_subsystem *system);
 extern int filter_add_subsystem_pred(struct event_subsystem *system,
 				     struct filter_pred *pred);
+extern int filter_current_check_discard(struct ftrace_event_call *call,
+					void *rec,
+					struct ring_buffer_event *event);
 
-static inline void
+static inline int
 filter_check_discard(struct ftrace_event_call *call, void *rec,
+		     struct ring_buffer *buffer,
 		     struct ring_buffer_event *event)
 {
-	if (unlikely(call->preds) && !filter_match_preds(call, rec))
-		ring_buffer_event_discard(event);
+	if (unlikely(call->preds) && !filter_match_preds(call, rec)) {
+		ring_buffer_discard_commit(buffer, event);
+		return 1;
+	}
+
+	return 0;
 }
 
 #define __common_field(type, item)					\

commit 77d9f465d46fd67cdb82ee5e1ab99dd57a17c486
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 2 01:16:59 2009 -0400

    tracing/filters: use ring_buffer_discard_commit for discarded events
    
    The ring_buffer_discard_commit makes better usage of the ring_buffer
    when an event has been discarded. It tries to remove it completely if
    possible.
    
    This patch converts the trace event filtering to use
    ring_buffer_discard_commit instead of the ring_buffer_event_discard.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3cf856fa597b..dfefffd7ae39 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -497,6 +497,7 @@ void trace_current_buffer_unlock_commit(struct ring_buffer_event *event,
 					unsigned long flags, int pc);
 void trace_nowake_buffer_unlock_commit(struct ring_buffer_event *event,
 					unsigned long flags, int pc);
+void trace_current_buffer_discard_commit(struct ring_buffer_event *event);
 
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);

commit e45f2e2bd298e1ff687448e5fd15a3588b5807ec
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Mar 31 00:49:16 2009 -0500

    tracing/filters: add TRACE_EVENT_FORMAT_NOFILTER event macro
    
    Frederic Weisbecker suggested that the trace_special event shouldn't be
    filterable; this patch adds a TRACE_EVENT_FORMAT_NOFILTER event macro
    that allows an event format to be exported without having a filter
    attached, and removes filtering from the trace_special event.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e7737281953f..3cf856fa597b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -915,6 +915,8 @@ do {									\
 #undef TRACE_EVENT_FORMAT
 #define TRACE_EVENT_FORMAT(call, proto, args, fmt, tstruct, tpfmt)	\
 	extern struct ftrace_event_call event_##call;
+#undef TRACE_EVENT_FORMAT_NOFILTER
+#define TRACE_EVENT_FORMAT_NOFILTER(call, proto, args, fmt, tstruct, tpfmt)
 #include "trace_event_types.h"
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit e1112b4d96859367a93468027c9635e2ac04eb3f
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Mar 31 00:48:49 2009 -0500

    tracing/filters: add run-time field descriptions to TRACE_EVENT_FORMAT events
    
    This patch adds run-time field descriptions to all the event formats
    exported using TRACE_EVENT_FORMAT.  It also hooks up all the tracers
    that use them (i.e. the tracers in the 'ftrace subsystem') so they can
    also have their output filtered by the event-filtering mechanism.
    
    When I was testing this, there were a couple of things that fooled me
    into thinking the filters weren't working, when actually they were -
    I'll mention them here so others don't make the same mistakes (and file
    bug reports. ;-)
    
    One is that some of the tracers trace multiple events e.g. the
    sched_switch tracer uses the context_switch and wakeup events, and if
    you don't set filters on all of the traced events, the unfiltered output
    from the events without filters on them can make it look like the
    filtering as a whole isn't working properly, when actually it is doing
    what it was asked to do - it just wasn't asked to do the right thing.
    
    The other is that for the really high-volume tracers e.g. the function
    tracer, the volume of filtered events can be so high that it pushes the
    unfiltered events out of the ring buffer before they can be read so e.g.
    cat'ing the trace file repeatedly shows either no output, or once in
    awhile some output but that isn't there the next time you read the
    trace, which isn't what you normally expect when reading the trace file.
    If you read from the trace_pipe file though, you can catch them before
    they disappear.
    
    Changes from v1:
    
    As suggested by Frederic Weisbecker:
    
    - get rid of externs in functions
    - added unlikely() to filter_check_discard()
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 34b94c3f40ad..e7737281953f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -866,6 +866,21 @@ extern void filter_free_subsystem_preds(struct event_subsystem *system);
 extern int filter_add_subsystem_pred(struct event_subsystem *system,
 				     struct filter_pred *pred);
 
+static inline void
+filter_check_discard(struct ftrace_event_call *call, void *rec,
+		     struct ring_buffer_event *event)
+{
+	if (unlikely(call->preds) && !filter_match_preds(call, rec))
+		ring_buffer_event_discard(event);
+}
+
+#define __common_field(type, item)					\
+	ret = trace_define_field(event_call, #type, "common_" #item,	\
+				 offsetof(typeof(field.ent), item),	\
+				 sizeof(field.ent.item));		\
+	if (ret)							\
+		return ret;
+
 void event_trace_printk(unsigned long ip, const char *fmt, ...);
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];
@@ -897,4 +912,9 @@ do {									\
 		__trace_printk(ip, fmt, ##args);			\
 } while (0)
 
+#undef TRACE_EVENT_FORMAT
+#define TRACE_EVENT_FORMAT(call, proto, args, fmt, tstruct, tpfmt)	\
+	extern struct ftrace_event_call event_##call;
+#include "trace_event_types.h"
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 02af61bb50f5d5f0322dbe5ab2a0d75808d25c7b
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Fri Apr 10 14:26:18 2009 +0800

    tracing, kmemtrace: Separate include/trace/kmemtrace.h to kmemtrace part and tracepoint part
    
    Impact: refactor code for future changes
    
    Current kmemtrace.h is used both as header file of kmemtrace and kmem's
    tracepoints definition.
    
    Tracepoints' definition file may be used by other code, and should only have
    definition of tracepoint.
    
    We can separate include/trace/kmemtrace.h into 2 files:
    
      include/linux/kmemtrace.h: header file for kmemtrace
      include/trace/kmem.h:      definition of kmem tracepoints
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Acked-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Acked-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <49DEE68A.5040902@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f76a8f8689d4..34b94c3f40ad 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -9,7 +9,7 @@
 #include <linux/mmiotrace.h>
 #include <linux/ftrace.h>
 #include <trace/boot.h>
-#include <trace/kmemtrace.h>
+#include <linux/kmemtrace.h>
 #include <trace/power.h>
 
 enum trace_type {

commit c93f216b5b985a12a18323e5ca2eb01db3d2f000
Merge: c61b79b6ef26 ab3c9c686e22
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 7 14:10:10 2009 -0700

    Merge branch 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'tracing-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip:
      branch tracer, intel-iommu: fix build with CONFIG_BRANCH_TRACER=y
      branch tracer: Fix for enabling branch profiling makes sparse unusable
      ftrace: Correct a text align for event format output
      Update /debug/tracing/README
      tracing/ftrace: alloc the started cpumask for the trace file
      tracing, x86: remove duplicated #include
      ftrace: Add check of sched_stopped for probe_sched_wakeup
      function-graph: add proper initialization for init task
      tracing/ftrace: fix missing include string.h
      tracing: fix incorrect return type of ns2usecs()
      tracing: remove CALLER_ADDR2 from wakeup tracer
      blktrace: fix pdu_len when tracing packet command requests
      blktrace: small cleanup in blk_msg_write()
      blktrace: NUL-terminate user space messages
      tracing: move scripts/trace/power.pl to scripts/tracing/power.pl

commit 5452af664f6fba26b80eb2c8c4ceae2999d5cf56
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 27 00:25:38 2009 +0100

    tracing/ftrace: factorize the tracing files creation
    
    Impact: cleanup
    
    Most of the tracing files creation follow the same pattern:
    
    ret = debugfs_create_file(...)
    if (!ret)
            pr_warning("Couldn't create ... entry\n")
    
    Unify it!
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1238109938-11840-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 47aa6d0c97a0..f76a8f8689d4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -470,6 +470,12 @@ void trace_wake_up(void);
 void tracing_reset(struct trace_array *tr, int cpu);
 void tracing_reset_online_cpus(struct trace_array *tr);
 int tracing_open_generic(struct inode *inode, struct file *filp);
+struct dentry *trace_create_file(const char *name,
+				 mode_t mode,
+				 struct dentry *parent,
+				 void *data,
+				 const struct file_operations *fops);
+
 struct dentry *tracing_init_dentry(void);
 void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 

commit 86665c75da41889f92b774f31ea5a9a436f392a8
Merge: 93776a8ec746 1bbe2a83ab68
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 14:41:14 2009 +0200

    Merge branch 'tracing/urgent' into tracing/ftrace

commit cf8e3474654f20433aab9aa35826d43b5f245008
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Mon Mar 30 13:48:00 2009 +0800

    tracing: fix incorrect return type of ns2usecs()
    
    Impact: fix time output bug in 32bits system
    
    ns2usecs() returns 'long', it's incorrect.
    
    (In i386)
    ...
              <idle>-0     [000]   521.442100: _spin_lock <-tick_do_update_jiffies64
              <idle>-0     [000]   521.442101: do_timer <-tick_do_update_jiffies64
              <idle>-0     [000]   521.442102: update_wall_time <-do_timer
              <idle>-0     [000]   521.442102: update_xtime_cache <-update_wall_time
    ....
    (It always print the time less than 2200 seconds besides ...)
    Because 'long' is 32bits in i386. ( (1<<31) useconds is about 2200 seconds)
    
    ...
              <idle>-0     [001] 4154502640.134759: rcu_bh_qsctr_inc <-__do_softirq
              <idle>-0     [001] 4154502640.134760: _local_bh_enable <-__do_softirq
              <idle>-0     [001] 4154502640.134761: idle_cpu <-irq_exit
    ...
    (very large value)
    Because 'long' is a signed type and it is 32bits in i386.
    
    Changes in v2:
    return 'unsigned long long' instead of 'cycle_t'
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <49D05D10.4030009@cn.fujitsu.com>
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cb0ce3fc36d3..0d81a4a2a4a5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -596,7 +596,7 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);
-extern long ns2usecs(cycle_t nsec);
+extern unsigned long long ns2usecs(cycle_t nsec);
 extern int
 trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
 extern int

commit 93776a8ec746cf9d32c36e5a5b23d28d8be28826
Merge: 34886c8bc590 d508afb437da
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 13:47:33 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: update to upstream tracing facilities
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 2e8844e13ab73f1107aea4317a53ff5879f2e1d7
Merge: c78a3956b982 d508afb437da
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 7 13:34:26 2009 +0200

    Merge branch 'linus' into tracing/hw-branch-tracing
    
    Merge reason: update to latest tracing and ptrace APIs
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit ca2b84cb3c4a0d4d2143b46ec072cdff5d1b3b87
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Mon Mar 23 15:12:24 2009 +0200

    kmemtrace: use tracepoints
    
    kmemtrace now uses tracepoints instead of markers. We no longer need to
    use format specifiers to pass arguments.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    [ folded: Use the new TP_PROTO and TP_ARGS to fix the build.     ]
    [ folded: fix build when CONFIG_KMEMTRACE is disabled.           ]
    [ folded: define tracepoints when CONFIG_TRACEPOINTS is enabled. ]
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    LKML-Reference: <ae61c0f37156db8ec8dc0d5778018edde60a92e3.1237813499.git.eduard.munteanu@linux360.ro>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cb0ce3fc36d3..cbc168f1e43d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -182,6 +182,12 @@ struct trace_power {
 	struct power_trace	state_data;
 };
 
+enum kmemtrace_type_id {
+	KMEMTRACE_TYPE_KMALLOC = 0,	/* kmalloc() or kfree(). */
+	KMEMTRACE_TYPE_CACHE,		/* kmem_cache_*(). */
+	KMEMTRACE_TYPE_PAGES,		/* __get_free_pages() and friends. */
+};
+
 struct kmemtrace_alloc_entry {
 	struct trace_entry	ent;
 	enum kmemtrace_type_id type_id;

commit 8b54e45b0005875f59cb8b1c44d429a161d5f245
Merge: 0a5d649018b1 9a8118baaeb0 b14b70a6a4e3 fee039a1d05c 18cea4591a98 548c31613790
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 31 17:46:40 2009 +0200

    Merge branches 'tracing/docs', 'tracing/filters', 'tracing/ftrace', 'tracing/kprobes', 'tracing/blktrace-v2' and 'tracing/textedit' into tracing/core-v2

commit a2a16d6a3156ef7309ca7328a20c35df9418e670
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 24 23:17:58 2009 -0400

    function-graph: add option to calculate graph time or not
    
    graph time is the time that a function is executing another function.
    Thus if function A calls B, if graph-time is set, then the time for
    A includes B. This is the default behavior. But if graph-time is off,
    then the time spent executing B is subtracted from A.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c66ca3b66050..e3429a8ab059 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -685,6 +685,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_LATENCY_FMT		= 0x40000,
 	TRACE_ITER_GLOBAL_CLK		= 0x80000,
 	TRACE_ITER_SLEEP_TIME		= 0x100000,
+	TRACE_ITER_GRAPH_TIME		= 0x200000,
 };
 
 /*

commit 0706f1c48ca8a7ab478090b4e38f2e578ae2bfe0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 23 23:12:58 2009 -0400

    tracing: adding function timings to function profiler
    
    If the function graph trace is enabled, the function profiler will
    use it to take the timing of the functions.
    
     cat /debug/tracing/trace_stat/functions
    
      Function                               Hit    Time
      --------                               ---    ----
      mwait_idle                             127    183028.4 us
      schedule                                26    151997.7 us
      __schedule                              31    151975.1 us
      sys_wait4                                2    74080.53 us
      do_wait                                  2    74077.80 us
      sys_newlstat                           138    39929.16 us
      do_path_lookup                         179    39845.79 us
      vfs_lstat_fd                           138    39761.97 us
      user_path_at                           153    39469.58 us
      path_walk                              179    39435.76 us
      __link_path_walk                       189    39143.73 us
    [...]
    
    Note the times are skewed due to the function graph tracer not taking
    into account schedules.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d7410bbb9a80..c66ca3b66050 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -605,6 +605,8 @@ extern unsigned long trace_flags;
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 extern enum print_line_t print_graph_function(struct trace_iterator *iter);
+extern enum print_line_t
+trace_print_graph_duration(unsigned long long duration, struct trace_seq *s);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 /* TODO: make this variable */
@@ -636,7 +638,6 @@ static inline int ftrace_graph_addr(unsigned long addr)
 	return 1;
 }
 #endif /* CONFIG_DYNAMIC_FTRACE */
-
 #else /* CONFIG_FUNCTION_GRAPH_TRACER */
 static inline enum print_line_t
 print_graph_function(struct trace_iterator *iter)

commit be6f164a02f394675e2ac2077dd354cebef5b4c0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 24 11:06:24 2009 -0400

    function-graph: add option for include sleep times
    
    Impact: give user a choice to show times spent while sleeping
    
    The user may want to see the time a function spent sleeping.
    This patch adds the trace option "sleep-time" to allow that.
    The "sleep-time" option is default on.
    
     echo sleep-time > /debug/tracing/trace_options
    
    produces:
    
     ------------------------------------------
     2)  avahi-d-3428  =>    <idle>-0
     ------------------------------------------
    
     2)               |      finish_task_switch() {
     2)   0.621 us    |        _spin_unlock_irq();
     2)   2.202 us    |      }
     2) ! 1002.197 us |    }
     2) ! 1003.521 us |  }
    
    where as,
    
     echo nosleep-time > /debug/tracing/trace_options
    
    produces:
    
     0)    <idle>-0    =>  yum-upd-3416
     ------------------------------------------
    
     0)               |              finish_task_switch() {
     0)   0.643 us    |                _spin_unlock_irq();
     0)   2.342 us    |              }
     0) + 41.302 us   |            }
     0) + 42.453 us   |          }
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7cfb741be200..d7410bbb9a80 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -683,6 +683,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_CONTEXT_INFO		= 0x20000, /* Print pid/cpu/time */
 	TRACE_ITER_LATENCY_FMT		= 0x40000,
 	TRACE_ITER_GLOBAL_CLK		= 0x80000,
+	TRACE_ITER_SLEEP_TIME		= 0x100000,
 };
 
 /*

commit 4bda2d517bfa3ce3d7044e06988cdddae7adffe2
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Mar 24 02:14:31 2009 -0500

    tracing/filters: use trace_seq_printf() to print filters
    
    Impact: cleanup
    
    Instead of just using the trace_seq buffer to print the filters, use
    trace_seq_printf() as it was intended to be used.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: =?ISO-8859-1?Q?Fr=E9d=E9ric?= Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237878871.8339.59.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 54fd9bcd0a65..90a848debcba 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -840,7 +840,8 @@ struct filter_pred {
 int trace_define_field(struct ftrace_event_call *call, char *type,
 		       char *name, int offset, int size);
 extern void filter_free_pred(struct filter_pred *pred);
-extern int filter_print_preds(struct filter_pred **preds, char *buf);
+extern void filter_print_preds(struct filter_pred **preds,
+			       struct trace_seq *s);
 extern int filter_parse(char **pbuf, struct filter_pred *pred);
 extern int filter_add_pred(struct ftrace_event_call *call,
 			   struct filter_pred *pred);

commit 07edf7121374609709ef1b0889f6e7b8d6a62ec1
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Mar 22 23:10:46 2009 +0100

    tracing/events: don't use wake up for events
    
    Impact: fix hard-lockup with sched switch events
    
    Some ftrace events, such as sched wakeup, can be traced
    while the runqueue lock is hold. Since they are using
    trace_current_buffer_unlock_commit(), they call wake_up()
    which can try to grab the runqueue lock too, resulting in
    a deadlock.
    
    Now for all event, we call a new helper:
    trace_nowake_buffer_unlock_commit() which do pretty the same than
    trace_current_buffer_unlock_commit() except than it doesn't call
    trace_wake_up().
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1237759847-21025-4-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f267723c3c52..54fd9bcd0a65 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -483,6 +483,8 @@ trace_current_buffer_lock_reserve(unsigned char type, unsigned long len,
 				  unsigned long flags, int pc);
 void trace_current_buffer_unlock_commit(struct ring_buffer_event *event,
 					unsigned long flags, int pc);
+void trace_nowake_buffer_unlock_commit(struct ring_buffer_event *event,
+					unsigned long flags, int pc);
 
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);

commit cfb180f3e71b2a280a254c8646a9ab1beab63f84
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Sun Mar 22 03:31:17 2009 -0500

    tracing: add per-subsystem filtering
    
    This patch adds per-subsystem filtering to the event tracing subsystem.
    
    It adds a 'filter' debugfs file to each subsystem directory.  This file
    can be written to to set filters; reading from it will display the
    current set of filters set for that subsystem.
    
    Basically what it does is propagate the filter down to each event
    contained in the subsystem.  If a particular event doesn't have a field
    with the name specified in the filter, it simply doesn't get set for
    that event.  You can verify whether or not the filter was set for a
    particular event by looking at the filter file for that event.
    
    As with per-event filters, compound expressions are supported, echoing
    '0' to the subsystem's filter file clears all filters in the subsystem,
    etc.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237710677.7703.49.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index d9eb39e4bb38..f267723c3c52 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -804,6 +804,18 @@ struct ftrace_event_call {
 #endif
 };
 
+struct event_subsystem {
+	struct list_head	list;
+	const char		*name;
+	struct dentry		*entry;
+	struct filter_pred	**preds;
+};
+
+#define events_for_each(event)						\
+	for (event = __start_ftrace_events;				\
+	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
+	     event++)
+
 #define MAX_FILTER_PRED 8
 
 struct filter_pred;
@@ -832,6 +844,9 @@ extern int filter_add_pred(struct ftrace_event_call *call,
 			   struct filter_pred *pred);
 extern void filter_free_preds(struct ftrace_event_call *call);
 extern int filter_match_preds(struct ftrace_event_call *call, void *rec);
+extern void filter_free_subsystem_preds(struct event_subsystem *system);
+extern int filter_add_subsystem_pred(struct event_subsystem *system,
+				     struct filter_pred *pred);
 
 void event_trace_printk(unsigned long ip, const char *fmt, ...);
 extern struct ftrace_event_call __start_ftrace_events[];

commit 7ce7e4249921d5073e764f7ff7ad83cfa9894bd7
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Sun Mar 22 03:31:04 2009 -0500

    tracing: add per-event filtering
    
    This patch adds per-event filtering to the event tracing subsystem.
    
    It adds a 'filter' debugfs file to each event directory.  This file can
    be written to to set filters; reading from it will display the current
    set of filters set for that event.
    
    Basically, any field listed in the 'format' file for an event can be
    filtered on (including strings, but not yet other array types) using
    either matching ('==') or non-matching ('!=') 'predicates'.  A
    'predicate' can be either a single expression:
    
     # echo pid != 0 > filter
    
     # cat filter
     pid != 0
    
    or a compound expression of up to 8 sub-expressions combined using '&&'
    or '||':
    
     # echo comm == Xorg > filter
     # echo "&& sig != 29" > filter
    
     # cat filter
     comm == Xorg
     && sig != 29
    
    Only events having field values matching an expression will be available
    in the trace output; non-matching events are discarded.
    
    Note that a compound expression is built up by echoing each
    sub-expression separately - it's not the most efficient way to do
    things, but it keeps the parser simple and assumes that compound
    expressions will be relatively uncommon.  In any case, a subsequent
    patch introducing a way to set filters for entire subsystems should
    mitigate any need to do this for lots of events.
    
    Setting a filter without an '&&' or '||' clears the previous filter
    completely and sets the filter to the new expression:
    
     # cat filter
     comm == Xorg
     && sig != 29
    
     # echo comm != Xorg
    
     # cat filter
     comm != Xorg
    
    To clear a filter, echo 0 to the filter file:
    
     # echo 0 > filter
     # cat filter
     none
    
    The limit of 8 predicates for a compound expression is arbitrary - for
    efficiency, it's implemented as an array of pointers to predicates, and
    8 seemed more than enough for any filter...
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237710665.7703.48.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9288dc7ad14f..d9eb39e4bb38 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -795,6 +795,7 @@ struct ftrace_event_call {
 	int			(*show_format)(struct trace_seq *s);
 	int			(*define_fields)(void);
 	struct list_head	fields;
+	struct filter_pred	**preds;
 
 #ifdef CONFIG_EVENT_PROFILE
 	atomic_t	profile_count;
@@ -803,8 +804,35 @@ struct ftrace_event_call {
 #endif
 };
 
+#define MAX_FILTER_PRED 8
+
+struct filter_pred;
+
+typedef int (*filter_pred_fn_t) (struct filter_pred *pred, void *event);
+
+struct filter_pred {
+	filter_pred_fn_t fn;
+	u64 val;
+	char *str_val;
+	int str_len;
+	char *field_name;
+	int offset;
+	int not;
+	int or;
+	int compound;
+	int clear;
+};
+
 int trace_define_field(struct ftrace_event_call *call, char *type,
 		       char *name, int offset, int size);
+extern void filter_free_pred(struct filter_pred *pred);
+extern int filter_print_preds(struct filter_pred **preds, char *buf);
+extern int filter_parse(char **pbuf, struct filter_pred *pred);
+extern int filter_add_pred(struct ftrace_event_call *call,
+			   struct filter_pred *pred);
+extern void filter_free_preds(struct ftrace_event_call *call);
+extern int filter_match_preds(struct ftrace_event_call *call, void *rec);
+
 void event_trace_printk(unsigned long ip, const char *fmt, ...);
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];

commit cf027f645e6aee4f0ca6197a6b6a57f327fdb13f
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Sun Mar 22 03:30:39 2009 -0500

    tracing: add run-time field descriptions for event filtering
    
    This patch makes the field descriptions defined for event tracing
    available at run-time, for the event-filtering mechanism introduced
    in a subsequent patch.
    
    The common event fields are prepended with 'common_' in the format
    display, allowing them to be distinguished from the other fields
    that might internally have same name and can therefore be
    unambiguously used in filters.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237710639.7703.46.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7cfb741be200..9288dc7ad14f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -775,16 +775,26 @@ enum {
 	TRACE_EVENT_TYPE_RAW		= 2,
 };
 
+struct ftrace_event_field {
+	struct list_head	link;
+	char			*name;
+	char			*type;
+	int			offset;
+	int			size;
+};
+
 struct ftrace_event_call {
-	char		*name;
-	char		*system;
-	struct dentry	*dir;
-	int		enabled;
-	int		(*regfunc)(void);
-	void		(*unregfunc)(void);
-	int		id;
-	int		(*raw_init)(void);
-	int		(*show_format)(struct trace_seq *s);
+	char			*name;
+	char			*system;
+	struct dentry		*dir;
+	int			enabled;
+	int			(*regfunc)(void);
+	void			(*unregfunc)(void);
+	int			id;
+	int			(*raw_init)(void);
+	int			(*show_format)(struct trace_seq *s);
+	int			(*define_fields)(void);
+	struct list_head	fields;
 
 #ifdef CONFIG_EVENT_PROFILE
 	atomic_t	profile_count;
@@ -793,6 +803,8 @@ struct ftrace_event_call {
 #endif
 };
 
+int trace_define_field(struct ftrace_event_call *call, char *type,
+		       char *name, int offset, int size);
 void event_trace_printk(unsigned long ip, const char *fmt, ...);
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];

commit ac199db0189c091f2863312061c0575937f68810
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 19 20:26:15 2009 +0100

    ftrace: event profile hooks
    
    Impact: new tracing infrastructure feature
    
    Provide infrastructure to generate software perf counter events
    from tracepoints.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20090319194233.557364871@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7c9a0cbf5dca..7cfb741be200 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -785,12 +785,23 @@ struct ftrace_event_call {
 	int		id;
 	int		(*raw_init)(void);
 	int		(*show_format)(struct trace_seq *s);
+
+#ifdef CONFIG_EVENT_PROFILE
+	atomic_t	profile_count;
+	int		(*profile_enable)(struct ftrace_event_call *);
+	void		(*profile_disable)(struct ftrace_event_call *);
+#endif
 };
 
 void event_trace_printk(unsigned long ip, const char *fmt, ...);
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];
 
+#define for_each_event(event)						\
+	for (event = __start_ftrace_events;				\
+	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
+	     event++)
+
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 

commit 40ce74f19c28077550646c76d96a075bf312e461
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 19 14:03:53 2009 -0400

    tracing: remove recording function depth from trace_printk
    
    The function depth in trace_printk was to facilitate the function
    graph output. Now that the function graph calculates the depth within
    the trace output, we no longer need to record the depth when the
    trace_printk is called.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 38276d1638e3..7c9a0cbf5dca 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -123,7 +123,6 @@ struct userstack_entry {
 struct bprint_entry {
 	struct trace_entry	ent;
 	unsigned long		ip;
-	int			depth;
 	const char		*fmt;
 	u32			buf[];
 };
@@ -131,7 +130,6 @@ struct bprint_entry {
 struct print_entry {
 	struct trace_entry	ent;
 	unsigned long		ip;
-	int			depth;
 	char			buf[];
 };
 
@@ -598,9 +596,9 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 extern void *head_page(struct trace_array_cpu *data);
 extern long ns2usecs(cycle_t nsec);
 extern int
-trace_vbprintk(unsigned long ip, int depth, const char *fmt, va_list args);
+trace_vbprintk(unsigned long ip, const char *fmt, va_list args);
 extern int
-trace_vprintk(unsigned long ip, int depth, const char *fmt, va_list args);
+trace_vprintk(unsigned long ip, const char *fmt, va_list args);
 
 extern unsigned long trace_flags;
 

commit 327019b01e068d66dada6a8b2571180ab3674d20
Merge: 03418c7efaa4 62524d55e5b9
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Mar 18 06:59:56 2009 +0100

    Merge branch 'tip/tracing/ftrace' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into tracing/ftrace

commit af4617bdba34aa556272b34c3986b0a4d588f568
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 17 18:09:55 2009 -0400

    tracing: add global-clock option to provide cross CPU clock to traces
    
    Impact: feature to allow better serialized clock
    
    This patch adds an option called "global-clock" that will allow
    the tracer to switch to a slower but more accurate (across CPUs)
    clock.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b0ecad8ecc34..26a7a28ca110 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -667,6 +667,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_PRINTK_MSGONLY	= 0x10000,
 	TRACE_ITER_CONTEXT_INFO		= 0x20000, /* Print pid/cpu/time */
 	TRACE_ITER_LATENCY_FMT		= 0x40000,
+	TRACE_ITER_GLOBAL_CLK		= 0x80000,
 };
 
 /*

commit 4176935b58eeb636917e65a2c388e4607e36cce6
Merge: c269fc8c537d 6adaad14d7d4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Mar 17 10:37:37 2009 +0100

    Merge branch 'tip/tracing/ftrace' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into tracing/ftrace

commit 4ca530852346be239b7c19e7bec5d2b78855bebe
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 16 19:20:15 2009 -0400

    tracing: protect reader of cmdline output
    
    Impact: fix to one cause of incorrect comm outputs in trace
    
    The spinlock only protected the creation of a comm <=> pid pair.
    But it was possible that a reader could look up a pid, and get the
    wrong comm because it had no locking.
    
    This also required changing trace_find_cmdline to copy the comm cache
    and not just send back a pointer to it.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 56ce34d90b03..b0ecad8ecc34 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -547,7 +547,7 @@ struct tracer_switch_ops {
 };
 #endif /* CONFIG_CONTEXT_SWITCH_TRACER */
 
-extern char *trace_find_cmdline(int pid);
+extern void trace_find_cmdline(int pid, char comm[]);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern unsigned long ftrace_update_tot_cnt;

commit 7243f2145a9b06e5cf9a49fc9b8b9a4fff6fb42e
Merge: b478b782e110 62395efdb0ef 5bee17f18b59
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Mar 16 09:12:42 2009 +0100

    Merge branches 'tracing/ftrace', 'tracing/syscalls' and 'linus' into tracing/core
    
    Conflicts:
            arch/parisc/kernel/irq.c

commit bed1ffca022cc876fb83161d26670e9b5d3cf36b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 13 15:42:11 2009 +0100

    tracing/syscalls: core infrastructure for syscalls tracing, enhancements
    
    Impact: new feature
    
    This adds the generic support for syscalls tracing. This is
    currently exploited through a devoted tracer but other tracing
    engines can use it. (They just have to play with
    {start,stop}_ftrace_syscalls() and use the display callbacks
    unless they want to override them.)
    
    The syscalls prototypes definitions are abused here to steal
    some metadata informations:
    
    - syscall name, param types, param names, number of params
    
    The syscall addr is not directly saved during this definition
    because we don't know if its prototype is available in the
    namespace. But we don't really need it. The arch has just to
    build a function able to resolve the syscall number to its
    metadata struct.
    
    The current tracer prints the syscall names, parameters names
    and values (and their types optionally). Currently the value is
    a raw hex but higher level values diplaying is on my TODO list.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1236955332-10133-2-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3d49daae47dc..d80ca0d464d9 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -194,6 +194,19 @@ struct kmemtrace_free_entry {
 	const void *ptr;
 };
 
+struct syscall_trace_enter {
+	struct trace_entry	ent;
+	int			nr;
+	unsigned long		args[];
+};
+
+struct syscall_trace_exit {
+	struct trace_entry	ent;
+	int			nr;
+	unsigned long		ret;
+};
+
+
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
@@ -306,6 +319,10 @@ extern void __ftrace_bad_type(void);
 			  TRACE_KMEM_ALLOC);	\
 		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
 			  TRACE_KMEM_FREE);	\
+		IF_ASSIGN(var, ent, struct syscall_trace_enter,		\
+			  TRACE_SYSCALL_ENTER);				\
+		IF_ASSIGN(var, ent, struct syscall_trace_exit,		\
+			  TRACE_SYSCALL_EXIT);				\
 		__ftrace_bad_type();					\
 	} while (0)
 

commit 321bb5e1ac461c04b6a93f795010d6eb01d8c5ca
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Fri Mar 13 10:50:27 2009 +0100

    x86, hw-branch-tracer: add selftest
    
    Add a selftest for the hw-branch-tracer.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    LKML-Reference: <20090313105027.A30183@sedona.ch.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 56ce34d90b03..e7fbc826f1e9 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -576,6 +576,8 @@ extern int trace_selftest_startup_sysprof(struct tracer *trace,
 					       struct trace_array *tr);
 extern int trace_selftest_startup_branch(struct tracer *trace,
 					 struct trace_array *tr);
+extern int trace_selftest_startup_hw_branches(struct tracer *trace,
+					      struct trace_array *tr);
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);

commit 62a394eb77a1ddea73273f53ed8c3ccf6e04f2fb
Merge: d2e82546ae98 1b3fa2ce6436
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 13 10:23:39 2009 +0100

    Merge branches 'tracing/ftrace' and 'tracing/syscalls'; commit 'v2.6.29-rc8' into tracing/core

commit ee08c6eccb7d1295516f7cf420fddf7b14e9146f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Mar 7 05:52:59 2009 +0100

    tracing/ftrace: syscall tracing infrastructure, basics
    
    Provide basic callbacks to do syscall tracing.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <1236401580-5758-2-git-send-email-fweisbec@gmail.com>
    [ simplified it to a trace_printk() for now. ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c5e1d8865fe4..3d49daae47dc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -30,6 +30,8 @@ enum trace_type {
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
 	TRACE_HW_BRANCHES,
+	TRACE_SYSCALL_ENTER,
+	TRACE_SYSCALL_EXIT,
 	TRACE_KMEM_ALLOC,
 	TRACE_KMEM_FREE,
 	TRACE_POWER,

commit bdc067582b8b71c7771bab076bbc51569c594fb4
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Mar 13 00:12:52 2009 -0400

    tracing: add comment for use of double __builtin_consant_p
    
    Impact: documentation
    
    The use of the double __builtin_contant_p checks in the event_trace_printk
    can be confusing to developers and reviewers. This patch adds a comment
    to explain why it is there.
    
    Requested-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    LKML-Reference: <20090313122235.43EB.A69D9226@jp.fujitsu.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 35cfa7bbaf38..67595b8f0f15 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -776,6 +776,11 @@ extern struct ftrace_event_call __stop_ftrace_events[];
 extern const char *__start___trace_bprintk_fmt[];
 extern const char *__stop___trace_bprintk_fmt[];
 
+/*
+ * The double __builtin_constant_p is because gcc will give us an error
+ * if we try to allocate the static variable to fmt if it is not a
+ * constant. Even with the outer if statement optimizing out.
+ */
 #define event_trace_printk(ip, fmt, args...)				\
 do {									\
 	__trace_printk_check_format(fmt, ##args);			\

commit e9fb2b6d5845e24f104713591286b6f39761c027
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 12 14:19:25 2009 -0400

    tracing: have event_trace_printk use static tracer
    
    Impact: speed up on event tracing
    
    The event_trace_printk is currently a wrapper function that calls
    trace_vprintk. Because it uses a variable for the fmt it misses out
    on the optimization of using the binary printk.
    
    This patch makes event_trace_printk into a macro wrapper to use the
    fmt as the same as the trace_printks.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cede1ab49d07..35cfa7bbaf38 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -773,4 +773,21 @@ void event_trace_printk(unsigned long ip, const char *fmt, ...);
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];
 
+extern const char *__start___trace_bprintk_fmt[];
+extern const char *__stop___trace_bprintk_fmt[];
+
+#define event_trace_printk(ip, fmt, args...)				\
+do {									\
+	__trace_printk_check_format(fmt, ##args);			\
+	tracing_record_cmdline(current);				\
+	if (__builtin_constant_p(fmt)) {				\
+		static const char *trace_printk_fmt			\
+		  __attribute__((section("__trace_printk_fmt"))) =	\
+			__builtin_constant_p(fmt) ? fmt : NULL;		\
+									\
+		__trace_bprintk(ip, trace_printk_fmt, ##args);		\
+	} else								\
+		__trace_printk(ip, fmt, ##args);			\
+} while (0)
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 48ead02030f849d011259244bb4ea9b985479006
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Mar 12 18:24:49 2009 +0100

    tracing/core: bring back raw trace_printk for dynamic formats strings
    
    Impact: fix callsites with dynamic format strings
    
    Since its new binary implementation, trace_printk() internally uses static
    containers for the format strings on each callsites. But the value is
    assigned once at build time, which means that it can't take dynamic
    formats.
    
    So this patch unearthes the raw trace_printk implementation for the callers
    that will need trace_printk to be able to carry these dynamic format
    strings. The trace_printk() macro will use the appropriate implementation
    for each callsite. Most of the time however, the binary implementation will
    still be used.
    
    The other impact of this patch is that mmiotrace_printk() will use the old
    implementation because it calls the low level trace_vprintk and we can't
    guess here whether the format passed in it is dynamic or not.
    
    Some parts of this patch have been written by Steven Rostedt (most notably
    the part that chooses the appropriate implementation for each callsites).
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 336324d717f8..cede1ab49d07 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -20,6 +20,7 @@ enum trace_type {
 	TRACE_WAKE,
 	TRACE_STACK,
 	TRACE_PRINT,
+	TRACE_BPRINT,
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
@@ -117,7 +118,7 @@ struct userstack_entry {
 /*
  * trace_printk entry:
  */
-struct print_entry {
+struct bprint_entry {
 	struct trace_entry	ent;
 	unsigned long		ip;
 	int			depth;
@@ -125,6 +126,13 @@ struct print_entry {
 	u32			buf[];
 };
 
+struct print_entry {
+	struct trace_entry	ent;
+	unsigned long		ip;
+	int			depth;
+	char			buf[];
+};
+
 #define TRACE_OLD_SIZE		88
 
 struct trace_field_cont {
@@ -286,6 +294,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct stack_entry, TRACE_STACK);	\
 		IF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
+		IF_ASSIGN(var, ent, struct bprint_entry, TRACE_BPRINT);	\
 		IF_ASSIGN(var, ent, struct special_entry, 0);		\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
 			  TRACE_MMIO_RW);				\
@@ -570,6 +579,8 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 extern void *head_page(struct trace_array_cpu *data);
 extern long ns2usecs(cycle_t nsec);
 extern int
+trace_vbprintk(unsigned long ip, int depth, const char *fmt, va_list args);
+extern int
 trace_vprintk(unsigned long ip, int depth, const char *fmt, va_list args);
 
 extern unsigned long trace_flags;

commit 1852fcce181faa237c010a3dbedb473cf9d4555f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Mar 11 14:33:00 2009 -0400

    tracing: expand the ring buffers when an event is activated
    
    To save memory, the tracer ring buffers are set to a minimum.
    The activating of a trace expands the ring buffer size. This patch
    adds this expanding, when an event is activated.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c5e1d8865fe4..336324d717f8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -737,6 +737,9 @@ static inline void trace_branch_disable(void)
 }
 #endif /* CONFIG_BRANCH_TRACER */
 
+/* set ring buffers to default size if not already done so */
+int tracing_update_buffers(void);
+
 /* trace event type bit fields, not numeric */
 enum {
 	TRACE_EVENT_TYPE_PRINTF		= 1,

commit da4d03020c2af32f73e8bfbab0a66620d85bb9bb
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 9 17:14:30 2009 -0400

    tracing: new format for specialized trace points
    
    Impact: clean up and enhancement
    
    The TRACE_EVENT_FORMAT macro looks quite ugly and is limited in its
    ability to save data as well as to print the record out. Working with
    Ingo Molnar, we came up with a new format that is much more pleasing to
    the eye of C developers. This new macro is more C style than the old
    macro, and is more obvious to what it does.
    
    Here's the example. The only updated macro in this patch is the
    sched_switch trace point.
    
    The old method looked like this:
    
     TRACE_EVENT_FORMAT(sched_switch,
            TP_PROTO(struct rq *rq, struct task_struct *prev,
                    struct task_struct *next),
            TP_ARGS(rq, prev, next),
            TP_FMT("task %s:%d ==> %s:%d",
                  prev->comm, prev->pid, next->comm, next->pid),
            TRACE_STRUCT(
                    TRACE_FIELD(pid_t, prev_pid, prev->pid)
                    TRACE_FIELD(int, prev_prio, prev->prio)
                    TRACE_FIELD_SPECIAL(char next_comm[TASK_COMM_LEN],
                                        next_comm,
                                        TP_CMD(memcpy(TRACE_ENTRY->next_comm,
                                                     next->comm,
                                                     TASK_COMM_LEN)))
                    TRACE_FIELD(pid_t, next_pid, next->pid)
                    TRACE_FIELD(int, next_prio, next->prio)
            ),
            TP_RAW_FMT("prev %d:%d ==> next %s:%d:%d")
            );
    
    The above method is hard to read and requires two format fields.
    
    The new method:
    
     /*
      * Tracepoint for task switches, performed by the scheduler:
      *
      * (NOTE: the 'rq' argument is not used by generic trace events,
      *        but used by the latency tracer plugin. )
      */
     TRACE_EVENT(sched_switch,
    
            TP_PROTO(struct rq *rq, struct task_struct *prev,
                     struct task_struct *next),
    
            TP_ARGS(rq, prev, next),
    
            TP_STRUCT__entry(
                    __array(        char,   prev_comm,      TASK_COMM_LEN   )
                    __field(        pid_t,  prev_pid                        )
                    __field(        int,    prev_prio                       )
                    __array(        char,   next_comm,      TASK_COMM_LEN   )
                    __field(        pid_t,  next_pid                        )
                    __field(        int,    next_prio                       )
            ),
    
            TP_printk("task %s:%d [%d] ==> %s:%d [%d]",
                    __entry->prev_comm, __entry->prev_pid, __entry->prev_prio,
                    __entry->next_comm, __entry->next_pid, __entry->next_prio),
    
            TP_fast_assign(
                    memcpy(__entry->next_comm, next->comm, TASK_COMM_LEN);
                    __entry->prev_pid       = prev->pid;
                    __entry->prev_prio      = prev->prio;
                    memcpy(__entry->prev_comm, prev->comm, TASK_COMM_LEN);
                    __entry->next_pid       = next->pid;
                    __entry->next_prio      = next->prio;
            )
     );
    
    This macro is called TRACE_EVENT, it is broken up into 5 parts:
    
     TP_PROTO:        the proto type of the trace point
     TP_ARGS:         the arguments of the trace point
     TP_STRUCT_entry: the structure layout of the entry in the ring buffer
     TP_printk:       the printk format
     TP_fast_assign:  the method used to write the entry into the ring buffer
    
    The structure is the definition of how the event will be saved in the
    ring buffer. The printk is used by the internal tracing in case of
    an oops, and the kernel needs to print out the format of the record
    to the console. This the TP_printk gives a means to show the records
    in a human readable format. It is also used to print out the data
    from the trace file.
    
    The TP_fast_assign is executed directly. It is basically like a C function,
    where the __entry is the handle to the record.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2bfb7d11fc17..c5e1d8865fe4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -751,12 +751,7 @@ struct ftrace_event_call {
 	int		(*regfunc)(void);
 	void		(*unregfunc)(void);
 	int		id;
-	struct dentry	*raw_dir;
-	int		raw_enabled;
-	int		type;
 	int		(*raw_init)(void);
-	int		(*raw_reg)(void);
-	void		(*raw_unreg)(void);
 	int		(*show_format)(struct trace_seq *s);
 };
 

commit 9de36825b321fe9fe9cf73260554251af579f4ca
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Mar 6 17:52:03 2009 +0100

    tracing: trace_bprintk() cleanups
    
    Impact: cleanup
    
    Remove a few leftovers and clean up the code a bit.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236356510-8381-5-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6140922392c8..2bfb7d11fc17 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -119,14 +119,11 @@ struct userstack_entry {
  */
 struct print_entry {
 	struct trace_entry	ent;
-	unsigned long 		ip;
+	unsigned long		ip;
 	int			depth;
 	const char		*fmt;
-	u32 			buf[];
+	u32			buf[];
 };
-#ifdef CONFIG_TRACE_BPRINTK
-extern int trace_bprintk_enable;
-#endif
 
 #define TRACE_OLD_SIZE		88
 
@@ -199,7 +196,7 @@ struct kmemtrace_free_entry {
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
  *  IRQS_OFF		- interrupts were disabled
- *  IRQS_NOSUPPORT 	- arch does not support irqs_disabled_flags
+ *  IRQS_NOSUPPORT	- arch does not support irqs_disabled_flags
  *  NEED_RESCED		- reschedule is requested
  *  HARDIRQ		- inside an interrupt handler
  *  SOFTIRQ		- inside a softirq handler
@@ -302,7 +299,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,	\
 			  TRACE_GRAPH_RET);		\
 		IF_ASSIGN(var, ent, struct hw_branch_entry, TRACE_HW_BRANCHES);\
- 		IF_ASSIGN(var, ent, struct trace_power, TRACE_POWER); \
+		IF_ASSIGN(var, ent, struct trace_power, TRACE_POWER); \
 		IF_ASSIGN(var, ent, struct kmemtrace_alloc_entry,	\
 			  TRACE_KMEM_ALLOC);	\
 		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
@@ -325,8 +322,8 @@ enum print_line_t {
  * flags value in struct tracer_flags.
  */
 struct tracer_opt {
-	const char 	*name; /* Will appear on the trace_options file */
-	u32 		bit; /* Mask assigned in val field in tracer_flags */
+	const char	*name; /* Will appear on the trace_options file */
+	u32		bit; /* Mask assigned in val field in tracer_flags */
 };
 
 /*
@@ -335,7 +332,7 @@ struct tracer_opt {
  */
 struct tracer_flags {
 	u32			val;
-	struct tracer_opt 	*opts;
+	struct tracer_opt	*opts;
 };
 
 /* Makes more easy to define a tracer opt */
@@ -390,7 +387,7 @@ struct tracer {
 	int			(*set_flag)(u32 old_flags, u32 bit, int set);
 	struct tracer		*next;
 	int			print_max;
-	struct tracer_flags 	*flags;
+	struct tracer_flags	*flags;
 	struct tracer_stat	*stats;
 };
 

commit 769b0441f438c4bb4872cb8560eb6fe51bcc09ee
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 6 17:21:49 2009 +0100

    tracing/core: drop the old trace_printk() implementation in favour of trace_bprintk()
    
    Impact: faster and lighter tracing
    
    Now that we have trace_bprintk() which is faster and consume lesser
    memory than trace_printk() and has the same purpose, we can now drop
    the old implementation in favour of the binary one from trace_bprintk(),
    which means we move all the implementation of trace_bprintk() to
    trace_printk(), so the Api doesn't change except that we must now use
    trace_seq_bprintk() to print the TRACE_PRINT entries.
    
    Some changes result of this:
    
    - Previously, trace_bprintk depended of a single tracer and couldn't
      work without. This tracer has been dropped and the whole implementation
      of trace_printk() (like the module formats management) is now integrated
      in the tracing core (comes with CONFIG_TRACING), though we keep the file
      trace_printk (previously trace_bprintk.c) where we can find the module
      management. Thus we don't overflow trace.c
    
    - changes some parts to use trace_seq_bprintk() to print TRACE_PRINT entries.
    
    - change a bit trace_printk/trace_vprintk macros to support non-builtin formats
      constants, and fix 'const' qualifiers warnings. But this is all transparent for
      developers.
    
    - etc...
    
    V2:
    
    - Rebase against last changes
    - Fix mispell on the changelog
    
    V3:
    
    - Rebase against last changes (moving trace_printk() to kernel.h)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236356510-8381-5-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0f5077f8f957..6140922392c8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -20,7 +20,6 @@ enum trace_type {
 	TRACE_WAKE,
 	TRACE_STACK,
 	TRACE_PRINT,
-	TRACE_BPRINTK,
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
@@ -120,16 +119,10 @@ struct userstack_entry {
  */
 struct print_entry {
 	struct trace_entry	ent;
-	unsigned long		ip;
+	unsigned long 		ip;
 	int			depth;
-	char			buf[];
-};
-
-struct bprintk_entry {
-	struct trace_entry ent;
-	unsigned long ip;
-	const char *fmt;
-	u32 buf[];
+	const char		*fmt;
+	u32 			buf[];
 };
 #ifdef CONFIG_TRACE_BPRINTK
 extern int trace_bprintk_enable;
@@ -296,7 +289,6 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct stack_entry, TRACE_STACK);	\
 		IF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
-		IF_ASSIGN(var, ent, struct bprintk_entry, TRACE_BPRINTK);\
 		IF_ASSIGN(var, ent, struct special_entry, 0);		\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
 			  TRACE_MMIO_RW);				\

commit 1427cdf0592368bdec57276edaf714040ee8744f
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Fri Mar 6 17:21:47 2009 +0100

    tracing: infrastructure for supporting binary record
    
    Impact: save on memory for tracing
    
    Current tracers are typically using a struct(like struct ftrace_entry,
    struct ctx_switch_entry, struct special_entr etc...)to record a binary
    event. These structs can only record a their own kind of events.
    A new kind of tracer need a new struct and a lot of code too handle it.
    
    So we need a generic binary record for events. This infrastructure
    is for this purpose.
    
    [fweisbec@gmail.com: rebase against latest -tip, make it safe while sched
    tracing as reported by Steven Rostedt]
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1236356510-8381-3-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8beff03fda68..0f5077f8f957 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -20,6 +20,7 @@ enum trace_type {
 	TRACE_WAKE,
 	TRACE_STACK,
 	TRACE_PRINT,
+	TRACE_BPRINTK,
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
@@ -124,6 +125,16 @@ struct print_entry {
 	char			buf[];
 };
 
+struct bprintk_entry {
+	struct trace_entry ent;
+	unsigned long ip;
+	const char *fmt;
+	u32 buf[];
+};
+#ifdef CONFIG_TRACE_BPRINTK
+extern int trace_bprintk_enable;
+#endif
+
 #define TRACE_OLD_SIZE		88
 
 struct trace_field_cont {
@@ -285,6 +296,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct stack_entry, TRACE_STACK);	\
 		IF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
+		IF_ASSIGN(var, ent, struct bprintk_entry, TRACE_BPRINTK);\
 		IF_ASSIGN(var, ent, struct special_entry, 0);		\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
 			  TRACE_MMIO_RW);				\

commit 5e1607a00bd082972629d3d68c95c8bcf902b55a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Mar 5 10:24:48 2009 +0100

    tracing: rename ftrace_printk() => trace_printk()
    
    Impact: cleanup
    
    Use a more generic name - this also allows the prototype to move
    to kernel.h and be generally available to kernel developers who
    want to do some quick tracing.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 12cd119cca32..8beff03fda68 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -115,7 +115,7 @@ struct userstack_entry {
 };
 
 /*
- * ftrace_printk entry:
+ * trace_printk entry:
  */
 struct print_entry {
 	struct trace_entry	ent;

commit c032ef64d680717e4e8ce3da65da6419a35f8a2c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Mar 4 20:34:24 2009 -0500

    tracing: add latency output format option
    
    With the removal of the latency_trace file, we lost the ability
    to see some of the finer details in a trace. Like the state of
    interrupts enabled, the preempt count, need resched, and if we
    are in an interrupt handler, softirq handler or not.
    
    This patch simply creates an option to bring back the old format.
    This also removes the warning about an unused variable that held
    the latency_trace file operations.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 561bb5c5d988..12cd119cca32 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -651,7 +651,8 @@ enum trace_iterator_flags {
 	TRACE_ITER_USERSTACKTRACE       = 0x4000,
 	TRACE_ITER_SYM_USEROBJ          = 0x8000,
 	TRACE_ITER_PRINTK_MSGONLY	= 0x10000,
-	TRACE_ITER_CONTEXT_INFO		= 0x20000 /* Print pid/cpu/time */
+	TRACE_ITER_CONTEXT_INFO		= 0x20000, /* Print pid/cpu/time */
+	TRACE_ITER_LATENCY_FMT		= 0x40000,
 };
 
 /*

commit 2cadf9135eb3b6d84b6427314be827ddd443c308
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Dec 1 22:20:19 2008 -0500

    tracing: add binary buffer files for use with splice
    
    Impact: new feature
    
    This patch creates a directory of files that correspond to the
    per CPU ring buffers. These are binary files and are made to
    be used with splice. This is the fastest way to extract data from
    the ftrace ring buffers.
    
    Thanks to Jiaying Zhang for pushing me to get this code fixed,
     and to Eduard - Gabriel Munteanu for his splice code that helped
     me debug my code.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e606633fb498..561bb5c5d988 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -217,6 +217,7 @@ enum trace_flag_type {
  */
 struct trace_array_cpu {
 	atomic_t		disabled;
+	void			*buffer_page;	/* ring buffer spare */
 
 	/* these fields get copied into max-trace: */
 	unsigned long		trace_idx;

commit 981d081ec8b958b7d962ee40d433581a55d40fc5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 2 13:53:59 2009 -0500

    tracing: add format file to describe event struct fields
    
    This patch adds the "format" file to the trace point event directory.
    This is based off of work by Tom Zanussi, in which a file is exported
    to be tread from user land such that a user space app may read the
    binary record stored in the ring buffer.
    
     # cat /debug/tracing/events/sched/sched_switch/format
            field:pid_t prev_pid;   offset:12;      size:4;
            field:int prev_prio;    offset:16;      size:4;
            field special:char next_comm[TASK_COMM_LEN];    offset:20;      size:16;
            field:pid_t next_pid;   offset:36;      size:4;
            field:int next_prio;    offset:40;      size:4;
    
    Idea-from: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cf6ba4181b14..e606633fb498 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -754,6 +754,7 @@ struct ftrace_event_call {
 	int		(*raw_init)(void);
 	int		(*raw_reg)(void);
 	void		(*raw_unreg)(void);
+	int		(*show_format)(struct trace_seq *s);
 };
 
 void event_trace_printk(unsigned long ip, const char *fmt, ...);

commit f9520750c4c9924c14325cd951efae5fae58104c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 2 14:04:40 2009 -0500

    tracing: make trace_seq_reset global and rename to trace_seq_init
    
    Impact: clean up
    
    The trace_seq functions may be used separately outside of the ftrace
    iterator. The trace_seq_reset is needed for these operations.
    
    This patch also renames trace_seq_reset to the more appropriate
    trace_seq_init.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f6fa0b9f83a8..cf6ba4181b14 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -395,6 +395,14 @@ struct trace_seq {
 	unsigned int		readpos;
 };
 
+static inline void
+trace_seq_init(struct trace_seq *s)
+{
+	s->len = 0;
+	s->readpos = 0;
+}
+
+
 #define TRACE_PIPE_ALL_CPU	-1
 
 /*

commit fd99498989f3b3feeab89dcadf537138ba136d24
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Feb 28 02:41:25 2009 -0500

    tracing: add raw fast tracing interface for trace events
    
    This patch adds the interface to enable the C style trace points.
    In the directory /debugfs/tracing/events/subsystem/event
    We now have three files:
    
     enable : values 0 or 1 to enable or disable the trace event.
    
     available_types: values 'raw' and 'printf' which indicate the tracing
           types available for the trace point. If a developer does not
           use the TRACE_EVENT_FORMAT macro and just uses the TRACE_FORMAT
           macro, then only 'printf' will be available. This file is
           read only.
    
     type: values 'raw' or 'printf'. This indicates which type of tracing
           is active for that trace point. 'printf' is the default and
           if 'raw' is not available, this file is read only.
    
     # echo raw > /debug/tracing/events/sched/sched_wakeup/type
     # echo 1 > /debug/tracing/events/sched/sched_wakeup/enable
    
     Will enable the C style tracing for the sched_wakeup trace point.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index aa1ab0cb80ab..f6fa0b9f83a8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -726,6 +726,12 @@ static inline void trace_branch_disable(void)
 }
 #endif /* CONFIG_BRANCH_TRACER */
 
+/* trace event type bit fields, not numeric */
+enum {
+	TRACE_EVENT_TYPE_PRINTF		= 1,
+	TRACE_EVENT_TYPE_RAW		= 2,
+};
+
 struct ftrace_event_call {
 	char		*name;
 	char		*system;
@@ -736,6 +742,7 @@ struct ftrace_event_call {
 	int		id;
 	struct dentry	*raw_dir;
 	int		raw_enabled;
+	int		type;
 	int		(*raw_init)(void);
 	int		(*raw_reg)(void);
 	void		(*raw_unreg)(void);

commit c32e827b25054cb17b79cf97fb5e63ae4ce2223c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Feb 27 19:12:30 2009 -0500

    tracing: add raw trace point recording infrastructure
    
    Impact: lower overhead tracing
    
    The current event tracer can automatically pick up trace points
    that are registered with the TRACE_FORMAT macro. But it required
    a printf format string and parsing. Although, this adds the ability
    to get guaranteed information like task names and such, it took
    a hit in overhead processing. This processing can add about 500-1000
    nanoseconds overhead, but in some cases that too is considered
    too much and we want to shave off as much from this overhead as
    possible.
    
    Tom Zanussi recently posted tracing patches to lkml that are based
    on a nice idea about capturing the data via C structs using
    STRUCT_ENTER, STRUCT_EXIT type of macros.
    
    I liked that method very much, but did not like the implementation
    that required a developer to add data/code in several disjoint
    locations.
    
    This patch extends the event_tracer macros to do a similar "raw C"
    approach that Tom Zanussi did. But instead of having the developers
    needing to tweak a bunch of code all over the place, they can do it
    all in one macro - preferably placed near the code that it is
    tracing. That makes it much more likely that tracepoints will be
    maintained on an ongoing basis by the code they modify.
    
    The new macro TRACE_EVENT_FORMAT is created for this approach. (Note,
    a developer may still utilize the more low level DECLARE_TRACE macros
    if they don't care about getting their traces automatically in the event
    tracer.)
    
    They can also use the existing TRACE_FORMAT if they don't need to code
    the tracepoint in C, but just want to use the convenience of printf.
    
    So if the developer wants to "hardwire" a tracepoint in the fastest
    possible way, and wants to acquire their data via a user space utility
    in a raw binary format, or wants to see it in the trace output but not
    sacrifice any performance, then they can implement the faster but
    more complex TRACE_EVENT_FORMAT macro.
    
    Here's what usage looks like:
    
      TRACE_EVENT_FORMAT(name,
            TPPROTO(proto),
            TPARGS(args),
            TPFMT(fmt, fmt_args),
            TRACE_STUCT(
                    TRACE_FIELD(type1, item1, assign1)
                    TRACE_FIELD(type2, item2, assign2)
                            [...]
            ),
            TPRAWFMT(raw_fmt)
            );
    
    Note name, proto, args, and fmt, are all identical to what TRACE_FORMAT
    uses.
    
     name: is the unique identifier of the trace point
     proto: The proto type that the trace point uses
     args: the args in the proto type
     fmt: printf format to use with the event printf tracer
     fmt_args: the printf argments to match fmt
    
     TRACE_STRUCT starts the ability to create a structure.
     Each item in the structure is defined with a TRACE_FIELD
    
      TRACE_FIELD(type, item, assign)
    
     type: the C type of item.
     item: the name of the item in the stucture
     assign: what to assign the item in the trace point callback
    
     raw_fmt is a way to pretty print the struct. It must match
      the order of the items are added in TRACE_STUCT
    
     An example of this would be:
    
     TRACE_EVENT_FORMAT(sched_wakeup,
            TPPROTO(struct rq *rq, struct task_struct *p, int success),
            TPARGS(rq, p, success),
            TPFMT("task %s:%d %s",
                  p->comm, p->pid, success?"succeeded":"failed"),
            TRACE_STRUCT(
                    TRACE_FIELD(pid_t, pid, p->pid)
                    TRACE_FIELD(int, success, success)
            ),
            TPRAWFMT("task %d success=%d")
            );
    
     This creates us a unique struct of:
    
     struct {
            pid_t           pid;
            int             success;
     };
    
     And the way the call back would assign these values would be:
    
            entry->pid = p->pid;
            entry->success = success;
    
    The nice part about this is that the creation of the assignent is done
    via macro magic in the event tracer.  Once the TRACE_EVENT_FORMAT is
    created, the developer will then have a faster method to record
    into the ring buffer. They do not need to worry about the tracer itself.
    
    The developer would only need to touch the files in include/trace/*.h
    
    Again, I would like to give special thanks to Tom Zanussi for this
    nice idea.
    
    Idea-from: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index adf161f6dd11..aa1ab0cb80ab 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -726,4 +726,23 @@ static inline void trace_branch_disable(void)
 }
 #endif /* CONFIG_BRANCH_TRACER */
 
+struct ftrace_event_call {
+	char		*name;
+	char		*system;
+	struct dentry	*dir;
+	int		enabled;
+	int		(*regfunc)(void);
+	void		(*unregfunc)(void);
+	int		id;
+	struct dentry	*raw_dir;
+	int		raw_enabled;
+	int		(*raw_init)(void);
+	int		(*raw_reg)(void);
+	void		(*raw_unreg)(void);
+};
+
+void event_trace_printk(unsigned long ip, const char *fmt, ...);
+extern struct ftrace_event_call __start_ftrace_events[];
+extern struct ftrace_event_call __stop_ftrace_events[];
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit ef5580d0fffce6e0a01043bac0625128b5d409a7
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Feb 27 19:38:04 2009 -0500

    tracing: add interface to write into current tracer buffer
    
    Right now all tracers must manage their own trace buffers. This was
    to enforce tracers to be independent in case we finally decide to
    allow each tracer to have their own trace buffer.
    
    But now we are adding event tracing that writes to the current tracer's
    buffer. This adds an interface to allow events to write to the current
    tracer buffer without having to manage its own. Since event tracing
    has no "tracer", and is just a way to hook into any other tracer.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 632191770aac..adf161f6dd11 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -442,6 +442,12 @@ void trace_buffer_unlock_commit(struct trace_array *tr,
 				struct ring_buffer_event *event,
 				unsigned long flags, int pc);
 
+struct ring_buffer_event *
+trace_current_buffer_lock_reserve(unsigned char type, unsigned long len,
+				  unsigned long flags, int pc);
+void trace_current_buffer_unlock_commit(struct ring_buffer_event *event,
+					unsigned long flags, int pc);
+
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);
 

commit d7350c3f45694104e820041969c8185c5f99e57c
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Feb 25 06:13:16 2009 +0100

    tracing/core: make the read callbacks reentrants
    
    Now that several per-cpu files can be read or spliced at the
    same, we want the read/splice callbacks for tracing files to be
    reentrants.
    
    Until now, a single global mutex (trace_types_lock) serialized
    the access to tracing_read_pipe(), tracing_splice_read_pipe(),
    and the seq helpers.
    
    Ie: it means that if a user tries to read trace_pipe0 and
    trace_pipe1 at the same time, the access to the function
    tracing_read_pipe() is contended and one reader must wait for
    the other to finish its read call.
    
    The trace_type_lock mutex is mostly here to serialize the access
    to the global current tracer (current_trace), which can be
    changed concurrently. Although the iter struct keeps a private
    pointer to this tracer, its callbacks can be changed by another
    function.
    
    The method used here is to not keep anymore private reference to
    the tracer inside the iterator but to make a copy of it inside
    the iterator. Then it checks on subsequents read calls if the
    tracer has changed. This is not costly because the current
    tracer is not expected to be changed often, so we use a branch
    prediction for that.
    
    Moreover, we add a private mutex to the iterator (there is one
    iterator per file descriptor) to serialize the accesses in case
    of multiple consumers per file descriptor (which would be a
    silly idea from the user). Note that this is not to protect the
    ring buffer, since the ring buffer already serializes the
    readers accesses. This is to prevent from traces weirdness in
    case of concurrent consumers. But these mutexes can be dropped
    anyway, that would not result in any crash. Just tell me what
    you think about it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 508235a39da6..632191770aac 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -405,8 +405,9 @@ struct trace_iterator {
 	struct trace_array	*tr;
 	struct tracer		*trace;
 	void			*private;
-	struct ring_buffer_iter	*buffer_iter[NR_CPUS];
 	int			cpu_file;
+	struct mutex		mutex;
+	struct ring_buffer_iter	*buffer_iter[NR_CPUS];
 
 	/* The below is zeroed out in pipe_read */
 	struct trace_seq	seq;

commit b04cc6b1f6398b0e0b60d37e27ce51b4899672ec
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Feb 25 03:22:28 2009 +0100

    tracing/core: introduce per cpu tracing files
    
    Impact: split up tracing output per cpu
    
    Currently, on the tracing debugfs directory, three files are
    available to the user to let him extracting the trace output:
    
    - trace is an iterator through the ring-buffer. It's a reader
      but not a consumer It doesn't block when no more traces are
      available.
    
    - trace pretty similar to the former, except that it adds more
      informations such as prempt count, irq flag, ...
    
    - trace_pipe is a reader and a consumer, it will also block
      waiting for traces if necessary (heh, yes it's a pipe).
    
    The traces coming from different cpus are curretly mixed up
    inside these files. Sometimes it messes up the informations,
    sometimes it's useful, depending on what does the tracer
    capture.
    
    The tracing_cpumask file is useful to filter the output and
    select only the traces captured a custom defined set of cpus.
    But still it is not enough powerful to extract at the same time
    one trace buffer per cpu.
    
    So this patch creates a new directory: /debug/tracing/per_cpu/.
    
    Inside this directory, you will now find one trace_pipe file and
    one trace file per cpu.
    
    Which means if you have two cpus, you will have:
    
     trace0
     trace1
     trace_pipe0
     trace_pipe1
    
    And of course, reading these files will have the same effect
    than with the usual tracing files, except that you will only see
    the traces from the given cpu.
    
    The original all-in-one cpu trace file are still available on
    their original place.
    
    Until now, only one consumer was allowed on trace_pipe to avoid
    racy consuming on the ring-buffer. Now the approach changed a
    bit, you can have only one consumer per cpu.
    
    Which means you are allowed to read concurrently trace_pipe0 and
    trace_pipe1 But you can't have two readers on trace_pipe0 or
    trace_pipe1.
    
    Following the same logic, if there is one reader on the common
    trace_pipe, you can not have at the same time another reader on
    trace_pipe0 or in trace_pipe1. Because in trace_pipe is already
    a consumer in all cpu buffers in essence.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index eed732c151fc..508235a39da6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -395,6 +395,8 @@ struct trace_seq {
 	unsigned int		readpos;
 };
 
+#define TRACE_PIPE_ALL_CPU	-1
+
 /*
  * Trace iterator - used by printout routines who present trace
  * results to users and which routines might sleep, etc:
@@ -404,6 +406,7 @@ struct trace_iterator {
 	struct tracer		*trace;
 	void			*private;
 	struct ring_buffer_iter	*buffer_iter[NR_CPUS];
+	int			cpu_file;
 
 	/* The below is zeroed out in pipe_read */
 	struct trace_seq	seq;

commit 6eaaa5d57e76c454479833fc8594cd7c3b75c789
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Feb 11 02:25:00 2009 +0100

    tracing/core: use appropriate waiting on trace_pipe
    
    Impact: api and pipe waiting change
    
    Currently, the waiting used in tracing_read_pipe() is done through a
    100 msecs schedule_timeout() loop which periodically check if there
    are traces on the buffer.
    
    This can cause small latencies for programs which are reading the incoming
    events.
    
    This patch makes the reader waiting for the trace_wait waitqueue except
    for few tracers such as the sched and functions tracers which might be
    already hold the runqueue lock while waking up the reader.
    
    This is performed through a new callback wait_pipe() on struct tracer.
    If none is implemented on a specific tracer, the default waiting for
    trace_wait queue is attached.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dbff0207b213..eed732c151fc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -337,18 +337,34 @@ struct tracer_flags {
 #define TRACER_OPT(s, b)	.name = #s, .bit = b
 
 
-/*
- * A specific tracer, represented by methods that operate on a trace array:
+/**
+ * struct tracer - a specific tracer and its callbacks to interact with debugfs
+ * @name: the name chosen to select it on the available_tracers file
+ * @init: called when one switches to this tracer (echo name > current_tracer)
+ * @reset: called when one switches to another tracer
+ * @start: called when tracing is unpaused (echo 1 > tracing_enabled)
+ * @stop: called when tracing is paused (echo 0 > tracing_enabled)
+ * @open: called when the trace file is opened
+ * @pipe_open: called when the trace_pipe file is opened
+ * @wait_pipe: override how the user waits for traces on trace_pipe
+ * @close: called when the trace file is released
+ * @read: override the default read callback on trace_pipe
+ * @splice_read: override the default splice_read callback on trace_pipe
+ * @selftest: selftest to run on boot (see trace_selftest.c)
+ * @print_headers: override the first lines that describe your columns
+ * @print_line: callback that prints a trace
+ * @set_flag: signals one of your private flags changed (trace_options file)
+ * @flags: your private flags
  */
 struct tracer {
 	const char		*name;
-	/* Your tracer should raise a warning if init fails */
 	int			(*init)(struct trace_array *tr);
 	void			(*reset)(struct trace_array *tr);
 	void			(*start)(struct trace_array *tr);
 	void			(*stop)(struct trace_array *tr);
 	void			(*open)(struct trace_iterator *iter);
 	void			(*pipe_open)(struct trace_iterator *iter);
+	void			(*wait_pipe)(struct trace_iterator *iter);
 	void			(*close)(struct trace_iterator *iter);
 	ssize_t			(*read)(struct trace_iterator *iter,
 					struct file *filp, char __user *ubuf,
@@ -432,6 +448,9 @@ void tracing_generic_entry_update(struct trace_entry *entry,
 				  unsigned long flags,
 				  int pc);
 
+void default_wait_pipe(struct trace_iterator *iter);
+void poll_wait_pipe(struct trace_iterator *iter);
+
 void ftrace(struct trace_array *tr,
 			    struct trace_array_cpu *data,
 			    unsigned long ip,

commit 3c56819b14b00dd449bd776303e61f8532fad09f
Author: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
Date:   Mon Feb 9 08:15:56 2009 +0200

    tracing: splice support for tracing_pipe
    
    Added and implemented tracing_pipe_fops->splice_read(). This allows
    userspace programs to get tracing data more efficiently.
    
    Signed-off-by: Eduard - Gabriel Munteanu <eduard.munteanu@linux360.ro>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7b0518adf6d7..dbff0207b213 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -353,6 +353,12 @@ struct tracer {
 	ssize_t			(*read)(struct trace_iterator *iter,
 					struct file *filp, char __user *ubuf,
 					size_t cnt, loff_t *ppos);
+	ssize_t			(*splice_read)(struct trace_iterator *iter,
+					       struct file *filp,
+					       loff_t *ppos,
+					       struct pipe_inode_info *pipe,
+					       size_t len,
+					       unsigned int flags);
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 	int			(*selftest)(struct tracer *trace,
 					    struct trace_array *tr);

commit b91facc367366b3f71375f337eb5997ec9ab4e69
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Feb 6 18:30:44 2009 +0100

    tracing/function-graph-tracer: handle the leaf functions from trace_pipe
    
    When one cats the trace file, the leaf functions are printed without brackets:
    
     function();
    
    whereas in the trace_pipe file we'll see the following:
    
     function() {
     }
    
    This is because the ring_buffer handling is not the same between those two files.
    On the trace file, when an entry is printed, the iterator advanced and then we can
    check the next entry.
    
    There is no iterator with trace_pipe, the current entry to print has been peeked
    and not consumed. So checking the next entry will still return the current one while
    we don't consume it.
    
    This patch introduces a new value for the output callbacks to ask the tracing
    core to not consume the current entry after printing it.
    
    We need it because we will have to consume the current entry ourself to check
    the next one.
    
    Now the trace_pipe is able to handle well the leaf functions.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 1ecfb9d2b365..7b0518adf6d7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -63,13 +63,13 @@ struct ftrace_entry {
 
 /* Function call entry */
 struct ftrace_graph_ent_entry {
-	struct trace_entry			ent;
+	struct trace_entry		ent;
 	struct ftrace_graph_ent		graph_ent;
 };
 
 /* Function return entry */
 struct ftrace_graph_ret_entry {
-	struct trace_entry			ent;
+	struct trace_entry		ent;
 	struct ftrace_graph_ret		ret;
 };
 extern struct tracer boot_tracer;
@@ -309,7 +309,8 @@ extern void __ftrace_bad_type(void);
 enum print_line_t {
 	TRACE_TYPE_PARTIAL_LINE	= 0,	/* Retry after flushing the seq */
 	TRACE_TYPE_HANDLED	= 1,
-	TRACE_TYPE_UNHANDLED	= 2	/* Relay to other output functions */
+	TRACE_TYPE_UNHANDLED	= 2,	/* Relay to other output functions */
+	TRACE_TYPE_NO_CONSUME	= 3	/* Handled but ask to not consume */
 };
 
 

commit 1292211058aaf872eeb2a0e2677d237916b4501f
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Feb 7 22:16:12 2009 +0100

    tracing/power: move the power trace headers to a dedicated file
    
    Impact: cleanup
    
    Move the power tracer headers to trace/power.h to keep ftrace.h and power bits
    more easy to maintain as separated topics.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Arjan van de Ven <arjan@infradead.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a011ec062225..1ecfb9d2b365 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -10,6 +10,7 @@
 #include <linux/ftrace.h>
 #include <trace/boot.h>
 #include <trace/kmemtrace.h>
+#include <trace/power.h>
 
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,

commit 7447dce96f2233d250bc39a4a10a42f7c3dd46fc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Feb 7 21:33:57 2009 +0100

    tracing/function-graph-tracer: provide a selftest for the function graph tracer
    
    Making it more easy to do a basic regression test for this tracer.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b9838f4a6929..a011ec062225 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -500,6 +500,8 @@ extern int DYN_FTRACE_TEST_NAME(void);
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 extern int trace_selftest_startup_function(struct tracer *trace,
 					   struct trace_array *tr);
+extern int trace_selftest_startup_function_graph(struct tracer *trace,
+						 struct trace_array *tr);
 extern int trace_selftest_startup_irqsoff(struct tracer *trace,
 					  struct trace_array *tr);
 extern int trace_selftest_startup_preemptoff(struct tracer *trace,

commit 44b0635481437140b0e29d6023f05e805d5e7620
Merge: 4ad476e11f94 57794a9d48b6
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Feb 9 10:35:12 2009 +0100

    Merge branch 'tip/tracing/core/devel' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into tracing/ftrace
    
    Conflicts:
            kernel/trace/trace_hw_branches.c

commit 57794a9d48b63e34acbe63282628c9f029603308
Author: Wenji Huang <wenji.huang@oracle.com>
Date:   Fri Feb 6 17:33:27 2009 +0800

    trace: trivial fixes in comment typos.
    
    Impact: clean up
    
    Fixed several typos in the comments.
    
    Signed-off-by: Wenji Huang <wenji.huang@oracle.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5efc4c707f7e..f92aba52a894 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -616,12 +616,12 @@ extern struct tracer nop_trace;
  * preempt_enable (after a disable), a schedule might take place
  * causing an infinite recursion.
  *
- * To prevent this, we read the need_recshed flag before
+ * To prevent this, we read the need_resched flag before
  * disabling preemption. When we want to enable preemption we
  * check the flag, if it is set, then we call preempt_enable_no_resched.
  * Otherwise, we call preempt_enable.
  *
- * The rational for doing the above is that if need resched is set
+ * The rational for doing the above is that if need_resched is set
  * and we have yet to reschedule, we are either in an atomic location
  * (where we do not need to check for scheduling) or we are inside
  * the scheduler and do not want to resched.
@@ -642,7 +642,7 @@ static inline int ftrace_preempt_disable(void)
  *
  * This is a scheduler safe way to enable preemption and not miss
  * any preemption checks. The disabled saved the state of preemption.
- * If resched is set, then we were either inside an atomic or
+ * If resched is set, then we are either inside an atomic or
  * are inside the scheduler (we would have already scheduled
  * otherwise). In this case, we do not want to call normal
  * preempt_enable, but preempt_enable_no_resched instead.

commit 1830b52d0de8c60c4f5dfbac134aa8f69d815801
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Feb 7 19:38:43 2009 -0500

    trace: remove deprecated entry->cpu
    
    Impact: fix to prevent developers from using entry->cpu
    
    With the new ring buffer infrastructure, the cpu for the entry is
    implicit with which CPU buffer it is on.
    
    The original code use to record the current cpu into the generic
    entry header, which can be retrieved by entry->cpu. When the
    ring buffer was introduced, the users were convert to use the
    the cpu number of which cpu ring buffer was in use (this was passed
    to the tracers by the iterator: iter->cpu).
    
    Unfortunately, the cpu item in the entry structure was never removed.
    This allowed for developers to use it instead of the proper iter->cpu,
    unknowingly, using an uninitialized variable. This was not the fault
    of the developers, since it would seem like the logical place to
    retrieve the cpu identifier.
    
    This patch removes the cpu item from the entry structure and fixes
    all the users that should have been using iter->cpu.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f0c7a0f08cac..5efc4c707f7e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -45,7 +45,6 @@ enum trace_type {
  */
 struct trace_entry {
 	unsigned char		type;
-	unsigned char		cpu;
 	unsigned char		flags;
 	unsigned char		preempt_count;
 	int			pid;

commit b6f11df26fdc28324cf9c9e3b77f2dc985c1bb13
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Feb 5 18:02:00 2009 -0200

    trace: Call tracing_reset_online_cpus before tracer->init()
    
    Impact: cleanup
    
    To make it easy for ftrace plugin writers, as this was open coded in
    the existing plugins
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Frédéric Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e03f157c772e..f2742fb1575a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -395,6 +395,7 @@ struct trace_iterator {
 	cpumask_var_t		started;
 };
 
+int tracer_init(struct tracer *t, struct trace_array *tr);
 int tracing_is_enabled(void);
 void trace_wake_up(void);
 void tracing_reset(struct trace_array *tr, int cpu);

commit 51a763dd84253bab1d0a1e68e11a7753d1b702ca
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Feb 5 16:14:13 2009 -0200

    tracing: Introduce trace_buffer_{lock_reserve,unlock_commit}
    
    Impact: new API
    
    These new functions do what previously was being open coded, reducing
    the number of details ftrace plugin writers have to worry about.
    
    It also standardizes the handling of stacktrace, userstacktrace and
    other trace options we may introduce in the future.
    
    With this patch, for instance, the blk tracer (and some others already
    in the tree) can use the "userstacktrace" /d/tracing/trace_options
    facility.
    
    $ codiff /tmp/vmlinux.before /tmp/vmlinux.after
    linux-2.6-tip/kernel/trace/trace.c:
      trace_vprintk              |   -5
      trace_graph_return         |  -22
      trace_graph_entry          |  -26
      trace_function             |  -45
      __ftrace_trace_stack       |  -27
      ftrace_trace_userstack     |  -29
      tracing_sched_switch_trace |  -66
      tracing_stop               |   +1
      trace_seq_to_user          |   -1
      ftrace_trace_special       |  -63
      ftrace_special             |   +1
      tracing_sched_wakeup_trace |  -70
      tracing_reset_online_cpus  |   -1
     13 functions changed, 2 bytes added, 355 bytes removed, diff: -353
    
    linux-2.6-tip/block/blktrace.c:
      __blk_add_trace |  -58
     1 function changed, 58 bytes removed, diff: -58
    
    linux-2.6-tip/kernel/trace/trace.c:
      trace_buffer_lock_reserve  |  +88
      trace_buffer_unlock_commit |  +86
     2 functions changed, 174 bytes added, diff: +174
    
    /tmp/vmlinux.after:
     16 functions changed, 176 bytes added, 413 bytes removed, diff: -237
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Acked-by: Frédéric Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index df627a948694..e03f157c772e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -403,6 +403,17 @@ int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *tracing_init_dentry(void);
 void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 
+struct ring_buffer_event;
+
+struct ring_buffer_event *trace_buffer_lock_reserve(struct trace_array *tr,
+						    unsigned char type,
+						    unsigned long len,
+						    unsigned long flags,
+						    int pc);
+void trace_buffer_unlock_commit(struct trace_array *tr,
+				struct ring_buffer_event *event,
+				unsigned long flags, int pc);
+
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);
 

commit 7be421510b91491d5aa5a29fa1005712039b95af
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Thu Feb 5 01:13:37 2009 -0500

    trace: Remove unused trace_array_cpu parameter
    
    Impact: cleanup
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f0c7a0f08cac..df627a948694 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -419,14 +419,12 @@ void ftrace(struct trace_array *tr,
 			    unsigned long parent_ip,
 			    unsigned long flags, int pc);
 void tracing_sched_switch_trace(struct trace_array *tr,
-				struct trace_array_cpu *data,
 				struct task_struct *prev,
 				struct task_struct *next,
 				unsigned long flags, int pc);
 void tracing_record_cmdline(struct task_struct *tsk);
 
 void tracing_sched_wakeup_trace(struct trace_array *tr,
-				struct trace_array_cpu *data,
 				struct task_struct *wakee,
 				struct task_struct *cur,
 				unsigned long flags, int pc);
@@ -436,7 +434,6 @@ void trace_special(struct trace_array *tr,
 		   unsigned long arg2,
 		   unsigned long arg3, int pc);
 void trace_function(struct trace_array *tr,
-		    struct trace_array_cpu *data,
 		    unsigned long ip,
 		    unsigned long parent_ip,
 		    unsigned long flags, int pc);
@@ -462,7 +459,6 @@ void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
 
 void __trace_stack(struct trace_array *tr,
-		   struct trace_array_cpu *data,
 		   unsigned long flags,
 		   int skip, int pc);
 

commit c4a8e8be2d43cc22b371e8e9c05c253409759d94
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Feb 2 20:29:21 2009 -0200

    trace: better manage the context info for events
    
    Impact: make trace_event more convenient for tracers
    
    All tracers (for the moment) that use the struct trace_event want to
    have the context info printed before their own output: the pid/cmdline,
    cpu, and timestamp.
    
    But some other tracers that want to implement their trace_event
    callbacks will not necessary need these information or they may want to
    format them as they want.
    
    This patch adds a new default-enabled trace option:
    TRACE_ITER_CONTEXT_INFO When disabled through:
    
    echo nocontext-info > /debugfs/tracing/trace_options
    
    The pid, cpu and timestamps headers will not be printed.
    
    IE with the sched_switch tracer with context-info (default):
    
         bash-2935 [001] 100.356561: 2935:120:S ==> [001]  0:140:R <idle>
       <idle>-0    [000] 100.412804:    0:140:R   + [000] 11:115:S events/0
       <idle>-0    [000] 100.412816:    0:140:R ==> [000] 11:115:R events/0
     events/0-11   [000] 100.412829:   11:115:S ==> [000]  0:140:R <idle>
    
    Without context-info:
    
     2935:120:S ==> [001]  0:140:R <idle>
        0:140:R   + [000] 11:115:S events/0
        0:140:R ==> [000] 11:115:R events/0
       11:115:S ==> [000]  0:140:R <idle>
    
    A tracer can disable it at runtime by clearing the bit
    TRACE_ITER_CONTEXT_INFO in trace_flags.
    
    The print routines were renamed to trace_print_context and
    trace_print_lat_context, so that they can be used by tracers if they
    want to use them for one of the trace_event callbacks.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e603a291134b..f0c7a0f08cac 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -405,6 +405,10 @@ void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);
+
+struct trace_entry *trace_find_next_entry(struct trace_iterator *iter,
+					  int *ent_cpu, u64 *ent_ts);
+
 void tracing_generic_entry_update(struct trace_entry *entry,
 				  unsigned long flags,
 				  int pc);
@@ -591,7 +595,8 @@ enum trace_iterator_flags {
 	TRACE_ITER_ANNOTATE		= 0x2000,
 	TRACE_ITER_USERSTACKTRACE       = 0x4000,
 	TRACE_ITER_SYM_USEROBJ          = 0x8000,
-	TRACE_ITER_PRINTK_MSGONLY	= 0x10000
+	TRACE_ITER_PRINTK_MSGONLY	= 0x10000,
+	TRACE_ITER_CONTEXT_INFO		= 0x20000 /* Print pid/cpu/time */
 };
 
 /*

commit c71a896154119f4ca9e89d6078f5f63ad60ef199
Author: Arnaldo Carvalho de Melo <acme@redhat.com>
Date:   Fri Jan 23 12:06:27 2009 -0200

    blktrace: add ftrace plugin
    
    Impact: New way of using the blktrace infrastructure
    
    This drops the requirement of userspace utilities to use the blktrace
    facility.
    
    Configuration is done thru sysfs, adding a "trace" directory to the
    partition directory where blktrace can be enabled for the associated
    request_queue.
    
    The same filters present in the IOCTL interface are present as sysfs
    device attributes.
    
    The /sys/block/sdX/sdXN/trace/enable file allows tracing without any
    filters.
    
    The other files in this directory: pid, act_mask, start_lba and end_lba
    can be used with the same meaning as with the IOCTL interface.
    
    Using the sysfs interface will only setup the request_queue->blk_trace
    fields, tracing will only take place when the "blk" tracer is selected
    via the ftrace interface, as in the following example:
    
    To see the trace, one can use the /d/tracing/trace file or the
    /d/tracign/trace_pipe file, with semantics defined in the ftrace
    documentation in Documentation/ftrace.txt.
    
    [root@f10-1 ~]# cat /t/trace
           kjournald-305   [000]  3046.491224:   8,1    A WBS 6367 + 8 <- (8,1) 6304
           kjournald-305   [000]  3046.491227:   8,1    Q   R 6367 + 8 [kjournald]
           kjournald-305   [000]  3046.491236:   8,1    G  RB 6367 + 8 [kjournald]
           kjournald-305   [000]  3046.491239:   8,1    P  NS [kjournald]
           kjournald-305   [000]  3046.491242:   8,1    I RBS 6367 + 8 [kjournald]
           kjournald-305   [000]  3046.491251:   8,1    D  WB 6367 + 8 [kjournald]
           kjournald-305   [000]  3046.491610:   8,1    U  WS [kjournald] 1
              <idle>-0     [000]  3046.511914:   8,1    C  RS 6367 + 8 [6367]
    [root@f10-1 ~]#
    
    The default line context (prefix) format is the one described in the ftrace
    documentation, with the blktrace specific bits using its existing format,
    described in blkparse(8).
    
    If one wants to have the classic blktrace formatting, this is possible by
    using:
    
    [root@f10-1 ~]# echo blk_classic > /t/trace_options
    [root@f10-1 ~]# cat /t/trace
      8,1    0  3046.491224   305  A WBS 6367 + 8 <- (8,1) 6304
      8,1    0  3046.491227   305  Q   R 6367 + 8 [kjournald]
      8,1    0  3046.491236   305  G  RB 6367 + 8 [kjournald]
      8,1    0  3046.491239   305  P  NS [kjournald]
      8,1    0  3046.491242   305  I RBS 6367 + 8 [kjournald]
      8,1    0  3046.491251   305  D  WB 6367 + 8 [kjournald]
      8,1    0  3046.491610   305  U  WS [kjournald] 1
      8,1    0  3046.511914     0  C  RS 6367 + 8 [6367]
    [root@f10-1 ~]#
    
    Using the ftrace standard format allows more flexibility, such
    as the ability of asking for backtraces via trace_options:
    
    [root@f10-1 ~]# echo noblk_classic > /t/trace_options
    [root@f10-1 ~]# echo stacktrace > /t/trace_options
    
    [root@f10-1 ~]# cat /t/trace
           kjournald-305   [000]  3318.826779:   8,1    A WBS 6375 + 8 <- (8,1) 6312
           kjournald-305   [000]  3318.826782:
     <= submit_bio
     <= submit_bh
     <= sync_dirty_buffer
     <= journal_commit_transaction
     <= kjournald
     <= kthread
     <= child_rip
           kjournald-305   [000]  3318.826836:   8,1    Q   R 6375 + 8 [kjournald]
           kjournald-305   [000]  3318.826837:
     <= generic_make_request
     <= submit_bio
     <= submit_bh
     <= sync_dirty_buffer
     <= journal_commit_transaction
     <= kjournald
     <= kthread
    
    Please read the ftrace documentation to use aditional, standardized
    tracing filters such as /d/tracing/trace_cpumask, etc.
    
    See also /d/tracing/trace_mark to add comments in the trace stream,
    that is equivalent to the /d/block/sdaN/msg interface.
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b96037d970df..e603a291134b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -32,6 +32,7 @@ enum trace_type {
 	TRACE_KMEM_ALLOC,
 	TRACE_KMEM_FREE,
 	TRACE_POWER,
+	TRACE_BLK,
 
 	__TRACE_LAST_TYPE,
 };

commit b1818748b0cf9427e48acf9713295e829a0d715f
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Mon Jan 19 10:31:01 2009 +0100

    x86, ftrace, hw-branch-tracer: dump trace on oops
    
    Dump the branch trace on an oops (based on ftrace_dump_on_oops).
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 54b72781e920..b96037d970df 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -438,7 +438,6 @@ void trace_function(struct trace_array *tr,
 
 void trace_graph_return(struct ftrace_graph_ret *trace);
 int trace_graph_entry(struct ftrace_graph_ent *trace);
-void trace_hw_branch(struct trace_array *tr, u64 from, u64 to);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);

commit a225cdd263f340c864febb1992802fb5b08bc328
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 15 23:06:03 2009 -0500

    ftrace: remove static from function tracer functions
    
    Impact: clean up
    
    After reorganizing the functions in trace.c and trace_function.c,
    they no longer need to be in global context. This patch makes the
    functions and one variable into static.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index bf39a369e4b3..54b72781e920 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -464,16 +464,6 @@ void __trace_stack(struct trace_array *tr,
 
 extern cycle_t ftrace_now(int cpu);
 
-#ifdef CONFIG_FUNCTION_TRACER
-void tracing_start_function_trace(void);
-void tracing_stop_function_trace(void);
-#else
-# define tracing_start_function_trace()		do { } while (0)
-# define tracing_stop_function_trace()		do { } while (0)
-#endif
-
-extern int ftrace_function_enabled;
-
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER
 typedef void
 (*tracer_switch_func_t)(void *private,

commit 5361499101306cfb776c3cfa0f69d0479bc63868
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 15 19:12:40 2009 -0500

    ftrace: add stack trace to function tracer
    
    Impact: new feature to stack trace any function
    
    Chris Mason asked about being able to pick and choose a function
    and get a stack trace from it. This feature enables his request.
    
     # echo io_schedule > /debug/tracing/set_ftrace_filter
     # echo function > /debug/tracing/current_tracer
     # echo func_stack_trace > /debug/tracing/trace_options
    
    Produces the following in /debug/tracing/trace:
    
           kjournald-702   [001]   135.673060: io_schedule <-sync_buffer
           kjournald-702   [002]   135.673671:
     <= sync_buffer
     <= __wait_on_bit
     <= out_of_line_wait_on_bit
     <= __wait_on_buffer
     <= sync_dirty_buffer
     <= journal_commit_transaction
     <= kjournald
    
    Note, be careful about turning this on without filtering the functions.
    You may find that you have a 10 second lag between typing and seeing
    what you typed. This is why the stack trace for the function tracer
    does not use the same stack_trace flag as the other tracers use.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 79c872100dd5..bf39a369e4b3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -457,6 +457,11 @@ void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
 
+void __trace_stack(struct trace_array *tr,
+		   struct trace_array_cpu *data,
+		   unsigned long flags,
+		   int skip, int pc);
+
 extern cycle_t ftrace_now(int cpu);
 
 #ifdef CONFIG_FUNCTION_TRACER
@@ -467,6 +472,8 @@ void tracing_stop_function_trace(void);
 # define tracing_stop_function_trace()		do { } while (0)
 #endif
 
+extern int ftrace_function_enabled;
+
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER
 typedef void
 (*tracer_switch_func_t)(void *private,

commit 002bb86d8d42f18937aef396c3ecd65c7e02e21a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Jan 10 11:34:13 2009 -0800

    tracing/ftrace: separate events tracing and stats tracing engine
    
    Impact: tracing's Api change
    
    Currently, the stat tracing depends on the events tracing.
    When you switch to a new tracer, the stats files of the previous tracer
    will disappear. But it's more scalable to separate those two engines.
    This way, we can keep the stat files of one or several tracers when we
    want, without bothering of multiple tracer stat files or tracer switching.
    
    To build/destroys its stats files, a tracer just have to call
    register_stat_tracer/unregister_stat_tracer everytimes it wants to.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b3f9ad1b4d84..79c872100dd5 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -334,24 +334,6 @@ struct tracer_flags {
 /* Makes more easy to define a tracer opt */
 #define TRACER_OPT(s, b)	.name = #s, .bit = b
 
-/*
- * If you want to provide a stat file (one-shot statistics), fill
- * an iterator with stat_start/stat_next and a stat_show callbacks.
- * The others callbacks are optional.
- */
-struct tracer_stat {
-	/* The name of your stat file */
-	const char		*name;
-	/* Iteration over statistic entries */
-	void			*(*stat_start)(void);
-	void			*(*stat_next)(void *prev, int idx);
-	/* Compare two entries for sorting (optional) for stats */
-	int			(*stat_cmp)(void *p1, void *p2);
-	/* Print a stat entry */
-	int			(*stat_show)(struct seq_file *s, void *p);
-	/* Print the headers of your stat entries */
-	int			(*stat_headers)(struct seq_file *s);
-};
 
 /*
  * A specific tracer, represented by methods that operate on a trace array:
@@ -466,8 +448,6 @@ void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 
-void init_tracer_stat(struct tracer *trace);
-
 extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 
 extern unsigned long tracing_max_latency;

commit 034939b65ad5ff64b9709210b3469a95153c51a3
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Jan 8 10:03:56 2009 -0800

    tracing/ftrace: handle more than one stat file per tracer
    
    Impact: new API for tracers
    
    Make the stat tracing API reentrant. And also provide the new directory
    /debugfs/tracing/trace_stat which will contain all the stat files for the
    current active tracer.
    
    Now a tracer will, if desired, want to provide a zero terminated array of
    tracer_stat structures.
    Each one contains the callbacks necessary for one stat file.
    It have to provide at least a name for its stat file, an iterator with
    stat_start/start_next callback and an output callback for one stat entry.
    
    Also adapt the branch tracer to this new API.
    We create two files "all" and "annotated" inside the /debugfs/tracing/trace_stat
    directory, making the both stats simultaneously available instead of needing
    to change an option to switch from one stat file to another.
    
    The output of these stats haven't changed.
    
    Changes in v2:
    
    _ Apply the previous memory leak fix (rebase against tip/master)
    
    Changes in v3:
    
    _ Merge the patch that adapted the branch tracer to this Api in this patch to
      not break the kernel build.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 94ed45e93a80..b3f9ad1b4d84 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -334,6 +334,25 @@ struct tracer_flags {
 /* Makes more easy to define a tracer opt */
 #define TRACER_OPT(s, b)	.name = #s, .bit = b
 
+/*
+ * If you want to provide a stat file (one-shot statistics), fill
+ * an iterator with stat_start/stat_next and a stat_show callbacks.
+ * The others callbacks are optional.
+ */
+struct tracer_stat {
+	/* The name of your stat file */
+	const char		*name;
+	/* Iteration over statistic entries */
+	void			*(*stat_start)(void);
+	void			*(*stat_next)(void *prev, int idx);
+	/* Compare two entries for sorting (optional) for stats */
+	int			(*stat_cmp)(void *p1, void *p2);
+	/* Print a stat entry */
+	int			(*stat_show)(struct seq_file *s, void *p);
+	/* Print the headers of your stat entries */
+	int			(*stat_headers)(struct seq_file *s);
+};
+
 /*
  * A specific tracer, represented by methods that operate on a trace array:
  */
@@ -361,21 +380,7 @@ struct tracer {
 	struct tracer		*next;
 	int			print_max;
 	struct tracer_flags 	*flags;
-
-	/*
-	 * If you change one of the following on tracing runtime, recall
-	 * init_tracer_stat()
-	 */
-
-	/* Iteration over statistic entries */
-	void			*(*stat_start)(void);
-	void			*(*stat_next)(void *prev, int idx);
-	/* Compare two entries for sorting (optional) for stats */
-	int			(*stat_cmp)(void *p1, void *p2);
-	/* Print a stat entry */
-	int			(*stat_show)(struct seq_file *s, void *p);
-	/* Print the headers of your stat entries */
-	int			(*stat_headers)(struct seq_file *s);
+	struct tracer_stat	*stats;
 };
 
 struct trace_seq {

commit 99793e3dbe39a50d871eedc361659a894601e2bf
Merge: 3e80680208ba a103e2ab7377
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 6 10:18:43 2009 +0100

    Merge branches 'tracing/kmemtrace2' and 'tracing/ftrace' into tracing/urgent

commit 3d7a96f5a485b7d06c2379f343d7312af89ec9e2
Merge: 723cbe077551 238c6d54830c
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Jan 6 09:53:05 2009 +0100

    Merge branch 'linus' into tracing/kmemtrace2

commit 4462344ee9ea9224d026801b877887f2f39774a3
Author: Rusty Russell <rusty@rustcorp.com.au>
Date:   Thu Jan 1 10:12:23 2009 +1030

    cpumask: convert kernel trace functions further
    
    Impact: Reduce future memory usage, use new cpumask API.
    
    Since the last patch was created and acked, more old cpumask users
    slipped into kernel/trace.
    
    Mostly trivial conversions, except struct trace_iterator's "started"
    member becomes a cpumask_var_t.
    
    Signed-off-by: Rusty Russell <rusty@rustcorp.com.au>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cc7a4f864036..4d3d381bfd95 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -368,7 +368,7 @@ struct trace_iterator {
 	loff_t			pos;
 	long			idx;
 
-	cpumask_t		started;
+	cpumask_var_t		started;
 };
 
 int tracing_is_enabled(void);

commit 36994e58a48fb8f9651c7dc845a6de298aba5bfc
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Dec 29 13:42:23 2008 -0800

    tracing/kmemtrace: normalize the raw tracer event to the unified tracing API
    
    Impact: new tracer plugin
    
    This patch adapts kmemtrace raw events tracing to the unified tracing API.
    
    To enable and use this tracer, just do the following:
    
     echo kmemtrace > /debugfs/tracing/current_tracer
     cat /debugfs/tracing/trace
    
    You will have the following output:
    
     # tracer: kmemtrace
     #
     #
     # ALLOC  TYPE  REQ   GIVEN  FLAGS           POINTER         NODE    CALLER
     # FREE   |      |     |       |              |   |            |        |
     # |
    
    type_id 1 call_site 18446744071565527833 ptr 18446612134395152256
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345164672 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345164912 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 0 call_site 18446744071565636711 ptr 18446612134345165152 bytes_req 240 bytes_alloc 240 gfp_flags 208 node -1
    type_id 0 call_site 18446744071566144042 ptr 18446612134346191680 bytes_req 1304 bytes_alloc 1312 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    type_id 0 call_site 18446744071565585597 ptr 18446612134405955584 bytes_req 4096 bytes_alloc 4096 gfp_flags 208 node -1
    type_id 1 call_site 18446744071565585534 ptr 18446612134405955584
    
    That was to stay backward compatible with the format output produced in
    inux/tracepoint.h.
    
    This is the default ouput, but note that I tried something else.
    
    If you change an option:
    
    echo kmem_minimalistic > /debugfs/trace_options
    
    and then cat /debugfs/trace, you will have the following output:
    
     # tracer: kmemtrace
     #
     #
     # ALLOC  TYPE  REQ   GIVEN  FLAGS           POINTER         NODE    CALLER
     # FREE   |      |     |       |              |   |            |        |
     # |
    
       -      C                            0xffff88007c088780          file_free_rcu
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc780     -1   d_alloc
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc870     -1   d_alloc
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       +      K    240    240   000000d0   0xffff8800790dc960     -1   d_alloc
       +      K   1304   1312   000000d0   0xffff8800791d7340     -1   reiserfs_alloc_inode
       -      C                            0xffff88007cad6000          putname
       +      K   4096   4096   000000d0   0xffff88007cad6000     -1   getname
       -      C                            0xffff88007cad6000          putname
       +      K    992   1000   000000d0   0xffff880079045b58     -1   alloc_inode
       +      K    768   1024   000080d0   0xffff88007c096400     -1   alloc_pipe_info
       +      K    240    240   000000d0   0xffff8800790dca50     -1   d_alloc
       +      K    272    320   000080d0   0xffff88007c088780     -1   get_empty_filp
       +      K    272    320   000080d0   0xffff88007c088000     -1   get_empty_filp
    
    Yeah I shall confess kmem_minimalistic should be: kmem_alternative.
    
    Whatever, I find it more readable but this a personal opinion of course.
    We can drop it if you want.
    
    On the ALLOC/FREE column, + means an allocation and - a free.
    
    On the type column, you have K = kmalloc, C = cache, P = page
    
    I would like the flags to be GFP_* strings but that would not be easy to not
    break the column with strings....
    
    About the node...it seems to always be -1. I don't know why but that shouldn't
    be difficult to find.
    
    I moved linux/tracepoint.h to trace/tracepoint.h as well. I think that would
    be more easy to find the tracer headers if they are all in their common
    directory.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cc7a4f864036..534505bb39b0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -9,6 +9,7 @@
 #include <linux/mmiotrace.h>
 #include <linux/ftrace.h>
 #include <trace/boot.h>
+#include <trace/kmemtrace.h>
 
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
@@ -29,6 +30,8 @@ enum trace_type {
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
 	TRACE_HW_BRANCHES,
+	TRACE_KMEM_ALLOC,
+	TRACE_KMEM_FREE,
 	TRACE_POWER,
 
 	__TRACE_LAST_TYPE
@@ -170,6 +173,24 @@ struct trace_power {
 	struct power_trace	state_data;
 };
 
+struct kmemtrace_alloc_entry {
+	struct trace_entry	ent;
+	enum kmemtrace_type_id type_id;
+	unsigned long call_site;
+	const void *ptr;
+	size_t bytes_req;
+	size_t bytes_alloc;
+	gfp_t gfp_flags;
+	int node;
+};
+
+struct kmemtrace_free_entry {
+	struct trace_entry	ent;
+	enum kmemtrace_type_id type_id;
+	unsigned long call_site;
+	const void *ptr;
+};
+
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
@@ -280,6 +301,10 @@ extern void __ftrace_bad_type(void);
 			  TRACE_GRAPH_RET);		\
 		IF_ASSIGN(var, ent, struct hw_branch_entry, TRACE_HW_BRANCHES);\
  		IF_ASSIGN(var, ent, struct trace_power, TRACE_POWER); \
+		IF_ASSIGN(var, ent, struct kmemtrace_alloc_entry,	\
+			  TRACE_KMEM_ALLOC);	\
+		IF_ASSIGN(var, ent, struct kmemtrace_free_entry,	\
+			  TRACE_KMEM_FREE);	\
 		__ftrace_bad_type();					\
 	} while (0)
 

commit f7d48cbde5c0710008caeaf7dbf14f4a9b064940
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Dec 29 13:02:17 2008 +0100

    tracing/ftrace: make trace_find_cmdline() generally available
    
    Impact: build fix
    
    On !CONFIG_CONTEXT_SWITCH_TRACER trace_find_cmdline() is not defined:
    
     kernel/trace/trace_output.c: In function 'trace_ctxwake_print':
     kernel/trace/trace_output.c:499: error: implicit declaration of function 'trace_find_cmdline'
     kernel/trace/trace_output.c:499: warning: assignment makes pointer from integer without a cast
    
    Move it to the generic section in trace.h.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 05fa804d1c16..a8b624ccd4d6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -469,10 +469,10 @@ struct tracer_switch_ops {
 	void				*private;
 	struct tracer_switch_ops	*next;
 };
-
-char *trace_find_cmdline(int pid);
 #endif /* CONFIG_CONTEXT_SWITCH_TRACER */
 
+extern char *trace_find_cmdline(int pid);
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern unsigned long ftrace_update_tot_cnt;
 #define DYN_FTRACE_TEST_NAME trace_selftest_dynamic_test_func

commit dbd0b4b33074aa6b7832a9d9a5bd985eca5c1aa2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Dec 28 20:44:51 2008 -0800

    tracing/ftrace: provide the base infrastructure for histogram tracing
    
    Impact: extend the tracing API
    
    The goal of this patch is to normalize and make more easy the
    implementation of statistical (histogram) tracing.
    
    It implements a trace_stat file into the /debugfs/tracing directory where
    one can print a one-shot output of statistics/histogram entries.
    
    A tracer has to provide two basic iterator callbacks:
    
      stat_start() => the first entry
      stat_next(prev, idx) => the next one.
    
    Note that it is adapted for arrays or hash tables or lists.... since it
    provides a pointer to the previous entry and the current index of the
    iterator.
    
    These two callbacks are called to get a snapshot of the statistics at each
    opening of the trace_stat file because. The values are so updated between
    two "cat trace_stat". And the tracer is free to lock its datas during the
    iteration to keep consistent values.
    
    Since it is almost always interesting to sort statisticals values to
    address the problems by priority, this infrastructure provides a "sorting"
    of the stat entries too if desired. A tracer has just to provide a
    stat_cmp callback to compare two entries and the stat tracing
    infrastructure will build a sorted list of the given entries.
    
    A last callback, called stat_headers, can be implemented by a tracer to
    output headers on its trace.
    
    If one of these callbacks is changed on runtime, it just have to signal it
    to the stat tracing API by calling the init_tracer_stat() helper.
    
    Changes in V2:
    
    - Fix a memory leak if the user opens multiple times the trace_stat file
      without closing it. Now we always free our list before rebuilding it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6bd71fa1e1c7..05fa804d1c16 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -336,6 +336,21 @@ struct tracer {
 	struct tracer		*next;
 	int			print_max;
 	struct tracer_flags 	*flags;
+
+	/*
+	 * If you change one of the following on tracing runtime, recall
+	 * init_tracer_stat()
+	 */
+
+	/* Iteration over statistic entries */
+	void			*(*stat_start)(void);
+	void			*(*stat_next)(void *prev, int idx);
+	/* Compare two entries for sorting (optional) for stats */
+	int			(*stat_cmp)(void *p1, void *p2);
+	/* Print a stat entry */
+	int			(*stat_show)(struct seq_file *s, void *p);
+	/* Print the headers of your stat entries */
+	int			(*stat_headers)(struct seq_file *s);
 };
 
 struct trace_seq {
@@ -421,6 +436,8 @@ void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 
+void init_tracer_stat(struct tracer *trace);
+
 extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 
 extern unsigned long tracing_max_latency;

commit f0868d1e23a8efec33beb3aa688aab7fdb1ae093
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 23 23:24:12 2008 -0500

    ftrace: set up trace event hash infrastructure
    
    Impact: simplify/generalize/refactor trace.c
    
    The trace.c file is becoming more difficult to maintain due to the
    growing number of events. There is several formats that an event may
    be printed. This patch sets up the infrastructure of an event hash to
    allow for events to register how they should be printed.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3a357382cce6..6bd71fa1e1c7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -30,7 +30,7 @@ enum trace_type {
 	TRACE_HW_BRANCHES,
 	TRACE_POWER,
 
-	__TRACE_LAST_TYPE
+	__TRACE_LAST_TYPE,
 };
 
 /*
@@ -484,12 +484,6 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);
-extern int trace_seq_printf(struct trace_seq *s, const char *fmt, ...);
-extern int
-seq_print_ip_sym(struct trace_seq *s, unsigned long ip,
-		unsigned long sym_flags);
-extern ssize_t trace_seq_to_user(struct trace_seq *s, char __user *ubuf,
-				 size_t cnt);
 extern long ns2usecs(cycle_t nsec);
 extern int
 trace_vprintk(unsigned long ip, int depth, const char *fmt, va_list args);

commit c47956d9ae3341d2d1998bff26620fa3338c01e4
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 23 23:24:11 2008 -0500

    ftrace: remove obsolete print continue functionality
    
    Impact: cleanup, remove obsolete code
    
    Now that the ring buffer used by ftrace allows for variable length
    entries, we do not need the 'cont' feature of the buffer.  This code
    makes other parts of ftrace more complex and by removing this it
    simplifies the ftrace code.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cc7a4f864036..3a357382cce6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -16,7 +16,6 @@ enum trace_type {
 	TRACE_FN,
 	TRACE_CTX,
 	TRACE_WAKE,
-	TRACE_CONT,
 	TRACE_STACK,
 	TRACE_PRINT,
 	TRACE_SPECIAL,
@@ -178,7 +177,6 @@ struct trace_power {
  *  NEED_RESCED		- reschedule is requested
  *  HARDIRQ		- inside an interrupt handler
  *  SOFTIRQ		- inside a softirq handler
- *  CONT		- multiple entries hold the trace item
  */
 enum trace_flag_type {
 	TRACE_FLAG_IRQS_OFF		= 0x01,
@@ -186,7 +184,6 @@ enum trace_flag_type {
 	TRACE_FLAG_NEED_RESCHED		= 0x04,
 	TRACE_FLAG_HARDIRQ		= 0x08,
 	TRACE_FLAG_SOFTIRQ		= 0x10,
-	TRACE_FLAG_CONT			= 0x20,
 };
 
 #define TRACE_BUF_SIZE		1024
@@ -262,7 +259,6 @@ extern void __ftrace_bad_type(void);
 	do {								\
 		IF_ASSIGN(var, ent, struct ftrace_entry, TRACE_FN);	\
 		IF_ASSIGN(var, ent, struct ctx_switch_entry, 0);	\
-		IF_ASSIGN(var, ent, struct trace_field_cont, TRACE_CONT); \
 		IF_ASSIGN(var, ent, struct stack_entry, TRACE_STACK);	\
 		IF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
@@ -489,9 +485,6 @@ extern int trace_selftest_startup_branch(struct tracer *trace,
 
 extern void *head_page(struct trace_array_cpu *data);
 extern int trace_seq_printf(struct trace_seq *s, const char *fmt, ...);
-extern void trace_seq_print_cont(struct trace_seq *s,
-				 struct trace_iterator *iter);
-
 extern int
 seq_print_ip_sym(struct trace_seq *s, unsigned long ip,
 		unsigned long sym_flags);

commit 213cc060797378059a28ebc5c539f3e9a80160bd
Author: Pekka J Enberg <penberg@cs.helsinki.fi>
Date:   Fri Dec 19 12:08:39 2008 +0200

    ftrace: introduce tracing_reset_online_cpus() helper
    
    Impact: cleanup
    
    This patch factors out common code from multiple tracers into a
    tracing_reset_online_cpus() function and converts the tracers to use it.
    
    Signed-off-by: Pekka Enberg <penberg@cs.helsinki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index fc75dce7a664..cc7a4f864036 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -374,6 +374,7 @@ struct trace_iterator {
 int tracing_is_enabled(void);
 void trace_wake_up(void);
 void tracing_reset(struct trace_array *tr, int cpu);
+void tracing_reset_online_cpus(struct trace_array *tr);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *tracing_init_dentry(void);
 void init_tracer_sysprof_debugfs(struct dentry *d_tracer);

commit 66896a85cf2890b6bbbc4c9ccdcd296600ffbf89
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Dec 13 20:18:13 2008 +0100

    tracing/ftrace: add the printk-msg-only option
    
    Impact: display ftrace_printk messages "as is"
    
    By default, ftrace_printk() messages find their output with some other
    informations like pid, caller, ...
    Sometimes a developer just want to have the ftrace_printk left "as is", without
    other information.
    
    This is done by providing a default-off option called printk-msg-only.
    To enable it, just do `echo printk-msg-only > /debugfs/tracing/trace_options`
    
    Before the patch:
    
               <...>-2739  [000]   145.692153: __might_sleep: I'm an ftrace_printk msg in __might_sleep
               <...>-2739  [000]   145.692155: __might_sleep: I'm another ftrace_printk msg in __might_sleep
    
    After the patch and the printk-msg-only option enabled:
    
    I'm an ftrace_printk msg in __might_sleep
    I'm another ftrace_printk msg in __might_sleep
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f07c246dd73d..fc75dce7a664 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -578,7 +578,8 @@ enum trace_iterator_flags {
 	TRACE_ITER_BRANCH		= 0x1000,
 	TRACE_ITER_ANNOTATE		= 0x2000,
 	TRACE_ITER_USERSTACKTRACE       = 0x4000,
-	TRACE_ITER_SYM_USEROBJ          = 0x8000
+	TRACE_ITER_SYM_USEROBJ          = 0x8000,
+	TRACE_ITER_PRINTK_MSGONLY	= 0x10000
 };
 
 /*

commit a93751cab71d63126687551823ed3e70cd85854a
Author: Markus Metzger <markut.t.metzger@intel.com>
Date:   Thu Dec 11 13:53:26 2008 +0100

    x86, bts, ftrace: adapt the hw-branch-tracer to the ds.c interface
    
    Impact: restructure code, cleanup
    
    Remove BTS bits from the hw-branch-tracer (renamed from bts-tracer) and
    use the ds interface.
    
    Signed-off-by: Markus Metzger <markut.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5ac697065a48..f07c246dd73d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -28,7 +28,7 @@ enum trace_type {
 	TRACE_GRAPH_RET,
 	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
-	TRACE_BTS,
+	TRACE_HW_BRANCHES,
 	TRACE_POWER,
 
 	__TRACE_LAST_TYPE
@@ -159,10 +159,10 @@ struct trace_branch {
 	char			correct;
 };
 
-struct bts_entry {
+struct hw_branch_entry {
 	struct trace_entry	ent;
-	unsigned long		from;
-	unsigned long		to;
+	u64			from;
+	u64			to;
 };
 
 struct trace_power {
@@ -278,7 +278,7 @@ extern void __ftrace_bad_type(void);
 			  TRACE_GRAPH_ENT);		\
 		IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,	\
 			  TRACE_GRAPH_RET);		\
-		IF_ASSIGN(var, ent, struct bts_entry, TRACE_BTS);\
+		IF_ASSIGN(var, ent, struct hw_branch_entry, TRACE_HW_BRANCHES);\
  		IF_ASSIGN(var, ent, struct trace_power, TRACE_POWER); \
 		__ftrace_bad_type();					\
 	} while (0)
@@ -414,9 +414,7 @@ void trace_function(struct trace_array *tr,
 
 void trace_graph_return(struct ftrace_graph_ret *trace);
 int trace_graph_entry(struct ftrace_graph_ent *trace);
-void trace_bts(struct trace_array *tr,
-	       unsigned long from,
-	       unsigned long to);
+void trace_hw_branch(struct trace_array *tr, u64 from, u64 to);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);

commit 77d683f3e0258d522c5506e7b5fd05c9411184d9
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Dec 4 23:49:47 2008 +0100

    tracing/ftrace: fix the check of ftrace_trace_task
    
    Impact: fix default empty traces on function-graph-tracer
    
    The actual ftrace_trace_task() checks if ftrace_pid_trace is allocated
    and return 1 if it is true.
    If it is NULL, it will check the bit of pid tracing flag for the current
    task (which are not set by default).
    So by default, a task is not traced.
    Actually all tasks should be traced by default and filter_by_pid when
    ftrace_pid_trace is allocated.
    
    The appropriate condition should be to return 1 if filter_by_pid is
    set.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acke-dby: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a71bbe0a3631..5ac697065a48 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -551,7 +551,7 @@ extern struct pid *ftrace_pid_trace;
 
 static inline int ftrace_trace_task(struct task_struct *task)
 {
-	if (ftrace_pid_trace)
+	if (!ftrace_pid_trace)
 		return 1;
 
 	return test_tsk_trace_trace(task);

commit 970987beb9c99ca806edc464518d411cc399fb4d
Merge: faec2ec505d3 1fd8f2a3f9a9 feaf3848a813
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Dec 5 14:45:22 2008 +0100

    Merge branches 'tracing/ftrace', 'tracing/function-graph-tracer' and 'tracing/urgent' into tracing/core

commit 1fd8f2a3f9a91b287a876cef830b21baafc8a799
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Dec 3 23:45:11 2008 +0100

    tracing/function-graph-tracer: handle ftrace_printk entries
    
    Handle the TRACE_PRINT entries from the function grapg tracer
    and output them as a C comment just below the function that called
    it, as if it was a comment inside this function.
    
    Example with an ftrace_printk inside might_sleep() function:
    
    void __might_sleep(char *file, int line)
    {
            static unsigned long prev_jiffy;        /* ratelimiting */
    
            ftrace_printk("Hi I'm a comment in might_sleep() :-)");
    
    A chunk of a resulting trace:
    
     0)               |        _reiserfs_free_block() {
     0)               |          reiserfs_read_bitmap_block() {
     0)               |            __bread() {
     0)               |              __getblk() {
     0)               |                __find_get_block() {
     0)   0.698 us    |                  mark_page_accessed();
     0)   2.267 us    |                }
     0)               |                __might_sleep() {
     0)               |                  /* Hi I'm a comment in might_sleep() :-) */
     0)   1.321 us    |                }
     0)   5.872 us    |              }
     0)   7.313 us    |            }
     0)   8.718 us    |          }
    
    And this patch brings two minor fixes:
    
    - The newline after a switch-out task has disappeared
    - The "|" sign just before the cpu number on task-switch has been deleted.
    
     0)   0.616 us    |                pick_next_task_rt();
     0)   1.457 us    |                _spin_trylock();
     0)   0.653 us    |                _spin_unlock();
     0)   0.728 us    |                _spin_trylock();
     0)   0.631 us    |                _spin_unlock();
     0)   0.729 us    |                native_load_sp0();
     0)   0.593 us    |                native_load_tls();
     ------------------------------------------
     0)    cat-2834    =>   migrati-3
     ------------------------------------------
    
     0)               |    finish_task_switch() {
     0)   0.841 us    |      _spin_unlock_irq();
     0)   0.616 us    |      post_schedule_rt();
     0)   3.882 us    |    }
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0565ae9a2210..fce98898205a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -117,6 +117,7 @@ struct userstack_entry {
 struct print_entry {
 	struct trace_entry	ent;
 	unsigned long		ip;
+	int			depth;
 	char			buf[];
 };
 
@@ -498,7 +499,8 @@ seq_print_ip_sym(struct trace_seq *s, unsigned long ip,
 extern ssize_t trace_seq_to_user(struct trace_seq *s, char __user *ubuf,
 				 size_t cnt);
 extern long ns2usecs(cycle_t nsec);
-extern int trace_vprintk(unsigned long ip, const char *fmt, va_list args);
+extern int
+trace_vprintk(unsigned long ip, int depth, const char *fmt, va_list args);
 
 extern unsigned long trace_flags;
 

commit 6b2539302bee8e88c99e3c7d80c16a04dbe5e2ad
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Dec 4 09:18:28 2008 +0100

    tracing: fix typo and missing inline function
    
    Impact: fix build bugs
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8b81b4d727bd..b4b7b735184d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -529,7 +529,11 @@ static inline int ftrace_graph_addr(unsigned long addr)
 #else
 static inline int ftrace_trace_addr(unsigned long addr)
 {
-	return 1
+	return 1;
+}
+static inline int ftrace_graph_addr(unsigned long addr)
+{
+	return 1;
 }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 

commit 978f3a45d9499c7a447ca7615455cefb63d44165
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Dec 4 00:26:40 2008 -0500

    ftrace: use struct pid
    
    Impact: clean up, extend PID filtering to PID namespaces
    
    Eric Biederman suggested using the struct pid for filtering on
    pids in the kernel. This patch is based off of a demonstration
    of an implementation that Eric sent me in an email.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 95fff37ed970..8b81b4d727bd 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -541,11 +541,11 @@ print_graph_function(struct trace_iterator *iter)
 }
 #endif /* CONFIG_FUNCTION_GRAPH_TRACER */
 
-extern int ftrace_pid_trace;
+extern struct pid *ftrace_pid_trace;
 
 static inline int ftrace_trace_task(struct task_struct *task)
 {
-	if (ftrace_pid_trace < 0)
+	if (ftrace_pid_trace)
 		return 1;
 
 	return test_tsk_trace_trace(task);

commit 804a685162a7080386714166776f57255a75238e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Dec 3 15:36:59 2008 -0500

    ftrace: trace single pid for function graph tracer
    
    Impact: New feature
    
    This patch makes the changes to set_ftrace_pid apply to the function
    graph tracer.
    
      # echo $$ > /debugfs/tracing/set_ftrace_pid
      # echo function_graph > /debugfs/tracing/current_tracer
    
    Will cause only the current task to be traced. Note, the trace flags are
    also inherited by child processes, so the children of the shell
    will also be traced.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 41f026bfc9ed..95fff37ed970 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -541,6 +541,16 @@ print_graph_function(struct trace_iterator *iter)
 }
 #endif /* CONFIG_FUNCTION_GRAPH_TRACER */
 
+extern int ftrace_pid_trace;
+
+static inline int ftrace_trace_task(struct task_struct *task)
+{
+	if (ftrace_pid_trace < 0)
+		return 1;
+
+	return test_tsk_trace_trace(task);
+}
+
 /*
  * trace_iterator_flags is an enumeration that defines bit
  * positions into trace_flags that controls the output.

commit ea4e2bc4d9f7370e57a343ccb5e7c0ad3222ec3c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Dec 3 15:36:57 2008 -0500

    ftrace: graph of a single function
    
    This patch adds the file:
    
       /debugfs/tracing/set_graph_function
    
    which can be used along with the function graph tracer.
    
    When this file is empty, the function graph tracer will act as
    usual. When the file has a function in it, the function graph
    tracer will only trace that function.
    
    For example:
    
     # echo blk_unplug > /debugfs/tracing/set_graph_function
     # cat /debugfs/tracing/trace
     [...]
     ------------------------------------------
     | 2)  make-19003  =>  kjournald-2219
     ------------------------------------------
    
     2)               |  blk_unplug() {
     2)               |    dm_unplug_all() {
     2)               |      dm_get_table() {
     2)      1.381 us |        _read_lock();
     2)      0.911 us |        dm_table_get();
     2)      1. 76 us |        _read_unlock();
     2) +   12.912 us |      }
     2)               |      dm_table_unplug_all() {
     2)               |        blk_unplug() {
     2)      0.778 us |          generic_unplug_device();
     2)      2.409 us |        }
     2)      5.992 us |      }
     2)      0.813 us |      dm_table_put();
     2) +   29. 90 us |    }
     2) +   34.532 us |  }
    
    You can add up to 32 functions into this file. Currently we limit it
    to 32, but this may change with later improvements.
    
    To add another function, use the append '>>':
    
      # echo sys_read >> /debugfs/tracing/set_graph_function
      # cat /debugfs/tracing/set_graph_function
      blk_unplug
      sys_read
    
    Using the '>' will clear out the function and write anew:
    
      # echo sys_write > /debug/tracing/set_graph_function
      # cat /debug/tracing/set_graph_function
      sys_write
    
    Note, if you have function graph running while doing this, the small
    time between clearing it and updating it will cause the graph to
    record all functions. This should not be an issue because after
    it sets the filter, only those functions will be recorded from then on.
    If you need to only record a particular function then set this
    file first before starting the function graph tracer. In the future
    this side effect may be corrected.
    
    The set_graph_function file is similar to the set_ftrace_filter but
    it does not take wild cards nor does it allow for more than one
    function to be set with a single write. There is no technical reason why
    this is the case, I just do not have the time yet to implement that.
    
    Note, dynamic ftrace must be enabled for this to appear because it
    uses the dynamic ftrace records to match the name to the mcount
    call sites.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0565ae9a2210..41f026bfc9ed 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -505,13 +505,41 @@ extern unsigned long trace_flags;
 /* Standard output formatting function used for function return traces */
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
 extern enum print_line_t print_graph_function(struct trace_iterator *iter);
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+/* TODO: make this variable */
+#define FTRACE_GRAPH_MAX_FUNCS		32
+extern int ftrace_graph_count;
+extern unsigned long ftrace_graph_funcs[FTRACE_GRAPH_MAX_FUNCS];
+
+static inline int ftrace_graph_addr(unsigned long addr)
+{
+	int i;
+
+	if (!ftrace_graph_count || test_tsk_trace_graph(current))
+		return 1;
+
+	for (i = 0; i < ftrace_graph_count; i++) {
+		if (addr == ftrace_graph_funcs[i])
+			return 1;
+	}
+
+	return 0;
+}
 #else
+static inline int ftrace_trace_addr(unsigned long addr)
+{
+	return 1
+}
+#endif /* CONFIG_DYNAMIC_FTRACE */
+
+#else /* CONFIG_FUNCTION_GRAPH_TRACER */
 static inline enum print_line_t
 print_graph_function(struct trace_iterator *iter)
 {
 	return TRACE_TYPE_UNHANDLED;
 }
-#endif
+#endif /* CONFIG_FUNCTION_GRAPH_TRACER */
 
 /*
  * trace_iterator_flags is an enumeration that defines bit

commit e49dc19c6a19ea112fcb94b7c62ec62cdd5c08aa
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Dec 2 23:50:05 2008 -0500

    ftrace: function graph return for function entry
    
    Impact: feature, let entry function decide to trace or not
    
    This patch lets the graph tracer entry function decide if the tracing
    should be done at the end as well. This requires all function graph
    entry functions return 1 if it should trace, or 0 if the return should
    not be traced.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f96f4e787ff3..0565ae9a2210 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -412,7 +412,7 @@ void trace_function(struct trace_array *tr,
 		    unsigned long flags, int pc);
 
 void trace_graph_return(struct ftrace_graph_ret *trace);
-void trace_graph_entry(struct ftrace_graph_ent *trace);
+int trace_graph_entry(struct ftrace_graph_ent *trace);
 void trace_bts(struct trace_array *tr,
 	       unsigned long from,
 	       unsigned long to);

commit c7cc77307669336a08928ab8668bdb3f3bcc021b
Merge: 0bfc24559d79 d144d5ee6a26 437f24fb897d f3f47a6768a2
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Nov 27 10:56:13 2008 +0100

    Merge branches 'tracing/blktrace', 'tracing/ftrace', 'tracing/function-graph-tracer' and 'tracing/power-tracer' into tracing/core

commit f3f47a6768a29448866da4422b6f6bee485c947f
Author: Arjan van de Ven <arjan@infradead.org>
Date:   Sun Nov 23 16:49:58 2008 -0800

    tracing: add "power-tracer": C/P state tracer to help power optimization
    
    Impact: new "power-tracer" ftrace plugin
    
    This patch adds a C/P-state ftrace plugin that will generate
    detailed statistics about the C/P-states that are being used,
    so that we can look at detailed decisions that the C/P-state
    code is making, rather than the too high level "average"
    that we have today.
    
    An example way of using this is:
    
     mount -t debugfs none /sys/kernel/debug
     echo cstate > /sys/kernel/debug/tracing/current_tracer
     echo 1 > /sys/kernel/debug/tracing/tracing_enabled
     sleep 1
     echo 0 > /sys/kernel/debug/tracing/tracing_enabled
     cat /sys/kernel/debug/tracing/trace | perl scripts/trace/cstate.pl > out.svg
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3abd645e8af2..4c453778a6ab 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -28,6 +28,7 @@ enum trace_type {
 	TRACE_FN_RET,
 	TRACE_USER_STACK,
 	TRACE_BTS,
+	TRACE_POWER,
 
 	__TRACE_LAST_TYPE
 };
@@ -160,6 +161,11 @@ struct bts_entry {
 	unsigned long		to;
 };
 
+struct trace_power {
+	struct trace_entry	ent;
+	struct power_trace	state_data;
+};
+
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
@@ -266,6 +272,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \
 		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET);\
 		IF_ASSIGN(var, ent, struct bts_entry, TRACE_BTS);\
+ 		IF_ASSIGN(var, ent, struct trace_power, TRACE_POWER); \
 		__ftrace_bad_type();					\
 	} while (0)
 

commit 660c7f9be96321fc80026d76411bd15e6f418a72
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Nov 26 00:16:26 2008 -0500

    ftrace: add thread comm to function graph tracer
    
    Impact: enhancement to function graph tracer
    
    Export the trace_find_cmdline so the function graph tracer can
    use it to print the comms of the threads.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ffe1bb1eb620..7adacf349ef7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -450,6 +450,7 @@ struct tracer_switch_ops {
 	struct tracer_switch_ops	*next;
 };
 
+char *trace_find_cmdline(int pid);
 #endif /* CONFIG_CONTEXT_SWITCH_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 287b6e68ca7209caec40b2f44f837c580a413bae
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Nov 26 00:57:25 2008 +0100

    tracing/function-return-tracer: set a more human readable output
    
    Impact: feature
    
    This patch sets a C-like output for the function graph tracing.
    For this aim, we now call two handler for each function: one on the entry
    and one other on return. This way we can draw a well-ordered call stack.
    
    The pid of the previous trace is loosely stored to be compared against
    the one of the current trace to see if there were a context switch.
    
    Without this little feature, the call tree would seem broken at
    some locations.
    We could use the sched_tracer to capture these sched_events but this
    way of processing is much more simpler.
    
    2 spaces have been chosen for indentation to fit the screen while deep
    calls. The time of execution in nanosecs is printed just after closed
    braces, it seems more easy this way to find the corresponding function.
    If the time was printed as a first column, it would be not so easy to
    find the corresponding function if it is called on a deep depth.
    
    I plan to output the return value but on 32 bits CPU, the return value
    can be 32 or 64, and its difficult to guess on which case we are.
    I don't know what would be the better solution on X86-32: only print
    eax (low-part) or even edx (high-part).
    
    Actually it's thee same problem when a function return a 8 bits value, the
    high part of eax could contain junk values...
    
    Here is an example of trace:
    
    sys_read() {
      fget_light() {
      } 526
      vfs_read() {
        rw_verify_area() {
          security_file_permission() {
            cap_file_permission() {
            } 519
          } 1564
        } 2640
        do_sync_read() {
          pipe_read() {
            __might_sleep() {
            } 511
            pipe_wait() {
              prepare_to_wait() {
              } 760
              deactivate_task() {
                dequeue_task() {
                  dequeue_task_fair() {
                    dequeue_entity() {
                      update_curr() {
                        update_min_vruntime() {
                        } 504
                      } 1587
                      clear_buddies() {
                      } 512
                      add_cfs_task_weight() {
                      } 519
                      update_min_vruntime() {
                      } 511
                    } 5602
                    dequeue_entity() {
                      update_curr() {
                        update_min_vruntime() {
                        } 496
                      } 1631
                      clear_buddies() {
                      } 496
                      update_min_vruntime() {
                      } 527
                    } 4580
                    hrtick_update() {
                      hrtick_start_fair() {
                      } 488
                    } 1489
                  } 13700
                } 14949
              } 16016
              msecs_to_jiffies() {
              } 496
              put_prev_task_fair() {
              } 504
              pick_next_task_fair() {
              } 489
              pick_next_task_rt() {
              } 496
              pick_next_task_fair() {
              } 489
              pick_next_task_idle() {
              } 489
    
    ------------8<---------- thread 4 ------------8<----------
    
    finish_task_switch() {
    } 1203
    do_softirq() {
      __do_softirq() {
        __local_bh_disable() {
        } 669
        rcu_process_callbacks() {
          __rcu_process_callbacks() {
            cpu_quiet() {
              rcu_start_batch() {
              } 503
            } 1647
          } 3128
          __rcu_process_callbacks() {
          } 542
        } 5362
        _local_bh_enable() {
        } 587
      } 8880
    } 9986
    kthread_should_stop() {
    } 669
    deactivate_task() {
      dequeue_task() {
        dequeue_task_fair() {
          dequeue_entity() {
            update_curr() {
              calc_delta_mine() {
              } 511
              update_min_vruntime() {
              } 511
            } 2813
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 72b5ef868765..ffe1bb1eb620 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -25,7 +25,8 @@ enum trace_type {
 	TRACE_BRANCH,
 	TRACE_BOOT_CALL,
 	TRACE_BOOT_RET,
-	TRACE_FN_RET,
+	TRACE_GRAPH_RET,
+	TRACE_GRAPH_ENT,
 	TRACE_USER_STACK,
 	TRACE_BTS,
 
@@ -56,14 +57,16 @@ struct ftrace_entry {
 	unsigned long		parent_ip;
 };
 
+/* Function call entry */
+struct ftrace_graph_ent_entry {
+	struct trace_entry			ent;
+	struct ftrace_graph_ent		graph_ent;
+};
+
 /* Function return entry */
-struct ftrace_graph_entry {
-	struct trace_entry	ent;
-	unsigned long		ip;
-	unsigned long		parent_ip;
-	unsigned long long	calltime;
-	unsigned long long	rettime;
-	unsigned long		overrun;
+struct ftrace_graph_ret_entry {
+	struct trace_entry			ent;
+	struct ftrace_graph_ret		ret;
 };
 extern struct tracer boot_tracer;
 
@@ -264,7 +267,10 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct trace_boot_call, TRACE_BOOT_CALL);\
 		IF_ASSIGN(var, ent, struct trace_boot_ret, TRACE_BOOT_RET);\
 		IF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \
-		IF_ASSIGN(var, ent, struct ftrace_graph_entry, TRACE_FN_RET);\
+		IF_ASSIGN(var, ent, struct ftrace_graph_ent_entry,	\
+			  TRACE_GRAPH_ENT);		\
+		IF_ASSIGN(var, ent, struct ftrace_graph_ret_entry,	\
+			  TRACE_GRAPH_RET);		\
 		IF_ASSIGN(var, ent, struct bts_entry, TRACE_BTS);\
 		__ftrace_bad_type();					\
 	} while (0)
@@ -397,9 +403,9 @@ void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
 		    unsigned long flags, int pc);
-void
-trace_function_graph(struct ftrace_graph_ret *trace);
 
+void trace_graph_return(struct ftrace_graph_ret *trace);
+void trace_graph_entry(struct ftrace_graph_ent *trace);
 void trace_bts(struct trace_array *tr,
 	       unsigned long from,
 	       unsigned long to);

commit fb52607afcd0629776f1dc9e657647ceae81dd50
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 25 21:07:04 2008 +0100

    tracing/function-return-tracer: change the name into function-graph-tracer
    
    Impact: cleanup
    
    This patch changes the name of the "return function tracer" into
    function-graph-tracer which is a more suitable name for a tracing
    which makes one able to retrieve the ordered call stack during
    the code flow.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3abd645e8af2..72b5ef868765 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -57,7 +57,7 @@ struct ftrace_entry {
 };
 
 /* Function return entry */
-struct ftrace_ret_entry {
+struct ftrace_graph_entry {
 	struct trace_entry	ent;
 	unsigned long		ip;
 	unsigned long		parent_ip;
@@ -264,7 +264,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct trace_boot_call, TRACE_BOOT_CALL);\
 		IF_ASSIGN(var, ent, struct trace_boot_ret, TRACE_BOOT_RET);\
 		IF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \
-		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET);\
+		IF_ASSIGN(var, ent, struct ftrace_graph_entry, TRACE_FN_RET);\
 		IF_ASSIGN(var, ent, struct bts_entry, TRACE_BTS);\
 		__ftrace_bad_type();					\
 	} while (0)
@@ -398,7 +398,7 @@ void trace_function(struct trace_array *tr,
 		    unsigned long parent_ip,
 		    unsigned long flags, int pc);
 void
-trace_function_return(struct ftrace_retfunc *trace);
+trace_function_graph(struct ftrace_graph_ret *trace);
 
 void trace_bts(struct trace_array *tr,
 	       unsigned long from,
@@ -489,11 +489,11 @@ extern int trace_vprintk(unsigned long ip, const char *fmt, va_list args);
 extern unsigned long trace_flags;
 
 /* Standard output formatting function used for function return traces */
-#ifdef CONFIG_FUNCTION_RET_TRACER
-extern enum print_line_t print_return_function(struct trace_iterator *iter);
+#ifdef CONFIG_FUNCTION_GRAPH_TRACER
+extern enum print_line_t print_graph_function(struct trace_iterator *iter);
 #else
 static inline enum print_line_t
-print_return_function(struct trace_iterator *iter)
+print_graph_function(struct trace_iterator *iter)
 {
 	return TRACE_TYPE_UNHANDLED;
 }

commit 1e9b51c28312f7334394aa30be56ff52c2b65b7e
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Tue Nov 25 09:24:15 2008 +0100

    x86, bts, ftrace: a BTS ftrace plug-in prototype
    
    Impact: add new ftrace plugin
    
    A prototype for a BTS ftrace plug-in.
    
    The tracer collects branch trace in a cyclic buffer for each cpu.
    
    The tracer is not configurable and the trace for each snapshot is
    appended when doing cat /debug/tracing/trace.
    
    This is a proof of concept that will be extended with future patches
    to become a (hopefully) useful tool.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 717f9f045c6f..3abd645e8af2 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -27,6 +27,7 @@ enum trace_type {
 	TRACE_BOOT_RET,
 	TRACE_FN_RET,
 	TRACE_USER_STACK,
+	TRACE_BTS,
 
 	__TRACE_LAST_TYPE
 };
@@ -153,6 +154,12 @@ struct trace_branch {
 	char			correct;
 };
 
+struct bts_entry {
+	struct trace_entry	ent;
+	unsigned long		from;
+	unsigned long		to;
+};
+
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
@@ -258,6 +265,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct trace_boot_ret, TRACE_BOOT_RET);\
 		IF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \
 		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET);\
+		IF_ASSIGN(var, ent, struct bts_entry, TRACE_BTS);\
 		__ftrace_bad_type();					\
 	} while (0)
 
@@ -392,6 +400,10 @@ void trace_function(struct trace_array *tr,
 void
 trace_function_return(struct ftrace_retfunc *trace);
 
+void trace_bts(struct trace_array *tr,
+	       unsigned long from,
+	       unsigned long to);
+
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
 void tracing_sched_switch_assign_trace(struct trace_array *tr);

commit 8bba1bf5e2434c83f2fe8b1422604ace9bbe4cb8
Author: Markus Metzger <markus.t.metzger@intel.com>
Date:   Tue Nov 25 09:12:31 2008 +0100

    x86, ftrace: call trace->open() before stopping tracing; add trace->print_header()
    
    Add a callback to allow an ftrace plug-in to write its own header.
    
    Move the call to trace->open() up a few lines.
    
    The changes are required by the BTS ftrace plug-in.
    
    Signed-off-by: Markus Metzger <markus.t.metzger@intel.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 28c15c2ebc22..717f9f045c6f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -311,6 +311,7 @@ struct tracer {
 	int			(*selftest)(struct tracer *trace,
 					    struct trace_array *tr);
 #endif
+	void			(*print_header)(struct seq_file *m);
 	enum print_line_t	(*print_line)(struct trace_iterator *iter);
 	/* If you handled the flag setting, return 0 */
 	int			(*set_flag)(u32 old_flags, u32 bit, int set);

commit b54d3de9f3b8956653b06f1a32e9f9321c6d9027
Author: Török Edwin <edwintorok@gmail.com>
Date:   Sat Nov 22 13:28:48 2008 +0200

    tracing: identify which executable object the userspace address belongs to
    
    Impact: modify+improve the userstacktrace tracing visualization feature
    
    Store thread group leader id, and use it to lookup the address in the
    process's map. We could have looked up the address on thread's map,
    but the thread might not exist by the time we are called. The process
    might not exist either, but if you are reading trace_pipe, that is
    unlikely.
    
    Example usage:
    
     mount -t debugfs nodev /sys/kernel/debug
     cd /sys/kernel/debug/tracing
     echo userstacktrace >iter_ctrl
     echo sym-userobj >iter_ctrl
     echo sched_switch >current_tracer
     echo 1 >tracing_enabled
     cat trace_pipe >/tmp/trace&
     .... run application ...
     echo 0 >tracing_enabled
     cat /tmp/trace
    
    You'll see stack entries like:
    
       /lib/libpthread-2.7.so[+0xd370]
    
    You can convert them to function/line using:
    
       addr2line -fie /lib/libpthread-2.7.so 0xd370
    
    Or:
    
       addr2line -fie /usr/lib/debug/libpthread-2.7.so 0xd370
    
    For non-PIC/PIE executables this won't work:
    
       a.out[+0x73b]
    
    You need to run the following: addr2line -fie a.out 0x40073b
    (where 0x400000 is the default load address of a.out)
    
    Signed-off-by: Török Edwin <edwintorok@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 17bb4c830b01..28c15c2ebc22 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -508,7 +508,8 @@ enum trace_iterator_flags {
 	TRACE_ITER_PREEMPTONLY		= 0x800,
 	TRACE_ITER_BRANCH		= 0x1000,
 	TRACE_ITER_ANNOTATE		= 0x2000,
-	TRACE_ITER_USERSTACKTRACE       = 0x4000
+	TRACE_ITER_USERSTACKTRACE       = 0x4000,
+	TRACE_ITER_SYM_USEROBJ          = 0x8000
 };
 
 /*

commit 02b67518e2b1c490787dac7f35e1204e74fe21ba
Author: Török Edwin <edwintorok@gmail.com>
Date:   Sat Nov 22 13:28:47 2008 +0200

    tracing: add support for userspace stacktraces in tracing/iter_ctrl
    
    Impact: add new (default-off) tracing visualization feature
    
    Usage example:
    
     mount -t debugfs nodev /sys/kernel/debug
     cd /sys/kernel/debug/tracing
     echo userstacktrace >iter_ctrl
     echo sched_switch >current_tracer
     echo 1 >tracing_enabled
     .... run application ...
     echo 0 >tracing_enabled
    
    Then read one of 'trace','latency_trace','trace_pipe'.
    
    To get the best output you can compile your userspace programs with
    frame pointers (at least glibc + the app you are tracing).
    
    Signed-off-by: Török Edwin <edwintorok@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2cb12fd98f6b..17bb4c830b01 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -26,6 +26,7 @@ enum trace_type {
 	TRACE_BOOT_CALL,
 	TRACE_BOOT_RET,
 	TRACE_FN_RET,
+	TRACE_USER_STACK,
 
 	__TRACE_LAST_TYPE
 };
@@ -42,6 +43,7 @@ struct trace_entry {
 	unsigned char		flags;
 	unsigned char		preempt_count;
 	int			pid;
+	int			tgid;
 };
 
 /*
@@ -99,6 +101,11 @@ struct stack_entry {
 	unsigned long		caller[FTRACE_STACK_ENTRIES];
 };
 
+struct userstack_entry {
+	struct trace_entry	ent;
+	unsigned long		caller[FTRACE_STACK_ENTRIES];
+};
+
 /*
  * ftrace_printk entry:
  */
@@ -240,6 +247,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct ctx_switch_entry, 0);	\
 		IF_ASSIGN(var, ent, struct trace_field_cont, TRACE_CONT); \
 		IF_ASSIGN(var, ent, struct stack_entry, TRACE_STACK);	\
+		IF_ASSIGN(var, ent, struct userstack_entry, TRACE_USER_STACK);\
 		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
 		IF_ASSIGN(var, ent, struct special_entry, 0);		\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
@@ -500,6 +508,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_PREEMPTONLY		= 0x800,
 	TRACE_ITER_BRANCH		= 0x1000,
 	TRACE_ITER_ANNOTATE		= 0x2000,
+	TRACE_ITER_USERSTACKTRACE       = 0x4000
 };
 
 /*

commit 0231022cc32d5f2e7f3c06b75691dda0ad6aec33
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Nov 17 03:22:41 2008 +0100

    tracing/function-return-tracer: add the overrun field
    
    Impact: help to find the better depth of trace
    
    We decided to arbitrary define the depth of function return trace as
    "20". Perhaps this is not enough. To help finding an optimal depth, we
    measure now the overrun: the number of functions that have been missed
    for the current thread. By default this is not displayed, we have to
    do set a particular flag on the return tracer: echo overrun >
    /debug/tracing/trace_options And the overrun will be printed on the
    right.
    
    As the trace shows below, the current 20 depth is not enough.
    
    update_wall_time+0x37f/0x8c0 -> update_xtime_cache (345 ns) (Overruns: 2838)
    update_wall_time+0x384/0x8c0 -> clocksource_get_next (1141 ns) (Overruns: 2838)
    do_timer+0x23/0x100 -> update_wall_time (3882 ns) (Overruns: 2838)
    tick_do_update_jiffies64+0xbf/0x160 -> do_timer (5339 ns) (Overruns: 2838)
    tick_sched_timer+0x6a/0xf0 -> tick_do_update_jiffies64 (7209 ns) (Overruns: 2838)
    vgacon_set_cursor_size+0x98/0x120 -> native_io_delay (2613 ns) (Overruns: 274)
    vgacon_cursor+0x16e/0x1d0 -> vgacon_set_cursor_size (33151 ns) (Overruns: 274)
    set_cursor+0x5f/0x80 -> vgacon_cursor (36432 ns) (Overruns: 274)
    con_flush_chars+0x34/0x40 -> set_cursor (38790 ns) (Overruns: 274)
    release_console_sem+0x1ec/0x230 -> up (721 ns) (Overruns: 274)
    release_console_sem+0x225/0x230 -> wake_up_klogd (316 ns) (Overruns: 274)
    con_flush_chars+0x39/0x40 -> release_console_sem (2996 ns) (Overruns: 274)
    con_write+0x22/0x30 -> con_flush_chars (46067 ns) (Overruns: 274)
    n_tty_write+0x1cc/0x360 -> con_write (292670 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2a/0x90 -> native_apic_mem_write (330 ns) (Overruns: 274)
    irq_enter+0x17/0x70 -> idle_cpu (413 ns) (Overruns: 274)
    smp_apic_timer_interrupt+0x2f/0x90 -> irq_enter (1525 ns) (Overruns: 274)
    ktime_get_ts+0x40/0x70 -> getnstimeofday (465 ns) (Overruns: 274)
    ktime_get_ts+0x60/0x70 -> set_normalized_timespec (436 ns) (Overruns: 274)
    ktime_get+0x16/0x30 -> ktime_get_ts (2501 ns) (Overruns: 274)
    hrtimer_interrupt+0x77/0x1a0 -> ktime_get (3439 ns) (Overruns: 274)
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9d22618bf99f..2cb12fd98f6b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -60,6 +60,7 @@ struct ftrace_ret_entry {
 	unsigned long		parent_ip;
 	unsigned long long	calltime;
 	unsigned long long	rettime;
+	unsigned long		overrun;
 };
 extern struct tracer boot_tracer;
 

commit adf9f19574334c9a29a2bc956009fcac7edf1a6b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Nov 17 19:23:42 2008 +0100

    tracing/ftrace: implement a set_flag callback for tracers
    
    Impact: give a way to send specific messages to tracers
    
    The current implementation of tracing uses some flags to control the
    output of general tracers. But we have no way to implement custom
    flags handling for a specific tracer. This patch proposes a new
    callback for the struct tracer which called set_flag and a structure
    that represents a 32 bits variable flag.
    
    A tracer can implement a struct tracer_flags on which it puts the
    initial value of the flag integer. Than it can place a range of flags
    with their name and their flag mask on the flag integer. The structure
    that implement a single flag is called struct tracer_opt.
    
    These custom flags will be available through the trace_options file
    like the general tracing flags. Changing their value is done like the
    other general flags. For example if you have a flag that calls "foo",
    you can activate it by writing "foo" or "nofoo" on trace_options.
    
    Note that the set_flag callback is optional and is only needed if you
    want the flags changing to be signaled to your tracer and let it to
    accept or refuse their assignment.
    
    V2: Some arrangements in coding style....
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 37947f6b92bf..9d22618bf99f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -259,6 +259,29 @@ enum print_line_t {
 	TRACE_TYPE_UNHANDLED	= 2	/* Relay to other output functions */
 };
 
+
+/*
+ * An option specific to a tracer. This is a boolean value.
+ * The bit is the bit index that sets its value on the
+ * flags value in struct tracer_flags.
+ */
+struct tracer_opt {
+	const char 	*name; /* Will appear on the trace_options file */
+	u32 		bit; /* Mask assigned in val field in tracer_flags */
+};
+
+/*
+ * The set of specific options for a tracer. Your tracer
+ * have to set the initial value of the flags val.
+ */
+struct tracer_flags {
+	u32			val;
+	struct tracer_opt 	*opts;
+};
+
+/* Makes more easy to define a tracer opt */
+#define TRACER_OPT(s, b)	.name = #s, .bit = b
+
 /*
  * A specific tracer, represented by methods that operate on a trace array:
  */
@@ -280,8 +303,11 @@ struct tracer {
 					    struct trace_array *tr);
 #endif
 	enum print_line_t	(*print_line)(struct trace_iterator *iter);
+	/* If you handled the flag setting, return 0 */
+	int			(*set_flag)(u32 old_flags, u32 bit, int set);
 	struct tracer		*next;
 	int			print_max;
+	struct tracer_flags 	*flags;
 };
 
 struct trace_seq {

commit 5a209c2d58e70f9bc415b9cdf0e3b9aaefb70371
Merge: 3f8e402f34ec 0c726da983de e270219f4372
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Nov 18 08:52:13 2008 +0100

    Merge branches 'tracing/branch-tracer' and 'tracing/urgent' into tracing/core

commit 0c726da983de0704254250ef6495ca152e7abcca
Author: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
Date:   Sun Nov 16 16:07:58 2008 +0530

    tracing: branch tracer, fix writing to trace/trace_options
    
    Impact: fix trace_options behavior
    
    writing to trace/trace_options use the index of the array
    to find the value of the flag. With branch tracer flag
    defined conditionally, this breaks writing to trace_options
    with branch tracer disabled.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.vnet.ibm.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 790ea8c0e1f3..b41d7b4c2cae 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -470,9 +470,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_SCHED_TREE		= 0x200,
 	TRACE_ITER_PRINTK		= 0x400,
 	TRACE_ITER_PREEMPTONLY		= 0x800,
-#ifdef CONFIG_BRANCH_TRACER
 	TRACE_ITER_BRANCH		= 0x1000,
-#endif
 	TRACE_ITER_ANNOTATE		= 0x2000,
 };
 

commit 1c80025a49855b12fa09bb6db71820e3367b1369
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Nov 16 05:57:26 2008 +0100

    tracing/ftrace: change the type of the init() callback
    
    Impact: extend the ->init() method with the ability to fail
    
    This bring a way to know if the initialization of a tracer successed.
    A tracer must return 0 on success and a traditional error (ie:
    -ENOMEM) if it fails.
    
    If a tracer fails to init, it is free to print a detailed warn. The
    tracing api will not and switch to a new tracer will just return the
    error from the init callback.
    
    Note: this will be used for the return tracer.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 790ea8c0e1f3..cdbd5cc22be8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -264,7 +264,8 @@ enum print_line_t {
  */
 struct tracer {
 	const char		*name;
-	void			(*init)(struct trace_array *tr);
+	/* Your tracer should raise a warning if init fails */
+	int			(*init)(struct trace_array *tr);
 	void			(*reset)(struct trace_array *tr);
 	void			(*start)(struct trace_array *tr);
 	void			(*stop)(struct trace_array *tr);

commit 12ef7d448613ead2babd41c3ebfa1fe03c20edef
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Nov 12 17:52:38 2008 -0500

    ftrace: CPU buffer start annotation clean ups
    
    Impact: better handling of CPU buffer start annotation
    
    Because of the confusion with the per CPU buffers wrapping where
    one CPU might be more active at the end of the trace than the other
    CPUs causing that one CPU to have a shorter history. Kernel
    developers were confused by the "missing" data of that one CPU
    at the beginning of the trace output. An annotation was added to
    the trace output to show that the buffer had started:
    
     # tracer: function
     #
     #           TASK-PID    CPU#    TIMESTAMP  FUNCTION
     #              | |       |          |         |
     ##### CPU 3 buffer started ####
              <idle>-0     [003]   158.192959: smp_apic_timer_interrupt
     [...]
               <idle>-0     [003]   161.556520: default_idle
     ##### CPU 1 buffer started ####
               <idle>-0     [001]   161.592494: hrtimer_force_reprogram
     [etc]
    
    But this annotation gets a bit messy when tracers do not fill the
    buffers. This patch does a couple of things:
    
     One) it adds a flag to trace_options to disable these annotations
    
     Two) it does not annotate if the tracer did not overflow its buffer.
    
    This makes the output much cleaner.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9e015f5bea1d..790ea8c0e1f3 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -473,6 +473,7 @@ enum trace_iterator_flags {
 #ifdef CONFIG_BRANCH_TRACER
 	TRACE_ITER_BRANCH		= 0x1000,
 #endif
+	TRACE_ITER_ANNOTATE		= 0x2000,
 };
 
 /*

commit 80e5ea4506791af206266c5921c97f11d3b17866
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 12 15:24:24 2008 -0500

    ftrace: add tracer called branch
    
    Impact: added new branch tracer
    
    Currently the tracing of branch profiling (unlikelys and likelys hit)
    is only activated by the iter_ctrl. This patch adds a tracer called
    "branch" that will just trace the branch profiling. The advantage
    of adding this tracer is that it can be added to the ftrace selftests
    on startup.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7fbf37b27453..9e015f5bea1d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -420,6 +420,8 @@ extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 					       struct trace_array *tr);
 extern int trace_selftest_startup_sysprof(struct tracer *trace,
 					       struct trace_array *tr);
+extern int trace_selftest_startup_branch(struct tracer *trace,
+					 struct trace_array *tr);
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);

commit 9f029e83e968e5661d7be045bbcb620dbb909938
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 12 15:24:24 2008 -0500

    ftrace: rename unlikely iter_ctrl to branch
    
    Impact: rename of iter_ctrl unlikely to branch
    
    The unlikely name is ugly. This patch converts the iter_ctrl command
    "unlikely" and "nounlikely" to "branch" and "nobranch" respectively.
    
    It also renames a lot of internal functions to use "branch" instead
    of "unlikely".
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index dccae6312941..7fbf37b27453 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -22,7 +22,7 @@ enum trace_type {
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
-	TRACE_UNLIKELY,
+	TRACE_BRANCH,
 	TRACE_BOOT_CALL,
 	TRACE_BOOT_RET,
 	TRACE_FN_RET,
@@ -137,7 +137,7 @@ struct trace_boot_ret {
 
 #define TRACE_FUNC_SIZE 30
 #define TRACE_FILE_SIZE 20
-struct trace_unlikely {
+struct trace_branch {
 	struct trace_entry	ent;
 	unsigned	        line;
 	char			func[TRACE_FUNC_SIZE+1];
@@ -247,7 +247,7 @@ extern void __ftrace_bad_type(void);
 			  TRACE_MMIO_MAP);				\
 		IF_ASSIGN(var, ent, struct trace_boot_call, TRACE_BOOT_CALL);\
 		IF_ASSIGN(var, ent, struct trace_boot_ret, TRACE_BOOT_RET);\
-		IF_ASSIGN(var, ent, struct trace_unlikely, TRACE_UNLIKELY); \
+		IF_ASSIGN(var, ent, struct trace_branch, TRACE_BRANCH); \
 		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET);\
 		__ftrace_bad_type();					\
 	} while (0)
@@ -469,7 +469,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_PRINTK		= 0x400,
 	TRACE_ITER_PREEMPTONLY		= 0x800,
 #ifdef CONFIG_BRANCH_TRACER
-	TRACE_ITER_UNLIKELY		= 0x1000,
+	TRACE_ITER_BRANCH		= 0x1000,
 #endif
 };
 
@@ -531,25 +531,25 @@ static inline void ftrace_preempt_enable(int resched)
 }
 
 #ifdef CONFIG_BRANCH_TRACER
-extern int enable_unlikely_tracing(struct trace_array *tr);
-extern void disable_unlikely_tracing(void);
-static inline int trace_unlikely_enable(struct trace_array *tr)
+extern int enable_branch_tracing(struct trace_array *tr);
+extern void disable_branch_tracing(void);
+static inline int trace_branch_enable(struct trace_array *tr)
 {
-	if (trace_flags & TRACE_ITER_UNLIKELY)
-		return enable_unlikely_tracing(tr);
+	if (trace_flags & TRACE_ITER_BRANCH)
+		return enable_branch_tracing(tr);
 	return 0;
 }
-static inline void trace_unlikely_disable(void)
+static inline void trace_branch_disable(void)
 {
 	/* due to races, always disable */
-	disable_unlikely_tracing();
+	disable_branch_tracing();
 }
 #else
-static inline int trace_unlikely_enable(struct trace_array *tr)
+static inline int trace_branch_enable(struct trace_array *tr)
 {
 	return 0;
 }
-static inline void trace_unlikely_disable(void)
+static inline void trace_branch_disable(void)
 {
 }
 #endif /* CONFIG_BRANCH_TRACER */

commit 2ed84eeb8808cf3c9f039213ca137ffd7d753f0e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 12 15:24:24 2008 -0500

    trace: rename unlikely profiler to branch profiler
    
    Impact: name change of unlikely tracer and profiler
    
    Ingo Molnar suggested changing the config from UNLIKELY_PROFILE
    to BRANCH_PROFILING. I never did like the "unlikely" name so I
    went one step farther, and renamed all the unlikely configurations
    to a "BRANCH" variant.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9635aa2c4fc1..dccae6312941 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -468,7 +468,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_SCHED_TREE		= 0x200,
 	TRACE_ITER_PRINTK		= 0x400,
 	TRACE_ITER_PREEMPTONLY		= 0x800,
-#ifdef CONFIG_UNLIKELY_TRACER
+#ifdef CONFIG_BRANCH_TRACER
 	TRACE_ITER_UNLIKELY		= 0x1000,
 #endif
 };
@@ -530,7 +530,7 @@ static inline void ftrace_preempt_enable(int resched)
 		preempt_enable_notrace();
 }
 
-#ifdef CONFIG_UNLIKELY_TRACER
+#ifdef CONFIG_BRANCH_TRACER
 extern int enable_unlikely_tracing(struct trace_array *tr);
 extern void disable_unlikely_tracing(void);
 static inline int trace_unlikely_enable(struct trace_array *tr)
@@ -552,6 +552,6 @@ static inline int trace_unlikely_enable(struct trace_array *tr)
 static inline void trace_unlikely_disable(void)
 {
 }
-#endif /* CONFIG_UNLIKELY_TRACER */
+#endif /* CONFIG_BRANCH_TRACER */
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 52f232cb720a7babb752849cbc2cab2d24021209
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Nov 12 00:14:40 2008 -0500

    tracing: likely/unlikely branch annotation tracer
    
    Impact: new likely/unlikely branch tracer
    
    This patch adds a way to record the instances of the likely() and unlikely()
    branch condition annotations.
    
    When "unlikely" is set in /debugfs/tracing/iter_ctrl the unlikely conditions
    will be added to any of the ftrace tracers. The change takes effect when
    a new tracer is passed into the current_tracer file.
    
    For example:
    
     bash-3471  [003]   357.014755: [INCORRECT] sched_info_dequeued:sched_stats.h:177
     bash-3471  [003]   357.014756: [correct] update_curr:sched_fair.c:489
     bash-3471  [003]   357.014758: [correct] calc_delta_fair:sched_fair.c:411
     bash-3471  [003]   357.014759: [correct] account_group_exec_runtime:sched_stats.h:356
     bash-3471  [003]   357.014761: [correct] update_curr:sched_fair.c:489
     bash-3471  [003]   357.014763: [INCORRECT] calc_delta_fair:sched_fair.c:411
     bash-3471  [003]   357.014765: [correct] calc_delta_mine:sched.c:1279
    
    Which shows the normal tracer heading, as well as whether the condition was
    correct "[correct]" or was mistaken "[INCORRECT]", followed by the function,
    file name and line number.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b5f91f198fd4..9635aa2c4fc1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -22,6 +22,7 @@ enum trace_type {
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
+	TRACE_UNLIKELY,
 	TRACE_BOOT_CALL,
 	TRACE_BOOT_RET,
 	TRACE_FN_RET,
@@ -134,6 +135,16 @@ struct trace_boot_ret {
 	struct boot_trace_ret boot_ret;
 };
 
+#define TRACE_FUNC_SIZE 30
+#define TRACE_FILE_SIZE 20
+struct trace_unlikely {
+	struct trace_entry	ent;
+	unsigned	        line;
+	char			func[TRACE_FUNC_SIZE+1];
+	char			file[TRACE_FILE_SIZE+1];
+	char			correct;
+};
+
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
@@ -236,6 +247,7 @@ extern void __ftrace_bad_type(void);
 			  TRACE_MMIO_MAP);				\
 		IF_ASSIGN(var, ent, struct trace_boot_call, TRACE_BOOT_CALL);\
 		IF_ASSIGN(var, ent, struct trace_boot_ret, TRACE_BOOT_RET);\
+		IF_ASSIGN(var, ent, struct trace_unlikely, TRACE_UNLIKELY); \
 		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET);\
 		__ftrace_bad_type();					\
 	} while (0)
@@ -456,6 +468,9 @@ enum trace_iterator_flags {
 	TRACE_ITER_SCHED_TREE		= 0x200,
 	TRACE_ITER_PRINTK		= 0x400,
 	TRACE_ITER_PREEMPTONLY		= 0x800,
+#ifdef CONFIG_UNLIKELY_TRACER
+	TRACE_ITER_UNLIKELY		= 0x1000,
+#endif
 };
 
 /*
@@ -515,4 +530,28 @@ static inline void ftrace_preempt_enable(int resched)
 		preempt_enable_notrace();
 }
 
+#ifdef CONFIG_UNLIKELY_TRACER
+extern int enable_unlikely_tracing(struct trace_array *tr);
+extern void disable_unlikely_tracing(void);
+static inline int trace_unlikely_enable(struct trace_array *tr)
+{
+	if (trace_flags & TRACE_ITER_UNLIKELY)
+		return enable_unlikely_tracing(tr);
+	return 0;
+}
+static inline void trace_unlikely_disable(void)
+{
+	/* due to races, always disable */
+	disable_unlikely_tracing();
+}
+#else
+static inline int trace_unlikely_enable(struct trace_array *tr)
+{
+	return 0;
+}
+static inline void trace_unlikely_disable(void)
+{
+}
+#endif /* CONFIG_UNLIKELY_TRACER */
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 74239072830ef3f1398edeb1bc1076fc330fd4a2
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 23:24:42 2008 +0100

    tracing/fastboot: Use the ring-buffer timestamp for initcall entries
    
    Impact: Split the boot tracer entries in two parts: call and return
    
    Now that we are using the sched tracer from the boot tracer, we want
    to use the same timestamp than the ring-buffer to have consistent time
    captures between sched events and initcall events.
    
    So we get rid of the old time capture by the boot tracer and split the
    initcall events in two parts: call and return. This way we have the
    ring buffer timestamp of both.
    
    An example trace:
    
    [   27.904149584] calling  net_ns_init+0x0/0x1c0 @ 1
    [   27.904429624] initcall net_ns_init+0x0/0x1c0 returned 0 after 0 msecs
    [   27.904575926] calling  reboot_init+0x0/0x20 @ 1
    [   27.904655399] initcall reboot_init+0x0/0x20 returned 0 after 0 msecs
    [   27.904800228] calling  sysctl_init+0x0/0x30 @ 1
    [   27.905142914] initcall sysctl_init+0x0/0x30 returned 0 after 0 msecs
    [   27.905287211] calling  ksysfs_init+0x0/0xb0 @ 1
     ##### CPU 0 buffer started ####
                init-1     [000]    27.905395:      1:120:R   + [001]    11:115:S
     ##### CPU 1 buffer started ####
              <idle>-0     [001]    27.905425:      0:140:R ==> [001]    11:115:R
                init-1     [000]    27.905426:      1:120:D ==> [000]     0:140:R
              <idle>-0     [000]    27.905431:      0:140:R   + [000]     4:115:S
              <idle>-0     [000]    27.905451:      0:140:R ==> [000]     4:115:R
         ksoftirqd/0-4     [000]    27.905456:      4:115:S ==> [000]     0:140:R
               udevd-11    [001]    27.905458:     11:115:R   + [001]    14:115:R
              <idle>-0     [000]    27.905459:      0:140:R   + [000]     4:115:S
              <idle>-0     [000]    27.905462:      0:140:R ==> [000]     4:115:R
               udevd-11    [001]    27.905462:     11:115:R ==> [001]    14:115:R
         ksoftirqd/0-4     [000]    27.905467:      4:115:S ==> [000]     0:140:R
              <idle>-0     [000]    27.905470:      0:140:R   + [000]     4:115:S
              <idle>-0     [000]    27.905473:      0:140:R ==> [000]     4:115:R
         ksoftirqd/0-4     [000]    27.905476:      4:115:S ==> [000]     0:140:R
              <idle>-0     [000]    27.905479:      0:140:R   + [000]     4:115:S
              <idle>-0     [000]    27.905482:      0:140:R ==> [000]     4:115:R
         ksoftirqd/0-4     [000]    27.905486:      4:115:S ==> [000]     0:140:R
               udevd-14    [001]    27.905499:     14:120:X ==> [001]    11:115:R
               udevd-11    [001]    27.905506:     11:115:R   + [000]     1:120:D
              <idle>-0     [000]    27.905515:      0:140:R ==> [000]     1:120:R
               udevd-11    [001]    27.905517:     11:115:S ==> [001]     0:140:R
    [   27.905557107] initcall ksysfs_init+0x0/0xb0 returned 0 after 3906 msecs
    [   27.905705736] calling  init_jiffies_clocksource+0x0/0x10 @ 1
    [   27.905779239] initcall init_jiffies_clocksource+0x0/0x10 returned 0 after 0 msecs
    [   27.906769814] calling  pm_init+0x0/0x30 @ 1
    [   27.906853627] initcall pm_init+0x0/0x30 returned 0 after 0 msecs
    [   27.906997803] calling  pm_disk_init+0x0/0x20 @ 1
    [   27.907076946] initcall pm_disk_init+0x0/0x20 returned 0 after 0 msecs
    [   27.907222556] calling  swsusp_header_init+0x0/0x30 @ 1
    [   27.907294325] initcall swsusp_header_init+0x0/0x30 returned 0 after 0 msecs
    [   27.907439620] calling  stop_machine_init+0x0/0x50 @ 1
                init-1     [000]    27.907485:      1:120:R   + [000]     2:115:S
                init-1     [000]    27.907490:      1:120:D ==> [000]     2:115:R
            kthreadd-2     [000]    27.907507:      2:115:R   + [001]    15:115:R
              <idle>-0     [001]    27.907517:      0:140:R ==> [001]    15:115:R
            kthreadd-2     [000]    27.907517:      2:115:D ==> [000]     0:140:R
              <idle>-0     [000]    27.907521:      0:140:R   + [000]     4:115:S
              <idle>-0     [000]    27.907524:      0:140:R ==> [000]     4:115:R
               udevd-15    [001]    27.907527:     15:115:D   + [000]     2:115:D
         ksoftirqd/0-4     [000]    27.907537:      4:115:S ==> [000]     2:115:R
               udevd-15    [001]    27.907537:     15:115:D ==> [001]     0:140:R
            kthreadd-2     [000]    27.907546:      2:115:R   + [000]     1:120:D
            kthreadd-2     [000]    27.907550:      2:115:S ==> [000]     1:120:R
                init-1     [000]    27.907584:      1:120:R   + [000]    15:  0:D
                init-1     [000]    27.907589:      1:120:R   + [000]     2:115:S
                init-1     [000]    27.907593:      1:120:D ==> [000]    15:  0:R
               udevd-15    [000]    27.907601:     15:  0:S ==> [000]     2:115:R
     ##### CPU 0 buffer started ####
            kthreadd-2     [000]    27.907616:      2:115:R   + [001]    16:115:R
     ##### CPU 1 buffer started ####
              <idle>-0     [001]    27.907620:      0:140:R ==> [001]    16:115:R
            kthreadd-2     [000]    27.907621:      2:115:D ==> [000]     0:140:R
               udevd-16    [001]    27.907625:     16:115:D   + [000]     2:115:D
              <idle>-0     [000]    27.907628:      0:140:R   + [000]     4:115:S
               udevd-16    [001]    27.907629:     16:115:D ==> [001]     0:140:R
              <idle>-0     [000]    27.907631:      0:140:R ==> [000]     4:115:R
         ksoftirqd/0-4     [000]    27.907636:      4:115:S ==> [000]     2:115:R
            kthreadd-2     [000]    27.907644:      2:115:R   + [000]     1:120:D
            kthreadd-2     [000]    27.907647:      2:115:S ==> [000]     1:120:R
                init-1     [000]    27.907657:      1:120:R   + [001]    16:  0:D
              <idle>-0     [001]    27.907666:      0:140:R ==> [001]    16:  0:R
    [   27.907703862] initcall stop_machine_init+0x0/0x50 returned 0 after 0 msecs
    [   27.907850704] calling  filelock_init+0x0/0x30 @ 1
    [   27.907926573] initcall filelock_init+0x0/0x30 returned 0 after 0 msecs
    [   27.908071327] calling  init_script_binfmt+0x0/0x10 @ 1
    [   27.908165195] initcall init_script_binfmt+0x0/0x10 returned 0 after 0 msecs
    [   27.908309461] calling  init_elf_binfmt+0x0/0x10 @ 1
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f69a5199596b..b5f91f198fd4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -22,7 +22,8 @@ enum trace_type {
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
-	TRACE_BOOT,
+	TRACE_BOOT_CALL,
+	TRACE_BOOT_RET,
 	TRACE_FN_RET,
 
 	__TRACE_LAST_TYPE
@@ -123,9 +124,14 @@ struct trace_mmiotrace_map {
 	struct mmiotrace_map	map;
 };
 
-struct trace_boot {
+struct trace_boot_call {
 	struct trace_entry	ent;
-	struct boot_trace	initcall;
+	struct boot_trace_call boot_call;
+};
+
+struct trace_boot_ret {
+	struct trace_entry	ent;
+	struct boot_trace_ret boot_ret;
 };
 
 /*
@@ -228,8 +234,9 @@ extern void __ftrace_bad_type(void);
 			  TRACE_MMIO_RW);				\
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\
 			  TRACE_MMIO_MAP);				\
-		IF_ASSIGN(var, ent, struct trace_boot, TRACE_BOOT);	\
-		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET); \
+		IF_ASSIGN(var, ent, struct trace_boot_call, TRACE_BOOT_CALL);\
+		IF_ASSIGN(var, ent, struct trace_boot_ret, TRACE_BOOT_RET);\
+		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET);\
 		__ftrace_bad_type();					\
 	} while (0)
 

commit 3f5ec13696fd4a33bde42f385406cbb1d3cc96fd
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 23:21:31 2008 +0100

    tracing/fastboot: move boot tracer structs and funcs into their own header.
    
    Impact: Cleanups on the boot tracer and ftrace
    
    This patch bring some cleanups about the boot tracer headers. The
    functions and structures of this tracer have nothing related to ftrace
    and should have so their own header file.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e40ce0c14690..f69a5199596b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -8,6 +8,7 @@
 #include <linux/ring_buffer.h>
 #include <linux/mmiotrace.h>
 #include <linux/ftrace.h>
+#include <trace/boot.h>
 
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,

commit 15e6cb3673ea6277999642802406a764b49391b0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Nov 11 07:14:25 2008 +0100

    tracing: add a tracer to catch execution time of kernel functions
    
    Impact: add new tracing plugin which can trace full (entry+exit) function calls
    
    This tracer uses the low level function return ftrace plugin to
    measure the execution time of the kernel functions.
    
    The first field is the caller of the function, the second is the
    measured function, and the last one is the execution time in
    nanoseconds.
    
    - v3:
    
    - HAVE_FUNCTION_RET_TRACER have been added. Each arch that support ftrace return
      should enable it.
    - ftrace_return_stub becomes ftrace_stub.
    - CONFIG_FUNCTION_RET_TRACER depends now on CONFIG_FUNCTION_TRACER
    - Return traces printing can be used for other tracers on trace.c
    - Adapt to the new tracing API (no more ctrl_update callback)
    - Correct the check of "disabled" during insertion.
    - Minor changes...
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 978145088fb8..e40ce0c14690 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -22,6 +22,7 @@ enum trace_type {
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
 	TRACE_BOOT,
+	TRACE_FN_RET,
 
 	__TRACE_LAST_TYPE
 };
@@ -48,6 +49,15 @@ struct ftrace_entry {
 	unsigned long		ip;
 	unsigned long		parent_ip;
 };
+
+/* Function return entry */
+struct ftrace_ret_entry {
+	struct trace_entry	ent;
+	unsigned long		ip;
+	unsigned long		parent_ip;
+	unsigned long long	calltime;
+	unsigned long long	rettime;
+};
 extern struct tracer boot_tracer;
 
 /*
@@ -218,6 +228,7 @@ extern void __ftrace_bad_type(void);
 		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\
 			  TRACE_MMIO_MAP);				\
 		IF_ASSIGN(var, ent, struct trace_boot, TRACE_BOOT);	\
+		IF_ASSIGN(var, ent, struct ftrace_ret_entry, TRACE_FN_RET); \
 		__ftrace_bad_type();					\
 	} while (0)
 
@@ -321,6 +332,8 @@ void trace_function(struct trace_array *tr,
 		    unsigned long ip,
 		    unsigned long parent_ip,
 		    unsigned long flags, int pc);
+void
+trace_function_return(struct ftrace_retfunc *trace);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
@@ -393,6 +406,10 @@ extern void *head_page(struct trace_array_cpu *data);
 extern int trace_seq_printf(struct trace_seq *s, const char *fmt, ...);
 extern void trace_seq_print_cont(struct trace_seq *s,
 				 struct trace_iterator *iter);
+
+extern int
+seq_print_ip_sym(struct trace_seq *s, unsigned long ip,
+		unsigned long sym_flags);
 extern ssize_t trace_seq_to_user(struct trace_seq *s, char __user *ubuf,
 				 size_t cnt);
 extern long ns2usecs(cycle_t nsec);
@@ -400,6 +417,17 @@ extern int trace_vprintk(unsigned long ip, const char *fmt, va_list args);
 
 extern unsigned long trace_flags;
 
+/* Standard output formatting function used for function return traces */
+#ifdef CONFIG_FUNCTION_RET_TRACER
+extern enum print_line_t print_return_function(struct trace_iterator *iter);
+#else
+static inline enum print_line_t
+print_return_function(struct trace_iterator *iter)
+{
+	return TRACE_TYPE_UNHANDLED;
+}
+#endif
+
 /*
  * trace_iterator_flags is an enumeration that defines bit
  * positions into trace_flags that controls the output.
@@ -422,6 +450,13 @@ enum trace_iterator_flags {
 	TRACE_ITER_PREEMPTONLY		= 0x800,
 };
 
+/*
+ * TRACE_ITER_SYM_MASK masks the options in trace_flags that
+ * control the output of kernel symbols.
+ */
+#define TRACE_ITER_SYM_MASK \
+	(TRACE_ITER_PRINT_PARENT|TRACE_ITER_SYM_OFFSET|TRACE_ITER_SYM_ADDR)
+
 extern struct tracer nop_trace;
 
 /**

commit a309720c876d7ad2e224bfd1982c92ae4364c82e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: display start of CPU buffer in trace output
    
    Impact: change in trace output
    
    Because the trace buffers are per cpu ring buffers, the start of
    the trace can be confusing. If one CPU is very active at the
    end of the trace, its history will not go as far back as the
    other CPU traces.  This means that output for a particular CPU
    may not appear for the first part of a trace.
    
    To help annotate what is happening, and to prevent any more
    confusion, this patch adds a line that annotates the start of
    a CPU buffer output.
    
    For example:
    
           automount-3495  [001]   184.596443: dnotify_parent <-vfs_write
    [...]
           automount-3495  [001]   184.596449: dput <-path_put
           automount-3496  [002]   184.596450: down_read_trylock <-do_page_fault
    [...]
               sshd-3497  [001]   184.597069: up_read <-do_page_fault
              <idle>-0     [000]   184.597074: __exit_idle <-exit_idle
    [...]
           automount-3496  [002]   184.597257: filemap_fault <-__do_fault
              <idle>-0     [003]   184.597261: exit_idle <-smp_apic_timer_interrupt
    
    Note, parsers of a trace output should always ignore any lines that
    start with a '#'.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cfda9d219e66..978145088fb8 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -277,6 +277,8 @@ struct trace_iterator {
 	unsigned long		iter_flags;
 	loff_t			pos;
 	long			idx;
+
+	cpumask_t		started;
 };
 
 int tracing_is_enabled(void);

commit c76f06945be50564f925799ddfb6235ee4c26aa0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: remove trace array ctrl
    
    Impact: remove obsolete variable in trace_array structure
    
    With the new start / stop method of ftrace, the ctrl variable
    in the trace_array structure is now obsolete. Remove it.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e481edaae1c4..cfda9d219e66 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -172,7 +172,6 @@ struct trace_iterator;
 struct trace_array {
 	struct ring_buffer	*buffer;
 	unsigned long		entries;
-	long			ctrl;
 	int			cpu;
 	cycle_t			time_start;
 	struct task_struct	*waiter;

commit bbf5b1a0cecb56de6236db8b01c5bfb7ab8ba8b2
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: remove ctrl_update method
    
    Impact: Remove the ctrl_update tracer method
    
    With the new quick start/stop method of tracing, the ctrl_update
    method is out of date.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 25abfc45f081..e481edaae1c4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -244,7 +244,6 @@ struct tracer {
 	ssize_t			(*read)(struct trace_iterator *iter,
 					struct file *filp, char __user *ubuf,
 					size_t cnt, loff_t *ppos);
-	void			(*ctrl_update)(struct trace_array *tr);
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 	int			(*selftest)(struct tracer *trace,
 					    struct trace_array *tr);

commit e168e0516e476070faa9e8e7b23dfcba79b76d82
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: fix sched_switch API
    
    Impact: fix for sched_switch that broke dynamic ftrace startup
    
    The commit: tracing/fastboot: use sched switch tracer from boot tracer
    broke the API of the sched_switch trace. The use of the
    tracing_start/stop_cmdline record is for only recording the cmdline,
    NOT recording the schedule switches themselves.
    
    Seeing that the boot tracer broke the API to do something that it
    wanted, this patch adds a new interface for the API while
    puting back the original interface of the old API.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index db12e16137e1..25abfc45f081 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -324,7 +324,9 @@ void trace_function(struct trace_array *tr,
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
-void tracing_cmdline_assign_trace(struct trace_array *tr);
+void tracing_sched_switch_assign_trace(struct trace_array *tr);
+void tracing_stop_sched_switch_record(void);
+void tracing_start_sched_switch_record(void);
 int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 

commit 75f5c47da386445ba0c5a8b7e3ca0c906e763369
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 7 22:36:02 2008 -0500

    ftrace: fix boot trace sched startup
    
    Impact: boot tracer startup modified
    
    The boot tracer calls into some of the schedule tracing private functions
    that should not be exported. This patch cleans it up, and makes
    way for further changes in the ftrace infrastructure.
    
    This patch adds a api to assign a tracer array to the schedule
    context switch tracer.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3422489fad5e..db12e16137e1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -49,7 +49,6 @@ struct ftrace_entry {
 	unsigned long		parent_ip;
 };
 extern struct tracer boot_tracer;
-extern struct tracer sched_switch_trace; /* Used by the boot tracer */
 
 /*
  * Context switch trace entry - which task (and prio) we switched from/to:
@@ -325,6 +324,7 @@ void trace_function(struct trace_array *tr,
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
+void tracing_cmdline_assign_trace(struct trace_array *tr);
 int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 

commit 9036990d462e09366f7297a2d1da6582c3e6b1d3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 5 16:05:44 2008 -0500

    ftrace: restructure tracing start/stop infrastructure
    
    Impact: change where tracing is started up and stopped
    
    Currently, when a new tracer is selected via echo'ing a tracer name into
    the current_tracer file, the startup is only done if tracing_enabled is
    set to one. If tracing_enabled is changed to zero (by echo'ing 0 into
    the tracing_enabled file) a full shutdown is performed.
    
    The full startup and shutdown of a tracer can be expensive and the
    user can lose out traces when echo'ing in 0 to the tracing_enabled file,
    because the process takes too long. There can also be places that
    the user would like to start and stop the tracer several times and
    doing the full startup and shutdown of a tracer might be too expensive.
    
    This patch performs the full startup and shutdown when a tracer is
    selected. It also adds a way to do a quick start or stop of a tracer.
    The quick version is just a flag that prevents the tracing from
    taking place, but the overhead of the code is still there.
    
    For example, the startup of a tracer may enable tracepoints, or enable
    the function tracer.  The stop and start will just set a flag to
    have the tracer ignore the calls when the tracepoint or function trace
    is called.  The overhead of the tracer may still be present when
    the tracer is stopped, but no tracing will occur. Setting the tracer
    to the 'nop' tracer (or any other tracer) will perform the shutdown
    of the tracer which will disable the tracepoint or disable the
    function tracer.
    
    The tracing_enabled file will simply start or stop tracing.
    
    This change is all internal. The end result for the user should be the same
    as before. If tracing_enabled is not set, no trace will happen.
    If tracing_enabled is set, then the trace will happen. The tracing_enabled
    variable is static between tracers. Enabling  tracing_enabled and
    going to another tracer will keep tracing_enabled enabled. Same
    is true with disabling tracing_enabled.
    
    This patch will now provide a fast start/stop method to the users
    for enabling or disabling tracing.
    
    Note: There were two methods to the struct tracer that were never
     used: The methods start and stop. These were to be used as a hook
     to the reading of the trace output, but ended up not being
     necessary. These two methods are now used to enable the start
     and stop of each tracer, in case the tracer needs to do more than
     just not write into the buffer. For example, the irqsoff tracer
     must stop recording max latencies when tracing is stopped.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cc14a6bc1094..3422489fad5e 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -237,11 +237,11 @@ struct tracer {
 	const char		*name;
 	void			(*init)(struct trace_array *tr);
 	void			(*reset)(struct trace_array *tr);
+	void			(*start)(struct trace_array *tr);
+	void			(*stop)(struct trace_array *tr);
 	void			(*open)(struct trace_iterator *iter);
 	void			(*pipe_open)(struct trace_iterator *iter);
 	void			(*close)(struct trace_iterator *iter);
-	void			(*start)(struct trace_iterator *iter);
-	void			(*stop)(struct trace_iterator *iter);
 	ssize_t			(*read)(struct trace_iterator *iter,
 					struct file *filp, char __user *ubuf,
 					size_t cnt, loff_t *ppos);
@@ -282,6 +282,7 @@ struct trace_iterator {
 	long			idx;
 };
 
+int tracing_is_enabled(void);
 void trace_wake_up(void);
 void tracing_reset(struct trace_array *tr, int cpu);
 int tracing_open_generic(struct inode *inode, struct file *filp);

commit 79c81d220c8e25163f56edcdfaf23f83a4c88e6b
Merge: 3299b4dd1180 79a9d461fd52
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Nov 6 07:43:47 2008 +0100

    Merge branch 'tracing/fastboot' into tracing/ftrace

commit d7ad44b697c9d13e445ddc7d16f736fbac333249
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Oct 31 13:20:08 2008 +0100

    tracing/fastboot: use sched switch tracer from boot tracer
    
    Impact: enhance boot trace output with scheduling events
    
    Use the sched_switch tracer from the boot tracer.
    
    We also can trace schedule events inside the initcalls.
    Sched tracing is disabled after the initcall has finished and
    then reenabled before the next one is started.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8465ad052707..9911277b268b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -49,6 +49,7 @@ struct ftrace_entry {
 	unsigned long		parent_ip;
 };
 extern struct tracer boot_tracer;
+extern struct tracer sched_switch_trace; /* Used by the boot tracer */
 
 /*
  * Context switch trace entry - which task (and prio) we switched from/to:

commit b2a866f9344cb30d7ddf5d67b5b8393daf8bef07
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Nov 3 23:15:57 2008 -0500

    ftrace: function tracer with irqs disabled
    
    Impact: disable interrupts during trace entry creation (as opposed to preempt)
    
    To help with performance, I set the ftracer to not disable interrupts,
    and only to disable preemption. If an interrupt occurred, it would not
    be traced, because the function tracer protects itself from recursion.
    This may be faster, but the trace output might miss some traces.
    
    This patch makes the fuction trace disable interrupts, but it also
    adds a runtime feature to disable preemption instead. It does this by
    having two different tracer functions. When the function tracer is
    enabled, it will check to see which version is requested (irqs disabled
    or preemption disabled). Then it will use the corresponding function
    as the tracer.
    
    Irq disabling is the default behaviour, but if the user wants better
    performance, with the chance of missing traces, then they can choose
    the preempt disabled version.
    
    Running hackbench 3 times with the irqs disabled and 3 times with
    the preempt disabled function tracer yielded:
    
    tracing type       times            entries recorded
    ------------      --------          ----------------
    irq disabled      43.393            166433066
                      43.282            166172618
                      43.298            166256704
    
    preempt disabled  38.969            159871710
                      38.943            159972935
                      39.325            161056510
    
    Average:
    
       irqs disabled:  43.324           166287462
    preempt disabled:  39.079           160300385
    
     preempt is 10.8 percent faster than irqs disabled.
    
    I wrote a patch to count function trace recursion and reran hackbench.
    
    With irq disabled: 1,150 times the function tracer did not trace due to
      recursion.
    with preempt disabled: 5,117,718 times.
    
    The thousand times with irq disabled could be due to NMIs, or simply a case
    where it called a function that was not protected by notrace.
    
    But we also see that a large amount of the trace is lost with the
    preempt version.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 10c6dae76894..bb547e933af7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -415,6 +415,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_STACKTRACE		= 0x100,
 	TRACE_ITER_SCHED_TREE		= 0x200,
 	TRACE_ITER_PRINTK		= 0x400,
+	TRACE_ITER_PREEMPTONLY		= 0x800,
 };
 
 extern struct tracer nop_trace;

commit 8f0a056fcb2f83a069fb5d60c2383304b7456687
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Nov 3 23:15:55 2008 -0500

    ftrace: introduce ftrace_preempt_disable()/enable()
    
    Impact: add new ftrace-plugin internal APIs
    
    Parts of the tracer needs to be careful about schedule recursion.
    If the NEED_RESCHED flag is set, a preempt_enable will call schedule.
    Inside the schedule function, the NEED_RESCHED flag is cleared.
    
    The problem arises when a trace happens in the schedule function but before
    NEED_RESCHED is cleared. The race is as follows:
    
    schedule()
      >> tracer called
    
        trace_function()
           preempt_disable()
           [ record trace ]
           preempt_enable()  <<- here's the issue.
    
             [check NEED_RESCHED]
              schedule()
              [ Repeat the above, over and over again ]
    
    The naive approach is simply to use preempt_enable_no_schedule instead.
    The problem with that approach is that, although we solve the schedule
    recursion issue, we now might lose a preemption check when not in the
    schedule function.
    
      trace_function()
        preempt_disable()
        [ record trace ]
        [Interrupt comes in and sets NEED_RESCHED]
        preempt_enable_no_resched()
        [continue without scheduling]
    
    The way ftrace handles this problem is with the following approach:
    
            int resched;
    
            resched = need_resched();
            preempt_disable_notrace();
            [record trace]
            if (resched)
                    preempt_enable_no_sched_notrace();
            else
                    preempt_enable_notrace();
    
    This may seem like the opposite of what we want. If resched is set
    then we call the "no_sched" version??  The reason we do this is because
    if NEED_RESCHED is set before we disable preemption, there's two reasons
    for that:
    
      1) we are in an atomic code path
      2) we are already on our way to the schedule function, and maybe even
         in the schedule function, but have yet to clear the flag.
    
    Both the above cases we do not want to schedule.
    
    This solution has already been implemented within the ftrace infrastructure.
    But the problem is that it has been implemented several times. This patch
    encapsulates this code to two nice functions.
    
      resched = ftrace_preempt_disable();
      [ record trace]
      ftrace_preempt_enable(resched);
    
    This way the tracers do not need to worry about getting it right.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8465ad052707..10c6dae76894 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -419,4 +419,52 @@ enum trace_iterator_flags {
 
 extern struct tracer nop_trace;
 
+/**
+ * ftrace_preempt_disable - disable preemption scheduler safe
+ *
+ * When tracing can happen inside the scheduler, there exists
+ * cases that the tracing might happen before the need_resched
+ * flag is checked. If this happens and the tracer calls
+ * preempt_enable (after a disable), a schedule might take place
+ * causing an infinite recursion.
+ *
+ * To prevent this, we read the need_recshed flag before
+ * disabling preemption. When we want to enable preemption we
+ * check the flag, if it is set, then we call preempt_enable_no_resched.
+ * Otherwise, we call preempt_enable.
+ *
+ * The rational for doing the above is that if need resched is set
+ * and we have yet to reschedule, we are either in an atomic location
+ * (where we do not need to check for scheduling) or we are inside
+ * the scheduler and do not want to resched.
+ */
+static inline int ftrace_preempt_disable(void)
+{
+	int resched;
+
+	resched = need_resched();
+	preempt_disable_notrace();
+
+	return resched;
+}
+
+/**
+ * ftrace_preempt_enable - enable preemption scheduler safe
+ * @resched: the return value from ftrace_preempt_disable
+ *
+ * This is a scheduler safe way to enable preemption and not miss
+ * any preemption checks. The disabled saved the state of preemption.
+ * If resched is set, then we were either inside an atomic or
+ * are inside the scheduler (we would have already scheduled
+ * otherwise). In this case, we do not want to call normal
+ * preempt_enable, but preempt_enable_no_resched instead.
+ */
+static inline void ftrace_preempt_enable(int resched)
+{
+	if (resched)
+		preempt_enable_no_resched_notrace();
+	else
+		preempt_enable_notrace();
+}
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 9244489a7b69fe0746dc7cb3957f02e05bd1ceb0
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Oct 24 09:42:59 2008 -0400

    ftrace: handle archs that do not support irqs_disabled_flags
    
    Impact: build fix on non-lockdep architectures
    
    Some architectures do not support a way to read the irq flags that
    is set from "local_irq_save(flags)" to determine if interrupts were
    disabled or enabled. Ftrace uses this information to display to the user
    if the trace occurred with interrupts enabled or disabled.
    
    Besides the fact that those archs that do not support this will fail to
    compile, unless they fix it, we do not want to have the trace simply
    say interrupts were not disabled or they were enabled, without knowing
    the real answer.
    
    This patch adds a 'X' in the output to let the user know that the
    architecture they are running on does not support a way for the tracer
    to determine if interrupts were enabled or disabled. It also lets those
    same archs compile with tracing enabled.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6889ca48f1f1..8465ad052707 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -120,18 +120,20 @@ struct trace_boot {
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
- *  IRQS_OFF	- interrupts were disabled
- *  NEED_RESCED - reschedule is requested
- *  HARDIRQ	- inside an interrupt handler
- *  SOFTIRQ	- inside a softirq handler
- *  CONT	- multiple entries hold the trace item
+ *  IRQS_OFF		- interrupts were disabled
+ *  IRQS_NOSUPPORT 	- arch does not support irqs_disabled_flags
+ *  NEED_RESCED		- reschedule is requested
+ *  HARDIRQ		- inside an interrupt handler
+ *  SOFTIRQ		- inside a softirq handler
+ *  CONT		- multiple entries hold the trace item
  */
 enum trace_flag_type {
 	TRACE_FLAG_IRQS_OFF		= 0x01,
-	TRACE_FLAG_NEED_RESCHED		= 0x02,
-	TRACE_FLAG_HARDIRQ		= 0x04,
-	TRACE_FLAG_SOFTIRQ		= 0x08,
-	TRACE_FLAG_CONT			= 0x10,
+	TRACE_FLAG_IRQS_NOSUPPORT	= 0x02,
+	TRACE_FLAG_NEED_RESCHED		= 0x04,
+	TRACE_FLAG_HARDIRQ		= 0x08,
+	TRACE_FLAG_SOFTIRQ		= 0x10,
+	TRACE_FLAG_CONT			= 0x20,
 };
 
 #define TRACE_BUF_SIZE		1024

commit 606576ce816603d9fe1fb453a88bc6eea16ca709
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Oct 6 19:06:12 2008 -0400

    ftrace: rename FTRACE to FUNCTION_TRACER
    
    Due to confusion between the ftrace infrastructure and the gcc profiling
    tracer "ftrace", this patch renames the config options from FTRACE to
    FUNCTION_TRACER.  The other two names that are offspring from FTRACE
    DYNAMIC_FTRACE and FTRACE_MCOUNT_RECORD will stay the same.
    
    This patch was generated mostly by script, and partially by hand.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f1f99572cde7..6889ca48f1f1 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -335,7 +335,7 @@ void update_max_tr_single(struct trace_array *tr,
 
 extern cycle_t ftrace_now(int cpu);
 
-#ifdef CONFIG_FTRACE
+#ifdef CONFIG_FUNCTION_TRACER
 void tracing_start_function_trace(void);
 void tracing_stop_function_trace(void);
 #else

commit 38697053fa006411224a1790e2adb8216440ab0f
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Oct 1 13:14:09 2008 -0400

    ftrace: preempt disable over interrupt disable
    
    With the new ring buffer infrastructure in ftrace, I'm trying to make
    ftrace a little more light weight.
    
    This patch converts a lot of the local_irq_save/restore into
    preempt_disable/enable.  The original preempt count in a lot of cases
    has to be sent in as a parameter so that it can be recorded correctly.
    Some places were recording it incorrectly before anyway.
    
    This is also laying the ground work to make ftrace a little bit
    more reentrant, and remove all locking. The function tracers must
    still protect from reentrancy.
    
    Note: All the function tracers must be careful when using preempt_disable.
      It must do the following:
    
      resched = need_resched();
      preempt_disable_notrace();
      [...]
      if (resched)
            preempt_enable_no_resched_notrace();
      else
            preempt_enable_notrace();
    
    The reason is that if this function traces schedule() itself, the
    preempt_enable_notrace() will cause a schedule, which will lead
    us into a recursive failure.
    
    If we needed to reschedule before calling preempt_disable, we
    should have already scheduled. Since we did not, this is most
    likely that we should not and are probably inside a schedule
    function.
    
    If resched was not set, we still need to catch the need resched
    flag being set when preemption was off and the if case at the
    end will catch that for us.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f02042d0d828..f1f99572cde7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -288,35 +288,36 @@ void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
 						struct trace_array_cpu *data);
 void tracing_generic_entry_update(struct trace_entry *entry,
-						unsigned long flags);
+				  unsigned long flags,
+				  int pc);
 
 void ftrace(struct trace_array *tr,
 			    struct trace_array_cpu *data,
 			    unsigned long ip,
 			    unsigned long parent_ip,
-			    unsigned long flags);
+			    unsigned long flags, int pc);
 void tracing_sched_switch_trace(struct trace_array *tr,
 				struct trace_array_cpu *data,
 				struct task_struct *prev,
 				struct task_struct *next,
-				unsigned long flags);
+				unsigned long flags, int pc);
 void tracing_record_cmdline(struct task_struct *tsk);
 
 void tracing_sched_wakeup_trace(struct trace_array *tr,
 				struct trace_array_cpu *data,
 				struct task_struct *wakee,
 				struct task_struct *cur,
-				unsigned long flags);
+				unsigned long flags, int pc);
 void trace_special(struct trace_array *tr,
 		   struct trace_array_cpu *data,
 		   unsigned long arg1,
 		   unsigned long arg2,
-		   unsigned long arg3);
+		   unsigned long arg3, int pc);
 void trace_function(struct trace_array *tr,
 		    struct trace_array_cpu *data,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-		    unsigned long flags);
+		    unsigned long flags, int pc);
 
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);

commit 7104f300c5a69b46dda00d898034dd05c9f21739
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Oct 1 10:52:51 2008 -0400

    ftrace: type cast filter+verifier
    
    The mmiotrace map had a bug that would typecast the entry from
    the trace to the wrong type. That is a known danger of C typecasts,
    there's absolutely zero checking done on them.
    
    Help that problem a bit by using a GCC extension to implement a
    type filter that restricts the types that a trace record can be
    cast into, and by adding a dynamic check (in debug mode) to verify
    the type of the entry.
    
    This patch adds a macro to assign all entries of ftrace using the type
    of the variable and checking the entry id. The typecasts are now done
    in the macro for only those types that it knows about, which should
    be all the types that are allowed to be read from the tracer.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a921ba5d292d..f02042d0d828 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -177,6 +177,48 @@ struct trace_array {
 	struct trace_array_cpu	*data[NR_CPUS];
 };
 
+#define FTRACE_CMP_TYPE(var, type) \
+	__builtin_types_compatible_p(typeof(var), type *)
+
+#undef IF_ASSIGN
+#define IF_ASSIGN(var, entry, etype, id)		\
+	if (FTRACE_CMP_TYPE(var, etype)) {		\
+		var = (typeof(var))(entry);		\
+		WARN_ON(id && (entry)->type != id);	\
+		break;					\
+	}
+
+/* Will cause compile errors if type is not found. */
+extern void __ftrace_bad_type(void);
+
+/*
+ * The trace_assign_type is a verifier that the entry type is
+ * the same as the type being assigned. To add new types simply
+ * add a line with the following format:
+ *
+ * IF_ASSIGN(var, ent, type, id);
+ *
+ *  Where "type" is the trace type that includes the trace_entry
+ *  as the "ent" item. And "id" is the trace identifier that is
+ *  used in the trace_type enum.
+ *
+ *  If the type can have more than one id, then use zero.
+ */
+#define trace_assign_type(var, ent)					\
+	do {								\
+		IF_ASSIGN(var, ent, struct ftrace_entry, TRACE_FN);	\
+		IF_ASSIGN(var, ent, struct ctx_switch_entry, 0);	\
+		IF_ASSIGN(var, ent, struct trace_field_cont, TRACE_CONT); \
+		IF_ASSIGN(var, ent, struct stack_entry, TRACE_STACK);	\
+		IF_ASSIGN(var, ent, struct print_entry, TRACE_PRINT);	\
+		IF_ASSIGN(var, ent, struct special_entry, 0);		\
+		IF_ASSIGN(var, ent, struct trace_mmiotrace_rw,		\
+			  TRACE_MMIO_RW);				\
+		IF_ASSIGN(var, ent, struct trace_mmiotrace_map,		\
+			  TRACE_MMIO_MAP);				\
+		IF_ASSIGN(var, ent, struct trace_boot, TRACE_BOOT);	\
+		__ftrace_bad_type();					\
+	} while (0)
 
 /* Return values for print_line callback */
 enum print_line_t {

commit 2c4f035f6c3e8fda661eb6105aa51ef07aa71607
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Mon Sep 29 20:18:34 2008 +0200

    tracing/ftrace: change the type of the print_line callback
    
    We need a kind of disambiguation when a print_line callback
    returns 0.
    
    _There is not enough space to print all the entry.
     Please flush the seq and retry.
    _I can't handle this type of entry
    
    This patch changes the type of this callback for better information.
    
    Also some changes have been made in this V2.
    
    _ Only relay to default functions after the print_line callback fails.
    _ This patch doesn't fix the issue with the broken pipe (see patch 2/4 for that)
    
    Some things are still in discussion:
    
    _ Find better names for the enum print_line_t values
    _ Change the type of print_trace_line into boolean.
    
    Patches to change that can be sent later.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Pekka Paalanen <pq@iki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index e541a6b7e312..a921ba5d292d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -177,6 +177,14 @@ struct trace_array {
 	struct trace_array_cpu	*data[NR_CPUS];
 };
 
+
+/* Return values for print_line callback */
+enum print_line_t {
+	TRACE_TYPE_PARTIAL_LINE	= 0,	/* Retry after flushing the seq */
+	TRACE_TYPE_HANDLED	= 1,
+	TRACE_TYPE_UNHANDLED	= 2	/* Relay to other output functions */
+};
+
 /*
  * A specific tracer, represented by methods that operate on a trace array:
  */
@@ -197,7 +205,7 @@ struct tracer {
 	int			(*selftest)(struct tracer *trace,
 					    struct trace_array *tr);
 #endif
-	int			(*print_line)(struct trace_iterator *iter);
+	enum print_line_t	(*print_line)(struct trace_iterator *iter);
 	struct tracer		*next;
 	int			print_max;
 };

commit 777e208d40d0953efc6fb4ab58590da3f7d8f02d
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Sep 29 23:02:42 2008 -0400

    ftrace: take advantage of variable length entries
    
    Now that the underlining ring buffer for ftrace now hold variable length
    entries, we can take advantage of this by only storing the size of the
    actual event into the buffer. This happens to increase the number of
    entries in the buffer dramatically.
    
    We can also get rid of the "trace_cont" operation, but I'm keeping that
    until we have no more users. Some of the ftrace tracers can now change
    their code to adapt to this new feature.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f6965f775b43..e541a6b7e312 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -26,10 +26,25 @@ enum trace_type {
 	__TRACE_LAST_TYPE
 };
 
+/*
+ * The trace entry - the most basic unit of tracing. This is what
+ * is printed in the end as a single line in the trace output, such as:
+ *
+ *     bash-15816 [01]   235.197585: idle_cpu <- irq_enter
+ */
+struct trace_entry {
+	unsigned char		type;
+	unsigned char		cpu;
+	unsigned char		flags;
+	unsigned char		preempt_count;
+	int			pid;
+};
+
 /*
  * Function trace entry - function address and parent function addres:
  */
 struct ftrace_entry {
+	struct trace_entry	ent;
 	unsigned long		ip;
 	unsigned long		parent_ip;
 };
@@ -39,6 +54,7 @@ extern struct tracer boot_tracer;
  * Context switch trace entry - which task (and prio) we switched from/to:
  */
 struct ctx_switch_entry {
+	struct trace_entry	ent;
 	unsigned int		prev_pid;
 	unsigned char		prev_prio;
 	unsigned char		prev_state;
@@ -52,6 +68,7 @@ struct ctx_switch_entry {
  * Special (free-form) trace entry:
  */
 struct special_entry {
+	struct trace_entry	ent;
 	unsigned long		arg1;
 	unsigned long		arg2;
 	unsigned long		arg3;
@@ -64,6 +81,7 @@ struct special_entry {
 #define FTRACE_STACK_ENTRIES	8
 
 struct stack_entry {
+	struct trace_entry	ent;
 	unsigned long		caller[FTRACE_STACK_ENTRIES];
 };
 
@@ -71,10 +89,34 @@ struct stack_entry {
  * ftrace_printk entry:
  */
 struct print_entry {
+	struct trace_entry	ent;
 	unsigned long		ip;
 	char			buf[];
 };
 
+#define TRACE_OLD_SIZE		88
+
+struct trace_field_cont {
+	unsigned char		type;
+	/* Temporary till we get rid of this completely */
+	char			buf[TRACE_OLD_SIZE - 1];
+};
+
+struct trace_mmiotrace_rw {
+	struct trace_entry	ent;
+	struct mmiotrace_rw	rw;
+};
+
+struct trace_mmiotrace_map {
+	struct trace_entry	ent;
+	struct mmiotrace_map	map;
+};
+
+struct trace_boot {
+	struct trace_entry	ent;
+	struct boot_trace	initcall;
+};
+
 /*
  * trace_flag_type is an enumeration that holds different
  * states when a trace occurs. These are:
@@ -92,46 +134,7 @@ enum trace_flag_type {
 	TRACE_FLAG_CONT			= 0x10,
 };
 
-/*
- * The trace field - the most basic unit of tracing. This is what
- * is printed in the end as a single line in the trace output, such as:
- *
- *     bash-15816 [01]   235.197585: idle_cpu <- irq_enter
- */
-struct trace_field {
-	char			cpu;
-	char			flags;
-	char			preempt_count;
-	int			pid;
-	union {
-		struct ftrace_entry		fn;
-		struct ctx_switch_entry		ctx;
-		struct special_entry		special;
-		struct stack_entry		stack;
-		struct print_entry		print;
-		struct mmiotrace_rw		mmiorw;
-		struct mmiotrace_map		mmiomap;
-		struct boot_trace		initcall;
-	};
-};
-
-struct trace_field_cont {
-	char				buf[sizeof(struct trace_field)];
-};
-
-struct trace_entry {
-	char 				type;
-	union {
-		struct trace_field	field;
-		struct trace_field_cont	cont;
-	};
-};
-
-#define TRACE_ENTRY_SIZE	sizeof(struct trace_entry)
 #define TRACE_BUF_SIZE		1024
-#define TRACE_PRINT_BUF_SIZE \
-	(sizeof(struct trace_field) - offsetof(struct trace_field, print.buf))
-#define TRACE_CONT_BUF_SIZE	sizeof(struct trace_field)
 
 /*
  * The CPU trace array - it consists of thousands of trace entries

commit 3928a8a2d98081d1bc3c0a84a2d70e29b90ecf1c
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon Sep 29 23:02:41 2008 -0400

    ftrace: make work with new ring buffer
    
    This patch ports ftrace over to the new ring buffer.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b28bf8812efc..f6965f775b43 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -5,6 +5,7 @@
 #include <asm/atomic.h>
 #include <linux/sched.h>
 #include <linux/clocksource.h>
+#include <linux/ring_buffer.h>
 #include <linux/mmiotrace.h>
 #include <linux/ftrace.h>
 
@@ -102,7 +103,6 @@ struct trace_field {
 	char			flags;
 	char			preempt_count;
 	int			pid;
-	cycle_t			t;
 	union {
 		struct ftrace_entry		fn;
 		struct ctx_switch_entry		ctx;
@@ -139,16 +139,9 @@ struct trace_entry {
  * the trace, etc.)
  */
 struct trace_array_cpu {
-	struct list_head	trace_pages;
 	atomic_t		disabled;
-	raw_spinlock_t		lock;
-	struct lock_class_key	lock_key;
 
 	/* these fields get copied into max-trace: */
-	unsigned		trace_head_idx;
-	unsigned		trace_tail_idx;
-	void			*trace_head; /* producer */
-	void			*trace_tail; /* consumer */
 	unsigned long		trace_idx;
 	unsigned long		overrun;
 	unsigned long		saved_latency;
@@ -172,6 +165,7 @@ struct trace_iterator;
  * They have on/off state as well:
  */
 struct trace_array {
+	struct ring_buffer	*buffer;
 	unsigned long		entries;
 	long			ctrl;
 	int			cpu;
@@ -219,27 +213,21 @@ struct trace_iterator {
 	struct trace_array	*tr;
 	struct tracer		*trace;
 	void			*private;
-	long			last_overrun[NR_CPUS];
-	long			overrun[NR_CPUS];
+	struct ring_buffer_iter	*buffer_iter[NR_CPUS];
 
 	/* The below is zeroed out in pipe_read */
 	struct trace_seq	seq;
 	struct trace_entry	*ent;
 	int			cpu;
-
-	struct trace_entry	*prev_ent;
-	int			prev_cpu;
+	u64			ts;
 
 	unsigned long		iter_flags;
 	loff_t			pos;
-	unsigned long		next_idx[NR_CPUS];
-	struct list_head	*next_page[NR_CPUS];
-	unsigned		next_page_idx[NR_CPUS];
 	long			idx;
 };
 
 void trace_wake_up(void);
-void tracing_reset(struct trace_array_cpu *data);
+void tracing_reset(struct trace_array *tr, int cpu);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *tracing_init_dentry(void);
 void init_tracer_sysprof_debugfs(struct dentry *d_tracer);

commit d13744cd6e3fef373a3fe656ac349b4e7c49ff79
Author: Frédéric Weisbecker <fweisbec@gmail.com>
Date:   Tue Sep 23 11:32:08 2008 +0100

    tracing/ftrace: add the boot tracer
    
    Add the boot/initcall tracer.
    
    It's primary purpose is to be able to trace the initcalls.
    
    It is intended to be used with scripts/bootgraph.pl after some small
    improvements.
    
    Note that it is not active after its init. To avoid tracing (and so
    crashing) before the whole tracing engine init, you have to explicitly
    call start_boot_trace() after do_pre_smp_initcalls() to enable it.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cb2c3fb7dd54..b28bf8812efc 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -6,6 +6,7 @@
 #include <linux/sched.h>
 #include <linux/clocksource.h>
 #include <linux/mmiotrace.h>
+#include <linux/ftrace.h>
 
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
@@ -19,6 +20,7 @@ enum trace_type {
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
+	TRACE_BOOT,
 
 	__TRACE_LAST_TYPE
 };
@@ -30,6 +32,7 @@ struct ftrace_entry {
 	unsigned long		ip;
 	unsigned long		parent_ip;
 };
+extern struct tracer boot_tracer;
 
 /*
  * Context switch trace entry - which task (and prio) we switched from/to:
@@ -108,6 +111,7 @@ struct trace_field {
 		struct print_entry		print;
 		struct mmiotrace_rw		mmiorw;
 		struct mmiotrace_map		mmiomap;
+		struct boot_trace		initcall;
 	};
 };
 

commit 43a15386c4faf913f7d70a47748c266d6210cd6e
Author: Frédéric Weisbecker <fweisbec@gmail.com>
Date:   Sun Sep 21 20:16:30 2008 +0200

    tracing/ftrace: replace none tracer by nop tracer
    
    Replace "none" tracer by the recently created "nop" tracer.
    Both are pretty similar except that nop accepts TRACE_PRINT
    or TRACE_SPECIAL entries.
    
    And as a consequence, changing the size of the ring buffer now
    requires that tracing has already been disabled.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Steven Noonan <steven@uplinklabs.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c8c687088b4d..cb2c3fb7dd54 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -369,4 +369,6 @@ enum trace_iterator_flags {
 	TRACE_ITER_PRINTK		= 0x400,
 };
 
+extern struct tracer nop_trace;
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 8925b394eca0bb0a444a6487d702d0f650e94a10
Author: Steven Noonan <steven@uplinklabs.net>
Date:   Sat Sep 20 01:00:38 2008 -0700

    trace: remove pointless ifdefs
    
    The functions are already 'extern' anyway, so there's no problem
    with linkage. Removing these ifdefs also helps find any potential
    compiler errors.
    
    Suggested by Andrew Morton.
    
    Signed-off-by: Steven Noonan <steven@uplinklabs.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 447d4b9b6391..c8c687088b4d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -319,38 +319,22 @@ extern int DYN_FTRACE_TEST_NAME(void);
 #endif
 
 #ifdef CONFIG_FTRACE_STARTUP_TEST
-#ifdef CONFIG_FTRACE
 extern int trace_selftest_startup_function(struct tracer *trace,
 					   struct trace_array *tr);
-#endif
-#ifdef CONFIG_IRQSOFF_TRACER
 extern int trace_selftest_startup_irqsoff(struct tracer *trace,
 					  struct trace_array *tr);
-#endif
-#ifdef CONFIG_PREEMPT_TRACER
 extern int trace_selftest_startup_preemptoff(struct tracer *trace,
 					     struct trace_array *tr);
-#endif
-#if defined(CONFIG_IRQSOFF_TRACER) && defined(CONFIG_PREEMPT_TRACER)
 extern int trace_selftest_startup_preemptirqsoff(struct tracer *trace,
 						 struct trace_array *tr);
-#endif
-#ifdef CONFIG_SCHED_TRACER
 extern int trace_selftest_startup_wakeup(struct tracer *trace,
 					 struct trace_array *tr);
-#endif
-#ifdef CONFIG_NOP_TRACER
 extern int trace_selftest_startup_nop(struct tracer *trace,
 					 struct trace_array *tr);
-#endif
-#ifdef CONFIG_CONTEXT_SWITCH_TRACER
 extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 					       struct trace_array *tr);
-#endif
-#ifdef CONFIG_SYSPROF_TRACER
 extern int trace_selftest_startup_sysprof(struct tracer *trace,
 					       struct trace_array *tr);
-#endif
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);

commit fb1b6d8b5154c692172a424e45fbd0573295cb93
Author: Steven Noonan <steven@uplinklabs.net>
Date:   Fri Sep 19 03:06:43 2008 -0700

    ftrace: add nop tracer
    
    A no-op tracer which can serve two purposes:
    
     1. A template for development of a new tracer.
     2. A convenient way to see ftrace_printk() calls without
        an irrelevant trace making the output messy.
    
    [ mingo@elte.hu: resolved conflicts ]
    Signed-off-by: Steven Noonan <steven@uplinklabs.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 42f65d0097f0..447d4b9b6391 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -339,6 +339,10 @@ extern int trace_selftest_startup_preemptirqsoff(struct tracer *trace,
 extern int trace_selftest_startup_wakeup(struct tracer *trace,
 					 struct trace_array *tr);
 #endif
+#ifdef CONFIG_NOP_TRACER
+extern int trace_selftest_startup_nop(struct tracer *trace,
+					 struct trace_array *tr);
+#endif
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER
 extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 					       struct trace_array *tr);

commit 5bf9a1ee350a10feb94107de32a203d81fbbe706
Author: Pekka Paalanen <pq@iki.fi>
Date:   Tue Sep 16 22:06:42 2008 +0300

    ftrace: inject markers via trace_marker file
    
    Allow a user to inject a marker (TRACE_PRINT entry) into the trace ring
    buffer. The related file operations are derived from code by Frédéric
    Weisbecker <fweisbec@gmail.com>.
    
    Signed-off-by: Pekka Paalanen <pq@iki.fi>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 648433d18ccb..42f65d0097f0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -124,6 +124,10 @@ struct trace_entry {
 };
 
 #define TRACE_ENTRY_SIZE	sizeof(struct trace_entry)
+#define TRACE_BUF_SIZE		1024
+#define TRACE_PRINT_BUF_SIZE \
+	(sizeof(struct trace_field) - offsetof(struct trace_field, print.buf))
+#define TRACE_CONT_BUF_SIZE	sizeof(struct trace_field)
 
 /*
  * The CPU trace array - it consists of thousands of trace entries

commit fc5e27ae4b45a0619701a83f30d9b7fad7ed9400
Author: Pekka Paalanen <pq@iki.fi>
Date:   Tue Sep 16 22:02:27 2008 +0300

    mmiotrace: handle TRACE_PRINT entries
    
    Also make trace_seq_print_cont() non-static, and add a newline if the
    seq buffer can't hold all data.
    
    Signed-off-by: Pekka Paalanen <pq@iki.fi>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index be3b3cf95f4b..648433d18ccb 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -71,6 +71,23 @@ struct print_entry {
 	char			buf[];
 };
 
+/*
+ * trace_flag_type is an enumeration that holds different
+ * states when a trace occurs. These are:
+ *  IRQS_OFF	- interrupts were disabled
+ *  NEED_RESCED - reschedule is requested
+ *  HARDIRQ	- inside an interrupt handler
+ *  SOFTIRQ	- inside a softirq handler
+ *  CONT	- multiple entries hold the trace item
+ */
+enum trace_flag_type {
+	TRACE_FLAG_IRQS_OFF		= 0x01,
+	TRACE_FLAG_NEED_RESCHED		= 0x02,
+	TRACE_FLAG_HARDIRQ		= 0x04,
+	TRACE_FLAG_SOFTIRQ		= 0x08,
+	TRACE_FLAG_CONT			= 0x10,
+};
+
 /*
  * The trace field - the most basic unit of tracing. This is what
  * is printed in the end as a single line in the trace output, such as:
@@ -330,6 +347,8 @@ extern int trace_selftest_startup_sysprof(struct tracer *trace,
 
 extern void *head_page(struct trace_array_cpu *data);
 extern int trace_seq_printf(struct trace_seq *s, const char *fmt, ...);
+extern void trace_seq_print_cont(struct trace_seq *s,
+				 struct trace_iterator *iter);
 extern ssize_t trace_seq_to_user(struct trace_seq *s, char __user *ubuf,
 				 size_t cnt);
 extern long ns2usecs(cycle_t nsec);

commit 801fe40001dfc263848552fb28924b766ed44ea4
Author: Pekka Paalanen <pq@iki.fi>
Date:   Tue Sep 16 21:58:24 2008 +0300

    ftrace: add trace_vprintk()
    
    trace_vprintk() for easier implementation of tracer specific *_printk
    functions. Add check check for no_tracer, and implement
    __ftrace_printk() as a wrapper.
    
    Signed-off-by: Pekka Paalanen <pq@iki.fi>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 9d39aa00a9c6..be3b3cf95f4b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -333,6 +333,7 @@ extern int trace_seq_printf(struct trace_seq *s, const char *fmt, ...);
 extern ssize_t trace_seq_to_user(struct trace_seq *s, char __user *ubuf,
 				 size_t cnt);
 extern long ns2usecs(cycle_t nsec);
+extern int trace_vprintk(unsigned long ip, const char *fmt, va_list args);
 
 extern unsigned long trace_flags;
 

commit 45dcd8b8a8ca855591e3ac882d3a7fc255d09d43
Author: Pekka Paalanen <pq@iki.fi>
Date:   Tue Sep 16 21:56:41 2008 +0300

    ftrace: move mmiotrace functions out of trace.c
    
    Moves the mmiotrace specific functions from trace.c to
    trace_mmiotrace.c. Functions trace_wake_up(), tracing_get_trace_entry(),
    and tracing_generic_entry_update() are therefore made available outside
    trace.c.
    
    Signed-off-by: Pekka Paalanen <pq@iki.fi>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 77c265f6a779..9d39aa00a9c6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -213,11 +213,17 @@ struct trace_iterator {
 	long			idx;
 };
 
+void trace_wake_up(void);
 void tracing_reset(struct trace_array_cpu *data);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *tracing_init_dentry(void);
 void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
 
+struct trace_entry *tracing_get_trace_entry(struct trace_array *tr,
+						struct trace_array_cpu *data);
+void tracing_generic_entry_update(struct trace_entry *entry,
+						unsigned long flags);
+
 void ftrace(struct trace_array *tr,
 			    struct trace_array_cpu *data,
 			    unsigned long ip,
@@ -291,15 +297,6 @@ extern unsigned long ftrace_update_tot_cnt;
 extern int DYN_FTRACE_TEST_NAME(void);
 #endif
 
-#ifdef CONFIG_MMIOTRACE
-extern void __trace_mmiotrace_rw(struct trace_array *tr,
-				struct trace_array_cpu *data,
-				struct mmiotrace_rw *rw);
-extern void __trace_mmiotrace_map(struct trace_array *tr,
-				struct trace_array_cpu *data,
-				struct mmiotrace_map *map);
-#endif
-
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 #ifdef CONFIG_FTRACE
 extern int trace_selftest_startup_function(struct tracer *trace,

commit 80b5e940050c273ba277acbf3a9fbc1d4441e681
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 4 10:24:16 2008 +0200

    ftrace: sched_switch: show the wakee's cpu
    
    While profiling the smp behaviour of the scheduler it was needed to know to
    which cpu a task got woken.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5f54c875c8be..77c265f6a779 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -41,6 +41,7 @@ struct ctx_switch_entry {
 	unsigned int		next_pid;
 	unsigned char		next_prio;
 	unsigned char		next_state;
+	unsigned int		next_cpu;
 };
 
 /*

commit f09ce573f57ddc35c67b39e51f34545877b30031
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Sep 4 10:24:14 2008 +0200

    ftrace: make ftrace_printk usable with the other tracers
    
    Currently ftrace_printk only works with the ftrace tracer, switch it to an
    iter_ctrl setting so we can make us of them with other tracers too.
    
    [rostedt@redhat.com: tweak to the disable condition]
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 50b6d7a6f01a..5f54c875c8be 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -356,6 +356,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_BLOCK		= 0x80,
 	TRACE_ITER_STACKTRACE		= 0x100,
 	TRACE_ITER_SCHED_TREE		= 0x200,
+	TRACE_ITER_PRINTK		= 0x400,
 };
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit dd0e545f061f90099a3dcc13aa77e29c6295cf23
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Aug 1 12:26:41 2008 -0400

    ftrace: printk formatting infrastructure
    
    This patch adds a feature that can help kernel developers debug their
    code using ftrace.
    
      int ftrace_printk(const char *fmt, ...);
    
    This records into the ftrace buffer using printf formatting. The entry
    size in the buffers are still a fixed length. A new type has been added
    that allows for more entries to be used for a single recording.
    
    The start of the print is still the same as the other entries.
    
    It returns the number of characters written to the ftrace buffer.
    
    For example:
    
    Having a module with the following code:
    
    static int __init ftrace_print_test(void)
    {
            ftrace_printk("jiffies are %ld\n", jiffies);
            return 0;
    }
    
    Gives me:
    
      insmod-5441  3...1 7569us : ftrace_print_test: jiffies are 4296626666
    
    for the latency_trace file and:
    
              insmod-5441  [03]  1959.370498: ftrace_print_test jiffies are 4296626666
    
    for the trace file.
    
    Note: Only the infrastructure should go into the kernel. It is to help
    facilitate debugging for other kernel developers. Calls to ftrace_printk
    is not intended to be left in the kernel, and should be frowned upon just
    like scattering printks around in the code.
    
    But having this easily at your fingertips helps the debugging go faster
    and bugs be solved quicker.
    
    Maybe later on, we can hook this with markers and have their printf format
    be sucked into ftrace output.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6ddd6a6556cf..50b6d7a6f01a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -13,7 +13,9 @@ enum trace_type {
 	TRACE_FN,
 	TRACE_CTX,
 	TRACE_WAKE,
+	TRACE_CONT,
 	TRACE_STACK,
+	TRACE_PRINT,
 	TRACE_SPECIAL,
 	TRACE_MMIO_RW,
 	TRACE_MMIO_MAP,
@@ -60,6 +62,14 @@ struct stack_entry {
 	unsigned long		caller[FTRACE_STACK_ENTRIES];
 };
 
+/*
+ * ftrace_printk entry:
+ */
+struct print_entry {
+	unsigned long		ip;
+	char			buf[];
+};
+
 /*
  * The trace field - the most basic unit of tracing. This is what
  * is printed in the end as a single line in the trace output, such as:
@@ -77,6 +87,7 @@ struct trace_field {
 		struct ctx_switch_entry		ctx;
 		struct special_entry		special;
 		struct stack_entry		stack;
+		struct print_entry		print;
 		struct mmiotrace_rw		mmiorw;
 		struct mmiotrace_map		mmiomap;
 	};

commit 2e2ca155cd2213b4f398031180fb3d399d5b7db9
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Fri Aug 1 12:26:40 2008 -0400

    ftrace: new continue entry - separate out from trace_entry
    
    Some tracers will need to work with more than one entry. In order to do this
    the trace_entry structure was split into two fields. One for the start of
    all entries, and one to continue an existing entry.
    
    The trace_entry structure now has a "field" entry that consists of the previous
    content of the trace_entry, and a "cont" entry that is just a string buffer
    the size of the "field" entry.
    
    Thanks to Andrew Morton for suggesting this idea.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f69f86788c2b..6ddd6a6556cf 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -61,13 +61,12 @@ struct stack_entry {
 };
 
 /*
- * The trace entry - the most basic unit of tracing. This is what
+ * The trace field - the most basic unit of tracing. This is what
  * is printed in the end as a single line in the trace output, such as:
  *
  *     bash-15816 [01]   235.197585: idle_cpu <- irq_enter
  */
-struct trace_entry {
-	char			type;
+struct trace_field {
 	char			cpu;
 	char			flags;
 	char			preempt_count;
@@ -83,6 +82,18 @@ struct trace_entry {
 	};
 };
 
+struct trace_field_cont {
+	char				buf[sizeof(struct trace_field)];
+};
+
+struct trace_entry {
+	char 				type;
+	union {
+		struct trace_field	field;
+		struct trace_field_cont	cont;
+	};
+};
+
 #define TRACE_ENTRY_SIZE	sizeof(struct trace_entry)
 
 /*

commit 6712e299b7dc78aa4971b85e803435ee6d49a9dd
Merge: ec1bb60bbff0 b2613e370dbe
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Jul 14 15:58:35 2008 +0200

    Merge branch 'tracing/ftrace' into auto-ftrace-next

commit 001b6767b1d0c89e458e5ddb039245b268f569fb
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Jul 10 20:58:10 2008 -0400

    ftrace: define function trace nop
    
    When CONFIG_FTRACE is not enabled, the tracing_start_functon_trace
    and tracing_stop_function_trace should be nops.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 6b8bd8800d04..df840f42be39 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -216,8 +216,6 @@ void trace_function(struct trace_array *tr,
 		    unsigned long parent_ip,
 		    unsigned long flags);
 
-void tracing_start_function_trace(void);
-void tracing_stop_function_trace(void);
 void tracing_start_cmdline_record(void);
 void tracing_stop_cmdline_record(void);
 int register_tracer(struct tracer *type);
@@ -234,6 +232,14 @@ void update_max_tr_single(struct trace_array *tr,
 
 extern cycle_t ftrace_now(int cpu);
 
+#ifdef CONFIG_FTRACE
+void tracing_start_function_trace(void);
+void tracing_stop_function_trace(void);
+#else
+# define tracing_start_function_trace()		do { } while (0)
+# define tracing_stop_function_trace()		do { } while (0)
+#endif
+
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER
 typedef void
 (*tracer_switch_func_t)(void *private,

commit ec1bb60bbff0386c3ec25360e7a8c72f467a6ff1
Merge: 5373fdbdc1db 37f5d732f34f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 10 11:43:08 2008 +0200

    Merge branch 'tracing/sysprof' into auto-ftrace-next

commit 5373fdbdc1dba69aa956098650f71b731d471885
Merge: bac0c9103b31 4d51c7587bb1
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 10 11:43:06 2008 +0200

    Merge branch 'tracing/mmiotrace' into auto-ftrace-next

commit 41bc8144d02028133bcd1d545023c6f49e8b2411
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu May 22 11:49:22 2008 -0400

    ftrace: fix up cmdline recording
    
    The new work with converting the trace hooks over to markers broke the
    command line recording of ftrace. This patch fixes it again.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c460e85e94ed..6b8bd8800d04 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -218,6 +218,8 @@ void trace_function(struct trace_array *tr,
 
 void tracing_start_function_trace(void);
 void tracing_stop_function_trace(void);
+void tracing_start_cmdline_record(void);
+void tracing_stop_cmdline_record(void);
 int register_tracer(struct tracer *type);
 void unregister_tracer(struct tracer *type);
 
@@ -226,8 +228,6 @@ extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 extern unsigned long tracing_max_latency;
 extern unsigned long tracing_thresh;
 
-extern atomic_t trace_record_cmdline_enabled;
-
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);

commit bd8ac686c73c7e925fcfe0b02dc4e7b947127864
Author: Pekka Paalanen <pq@iki.fi>
Date:   Mon May 12 21:20:57 2008 +0200

    ftrace: mmiotrace, updates
    
    here is a patch that makes mmiotrace work almost well within the tracing
    framework. The patch applies on top of my previous patch. I have my own
    output formatting in place now.
    
    Summary of changes:
    - fix the NULL dereference that was due to not calling tracing_reset()
    - add print_line() callback into struct tracer
    - implement print_line() for mmiotrace, producing up-to-spec text
    - add my output header, but that is not really called in the right place
    - rewrote the main structs in mmiotrace
    - added two new trace entry types: TRACE_MMIO_RW and TRACE_MMIO_MAP
    - made some functions in trace.c non-static
    - check current==NULL in tracing_generic_entry_update()
    - fix(?) comparison in trace_seq_printf()
    
    Things seem to work fine except a few issues. Markers (text lines injected
    into mmiotrace log) are missing, I did not feel hacking them in before we
    have variable length entries. My output header is printed only for 'trace'
    file, but not 'trace_pipe'. For some reason, despite my quick fix,
    iter->trace is NULL in print_trace_line() when called from 'trace_pipe'
    file, which means I don't get proper output formatting.
    
    I only tried by loading nouveau.ko, which just detects the card, and that
    is traced fine. I didn't try further. Map, two reads and unmap. Works
    perfectly.
    
    I am missing the information about overflows, I'd prefer to have a
    counter for lost events. I didn't try, but I guess currently there is no
    way of knowning when it overflows?
    
    So, not too far from being fully operational, it seems :-)
    And looking at the diffstat, there also is some 700-900 lines of user space
    code that just became obsolete.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c460e85e94ed..0ef9ef74c806 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -5,6 +5,7 @@
 #include <asm/atomic.h>
 #include <linux/sched.h>
 #include <linux/clocksource.h>
+#include <linux/mmiotrace.h>
 
 enum trace_type {
 	__TRACE_FIRST_TYPE = 0,
@@ -14,6 +15,8 @@ enum trace_type {
 	TRACE_WAKE,
 	TRACE_STACK,
 	TRACE_SPECIAL,
+	TRACE_MMIO_RW,
+	TRACE_MMIO_MAP,
 
 	__TRACE_LAST_TYPE
 };
@@ -75,6 +78,8 @@ struct trace_entry {
 		struct ctx_switch_entry		ctx;
 		struct special_entry		special;
 		struct stack_entry		stack;
+		struct mmiotrace_rw		mmiorw;
+		struct mmiotrace_map		mmiomap;
 	};
 };
 
@@ -255,6 +260,15 @@ extern unsigned long ftrace_update_tot_cnt;
 extern int DYN_FTRACE_TEST_NAME(void);
 #endif
 
+#ifdef CONFIG_MMIOTRACE
+extern void __trace_mmiotrace_rw(struct trace_array *tr,
+				struct trace_array_cpu *data,
+				struct mmiotrace_rw *rw);
+extern void __trace_mmiotrace_map(struct trace_array *tr,
+				struct trace_array_cpu *data,
+				struct mmiotrace_map *map);
+#endif
+
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 #ifdef CONFIG_FTRACE
 extern int trace_selftest_startup_function(struct tracer *trace,

commit d618b3e6e50970a6248ac857653fdd49bcd3c045
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:49 2008 +0200

    ftrace: sysprof updates
    
    make the sample period configurable.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b2198bc830ae..b7f85d9c80d7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -188,6 +188,8 @@ struct trace_iterator {
 void tracing_reset(struct trace_array_cpu *data);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *tracing_init_dentry(void);
+void init_tracer_sysprof_debugfs(struct dentry *d_tracer);
+
 void ftrace(struct trace_array *tr,
 			    struct trace_array_cpu *data,
 			    unsigned long ip,

commit a6dd24f8d00cbccb560b19a723e6fb9bdfb20799
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:47 2008 +0200

    ftrace: sysprof-plugin, add self-tests
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c460e85e94ed..b2198bc830ae 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -280,6 +280,10 @@ extern int trace_selftest_startup_wakeup(struct tracer *trace,
 extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 					       struct trace_array *tr);
 #endif
+#ifdef CONFIG_SYSPROF_TRACER
+extern int trace_selftest_startup_sysprof(struct tracer *trace,
+					       struct trace_array *tr);
+#endif
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);

commit 74f4e369fc5b52433ad824cef32d3bf1304549be
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:21:15 2008 +0200

    ftrace: stacktrace fix
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f5de0601b408..c460e85e94ed 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -51,7 +51,7 @@ struct special_entry {
  * Stack-trace entry:
  */
 
-#define FTRACE_STACK_ENTRIES	5
+#define FTRACE_STACK_ENTRIES	8
 
 struct stack_entry {
 	unsigned long		caller[FTRACE_STACK_ENTRIES];

commit 5b82a1b08a00b2adca3d9dd9777efff40b7aaaa1
Author: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
Date:   Mon May 12 21:21:10 2008 +0200

    Port ftrace to markers
    
    Porting ftrace to the marker infrastructure.
    
    Don't need to chain to the wakeup tracer from the sched tracer, because markers
    support multiple probes connected.
    
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    CC: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8845033ab49d..f5de0601b408 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -234,25 +234,10 @@ void update_max_tr_single(struct trace_array *tr,
 
 extern cycle_t ftrace_now(int cpu);
 
-#ifdef CONFIG_SCHED_TRACER
-extern void
-wakeup_sched_switch(struct task_struct *prev, struct task_struct *next);
-extern void
-wakeup_sched_wakeup(struct task_struct *wakee, struct task_struct *curr);
-#else
-static inline void
-wakeup_sched_switch(struct task_struct *prev, struct task_struct *next)
-{
-}
-static inline void
-wakeup_sched_wakeup(struct task_struct *wakee, struct task_struct *curr)
-{
-}
-#endif
-
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER
 typedef void
 (*tracer_switch_func_t)(void *private,
+			void *__rq,
 			struct task_struct *prev,
 			struct task_struct *next);
 
@@ -262,9 +247,6 @@ struct tracer_switch_ops {
 	struct tracer_switch_ops	*next;
 };
 
-extern int register_tracer_switch(struct tracer_switch_ops *ops);
-extern int unregister_tracer_switch(struct tracer_switch_ops *ops);
-
 #endif /* CONFIG_CONTEXT_SWITCH_TRACER */
 
 #ifdef CONFIG_DYNAMIC_FTRACE

commit 6c6c27969a4c6024e6c8838829546c02aaddca18
Author: Pekka Paalanen <pq@iki.fi>
Date:   Mon May 12 21:21:02 2008 +0200

    ftrace: add readpos to struct trace_seq; add trace_seq_to_user()
    
    Refactor code from tracing_read_pipe() and create trace_seq_to_user().
    Moved trace_seq_reset() call before iter->trace->read() call so that
    when all leftover data is returned, trace_seq is reset automatically.
    
    Signed-off-by: Pekka Paalanen <pq@iki.fi>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index ee53d706066f..8845033ab49d 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -155,6 +155,7 @@ struct tracer {
 struct trace_seq {
 	unsigned char		buffer[PAGE_SIZE];
 	unsigned int		len;
+	unsigned int		readpos;
 };
 
 /*
@@ -301,6 +302,8 @@ extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 
 extern void *head_page(struct trace_array_cpu *data);
 extern int trace_seq_printf(struct trace_seq *s, const char *fmt, ...);
+extern ssize_t trace_seq_to_user(struct trace_seq *s, char __user *ubuf,
+				 size_t cnt);
 extern long ns2usecs(cycle_t nsec);
 
 extern unsigned long trace_flags;

commit 107bad8bef5ab2c3a3bff7648c18c9dc3abdc13b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:21:01 2008 +0200

    ftrace: add trace pipe header pluggin
    
    This patch adds a method for open_pipe and open_read to the pluggins
    so that they can add a header to the trace pipe call.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index c1ec134ac356..ee53d706066f 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -135,9 +135,13 @@ struct tracer {
 	void			(*init)(struct trace_array *tr);
 	void			(*reset)(struct trace_array *tr);
 	void			(*open)(struct trace_iterator *iter);
+	void			(*pipe_open)(struct trace_iterator *iter);
 	void			(*close)(struct trace_iterator *iter);
 	void			(*start)(struct trace_iterator *iter);
 	void			(*stop)(struct trace_iterator *iter);
+	ssize_t			(*read)(struct trace_iterator *iter,
+					struct file *filp, char __user *ubuf,
+					size_t cnt, loff_t *ppos);
 	void			(*ctrl_update)(struct trace_array *tr);
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 	int			(*selftest)(struct tracer *trace,
@@ -160,6 +164,7 @@ struct trace_seq {
 struct trace_iterator {
 	struct trace_array	*tr;
 	struct tracer		*trace;
+	void			*private;
 	long			last_overrun[NR_CPUS];
 	long			overrun[NR_CPUS];
 

commit 53d0aa773053ab18287781e25d52c5faf9e0e09e
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:21:01 2008 +0200

    ftrace: add logic to record overruns
    
    This patch sets up the infrastructure to record overruns of the tracing
    buffer.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 8991c5efcc74..c1ec134ac356 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -97,6 +97,7 @@ struct trace_array_cpu {
 	void			*trace_head; /* producer */
 	void			*trace_tail; /* consumer */
 	unsigned long		trace_idx;
+	unsigned long		overrun;
 	unsigned long		saved_latency;
 	unsigned long		critical_start;
 	unsigned long		critical_end;
@@ -157,10 +158,13 @@ struct trace_seq {
  * results to users and which routines might sleep, etc:
  */
 struct trace_iterator {
-	struct trace_seq	seq;
 	struct trace_array	*tr;
 	struct tracer		*trace;
+	long			last_overrun[NR_CPUS];
+	long			overrun[NR_CPUS];
 
+	/* The below is zeroed out in pipe_read */
+	struct trace_seq	seq;
 	struct trace_entry	*ent;
 	int			cpu;
 

commit 25b0b44a1c732ccfc58095cdd8438955a0a19fff
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:21:00 2008 +0200

    ftrace: fix comm on function trace output
    
    In cleaning up of the sched_switch code, the function trace recording
    of task comms was removed. This patch adds back the recording of comms
    for function trace. The output of ftrace now has the task comm instead
    of <...>.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 21c29ee13e53..8991c5efcc74 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -216,6 +216,8 @@ extern unsigned long nsecs_to_usecs(unsigned long nsecs);
 extern unsigned long tracing_max_latency;
 extern unsigned long tracing_thresh;
 
+extern atomic_t trace_record_cmdline_enabled;
+
 void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);

commit 4fcdae83cebda24b519a89d3dd976081fff1ca80
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:21:00 2008 +0200

    ftrace: comment code
    
    This is first installment of adding documentation to the ftrace.
    Expect many more patches of this kind in the near future.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b0ca7473671b..21c29ee13e53 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -294,6 +294,13 @@ extern long ns2usecs(cycle_t nsec);
 
 extern unsigned long trace_flags;
 
+/*
+ * trace_iterator_flags is an enumeration that defines bit
+ * positions into trace_flags that controls the output.
+ *
+ * NOTE: These bits must match the trace_options array in
+ *       trace.c.
+ */
 enum trace_iterator_flags {
 	TRACE_ITER_PRINT_PARENT		= 0x01,
 	TRACE_ITER_SYM_OFFSET		= 0x02,

commit 72829bc3d63cdc592d8f7dd86ad3b3fe8900fb74
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri May 23 21:37:28 2008 +0200

    ftrace: move enums to ftrace.h and make helper function global
    
    picked from the mmiotracer patches to distangle the patch queues.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 25cba28eb9ba..b0ca7473671b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -6,6 +6,18 @@
 #include <linux/sched.h>
 #include <linux/clocksource.h>
 
+enum trace_type {
+	__TRACE_FIRST_TYPE = 0,
+
+	TRACE_FN,
+	TRACE_CTX,
+	TRACE_WAKE,
+	TRACE_STACK,
+	TRACE_SPECIAL,
+
+	__TRACE_LAST_TYPE
+};
+
 /*
  * Function trace entry - function address and parent function addres:
  */
@@ -130,6 +142,7 @@ struct tracer {
 	int			(*selftest)(struct tracer *trace,
 					    struct trace_array *tr);
 #endif
+	int			(*print_line)(struct trace_iterator *iter);
 	struct tracer		*next;
 	int			print_max;
 };
@@ -276,6 +289,8 @@ extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
 extern void *head_page(struct trace_array_cpu *data);
+extern int trace_seq_printf(struct trace_seq *s, const char *fmt, ...);
+extern long ns2usecs(cycle_t nsec);
 
 extern unsigned long trace_flags;
 

commit 92205c2343527a863d660360599a4bf8cede77b0
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:20:55 2008 +0200

    ftrace: user raw_spin_lock in tracing
    
    Lock debugging enabled cause huge performance problems for tracing. Having
    the lock verification happening for every function that is called
    because mcount calls spin_lock can cripple the system.
    
    This patch converts the spin_locks used by ftrace into raw_spin_locks.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 98cbfd05d754..25cba28eb9ba 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -76,7 +76,7 @@ struct trace_entry {
 struct trace_array_cpu {
 	struct list_head	trace_pages;
 	atomic_t		disabled;
-	spinlock_t		lock;
+	raw_spinlock_t		lock;
 	struct lock_class_key	lock_key;
 
 	/* these fields get copied into max-trace: */

commit d05cdb25d80f06f77aa6bddb53cd1390d4d91a0b
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Mon May 12 21:20:54 2008 +0200

    ftrace: fix dynamic ftrace selftest
    
    With the adding of the configuration changes in the Makefile to prevent
    tracing of functions in the ftrace code, all tracing of all the ftrace
    code has been removed. Unfortunately, one of the selftests, relied on
    a function to be traced. With the new change, the function was no longer
    traced and the test failed.
    
    This patch separates out the test function into its own file so that
    we can add the "-pg" flag to the compilation of that function and the
    adding of the mcount call to that function.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 96951a8d09a4..98cbfd05d754 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -244,6 +244,8 @@ extern int unregister_tracer_switch(struct tracer_switch_ops *ops);
 
 #ifdef CONFIG_DYNAMIC_FTRACE
 extern unsigned long ftrace_update_tot_cnt;
+#define DYN_FTRACE_TEST_NAME trace_selftest_dynamic_test_func
+extern int DYN_FTRACE_TEST_NAME(void);
 #endif
 
 #ifdef CONFIG_FTRACE_STARTUP_TEST

commit bac524d3f3dfeffa3a9d44f2c64035b88bcaacb4
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Mon May 12 21:20:53 2008 +0200

    ftrace: trace next state
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index a52015702a28..96951a8d09a4 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -23,6 +23,7 @@ struct ctx_switch_entry {
 	unsigned char		prev_state;
 	unsigned int		next_pid;
 	unsigned char		next_prio;
+	unsigned char		next_state;
 };
 
 /*

commit 4ac3ba41d372e3a9e420b36bc43589662b188a14
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:52 2008 +0200

    ftrace: trace scheduler rbtree
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 75e237475674..a52015702a28 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -286,6 +286,7 @@ enum trace_iterator_flags {
 	TRACE_ITER_BIN			= 0x40,
 	TRACE_ITER_BLOCK		= 0x80,
 	TRACE_ITER_STACKTRACE		= 0x100,
+	TRACE_ITER_SCHED_TREE		= 0x200,
 };
 
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 4e65551905fb0300ae7e667cbaa41ee2e3f29a13
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:52 2008 +0200

    ftrace: sched tracer, trace full rbtree
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 387bdcf45e28..75e237475674 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -274,4 +274,18 @@ extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 
 extern void *head_page(struct trace_array_cpu *data);
 
+extern unsigned long trace_flags;
+
+enum trace_iterator_flags {
+	TRACE_ITER_PRINT_PARENT		= 0x01,
+	TRACE_ITER_SYM_OFFSET		= 0x02,
+	TRACE_ITER_SYM_ADDR		= 0x04,
+	TRACE_ITER_VERBOSE		= 0x08,
+	TRACE_ITER_RAW			= 0x10,
+	TRACE_ITER_HEX			= 0x20,
+	TRACE_ITER_BIN			= 0x40,
+	TRACE_ITER_BLOCK		= 0x80,
+	TRACE_ITER_STACKTRACE		= 0x100,
+};
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 86387f7ee5d3273ff4859e2c64ce656639b6ca65
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:51 2008 +0200

    ftrace: add stack tracing
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 90e0ba0f6eba..387bdcf45e28 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -34,6 +34,16 @@ struct special_entry {
 	unsigned long		arg3;
 };
 
+/*
+ * Stack-trace entry:
+ */
+
+#define FTRACE_STACK_ENTRIES	5
+
+struct stack_entry {
+	unsigned long		caller[FTRACE_STACK_ENTRIES];
+};
+
 /*
  * The trace entry - the most basic unit of tracing. This is what
  * is printed in the end as a single line in the trace output, such as:
@@ -51,6 +61,7 @@ struct trace_entry {
 		struct ftrace_entry		fn;
 		struct ctx_switch_entry		ctx;
 		struct special_entry		special;
+		struct stack_entry		stack;
 	};
 };
 

commit 57422797dc009fc83766bcf230d29dbe6e08e21e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:51 2008 +0200

    ftrace: add wakeup events to sched tracer
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 2b7352bf1ce6..90e0ba0f6eba 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -164,6 +164,12 @@ void tracing_sched_switch_trace(struct trace_array *tr,
 				struct task_struct *next,
 				unsigned long flags);
 void tracing_record_cmdline(struct task_struct *tsk);
+
+void tracing_sched_wakeup_trace(struct trace_array *tr,
+				struct trace_array_cpu *data,
+				struct task_struct *wakee,
+				struct task_struct *cur,
+				unsigned long flags);
 void trace_special(struct trace_array *tr,
 		   struct trace_array_cpu *data,
 		   unsigned long arg1,
@@ -194,11 +200,17 @@ extern cycle_t ftrace_now(int cpu);
 #ifdef CONFIG_SCHED_TRACER
 extern void
 wakeup_sched_switch(struct task_struct *prev, struct task_struct *next);
+extern void
+wakeup_sched_wakeup(struct task_struct *wakee, struct task_struct *curr);
 #else
 static inline void
 wakeup_sched_switch(struct task_struct *prev, struct task_struct *next)
 {
 }
+static inline void
+wakeup_sched_wakeup(struct task_struct *wakee, struct task_struct *curr)
+{
+}
 #endif
 
 #ifdef CONFIG_CONTEXT_SWITCH_TRACER

commit e309b41dd65aa953f86765eeeecc941d8e1e8b8f
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:51 2008 +0200

    ftrace: remove notrace
    
    now that we have a kbuild method for notrace, no need to pollute the
    C code with the annotations.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index faf9f67246ac..2b7352bf1ce6 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -150,7 +150,7 @@ struct trace_iterator {
 	long			idx;
 };
 
-void notrace tracing_reset(struct trace_array_cpu *data);
+void tracing_reset(struct trace_array_cpu *data);
 int tracing_open_generic(struct inode *inode, struct file *filp);
 struct dentry *tracing_init_dentry(void);
 void ftrace(struct trace_array *tr,
@@ -189,10 +189,10 @@ void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
 
-extern notrace cycle_t ftrace_now(int cpu);
+extern cycle_t ftrace_now(int cpu);
 
 #ifdef CONFIG_SCHED_TRACER
-extern void notrace
+extern void
 wakeup_sched_switch(struct task_struct *prev, struct task_struct *next);
 #else
 static inline void

commit 6fb44b717c10ecf37beaaebd312f3afa93fed714
Author: Steven Rostedt <srostedt@srostedt@redhat.com>
Date:   Mon May 12 21:20:49 2008 +0200

    ftrace: add trace_function api for other tracers to use
    
    A new check was added in the ftrace function that wont trace if the CPU
    trace buffer is disabled.  Unfortunately, other tracers used ftrace() to
    write to the buffer after they disabled it. The new disable check makes
    these calls into a nop.
    
    This patch changes the __ftrace that is called without the check into a
    new api for the other tracers to use, called "trace_function". The other
    tracers use this interface instead when the trace CPU buffer is already
    disabled.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 7bdfef35c05a..faf9f67246ac 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -169,6 +169,11 @@ void trace_special(struct trace_array *tr,
 		   unsigned long arg1,
 		   unsigned long arg2,
 		   unsigned long arg3);
+void trace_function(struct trace_array *tr,
+		    struct trace_array_cpu *data,
+		    unsigned long ip,
+		    unsigned long parent_ip,
+		    unsigned long flags);
 
 void tracing_start_function_trace(void);
 void tracing_stop_function_trace(void);

commit f0a920d5752e1788c0cba2add103076bcc0f7a49
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:47 2008 +0200

    ftrace: add trace_special()
    
    for ad-hoc tracing.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 27fa2d06f499..7bdfef35c05a 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -25,6 +25,15 @@ struct ctx_switch_entry {
 	unsigned char		next_prio;
 };
 
+/*
+ * Special (free-form) trace entry:
+ */
+struct special_entry {
+	unsigned long		arg1;
+	unsigned long		arg2;
+	unsigned long		arg3;
+};
+
 /*
  * The trace entry - the most basic unit of tracing. This is what
  * is printed in the end as a single line in the trace output, such as:
@@ -41,6 +50,7 @@ struct trace_entry {
 	union {
 		struct ftrace_entry		fn;
 		struct ctx_switch_entry		ctx;
+		struct special_entry		special;
 	};
 };
 
@@ -154,6 +164,11 @@ void tracing_sched_switch_trace(struct trace_array *tr,
 				struct task_struct *next,
 				unsigned long flags);
 void tracing_record_cmdline(struct task_struct *tsk);
+void trace_special(struct trace_array *tr,
+		   struct trace_array_cpu *data,
+		   unsigned long arg1,
+		   unsigned long arg2,
+		   unsigned long arg3);
 
 void tracing_start_function_trace(void);
 void tracing_stop_function_trace(void);

commit cdd31cd2d7a0dcbec2cce3974f7129dd4fc8c879
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:46 2008 +0200

    ftrace: remove-idx-sync
    
    remove idx syncing - it's expensive on SMP.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 30cad677e9d0..27fa2d06f499 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -38,7 +38,6 @@ struct trace_entry {
 	char			preempt_count;
 	int			pid;
 	cycle_t			t;
-	unsigned long		idx;
 	union {
 		struct ftrace_entry		fn;
 		struct ctx_switch_entry		ctx;
@@ -57,7 +56,6 @@ struct trace_array_cpu {
 	atomic_t		disabled;
 	spinlock_t		lock;
 	struct lock_class_key	lock_key;
-	cycle_t			time_offset;
 
 	/* these fields get copied into max-trace: */
 	unsigned		trace_head_idx;

commit 750ed1a40783432d0dcb0e6c2e813a12615d7664
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:46 2008 +0200

    ftrace: timestamp syncing, prepare
    
    rename and uninline now() to ftrace_now().
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index b0408356f0e0..30cad677e9d0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -171,10 +171,7 @@ void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
 void update_max_tr_single(struct trace_array *tr,
 			  struct task_struct *tsk, int cpu);
 
-static inline notrace cycle_t now(int cpu)
-{
-	return cpu_clock(cpu);
-}
+extern notrace cycle_t ftrace_now(int cpu);
 
 #ifdef CONFIG_SCHED_TRACER
 extern void notrace

commit d4c5a2f5870939d837293de87b41dda0012a4572
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:46 2008 +0200

    ftrace: fix locking
    
    we can hold all cpu trace buffer locks at once - put each into a
    separate lock class.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 29a7ea59de50..b0408356f0e0 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -56,6 +56,7 @@ struct trace_array_cpu {
 	struct list_head	trace_pages;
 	atomic_t		disabled;
 	spinlock_t		lock;
+	struct lock_class_key	lock_key;
 	cycle_t			time_offset;
 
 	/* these fields get copied into max-trace: */

commit b3806b4316306dc9c542eff6c23d7d42918f3504
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:46 2008 +0200

    ftrace: user run time file reading
    
    This patch creates a file called trace_pipe in the tracing
    debug directory. This file is a consumer of the trace buffers.
    This means that reads of this file consumes the entries from
    the trace buffers so that they will not be read a second time,
    as contrast to the static buffers latency_trace and trace.
    
    Reading from the trace_pipe will remove the entries from trace
    and latency_trace too.
    
    The advantage that trace_pipe has is that it can record live
    traces. It will block when there is nothing in the buffer,
    and read the entries as they are entered.  An EOF happens when
    tracing is disabled (tracing_enabled = 0).
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index f5b32ca0b457..29a7ea59de50 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -55,6 +55,7 @@ struct trace_entry {
 struct trace_array_cpu {
 	struct list_head	trace_pages;
 	atomic_t		disabled;
+	spinlock_t		lock;
 	cycle_t			time_offset;
 
 	/* these fields get copied into max-trace: */
@@ -88,6 +89,7 @@ struct trace_array {
 	long			ctrl;
 	int			cpu;
 	cycle_t			time_start;
+	struct task_struct	*waiter;
 	struct trace_array_cpu	*data[NR_CPUS];
 };
 

commit 214023c3d13a71525e463b5e54e360b926b4dc90
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:46 2008 +0200

    ftrace: add a buffer for output
    
    Later patches will need to print the same things as the seq output
    does. But those outputs will not use the seq utility. This patch
    adds a buffer to the iterator, that can be used by either the
    seq utility or other output.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 0ce127455b4b..f5b32ca0b457 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -111,11 +111,17 @@ struct tracer {
 	int			print_max;
 };
 
+struct trace_seq {
+	unsigned char		buffer[PAGE_SIZE];
+	unsigned int		len;
+};
+
 /*
  * Trace iterator - used by printout routines who present trace
  * results to users and which routines might sleep, etc:
  */
 struct trace_iterator {
+	struct trace_seq	seq;
 	struct trace_array	*tr;
 	struct tracer		*trace;
 

commit 93a588f459da134be6ab17c4104e28441beb0d22
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:45 2008 +0200

    ftrace: change buffers to producer consumer
    
    This patch changes the way the CPU trace buffers are handled.
    Instead of always starting from the trace page head, the logic
    is changed to a producer consumer logic. This allows for the
    buffers to be drained while they are alive.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 5df8ff2b84a7..0ce127455b4b 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -53,13 +53,15 @@ struct trace_entry {
  * the trace, etc.)
  */
 struct trace_array_cpu {
-	void			*trace_current;
 	struct list_head	trace_pages;
 	atomic_t		disabled;
 	cycle_t			time_offset;
 
 	/* these fields get copied into max-trace: */
-	unsigned		trace_current_idx;
+	unsigned		trace_head_idx;
+	unsigned		trace_tail_idx;
+	void			*trace_head; /* producer */
+	void			*trace_tail; /* consumer */
 	unsigned long		trace_idx;
 	unsigned long		saved_latency;
 	unsigned long		critical_start;

commit 4e3c3333f3bd7eedfd21b1155b3c7cd24fc7f754
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:45 2008 +0200

    ftrace: fix time offset
    
    fix time offset calculations and ordering, plus make code more consistent.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index cc1d34b8b771..5df8ff2b84a7 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -56,6 +56,8 @@ struct trace_array_cpu {
 	void			*trace_current;
 	struct list_head	trace_pages;
 	atomic_t		disabled;
+	cycle_t			time_offset;
+
 	/* these fields get copied into max-trace: */
 	unsigned		trace_current_idx;
 	unsigned long		trace_idx;
@@ -114,14 +116,19 @@ struct tracer {
 struct trace_iterator {
 	struct trace_array	*tr;
 	struct tracer		*trace;
+
 	struct trace_entry	*ent;
+	int			cpu;
+
+	struct trace_entry	*prev_ent;
+	int			prev_cpu;
+
 	unsigned long		iter_flags;
 	loff_t			pos;
 	unsigned long		next_idx[NR_CPUS];
 	struct list_head	*next_page[NR_CPUS];
 	unsigned		next_page_idx[NR_CPUS];
 	long			idx;
-	int			cpu;
 };
 
 void notrace tracing_reset(struct trace_array_cpu *data);

commit c7aafc549766b87819285d3480648fc652a47bc4
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon May 12 21:20:45 2008 +0200

    ftrace: cleanups
    
    factor out code and clean it up.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 88edbf1f6788..cc1d34b8b771 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -53,12 +53,12 @@ struct trace_entry {
  * the trace, etc.)
  */
 struct trace_array_cpu {
-	void			*trace;
 	void			*trace_current;
-	unsigned		trace_current_idx;
 	struct list_head	trace_pages;
-	unsigned long		trace_idx;
 	atomic_t		disabled;
+	/* these fields get copied into max-trace: */
+	unsigned		trace_current_idx;
+	unsigned long		trace_idx;
 	unsigned long		saved_latency;
 	unsigned long		critical_start;
 	unsigned long		critical_end;
@@ -216,4 +216,6 @@ extern int trace_selftest_startup_sched_switch(struct tracer *trace,
 #endif
 #endif /* CONFIG_FTRACE_STARTUP_TEST */
 
+extern void *head_page(struct trace_array_cpu *data);
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 60a11774b38fef1ab90b18c5353bd1c7c4d311c8
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:44 2008 +0200

    ftrace: add self-tests
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 83e257e38084..88edbf1f6788 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -99,6 +99,10 @@ struct tracer {
 	void			(*start)(struct trace_iterator *iter);
 	void			(*stop)(struct trace_iterator *iter);
 	void			(*ctrl_update)(struct trace_array *tr);
+#ifdef CONFIG_FTRACE_STARTUP_TEST
+	int			(*selftest)(struct tracer *trace,
+					    struct trace_array *tr);
+#endif
 	struct tracer		*next;
 	int			print_max;
 };
@@ -185,4 +189,31 @@ extern int unregister_tracer_switch(struct tracer_switch_ops *ops);
 extern unsigned long ftrace_update_tot_cnt;
 #endif
 
+#ifdef CONFIG_FTRACE_STARTUP_TEST
+#ifdef CONFIG_FTRACE
+extern int trace_selftest_startup_function(struct tracer *trace,
+					   struct trace_array *tr);
+#endif
+#ifdef CONFIG_IRQSOFF_TRACER
+extern int trace_selftest_startup_irqsoff(struct tracer *trace,
+					  struct trace_array *tr);
+#endif
+#ifdef CONFIG_PREEMPT_TRACER
+extern int trace_selftest_startup_preemptoff(struct tracer *trace,
+					     struct trace_array *tr);
+#endif
+#if defined(CONFIG_IRQSOFF_TRACER) && defined(CONFIG_PREEMPT_TRACER)
+extern int trace_selftest_startup_preemptirqsoff(struct tracer *trace,
+						 struct trace_array *tr);
+#endif
+#ifdef CONFIG_SCHED_TRACER
+extern int trace_selftest_startup_wakeup(struct tracer *trace,
+					 struct trace_array *tr);
+#endif
+#ifdef CONFIG_CONTEXT_SWITCH_TRACER
+extern int trace_selftest_startup_sched_switch(struct tracer *trace,
+					       struct trace_array *tr);
+#endif
+#endif /* CONFIG_FTRACE_STARTUP_TEST */
+
 #endif /* _LINUX_KERNEL_TRACE_H */

commit 4c11d7aed389375253b59e2b1865eec96663c65d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:43 2008 +0200

    ftrace: convert single large buffer into single pages.
    
    Allocating large buffers for the tracer may fail easily.
    This patch converts the buffer from a large ordered allocation
    to single pages. It uses the struct page LRU field to link the
    pages together.
    
    Later patches may also implement dynamic increasing and decreasing
    of the trace buffers.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
index 3173a93561d4..83e257e38084 100644
--- a/kernel/trace/trace.h
+++ b/kernel/trace/trace.h
@@ -54,9 +54,11 @@ struct trace_entry {
  */
 struct trace_array_cpu {
 	void			*trace;
+	void			*trace_current;
+	unsigned		trace_current_idx;
+	struct list_head	trace_pages;
 	unsigned long		trace_idx;
 	atomic_t		disabled;
-	atomic_t		underrun;
 	unsigned long		saved_latency;
 	unsigned long		critical_start;
 	unsigned long		critical_end;
@@ -112,8 +114,10 @@ struct trace_iterator {
 	unsigned long		iter_flags;
 	loff_t			pos;
 	unsigned long		next_idx[NR_CPUS];
+	struct list_head	*next_page[NR_CPUS];
+	unsigned		next_page_idx[NR_CPUS];
+	long			idx;
 	int			cpu;
-	int			idx;
 };
 
 void notrace tracing_reset(struct trace_array_cpu *data);

commit bc0c38d139ec7fcd5c030aea16b008f3732e42ac
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 12 21:20:42 2008 +0200

    ftrace: latency tracer infrastructure
    
    This patch adds the latency tracer infrastructure. This patch
    does not add anything that will select and turn it on, but will
    be used by later patches.
    
    If it were to be compiled, it would add the following files
    to the debugfs:
    
     The root tracing directory:
    
      /debugfs/tracing/
    
    This patch also adds the following files:
    
      available_tracers
         list of available tracers. Currently no tracers are
         available. Looking into this file only shows
         "none" which is used to unregister all tracers.
    
      current_tracer
         The trace that is currently active. Empty on start up.
         To switch to a tracer simply echo one of the tracers that
         are listed in available_tracers:
    
       example: (used with later patches)
    
          echo function > /debugfs/tracing/current_tracer
    
         To disable the tracer:
    
           echo disable > /debugfs/tracing/current_tracer
    
      tracing_enabled
         echoing "1" into this file starts the ftrace function tracing
          (if sysctl kernel.ftrace_enabled=1)
         echoing "0" turns it off.
    
      latency_trace
          This file is readonly and holds the result of the trace.
    
      trace
          This file outputs a easier to read version of the trace.
    
      iter_ctrl
          Controls the way the output of traces look.
          So far there's two controls:
            echoing in "symonly" will only show the kallsyms variables
                without the addresses (if kallsyms was configured)
            echoing in "verbose" will change the output to show
                a lot more data, but not very easy to understand by
                humans.
            echoing in "nosymonly" turns off symonly.
            echoing in "noverbose" turns off verbose.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/trace/trace.h b/kernel/trace/trace.h
new file mode 100644
index 000000000000..3173a93561d4
--- /dev/null
+++ b/kernel/trace/trace.h
@@ -0,0 +1,184 @@
+#ifndef _LINUX_KERNEL_TRACE_H
+#define _LINUX_KERNEL_TRACE_H
+
+#include <linux/fs.h>
+#include <asm/atomic.h>
+#include <linux/sched.h>
+#include <linux/clocksource.h>
+
+/*
+ * Function trace entry - function address and parent function addres:
+ */
+struct ftrace_entry {
+	unsigned long		ip;
+	unsigned long		parent_ip;
+};
+
+/*
+ * Context switch trace entry - which task (and prio) we switched from/to:
+ */
+struct ctx_switch_entry {
+	unsigned int		prev_pid;
+	unsigned char		prev_prio;
+	unsigned char		prev_state;
+	unsigned int		next_pid;
+	unsigned char		next_prio;
+};
+
+/*
+ * The trace entry - the most basic unit of tracing. This is what
+ * is printed in the end as a single line in the trace output, such as:
+ *
+ *     bash-15816 [01]   235.197585: idle_cpu <- irq_enter
+ */
+struct trace_entry {
+	char			type;
+	char			cpu;
+	char			flags;
+	char			preempt_count;
+	int			pid;
+	cycle_t			t;
+	unsigned long		idx;
+	union {
+		struct ftrace_entry		fn;
+		struct ctx_switch_entry		ctx;
+	};
+};
+
+#define TRACE_ENTRY_SIZE	sizeof(struct trace_entry)
+
+/*
+ * The CPU trace array - it consists of thousands of trace entries
+ * plus some other descriptor data: (for example which task started
+ * the trace, etc.)
+ */
+struct trace_array_cpu {
+	void			*trace;
+	unsigned long		trace_idx;
+	atomic_t		disabled;
+	atomic_t		underrun;
+	unsigned long		saved_latency;
+	unsigned long		critical_start;
+	unsigned long		critical_end;
+	unsigned long		critical_sequence;
+	unsigned long		nice;
+	unsigned long		policy;
+	unsigned long		rt_priority;
+	cycle_t			preempt_timestamp;
+	pid_t			pid;
+	uid_t			uid;
+	char			comm[TASK_COMM_LEN];
+};
+
+struct trace_iterator;
+
+/*
+ * The trace array - an array of per-CPU trace arrays. This is the
+ * highest level data structure that individual tracers deal with.
+ * They have on/off state as well:
+ */
+struct trace_array {
+	unsigned long		entries;
+	long			ctrl;
+	int			cpu;
+	cycle_t			time_start;
+	struct trace_array_cpu	*data[NR_CPUS];
+};
+
+/*
+ * A specific tracer, represented by methods that operate on a trace array:
+ */
+struct tracer {
+	const char		*name;
+	void			(*init)(struct trace_array *tr);
+	void			(*reset)(struct trace_array *tr);
+	void			(*open)(struct trace_iterator *iter);
+	void			(*close)(struct trace_iterator *iter);
+	void			(*start)(struct trace_iterator *iter);
+	void			(*stop)(struct trace_iterator *iter);
+	void			(*ctrl_update)(struct trace_array *tr);
+	struct tracer		*next;
+	int			print_max;
+};
+
+/*
+ * Trace iterator - used by printout routines who present trace
+ * results to users and which routines might sleep, etc:
+ */
+struct trace_iterator {
+	struct trace_array	*tr;
+	struct tracer		*trace;
+	struct trace_entry	*ent;
+	unsigned long		iter_flags;
+	loff_t			pos;
+	unsigned long		next_idx[NR_CPUS];
+	int			cpu;
+	int			idx;
+};
+
+void notrace tracing_reset(struct trace_array_cpu *data);
+int tracing_open_generic(struct inode *inode, struct file *filp);
+struct dentry *tracing_init_dentry(void);
+void ftrace(struct trace_array *tr,
+			    struct trace_array_cpu *data,
+			    unsigned long ip,
+			    unsigned long parent_ip,
+			    unsigned long flags);
+void tracing_sched_switch_trace(struct trace_array *tr,
+				struct trace_array_cpu *data,
+				struct task_struct *prev,
+				struct task_struct *next,
+				unsigned long flags);
+void tracing_record_cmdline(struct task_struct *tsk);
+
+void tracing_start_function_trace(void);
+void tracing_stop_function_trace(void);
+int register_tracer(struct tracer *type);
+void unregister_tracer(struct tracer *type);
+
+extern unsigned long nsecs_to_usecs(unsigned long nsecs);
+
+extern unsigned long tracing_max_latency;
+extern unsigned long tracing_thresh;
+
+void update_max_tr(struct trace_array *tr, struct task_struct *tsk, int cpu);
+void update_max_tr_single(struct trace_array *tr,
+			  struct task_struct *tsk, int cpu);
+
+static inline notrace cycle_t now(int cpu)
+{
+	return cpu_clock(cpu);
+}
+
+#ifdef CONFIG_SCHED_TRACER
+extern void notrace
+wakeup_sched_switch(struct task_struct *prev, struct task_struct *next);
+#else
+static inline void
+wakeup_sched_switch(struct task_struct *prev, struct task_struct *next)
+{
+}
+#endif
+
+#ifdef CONFIG_CONTEXT_SWITCH_TRACER
+typedef void
+(*tracer_switch_func_t)(void *private,
+			struct task_struct *prev,
+			struct task_struct *next);
+
+struct tracer_switch_ops {
+	tracer_switch_func_t		func;
+	void				*private;
+	struct tracer_switch_ops	*next;
+};
+
+extern int register_tracer_switch(struct tracer_switch_ops *ops);
+extern int unregister_tracer_switch(struct tracer_switch_ops *ops);
+
+#endif /* CONFIG_CONTEXT_SWITCH_TRACER */
+
+#ifdef CONFIG_DYNAMIC_FTRACE
+extern unsigned long ftrace_update_tot_cnt;
+#endif
+
+#endif /* _LINUX_KERNEL_TRACE_H */
