commit 65133033ee6ee34724ea3d82d5d1cfc6839ffdae
Merge: 27a0a90d6301 652521d460cb
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Oct 28 12:38:26 2019 +0100

    Merge branch 'perf/urgent' into perf/core, to pick up fixes
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6b1340cc00edeadd52ebd8a45171f38c8de2a387
Author: Prateek Sood <prsood@codeaurora.org>
Date:   Tue Oct 15 11:47:25 2019 +0530

    tracing: Fix race in perf_trace_buf initialization
    
    A race condition exists while initialiazing perf_trace_buf from
    perf_trace_init() and perf_kprobe_init().
    
          CPU0                                        CPU1
    perf_trace_init()
      mutex_lock(&event_mutex)
        perf_trace_event_init()
          perf_trace_event_reg()
            total_ref_count == 0
            buf = alloc_percpu()
            perf_trace_buf[i] = buf
            tp_event->class->reg() //fails       perf_kprobe_init()
            goto fail                              perf_trace_event_init()
                                                     perf_trace_event_reg()
            fail:
              total_ref_count == 0
    
                                                       total_ref_count == 0
                                                       buf = alloc_percpu()
                                                       perf_trace_buf[i] = buf
                                                       tp_event->class->reg()
                                                       total_ref_count++
    
              free_percpu(perf_trace_buf[i])
              perf_trace_buf[i] = NULL
    
    Any subsequent call to perf_trace_event_reg() will observe total_ref_count > 0,
    causing the perf_trace_buf to be always NULL. This can result in perf_trace_buf
    getting accessed from perf_trace_buf_alloc() without being initialized. Acquiring
    event_mutex in perf_kprobe_init() before calling perf_trace_event_init() should
    fix this race.
    
    The race caused the following bug:
    
     Unable to handle kernel paging request at virtual address 0000003106f2003c
     Mem abort info:
       ESR = 0x96000045
       Exception class = DABT (current EL), IL = 32 bits
       SET = 0, FnV = 0
       EA = 0, S1PTW = 0
     Data abort info:
       ISV = 0, ISS = 0x00000045
       CM = 0, WnR = 1
     user pgtable: 4k pages, 39-bit VAs, pgdp = ffffffc034b9b000
     [0000003106f2003c] pgd=0000000000000000, pud=0000000000000000
     Internal error: Oops: 96000045 [#1] PREEMPT SMP
     Process syz-executor (pid: 18393, stack limit = 0xffffffc093190000)
     pstate: 80400005 (Nzcv daif +PAN -UAO)
     pc : __memset+0x20/0x1ac
     lr : memset+0x3c/0x50
     sp : ffffffc09319fc50
    
      __memset+0x20/0x1ac
      perf_trace_buf_alloc+0x140/0x1a0
      perf_trace_sys_enter+0x158/0x310
      syscall_trace_enter+0x348/0x7c0
      el0_svc_common+0x11c/0x368
      el0_svc_handler+0x12c/0x198
      el0_svc+0x8/0xc
    
    Ramdumps showed the following:
      total_ref_count = 3
      perf_trace_buf = (
          0x0 -> NULL,
          0x0 -> NULL,
          0x0 -> NULL,
          0x0 -> NULL)
    
    Link: http://lkml.kernel.org/r/1571120245-4186-1-git-send-email-prsood@codeaurora.org
    
    Cc: stable@vger.kernel.org
    Fixes: e12f03d7031a9 ("perf/core: Implement the 'perf_kprobe' PMU")
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Prateek Sood <prsood@codeaurora.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 0892e38ed6fb..a9dfa04ffa44 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -272,9 +272,11 @@ int perf_kprobe_init(struct perf_event *p_event, bool is_retprobe)
 		goto out;
 	}
 
+	mutex_lock(&event_mutex);
 	ret = perf_trace_event_init(tp_event, p_event);
 	if (ret)
 		destroy_local_trace_kprobe(tp_event);
+	mutex_unlock(&event_mutex);
 out:
 	kfree(func);
 	return ret;
@@ -282,8 +284,10 @@ int perf_kprobe_init(struct perf_event *p_event, bool is_retprobe)
 
 void perf_kprobe_destroy(struct perf_event *p_event)
 {
+	mutex_lock(&event_mutex);
 	perf_trace_event_close(p_event);
 	perf_trace_event_unreg(p_event);
+	mutex_unlock(&event_mutex);
 
 	destroy_local_trace_kprobe(p_event->tp_event);
 }

commit da97e18458fb42d7c00fac5fd1c56a3896ec666e
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Mon Oct 14 13:03:08 2019 -0400

    perf_event: Add support for LSM and SELinux checks
    
    In current mainline, the degree of access to perf_event_open(2) system
    call depends on the perf_event_paranoid sysctl.  This has a number of
    limitations:
    
    1. The sysctl is only a single value. Many types of accesses are controlled
       based on the single value thus making the control very limited and
       coarse grained.
    2. The sysctl is global, so if the sysctl is changed, then that means
       all processes get access to perf_event_open(2) opening the door to
       security issues.
    
    This patch adds LSM and SELinux access checking which will be used in
    Android to access perf_event_open(2) for the purposes of attaching BPF
    programs to tracepoints, perf profiling and other operations from
    userspace. These operations are intended for production systems.
    
    5 new LSM hooks are added:
    1. perf_event_open: This controls access during the perf_event_open(2)
       syscall itself. The hook is called from all the places that the
       perf_event_paranoid sysctl is checked to keep it consistent with the
       systctl. The hook gets passed a 'type' argument which controls CPU,
       kernel and tracepoint accesses (in this context, CPU, kernel and
       tracepoint have the same semantics as the perf_event_paranoid sysctl).
       Additionally, I added an 'open' type which is similar to
       perf_event_paranoid sysctl == 3 patch carried in Android and several other
       distros but was rejected in mainline [1] in 2016.
    
    2. perf_event_alloc: This allocates a new security object for the event
       which stores the current SID within the event. It will be useful when
       the perf event's FD is passed through IPC to another process which may
       try to read the FD. Appropriate security checks will limit access.
    
    3. perf_event_free: Called when the event is closed.
    
    4. perf_event_read: Called from the read(2) and mmap(2) syscalls for the event.
    
    5. perf_event_write: Called from the ioctl(2) syscalls for the event.
    
    [1] https://lwn.net/Articles/696240/
    
    Since Peter had suggest LSM hooks in 2016 [1], I am adding his
    Suggested-by tag below.
    
    To use this patch, we set the perf_event_paranoid sysctl to -1 and then
    apply selinux checking as appropriate (default deny everything, and then
    add policy rules to give access to domains that need it). In the future
    we can remove the perf_event_paranoid sysctl altogether.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Co-developed-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: James Morris <jmorris@namei.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: rostedt@goodmis.org
    Cc: Yonghong Song <yhs@fb.com>
    Cc: Kees Cook <keescook@chromium.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Alexei Starovoitov <ast@kernel.org>
    Cc: jeffv@google.com
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Daniel Borkmann <daniel@iogearbox.net>
    Cc: primiano@google.com
    Cc: Song Liu <songliubraving@fb.com>
    Cc: rsavitski@google.com
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Matthew Garrett <matthewgarrett@google.com>
    Link: https://lkml.kernel.org/r/20191014170308.70668-1-joel@joelfernandes.org

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 0892e38ed6fb..0917fee6ee7c 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -8,6 +8,7 @@
 
 #include <linux/module.h>
 #include <linux/kprobes.h>
+#include <linux/security.h>
 #include "trace.h"
 #include "trace_probe.h"
 
@@ -26,8 +27,10 @@ static int	total_ref_count;
 static int perf_trace_event_perm(struct trace_event_call *tp_event,
 				 struct perf_event *p_event)
 {
+	int ret;
+
 	if (tp_event->perf_perm) {
-		int ret = tp_event->perf_perm(tp_event, p_event);
+		ret = tp_event->perf_perm(tp_event, p_event);
 		if (ret)
 			return ret;
 	}
@@ -46,8 +49,9 @@ static int perf_trace_event_perm(struct trace_event_call *tp_event,
 
 	/* The ftrace function trace is allowed only for root. */
 	if (ftrace_event_is_function(tp_event)) {
-		if (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
-			return -EPERM;
+		ret = perf_allow_tracepoint(&p_event->attr);
+		if (ret)
+			return ret;
 
 		if (!is_sampling_event(p_event))
 			return 0;
@@ -82,8 +86,9 @@ static int perf_trace_event_perm(struct trace_event_call *tp_event,
 	 * ...otherwise raw tracepoint data can be a severe data leak,
 	 * only allow root to have these.
 	 */
-	if (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	ret = perf_allow_tracepoint(&p_event->attr);
+	if (ret)
+		return ret;
 
 	return 0;
 }

commit 46710f3a34b592ac5c51a95f696b2d2a2a0d9419
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Sat May 25 09:57:59 2019 -0700

    tracing: Pass type into tracing_generic_entry_update()
    
    All callers of tracing_generic_entry_update() have to initialize
    entry->type, so let's just simply move it inside.
    Link: http://lkml.kernel.org/r/20190525165802.25944-2-xiyou.wangcong@gmail.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 4629a6104474..0892e38ed6fb 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -416,8 +416,7 @@ void perf_trace_buf_update(void *record, u16 type)
 	unsigned long flags;
 
 	local_save_flags(flags);
-	tracing_generic_entry_update(entry, flags, pc);
-	entry->type = type;
+	tracing_generic_entry_update(entry, type, flags, pc);
 }
 NOKPROBE_SYMBOL(perf_trace_buf_update);
 

commit 83540fbc8812a580b6ad8f93f4c29e62e417687e
Author: Jann Horn <jannh@google.com>
Date:   Wed Feb 20 17:54:43 2019 +0100

    tracing/perf: Use strndup_user() instead of buggy open-coded version
    
    The first version of this method was missing the check for
    `ret == PATH_MAX`; then such a check was added, but it didn't call kfree()
    on error, so there was still a small memory leak in the error case.
    Fix it by using strndup_user() instead of open-coding it.
    
    Link: http://lkml.kernel.org/r/20190220165443.152385-1-jannh@google.com
    
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: stable@vger.kernel.org
    Fixes: 0eadcc7a7bc0 ("perf/core: Fix perf_uprobe_init()")
    Reviewed-by: Masami Hiramatsu <mhiramat@kernel.org>
    Acked-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Jann Horn <jannh@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 76217bbef815..4629a6104474 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -299,15 +299,13 @@ int perf_uprobe_init(struct perf_event *p_event,
 
 	if (!p_event->attr.uprobe_path)
 		return -EINVAL;
-	path = kzalloc(PATH_MAX, GFP_KERNEL);
-	if (!path)
-		return -ENOMEM;
-	ret = strncpy_from_user(
-		path, u64_to_user_ptr(p_event->attr.uprobe_path), PATH_MAX);
-	if (ret == PATH_MAX)
-		return -E2BIG;
-	if (ret < 0)
-		goto out;
+
+	path = strndup_user(u64_to_user_ptr(p_event->attr.uprobe_path),
+			    PATH_MAX);
+	if (IS_ERR(path)) {
+		ret = PTR_ERR(path);
+		return (ret == -EINVAL) ? -E2BIG : ret;
+	}
 	if (path[0] == '\0') {
 		ret = -EINVAL;
 		goto out;

commit a6ca88b241d5e929e6e60b12ad8cd288f0ffa256
Author: Song Liu <songliubraving@fb.com>
Date:   Mon Oct 1 22:36:36 2018 -0700

    trace_uprobe: support reference counter in fd-based uprobe
    
    This patch enables uprobes with reference counter in fd-based uprobe.
    Highest 32 bits of perf_event_attr.config is used to stored offset
    of the reference count (semaphore).
    
    Format information in /sys/bus/event_source/devices/uprobe/format/ is
    updated to reflect this new feature.
    
    Link: http://lkml.kernel.org/r/20181002053636.1896903-1-songliubraving@fb.com
    
    Cc: Oleg Nesterov <oleg@redhat.com>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-and-tested-by: Ravi Bangoria <ravi.bangoria@linux.ibm.com>
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 69a3fe926e8c..76217bbef815 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -290,7 +290,8 @@ void perf_kprobe_destroy(struct perf_event *p_event)
 #endif /* CONFIG_KPROBE_EVENTS */
 
 #ifdef CONFIG_UPROBE_EVENTS
-int perf_uprobe_init(struct perf_event *p_event, bool is_retprobe)
+int perf_uprobe_init(struct perf_event *p_event,
+		     unsigned long ref_ctr_offset, bool is_retprobe)
 {
 	int ret;
 	char *path = NULL;
@@ -312,8 +313,8 @@ int perf_uprobe_init(struct perf_event *p_event, bool is_retprobe)
 		goto out;
 	}
 
-	tp_event = create_local_trace_uprobe(
-		path, p_event->attr.probe_offset, is_retprobe);
+	tp_event = create_local_trace_uprobe(path, p_event->attr.probe_offset,
+					     ref_ctr_offset, is_retprobe);
 	if (IS_ERR(tp_event)) {
 		ret = PTR_ERR(tp_event);
 		goto out;

commit bcea3f96e11cf2f0232d851e0fdb854f5ada425a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 16 11:23:53 2018 -0400

    tracing: Add SPDX License format tags to tracing files
    
    Add the SPDX License header to ease license compliance management.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index c79193e598f5..69a3fe926e8c 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * trace event based perf event profiling/tracing
  *

commit 0eadcc7a7bc03e991d2da1cf88143fb7cc0342c1
Author: Song Liu <songliubraving@fb.com>
Date:   Mon Apr 9 18:31:30 2018 +0000

    perf/core: Fix perf_uprobe_init()
    
    Similarly to the uprobe PMU fix in perf_kprobe_init(), fix error
    handling in perf_uprobe_init() as well.
    
    Reported-by: 范龙飞 <long7573@126.com>
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: e12f03d7031a ("perf/core: Implement the 'perf_kprobe' PMU")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 94600f1f7efa..c79193e598f5 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -302,6 +302,8 @@ int perf_uprobe_init(struct perf_event *p_event, bool is_retprobe)
 		return -ENOMEM;
 	ret = strncpy_from_user(
 		path, u64_to_user_ptr(p_event->attr.uprobe_path), PATH_MAX);
+	if (ret == PATH_MAX)
+		return -E2BIG;
 	if (ret < 0)
 		goto out;
 	if (path[0] == '\0') {

commit 5da13ab8b0dcaa984c45ae43edf5a4d148603d42
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Mon Apr 9 21:16:54 2018 +0900

    perf/core: Fix perf_kprobe_init()
    
    Fix error handling in perf_kprobe_init():
    
            ==================================================================
            BUG: KASAN: slab-out-of-bounds in strlen+0x8e/0xa0 lib/string.c:482
            Read of size 1 at addr ffff88003f9cc5c0 by task syz-executor2/23095
    
            CPU: 0 PID: 23095 Comm: syz-executor2 Not tainted 4.16.0+ #24
            Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS Ubuntu-1.8.2-1ubuntu1 04/01/2014
            Call Trace:
             __dump_stack lib/dump_stack.c:77 [inline]
             dump_stack+0xca/0x13e lib/dump_stack.c:113
             print_address_description+0x6e/0x2c0 mm/kasan/report.c:256
             kasan_report_error mm/kasan/report.c:354 [inline]
             kasan_report+0x256/0x380 mm/kasan/report.c:412
             strlen+0x8e/0xa0 lib/string.c:482
             kstrdup+0x21/0x70 mm/util.c:55
             alloc_trace_kprobe+0xc8/0x930 kernel/trace/trace_kprobe.c:325
             create_local_trace_kprobe+0x4f/0x3a0 kernel/trace/trace_kprobe.c:1438
             perf_kprobe_init+0x149/0x1f0 kernel/trace/trace_event_perf.c:264
             perf_kprobe_event_init+0xa8/0x120 kernel/events/core.c:8407
             perf_try_init_event+0xcb/0x2a0 kernel/events/core.c:9719
             perf_init_event kernel/events/core.c:9750 [inline]
             perf_event_alloc+0x1367/0x1e20 kernel/events/core.c:10022
             SYSC_perf_event_open+0x242/0x2330 kernel/events/core.c:10477
             do_syscall_64+0x198/0x640 arch/x86/entry/common.c:287
             entry_SYSCALL_64_after_hwframe+0x42/0xb7
    
    Reported-by: 范龙飞 <long7573@126.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Song Liu <songliubraving@fb.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Fixes: e12f03d7031a ("perf/core: Implement the 'perf_kprobe' PMU")
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 2c416509b834..94600f1f7efa 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -252,6 +252,8 @@ int perf_kprobe_init(struct perf_event *p_event, bool is_retprobe)
 		ret = strncpy_from_user(
 			func, u64_to_user_ptr(p_event->attr.kprobe_func),
 			KSYM_NAME_LEN);
+		if (ret == KSYM_NAME_LEN)
+			ret = -E2BIG;
 		if (ret < 0)
 			goto out;
 

commit 33ea4b24277b06dbc55d7f5772a46f029600255e
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Dec 6 14:45:16 2017 -0800

    perf/core: Implement the 'perf_uprobe' PMU
    
    This patch adds perf_uprobe support with similar pattern as previous
    patch (for kprobe).
    
    Two functions, create_local_trace_uprobe() and
    destroy_local_trace_uprobe(), are created so a uprobe can be created
    and attached to the file descriptor created by perf_event_open().
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Yonghong Song <yhs@fb.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Cc: <daniel@iogearbox.net>
    Cc: <davem@davemloft.net>
    Cc: <kernel-team@fb.com>
    Cc: <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171206224518.3598254-7-songliubraving@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 779baade9114..2c416509b834 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -286,6 +286,59 @@ void perf_kprobe_destroy(struct perf_event *p_event)
 }
 #endif /* CONFIG_KPROBE_EVENTS */
 
+#ifdef CONFIG_UPROBE_EVENTS
+int perf_uprobe_init(struct perf_event *p_event, bool is_retprobe)
+{
+	int ret;
+	char *path = NULL;
+	struct trace_event_call *tp_event;
+
+	if (!p_event->attr.uprobe_path)
+		return -EINVAL;
+	path = kzalloc(PATH_MAX, GFP_KERNEL);
+	if (!path)
+		return -ENOMEM;
+	ret = strncpy_from_user(
+		path, u64_to_user_ptr(p_event->attr.uprobe_path), PATH_MAX);
+	if (ret < 0)
+		goto out;
+	if (path[0] == '\0') {
+		ret = -EINVAL;
+		goto out;
+	}
+
+	tp_event = create_local_trace_uprobe(
+		path, p_event->attr.probe_offset, is_retprobe);
+	if (IS_ERR(tp_event)) {
+		ret = PTR_ERR(tp_event);
+		goto out;
+	}
+
+	/*
+	 * local trace_uprobe need to hold event_mutex to call
+	 * uprobe_buffer_enable() and uprobe_buffer_disable().
+	 * event_mutex is not required for local trace_kprobes.
+	 */
+	mutex_lock(&event_mutex);
+	ret = perf_trace_event_init(tp_event, p_event);
+	if (ret)
+		destroy_local_trace_uprobe(tp_event);
+	mutex_unlock(&event_mutex);
+out:
+	kfree(path);
+	return ret;
+}
+
+void perf_uprobe_destroy(struct perf_event *p_event)
+{
+	mutex_lock(&event_mutex);
+	perf_trace_event_close(p_event);
+	perf_trace_event_unreg(p_event);
+	mutex_unlock(&event_mutex);
+	destroy_local_trace_uprobe(p_event->tp_event);
+}
+#endif /* CONFIG_UPROBE_EVENTS */
+
 int perf_trace_add(struct perf_event *p_event, int flags)
 {
 	struct trace_event_call *tp_event = p_event->tp_event;

commit e12f03d7031a977356e3d7b75a68c2185ff8d155
Author: Song Liu <songliubraving@fb.com>
Date:   Wed Dec 6 14:45:15 2017 -0800

    perf/core: Implement the 'perf_kprobe' PMU
    
    A new PMU type, perf_kprobe is added. Based on attr from perf_event_open(),
    perf_kprobe creates a kprobe (or kretprobe) for the perf_event. This
    kprobe is private to this perf_event, and thus not added to global
    lists, and not available in tracefs.
    
    Two functions, create_local_trace_kprobe() and
    destroy_local_trace_kprobe()  are added to created and destroy these
    local trace_kprobe.
    
    Signed-off-by: Song Liu <songliubraving@fb.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Yonghong Song <yhs@fb.com>
    Reviewed-by: Josef Bacik <jbacik@fb.com>
    Cc: <daniel@iogearbox.net>
    Cc: <davem@davemloft.net>
    Cc: <kernel-team@fb.com>
    Cc: <rostedt@goodmis.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: http://lkml.kernel.org/r/20171206224518.3598254-6-songliubraving@fb.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 55d6dff37daf..779baade9114 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -8,6 +8,7 @@
 #include <linux/module.h>
 #include <linux/kprobes.h>
 #include "trace.h"
+#include "trace_probe.h"
 
 static char __percpu *perf_trace_buf[PERF_NR_CONTEXTS];
 
@@ -237,6 +238,54 @@ void perf_trace_destroy(struct perf_event *p_event)
 	mutex_unlock(&event_mutex);
 }
 
+#ifdef CONFIG_KPROBE_EVENTS
+int perf_kprobe_init(struct perf_event *p_event, bool is_retprobe)
+{
+	int ret;
+	char *func = NULL;
+	struct trace_event_call *tp_event;
+
+	if (p_event->attr.kprobe_func) {
+		func = kzalloc(KSYM_NAME_LEN, GFP_KERNEL);
+		if (!func)
+			return -ENOMEM;
+		ret = strncpy_from_user(
+			func, u64_to_user_ptr(p_event->attr.kprobe_func),
+			KSYM_NAME_LEN);
+		if (ret < 0)
+			goto out;
+
+		if (func[0] == '\0') {
+			kfree(func);
+			func = NULL;
+		}
+	}
+
+	tp_event = create_local_trace_kprobe(
+		func, (void *)(unsigned long)(p_event->attr.kprobe_addr),
+		p_event->attr.probe_offset, is_retprobe);
+	if (IS_ERR(tp_event)) {
+		ret = PTR_ERR(tp_event);
+		goto out;
+	}
+
+	ret = perf_trace_event_init(tp_event, p_event);
+	if (ret)
+		destroy_local_trace_kprobe(tp_event);
+out:
+	kfree(func);
+	return ret;
+}
+
+void perf_kprobe_destroy(struct perf_event *p_event)
+{
+	perf_trace_event_close(p_event);
+	perf_trace_event_unreg(p_event);
+
+	destroy_local_trace_kprobe(p_event->tp_event);
+}
+#endif /* CONFIG_KPROBE_EVENTS */
+
 int perf_trace_add(struct perf_event *p_event, int flags)
 {
 	struct trace_event_call *tp_event = p_event->tp_event;

commit 1dd311e6dcda4020c603bcf9f390a577d439d509
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 11 09:45:31 2017 +0200

    perf/ftrace: Small cleanup
    
    ops->flags _should_ be 0 at this point, so setting the flag using
    bitwise or is a bit daft.
    
    Link: http://lkml.kernel.org/r/20171011080224.315585202@infradead.org
    
    Requested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index e73f9ab15939..55d6dff37daf 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -363,7 +363,7 @@ static int perf_ftrace_function_register(struct perf_event *event)
 {
 	struct ftrace_ops *ops = &event->ftrace_ops;
 
-	ops->flags   |= FTRACE_OPS_FL_RCU;
+	ops->flags   = FTRACE_OPS_FL_RCU;
 	ops->func    = perf_ftrace_function_call;
 	ops->private = (void *)(unsigned long)nr_cpu_ids;
 

commit 466c81c45b650deca213fda3d0ec4761667379a9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue Oct 10 17:15:47 2017 +0200

    perf/ftrace: Fix function trace events
    
    The function-trace <-> perf interface is a tad messed up. Where all
    the other trace <-> perf interfaces use a single trace hook
    registration and use per-cpu RCU based hlist to iterate the events,
    function-trace actually needs multiple hook registrations in order to
    minimize function entry patching when filters are present.
    
    The end result is that we iterate events both on the trace hook and on
    the hlist, which results in reporting events multiple times.
    
    Since function-trace cannot use the regular scheme, fix it the other
    way around, use singleton hlists.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 562fa69df5d3..e73f9ab15939 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -240,27 +240,41 @@ void perf_trace_destroy(struct perf_event *p_event)
 int perf_trace_add(struct perf_event *p_event, int flags)
 {
 	struct trace_event_call *tp_event = p_event->tp_event;
-	struct hlist_head __percpu *pcpu_list;
-	struct hlist_head *list;
-
-	pcpu_list = tp_event->perf_events;
-	if (WARN_ON_ONCE(!pcpu_list))
-		return -EINVAL;
 
 	if (!(flags & PERF_EF_START))
 		p_event->hw.state = PERF_HES_STOPPED;
 
-	list = this_cpu_ptr(pcpu_list);
-	hlist_add_head_rcu(&p_event->hlist_entry, list);
+	/*
+	 * If TRACE_REG_PERF_ADD returns false; no custom action was performed
+	 * and we need to take the default action of enqueueing our event on
+	 * the right per-cpu hlist.
+	 */
+	if (!tp_event->class->reg(tp_event, TRACE_REG_PERF_ADD, p_event)) {
+		struct hlist_head __percpu *pcpu_list;
+		struct hlist_head *list;
+
+		pcpu_list = tp_event->perf_events;
+		if (WARN_ON_ONCE(!pcpu_list))
+			return -EINVAL;
+
+		list = this_cpu_ptr(pcpu_list);
+		hlist_add_head_rcu(&p_event->hlist_entry, list);
+	}
 
-	return tp_event->class->reg(tp_event, TRACE_REG_PERF_ADD, p_event);
+	return 0;
 }
 
 void perf_trace_del(struct perf_event *p_event, int flags)
 {
 	struct trace_event_call *tp_event = p_event->tp_event;
-	hlist_del_rcu(&p_event->hlist_entry);
-	tp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event);
+
+	/*
+	 * If TRACE_REG_PERF_DEL returns false; no custom action was performed
+	 * and we need to take the default action of dequeueing our event from
+	 * the right per-cpu hlist.
+	 */
+	if (!tp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event))
+		hlist_del_rcu(&p_event->hlist_entry);
 }
 
 void *perf_trace_buf_alloc(int size, struct pt_regs **regs, int *rctxp)
@@ -307,14 +321,24 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *ops, struct pt_regs *pt_regs)
 {
 	struct ftrace_entry *entry;
-	struct hlist_head *head;
+	struct perf_event *event;
+	struct hlist_head head;
 	struct pt_regs regs;
 	int rctx;
 
-	head = this_cpu_ptr(event_function.perf_events);
-	if (hlist_empty(head))
+	if ((unsigned long)ops->private != smp_processor_id())
 		return;
 
+	event = container_of(ops, struct perf_event, ftrace_ops);
+
+	/*
+	 * @event->hlist entry is NULL (per INIT_HLIST_NODE), and all
+	 * the perf code does is hlist_for_each_entry_rcu(), so we can
+	 * get away with simply setting the @head.first pointer in order
+	 * to create a singular list.
+	 */
+	head.first = &event->hlist_entry;
+
 #define ENTRY_SIZE (ALIGN(sizeof(struct ftrace_entry) + sizeof(u32), \
 		    sizeof(u64)) - sizeof(u32))
 
@@ -330,7 +354,7 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 	entry->ip = ip;
 	entry->parent_ip = parent_ip;
 	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, TRACE_FN,
-			      1, &regs, head, NULL);
+			      1, &regs, &head, NULL);
 
 #undef ENTRY_SIZE
 }
@@ -339,8 +363,10 @@ static int perf_ftrace_function_register(struct perf_event *event)
 {
 	struct ftrace_ops *ops = &event->ftrace_ops;
 
-	ops->flags |= FTRACE_OPS_FL_PER_CPU | FTRACE_OPS_FL_RCU;
-	ops->func = perf_ftrace_function_call;
+	ops->flags   |= FTRACE_OPS_FL_RCU;
+	ops->func    = perf_ftrace_function_call;
+	ops->private = (void *)(unsigned long)nr_cpu_ids;
+
 	return register_ftrace_function(ops);
 }
 
@@ -352,19 +378,11 @@ static int perf_ftrace_function_unregister(struct perf_event *event)
 	return ret;
 }
 
-static void perf_ftrace_function_enable(struct perf_event *event)
-{
-	ftrace_function_local_enable(&event->ftrace_ops);
-}
-
-static void perf_ftrace_function_disable(struct perf_event *event)
-{
-	ftrace_function_local_disable(&event->ftrace_ops);
-}
-
 int perf_ftrace_event_register(struct trace_event_call *call,
 			       enum trace_reg type, void *data)
 {
+	struct perf_event *event = data;
+
 	switch (type) {
 	case TRACE_REG_REGISTER:
 	case TRACE_REG_UNREGISTER:
@@ -377,11 +395,11 @@ int perf_ftrace_event_register(struct trace_event_call *call,
 	case TRACE_REG_PERF_CLOSE:
 		return perf_ftrace_function_unregister(data);
 	case TRACE_REG_PERF_ADD:
-		perf_ftrace_function_enable(data);
-		return 0;
+		event->ftrace_ops.private = (void *)(unsigned long)smp_processor_id();
+		return 1;
 	case TRACE_REG_PERF_DEL:
-		perf_ftrace_function_disable(data);
-		return 0;
+		event->ftrace_ops.private = (void *)(unsigned long)nr_cpu_ids;
+		return 1;
 	}
 
 	return -EINVAL;

commit 8fd0fbbe8888f295eb34172a7e47bf7d3a0a4687
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 11 09:45:29 2017 +0200

    perf/ftrace: Revert ("perf/ftrace: Fix double traces of perf on ftrace:function")
    
    Revert commit:
    
      75e8387685f6 ("perf/ftrace: Fix double traces of perf on ftrace:function")
    
    The reason I instantly stumbled on that patch is that it only addresses the
    ftrace situation and doesn't mention the other _5_ places that use this
    interface. It doesn't explain why those don't have the problem and if not, why
    their solution doesn't work for ftrace.
    
    It doesn't, but this is just putting more duct tape on.
    
    Link: http://lkml.kernel.org/r/20171011080224.200565770@infradead.org
    
    Cc: Zhou Chengming <zhouchengming1@huawei.com>
    Cc: Jiri Olsa <jolsa@kernel.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 13ba2d3f6a91..562fa69df5d3 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -306,7 +306,6 @@ static void
 perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *ops, struct pt_regs *pt_regs)
 {
-	struct perf_event *event;
 	struct ftrace_entry *entry;
 	struct hlist_head *head;
 	struct pt_regs regs;
@@ -330,9 +329,8 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 
 	entry->ip = ip;
 	entry->parent_ip = parent_ip;
-	event = container_of(ops, struct perf_event, ftrace_ops);
 	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, TRACE_FN,
-			      1, &regs, head, NULL, event);
+			      1, &regs, head, NULL);
 
 #undef ENTRY_SIZE
 }

commit 75e8387685f6c65feb195a4556110b58f852b848
Author: Zhou Chengming <zhouchengming1@huawei.com>
Date:   Fri Aug 25 21:49:37 2017 +0800

    perf/ftrace: Fix double traces of perf on ftrace:function
    
    When running perf on the ftrace:function tracepoint, there is a bug
    which can be reproduced by:
    
      perf record -e ftrace:function -a sleep 20 &
      perf record -e ftrace:function ls
      perf script
    
                  ls 10304 [005]   171.853235: ftrace:function:
      perf_output_begin
                  ls 10304 [005]   171.853237: ftrace:function:
      perf_output_begin
                  ls 10304 [005]   171.853239: ftrace:function:
      task_tgid_nr_ns
                  ls 10304 [005]   171.853240: ftrace:function:
      task_tgid_nr_ns
                  ls 10304 [005]   171.853242: ftrace:function:
      __task_pid_nr_ns
                  ls 10304 [005]   171.853244: ftrace:function:
      __task_pid_nr_ns
    
    We can see that all the function traces are doubled.
    
    The problem is caused by the inconsistency of the register
    function perf_ftrace_event_register() with the probe function
    perf_ftrace_function_call(). The former registers one probe
    for every perf_event. And the latter handles all perf_events
    on the current cpu. So when two perf_events on the current cpu,
    the traces of them will be doubled.
    
    So this patch adds an extra parameter "event" for perf_tp_event,
    only send sample data to this event when it's not NULL.
    
    Signed-off-by: Zhou Chengming <zhouchengming1@huawei.com>
    Reviewed-by: Jiri Olsa <jolsa@kernel.org>
    Acked-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: acme@kernel.org
    Cc: alexander.shishkin@linux.intel.com
    Cc: huawei.libin@huawei.com
    Link: http://lkml.kernel.org/r/1503668977-12526-1-git-send-email-zhouchengming1@huawei.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 562fa69df5d3..13ba2d3f6a91 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -306,6 +306,7 @@ static void
 perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *ops, struct pt_regs *pt_regs)
 {
+	struct perf_event *event;
 	struct ftrace_entry *entry;
 	struct hlist_head *head;
 	struct pt_regs regs;
@@ -329,8 +330,9 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 
 	entry->ip = ip;
 	entry->parent_ip = parent_ip;
+	event = container_of(ops, struct perf_event, ftrace_ops);
 	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, TRACE_FN,
-			      1, &regs, head, NULL);
+			      1, &regs, head, NULL, event);
 
 #undef ENTRY_SIZE
 }

commit a7fd20d1c476af4563e66865213474a2f9f473a4
Merge: b80fed959551 917fa5353da0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue May 17 16:26:30 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       1) Support SPI based w5100 devices, from Akinobu Mita.
    
       2) Partial Segmentation Offload, from Alexander Duyck.
    
       3) Add GMAC4 support to stmmac driver, from Alexandre TORGUE.
    
       4) Allow cls_flower stats offload, from Amir Vadai.
    
       5) Implement bpf blinding, from Daniel Borkmann.
    
       6) Optimize _ASYNC_ bit twiddling on sockets, unless the socket is
          actually using FASYNC these atomics are superfluous.  From Eric
          Dumazet.
    
       7) Run TCP more preemptibly, also from Eric Dumazet.
    
       8) Support LED blinking, EEPROM dumps, and rxvlan offloading in mlx5e
          driver, from Gal Pressman.
    
       9) Allow creating ppp devices via rtnetlink, from Guillaume Nault.
    
      10) Improve BPF usage documentation, from Jesper Dangaard Brouer.
    
      11) Support tunneling offloads in qed, from Manish Chopra.
    
      12) aRFS offloading in mlx5e, from Maor Gottlieb.
    
      13) Add RFS and RPS support to SCTP protocol, from Marcelo Ricardo
          Leitner.
    
      14) Add MSG_EOR support to TCP, this allows controlling packet
          coalescing on application record boundaries for more accurate
          socket timestamp sampling.  From Martin KaFai Lau.
    
      15) Fix alignment of 64-bit netlink attributes across the board, from
          Nicolas Dichtel.
    
      16) Per-vlan stats in bridging, from Nikolay Aleksandrov.
    
      17) Several conversions of drivers to ethtool ksettings, from Philippe
          Reynes.
    
      18) Checksum neutral ILA in ipv6, from Tom Herbert.
    
      19) Factorize all of the various marvell dsa drivers into one, from
          Vivien Didelot
    
      20) Add VF support to qed driver, from Yuval Mintz"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1649 commits)
      Revert "phy dp83867: Fix compilation with CONFIG_OF_MDIO=m"
      Revert "phy dp83867: Make rgmii parameters optional"
      r8169: default to 64-bit DMA on recent PCIe chips
      phy dp83867: Make rgmii parameters optional
      phy dp83867: Fix compilation with CONFIG_OF_MDIO=m
      bpf: arm64: remove callee-save registers use for tmp registers
      asix: Fix offset calculation in asix_rx_fixup() causing slow transmissions
      switchdev: pass pointer to fib_info instead of copy
      net_sched: close another race condition in tcf_mirred_release()
      tipc: fix nametable publication field in nl compat
      drivers: net: Don't print unpopulated net_device name
      qed: add support for dcbx.
      ravb: Add missing free_irq() calls to ravb_close()
      qed: Remove a stray tab
      net: ethernet: fec-mpc52xx: use phy_ethtool_{get|set}_link_ksettings
      net: ethernet: fec-mpc52xx: use phydev from struct net_device
      bpf, doc: fix typo on bpf_asm descriptions
      stmmac: hardware TX COE doesn't work when force_thresh_dma_mode is set
      net: ethernet: fs-enet: use phy_ethtool_{get|set}_link_ksettings
      net: ethernet: fs-enet: use phydev from struct net_device
      ...

commit 1e1dcd93b468901e114f279c94a0b356adc5e7cd
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 6 18:43:24 2016 -0700

    perf: split perf_trace_buf_prepare into alloc and update parts
    
    split allows to move expensive update of 'struct trace_entry' to later phase.
    Repurpose unused 1st argument of perf_tp_event() to indicate event type.
    
    While splitting use temp variable 'rctx' instead of '*rctx' to avoid
    unnecessary loads done by the compiler due to -fno-strict-aliasing
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 7a68afca8249..5a927075977f 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -260,42 +260,43 @@ void perf_trace_del(struct perf_event *p_event, int flags)
 	tp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event);
 }
 
-void *perf_trace_buf_prepare(int size, unsigned short type,
-			     struct pt_regs **regs, int *rctxp)
+void *perf_trace_buf_alloc(int size, struct pt_regs **regs, int *rctxp)
 {
-	struct trace_entry *entry;
-	unsigned long flags;
 	char *raw_data;
-	int pc;
+	int rctx;
 
 	BUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));
 
 	if (WARN_ONCE(size > PERF_MAX_TRACE_SIZE,
-			"perf buffer not large enough"))
+		      "perf buffer not large enough"))
 		return NULL;
 
-	pc = preempt_count();
-
-	*rctxp = perf_swevent_get_recursion_context();
-	if (*rctxp < 0)
+	*rctxp = rctx = perf_swevent_get_recursion_context();
+	if (rctx < 0)
 		return NULL;
 
 	if (regs)
-		*regs = this_cpu_ptr(&__perf_regs[*rctxp]);
-	raw_data = this_cpu_ptr(perf_trace_buf[*rctxp]);
+		*regs = this_cpu_ptr(&__perf_regs[rctx]);
+	raw_data = this_cpu_ptr(perf_trace_buf[rctx]);
 
 	/* zero the dead bytes from align to not leak stack to user */
 	memset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));
+	return raw_data;
+}
+EXPORT_SYMBOL_GPL(perf_trace_buf_alloc);
+NOKPROBE_SYMBOL(perf_trace_buf_alloc);
+
+void perf_trace_buf_update(void *record, u16 type)
+{
+	struct trace_entry *entry = record;
+	int pc = preempt_count();
+	unsigned long flags;
 
-	entry = (struct trace_entry *)raw_data;
 	local_save_flags(flags);
 	tracing_generic_entry_update(entry, flags, pc);
 	entry->type = type;
-
-	return raw_data;
 }
-EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);
-NOKPROBE_SYMBOL(perf_trace_buf_prepare);
+NOKPROBE_SYMBOL(perf_trace_buf_update);
 
 #ifdef CONFIG_FUNCTION_TRACER
 static void
@@ -319,13 +320,13 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 	memset(&regs, 0, sizeof(regs));
 	perf_fetch_caller_regs(&regs);
 
-	entry = perf_trace_buf_prepare(ENTRY_SIZE, TRACE_FN, NULL, &rctx);
+	entry = perf_trace_buf_alloc(ENTRY_SIZE, NULL, &rctx);
 	if (!entry)
 		return;
 
 	entry->ip = ip;
 	entry->parent_ip = parent_ip;
-	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, 0,
+	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, TRACE_FN,
 			      1, &regs, head, NULL);
 
 #undef ENTRY_SIZE

commit ec5e099d6e941668d121ea9ca7057f4fa00830b0
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 6 18:43:22 2016 -0700

    perf: optimize perf_fetch_caller_regs
    
    avoid memset in perf_fetch_caller_regs, since it's the critical path of all tracepoints.
    It's called from perf_sw_event_sched, perf_event_task_sched_in and all of perf_trace_##call
    with this_cpu_ptr(&__perf_regs[..]) which are zero initialized by perpcu init logic and
    subsequent call to perf_arch_fetch_caller_regs initializes the same fields on all archs,
    so we can safely drop memset from all of the above cases and move it into
    perf_ftrace_function_call that calls it with stack allocated pt_regs.
    
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 00df25fd86ef..7a68afca8249 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -316,6 +316,7 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 
 	BUILD_BUG_ON(ENTRY_SIZE > PERF_MAX_TRACE_SIZE);
 
+	memset(&regs, 0, sizeof(regs));
 	perf_fetch_caller_regs(&regs);
 
 	entry = perf_trace_buf_prepare(ENTRY_SIZE, TRACE_FN, NULL, &rctx);

commit 0a74c5b3d20d2a8693848b6ae4f1a97624f5b781
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Wed Mar 16 15:34:29 2016 +0100

    ftrace/perf: Check sample types only for sampling events
    
    Currently we check sample type for ftrace:function events
    even if it's not created as a sampling event. That prevents
    creating ftrace_function event in counting mode.
    
    Make sure we check sample types only for sampling events.
    
    Before:
      $ sudo perf stat -e ftrace:function ls
      ...
    
       Performance counter stats for 'ls':
    
         <not supported>      ftrace:function
    
             0.001983662 seconds time elapsed
    
    After:
      $ sudo perf stat -e ftrace:function ls
      ...
    
       Performance counter stats for 'ls':
    
                  44,498      ftrace:function
    
             0.037534722 seconds time elapsed
    
    Suggested-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Alexander Shishkin <alexander.shishkin@linux.intel.com>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Link: http://lkml.kernel.org/r/1458138873-1553-2-git-send-email-jolsa@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 00df25fd86ef..e11108f1d197 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -47,6 +47,9 @@ static int perf_trace_event_perm(struct trace_event_call *tp_event,
 		if (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
 			return -EPERM;
 
+		if (!is_sampling_event(p_event))
+			return 0;
+
 		/*
 		 * We don't allow user space callchains for  function trace
 		 * event, due to issues with page faults while tracing page

commit c17488d06666153a14dd3f21bd10eba58383f6c1
Merge: 34a9304a96d6 5156dca34a3e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Jan 12 20:04:15 2016 -0800

    Merge tag 'trace-v4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Not much new with tracing for this release.  Mostly just clean ups and
      minor fixes.
    
      Here's what else is new:
    
       - A new TRACE_EVENT_FN_COND macro, combining both _FN and _COND for
         those that want both.
    
       - New selftest to test the instance create and delete
    
       - Better debug output when ftrace fails"
    
    * tag 'trace-v4.5' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (24 commits)
      ftrace: Fix the race between ftrace and insmod
      ftrace: Add infrastructure for delayed enabling of module functions
      x86: ftrace: Fix the comments for ftrace_modify_code_direct()
      tracing: Fix comment to use tracing_on over tracing_enable
      metag: ftrace: Fix the comments for ftrace_modify_code
      sh: ftrace: Fix the comments for ftrace_modify_code()
      ia64: ftrace: Fix the comments for ftrace_modify_code()
      ftrace: Clean up ftrace_module_init() code
      ftrace: Join functions ftrace_module_init() and ftrace_init_module()
      tracing: Introduce TRACE_EVENT_FN_COND macro
      tracing: Use seq_buf_used() in seq_buf_to_user() instead of len
      bpf: Constify bpf_verifier_ops structure
      ftrace: Have ftrace_ops_get_func() handle RCU and PER_CPU flags too
      ftrace: Remove use of control list and ops
      ftrace: Fix output of enabled_functions for showing tramp
      ftrace: Fix a typo in comment
      ftrace: Show all tramps registered to a record on ftrace_bug()
      ftrace: Add variable ftrace_expected for archs to show expected code
      ftrace: Add new type to distinguish what kind of ftrace_bug()
      tracing: Update cond flag when enabling or disabling a trigger
      ...

commit ba27f2bc731135a0396f3968bdddb54f3bc72e64
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 30 17:23:39 2015 -0500

    ftrace: Remove use of control list and ops
    
    Currently perf has its own list function within the ftrace infrastructure
    that seems to be used only to allow for it to have per-cpu disabling as well
    as a check to make sure that it's not called while RCU is not watching. It
    uses something called the "control_ops" which is used to iterate over ops
    under it with the control_list_func().
    
    The problem is that this control_ops and control_list_func unnecessarily
    complicates the code. By replacing FTRACE_OPS_FL_CONTROL with two new flags
    (FTRACE_OPS_FL_RCU and FTRACE_OPS_FL_PER_CPU) we can remove all the code
    that is special with the control ops and add the needed checks within the
    generic ftrace_list_func().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index abfc903e741e..2649c85cd162 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -334,7 +334,7 @@ static int perf_ftrace_function_register(struct perf_event *event)
 {
 	struct ftrace_ops *ops = &event->ftrace_ops;
 
-	ops->flags |= FTRACE_OPS_FL_CONTROL;
+	ops->flags |= FTRACE_OPS_FL_PER_CPU | FTRACE_OPS_FL_RCU;
 	ops->func = perf_ftrace_function_call;
 	return register_ftrace_function(ops);
 }

commit 90eec103b96e30401c0b846045bf8a1c7159b6da
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Mon Nov 16 11:08:45 2015 +0100

    treewide: Remove old email address
    
    There were still a number of references to my old Red Hat email
    address in the kernel source. Remove these while keeping the
    Red Hat copyright notices intact.
    
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Stephane Eranian <eranian@google.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index abfc903e741e..cc9f7a9319be 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -1,7 +1,7 @@
 /*
  * trace event based perf event profiling/tracing
  *
- * Copyright (C) 2009 Red Hat Inc, Peter Zijlstra <pzijlstr@redhat.com>
+ * Copyright (C) 2009 Red Hat Inc, Peter Zijlstra
  * Copyright (C) 2009-2010 Frederic Weisbecker <fweisbec@gmail.com>
  */
 

commit 2425bcb9240f8c97d793cb31c8e8d8d0a843fa29
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 11:45:27 2015 -0400

    tracing: Rename ftrace_event_{call,class} to trace_event_{call,class}
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structures ftrace_event_call and
    ftrace_event_class have nothing to do with the function hooks, and are
    really trace_event structures. Rename ftrace_event_* to trace_event_*.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 6fa484de2ba1..abfc903e741e 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -21,7 +21,7 @@ typedef typeof(unsigned long [PERF_MAX_TRACE_SIZE / sizeof(unsigned long)])
 /* Count the events in use (per event id, not per instance) */
 static int	total_ref_count;
 
-static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
+static int perf_trace_event_perm(struct trace_event_call *tp_event,
 				 struct perf_event *p_event)
 {
 	if (tp_event->perf_perm) {
@@ -83,7 +83,7 @@ static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 	return 0;
 }
 
-static int perf_trace_event_reg(struct ftrace_event_call *tp_event,
+static int perf_trace_event_reg(struct trace_event_call *tp_event,
 				struct perf_event *p_event)
 {
 	struct hlist_head __percpu *list;
@@ -143,7 +143,7 @@ static int perf_trace_event_reg(struct ftrace_event_call *tp_event,
 
 static void perf_trace_event_unreg(struct perf_event *p_event)
 {
-	struct ftrace_event_call *tp_event = p_event->tp_event;
+	struct trace_event_call *tp_event = p_event->tp_event;
 	int i;
 
 	if (--tp_event->perf_refcount > 0)
@@ -172,17 +172,17 @@ static void perf_trace_event_unreg(struct perf_event *p_event)
 
 static int perf_trace_event_open(struct perf_event *p_event)
 {
-	struct ftrace_event_call *tp_event = p_event->tp_event;
+	struct trace_event_call *tp_event = p_event->tp_event;
 	return tp_event->class->reg(tp_event, TRACE_REG_PERF_OPEN, p_event);
 }
 
 static void perf_trace_event_close(struct perf_event *p_event)
 {
-	struct ftrace_event_call *tp_event = p_event->tp_event;
+	struct trace_event_call *tp_event = p_event->tp_event;
 	tp_event->class->reg(tp_event, TRACE_REG_PERF_CLOSE, p_event);
 }
 
-static int perf_trace_event_init(struct ftrace_event_call *tp_event,
+static int perf_trace_event_init(struct trace_event_call *tp_event,
 				 struct perf_event *p_event)
 {
 	int ret;
@@ -206,7 +206,7 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 
 int perf_trace_init(struct perf_event *p_event)
 {
-	struct ftrace_event_call *tp_event;
+	struct trace_event_call *tp_event;
 	u64 event_id = p_event->attr.config;
 	int ret = -EINVAL;
 
@@ -236,7 +236,7 @@ void perf_trace_destroy(struct perf_event *p_event)
 
 int perf_trace_add(struct perf_event *p_event, int flags)
 {
-	struct ftrace_event_call *tp_event = p_event->tp_event;
+	struct trace_event_call *tp_event = p_event->tp_event;
 	struct hlist_head __percpu *pcpu_list;
 	struct hlist_head *list;
 
@@ -255,7 +255,7 @@ int perf_trace_add(struct perf_event *p_event, int flags)
 
 void perf_trace_del(struct perf_event *p_event, int flags)
 {
-	struct ftrace_event_call *tp_event = p_event->tp_event;
+	struct trace_event_call *tp_event = p_event->tp_event;
 	hlist_del_rcu(&p_event->hlist_entry);
 	tp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event);
 }
@@ -357,7 +357,7 @@ static void perf_ftrace_function_disable(struct perf_event *event)
 	ftrace_function_local_disable(&event->ftrace_ops);
 }
 
-int perf_ftrace_event_register(struct ftrace_event_call *call,
+int perf_ftrace_event_register(struct trace_event_call *call,
 			       enum trace_reg type, void *data)
 {
 	switch (type) {

commit 86038c5ea81b519a8a1fcfcd5e4599aab0cdd119
Author: Peter Zijlstra (Intel) <peterz@infradead.org>
Date:   Tue Dec 16 12:47:34 2014 +0100

    perf: Avoid horrible stack usage
    
    Both Linus (most recent) and Steve (a while ago) reported that perf
    related callbacks have massive stack bloat.
    
    The problem is that software events need a pt_regs in order to
    properly report the event location and unwind stack. And because we
    could not assume one was present we allocated one on stack and filled
    it with minimal bits required for operation.
    
    Now, pt_regs is quite large, so this is undesirable. Furthermore it
    turns out that most sites actually have a pt_regs pointer available,
    making this even more onerous, as the stack space is pointless waste.
    
    This patch addresses the problem by observing that software events
    have well defined nesting semantics, therefore we can use static
    per-cpu storage instead of on-stack.
    
    Linus made the further observation that all but the scheduler callers
    of perf_sw_event() have a pt_regs available, so we change the regular
    perf_sw_event() to require a valid pt_regs (where it used to be
    optional) and add perf_sw_event_sched() for the scheduler.
    
    We have a scheduler specific call instead of a more generic _noregs()
    like construct because we can assume non-recursion from the scheduler
    and thereby simplify the code further (_noregs would have to put the
    recursion context call inline in order to assertain which __perf_regs
    element to use).
    
    One last note on the implementation of perf_trace_buf_prepare(); we
    allow .regs = NULL for those cases where we already have a pt_regs
    pointer available and do not need another.
    
    Reported-by: Linus Torvalds <torvalds@linux-foundation.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Javi Merino <javi.merino@arm.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Petr Mladek <pmladek@suse.cz>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Link: http://lkml.kernel.org/r/20141216115041.GW3337@twins.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 4b9c114ee9de..6fa484de2ba1 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -261,7 +261,7 @@ void perf_trace_del(struct perf_event *p_event, int flags)
 }
 
 void *perf_trace_buf_prepare(int size, unsigned short type,
-			     struct pt_regs *regs, int *rctxp)
+			     struct pt_regs **regs, int *rctxp)
 {
 	struct trace_entry *entry;
 	unsigned long flags;
@@ -280,6 +280,8 @@ void *perf_trace_buf_prepare(int size, unsigned short type,
 	if (*rctxp < 0)
 		return NULL;
 
+	if (regs)
+		*regs = this_cpu_ptr(&__perf_regs[*rctxp]);
 	raw_data = this_cpu_ptr(perf_trace_buf[*rctxp]);
 
 	/* zero the dead bytes from align to not leak stack to user */

commit f4be073db878d0e79f74bc36f1642847781791a0
Author: Jiri Olsa <jolsa@kernel.org>
Date:   Wed Jul 16 14:33:29 2014 +0200

    perf: Check permission only for parent tracepoint event
    
    There's no need to check cloned event's permission once the
    parent was already checked.
    
    Also the code is checking 'current' process permissions, which
    is not owner process for cloned events, thus could end up with
    wrong permission check result.
    
    Reported-by: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
    Tested-by: Alexander Yarygin <yarygin@linux.vnet.ibm.com>
    Signed-off-by: Jiri Olsa <jolsa@kernel.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Arnaldo Carvalho de Melo <acme@kernel.org>
    Cc: Corey Ashford <cjashfor@linux.vnet.ibm.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/1405079782-8139-1-git-send-email-jolsa@kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 5d12bb407b44..4b9c114ee9de 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -30,6 +30,18 @@ static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 			return ret;
 	}
 
+	/*
+	 * We checked and allowed to create parent,
+	 * allow children without checking.
+	 */
+	if (p_event->parent)
+		return 0;
+
+	/*
+	 * It's ok to check current process (owner) permissions in here,
+	 * because code below is called only via perf_event_open syscall.
+	 */
+
 	/* The ftrace function trace is allowed only for root. */
 	if (ftrace_event_is_function(tp_event)) {
 		if (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))

commit 3da0f18007e5b87b573cf6ae8c445d59e757d274
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu Apr 17 17:18:28 2014 +0900

    kprobes, ftrace: Use NOKPROBE_SYMBOL macro in ftrace
    
    Use NOKPROBE_SYMBOL macro to protect functions from
    kprobes instead of __kprobes annotation in ftrace.
    This applies nokprobe_inline annotation for some cases,
    because NOKPROBE_SYMBOL() will inhibit inlining by
    referring the symbol address.
    
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20140417081828.26341.55152.stgit@ltc230.yrl.intra.hitachi.co.jp
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index c894614de14d..5d12bb407b44 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -248,8 +248,8 @@ void perf_trace_del(struct perf_event *p_event, int flags)
 	tp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event);
 }
 
-__kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
-				       struct pt_regs *regs, int *rctxp)
+void *perf_trace_buf_prepare(int size, unsigned short type,
+			     struct pt_regs *regs, int *rctxp)
 {
 	struct trace_entry *entry;
 	unsigned long flags;
@@ -281,6 +281,7 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	return raw_data;
 }
 EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);
+NOKPROBE_SYMBOL(perf_trace_buf_prepare);
 
 #ifdef CONFIG_FUNCTION_TRACER
 static void

commit 63c45f4ba533e9749da16298db53e491c25d805b
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Sun Mar 2 16:56:39 2014 +0100

    perf: Disallow user-space stack dumps for function trace events
    
    Recent issues with user space callchains processing within
    page fault handler tracing showed as Peter said 'there's
    just too much fail surface'.
    
    The user space stack dump is just another source of the this issue.
    
    Related list discussions:
      http://marc.info/?t=139302086500001&r=1&w=2
      http://marc.info/?t=139301437300003&r=1&w=2
    
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1393775800-13524-3-git-send-email-jolsa@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index d5e01c3f4e69..c894614de14d 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -42,6 +42,13 @@ static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 		 */
 		if (!p_event->attr.exclude_callchain_user)
 			return -EINVAL;
+
+		/*
+		 * Same reason to disable user stack dump as for user space
+		 * callchains above.
+		 */
+		if (p_event->attr.sample_type & PERF_SAMPLE_STACK_USER)
+			return -EINVAL;
 	}
 
 	/* No tracing, just counting, so no obvious leak */

commit cfa77bc4af2c75c0781ee76cde2dd104c6c8e2b7
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Sun Mar 2 16:56:38 2014 +0100

    perf: Disallow user-space callchains for function trace events
    
    Recent issues with user space callchains processing within
    page fault handler tracing showed as Peter said 'there's
    just too much fail surface'.
    
    Related list discussions:
    
      http://marc.info/?t=139302086500001&r=1&w=2
      http://marc.info/?t=139301437300003&r=1&w=2
    
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: "H. Peter Anvin" <hpa@zytor.com>
    Cc: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1393775800-13524-2-git-send-email-jolsa@redhat.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index e854f420e033..d5e01c3f4e69 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -31,9 +31,18 @@ static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 	}
 
 	/* The ftrace function trace is allowed only for root. */
-	if (ftrace_event_is_function(tp_event) &&
-	    perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
-		return -EPERM;
+	if (ftrace_event_is_function(tp_event)) {
+		if (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
+			return -EPERM;
+
+		/*
+		 * We don't allow user space callchains for  function trace
+		 * event, due to issues with page faults while tracing page
+		 * fault handler and its overall trickiness nature.
+		 */
+		if (!p_event->attr.exclude_callchain_user)
+			return -EINVAL;
+	}
 
 	/* No tracing, just counting, so no obvious leak */
 	if (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))

commit 0022cedd4a7d8a87841351e2b018bb6794cf2e67
Author: Vince Weaver <vincent.weaver@maine.edu>
Date:   Fri Nov 15 12:39:45 2013 -0500

    perf/trace: Properly use u64 to hold event_id
    
    The 64-bit attr.config value for perf trace events was being copied into
    an "int" before doing a comparison, meaning the top 32 bits were
    being truncated.
    
    As far as I can tell this didn't cause any errors, but it did mean
    it was possible to create valid aliases for all the tracepoint ids
    which I don't think was intended.  (For example, 0xffffffff00000018
    and 0x18 both enable the same tracepoint).
    
    Signed-off-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/alpine.DEB.2.10.1311151236100.11932@vincent-weaver-1.um.maine.edu
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 630889f68b1d..e854f420e033 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -179,7 +179,7 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 int perf_trace_init(struct perf_event *p_event)
 {
 	struct ftrace_event_call *tp_event;
-	int event_id = p_event->attr.config;
+	u64 event_id = p_event->attr.config;
 	int ret = -EINVAL;
 
 	mutex_lock(&event_mutex);

commit d5b5f391d434c5cc8bcb1ab2d759738797b85f52
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Nov 14 16:23:04 2013 +0100

    ftrace, perf: Avoid infinite event generation loop
    
    Vince's perf-trinity fuzzer found yet another 'interesting' problem.
    
    When we sample the irq_work_exit tracepoint with period==1 (or
    PERF_SAMPLE_PERIOD) and we add an fasync SIGNAL handler we create an
    infinite event generation loop:
    
      ,-> <IPI>
      |     irq_work_exit() ->
      |       trace_irq_work_exit() ->
      |         ...
      |           __perf_event_overflow() -> (due to fasync)
      |             irq_work_queue() -> (irq_work_list must be empty)
      '---------      arch_irq_work_raise()
    
    Similar things can happen due to regular poll() wakeups if we exceed
    the ring-buffer wakeup watermark, or have an event_limit.
    
    To avoid this, dis-allow sampling this particular tracepoint.
    
    In order to achieve this, create a special perf_perm function pointer
    for each event and call this (when set) on trying to create a
    tracepoint perf event.
    
    [ roasted: use expr... to allow for ',' in your expression ]
    
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Tested-by: Vince Weaver <vincent.weaver@maine.edu>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/20131114152304.GC5364@laptop.programming.kicks-ass.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 78e27e3b52ac..630889f68b1d 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -24,6 +24,12 @@ static int	total_ref_count;
 static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 				 struct perf_event *p_event)
 {
+	if (tp_event->perf_perm) {
+		int ret = tp_event->perf_perm(tp_event, p_event);
+		if (ret)
+			return ret;
+	}
+
 	/* The ftrace function trace is allowed only for root. */
 	if (ftrace_event_is_function(tp_event) &&
 	    perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))

commit 12ae030d54ef250706da5642fc7697cc60ad0df7
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Nov 5 12:51:11 2013 -0500

    perf/ftrace: Fix paranoid level for enabling function tracer
    
    The current default perf paranoid level is "1" which has
    "perf_paranoid_kernel()" return false, and giving any operations that
    use it, access to normal users. Unfortunately, this includes function
    tracing and normal users should not be allowed to enable function
    tracing by default.
    
    The proper level is defined at "-1" (full perf access), which
    "perf_paranoid_tracepoint_raw()" will only give access to. Use that
    check instead for enabling function tracing.
    
    Reported-by: Dave Jones <davej@redhat.com>
    Reported-by: Vince Weaver <vincent.weaver@maine.edu>
    Tested-by: Vince Weaver <vincent.weaver@maine.edu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ingo Molnar <mingo@kernel.org>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: stable@vger.kernel.org # 3.4+
    CVE: CVE-2013-2930
    Fixes: ced39002f5ea ("ftrace, perf: Add support to use function tracepoint in perf")
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 80c36bcf66e8..78e27e3b52ac 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -26,7 +26,7 @@ static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 {
 	/* The ftrace function trace is allowed only for root. */
 	if (ftrace_event_is_function(tp_event) &&
-	    perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
+	    perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
 		return -EPERM;
 
 	/* No tracing, just counting, so no obvious leak */

commit cd92bf61d6d70bd3eb33b46d600e3f3eb9c5778a
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jun 17 19:02:11 2013 +0200

    tracing/perf: Move the PERF_MAX_TRACE_SIZE check into perf_trace_buf_prepare()
    
    Every perf_trace_buf_prepare() caller does
    WARN_ONCE(size > PERF_MAX_TRACE_SIZE, message) and "message" is
    almost the same.
    
    Shift this WARN_ONCE() into perf_trace_buf_prepare(). This changes
    the meaning of _ONCE, but I think this is fine.
    
            - 4947014 2932448 10104832  17984294  1126b26 vmlinux
            + 4948422 2932448 10104832  17985702  11270a6 vmlinux
    
    on my build.
    
    Link: http://lkml.kernel.org/r/20130617170211.GA19813@redhat.com
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 12df5573086e..80c36bcf66e8 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -236,6 +236,10 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 
 	BUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));
 
+	if (WARN_ONCE(size > PERF_MAX_TRACE_SIZE,
+			"perf buffer not large enough"))
+		return NULL;
+
 	pc = preempt_count();
 
 	*rctxp = perf_swevent_get_recursion_context();

commit b8ebfd3f7113b63dda93d76bfec638c00e6bd514
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jun 17 19:02:04 2013 +0200

    tracing/function: Avoid perf_trace_buf_*() if event_function.perf_events is empty
    
    perf_trace_buf_prepare() + perf_trace_buf_submit(head, task => NULL)
    make no sense if hlist_empty(head). Change perf_ftrace_function_call()
    to check event_function.perf_events beforehand.
    
    Link: http://lkml.kernel.org/r/20130617170204.GA19803@redhat.com
    
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 84b1e045faba..12df5573086e 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -266,6 +266,10 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 	struct pt_regs regs;
 	int rctx;
 
+	head = this_cpu_ptr(event_function.perf_events);
+	if (hlist_empty(head))
+		return;
+
 #define ENTRY_SIZE (ALIGN(sizeof(struct ftrace_entry) + sizeof(u32), \
 		    sizeof(u64)) - sizeof(u32))
 
@@ -279,8 +283,6 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
 
 	entry->ip = ip;
 	entry->parent_ip = parent_ip;
-
-	head = this_cpu_ptr(event_function.perf_events);
 	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, 0,
 			      1, &regs, head, NULL);
 

commit bcada3d4b8c96b8792c2306f363992ca5ab9da42
Merge: 26198c21d1b2 000078bc3ee6
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Aug 21 11:27:00 2012 +0200

    Merge tag 'perf-core-for-mingo' of git://git.kernel.org/pub/scm/linux/kernel/git/acme/linux into perf/core
    
    Pull perf/core improvements and fixes from Arnaldo Carvalho de Melo:
    
     * Fix include order for bison/flex-generated C files, from Ben Hutchings
    
     * Build fixes and documentation corrections from David Ahern
    
     * Group parsing support, from Jiri Olsa
    
     * UI/gtk refactorings and improvements from Namhyung Kim
    
     * NULL deref fix for perf script, from Namhyung Kim
    
     * Assorted cleanups from Robert Richter
    
     * Let O= makes handle relative paths, from Steven Rostedt
    
     * perf script python fixes, from Feng Tang.
    
     * Improve 'perf lock' error message when the needed tracepoints
       are not present, from David Ahern.
    
     * Initial bash completion support, from Frederic Weisbecker
    
     * Allow building without libelf, from Namhyung Kim.
    
     * Support DWARF CFI based unwind to have callchains when %bp
       based unwinding is not possible, from Jiri Olsa.
    
     * Symbol resolution fixes, while fixing support PPC64 files with an .opt ELF
       section was the end goal, several fixes for code that handles all
       architectures and cleanups are included, from Cody Schafer.
    
     * Add a description for the JIT interface, from Andi Kleen.
    
     * Assorted fixes for Documentation and build in 32 bit, from Robert Richter
    
     * Add support for non-tracepoint events in perf script python, from Feng Tang
    
     * Cache the libtraceevent event_format associated to each evsel early, so that we
       avoid relookups, i.e. calling pevent_find_event repeatedly when processing
       tracepoint events.
    
       [ This is to reduce the surface contact with libtraceevents and make clear what
         is that the perf tools needs from that lib: so far parsing the common and per
         event fields. ]
    
    Signed-off-by: Arnaldo Carvalho de Melo <acme@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit e6dab5ffab59e910ec0e3355f4a6f29f7a7be474
Author: Andrew Vagin <avagin@openvz.org>
Date:   Wed Jul 11 18:14:58 2012 +0400

    perf/trace: Add ability to set a target task for events
    
    A few events are interesting not only for a current task.
    For example, sched_stat_* events are interesting for a task
    which wakes up. For this reason, it will be good if such
    events will be delivered to a target task too.
    
    Now a target task can be set by using __perf_task().
    
    The original idea and a draft patch belongs to Peter Zijlstra.
    
    I need these events for profiling sleep times. sched_switch is used for
    getting callchains and sched_stat_* is used for getting time periods.
    These events are combined in user space, then it can be analyzed by
    perf tools.
    
    Inspired-by: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@ghostprotocols.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Arun Sharma <asharma@fb.com>
    Signed-off-by: Andrew Vagin <avagin@openvz.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Link: http://lkml.kernel.org/r/1342016098-213063-1-git-send-email-avagin@openvz.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index fee3752ae8f6..8a6d2ee2086c 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -281,7 +281,7 @@ perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip)
 
 	head = this_cpu_ptr(event_function.perf_events);
 	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, 0,
-			      1, &regs, head);
+			      1, &regs, head, NULL);
 
 #undef ENTRY_SIZE
 }

commit a1e2e31d175a1349274eba3465d17616c6725f8c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 9 12:50:46 2011 -0400

    ftrace: Return pt_regs to function trace callback
    
    Return as the 4th paramater to the function tracer callback the pt_regs.
    
    Later patches that implement regs passing for the architectures will require
    having the ftrace_ops set the SAVE_REGS flag, which will tell the arch
    to take the time to pass a full set of pt_regs to the ftrace_ops callback
    function. If the arch does not support it then it should pass NULL.
    
    If an arch can pass full regs, then it should define:
     ARCH_SUPPORTS_FTRACE_SAVE_REGS to 1
    
    Link: http://lkml.kernel.org/r/20120702201821.019966811@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index a872a9a298a0..9824419c8404 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -259,7 +259,7 @@ EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);
 #ifdef CONFIG_FUNCTION_TRACER
 static void
 perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
-			  struct ftrace_ops *ops)
+			  struct ftrace_ops *ops, struct pt_regs *pt_regs)
 {
 	struct ftrace_entry *entry;
 	struct hlist_head *head;

commit 2f5f6ad9390c1ebbf738d130dbfe80b60eaa167e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 8 16:57:47 2011 -0400

    ftrace: Pass ftrace_ops as third parameter to function trace callback
    
    Currently the function trace callback receives only the ip and parent_ip
    of the function that it traced. It would be more powerful to also return
    the ops that registered the function as well. This allows the same function
    to act differently depending on what ftrace_ops registered it.
    
    Link: http://lkml.kernel.org/r/20120612225424.267254552@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index fee3752ae8f6..a872a9a298a0 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -258,7 +258,8 @@ EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);
 
 #ifdef CONFIG_FUNCTION_TRACER
 static void
-perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip)
+perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip,
+			  struct ftrace_ops *ops)
 {
 	struct ftrace_entry *entry;
 	struct hlist_head *head;

commit 5500fa51199aee770ce53718853732600543619e
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:54 2012 +0100

    ftrace, perf: Add filter support for function trace event
    
    Adding support to filter function trace event via perf
    interface. It is now possible to use filter interface
    in the perf tool like:
    
      perf record -e ftrace:function --filter="(ip == mm_*)" ls
    
    The filter syntax is restricted to the the 'ip' field only,
    and following operators are accepted '==' '!=' '||', ending
    up with the filter strings like:
    
      ip == f1[, ]f2 ... || ip != f3[, ]f4 ...
    
    with comma ',' or space ' ' as a function separator. If the
    space ' ' is used as a separator, the right side of the
    assignment needs to be enclosed in double quotes '"', e.g.:
    
      perf record -e ftrace:function --filter '(ip == do_execve,sys_*,ext*)' ls
      perf record -e ftrace:function --filter '(ip == "do_execve,sys_*,ext*")' ls
      perf record -e ftrace:function --filter '(ip == "do_execve sys_* ext*")' ls
    
    The '==' operator adds trace filter with same effect as would
    be added via set_ftrace_filter file.
    
    The '!=' operator adds trace filter with same effect as would
    be added via set_ftrace_notrace file.
    
    The right side of the '!=', '==' operators is list of functions
    or regexp. to be added to filter separated by space.
    
    The '||' operator is used for connecting multiple filter definitions
    together. It is possible to have more than one '==' and '!='
    operators within one filter string.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-8-git-send-email-jolsa@redhat.com
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index fdeeb5c49627..fee3752ae8f6 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -298,7 +298,9 @@ static int perf_ftrace_function_register(struct perf_event *event)
 static int perf_ftrace_function_unregister(struct perf_event *event)
 {
 	struct ftrace_ops *ops = &event->ftrace_ops;
-	return unregister_ftrace_function(ops);
+	int ret = unregister_ftrace_function(ops);
+	ftrace_free_filter(ops);
+	return ret;
 }
 
 static void perf_ftrace_function_enable(struct perf_event *event)

commit ced39002f5ea736b716ae233fb68b26d59783912
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:52 2012 +0100

    ftrace, perf: Add support to use function tracepoint in perf
    
    Adding perf registration support for the ftrace function event,
    so it is now possible to register it via perf interface.
    
    The perf_event struct statically contains ftrace_ops as a handle
    for function tracer. The function tracer is registered/unregistered
    in open/close actions.
    
    To be efficient, we enable/disable ftrace_ops each time the traced
    process is scheduled in/out (via TRACE_REG_PERF_(ADD|DELL) handlers).
    This way tracing is enabled only when the process is running.
    Intentionally using this way instead of the event's hw state
    PERF_HES_STOPPED, which would not disable the ftrace_ops.
    
    It is now possible to use function trace within perf commands
    like:
    
      perf record -e ftrace:function ls
      perf stat -e ftrace:function ls
    
    Allowed only for root.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-6-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index d72af0b03822..fdeeb5c49627 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -24,6 +24,11 @@ static int	total_ref_count;
 static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 				 struct perf_event *p_event)
 {
+	/* The ftrace function trace is allowed only for root. */
+	if (ftrace_event_is_function(tp_event) &&
+	    perf_paranoid_kernel() && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
 	/* No tracing, just counting, so no obvious leak */
 	if (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))
 		return 0;
@@ -250,3 +255,84 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	return raw_data;
 }
 EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);
+
+#ifdef CONFIG_FUNCTION_TRACER
+static void
+perf_ftrace_function_call(unsigned long ip, unsigned long parent_ip)
+{
+	struct ftrace_entry *entry;
+	struct hlist_head *head;
+	struct pt_regs regs;
+	int rctx;
+
+#define ENTRY_SIZE (ALIGN(sizeof(struct ftrace_entry) + sizeof(u32), \
+		    sizeof(u64)) - sizeof(u32))
+
+	BUILD_BUG_ON(ENTRY_SIZE > PERF_MAX_TRACE_SIZE);
+
+	perf_fetch_caller_regs(&regs);
+
+	entry = perf_trace_buf_prepare(ENTRY_SIZE, TRACE_FN, NULL, &rctx);
+	if (!entry)
+		return;
+
+	entry->ip = ip;
+	entry->parent_ip = parent_ip;
+
+	head = this_cpu_ptr(event_function.perf_events);
+	perf_trace_buf_submit(entry, ENTRY_SIZE, rctx, 0,
+			      1, &regs, head);
+
+#undef ENTRY_SIZE
+}
+
+static int perf_ftrace_function_register(struct perf_event *event)
+{
+	struct ftrace_ops *ops = &event->ftrace_ops;
+
+	ops->flags |= FTRACE_OPS_FL_CONTROL;
+	ops->func = perf_ftrace_function_call;
+	return register_ftrace_function(ops);
+}
+
+static int perf_ftrace_function_unregister(struct perf_event *event)
+{
+	struct ftrace_ops *ops = &event->ftrace_ops;
+	return unregister_ftrace_function(ops);
+}
+
+static void perf_ftrace_function_enable(struct perf_event *event)
+{
+	ftrace_function_local_enable(&event->ftrace_ops);
+}
+
+static void perf_ftrace_function_disable(struct perf_event *event)
+{
+	ftrace_function_local_disable(&event->ftrace_ops);
+}
+
+int perf_ftrace_event_register(struct ftrace_event_call *call,
+			       enum trace_reg type, void *data)
+{
+	switch (type) {
+	case TRACE_REG_REGISTER:
+	case TRACE_REG_UNREGISTER:
+		break;
+	case TRACE_REG_PERF_REGISTER:
+	case TRACE_REG_PERF_UNREGISTER:
+		return 0;
+	case TRACE_REG_PERF_OPEN:
+		return perf_ftrace_function_register(data);
+	case TRACE_REG_PERF_CLOSE:
+		return perf_ftrace_function_unregister(data);
+	case TRACE_REG_PERF_ADD:
+		perf_ftrace_function_enable(data);
+		return 0;
+	case TRACE_REG_PERF_DEL:
+		perf_ftrace_function_disable(data);
+		return 0;
+	}
+
+	return -EINVAL;
+}
+#endif /* CONFIG_FUNCTION_TRACER */

commit 489c75c3b333dfda4c8d2b7ad1b00e5da024bfa7
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:50 2012 +0100

    ftrace, perf: Add add/del tracepoint perf registration actions
    
    Adding TRACE_REG_PERF_ADD and TRACE_REG_PERF_DEL to handle
    perf event schedule in/out actions.
    
    The add action is invoked for when the perf event is scheduled in,
    while the del action is invoked when the event is scheduled out.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-4-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 0cfcc37f63de..d72af0b03822 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -211,12 +211,14 @@ int perf_trace_add(struct perf_event *p_event, int flags)
 	list = this_cpu_ptr(pcpu_list);
 	hlist_add_head_rcu(&p_event->hlist_entry, list);
 
-	return 0;
+	return tp_event->class->reg(tp_event, TRACE_REG_PERF_ADD, p_event);
 }
 
 void perf_trace_del(struct perf_event *p_event, int flags)
 {
+	struct ftrace_event_call *tp_event = p_event->tp_event;
 	hlist_del_rcu(&p_event->hlist_entry);
+	tp_event->class->reg(tp_event, TRACE_REG_PERF_DEL, p_event);
 }
 
 __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,

commit ceec0b6fc7cd43b38a40c2d40223f9cd0616f0cd
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:49 2012 +0100

    ftrace, perf: Add open/close tracepoint perf registration actions
    
    Adding TRACE_REG_PERF_OPEN and TRACE_REG_PERF_CLOSE to differentiate
    register/unregister from open/close actions.
    
    The register/unregister actions are invoked for the first/last
    tracepoint user when opening/closing the event.
    
    The open/close actions are invoked for each tracepoint user when
    opening/closing the event.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-3-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 19a359d5e6d5..0cfcc37f63de 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -44,23 +44,17 @@ static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
 	return 0;
 }
 
-static int perf_trace_event_init(struct ftrace_event_call *tp_event,
-				 struct perf_event *p_event)
+static int perf_trace_event_reg(struct ftrace_event_call *tp_event,
+				struct perf_event *p_event)
 {
 	struct hlist_head __percpu *list;
-	int ret;
+	int ret = -ENOMEM;
 	int cpu;
 
-	ret = perf_trace_event_perm(tp_event, p_event);
-	if (ret)
-		return ret;
-
 	p_event->tp_event = tp_event;
 	if (tp_event->perf_refcount++ > 0)
 		return 0;
 
-	ret = -ENOMEM;
-
 	list = alloc_percpu(struct hlist_head);
 	if (!list)
 		goto fail;
@@ -83,7 +77,7 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 		}
 	}
 
-	ret = tp_event->class->reg(tp_event, TRACE_REG_PERF_REGISTER);
+	ret = tp_event->class->reg(tp_event, TRACE_REG_PERF_REGISTER, NULL);
 	if (ret)
 		goto fail;
 
@@ -108,6 +102,69 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 	return ret;
 }
 
+static void perf_trace_event_unreg(struct perf_event *p_event)
+{
+	struct ftrace_event_call *tp_event = p_event->tp_event;
+	int i;
+
+	if (--tp_event->perf_refcount > 0)
+		goto out;
+
+	tp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER, NULL);
+
+	/*
+	 * Ensure our callback won't be called anymore. The buffers
+	 * will be freed after that.
+	 */
+	tracepoint_synchronize_unregister();
+
+	free_percpu(tp_event->perf_events);
+	tp_event->perf_events = NULL;
+
+	if (!--total_ref_count) {
+		for (i = 0; i < PERF_NR_CONTEXTS; i++) {
+			free_percpu(perf_trace_buf[i]);
+			perf_trace_buf[i] = NULL;
+		}
+	}
+out:
+	module_put(tp_event->mod);
+}
+
+static int perf_trace_event_open(struct perf_event *p_event)
+{
+	struct ftrace_event_call *tp_event = p_event->tp_event;
+	return tp_event->class->reg(tp_event, TRACE_REG_PERF_OPEN, p_event);
+}
+
+static void perf_trace_event_close(struct perf_event *p_event)
+{
+	struct ftrace_event_call *tp_event = p_event->tp_event;
+	tp_event->class->reg(tp_event, TRACE_REG_PERF_CLOSE, p_event);
+}
+
+static int perf_trace_event_init(struct ftrace_event_call *tp_event,
+				 struct perf_event *p_event)
+{
+	int ret;
+
+	ret = perf_trace_event_perm(tp_event, p_event);
+	if (ret)
+		return ret;
+
+	ret = perf_trace_event_reg(tp_event, p_event);
+	if (ret)
+		return ret;
+
+	ret = perf_trace_event_open(p_event);
+	if (ret) {
+		perf_trace_event_unreg(p_event);
+		return ret;
+	}
+
+	return 0;
+}
+
 int perf_trace_init(struct perf_event *p_event)
 {
 	struct ftrace_event_call *tp_event;
@@ -130,6 +187,14 @@ int perf_trace_init(struct perf_event *p_event)
 	return ret;
 }
 
+void perf_trace_destroy(struct perf_event *p_event)
+{
+	mutex_lock(&event_mutex);
+	perf_trace_event_close(p_event);
+	perf_trace_event_unreg(p_event);
+	mutex_unlock(&event_mutex);
+}
+
 int perf_trace_add(struct perf_event *p_event, int flags)
 {
 	struct ftrace_event_call *tp_event = p_event->tp_event;
@@ -154,37 +219,6 @@ void perf_trace_del(struct perf_event *p_event, int flags)
 	hlist_del_rcu(&p_event->hlist_entry);
 }
 
-void perf_trace_destroy(struct perf_event *p_event)
-{
-	struct ftrace_event_call *tp_event = p_event->tp_event;
-	int i;
-
-	mutex_lock(&event_mutex);
-	if (--tp_event->perf_refcount > 0)
-		goto out;
-
-	tp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER);
-
-	/*
-	 * Ensure our callback won't be called anymore. The buffers
-	 * will be freed after that.
-	 */
-	tracepoint_synchronize_unregister();
-
-	free_percpu(tp_event->perf_events);
-	tp_event->perf_events = NULL;
-
-	if (!--total_ref_count) {
-		for (i = 0; i < PERF_NR_CONTEXTS; i++) {
-			free_percpu(perf_trace_buf[i]);
-			perf_trace_buf[i] = NULL;
-		}
-	}
-out:
-	module_put(tp_event->mod);
-	mutex_unlock(&event_mutex);
-}
-
 __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 				       struct pt_regs *regs, int *rctxp)
 {

commit 61c32659b12c44e62de32fbf99f7e4ca783dc38b
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Nov 18 01:39:17 2010 +0100

    tracing: New flag to allow non privileged users to use a trace event
    
    This adds a new trace event internal flag that allows them to be
    used in perf by non privileged users in case of task bound tracing.
    
    This is desired for syscalls tracepoint because they don't leak
    global system informations, like some other tracepoints.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Jason Baron <jbaron@redhat.com>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 39c059ca670e..19a359d5e6d5 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -21,17 +21,46 @@ typedef typeof(unsigned long [PERF_MAX_TRACE_SIZE / sizeof(unsigned long)])
 /* Count the events in use (per event id, not per instance) */
 static int	total_ref_count;
 
+static int perf_trace_event_perm(struct ftrace_event_call *tp_event,
+				 struct perf_event *p_event)
+{
+	/* No tracing, just counting, so no obvious leak */
+	if (!(p_event->attr.sample_type & PERF_SAMPLE_RAW))
+		return 0;
+
+	/* Some events are ok to be traced by non-root users... */
+	if (p_event->attach_state == PERF_ATTACH_TASK) {
+		if (tp_event->flags & TRACE_EVENT_FL_CAP_ANY)
+			return 0;
+	}
+
+	/*
+	 * ...otherwise raw tracepoint data can be a severe data leak,
+	 * only allow root to have these.
+	 */
+	if (perf_paranoid_tracepoint_raw() && !capable(CAP_SYS_ADMIN))
+		return -EPERM;
+
+	return 0;
+}
+
 static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 				 struct perf_event *p_event)
 {
 	struct hlist_head __percpu *list;
-	int ret = -ENOMEM;
+	int ret;
 	int cpu;
 
+	ret = perf_trace_event_perm(tp_event, p_event);
+	if (ret)
+		return ret;
+
 	p_event->tp_event = tp_event;
 	if (tp_event->perf_refcount++ > 0)
 		return 0;
 
+	ret = -ENOMEM;
+
 	list = alloc_percpu(struct hlist_head);
 	if (!list)
 		goto fail;

commit a4eaf7f14675cb512d69f0c928055e73d0c6d252
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed Jun 16 14:37:10 2010 +0200

    perf: Rework the PMU methods
    
    Replace pmu::{enable,disable,start,stop,unthrottle} with
    pmu::{add,del,start,stop}, all of which take a flags argument.
    
    The new interface extends the capability to stop a counter while
    keeping it scheduled on the PMU. We replace the throttled state with
    the generic stopped state.
    
    This also allows us to efficiently stop/start counters over certain
    code paths (like IRQ handlers).
    
    It also allows scheduling a counter without it starting, allowing for
    a generic frozen state (useful for rotating stopped counters).
    
    The stopped state is implemented in two different ways, depending on
    how the architecture implemented the throttled state:
    
     1) We disable the counter:
        a) the pmu has per-counter enable bits, we flip that
        b) we program a NOP event, preserving the counter state
    
     2) We store the counter state and ignore all read/overflow events
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: paulus <paulus@samba.org>
    Cc: stephane eranian <eranian@googlemail.com>
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Will Deacon <will.deacon@arm.com>
    Cc: Paul Mundt <lethal@linux-sh.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Cyrill Gorcunov <gorcunov@gmail.com>
    Cc: Lin Ming <ming.m.lin@intel.com>
    Cc: Yanmin <yanmin_zhang@linux.intel.com>
    Cc: Deng-Cheng Zhu <dengcheng.zhu@gmail.com>
    Cc: David Miller <davem@davemloft.net>
    Cc: Michael Cree <mcree@orcon.net.nz>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index f3bbcd1c90c8..39c059ca670e 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -101,7 +101,7 @@ int perf_trace_init(struct perf_event *p_event)
 	return ret;
 }
 
-int perf_trace_enable(struct perf_event *p_event)
+int perf_trace_add(struct perf_event *p_event, int flags)
 {
 	struct ftrace_event_call *tp_event = p_event->tp_event;
 	struct hlist_head __percpu *pcpu_list;
@@ -111,13 +111,16 @@ int perf_trace_enable(struct perf_event *p_event)
 	if (WARN_ON_ONCE(!pcpu_list))
 		return -EINVAL;
 
+	if (!(flags & PERF_EF_START))
+		p_event->hw.state = PERF_HES_STOPPED;
+
 	list = this_cpu_ptr(pcpu_list);
 	hlist_add_head_rcu(&p_event->hlist_entry, list);
 
 	return 0;
 }
 
-void perf_trace_disable(struct perf_event *p_event)
+void perf_trace_del(struct perf_event *p_event, int flags)
 {
 	hlist_del_rcu(&p_event->hlist_entry);
 }

commit 2aa61274efb9f532deaebc9812675a27af1994cb
Merge: 359d5106a2ff 5e11637e2c92
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Sep 9 20:40:06 2010 +0200

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: Pick up pending fixes before applying dependent new changes.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 9cb627d5f38830ca19aa0dca52d1d3a633018bf7
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Sep 1 12:58:43 2010 +0200

    perf, trace: Fix module leak
    
    Commit 1c024eca (perf, trace: Optimize tracepoints by using
    per-tracepoint-per-cpu hlist to track events) caused a module
    refcount leak.
    
    Reported-And-Tested-by: Avi Kivity <avi@redhat.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4C7E1F12.8030304@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 000e6e85b445..31cc4cb0dbf2 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -91,6 +91,8 @@ int perf_trace_init(struct perf_event *p_event)
 		    tp_event->class && tp_event->class->reg &&
 		    try_module_get(tp_event->mod)) {
 			ret = perf_trace_event_init(tp_event, p_event);
+			if (ret)
+				module_put(tp_event->mod);
 			break;
 		}
 	}
@@ -146,6 +148,7 @@ void perf_trace_destroy(struct perf_event *p_event)
 		}
 	}
 out:
+	module_put(tp_event->mod);
 	mutex_unlock(&event_mutex);
 }
 

commit 6016ee13db518ab1cd0cbf43fc2ad5712021e338
Author: Namhyung Kim <namhyung@gmail.com>
Date:   Wed Aug 11 12:47:59 2010 +0900

    perf, tracing: add missing __percpu markups
    
    ftrace_event_call->perf_events, perf_trace_buf,
    fgraph_data->cpu_data and some local variables are percpu pointers
    missing __percpu markups. Add them.
    
    Signed-off-by: Namhyung Kim <namhyung@gmail.com>
    Acked-by: Tejun Heo <tj@kernel.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>
    LKML-Reference: <1281498479-28551-1-git-send-email-namhyung@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index db2eae2efcf2..92f5477a006a 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -9,7 +9,7 @@
 #include <linux/kprobes.h>
 #include "trace.h"
 
-static char *perf_trace_buf[PERF_NR_CONTEXTS];
+static char __percpu *perf_trace_buf[PERF_NR_CONTEXTS];
 
 /*
  * Force it to be aligned to unsigned long to avoid misaligned accesses
@@ -24,7 +24,7 @@ static int	total_ref_count;
 static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 				 struct perf_event *p_event)
 {
-	struct hlist_head *list;
+	struct hlist_head __percpu *list;
 	int ret = -ENOMEM;
 	int cpu;
 
@@ -42,11 +42,11 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 	tp_event->perf_events = list;
 
 	if (!total_ref_count) {
-		char *buf;
+		char __percpu *buf;
 		int i;
 
 		for (i = 0; i < PERF_NR_CONTEXTS; i++) {
-			buf = (char *)alloc_percpu(perf_trace_t);
+			buf = (char __percpu *)alloc_percpu(perf_trace_t);
 			if (!buf)
 				goto fail;
 
@@ -102,13 +102,14 @@ int perf_trace_init(struct perf_event *p_event)
 int perf_trace_enable(struct perf_event *p_event)
 {
 	struct ftrace_event_call *tp_event = p_event->tp_event;
+	struct hlist_head __percpu *pcpu_list;
 	struct hlist_head *list;
 
-	list = tp_event->perf_events;
-	if (WARN_ON_ONCE(!list))
+	pcpu_list = tp_event->perf_events;
+	if (WARN_ON_ONCE(!pcpu_list))
 		return -EINVAL;
 
-	list = this_cpu_ptr(list);
+	list = this_cpu_ptr(pcpu_list);
 	hlist_add_head_rcu(&p_event->hlist_entry, list);
 
 	return 0;

commit 7ae07ea3a48d30689ee037cb136bc21f0b37d8ae
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sat Aug 14 20:45:13 2010 +0200

    perf: Humanize the number of contexts
    
    Instead of hardcoding the number of contexts for the recursions
    barriers, define a cpp constant to make the code more
    self-explanatory.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Stephane Eranian <eranian@google.com>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 000e6e85b445..db2eae2efcf2 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -9,7 +9,7 @@
 #include <linux/kprobes.h>
 #include "trace.h"
 
-static char *perf_trace_buf[4];
+static char *perf_trace_buf[PERF_NR_CONTEXTS];
 
 /*
  * Force it to be aligned to unsigned long to avoid misaligned accesses
@@ -45,7 +45,7 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 		char *buf;
 		int i;
 
-		for (i = 0; i < 4; i++) {
+		for (i = 0; i < PERF_NR_CONTEXTS; i++) {
 			buf = (char *)alloc_percpu(perf_trace_t);
 			if (!buf)
 				goto fail;
@@ -65,7 +65,7 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 	if (!total_ref_count) {
 		int i;
 
-		for (i = 0; i < 4; i++) {
+		for (i = 0; i < PERF_NR_CONTEXTS; i++) {
 			free_percpu(perf_trace_buf[i]);
 			perf_trace_buf[i] = NULL;
 		}
@@ -140,7 +140,7 @@ void perf_trace_destroy(struct perf_event *p_event)
 	tp_event->perf_events = NULL;
 
 	if (!--total_ref_count) {
-		for (i = 0; i < 4; i++) {
+		for (i = 0; i < PERF_NR_CONTEXTS; i++) {
 			free_percpu(perf_trace_buf[i]);
 			perf_trace_buf[i] = NULL;
 		}

commit 669336e4cf3e1cb95800f3f5924558a76d723c21
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Jul 20 17:29:54 2010 +0200

    perf: Use tracepoint_synchronize_unregister() to flush any pending tracepoint call
    
    We use synchronize_sched() to ensure a tracepoint won't be called
    while/after we release the perf buffers it references.
    
    But the tracepoint API has its own API for that:
    tracepoint_synchronize_unregister(). Use it instead as it's
    self-explanatory and eases maintainance.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 23751659582e..000e6e85b445 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -131,10 +131,10 @@ void perf_trace_destroy(struct perf_event *p_event)
 	tp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER);
 
 	/*
-	 * Ensure our callback won't be called anymore. See
-	 * tracepoint_probe_unregister() and __DO_TRACE().
+	 * Ensure our callback won't be called anymore. The buffers
+	 * will be freed after that.
 	 */
-	synchronize_sched();
+	tracepoint_synchronize_unregister();
 
 	free_percpu(tp_event->perf_events);
 	tp_event->perf_events = NULL;

commit a1d0ce8213e9ddf4046ef5ba95c55762d075f541
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 8 11:22:06 2010 -0400

    tracing: Use class->reg() for all registering of events
    
    Because kprobes and syscalls need special processing to register
    events, the class->reg() method was created to handle the differences.
    
    But instead of creating a default ->reg for perf and ftrace events,
    the code was scattered with:
    
            if (class->reg)
                    class->reg();
            else
                    default_reg();
    
    This is messy and can also lead to bugs.
    
    This patch cleans up this code and creates a default reg() entry for
    the events allowing for the code to directly call the class->reg()
    without the condition.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 6053982dc302..23751659582e 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -54,13 +54,7 @@ static int perf_trace_event_init(struct ftrace_event_call *tp_event,
 		}
 	}
 
-	if (tp_event->class->reg)
-		ret = tp_event->class->reg(tp_event, TRACE_REG_PERF_REGISTER);
-	else
-		ret = tracepoint_probe_register(tp_event->name,
-						tp_event->class->perf_probe,
-						tp_event);
-
+	ret = tp_event->class->reg(tp_event, TRACE_REG_PERF_REGISTER);
 	if (ret)
 		goto fail;
 
@@ -94,9 +88,7 @@ int perf_trace_init(struct perf_event *p_event)
 	mutex_lock(&event_mutex);
 	list_for_each_entry(tp_event, &ftrace_events, list) {
 		if (tp_event->event.type == event_id &&
-		    tp_event->class &&
-		    (tp_event->class->perf_probe ||
-		     tp_event->class->reg) &&
+		    tp_event->class && tp_event->class->reg &&
 		    try_module_get(tp_event->mod)) {
 			ret = perf_trace_event_init(tp_event, p_event);
 			break;
@@ -136,12 +128,7 @@ void perf_trace_destroy(struct perf_event *p_event)
 	if (--tp_event->perf_refcount > 0)
 		goto out;
 
-	if (tp_event->class->reg)
-		tp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER);
-	else
-		tracepoint_probe_unregister(tp_event->name,
-					    tp_event->class->perf_probe,
-					    tp_event);
+	tp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER);
 
 	/*
 	 * Ensure our callback won't be called anymore. See

commit f384c954c9fe3d3c6fce5ae66b67f2ddd947d098
Merge: 9a15a07fe217 5904b3b81d25
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon Jun 28 22:33:13 2010 +0200

    Merge branch 'linus' into perf/core
    
    Reason: Further changes conflict with upstream fixes
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

commit a8fb2608053547bc3152ea61a5ec7cdfce5d942c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 10 14:53:16 2010 -0400

    perf/tracing: Fix regression of perf losing kprobe events
    
    With the addition of the code to shrink the kernel tracepoint
    infrastructure, we lost kprobes being traced by perf. The reason
    is that I tested if the "tp_event->class->perf_probe" existed before
    enabling it. This prevents "ftrace only" events (like the function
    trace events) from being enabled by perf.
    
    Unfortunately, kprobe events do not use perf_probe. This causes
    kprobes to be missed by perf. To fix this, we add the test to
    see if "tp_event->class->reg" exists as well as perf_probe.
    
    Normal trace events have only "perf_probe" but no "reg" function,
    and kprobes and syscalls have the "reg" but no "perf_probe".
    The ftrace unique events do not have either, so this is a valid
    test. If a kprobe or syscall is not to be probed by perf, the
    "reg" function is called anyway, and will return a failure and
    prevent perf from probing it.
    
    Reported-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Tested-by: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Acked-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index e6f65887842c..8a2b73f7c068 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -96,7 +96,9 @@ int perf_trace_init(struct perf_event *p_event)
 	mutex_lock(&event_mutex);
 	list_for_each_entry(tp_event, &ftrace_events, list) {
 		if (tp_event->event.type == event_id &&
-		    tp_event->class && tp_event->class->perf_probe &&
+		    tp_event->class &&
+		    (tp_event->class->perf_probe ||
+		     tp_event->class->reg) &&
 		    try_module_get(tp_event->mod)) {
 			ret = perf_trace_event_init(tp_event, p_event);
 			break;

commit c726b61c6a5acc54c55ed7a0e7638cc4c5a100a8
Merge: 7be7923633a1 018378c55b03
Author: Ingo Molnar <mingo@elte.hu>
Date:   Wed Jun 9 18:55:20 2010 +0200

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/frederic/random-tracing into perf/core

commit b0f82b81fe6bbcf78d478071f33e44554726bc81
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu May 20 07:47:21 2010 +0200

    perf: Drop the skip argument from perf_arch_fetch_regs_caller
    
    Drop this argument now that we always want to rewind only to the
    state of the first caller.
    It means frame pointers are not necessary anymore to reliably get
    the source of an event. But this also means we need this helper
    to be a macro now, as an inline function is not an option since
    we need to know when to provide a default implentation.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Paul Mackerras <paulus@samba.org>
    Cc: David Miller <davem@davemloft.net>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index cb6f365016e4..21db1d3a48d0 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -9,8 +9,6 @@
 #include <linux/kprobes.h>
 #include "trace.h"
 
-EXPORT_SYMBOL_GPL(perf_arch_fetch_caller_regs);
-
 static char *perf_trace_buf[4];
 
 /*

commit 2e97942fe57864588774f173cf4cd7bb68968b76
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 21 16:22:33 2010 +0200

    perf_events, trace: Fix perf_trace_destroy(), mutex went missing
    
    Steve spotted I forgot to do the destroy under event_mutex.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1274451913.1674.1707.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 49c7abf2ba5c..e6f65887842c 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -132,8 +132,9 @@ void perf_trace_destroy(struct perf_event *p_event)
 	struct ftrace_event_call *tp_event = p_event->tp_event;
 	int i;
 
+	mutex_lock(&event_mutex);
 	if (--tp_event->perf_refcount > 0)
-		return;
+		goto out;
 
 	if (tp_event->class->reg)
 		tp_event->class->reg(tp_event, TRACE_REG_PERF_UNREGISTER);
@@ -157,6 +158,8 @@ void perf_trace_destroy(struct perf_event *p_event)
 			perf_trace_buf[i] = NULL;
 		}
 	}
+out:
+	mutex_unlock(&event_mutex);
 }
 
 __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,

commit 3771f0771154675d4a0ca780be2411f3cc357208
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Fri May 21 12:31:09 2010 +0200

    perf_events, trace: Fix probe unregister race
    
    tracepoint_probe_unregister() does not synchronize against the probe
    callbacks, so do that explicitly. This properly serializes the callbacks
    and the free of the data used therein.
    
    Also, use this_cpu_ptr() where possible.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <1274438476.1674.1702.camel@laptop>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index cb6f365016e4..49c7abf2ba5c 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -116,7 +116,7 @@ int perf_trace_enable(struct perf_event *p_event)
 	if (WARN_ON_ONCE(!list))
 		return -EINVAL;
 
-	list = per_cpu_ptr(list, smp_processor_id());
+	list = this_cpu_ptr(list);
 	hlist_add_head_rcu(&p_event->hlist_entry, list);
 
 	return 0;
@@ -142,6 +142,12 @@ void perf_trace_destroy(struct perf_event *p_event)
 					    tp_event->class->perf_probe,
 					    tp_event);
 
+	/*
+	 * Ensure our callback won't be called anymore. See
+	 * tracepoint_probe_unregister() and __DO_TRACE().
+	 */
+	synchronize_sched();
+
 	free_percpu(tp_event->perf_events);
 	tp_event->perf_events = NULL;
 
@@ -169,7 +175,7 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	if (*rctxp < 0)
 		return NULL;
 
-	raw_data = per_cpu_ptr(perf_trace_buf[*rctxp], smp_processor_id());
+	raw_data = this_cpu_ptr(perf_trace_buf[*rctxp]);
 
 	/* zero the dead bytes from align to not leak stack to user */
 	memset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));

commit 87f44bbc246c5244c76a701f8eefba7788bce64a
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Tue May 25 11:02:55 2010 +0200

    perf, trace: Fix !x86 build bug
    
    Patch b7e2ecef92 (perf, trace: Optimize tracepoints by removing
    IRQ-disable from perf/tracepoint interaction) made the
    unfortunate mistake of assuming the world is x86 only, correct
    this.
    
    The problem was that perf_fetch_caller_regs() did
    local_save_flags() into regs->flags, and I re-used that to
    remove another local_save_flags(), forgetting !x86 doesn't have
    regs->flags.
    
    Do the reverse, remove the local_save_flags() from
    perf_fetch_caller_regs() and let the ftrace site do the
    local_save_flags() instead.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Acked-by: Paul Mackerras <paulus@samba.org>
    Cc: acme@redhat.com
    Cc: efault@gmx.de
    Cc: fweisbec@gmail.com
    Cc: rostedt@goodmis.org
    LKML-Reference: <1274778175.5882.623.camel@twins>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 26b8607a0abc..cb6f365016e4 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -157,6 +157,7 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 				       struct pt_regs *regs, int *rctxp)
 {
 	struct trace_entry *entry;
+	unsigned long flags;
 	char *raw_data;
 	int pc;
 
@@ -174,7 +175,8 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	memset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));
 
 	entry = (struct trace_entry *)raw_data;
-	tracing_generic_entry_update(entry, regs->flags, pc);
+	local_save_flags(flags);
+	tracing_generic_entry_update(entry, flags, pc);
 	entry->type = type;
 
 	return raw_data;

commit ff5f149b6aec8edbfa3698721667acd043009a33
Merge: f0218b3e9974 580d607cd666
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 21 11:49:57 2010 -0400

    Merge branch 'perf/core' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip into trace/tip/tracing/core-7
    
    Conflicts:
            include/linux/ftrace_event.h
            include/trace/ftrace.h
            kernel/trace/trace_event_perf.c
            kernel/trace/trace_kprobe.c
            kernel/trace/trace_syscalls.c
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 1c024eca51fdc965290acf342ae16a476c2189d0
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed May 19 14:02:22 2010 +0200

    perf, trace: Optimize tracepoints by using per-tracepoint-per-cpu hlist to track events
    
    Avoid the swevent hash-table by using per-tracepoint
    hlists.
    
    Also, avoid conditionals on the fast path by ordering
    with probe unregister so that we should never get on
    the callback path without the data being there.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20100521090710.473188012@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index a1304f8c4440..39d5ea7b0653 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -23,14 +23,25 @@ typedef typeof(unsigned long [PERF_MAX_TRACE_SIZE / sizeof(unsigned long)])
 /* Count the events in use (per event id, not per instance) */
 static int	total_ref_count;
 
-static int perf_trace_event_enable(struct ftrace_event_call *event, void *data)
+static int perf_trace_event_init(struct ftrace_event_call *tp_event,
+				 struct perf_event *p_event)
 {
+	struct hlist_head *list;
 	int ret = -ENOMEM;
+	int cpu;
 
-	if (event->perf_refcount++ > 0) {
-		event->perf_data = NULL;
+	p_event->tp_event = tp_event;
+	if (tp_event->perf_refcount++ > 0)
 		return 0;
-	}
+
+	list = alloc_percpu(struct hlist_head);
+	if (!list)
+		goto fail;
+
+	for_each_possible_cpu(cpu)
+		INIT_HLIST_HEAD(per_cpu_ptr(list, cpu));
+
+	tp_event->perf_events = list;
 
 	if (!total_ref_count) {
 		char *buf;
@@ -39,20 +50,20 @@ static int perf_trace_event_enable(struct ftrace_event_call *event, void *data)
 		for (i = 0; i < 4; i++) {
 			buf = (char *)alloc_percpu(perf_trace_t);
 			if (!buf)
-				goto fail_buf;
+				goto fail;
 
-			rcu_assign_pointer(perf_trace_buf[i], buf);
+			perf_trace_buf[i] = buf;
 		}
 	}
 
-	ret = event->perf_event_enable(event);
-	if (!ret) {
-		event->perf_data = data;
-		total_ref_count++;
-		return 0;
-	}
+	ret = tp_event->perf_event_enable(tp_event);
+	if (ret)
+		goto fail;
 
-fail_buf:
+	total_ref_count++;
+	return 0;
+
+fail:
 	if (!total_ref_count) {
 		int i;
 
@@ -61,21 +72,26 @@ static int perf_trace_event_enable(struct ftrace_event_call *event, void *data)
 			perf_trace_buf[i] = NULL;
 		}
 	}
-	event->perf_refcount--;
+
+	if (!--tp_event->perf_refcount) {
+		free_percpu(tp_event->perf_events);
+		tp_event->perf_events = NULL;
+	}
 
 	return ret;
 }
 
-int perf_trace_enable(int event_id, void *data)
+int perf_trace_init(struct perf_event *p_event)
 {
-	struct ftrace_event_call *event;
+	struct ftrace_event_call *tp_event;
+	int event_id = p_event->attr.config;
 	int ret = -EINVAL;
 
 	mutex_lock(&event_mutex);
-	list_for_each_entry(event, &ftrace_events, list) {
-		if (event->id == event_id && event->perf_event_enable &&
-		    try_module_get(event->mod)) {
-			ret = perf_trace_event_enable(event, data);
+	list_for_each_entry(tp_event, &ftrace_events, list) {
+		if (tp_event->id == event_id && tp_event->perf_event_enable &&
+		    try_module_get(tp_event->mod)) {
+			ret = perf_trace_event_init(tp_event, p_event);
 			break;
 		}
 	}
@@ -84,53 +100,52 @@ int perf_trace_enable(int event_id, void *data)
 	return ret;
 }
 
-static void perf_trace_event_disable(struct ftrace_event_call *event)
+int perf_trace_enable(struct perf_event *p_event)
 {
-	if (--event->perf_refcount > 0)
-		return;
+	struct ftrace_event_call *tp_event = p_event->tp_event;
+	struct hlist_head *list;
 
-	event->perf_event_disable(event);
+	list = tp_event->perf_events;
+	if (WARN_ON_ONCE(!list))
+		return -EINVAL;
 
-	if (!--total_ref_count) {
-		char *buf[4];
-		int i;
-
-		for (i = 0; i < 4; i++) {
-			buf[i] = perf_trace_buf[i];
-			rcu_assign_pointer(perf_trace_buf[i], NULL);
-		}
+	list = per_cpu_ptr(list, smp_processor_id());
+	hlist_add_head_rcu(&p_event->hlist_entry, list);
 
-		/*
-		 * Ensure every events in profiling have finished before
-		 * releasing the buffers
-		 */
-		synchronize_sched();
+	return 0;
+}
 
-		for (i = 0; i < 4; i++)
-			free_percpu(buf[i]);
-	}
+void perf_trace_disable(struct perf_event *p_event)
+{
+	hlist_del_rcu(&p_event->hlist_entry);
 }
 
-void perf_trace_disable(int event_id)
+void perf_trace_destroy(struct perf_event *p_event)
 {
-	struct ftrace_event_call *event;
+	struct ftrace_event_call *tp_event = p_event->tp_event;
+	int i;
 
-	mutex_lock(&event_mutex);
-	list_for_each_entry(event, &ftrace_events, list) {
-		if (event->id == event_id) {
-			perf_trace_event_disable(event);
-			module_put(event->mod);
-			break;
+	if (--tp_event->perf_refcount > 0)
+		return;
+
+	tp_event->perf_event_disable(tp_event);
+
+	free_percpu(tp_event->perf_events);
+	tp_event->perf_events = NULL;
+
+	if (!--total_ref_count) {
+		for (i = 0; i < 4; i++) {
+			free_percpu(perf_trace_buf[i]);
+			perf_trace_buf[i] = NULL;
 		}
 	}
-	mutex_unlock(&event_mutex);
 }
 
 __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 				       struct pt_regs *regs, int *rctxp)
 {
 	struct trace_entry *entry;
-	char *trace_buf, *raw_data;
+	char *raw_data;
 	int pc;
 
 	BUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));
@@ -139,13 +154,9 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 
 	*rctxp = perf_swevent_get_recursion_context();
 	if (*rctxp < 0)
-		goto err_recursion;
-
-	trace_buf = rcu_dereference_sched(perf_trace_buf[*rctxp]);
-	if (!trace_buf)
-		goto err;
+		return NULL;
 
-	raw_data = per_cpu_ptr(trace_buf, smp_processor_id());
+	raw_data = per_cpu_ptr(perf_trace_buf[*rctxp], smp_processor_id());
 
 	/* zero the dead bytes from align to not leak stack to user */
 	memset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));
@@ -155,9 +166,5 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	entry->type = type;
 
 	return raw_data;
-err:
-	perf_swevent_put_recursion_context(*rctxp);
-err_recursion:
-	return NULL;
 }
 EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);

commit b7e2ecef92d2e7785e6d76b41e5ba8bcbc45259d
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Wed May 19 10:52:27 2010 +0200

    perf, trace: Optimize tracepoints by removing IRQ-disable from perf/tracepoint interaction
    
    Improves performance.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1274259525.5605.10352.camel@twins>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 89b780a7c522..a1304f8c4440 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -9,13 +9,9 @@
 #include <linux/kprobes.h>
 #include "trace.h"
 
-DEFINE_PER_CPU(struct pt_regs, perf_trace_regs);
-EXPORT_PER_CPU_SYMBOL_GPL(perf_trace_regs);
-
 EXPORT_SYMBOL_GPL(perf_arch_fetch_caller_regs);
 
-static char *perf_trace_buf;
-static char *perf_trace_buf_nmi;
+static char *perf_trace_buf[4];
 
 /*
  * Force it to be aligned to unsigned long to avoid misaligned accesses
@@ -29,7 +25,6 @@ static int	total_ref_count;
 
 static int perf_trace_event_enable(struct ftrace_event_call *event, void *data)
 {
-	char *buf;
 	int ret = -ENOMEM;
 
 	if (event->perf_refcount++ > 0) {
@@ -38,17 +33,16 @@ static int perf_trace_event_enable(struct ftrace_event_call *event, void *data)
 	}
 
 	if (!total_ref_count) {
-		buf = (char *)alloc_percpu(perf_trace_t);
-		if (!buf)
-			goto fail_buf;
-
-		rcu_assign_pointer(perf_trace_buf, buf);
+		char *buf;
+		int i;
 
-		buf = (char *)alloc_percpu(perf_trace_t);
-		if (!buf)
-			goto fail_buf_nmi;
+		for (i = 0; i < 4; i++) {
+			buf = (char *)alloc_percpu(perf_trace_t);
+			if (!buf)
+				goto fail_buf;
 
-		rcu_assign_pointer(perf_trace_buf_nmi, buf);
+			rcu_assign_pointer(perf_trace_buf[i], buf);
+		}
 	}
 
 	ret = event->perf_event_enable(event);
@@ -58,14 +52,15 @@ static int perf_trace_event_enable(struct ftrace_event_call *event, void *data)
 		return 0;
 	}
 
-fail_buf_nmi:
+fail_buf:
 	if (!total_ref_count) {
-		free_percpu(perf_trace_buf_nmi);
-		free_percpu(perf_trace_buf);
-		perf_trace_buf_nmi = NULL;
-		perf_trace_buf = NULL;
+		int i;
+
+		for (i = 0; i < 4; i++) {
+			free_percpu(perf_trace_buf[i]);
+			perf_trace_buf[i] = NULL;
+		}
 	}
-fail_buf:
 	event->perf_refcount--;
 
 	return ret;
@@ -91,19 +86,19 @@ int perf_trace_enable(int event_id, void *data)
 
 static void perf_trace_event_disable(struct ftrace_event_call *event)
 {
-	char *buf, *nmi_buf;
-
 	if (--event->perf_refcount > 0)
 		return;
 
 	event->perf_event_disable(event);
 
 	if (!--total_ref_count) {
-		buf = perf_trace_buf;
-		rcu_assign_pointer(perf_trace_buf, NULL);
+		char *buf[4];
+		int i;
 
-		nmi_buf = perf_trace_buf_nmi;
-		rcu_assign_pointer(perf_trace_buf_nmi, NULL);
+		for (i = 0; i < 4; i++) {
+			buf[i] = perf_trace_buf[i];
+			rcu_assign_pointer(perf_trace_buf[i], NULL);
+		}
 
 		/*
 		 * Ensure every events in profiling have finished before
@@ -111,8 +106,8 @@ static void perf_trace_event_disable(struct ftrace_event_call *event)
 		 */
 		synchronize_sched();
 
-		free_percpu(buf);
-		free_percpu(nmi_buf);
+		for (i = 0; i < 4; i++)
+			free_percpu(buf[i]);
 	}
 }
 
@@ -132,47 +127,37 @@ void perf_trace_disable(int event_id)
 }
 
 __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
-				       int *rctxp, unsigned long *irq_flags)
+				       struct pt_regs *regs, int *rctxp)
 {
 	struct trace_entry *entry;
 	char *trace_buf, *raw_data;
-	int pc, cpu;
+	int pc;
 
 	BUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));
 
 	pc = preempt_count();
 
-	/* Protect the per cpu buffer, begin the rcu read side */
-	local_irq_save(*irq_flags);
-
 	*rctxp = perf_swevent_get_recursion_context();
 	if (*rctxp < 0)
 		goto err_recursion;
 
-	cpu = smp_processor_id();
-
-	if (in_nmi())
-		trace_buf = rcu_dereference_sched(perf_trace_buf_nmi);
-	else
-		trace_buf = rcu_dereference_sched(perf_trace_buf);
-
+	trace_buf = rcu_dereference_sched(perf_trace_buf[*rctxp]);
 	if (!trace_buf)
 		goto err;
 
-	raw_data = per_cpu_ptr(trace_buf, cpu);
+	raw_data = per_cpu_ptr(trace_buf, smp_processor_id());
 
 	/* zero the dead bytes from align to not leak stack to user */
 	memset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));
 
 	entry = (struct trace_entry *)raw_data;
-	tracing_generic_entry_update(entry, *irq_flags, pc);
+	tracing_generic_entry_update(entry, regs->flags, pc);
 	entry->type = type;
 
 	return raw_data;
 err:
 	perf_swevent_put_recursion_context(*rctxp);
 err_recursion:
-	local_irq_restore(*irq_flags);
 	return NULL;
 }
 EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);

commit 4f41c013f553957765902fb01475972f0af3e8e7
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Tue May 18 18:08:32 2010 +0200

    perf/ftrace: Optimize perf/tracepoint interaction for single events
    
    When we've got but a single event per tracepoint
    there is no reason to try and multiplex it so don't.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Tested-by: Ingo Molnar <mingo@elte.hu>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 0565bb42566f..89b780a7c522 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -27,13 +27,15 @@ typedef typeof(unsigned long [PERF_MAX_TRACE_SIZE / sizeof(unsigned long)])
 /* Count the events in use (per event id, not per instance) */
 static int	total_ref_count;
 
-static int perf_trace_event_enable(struct ftrace_event_call *event)
+static int perf_trace_event_enable(struct ftrace_event_call *event, void *data)
 {
 	char *buf;
 	int ret = -ENOMEM;
 
-	if (event->perf_refcount++ > 0)
+	if (event->perf_refcount++ > 0) {
+		event->perf_data = NULL;
 		return 0;
+	}
 
 	if (!total_ref_count) {
 		buf = (char *)alloc_percpu(perf_trace_t);
@@ -51,6 +53,7 @@ static int perf_trace_event_enable(struct ftrace_event_call *event)
 
 	ret = event->perf_event_enable(event);
 	if (!ret) {
+		event->perf_data = data;
 		total_ref_count++;
 		return 0;
 	}
@@ -68,7 +71,7 @@ static int perf_trace_event_enable(struct ftrace_event_call *event)
 	return ret;
 }
 
-int perf_trace_enable(int event_id)
+int perf_trace_enable(int event_id, void *data)
 {
 	struct ftrace_event_call *event;
 	int ret = -EINVAL;
@@ -77,7 +80,7 @@ int perf_trace_enable(int event_id)
 	list_for_each_entry(event, &ftrace_events, list) {
 		if (event->id == event_id && event->perf_event_enable &&
 		    try_module_get(event->mod)) {
-			ret = perf_trace_event_enable(event);
+			ret = perf_trace_event_enable(event, data);
 			break;
 		}
 	}

commit 32c0edaeaad74a7883e736ae0f3798784cfc2a80
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 23 10:38:03 2010 -0400

    tracing: Remove duplicate id information in event structure
    
    Now that the trace_event structure is embedded in the ftrace_event_call
    structure, there is no need for the ftrace_event_call id field.
    The id field is the same as the trace_event type field.
    
    Removing the id and re-arranging the structure brings down the tracepoint
    footprint by another 5K.
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4895024 1023812  861512 6780348  6775bc vmlinux.print
    4894944 1018052  861512 6774508  675eec vmlinux.id
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 196fe9d26773..0a47e8d6b491 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -80,7 +80,7 @@ int perf_trace_enable(int event_id)
 
 	mutex_lock(&event_mutex);
 	list_for_each_entry(event, &ftrace_events, list) {
-		if (event->id == event_id &&
+		if (event->event.type == event_id &&
 		    event->class && event->class->perf_probe &&
 		    try_module_get(event->mod)) {
 			ret = perf_trace_event_enable(event);
@@ -128,7 +128,7 @@ void perf_trace_disable(int event_id)
 
 	mutex_lock(&event_mutex);
 	list_for_each_entry(event, &ftrace_events, list) {
-		if (event->id == event_id) {
+		if (event->event.type == event_id) {
 			perf_trace_event_disable(event);
 			module_put(event->mod);
 			break;

commit 2239291aeb0379fe47980b0e560e0eb9fd7e82ec
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Apr 21 12:27:06 2010 -0400

    tracing: Remove per event trace registering
    
    This patch removes the register functions of TRACE_EVENT() to enable
    and disable tracepoints. The registering of a event is now down
    directly in the trace_events.c file. The tracepoint_probe_register()
    is now called directly.
    
    The prototypes are no longer type checked, but this should not be
    an issue since the tracepoints are created automatically by the
    macros. If a prototype is incorrect in the TRACE_EVENT() macro, then
    other macros will catch it.
    
    The trace_event_class structure now holds the probes to be called
    by the callbacks. This removes needing to have each event have
    a separate pointer for the probe.
    
    To handle kprobes and syscalls, since they register probes in a
    different manner, a "reg" field is added to the ftrace_event_class
    structure. If the "reg" field is assigned, then it will be called for
    enabling and disabling of the probe for either ftrace or perf. To let
    the reg function know what is happening, a new enum (trace_reg) is
    created that has the type of control that is needed.
    
    With this new rework, the 82 kernel events and 618 syscall events
    has their footprint dramatically lowered:
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4914025 1088868  861512 6864405  68be15 vmlinux.class
    4918492 1084612  861512 6864616  68bee8 vmlinux.tracepoint
    4900252 1057412  861512 6819176  680d68 vmlinux.regs
    
    The size went from 6863829 to 6819176, that's a total of 44K
    in savings. With tracepoints being continuously added, this is
    critical that the footprint becomes minimal.
    
    v5: Added #ifdef CONFIG_PERF_EVENTS around a reference to perf
        specific structure in trace_events.c.
    
    v4: Fixed trace self tests to check probe because regfunc no longer
        exists.
    
    v3: Updated to handle void *data in beginning of probe parameters.
        Also added the tracepoint: check_trace_callback_type_##call().
    
    v2: Changed the callback probes to pass void * and typecast the
        value within the function.
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 0565bb42566f..196fe9d26773 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -49,7 +49,12 @@ static int perf_trace_event_enable(struct ftrace_event_call *event)
 		rcu_assign_pointer(perf_trace_buf_nmi, buf);
 	}
 
-	ret = event->perf_event_enable(event);
+	if (event->class->reg)
+		ret = event->class->reg(event, TRACE_REG_PERF_REGISTER);
+	else
+		ret = tracepoint_probe_register(event->name,
+						event->class->perf_probe,
+						event);
 	if (!ret) {
 		total_ref_count++;
 		return 0;
@@ -75,7 +80,8 @@ int perf_trace_enable(int event_id)
 
 	mutex_lock(&event_mutex);
 	list_for_each_entry(event, &ftrace_events, list) {
-		if (event->id == event_id && event->perf_event_enable &&
+		if (event->id == event_id &&
+		    event->class && event->class->perf_probe &&
 		    try_module_get(event->mod)) {
 			ret = perf_trace_event_enable(event);
 			break;
@@ -93,7 +99,10 @@ static void perf_trace_event_disable(struct ftrace_event_call *event)
 	if (--event->perf_refcount > 0)
 		return;
 
-	event->perf_event_disable(event);
+	if (event->class->reg)
+		event->class->reg(event, TRACE_REG_PERF_UNREGISTER);
+	else
+		tracepoint_probe_unregister(event->name, event->class->perf_probe, event);
 
 	if (!--total_ref_count) {
 		buf = perf_trace_buf;

commit eb1e79611cc9bfe21978230e3521e77ea2d7874a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Mar 23 00:08:59 2010 +0100

    perf: Correctly align perf event tracing buffer
    
    The trace event buffer used by perf to record raw sample events
    is typed as an array of char and may then not be aligned to 8
    by alloc_percpu().
    
    But we need it to be aligned to 8 in sparc64 because we cast
    this buffer into a random structure type built by the TRACE_EVENT()
    macro to store the traces. So if a random 64 bits field is accessed
    inside, it may be not under an expected good alignment.
    
    Use an array of long instead to force the appropriate alignment, and
    perform a compile time check to ensure the size in byte of the buffer
    is a multiple of sizeof(long) so that its actual size doesn't get
    shrinked under us.
    
    This fixes unaligned accesses reported while using perf lock
    in sparc 64.
    
    Suggested-by: David Miller <davem@davemloft.net>
    Suggested-by: Tejun Heo <htejun@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: David Miller <davem@davemloft.net>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 81f691eb3a30..0565bb42566f 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -17,7 +17,12 @@ EXPORT_SYMBOL_GPL(perf_arch_fetch_caller_regs);
 static char *perf_trace_buf;
 static char *perf_trace_buf_nmi;
 
-typedef typeof(char [PERF_MAX_TRACE_SIZE]) perf_trace_t ;
+/*
+ * Force it to be aligned to unsigned long to avoid misaligned accesses
+ * suprises
+ */
+typedef typeof(unsigned long [PERF_MAX_TRACE_SIZE / sizeof(unsigned long)])
+	perf_trace_t;
 
 /* Count the events in use (per event id, not per instance) */
 static int	total_ref_count;
@@ -130,6 +135,8 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	char *trace_buf, *raw_data;
 	int pc, cpu;
 
+	BUILD_BUG_ON(PERF_MAX_TRACE_SIZE % sizeof(unsigned long));
+
 	pc = preempt_count();
 
 	/* Protect the per cpu buffer, begin the rcu read side */
@@ -152,7 +159,7 @@ __kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
 	raw_data = per_cpu_ptr(trace_buf, cpu);
 
 	/* zero the dead bytes from align to not leak stack to user */
-	*(u64 *)(&raw_data[size - sizeof(u64)]) = 0ULL;
+	memset(&raw_data[size - sizeof(u64)], 0, sizeof(u64));
 
 	entry = (struct trace_entry *)raw_data;
 	tracing_generic_entry_update(entry, *irq_flags, pc);

commit f82c37e7bb4c4d9b6a476c642d5c2d2efbd6f240
Merge: c6b9e73f2fee dcd5c1662db5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Mar 18 16:52:46 2010 -0700

    Merge branch 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip
    
    * 'perf-fixes-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/linux-2.6-tip: (35 commits)
      perf: Fix unexported generic perf_arch_fetch_caller_regs
      perf record: Don't try to find buildids in a zero sized file
      perf: export perf_trace_regs and perf_arch_fetch_caller_regs
      perf, x86: Fix hw_perf_enable() event assignment
      perf, ppc: Fix compile error due to new cpu notifiers
      perf: Make the install relative to DESTDIR if specified
      kprobes: Calculate the index correctly when freeing the out-of-line execution slot
      perf tools: Fix sparse CPU numbering related bugs
      perf_event: Fix oops triggered by cpu offline/online
      perf: Drop the obsolete profile naming for trace events
      perf: Take a hot regs snapshot for trace events
      perf: Introduce new perf_fetch_caller_regs() for hot regs snapshot
      perf/x86-64: Use frame pointer to walk on irq and process stacks
      lockdep: Move lock events under lockdep recursion protection
      perf report: Print the map table just after samples for which no map was found
      perf report: Add multiple event support
      perf session: Change perf_session post processing functions to take histogram tree
      perf session: Add storage for seperating event types in report
      perf session: Change add_hist_entry to take the tree root instead of session
      perf record: Add ID and to recorded event data when recording multiple events
      ...

commit dcd5c1662db59a6b82942f47fb6ac9dd63f6d3dd
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Mar 16 01:05:02 2010 +0100

    perf: Fix unexported generic perf_arch_fetch_caller_regs
    
    perf_arch_fetch_caller_regs() is exported for the overriden x86
    version, but not for the generic weak version.
    
    As a general rule, weak functions should not have their symbol
    exported in the same file they are defined.
    
    So let's export it on trace_event_perf.c as it is used by trace
    events only.
    
    This fixes:
    
            ERROR: ".perf_arch_fetch_caller_regs" [fs/xfs/xfs.ko] undefined!
            ERROR: ".perf_arch_fetch_caller_regs" [arch/powerpc/platforms/cell/spufs/spufs.ko] undefined!
    
    -v2: And also only build it if trace events are enabled.
    -v3: Fix changelog mistake
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Cc: Paul Mackerras <paulus@samba.org>
    LKML-Reference: <1268697902-9518-1-git-send-regression-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index 0709e4f75114..7d79a10c3cde 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -12,6 +12,8 @@
 DEFINE_PER_CPU(struct pt_regs, perf_trace_regs);
 EXPORT_PER_CPU_SYMBOL_GPL(perf_trace_regs);
 
+EXPORT_SYMBOL_GPL(perf_arch_fetch_caller_regs);
+
 static char *perf_trace_buf;
 static char *perf_trace_buf_nmi;
 

commit 639fe4b12f92b54c9c3b38c82cdafaa38cfd3e63
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Mar 11 15:30:35 2010 +0800

    perf: export perf_trace_regs and perf_arch_fetch_caller_regs
    
    Export perf_trace_regs and perf_arch_fetch_caller_regs since module will
    use these.
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    [ use EXPORT_PER_CPU_SYMBOL_GPL() ]
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4B989C1B.2090407@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
index f315b12a41d8..0709e4f75114 100644
--- a/kernel/trace/trace_event_perf.c
+++ b/kernel/trace/trace_event_perf.c
@@ -10,6 +10,7 @@
 #include "trace.h"
 
 DEFINE_PER_CPU(struct pt_regs, perf_trace_regs);
+EXPORT_PER_CPU_SYMBOL_GPL(perf_trace_regs);
 
 static char *perf_trace_buf;
 static char *perf_trace_buf_nmi;

commit 97d5a22005f38057b4bc0d95f81cd26510268794
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 5 05:35:37 2010 +0100

    perf: Drop the obsolete profile naming for trace events
    
    Drop the obsolete "profile" naming used by perf for trace events.
    Perf can now do more than simple events counting, so generalize
    the API naming.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Jason Baron <jbaron@redhat.com>

diff --git a/kernel/trace/trace_event_perf.c b/kernel/trace/trace_event_perf.c
new file mode 100644
index 000000000000..f315b12a41d8
--- /dev/null
+++ b/kernel/trace/trace_event_perf.c
@@ -0,0 +1,165 @@
+/*
+ * trace event based perf event profiling/tracing
+ *
+ * Copyright (C) 2009 Red Hat Inc, Peter Zijlstra <pzijlstr@redhat.com>
+ * Copyright (C) 2009-2010 Frederic Weisbecker <fweisbec@gmail.com>
+ */
+
+#include <linux/module.h>
+#include <linux/kprobes.h>
+#include "trace.h"
+
+DEFINE_PER_CPU(struct pt_regs, perf_trace_regs);
+
+static char *perf_trace_buf;
+static char *perf_trace_buf_nmi;
+
+typedef typeof(char [PERF_MAX_TRACE_SIZE]) perf_trace_t ;
+
+/* Count the events in use (per event id, not per instance) */
+static int	total_ref_count;
+
+static int perf_trace_event_enable(struct ftrace_event_call *event)
+{
+	char *buf;
+	int ret = -ENOMEM;
+
+	if (event->perf_refcount++ > 0)
+		return 0;
+
+	if (!total_ref_count) {
+		buf = (char *)alloc_percpu(perf_trace_t);
+		if (!buf)
+			goto fail_buf;
+
+		rcu_assign_pointer(perf_trace_buf, buf);
+
+		buf = (char *)alloc_percpu(perf_trace_t);
+		if (!buf)
+			goto fail_buf_nmi;
+
+		rcu_assign_pointer(perf_trace_buf_nmi, buf);
+	}
+
+	ret = event->perf_event_enable(event);
+	if (!ret) {
+		total_ref_count++;
+		return 0;
+	}
+
+fail_buf_nmi:
+	if (!total_ref_count) {
+		free_percpu(perf_trace_buf_nmi);
+		free_percpu(perf_trace_buf);
+		perf_trace_buf_nmi = NULL;
+		perf_trace_buf = NULL;
+	}
+fail_buf:
+	event->perf_refcount--;
+
+	return ret;
+}
+
+int perf_trace_enable(int event_id)
+{
+	struct ftrace_event_call *event;
+	int ret = -EINVAL;
+
+	mutex_lock(&event_mutex);
+	list_for_each_entry(event, &ftrace_events, list) {
+		if (event->id == event_id && event->perf_event_enable &&
+		    try_module_get(event->mod)) {
+			ret = perf_trace_event_enable(event);
+			break;
+		}
+	}
+	mutex_unlock(&event_mutex);
+
+	return ret;
+}
+
+static void perf_trace_event_disable(struct ftrace_event_call *event)
+{
+	char *buf, *nmi_buf;
+
+	if (--event->perf_refcount > 0)
+		return;
+
+	event->perf_event_disable(event);
+
+	if (!--total_ref_count) {
+		buf = perf_trace_buf;
+		rcu_assign_pointer(perf_trace_buf, NULL);
+
+		nmi_buf = perf_trace_buf_nmi;
+		rcu_assign_pointer(perf_trace_buf_nmi, NULL);
+
+		/*
+		 * Ensure every events in profiling have finished before
+		 * releasing the buffers
+		 */
+		synchronize_sched();
+
+		free_percpu(buf);
+		free_percpu(nmi_buf);
+	}
+}
+
+void perf_trace_disable(int event_id)
+{
+	struct ftrace_event_call *event;
+
+	mutex_lock(&event_mutex);
+	list_for_each_entry(event, &ftrace_events, list) {
+		if (event->id == event_id) {
+			perf_trace_event_disable(event);
+			module_put(event->mod);
+			break;
+		}
+	}
+	mutex_unlock(&event_mutex);
+}
+
+__kprobes void *perf_trace_buf_prepare(int size, unsigned short type,
+				       int *rctxp, unsigned long *irq_flags)
+{
+	struct trace_entry *entry;
+	char *trace_buf, *raw_data;
+	int pc, cpu;
+
+	pc = preempt_count();
+
+	/* Protect the per cpu buffer, begin the rcu read side */
+	local_irq_save(*irq_flags);
+
+	*rctxp = perf_swevent_get_recursion_context();
+	if (*rctxp < 0)
+		goto err_recursion;
+
+	cpu = smp_processor_id();
+
+	if (in_nmi())
+		trace_buf = rcu_dereference(perf_trace_buf_nmi);
+	else
+		trace_buf = rcu_dereference(perf_trace_buf);
+
+	if (!trace_buf)
+		goto err;
+
+	raw_data = per_cpu_ptr(trace_buf, cpu);
+
+	/* zero the dead bytes from align to not leak stack to user */
+	*(u64 *)(&raw_data[size - sizeof(u64)]) = 0ULL;
+
+	entry = (struct trace_entry *)raw_data;
+	tracing_generic_entry_update(entry, *irq_flags, pc);
+	entry->type = type;
+
+	return raw_data;
+err:
+	perf_swevent_put_recursion_context(*rctxp);
+err_recursion:
+	local_irq_restore(*irq_flags);
+	return NULL;
+}
+EXPORT_SYMBOL_GPL(perf_trace_buf_prepare);
