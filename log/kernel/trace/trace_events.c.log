commit 2d19bd79ae6509858582a9cade739c2e9a4fdca8
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Fri Apr 3 14:31:21 2020 -0500

    tracing: Add hist_debug trace event files for histogram debugging
    
    Add a new "hist_debug" file for each trace event, which when read will
    dump out a bunch of internal details about the hist triggers defined
    on that event.
    
    This is normally off but can be enabled by saying 'y' to the new
    CONFIG_HIST_TRIGGERS_DEBUG config option.
    
    This is in support of the new Documentation file describing histogram
    internals, Documentation/trace/histogram-design.rst, which was
    requested by developers trying to understand the internals when
    extending or making use of the hist triggers for higher-level tools.
    
    The histogram-design.rst documentation refers to the hist_debug files
    and demonstrates their use with output in the test examples.
    
    Link: http://lkml.kernel.org/r/77914c22b0ba493d9783c53bbfbc6087d6a7e1b1.1585941485.git.zanussi@kernel.org
    
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 242f59e7f17d..f6f55682d3e2 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2208,6 +2208,10 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 #ifdef CONFIG_HIST_TRIGGERS
 	trace_create_file("hist", 0444, file->dir, file,
 			  &event_hist_fops);
+#endif
+#ifdef CONFIG_HIST_TRIGGERS_DEBUG
+	trace_create_file("hist_debug", 0444, file->dir, file,
+			  &event_hist_debug_fops);
 #endif
 	trace_create_file("format", 0444, file->dir, call,
 			  &ftrace_event_format_fops);

commit 2768362603018da2be44ae4d01f22406152db05a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Mar 25 19:51:19 2020 -0400

    tracing: Create set_event_notrace_pid to not trace tasks
    
    There's currently a way to select a task that should only have its events
    traced, but there's no way to select a task not to have itsevents traced.
    Add a set_event_notrace_pid file that acts the same as set_event_pid (and is
    also affected by event-fork), but the task pids in this file will not be
    traced even if they are listed in the set_event_pid file. This makes it easy
    for tools like trace-cmd to "hide" itself from beint traced by events when
    it is recording other tasks.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c196d0dc5871..242f59e7f17d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -232,10 +232,13 @@ bool trace_event_ignore_this_pid(struct trace_event_file *trace_file)
 {
 	struct trace_array *tr = trace_file->tr;
 	struct trace_array_cpu *data;
+	struct trace_pid_list *no_pid_list;
 	struct trace_pid_list *pid_list;
 
 	pid_list = rcu_dereference_raw(tr->filtered_pids);
-	if (!pid_list)
+	no_pid_list = rcu_dereference_raw(tr->filtered_no_pids);
+
+	if (!pid_list && !no_pid_list)
 		return false;
 
 	data = this_cpu_ptr(tr->array_buffer.data);
@@ -510,6 +513,9 @@ event_filter_pid_sched_process_exit(void *data, struct task_struct *task)
 
 	pid_list = rcu_dereference_raw(tr->filtered_pids);
 	trace_filter_add_remove_task(pid_list, NULL, task);
+
+	pid_list = rcu_dereference_raw(tr->filtered_no_pids);
+	trace_filter_add_remove_task(pid_list, NULL, task);
 }
 
 static void
@@ -522,6 +528,9 @@ event_filter_pid_sched_process_fork(void *data,
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 	trace_filter_add_remove_task(pid_list, self, task);
+
+	pid_list = rcu_dereference_sched(tr->filtered_no_pids);
+	trace_filter_add_remove_task(pid_list, self, task);
 }
 
 void trace_event_follow_fork(struct trace_array *tr, bool enable)
@@ -544,13 +553,23 @@ event_filter_pid_sched_switch_probe_pre(void *data, bool preempt,
 		    struct task_struct *prev, struct task_struct *next)
 {
 	struct trace_array *tr = data;
+	struct trace_pid_list *no_pid_list;
 	struct trace_pid_list *pid_list;
+	bool ret;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	no_pid_list = rcu_dereference_sched(tr->filtered_no_pids);
 
-	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, NULL, prev) &&
-		       trace_ignore_this_task(pid_list, NULL, next));
+	/*
+	 * Sched switch is funny, as we only want to ignore it
+	 * in the notrace case if both prev and next should be ignored.
+	 */
+	ret = trace_ignore_this_task(NULL, no_pid_list, prev) &&
+		trace_ignore_this_task(NULL, no_pid_list, next);
+
+	this_cpu_write(tr->array_buffer.data->ignore_pid, ret ||
+		       (trace_ignore_this_task(pid_list, NULL, prev) &&
+			trace_ignore_this_task(pid_list, NULL, next)));
 }
 
 static void
@@ -558,18 +577,21 @@ event_filter_pid_sched_switch_probe_post(void *data, bool preempt,
 		    struct task_struct *prev, struct task_struct *next)
 {
 	struct trace_array *tr = data;
+	struct trace_pid_list *no_pid_list;
 	struct trace_pid_list *pid_list;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	no_pid_list = rcu_dereference_sched(tr->filtered_no_pids);
 
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, NULL, next));
+		       trace_ignore_this_task(pid_list, no_pid_list, next));
 }
 
 static void
 event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
 {
 	struct trace_array *tr = data;
+	struct trace_pid_list *no_pid_list;
 	struct trace_pid_list *pid_list;
 
 	/* Nothing to do if we are already tracing */
@@ -577,15 +599,17 @@ event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
 		return;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	no_pid_list = rcu_dereference_sched(tr->filtered_no_pids);
 
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, NULL, task));
+		       trace_ignore_this_task(pid_list, no_pid_list, task));
 }
 
 static void
 event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
 {
 	struct trace_array *tr = data;
+	struct trace_pid_list *no_pid_list;
 	struct trace_pid_list *pid_list;
 
 	/* Nothing to do if we are not tracing */
@@ -593,23 +617,15 @@ event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
 		return;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	no_pid_list = rcu_dereference_sched(tr->filtered_no_pids);
 
 	/* Set tracing if current is enabled */
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, NULL, current));
+		       trace_ignore_this_task(pid_list, no_pid_list, current));
 }
 
-static void __ftrace_clear_event_pids(struct trace_array *tr)
+static void unregister_pid_events(struct trace_array *tr)
 {
-	struct trace_pid_list *pid_list;
-	struct trace_event_file *file;
-	int cpu;
-
-	pid_list = rcu_dereference_protected(tr->filtered_pids,
-					     lockdep_is_held(&event_mutex));
-	if (!pid_list)
-		return;
-
 	unregister_trace_sched_switch(event_filter_pid_sched_switch_probe_pre, tr);
 	unregister_trace_sched_switch(event_filter_pid_sched_switch_probe_post, tr);
 
@@ -621,26 +637,55 @@ static void __ftrace_clear_event_pids(struct trace_array *tr)
 
 	unregister_trace_sched_waking(event_filter_pid_sched_wakeup_probe_pre, tr);
 	unregister_trace_sched_waking(event_filter_pid_sched_wakeup_probe_post, tr);
+}
 
-	list_for_each_entry(file, &tr->events, list) {
-		clear_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);
+static void __ftrace_clear_event_pids(struct trace_array *tr, int type)
+{
+	struct trace_pid_list *pid_list;
+	struct trace_pid_list *no_pid_list;
+	struct trace_event_file *file;
+	int cpu;
+
+	pid_list = rcu_dereference_protected(tr->filtered_pids,
+					     lockdep_is_held(&event_mutex));
+	no_pid_list = rcu_dereference_protected(tr->filtered_no_pids,
+					     lockdep_is_held(&event_mutex));
+
+	/* Make sure there's something to do */
+	if (!pid_type_enabled(type, pid_list, no_pid_list))
+		return;
+
+	if (!still_need_pid_events(type, pid_list, no_pid_list)) {
+		unregister_pid_events(tr);
+
+		list_for_each_entry(file, &tr->events, list) {
+			clear_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);
+		}
+
+		for_each_possible_cpu(cpu)
+			per_cpu_ptr(tr->array_buffer.data, cpu)->ignore_pid = false;
 	}
 
-	for_each_possible_cpu(cpu)
-		per_cpu_ptr(tr->array_buffer.data, cpu)->ignore_pid = false;
+	if (type & TRACE_PIDS)
+		rcu_assign_pointer(tr->filtered_pids, NULL);
 
-	rcu_assign_pointer(tr->filtered_pids, NULL);
+	if (type & TRACE_NO_PIDS)
+		rcu_assign_pointer(tr->filtered_no_pids, NULL);
 
 	/* Wait till all users are no longer using pid filtering */
 	tracepoint_synchronize_unregister();
 
-	trace_free_pid_list(pid_list);
+	if ((type & TRACE_PIDS) && pid_list)
+		trace_free_pid_list(pid_list);
+
+	if ((type & TRACE_NO_PIDS) && no_pid_list)
+		trace_free_pid_list(no_pid_list);
 }
 
-static void ftrace_clear_event_pids(struct trace_array *tr)
+static void ftrace_clear_event_pids(struct trace_array *tr, int type)
 {
 	mutex_lock(&event_mutex);
-	__ftrace_clear_event_pids(tr);
+	__ftrace_clear_event_pids(tr, type);
 	mutex_unlock(&event_mutex);
 }
 
@@ -1013,15 +1058,32 @@ static void t_stop(struct seq_file *m, void *p)
 }
 
 static void *
-p_next(struct seq_file *m, void *v, loff_t *pos)
+__next(struct seq_file *m, void *v, loff_t *pos, int type)
 {
 	struct trace_array *tr = m->private;
-	struct trace_pid_list *pid_list = rcu_dereference_sched(tr->filtered_pids);
+	struct trace_pid_list *pid_list;
+
+	if (type == TRACE_PIDS)
+		pid_list = rcu_dereference_sched(tr->filtered_pids);
+	else
+		pid_list = rcu_dereference_sched(tr->filtered_no_pids);
 
 	return trace_pid_next(pid_list, v, pos);
 }
 
-static void *p_start(struct seq_file *m, loff_t *pos)
+static void *
+p_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	return __next(m, v, pos, TRACE_PIDS);
+}
+
+static void *
+np_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	return __next(m, v, pos, TRACE_NO_PIDS);
+}
+
+static void *__start(struct seq_file *m, loff_t *pos, int type)
 	__acquires(RCU)
 {
 	struct trace_pid_list *pid_list;
@@ -1036,7 +1098,10 @@ static void *p_start(struct seq_file *m, loff_t *pos)
 	mutex_lock(&event_mutex);
 	rcu_read_lock_sched();
 
-	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	if (type == TRACE_PIDS)
+		pid_list = rcu_dereference_sched(tr->filtered_pids);
+	else
+		pid_list = rcu_dereference_sched(tr->filtered_no_pids);
 
 	if (!pid_list)
 		return NULL;
@@ -1044,6 +1109,18 @@ static void *p_start(struct seq_file *m, loff_t *pos)
 	return trace_pid_start(pid_list, pos);
 }
 
+static void *p_start(struct seq_file *m, loff_t *pos)
+	__acquires(RCU)
+{
+	return __start(m, pos, TRACE_PIDS);
+}
+
+static void *np_start(struct seq_file *m, loff_t *pos)
+	__acquires(RCU)
+{
+	return __start(m, pos, TRACE_NO_PIDS);
+}
+
 static void p_stop(struct seq_file *m, void *p)
 	__releases(RCU)
 {
@@ -1588,6 +1665,7 @@ static void ignore_task_cpu(void *data)
 {
 	struct trace_array *tr = data;
 	struct trace_pid_list *pid_list;
+	struct trace_pid_list *no_pid_list;
 
 	/*
 	 * This function is called by on_each_cpu() while the
@@ -1595,18 +1673,50 @@ static void ignore_task_cpu(void *data)
 	 */
 	pid_list = rcu_dereference_protected(tr->filtered_pids,
 					     mutex_is_locked(&event_mutex));
+	no_pid_list = rcu_dereference_protected(tr->filtered_no_pids,
+					     mutex_is_locked(&event_mutex));
 
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, NULL, current));
+		       trace_ignore_this_task(pid_list, no_pid_list, current));
+}
+
+static void register_pid_events(struct trace_array *tr)
+{
+	/*
+	 * Register a probe that is called before all other probes
+	 * to set ignore_pid if next or prev do not match.
+	 * Register a probe this is called after all other probes
+	 * to only keep ignore_pid set if next pid matches.
+	 */
+	register_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_pre,
+					 tr, INT_MAX);
+	register_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_post,
+					 tr, 0);
+
+	register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_pre,
+					 tr, INT_MAX);
+	register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_post,
+					 tr, 0);
+
+	register_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_pre,
+					     tr, INT_MAX);
+	register_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_post,
+					     tr, 0);
+
+	register_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_pre,
+					 tr, INT_MAX);
+	register_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_post,
+					 tr, 0);
 }
 
 static ssize_t
-ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
-		       size_t cnt, loff_t *ppos)
+event_pid_write(struct file *filp, const char __user *ubuf,
+		size_t cnt, loff_t *ppos, int type)
 {
 	struct seq_file *m = filp->private_data;
 	struct trace_array *tr = m->private;
 	struct trace_pid_list *filtered_pids = NULL;
+	struct trace_pid_list *other_pids = NULL;
 	struct trace_pid_list *pid_list;
 	struct trace_event_file *file;
 	ssize_t ret;
@@ -1620,14 +1730,26 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 
 	mutex_lock(&event_mutex);
 
-	filtered_pids = rcu_dereference_protected(tr->filtered_pids,
-					     lockdep_is_held(&event_mutex));
+	if (type == TRACE_PIDS) {
+		filtered_pids = rcu_dereference_protected(tr->filtered_pids,
+							  lockdep_is_held(&event_mutex));
+		other_pids = rcu_dereference_protected(tr->filtered_no_pids,
+							  lockdep_is_held(&event_mutex));
+	} else {
+		filtered_pids = rcu_dereference_protected(tr->filtered_no_pids,
+							  lockdep_is_held(&event_mutex));
+		other_pids = rcu_dereference_protected(tr->filtered_pids,
+							  lockdep_is_held(&event_mutex));
+	}
 
 	ret = trace_pid_write(filtered_pids, &pid_list, ubuf, cnt);
 	if (ret < 0)
 		goto out;
 
-	rcu_assign_pointer(tr->filtered_pids, pid_list);
+	if (type == TRACE_PIDS)
+		rcu_assign_pointer(tr->filtered_pids, pid_list);
+	else
+		rcu_assign_pointer(tr->filtered_no_pids, pid_list);
 
 	list_for_each_entry(file, &tr->events, list) {
 		set_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);
@@ -1636,32 +1758,8 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	if (filtered_pids) {
 		tracepoint_synchronize_unregister();
 		trace_free_pid_list(filtered_pids);
-	} else if (pid_list) {
-		/*
-		 * Register a probe that is called before all other probes
-		 * to set ignore_pid if next or prev do not match.
-		 * Register a probe this is called after all other probes
-		 * to only keep ignore_pid set if next pid matches.
-		 */
-		register_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_pre,
-						 tr, INT_MAX);
-		register_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_post,
-						 tr, 0);
-
-		register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_pre,
-						 tr, INT_MAX);
-		register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_post,
-						 tr, 0);
-
-		register_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_pre,
-						     tr, INT_MAX);
-		register_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_post,
-						     tr, 0);
-
-		register_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_pre,
-						 tr, INT_MAX);
-		register_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_post,
-						 tr, 0);
+	} else if (pid_list && !other_pids) {
+		register_pid_events(tr);
 	}
 
 	/*
@@ -1680,9 +1778,24 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	return ret;
 }
 
+static ssize_t
+ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
+		       size_t cnt, loff_t *ppos)
+{
+	return event_pid_write(filp, ubuf, cnt, ppos, TRACE_PIDS);
+}
+
+static ssize_t
+ftrace_event_npid_write(struct file *filp, const char __user *ubuf,
+			size_t cnt, loff_t *ppos)
+{
+	return event_pid_write(filp, ubuf, cnt, ppos, TRACE_NO_PIDS);
+}
+
 static int ftrace_event_avail_open(struct inode *inode, struct file *file);
 static int ftrace_event_set_open(struct inode *inode, struct file *file);
 static int ftrace_event_set_pid_open(struct inode *inode, struct file *file);
+static int ftrace_event_set_npid_open(struct inode *inode, struct file *file);
 static int ftrace_event_release(struct inode *inode, struct file *file);
 
 static const struct seq_operations show_event_seq_ops = {
@@ -1706,6 +1819,13 @@ static const struct seq_operations show_set_pid_seq_ops = {
 	.stop = p_stop,
 };
 
+static const struct seq_operations show_set_no_pid_seq_ops = {
+	.start = np_start,
+	.next = np_next,
+	.show = trace_pid_show,
+	.stop = p_stop,
+};
+
 static const struct file_operations ftrace_avail_fops = {
 	.open = ftrace_event_avail_open,
 	.read = seq_read,
@@ -1729,6 +1849,14 @@ static const struct file_operations ftrace_set_event_pid_fops = {
 	.release = ftrace_event_release,
 };
 
+static const struct file_operations ftrace_set_event_notrace_pid_fops = {
+	.open = ftrace_event_set_npid_open,
+	.read = seq_read,
+	.write = ftrace_event_npid_write,
+	.llseek = seq_lseek,
+	.release = ftrace_event_release,
+};
+
 static const struct file_operations ftrace_enable_fops = {
 	.open = tracing_open_generic,
 	.read = event_enable_read,
@@ -1858,7 +1986,28 @@ ftrace_event_set_pid_open(struct inode *inode, struct file *file)
 
 	if ((file->f_mode & FMODE_WRITE) &&
 	    (file->f_flags & O_TRUNC))
-		ftrace_clear_event_pids(tr);
+		ftrace_clear_event_pids(tr, TRACE_PIDS);
+
+	ret = ftrace_event_open(inode, file, seq_ops);
+	if (ret < 0)
+		trace_array_put(tr);
+	return ret;
+}
+
+static int
+ftrace_event_set_npid_open(struct inode *inode, struct file *file)
+{
+	const struct seq_operations *seq_ops = &show_set_no_pid_seq_ops;
+	struct trace_array *tr = inode->i_private;
+	int ret;
+
+	ret = tracing_check_open_get_tr(tr);
+	if (ret)
+		return ret;
+
+	if ((file->f_mode & FMODE_WRITE) &&
+	    (file->f_flags & O_TRUNC))
+		ftrace_clear_event_pids(tr, TRACE_NO_PIDS);
 
 	ret = ftrace_event_open(inode, file, seq_ops);
 	if (ret < 0)
@@ -3075,6 +3224,11 @@ create_event_toplevel_files(struct dentry *parent, struct trace_array *tr)
 	if (!entry)
 		pr_warn("Could not create tracefs 'set_event_pid' entry\n");
 
+	entry = tracefs_create_file("set_event_notrace_pid", 0644, parent,
+				    tr, &ftrace_set_event_notrace_pid_fops);
+	if (!entry)
+		pr_warn("Could not create tracefs 'set_event_notrace_pid' entry\n");
+
 	/* ring buffer internal formats */
 	entry = trace_create_file("header_page", 0444, d_events,
 				  ring_buffer_print_page_header,
@@ -3158,7 +3312,7 @@ int event_trace_del_tracer(struct trace_array *tr)
 	clear_event_triggers(tr);
 
 	/* Clear the pid list */
-	__ftrace_clear_event_pids(tr);
+	__ftrace_clear_event_pids(tr, TRACE_PIDS | TRACE_NO_PIDS);
 
 	/* Disable any running events */
 	__ftrace_set_clr_event_nolock(tr, NULL, NULL, NULL, 0);

commit b3b1e6ededa4337940adba6cf06e8351056e3097
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Mar 19 23:19:06 2020 -0400

    ftrace: Create set_ftrace_notrace_pid to not trace tasks
    
    There's currently a way to select a task that should only be traced by
    functions, but there's no way to select a task not to be traced by the
    function tracer. Add a set_ftrace_notrace_pid file that acts the same as
    set_ftrace_pid (and is also affected by function-fork), but the task pids in
    this file will not be traced even if they are listed in the set_ftrace_pid
    file. This makes it easy for tools like trace-cmd to "hide" itself from the
    function tracer when it is recording other tasks.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f38234ecea18..c196d0dc5871 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -549,8 +549,8 @@ event_filter_pid_sched_switch_probe_pre(void *data, bool preempt,
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, prev) &&
-		       trace_ignore_this_task(pid_list, next));
+		       trace_ignore_this_task(pid_list, NULL, prev) &&
+		       trace_ignore_this_task(pid_list, NULL, next));
 }
 
 static void
@@ -563,7 +563,7 @@ event_filter_pid_sched_switch_probe_post(void *data, bool preempt,
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, next));
+		       trace_ignore_this_task(pid_list, NULL, next));
 }
 
 static void
@@ -579,7 +579,7 @@ event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, task));
+		       trace_ignore_this_task(pid_list, NULL, task));
 }
 
 static void
@@ -596,7 +596,7 @@ event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
 
 	/* Set tracing if current is enabled */
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, current));
+		       trace_ignore_this_task(pid_list, NULL, current));
 }
 
 static void __ftrace_clear_event_pids(struct trace_array *tr)
@@ -1597,7 +1597,7 @@ static void ignore_task_cpu(void *data)
 					     mutex_is_locked(&event_mutex));
 
 	this_cpu_write(tr->array_buffer.data->ignore_pid,
-		       trace_ignore_this_task(pid_list, current));
+		       trace_ignore_this_task(pid_list, NULL, current));
 }
 
 static ssize_t

commit e310396bb8d7db977a0e10ef7b5040e98b89c34c
Merge: c1ef57a3a3f5 a00574036c26
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 6 07:12:11 2020 +0000

    Merge tag 'trace-v5.6-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
    
     - Added new "bootconfig".
    
       This looks for a file appended to initrd to add boot config options,
       and has been discussed thoroughly at Linux Plumbers.
    
       Very useful for adding kprobes at bootup.
    
       Only enabled if "bootconfig" is on the real kernel command line.
    
     - Created dynamic event creation.
    
       Merges common code between creating synthetic events and kprobe
       events.
    
     - Rename perf "ring_buffer" structure to "perf_buffer"
    
     - Rename ftrace "ring_buffer" structure to "trace_buffer"
    
       Had to rename existing "trace_buffer" to "array_buffer"
    
     - Allow trace_printk() to work withing (some) tracing code.
    
     - Sort of tracing configs to be a little better organized
    
     - Fixed bug where ftrace_graph hash was not being protected properly
    
     - Various other small fixes and clean ups
    
    * tag 'trace-v5.6-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (88 commits)
      bootconfig: Show the number of nodes on boot message
      tools/bootconfig: Show the number of bootconfig nodes
      bootconfig: Add more parse error messages
      bootconfig: Use bootconfig instead of boot config
      ftrace: Protect ftrace_graph_hash with ftrace_sync
      ftrace: Add comment to why rcu_dereference_sched() is open coded
      tracing: Annotate ftrace_graph_notrace_hash pointer with __rcu
      tracing: Annotate ftrace_graph_hash pointer with __rcu
      bootconfig: Only load bootconfig if "bootconfig" is on the kernel cmdline
      tracing: Use seq_buf for building dynevent_cmd string
      tracing: Remove useless code in dynevent_arg_pair_add()
      tracing: Remove check_arg() callbacks from dynevent args
      tracing: Consolidate some synth_event_trace code
      tracing: Fix now invalid var_ref_vals assumption in trace action
      tracing: Change trace_boot to use synth_event interface
      tracing: Move tracing selftests to bottom of menu
      tracing: Move mmio tracer config up with the other tracers
      tracing: Move tracing test module configs together
      tracing: Move all function tracing configs together
      tracing: Documentation for in-kernel synthetic event API
      ...

commit 72f582ff8569900ccc4439b26bbe5e2fff509f08
Merge: bddea11b1b93 a3d1e7eb5abe
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Feb 5 05:09:46 2020 +0000

    Merge branch 'work.recursive_removal' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull vfs recursive removal updates from Al Viro:
     "We have quite a few places where synthetic filesystems do an
      equivalent of 'rm -rf', with varying amounts of code duplication,
      wrong locking, etc. That really ought to be a library helper.
    
      Only debugfs (and very similar tracefs) are converted here - I have
      more conversions, but they'd never been in -next, so they'll have to
      wait"
    
    * 'work.recursive_removal' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      simple_recursive_removal(): kernel-side rm -rf for ramfs-style filesystems

commit e3e2a2cc9c96725457ad6f31712ea7681a55666e
Author: Tom Zanussi <zanussi@kernel.org>
Date:   Wed Jan 29 12:59:22 2020 -0600

    tracing: Add trace_get/put_event_file()
    
    Add a function to get an event file and prevent it from going away on
    module or instance removal.
    
    trace_get_event_file() will find an event file in a given instance (if
    instance is NULL, it assumes the top trace array) and return it,
    pinning the instance's trace array as well as the event's module, if
    applicable, so they won't go away while in use.
    
    trace_put_event_file() does the matching release.
    
    Link: http://lkml.kernel.org/r/bb31ac4bdda168d5ed3c4b5f5a4c8f633e8d9118.1580323897.git.zanussi@kernel.org
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Tom Zanussi <zanussi@kernel.org>
    [ Moved trace_array_put() to end of trace_put_event_file() ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index dfb736a964d6..da62472b1297 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2536,6 +2536,91 @@ find_event_file(struct trace_array *tr, const char *system, const char *event)
 	return file;
 }
 
+/**
+ * trace_get_event_file - Find and return a trace event file
+ * @instance: The name of the trace instance containing the event
+ * @system: The name of the system containing the event
+ * @event: The name of the event
+ *
+ * Return a trace event file given the trace instance name, trace
+ * system, and trace event name.  If the instance name is NULL, it
+ * refers to the top-level trace array.
+ *
+ * This function will look it up and return it if found, after calling
+ * trace_array_get() to prevent the instance from going away, and
+ * increment the event's module refcount to prevent it from being
+ * removed.
+ *
+ * To release the file, call trace_put_event_file(), which will call
+ * trace_array_put() and decrement the event's module refcount.
+ *
+ * Return: The trace event on success, ERR_PTR otherwise.
+ */
+struct trace_event_file *trace_get_event_file(const char *instance,
+					      const char *system,
+					      const char *event)
+{
+	struct trace_array *tr = top_trace_array();
+	struct trace_event_file *file = NULL;
+	int ret = -EINVAL;
+
+	if (instance) {
+		tr = trace_array_find_get(instance);
+		if (!tr)
+			return ERR_PTR(-ENOENT);
+	} else {
+		ret = trace_array_get(tr);
+		if (ret)
+			return ERR_PTR(ret);
+	}
+
+	mutex_lock(&event_mutex);
+
+	file = find_event_file(tr, system, event);
+	if (!file) {
+		trace_array_put(tr);
+		ret = -EINVAL;
+		goto out;
+	}
+
+	/* Don't let event modules unload while in use */
+	ret = try_module_get(file->event_call->mod);
+	if (!ret) {
+		trace_array_put(tr);
+		ret = -EBUSY;
+		goto out;
+	}
+
+	ret = 0;
+ out:
+	mutex_unlock(&event_mutex);
+
+	if (ret)
+		file = ERR_PTR(ret);
+
+	return file;
+}
+EXPORT_SYMBOL_GPL(trace_get_event_file);
+
+/**
+ * trace_put_event_file - Release a file from trace_get_event_file()
+ * @file: The trace event file
+ *
+ * If a file was retrieved using trace_get_event_file(), this should
+ * be called when it's no longer needed.  It will cancel the previous
+ * trace_array_get() called by that function, and decrement the
+ * event's module refcount.
+ */
+void trace_put_event_file(struct trace_event_file *file)
+{
+	mutex_lock(&event_mutex);
+	module_put(file->event_call->mod);
+	mutex_unlock(&event_mutex);
+
+	trace_array_put(file->tr);
+}
+EXPORT_SYMBOL_GPL(trace_put_event_file);
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 
 /* Avoid typos */

commit 8cfcf15503f607e9597de19afeaa621897ae397e
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Sat Jan 11 01:05:31 2020 +0900

    tracing: kprobes: Output kprobe event to printk buffer
    
    Since kprobe-events use event_trigger_unlock_commit_regs() directly,
    that events doesn't show up in printk buffer if "tp_printk" is set.
    
    Use trace_event_buffer_commit() in kprobe events so that it can
    invoke output_printk() as same as other trace events.
    
    Link: http://lkml.kernel.org/r/157867233085.17873.5210928676787339604.stgit@devnote2
    
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    [ Adjusted data var declaration placement in __kretprobe_trace_func() ]
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a16d1b601c5c..dfb736a964d6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -272,6 +272,7 @@ void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
 	if (!fbuffer->event)
 		return NULL;
 
+	fbuffer->regs = NULL;
 	fbuffer->entry = ring_buffer_event_data(fbuffer->event);
 	return fbuffer->entry;
 }

commit 13292494379f92f532de71b31a54018336adc589
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 13 13:58:57 2019 -0500

    tracing: Make struct ring_buffer less ambiguous
    
    As there's two struct ring_buffers in the kernel, it causes some confusion.
    The other one being the perf ring buffer. It was agreed upon that as neither
    of the ring buffers are generic enough to be used globally, they should be
    renamed as:
    
       perf's ring_buffer -> perf_buffer
       ftrace's ring_buffer -> trace_buffer
    
    This implements the changes to the ring buffer that ftrace uses.
    
    Link: https://lore.kernel.org/r/20191213140531.116b3200@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ac557f685f0b..a16d1b601c5c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -3391,8 +3391,8 @@ static void __init
 function_test_events_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
+	struct trace_buffer *buffer;
 	struct ring_buffer_event *event;
-	struct ring_buffer *buffer;
 	struct ftrace_entry *entry;
 	unsigned long flags;
 	long disabled;

commit 1c5eb4481e0151d579f738175497f998840f7bbc
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jan 9 18:53:48 2020 -0500

    tracing: Rename trace_buffer to array_buffer
    
    As we are working to remove the generic "ring_buffer" name that is used by
    both tracing and perf, the ring_buffer name for tracing will be renamed to
    trace_buffer, and perf's ring buffer will be renamed to perf_buffer.
    
    As there already exists a trace_buffer that is used by the trace_arrays, it
    needs to be first renamed to array_buffer.
    
    Link: https://lore.kernel.org/r/20191213153553.GE20583@krava
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a5b614cc3887..ac557f685f0b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -237,7 +237,7 @@ bool trace_event_ignore_this_pid(struct trace_event_file *trace_file)
 	if (!pid_list)
 		return false;
 
-	data = this_cpu_ptr(tr->trace_buffer.data);
+	data = this_cpu_ptr(tr->array_buffer.data);
 
 	return data->ignore_pid;
 }
@@ -546,7 +546,7 @@ event_filter_pid_sched_switch_probe_pre(void *data, bool preempt,
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, prev) &&
 		       trace_ignore_this_task(pid_list, next));
 }
@@ -560,7 +560,7 @@ event_filter_pid_sched_switch_probe_post(void *data, bool preempt,
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, next));
 }
 
@@ -571,12 +571,12 @@ event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
 	struct trace_pid_list *pid_list;
 
 	/* Nothing to do if we are already tracing */
-	if (!this_cpu_read(tr->trace_buffer.data->ignore_pid))
+	if (!this_cpu_read(tr->array_buffer.data->ignore_pid))
 		return;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, task));
 }
 
@@ -587,13 +587,13 @@ event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
 	struct trace_pid_list *pid_list;
 
 	/* Nothing to do if we are not tracing */
-	if (this_cpu_read(tr->trace_buffer.data->ignore_pid))
+	if (this_cpu_read(tr->array_buffer.data->ignore_pid))
 		return;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	/* Set tracing if current is enabled */
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, current));
 }
 
@@ -625,7 +625,7 @@ static void __ftrace_clear_event_pids(struct trace_array *tr)
 	}
 
 	for_each_possible_cpu(cpu)
-		per_cpu_ptr(tr->trace_buffer.data, cpu)->ignore_pid = false;
+		per_cpu_ptr(tr->array_buffer.data, cpu)->ignore_pid = false;
 
 	rcu_assign_pointer(tr->filtered_pids, NULL);
 
@@ -1594,7 +1594,7 @@ static void ignore_task_cpu(void *data)
 	pid_list = rcu_dereference_protected(tr->filtered_pids,
 					     mutex_is_locked(&event_mutex));
 
-	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+	this_cpu_write(tr->array_buffer.data->ignore_pid,
 		       trace_ignore_this_task(pid_list, current));
 }
 

commit 46f5cfc13d54962097cb0cc5f97593e2a98a6aed
Merge: 46cf053efec6 2040cf9f5903
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Dec 25 10:43:08 2019 +0100

    Merge branch 'core/kprobes' into perf/core, to pick up a completed branch
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 3a53acf1d9bea11b57c1f6205e3fe73f9d8a3688
Author: Prateek Sood <prsood@codeaurora.org>
Date:   Tue Dec 10 09:15:16 2019 +0000

    tracing: Fix lock inversion in trace_event_enable_tgid_record()
    
           Task T2                             Task T3
    trace_options_core_write()            subsystem_open()
    
     mutex_lock(trace_types_lock)           mutex_lock(event_mutex)
    
     set_tracer_flag()
    
       trace_event_enable_tgid_record()       mutex_lock(trace_types_lock)
    
        mutex_lock(event_mutex)
    
    This gives a circular dependency deadlock between trace_types_lock and
    event_mutex. To fix this invert the usage of trace_types_lock and
    event_mutex in trace_options_core_write(). This keeps the sequence of
    lock usage consistent.
    
    Link: http://lkml.kernel.org/r/0101016eef175e38-8ca71caf-a4eb-480d-a1e6-6f0bbc015495-000000@us-west-2.amazonses.com
    
    Cc: stable@vger.kernel.org
    Fixes: d914ba37d7145 ("tracing: Add support for recording tgid of tasks")
    Signed-off-by: Prateek Sood <prsood@codeaurora.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c6de3cebc127..a5b614cc3887 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -320,7 +320,8 @@ void trace_event_enable_cmd_record(bool enable)
 	struct trace_event_file *file;
 	struct trace_array *tr;
 
-	mutex_lock(&event_mutex);
+	lockdep_assert_held(&event_mutex);
+
 	do_for_each_event_file(tr, file) {
 
 		if (!(file->flags & EVENT_FILE_FL_ENABLED))
@@ -334,7 +335,6 @@ void trace_event_enable_cmd_record(bool enable)
 			clear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 		}
 	} while_for_each_event_file();
-	mutex_unlock(&event_mutex);
 }
 
 void trace_event_enable_tgid_record(bool enable)
@@ -342,7 +342,8 @@ void trace_event_enable_tgid_record(bool enable)
 	struct trace_event_file *file;
 	struct trace_array *tr;
 
-	mutex_lock(&event_mutex);
+	lockdep_assert_held(&event_mutex);
+
 	do_for_each_event_file(tr, file) {
 		if (!(file->flags & EVENT_FILE_FL_ENABLED))
 			continue;
@@ -356,7 +357,6 @@ void trace_event_enable_tgid_record(bool enable)
 				  &file->flags);
 		}
 	} while_for_each_event_file();
-	mutex_unlock(&event_mutex);
 }
 
 static int __ftrace_event_enable_disable(struct trace_event_file *file,

commit a3d1e7eb5abe3aa1095bc75d1a6760d3809bd672
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Mon Nov 18 09:43:10 2019 -0500

    simple_recursive_removal(): kernel-side rm -rf for ramfs-style filesystems
    
    two requirements: no file creations in IS_DEADDIR and no cross-directory
    renames whatsoever.
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c6de3cebc127..1faf3a1e24a4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -697,7 +697,7 @@ static void remove_subsystem(struct trace_subsystem_dir *dir)
 		return;
 
 	if (!--dir->nr_events) {
-		tracefs_remove_recursive(dir->entry);
+		tracefs_remove(dir->entry);
 		list_del(&dir->list);
 		__put_system_dir(dir);
 	}
@@ -716,7 +716,7 @@ static void remove_event_file_dir(struct trace_event_file *file)
 		}
 		spin_unlock(&dir->d_lock);
 
-		tracefs_remove_recursive(dir);
+		tracefs_remove(dir);
 	}
 
 	list_del(&file->list);
@@ -3064,7 +3064,7 @@ int event_trace_del_tracer(struct trace_array *tr)
 
 	down_write(&trace_event_sem);
 	__trace_remove_event_dirs(tr);
-	tracefs_remove_recursive(tr->event_dir);
+	tracefs_remove(tr->event_dir);
 	up_write(&trace_event_sem);
 
 	tr->event_dir = NULL;

commit 2040cf9f59037aa8aec749363e69ead165b67b43
Merge: f66c0447cca1 e42617b825f8
Author: Ingo Molnar <mingo@kernel.org>
Date:   Tue Dec 10 10:11:00 2019 +0100

    Merge tag 'v5.5-rc1' into core/kprobes, to resolve conflicts
    
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

commit 6c3edaf9fd6a3be7fb5bc6931897c24cd3848f84
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Fri Nov 29 20:52:18 2019 -0800

    tracing: Introduce trace event injection
    
    We have been trying to use rasdaemon to monitor hardware errors like
    correctable memory errors. rasdaemon uses trace events to monitor
    various hardware errors. In order to test it, we have to inject some
    hardware errors, unfortunately not all of them provide error
    injections. MCE does provide a way to inject MCE errors, but errors
    like PCI error and devlink error don't, it is not easy to add error
    injection to each of them. Instead, it is relatively easier to just
    allow users to inject trace events in a generic way so that all trace
    events can be injected.
    
    This patch introduces trace event injection, where a new 'inject' is
    added to each tracepoint directory. Users could write into this file
    with key=value pairs to specify the value of each fields of the trace
    event, all unspecified fields are set to zero values by default.
    
    For example, for the net/net_dev_queue tracepoint, we can inject:
    
      INJECT=/sys/kernel/debug/tracing/events/net/net_dev_queue/inject
      echo "" > $INJECT
      echo "name='test'" > $INJECT
      echo "name='test' len=1024" > $INJECT
      cat /sys/kernel/debug/tracing/trace
      ...
       <...>-614   [000] ....    36.571483: net_dev_queue: dev= skbaddr=00000000fbf338c2 len=0
       <...>-614   [001] ....   136.588252: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=0
       <...>-614   [001] .N..   208.431878: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=1024
    
    Triggers could be triggered as usual too:
    
      echo "stacktrace if len == 1025" > /sys/kernel/debug/tracing/events/net/net_dev_queue/trigger
      echo "len=1025" > $INJECT
      cat /sys/kernel/debug/tracing/trace
      ...
          bash-614   [000] ....    36.571483: net_dev_queue: dev= skbaddr=00000000fbf338c2 len=0
          bash-614   [001] ....   136.588252: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=0
          bash-614   [001] .N..   208.431878: net_dev_queue: dev=test skbaddr=00000000fbf338c2 len=1024
          bash-614   [001] .N.1   284.236349: <stack trace>
     => event_inject_write
     => vfs_write
     => ksys_write
     => do_syscall_64
     => entry_SYSCALL_64_after_hwframe
    
    The only thing that can't be injected is string pointers as they
    require constant string pointers, this can't be done at run time.
    
    Link: http://lkml.kernel.org/r/20191130045218.18979-1-xiyou.wangcong@gmail.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 6b3a69e9aa6a..c6de3cebc127 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2044,6 +2044,12 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 	trace_create_file("format", 0444, file->dir, call,
 			  &ftrace_event_format_fops);
 
+#ifdef CONFIG_TRACE_EVENT_INJECT
+	if (call->event.type && call->class->reg)
+		trace_create_file("inject", 0200, file->dir, file,
+				  &event_inject_fops);
+#endif
+
 	return 0;
 }
 

commit 04ae87a52074e2d448fc66143f1bd2c7d694d2b9
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Thu Oct 24 22:26:59 2019 +0200

    ftrace: Rework event_create_dir()
    
    Rework event_create_dir() to use an array of static data instead of
    function pointers where possible.
    
    The problem is that it would call the function pointer on module load
    before parse_args(), possibly even before jump_labels were initialized.
    Luckily the generated functions don't use jump_labels but it still seems
    fragile. It also gets in the way of changing when we make the module map
    executable.
    
    The generated function are basically calling trace_define_field() with a
    bunch of static arguments. So instead of a function, capture these
    arguments in a static array, avoiding the function call.
    
    Now there are a number of cases where the fields are dynamic (syscall
    arguments, kprobes and uprobes), in which case a static array does not
    work, for these we preserve the function call. Luckily all these cases
    are not related to modules and so we can retain the function call for
    them.
    
    Also fix up all broken tracepoint definitions that now generate a
    compile error.
    
    Tested-by: Alexei Starovoitov <ast@kernel.org>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Reviewed-by: Steven Rostedt (VMware) <rostedt@goodmis.org>
    Acked-by: Alexei Starovoitov <ast@kernel.org>
    Cc: Andy Lutomirski <luto@kernel.org>
    Cc: Borislav Petkov <bp@alien8.de>
    Cc: Brian Gerst <brgerst@gmail.com>
    Cc: Denys Vlasenko <dvlasenk@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Link: https://lkml.kernel.org/r/20191111132458.342979914@infradead.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fba87d10f0c1..5ab10c3dce78 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -24,6 +24,7 @@
 #include <linux/delay.h>
 
 #include <trace/events/sched.h>
+#include <trace/syscall.h>
 
 #include <asm/setup.h>
 
@@ -1990,7 +1991,24 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 	 */
 	head = trace_get_fields(call);
 	if (list_empty(head)) {
-		ret = call->class->define_fields(call);
+		struct trace_event_fields *field = call->class->fields_array;
+		unsigned int offset = sizeof(struct trace_entry);
+
+		for (; field->type; field++) {
+			if (field->type == TRACE_FUNCTION_TYPE) {
+				ret = field->define_fields(call);
+				break;
+			}
+
+			offset = ALIGN(offset, field->align);
+			ret = trace_define_field(call, field->type, field->name,
+						 offset, field->size,
+						 field->is_signed, field->filter_type);
+			if (ret)
+				break;
+
+			offset += field->size;
+		}
 		if (ret < 0) {
 			pr_warn("Could not initialize trace point events/%s\n",
 				name);

commit 28879787147358e8ffcae397f11748de3dd26577
Author: Divya Indi <divya.indi@oracle.com>
Date:   Wed Nov 20 11:08:38 2019 -0800

    tracing: Adding new functions for kernel access to Ftrace instances
    
    Adding 2 new functions -
    1) struct trace_array *trace_array_get_by_name(const char *name);
    
    Return pointer to a trace array with given name. If it does not exist,
    create and return pointer to the new trace array.
    
    2) int trace_array_set_clr_event(struct trace_array *tr,
    const char *system ,const char *event, bool enable);
    
    Enable/Disable events to this trace array.
    
    Additionally,
    - To handle reference counters, export trace_array_put()
    - Due to introduction of the above 2 new functions, we no longer need to
      export - ftrace_set_clr_event & trace_array_create APIs.
    
    Link: http://lkml.kernel.org/r/1574276919-11119-2-git-send-email-divya.indi@oracle.com
    
    Signed-off-by: Divya Indi <divya.indi@oracle.com>
    Reviewed-by: Aruna Ramakrishna <aruna.ramakrishna@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2a3ac2365445..6b3a69e9aa6a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -827,7 +827,6 @@ int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
 
 	return ret;
 }
-EXPORT_SYMBOL_GPL(ftrace_set_clr_event);
 
 /**
  * trace_set_clr_event - enable or disable an event
@@ -852,6 +851,32 @@ int trace_set_clr_event(const char *system, const char *event, int set)
 }
 EXPORT_SYMBOL_GPL(trace_set_clr_event);
 
+/**
+ * trace_array_set_clr_event - enable or disable an event for a trace array.
+ * @tr: concerned trace array.
+ * @system: system name to match (NULL for any system)
+ * @event: event name to match (NULL for all events, within system)
+ * @enable: true to enable, false to disable
+ *
+ * This is a way for other parts of the kernel to enable or disable
+ * event recording.
+ *
+ * Returns 0 on success, -EINVAL if the parameters do not match any
+ * registered events.
+ */
+int trace_array_set_clr_event(struct trace_array *tr, const char *system,
+		const char *event, bool enable)
+{
+	int set;
+
+	if (!tr)
+		return -ENOENT;
+
+	set = (enable == true) ? 1 : 0;
+	return __ftrace_set_clr_event(tr, NULL, system, event, set);
+}
+EXPORT_SYMBOL_GPL(trace_array_set_clr_event);
+
 /* 128 should be much more than enough */
 #define EVENT_BUF_SIZE		127
 

commit 953ae45a0c25e09428d4a03d7654f97ab8a36647
Author: Divya Indi <divya.indi@oracle.com>
Date:   Wed Aug 14 10:55:25 2019 -0700

    tracing: Adding NULL checks for trace_array descriptor pointer
    
    As part of commit f45d1225adb0 ("tracing: Kernel access to Ftrace
    instances") we exported certain functions. Here, we are adding some additional
    NULL checks to ensure safe usage by users of these APIs.
    
    Link: http://lkml.kernel.org/r/1565805327-579-4-git-send-email-divya.indi@oracle.com
    
    Signed-off-by: Divya Indi <divya.indi@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fba87d10f0c1..2a3ac2365445 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -793,6 +793,8 @@ int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
 	char *event = NULL, *sub = NULL, *match;
 	int ret;
 
+	if (!tr)
+		return -ENOENT;
 	/*
 	 * The buf format can be <subsystem>:<event-name>
 	 *  *:<event-name> means any event by that name.

commit 17911ff38aa58d3c95c07589dbf5d3564c4cf3c5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Oct 11 17:22:50 2019 -0400

    tracing: Add locked_down checks to the open calls of files created for tracefs
    
    Added various checks on open tracefs calls to see if tracefs is in lockdown
    mode, and if so, to return -EPERM.
    
    Note, the event format files (which are basically standard on all machines)
    as well as the enabled_functions file (which shows what is currently being
    traced) are not lockde down. Perhaps they should be, but it seems counter
    intuitive to lockdown information to help you know if the system has been
    modified.
    
    Link: http://lkml.kernel.org/r/CAHk-=wj7fGPKUspr579Cii-w_y60PtRaiDgKuxVtBAMK0VNNkA@mail.gmail.com
    
    Suggested-by: Linus Torvalds <torvalds@linux-foundation.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 71ab0a3660f4..fba87d10f0c1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -12,6 +12,7 @@
 #define pr_fmt(fmt) fmt
 
 #include <linux/workqueue.h>
+#include <linux/security.h>
 #include <linux/spinlock.h>
 #include <linux/kthread.h>
 #include <linux/tracefs.h>
@@ -1294,6 +1295,8 @@ static int trace_format_open(struct inode *inode, struct file *file)
 	struct seq_file *m;
 	int ret;
 
+	/* Do we want to hide event format files on tracefs lockdown? */
+
 	ret = seq_open(file, &trace_format_seq_ops);
 	if (ret < 0)
 		return ret;
@@ -1760,6 +1763,10 @@ ftrace_event_open(struct inode *inode, struct file *file,
 	struct seq_file *m;
 	int ret;
 
+	ret = security_locked_down(LOCKDOWN_TRACEFS);
+	if (ret)
+		return ret;
+
 	ret = seq_open(file, seq_ops);
 	if (ret < 0)
 		return ret;
@@ -1784,6 +1791,7 @@ ftrace_event_avail_open(struct inode *inode, struct file *file)
 {
 	const struct seq_operations *seq_ops = &show_event_seq_ops;
 
+	/* Checks for tracefs lockdown */
 	return ftrace_event_open(inode, file, seq_ops);
 }
 

commit 8530dec63e7b486e3761cc3d74a22de301845ff5
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Oct 11 17:39:57 2019 -0400

    tracing: Add tracing_check_open_get_tr()
    
    Currently, most files in the tracefs directory test if tracing_disabled is
    set. If so, it should return -ENODEV. The tracing_disabled is called when
    tracing is found to be broken. Originally it was done in case the ring
    buffer was found to be corrupted, and we wanted to prevent reading it from
    crashing the kernel. But it's also called if a tracing selftest fails on
    boot. It's a one way switch. That is, once it is triggered, tracing is
    disabled until reboot.
    
    As most tracefs files can also be used by instances in the tracefs
    directory, they need to be carefully done. Each instance has a trace_array
    associated to it, and when the instance is removed, the trace_array is
    freed. But if an instance is opened with a reference to the trace_array,
    then it requires looking up the trace_array to get its ref counter (as there
    could be a race with it being deleted and the open itself). Once it is
    found, a reference is added to prevent the instance from being removed (and
    the trace_array associated with it freed).
    
    Combine the two checks (tracing_disabled and trace_array_get()) into a
    single helper function. This will also make it easier to add lockdown to
    tracefs later.
    
    Link: http://lkml.kernel.org/r/20191011135458.7399da44@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9613a757c902..71ab0a3660f4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1794,8 +1794,9 @@ ftrace_event_set_open(struct inode *inode, struct file *file)
 	struct trace_array *tr = inode->i_private;
 	int ret;
 
-	if (trace_array_get(tr) < 0)
-		return -ENODEV;
+	ret = tracing_check_open_get_tr(tr);
+	if (ret)
+		return ret;
 
 	if ((file->f_mode & FMODE_WRITE) &&
 	    (file->f_flags & O_TRUNC))
@@ -1814,8 +1815,9 @@ ftrace_event_set_pid_open(struct inode *inode, struct file *file)
 	struct trace_array *tr = inode->i_private;
 	int ret;
 
-	if (trace_array_get(tr) < 0)
-		return -ENODEV;
+	ret = tracing_check_open_get_tr(tr);
+	if (ret)
+		return ret;
 
 	if ((file->f_mode & FMODE_WRITE) &&
 	    (file->f_flags & O_TRUNC))

commit aa07d71f1bc7ea20e442e812b5de9d632b7f84c6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Oct 11 19:12:21 2019 -0400

    tracing: Have trace events system open call tracing_open_generic_tr()
    
    Instead of having the trace events system open call open code the taking of
    the trace_array descriptor (with trace_array_get()) and then calling
    trace_open_generic(), have it use the tracing_open_generic_tr() that does
    the combination of the two. This requires making tracing_open_generic_tr()
    global.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b89cdfe20bc1..9613a757c902 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1440,28 +1440,17 @@ static int system_tr_open(struct inode *inode, struct file *filp)
 	struct trace_array *tr = inode->i_private;
 	int ret;
 
-	if (tracing_is_disabled())
-		return -ENODEV;
-
-	if (trace_array_get(tr) < 0)
-		return -ENODEV;
-
 	/* Make a temporary dir that has no system but points to tr */
 	dir = kzalloc(sizeof(*dir), GFP_KERNEL);
-	if (!dir) {
-		trace_array_put(tr);
+	if (!dir)
 		return -ENOMEM;
-	}
-
-	dir->tr = tr;
 
-	ret = tracing_open_generic(inode, filp);
+	ret = tracing_open_generic_tr(inode, filp);
 	if (ret < 0) {
-		trace_array_put(tr);
 		kfree(dir);
 		return ret;
 	}
-
+	dir->tr = tr;
 	filp->private_data = dir;
 
 	return 0;

commit 7e67a859997aad47727aff9c5a32e160da079ce3
Merge: 772c1d06bd40 563c4f85f9f0
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Sep 16 17:25:49 2019 -0700

    Merge branch 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip
    
    Pull scheduler updates from Ingo Molnar:
    
     - MAINTAINERS: Add Mark Rutland as perf submaintainer, Juri Lelli and
       Vincent Guittot as scheduler submaintainers. Add Dietmar Eggemann,
       Steven Rostedt, Ben Segall and Mel Gorman as scheduler reviewers.
    
       As perf and the scheduler is getting bigger and more complex,
       document the status quo of current responsibilities and interests,
       and spread the review pain^H^H^H^H fun via an increase in the Cc:
       linecount generated by scripts/get_maintainer.pl. :-)
    
     - Add another series of patches that brings the -rt (PREEMPT_RT) tree
       closer to mainline: split the monolithic CONFIG_PREEMPT dependencies
       into a new CONFIG_PREEMPTION category that will allow the eventual
       introduction of CONFIG_PREEMPT_RT. Still a few more hundred patches
       to go though.
    
     - Extend the CPU cgroup controller with uclamp.min and uclamp.max to
       allow the finer shaping of CPU bandwidth usage.
    
     - Micro-optimize energy-aware wake-ups from O(CPUS^2) to O(CPUS).
    
     - Improve the behavior of high CPU count, high thread count
       applications running under cpu.cfs_quota_us constraints.
    
     - Improve balancing with SCHED_IDLE (SCHED_BATCH) tasks present.
    
     - Improve CPU isolation housekeeping CPU allocation NUMA locality.
    
     - Fix deadline scheduler bandwidth calculations and logic when cpusets
       rebuilds the topology, or when it gets deadline-throttled while it's
       being offlined.
    
     - Convert the cpuset_mutex to percpu_rwsem, to allow it to be used from
       setscheduler() system calls without creating global serialization.
       Add new synchronization between cpuset topology-changing events and
       the deadline acceptance tests in setscheduler(), which were broken
       before.
    
     - Rework the active_mm state machine to be less confusing and more
       optimal.
    
     - Rework (simplify) the pick_next_task() slowpath.
    
     - Improve load-balancing on AMD EPYC systems.
    
     - ... and misc cleanups, smaller fixes and improvements - please see
       the Git log for more details.
    
    * 'sched-core-for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip: (53 commits)
      sched/psi: Correct overly pessimistic size calculation
      sched/fair: Speed-up energy-aware wake-ups
      sched/uclamp: Always use 'enum uclamp_id' for clamp_id values
      sched/uclamp: Update CPU's refcount on TG's clamp changes
      sched/uclamp: Use TG's clamps to restrict TASK's clamps
      sched/uclamp: Propagate system defaults to the root group
      sched/uclamp: Propagate parent clamps
      sched/uclamp: Extend CPU's cgroup controller
      sched/topology: Improve load balancing on AMD EPYC systems
      arch, ia64: Make NUMA select SMP
      sched, perf: MAINTAINERS update, add submaintainers and reviewers
      sched/fair: Use rq_lock/unlock in online_fair_sched_group
      cpufreq: schedutil: fix equation in comment
      sched: Rework pick_next_task() slow-path
      sched: Allow put_prev_task() to drop rq->lock
      sched/fair: Expose newidle_balance()
      sched: Add task_struct pointer to sched_class::set_curr_task
      sched: Rework CPU hotplug task selection
      sched/{rt,deadline}: Fix set_next_task vs pick_next_task
      sched: Fix kerneldoc comment for ia64_set_curr_task
      ...

commit 595a438c78dbdc43d6c9db4f437267f0bd1548bf
Author: Denis Efremov <efremov@linux.com>
Date:   Thu Jul 4 20:21:10 2019 +0300

    tracing: Make exported ftrace_set_clr_event non-static
    
    The function ftrace_set_clr_event is declared static and marked
    EXPORT_SYMBOL_GPL(), which is at best an odd combination. Because the
    function was decided to be a part of API, this commit removes the static
    attribute and adds the declaration to the header.
    
    Link: http://lkml.kernel.org/r/20190704172110.27041-1-efremov@linux.com
    
    Fixes: f45d1225adb04 ("tracing: Kernel access to Ftrace instances")
    Reviewed-by: Joe Jin <joe.jin@oracle.com>
    Signed-off-by: Denis Efremov <efremov@linux.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c7506bc81b75..648930823b57 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -787,7 +787,7 @@ static int __ftrace_set_clr_event(struct trace_array *tr, const char *match,
 	return ret;
 }
 
-static int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
+int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
 {
 	char *event = NULL, *sub = NULL, *match;
 	int ret;

commit 30c937043b2db09ae3408f5534824f9ececdb581
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 26 23:19:40 2019 +0200

    tracing: Use CONFIG_PREEMPTION
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by
    CONFIG_PREEMPT_RT. Both PREEMPT and PREEMPT_RT require the same
    functionality which today depends on CONFIG_PREEMPT.
    
    Switch the conditionals in the tracer over to CONFIG_PREEMPTION.
    
    This is the first step to make the tracer work on RT. The other small
    tweaks are submitted separately.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20190726212124.409766323@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c7506bc81b75..5a189fb8ec23 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -255,12 +255,12 @@ void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
 	local_save_flags(fbuffer->flags);
 	fbuffer->pc = preempt_count();
 	/*
-	 * If CONFIG_PREEMPT is enabled, then the tracepoint itself disables
+	 * If CONFIG_PREEMPTION is enabled, then the tracepoint itself disables
 	 * preemption (adding one to the preempt_count). Since we are
 	 * interested in the preempt_count at the time the tracepoint was
 	 * hit, we need to subtract one to offset the increment.
 	 */
-	if (IS_ENABLED(CONFIG_PREEMPT))
+	if (IS_ENABLED(CONFIG_PREEMPTION))
 		fbuffer->pc--;
 	fbuffer->trace_file = trace_file;
 

commit 0aeb1def44169cbe7119f26cf10b974a2046142e
Author: Cong Wang <xiyou.wangcong@gmail.com>
Date:   Sat May 25 09:58:01 2019 -0700

    tracing: Make trace_get_fields() global
    
    trace_get_fields() is the only way to read tracepoint fields at
    run time, as their fields are defined at compile-time with macros.
    Make this function visible to all users and it will be used by
    trace event injection code to calculate the size of a tracepoint
    entry.
    Link: http://lkml.kernel.org/r/20190525165802.25944-4-xiyou.wangcong@gmail.com
    
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Cong Wang <xiyou.wangcong@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index edc72f3b080c..c7506bc81b75 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -70,14 +70,6 @@ static int system_refcount_dec(struct event_subsystem *system)
 #define while_for_each_event_file()		\
 	}
 
-static struct list_head *
-trace_get_fields(struct trace_event_call *event_call)
-{
-	if (!event_call->class->get_fields)
-		return &event_call->class->fields;
-	return event_call->class->get_fields(event_call);
-}
-
 static struct ftrace_event_field *
 __find_event_field(struct list_head *head, char *name)
 {

commit b3015fe41d9af7515a7b7b6f7f8f172d193fb3a6
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu May 23 19:40:17 2019 -0400

    tracing: Make a separate config for trace event self tests
    
    The trace event self tests enable loop through *all* events, enables each
    one, one at a time, runs some code to trigger various events (not
    necessarily the same events), and checks if anything went wrong. The issue
    is that trace events are usually the least likely start up test to cause a
    problem, but they take the longest to run (because there are so many
    events). When one of the other tests trigger a bug, the trace event start up
    tests causes the bisect to take much longer, because it takes 10s of seconds
    to get through the trace event tests.
    
    By making them a separate config (even though they are enabled by default if
    start up tests are set), it is possible to turn them off and still run the
    other tracing start up tests much quicker.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0ce3db67f556..edc72f3b080c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -3190,7 +3190,7 @@ void __init trace_event_init(void)
 	event_trace_enable();
 }
 
-#ifdef CONFIG_FTRACE_STARTUP_TEST
+#ifdef CONFIG_EVENT_TRACE_STARTUP_TEST
 
 static DEFINE_SPINLOCK(test_spinlock);
 static DEFINE_SPINLOCK(test_spinlock_irq);

commit cbe08bcbbe787315c425dde284dcb715cfbf3f39
Author: Elazar Leibovich <elazar@lightbitslabs.com>
Date:   Mon Dec 31 13:58:37 2018 +0200

    tracing: Fix partial reading of trace event's id file
    
    When reading only part of the id file, the ppos isn't tracked correctly.
    This is taken care by simple_read_from_buffer.
    
    Reading a single byte, and then the next byte would result EOF.
    
    While this seems like not a big deal, this breaks abstractions that
    reads information from files unbuffered. See for example
    https://github.com/golang/go/issues/29399
    
    This code was mentioned as problematic in
    commit cd458ba9d5a5
    ("tracing: Do not (ab)use trace_seq in event_id_read()")
    
    An example C code that show this bug is:
    
      #include <stdio.h>
      #include <stdint.h>
    
      #include <sys/types.h>
      #include <sys/stat.h>
      #include <fcntl.h>
      #include <unistd.h>
    
      int main(int argc, char **argv) {
        if (argc < 2)
          return 1;
        int fd = open(argv[1], O_RDONLY);
        char c;
        read(fd, &c, 1);
        printf("First  %c\n", c);
        read(fd, &c, 1);
        printf("Second %c\n", c);
      }
    
    Then run with, e.g.
    
      sudo ./a.out /sys/kernel/debug/tracing/events/tcp/tcp_set_state/id
    
    You'll notice you're getting the first character twice, instead of the
    first two characters in the id file.
    
    Link: http://lkml.kernel.org/r/20181231115837.4932-1-elazar@lightbitslabs.com
    
    Cc: Orit Wasserman <orit.was@gmail.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: stable@vger.kernel.org
    Fixes: 23725aeeab10b ("ftrace: provide an id file for each event")
    Signed-off-by: Elazar Leibovich <elazar@lightbitslabs.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 81c038ed6cee..0ce3db67f556 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1319,9 +1319,6 @@ event_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 	char buf[32];
 	int len;
 
-	if (*ppos)
-		return 0;
-
 	if (unlikely(!id))
 		return -ENODEV;
 

commit f45d1225adb0479478cee989e2ae2d7d2c62b31b
Author: Divya Indi <divya.indi@oracle.com>
Date:   Wed Mar 20 11:28:51 2019 -0700

    tracing: Kernel access to Ftrace instances
    
    Ftrace provides the feature instances that provides the capability to
    create multiple Ftrace ring buffers. However, currently these buffers
    are created/accessed via userspace only. The kernel APIs providing these
    features are not exported, hence cannot be used by other kernel
    components.
    
    This patch aims to extend this infrastructure to provide the
    flexibility to create/log/remove/ enable-disable existing trace events
    to these buffers from within the kernel.
    
    Link: http://lkml.kernel.org/r/1553106531-3281-2-git-send-email-divya.indi@oracle.com
    
    Signed-off-by: Divya Indi <divya.indi@oracle.com>
    Reviewed-by: Joe Jin <joe.jin@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5b3b0c3c8a47..81c038ed6cee 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -832,6 +832,7 @@ static int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(ftrace_set_clr_event);
 
 /**
  * trace_set_clr_event - enable or disable an event

commit b6b2735514bcd70ad1556a33892a636b20ece671
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Dec 20 13:20:07 2018 -0500

    tracing: Use str_has_prefix() instead of using fixed sizes
    
    There are several instances of strncmp(str, "const", 123), where 123 is the
    strlen of the const string to check if "const" is the prefix of str. But
    this can be error prone. Use str_has_prefix() instead.
    
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index bd0162c0467c..5b3b0c3c8a47 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1251,7 +1251,7 @@ static int f_show(struct seq_file *m, void *v)
 	 */
 	array_descriptor = strchr(field->type, '[');
 
-	if (!strncmp(field->type, "__data_loc", 10))
+	if (str_has_prefix(field->type, "__data_loc"))
 		array_descriptor = NULL;
 
 	if (!array_descriptor)

commit 7e1413edd6194a9807aa5f3ac0378b9b4b9da879
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Dec 4 13:35:45 2018 -0500

    tracing: Consolidate trace_add/remove_event_call back to the nolock functions
    
    The trace_add/remove_event_call_nolock() functions were added to allow
    the tace_add/remove_event_call() code be called when the event_mutex
    lock was already taken. Now that all callers are done within the
    event_mutex, there's no reason to have two different interfaces.
    
    Remove the current wrapper trace_add/remove_event_call()s and rename the
    _nolock versions back to the original names.
    
    Link: http://lkml.kernel.org/r/154140866955.17322.2081425494660638846.stgit@devbox
    
    Acked-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a3b157f689ee..bd0162c0467c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2305,7 +2305,8 @@ __trace_early_add_new_event(struct trace_event_call *call,
 struct ftrace_module_file_ops;
 static void __add_event_to_tracers(struct trace_event_call *call);
 
-int trace_add_event_call_nolock(struct trace_event_call *call)
+/* Add an additional event_call dynamically */
+int trace_add_event_call(struct trace_event_call *call)
 {
 	int ret;
 	lockdep_assert_held(&event_mutex);
@@ -2320,17 +2321,6 @@ int trace_add_event_call_nolock(struct trace_event_call *call)
 	return ret;
 }
 
-/* Add an additional event_call dynamically */
-int trace_add_event_call(struct trace_event_call *call)
-{
-	int ret;
-
-	mutex_lock(&event_mutex);
-	ret = trace_add_event_call_nolock(call);
-	mutex_unlock(&event_mutex);
-	return ret;
-}
-
 /*
  * Must be called under locking of trace_types_lock, event_mutex and
  * trace_event_sem.
@@ -2376,8 +2366,8 @@ static int probe_remove_event_call(struct trace_event_call *call)
 	return 0;
 }
 
-/* no event_mutex version */
-int trace_remove_event_call_nolock(struct trace_event_call *call)
+/* Remove an event_call */
+int trace_remove_event_call(struct trace_event_call *call)
 {
 	int ret;
 
@@ -2392,18 +2382,6 @@ int trace_remove_event_call_nolock(struct trace_event_call *call)
 	return ret;
 }
 
-/* Remove an event_call */
-int trace_remove_event_call(struct trace_event_call *call)
-{
-	int ret;
-
-	mutex_lock(&event_mutex);
-	ret = trace_remove_event_call_nolock(call);
-	mutex_unlock(&event_mutex);
-
-	return ret;
-}
-
 #define for_each_event(event, start, end)			\
 	for (event = start;					\
 	     (unsigned long)event < (unsigned long)end;		\

commit fc800a10be26017f8f338bc8e500d48e3e6429d9
Author: Masami Hiramatsu <mhiramat@kernel.org>
Date:   Mon Nov 5 18:00:43 2018 +0900

    tracing: Lock event_mutex before synth_event_mutex
    
    synthetic event is using synth_event_mutex for protecting
    synth_event_list, and event_trigger_write() path acquires
    locks as below order.
    
    event_trigger_write(event_mutex)
      ->trigger_process_regex(trigger_cmd_mutex)
        ->event_hist_trigger_func(synth_event_mutex)
    
    On the other hand, synthetic event creation and deletion paths
    call trace_add_event_call() and trace_remove_event_call()
    which acquires event_mutex. In that case, if we keep the
    synth_event_mutex locked while registering/unregistering synthetic
    events, its dependency will be inversed.
    
    To avoid this issue, current synthetic event is using a 2 phase
    process to create/delete events. For example, it searches existing
    events under synth_event_mutex to check for event-name conflicts, and
    unlocks synth_event_mutex, then registers a new event under event_mutex
    locked. Finally, it locks synth_event_mutex and tries to add the
    new event to the list. But it can introduce complexity and a chance
    for name conflicts.
    
    To solve this simpler, this introduces trace_add_event_call_nolock()
    and trace_remove_event_call_nolock() which don't acquire
    event_mutex inside. synthetic event can lock event_mutex before
    synth_event_mutex to solve the lock dependency issue simpler.
    
    Link: http://lkml.kernel.org/r/154140844377.17322.13781091165954002713.stgit@devbox
    
    Reviewed-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Masami Hiramatsu <mhiramat@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f94be0c2827b..a3b157f689ee 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2305,11 +2305,11 @@ __trace_early_add_new_event(struct trace_event_call *call,
 struct ftrace_module_file_ops;
 static void __add_event_to_tracers(struct trace_event_call *call);
 
-/* Add an additional event_call dynamically */
-int trace_add_event_call(struct trace_event_call *call)
+int trace_add_event_call_nolock(struct trace_event_call *call)
 {
 	int ret;
-	mutex_lock(&event_mutex);
+	lockdep_assert_held(&event_mutex);
+
 	mutex_lock(&trace_types_lock);
 
 	ret = __register_event(call, NULL);
@@ -2317,6 +2317,16 @@ int trace_add_event_call(struct trace_event_call *call)
 		__add_event_to_tracers(call);
 
 	mutex_unlock(&trace_types_lock);
+	return ret;
+}
+
+/* Add an additional event_call dynamically */
+int trace_add_event_call(struct trace_event_call *call)
+{
+	int ret;
+
+	mutex_lock(&event_mutex);
+	ret = trace_add_event_call_nolock(call);
 	mutex_unlock(&event_mutex);
 	return ret;
 }
@@ -2366,17 +2376,29 @@ static int probe_remove_event_call(struct trace_event_call *call)
 	return 0;
 }
 
-/* Remove an event_call */
-int trace_remove_event_call(struct trace_event_call *call)
+/* no event_mutex version */
+int trace_remove_event_call_nolock(struct trace_event_call *call)
 {
 	int ret;
 
-	mutex_lock(&event_mutex);
+	lockdep_assert_held(&event_mutex);
+
 	mutex_lock(&trace_types_lock);
 	down_write(&trace_event_sem);
 	ret = probe_remove_event_call(call);
 	up_write(&trace_event_sem);
 	mutex_unlock(&trace_types_lock);
+
+	return ret;
+}
+
+/* Remove an event_call */
+int trace_remove_event_call(struct trace_event_call *call)
+{
+	int ret;
+
+	mutex_lock(&event_mutex);
+	ret = trace_remove_event_call_nolock(call);
 	mutex_unlock(&event_mutex);
 
 	return ret;

commit bcea3f96e11cf2f0232d851e0fdb854f5ada425a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 16 11:23:53 2018 -0400

    tracing: Add SPDX License format tags to tracing files
    
    Add the SPDX License header to ease license compliance management.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 808cf29febe2..f94be0c2827b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * event tracer
  *

commit e0a568dcd18bdaa77877d558700ce4d3bbbb12b8
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 9 15:31:48 2018 -0400

    tracing: Fix synchronizing to event changes with tracepoint_synchronize_unregister()
    
    Now that some trace events can be protected by srcu_read_lock(tracepoint_srcu),
    we need to make sure all locations that depend on this are also protected.
    There were many places that did a synchronize_sched() thinking that it was
    enough to protect againts access to trace events. This use to be the case,
    but now that we use SRCU for _rcuidle() trace events, they may not be
    protected by synchronize_sched(), as they may be called in paths that RCU is
    not watching for preempt disable.
    
    Fixes: e6753f23d961d ("tracepoint: Make rcuidle tracepoint callers use SRCU")
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7b508ce8ac44..808cf29febe2 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -636,7 +636,7 @@ static void __ftrace_clear_event_pids(struct trace_array *tr)
 	rcu_assign_pointer(tr->filtered_pids, NULL);
 
 	/* Wait till all users are no longer using pid filtering */
-	synchronize_sched();
+	tracepoint_synchronize_unregister();
 
 	trace_free_pid_list(pid_list);
 }
@@ -1622,7 +1622,7 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	}
 
 	if (filtered_pids) {
-		synchronize_sched();
+		tracepoint_synchronize_unregister();
 		trace_free_pid_list(filtered_pids);
 	} else if (pid_list) {
 		/*
@@ -3036,8 +3036,8 @@ int event_trace_del_tracer(struct trace_array *tr)
 	/* Disable any running events */
 	__ftrace_set_clr_event_nolock(tr, NULL, NULL, NULL, 0);
 
-	/* Access to events are within rcu_read_lock_sched() */
-	synchronize_sched();
+	/* Make sure no more events are being executed */
+	tracepoint_synchronize_unregister();
 
 	down_write(&trace_event_sem);
 	__trace_remove_event_dirs(tr);

commit da25a672cf0e2c143bffb40acb507a342e25b4f4
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Thu Aug 2 19:34:07 2018 -0700

    trace: Use rcu_dereference_raw for hooks from trace-event subsystem
    
    Since we switched to using SRCU for tracepoints used in the idle path,
    we can no longer use rcu_dereference_sched for dereferencing points in
    trace-event hooks.
    
    Since tracepoints can now use either SRCU or sched-RCU, just use
    rcu_dereference_raw for traceevents just like we're doing when
    dereferencing the tracepoint table.
    
    This prevents an RCU warning reported by Masami:
    
    [  282.060593] WARNING: can't dereference registers at 00000000f3c7f62b
    [  282.063200] =============================
    [  282.064082] WARNING: suspicious RCU usage
    [  282.064963] 4.18.0-rc6+ #15 Tainted: G        W
    [  282.066048] -----------------------------
    [  282.066923] /home/mhiramat/ksrc/linux/kernel/trace/trace_events.c:242
                                    suspicious rcu_dereference_check() usage!
    [  282.068974]
    [  282.068974] other info that might help us debug this:
    [  282.068974]
    [  282.070770]
    [  282.070770] RCU used illegally from idle CPU!
    [  282.070770] rcu_scheduler_active = 2, debug_locks = 1
    [  282.072938] RCU used illegally from extended quiescent state!
    [  282.074183] no locks held by swapper/0/0.
    [  282.075071]
    [  282.075071] stack backtrace:
    [  282.076121] CPU: 0 PID: 0 Comm: swapper/0 Tainted: G        W
    [  282.077782] Hardware name: QEMU Standard PC (i440FX + PIIX, 1996)
    [  282.079604] Call Trace:
    [  282.080212]  <IRQ>
    [  282.080755]  dump_stack+0x85/0xcb
    [  282.081523]  trace_event_ignore_this_pid+0x66/0x70
    [  282.082541]  trace_event_raw_event_preemptirq_template+0xa2/0xb0
    [  282.083774]  ? interrupt_entry+0xc4/0xe0
    [  282.084665]  ? trace_hardirqs_off_thunk+0x1a/0x1c
    [  282.085669]  trace_hardirqs_off_caller+0x90/0xd0
    [  282.086597]  trace_hardirqs_off_thunk+0x1a/0x1c
    [  282.087433]  ? call_function_interrupt+0xa/0x20
    [  282.088201]  interrupt_entry+0xc4/0xe0
    [  282.088848]  ? call_function_interrupt+0xa/0x20
    [  282.089579]  </IRQ>
    [  282.090029]  ? native_safe_halt+0x2/0x10
    [  282.090695]  ? default_idle+0x1f/0x160
    [  282.091330]  ? default_idle_call+0x24/0x40
    [  282.091997]  ? do_idle+0x210/0x250
    [  282.092658]  ? cpu_startup_entry+0x6f/0x80
    [  282.093338]  ? start_kernel+0x49d/0x4bd
    [  282.093987]  ? secondary_startup_64+0xa5/0xb0
    
    Link: http://lkml.kernel.org/r/20180803023407.225852-1-joel@joelfernandes.org
    
    Reported-by: Masami Hiramatsu <mhiramat@kernel.org>
    Tested-by: Masami Hiramatsu <mhiramat@kernel.org>
    Fixes: e6753f23d961 ("tracepoint: Make rcuidle tracepoint callers use SRCU")
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 14ff4ff3caab..7b508ce8ac44 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -239,7 +239,7 @@ bool trace_event_ignore_this_pid(struct trace_event_file *trace_file)
 	struct trace_array_cpu *data;
 	struct trace_pid_list *pid_list;
 
-	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	pid_list = rcu_dereference_raw(tr->filtered_pids);
 	if (!pid_list)
 		return false;
 
@@ -512,7 +512,7 @@ event_filter_pid_sched_process_exit(void *data, struct task_struct *task)
 	struct trace_pid_list *pid_list;
 	struct trace_array *tr = data;
 
-	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	pid_list = rcu_dereference_raw(tr->filtered_pids);
 	trace_filter_add_remove_task(pid_list, NULL, task);
 }
 

commit 5d948c86bb332cd143f4b2dc21d949baf38e29aa
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed May 9 09:48:41 2018 -0400

    tracing: Do not show filter file for ftrace internal events
    
    The filter file in the ftrace internal events, like in
    /sys/kernel/tracing/events/ftrace/function/filter is not attached to any
    functionality. Do not create them as they are meaningless.
    
    In the future, if an ftrace internal event gets filter functionality, then
    it will need to create it directly.
    
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5b647904a65b..14ff4ff3caab 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2007,16 +2007,18 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 			return -1;
 		}
 	}
-	trace_create_file("filter", 0644, file->dir, file,
-			  &ftrace_event_filter_fops);
 
 	/*
 	 * Only event directories that can be enabled should have
-	 * triggers.
+	 * triggers or filters.
 	 */
-	if (!(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))
+	if (!(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)) {
+		trace_create_file("filter", 0644, file->dir, file,
+				  &ftrace_event_filter_fops);
+
 		trace_create_file("trigger", 0644, file->dir, file,
 				  &event_trigger_fops);
+	}
 
 #ifdef CONFIG_HIST_TRIGGERS
 	trace_create_file("hist", 0444, file->dir, file,

commit 58b9254757e026102a68cb44a0a15ba63787d0c0
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue May 8 15:09:27 2018 -0400

    tracing: Have event_trace_init() called by trace_init_tracefs()
    
    Instead of having both trace_init_tracefs() and event_trace_init() be called
    by fs_initcall() routines, have event_trace_init() called directly by
    trace_init_tracefs(). This will guarantee order of how the events are
    created with respect to the rest of the ftrace infrastructure. This is
    needed to be able to assoctiate event files with ftrace internal events,
    such as the trace_marker.
    
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index caf76355f0a3..5b647904a65b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -3144,7 +3144,7 @@ static __init int event_trace_enable_again(void)
 
 early_initcall(event_trace_enable_again);
 
-static __init int event_trace_init(void)
+__init int event_trace_init(void)
 {
 	struct trace_array *tr;
 	struct dentry *d_tracer;
@@ -3189,8 +3189,6 @@ void __init trace_event_init(void)
 	event_trace_enable();
 }
 
-fs_initcall(event_trace_init);
-
 #ifdef CONFIG_FTRACE_STARTUP_TEST
 
 static DEFINE_SPINLOCK(test_spinlock);

commit 3c96529c0739959e2aa235d44e47f5c68c1e40de
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue May 8 15:06:38 2018 -0400

    tracing: Add __find_event_file() to find event files without restrictions
    
    By adding the function __find_event_file() that can search for files without
    restrictions, such as if the event associated with the file has a reg
    function, or if it has the "ignore" flag set, the files that are associated
    to ftrace internal events (like trace_marker and function events) can be
    found and used.
    
    find_event_file() still returns a "filtered" file, as most callers need a
    valid trace event file. One created by the trace_events.h macros and not one
    created for parsing ftrace specific events.
    
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 05c7172c6667..caf76355f0a3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2473,8 +2473,9 @@ __trace_add_event_dirs(struct trace_array *tr)
 	}
 }
 
+/* Returns any file that matches the system and event */
 struct trace_event_file *
-find_event_file(struct trace_array *tr, const char *system,  const char *event)
+__find_event_file(struct trace_array *tr, const char *system, const char *event)
 {
 	struct trace_event_file *file;
 	struct trace_event_call *call;
@@ -2485,10 +2486,7 @@ find_event_file(struct trace_array *tr, const char *system,  const char *event)
 		call = file->event_call;
 		name = trace_event_name(call);
 
-		if (!name || !call->class || !call->class->reg)
-			continue;
-
-		if (call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)
+		if (!name || !call->class)
 			continue;
 
 		if (strcmp(event, name) == 0 &&
@@ -2498,6 +2496,20 @@ find_event_file(struct trace_array *tr, const char *system,  const char *event)
 	return NULL;
 }
 
+/* Returns valid trace event files that match system and event */
+struct trace_event_file *
+find_event_file(struct trace_array *tr, const char *system, const char *event)
+{
+	struct trace_event_file *file;
+
+	file = __find_event_file(tr, system, event);
+	if (!file || !file->event_call->class->reg ||
+	    file->event_call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)
+		return NULL;
+
+	return file;
+}
+
 #ifdef CONFIG_DYNAMIC_FTRACE
 
 /* Avoid typos */

commit f4d0706cde27f29ff89e6bf94ded4113f8fe6e80
Author: Changbin Du <changbin.du@intel.com>
Date:   Tue Jan 16 17:02:30 2018 +0800

    tracing: Make sure the parsed string always terminates with '\0'
    
    Always mark the parsed string with a terminated nul '\0' character. This removes
    the need for the users to have to append the '\0' before using the parsed string.
    
    Link: http://lkml.kernel.org/r/1516093350-12045-4-git-send-email-changbin.du@intel.com
    
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Changbin Du <changbin.du@intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1b87157edbff..05c7172c6667 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -885,8 +885,6 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 		if (*parser.buffer == '!')
 			set = 0;
 
-		parser.buffer[parser.idx] = 0;
-
 		ret = ftrace_set_clr_event(tr, parser.buffer + !set, set);
 		if (ret)
 			goto out_put;

commit 1ebe1eaf2f02784921759992ae1fde1a9bec8fd0
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Jan 18 15:53:10 2018 -0500

    tracing: Fix converting enum's from the map in trace_event_eval_update()
    
    Since enums do not get converted by the TRACE_EVENT macro into their values,
    the event format displaces the enum name and not the value. This breaks
    tools like perf and trace-cmd that need to interpret the raw binary data. To
    solve this, an enum map was created to convert these enums into their actual
    numbers on boot up. This is done by TRACE_EVENTS() adding a
    TRACE_DEFINE_ENUM() macro.
    
    Some enums were not being converted. This was caused by an optization that
    had a bug in it.
    
    All calls get checked against this enum map to see if it should be converted
    or not, and it compares the call's system to the system that the enum map
    was created under. If they match, then they call is processed.
    
    To cut down on the number of iterations needed to find the maps with a
    matching system, since calls and maps are grouped by system, when a match is
    made, the index into the map array is saved, so that the next call, if it
    belongs to the same system as the previous call, could start right at that
    array index and not have to scan all the previous arrays.
    
    The problem was, the saved index was used as the variable to know if this is
    a call in a new system or not. If the index was zero, it was assumed that
    the call is in a new system and would keep incrementing the saved index
    until it found a matching system. The issue arises when the first matching
    system was at index zero. The next map, if it belonged to the same system,
    would then think it was the first match and increment the index to one. If
    the next call belong to the same system, it would begin its search of the
    maps off by one, and miss the first enum that should be converted. This left
    a single enum not converted properly.
    
    Also add a comment to describe exactly what that index was for. It took me a
    bit too long to figure out what I was thinking when debugging this issue.
    
    Link: http://lkml.kernel.org/r/717BE572-2070-4C1E-9902-9F2E0FEDA4F8@oracle.com
    
    Cc: stable@vger.kernel.org
    Fixes: 0c564a538aa93 ("tracing: Add TRACE_DEFINE_ENUM() macro to map enums to their values")
    Reported-by: Chuck Lever <chuck.lever@oracle.com>
    Teste-by: Chuck Lever <chuck.lever@oracle.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ec0f9aa4e151..1b87157edbff 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2213,6 +2213,7 @@ void trace_event_eval_update(struct trace_eval_map **map, int len)
 {
 	struct trace_event_call *call, *p;
 	const char *last_system = NULL;
+	bool first = false;
 	int last_i;
 	int i;
 
@@ -2220,15 +2221,28 @@ void trace_event_eval_update(struct trace_eval_map **map, int len)
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
 		/* events are usually grouped together with systems */
 		if (!last_system || call->class->system != last_system) {
+			first = true;
 			last_i = 0;
 			last_system = call->class->system;
 		}
 
+		/*
+		 * Since calls are grouped by systems, the likelyhood that the
+		 * next call in the iteration belongs to the same system as the
+		 * previous call is high. As an optimization, we skip seaching
+		 * for a map[] that matches the call's system if the last call
+		 * was from the same system. That's what last_i is for. If the
+		 * call has the same system as the previous call, then last_i
+		 * will be the index of the first map[] that has a matching
+		 * system.
+		 */
 		for (i = last_i; i < len; i++) {
 			if (call->class->system == map[i]->system) {
 				/* Save the first system if need be */
-				if (!last_i)
+				if (first) {
 					last_i = i;
+					first = false;
+				}
 				update_event_printk(call, map[i]);
 			}
 		}

commit 12ecef0cb12102d8c034770173d2d1363cb97d52
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Sep 21 16:22:49 2017 -0400

    tracing: Reverse the order of trace_types_lock and event_mutex
    
    In order to make future changes where we need to call
    tracing_set_clock() from within an event command, the order of
    trace_types_lock and event_mutex must be reversed, as the event command
    will hold event_mutex and the trace_types_lock is taken from within
    tracing_set_clock().
    
    Link: http://lkml.kernel.org/r/20170921162249.0dde3dca@gandalf.local.home
    
    Requested-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 87468398b9ed..ec0f9aa4e151 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1406,8 +1406,8 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 		return -ENODEV;
 
 	/* Make sure the system still exists */
-	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
+	mutex_lock(&trace_types_lock);
 	list_for_each_entry(tr, &ftrace_trace_arrays, list) {
 		list_for_each_entry(dir, &tr->systems, list) {
 			if (dir == inode->i_private) {
@@ -1421,8 +1421,8 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 		}
 	}
  exit_loop:
-	mutex_unlock(&event_mutex);
 	mutex_unlock(&trace_types_lock);
+	mutex_unlock(&event_mutex);
 
 	if (!system)
 		return -ENODEV;
@@ -2294,15 +2294,15 @@ static void __add_event_to_tracers(struct trace_event_call *call);
 int trace_add_event_call(struct trace_event_call *call)
 {
 	int ret;
-	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
+	mutex_lock(&trace_types_lock);
 
 	ret = __register_event(call, NULL);
 	if (ret >= 0)
 		__add_event_to_tracers(call);
 
-	mutex_unlock(&event_mutex);
 	mutex_unlock(&trace_types_lock);
+	mutex_unlock(&event_mutex);
 	return ret;
 }
 
@@ -2356,13 +2356,13 @@ int trace_remove_event_call(struct trace_event_call *call)
 {
 	int ret;
 
-	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
+	mutex_lock(&trace_types_lock);
 	down_write(&trace_event_sem);
 	ret = probe_remove_event_call(call);
 	up_write(&trace_event_sem);
-	mutex_unlock(&event_mutex);
 	mutex_unlock(&trace_types_lock);
+	mutex_unlock(&event_mutex);
 
 	return ret;
 }
@@ -2424,8 +2424,8 @@ static int trace_module_notify(struct notifier_block *self,
 {
 	struct module *mod = data;
 
-	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
+	mutex_lock(&trace_types_lock);
 	switch (val) {
 	case MODULE_STATE_COMING:
 		trace_module_add_events(mod);
@@ -2434,8 +2434,8 @@ static int trace_module_notify(struct notifier_block *self,
 		trace_module_remove_events(mod);
 		break;
 	}
-	mutex_unlock(&event_mutex);
 	mutex_unlock(&trace_types_lock);
+	mutex_unlock(&event_mutex);
 
 	return 0;
 }
@@ -2950,24 +2950,24 @@ create_event_toplevel_files(struct dentry *parent, struct trace_array *tr)
  * creates the event hierachry in the @parent/events directory.
  *
  * Returns 0 on success.
+ *
+ * Must be called with event_mutex held.
  */
 int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
 {
 	int ret;
 
-	mutex_lock(&event_mutex);
+	lockdep_assert_held(&event_mutex);
 
 	ret = create_event_toplevel_files(parent, tr);
 	if (ret)
-		goto out_unlock;
+		goto out;
 
 	down_write(&trace_event_sem);
 	__trace_add_event_dirs(tr);
 	up_write(&trace_event_sem);
 
- out_unlock:
-	mutex_unlock(&event_mutex);
-
+ out:
 	return ret;
 }
 
@@ -2996,9 +2996,10 @@ early_event_add_tracer(struct dentry *parent, struct trace_array *tr)
 	return ret;
 }
 
+/* Must be called with event_mutex held */
 int event_trace_del_tracer(struct trace_array *tr)
 {
-	mutex_lock(&event_mutex);
+	lockdep_assert_held(&event_mutex);
 
 	/* Disable any event triggers and associated soft-disabled events */
 	clear_event_triggers(tr);
@@ -3019,8 +3020,6 @@ int event_trace_del_tracer(struct trace_array *tr)
 
 	tr->event_dir = NULL;
 
-	mutex_unlock(&event_mutex);
-
 	return 0;
 }
 

commit 7685ab6c58557c6234f3540260195ecbee7fc4b3
Author: Chunyu Hu <chuhu@redhat.com>
Date:   Tue Sep 5 13:36:46 2017 +0800

    tracing: Fix clear of RECORDED_TGID flag when disabling trace event
    
    When disabling one trace event, the RECORDED_TGID flag in the event
    file is not correctly cleared. It's clearing RECORDED_CMD flag when
    it should clear RECORDED_TGID flag.
    
    Link: http://lkml.kernel.org/r/1504589806-8425-1-git-send-email-chuhu@redhat.com
    
    Cc: Joel Fernandes <joelaf@google.com>
    Cc: stable@vger.kernel.org
    Fixes: d914ba37d7 ("tracing: Add support for recording tgid of tasks")
    Signed-off-by: Chunyu Hu <chuhu@redhat.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c93540c5df21..87468398b9ed 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -406,7 +406,7 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 
 			if (file->flags & EVENT_FILE_FL_RECORDED_TGID) {
 				tracing_stop_tgid_record();
-				clear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
+				clear_bit(EVENT_FILE_FL_RECORDED_TGID_BIT, &file->flags);
 			}
 
 			call->class->reg(call, TRACE_REG_UNREGISTER, file);

commit 065e63f951432068ba89a844fcbff68ea16ee186
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 31 17:03:47 2017 -0400

    tracing: Only have rmmod clear buffers that its events were active in
    
    Currently, when a module event is enabled, when that module is removed, it
    clears all ring buffers. This is to prevent another module from being loaded
    and having one of its trace event IDs from reusing a trace event ID of the
    removed module. This could cause undesirable effects as the trace event of
    the new module would be using its own processing algorithms to process raw
    data of another event. To prevent this, when a module is loaded, if any of
    its events have been used (signified by the WAS_ENABLED event call flag,
    which is never cleared), all ring buffers are cleared, just in case any one
    of them contains event data of the removed event.
    
    The problem is, there's no reason to clear all ring buffers if only one (or
    less than all of them) uses one of the events. Instead, only clear the ring
    buffers that recorded the events of a module that is being removed.
    
    To do this, instead of keeping the WAS_ENABLED flag with the trace event
    call, move it to the per instance (per ring buffer) event file descriptor.
    The event file descriptor maps each event to a separate ring buffer
    instance. Then when the module is removed, only the ring buffers that
    activated one of the module's events get cleared. The rest are not touched.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 36132f9280e6..c93540c5df21 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -466,7 +466,7 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 			set_bit(EVENT_FILE_FL_ENABLED_BIT, &file->flags);
 
 			/* WAS_ENABLED gets set but never cleared. */
-			call->flags |= TRACE_EVENT_FL_WAS_ENABLED;
+			set_bit(EVENT_FILE_FL_WAS_ENABLED_BIT, &file->flags);
 		}
 		break;
 	}
@@ -2058,6 +2058,10 @@ static void event_remove(struct trace_event_call *call)
 	do_for_each_event_file(tr, file) {
 		if (file->event_call != call)
 			continue;
+
+		if (file->flags & EVENT_FILE_FL_WAS_ENABLED)
+			tr->clear_trace = true;
+
 		ftrace_event_enable_disable(file, 0);
 		/*
 		 * The do_for_each_event_file() is
@@ -2396,15 +2400,11 @@ static void trace_module_add_events(struct module *mod)
 static void trace_module_remove_events(struct module *mod)
 {
 	struct trace_event_call *call, *p;
-	bool clear_trace = false;
 
 	down_write(&trace_event_sem);
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
-		if (call->mod == mod) {
-			if (call->flags & TRACE_EVENT_FL_WAS_ENABLED)
-				clear_trace = true;
+		if (call->mod == mod)
 			__trace_remove_event_call(call);
-		}
 	}
 	up_write(&trace_event_sem);
 
@@ -2416,8 +2416,7 @@ static void trace_module_remove_events(struct module *mod)
 	 * over from this module may be passed to the new module events and
 	 * unexpected results may occur.
 	 */
-	if (clear_trace)
-		tracing_reset_all_online_cpus();
+	tracing_reset_all_online_cpus();
 }
 
 static int trace_module_notify(struct notifier_block *self,

commit d914ba37d7145acb9fd3bb23075c2d56e5a44eb6
Author: Joel Fernandes <joelaf@google.com>
Date:   Mon Jun 26 19:01:55 2017 -0700

    tracing: Add support for recording tgid of tasks
    
    Inorder to support recording of tgid, the following changes are made:
    
    * Introduce a new API (tracing_record_taskinfo) to additionally record the tgid
      along with the task's comm at the same time. This has has the benefit of not
      setting trace_cmdline_save before all the information for a task is saved.
    * Add a new API tracing_record_taskinfo_sched_switch to record task information
      for 2 tasks at a time (previous and next) and use it from sched_switch probe.
    * Preserve the old API (tracing_record_cmdline) and create it as a wrapper
      around the new one so that existing callers aren't affected.
    * Reuse the existing sched_switch and sched_wakeup probes to record tgid
      information and add a new option 'record-tgid' to enable recording of tgid
    
    When record-tgid option isn't enabled to being with, we take care to make sure
    that there's isn't memory or runtime overhead.
    
    Link: http://lkml.kernel.org/r/20170627020155.5139-1-joelaf@google.com
    
    Cc: kernel-team@android.com
    Cc: Ingo Molnar <mingo@redhat.com>
    Tested-by: Michael Sartain <mikesart@gmail.com>
    Signed-off-by: Joel Fernandes <joelaf@google.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 83dfd0dbbbfe..36132f9280e6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -343,6 +343,28 @@ void trace_event_enable_cmd_record(bool enable)
 	mutex_unlock(&event_mutex);
 }
 
+void trace_event_enable_tgid_record(bool enable)
+{
+	struct trace_event_file *file;
+	struct trace_array *tr;
+
+	mutex_lock(&event_mutex);
+	do_for_each_event_file(tr, file) {
+		if (!(file->flags & EVENT_FILE_FL_ENABLED))
+			continue;
+
+		if (enable) {
+			tracing_start_tgid_record();
+			set_bit(EVENT_FILE_FL_RECORDED_TGID_BIT, &file->flags);
+		} else {
+			tracing_stop_tgid_record();
+			clear_bit(EVENT_FILE_FL_RECORDED_TGID_BIT,
+				  &file->flags);
+		}
+	} while_for_each_event_file();
+	mutex_unlock(&event_mutex);
+}
+
 static int __ftrace_event_enable_disable(struct trace_event_file *file,
 					 int enable, int soft_disable)
 {
@@ -381,6 +403,12 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 				tracing_stop_cmdline_record();
 				clear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 			}
+
+			if (file->flags & EVENT_FILE_FL_RECORDED_TGID) {
+				tracing_stop_tgid_record();
+				clear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
+			}
+
 			call->class->reg(call, TRACE_REG_UNREGISTER, file);
 		}
 		/* If in SOFT_MODE, just set the SOFT_DISABLE_BIT, else clear it */
@@ -407,18 +435,30 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 		}
 
 		if (!(file->flags & EVENT_FILE_FL_ENABLED)) {
+			bool cmd = false, tgid = false;
 
 			/* Keep the event disabled, when going to SOFT_MODE. */
 			if (soft_disable)
 				set_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);
 
 			if (tr->trace_flags & TRACE_ITER_RECORD_CMD) {
+				cmd = true;
 				tracing_start_cmdline_record();
 				set_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 			}
+
+			if (tr->trace_flags & TRACE_ITER_RECORD_TGID) {
+				tgid = true;
+				tracing_start_tgid_record();
+				set_bit(EVENT_FILE_FL_RECORDED_TGID_BIT, &file->flags);
+			}
+
 			ret = call->class->reg(call, TRACE_REG_REGISTER, file);
 			if (ret) {
-				tracing_stop_cmdline_record();
+				if (cmd)
+					tracing_stop_cmdline_record();
+				if (tgid)
+					tracing_stop_tgid_record();
 				pr_info("event trace: Could not enable event "
 					"%s\n", trace_event_name(call));
 				break;

commit 67ec0d85955630924b971e04c0954370a74b8706
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed May 31 16:56:49 2017 -0500

    tracing: Rename enum_replace to eval_replace
    
    The enum_replace stanza works as is for sizeof()
    calls as well as enums. Rename it as well.
    
    Link: http://lkml.kernel.org/r/20170531215653.3240-9-jeremy.linton@arm.com
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e6897b005947..83dfd0dbbbfe 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2067,12 +2067,12 @@ __register_event(struct trace_event_call *call, struct module *mod)
 	return 0;
 }
 
-static char *enum_replace(char *ptr, struct trace_eval_map *map, int len)
+static char *eval_replace(char *ptr, struct trace_eval_map *map, int len)
 {
 	int rlen;
 	int elen;
 
-	/* Find the length of the enum value as a string */
+	/* Find the length of the eval value as a string */
 	elen = snprintf(ptr, 0, "%ld", map->eval_value);
 	/* Make sure there's enough room to replace the string with the value */
 	if (len < elen)
@@ -2127,14 +2127,14 @@ static void update_event_printk(struct trace_event_call *call,
 		if (isalpha(*ptr) || *ptr == '_') {
 			if (strncmp(map->eval_string, ptr, len) == 0 &&
 			    !isalnum(ptr[len]) && ptr[len] != '_') {
-				ptr = enum_replace(ptr, map, len);
-				/* Hmm, enum string smaller than value */
+				ptr = eval_replace(ptr, map, len);
+				/* enum/sizeof string smaller than value */
 				if (WARN_ON_ONCE(!ptr))
 					return;
 				/*
-				 * No need to decrement here, as enum_replace()
+				 * No need to decrement here, as eval_replace()
 				 * returns the pointer to the character passed
-				 * the enum, and two enums can not be placed
+				 * the eval, and two evals can not be placed
 				 * back to back without something in between.
 				 * We can skip that something in between.
 				 */

commit f57a41434fc51732dd5e35e0e1aa9e607f1a05d6
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed May 31 16:56:48 2017 -0500

    trace: rename enum_map functions
    
    Rename the core trace enum routines to use eval, to
    reflect their use by more than just enum to value mapping.
    
    Link: http://lkml.kernel.org/r/20170531215653.3240-8-jeremy.linton@arm.com
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index cf5b9aa4d732..e6897b005947 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2165,7 +2165,7 @@ static void update_event_printk(struct trace_event_call *call,
 	}
 }
 
-void trace_event_enum_update(struct trace_eval_map **map, int len)
+void trace_event_eval_update(struct trace_eval_map **map, int len)
 {
 	struct trace_event_call *call, *p;
 	const char *last_system = NULL;

commit 00f4b652b6f1dbfd4e1d5419d7f1cc23b1374da8
Author: Jeremy Linton <jeremy.linton@arm.com>
Date:   Wed May 31 16:56:43 2017 -0500

    trace: rename trace_enum_map to trace_eval_map
    
    Each enum is loaded into the trace_enum_map, as we
    are now using this for more than enums rename it.
    
    Link: http://lkml.kernel.org/r/20170531215653.3240-3-jeremy.linton@arm.com
    
    Signed-off-by: Jeremy Linton <jeremy.linton@arm.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e7973e10398c..cf5b9aa4d732 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2067,18 +2067,18 @@ __register_event(struct trace_event_call *call, struct module *mod)
 	return 0;
 }
 
-static char *enum_replace(char *ptr, struct trace_enum_map *map, int len)
+static char *enum_replace(char *ptr, struct trace_eval_map *map, int len)
 {
 	int rlen;
 	int elen;
 
 	/* Find the length of the enum value as a string */
-	elen = snprintf(ptr, 0, "%ld", map->enum_value);
+	elen = snprintf(ptr, 0, "%ld", map->eval_value);
 	/* Make sure there's enough room to replace the string with the value */
 	if (len < elen)
 		return NULL;
 
-	snprintf(ptr, elen + 1, "%ld", map->enum_value);
+	snprintf(ptr, elen + 1, "%ld", map->eval_value);
 
 	/* Get the rest of the string of ptr */
 	rlen = strlen(ptr + len);
@@ -2090,11 +2090,11 @@ static char *enum_replace(char *ptr, struct trace_enum_map *map, int len)
 }
 
 static void update_event_printk(struct trace_event_call *call,
-				struct trace_enum_map *map)
+				struct trace_eval_map *map)
 {
 	char *ptr;
 	int quote = 0;
-	int len = strlen(map->enum_string);
+	int len = strlen(map->eval_string);
 
 	for (ptr = call->print_fmt; *ptr; ptr++) {
 		if (*ptr == '\\') {
@@ -2125,7 +2125,7 @@ static void update_event_printk(struct trace_event_call *call,
 			continue;
 		}
 		if (isalpha(*ptr) || *ptr == '_') {
-			if (strncmp(map->enum_string, ptr, len) == 0 &&
+			if (strncmp(map->eval_string, ptr, len) == 0 &&
 			    !isalnum(ptr[len]) && ptr[len] != '_') {
 				ptr = enum_replace(ptr, map, len);
 				/* Hmm, enum string smaller than value */
@@ -2165,7 +2165,7 @@ static void update_event_printk(struct trace_event_call *call,
 	}
 }
 
-void trace_event_enum_update(struct trace_enum_map **map, int len)
+void trace_event_enum_update(struct trace_eval_map **map, int len)
 {
 	struct trace_event_call *call, *p;
 	const char *last_system = NULL;

commit 6e4443199e5354255e8a4c1e8e5cfc8ef064c3ce
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 19 22:39:44 2017 -0400

    tracing/ftrace: Add a better way to pass data via the probe functions
    
    With the redesign of the registration and execution of the function probes
    (triggers), data can now be passed from the setup of the probe to the probe
    callers that are specific to the trace_array it is on. Although, all probes
    still only affect the toplevel trace array, this change will allow for
    instances to have their own probes separated from other instances and the
    top array.
    
    That is, something like the stacktrace probe can be set to trace only in an
    instance and not the toplevel trace array. This isn't implement yet, but
    this change sets the ground work for the change.
    
    When a probe callback is triggered (someone writes the probe format into
    set_ftrace_filter), it calls register_ftrace_function_probe() passing in
    init_data that will be used to initialize the probe. Then for every matching
    function, register_ftrace_function_probe() will call the probe_ops->init()
    function with the init data that was passed to it, as well as an address to
    a place holder that is associated with the probe and the instance. The first
    occurrence will have a NULL in the pointer. The init() function will then
    initialize it. If other probes are added, or more functions are part of the
    probe, the place holder will be passed to the init() function with the place
    holder data that it was initialized to the last time.
    
    Then this place_holder is passed to each of the other probe_ops functions,
    where it can be used in the function callback. When the probe_ops free()
    function is called, it can be called either with the rip of the function
    that is being removed from the probe, or zero, indicating that there are no
    more functions attached to the probe, and the place holder is about to be
    freed. This gives the probe_ops a way to free the data it assigned to the
    place holder if it was allocade during the first init call.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 48c7f70cbac7..e7973e10398c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2471,54 +2471,54 @@ static void update_event_probe(struct event_probe_data *data)
 static void
 event_enable_probe(unsigned long ip, unsigned long parent_ip,
 		   struct trace_array *tr, struct ftrace_probe_ops *ops,
-		   void **_data)
+		   void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
-	struct event_probe_data *data;
+	struct ftrace_func_mapper *mapper = data;
+	struct event_probe_data *edata;
 	void **pdata;
 
 	pdata = ftrace_func_mapper_find_ip(mapper, ip);
 	if (!pdata || !*pdata)
 		return;
 
-	data = *pdata;
-	update_event_probe(data);
+	edata = *pdata;
+	update_event_probe(edata);
 }
 
 static void
 event_enable_count_probe(unsigned long ip, unsigned long parent_ip,
 			 struct trace_array *tr, struct ftrace_probe_ops *ops,
-			 void **_data)
+			 void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
-	struct event_probe_data *data;
+	struct ftrace_func_mapper *mapper = data;
+	struct event_probe_data *edata;
 	void **pdata;
 
 	pdata = ftrace_func_mapper_find_ip(mapper, ip);
 	if (!pdata || !*pdata)
 		return;
 
-	data = *pdata;
+	edata = *pdata;
 
-	if (!data->count)
+	if (!edata->count)
 		return;
 
 	/* Skip if the event is in a state we want to switch to */
-	if (data->enable == !(data->file->flags & EVENT_FILE_FL_SOFT_DISABLED))
+	if (edata->enable == !(edata->file->flags & EVENT_FILE_FL_SOFT_DISABLED))
 		return;
 
-	if (data->count != -1)
-		(data->count)--;
+	if (edata->count != -1)
+		(edata->count)--;
 
-	update_event_probe(data);
+	update_event_probe(edata);
 }
 
 static int
 event_enable_print(struct seq_file *m, unsigned long ip,
-		   struct ftrace_probe_ops *ops, void *_data)
+		   struct ftrace_probe_ops *ops, void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
-	struct event_probe_data *data;
+	struct ftrace_func_mapper *mapper = data;
+	struct event_probe_data *edata;
 	void **pdata;
 
 	pdata = ftrace_func_mapper_find_ip(mapper, ip);
@@ -2526,62 +2526,84 @@ event_enable_print(struct seq_file *m, unsigned long ip,
 	if (WARN_ON_ONCE(!pdata || !*pdata))
 		return 0;
 
-	data = *pdata;
+	edata = *pdata;
 
 	seq_printf(m, "%ps:", (void *)ip);
 
 	seq_printf(m, "%s:%s:%s",
-		   data->enable ? ENABLE_EVENT_STR : DISABLE_EVENT_STR,
-		   data->file->event_call->class->system,
-		   trace_event_name(data->file->event_call));
+		   edata->enable ? ENABLE_EVENT_STR : DISABLE_EVENT_STR,
+		   edata->file->event_call->class->system,
+		   trace_event_name(edata->file->event_call));
 
-	if (data->count == -1)
+	if (edata->count == -1)
 		seq_puts(m, ":unlimited\n");
 	else
-		seq_printf(m, ":count=%ld\n", data->count);
+		seq_printf(m, ":count=%ld\n", edata->count);
 
 	return 0;
 }
 
 static int
 event_enable_init(struct ftrace_probe_ops *ops, struct trace_array *tr,
-		  unsigned long ip, void *_data)
+		  unsigned long ip, void *init_data, void **data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
-	struct event_probe_data *data = _data;
+	struct ftrace_func_mapper *mapper = *data;
+	struct event_probe_data *edata = init_data;
 	int ret;
 
-	ret = ftrace_func_mapper_add_ip(mapper, ip, data);
+	if (!mapper) {
+		mapper = allocate_ftrace_func_mapper();
+		if (!mapper)
+			return -ENODEV;
+		*data = mapper;
+	}
+
+	ret = ftrace_func_mapper_add_ip(mapper, ip, edata);
 	if (ret < 0)
 		return ret;
 
-	data->ref++;
+	edata->ref++;
+
+	return 0;
+}
+
+static int free_probe_data(void *data)
+{
+	struct event_probe_data *edata = data;
 
+	edata->ref--;
+	if (!edata->ref) {
+		/* Remove the SOFT_MODE flag */
+		__ftrace_event_enable_disable(edata->file, 0, 1);
+		module_put(edata->file->event_call->mod);
+		kfree(edata);
+	}
 	return 0;
 }
 
 static void
 event_enable_free(struct ftrace_probe_ops *ops, struct trace_array *tr,
-		  unsigned long ip, void **_data)
+		  unsigned long ip, void *data)
 {
-	struct ftrace_func_mapper *mapper = ops->private_data;
-	struct event_probe_data *data;
+	struct ftrace_func_mapper *mapper = data;
+	struct event_probe_data *edata;
+
+	if (!ip) {
+		if (!mapper)
+			return;
+		free_ftrace_func_mapper(mapper, free_probe_data);
+		return;
+	}
 
-	data = ftrace_func_mapper_remove_ip(mapper, ip);
+	edata = ftrace_func_mapper_remove_ip(mapper, ip);
 
-	if (WARN_ON_ONCE(!data))
+	if (WARN_ON_ONCE(!edata))
 		return;
 
-	if (WARN_ON_ONCE(data->ref <= 0))
+	if (WARN_ON_ONCE(edata->ref <= 0))
 		return;
 
-	data->ref--;
-	if (!data->ref) {
-		/* Remove the SOFT_MODE flag */
-		__ftrace_event_enable_disable(data->file, 0, 1);
-		module_put(data->file->event_call->mod);
-		kfree(data);
-	}
+	free_probe_data(edata);
 }
 
 static struct ftrace_probe_ops event_enable_probe_ops = {
@@ -2659,12 +2681,6 @@ event_enable_func(struct trace_array *tr, struct ftrace_hash *hash,
 
 	ret = -ENOMEM;
 
-	if (!ops->private_data) {
-		ops->private_data = allocate_ftrace_func_mapper();
-		if (!ops->private_data)
-			goto out;
-	}
-
 	data = kzalloc(sizeof(*data), GFP_KERNEL);
 	if (!data)
 		goto out;

commit 7b60f3d8761561d95d7e962522d6338143fc2329
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 18 14:50:39 2017 -0400

    ftrace: Dynamically create the probe ftrace_ops for the trace_array
    
    In order to eventually have each trace_array instance have its own unique
    set of function probes (triggers), the trace array needs to hold the ops and
    the filters for the probes.
    
    This is the first step to accomplish this. Instead of having the private
    data of the probe ops point to the trace_array, create a separate list that
    the trace_array holds. There's only one private_data for a probe, we need
    one per trace_array. The probe ftrace_ops will be dynamically created for
    each instance, instead of being static.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 713bec614312..48c7f70cbac7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2653,7 +2653,7 @@ event_enable_func(struct trace_array *tr, struct ftrace_hash *hash,
 		ops = param ? &event_disable_count_probe_ops : &event_disable_probe_ops;
 
 	if (glob[0] == '!') {
-		ret = unregister_ftrace_function_probe_func(glob+1, ops);
+		ret = unregister_ftrace_function_probe_func(glob+1, tr, ops);
 		goto out;
 	}
 

commit b5f081b563a6cdcb85a543df8c851951a8978275
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 10 22:30:05 2017 -0400

    tracing: Pass the trace_array into ftrace_probe_ops functions
    
    Pass the trace_array associated to a ftrace_probe_ops into the probe_ops
    func(), init() and free() functions. The trace_array is the descriptor that
    describes a tracing instance. This will help create the infrastructure that
    will allow having function probes unique to tracing instances.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f0d6e5aef53e..713bec614312 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2470,7 +2470,8 @@ static void update_event_probe(struct event_probe_data *data)
 
 static void
 event_enable_probe(unsigned long ip, unsigned long parent_ip,
-		   struct ftrace_probe_ops *ops, void **_data)
+		   struct trace_array *tr, struct ftrace_probe_ops *ops,
+		   void **_data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 	struct event_probe_data *data;
@@ -2486,7 +2487,8 @@ event_enable_probe(unsigned long ip, unsigned long parent_ip,
 
 static void
 event_enable_count_probe(unsigned long ip, unsigned long parent_ip,
-			 struct ftrace_probe_ops *ops, void **_data)
+			 struct trace_array *tr, struct ftrace_probe_ops *ops,
+			 void **_data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 	struct event_probe_data *data;
@@ -2513,7 +2515,7 @@ event_enable_count_probe(unsigned long ip, unsigned long parent_ip,
 
 static int
 event_enable_print(struct seq_file *m, unsigned long ip,
-		      struct ftrace_probe_ops *ops, void *_data)
+		   struct ftrace_probe_ops *ops, void *_data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 	struct event_probe_data *data;
@@ -2542,8 +2544,8 @@ event_enable_print(struct seq_file *m, unsigned long ip,
 }
 
 static int
-event_enable_init(struct ftrace_probe_ops *ops, unsigned long ip,
-		  void *_data)
+event_enable_init(struct ftrace_probe_ops *ops, struct trace_array *tr,
+		  unsigned long ip, void *_data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 	struct event_probe_data *data = _data;
@@ -2559,8 +2561,8 @@ event_enable_init(struct ftrace_probe_ops *ops, unsigned long ip,
 }
 
 static void
-event_enable_free(struct ftrace_probe_ops *ops, unsigned long ip,
-		  void **_data)
+event_enable_free(struct ftrace_probe_ops *ops, struct trace_array *tr,
+		  unsigned long ip, void **_data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
 	struct event_probe_data *data;

commit 04ec7bb642b77374b53731b795b5654b5aff1c00
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Apr 5 13:12:55 2017 -0400

    tracing: Have the trace_array hold the list of registered func probes
    
    Add a link list to the trace_array to hold func probes that are registered.
    Currently, all function probes are the same for all instances as it was
    before, that is, only the top level trace_array holds the function probes.
    But this lays the ground work to have function probes be attached to
    individual instances, and having the event trigger only affect events in the
    given instance. But that work is still to be done.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9e07a5b3869b..f0d6e5aef53e 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2611,10 +2611,9 @@ static struct ftrace_probe_ops event_disable_count_probe_ops = {
 };
 
 static int
-event_enable_func(struct ftrace_hash *hash,
+event_enable_func(struct trace_array *tr, struct ftrace_hash *hash,
 		  char *glob, char *cmd, char *param, int enabled)
 {
-	struct trace_array *tr = top_trace_array();
 	struct trace_event_file *file;
 	struct ftrace_probe_ops *ops;
 	struct event_probe_data *data;
@@ -2701,7 +2700,7 @@ event_enable_func(struct ftrace_hash *hash,
 	if (ret < 0)
 		goto out_put;
 
-	ret = register_ftrace_function_probe(glob, ops, data);
+	ret = register_ftrace_function_probe(glob, tr, ops, data);
 	/*
 	 * The above returns on success the # of functions enabled,
 	 * but if it didn't find any functions it returns zero.

commit d3d532d798c5720055ab02a10bf7829a33c3645a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 16:44:43 2017 -0400

    ftrace: Have unregister_ftrace_function_probe_func() return a value
    
    Currently unregister_ftrace_function_probe_func() is a void function. It
    does not give any feedback if an error occurred or no item was found to
    remove and nothing was done.
    
    Change it to return status and success if it removed something. Also update
    the callers to return that feedback to the user.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 37902107c44f..9e07a5b3869b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2652,8 +2652,7 @@ event_enable_func(struct ftrace_hash *hash,
 		ops = param ? &event_disable_count_probe_ops : &event_disable_probe_ops;
 
 	if (glob[0] == '!') {
-		unregister_ftrace_function_probe_func(glob+1, ops);
-		ret = 0;
+		ret = unregister_ftrace_function_probe_func(glob+1, ops);
 		goto out;
 	}
 

commit 1a48df0041c2756194e700affb0e2ff084092e28
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Tue Apr 4 10:27:51 2017 -0400

    ftrace: Remove data field from ftrace_func_probe structure
    
    No users of the function probes uses the data field anymore. Remove it, and
    change the init function to take a void *data parameter instead of a
    void **data, because the init will just get the data that the registering
    function was received, and there's no state after it is called.
    
    The other functions for ftrace_probe_ops still take the data parameter, but
    it will currently only be passed NULL. It will stay as a parameter for
    future data to be passed to these functions.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ee308312fe87..37902107c44f 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2543,11 +2543,10 @@ event_enable_print(struct seq_file *m, unsigned long ip,
 
 static int
 event_enable_init(struct ftrace_probe_ops *ops, unsigned long ip,
-		  void **_data)
+		  void *_data)
 {
 	struct ftrace_func_mapper *mapper = ops->private_data;
-	struct event_probe_data **pdata = (struct event_probe_data **)_data;
-	struct event_probe_data *data = *pdata;
+	struct event_probe_data *data = _data;
 	int ret;
 
 	ret = ftrace_func_mapper_add_ip(mapper, ip, data);

commit 41794f190780c28784fa62b22001691e5876d149
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 3 20:58:35 2017 -0400

    ftrace: Added ftrace_func_mapper for function probe triggers
    
    In order to move the ops to the function probes directly, they need a way to
    map function ips to their own data without depending on the infrastructure
    of the function probes, as the data field will be going away.
    
    New helper functions are added that are based on the ftrace_hash code.
    ftrace_func_mapper functions are there to let the probes map ips to their
    data. These can be allocated by the probe ops, and referenced in the
    function callbacks.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9dbac1881b03..ee308312fe87 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2460,32 +2460,44 @@ struct event_probe_data {
 	bool				enable;
 };
 
+static void update_event_probe(struct event_probe_data *data)
+{
+	if (data->enable)
+		clear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);
+	else
+		set_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);
+}
+
 static void
 event_enable_probe(unsigned long ip, unsigned long parent_ip,
 		   struct ftrace_probe_ops *ops, void **_data)
 {
-	struct event_probe_data **pdata = (struct event_probe_data **)_data;
-	struct event_probe_data *data = *pdata;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct event_probe_data *data;
+	void **pdata;
 
-	if (!data)
+	pdata = ftrace_func_mapper_find_ip(mapper, ip);
+	if (!pdata || !*pdata)
 		return;
 
-	if (data->enable)
-		clear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);
-	else
-		set_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);
+	data = *pdata;
+	update_event_probe(data);
 }
 
 static void
 event_enable_count_probe(unsigned long ip, unsigned long parent_ip,
 			 struct ftrace_probe_ops *ops, void **_data)
 {
-	struct event_probe_data **pdata = (struct event_probe_data **)_data;
-	struct event_probe_data *data = *pdata;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct event_probe_data *data;
+	void **pdata;
 
-	if (!data)
+	pdata = ftrace_func_mapper_find_ip(mapper, ip);
+	if (!pdata || !*pdata)
 		return;
 
+	data = *pdata;
+
 	if (!data->count)
 		return;
 
@@ -2496,14 +2508,23 @@ event_enable_count_probe(unsigned long ip, unsigned long parent_ip,
 	if (data->count != -1)
 		(data->count)--;
 
-	event_enable_probe(ip, parent_ip, ops, _data);
+	update_event_probe(data);
 }
 
 static int
 event_enable_print(struct seq_file *m, unsigned long ip,
 		      struct ftrace_probe_ops *ops, void *_data)
 {
-	struct event_probe_data *data = _data;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct event_probe_data *data;
+	void **pdata;
+
+	pdata = ftrace_func_mapper_find_ip(mapper, ip);
+
+	if (WARN_ON_ONCE(!pdata || !*pdata))
+		return 0;
+
+	data = *pdata;
 
 	seq_printf(m, "%ps:", (void *)ip);
 
@@ -2524,10 +2545,17 @@ static int
 event_enable_init(struct ftrace_probe_ops *ops, unsigned long ip,
 		  void **_data)
 {
+	struct ftrace_func_mapper *mapper = ops->private_data;
 	struct event_probe_data **pdata = (struct event_probe_data **)_data;
 	struct event_probe_data *data = *pdata;
+	int ret;
+
+	ret = ftrace_func_mapper_add_ip(mapper, ip, data);
+	if (ret < 0)
+		return ret;
 
 	data->ref++;
+
 	return 0;
 }
 
@@ -2535,8 +2563,13 @@ static void
 event_enable_free(struct ftrace_probe_ops *ops, unsigned long ip,
 		  void **_data)
 {
-	struct event_probe_data **pdata = (struct event_probe_data **)_data;
-	struct event_probe_data *data = *pdata;
+	struct ftrace_func_mapper *mapper = ops->private_data;
+	struct event_probe_data *data;
+
+	data = ftrace_func_mapper_remove_ip(mapper, ip);
+
+	if (WARN_ON_ONCE(!data))
+		return;
 
 	if (WARN_ON_ONCE(data->ref <= 0))
 		return;
@@ -2548,7 +2581,6 @@ event_enable_free(struct ftrace_probe_ops *ops, unsigned long ip,
 		module_put(data->file->event_call->mod);
 		kfree(data);
 	}
-	*pdata = NULL;
 }
 
 static struct ftrace_probe_ops event_enable_probe_ops = {
@@ -2627,6 +2659,13 @@ event_enable_func(struct ftrace_hash *hash,
 	}
 
 	ret = -ENOMEM;
+
+	if (!ops->private_data) {
+		ops->private_data = allocate_ftrace_func_mapper();
+		if (!ops->private_data)
+			goto out;
+	}
+
 	data = kzalloc(sizeof(*data), GFP_KERNEL);
 	if (!data)
 		goto out;
@@ -2663,6 +2702,7 @@ event_enable_func(struct ftrace_hash *hash,
 	ret = __ftrace_event_enable_disable(file, 1, 1);
 	if (ret < 0)
 		goto out_put;
+
 	ret = register_ftrace_function_probe(glob, ops, data);
 	/*
 	 * The above returns on success the # of functions enabled,

commit bca6c8d0480a8aa5c86f8f416db96c71f6b79e29
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon Apr 3 18:18:47 2017 -0400

    ftrace: Pass probe ops to probe function
    
    In preparation to cleaning up the probe function registration code, the
    "data" parameter will eventually be removed from the probe->func() call.
    Instead it will receive its own "ops" function, in which it can set up its
    own data that it needs to map.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 93116549a284..9dbac1881b03 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2461,7 +2461,8 @@ struct event_probe_data {
 };
 
 static void
-event_enable_probe(unsigned long ip, unsigned long parent_ip, void **_data)
+event_enable_probe(unsigned long ip, unsigned long parent_ip,
+		   struct ftrace_probe_ops *ops, void **_data)
 {
 	struct event_probe_data **pdata = (struct event_probe_data **)_data;
 	struct event_probe_data *data = *pdata;
@@ -2476,7 +2477,8 @@ event_enable_probe(unsigned long ip, unsigned long parent_ip, void **_data)
 }
 
 static void
-event_enable_count_probe(unsigned long ip, unsigned long parent_ip, void **_data)
+event_enable_count_probe(unsigned long ip, unsigned long parent_ip,
+			 struct ftrace_probe_ops *ops, void **_data)
 {
 	struct event_probe_data **pdata = (struct event_probe_data **)_data;
 	struct event_probe_data *data = *pdata;
@@ -2494,7 +2496,7 @@ event_enable_count_probe(unsigned long ip, unsigned long parent_ip, void **_data
 	if (data->count != -1)
 		(data->count)--;
 
-	event_enable_probe(ip, parent_ip, _data);
+	event_enable_probe(ip, parent_ip, ops, _data);
 }
 
 static int

commit 989a0a3d248192b6f5d16cc2aea95faed89bb7ce
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 28 13:54:57 2016 -0500

    tracing: Have system enable return error if one of the events fail
    
    If one of the events within a system fails to enable when "1" is written
    to the system "enable" file, it should return an error. Note, some events
    may still be enabled, but the user should know that something did go wrong.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d35fc2b0d304..93116549a284 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -702,6 +702,7 @@ __ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,
 	struct trace_event_call *call;
 	const char *name;
 	int ret = -EINVAL;
+	int eret = 0;
 
 	list_for_each_entry(file, &tr->events, list) {
 
@@ -725,9 +726,17 @@ __ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,
 		if (event && strcmp(event, name) != 0)
 			continue;
 
-		ftrace_event_enable_disable(file, set);
+		ret = ftrace_event_enable_disable(file, set);
 
-		ret = 0;
+		/*
+		 * Save the first error and return that. Some events
+		 * may still have been enabled, but let the user
+		 * know that something went wrong.
+		 */
+		if (ret && !eret)
+			eret = ret;
+
+		ret = eret;
 	}
 
 	return ret;

commit 4239174570da080f3623724d97062bf55de7e36b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Nov 23 15:52:45 2016 -0500

    tracing: Make tracepoint_printk a static_key
    
    Currently, when tracepoint_printk is set (enabled by the "tp_printk" kernel
    command line), it causes trace events to print via printk(). This is a very
    dangerous operation, but is useful for debugging.
    
    The issue is, it's seldom used, but it is always checked even if it's not
    enabled by the kernel command line. Instead of having this feature called by
    a branch against a variable, turn that variable into a static key, and this
    will remove the test and jump.
    
    To simplify things, the functions output_printk() and
    trace_event_buffer_commit() were moved from trace_events.c to trace.c.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ba67ede48822..d35fc2b0d304 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -283,46 +283,6 @@ void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
 }
 EXPORT_SYMBOL_GPL(trace_event_buffer_reserve);
 
-static DEFINE_SPINLOCK(tracepoint_iter_lock);
-
-static void output_printk(struct trace_event_buffer *fbuffer)
-{
-	struct trace_event_call *event_call;
-	struct trace_event *event;
-	unsigned long flags;
-	struct trace_iterator *iter = tracepoint_print_iter;
-
-	if (!iter)
-		return;
-
-	event_call = fbuffer->trace_file->event_call;
-	if (!event_call || !event_call->event.funcs ||
-	    !event_call->event.funcs->trace)
-		return;
-
-	event = &fbuffer->trace_file->event_call->event;
-
-	spin_lock_irqsave(&tracepoint_iter_lock, flags);
-	trace_seq_init(&iter->seq);
-	iter->ent = fbuffer->entry;
-	event_call->event.funcs->trace(iter, 0, event);
-	trace_seq_putc(&iter->seq, 0);
-	printk("%s", iter->seq.buffer);
-
-	spin_unlock_irqrestore(&tracepoint_iter_lock, flags);
-}
-
-void trace_event_buffer_commit(struct trace_event_buffer *fbuffer)
-{
-	if (tracepoint_printk)
-		output_printk(fbuffer);
-
-	event_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,
-				    fbuffer->event, fbuffer->entry,
-				    fbuffer->flags, fbuffer->pc);
-}
-EXPORT_SYMBOL_GPL(trace_event_buffer_commit);
-
 int trace_event_reg(struct trace_event_call *call,
 		    enum trace_reg type, void *data)
 {

commit 7d436400223bb46e9f88e6bba6f8d867acf0d82c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 22 18:32:03 2016 -0500

    tracing: Add error checks to creation of event files
    
    The creation of the set_event_pid file was assigned to a variable "entry"
    but that variable was never used. Ideally, it should be used to check if the
    file was created and warn if it was not.
    
    The files header_page, header_event should also be checked and a warning if
    they fail to be created.
    
    The "enable" file was moved up, as it is a more crucial file to have and a
    hard failure (return -ENOMEM) should be returned if it is not created.
    
    Reported-by: David Binderman <dcb314@hotmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 03c0a48c3ac4..ba67ede48822 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2843,20 +2843,32 @@ create_event_toplevel_files(struct dentry *parent, struct trace_array *tr)
 		return -ENOMEM;
 	}
 
+	entry = trace_create_file("enable", 0644, d_events,
+				  tr, &ftrace_tr_enable_fops);
+	if (!entry) {
+		pr_warn("Could not create tracefs 'enable' entry\n");
+		return -ENOMEM;
+	}
+
+	/* There are not as crucial, just warn if they are not created */
+
 	entry = tracefs_create_file("set_event_pid", 0644, parent,
 				    tr, &ftrace_set_event_pid_fops);
+	if (!entry)
+		pr_warn("Could not create tracefs 'set_event_pid' entry\n");
 
 	/* ring buffer internal formats */
-	trace_create_file("header_page", 0444, d_events,
-			  ring_buffer_print_page_header,
-			  &ftrace_show_header_fops);
-
-	trace_create_file("header_event", 0444, d_events,
-			  ring_buffer_print_entry_header,
-			  &ftrace_show_header_fops);
+	entry = trace_create_file("header_page", 0444, d_events,
+				  ring_buffer_print_page_header,
+				  &ftrace_show_header_fops);
+	if (!entry)
+		pr_warn("Could not create tracefs 'header_page' entry\n");
 
-	trace_create_file("enable", 0644, d_events,
-			  tr, &ftrace_tr_enable_fops);
+	entry = trace_create_file("header_event", 0444, d_events,
+				  ring_buffer_print_entry_header,
+				  &ftrace_show_header_fops);
+	if (!entry)
+		pr_warn("Could not create tracefs 'header_event' entry\n");
 
 	tr->event_dir = d_events;
 

commit e947841c0dce9db675a957182214ef8091ac3d61
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Jun 17 17:40:58 2016 -0400

    tracing: Show the preempt count of when the event was called
    
    Because tracepoint callbacks are done with preemption enabled, the trace
    events are always called with preempt disable due to the
    rcu_read_lock_sched_notrace() in __DO_TRACE(). This causes the preempt count
    shown in the recorded trace event to be inaccurate. It is always one more
    that what the preempt_count was when the tracepoint was called.
    
    If CONFIG_PREEMPT is enabled, subtract 1 from the preempt_count before
    recording it in the trace buffer.
    
    Link: http://lkml.kernel.org/r/20160525132537.GA10808@linutronix.de
    
    Reported-by: Sebastian Andrzej Siewior <bigeasy@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fd449eb138cf..03c0a48c3ac4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -261,6 +261,14 @@ void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
 
 	local_save_flags(fbuffer->flags);
 	fbuffer->pc = preempt_count();
+	/*
+	 * If CONFIG_PREEMPT is enabled, then the tracepoint itself disables
+	 * preemption (adding one to the preempt_count). Since we are
+	 * interested in the preempt_count at the time the tracepoint was
+	 * hit, we need to subtract one to offset the increment.
+	 */
+	if (IS_ENABLED(CONFIG_PREEMPT))
+		fbuffer->pc--;
 	fbuffer->trace_file = trace_file;
 
 	fbuffer->event =

commit 76c813e26606d35ea9d8d6f96e646b3944c730a9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Apr 21 11:35:30 2016 -0400

    tracing: Move pid_list write processing into its own function
    
    The addition of PIDs into a pid_list via the write operation of
    set_event_pid is a bit complex. The same operation will be needed for
    function tracing pids. Move the code into its own generic function in
    trace.c, so that we can avoid duplication of this code.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fd831a972bae..fd449eb138cf 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -15,7 +15,6 @@
 #include <linux/kthread.h>
 #include <linux/tracefs.h>
 #include <linux/uaccess.h>
-#include <linux/vmalloc.h>
 #include <linux/module.h>
 #include <linux/ctype.h>
 #include <linux/sort.h>
@@ -499,9 +498,6 @@ static void ftrace_clear_events(struct trace_array *tr)
 	mutex_unlock(&event_mutex);
 }
 
-/* Shouldn't this be in a header? */
-extern int pid_max;
-
 static void
 event_filter_pid_sched_process_exit(void *data, struct task_struct *task)
 {
@@ -634,8 +630,7 @@ static void __ftrace_clear_event_pids(struct trace_array *tr)
 	/* Wait till all users are no longer using pid filtering */
 	synchronize_sched();
 
-	vfree(pid_list->pids);
-	kfree(pid_list);
+	trace_free_pid_list(pid_list);
 }
 
 static void ftrace_clear_event_pids(struct trace_array *tr)
@@ -1587,13 +1582,7 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	struct trace_pid_list *filtered_pids = NULL;
 	struct trace_pid_list *pid_list;
 	struct trace_event_file *file;
-	struct trace_parser parser;
-	unsigned long val;
-	loff_t this_pos;
-	ssize_t read = 0;
-	ssize_t ret = 0;
-	pid_t pid;
-	int nr_pids = 0;
+	ssize_t ret;
 
 	if (!cnt)
 		return 0;
@@ -1602,93 +1591,15 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	if (ret < 0)
 		return ret;
 
-	if (trace_parser_get_init(&parser, EVENT_BUF_SIZE + 1))
-		return -ENOMEM;
-
 	mutex_lock(&event_mutex);
+
 	filtered_pids = rcu_dereference_protected(tr->filtered_pids,
 					     lockdep_is_held(&event_mutex));
 
-	/*
-	 * Always recreate a new array. The write is an all or nothing
-	 * operation. Always create a new array when adding new pids by
-	 * the user. If the operation fails, then the current list is
-	 * not modified.
-	 */
-	pid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);
-	if (!pid_list) {
-		read = -ENOMEM;
-		goto out;
-	}
-	pid_list->pid_max = READ_ONCE(pid_max);
-	/* Only truncating will shrink pid_max */
-	if (filtered_pids && filtered_pids->pid_max > pid_list->pid_max)
-		pid_list->pid_max = filtered_pids->pid_max;
-	pid_list->pids = vzalloc((pid_list->pid_max + 7) >> 3);
-	if (!pid_list->pids) {
-		kfree(pid_list);
-		read = -ENOMEM;
-		goto out;
-	}
-	if (filtered_pids) {
-		/* copy the current bits to the new max */
-		pid = find_first_bit(filtered_pids->pids,
-				     filtered_pids->pid_max);
-		while (pid < filtered_pids->pid_max) {
-			set_bit(pid, pid_list->pids);
-			pid = find_next_bit(filtered_pids->pids,
-					    filtered_pids->pid_max,
-					    pid + 1);
-			nr_pids++;
-		}
-	}
-
-	while (cnt > 0) {
-
-		this_pos = 0;
-
-		ret = trace_get_user(&parser, ubuf, cnt, &this_pos);
-		if (ret < 0 || !trace_parser_loaded(&parser))
-			break;
-
-		read += ret;
-		ubuf += ret;
-		cnt -= ret;
-
-		parser.buffer[parser.idx] = 0;
-
-		ret = -EINVAL;
-		if (kstrtoul(parser.buffer, 0, &val))
-			break;
-		if (val >= pid_list->pid_max)
-			break;
-
-		pid = (pid_t)val;
-
-		set_bit(pid, pid_list->pids);
-		nr_pids++;
-
-		trace_parser_clear(&parser);
-		ret = 0;
-	}
-	trace_parser_put(&parser);
-
-	if (ret < 0) {
-		vfree(pid_list->pids);
-		kfree(pid_list);
-		read = ret;
+	ret = trace_pid_write(filtered_pids, &pid_list, ubuf, cnt);
+	if (ret < 0)
 		goto out;
-	}
 
-	if (!nr_pids) {
-		/* Cleared the list of pids */
-		vfree(pid_list->pids);
-		kfree(pid_list);
-		read = ret;
-		if (!filtered_pids)
-			goto out;
-		pid_list = NULL;
-	}
 	rcu_assign_pointer(tr->filtered_pids, pid_list);
 
 	list_for_each_entry(file, &tr->events, list) {
@@ -1697,10 +1608,8 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 
 	if (filtered_pids) {
 		synchronize_sched();
-
-		vfree(filtered_pids->pids);
-		kfree(filtered_pids);
-	} else {
+		trace_free_pid_list(filtered_pids);
+	} else if (pid_list) {
 		/*
 		 * Register a probe that is called before all other probes
 		 * to set ignore_pid if next or prev do not match.
@@ -1738,9 +1647,8 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
  out:
 	mutex_unlock(&event_mutex);
 
-	ret = read;
-	if (read > 0)
-		*ppos += read;
+	if (ret > 0)
+		*ppos += ret;
 
 	return ret;
 }

commit 5cc8976bd52153678ca37cc1e3000833b20276f3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 20 15:19:54 2016 -0400

    tracing: Move the pid_list seq_file functions to be global
    
    To allow other aspects of ftrace to use the pid_list logic, we need to reuse
    the seq_file functions. Making the generic part into functions that can be
    called by other files will help in this regard.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a11e6d9a3841..fd831a972bae 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -983,18 +983,8 @@ p_next(struct seq_file *m, void *v, loff_t *pos)
 {
 	struct trace_array *tr = m->private;
 	struct trace_pid_list *pid_list = rcu_dereference_sched(tr->filtered_pids);
-	unsigned long pid = (unsigned long)v;
 
-	(*pos)++;
-
-	/* pid already is +1 of the actual prevous bit */
-	pid = find_next_bit(pid_list->pids, pid_list->pid_max, pid);
-
-	/* Return pid + 1 to allow zero to be represented */
-	if (pid < pid_list->pid_max)
-		return (void *)(pid + 1);
-
-	return NULL;
+	return trace_pid_next(pid_list, v, pos);
 }
 
 static void *p_start(struct seq_file *m, loff_t *pos)
@@ -1002,8 +992,6 @@ static void *p_start(struct seq_file *m, loff_t *pos)
 {
 	struct trace_pid_list *pid_list;
 	struct trace_array *tr = m->private;
-	unsigned long pid;
-	loff_t l = 0;
 
 	/*
 	 * Grab the mutex, to keep calls to p_next() having the same
@@ -1019,15 +1007,7 @@ static void *p_start(struct seq_file *m, loff_t *pos)
 	if (!pid_list)
 		return NULL;
 
-	pid = find_first_bit(pid_list->pids, pid_list->pid_max);
-	if (pid >= pid_list->pid_max)
-		return NULL;
-
-	/* Return pid + 1 so that zero can be the exit value */
-	for (pid++; pid && l < *pos;
-	     pid = (unsigned long)p_next(m, (void *)pid, &l))
-		;
-	return (void *)pid;
+	return trace_pid_start(pid_list, pos);
 }
 
 static void p_stop(struct seq_file *m, void *p)
@@ -1037,14 +1017,6 @@ static void p_stop(struct seq_file *m, void *p)
 	mutex_unlock(&event_mutex);
 }
 
-static int p_show(struct seq_file *m, void *v)
-{
-	unsigned long pid = (unsigned long)v - 1;
-
-	seq_printf(m, "%lu\n", pid);
-	return 0;
-}
-
 static ssize_t
 event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
@@ -1795,7 +1767,7 @@ static const struct seq_operations show_set_event_seq_ops = {
 static const struct seq_operations show_set_pid_seq_ops = {
 	.start = p_start,
 	.next = p_next,
-	.show = p_show,
+	.show = trace_pid_show,
 	.stop = p_stop,
 };
 

commit d8275c454dcdba296675221b4c12f19d1b6e0ee8
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Apr 14 12:15:22 2016 -0400

    tracing: Move filtered_pid helper functions into trace.c
    
    As the filtered_pid functions are going to be used by function tracer as
    well as trace_events, move the code into the generic trace.c file.
    
    The functions moved are:
    
     trace_find_filtered_pid()
     trace_ignore_this_task()
     trace_filter_add_remove_task()
    
    Kernel Doc text was also added.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b5e514c4dada..a11e6d9a3841 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -502,57 +502,6 @@ static void ftrace_clear_events(struct trace_array *tr)
 /* Shouldn't this be in a header? */
 extern int pid_max;
 
-/* Returns true if found in filter */
-bool
-trace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)
-{
-	/*
-	 * If pid_max changed after filtered_pids was created, we
-	 * by default ignore all pids greater than the previous pid_max.
-	 */
-	if (search_pid >= filtered_pids->pid_max)
-		return false;
-
-	return test_bit(search_pid, filtered_pids->pids);
-}
-
-bool
-trace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
-{
-	/*
-	 * Return false, because if filtered_pids does not exist,
-	 * all pids are good to trace.
-	 */
-	if (!filtered_pids)
-		return false;
-
-	return !trace_find_filtered_pid(filtered_pids, task->pid);
-}
-
-void trace_filter_add_remove_task(struct trace_pid_list *pid_list,
-				  struct task_struct *self,
-				  struct task_struct *task)
-{
-	if (!pid_list)
-		return;
-
-	/* For forks, we only add if the forking task is listed */
-	if (self) {
-		if (!trace_find_filtered_pid(pid_list, self->pid))
-			return;
-	}
-
-	/* Sorry, but we don't support pid_max changing after setting */
-	if (task->pid >= pid_list->pid_max)
-		return;
-
-	/* "self" is set for forks, and NULL for exits */
-	if (self)
-		set_bit(task->pid, pid_list->pids);
-	else
-		clear_bit(task->pid, pid_list->pids);
-}
-
 static void
 event_filter_pid_sched_process_exit(void *data, struct task_struct *task)
 {

commit 4e267db135c44d0b18e553899fe7df32b89211a5
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Apr 14 07:38:13 2016 -0400

    tracing: Make the pid filtering helper functions global
    
    Make the functions used for pid filtering global for tracing, such that the
    function tracer can use the pid code as well.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 3d4155892a1e..b5e514c4dada 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -503,8 +503,8 @@ static void ftrace_clear_events(struct trace_array *tr)
 extern int pid_max;
 
 /* Returns true if found in filter */
-static bool
-find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)
+bool
+trace_find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)
 {
 	/*
 	 * If pid_max changed after filtered_pids was created, we
@@ -516,8 +516,8 @@ find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)
 	return test_bit(search_pid, filtered_pids->pids);
 }
 
-static bool
-ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
+bool
+trace_ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
 {
 	/*
 	 * Return false, because if filtered_pids does not exist,
@@ -526,19 +526,19 @@ ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
 	if (!filtered_pids)
 		return false;
 
-	return !find_filtered_pid(filtered_pids, task->pid);
+	return !trace_find_filtered_pid(filtered_pids, task->pid);
 }
 
-static void filter_add_remove_task(struct trace_pid_list *pid_list,
-				   struct task_struct *self,
-				   struct task_struct *task)
+void trace_filter_add_remove_task(struct trace_pid_list *pid_list,
+				  struct task_struct *self,
+				  struct task_struct *task)
 {
 	if (!pid_list)
 		return;
 
 	/* For forks, we only add if the forking task is listed */
 	if (self) {
-		if (!find_filtered_pid(pid_list, self->pid))
+		if (!trace_find_filtered_pid(pid_list, self->pid))
 			return;
 	}
 
@@ -560,7 +560,7 @@ event_filter_pid_sched_process_exit(void *data, struct task_struct *task)
 	struct trace_array *tr = data;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
-	filter_add_remove_task(pid_list, NULL, task);
+	trace_filter_add_remove_task(pid_list, NULL, task);
 }
 
 static void
@@ -572,7 +572,7 @@ event_filter_pid_sched_process_fork(void *data,
 	struct trace_array *tr = data;
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
-	filter_add_remove_task(pid_list, self, task);
+	trace_filter_add_remove_task(pid_list, self, task);
 }
 
 void trace_event_follow_fork(struct trace_array *tr, bool enable)
@@ -600,8 +600,8 @@ event_filter_pid_sched_switch_probe_pre(void *data, bool preempt,
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       ignore_this_task(pid_list, prev) &&
-		       ignore_this_task(pid_list, next));
+		       trace_ignore_this_task(pid_list, prev) &&
+		       trace_ignore_this_task(pid_list, next));
 }
 
 static void
@@ -614,7 +614,7 @@ event_filter_pid_sched_switch_probe_post(void *data, bool preempt,
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       ignore_this_task(pid_list, next));
+		       trace_ignore_this_task(pid_list, next));
 }
 
 static void
@@ -630,7 +630,7 @@ event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       ignore_this_task(pid_list, task));
+		       trace_ignore_this_task(pid_list, task));
 }
 
 static void
@@ -647,7 +647,7 @@ event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
 
 	/* Set tracing if current is enabled */
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       ignore_this_task(pid_list, current));
+		       trace_ignore_this_task(pid_list, current));
 }
 
 static void __ftrace_clear_event_pids(struct trace_array *tr)
@@ -1654,7 +1654,7 @@ static void ignore_task_cpu(void *data)
 					     mutex_is_locked(&event_mutex));
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       ignore_this_task(pid_list, current));
+		       trace_ignore_this_task(pid_list, current));
 }
 
 static ssize_t

commit 2600a46ee0ed57c0e0a382c2a37ebac64d374d20
Merge: 03e1aa1cbb65 0fc1b09ff1ff
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed May 18 18:55:19 2016 -0700

    Merge tag 'trace-v4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This includes two new updates for the ftrace infrastructure.
    
       - With the changing of the code for filtering events by pid, from a
         list of pids to a bitmask, we can now easily implement following
         forks.  With a new tracing option "event-fork" which, when set,
         will have tasks with pids in set_event_pid, when they fork, to have
         their child pids added to set_event_pid and the child will be
         traced as well.
    
         Note, if "event-fork" is set and a task with its pid in
         set_event_pid exits, its pid will be removed from set_event_pid
    
       - The addition of Tom Zanussi's hist triggers.  This includes a very
         thorough documentatino on how to use the hist triggers with events.
         This introduces a quick and easy way to get histogram data from
         events and their fields.
    
      Some other cleanups and updates were added as well.  Like Masami
      Hiramatsu added test cases for the event trigger and hist triggers.
      Also I added a speed up of filtering by using a temp buffer when
      filters are set"
    
    * tag 'trace-v4.7' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (45 commits)
      tracing: Use temp buffer when filtering events
      tracing: Remove TRACE_EVENT_FL_USE_CALL_FILTER logic
      tracing: Remove unused function trace_current_buffer_lock_reserve()
      tracing: Remove one use of trace_current_buffer_lock_reserve()
      tracing: Have trace_buffer_unlock_commit() call the _regs version with NULL
      tracing: Remove unused function trace_current_buffer_discard_commit()
      tracing: Move trace_buffer_unlock_commit{_regs}() to local header
      tracing: Fold filter_check_discard() into its only user
      tracing: Make filter_check_discard() local
      tracing: Move event_trigger_unlock_commit{_regs}() to local header
      tracing: Don't use the address of the buffer array name in copy_from_user
      tracing: Handle tracing_map_alloc_elts() error path correctly
      tracing: Add check for NULL event field when creating hist field
      tracing: checking for NULL instead of IS_ERR()
      tracing: Do not inherit event-fork option for instances
      tracing: Fix unsigned comparison to zero in hist trigger code
      kselftests/ftrace: Add a test for log2 modifier of hist trigger
      tracing: Add hist trigger 'log2' modifier
      kselftests/ftrace: Add hist trigger testcases
      kselftests/ftrace : Add event trigger testcases
      ...

commit e800072c18f0d7b89a80fa46dceb3d080c80e09c
Merge: e8ed77dfa90d b507146bb6b9
Author: David S. Miller <davem@davemloft.net>
Date:   Mon May 9 15:59:24 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    In netdevice.h we removed the structure in net-next that is being
    changes in 'net'.  In macsec.c and rtnetlink.c we have overlaps
    between fixes in 'net' and the u64 attribute changes in 'net-next'.
    
    The mlx5 conflicts have to do with vxlan support dependencies.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 0fc1b09ff1ff404ddf753f5ffa5cd0adc8fdcdc9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 3 17:15:43 2016 -0400

    tracing: Use temp buffer when filtering events
    
    Filtering of events requires the data to be written to the ring buffer
    before it can be decided to filter or not. This is because the parameters of
    the filter are based on the result that is written to the ring buffer and
    not on the parameters that are passed into the trace functions.
    
    The ftrace ring buffer is optimized for writing into the ring buffer and
    committing. The discard procedure used when filtering decides the event
    should be discarded is much more heavy weight. Thus, using a temporary
    filter when filtering events can speed things up drastically.
    
    Without a temp buffer we have:
    
     # trace-cmd start -p nop
     # perf stat -r 10 hackbench 50
           0.790706626 seconds time elapsed ( +-  0.71% )
    
     # trace-cmd start -e all
     # perf stat -r 10 hackbench 50
           1.566904059 seconds time elapsed ( +-  0.27% )
    
     # trace-cmd start -e all -f 'common_preempt_count==20'
     # perf stat -r 10 hackbench 50
           1.690598511 seconds time elapsed ( +-  0.19% )
    
     # trace-cmd start -e all -f 'common_preempt_count!=20'
     # perf stat -r 10 hackbench 50
           1.707486364 seconds time elapsed ( +-  0.30% )
    
    The first run above is without any tracing, just to get a based figure.
    hackbench takes ~0.79 seconds to run on the system.
    
    The second run enables tracing all events where nothing is filtered. This
    increases the time by 100% and hackbench takes 1.57 seconds to run.
    
    The third run filters all events where the preempt count will equal "20"
    (this should never happen) thus all events are discarded. This takes 1.69
    seconds to run. This is 10% slower than just committing the events!
    
    The last run enables all events and filters where the filter will commit all
    events, and this takes 1.70 seconds to run. The filtering overhead is
    approximately 10%. Thus, the discard and commit of an event from the ring
    buffer may be about the same time.
    
    With this patch, the numbers change:
    
     # trace-cmd start -p nop
     # perf stat -r 10 hackbench 50
           0.778233033 seconds time elapsed ( +-  0.38% )
    
     # trace-cmd start -e all
     # perf stat -r 10 hackbench 50
           1.582102692 seconds time elapsed ( +-  0.28% )
    
     # trace-cmd start -e all -f 'common_preempt_count==20'
     # perf stat -r 10 hackbench 50
           1.309230710 seconds time elapsed ( +-  0.22% )
    
     # trace-cmd start -e all -f 'common_preempt_count!=20'
     # perf stat -r 10 hackbench 50
           1.786001924 seconds time elapsed ( +-  0.20% )
    
    The first run is again the base with no tracing.
    
    The second run is all tracing with no filtering. It is a little slower, but
    that may be well within the noise.
    
    The third run shows that discarding all events only took 1.3 seconds. This
    is a speed up of 23%! The discard is much faster than even the commit.
    
    The one downside is shown in the last run. Events that are not discarded by
    the filter will take longer to add, this is due to the extra copy of the
    event.
    
    Cc: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index da1eeb6190e3..4d006707b947 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -363,6 +363,7 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 {
 	struct trace_event_call *call = file->event_call;
 	struct trace_array *tr = file->tr;
+	unsigned long file_flags = file->flags;
 	int ret = 0;
 	int disable;
 
@@ -445,6 +446,15 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 		break;
 	}
 
+	/* Enable or disable use of trace_buffered_event */
+	if ((file_flags & EVENT_FILE_FL_SOFT_DISABLED) !=
+	    (file->flags & EVENT_FILE_FL_SOFT_DISABLED)) {
+		if (file->flags & EVENT_FILE_FL_SOFT_DISABLED)
+			trace_buffered_event_enable();
+		else
+			trace_buffered_event_disable();
+	}
+
 	return ret;
 }
 

commit 854145e0a8e9a05f7366d240e2f99d9c1ca6d6dd
Author: Chunyu Hu <chuhu@redhat.com>
Date:   Tue May 3 19:34:34 2016 +0800

    tracing: Don't display trigger file for events that can't be enabled
    
    Currently register functions for events will be called
    through the 'reg' field of event class directly without
    any check when seting up triggers.
    
    Triggers for events that don't support register through
    debug fs (events under events/ftrace are for trace-cmd to
    read event format, and most of them don't have a register
    function except events/ftrace/functionx) can't be enabled
    at all, and an oops will be hit when setting up trigger
    for those events, so just not creating them is an easy way
    to avoid the oops.
    
    Link: http://lkml.kernel.org/r/1462275274-3911-1-git-send-email-chuhu@redhat.com
    
    Cc: stable@vger.kernel.org # 3.14+
    Fixes: 85f2b08268c01 ("tracing: Add basic event trigger framework")
    Signed-off-by: Chunyu Hu <chuhu@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 05ddc0820771..6f965864cc02 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2095,8 +2095,13 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 	trace_create_file("filter", 0644, file->dir, file,
 			  &ftrace_event_filter_fops);
 
-	trace_create_file("trigger", 0644, file->dir, file,
-			  &event_trigger_fops);
+	/*
+	 * Only event directories that can be enabled should have
+	 * triggers.
+	 */
+	if (!(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))
+		trace_create_file("trigger", 0644, file->dir, file,
+				  &event_trigger_fops);
 
 	trace_create_file("format", 0444, file->dir, call,
 			  &ftrace_event_format_fops);

commit 9b9db275051cd9191e7776c4fd79ccd4318aa2dc
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 29 18:10:21 2016 -0400

    tracing: Remove one use of trace_current_buffer_lock_reserve()
    
    The only user of trace_current_buffer_lock_reserve() is in the boot up self
    tests. Restructure the code a little to have that code use what everything
    else uses: trace_event_buffer_lock_reserve().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e7cb983ee93c..da1eeb6190e3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -3392,7 +3392,7 @@ static __init void event_trace_self_tests(void)
 
 static DEFINE_PER_CPU(atomic_t, ftrace_test_event_disable);
 
-static struct trace_array *event_tr;
+static struct trace_event_file event_trace_file __initdata;
 
 static void __init
 function_test_events_call(unsigned long ip, unsigned long parent_ip,
@@ -3416,17 +3416,17 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip,
 
 	local_save_flags(flags);
 
-	event = trace_current_buffer_lock_reserve(&buffer,
-						  TRACE_FN, sizeof(*entry),
-						  flags, pc);
+	event = trace_event_buffer_lock_reserve(&buffer, &event_trace_file,
+						TRACE_FN, sizeof(*entry),
+						flags, pc);
 	if (!event)
 		goto out;
 	entry	= ring_buffer_event_data(event);
 	entry->ip			= ip;
 	entry->parent_ip		= parent_ip;
 
-	trace_buffer_unlock_commit(event_tr, buffer, event, flags, pc);
-
+	event_trigger_unlock_commit(&event_trace_file, buffer, event,
+				    entry, flags, pc);
  out:
 	atomic_dec(&per_cpu(ftrace_test_event_disable, cpu));
 	preempt_enable_notrace();
@@ -3441,9 +3441,11 @@ static struct ftrace_ops trace_ops __initdata  =
 static __init void event_trace_self_test_with_function(void)
 {
 	int ret;
-	event_tr = top_trace_array();
-	if (WARN_ON(!event_tr))
+
+	event_trace_file.tr = top_trace_array();
+	if (WARN_ON(!event_trace_file.tr))
 		return;
+
 	ret = register_ftrace_function(&trace_ops);
 	if (WARN_ON(ret < 0)) {
 		pr_info("Failed to enable function tracer for event tests\n");

commit 7ef224d1d0e3a1ade02d02c01ce1dcffb736d2c3
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Mar 3 12:54:42 2016 -0600

    tracing: Add 'hist' event trigger command
    
    'hist' triggers allow users to continually aggregate trace events,
    which can then be viewed afterwards by simply reading a 'hist' file
    containing the aggregation in a human-readable format.
    
    The basic idea is very simple and boils down to a mechanism whereby
    trace events, rather than being exhaustively dumped in raw form and
    viewed directly, are automatically 'compressed' into meaningful tables
    completely defined by the user.
    
    This is done strictly via single-line command-line commands and
    without the aid of any kind of programming language or interpreter.
    
    A surprising number of typical use cases can be accomplished by users
    via this simple mechanism.  In fact, a large number of the tasks that
    users typically do using the more complicated script-based tracing
    tools, at least during the initial stages of an investigation, can be
    accomplished by simply specifying a set of keys and values to be used
    in the creation of a hash table.
    
    The Linux kernel trace event subsystem happens to provide an extensive
    list of keys and values ready-made for such a purpose in the form of
    the event format files associated with each trace event.  By simply
    consulting the format file for field names of interest and by plugging
    them into the hist trigger command, users can create an endless number
    of useful aggregations to help with investigating various properties
    of the system.  See Documentation/trace/events.txt for examples.
    
    hist triggers are implemented on top of the existing event trigger
    infrastructure, and as such are consistent with the existing triggers
    from a user's perspective as well.
    
    The basic syntax follows the existing trigger syntax.  Users start an
    aggregation by writing a 'hist' trigger to the event of interest's
    trigger file:
    
      # echo hist:keys=xxx [ if filter] > event/trigger
    
    Once a hist trigger has been set up, by default it continually
    aggregates every matching event into a hash table using the event key
    and a value field named 'hitcount'.
    
    To view the aggregation at any point in time, simply read the 'hist'
    file in the same directory as the 'trigger' file:
    
      # cat event/hist
    
    The detailed syntax provides additional options for user control, and
    is described exhaustively in Documentation/trace/events.txt and in the
    virtual tracing/README file in the tracing subsystem.
    
    Link: http://lkml.kernel.org/r/72d263b5e1853fe9c314953b65833c3aa75479f2.1457029949.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Reviewed-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index add81dff7520..e7cb983ee93c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2141,6 +2141,10 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 	trace_create_file("trigger", 0644, file->dir, file,
 			  &event_trigger_fops);
 
+#ifdef CONFIG_HIST_TRIGGERS
+	trace_create_file("hist", 0444, file->dir, file,
+			  &event_hist_fops);
+#endif
 	trace_create_file("format", 0444, file->dir, call,
 			  &ftrace_event_format_fops);
 

commit c37775d57830a36382a9774bb84eca4ce3d019cc
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Apr 13 16:59:18 2016 -0400

    tracing: Add infrastructure to allow set_event_pid to follow children
    
    Add the infrastructure needed to have the PIDs in set_event_pid to
    automatically add PIDs of the children of the tasks that have their PIDs in
    set_event_pid. This will also remove PIDs from set_event_pid when a task
    exits
    
    This is implemented by adding hooks into the fork and exit tracepoints. On
    fork, the PIDs are added to the list, and on exit, they are removed.
    
    Add a new option called event_fork that when set, PIDs in set_event_pid will
    automatically get their children PIDs added when they fork, as well as any
    task that exits will have its PID removed from set_event_pid.
    
    This works for instances as well.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 45f7cc72bf25..add81dff7520 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -474,11 +474,23 @@ static void ftrace_clear_events(struct trace_array *tr)
 /* Shouldn't this be in a header? */
 extern int pid_max;
 
+/* Returns true if found in filter */
 static bool
-ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
+find_filtered_pid(struct trace_pid_list *filtered_pids, pid_t search_pid)
 {
-	pid_t pid;
+	/*
+	 * If pid_max changed after filtered_pids was created, we
+	 * by default ignore all pids greater than the previous pid_max.
+	 */
+	if (search_pid >= filtered_pids->pid_max)
+		return false;
+
+	return test_bit(search_pid, filtered_pids->pids);
+}
 
+static bool
+ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
+{
 	/*
 	 * Return false, because if filtered_pids does not exist,
 	 * all pids are good to trace.
@@ -486,16 +498,68 @@ ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
 	if (!filtered_pids)
 		return false;
 
-	pid = task->pid;
+	return !find_filtered_pid(filtered_pids, task->pid);
+}
 
-	/*
-	 * If pid_max changed after filtered_pids was created, we
-	 * by default ignore all pids greater than the previous pid_max.
-	 */
-	if (task->pid >= filtered_pids->pid_max)
-		return true;
+static void filter_add_remove_task(struct trace_pid_list *pid_list,
+				   struct task_struct *self,
+				   struct task_struct *task)
+{
+	if (!pid_list)
+		return;
+
+	/* For forks, we only add if the forking task is listed */
+	if (self) {
+		if (!find_filtered_pid(pid_list, self->pid))
+			return;
+	}
+
+	/* Sorry, but we don't support pid_max changing after setting */
+	if (task->pid >= pid_list->pid_max)
+		return;
+
+	/* "self" is set for forks, and NULL for exits */
+	if (self)
+		set_bit(task->pid, pid_list->pids);
+	else
+		clear_bit(task->pid, pid_list->pids);
+}
+
+static void
+event_filter_pid_sched_process_exit(void *data, struct task_struct *task)
+{
+	struct trace_pid_list *pid_list;
+	struct trace_array *tr = data;
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	filter_add_remove_task(pid_list, NULL, task);
+}
 
-	return !test_bit(task->pid, filtered_pids->pids);
+static void
+event_filter_pid_sched_process_fork(void *data,
+				    struct task_struct *self,
+				    struct task_struct *task)
+{
+	struct trace_pid_list *pid_list;
+	struct trace_array *tr = data;
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	filter_add_remove_task(pid_list, self, task);
+}
+
+void trace_event_follow_fork(struct trace_array *tr, bool enable)
+{
+	if (enable) {
+		register_trace_prio_sched_process_fork(event_filter_pid_sched_process_fork,
+						       tr, INT_MIN);
+		register_trace_prio_sched_process_exit(event_filter_pid_sched_process_exit,
+						       tr, INT_MAX);
+	} else {
+		unregister_trace_sched_process_fork(event_filter_pid_sched_process_fork,
+						    tr);
+		unregister_trace_sched_process_exit(event_filter_pid_sched_process_exit,
+						    tr);
+	}
 }
 
 static void

commit f4d34a87e9c10f0ffd03d3548db6bfb200d06cdf
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Wed Apr 13 16:27:49 2016 -0400

    tracing: Use pid bitmap instead of a pid array for set_event_pid
    
    In order to add the ability to let tasks that are filtered by the events
    have their children also be traced on fork (and then not traced on exit),
    convert the array into a pid bitmask. Most of the time the number of pids is
    only 32768 pids or a 4k bitmask, which is the same size as the default list
    currently is, and that list could grow if more pids are listed.
    
    This also greatly simplifies the code.
    
    Suggested-by: "H. Peter Anvin" <hpa@zytor.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 598a18675a6b..45f7cc72bf25 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -15,7 +15,7 @@
 #include <linux/kthread.h>
 #include <linux/tracefs.h>
 #include <linux/uaccess.h>
-#include <linux/bsearch.h>
+#include <linux/vmalloc.h>
 #include <linux/module.h>
 #include <linux/ctype.h>
 #include <linux/sort.h>
@@ -471,23 +471,13 @@ static void ftrace_clear_events(struct trace_array *tr)
 	mutex_unlock(&event_mutex);
 }
 
-static int cmp_pid(const void *key, const void *elt)
-{
-	const pid_t *search_pid = key;
-	const pid_t *pid = elt;
-
-	if (*search_pid == *pid)
-		return 0;
-	if (*search_pid < *pid)
-		return -1;
-	return 1;
-}
+/* Shouldn't this be in a header? */
+extern int pid_max;
 
 static bool
 ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
 {
-	pid_t search_pid;
-	pid_t *pid;
+	pid_t pid;
 
 	/*
 	 * Return false, because if filtered_pids does not exist,
@@ -496,15 +486,16 @@ ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
 	if (!filtered_pids)
 		return false;
 
-	search_pid = task->pid;
+	pid = task->pid;
 
-	pid = bsearch(&search_pid, filtered_pids->pids,
-		      filtered_pids->nr_pids, sizeof(pid_t),
-		      cmp_pid);
-	if (!pid)
+	/*
+	 * If pid_max changed after filtered_pids was created, we
+	 * by default ignore all pids greater than the previous pid_max.
+	 */
+	if (task->pid >= filtered_pids->pid_max)
 		return true;
 
-	return false;
+	return !test_bit(task->pid, filtered_pids->pids);
 }
 
 static void
@@ -602,7 +593,7 @@ static void __ftrace_clear_event_pids(struct trace_array *tr)
 	/* Wait till all users are no longer using pid filtering */
 	synchronize_sched();
 
-	free_pages((unsigned long)pid_list->pids, pid_list->order);
+	vfree(pid_list->pids);
 	kfree(pid_list);
 }
 
@@ -946,11 +937,32 @@ static void t_stop(struct seq_file *m, void *p)
 	mutex_unlock(&event_mutex);
 }
 
+static void *
+p_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct trace_array *tr = m->private;
+	struct trace_pid_list *pid_list = rcu_dereference_sched(tr->filtered_pids);
+	unsigned long pid = (unsigned long)v;
+
+	(*pos)++;
+
+	/* pid already is +1 of the actual prevous bit */
+	pid = find_next_bit(pid_list->pids, pid_list->pid_max, pid);
+
+	/* Return pid + 1 to allow zero to be represented */
+	if (pid < pid_list->pid_max)
+		return (void *)(pid + 1);
+
+	return NULL;
+}
+
 static void *p_start(struct seq_file *m, loff_t *pos)
 	__acquires(RCU)
 {
 	struct trace_pid_list *pid_list;
 	struct trace_array *tr = m->private;
+	unsigned long pid;
+	loff_t l = 0;
 
 	/*
 	 * Grab the mutex, to keep calls to p_next() having the same
@@ -963,10 +975,18 @@ static void *p_start(struct seq_file *m, loff_t *pos)
 
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
-	if (!pid_list || *pos >= pid_list->nr_pids)
+	if (!pid_list)
+		return NULL;
+
+	pid = find_first_bit(pid_list->pids, pid_list->pid_max);
+	if (pid >= pid_list->pid_max)
 		return NULL;
 
-	return (void *)&pid_list->pids[*pos];
+	/* Return pid + 1 so that zero can be the exit value */
+	for (pid++; pid && l < *pos;
+	     pid = (unsigned long)p_next(m, (void *)pid, &l))
+		;
+	return (void *)pid;
 }
 
 static void p_stop(struct seq_file *m, void *p)
@@ -976,25 +996,11 @@ static void p_stop(struct seq_file *m, void *p)
 	mutex_unlock(&event_mutex);
 }
 
-static void *
-p_next(struct seq_file *m, void *v, loff_t *pos)
-{
-	struct trace_array *tr = m->private;
-	struct trace_pid_list *pid_list = rcu_dereference_sched(tr->filtered_pids);
-
-	(*pos)++;
-
-	if (*pos >= pid_list->nr_pids)
-		return NULL;
-
-	return (void *)&pid_list->pids[*pos];
-}
-
 static int p_show(struct seq_file *m, void *v)
 {
-	pid_t *pid = v;
+	unsigned long pid = (unsigned long)v - 1;
 
-	seq_printf(m, "%d\n", *pid);
+	seq_printf(m, "%lu\n", pid);
 	return 0;
 }
 
@@ -1543,11 +1549,6 @@ show_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 	return r;
 }
 
-static int max_pids(struct trace_pid_list *pid_list)
-{
-	return (PAGE_SIZE << pid_list->order) / sizeof(pid_t);
-}
-
 static void ignore_task_cpu(void *data)
 {
 	struct trace_array *tr = data;
@@ -1571,7 +1572,7 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	struct seq_file *m = filp->private_data;
 	struct trace_array *tr = m->private;
 	struct trace_pid_list *filtered_pids = NULL;
-	struct trace_pid_list *pid_list = NULL;
+	struct trace_pid_list *pid_list;
 	struct trace_event_file *file;
 	struct trace_parser parser;
 	unsigned long val;
@@ -1579,7 +1580,7 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	ssize_t read = 0;
 	ssize_t ret = 0;
 	pid_t pid;
-	int i;
+	int nr_pids = 0;
 
 	if (!cnt)
 		return 0;
@@ -1592,10 +1593,43 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 		return -ENOMEM;
 
 	mutex_lock(&event_mutex);
+	filtered_pids = rcu_dereference_protected(tr->filtered_pids,
+					     lockdep_is_held(&event_mutex));
+
 	/*
-	 * Load as many pids into the array before doing a
-	 * swap from the tr->filtered_pids to the new list.
+	 * Always recreate a new array. The write is an all or nothing
+	 * operation. Always create a new array when adding new pids by
+	 * the user. If the operation fails, then the current list is
+	 * not modified.
 	 */
+	pid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);
+	if (!pid_list) {
+		read = -ENOMEM;
+		goto out;
+	}
+	pid_list->pid_max = READ_ONCE(pid_max);
+	/* Only truncating will shrink pid_max */
+	if (filtered_pids && filtered_pids->pid_max > pid_list->pid_max)
+		pid_list->pid_max = filtered_pids->pid_max;
+	pid_list->pids = vzalloc((pid_list->pid_max + 7) >> 3);
+	if (!pid_list->pids) {
+		kfree(pid_list);
+		read = -ENOMEM;
+		goto out;
+	}
+	if (filtered_pids) {
+		/* copy the current bits to the new max */
+		pid = find_first_bit(filtered_pids->pids,
+				     filtered_pids->pid_max);
+		while (pid < filtered_pids->pid_max) {
+			set_bit(pid, pid_list->pids);
+			pid = find_next_bit(filtered_pids->pids,
+					    filtered_pids->pid_max,
+					    pid + 1);
+			nr_pids++;
+		}
+	}
+
 	while (cnt > 0) {
 
 		this_pos = 0;
@@ -1613,92 +1647,35 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 		ret = -EINVAL;
 		if (kstrtoul(parser.buffer, 0, &val))
 			break;
-		if (val > INT_MAX)
+		if (val >= pid_list->pid_max)
 			break;
 
 		pid = (pid_t)val;
 
-		ret = -ENOMEM;
-		if (!pid_list) {
-			pid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);
-			if (!pid_list)
-				break;
-
-			filtered_pids = rcu_dereference_protected(tr->filtered_pids,
-							lockdep_is_held(&event_mutex));
-			if (filtered_pids)
-				pid_list->order = filtered_pids->order;
-			else
-				pid_list->order = 0;
-
-			pid_list->pids = (void *)__get_free_pages(GFP_KERNEL,
-								  pid_list->order);
-			if (!pid_list->pids)
-				break;
-
-			if (filtered_pids) {
-				pid_list->nr_pids = filtered_pids->nr_pids;
-				memcpy(pid_list->pids, filtered_pids->pids,
-				       pid_list->nr_pids * sizeof(pid_t));
-			} else
-				pid_list->nr_pids = 0;
-		}
-
-		if (pid_list->nr_pids >= max_pids(pid_list)) {
-			pid_t *pid_page;
-
-			pid_page = (void *)__get_free_pages(GFP_KERNEL,
-							    pid_list->order + 1);
-			if (!pid_page)
-				break;
-			memcpy(pid_page, pid_list->pids,
-			       pid_list->nr_pids * sizeof(pid_t));
-			free_pages((unsigned long)pid_list->pids, pid_list->order);
-
-			pid_list->order++;
-			pid_list->pids = pid_page;
-		}
+		set_bit(pid, pid_list->pids);
+		nr_pids++;
 
-		pid_list->pids[pid_list->nr_pids++] = pid;
 		trace_parser_clear(&parser);
 		ret = 0;
 	}
 	trace_parser_put(&parser);
 
 	if (ret < 0) {
-		if (pid_list)
-			free_pages((unsigned long)pid_list->pids, pid_list->order);
+		vfree(pid_list->pids);
 		kfree(pid_list);
-		mutex_unlock(&event_mutex);
-		return ret;
-	}
-
-	if (!pid_list) {
-		mutex_unlock(&event_mutex);
-		return ret;
+		read = ret;
+		goto out;
 	}
 
-	sort(pid_list->pids, pid_list->nr_pids, sizeof(pid_t), cmp_pid, NULL);
-
-	/* Remove duplicates */
-	for (i = 1; i < pid_list->nr_pids; i++) {
-		int start = i;
-
-		while (i < pid_list->nr_pids &&
-		       pid_list->pids[i - 1] == pid_list->pids[i])
-			i++;
-
-		if (start != i) {
-			if (i < pid_list->nr_pids) {
-				memmove(&pid_list->pids[start], &pid_list->pids[i],
-					(pid_list->nr_pids - i) * sizeof(pid_t));
-				pid_list->nr_pids -= i - start;
-				i = start;
-			} else
-				pid_list->nr_pids = start;
-		}
+	if (!nr_pids) {
+		/* Cleared the list of pids */
+		vfree(pid_list->pids);
+		kfree(pid_list);
+		read = ret;
+		if (!filtered_pids)
+			goto out;
+		pid_list = NULL;
 	}
-
 	rcu_assign_pointer(tr->filtered_pids, pid_list);
 
 	list_for_each_entry(file, &tr->events, list) {
@@ -1708,7 +1685,7 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	if (filtered_pids) {
 		synchronize_sched();
 
-		free_pages((unsigned long)filtered_pids->pids, filtered_pids->order);
+		vfree(filtered_pids->pids);
 		kfree(filtered_pids);
 	} else {
 		/*
@@ -1745,10 +1722,12 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 	 */
 	on_each_cpu(ignore_task_cpu, tr, 1);
 
+ out:
 	mutex_unlock(&event_mutex);
 
 	ret = read;
-	*ppos += read;
+	if (read > 0)
+		*ppos += read;
 
 	return ret;
 }

commit 9ebc57cfaad21aacbc363eecead579269a46b493
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Apr 12 20:39:55 2016 -0400

    tracing: Rename check_ignore_pid() to ignore_this_task()
    
    The name "check_ignore_pid" is confusing in trying to figure out if the pid
    should be ignored or not. Rename it to "ignore_this_task" which is pretty
    straight forward, as a task (not a pid) is passed in, and should if true
    should be ignored.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 05ddc0820771..598a18675a6b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -484,7 +484,7 @@ static int cmp_pid(const void *key, const void *elt)
 }
 
 static bool
-check_ignore_pid(struct trace_pid_list *filtered_pids, struct task_struct *task)
+ignore_this_task(struct trace_pid_list *filtered_pids, struct task_struct *task)
 {
 	pid_t search_pid;
 	pid_t *pid;
@@ -517,8 +517,8 @@ event_filter_pid_sched_switch_probe_pre(void *data, bool preempt,
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       check_ignore_pid(pid_list, prev) &&
-		       check_ignore_pid(pid_list, next));
+		       ignore_this_task(pid_list, prev) &&
+		       ignore_this_task(pid_list, next));
 }
 
 static void
@@ -531,7 +531,7 @@ event_filter_pid_sched_switch_probe_post(void *data, bool preempt,
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       check_ignore_pid(pid_list, next));
+		       ignore_this_task(pid_list, next));
 }
 
 static void
@@ -547,7 +547,7 @@ event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
 	pid_list = rcu_dereference_sched(tr->filtered_pids);
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       check_ignore_pid(pid_list, task));
+		       ignore_this_task(pid_list, task));
 }
 
 static void
@@ -564,7 +564,7 @@ event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
 
 	/* Set tracing if current is enabled */
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       check_ignore_pid(pid_list, current));
+		       ignore_this_task(pid_list, current));
 }
 
 static void __ftrace_clear_event_pids(struct trace_array *tr)
@@ -1561,7 +1561,7 @@ static void ignore_task_cpu(void *data)
 					     mutex_is_locked(&event_mutex));
 
 	this_cpu_write(tr->trace_buffer.data->ignore_pid,
-		       check_ignore_pid(pid_list, current));
+		       ignore_this_task(pid_list, current));
 }
 
 static ssize_t

commit 32bbe0078afe86a8bf4c67c6b3477781b15e94dc
Author: Alexei Starovoitov <ast@fb.com>
Date:   Wed Apr 6 18:43:28 2016 -0700

    bpf: sanitize bpf tracepoint access
    
    during bpf program loading remember the last byte of ctx access
    and at the time of attaching the program to tracepoint check that
    the program doesn't access bytes beyond defined in tracepoint fields
    
    This also disallows access to __dynamic_array fields, but can be
    relaxed in the future.
    
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 05ddc0820771..ced963049e0a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -204,6 +204,24 @@ static void trace_destroy_fields(struct trace_event_call *call)
 	}
 }
 
+/*
+ * run-time version of trace_event_get_offsets_<call>() that returns the last
+ * accessible offset of trace fields excluding __dynamic_array bytes
+ */
+int trace_event_get_offsets(struct trace_event_call *call)
+{
+	struct ftrace_event_field *tail;
+	struct list_head *head;
+
+	head = trace_get_fields(call);
+	/*
+	 * head->next points to the last field with the largest offset,
+	 * since it was added last by trace_define_field()
+	 */
+	tail = list_first_entry(head, struct ftrace_event_field, link);
+	return tail->offset + tail->size;
+}
+
 int trace_event_raw_init(struct trace_event_call *call)
 {
 	int id;

commit e57cbaf0eb006eaa207395f3bfd7ce52c1b5539c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Mar 3 17:18:20 2016 -0500

    tracing: Do not have 'comm' filter override event 'comm' field
    
    Commit 9f61668073a8d "tracing: Allow triggers to filter for CPU ids and
    process names" added a 'comm' filter that will filter events based on the
    current tasks struct 'comm'. But this now hides the ability to filter events
    that have a 'comm' field too. For example, sched_migrate_task trace event.
    That has a 'comm' field of the task to be migrated.
    
     echo 'comm == "bash"' > events/sched_migrate_task/filter
    
    will now filter all sched_migrate_task events for tasks named "bash" that
    migrates other tasks (in interrupt context), instead of seeing when "bash"
    itself gets migrated.
    
    This fix requires a couple of changes.
    
    1) Change the look up order for filter predicates to look at the events
       fields before looking at the generic filters.
    
    2) Instead of basing the filter function off of the "comm" name, have the
       generic "comm" filter have its own filter_type (FILTER_COMM). Test
       against the type instead of the name to assign the filter function.
    
    3) Add a new "COMM" filter that works just like "comm" but will filter based
       on the current task, even if the trace event contains a "comm" field.
    
    Do the same for "cpu" field, adding a FILTER_CPU and a filter "CPU".
    
    Cc: stable@vger.kernel.org # v4.3+
    Fixes: 9f61668073a8d "tracing: Allow triggers to filter for CPU ids and process names"
    Reported-by: Matt Fleming <matt@codeblueprint.co.uk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ab09829d3b97..05ddc0820771 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -97,16 +97,16 @@ trace_find_event_field(struct trace_event_call *call, char *name)
 	struct ftrace_event_field *field;
 	struct list_head *head;
 
-	field = __find_event_field(&ftrace_generic_fields, name);
+	head = trace_get_fields(call);
+	field = __find_event_field(head, name);
 	if (field)
 		return field;
 
-	field = __find_event_field(&ftrace_common_fields, name);
+	field = __find_event_field(&ftrace_generic_fields, name);
 	if (field)
 		return field;
 
-	head = trace_get_fields(call);
-	return __find_event_field(head, name);
+	return __find_event_field(&ftrace_common_fields, name);
 }
 
 static int __trace_define_field(struct list_head *head, const char *type,
@@ -171,8 +171,10 @@ static int trace_define_generic_fields(void)
 {
 	int ret;
 
-	__generic_field(int, cpu, FILTER_OTHER);
-	__generic_field(char *, comm, FILTER_PTR_STRING);
+	__generic_field(int, CPU, FILTER_CPU);
+	__generic_field(int, cpu, FILTER_CPU);
+	__generic_field(char *, COMM, FILTER_COMM);
+	__generic_field(char *, comm, FILTER_COMM);
 
 	return ret;
 }

commit d045437a169f899dfb0f6f7ede24cc042543ced9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Feb 24 09:04:24 2016 -0500

    tracing: Fix showing function event in available_events
    
    The ftrace:function event is only displayed for parsing the function tracer
    data. It is not used to enable function tracing, and does not include an
    "enable" file in its event directory.
    
    Originally, this event was kept separate from other events because it did
    not have a ->reg parameter. But perf added a "reg" parameter for its use
    which caused issues, because it made the event available to functions where
    it was not compatible for.
    
    Commit 9b63776fa3ca9 "tracing: Do not enable function event with enable"
    added a TRACE_EVENT_FL_IGNORE_ENABLE flag that prevented the function event
    from being enabled by normal trace events. But this commit missed keeping
    the function event from being displayed by the "available_events" directory,
    which is used to show what events can be enabled by set_event.
    
    One documented way to enable all events is to:
    
     cat available_events > set_event
    
    But because the function event is displayed in the available_events, this
    now causes an INVALID error:
    
     cat: write error: Invalid argument
    
    Reported-by: Chunyu Hu <chuhu@redhat.com>
    Fixes: 9b63776fa3ca9 "tracing: Do not enable function event with enable"
    Cc: stable@vger.kernel.org # 3.4+
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f333e57c4614..ab09829d3b97 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -869,7 +869,8 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 		 * The ftrace subsystem is for showing formats only.
 		 * They can not be enabled or disabled via the event files.
 		 */
-		if (call->class && call->class->reg)
+		if (call->class && call->class->reg &&
+		    !(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))
 			return file;
 	}
 

commit 70f6cbb6f9c95535acd327d1ac1ce5fd078cff1e
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Thu Dec 24 00:13:10 2015 -0500

    kernel/*: switch to memdup_user_nul()
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 4f6ef6912e00..f333e57c4614 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1340,15 +1340,9 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	if (cnt >= PAGE_SIZE)
 		return -EINVAL;
 
-	buf = (char *)__get_free_page(GFP_TEMPORARY);
-	if (!buf)
-		return -ENOMEM;
-
-	if (copy_from_user(buf, ubuf, cnt)) {
-		free_page((unsigned long) buf);
-		return -EFAULT;
-	}
-	buf[cnt] = '\0';
+	buf = memdup_user_nul(ubuf, cnt);
+	if (IS_ERR(buf))
+		return PTR_ERR(buf);
 
 	mutex_lock(&event_mutex);
 	file = event_file_data(filp);
@@ -1356,7 +1350,7 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		err = apply_event_filter(file, buf);
 	mutex_unlock(&event_mutex);
 
-	free_page((unsigned long) buf);
+	kfree(buf);
 	if (err < 0)
 		return err;
 
@@ -1507,18 +1501,12 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	if (cnt >= PAGE_SIZE)
 		return -EINVAL;
 
-	buf = (char *)__get_free_page(GFP_TEMPORARY);
-	if (!buf)
-		return -ENOMEM;
-
-	if (copy_from_user(buf, ubuf, cnt)) {
-		free_page((unsigned long) buf);
-		return -EFAULT;
-	}
-	buf[cnt] = '\0';
+	buf = memdup_user_nul(ubuf, cnt);
+	if (IS_ERR(buf))
+		return PTR_ERR(buf);
 
 	err = apply_subsystem_event_filter(dir, buf);
-	free_page((unsigned long) buf);
+	kfree(buf);
 	if (err < 0)
 		return err;
 

commit 0f72e37e42a8ce427caa1b96f7f51e450f2ecb82
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Dec 1 16:08:05 2015 -0500

    tracing: Add sched_wakeup_new and sched_waking tracepoints for pid filter
    
    The set_event_pid filter relies on attaching to the sched_switch and
    sched_wakeup tracepoints to see if it should filter the tracing on schedule
    tracepoints. By adding the callbacks to sched_wakeup, pids in the
    set_event_pid file will trace the wakeups of those tasks with those pids.
    
    But sched_wakeup_new and sched_waking were missed. These two should also be
    traced. Luckily, these tracepoints share the same class as sched_wakeup
    which means they can use the same pre and post callbacks as sched_wakeup
    does.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 6bbc5f652355..4f6ef6912e00 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -582,6 +582,12 @@ static void __ftrace_clear_event_pids(struct trace_array *tr)
 	unregister_trace_sched_wakeup(event_filter_pid_sched_wakeup_probe_pre, tr);
 	unregister_trace_sched_wakeup(event_filter_pid_sched_wakeup_probe_post, tr);
 
+	unregister_trace_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_pre, tr);
+	unregister_trace_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_post, tr);
+
+	unregister_trace_sched_waking(event_filter_pid_sched_wakeup_probe_pre, tr);
+	unregister_trace_sched_waking(event_filter_pid_sched_wakeup_probe_post, tr);
+
 	list_for_each_entry(file, &tr->events, list) {
 		clear_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);
 	}
@@ -1729,6 +1735,16 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 						 tr, INT_MAX);
 		register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_post,
 						 tr, 0);
+
+		register_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_pre,
+						     tr, INT_MAX);
+		register_trace_prio_sched_wakeup_new(event_filter_pid_sched_wakeup_probe_post,
+						     tr, 0);
+
+		register_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_pre,
+						 tr, INT_MAX);
+		register_trace_prio_sched_waking(event_filter_pid_sched_wakeup_probe_post,
+						 tr, 0);
 	}
 
 	/*

commit 22402cd0af685c1a5d067c87db3051db7fff7709
Merge: 7c623cac4939 d227c3ae4e94
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 6 13:30:20 2015 -0800

    Merge tag 'trace-v4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracking updates from Steven Rostedt:
     "Most of the changes are clean ups and small fixes.  Some of them have
      stable tags to them.  I searched through my INBOX just as the merge
      window opened and found lots of patches to pull.  I ran them through
      all my tests and they were in linux-next for a few days.
    
      Features added this release:
      ----------------------------
    
       - Module globbing.  You can now filter function tracing to several
         modules.  # echo '*:mod:*snd*' > set_ftrace_filter (Dmitry Safonov)
    
       - Tracer specific options are now visible even when the tracer is not
         active.  It was rather annoying that you can only see and modify
         tracer options after enabling the tracer.  Now they are in the
         options/ directory even when the tracer is not active.  Although
         they are still only visible when the tracer is active in the
         trace_options file.
    
       - Trace options are now per instance (although some of the tracer
         specific options are global)
    
       - New tracefs file: set_event_pid.  If any pid is added to this file,
         then all events in the instance will filter out events that are not
         part of this pid.  sched_switch and sched_wakeup events handle next
         and the wakee pids"
    
    * tag 'trace-v4.4' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (68 commits)
      tracefs: Fix refcount imbalance in start_creating()
      tracing: Put back comma for empty fields in boot string parsing
      tracing: Apply tracer specific options from kernel command line.
      tracing: Add some documentation about set_event_pid
      ring_buffer: Remove unneeded smp_wmb() before wakeup of reader benchmark
      tracing: Allow dumping traces without tracking trace started cpus
      ring_buffer: Fix more races when terminating the producer in the benchmark
      ring_buffer: Do no not complete benchmark reader too early
      tracing: Remove redundant TP_ARGS redefining
      tracing: Rename max_stack_lock to stack_trace_max_lock
      tracing: Allow arch-specific stack tracer
      recordmcount: arm64: Replace the ignored mcount call into nop
      recordmcount: Fix endianness handling bug for nop_mcount
      tracepoints: Fix documentation of RCU lockdep checks
      tracing: ftrace_event_is_function() can return boolean
      tracing: is_legal_op() can return boolean
      ring-buffer: rb_event_is_commit() can return boolean
      ring-buffer: rb_per_cpu_empty() can return boolean
      ring_buffer: ring_buffer_empty{cpu}() can return boolean
      ring-buffer: rb_is_reader_page() can return boolean
      ...

commit 43ed384339ae67a74a8ba4851268b23216ef7a44
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 3 22:15:14 2015 -0500

    tracing: Put back comma for empty fields in boot string parsing
    
    Both early_enable_events() and apply_trace_boot_options() parse a boot
    string that may get parsed later on. They both use strsep() which converts a
    comma into a nul character. To still allow the boot string to be parsed
    again the same way, the nul character gets converted back to a comma after
    the token is processed.
    
    The problem is that these two functions check for an empty parameter (two
    commas in a row ",,"), and continue the loop if the parameter is empty, but
    fails to place the comma back. In this case, the second parsing will end at
    this blank field, and not process fields afterward.
    
    In most cases, users should not have an empty field, but if its going to be
    checked, the code might as well be correct.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 292bccf3e011..bee1e1530052 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -3042,16 +3042,16 @@ early_enable_events(struct trace_array *tr, bool disable_first)
 
 		if (!token)
 			break;
-		if (!*token)
-			continue;
 
-		/* Restarting syscalls requires that we stop them first */
-		if (disable_first)
-			ftrace_set_clr_event(tr, token, 0);
+		if (*token) {
+			/* Restarting syscalls requires that we stop them first */
+			if (disable_first)
+				ftrace_set_clr_event(tr, token, 0);
 
-		ret = ftrace_set_clr_event(tr, token, 1);
-		if (ret)
-			pr_warn("Failed to enable trace event: %s\n", token);
+			ret = ftrace_set_clr_event(tr, token, 1);
+			if (ret)
+				pr_warn("Failed to enable trace event: %s\n", token);
+		}
 
 		/* Put back the comma to allow this to be called again */
 		if (buf)

commit 799fd44cf5bbcc51c46b674035bfc49cbf6907ba
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Nov 2 13:08:26 2015 -0500

    tracing: Call on_each_cpu() when adding or removing single pids from set_event_pid
    
    For the case where pids are already in set_event_pid, and one is added or
    removed then each CPU should be checked to make sure that the new or old pid
    is on or not on a CPU.
    
     For example:
    
     # echo 123 >> set_event_pid
    
    or
    
     # echo '!123' >> set_event_pid
    
    Link: http://lkml.kernel.org/r/20151030061643.GA19480@cac
    
    Suggested-by: Jiaxing Wang <hello.wjx@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fb0261e90acc..292bccf3e011 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1729,14 +1729,15 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 						 tr, INT_MAX);
 		register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_post,
 						 tr, 0);
-
-		/*
-		 * Ignoring of pids is done at task switch. But we have to
-		 * check for those tasks that are currently running.
-		 */
-		on_each_cpu(ignore_task_cpu, tr, 1);
 	}
 
+	/*
+	 * Ignoring of pids is done at task switch. But we have to
+	 * check for those tasks that are currently running.
+	 * Always do this in case a pid was appended or removed.
+	 */
+	on_each_cpu(ignore_task_cpu, tr, 1);
+
 	mutex_unlock(&event_mutex);
 
 	ret = read;

commit fb662288284e8f2ec26f13d50a6b0d5781771648
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Oct 26 03:45:22 2015 -0400

    tracing: Fix sparse RCU warning
    
    p_start() and p_stop() are seq_file functions that match. Teach sparse to
    know that rcu_read_lock_sched() that is taken by p_start() is released by
    p_stop.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2b7fccd499c6..fb0261e90acc 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -938,6 +938,7 @@ static void t_stop(struct seq_file *m, void *p)
 }
 
 static void *p_start(struct seq_file *m, loff_t *pos)
+	__acquires(RCU)
 {
 	struct trace_pid_list *pid_list;
 	struct trace_array *tr = m->private;
@@ -960,6 +961,7 @@ static void *p_start(struct seq_file *m, loff_t *pos)
 }
 
 static void p_stop(struct seq_file *m, void *p)
+	__releases(RCU)
 {
 	rcu_read_unlock_sched();
 	mutex_unlock(&event_mutex);

commit 8ca532ad2b050da0d0db3544d9ab8b40675e4ca1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Oct 21 15:27:36 2015 -0400

    tracing: Check all tasks on each CPU when filtering pids
    
    My tests found that if a task is running but not filtered when set_event_pid
    is modified, then it can still be traced.
    
    Call on_each_cpu() to check if the current running task should be filtered
    and update the per cpu flags of tr->data appropriately.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ab07058e27c1..2b7fccd499c6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1549,6 +1549,22 @@ static int max_pids(struct trace_pid_list *pid_list)
 	return (PAGE_SIZE << pid_list->order) / sizeof(pid_t);
 }
 
+static void ignore_task_cpu(void *data)
+{
+	struct trace_array *tr = data;
+	struct trace_pid_list *pid_list;
+
+	/*
+	 * This function is called by on_each_cpu() while the
+	 * event_mutex is held.
+	 */
+	pid_list = rcu_dereference_protected(tr->filtered_pids,
+					     mutex_is_locked(&event_mutex));
+
+	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+		       check_ignore_pid(pid_list, current));
+}
+
 static ssize_t
 ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 		       size_t cnt, loff_t *ppos)
@@ -1711,6 +1727,12 @@ ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 						 tr, INT_MAX);
 		register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_post,
 						 tr, 0);
+
+		/*
+		 * Ignoring of pids is done at task switch. But we have to
+		 * check for those tasks that are currently running.
+		 */
+		on_each_cpu(ignore_task_cpu, tr, 1);
 	}
 
 	mutex_unlock(&event_mutex);

commit 3fdaf80f4a836911c0eda1cee92f8aa625f90197
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 25 12:58:44 2015 -0400

    tracing: Implement event pid filtering
    
    Add the necessary hooks to use the pids loaded in set_event_pid to filter
    all the events enabled in the tracing instance that match the pids listed.
    
    Two probes are added to both sched_switch and sched_wakeup tracepoints to be
    called before other probes are called and after the other probes are called.
    The first is used to set the necessary flags to let the probes know to test
    if they should be traced or not.
    
    The sched_switch pre probe will set the "ignore_pid" flag if neither the
    previous or next task has a matching pid.
    
    The sched_switch probe will set the "ignore_pid" flag if the next task
    does not match the matching pid.
    
    The pre probe allows for probes tracing sched_switch to be traced if
    necessary.
    
    The sched_wakeup pre probe will set the "ignore_pid" flag if neither the
    current task nor the wakee task has a matching pid.
    
    The sched_wakeup post probe will set the "ignore_pid" flag if the current
    task does not have a matching pid.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2ad7014707ee..ab07058e27c1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -22,6 +22,8 @@
 #include <linux/slab.h>
 #include <linux/delay.h>
 
+#include <trace/events/sched.h>
+
 #include <asm/setup.h>
 
 #include "trace_output.h"
@@ -212,12 +214,32 @@ int trace_event_raw_init(struct trace_event_call *call)
 }
 EXPORT_SYMBOL_GPL(trace_event_raw_init);
 
+bool trace_event_ignore_this_pid(struct trace_event_file *trace_file)
+{
+	struct trace_array *tr = trace_file->tr;
+	struct trace_array_cpu *data;
+	struct trace_pid_list *pid_list;
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+	if (!pid_list)
+		return false;
+
+	data = this_cpu_ptr(tr->trace_buffer.data);
+
+	return data->ignore_pid;
+}
+EXPORT_SYMBOL_GPL(trace_event_ignore_this_pid);
+
 void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
 				 struct trace_event_file *trace_file,
 				 unsigned long len)
 {
 	struct trace_event_call *event_call = trace_file->event_call;
 
+	if ((trace_file->flags & EVENT_FILE_FL_PID_FILTER) &&
+	    trace_event_ignore_this_pid(trace_file))
+		return NULL;
+
 	local_save_flags(fbuffer->flags);
 	fbuffer->pc = preempt_count();
 	fbuffer->trace_file = trace_file;
@@ -459,15 +481,114 @@ static int cmp_pid(const void *key, const void *elt)
 	return 1;
 }
 
+static bool
+check_ignore_pid(struct trace_pid_list *filtered_pids, struct task_struct *task)
+{
+	pid_t search_pid;
+	pid_t *pid;
+
+	/*
+	 * Return false, because if filtered_pids does not exist,
+	 * all pids are good to trace.
+	 */
+	if (!filtered_pids)
+		return false;
+
+	search_pid = task->pid;
+
+	pid = bsearch(&search_pid, filtered_pids->pids,
+		      filtered_pids->nr_pids, sizeof(pid_t),
+		      cmp_pid);
+	if (!pid)
+		return true;
+
+	return false;
+}
+
+static void
+event_filter_pid_sched_switch_probe_pre(void *data,
+		    struct task_struct *prev, struct task_struct *next)
+{
+	struct trace_array *tr = data;
+	struct trace_pid_list *pid_list;
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+
+	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+		       check_ignore_pid(pid_list, prev) &&
+		       check_ignore_pid(pid_list, next));
+}
+
+static void
+event_filter_pid_sched_switch_probe_post(void *data,
+		    struct task_struct *prev, struct task_struct *next)
+{
+	struct trace_array *tr = data;
+	struct trace_pid_list *pid_list;
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+
+	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+		       check_ignore_pid(pid_list, next));
+}
+
+static void
+event_filter_pid_sched_wakeup_probe_pre(void *data, struct task_struct *task)
+{
+	struct trace_array *tr = data;
+	struct trace_pid_list *pid_list;
+
+	/* Nothing to do if we are already tracing */
+	if (!this_cpu_read(tr->trace_buffer.data->ignore_pid))
+		return;
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+
+	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+		       check_ignore_pid(pid_list, task));
+}
+
+static void
+event_filter_pid_sched_wakeup_probe_post(void *data, struct task_struct *task)
+{
+	struct trace_array *tr = data;
+	struct trace_pid_list *pid_list;
+
+	/* Nothing to do if we are not tracing */
+	if (this_cpu_read(tr->trace_buffer.data->ignore_pid))
+		return;
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+
+	/* Set tracing if current is enabled */
+	this_cpu_write(tr->trace_buffer.data->ignore_pid,
+		       check_ignore_pid(pid_list, current));
+}
+
 static void __ftrace_clear_event_pids(struct trace_array *tr)
 {
 	struct trace_pid_list *pid_list;
+	struct trace_event_file *file;
+	int cpu;
 
 	pid_list = rcu_dereference_protected(tr->filtered_pids,
 					     lockdep_is_held(&event_mutex));
 	if (!pid_list)
 		return;
 
+	unregister_trace_sched_switch(event_filter_pid_sched_switch_probe_pre, tr);
+	unregister_trace_sched_switch(event_filter_pid_sched_switch_probe_post, tr);
+
+	unregister_trace_sched_wakeup(event_filter_pid_sched_wakeup_probe_pre, tr);
+	unregister_trace_sched_wakeup(event_filter_pid_sched_wakeup_probe_post, tr);
+
+	list_for_each_entry(file, &tr->events, list) {
+		clear_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);
+	}
+
+	for_each_possible_cpu(cpu)
+		per_cpu_ptr(tr->trace_buffer.data, cpu)->ignore_pid = false;
+
 	rcu_assign_pointer(tr->filtered_pids, NULL);
 
 	/* Wait till all users are no longer using pid filtering */
@@ -1429,13 +1550,14 @@ static int max_pids(struct trace_pid_list *pid_list)
 }
 
 static ssize_t
-ftrace_event_pid_write(struct file *file, const char __user *ubuf,
+ftrace_event_pid_write(struct file *filp, const char __user *ubuf,
 		       size_t cnt, loff_t *ppos)
 {
-	struct seq_file *m = file->private_data;
+	struct seq_file *m = filp->private_data;
 	struct trace_array *tr = m->private;
 	struct trace_pid_list *filtered_pids = NULL;
 	struct trace_pid_list *pid_list = NULL;
+	struct trace_event_file *file;
 	struct trace_parser parser;
 	unsigned long val;
 	loff_t this_pos;
@@ -1564,15 +1686,35 @@ ftrace_event_pid_write(struct file *file, const char __user *ubuf,
 
 	rcu_assign_pointer(tr->filtered_pids, pid_list);
 
-	mutex_unlock(&event_mutex);
+	list_for_each_entry(file, &tr->events, list) {
+		set_bit(EVENT_FILE_FL_PID_FILTER_BIT, &file->flags);
+	}
 
 	if (filtered_pids) {
 		synchronize_sched();
 
 		free_pages((unsigned long)filtered_pids->pids, filtered_pids->order);
 		kfree(filtered_pids);
+	} else {
+		/*
+		 * Register a probe that is called before all other probes
+		 * to set ignore_pid if next or prev do not match.
+		 * Register a probe this is called after all other probes
+		 * to only keep ignore_pid set if next pid matches.
+		 */
+		register_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_pre,
+						 tr, INT_MAX);
+		register_trace_prio_sched_switch(event_filter_pid_sched_switch_probe_post,
+						 tr, 0);
+
+		register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_pre,
+						 tr, INT_MAX);
+		register_trace_prio_sched_wakeup(event_filter_pid_sched_wakeup_probe_post,
+						 tr, 0);
 	}
 
+	mutex_unlock(&event_mutex);
+
 	ret = read;
 	*ppos += read;
 

commit 4909010788640b7101bf50cddb7c5e60172b4433
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu Sep 24 11:33:26 2015 -0400

    tracing: Add set_event_pid directory for future use
    
    Create a tracing directory called set_event_pid, which currently has no
    function, but will be used to filter all events for the tracing instance or
    the pids that are added to the file.
    
    The reason no functionality is added with this commit is that this commit
    focuses on the creation and removal of the pids in a safe manner. And tests
    can be made against this change to make sure things are correct before
    hooking features to the list of pids.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d120cfe3cca7..2ad7014707ee 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -15,8 +15,10 @@
 #include <linux/kthread.h>
 #include <linux/tracefs.h>
 #include <linux/uaccess.h>
+#include <linux/bsearch.h>
 #include <linux/module.h>
 #include <linux/ctype.h>
+#include <linux/sort.h>
 #include <linux/slab.h>
 #include <linux/delay.h>
 
@@ -445,6 +447,43 @@ static void ftrace_clear_events(struct trace_array *tr)
 	mutex_unlock(&event_mutex);
 }
 
+static int cmp_pid(const void *key, const void *elt)
+{
+	const pid_t *search_pid = key;
+	const pid_t *pid = elt;
+
+	if (*search_pid == *pid)
+		return 0;
+	if (*search_pid < *pid)
+		return -1;
+	return 1;
+}
+
+static void __ftrace_clear_event_pids(struct trace_array *tr)
+{
+	struct trace_pid_list *pid_list;
+
+	pid_list = rcu_dereference_protected(tr->filtered_pids,
+					     lockdep_is_held(&event_mutex));
+	if (!pid_list)
+		return;
+
+	rcu_assign_pointer(tr->filtered_pids, NULL);
+
+	/* Wait till all users are no longer using pid filtering */
+	synchronize_sched();
+
+	free_pages((unsigned long)pid_list->pids, pid_list->order);
+	kfree(pid_list);
+}
+
+static void ftrace_clear_event_pids(struct trace_array *tr)
+{
+	mutex_lock(&event_mutex);
+	__ftrace_clear_event_pids(tr);
+	mutex_unlock(&event_mutex);
+}
+
 static void __put_system(struct event_subsystem *system)
 {
 	struct event_filter *filter = system->filter;
@@ -777,6 +816,56 @@ static void t_stop(struct seq_file *m, void *p)
 	mutex_unlock(&event_mutex);
 }
 
+static void *p_start(struct seq_file *m, loff_t *pos)
+{
+	struct trace_pid_list *pid_list;
+	struct trace_array *tr = m->private;
+
+	/*
+	 * Grab the mutex, to keep calls to p_next() having the same
+	 * tr->filtered_pids as p_start() has.
+	 * If we just passed the tr->filtered_pids around, then RCU would
+	 * have been enough, but doing that makes things more complex.
+	 */
+	mutex_lock(&event_mutex);
+	rcu_read_lock_sched();
+
+	pid_list = rcu_dereference_sched(tr->filtered_pids);
+
+	if (!pid_list || *pos >= pid_list->nr_pids)
+		return NULL;
+
+	return (void *)&pid_list->pids[*pos];
+}
+
+static void p_stop(struct seq_file *m, void *p)
+{
+	rcu_read_unlock_sched();
+	mutex_unlock(&event_mutex);
+}
+
+static void *
+p_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct trace_array *tr = m->private;
+	struct trace_pid_list *pid_list = rcu_dereference_sched(tr->filtered_pids);
+
+	(*pos)++;
+
+	if (*pos >= pid_list->nr_pids)
+		return NULL;
+
+	return (void *)&pid_list->pids[*pos];
+}
+
+static int p_show(struct seq_file *m, void *v)
+{
+	pid_t *pid = v;
+
+	seq_printf(m, "%d\n", *pid);
+	return 0;
+}
+
 static ssize_t
 event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
@@ -1334,8 +1423,165 @@ show_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 	return r;
 }
 
+static int max_pids(struct trace_pid_list *pid_list)
+{
+	return (PAGE_SIZE << pid_list->order) / sizeof(pid_t);
+}
+
+static ssize_t
+ftrace_event_pid_write(struct file *file, const char __user *ubuf,
+		       size_t cnt, loff_t *ppos)
+{
+	struct seq_file *m = file->private_data;
+	struct trace_array *tr = m->private;
+	struct trace_pid_list *filtered_pids = NULL;
+	struct trace_pid_list *pid_list = NULL;
+	struct trace_parser parser;
+	unsigned long val;
+	loff_t this_pos;
+	ssize_t read = 0;
+	ssize_t ret = 0;
+	pid_t pid;
+	int i;
+
+	if (!cnt)
+		return 0;
+
+	ret = tracing_update_buffers();
+	if (ret < 0)
+		return ret;
+
+	if (trace_parser_get_init(&parser, EVENT_BUF_SIZE + 1))
+		return -ENOMEM;
+
+	mutex_lock(&event_mutex);
+	/*
+	 * Load as many pids into the array before doing a
+	 * swap from the tr->filtered_pids to the new list.
+	 */
+	while (cnt > 0) {
+
+		this_pos = 0;
+
+		ret = trace_get_user(&parser, ubuf, cnt, &this_pos);
+		if (ret < 0 || !trace_parser_loaded(&parser))
+			break;
+
+		read += ret;
+		ubuf += ret;
+		cnt -= ret;
+
+		parser.buffer[parser.idx] = 0;
+
+		ret = -EINVAL;
+		if (kstrtoul(parser.buffer, 0, &val))
+			break;
+		if (val > INT_MAX)
+			break;
+
+		pid = (pid_t)val;
+
+		ret = -ENOMEM;
+		if (!pid_list) {
+			pid_list = kmalloc(sizeof(*pid_list), GFP_KERNEL);
+			if (!pid_list)
+				break;
+
+			filtered_pids = rcu_dereference_protected(tr->filtered_pids,
+							lockdep_is_held(&event_mutex));
+			if (filtered_pids)
+				pid_list->order = filtered_pids->order;
+			else
+				pid_list->order = 0;
+
+			pid_list->pids = (void *)__get_free_pages(GFP_KERNEL,
+								  pid_list->order);
+			if (!pid_list->pids)
+				break;
+
+			if (filtered_pids) {
+				pid_list->nr_pids = filtered_pids->nr_pids;
+				memcpy(pid_list->pids, filtered_pids->pids,
+				       pid_list->nr_pids * sizeof(pid_t));
+			} else
+				pid_list->nr_pids = 0;
+		}
+
+		if (pid_list->nr_pids >= max_pids(pid_list)) {
+			pid_t *pid_page;
+
+			pid_page = (void *)__get_free_pages(GFP_KERNEL,
+							    pid_list->order + 1);
+			if (!pid_page)
+				break;
+			memcpy(pid_page, pid_list->pids,
+			       pid_list->nr_pids * sizeof(pid_t));
+			free_pages((unsigned long)pid_list->pids, pid_list->order);
+
+			pid_list->order++;
+			pid_list->pids = pid_page;
+		}
+
+		pid_list->pids[pid_list->nr_pids++] = pid;
+		trace_parser_clear(&parser);
+		ret = 0;
+	}
+	trace_parser_put(&parser);
+
+	if (ret < 0) {
+		if (pid_list)
+			free_pages((unsigned long)pid_list->pids, pid_list->order);
+		kfree(pid_list);
+		mutex_unlock(&event_mutex);
+		return ret;
+	}
+
+	if (!pid_list) {
+		mutex_unlock(&event_mutex);
+		return ret;
+	}
+
+	sort(pid_list->pids, pid_list->nr_pids, sizeof(pid_t), cmp_pid, NULL);
+
+	/* Remove duplicates */
+	for (i = 1; i < pid_list->nr_pids; i++) {
+		int start = i;
+
+		while (i < pid_list->nr_pids &&
+		       pid_list->pids[i - 1] == pid_list->pids[i])
+			i++;
+
+		if (start != i) {
+			if (i < pid_list->nr_pids) {
+				memmove(&pid_list->pids[start], &pid_list->pids[i],
+					(pid_list->nr_pids - i) * sizeof(pid_t));
+				pid_list->nr_pids -= i - start;
+				i = start;
+			} else
+				pid_list->nr_pids = start;
+		}
+	}
+
+	rcu_assign_pointer(tr->filtered_pids, pid_list);
+
+	mutex_unlock(&event_mutex);
+
+	if (filtered_pids) {
+		synchronize_sched();
+
+		free_pages((unsigned long)filtered_pids->pids, filtered_pids->order);
+		kfree(filtered_pids);
+	}
+
+	ret = read;
+	*ppos += read;
+
+	return ret;
+}
+
 static int ftrace_event_avail_open(struct inode *inode, struct file *file);
 static int ftrace_event_set_open(struct inode *inode, struct file *file);
+static int ftrace_event_set_pid_open(struct inode *inode, struct file *file);
 static int ftrace_event_release(struct inode *inode, struct file *file);
 
 static const struct seq_operations show_event_seq_ops = {
@@ -1352,6 +1598,13 @@ static const struct seq_operations show_set_event_seq_ops = {
 	.stop = t_stop,
 };
 
+static const struct seq_operations show_set_pid_seq_ops = {
+	.start = p_start,
+	.next = p_next,
+	.show = p_show,
+	.stop = p_stop,
+};
+
 static const struct file_operations ftrace_avail_fops = {
 	.open = ftrace_event_avail_open,
 	.read = seq_read,
@@ -1367,6 +1620,14 @@ static const struct file_operations ftrace_set_event_fops = {
 	.release = ftrace_event_release,
 };
 
+static const struct file_operations ftrace_set_event_pid_fops = {
+	.open = ftrace_event_set_pid_open,
+	.read = seq_read,
+	.write = ftrace_event_pid_write,
+	.llseek = seq_lseek,
+	.release = ftrace_event_release,
+};
+
 static const struct file_operations ftrace_enable_fops = {
 	.open = tracing_open_generic,
 	.read = event_enable_read,
@@ -1477,6 +1738,26 @@ ftrace_event_set_open(struct inode *inode, struct file *file)
 	return ret;
 }
 
+static int
+ftrace_event_set_pid_open(struct inode *inode, struct file *file)
+{
+	const struct seq_operations *seq_ops = &show_set_pid_seq_ops;
+	struct trace_array *tr = inode->i_private;
+	int ret;
+
+	if (trace_array_get(tr) < 0)
+		return -ENODEV;
+
+	if ((file->f_mode & FMODE_WRITE) &&
+	    (file->f_flags & O_TRUNC))
+		ftrace_clear_event_pids(tr);
+
+	ret = ftrace_event_open(inode, file, seq_ops);
+	if (ret < 0)
+		trace_array_put(tr);
+	return ret;
+}
+
 static struct event_subsystem *
 create_new_subsystem(const char *name)
 {
@@ -2471,6 +2752,9 @@ create_event_toplevel_files(struct dentry *parent, struct trace_array *tr)
 		return -ENOMEM;
 	}
 
+	entry = tracefs_create_file("set_event_pid", 0644, parent,
+				    tr, &ftrace_set_event_pid_fops);
+
 	/* ring buffer internal formats */
 	trace_create_file("header_page", 0444, d_events,
 			  ring_buffer_print_page_header,
@@ -2551,6 +2835,9 @@ int event_trace_del_tracer(struct trace_array *tr)
 	/* Disable any event triggers and associated soft-disabled events */
 	clear_event_triggers(tr);
 
+	/* Clear the pid list */
+	__ftrace_clear_event_pids(tr);
+
 	/* Disable any running events */
 	__ftrace_set_clr_event_nolock(tr, NULL, NULL, NULL, 0);
 

commit 79ac6ef521075d0f40805df77e8890c55f538fe4
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Wed Sep 9 23:24:01 2015 +0200

    tracing: Use kstrdup_const instead of private implementation
    
    The kernel now has kstrdup_const/kfree_const for reusing .rodata
    (typically string literals) when possible; there's no reason to
    duplicate that logic in the tracing system. Moreover, as the comment
    above core_kernel_data states, it may not always return true for
    .rodata - that is for example the case on x86_64, where we thus end up
    kstrdup'ing all the passed-in strings.
    
    Arguably, testing for .rodata explicitly (as kstrdup_const does) is
    also more correct: I don't think one is supposed to be able to change
    the name after creating the event_subsystem by passing the address of
    a static char (but non-const) array.
    
    Link: http://lkml.kernel.org/r/1441833841-12955-1-git-send-email-linux@rasmusvillemoes.dk
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 57c9e709772c..d120cfe3cca7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -38,21 +38,19 @@ static LIST_HEAD(ftrace_common_fields);
 static struct kmem_cache *field_cachep;
 static struct kmem_cache *file_cachep;
 
-#define SYSTEM_FL_FREE_NAME		(1 << 31)
-
 static inline int system_refcount(struct event_subsystem *system)
 {
-	return system->ref_count & ~SYSTEM_FL_FREE_NAME;
+	return system->ref_count;
 }
 
 static int system_refcount_inc(struct event_subsystem *system)
 {
-	return (system->ref_count++) & ~SYSTEM_FL_FREE_NAME;
+	return system->ref_count++;
 }
 
 static int system_refcount_dec(struct event_subsystem *system)
 {
-	return (--system->ref_count) & ~SYSTEM_FL_FREE_NAME;
+	return --system->ref_count;
 }
 
 /* Double loops, do not use break, only goto's work */
@@ -461,8 +459,7 @@ static void __put_system(struct event_subsystem *system)
 		kfree(filter->filter_string);
 		kfree(filter);
 	}
-	if (system->ref_count & SYSTEM_FL_FREE_NAME)
-		kfree(system->name);
+	kfree_const(system->name);
 	kfree(system);
 }
 
@@ -1493,13 +1490,9 @@ create_new_subsystem(const char *name)
 	system->ref_count = 1;
 
 	/* Only allocate if dynamic (kprobes and modules) */
-	if (!core_kernel_data((unsigned long)name)) {
-		system->ref_count |= SYSTEM_FL_FREE_NAME;
-		system->name = kstrdup(name, GFP_KERNEL);
-		if (!system->name)
-			goto out_free;
-	} else
-		system->name = name;
+	system->name = kstrdup_const(name, GFP_KERNEL);
+	if (!system->name)
+		goto out_free;
 
 	system->filter = NULL;
 
@@ -1512,8 +1505,7 @@ create_new_subsystem(const char *name)
 	return system;
 
  out_free:
-	if (system->ref_count & SYSTEM_FL_FREE_NAME)
-		kfree(system->name);
+	kfree_const(system->name);
 	kfree(system);
 	return NULL;
 }

commit 2d34f48955158cfdf18704256c84b04fe3a16c7b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Sep 30 11:45:22 2015 -0400

    tracing: Make ftrace_trace_stack() depend on general trace_array flag
    
    In preparation for the multi buffer instances to have their own trace_flags,
    the check in ftrace_trace_stack() needs to test the trace_array descriptor
    flag that is for the current event, not the global_trace descriptor.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0f394112a0a7..57c9e709772c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2941,15 +2941,15 @@ static struct ftrace_ops trace_ops __initdata  =
 static __init void event_trace_self_test_with_function(void)
 {
 	int ret;
+	event_tr = top_trace_array();
+	if (WARN_ON(!event_tr))
+		return;
 	ret = register_ftrace_function(&trace_ops);
 	if (WARN_ON(ret < 0)) {
 		pr_info("Failed to enable function tracer for event tests\n");
 		return;
 	}
 	pr_info("Running tests again, along with the function tracer\n");
-	event_tr = top_trace_array();
-	if (WARN_ON(!event_tr))
-		return;
 	event_trace_self_tests();
 	unregister_ftrace_function(&trace_ops);
 }

commit 983f938ae69585213bbb779d841b90e75f93f545
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Sep 30 09:42:05 2015 -0400

    tracing: Move trace_flags from global to a trace_array field
    
    In preparation to make trace options per instance, the global trace_flags
    needs to be moved from being a global variable to a field within the trace
    instance trace_array structure.
    
    There's still more work to do, as there's some functions that use
    trace_flags without passing in a way to get to the current_trace array. For
    those, the global_trace is used directly (from trace.c). This includes
    setting and clearing the trace_flags. This means that when a new instance is
    created, it just gets the trace_flags of the global_trace and will not be
    able to modify them. Depending on the functions that have access to the
    trace_array, the flags of an instance may not affect parts of its trace,
    where the global_trace is used. These will be fixed in future changes.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b2e3d8d80df8..0f394112a0a7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -338,6 +338,7 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 					 int enable, int soft_disable)
 {
 	struct trace_event_call *call = file->event_call;
+	struct trace_array *tr = file->tr;
 	int ret = 0;
 	int disable;
 
@@ -401,7 +402,7 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 			if (soft_disable)
 				set_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);
 
-			if (trace_flags & TRACE_ITER_RECORD_CMD) {
+			if (tr->trace_flags & TRACE_ITER_RECORD_CMD) {
 				tracing_start_cmdline_record();
 				set_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 			}

commit b7f0c959edfb4448f94bd33c39fda08e10ce6ede
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Sep 25 17:38:44 2015 -0400

    tracing: Pass trace_array into trace_buffer_unlock_commit()
    
    In preparation for having trace options be per instance, the trace_array
    needs to be passed to the trace_buffer_unlock_commit(). The
    trace_event_buffer_lock_reserve() already passes in the trace_event_file
    where the trace_array can be derived from.
    
    Also added a "__init" to the boot up test event plus function tracing
    function function_test_events_call().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7ca09cdc20c2..b2e3d8d80df8 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2891,7 +2891,9 @@ static __init void event_trace_self_tests(void)
 
 static DEFINE_PER_CPU(atomic_t, ftrace_test_event_disable);
 
-static void
+static struct trace_array *event_tr;
+
+static void __init
 function_test_events_call(unsigned long ip, unsigned long parent_ip,
 			  struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
@@ -2922,7 +2924,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip,
 	entry->ip			= ip;
 	entry->parent_ip		= parent_ip;
 
-	trace_buffer_unlock_commit(buffer, event, flags, pc);
+	trace_buffer_unlock_commit(event_tr, buffer, event, flags, pc);
 
  out:
 	atomic_dec(&per_cpu(ftrace_test_event_disable, cpu));
@@ -2944,6 +2946,9 @@ static __init void event_trace_self_test_with_function(void)
 		return;
 	}
 	pr_info("Running tests again, along with the function tracer\n");
+	event_tr = top_trace_array();
+	if (WARN_ON(!event_tr))
+		return;
 	event_trace_self_tests();
 	unregister_ftrace_function(&trace_ops);
 }

commit 9f61668073a8d80650622e792aff876db9ca23c6
Author: Daniel Wagner <daniel.wagner@bmw-carit.de>
Date:   Mon Aug 10 14:35:46 2015 +0200

    tracing: Allow triggers to filter for CPU ids and process names
    
    By extending the filter rules by more generic fields
    we can write triggers filters like
    
      echo 'stacktrace if cpu == 1' > \
            /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger
    
    or
    
      echo 'stacktrace if comm == sshd'  > \
            /sys/kernel/debug/tracing/events/raw_syscalls/sys_enter/trigger
    
    CPU and COMM are not part of struct trace_entry. We could add the two
    new fields to ftrace_common_field list and fix up all depending
    sides. But that looks pretty ugly. Another thing I would like to
    avoid that the 'format' file contents changes.
    
    All this can be avoided by introducing another list which contains
    non field members of struct trace_entry.
    
    Link: http://lkml.kernel.org/r/1439210146-24707-1-git-send-email-daniel.wagner@bmw-carit.de
    
    Signed-off-by: Daniel Wagner <daniel.wagner@bmw-carit.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 404a372ad85a..7ca09cdc20c2 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -30,6 +30,7 @@
 DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
+static LIST_HEAD(ftrace_generic_fields);
 static LIST_HEAD(ftrace_common_fields);
 
 #define GFP_TRACE (GFP_KERNEL | __GFP_ZERO)
@@ -94,6 +95,10 @@ trace_find_event_field(struct trace_event_call *call, char *name)
 	struct ftrace_event_field *field;
 	struct list_head *head;
 
+	field = __find_event_field(&ftrace_generic_fields, name);
+	if (field)
+		return field;
+
 	field = __find_event_field(&ftrace_common_fields, name);
 	if (field)
 		return field;
@@ -144,6 +149,13 @@ int trace_define_field(struct trace_event_call *call, const char *type,
 }
 EXPORT_SYMBOL_GPL(trace_define_field);
 
+#define __generic_field(type, item, filter_type)			\
+	ret = __trace_define_field(&ftrace_generic_fields, #type,	\
+				   #item, 0, 0, is_signed_type(type),	\
+				   filter_type);			\
+	if (ret)							\
+		return ret;
+
 #define __common_field(type, item)					\
 	ret = __trace_define_field(&ftrace_common_fields, #type,	\
 				   "common_" #item,			\
@@ -153,6 +165,16 @@ EXPORT_SYMBOL_GPL(trace_define_field);
 	if (ret)							\
 		return ret;
 
+static int trace_define_generic_fields(void)
+{
+	int ret;
+
+	__generic_field(int, cpu, FILTER_OTHER);
+	__generic_field(char *, comm, FILTER_PTR_STRING);
+
+	return ret;
+}
+
 static int trace_define_common_fields(void)
 {
 	int ret;
@@ -2671,6 +2693,9 @@ static __init int event_trace_init(void)
 	if (!entry)
 		pr_warn("Could not create tracefs 'available_events' entry\n");
 
+	if (trace_define_generic_fields())
+		pr_warn("tracing: Failed to allocated generic fields");
+
 	if (trace_define_common_fields())
 		pr_warn("tracing: Failed to allocate common fields");
 

commit 5d6ad960a71f0b36d95d74ef93285733b9f62f59
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 15:12:33 2015 -0400

    tracing: Rename FTRACE_EVENT_FL_* flags to EVENT_FILE_FL_*
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The FTRACE_EVENT_FL_* flags are flags to
    do with the trace_event files in the tracefs directory. They are not related
    to function tracing. Rename them to a more descriptive name.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 58984c252aac..404a372ad85a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -298,15 +298,15 @@ void trace_event_enable_cmd_record(bool enable)
 	mutex_lock(&event_mutex);
 	do_for_each_event_file(tr, file) {
 
-		if (!(file->flags & FTRACE_EVENT_FL_ENABLED))
+		if (!(file->flags & EVENT_FILE_FL_ENABLED))
 			continue;
 
 		if (enable) {
 			tracing_start_cmdline_record();
-			set_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
+			set_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 		} else {
 			tracing_stop_cmdline_record();
-			clear_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
+			clear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 		}
 	} while_for_each_event_file();
 	mutex_unlock(&event_mutex);
@@ -337,24 +337,24 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 		if (soft_disable) {
 			if (atomic_dec_return(&file->sm_ref) > 0)
 				break;
-			disable = file->flags & FTRACE_EVENT_FL_SOFT_DISABLED;
-			clear_bit(FTRACE_EVENT_FL_SOFT_MODE_BIT, &file->flags);
+			disable = file->flags & EVENT_FILE_FL_SOFT_DISABLED;
+			clear_bit(EVENT_FILE_FL_SOFT_MODE_BIT, &file->flags);
 		} else
-			disable = !(file->flags & FTRACE_EVENT_FL_SOFT_MODE);
+			disable = !(file->flags & EVENT_FILE_FL_SOFT_MODE);
 
-		if (disable && (file->flags & FTRACE_EVENT_FL_ENABLED)) {
-			clear_bit(FTRACE_EVENT_FL_ENABLED_BIT, &file->flags);
-			if (file->flags & FTRACE_EVENT_FL_RECORDED_CMD) {
+		if (disable && (file->flags & EVENT_FILE_FL_ENABLED)) {
+			clear_bit(EVENT_FILE_FL_ENABLED_BIT, &file->flags);
+			if (file->flags & EVENT_FILE_FL_RECORDED_CMD) {
 				tracing_stop_cmdline_record();
-				clear_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
+				clear_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 			}
 			call->class->reg(call, TRACE_REG_UNREGISTER, file);
 		}
 		/* If in SOFT_MODE, just set the SOFT_DISABLE_BIT, else clear it */
-		if (file->flags & FTRACE_EVENT_FL_SOFT_MODE)
-			set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
+		if (file->flags & EVENT_FILE_FL_SOFT_MODE)
+			set_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);
 		else
-			clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
+			clear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);
 		break;
 	case 1:
 		/*
@@ -366,22 +366,22 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 		 * it still seems to be disabled.
 		 */
 		if (!soft_disable)
-			clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
+			clear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);
 		else {
 			if (atomic_inc_return(&file->sm_ref) > 1)
 				break;
-			set_bit(FTRACE_EVENT_FL_SOFT_MODE_BIT, &file->flags);
+			set_bit(EVENT_FILE_FL_SOFT_MODE_BIT, &file->flags);
 		}
 
-		if (!(file->flags & FTRACE_EVENT_FL_ENABLED)) {
+		if (!(file->flags & EVENT_FILE_FL_ENABLED)) {
 
 			/* Keep the event disabled, when going to SOFT_MODE. */
 			if (soft_disable)
-				set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
+				set_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &file->flags);
 
 			if (trace_flags & TRACE_ITER_RECORD_CMD) {
 				tracing_start_cmdline_record();
-				set_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
+				set_bit(EVENT_FILE_FL_RECORDED_CMD_BIT, &file->flags);
 			}
 			ret = call->class->reg(call, TRACE_REG_REGISTER, file);
 			if (ret) {
@@ -390,7 +390,7 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 					"%s\n", trace_event_name(call));
 				break;
 			}
-			set_bit(FTRACE_EVENT_FL_ENABLED_BIT, &file->flags);
+			set_bit(EVENT_FILE_FL_ENABLED_BIT, &file->flags);
 
 			/* WAS_ENABLED gets set but never cleared. */
 			call->flags |= TRACE_EVENT_FL_WAS_ENABLED;
@@ -716,7 +716,7 @@ s_next(struct seq_file *m, void *v, loff_t *pos)
 	(*pos)++;
 
 	list_for_each_entry_continue(file, &tr->events, list) {
-		if (file->flags & FTRACE_EVENT_FL_ENABLED)
+		if (file->flags & EVENT_FILE_FL_ENABLED)
 			return file;
 	}
 
@@ -774,12 +774,12 @@ event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	if (!file)
 		return -ENODEV;
 
-	if (flags & FTRACE_EVENT_FL_ENABLED &&
-	    !(flags & FTRACE_EVENT_FL_SOFT_DISABLED))
+	if (flags & EVENT_FILE_FL_ENABLED &&
+	    !(flags & EVENT_FILE_FL_SOFT_DISABLED))
 		strcpy(buf, "1");
 
-	if (flags & FTRACE_EVENT_FL_SOFT_DISABLED ||
-	    flags & FTRACE_EVENT_FL_SOFT_MODE)
+	if (flags & EVENT_FILE_FL_SOFT_DISABLED ||
+	    flags & EVENT_FILE_FL_SOFT_MODE)
 		strcat(buf, "*");
 
 	strcat(buf, "\n");
@@ -851,7 +851,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		 * or if all events or cleared, or if we have
 		 * a mixture.
 		 */
-		set |= (1 << !!(file->flags & FTRACE_EVENT_FL_ENABLED));
+		set |= (1 << !!(file->flags & EVENT_FILE_FL_ENABLED));
 
 		/*
 		 * If we have a mixture, no need to look further.
@@ -1932,10 +1932,10 @@ static int probe_remove_event_call(struct trace_event_call *call)
 			continue;
 		/*
 		 * We can't rely on ftrace_event_enable_disable(enable => 0)
-		 * we are going to do, FTRACE_EVENT_FL_SOFT_MODE can suppress
+		 * we are going to do, EVENT_FILE_FL_SOFT_MODE can suppress
 		 * TRACE_REG_UNREGISTER.
 		 */
-		if (file->flags & FTRACE_EVENT_FL_ENABLED)
+		if (file->flags & EVENT_FILE_FL_ENABLED)
 			return -EBUSY;
 		/*
 		 * The do_for_each_event_file_safe() is
@@ -2114,9 +2114,9 @@ event_enable_probe(unsigned long ip, unsigned long parent_ip, void **_data)
 		return;
 
 	if (data->enable)
-		clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &data->file->flags);
+		clear_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);
 	else
-		set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &data->file->flags);
+		set_bit(EVENT_FILE_FL_SOFT_DISABLED_BIT, &data->file->flags);
 }
 
 static void
@@ -2132,7 +2132,7 @@ event_enable_count_probe(unsigned long ip, unsigned long parent_ip, void **_data
 		return;
 
 	/* Skip if the event is in a state we want to switch to */
-	if (data->enable == !(data->file->flags & FTRACE_EVENT_FL_SOFT_DISABLED))
+	if (data->enable == !(data->file->flags & EVENT_FILE_FL_SOFT_DISABLED))
 		return;
 
 	if (data->count != -1)
@@ -2793,7 +2793,7 @@ static __init void event_trace_self_tests(void)
 		 * If an event is already enabled, someone is using
 		 * it and the self test should not be on.
 		 */
-		if (file->flags & FTRACE_EVENT_FL_ENABLED) {
+		if (file->flags & EVENT_FILE_FL_ENABLED) {
 			pr_warn("Enabled event during self test!\n");
 			WARN_ON_ONCE(1);
 			continue;

commit 7967b3e0c40ff72fb2cf44d3b50e2cb388ef6c67
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 14:59:40 2015 -0400

    tracing: Rename struct ftrace_subsystem_dir to trace_subsystem_dir
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structure ftrace_subsystem_dir holds
    the information about trace event subsystems. It should not be named
    ftrace, rename it to trace_subsystem_dir.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index df491ce4f3b0..58984c252aac 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -449,14 +449,14 @@ static void __get_system(struct event_subsystem *system)
 	system_refcount_inc(system);
 }
 
-static void __get_system_dir(struct ftrace_subsystem_dir *dir)
+static void __get_system_dir(struct trace_subsystem_dir *dir)
 {
 	WARN_ON_ONCE(dir->ref_count == 0);
 	dir->ref_count++;
 	__get_system(dir->subsystem);
 }
 
-static void __put_system_dir(struct ftrace_subsystem_dir *dir)
+static void __put_system_dir(struct trace_subsystem_dir *dir)
 {
 	WARN_ON_ONCE(dir->ref_count == 0);
 	/* If the subsystem is about to be freed, the dir must be too */
@@ -467,14 +467,14 @@ static void __put_system_dir(struct ftrace_subsystem_dir *dir)
 		kfree(dir);
 }
 
-static void put_system(struct ftrace_subsystem_dir *dir)
+static void put_system(struct trace_subsystem_dir *dir)
 {
 	mutex_lock(&event_mutex);
 	__put_system_dir(dir);
 	mutex_unlock(&event_mutex);
 }
 
-static void remove_subsystem(struct ftrace_subsystem_dir *dir)
+static void remove_subsystem(struct trace_subsystem_dir *dir)
 {
 	if (!dir)
 		return;
@@ -828,7 +828,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
 	const char set_to_char[4] = { '?', '0', '1', 'X' };
-	struct ftrace_subsystem_dir *dir = filp->private_data;
+	struct trace_subsystem_dir *dir = filp->private_data;
 	struct event_subsystem *system = dir->subsystem;
 	struct trace_event_call *call;
 	struct trace_event_file *file;
@@ -873,7 +873,7 @@ static ssize_t
 system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		    loff_t *ppos)
 {
-	struct ftrace_subsystem_dir *dir = filp->private_data;
+	struct trace_subsystem_dir *dir = filp->private_data;
 	struct event_subsystem *system = dir->subsystem;
 	const char *name = NULL;
 	unsigned long val;
@@ -1132,7 +1132,7 @@ static LIST_HEAD(event_subsystems);
 static int subsystem_open(struct inode *inode, struct file *filp)
 {
 	struct event_subsystem *system = NULL;
-	struct ftrace_subsystem_dir *dir = NULL; /* Initialize for gcc */
+	struct trace_subsystem_dir *dir = NULL; /* Initialize for gcc */
 	struct trace_array *tr;
 	int ret;
 
@@ -1181,7 +1181,7 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 
 static int system_tr_open(struct inode *inode, struct file *filp)
 {
-	struct ftrace_subsystem_dir *dir;
+	struct trace_subsystem_dir *dir;
 	struct trace_array *tr = inode->i_private;
 	int ret;
 
@@ -1214,7 +1214,7 @@ static int system_tr_open(struct inode *inode, struct file *filp)
 
 static int subsystem_release(struct inode *inode, struct file *file)
 {
-	struct ftrace_subsystem_dir *dir = file->private_data;
+	struct trace_subsystem_dir *dir = file->private_data;
 
 	trace_array_put(dir->tr);
 
@@ -1235,7 +1235,7 @@ static ssize_t
 subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 		      loff_t *ppos)
 {
-	struct ftrace_subsystem_dir *dir = filp->private_data;
+	struct trace_subsystem_dir *dir = filp->private_data;
 	struct event_subsystem *system = dir->subsystem;
 	struct trace_seq *s;
 	int r;
@@ -1262,7 +1262,7 @@ static ssize_t
 subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		       loff_t *ppos)
 {
-	struct ftrace_subsystem_dir *dir = filp->private_data;
+	struct trace_subsystem_dir *dir = filp->private_data;
 	char *buf;
 	int err;
 
@@ -1499,7 +1499,7 @@ static struct dentry *
 event_subsystem_dir(struct trace_array *tr, const char *name,
 		    struct trace_event_file *file, struct dentry *parent)
 {
-	struct ftrace_subsystem_dir *dir;
+	struct trace_subsystem_dir *dir;
 	struct event_subsystem *system;
 	struct dentry *entry;
 
@@ -2754,7 +2754,7 @@ static __init void event_test_stuff(void)
  */
 static __init void event_trace_self_tests(void)
 {
-	struct ftrace_subsystem_dir *dir;
+	struct trace_subsystem_dir *dir;
 	struct trace_event_file *file;
 	struct trace_event_call *call;
 	struct event_subsystem *system;

commit 687fcc4aee4567df14e31e82d6993418b826f408
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed May 13 14:20:14 2015 -0400

    tracing: Rename ftrace_event_name() to trace_event_name()
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. ftrace_event_name() returns the name of
    an event tracepoint, has nothing to do with function tracing. Rename it
    to trace_event_name().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e5638c43b04d..df491ce4f3b0 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -387,7 +387,7 @@ static int __ftrace_event_enable_disable(struct trace_event_file *file,
 			if (ret) {
 				tracing_stop_cmdline_record();
 				pr_info("event trace: Could not enable event "
-					"%s\n", ftrace_event_name(call));
+					"%s\n", trace_event_name(call));
 				break;
 			}
 			set_bit(FTRACE_EVENT_FL_ENABLED_BIT, &file->flags);
@@ -523,7 +523,7 @@ __ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,
 	list_for_each_entry(file, &tr->events, list) {
 
 		call = file->event_call;
-		name = ftrace_event_name(call);
+		name = trace_event_name(call);
 
 		if (!name || !call->class || !call->class->reg)
 			continue;
@@ -747,7 +747,7 @@ static int t_show(struct seq_file *m, void *v)
 
 	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
 		seq_printf(m, "%s:", call->class->system);
-	seq_printf(m, "%s\n", ftrace_event_name(call));
+	seq_printf(m, "%s\n", trace_event_name(call));
 
 	return 0;
 }
@@ -840,7 +840,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	mutex_lock(&event_mutex);
 	list_for_each_entry(file, &tr->events, list) {
 		call = file->event_call;
-		if (!ftrace_event_name(call) || !call->class || !call->class->reg)
+		if (!trace_event_name(call) || !call->class || !call->class->reg)
 			continue;
 
 		if (system && strcmp(call->class->system, system->name) != 0)
@@ -955,7 +955,7 @@ static int f_show(struct seq_file *m, void *v)
 
 	switch ((unsigned long)v) {
 	case FORMAT_HEADER:
-		seq_printf(m, "name: %s\n", ftrace_event_name(call));
+		seq_printf(m, "name: %s\n", trace_event_name(call));
 		seq_printf(m, "ID: %d\n", call->event.type);
 		seq_puts(m, "format:\n");
 		return 0;
@@ -1591,7 +1591,7 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 	} else
 		d_events = parent;
 
-	name = ftrace_event_name(call);
+	name = trace_event_name(call);
 	file->dir = tracefs_create_dir(name, d_events);
 	if (!file->dir) {
 		pr_warn("Could not create tracefs '%s' directory\n", name);
@@ -1683,7 +1683,7 @@ static int event_init(struct trace_event_call *call)
 	int ret = 0;
 	const char *name;
 
-	name = ftrace_event_name(call);
+	name = trace_event_name(call);
 	if (WARN_ON(!name))
 		return -EINVAL;
 
@@ -2062,7 +2062,7 @@ __trace_add_event_dirs(struct trace_array *tr)
 		ret = __trace_add_new_event(call, tr);
 		if (ret < 0)
 			pr_warn("Could not create directory for event %s\n",
-				ftrace_event_name(call));
+				trace_event_name(call));
 	}
 }
 
@@ -2076,7 +2076,7 @@ find_event_file(struct trace_array *tr, const char *system,  const char *event)
 	list_for_each_entry(file, &tr->events, list) {
 
 		call = file->event_call;
-		name = ftrace_event_name(call);
+		name = trace_event_name(call);
 
 		if (!name || !call->class || !call->class->reg)
 			continue;
@@ -2152,7 +2152,7 @@ event_enable_print(struct seq_file *m, unsigned long ip,
 	seq_printf(m, "%s:%s:%s",
 		   data->enable ? ENABLE_EVENT_STR : DISABLE_EVENT_STR,
 		   data->file->event_call->class->system,
-		   ftrace_event_name(data->file->event_call));
+		   trace_event_name(data->file->event_call));
 
 	if (data->count == -1)
 		seq_puts(m, ":unlimited\n");
@@ -2375,7 +2375,7 @@ __trace_early_add_event_dirs(struct trace_array *tr)
 		ret = event_create_dir(tr->event_dir, file);
 		if (ret < 0)
 			pr_warn("Could not create directory for event %s\n",
-				ftrace_event_name(file->event_call));
+				trace_event_name(file->event_call));
 	}
 }
 
@@ -2399,7 +2399,7 @@ __trace_early_add_events(struct trace_array *tr)
 		ret = __trace_early_add_new_event(call, tr);
 		if (ret < 0)
 			pr_warn("Could not create early event %s\n",
-				ftrace_event_name(call));
+				trace_event_name(call));
 	}
 }
 
@@ -2787,7 +2787,7 @@ static __init void event_trace_self_tests(void)
 			continue;
 #endif
 
-		pr_info("Testing event %s: ", ftrace_event_name(call));
+		pr_info("Testing event %s: ", trace_event_name(call));
 
 		/*
 		 * If an event is already enabled, someone is using

commit 3f795dcfc7364cd811c3f6f03d115fcefbbdc1ca
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 13:18:46 2015 -0400

    tracing: Rename ftrace_event_buffer to trace_event_buffer.
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The ftrace_event_buffer functions and data
    structures are for trace_events and not for function hooks. Rename them
    to trace_event_buffer*.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8df615ce3dc4..e5638c43b04d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -190,9 +190,9 @@ int trace_event_raw_init(struct trace_event_call *call)
 }
 EXPORT_SYMBOL_GPL(trace_event_raw_init);
 
-void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
-				  struct trace_event_file *trace_file,
-				  unsigned long len)
+void *trace_event_buffer_reserve(struct trace_event_buffer *fbuffer,
+				 struct trace_event_file *trace_file,
+				 unsigned long len)
 {
 	struct trace_event_call *event_call = trace_file->event_call;
 
@@ -210,11 +210,11 @@ void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
 	fbuffer->entry = ring_buffer_event_data(fbuffer->event);
 	return fbuffer->entry;
 }
-EXPORT_SYMBOL_GPL(ftrace_event_buffer_reserve);
+EXPORT_SYMBOL_GPL(trace_event_buffer_reserve);
 
 static DEFINE_SPINLOCK(tracepoint_iter_lock);
 
-static void output_printk(struct ftrace_event_buffer *fbuffer)
+static void output_printk(struct trace_event_buffer *fbuffer)
 {
 	struct trace_event_call *event_call;
 	struct trace_event *event;
@@ -241,7 +241,7 @@ static void output_printk(struct ftrace_event_buffer *fbuffer)
 	spin_unlock_irqrestore(&tracepoint_iter_lock, flags);
 }
 
-void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer)
+void trace_event_buffer_commit(struct trace_event_buffer *fbuffer)
 {
 	if (tracepoint_printk)
 		output_printk(fbuffer);
@@ -250,7 +250,7 @@ void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer)
 				    fbuffer->event, fbuffer->entry,
 				    fbuffer->flags, fbuffer->pc);
 }
-EXPORT_SYMBOL_GPL(ftrace_event_buffer_commit);
+EXPORT_SYMBOL_GPL(trace_event_buffer_commit);
 
 int trace_event_reg(struct trace_event_call *call,
 		    enum trace_reg type, void *data)

commit 2425bcb9240f8c97d793cb31c8e8d8d0a843fa29
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 11:45:27 2015 -0400

    tracing: Rename ftrace_event_{call,class} to trace_event_{call,class}
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structures ftrace_event_call and
    ftrace_event_class have nothing to do with the function hooks, and are
    really trace_event structures. Rename ftrace_event_* to trace_event_*.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 4a7cc4630ced..8df615ce3dc4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -68,7 +68,7 @@ static int system_refcount_dec(struct event_subsystem *system)
 	}
 
 static struct list_head *
-trace_get_fields(struct ftrace_event_call *event_call)
+trace_get_fields(struct trace_event_call *event_call)
 {
 	if (!event_call->class->get_fields)
 		return &event_call->class->fields;
@@ -89,7 +89,7 @@ __find_event_field(struct list_head *head, char *name)
 }
 
 struct ftrace_event_field *
-trace_find_event_field(struct ftrace_event_call *call, char *name)
+trace_find_event_field(struct trace_event_call *call, char *name)
 {
 	struct ftrace_event_field *field;
 	struct list_head *head;
@@ -129,7 +129,7 @@ static int __trace_define_field(struct list_head *head, const char *type,
 	return 0;
 }
 
-int trace_define_field(struct ftrace_event_call *call, const char *type,
+int trace_define_field(struct trace_event_call *call, const char *type,
 		       const char *name, int offset, int size, int is_signed,
 		       int filter_type)
 {
@@ -166,7 +166,7 @@ static int trace_define_common_fields(void)
 	return ret;
 }
 
-static void trace_destroy_fields(struct ftrace_event_call *call)
+static void trace_destroy_fields(struct trace_event_call *call)
 {
 	struct ftrace_event_field *field, *next;
 	struct list_head *head;
@@ -178,7 +178,7 @@ static void trace_destroy_fields(struct ftrace_event_call *call)
 	}
 }
 
-int trace_event_raw_init(struct ftrace_event_call *call)
+int trace_event_raw_init(struct trace_event_call *call)
 {
 	int id;
 
@@ -194,7 +194,7 @@ void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
 				  struct trace_event_file *trace_file,
 				  unsigned long len)
 {
-	struct ftrace_event_call *event_call = trace_file->event_call;
+	struct trace_event_call *event_call = trace_file->event_call;
 
 	local_save_flags(fbuffer->flags);
 	fbuffer->pc = preempt_count();
@@ -216,7 +216,7 @@ static DEFINE_SPINLOCK(tracepoint_iter_lock);
 
 static void output_printk(struct ftrace_event_buffer *fbuffer)
 {
-	struct ftrace_event_call *event_call;
+	struct trace_event_call *event_call;
 	struct trace_event *event;
 	unsigned long flags;
 	struct trace_iterator *iter = tracepoint_print_iter;
@@ -252,7 +252,7 @@ void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer)
 }
 EXPORT_SYMBOL_GPL(ftrace_event_buffer_commit);
 
-int trace_event_reg(struct ftrace_event_call *call,
+int trace_event_reg(struct trace_event_call *call,
 		    enum trace_reg type, void *data)
 {
 	struct trace_event_file *file = data;
@@ -315,7 +315,7 @@ void trace_event_enable_cmd_record(bool enable)
 static int __ftrace_event_enable_disable(struct trace_event_file *file,
 					 int enable, int soft_disable)
 {
-	struct ftrace_event_call *call = file->event_call;
+	struct trace_event_call *call = file->event_call;
 	int ret = 0;
 	int disable;
 
@@ -516,7 +516,7 @@ __ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,
 			      const char *sub, const char *event, int set)
 {
 	struct trace_event_file *file;
-	struct ftrace_event_call *call;
+	struct trace_event_call *call;
 	const char *name;
 	int ret = -EINVAL;
 
@@ -672,7 +672,7 @@ static void *
 t_next(struct seq_file *m, void *v, loff_t *pos)
 {
 	struct trace_event_file *file = v;
-	struct ftrace_event_call *call;
+	struct trace_event_call *call;
 	struct trace_array *tr = m->private;
 
 	(*pos)++;
@@ -743,7 +743,7 @@ static void *s_start(struct seq_file *m, loff_t *pos)
 static int t_show(struct seq_file *m, void *v)
 {
 	struct trace_event_file *file = v;
-	struct ftrace_event_call *call = file->event_call;
+	struct trace_event_call *call = file->event_call;
 
 	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
 		seq_printf(m, "%s:", call->class->system);
@@ -830,7 +830,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	const char set_to_char[4] = { '?', '0', '1', 'X' };
 	struct ftrace_subsystem_dir *dir = filp->private_data;
 	struct event_subsystem *system = dir->subsystem;
-	struct ftrace_event_call *call;
+	struct trace_event_call *call;
 	struct trace_event_file *file;
 	struct trace_array *tr = dir->tr;
 	char buf[2];
@@ -917,7 +917,7 @@ enum {
 
 static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_call *call = event_file_data(m->private);
+	struct trace_event_call *call = event_file_data(m->private);
 	struct list_head *common_head = &ftrace_common_fields;
 	struct list_head *head = trace_get_fields(call);
 	struct list_head *node = v;
@@ -949,7 +949,7 @@ static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 
 static int f_show(struct seq_file *m, void *v)
 {
-	struct ftrace_event_call *call = event_file_data(m->private);
+	struct trace_event_call *call = event_file_data(m->private);
 	struct ftrace_event_field *field;
 	const char *array_descriptor;
 
@@ -1573,7 +1573,7 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
 static int
 event_create_dir(struct dentry *parent, struct trace_event_file *file)
 {
-	struct ftrace_event_call *call = file->event_call;
+	struct trace_event_call *call = file->event_call;
 	struct trace_array *tr = file->tr;
 	struct list_head *head;
 	struct dentry *d_events;
@@ -1634,7 +1634,7 @@ event_create_dir(struct dentry *parent, struct trace_event_file *file)
 	return 0;
 }
 
-static void remove_event_from_tracers(struct ftrace_event_call *call)
+static void remove_event_from_tracers(struct trace_event_call *call)
 {
 	struct trace_event_file *file;
 	struct trace_array *tr;
@@ -1654,7 +1654,7 @@ static void remove_event_from_tracers(struct ftrace_event_call *call)
 	} while_for_each_event_file();
 }
 
-static void event_remove(struct ftrace_event_call *call)
+static void event_remove(struct trace_event_call *call)
 {
 	struct trace_array *tr;
 	struct trace_event_file *file;
@@ -1678,7 +1678,7 @@ static void event_remove(struct ftrace_event_call *call)
 	list_del(&call->list);
 }
 
-static int event_init(struct ftrace_event_call *call)
+static int event_init(struct trace_event_call *call)
 {
 	int ret = 0;
 	const char *name;
@@ -1697,7 +1697,7 @@ static int event_init(struct ftrace_event_call *call)
 }
 
 static int
-__register_event(struct ftrace_event_call *call, struct module *mod)
+__register_event(struct trace_event_call *call, struct module *mod)
 {
 	int ret;
 
@@ -1733,7 +1733,7 @@ static char *enum_replace(char *ptr, struct trace_enum_map *map, int len)
 	return ptr + elen;
 }
 
-static void update_event_printk(struct ftrace_event_call *call,
+static void update_event_printk(struct trace_event_call *call,
 				struct trace_enum_map *map)
 {
 	char *ptr;
@@ -1811,7 +1811,7 @@ static void update_event_printk(struct ftrace_event_call *call,
 
 void trace_event_enum_update(struct trace_enum_map **map, int len)
 {
-	struct ftrace_event_call *call, *p;
+	struct trace_event_call *call, *p;
 	const char *last_system = NULL;
 	int last_i;
 	int i;
@@ -1837,7 +1837,7 @@ void trace_event_enum_update(struct trace_enum_map **map, int len)
 }
 
 static struct trace_event_file *
-trace_create_new_event(struct ftrace_event_call *call,
+trace_create_new_event(struct trace_event_call *call,
 		       struct trace_array *tr)
 {
 	struct trace_event_file *file;
@@ -1858,7 +1858,7 @@ trace_create_new_event(struct ftrace_event_call *call,
 
 /* Add an event to a trace directory */
 static int
-__trace_add_new_event(struct ftrace_event_call *call, struct trace_array *tr)
+__trace_add_new_event(struct trace_event_call *call, struct trace_array *tr)
 {
 	struct trace_event_file *file;
 
@@ -1875,7 +1875,7 @@ __trace_add_new_event(struct ftrace_event_call *call, struct trace_array *tr)
  * the filesystem is initialized.
  */
 static __init int
-__trace_early_add_new_event(struct ftrace_event_call *call,
+__trace_early_add_new_event(struct trace_event_call *call,
 			    struct trace_array *tr)
 {
 	struct trace_event_file *file;
@@ -1888,10 +1888,10 @@ __trace_early_add_new_event(struct ftrace_event_call *call,
 }
 
 struct ftrace_module_file_ops;
-static void __add_event_to_tracers(struct ftrace_event_call *call);
+static void __add_event_to_tracers(struct trace_event_call *call);
 
 /* Add an additional event_call dynamically */
-int trace_add_event_call(struct ftrace_event_call *call)
+int trace_add_event_call(struct trace_event_call *call)
 {
 	int ret;
 	mutex_lock(&trace_types_lock);
@@ -1910,7 +1910,7 @@ int trace_add_event_call(struct ftrace_event_call *call)
  * Must be called under locking of trace_types_lock, event_mutex and
  * trace_event_sem.
  */
-static void __trace_remove_event_call(struct ftrace_event_call *call)
+static void __trace_remove_event_call(struct trace_event_call *call)
 {
 	event_remove(call);
 	trace_destroy_fields(call);
@@ -1918,7 +1918,7 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 	call->filter = NULL;
 }
 
-static int probe_remove_event_call(struct ftrace_event_call *call)
+static int probe_remove_event_call(struct trace_event_call *call)
 {
 	struct trace_array *tr;
 	struct trace_event_file *file;
@@ -1952,7 +1952,7 @@ static int probe_remove_event_call(struct ftrace_event_call *call)
 }
 
 /* Remove an event_call */
-int trace_remove_event_call(struct ftrace_event_call *call)
+int trace_remove_event_call(struct trace_event_call *call)
 {
 	int ret;
 
@@ -1976,7 +1976,7 @@ int trace_remove_event_call(struct ftrace_event_call *call)
 
 static void trace_module_add_events(struct module *mod)
 {
-	struct ftrace_event_call **call, **start, **end;
+	struct trace_event_call **call, **start, **end;
 
 	if (!mod->num_trace_events)
 		return;
@@ -1999,7 +1999,7 @@ static void trace_module_add_events(struct module *mod)
 
 static void trace_module_remove_events(struct module *mod)
 {
-	struct ftrace_event_call *call, *p;
+	struct trace_event_call *call, *p;
 	bool clear_trace = false;
 
 	down_write(&trace_event_sem);
@@ -2055,7 +2055,7 @@ static struct notifier_block trace_module_nb = {
 static void
 __trace_add_event_dirs(struct trace_array *tr)
 {
-	struct ftrace_event_call *call;
+	struct trace_event_call *call;
 	int ret;
 
 	list_for_each_entry(call, &ftrace_events, list) {
@@ -2070,7 +2070,7 @@ struct trace_event_file *
 find_event_file(struct trace_array *tr, const char *system,  const char *event)
 {
 	struct trace_event_file *file;
-	struct ftrace_event_call *call;
+	struct trace_event_call *call;
 	const char *name;
 
 	list_for_each_entry(file, &tr->events, list) {
@@ -2388,7 +2388,7 @@ __trace_early_add_event_dirs(struct trace_array *tr)
 static __init void
 __trace_early_add_events(struct trace_array *tr)
 {
-	struct ftrace_event_call *call;
+	struct trace_event_call *call;
 	int ret;
 
 	list_for_each_entry(call, &ftrace_events, list) {
@@ -2413,7 +2413,7 @@ __trace_remove_event_dirs(struct trace_array *tr)
 		remove_event_file_dir(file);
 }
 
-static void __add_event_to_tracers(struct ftrace_event_call *call)
+static void __add_event_to_tracers(struct trace_event_call *call)
 {
 	struct trace_array *tr;
 
@@ -2421,8 +2421,8 @@ static void __add_event_to_tracers(struct ftrace_event_call *call)
 		__trace_add_new_event(call, tr);
 }
 
-extern struct ftrace_event_call *__start_ftrace_events[];
-extern struct ftrace_event_call *__stop_ftrace_events[];
+extern struct trace_event_call *__start_ftrace_events[];
+extern struct trace_event_call *__stop_ftrace_events[];
 
 static char bootup_event_buf[COMMAND_LINE_SIZE] __initdata;
 
@@ -2593,7 +2593,7 @@ early_enable_events(struct trace_array *tr, bool disable_first)
 static __init int event_trace_enable(void)
 {
 	struct trace_array *tr = top_trace_array();
-	struct ftrace_event_call **iter, *call;
+	struct trace_event_call **iter, *call;
 	int ret;
 
 	if (!tr)
@@ -2756,7 +2756,7 @@ static __init void event_trace_self_tests(void)
 {
 	struct ftrace_subsystem_dir *dir;
 	struct trace_event_file *file;
-	struct ftrace_event_call *call;
+	struct trace_event_call *call;
 	struct event_subsystem *system;
 	struct trace_array *tr;
 	int ret;

commit 7f1d2f8210195c8c309d424a77dbf06a6d2186f4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 10:09:53 2015 -0400

    tracing: Rename ftrace_event_file to trace_event_file
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The structure ftrace_event_file is really
    about trace events and not "ftrace". Rename it to trace_event_file.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5fbb06c6c3ec..4a7cc4630ced 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -61,7 +61,7 @@ static int system_refcount_dec(struct event_subsystem *system)
 
 #define do_for_each_event_file_safe(tr, file)			\
 	list_for_each_entry(tr, &ftrace_trace_arrays, list) {	\
-		struct ftrace_event_file *___n;				\
+		struct trace_event_file *___n;				\
 		list_for_each_entry_safe(file, ___n, &tr->events, list)
 
 #define while_for_each_event_file()		\
@@ -191,17 +191,17 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 EXPORT_SYMBOL_GPL(trace_event_raw_init);
 
 void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
-				  struct ftrace_event_file *ftrace_file,
+				  struct trace_event_file *trace_file,
 				  unsigned long len)
 {
-	struct ftrace_event_call *event_call = ftrace_file->event_call;
+	struct ftrace_event_call *event_call = trace_file->event_call;
 
 	local_save_flags(fbuffer->flags);
 	fbuffer->pc = preempt_count();
-	fbuffer->ftrace_file = ftrace_file;
+	fbuffer->trace_file = trace_file;
 
 	fbuffer->event =
-		trace_event_buffer_lock_reserve(&fbuffer->buffer, ftrace_file,
+		trace_event_buffer_lock_reserve(&fbuffer->buffer, trace_file,
 						event_call->event.type, len,
 						fbuffer->flags, fbuffer->pc);
 	if (!fbuffer->event)
@@ -224,12 +224,12 @@ static void output_printk(struct ftrace_event_buffer *fbuffer)
 	if (!iter)
 		return;
 
-	event_call = fbuffer->ftrace_file->event_call;
+	event_call = fbuffer->trace_file->event_call;
 	if (!event_call || !event_call->event.funcs ||
 	    !event_call->event.funcs->trace)
 		return;
 
-	event = &fbuffer->ftrace_file->event_call->event;
+	event = &fbuffer->trace_file->event_call->event;
 
 	spin_lock_irqsave(&tracepoint_iter_lock, flags);
 	trace_seq_init(&iter->seq);
@@ -246,7 +246,7 @@ void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer)
 	if (tracepoint_printk)
 		output_printk(fbuffer);
 
-	event_trigger_unlock_commit(fbuffer->ftrace_file, fbuffer->buffer,
+	event_trigger_unlock_commit(fbuffer->trace_file, fbuffer->buffer,
 				    fbuffer->event, fbuffer->entry,
 				    fbuffer->flags, fbuffer->pc);
 }
@@ -255,7 +255,7 @@ EXPORT_SYMBOL_GPL(ftrace_event_buffer_commit);
 int trace_event_reg(struct ftrace_event_call *call,
 		    enum trace_reg type, void *data)
 {
-	struct ftrace_event_file *file = data;
+	struct trace_event_file *file = data;
 
 	WARN_ON(!(call->flags & TRACE_EVENT_FL_TRACEPOINT));
 	switch (type) {
@@ -292,7 +292,7 @@ EXPORT_SYMBOL_GPL(trace_event_reg);
 
 void trace_event_enable_cmd_record(bool enable)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct trace_array *tr;
 
 	mutex_lock(&event_mutex);
@@ -312,7 +312,7 @@ void trace_event_enable_cmd_record(bool enable)
 	mutex_unlock(&event_mutex);
 }
 
-static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
+static int __ftrace_event_enable_disable(struct trace_event_file *file,
 					 int enable, int soft_disable)
 {
 	struct ftrace_event_call *call = file->event_call;
@@ -401,13 +401,13 @@ static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
 	return ret;
 }
 
-int trace_event_enable_disable(struct ftrace_event_file *file,
+int trace_event_enable_disable(struct trace_event_file *file,
 			       int enable, int soft_disable)
 {
 	return __ftrace_event_enable_disable(file, enable, soft_disable);
 }
 
-static int ftrace_event_enable_disable(struct ftrace_event_file *file,
+static int ftrace_event_enable_disable(struct trace_event_file *file,
 				       int enable)
 {
 	return __ftrace_event_enable_disable(file, enable, 0);
@@ -415,7 +415,7 @@ static int ftrace_event_enable_disable(struct ftrace_event_file *file,
 
 static void ftrace_clear_events(struct trace_array *tr)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 
 	mutex_lock(&event_mutex);
 	list_for_each_entry(file, &tr->events, list) {
@@ -486,7 +486,7 @@ static void remove_subsystem(struct ftrace_subsystem_dir *dir)
 	}
 }
 
-static void remove_event_file_dir(struct ftrace_event_file *file)
+static void remove_event_file_dir(struct trace_event_file *file)
 {
 	struct dentry *dir = file->dir;
 	struct dentry *child;
@@ -515,7 +515,7 @@ static int
 __ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,
 			      const char *sub, const char *event, int set)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct ftrace_event_call *call;
 	const char *name;
 	int ret = -EINVAL;
@@ -671,7 +671,7 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 static void *
 t_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_file *file = v;
+	struct trace_event_file *file = v;
 	struct ftrace_event_call *call;
 	struct trace_array *tr = m->private;
 
@@ -692,13 +692,13 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct trace_array *tr = m->private;
 	loff_t l;
 
 	mutex_lock(&event_mutex);
 
-	file = list_entry(&tr->events, struct ftrace_event_file, list);
+	file = list_entry(&tr->events, struct trace_event_file, list);
 	for (l = 0; l <= *pos; ) {
 		file = t_next(m, file, &l);
 		if (!file)
@@ -710,7 +710,7 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 static void *
 s_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_file *file = v;
+	struct trace_event_file *file = v;
 	struct trace_array *tr = m->private;
 
 	(*pos)++;
@@ -725,13 +725,13 @@ s_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *s_start(struct seq_file *m, loff_t *pos)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct trace_array *tr = m->private;
 	loff_t l;
 
 	mutex_lock(&event_mutex);
 
-	file = list_entry(&tr->events, struct ftrace_event_file, list);
+	file = list_entry(&tr->events, struct trace_event_file, list);
 	for (l = 0; l <= *pos; ) {
 		file = s_next(m, file, &l);
 		if (!file)
@@ -742,7 +742,7 @@ static void *s_start(struct seq_file *m, loff_t *pos)
 
 static int t_show(struct seq_file *m, void *v)
 {
-	struct ftrace_event_file *file = v;
+	struct trace_event_file *file = v;
 	struct ftrace_event_call *call = file->event_call;
 
 	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
@@ -761,7 +761,7 @@ static ssize_t
 event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	unsigned long flags;
 	char buf[4] = "0";
 
@@ -791,7 +791,7 @@ static ssize_t
 event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	unsigned long val;
 	int ret;
 
@@ -831,7 +831,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	struct ftrace_subsystem_dir *dir = filp->private_data;
 	struct event_subsystem *system = dir->subsystem;
 	struct ftrace_event_call *call;
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct trace_array *tr = dir->tr;
 	char buf[2];
 	int set = 0;
@@ -1062,7 +1062,7 @@ static ssize_t
 event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct trace_seq *s;
 	int r = -ENODEV;
 
@@ -1095,7 +1095,7 @@ static ssize_t
 event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	char *buf;
 	int err = -ENODEV;
 
@@ -1497,7 +1497,7 @@ create_new_subsystem(const char *name)
 
 static struct dentry *
 event_subsystem_dir(struct trace_array *tr, const char *name,
-		    struct ftrace_event_file *file, struct dentry *parent)
+		    struct trace_event_file *file, struct dentry *parent)
 {
 	struct ftrace_subsystem_dir *dir;
 	struct event_subsystem *system;
@@ -1571,7 +1571,7 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
 }
 
 static int
-event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
+event_create_dir(struct dentry *parent, struct trace_event_file *file)
 {
 	struct ftrace_event_call *call = file->event_call;
 	struct trace_array *tr = file->tr;
@@ -1636,7 +1636,7 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 
 static void remove_event_from_tracers(struct ftrace_event_call *call)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct trace_array *tr;
 
 	do_for_each_event_file_safe(tr, file) {
@@ -1657,7 +1657,7 @@ static void remove_event_from_tracers(struct ftrace_event_call *call)
 static void event_remove(struct ftrace_event_call *call)
 {
 	struct trace_array *tr;
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 
 	do_for_each_event_file(tr, file) {
 		if (file->event_call != call)
@@ -1836,11 +1836,11 @@ void trace_event_enum_update(struct trace_enum_map **map, int len)
 	up_write(&trace_event_sem);
 }
 
-static struct ftrace_event_file *
+static struct trace_event_file *
 trace_create_new_event(struct ftrace_event_call *call,
 		       struct trace_array *tr)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 
 	file = kmem_cache_alloc(file_cachep, GFP_TRACE);
 	if (!file)
@@ -1860,7 +1860,7 @@ trace_create_new_event(struct ftrace_event_call *call,
 static int
 __trace_add_new_event(struct ftrace_event_call *call, struct trace_array *tr)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 
 	file = trace_create_new_event(call, tr);
 	if (!file)
@@ -1878,7 +1878,7 @@ static __init int
 __trace_early_add_new_event(struct ftrace_event_call *call,
 			    struct trace_array *tr)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 
 	file = trace_create_new_event(call, tr);
 	if (!file)
@@ -1921,7 +1921,7 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 static int probe_remove_event_call(struct ftrace_event_call *call)
 {
 	struct trace_array *tr;
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 
 #ifdef CONFIG_PERF_EVENTS
 	if (call->perf_refcount)
@@ -2066,10 +2066,10 @@ __trace_add_event_dirs(struct trace_array *tr)
 	}
 }
 
-struct ftrace_event_file *
+struct trace_event_file *
 find_event_file(struct trace_array *tr, const char *system,  const char *event)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct ftrace_event_call *call;
 	const char *name;
 
@@ -2098,7 +2098,7 @@ find_event_file(struct trace_array *tr, const char *system,  const char *event)
 #define DISABLE_EVENT_STR	"disable_event"
 
 struct event_probe_data {
-	struct ftrace_event_file	*file;
+	struct trace_event_file	*file;
 	unsigned long			count;
 	int				ref;
 	bool				enable;
@@ -2226,7 +2226,7 @@ event_enable_func(struct ftrace_hash *hash,
 		  char *glob, char *cmd, char *param, int enabled)
 {
 	struct trace_array *tr = top_trace_array();
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct ftrace_probe_ops *ops;
 	struct event_probe_data *data;
 	const char *system;
@@ -2358,7 +2358,7 @@ static inline int register_event_cmds(void) { return 0; }
 #endif /* CONFIG_DYNAMIC_FTRACE */
 
 /*
- * The top level array has already had its ftrace_event_file
+ * The top level array has already had its trace_event_file
  * descriptors created in order to allow for early events to
  * be recorded. This function is called after the tracefs has been
  * initialized, and we now have to create the files associated
@@ -2367,7 +2367,7 @@ static inline int register_event_cmds(void) { return 0; }
 static __init void
 __trace_early_add_event_dirs(struct trace_array *tr)
 {
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	int ret;
 
 
@@ -2407,7 +2407,7 @@ __trace_early_add_events(struct trace_array *tr)
 static void
 __trace_remove_event_dirs(struct trace_array *tr)
 {
-	struct ftrace_event_file *file, *next;
+	struct trace_event_file *file, *next;
 
 	list_for_each_entry_safe(file, next, &tr->events, list)
 		remove_event_file_dir(file);
@@ -2557,7 +2557,7 @@ int event_trace_del_tracer(struct trace_array *tr)
 static __init int event_trace_memsetup(void)
 {
 	field_cachep = KMEM_CACHE(ftrace_event_field, SLAB_PANIC);
-	file_cachep = KMEM_CACHE(ftrace_event_file, SLAB_PANIC);
+	file_cachep = KMEM_CACHE(trace_event_file, SLAB_PANIC);
 	return 0;
 }
 
@@ -2755,7 +2755,7 @@ static __init void event_test_stuff(void)
 static __init void event_trace_self_tests(void)
 {
 	struct ftrace_subsystem_dir *dir;
-	struct ftrace_event_file *file;
+	struct trace_event_file *file;
 	struct ftrace_event_call *call;
 	struct event_subsystem *system;
 	struct trace_array *tr;

commit 9023c930902fbbcf0cebf6110828700f792989a4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue May 5 09:39:12 2015 -0400

    tracing: Rename (un)register_ftrace_event() to (un)register_trace_event()
    
    The name "ftrace" really refers to the function hook infrastructure. It
    is not about the trace_events. The functions (un)register_ftrace_event() is
    really about trace_events, and the name should be register_trace_event()
    instead.
    
    Also renamed ftrace_event_reg() to trace_event_reg() for the same reason.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c4de47fc5cca..5fbb06c6c3ec 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -182,7 +182,7 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 {
 	int id;
 
-	id = register_ftrace_event(&call->event);
+	id = register_trace_event(&call->event);
 	if (!id)
 		return -ENODEV;
 
@@ -252,8 +252,8 @@ void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer)
 }
 EXPORT_SYMBOL_GPL(ftrace_event_buffer_commit);
 
-int ftrace_event_reg(struct ftrace_event_call *call,
-		     enum trace_reg type, void *data)
+int trace_event_reg(struct ftrace_event_call *call,
+		    enum trace_reg type, void *data)
 {
 	struct ftrace_event_file *file = data;
 
@@ -288,7 +288,7 @@ int ftrace_event_reg(struct ftrace_event_call *call,
 	}
 	return 0;
 }
-EXPORT_SYMBOL_GPL(ftrace_event_reg);
+EXPORT_SYMBOL_GPL(trace_event_reg);
 
 void trace_event_enable_cmd_record(bool enable)
 {
@@ -1673,7 +1673,7 @@ static void event_remove(struct ftrace_event_call *call)
 	} while_for_each_event_file();
 
 	if (call->event.funcs)
-		__unregister_ftrace_event(&call->event);
+		__unregister_trace_event(&call->event);
 	remove_event_from_tracers(call);
 	list_del(&call->list);
 }

commit 9ec3a646fe09970f801ab15e0f1694060b9f19af
Merge: c8b3fd0ce313 3cab989afd8d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Apr 26 15:48:49 2015 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs
    
    Pull fourth vfs update from Al Viro:
     "d_inode() annotations from David Howells (sat in for-next since before
      the beginning of merge window) + four assorted fixes"
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs:
      RCU pathwalk breakage when running into a symlink overmounting something
      fix I_DIO_WAKEUP definition
      direct-io: only inc/dec inode->i_dio_count for file systems
      fs/9p: fix readdir()
      VFS: assorted d_backing_inode() annotations
      VFS: fs/inode.c helpers: d_inode() annotations
      VFS: fs/cachefiles: d_backing_inode() annotations
      VFS: fs library helpers: d_inode() annotations
      VFS: assorted weird filesystems: d_inode() annotations
      VFS: normal filesystems (and lustre): d_inode() annotations
      VFS: security/: d_inode() annotations
      VFS: security/: d_backing_inode() annotations
      VFS: net/: d_inode() annotations
      VFS: net/unix: d_backing_inode() annotations
      VFS: kernel/: d_inode() annotations
      VFS: audit: d_backing_inode() annotations
      VFS: Fix up some ->d_inode accesses in the chelsio driver
      VFS: Cachefiles should perform fs modifications on the top layer only
      VFS: AF_UNIX sockets should call mknod on the top layer only

commit 4f2112351b4ac964b0249bdd883f7b79601f39d8
Merge: 9b60afee5042 3193899d4dd5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Apr 22 11:27:36 2015 -0700

    Merge tag 'trace-v4.1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing fixes from Steven Rostedt:
     "This adds three fixes for the tracing code.
    
      The first is a bug when ftrace_dump_on_oops is triggered in atomic
      context and function graph tracer is the tracer that is being
      reported.
    
      The second fix is bad parsing of the trace_events from the kernel
      command line, where it would ignore specific events if the system name
      is used with defining the event(it enables all events within the
      system).
    
      The last one is a fix to the TRACE_DEFINE_ENUM(), where a check was
      missing to see if the ptr was incremented to the end of the string,
      but the loop increments it again and can miss the nul delimiter to
      stop processing"
    
    * tag 'trace-v4.1-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Fix possible out of bounds memory access when parsing enums
      tracing: Fix incorrect enabling of trace events by boot cmdline
      tracing: Handle ftrace_dump() atomic context in graph_trace_open()

commit 3193899d4dd54056f8c2e0b1e40dd6e2f0009f28
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Apr 17 10:27:57 2015 -0400

    tracing: Fix possible out of bounds memory access when parsing enums
    
    The code that replaces the enum names with the enum values in the
    tracepoints' format files could possible miss the end of string nul
    character. This was caused by processing things like backslashes, quotes
    and other tokens. After processing the tokens, a check for the nul
    character needed to be done before continuing the loop, because the loop
    incremented the pointer before doing the check, which could bypass the nul
    character.
    
    Link: http://lkml.kernel.org/r/552E661D.5060502@oracle.com
    
    Reported-by: Sasha Levin <sasha.levin@oracle.com> # via KASan
    Tested-by: Andrey Ryabinin <a.ryabinin@samsung.com>
    Fixes: 0c564a538aa9 "tracing: Add TRACE_DEFINE_ENUM() macro to map enums to their values"
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 36a957c996c7..b49c107f82ac 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1760,6 +1760,8 @@ static void update_event_printk(struct ftrace_event_call *call,
 				ptr++;
 				/* Check for alpha chars like ULL */
 			} while (isalnum(*ptr));
+			if (!*ptr)
+				break;
 			/*
 			 * A number must have some kind of delimiter after
 			 * it, and we can ignore that too.
@@ -1786,12 +1788,16 @@ static void update_event_printk(struct ftrace_event_call *call,
 			do {
 				ptr++;
 			} while (isalnum(*ptr) || *ptr == '_');
+			if (!*ptr)
+				break;
 			/*
 			 * If what comes after this variable is a '.' or
 			 * '->' then we can continue to ignore that string.
 			 */
 			if (*ptr == '.' || (ptr[0] == '-' && ptr[1] == '>')) {
 				ptr += *ptr == '.' ? 1 : 2;
+				if (!*ptr)
+					break;
 				goto skip_more;
 			}
 			/*

commit 84fce9db4d7eaebd6cb2ee30c15da6d4e4daf846
Author: Joonsoo Kim <iamjoonsoo.kim@lge.com>
Date:   Thu Apr 16 13:44:44 2015 +0900

    tracing: Fix incorrect enabling of trace events by boot cmdline
    
    There is a problem that trace events are not properly enabled with
    boot cmdline. The problem is that if we pass "trace_event=kmem:mm_page_alloc"
    to the boot cmdline, it enables all kmem trace events, and not just
    the page_alloc event.
    
    This is caused by the parsing mechanism. When we parse the cmdline, the buffer
    contents is modified due to tokenization. And, if we use this buffer
    again, we will get the wrong result.
    
    Unfortunately, this buffer is be accessed three times to set trace events
    properly at boot time. So, we need to handle this situation.
    
    There is already code handling ",", but we need another for ":".
    This patch adds it.
    
    Link: http://lkml.kernel.org/r/1429159484-22977-1-git-send-email-iamjoonsoo.kim@lge.com
    
    Cc: stable@vger.kernel.org # 3.19+
    Signed-off-by: Joonsoo Kim <iamjoonsoo.kim@lge.com>
    [ added missing return ret; ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a576bbe75577..36a957c996c7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -565,6 +565,7 @@ static int __ftrace_set_clr_event(struct trace_array *tr, const char *match,
 static int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
 {
 	char *event = NULL, *sub = NULL, *match;
+	int ret;
 
 	/*
 	 * The buf format can be <subsystem>:<event-name>
@@ -590,7 +591,13 @@ static int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
 			event = NULL;
 	}
 
-	return __ftrace_set_clr_event(tr, match, sub, event, set);
+	ret = __ftrace_set_clr_event(tr, match, sub, event, set);
+
+	/* Put back the colon to allow this to be called again */
+	if (buf)
+		*(buf - 1) = ':';
+
+	return ret;
 }
 
 /**

commit 7682c918439d42291df0d76b3e06627f27fbcdef
Author: David Howells <dhowells@redhat.com>
Date:   Tue Mar 17 22:26:16 2015 +0000

    VFS: kernel/: d_inode() annotations
    
    relayfs and tracefs are dealing with inodes of their own;
    those two act as filesystem drivers
    
    Signed-off-by: David Howells <dhowells@redhat.com>
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index db54dda10ccc..e19e7578a0d3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -494,8 +494,8 @@ static void remove_event_file_dir(struct ftrace_event_file *file)
 	if (dir) {
 		spin_lock(&dir->d_lock);	/* probably unneeded */
 		list_for_each_entry(child, &dir->d_subdirs, d_child) {
-			if (child->d_inode)	/* probably unneeded */
-				child->d_inode->i_private = NULL;
+			if (d_really_is_positive(child))	/* probably unneeded */
+				d_inode(child)->i_private = NULL;
 		}
 		spin_unlock(&dir->d_lock);
 

commit eeee78cf77df0450ca285a7cd6d73842181e825c
Merge: 3f3c73de77b5 9828413d4715
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Apr 14 10:49:03 2015 -0700

    Merge tag 'trace-v4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Some clean ups and small fixes, but the biggest change is the addition
      of the TRACE_DEFINE_ENUM() macro that can be used by tracepoints.
    
      Tracepoints have helper functions for the TP_printk() called
      __print_symbolic() and __print_flags() that lets a numeric number be
      displayed as a a human comprehensible text.  What is placed in the
      TP_printk() is also shown in the tracepoint format file such that user
      space tools like perf and trace-cmd can parse the binary data and
      express the values too.  Unfortunately, the way the TRACE_EVENT()
      macro works, anything placed in the TP_printk() will be shown pretty
      much exactly as is.  The problem arises when enums are used.  That's
      because unlike macros, enums will not be changed into their values by
      the C pre-processor.  Thus, the enum string is exported to the format
      file, and this makes it useless for user space tools.
    
      The TRACE_DEFINE_ENUM() solves this by converting the enum strings in
      the TP_printk() format into their number, and that is what is shown to
      user space.  For example, the tracepoint tlb_flush currently has this
      in its format file:
    
         __print_symbolic(REC->reason,
            { TLB_FLUSH_ON_TASK_SWITCH, "flush on task switch" },
            { TLB_REMOTE_SHOOTDOWN, "remote shootdown" },
            { TLB_LOCAL_SHOOTDOWN, "local shootdown" },
            { TLB_LOCAL_MM_SHOOTDOWN, "local mm shootdown" })
    
      After adding:
    
         TRACE_DEFINE_ENUM(TLB_FLUSH_ON_TASK_SWITCH);
         TRACE_DEFINE_ENUM(TLB_REMOTE_SHOOTDOWN);
         TRACE_DEFINE_ENUM(TLB_LOCAL_SHOOTDOWN);
         TRACE_DEFINE_ENUM(TLB_LOCAL_MM_SHOOTDOWN);
    
      Its format file will contain this:
    
         __print_symbolic(REC->reason,
            { 0, "flush on task switch" },
            { 1, "remote shootdown" },
            { 2, "local shootdown" },
            { 3, "local mm shootdown" })"
    
    * tag 'trace-v4.1' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (27 commits)
      tracing: Add enum_map file to show enums that have been mapped
      writeback: Export enums used by tracepoint to user space
      v4l: Export enums used by tracepoints to user space
      SUNRPC: Export enums in tracepoints to user space
      mm: tracing: Export enums in tracepoints to user space
      irq/tracing: Export enums in tracepoints to user space
      f2fs: Export the enums in the tracepoints to userspace
      net/9p/tracing: Export enums in tracepoints to userspace
      x86/tlb/trace: Export enums in used by tlb_flush tracepoint
      tracing/samples: Update the trace-event-sample.h with TRACE_DEFINE_ENUM()
      tracing: Allow for modules to convert their enums to values
      tracing: Add TRACE_DEFINE_ENUM() macro to map enums to their values
      tracing: Update trace-event-sample with TRACE_SYSTEM_VAR documentation
      tracing: Give system name a pointer
      brcmsmac: Move each system tracepoints to their own header
      iwlwifi: Move each system tracepoints to their own header
      mac80211: Move message tracepoints to their own header
      tracing: Add TRACE_SYSTEM_VAR to xhci-hcd
      tracing: Add TRACE_SYSTEM_VAR to kvm-s390
      tracing: Add TRACE_SYSTEM_VAR to intel-sst
      ...

commit 3673b8e4ce7237160fa31ee8d7e94a4d5a9976a1
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Mar 25 15:44:21 2015 -0400

    tracing: Allow for modules to convert their enums to values
    
    Update the infrastructure such that modules that declare TRACE_DEFINE_ENUM()
    will have those enums converted into their values in the tracepoint
    print fmt strings.
    
    Link: http://lkml.kernel.org/r/87vbhjp74q.fsf@rustcorp.com.au
    
    Acked-by: Rusty Russell <rusty@rustcorp.com.au>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fc58c50fbf01..a576bbe75577 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2034,7 +2034,7 @@ static int trace_module_notify(struct notifier_block *self,
 
 static struct notifier_block trace_module_nb = {
 	.notifier_call = trace_module_notify,
-	.priority = 0,
+	.priority = 1, /* higher than trace.c module notify */
 };
 #endif /* CONFIG_MODULES */
 

commit 0c564a538aa934ad15b2145aaf8b64f3feb0be63
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Mar 24 17:58:09 2015 -0400

    tracing: Add TRACE_DEFINE_ENUM() macro to map enums to their values
    
    Several tracepoints use the helper functions __print_symbolic() or
    __print_flags() and pass in enums that do the mapping between the
    binary data stored and the value to print. This works well for reading
    the ASCII trace files, but when the data is read via userspace tools
    such as perf and trace-cmd, the conversion of the binary value to a
    human string format is lost if an enum is used, as userspace does not
    have access to what the ENUM is.
    
    For example, the tracepoint trace_tlb_flush() has:
    
     __print_symbolic(REC->reason,
        { TLB_FLUSH_ON_TASK_SWITCH, "flush on task switch" },
        { TLB_REMOTE_SHOOTDOWN, "remote shootdown" },
        { TLB_LOCAL_SHOOTDOWN, "local shootdown" },
        { TLB_LOCAL_MM_SHOOTDOWN, "local mm shootdown" })
    
    Which maps the enum values to the strings they represent. But perf and
    trace-cmd do no know what value TLB_LOCAL_MM_SHOOTDOWN is, and would
    not be able to map it.
    
    With TRACE_DEFINE_ENUM(), developers can place these in the event header
    files and ftrace will convert the enums to their values:
    
    By adding:
    
     TRACE_DEFINE_ENUM(TLB_FLUSH_ON_TASK_SWITCH);
     TRACE_DEFINE_ENUM(TLB_REMOTE_SHOOTDOWN);
     TRACE_DEFINE_ENUM(TLB_LOCAL_SHOOTDOWN);
     TRACE_DEFINE_ENUM(TLB_LOCAL_MM_SHOOTDOWN);
    
     $ cat /sys/kernel/debug/tracing/events/tlb/tlb_flush/format
    [...]
     __print_symbolic(REC->reason,
        { 0, "flush on task switch" },
        { 1, "remote shootdown" },
        { 2, "local shootdown" },
        { 3, "local mm shootdown" })
    
    The above is what userspace expects to see, and tools do not need to
    be modified to parse them.
    
    Link: http://lkml.kernel.org/r/20150403013802.220157513@goodmis.org
    
    Cc: Guilherme Cox <cox@computer.org>
    Cc: Tony Luck <tony.luck@gmail.com>
    Cc: Xie XiuQi <xiexiuqi@huawei.com>
    Acked-by: Namhyung Kim <namhyung@kernel.org>
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index db54dda10ccc..fc58c50fbf01 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1704,6 +1704,125 @@ __register_event(struct ftrace_event_call *call, struct module *mod)
 	return 0;
 }
 
+static char *enum_replace(char *ptr, struct trace_enum_map *map, int len)
+{
+	int rlen;
+	int elen;
+
+	/* Find the length of the enum value as a string */
+	elen = snprintf(ptr, 0, "%ld", map->enum_value);
+	/* Make sure there's enough room to replace the string with the value */
+	if (len < elen)
+		return NULL;
+
+	snprintf(ptr, elen + 1, "%ld", map->enum_value);
+
+	/* Get the rest of the string of ptr */
+	rlen = strlen(ptr + len);
+	memmove(ptr + elen, ptr + len, rlen);
+	/* Make sure we end the new string */
+	ptr[elen + rlen] = 0;
+
+	return ptr + elen;
+}
+
+static void update_event_printk(struct ftrace_event_call *call,
+				struct trace_enum_map *map)
+{
+	char *ptr;
+	int quote = 0;
+	int len = strlen(map->enum_string);
+
+	for (ptr = call->print_fmt; *ptr; ptr++) {
+		if (*ptr == '\\') {
+			ptr++;
+			/* paranoid */
+			if (!*ptr)
+				break;
+			continue;
+		}
+		if (*ptr == '"') {
+			quote ^= 1;
+			continue;
+		}
+		if (quote)
+			continue;
+		if (isdigit(*ptr)) {
+			/* skip numbers */
+			do {
+				ptr++;
+				/* Check for alpha chars like ULL */
+			} while (isalnum(*ptr));
+			/*
+			 * A number must have some kind of delimiter after
+			 * it, and we can ignore that too.
+			 */
+			continue;
+		}
+		if (isalpha(*ptr) || *ptr == '_') {
+			if (strncmp(map->enum_string, ptr, len) == 0 &&
+			    !isalnum(ptr[len]) && ptr[len] != '_') {
+				ptr = enum_replace(ptr, map, len);
+				/* Hmm, enum string smaller than value */
+				if (WARN_ON_ONCE(!ptr))
+					return;
+				/*
+				 * No need to decrement here, as enum_replace()
+				 * returns the pointer to the character passed
+				 * the enum, and two enums can not be placed
+				 * back to back without something in between.
+				 * We can skip that something in between.
+				 */
+				continue;
+			}
+		skip_more:
+			do {
+				ptr++;
+			} while (isalnum(*ptr) || *ptr == '_');
+			/*
+			 * If what comes after this variable is a '.' or
+			 * '->' then we can continue to ignore that string.
+			 */
+			if (*ptr == '.' || (ptr[0] == '-' && ptr[1] == '>')) {
+				ptr += *ptr == '.' ? 1 : 2;
+				goto skip_more;
+			}
+			/*
+			 * Once again, we can skip the delimiter that came
+			 * after the string.
+			 */
+			continue;
+		}
+	}
+}
+
+void trace_event_enum_update(struct trace_enum_map **map, int len)
+{
+	struct ftrace_event_call *call, *p;
+	const char *last_system = NULL;
+	int last_i;
+	int i;
+
+	down_write(&trace_event_sem);
+	list_for_each_entry_safe(call, p, &ftrace_events, list) {
+		/* events are usually grouped together with systems */
+		if (!last_system || call->class->system != last_system) {
+			last_i = 0;
+			last_system = call->class->system;
+		}
+
+		for (i = last_i; i < len; i++) {
+			if (call->class->system == map[i]->system) {
+				/* Save the first system if need be */
+				if (!last_i)
+					last_i = i;
+				update_event_printk(call, map[i]);
+			}
+		}
+	}
+	up_write(&trace_event_sem);
+}
+
 static struct ftrace_event_file *
 trace_create_new_event(struct ftrace_event_call *call,
 		       struct trace_array *tr)

commit 41cbc01f6e49e48bc3d78158cec0a2d4ff6c906d
Merge: 12df4289ee8e 1e0d6714aceb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 12 08:37:41 2015 -0800

    Merge tag 'trace-v3.20' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "The updates included in this pull request for ftrace are:
    
       o Several clean ups to the code
    
         One such clean up was to convert to 64 bit time keeping, in the
         ring buffer benchmark code.
    
       o Adding of __print_array() helper macro for TRACE_EVENT()
    
       o Updating the sample/trace_events/ to add samples of different ways
         to make trace events.  Lots of features have been added since the
         sample code was made, and these features are mostly unknown.
         Developers have been making their own hacks to do things that are
         already available.
    
       o Performance improvements.  Most notably, I found a performance bug
         where a waiter that is waiting for a full page from the ring buffer
         will see that a full page is not available, and go to sleep.  The
         sched event caused by it going to sleep would cause it to wake up
         again.  It would see that there was still not a full page, and go
         back to sleep again, and that would wake it up again, until finally
         it would see a full page.  This change has been marked for stable.
    
      Other improvements include removing global locks from fast paths"
    
    * tag 'trace-v3.20' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      ring-buffer: Do not wake up a splice waiter when page is not full
      tracing: Fix unmapping loop in tracing_mark_write
      tracing: Add samples of DECLARE_EVENT_CLASS() and DEFINE_EVENT()
      tracing: Add TRACE_EVENT_FN example
      tracing: Add TRACE_EVENT_CONDITION sample
      tracing: Update the TRACE_EVENT fields available in the sample code
      tracing: Separate out initializing top level dir from instances
      tracing: Make tracing_init_dentry_tr() static
      trace: Use 64-bit timekeeping
      tracing: Add array printing helper
      tracing: Remove newline from trace_printk warning banner
      tracing: Use IS_ERR() check for return value of tracing_init_dentry()
      tracing: Remove unneeded includes of debugfs.h and fs.h
      tracing: Remove taking of trace_types_lock in pipe files
      tracing: Add ref count to tracer for when they are being read by pipe

commit 8434dc9340cd2e117fc944cf7526263bf490a52a
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 20 12:13:40 2015 -0500

    tracing: Convert the tracing facility over to use tracefs
    
    debugfs was fine for the tracing facility as a quick way to get
    an interface. Now that tracing has matured, it should separate itself
    from debugfs such that it can be mounted separately without needing
    to mount all of debugfs with it. That is, users resist using tracing
    because it requires mounting debugfs. Having tracing have its own file
    system lets users get the features of tracing without needing to bring
    in the rest of the kernel's debug infrastructure.
    
    Another reason for tracefs is that debubfs does not support mkdir.
    Currently, to create instances, one does a mkdir in the tracing/instance
    directory. This is implemented via a hack that forces debugfs to do
    something it is not intended on doing. By converting over to tracefs, this
    hack can be removed and mkdir can be properly implemented. This patch does
    not address this yet, but it lays the ground work for that to be done.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index db54dda10ccc..0d2e47370ee7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -13,7 +13,7 @@
 #include <linux/workqueue.h>
 #include <linux/spinlock.h>
 #include <linux/kthread.h>
-#include <linux/debugfs.h>
+#include <linux/tracefs.h>
 #include <linux/uaccess.h>
 #include <linux/module.h>
 #include <linux/ctype.h>
@@ -480,7 +480,7 @@ static void remove_subsystem(struct ftrace_subsystem_dir *dir)
 		return;
 
 	if (!--dir->nr_events) {
-		debugfs_remove_recursive(dir->entry);
+		tracefs_remove_recursive(dir->entry);
 		list_del(&dir->list);
 		__put_system_dir(dir);
 	}
@@ -499,7 +499,7 @@ static void remove_event_file_dir(struct ftrace_event_file *file)
 		}
 		spin_unlock(&dir->d_lock);
 
-		debugfs_remove_recursive(dir);
+		tracefs_remove_recursive(dir);
 	}
 
 	list_del(&file->list);
@@ -1526,7 +1526,7 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
 	} else
 		__get_system(system);
 
-	dir->entry = debugfs_create_dir(name, parent);
+	dir->entry = tracefs_create_dir(name, parent);
 	if (!dir->entry) {
 		pr_warn("Failed to create system directory %s\n", name);
 		__put_system(system);
@@ -1539,12 +1539,12 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
 	dir->subsystem = system;
 	file->system = dir;
 
-	entry = debugfs_create_file("filter", 0644, dir->entry, dir,
+	entry = tracefs_create_file("filter", 0644, dir->entry, dir,
 				    &ftrace_subsystem_filter_fops);
 	if (!entry) {
 		kfree(system->filter);
 		system->filter = NULL;
-		pr_warn("Could not create debugfs '%s/filter' entry\n", name);
+		pr_warn("Could not create tracefs '%s/filter' entry\n", name);
 	}
 
 	trace_create_file("enable", 0644, dir->entry, dir,
@@ -1585,9 +1585,9 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 		d_events = parent;
 
 	name = ftrace_event_name(call);
-	file->dir = debugfs_create_dir(name, d_events);
+	file->dir = tracefs_create_dir(name, d_events);
 	if (!file->dir) {
-		pr_warn("Could not create debugfs '%s' directory\n", name);
+		pr_warn("Could not create tracefs '%s' directory\n", name);
 		return -1;
 	}
 
@@ -2228,7 +2228,7 @@ static inline int register_event_cmds(void) { return 0; }
 /*
  * The top level array has already had its ftrace_event_file
  * descriptors created in order to allow for early events to
- * be recorded. This function is called after the debugfs has been
+ * be recorded. This function is called after the tracefs has been
  * initialized, and we now have to create the files associated
  * to the events.
  */
@@ -2311,16 +2311,16 @@ create_event_toplevel_files(struct dentry *parent, struct trace_array *tr)
 	struct dentry *d_events;
 	struct dentry *entry;
 
-	entry = debugfs_create_file("set_event", 0644, parent,
+	entry = tracefs_create_file("set_event", 0644, parent,
 				    tr, &ftrace_set_event_fops);
 	if (!entry) {
-		pr_warn("Could not create debugfs 'set_event' entry\n");
+		pr_warn("Could not create tracefs 'set_event' entry\n");
 		return -ENOMEM;
 	}
 
-	d_events = debugfs_create_dir("events", parent);
+	d_events = tracefs_create_dir("events", parent);
 	if (!d_events) {
-		pr_warn("Could not create debugfs 'events' directory\n");
+		pr_warn("Could not create tracefs 'events' directory\n");
 		return -ENOMEM;
 	}
 
@@ -2412,7 +2412,7 @@ int event_trace_del_tracer(struct trace_array *tr)
 
 	down_write(&trace_event_sem);
 	__trace_remove_event_dirs(tr);
-	debugfs_remove_recursive(tr->event_dir);
+	tracefs_remove_recursive(tr->event_dir);
 	up_write(&trace_event_sem);
 
 	tr->event_dir = NULL;
@@ -2534,10 +2534,10 @@ static __init int event_trace_init(void)
 	if (IS_ERR(d_tracer))
 		return 0;
 
-	entry = debugfs_create_file("available_events", 0444, d_tracer,
+	entry = tracefs_create_file("available_events", 0444, d_tracer,
 				    tr, &ftrace_avail_fops);
 	if (!entry)
-		pr_warn("Could not create debugfs 'available_events' entry\n");
+		pr_warn("Could not create tracefs 'available_events' entry\n");
 
 	if (trace_define_common_fields())
 		pr_warn("tracing: Failed to allocate common fields");

commit dfbc1534eac550b37df592a8a0b550cd47539d0b
Merge: 7eeafbcab47f 163f9eb95a10
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Mon Feb 2 11:47:31 2015 -0500

    Merge branch 'debugfs_automount' of git://git.kernel.org/pub/scm/linux/kernel/git/viro/vfs into trace/ftrace/tracefs
    
    Pull in Al Viro's changes to debugfs that implement the new primitive:
    debugfs_create_automount(), that creates a directory in debugfs that will
    safely mount another file system automatically when debugfs is mounted.
    
    This will let tracefs automount itself on top of debugfs/tracing directory.

commit 14a5ae40f0def33a422a45b2ed09198adb7bf11c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jan 20 11:14:16 2015 -0500

    tracing: Use IS_ERR() check for return value of tracing_init_dentry()
    
    tracing_init_dentry() will soon return NULL as a valid pointer for the
    top level tracing directroy. NULL can not be used as an error value.
    Instead, switch to ERR_PTR() and check the return status with
    IS_ERR().
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 366a78a3e61e..4ff8c1394017 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2490,7 +2490,7 @@ static __init int event_trace_init(void)
 		return -ENODEV;
 
 	d_tracer = tracing_init_dentry();
-	if (!d_tracer)
+	if (IS_ERR(d_tracer))
 		return 0;
 
 	entry = debugfs_create_file("available_events", 0444, d_tracer,

commit ce1039bd3a89e99e4f624e75fb1777fc92d76eb3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jan 14 12:53:45 2015 -0500

    tracing: Fix enabling of syscall events on the command line
    
    Commit 5f893b2639b2 "tracing: Move enabling tracepoints to just after
    rcu_init()" broke the enabling of system call events from the command
    line. The reason was that the enabling of command line trace events
    was moved before PID 1 started, and the syscall tracepoints require
    that all tasks have the TIF_SYSCALL_TRACEPOINT flag set. But the
    swapper task (pid 0) is not part of that. Since the swapper task is the
    only task that is running at this early in boot, no task gets the
    flag set, and the tracepoint never gets reached.
    
    Instead of setting the swapper task flag (there should be no reason to
    do that), re-enabled trace events again after the init thread (PID 1)
    has been started. It requires disabling all command line events and
    re-enabling them, as just enabling them again will not reset the logic
    to set the TIF_SYSCALL_TRACEPOINT flag, as the syscall tracepoint will
    be fooled into thinking that it was already set, and wont try setting
    it again. For this reason, we must first disable it and re-enable it.
    
    Link: http://lkml.kernel.org/r/1421188517-18312-1-git-send-email-mpe@ellerman.id.au
    Link: http://lkml.kernel.org/r/20150115040506.216066449@goodmis.org
    
    Reported-by: Michael Ellerman <mpe@ellerman.id.au>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 366a78a3e61e..b03a0ea77b99 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2429,12 +2429,39 @@ static __init int event_trace_memsetup(void)
 	return 0;
 }
 
+static __init void
+early_enable_events(struct trace_array *tr, bool disable_first)
+{
+	char *buf = bootup_event_buf;
+	char *token;
+	int ret;
+
+	while (true) {
+		token = strsep(&buf, ",");
+
+		if (!token)
+			break;
+		if (!*token)
+			continue;
+
+		/* Restarting syscalls requires that we stop them first */
+		if (disable_first)
+			ftrace_set_clr_event(tr, token, 0);
+
+		ret = ftrace_set_clr_event(tr, token, 1);
+		if (ret)
+			pr_warn("Failed to enable trace event: %s\n", token);
+
+		/* Put back the comma to allow this to be called again */
+		if (buf)
+			*(buf - 1) = ',';
+	}
+}
+
 static __init int event_trace_enable(void)
 {
 	struct trace_array *tr = top_trace_array();
 	struct ftrace_event_call **iter, *call;
-	char *buf = bootup_event_buf;
-	char *token;
 	int ret;
 
 	if (!tr)
@@ -2456,18 +2483,7 @@ static __init int event_trace_enable(void)
 	 */
 	__trace_early_add_events(tr);
 
-	while (true) {
-		token = strsep(&buf, ",");
-
-		if (!token)
-			break;
-		if (!*token)
-			continue;
-
-		ret = ftrace_set_clr_event(tr, token, 1);
-		if (ret)
-			pr_warn("Failed to enable trace event: %s\n", token);
-	}
+	early_enable_events(tr, false);
 
 	trace_printk_start_comm();
 
@@ -2478,6 +2494,31 @@ static __init int event_trace_enable(void)
 	return 0;
 }
 
+/*
+ * event_trace_enable() is called from trace_event_init() first to
+ * initialize events and perhaps start any events that are on the
+ * command line. Unfortunately, there are some events that will not
+ * start this early, like the system call tracepoints that need
+ * to set the TIF_SYSCALL_TRACEPOINT flag of pid 1. But event_trace_enable()
+ * is called before pid 1 starts, and this flag is never set, making
+ * the syscall tracepoint never get reached, but the event is enabled
+ * regardless (and not doing anything).
+ */
+static __init int event_trace_enable_again(void)
+{
+	struct trace_array *tr;
+
+	tr = top_trace_array();
+	if (!tr)
+		return -ENODEV;
+
+	early_enable_events(tr, true);
+
+	return 0;
+}
+
+early_initcall(event_trace_enable_again);
+
 static __init int event_trace_init(void)
 {
 	struct trace_array *tr;

commit a7c180aa7e76a55642e8492f28353303a50292c2
Merge: 2dbfca5a1819 0daa2302968c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Dec 16 12:53:59 2014 -0800

    Merge tag 'trace-3.19-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "As the merge window is still open, and this code was not as complex as
      I thought it might be.  I'm pushing this in now.
    
      This will allow Thomas to debug his irq work for 3.20.
    
      This adds two new features:
    
      1) Allow traceopoints to be enabled right after mm_init().
    
         By passing in the trace_event= kernel command line parameter,
         tracepoints can be enabled at boot up.  For debugging things like
         the initialization of interrupts, it is needed to have tracepoints
         enabled very early.  People have asked about this before and this
         has been on my todo list.  As it can be helpful for Thomas to debug
         his upcoming 3.20 IRQ work, I'm pushing this now.  This way he can
         add tracepoints into the IRQ set up and have users enable them when
         things go wrong.
    
      2) Have the tracepoints printed via printk() (the console) when they
         are triggered.
    
         If the irq code locks up or reboots the box, having the tracepoint
         output go into the kernel ring buffer is useless for debugging.
         But being able to add the tp_printk kernel command line option
         along with the trace_event= option will have these tracepoints
         printed as they occur, and that can be really useful for debugging
         early lock up or reboot problems.
    
      This code is not that intrusive and it passed all my tests.  Thomas
      tried them out too and it works for his needs.
    
       Link: http://lkml.kernel.org/r/20141214201609.126831471@goodmis.org"
    
    * tag 'trace-3.19-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Add tp_printk cmdline to have tracepoints go to printk()
      tracing: Move enabling tracepoints to just after rcu_init()

commit 0daa2302968c13b657118d6ac92471f8fd2f3f28
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 12 22:27:10 2014 -0500

    tracing: Add tp_printk cmdline to have tracepoints go to printk()
    
    Add the kernel command line tp_printk option that will have tracepoints
    that are active sent to printk() as well as to the trace buffer.
    
    Passing "tp_printk" will activate this. To turn it off, the sysctl
    /proc/sys/kernel/tracepoint_printk can have '0' echoed into it. Note,
    this only works if the cmdline option is used. Echoing 1 into the sysctl
    file without the cmdline option will have no affect.
    
    Note, this is a dangerous option. Having high frequency tracepoints send
    their data to printk() can possibly cause a live lock. This is another
    reason why this is only active if the command line option is used.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1412121539300.16494@nanos
    
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fd9deb0e03f0..9f7175a3df71 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -212,8 +212,40 @@ void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
 }
 EXPORT_SYMBOL_GPL(ftrace_event_buffer_reserve);
 
+static DEFINE_SPINLOCK(tracepoint_iter_lock);
+
+static void output_printk(struct ftrace_event_buffer *fbuffer)
+{
+	struct ftrace_event_call *event_call;
+	struct trace_event *event;
+	unsigned long flags;
+	struct trace_iterator *iter = tracepoint_print_iter;
+
+	if (!iter)
+		return;
+
+	event_call = fbuffer->ftrace_file->event_call;
+	if (!event_call || !event_call->event.funcs ||
+	    !event_call->event.funcs->trace)
+		return;
+
+	event = &fbuffer->ftrace_file->event_call->event;
+
+	spin_lock_irqsave(&tracepoint_iter_lock, flags);
+	trace_seq_init(&iter->seq);
+	iter->ent = fbuffer->entry;
+	event_call->event.funcs->trace(iter, 0, event);
+	trace_seq_putc(&iter->seq, 0);
+	printk("%s", iter->seq.buffer);
+
+	spin_unlock_irqrestore(&tracepoint_iter_lock, flags);
+}
+
 void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer)
 {
+	if (tracepoint_printk)
+		output_printk(fbuffer);
+
 	event_trigger_unlock_commit(fbuffer->ftrace_file, fbuffer->buffer,
 				    fbuffer->event, fbuffer->entry,
 				    fbuffer->flags, fbuffer->pc);

commit 5f893b2639b21ffe6834b1aebba392c37d2b83f9
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 12 20:05:10 2014 -0500

    tracing: Move enabling tracepoints to just after rcu_init()
    
    Enabling tracepoints at boot up can be very useful. The tracepoint
    can be initialized right after RCU has been. There's no need to
    wait for the early_initcall() to be called. That's too late for some
    things that can use tracepoints for debugging. Move the logic to
    enable tracepoints out of the initcalls and into init/main.c to
    right after rcu_init().
    
    This also allows trace_printk() to be used early too.
    
    Link: http://lkml.kernel.org/r/alpine.DEB.2.11.1412121539300.16494@nanos
    Link: http://lkml.kernel.org/r/20141214164104.307127356@goodmis.org
    
    Reviewed-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Suggested-by: Thomas Gleixner <tglx@linutronix.de>
    Tested-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f9d0cbe014b7..fd9deb0e03f0 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2477,8 +2477,14 @@ static __init int event_trace_init(void)
 #endif
 	return 0;
 }
-early_initcall(event_trace_memsetup);
-core_initcall(event_trace_enable);
+
+void __init trace_event_init(void)
+{
+	event_trace_memsetup();
+	init_ftrace_syscalls();
+	event_trace_enable();
+}
+
 fs_initcall(event_trace_init);
 
 #ifdef CONFIG_FTRACE_STARTUP_TEST

commit 350e4f4985472e29091b899bc227d75d2a66fb4c
Merge: c32809521de5 db0865543739
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 20:35:41 2014 -0800

    Merge tag 'trace-seq-buf-3.19' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull nmi-safe seq_buf printk update from Steven Rostedt:
     "This code is a fork from the trace-3.19 pull as it needed the
      trace_seq clean ups from that branch.
    
      This code solves the issue of performing stack dumps from NMI context.
      The issue is that printk() is not safe from NMI context as if the NMI
      were to trigger when a printk() was being performed, the NMI could
      deadlock from the printk() internal locks.  This has been seen in
      practice.
    
      With lots of review from Petr Mladek, this code went through several
      iterations, and we feel that it is now at a point of quality to be
      accepted into mainline.
    
      Here's what is contained in this patch set:
    
       - Creates a "seq_buf" generic buffer utility that allows a descriptor
         to be passed around where functions can write their own "printk()"
         formatted strings into it.  The generic version was pulled out of
         the trace_seq() code that was made specifically for tracing.
    
       - The seq_buf code was change to model the seq_file code.  I have a
         patch (not included for 3.19) that converts the seq_file.c code
         over to use seq_buf.c like the trace_seq.c code does.  This was
         done to make sure that seq_buf.c is compatible with seq_file.c.  I
         may try to get that patch in for 3.20.
    
       - The seq_buf.c file was moved to lib/ to remove it from being
         dependent on CONFIG_TRACING.
    
       - The printk() was updated to allow for a per_cpu "override" of the
         internal calls.  That is, instead of writing to the console, a call
         to printk() may do something else.  This made it easier to allow
         the NMI to change what printk() does in order to call dump_stack()
         without needing to update that code as well.
    
       - Finally, the dump_stack from all CPUs via NMI code was converted to
         use the seq_buf code.  The caller to trigger the NMI code would
         wait till all the NMIs finished, and then it would print the
         seq_buf data to the console safely from a non NMI context
    
      One added bonus is that this code also makes the NMI dump stack work
      on PREEMPT_RT kernels.  As printk() includes sleeping locks on
      PREEMPT_RT, printk() only writes to console if the console does not
      use any rt_mutex converted spin locks.  Which a lot do"
    
    * tag 'trace-seq-buf-3.19' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      x86/nmi: Fix use of unallocated cpumask_var_t
      printk/percpu: Define printk_func when printk is not defined
      x86/nmi: Perform a safe NMI stack trace on all CPUs
      printk: Add per_cpu printk func to allow printk to be diverted
      seq_buf: Move the seq_buf code to lib/
      seq-buf: Make seq_buf_bprintf() conditional on CONFIG_BINARY_PRINTF
      tracing: Add seq_buf_get_buf() and seq_buf_commit() helper functions
      tracing: Have seq_buf use full buffer
      seq_buf: Add seq_buf_can_fit() helper function
      tracing: Add paranoid size check in trace_printk_seq()
      tracing: Use trace_seq_used() and seq_buf_used() instead of len
      tracing: Clean up tracing_fill_pipe_page()
      seq_buf: Create seq_buf_used() to find out how much was written
      tracing: Add a seq_buf_clear() helper and clear len and readpos in init
      tracing: Convert seq_buf fields to be like seq_file fields
      tracing: Convert seq_buf_path() to be like seq_path()
      tracing: Create seq_buf layer in trace_seq

commit 1dd7dcb6eaa677b034e7ef63df8320277507ae70
Merge: b6da0076bab5 3558a5ac50db
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Dec 10 19:58:13 2014 -0800

    Merge tag 'trace-3.19' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "There was a lot of clean ups and minor fixes.  One of those clean ups
      was to the trace_seq code.  It also removed the return values to the
      trace_seq_*() functions and use trace_seq_has_overflowed() to see if
      the buffer filled up or not.  This is similar to work being done to
      the seq_file code as well in another tree.
    
      Some of the other goodies include:
    
       - Added some "!" (NOT) logic to the tracing filter.
    
       - Fixed the frame pointer logic to the x86_64 mcount trampolines
    
       - Added the logic for dynamic trampolines on !CONFIG_PREEMPT systems.
         That is, the ftrace trampoline can be dynamically allocated and be
         called directly by functions that only have a single hook to them"
    
    * tag 'trace-3.19' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (55 commits)
      tracing: Truncated output is better than nothing
      tracing: Add additional marks to signal very large time deltas
      Documentation: describe trace_buf_size parameter more accurately
      tracing: Allow NOT to filter AND and OR clauses
      tracing: Add NOT to filtering logic
      ftrace/fgraph/x86: Have prepare_ftrace_return() take ip as first parameter
      ftrace/x86: Get rid of ftrace_caller_setup
      ftrace/x86: Have save_mcount_regs macro also save stack frames if needed
      ftrace/x86: Add macro MCOUNT_REG_SIZE for amount of stack used to save mcount regs
      ftrace/x86: Simplify save_mcount_regs on getting RIP
      ftrace/x86: Have save_mcount_regs store RIP in %rdi for first parameter
      ftrace/x86: Rename MCOUNT_SAVE_FRAME and add more detailed comments
      ftrace/x86: Move MCOUNT_SAVE_FRAME out of header file
      ftrace/x86: Have static tracing also use ftrace_caller_setup
      ftrace/x86: Have static function tracing always test for function graph
      kprobes: Add IPMODIFY flag to kprobe_ftrace_ops
      ftrace, kprobes: Support IPMODIFY flag to find IP modify conflict
      kprobes/ftrace: Recover original IP if pre_handler doesn't change it
      tracing/trivial: Fix typos and make an int into a bool
      tracing: Deletion of an unnecessary check before iput()
      ...

commit 5ac48378414dccca735897c4d7f4e19987c8977c
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Nov 14 15:49:41 2014 -0500

    tracing: Use trace_seq_used() and seq_buf_used() instead of len
    
    As the seq_buf->len will soon be +1 size when there's an overflow, we
    must use trace_seq_used() or seq_buf_used() methods to get the real
    length. This will prevent buffer overflow issues if just the len
    of the seq_buf descriptor is used to copy memory.
    
    Link: http://lkml.kernel.org/r/20141114121911.09ba3d38@gandalf.local.home
    
    Reported-by: Petr Mladek <pmladek@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 4d0067dd7f88..935cbea78532 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1044,7 +1044,8 @@ event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 	mutex_unlock(&event_mutex);
 
 	if (file)
-		r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->seq.len);
+		r = simple_read_from_buffer(ubuf, cnt, ppos,
+					    s->buffer, trace_seq_used(s));
 
 	kfree(s);
 
@@ -1210,7 +1211,8 @@ subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 	trace_seq_init(s);
 
 	print_subsystem_event_filter(system, s);
-	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->seq.len);
+	r = simple_read_from_buffer(ubuf, cnt, ppos,
+				    s->buffer, trace_seq_used(s));
 
 	kfree(s);
 
@@ -1265,7 +1267,8 @@ show_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 	trace_seq_init(s);
 
 	func(s);
-	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->seq.len);
+	r = simple_read_from_buffer(ubuf, cnt, ppos,
+				    s->buffer, trace_seq_used(s));
 
 	kfree(s);
 

commit 3a161d99c43ce74c76aecff309be4c3ba455e823
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jun 25 15:54:42 2014 -0400

    tracing: Create seq_buf layer in trace_seq
    
    Create a seq_buf layer that trace_seq sits on. The seq_buf will not
    be limited to page size. This will allow other usages of seq_buf
    instead of a hard set PAGE_SIZE one that trace_seq has.
    
    Link: http://lkml.kernel.org/r/20141104160221.864997179@goodmis.org
    Link: http://lkml.kernel.org/r/20141114011412.170377300@goodmis.org
    
    Tested-by: Jiri Kosina <jkosina@suse.cz>
    Acked-by: Jiri Kosina <jkosina@suse.cz>
    Reviewed-by: Petr Mladek <pmladek@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f9d0cbe014b7..4d0067dd7f88 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1044,7 +1044,7 @@ event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 	mutex_unlock(&event_mutex);
 
 	if (file)
-		r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
+		r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->seq.len);
 
 	kfree(s);
 
@@ -1210,7 +1210,7 @@ subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 	trace_seq_init(s);
 
 	print_subsystem_event_filter(system, s);
-	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->seq.len);
 
 	kfree(s);
 
@@ -1265,7 +1265,7 @@ show_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 	trace_seq_init(s);
 
 	func(s);
-	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->seq.len);
 
 	kfree(s);
 

commit fa6f0cc751d377af3f4f1484bceb47dc10163753
Author: Rasmus Villemoes <linux@rasmusvillemoes.dk>
Date:   Sat Nov 8 21:42:10 2014 +0100

    tracing: Replace seq_printf by simpler equivalents
    
    Using seq_printf to print a simple string or a single character is a
    lot more expensive than it needs to be, since seq_puts and seq_putc
    exist.
    
    These patches do
    
      seq_printf(m, s) -> seq_puts(m, s)
      seq_printf(m, "%s", s) -> seq_puts(m, s)
      seq_printf(m, "%c", c) -> seq_putc(m, c)
    
    Subsequent patches will simplify further.
    
    Link: http://lkml.kernel.org/r/1415479332-25944-2-git-send-email-linux@rasmusvillemoes.dk
    
    Signed-off-by: Rasmus Villemoes <linux@rasmusvillemoes.dk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0cc51edde3a8..f9d0cbe014b7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -918,7 +918,7 @@ static int f_show(struct seq_file *m, void *v)
 	case FORMAT_HEADER:
 		seq_printf(m, "name: %s\n", ftrace_event_name(call));
 		seq_printf(m, "ID: %d\n", call->event.type);
-		seq_printf(m, "format:\n");
+		seq_puts(m, "format:\n");
 		return 0;
 
 	case FORMAT_FIELD_SEPERATOR:
@@ -1988,7 +1988,7 @@ event_enable_print(struct seq_file *m, unsigned long ip,
 		   ftrace_event_name(data->file->event_call));
 
 	if (data->count == -1)
-		seq_printf(m, ":unlimited\n");
+		seq_puts(m, ":unlimited\n");
 	else
 		seq_printf(m, ":count=%ld\n", data->count);
 

commit 946e51f2bf37f1656916eb75bd0742ba33983c28
Author: Al Viro <viro@zeniv.linux.org.uk>
Date:   Sun Oct 26 19:19:16 2014 -0400

    move d_rcu from overlapping d_child to overlapping d_alias
    
    Signed-off-by: Al Viro <viro@zeniv.linux.org.uk>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0cc51edde3a8..1b0df1e504f0 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -461,7 +461,7 @@ static void remove_event_file_dir(struct ftrace_event_file *file)
 
 	if (dir) {
 		spin_lock(&dir->d_lock);	/* probably unneeded */
-		list_for_each_entry(child, &dir->d_subdirs, d_u.d_child) {
+		list_for_each_entry(child, &dir->d_subdirs, d_child) {
 			if (child->d_inode)	/* probably unneeded */
 				child->d_inode->i_private = NULL;
 		}

commit fe0e01c77dd9f7a60916aec2149d8a1182baf63c
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Oct 8 18:51:10 2014 +0200

    tracing: Robustify wait loop
    
    The pending nested sleep debugging triggered on the potential stale
    TASK_INTERRUPTIBLE in this code.
    
    While there, fix the loop such that we won't revert to a while(1)
    yield() 'spin' loop if we ever get a spurious wakeup.
    
    And fix the actual issue by properly terminating the 'wait' loop by
    setting TASK_RUNNING.
    
    Link: http://lkml.kernel.org/p/20141008165110.GA14547@worktop.programming.kicks-ass.net
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ef06ce7e9cf8..0cc51edde3a8 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2513,8 +2513,11 @@ static __init int event_test_thread(void *unused)
 	kfree(test_malloc);
 
 	set_current_state(TASK_INTERRUPTIBLE);
-	while (!kthread_should_stop())
+	while (!kthread_should_stop()) {
 		schedule();
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
 
 	return 0;
 }

commit c9b88e9581828bb8bba06c5e7ee8ed1761172b6e
Merge: b8c0aa46b3e8 6355d54438bf
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 12:02:48 2014 -0700

    Merge tag 'trace-3.17-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing filter cleanups from Steven Rostedt:
     "Oleg Nesterov did several clean ups with the tracing filter code.  As
      he found some small bugs that went into 3.16, and these changes were
      based on that, I had to apply his changes to a separate branch than my
      main development branch.
    
      This was based on work that was already pulled into 3.16, and is a
      separate pull request to keep from having local merges in my pull
      request"
    
    * tag 'trace-3.17-2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Kill "filter_string" arg of replace_preds()
      tracing: Change apply_subsystem_event_filter() paths to check file->system == dir
      tracing: Kill ftrace_event_call->files
      tracing/uprobes: Kill the dead TRACE_EVENT_FL_USE_CALL_FILTER logic
      tracing: Kill call_filter_disable()
      tracing: Kill destroy_call_preds()
      tracing: Kill destroy_preds() and destroy_file_preds()

commit b8c0aa46b3e86083721b57ed2eec6bd2c29ebfba
Merge: c7ed326fa7ca dc6f03f26f57
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Aug 4 11:50:00 2014 -0700

    Merge tag 'trace-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This pull request has a lot of work done.  The main thing is the
      changes to the ftrace function callback infrastructure.  It's
      introducing a way to allow different functions to call directly
      different trampolines instead of all calling the same "mcount" one.
    
      The only user of this for now is the function graph tracer, which
      always had a different trampoline, but the function tracer trampoline
      was called and did basically nothing, and then the function graph
      tracer trampoline was called.  The difference now, is that the
      function graph tracer trampoline can be called directly if a function
      is only being traced by the function graph trampoline.  If function
      tracing is also happening on the same function, the old way is still
      done.
    
      The accounting for this takes up more memory when function graph
      tracing is activated, as it needs to keep track of which functions it
      uses.  I have a new way that wont take as much memory, but it's not
      ready yet for this merge window, and will have to wait for the next
      one.
    
      Another big change was the removal of the ftrace_start/stop() calls
      that were used by the suspend/resume code that stopped function
      tracing when entering into suspend and resume paths.  The stop of
      ftrace was done because there was some function that would crash the
      system if one called smp_processor_id()! The stop/start was a big
      hammer to solve the issue at the time, which was when ftrace was first
      introduced into Linux.  Now ftrace has better infrastructure to debug
      such issues, and I found the problem function and labeled it with
      "notrace" and function tracing can now safely be activated all the way
      down into the guts of suspend and resume
    
      Other changes include clean ups of uprobe code, clean up of the
      trace_seq() code, and other various small fixes and clean ups to
      ftrace and tracing"
    
    * tag 'trace-3.17' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (57 commits)
      ftrace: Add warning if tramp hash does not match nr_trampolines
      ftrace: Fix trampoline hash update check on rec->flags
      ring-buffer: Use rb_page_size() instead of open coded head_page size
      ftrace: Rename ftrace_ops field from trampolines to nr_trampolines
      tracing: Convert local function_graph functions to static
      ftrace: Do not copy old hash when resetting
      tracing: let user specify tracing_thresh after selecting function_graph
      ring-buffer: Always run per-cpu ring buffer resize with schedule_work_on()
      tracing: Remove function_trace_stop and HAVE_FUNCTION_TRACE_MCOUNT_TEST
      s390/ftrace: remove check of obsolete variable function_trace_stop
      arm64, ftrace: Remove check of obsolete variable function_trace_stop
      Blackfin: ftrace: Remove check of obsolete variable function_trace_stop
      metag: ftrace: Remove check of obsolete variable function_trace_stop
      microblaze: ftrace: Remove check of obsolete variable function_trace_stop
      MIPS: ftrace: Remove check of obsolete variable function_trace_stop
      parisc: ftrace: Remove check of obsolete variable function_trace_stop
      sh: ftrace: Remove check of obsolete variable function_trace_stop
      sparc64,ftrace: Remove check of obsolete variable function_trace_stop
      tile: ftrace: Remove check of obsolete variable function_trace_stop
      ftrace: x86: Remove check of obsolete variable function_trace_stop
      ...

commit 57375747b6fac0f6cf7b302c4a8adb9043ea8e3b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jul 15 20:48:16 2014 +0200

    tracing: Kill destroy_call_preds()
    
    Remove destroy_call_preds(). Its only caller, __trace_remove_event_call(),
    can use free_event_filter() and nullify ->filter by hand.
    
    Perhaps we could keep this trivial helper although imo it is pointless, but
    then it should be static in trace_events.c.
    
    Link: http://lkml.kernel.org/p/20140715184816.GA20495@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 85914edf5059..0d8ee29f6b9a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1748,7 +1748,8 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 {
 	event_remove(call);
 	trace_destroy_fields(call);
-	destroy_call_preds(call);
+	free_event_filter(call->filter);
+	call->filter = NULL;
 }
 
 static int probe_remove_event_call(struct ftrace_event_call *call)

commit 3e5454d6568c203bca712e1976b052c345f47b44
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Tue Jul 15 20:48:13 2014 +0200

    tracing: Kill destroy_preds() and destroy_file_preds()
    
    destroy_preds() makes no sense.
    
    The only caller, event_remove(), actually wants destroy_file_preds().
    __trace_remove_event_call() does destroy_call_preds() which takes care
    of call->filter.
    
    And after the previous change we can simply remove destroy_preds() from
    event_remove(), we are going to call remove_event_from_tracers() which
    in turn calls remove_event_file_dir()->free_event_filter().
    
    Link: http://lkml.kernel.org/p/20140715184813.GA20488@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2de53628689f..85914edf5059 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1621,7 +1621,6 @@ static void event_remove(struct ftrace_event_call *call)
 		if (file->event_call != call)
 			continue;
 		ftrace_event_enable_disable(file, 0);
-		destroy_preds(file);
 		/*
 		 * The do_for_each_event_file() is
 		 * a double loop. After finding the call for this

commit 2448e3493cb3874baa90725c87869455ebf11cd2
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 11 21:06:38 2014 +0200

    tracing: instance_rmdir() leaks ftrace_event_file->filter
    
    instance_rmdir() path destroys the event files but forgets to free
    file->filter. Change remove_event_file_dir() to free_event_filter().
    
    Link: http://lkml.kernel.org/p/20140711190638.GA19517@redhat.com
    
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Cc: Namhyung Kim <namhyung@kernel.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Cc: "zhangwei(Jovi)" <jovi.zhangwei@huawei.com>
    Cc: stable@vger.kernel.org # 3.11+
    Fixes: f6a84bdc75b5 "tracing: Introduce remove_event_file_dir()"
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f99e0b3bca8c..2de53628689f 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -470,6 +470,7 @@ static void remove_event_file_dir(struct ftrace_event_file *file)
 
 	list_del(&file->list);
 	remove_subsystem(file->system);
+	free_event_filter(file->filter);
 	kmem_cache_free(file_cachep, file);
 }
 

commit 3448bac32953f051be91cef6d67025869f08dc4d
Author: Fabian Frederick <fabf@skynet.be>
Date:   Sat Jun 7 13:43:08 2014 +0200

    tracing: Convert pr_warning() to pr_warn() in trace_events.c
    
    Convert pr_warning to standard pr_warn
    Define pr_fmt(fmt) fmt to avoid any future default fmt definition
    
    Link: http://lkml.kernel.org/p/1402141388-21144-1-git-send-email-fabf@skynet.be
    
    Signed-off-by: Fabian Frederick <fabf@skynet.be>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f99e0b3bca8c..e7a814b3906b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -8,6 +8,8 @@
  *
  */
 
+#define pr_fmt(fmt) fmt
+
 #include <linux/workqueue.h>
 #include <linux/spinlock.h>
 #include <linux/kthread.h>
@@ -1490,7 +1492,7 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
 
 	dir->entry = debugfs_create_dir(name, parent);
 	if (!dir->entry) {
-		pr_warning("Failed to create system directory %s\n", name);
+		pr_warn("Failed to create system directory %s\n", name);
 		__put_system(system);
 		goto out_free;
 	}
@@ -1506,7 +1508,7 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
 	if (!entry) {
 		kfree(system->filter);
 		system->filter = NULL;
-		pr_warning("Could not create debugfs '%s/filter' entry\n", name);
+		pr_warn("Could not create debugfs '%s/filter' entry\n", name);
 	}
 
 	trace_create_file("enable", 0644, dir->entry, dir,
@@ -1521,8 +1523,7 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
  out_fail:
 	/* Only print this message if failed on memory allocation */
 	if (!dir || !system)
-		pr_warning("No memory to create event subsystem %s\n",
-			   name);
+		pr_warn("No memory to create event subsystem %s\n", name);
 	return NULL;
 }
 
@@ -1550,8 +1551,7 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 	name = ftrace_event_name(call);
 	file->dir = debugfs_create_dir(name, d_events);
 	if (!file->dir) {
-		pr_warning("Could not create debugfs '%s' directory\n",
-			   name);
+		pr_warn("Could not create debugfs '%s' directory\n", name);
 		return -1;
 	}
 
@@ -1574,8 +1574,8 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 	if (list_empty(head)) {
 		ret = call->class->define_fields(call);
 		if (ret < 0) {
-			pr_warning("Could not initialize trace point"
-				   " events/%s\n", name);
+			pr_warn("Could not initialize trace point events/%s\n",
+				name);
 			return -1;
 		}
 	}
@@ -1648,8 +1648,7 @@ static int event_init(struct ftrace_event_call *call)
 	if (call->class->raw_init) {
 		ret = call->class->raw_init(call);
 		if (ret < 0 && ret != -ENOSYS)
-			pr_warn("Could not initialize trace events/%s\n",
-				name);
+			pr_warn("Could not initialize trace events/%s\n", name);
 	}
 
 	return ret;
@@ -1894,8 +1893,8 @@ __trace_add_event_dirs(struct trace_array *tr)
 	list_for_each_entry(call, &ftrace_events, list) {
 		ret = __trace_add_new_event(call, tr);
 		if (ret < 0)
-			pr_warning("Could not create directory for event %s\n",
-				   ftrace_event_name(call));
+			pr_warn("Could not create directory for event %s\n",
+				ftrace_event_name(call));
 	}
 }
 
@@ -2207,8 +2206,8 @@ __trace_early_add_event_dirs(struct trace_array *tr)
 	list_for_each_entry(file, &tr->events, list) {
 		ret = event_create_dir(tr->event_dir, file);
 		if (ret < 0)
-			pr_warning("Could not create directory for event %s\n",
-				   ftrace_event_name(file->event_call));
+			pr_warn("Could not create directory for event %s\n",
+				ftrace_event_name(file->event_call));
 	}
 }
 
@@ -2231,8 +2230,8 @@ __trace_early_add_events(struct trace_array *tr)
 
 		ret = __trace_early_add_new_event(call, tr);
 		if (ret < 0)
-			pr_warning("Could not create early event %s\n",
-				   ftrace_event_name(call));
+			pr_warn("Could not create early event %s\n",
+				ftrace_event_name(call));
 	}
 }
 
@@ -2279,13 +2278,13 @@ create_event_toplevel_files(struct dentry *parent, struct trace_array *tr)
 	entry = debugfs_create_file("set_event", 0644, parent,
 				    tr, &ftrace_set_event_fops);
 	if (!entry) {
-		pr_warning("Could not create debugfs 'set_event' entry\n");
+		pr_warn("Could not create debugfs 'set_event' entry\n");
 		return -ENOMEM;
 	}
 
 	d_events = debugfs_create_dir("events", parent);
 	if (!d_events) {
-		pr_warning("Could not create debugfs 'events' directory\n");
+		pr_warn("Could not create debugfs 'events' directory\n");
 		return -ENOMEM;
 	}
 
@@ -2461,11 +2460,10 @@ static __init int event_trace_init(void)
 	entry = debugfs_create_file("available_events", 0444, d_tracer,
 				    tr, &ftrace_avail_fops);
 	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'available_events' entry\n");
+		pr_warn("Could not create debugfs 'available_events' entry\n");
 
 	if (trace_define_common_fields())
-		pr_warning("tracing: Failed to allocate common fields");
+		pr_warn("tracing: Failed to allocate common fields");
 
 	ret = early_event_add_tracer(d_tracer, tr);
 	if (ret)
@@ -2474,7 +2472,7 @@ static __init int event_trace_init(void)
 #ifdef CONFIG_MODULES
 	ret = register_module_notifier(&trace_module_nb);
 	if (ret)
-		pr_warning("Failed to register trace events module notifier\n");
+		pr_warn("Failed to register trace events module notifier\n");
 #endif
 	return 0;
 }
@@ -2578,7 +2576,7 @@ static __init void event_trace_self_tests(void)
 		 * it and the self test should not be on.
 		 */
 		if (file->flags & FTRACE_EVENT_FL_ENABLED) {
-			pr_warning("Enabled event during self test!\n");
+			pr_warn("Enabled event during self test!\n");
 			WARN_ON_ONCE(1);
 			continue;
 		}
@@ -2606,8 +2604,8 @@ static __init void event_trace_self_tests(void)
 
 		ret = __ftrace_set_clr_event(tr, NULL, system->name, NULL, 1);
 		if (WARN_ON_ONCE(ret)) {
-			pr_warning("error enabling system %s\n",
-				   system->name);
+			pr_warn("error enabling system %s\n",
+				system->name);
 			continue;
 		}
 
@@ -2615,8 +2613,8 @@ static __init void event_trace_self_tests(void)
 
 		ret = __ftrace_set_clr_event(tr, NULL, system->name, NULL, 0);
 		if (WARN_ON_ONCE(ret)) {
-			pr_warning("error disabling system %s\n",
-				   system->name);
+			pr_warn("error disabling system %s\n",
+				system->name);
 			continue;
 		}
 
@@ -2630,7 +2628,7 @@ static __init void event_trace_self_tests(void)
 
 	ret = __ftrace_set_clr_event(tr, NULL, NULL, NULL, 1);
 	if (WARN_ON_ONCE(ret)) {
-		pr_warning("error enabling all events\n");
+		pr_warn("error enabling all events\n");
 		return;
 	}
 
@@ -2639,7 +2637,7 @@ static __init void event_trace_self_tests(void)
 	/* reset sysname */
 	ret = __ftrace_set_clr_event(tr, NULL, NULL, NULL, 0);
 	if (WARN_ON_ONCE(ret)) {
-		pr_warning("error disabling all events\n");
+		pr_warn("error disabling all events\n");
 		return;
 	}
 

commit dc81e5e3abb9f98a3cb6f269c0bee595b2c1235d
Author: Yoshihiro YUNOMAE <yoshihiro.yunomae.ez@hitachi.com>
Date:   Fri Jun 6 07:35:17 2014 +0900

    tracing: Return error if ftrace_trace_arrays list is empty
    
    ftrace_trace_arrays links global_trace.list. However, global_trace
    is not added to ftrace_trace_arrays if trace_alloc_buffers() failed.
    As the result, ftrace_trace_arrays becomes an empty list. If
    ftrace_trace_arrays is an empty list, current top_trace_array() returns
    an invalid pointer. As the result, the kernel can induce memory corruption
    or panic.
    
    Current implementation does not check whether ftrace_trace_arrays is empty
    list or not. So, in this patch, if ftrace_trace_arrays is empty list,
    top_trace_array() returns NULL. Moreover, this patch makes all functions
    calling top_trace_array() handle it appropriately.
    
    Link: http://lkml.kernel.org/p/20140605223517.32311.99233.stgit@yunodevel
    
    Signed-off-by: Yoshihiro YUNOMAE <yoshihiro.yunomae.ez@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 3ddfd8f62c05..f99e0b3bca8c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -574,6 +574,9 @@ int trace_set_clr_event(const char *system, const char *event, int set)
 {
 	struct trace_array *tr = top_trace_array();
 
+	if (!tr)
+		return -ENODEV;
+
 	return __ftrace_set_clr_event(tr, NULL, system, event, set);
 }
 EXPORT_SYMBOL_GPL(trace_set_clr_event);
@@ -2065,6 +2068,9 @@ event_enable_func(struct ftrace_hash *hash,
 	bool enable;
 	int ret;
 
+	if (!tr)
+		return -ENODEV;
+
 	/* hash funcs only work with set_ftrace_filter */
 	if (!enabled || !param)
 		return -EINVAL;
@@ -2396,6 +2402,9 @@ static __init int event_trace_enable(void)
 	char *token;
 	int ret;
 
+	if (!tr)
+		return -ENODEV;
+
 	for_each_event(iter, __start_ftrace_events, __stop_ftrace_events) {
 
 		call = *iter;
@@ -2442,6 +2451,8 @@ static __init int event_trace_init(void)
 	int ret;
 
 	tr = top_trace_array();
+	if (!tr)
+		return -ENODEV;
 
 	d_tracer = tracing_init_dentry();
 	if (!d_tracer)
@@ -2535,6 +2546,8 @@ static __init void event_trace_self_tests(void)
 	int ret;
 
 	tr = top_trace_array();
+	if (!tr)
+		return;
 
 	pr_info("Running tests on trace events:\n");
 

commit de7b2973903c6cc50b31ee5682a69b2219b9919d
Author: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
Date:   Tue Apr 8 17:26:21 2014 -0400

    tracepoint: Use struct pointer instead of name hash for reg/unreg tracepoints
    
    Register/unregister tracepoint probes with struct tracepoint pointer
    rather than tracepoint name.
    
    This change, which vastly simplifies tracepoint.c, has been proposed by
    Steven Rostedt. It also removes 8.8kB (mostly of text) to the vmlinux
    size.
    
    From this point on, the tracers need to pass a struct tracepoint pointer
    to probe register/unregister. A probe can now only be connected to a
    tracepoint that exists. Moreover, tracers are responsible for
    unregistering the probe before the module containing its associated
    tracepoint is unloaded.
    
       text    data     bss     dec     hex filename
    10443444        4282528 10391552        25117524        17f4354 vmlinux.orig
    10434930        4282848 10391552        25109330        17f2352 vmlinux
    
    Link: http://lkml.kernel.org/r/1396992381-23785-2-git-send-email-mathieu.desnoyers@efficios.com
    
    CC: Ingo Molnar <mingo@kernel.org>
    CC: Frederic Weisbecker <fweisbec@gmail.com>
    CC: Andrew Morton <akpm@linux-foundation.org>
    CC: Frank Ch. Eigler <fche@redhat.com>
    CC: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    [ SDR - fixed return val in void func in tracepoint_module_going() ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 83a4378dc5e0..3ddfd8f62c05 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -223,24 +223,25 @@ int ftrace_event_reg(struct ftrace_event_call *call,
 {
 	struct ftrace_event_file *file = data;
 
+	WARN_ON(!(call->flags & TRACE_EVENT_FL_TRACEPOINT));
 	switch (type) {
 	case TRACE_REG_REGISTER:
-		return tracepoint_probe_register(call->name,
+		return tracepoint_probe_register(call->tp,
 						 call->class->probe,
 						 file);
 	case TRACE_REG_UNREGISTER:
-		tracepoint_probe_unregister(call->name,
+		tracepoint_probe_unregister(call->tp,
 					    call->class->probe,
 					    file);
 		return 0;
 
 #ifdef CONFIG_PERF_EVENTS
 	case TRACE_REG_PERF_REGISTER:
-		return tracepoint_probe_register(call->name,
+		return tracepoint_probe_register(call->tp,
 						 call->class->perf_probe,
 						 call);
 	case TRACE_REG_PERF_UNREGISTER:
-		tracepoint_probe_unregister(call->name,
+		tracepoint_probe_unregister(call->tp,
 					    call->class->perf_probe,
 					    call);
 		return 0;
@@ -352,7 +353,7 @@ static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
 			if (ret) {
 				tracing_stop_cmdline_record();
 				pr_info("event trace: Could not enable event "
-					"%s\n", call->name);
+					"%s\n", ftrace_event_name(call));
 				break;
 			}
 			set_bit(FTRACE_EVENT_FL_ENABLED_BIT, &file->flags);
@@ -481,27 +482,29 @@ __ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,
 {
 	struct ftrace_event_file *file;
 	struct ftrace_event_call *call;
+	const char *name;
 	int ret = -EINVAL;
 
 	list_for_each_entry(file, &tr->events, list) {
 
 		call = file->event_call;
+		name = ftrace_event_name(call);
 
-		if (!call->name || !call->class || !call->class->reg)
+		if (!name || !call->class || !call->class->reg)
 			continue;
 
 		if (call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)
 			continue;
 
 		if (match &&
-		    strcmp(match, call->name) != 0 &&
+		    strcmp(match, name) != 0 &&
 		    strcmp(match, call->class->system) != 0)
 			continue;
 
 		if (sub && strcmp(sub, call->class->system) != 0)
 			continue;
 
-		if (event && strcmp(event, call->name) != 0)
+		if (event && strcmp(event, name) != 0)
 			continue;
 
 		ftrace_event_enable_disable(file, set);
@@ -699,7 +702,7 @@ static int t_show(struct seq_file *m, void *v)
 
 	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
 		seq_printf(m, "%s:", call->class->system);
-	seq_printf(m, "%s\n", call->name);
+	seq_printf(m, "%s\n", ftrace_event_name(call));
 
 	return 0;
 }
@@ -792,7 +795,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	mutex_lock(&event_mutex);
 	list_for_each_entry(file, &tr->events, list) {
 		call = file->event_call;
-		if (!call->name || !call->class || !call->class->reg)
+		if (!ftrace_event_name(call) || !call->class || !call->class->reg)
 			continue;
 
 		if (system && strcmp(call->class->system, system->name) != 0)
@@ -907,7 +910,7 @@ static int f_show(struct seq_file *m, void *v)
 
 	switch ((unsigned long)v) {
 	case FORMAT_HEADER:
-		seq_printf(m, "name: %s\n", call->name);
+		seq_printf(m, "name: %s\n", ftrace_event_name(call));
 		seq_printf(m, "ID: %d\n", call->event.type);
 		seq_printf(m, "format:\n");
 		return 0;
@@ -1527,6 +1530,7 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 	struct trace_array *tr = file->tr;
 	struct list_head *head;
 	struct dentry *d_events;
+	const char *name;
 	int ret;
 
 	/*
@@ -1540,10 +1544,11 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 	} else
 		d_events = parent;
 
-	file->dir = debugfs_create_dir(call->name, d_events);
+	name = ftrace_event_name(call);
+	file->dir = debugfs_create_dir(name, d_events);
 	if (!file->dir) {
 		pr_warning("Could not create debugfs '%s' directory\n",
-			   call->name);
+			   name);
 		return -1;
 	}
 
@@ -1567,7 +1572,7 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 		ret = call->class->define_fields(call);
 		if (ret < 0) {
 			pr_warning("Could not initialize trace point"
-				   " events/%s\n", call->name);
+				   " events/%s\n", name);
 			return -1;
 		}
 	}
@@ -1631,15 +1636,17 @@ static void event_remove(struct ftrace_event_call *call)
 static int event_init(struct ftrace_event_call *call)
 {
 	int ret = 0;
+	const char *name;
 
-	if (WARN_ON(!call->name))
+	name = ftrace_event_name(call);
+	if (WARN_ON(!name))
 		return -EINVAL;
 
 	if (call->class->raw_init) {
 		ret = call->class->raw_init(call);
 		if (ret < 0 && ret != -ENOSYS)
 			pr_warn("Could not initialize trace events/%s\n",
-				call->name);
+				name);
 	}
 
 	return ret;
@@ -1885,7 +1892,7 @@ __trace_add_event_dirs(struct trace_array *tr)
 		ret = __trace_add_new_event(call, tr);
 		if (ret < 0)
 			pr_warning("Could not create directory for event %s\n",
-				   call->name);
+				   ftrace_event_name(call));
 	}
 }
 
@@ -1894,18 +1901,20 @@ find_event_file(struct trace_array *tr, const char *system,  const char *event)
 {
 	struct ftrace_event_file *file;
 	struct ftrace_event_call *call;
+	const char *name;
 
 	list_for_each_entry(file, &tr->events, list) {
 
 		call = file->event_call;
+		name = ftrace_event_name(call);
 
-		if (!call->name || !call->class || !call->class->reg)
+		if (!name || !call->class || !call->class->reg)
 			continue;
 
 		if (call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)
 			continue;
 
-		if (strcmp(event, call->name) == 0 &&
+		if (strcmp(event, name) == 0 &&
 		    strcmp(system, call->class->system) == 0)
 			return file;
 	}
@@ -1973,7 +1982,7 @@ event_enable_print(struct seq_file *m, unsigned long ip,
 	seq_printf(m, "%s:%s:%s",
 		   data->enable ? ENABLE_EVENT_STR : DISABLE_EVENT_STR,
 		   data->file->event_call->class->system,
-		   data->file->event_call->name);
+		   ftrace_event_name(data->file->event_call));
 
 	if (data->count == -1)
 		seq_printf(m, ":unlimited\n");
@@ -2193,7 +2202,7 @@ __trace_early_add_event_dirs(struct trace_array *tr)
 		ret = event_create_dir(tr->event_dir, file);
 		if (ret < 0)
 			pr_warning("Could not create directory for event %s\n",
-				   file->event_call->name);
+				   ftrace_event_name(file->event_call));
 	}
 }
 
@@ -2217,7 +2226,7 @@ __trace_early_add_events(struct trace_array *tr)
 		ret = __trace_early_add_new_event(call, tr);
 		if (ret < 0)
 			pr_warning("Could not create early event %s\n",
-				   call->name);
+				   ftrace_event_name(call));
 	}
 }
 
@@ -2549,7 +2558,7 @@ static __init void event_trace_self_tests(void)
 			continue;
 #endif
 
-		pr_info("Testing event %s: ", call->name);
+		pr_info("Testing event %s: ", ftrace_event_name(call));
 
 		/*
 		 * If an event is already enabled, someone is using

commit 68114e5eb862ad0a7a261b91497281b026102715
Merge: 59ecc26004e7 3862807880ac
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Apr 3 10:26:31 2014 -0700

    Merge tag 'trace-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "Most of the changes were largely clean ups, and some documentation.
      But there were a few features that were added:
    
      Uprobes now work with event triggers and multi buffers and have
      support under ftrace and perf.
    
      The big feature is that the function tracer can now be used within the
      multi buffer instances.  That is, you can now trace some functions in
      one buffer, others in another buffer, all functions in a third buffer
      and so on.  They are basically agnostic from each other.  This only
      works for the function tracer and not for the function graph trace,
      although you can have the function graph tracer running in the top
      level buffer (or any tracer for that matter) and have different
      function tracing going on in the sub buffers"
    
    * tag 'trace-3.15' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (45 commits)
      tracing: Add BUG_ON when stack end location is over written
      tracepoint: Remove unused API functions
      Revert "tracing: Move event storage for array from macro to standalone function"
      ftrace: Constify ftrace_text_reserved
      tracepoints: API doc update to tracepoint_probe_register() return value
      tracepoints: API doc update to data argument
      ftrace: Fix compilation warning about control_ops_free
      ftrace/x86: BUG when ftrace recovery fails
      ftrace: Warn on error when modifying ftrace function
      ftrace: Remove freelist from struct dyn_ftrace
      ftrace: Do not pass data to ftrace_dyn_arch_init
      ftrace: Pass retval through return in ftrace_dyn_arch_init()
      ftrace: Inline the code from ftrace_dyn_table_alloc()
      ftrace: Cleanup of global variables ftrace_new_pgs and ftrace_update_cnt
      tracing: Evaluate len expression only once in __dynamic_array macro
      tracing: Correctly expand len expressions from __dynamic_array macro
      tracing/module: Replace include of tracepoint.h with jump_label.h in module.h
      tracing: Fix event header migrate.h to include tracepoint.h
      tracing: Fix event header writeback.h to include tracepoint.h
      tracing: Warn if a tracepoint is not set via debugfs
      ...

commit bc4c426ee2431d1f717004d3bbaacbd819b544fd
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Mar 21 08:23:38 2014 -0400

    Revert "tracing: Move event storage for array from macro to standalone function"
    
    I originally wrote commit 35bb4399bd0e to shrink the size of the overhead of
    tracepoints by several kilobytes. Later, I received a patch from Vaibhav
    Nagarnaik that fixed a bug in the same code that this commit touches. Not
    only did it fix a bug, it also removed code and shrunk the size of the
    overhead of trace events even more than this commit did.
    
    Since this commit is scheduled for 3.15 and Vaibhav's patch is already in
    mainline, I need to revert this patch in order to keep it from conflicting
    with Vaibhav's patch. Not to mention, Vaibhav's patch makes this patch
    obsolete.
    
    Link: http://lkml.kernel.org/r/20140320225637.0226041b@gandalf.local.home
    
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b8f73b333a3c..2f7b8e31e3a4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -27,6 +27,12 @@
 
 DEFINE_MUTEX(event_mutex);
 
+DEFINE_MUTEX(event_storage_mutex);
+EXPORT_SYMBOL_GPL(event_storage_mutex);
+
+char event_storage[EVENT_STORAGE_SIZE];
+EXPORT_SYMBOL_GPL(event_storage);
+
 LIST_HEAD(ftrace_events);
 static LIST_HEAD(ftrace_common_fields);
 

commit 87291347c49dc40aa339f587b209618201c2e527
Author: Vaibhav Nagarnaik <vnagarnaik@google.com>
Date:   Thu Feb 13 19:51:48 2014 -0800

    tracing: Fix array size mismatch in format string
    
    In event format strings, the array size is reported in two locations.
    One in array subscript and then via the "size:" attribute. The values
    reported there have a mismatch.
    
    For e.g., in sched:sched_switch the prev_comm and next_comm character
    arrays have subscript values as [32] where as the actual field size is
    16.
    
    name: sched_switch
    ID: 301
    format:
            field:unsigned short common_type;       offset:0;       size:2; signed:0;
            field:unsigned char common_flags;       offset:2;       size:1; signed:0;
            field:unsigned char common_preempt_count;       offset:3;       size:1;signed:0;
            field:int common_pid;   offset:4;       size:4; signed:1;
    
            field:char prev_comm[32];       offset:8;       size:16;        signed:1;
            field:pid_t prev_pid;   offset:24;      size:4; signed:1;
            field:int prev_prio;    offset:28;      size:4; signed:1;
            field:long prev_state;  offset:32;      size:8; signed:1;
            field:char next_comm[32];       offset:40;      size:16;        signed:1;
            field:pid_t next_pid;   offset:56;      size:4; signed:1;
            field:int next_prio;    offset:60;      size:4; signed:1;
    
    After bisection, the following commit was blamed:
    92edca0 tracing: Use direct field, type and system names
    
    This commit removes the duplication of strings for field->name and
    field->type assuming that all the strings passed in
    __trace_define_field() are immutable. This is not true for arrays, where
    the type string is created in event_storage variable and field->type for
    all array fields points to event_storage.
    
    Use __stringify() to create a string constant for the type string.
    
    Also, get rid of event_storage and event_storage_mutex that are not
    needed anymore.
    
    also, an added benefit is that this reduces the overhead of events a bit more:
    
       text    data     bss     dec     hex filename
    8424787 2036472 1302528 11763787         b3804b vmlinux
    8420814 2036408 1302528 11759750         b37086 vmlinux.patched
    
    Link: http://lkml.kernel.org/r/1392349908-29685-1-git-send-email-vnagarnaik@google.com
    
    Cc: Laurent Chavey <chavey@google.com>
    Cc: stable@vger.kernel.org # 3.10+
    Signed-off-by: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f3989ceb5cd5..7b16d40bd64d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -27,12 +27,6 @@
 
 DEFINE_MUTEX(event_mutex);
 
-DEFINE_MUTEX(event_storage_mutex);
-EXPORT_SYMBOL_GPL(event_storage_mutex);
-
-char event_storage[EVENT_STORAGE_SIZE];
-EXPORT_SYMBOL_GPL(event_storage);
-
 LIST_HEAD(ftrace_events);
 static LIST_HEAD(ftrace_common_fields);
 

commit 3fd40d1ee6a317523172ab95b6f7ea41ba8fcee3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Aug 9 22:42:57 2012 -0400

    tracing: Use helper functions in event assignment to shrink macro size
    
    The functions that assign the contents for the ftrace events are
    defined by the TRACE_EVENT() macros. Each event has its own unique
    way to assign data to its buffer. When you have over 500 events,
    that means there's 500 functions assigning data uniquely for each
    event (not really that many, as DECLARE_EVENT_CLASS() and multiple
    DEFINE_EVENT()s will only need a single function).
    
    By making helper functions in the core kernel to do some of the work
    instead, we can shrink the size of the kernel down a bit.
    
    With a kernel configured with 502 events, the change in size was:
    
       text    data     bss     dec     hex filename
    12987390        1913504 9785344 24686238        178ae9e /tmp/vmlinux
    12959102        1913504 9785344 24657950        178401e /tmp/vmlinux.patched
    
    That's a total of 28288 bytes, which comes down to 56 bytes per event.
    
    Link: http://lkml.kernel.org/r/20120810034708.370808175@goodmis.org
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 22826c73a9da..b8f73b333a3c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -188,6 +188,36 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 }
 EXPORT_SYMBOL_GPL(trace_event_raw_init);
 
+void *ftrace_event_buffer_reserve(struct ftrace_event_buffer *fbuffer,
+				  struct ftrace_event_file *ftrace_file,
+				  unsigned long len)
+{
+	struct ftrace_event_call *event_call = ftrace_file->event_call;
+
+	local_save_flags(fbuffer->flags);
+	fbuffer->pc = preempt_count();
+	fbuffer->ftrace_file = ftrace_file;
+
+	fbuffer->event =
+		trace_event_buffer_lock_reserve(&fbuffer->buffer, ftrace_file,
+						event_call->event.type, len,
+						fbuffer->flags, fbuffer->pc);
+	if (!fbuffer->event)
+		return NULL;
+
+	fbuffer->entry = ring_buffer_event_data(fbuffer->event);
+	return fbuffer->entry;
+}
+EXPORT_SYMBOL_GPL(ftrace_event_buffer_reserve);
+
+void ftrace_event_buffer_commit(struct ftrace_event_buffer *fbuffer)
+{
+	event_trigger_unlock_commit(fbuffer->ftrace_file, fbuffer->buffer,
+				    fbuffer->event, fbuffer->entry,
+				    fbuffer->flags, fbuffer->pc);
+}
+EXPORT_SYMBOL_GPL(ftrace_event_buffer_commit);
+
 int ftrace_event_reg(struct ftrace_event_call *call,
 		     enum trace_reg type, void *data)
 {

commit 35bb4399bd0ef16b8a57fccea0047d98b6b0e7fb
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Aug 9 22:26:46 2012 -0400

    tracing: Move event storage for array from macro to standalone function
    
    The code that shows array fields for events is defined for all events.
    This can add up quite a bit when you have over 500 events.
    
    By making helper functions in the core kernel to do the work
    instead, we can shrink the size of the kernel down a bit.
    
    With a kernel configured with 502 events, the change in size was:
    
       text    data     bss     dec     hex filename
    12990946        1913568 9785344 24689858        178bcc2 /tmp/vmlinux
    12987390        1913504 9785344 24686238        178ae9e /tmp/vmlinux.patched
    
    That's a total of 3556 bytes, which comes down to 7 bytes per event.
    Although it's not much, this code is just called at initialization of
    the events.
    
    Link: http://lkml.kernel.org/r/20120810034708.084036335@goodmis.org
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e71ffd4eccb5..22826c73a9da 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -27,12 +27,6 @@
 
 DEFINE_MUTEX(event_mutex);
 
-DEFINE_MUTEX(event_storage_mutex);
-EXPORT_SYMBOL_GPL(event_storage_mutex);
-
-char event_storage[EVENT_STORAGE_SIZE];
-EXPORT_SYMBOL_GPL(event_storage);
-
 LIST_HEAD(ftrace_events);
 static LIST_HEAD(ftrace_common_fields);
 

commit 45ab2813d40d88fc575e753c38478de242d03f88
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Feb 26 13:37:38 2014 -0500

    tracing: Do not add event files for modules that fail tracepoints
    
    If a module fails to add its tracepoints due to module tainting, do not
    create the module event infrastructure in the debugfs directory. As the events
    will not work and worse yet, they will silently fail, making the user wonder
    why the events they enable do not display anything.
    
    Having a warning on module load and the events not visible to the users
    will make the cause of the problem much clearer.
    
    Link: http://lkml.kernel.org/r/20140227154923.265882695@goodmis.org
    
    Fixes: 6d723736e472 "tracing/events: add support for modules to TRACE_EVENT"
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Cc: stable@vger.kernel.org # 2.6.31+
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e71ffd4eccb5..f3989ceb5cd5 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1777,6 +1777,16 @@ static void trace_module_add_events(struct module *mod)
 {
 	struct ftrace_event_call **call, **start, **end;
 
+	if (!mod->num_trace_events)
+		return;
+
+	/* Don't add infrastructure for mods without tracepoints */
+	if (trace_module_has_bad_taint(mod)) {
+		pr_err("%s: module has bad taint, not creating trace events\n",
+		       mod->name);
+		return;
+	}
+
 	start = mod->trace_events;
 	end = mod->trace_events + mod->num_trace_events;
 

commit 2875a08b2d1da7bae58fc01badb9b0ef1e8fc1a4
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Fri Dec 20 23:23:05 2013 -0500

    tracing: Move ftrace_event_file() out of DYNAMIC_FTRACE ifdef
    
    Now that event triggers use ftrace_event_file(), it needs to be outside
    the #ifdef CONFIG_DYNAMIC_FTRACE, as it can now be used when that is
    not defined.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9a974bd843d4..e71ffd4eccb5 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1855,19 +1855,6 @@ __trace_add_event_dirs(struct trace_array *tr)
 	}
 }
 
-#ifdef CONFIG_DYNAMIC_FTRACE
-
-/* Avoid typos */
-#define ENABLE_EVENT_STR	"enable_event"
-#define DISABLE_EVENT_STR	"disable_event"
-
-struct event_probe_data {
-	struct ftrace_event_file	*file;
-	unsigned long			count;
-	int				ref;
-	bool				enable;
-};
-
 struct ftrace_event_file *
 find_event_file(struct trace_array *tr, const char *system,  const char *event)
 {
@@ -1891,6 +1878,19 @@ find_event_file(struct trace_array *tr, const char *system,  const char *event)
 	return NULL;
 }
 
+#ifdef CONFIG_DYNAMIC_FTRACE
+
+/* Avoid typos */
+#define ENABLE_EVENT_STR	"enable_event"
+#define DISABLE_EVENT_STR	"disable_event"
+
+struct event_probe_data {
+	struct ftrace_event_file	*file;
+	unsigned long			count;
+	int				ref;
+	bool				enable;
+};
+
 static void
 event_enable_probe(unsigned long ip, unsigned long parent_ip, void **_data)
 {

commit 7862ad1846e994574cb47dc503cc2b1646ea6593
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:59:28 2013 -0500

    tracing: Add 'enable_event' and 'disable_event' event trigger commands
    
    Add 'enable_event' and 'disable_event' event_command commands.
    
    enable_event and disable_event event triggers are added by the user
    via these commands in a similar way and using practically the same
    syntax as the analagous 'enable_event' and 'disable_event' ftrace
    function commands, but instead of writing to the set_ftrace_filter
    file, the enable_event and disable_event triggers are written to the
    per-event 'trigger' files:
    
        echo 'enable_event:system:event' > .../othersys/otherevent/trigger
        echo 'disable_event:system:event' > .../othersys/otherevent/trigger
    
    The above commands will enable or disable the 'system:event' trace
    events whenever the othersys:otherevent events are hit.
    
    This also adds a 'count' version that limits the number of times the
    command will be invoked:
    
        echo 'enable_event:system:event:N' > .../othersys/otherevent/trigger
        echo 'disable_event:system:event:N' > .../othersys/otherevent/trigger
    
    Where N is the number of times the command will be invoked.
    
    The above commands will will enable or disable the 'system:event'
    trace events whenever the othersys:otherevent events are hit, but only
    N times.
    
    This also makes the find_event_file() helper function extern, since
    it's useful to use from other places, such as the event triggers code,
    so make it accessible.
    
    Link: http://lkml.kernel.org/r/f825f3048c3f6b026ee37ae5825f9fc373451828.1382622043.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 442775c9dbf3..9a974bd843d4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1868,7 +1868,7 @@ struct event_probe_data {
 	bool				enable;
 };
 
-static struct ftrace_event_file *
+struct ftrace_event_file *
 find_event_file(struct trace_array *tr, const char *system,  const char *event)
 {
 	struct ftrace_event_file *file;

commit 85f2b08268c014e290b600ba49fa85530600eaa1
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:59:24 2013 -0500

    tracing: Add basic event trigger framework
    
    Add a 'trigger' file for each trace event, enabling 'trace event
    triggers' to be set for trace events.
    
    'trace event triggers' are patterned after the existing 'ftrace
    function triggers' implementation except that triggers are written to
    per-event 'trigger' files instead of to a single file such as the
    'set_ftrace_filter' used for ftrace function triggers.
    
    The implementation is meant to be entirely separate from ftrace
    function triggers, in order to keep the respective implementations
    relatively simple and to allow them to diverge.
    
    The event trigger functionality is built on top of SOFT_DISABLE
    functionality.  It adds a TRIGGER_MODE bit to the ftrace_event_file
    flags which is checked when any trace event fires.  Triggers set for a
    particular event need to be checked regardless of whether that event
    is actually enabled or not - getting an event to fire even if it's not
    enabled is what's already implemented by SOFT_DISABLE mode, so trigger
    mode directly reuses that.  Event trigger essentially inherit the soft
    disable logic in __ftrace_event_enable_disable() while adding a bit of
    logic and trigger reference counting via tm_ref on top of that in a
    new trace_event_trigger_enable_disable() function.  Because the base
    __ftrace_event_enable_disable() code now needs to be invoked from
    outside trace_events.c, a wrapper is also added for those usages.
    
    The triggers for an event are actually invoked via a new function,
    event_triggers_call(), and code is also added to invoke them for
    ftrace_raw_event calls as well as syscall events.
    
    The main part of the patch creates a new trace_events_trigger.c file
    to contain the trace event triggers implementation.
    
    The standard open, read, and release file operations are implemented
    here.
    
    The open() implementation sets up for the various open modes of the
    'trigger' file.  It creates and attaches the trigger iterator and sets
    up the command parser.  If opened for reading set up the trigger
    seq_ops.
    
    The read() implementation parses the event trigger written to the
    'trigger' file, looks up the trigger command, and passes it along to
    that event_command's func() implementation for command-specific
    processing.
    
    The release() implementation does whatever cleanup is needed to
    release the 'trigger' file, like releasing the parser and trigger
    iterator, etc.
    
    A couple of functions for event command registration and
    unregistration are added, along with a list to add them to and a mutex
    to protect them, as well as an (initially empty) registration function
    to add the set of commands that will be added by future commits, and
    call to it from the trace event initialization code.
    
    also added are a couple trigger-specific data structures needed for
    these implementations such as a trigger iterator and a struct for
    trigger-specific data.
    
    A couple structs consisting mostly of function meant to be implemented
    in command-specific ways, event_command and event_trigger_ops, are
    used by the generic event trigger command implementations.  They're
    being put into trace.h alongside the other trace_event data structures
    and functions, in the expectation that they'll be needed in several
    trace_event-related files such as trace_events_trigger.c and
    trace_events.c.
    
    The event_command.func() function is meant to be called by the trigger
    parsing code in order to add a trigger instance to the corresponding
    event.  It essentially coordinates adding a live trigger instance to
    the event, and arming the triggering the event.
    
    Every event_command func() implementation essentially does the
    same thing for any command:
    
       - choose ops - use the value of param to choose either a number or
         count version of event_trigger_ops specific to the command
       - do the register or unregister of those ops
       - associate a filter, if specified, with the triggering event
    
    The reg() and unreg() ops allow command-specific implementations for
    event_trigger_op registration and unregistration, and the
    get_trigger_ops() op allows command-specific event_trigger_ops
    selection to be parameterized.  When a trigger instance is added, the
    reg() op essentially adds that trigger to the triggering event and
    arms it, while unreg() does the opposite.  The set_filter() function
    is used to associate a filter with the trigger - if the command
    doesn't specify a set_filter() implementation, the command will ignore
    filters.
    
    Each command has an associated trigger_type, which serves double duty,
    both as a unique identifier for the command as well as a value that
    can be used for setting a trigger mode bit during trigger invocation.
    
    The signature of func() adds a pointer to the event_command struct,
    used to invoke those functions, along with a command_data param that
    can be passed to the reg/unreg functions.  This allows func()
    implementations to use command-specific blobs and supports code
    re-use.
    
    The event_trigger_ops.func() command corrsponds to the trigger 'probe'
    function that gets called when the triggering event is actually
    invoked.  The other functions are used to list the trigger when
    needed, along with a couple mundane book-keeping functions.
    
    This also moves event_file_data() into trace.h so it can be used
    outside of trace_events.c.
    
    Link: http://lkml.kernel.org/r/316d95061accdee070aac8e5750afba0192fa5b9.1382622043.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Idea-by: Steve Rostedt <rostedt@goodmis.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a11800ae96de..442775c9dbf3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -342,6 +342,12 @@ static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
 	return ret;
 }
 
+int trace_event_enable_disable(struct ftrace_event_file *file,
+			       int enable, int soft_disable)
+{
+	return __ftrace_event_enable_disable(file, enable, soft_disable);
+}
+
 static int ftrace_event_enable_disable(struct ftrace_event_file *file,
 				       int enable)
 {
@@ -421,11 +427,6 @@ static void remove_subsystem(struct ftrace_subsystem_dir *dir)
 	}
 }
 
-static void *event_file_data(struct file *filp)
-{
-	return ACCESS_ONCE(file_inode(filp)->i_private);
-}
-
 static void remove_event_file_dir(struct ftrace_event_file *file)
 {
 	struct dentry *dir = file->dir;
@@ -1549,6 +1550,9 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 	trace_create_file("filter", 0644, file->dir, file,
 			  &ftrace_event_filter_fops);
 
+	trace_create_file("trigger", 0644, file->dir, file,
+			  &event_trigger_fops);
+
 	trace_create_file("format", 0444, file->dir, call,
 			  &ftrace_event_format_fops);
 
@@ -1645,6 +1649,8 @@ trace_create_new_event(struct ftrace_event_call *call,
 	file->event_call = call;
 	file->tr = tr;
 	atomic_set(&file->sm_ref, 0);
+	atomic_set(&file->tm_ref, 0);
+	INIT_LIST_HEAD(&file->triggers);
 	list_add(&file->list, &tr->events);
 
 	return file;
@@ -2311,6 +2317,9 @@ int event_trace_del_tracer(struct trace_array *tr)
 {
 	mutex_lock(&event_mutex);
 
+	/* Disable any event triggers and associated soft-disabled events */
+	clear_event_triggers(tr);
+
 	/* Disable any running events */
 	__ftrace_set_clr_event_nolock(tr, NULL, NULL, NULL, 0);
 
@@ -2377,6 +2386,8 @@ static __init int event_trace_enable(void)
 
 	register_event_cmds();
 
+	register_trigger_cmds();
+
 	return 0;
 }
 

commit 3ccb01239201af06a07482ec686b14cd148102a5
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Tue Dec 3 12:41:20 2013 -0500

    tracing: Only run synchronize_sched() at instance deletion time
    
    It has been reported that boot up with FTRACE_SELFTEST enabled can take a
    very long time. There can be stalls of over a minute.
    
    This was tracked down to the synchronize_sched() called when a system call
    event is disabled. As the self tests enable and disable thousands of events,
    this makes the synchronize_sched() get called thousands of times.
    
    The synchornize_sched() was added with d562aff93bfb53 "tracing: Add support
    for SOFT_DISABLE to syscall events" which caused this regression (added
    in 3.13-rc1).
    
    The synchronize_sched() is to protect against the events being accessed
    when a tracer instance is being deleted. When an instance is being deleted
    all the events associated to it are unregistered. The synchronize_sched()
    makes sure that no more users are running when it finishes.
    
    Instead of calling synchronize_sched() for all syscall events, we only
    need to call it once, after the events are unregistered and before the
    instance is deleted. The event_mutex is held during this action to
    prevent new users from enabling events.
    
    Link: http://lkml.kernel.org/r/20131203124120.427b9661@gandalf.local.home
    
    Reported-by: Petr Mladek <pmladek@suse.cz>
    Acked-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Acked-by: Petr Mladek <pmladek@suse.cz>
    Tested-by: Petr Mladek <pmladek@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f919a2e21bf3..a11800ae96de 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2314,6 +2314,9 @@ int event_trace_del_tracer(struct trace_array *tr)
 	/* Disable any running events */
 	__ftrace_set_clr_event_nolock(tr, NULL, NULL, NULL, 0);
 
+	/* Access to events are within rcu_read_lock_sched() */
+	synchronize_sched();
+
 	down_write(&trace_event_sem);
 	__trace_remove_event_dirs(tr);
 	debugfs_remove_recursive(tr->event_dir);

commit d6d3523caab75196560c85aae80cb8f4a8e380ea
Author: Geyslan G. Bem <geyslan@gmail.com>
Date:   Wed Nov 6 16:02:51 2013 -0300

    tracing: Do not assign filp->private_data to freed memory
    
    In system_tr_open(), the filp->private_data can be assigned the 'dir'
    variable even if it was freed. This is on the error path, and is
    harmless because the error return code will prevent filp->private_data
    from being used. But for correctness, we should not assign it to
    a recently freed variable, as that can cause static tools to give
    false warnings.
    
    Also have both subsystem_open() and system_tr_open() return -ENODEV
    if tracing has been disabled.
    
    Link: http://lkml.kernel.org/r/1383764571-7318-1-git-send-email-geyslan@gmail.com
    
    Signed-off-by: Geyslan G. Bem <geyslan@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 043f833246a0..f919a2e21bf3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1062,6 +1062,9 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 	struct trace_array *tr;
 	int ret;
 
+	if (tracing_is_disabled())
+		return -ENODEV;
+
 	/* Make sure the system still exists */
 	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
@@ -1108,6 +1111,9 @@ static int system_tr_open(struct inode *inode, struct file *filp)
 	struct trace_array *tr = inode->i_private;
 	int ret;
 
+	if (tracing_is_disabled())
+		return -ENODEV;
+
 	if (trace_array_get(tr) < 0)
 		return -ENODEV;
 
@@ -1124,11 +1130,12 @@ static int system_tr_open(struct inode *inode, struct file *filp)
 	if (ret < 0) {
 		trace_array_put(tr);
 		kfree(dir);
+		return ret;
 	}
 
 	filp->private_data = dir;
 
-	return ret;
+	return 0;
 }
 
 static int subsystem_release(struct inode *inode, struct file *file)

commit f306cc82a93d6b19f01634b80c580b9755c8b7cc
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Thu Oct 24 08:34:17 2013 -0500

    tracing: Update event filters for multibuffer
    
    The trace event filters are still tied to event calls rather than
    event files, which means you don't get what you'd expect when using
    filters in the multibuffer case:
    
    Before:
    
      # echo 'bytes_alloc > 8192' > /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 8192
      # mkdir /sys/kernel/debug/tracing/instances/test1
      # echo 'bytes_alloc > 2048' > /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 2048
      # cat /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      bytes_alloc > 2048
    
    Setting the filter in tracing/instances/test1/events shouldn't affect
    the same event in tracing/events as it does above.
    
    After:
    
      # echo 'bytes_alloc > 8192' > /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 8192
      # mkdir /sys/kernel/debug/tracing/instances/test1
      # echo 'bytes_alloc > 2048' > /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      # cat /sys/kernel/debug/tracing/events/kmem/kmalloc/filter
      bytes_alloc > 8192
      # cat /sys/kernel/debug/tracing/instances/test1/events/kmem/kmalloc/filter
      bytes_alloc > 2048
    
    We'd like to just move the filter directly from ftrace_event_call to
    ftrace_event_file, but there are a couple cases that don't yet have
    multibuffer support and therefore have to continue using the current
    event_call-based filters.  For those cases, a new USE_CALL_FILTER bit
    is added to the event_call flags, whose main purpose is to keep the
    old behavior for those cases until they can be updated with
    multibuffer support; at that point, the USE_CALL_FILTER flag (and the
    new associated call_filter_check_discard() function) can go away.
    
    The multibuffer support also made filter_current_check_discard()
    redundant, so this change removes that function as well and replaces
    it with filter_check_discard() (or call_filter_check_discard() as
    appropriate).
    
    Link: http://lkml.kernel.org/r/f16e9ce4270c62f46b2e966119225e1c3cca7e60.1382620672.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 368a4d50cc30..043f833246a0 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -989,7 +989,7 @@ static ssize_t
 event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
-	struct ftrace_event_call *call;
+	struct ftrace_event_file *file;
 	struct trace_seq *s;
 	int r = -ENODEV;
 
@@ -1004,12 +1004,12 @@ event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 	trace_seq_init(s);
 
 	mutex_lock(&event_mutex);
-	call = event_file_data(filp);
-	if (call)
-		print_event_filter(call, s);
+	file = event_file_data(filp);
+	if (file)
+		print_event_filter(file, s);
 	mutex_unlock(&event_mutex);
 
-	if (call)
+	if (file)
 		r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
@@ -1021,7 +1021,7 @@ static ssize_t
 event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
-	struct ftrace_event_call *call;
+	struct ftrace_event_file *file;
 	char *buf;
 	int err = -ENODEV;
 
@@ -1039,9 +1039,9 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	buf[cnt] = '\0';
 
 	mutex_lock(&event_mutex);
-	call = event_file_data(filp);
-	if (call)
-		err = apply_event_filter(call, buf);
+	file = event_file_data(filp);
+	if (file)
+		err = apply_event_filter(file, buf);
 	mutex_unlock(&event_mutex);
 
 	free_page((unsigned long) buf);
@@ -1539,7 +1539,7 @@ event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 			return -1;
 		}
 	}
-	trace_create_file("filter", 0644, file->dir, call,
+	trace_create_file("filter", 0644, file->dir, file,
 			  &ftrace_event_filter_fops);
 
 	trace_create_file("format", 0444, file->dir, call,
@@ -1577,6 +1577,7 @@ static void event_remove(struct ftrace_event_call *call)
 		if (file->event_call != call)
 			continue;
 		ftrace_event_enable_disable(file, 0);
+		destroy_preds(file);
 		/*
 		 * The do_for_each_event_file() is
 		 * a double loop. After finding the call for this
@@ -1700,7 +1701,7 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 {
 	event_remove(call);
 	trace_destroy_fields(call);
-	destroy_preds(call);
+	destroy_call_preds(call);
 }
 
 static int probe_remove_event_call(struct ftrace_event_call *call)

commit 836d481ed7c91152c6144ea3a3363cad3940b3e0
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 31 19:31:37 2013 +0200

    tracing: Kill the !CONFIG_MODULES code in trace_events.c
    
    Move trace_module_nb under CONFIG_MODULES and kill the dummy
    trace_module_notify(). Imho it doesn't make sense to define
    "struct notifier_block" and its .notifier_call just to avoid
    "ifdef" in event_trace_init(), and all other !CONFIG_MODULES
    code has already gone away.
    
    Link: http://lkml.kernel.org/r/20130731173137.GA31043@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 4e706a01f1f9..368a4d50cc30 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1820,12 +1820,10 @@ static int trace_module_notify(struct notifier_block *self,
 	return 0;
 }
 
-#else
-static inline int trace_module_notify(struct notifier_block *self,
-				      unsigned long val, void *data)
-{
-	return 0;
-}
+static struct notifier_block trace_module_nb = {
+	.notifier_call = trace_module_notify,
+	.priority = 0,
+};
 #endif /* CONFIG_MODULES */
 
 /* Create a new event directory structure for a trace directory. */
@@ -2193,11 +2191,6 @@ static void __add_event_to_tracers(struct ftrace_event_call *call)
 		__trace_add_new_event(call, tr);
 }
 
-static struct notifier_block trace_module_nb = {
-	.notifier_call = trace_module_notify,
-	.priority = 0,
-};
-
 extern struct ftrace_event_call *__start_ftrace_events[];
 extern struct ftrace_event_call *__stop_ftrace_events[];
 
@@ -2402,10 +2395,11 @@ static __init int event_trace_init(void)
 	if (ret)
 		return ret;
 
+#ifdef CONFIG_MODULES
 	ret = register_module_notifier(&trace_module_nb);
 	if (ret)
 		pr_warning("Failed to register trace events module notifier\n");
-
+#endif
 	return 0;
 }
 early_initcall(event_trace_memsetup);

commit 620a30e97febc8332590376c94ed0e9dba522bc8
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 31 19:31:35 2013 +0200

    tracing: Don't pass file_operations array to event_create_dir()
    
    Now that event_create_dir() and __trace_add_new_event() always
    use the same file_operations we can kill these arguments and
    simplify the code.
    
    Link: http://lkml.kernel.org/r/20130731173135.GA31040@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2ec82734b8a7..4e706a01f1f9 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1489,12 +1489,7 @@ event_subsystem_dir(struct trace_array *tr, const char *name,
 }
 
 static int
-event_create_dir(struct dentry *parent,
-		 struct ftrace_event_file *file,
-		 const struct file_operations *id,
-		 const struct file_operations *enable,
-		 const struct file_operations *filter,
-		 const struct file_operations *format)
+event_create_dir(struct dentry *parent, struct ftrace_event_file *file)
 {
 	struct ftrace_event_call *call = file->event_call;
 	struct trace_array *tr = file->tr;
@@ -1522,12 +1517,13 @@ event_create_dir(struct dentry *parent,
 
 	if (call->class->reg && !(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))
 		trace_create_file("enable", 0644, file->dir, file,
-				  enable);
+				  &ftrace_enable_fops);
 
 #ifdef CONFIG_PERF_EVENTS
 	if (call->event.type && call->class->reg)
 		trace_create_file("id", 0444, file->dir,
-				  (void *)(long)call->event.type, id);
+				  (void *)(long)call->event.type,
+				  &ftrace_event_id_fops);
 #endif
 
 	/*
@@ -1544,10 +1540,10 @@ event_create_dir(struct dentry *parent,
 		}
 	}
 	trace_create_file("filter", 0644, file->dir, call,
-			  filter);
+			  &ftrace_event_filter_fops);
 
 	trace_create_file("format", 0444, file->dir, call,
-			  format);
+			  &ftrace_event_format_fops);
 
 	return 0;
 }
@@ -1648,12 +1644,7 @@ trace_create_new_event(struct ftrace_event_call *call,
 
 /* Add an event to a trace directory */
 static int
-__trace_add_new_event(struct ftrace_event_call *call,
-		      struct trace_array *tr,
-		      const struct file_operations *id,
-		      const struct file_operations *enable,
-		      const struct file_operations *filter,
-		      const struct file_operations *format)
+__trace_add_new_event(struct ftrace_event_call *call, struct trace_array *tr)
 {
 	struct ftrace_event_file *file;
 
@@ -1661,7 +1652,7 @@ __trace_add_new_event(struct ftrace_event_call *call,
 	if (!file)
 		return -ENOMEM;
 
-	return event_create_dir(tr->event_dir, file, id, enable, filter, format);
+	return event_create_dir(tr->event_dir, file);
 }
 
 /*
@@ -1845,11 +1836,7 @@ __trace_add_event_dirs(struct trace_array *tr)
 	int ret;
 
 	list_for_each_entry(call, &ftrace_events, list) {
-		ret = __trace_add_new_event(call, tr,
-					    &ftrace_event_id_fops,
-					    &ftrace_enable_fops,
-					    &ftrace_event_filter_fops,
-					    &ftrace_event_format_fops);
+		ret = __trace_add_new_event(call, tr);
 		if (ret < 0)
 			pr_warning("Could not create directory for event %s\n",
 				   call->name);
@@ -2157,11 +2144,7 @@ __trace_early_add_event_dirs(struct trace_array *tr)
 
 
 	list_for_each_entry(file, &tr->events, list) {
-		ret = event_create_dir(tr->event_dir, file,
-				       &ftrace_event_id_fops,
-				       &ftrace_enable_fops,
-				       &ftrace_event_filter_fops,
-				       &ftrace_event_format_fops);
+		ret = event_create_dir(tr->event_dir, file);
 		if (ret < 0)
 			pr_warning("Could not create directory for event %s\n",
 				   file->event_call->name);
@@ -2206,13 +2189,8 @@ static void __add_event_to_tracers(struct ftrace_event_call *call)
 {
 	struct trace_array *tr;
 
-	list_for_each_entry(tr, &ftrace_trace_arrays, list) {
-		__trace_add_new_event(call, tr,
-				      &ftrace_event_id_fops,
-				      &ftrace_enable_fops,
-				      &ftrace_event_filter_fops,
-				      &ftrace_event_format_fops);
-	}
+	list_for_each_entry(tr, &ftrace_trace_arrays, list)
+		__trace_add_new_event(call, tr);
 }
 
 static struct notifier_block trace_module_nb = {

commit 779c5e379158de3e96112630c543d3c7b37efab9
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Wed Jul 31 19:31:32 2013 +0200

    tracing: Kill trace_create_file_ops() and friends
    
    trace_create_file_ops() allocates the copy of id/filter/format/enable
    file_operations to set "f_op->owner = mod" for fops_get().
    
    However after the recent changes there is no reason to prevent rmmod
    even if one of these files is opened. A file operation can do nothing
    but fail after remove_event_file_dir() clears ->i_private for every
    file removed by trace_module_remove_events().
    
    Kill "struct ftrace_module_file_ops" and fix the compilation errors.
    
    Link: http://lkml.kernel.org/r/20130731173132.GA31033@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 29a7ebcfb426..2ec82734b8a7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1683,8 +1683,7 @@ __trace_early_add_new_event(struct ftrace_event_call *call,
 }
 
 struct ftrace_module_file_ops;
-static void __add_event_to_tracers(struct ftrace_event_call *call,
-				   struct ftrace_module_file_ops *file_ops);
+static void __add_event_to_tracers(struct ftrace_event_call *call);
 
 /* Add an additional event_call dynamically */
 int trace_add_event_call(struct ftrace_event_call *call)
@@ -1695,7 +1694,7 @@ int trace_add_event_call(struct ftrace_event_call *call)
 
 	ret = __register_event(call, NULL);
 	if (ret >= 0)
-		__add_event_to_tracers(call, NULL);
+		__add_event_to_tracers(call);
 
 	mutex_unlock(&event_mutex);
 	mutex_unlock(&trace_types_lock);
@@ -1769,100 +1768,21 @@ int trace_remove_event_call(struct ftrace_event_call *call)
 
 #ifdef CONFIG_MODULES
 
-static LIST_HEAD(ftrace_module_file_list);
-
-/*
- * Modules must own their file_operations to keep up with
- * reference counting.
- */
-struct ftrace_module_file_ops {
-	struct list_head		list;
-	struct module			*mod;
-	struct file_operations		id;
-	struct file_operations		enable;
-	struct file_operations		format;
-	struct file_operations		filter;
-};
-
-static struct ftrace_module_file_ops *
-find_ftrace_file_ops(struct ftrace_module_file_ops *file_ops, struct module *mod)
-{
-	/*
-	 * As event_calls are added in groups by module,
-	 * when we find one file_ops, we don't need to search for
-	 * each call in that module, as the rest should be the
-	 * same. Only search for a new one if the last one did
-	 * not match.
-	 */
-	if (file_ops && mod == file_ops->mod)
-		return file_ops;
-
-	list_for_each_entry(file_ops, &ftrace_module_file_list, list) {
-		if (file_ops->mod == mod)
-			return file_ops;
-	}
-	return NULL;
-}
-
-static struct ftrace_module_file_ops *
-trace_create_file_ops(struct module *mod)
-{
-	struct ftrace_module_file_ops *file_ops;
-
-	/*
-	 * This is a bit of a PITA. To allow for correct reference
-	 * counting, modules must "own" their file_operations.
-	 * To do this, we allocate the file operations that will be
-	 * used in the event directory.
-	 */
-
-	file_ops = kmalloc(sizeof(*file_ops), GFP_KERNEL);
-	if (!file_ops)
-		return NULL;
-
-	file_ops->mod = mod;
-
-	file_ops->id = ftrace_event_id_fops;
-	file_ops->id.owner = mod;
-
-	file_ops->enable = ftrace_enable_fops;
-	file_ops->enable.owner = mod;
-
-	file_ops->filter = ftrace_event_filter_fops;
-	file_ops->filter.owner = mod;
-
-	file_ops->format = ftrace_event_format_fops;
-	file_ops->format.owner = mod;
-
-	list_add(&file_ops->list, &ftrace_module_file_list);
-
-	return file_ops;
-}
-
 static void trace_module_add_events(struct module *mod)
 {
-	struct ftrace_module_file_ops *file_ops = NULL;
 	struct ftrace_event_call **call, **start, **end;
 
 	start = mod->trace_events;
 	end = mod->trace_events + mod->num_trace_events;
 
-	if (start == end)
-		return;
-
-	file_ops = trace_create_file_ops(mod);
-	if (!file_ops)
-		return;
-
 	for_each_event(call, start, end) {
 		__register_event(*call, mod);
-		__add_event_to_tracers(*call, file_ops);
+		__add_event_to_tracers(*call);
 	}
 }
 
 static void trace_module_remove_events(struct module *mod)
 {
-	struct ftrace_module_file_ops *file_ops;
 	struct ftrace_event_call *call, *p;
 	bool clear_trace = false;
 
@@ -1874,16 +1794,6 @@ static void trace_module_remove_events(struct module *mod)
 			__trace_remove_event_call(call);
 		}
 	}
-
-	/* Now free the file_operations */
-	list_for_each_entry(file_ops, &ftrace_module_file_list, list) {
-		if (file_ops->mod == mod)
-			break;
-	}
-	if (&file_ops->list != &ftrace_module_file_list) {
-		list_del(&file_ops->list);
-		kfree(file_ops);
-	}
 	up_write(&trace_event_sem);
 
 	/*
@@ -1919,62 +1829,22 @@ static int trace_module_notify(struct notifier_block *self,
 	return 0;
 }
 
-static int
-__trace_add_new_mod_event(struct ftrace_event_call *call,
-			  struct trace_array *tr,
-			  struct ftrace_module_file_ops *file_ops)
-{
-	return __trace_add_new_event(call, tr,
-				     &file_ops->id, &file_ops->enable,
-				     &file_ops->filter, &file_ops->format);
-}
-
 #else
-static inline struct ftrace_module_file_ops *
-find_ftrace_file_ops(struct ftrace_module_file_ops *file_ops, struct module *mod)
-{
-	return NULL;
-}
 static inline int trace_module_notify(struct notifier_block *self,
 				      unsigned long val, void *data)
 {
 	return 0;
 }
-static inline int
-__trace_add_new_mod_event(struct ftrace_event_call *call,
-			  struct trace_array *tr,
-			  struct ftrace_module_file_ops *file_ops)
-{
-	return -ENODEV;
-}
 #endif /* CONFIG_MODULES */
 
 /* Create a new event directory structure for a trace directory. */
 static void
 __trace_add_event_dirs(struct trace_array *tr)
 {
-	struct ftrace_module_file_ops *file_ops = NULL;
 	struct ftrace_event_call *call;
 	int ret;
 
 	list_for_each_entry(call, &ftrace_events, list) {
-		if (call->mod) {
-			/*
-			 * Directories for events by modules need to
-			 * keep module ref counts when opened (as we don't
-			 * want the module to disappear when reading one
-			 * of these files). The file_ops keep account of
-			 * the module ref count.
-			 */
-			file_ops = find_ftrace_file_ops(file_ops, call->mod);
-			if (!file_ops)
-				continue; /* Warn? */
-			ret = __trace_add_new_mod_event(call, tr, file_ops);
-			if (ret < 0)
-				pr_warning("Could not create directory for event %s\n",
-					   call->name);
-			continue;
-		}
 		ret = __trace_add_new_event(call, tr,
 					    &ftrace_event_id_fops,
 					    &ftrace_enable_fops,
@@ -2332,21 +2202,16 @@ __trace_remove_event_dirs(struct trace_array *tr)
 		remove_event_file_dir(file);
 }
 
-static void
-__add_event_to_tracers(struct ftrace_event_call *call,
-		       struct ftrace_module_file_ops *file_ops)
+static void __add_event_to_tracers(struct ftrace_event_call *call)
 {
 	struct trace_array *tr;
 
 	list_for_each_entry(tr, &ftrace_trace_arrays, list) {
-		if (file_ops)
-			__trace_add_new_mod_event(call, tr, file_ops);
-		else
-			__trace_add_new_event(call, tr,
-					      &ftrace_event_id_fops,
-					      &ftrace_enable_fops,
-					      &ftrace_event_filter_fops,
-					      &ftrace_event_format_fops);
+		__trace_add_new_event(call, tr,
+				      &ftrace_event_id_fops,
+				      &ftrace_enable_fops,
+				      &ftrace_event_filter_fops,
+				      &ftrace_event_format_fops);
 	}
 }
 

commit 2ba64035d0ca966fd189bc3e0826343fc81bf482
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Jul 31 13:16:22 2013 -0400

    tracing: Add comment to describe special break case in probe_remove_event_call()
    
    The "break" used in the do_for_each_event_file() is used as an optimization
    as the loop is really a double loop. The loop searches all event files
    for each trace_array. There's only one matching event file per trace_array
    and after we find the event file for the trace_array, the break is used
    to jump to the next trace_array and start the search there.
    
    As this is not a standard way of using "break" in C code, it requires
    a comment right before the break to let people know what is going on.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ec04836273c0..29a7ebcfb426 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1732,6 +1732,12 @@ static int probe_remove_event_call(struct ftrace_event_call *call)
 		 */
 		if (file->flags & FTRACE_EVENT_FL_ENABLED)
 			return -EBUSY;
+		/*
+		 * The do_for_each_event_file_safe() is
+		 * a double loop. After finding the call for this
+		 * trace_array, we use break to jump to the next
+		 * trace_array.
+		 */
 		break;
 	} while_for_each_event_file();
 

commit 2816c551c796ec14620325b2c9ed75b9979d3125
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Mon Jul 29 19:50:33 2013 +0200

    tracing: trace_remove_event_call() should fail if call/file is in use
    
    Change trace_remove_event_call(call) to return the error if this
    call is active. This is what the callers assume but can't verify
    outside of the tracing locks. Both trace_kprobe.c/trace_uprobe.c
    need the additional changes, unregister_trace_probe() should abort
    if trace_remove_event_call() fails.
    
    The caller is going to free this call/file so we must ensure that
    nobody can use them after trace_remove_event_call() succeeds.
    debugfs should be fine after the previous changes and event_remove()
    does TRACE_REG_UNREGISTER, but still there are 2 reasons why we need
    the additional checks:
    
    - There could be a perf_event(s) attached to this tp_event, so the
      patch checks ->perf_refcount.
    
    - TRACE_REG_UNREGISTER can be suppressed by FTRACE_EVENT_FL_SOFT_MODE,
      so we simply check FTRACE_EVENT_FL_ENABLED protected by event_mutex.
    
    Link: http://lkml.kernel.org/r/20130729175033.GB26284@redhat.com
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a67c913e2f9f..ec04836273c0 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1713,16 +1713,47 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 	destroy_preds(call);
 }
 
+static int probe_remove_event_call(struct ftrace_event_call *call)
+{
+	struct trace_array *tr;
+	struct ftrace_event_file *file;
+
+#ifdef CONFIG_PERF_EVENTS
+	if (call->perf_refcount)
+		return -EBUSY;
+#endif
+	do_for_each_event_file(tr, file) {
+		if (file->event_call != call)
+			continue;
+		/*
+		 * We can't rely on ftrace_event_enable_disable(enable => 0)
+		 * we are going to do, FTRACE_EVENT_FL_SOFT_MODE can suppress
+		 * TRACE_REG_UNREGISTER.
+		 */
+		if (file->flags & FTRACE_EVENT_FL_ENABLED)
+			return -EBUSY;
+		break;
+	} while_for_each_event_file();
+
+	__trace_remove_event_call(call);
+
+	return 0;
+}
+
 /* Remove an event_call */
-void trace_remove_event_call(struct ftrace_event_call *call)
+int trace_remove_event_call(struct ftrace_event_call *call)
 {
+	int ret;
+
 	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
 	down_write(&trace_event_sem);
-	__trace_remove_event_call(call);
+	ret = probe_remove_event_call(call);
 	up_write(&trace_event_sem);
 	mutex_unlock(&event_mutex);
 	mutex_unlock(&trace_types_lock);
+
+	return ret;
 }
 
 #define for_each_event(event, start, end)			\

commit bf682c3159c4d298d1126a56793ed3f5e80395f7
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Sun Jul 28 20:35:27 2013 +0200

    tracing: Change remove_event_file_dir() to clear "d_subdirs"->i_private
    
    Change remove_event_file_dir() to clear ->i_private for every
    file we are going to remove.
    
    We need to check file->dir != NULL because event_create_dir()
    can fail. debugfs_remove_recursive(NULL) is fine but the patch
    moves it under the same check anyway for readability.
    
    spin_lock(d_lock) and "d_inode != NULL" check are not needed
    afaics, but I do not understand this code enough.
    
    tracing_open_generic_file() and tracing_release_generic_file()
    can go away, ftrace_enable_fops and ftrace_event_filter_fops()
    use tracing_open_generic() but only to check tracing_disabled.
    
    This fixes all races with event_remove() or instance_delete().
    f_op->read/write/whatever can never use the freed file/call,
    all event/* files were changed to check and use ->i_private
    under event_mutex.
    
    Note: this doesn't not fix other problems, event_remove() can
    destroy the active ftrace_event_call, we need more changes but
    those changes are completely orthogonal.
    
    Link: http://lkml.kernel.org/r/20130728183527.GB16723@redhat.com
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 05d647ecd01a..a67c913e2f9f 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -428,41 +428,25 @@ static void *event_file_data(struct file *filp)
 
 static void remove_event_file_dir(struct ftrace_event_file *file)
 {
+	struct dentry *dir = file->dir;
+	struct dentry *child;
+
+	if (dir) {
+		spin_lock(&dir->d_lock);	/* probably unneeded */
+		list_for_each_entry(child, &dir->d_subdirs, d_u.d_child) {
+			if (child->d_inode)	/* probably unneeded */
+				child->d_inode->i_private = NULL;
+		}
+		spin_unlock(&dir->d_lock);
+
+		debugfs_remove_recursive(dir);
+	}
+
 	list_del(&file->list);
-	debugfs_remove_recursive(file->dir);
 	remove_subsystem(file->system);
 	kmem_cache_free(file_cachep, file);
 }
 
-/*
- * Open and update trace_array ref count.
- * Must have the current trace_array passed to it.
- */
-static int tracing_open_generic_file(struct inode *inode, struct file *filp)
-{
-	struct ftrace_event_file *file = inode->i_private;
-	struct trace_array *tr = file->tr;
-	int ret;
-
-	if (trace_array_get(tr) < 0)
-		return -ENODEV;
-
-	ret = tracing_open_generic(inode, filp);
-	if (ret < 0)
-		trace_array_put(tr);
-	return ret;
-}
-
-static int tracing_release_generic_file(struct inode *inode, struct file *filp)
-{
-	struct ftrace_event_file *file = inode->i_private;
-	struct trace_array *tr = file->tr;
-
-	trace_array_put(tr);
-
-	return 0;
-}
-
 /*
  * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.
  */
@@ -1281,10 +1265,9 @@ static const struct file_operations ftrace_set_event_fops = {
 };
 
 static const struct file_operations ftrace_enable_fops = {
-	.open = tracing_open_generic_file,
+	.open = tracing_open_generic,
 	.read = event_enable_read,
 	.write = event_enable_write,
-	.release = tracing_release_generic_file,
 	.llseek = default_llseek,
 };
 

commit f6a84bdc75b5c11621dec58db73fe102cbaf40cc
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 26 19:25:47 2013 +0200

    tracing: Introduce remove_event_file_dir()
    
    Preparation for the next patch. Extract the common code from
    remove_event_from_tracers() and __trace_remove_event_dirs()
    into the new helper, remove_event_file_dir().
    
    The patch looks more complicated than it actually is, it also
    moves remove_subsystem() up to avoid the forward declaration.
    
    Link: http://lkml.kernel.org/r/20130726172547.GA3629@redhat.com
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 50dc8b2e5435..05d647ecd01a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -409,11 +409,31 @@ static void put_system(struct ftrace_subsystem_dir *dir)
 	mutex_unlock(&event_mutex);
 }
 
+static void remove_subsystem(struct ftrace_subsystem_dir *dir)
+{
+	if (!dir)
+		return;
+
+	if (!--dir->nr_events) {
+		debugfs_remove_recursive(dir->entry);
+		list_del(&dir->list);
+		__put_system_dir(dir);
+	}
+}
+
 static void *event_file_data(struct file *filp)
 {
 	return ACCESS_ONCE(file_inode(filp)->i_private);
 }
 
+static void remove_event_file_dir(struct ftrace_event_file *file)
+{
+	list_del(&file->list);
+	debugfs_remove_recursive(file->dir);
+	remove_subsystem(file->system);
+	kmem_cache_free(file_cachep, file);
+}
+
 /*
  * Open and update trace_array ref count.
  * Must have the current trace_array passed to it.
@@ -1549,33 +1569,16 @@ event_create_dir(struct dentry *parent,
 	return 0;
 }
 
-static void remove_subsystem(struct ftrace_subsystem_dir *dir)
-{
-	if (!dir)
-		return;
-
-	if (!--dir->nr_events) {
-		debugfs_remove_recursive(dir->entry);
-		list_del(&dir->list);
-		__put_system_dir(dir);
-	}
-}
-
 static void remove_event_from_tracers(struct ftrace_event_call *call)
 {
 	struct ftrace_event_file *file;
 	struct trace_array *tr;
 
 	do_for_each_event_file_safe(tr, file) {
-
 		if (file->event_call != call)
 			continue;
 
-		list_del(&file->list);
-		debugfs_remove_recursive(file->dir);
-		remove_subsystem(file->system);
-		kmem_cache_free(file_cachep, file);
-
+		remove_event_file_dir(file);
 		/*
 		 * The do_for_each_event_file_safe() is
 		 * a double loop. After finding the call for this
@@ -2305,12 +2308,8 @@ __trace_remove_event_dirs(struct trace_array *tr)
 {
 	struct ftrace_event_file *file, *next;
 
-	list_for_each_entry_safe(file, next, &tr->events, list) {
-		list_del(&file->list);
-		debugfs_remove_recursive(file->dir);
-		remove_subsystem(file->system);
-		kmem_cache_free(file_cachep, file);
-	}
+	list_for_each_entry_safe(file, next, &tr->events, list)
+		remove_event_file_dir(file);
 }
 
 static void

commit c5a44a1200c6eda2202434f25325e8ad19533fca
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 26 19:25:43 2013 +0200

    tracing: Change f_start() to take event_mutex and verify i_private != NULL
    
    trace_format_open() and trace_format_seq_ops are racy, nothing
    protects ftrace_event_call from trace_remove_event_call().
    
    Change f_start() to take event_mutex and verify i_private != NULL,
    change f_stop() to drop this lock.
    
    This fixes nothing, but now we can change debugfs_remove("format")
    callers to nullify ->i_private and fix the the problem.
    
    Note: the usage of event_mutex is sub-optimal but simple, we can
    change this later.
    
    Link: http://lkml.kernel.org/r/20130726172543.GA3622@redhat.com
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1d7b6d03cd51..50dc8b2e5435 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -840,7 +840,7 @@ enum {
 
 static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_call *call = m->private;
+	struct ftrace_event_call *call = event_file_data(m->private);
 	struct list_head *common_head = &ftrace_common_fields;
 	struct list_head *head = trace_get_fields(call);
 	struct list_head *node = v;
@@ -872,7 +872,7 @@ static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 
 static int f_show(struct seq_file *m, void *v)
 {
-	struct ftrace_event_call *call = m->private;
+	struct ftrace_event_call *call = event_file_data(m->private);
 	struct ftrace_event_field *field;
 	const char *array_descriptor;
 
@@ -925,6 +925,11 @@ static void *f_start(struct seq_file *m, loff_t *pos)
 	void *p = (void *)FORMAT_HEADER;
 	loff_t l = 0;
 
+	/* ->stop() is called even if ->start() fails */
+	mutex_lock(&event_mutex);
+	if (!event_file_data(m->private))
+		return ERR_PTR(-ENODEV);
+
 	while (l < *pos && p)
 		p = f_next(m, p, &l);
 
@@ -933,6 +938,7 @@ static void *f_start(struct seq_file *m, loff_t *pos)
 
 static void f_stop(struct seq_file *m, void *p)
 {
+	mutex_unlock(&event_mutex);
 }
 
 static const struct seq_operations trace_format_seq_ops = {
@@ -944,7 +950,6 @@ static const struct seq_operations trace_format_seq_ops = {
 
 static int trace_format_open(struct inode *inode, struct file *file)
 {
-	struct ftrace_event_call *call = inode->i_private;
 	struct seq_file *m;
 	int ret;
 
@@ -953,7 +958,7 @@ static int trace_format_open(struct inode *inode, struct file *file)
 		return ret;
 
 	m = file->private_data;
-	m->private = call;
+	m->private = file;
 
 	return 0;
 }

commit e2912b091c26b8ea95e5e00a43a7ac620f6c94a6
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 26 19:25:40 2013 +0200

    tracing: Change event_filter_read/write to verify i_private != NULL
    
    event_filter_read/write() are racy, ftrace_event_call can be already
    freed by trace_remove_event_call() callers.
    
    1. Shift mutex_lock(event_mutex) from print/apply_event_filter to
       the callers.
    
    2. Change the callers, event_filter_read() and event_filter_write()
       to read i_private under this mutex and abort if it is NULL.
    
    This fixes nothing, but now we can change debugfs_remove("filter")
    callers to nullify ->i_private and fix the the problem.
    
    Link: http://lkml.kernel.org/r/20130726172540.GA3619@redhat.com
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 3dfa8419d0dc..1d7b6d03cd51 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -980,21 +980,28 @@ static ssize_t
 event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
-	struct ftrace_event_call *call = filp->private_data;
+	struct ftrace_event_call *call;
 	struct trace_seq *s;
-	int r;
+	int r = -ENODEV;
 
 	if (*ppos)
 		return 0;
 
 	s = kmalloc(sizeof(*s), GFP_KERNEL);
+
 	if (!s)
 		return -ENOMEM;
 
 	trace_seq_init(s);
 
-	print_event_filter(call, s);
-	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
+	mutex_lock(&event_mutex);
+	call = event_file_data(filp);
+	if (call)
+		print_event_filter(call, s);
+	mutex_unlock(&event_mutex);
+
+	if (call)
+		r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
 
@@ -1005,9 +1012,9 @@ static ssize_t
 event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
-	struct ftrace_event_call *call = filp->private_data;
+	struct ftrace_event_call *call;
 	char *buf;
-	int err;
+	int err = -ENODEV;
 
 	if (cnt >= PAGE_SIZE)
 		return -EINVAL;
@@ -1022,7 +1029,12 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	}
 	buf[cnt] = '\0';
 
-	err = apply_event_filter(call, buf);
+	mutex_lock(&event_mutex);
+	call = event_file_data(filp);
+	if (call)
+		err = apply_event_filter(call, buf);
+	mutex_unlock(&event_mutex);
+
 	free_page((unsigned long) buf);
 	if (err < 0)
 		return err;

commit bc6f6b08dee5645770efb4b76186ded313f23752
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 26 19:25:36 2013 +0200

    tracing: Change event_enable/disable_read() to verify i_private != NULL
    
    tracing_open_generic_file() is racy, ftrace_event_file can be
    already freed by rmdir or trace_remove_event_call().
    
    Change event_enable_read() and event_disable_read() to read and
    verify "file = i_private" under event_mutex.
    
    This fixes nothing, but now we can change debugfs_remove("enable")
    callers to nullify ->i_private and fix the the problem.
    
    Link: http://lkml.kernel.org/r/20130726172536.GA3612@redhat.com
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c2d13c528c3c..3dfa8419d0dc 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -684,15 +684,25 @@ static ssize_t
 event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
-	struct ftrace_event_file *file = filp->private_data;
+	struct ftrace_event_file *file;
+	unsigned long flags;
 	char buf[4] = "0";
 
-	if (file->flags & FTRACE_EVENT_FL_ENABLED &&
-	    !(file->flags & FTRACE_EVENT_FL_SOFT_DISABLED))
+	mutex_lock(&event_mutex);
+	file = event_file_data(filp);
+	if (likely(file))
+		flags = file->flags;
+	mutex_unlock(&event_mutex);
+
+	if (!file)
+		return -ENODEV;
+
+	if (flags & FTRACE_EVENT_FL_ENABLED &&
+	    !(flags & FTRACE_EVENT_FL_SOFT_DISABLED))
 		strcpy(buf, "1");
 
-	if (file->flags & FTRACE_EVENT_FL_SOFT_DISABLED ||
-	    file->flags & FTRACE_EVENT_FL_SOFT_MODE)
+	if (flags & FTRACE_EVENT_FL_SOFT_DISABLED ||
+	    flags & FTRACE_EVENT_FL_SOFT_MODE)
 		strcat(buf, "*");
 
 	strcat(buf, "\n");
@@ -704,13 +714,10 @@ static ssize_t
 event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
-	struct ftrace_event_file *file = filp->private_data;
+	struct ftrace_event_file *file;
 	unsigned long val;
 	int ret;
 
-	if (!file)
-		return -EINVAL;
-
 	ret = kstrtoul_from_user(ubuf, cnt, 10, &val);
 	if (ret)
 		return ret;
@@ -722,8 +729,11 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	switch (val) {
 	case 0:
 	case 1:
+		ret = -ENODEV;
 		mutex_lock(&event_mutex);
-		ret = ftrace_event_enable_disable(file, val);
+		file = event_file_data(filp);
+		if (likely(file))
+			ret = ftrace_event_enable_disable(file, val);
 		mutex_unlock(&event_mutex);
 		break;
 

commit 1a11126bcb7c93c289bf3218fa546fd3b0c0df8b
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Fri Jul 26 19:25:32 2013 +0200

    tracing: Turn event/id->i_private into call->event.type
    
    event_id_read() is racy, ftrace_event_call can be already freed
    by trace_remove_event_call() callers.
    
    Change event_create_dir() to pass "data = call->event.type", this
    is all event_id_read() needs. ftrace_event_id_fops no longer needs
    tracing_open_generic().
    
    We add the new helper, event_file_data(), to read ->i_private, it
    will have more users.
    
    Note: currently ACCESS_ONCE() and "id != 0" check are not needed,
    but we are going to change event_remove/rmdir to clear ->i_private.
    
    Link: http://lkml.kernel.org/r/20130726172532.GA3605@redhat.com
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 898f868833f2..c2d13c528c3c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -409,6 +409,11 @@ static void put_system(struct ftrace_subsystem_dir *dir)
 	mutex_unlock(&event_mutex);
 }
 
+static void *event_file_data(struct file *filp)
+{
+	return ACCESS_ONCE(file_inode(filp)->i_private);
+}
+
 /*
  * Open and update trace_array ref count.
  * Must have the current trace_array passed to it.
@@ -946,14 +951,18 @@ static int trace_format_open(struct inode *inode, struct file *file)
 static ssize_t
 event_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 {
-	struct ftrace_event_call *call = filp->private_data;
+	int id = (long)event_file_data(filp);
 	char buf[32];
 	int len;
 
 	if (*ppos)
 		return 0;
 
-	len = sprintf(buf, "%d\n", call->event.type);
+	if (unlikely(!id))
+		return -ENODEV;
+
+	len = sprintf(buf, "%d\n", id);
+
 	return simple_read_from_buffer(ubuf, cnt, ppos, buf, len);
 }
 
@@ -1240,7 +1249,6 @@ static const struct file_operations ftrace_event_format_fops = {
 };
 
 static const struct file_operations ftrace_event_id_fops = {
-	.open = tracing_open_generic,
 	.read = event_id_read,
 	.llseek = default_llseek,
 };
@@ -1488,8 +1496,8 @@ event_create_dir(struct dentry *parent,
 
 #ifdef CONFIG_PERF_EVENTS
 	if (call->event.type && call->class->reg)
-		trace_create_file("id", 0444, file->dir, call,
-		 		  id);
+		trace_create_file("id", 0444, file->dir,
+				  (void *)(long)call->event.type, id);
 #endif
 
 	/*

commit cd458ba9d5a5592d37b5145e560071e91ea762ac
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jul 18 20:47:12 2013 +0200

    tracing: Do not (ab)use trace_seq in event_id_read()
    
    event_id_read() has no reason to kmalloc "struct trace_seq"
    (more than PAGE_SIZE!), it can use a small buffer instead.
    
    Note: "if (*ppos) return 0" looks strange and even wrong,
    simple_read_from_buffer() handles ppos != 0 case corrrectly.
    
    And it seems that almost every user of trace_seq in this file
    should be converted too. Unless you use seq_open(), trace_seq
    buys nothing compared to the raw buffer, but it needs a bit
    more memory and code.
    
    Link: http://lkml.kernel.org/r/20130718184712.GA4786@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 76defd91f9b4..898f868833f2 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -947,23 +947,14 @@ static ssize_t
 event_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 {
 	struct ftrace_event_call *call = filp->private_data;
-	struct trace_seq *s;
-	int r;
+	char buf[32];
+	int len;
 
 	if (*ppos)
 		return 0;
 
-	s = kmalloc(sizeof(*s), GFP_KERNEL);
-	if (!s)
-		return -ENOMEM;
-
-	trace_seq_init(s);
-	trace_seq_printf(s, "%d\n", call->event.type);
-
-	r = simple_read_from_buffer(ubuf, cnt, ppos,
-				    s->buffer, s->len);
-	kfree(s);
-	return r;
+	len = sprintf(buf, "%d\n", call->event.type);
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, len);
 }
 
 static ssize_t

commit 7710b639953b791610f0022a7d52d9801c93b969
Author: Oleg Nesterov <oleg@redhat.com>
Date:   Thu Jul 18 20:47:10 2013 +0200

    tracing: Simplify the iteration logic in f_start/f_next
    
    f_next() looks overcomplicated, and it is not strictly correct
    even if this doesn't matter.
    
    Say, FORMAT_FIELD_SEPERATOR should not return NULL (means EOF)
    if trace_get_fields() returns an empty list, we should simply
    advance to FORMAT_PRINTFMT as we do when we find the end of list.
    
    1. Change f_next() to return "struct list_head *" rather than
       "ftrace_event_field *", and change f_show() to do list_entry().
    
       This simplifies the code a bit, only f_show() needs to know
       about ftrace_event_field, and f_next() can play with ->prev
       directly
    
    2. Change f_next() to not play with ->prev / return inside the
       switch() statement. It can simply set node = head/common_head,
       the prev-or-advance-to-the-next-magic below does all work.
    
    While at it. f_start() looks overcomplicated too. I don't think
    *pos == 0 makes sense as a separate case, just change this code
    to do "while" instead of "do/while".
    
    The patch also moves f_start() down, close to f_stop(). This is
    purely cosmetic, just to make the locking added by the next patch
    more clear/visible.
    
    Link: http://lkml.kernel.org/r/20130718184710.GA4783@redhat.com
    
    Signed-off-by: Oleg Nesterov <oleg@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7a75cb22eab7..76defd91f9b4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -826,59 +826,33 @@ enum {
 static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 {
 	struct ftrace_event_call *call = m->private;
-	struct ftrace_event_field *field;
 	struct list_head *common_head = &ftrace_common_fields;
 	struct list_head *head = trace_get_fields(call);
+	struct list_head *node = v;
 
 	(*pos)++;
 
 	switch ((unsigned long)v) {
 	case FORMAT_HEADER:
-		if (unlikely(list_empty(common_head)))
-			return NULL;
-
-		field = list_entry(common_head->prev,
-				   struct ftrace_event_field, link);
-		return field;
+		node = common_head;
+		break;
 
 	case FORMAT_FIELD_SEPERATOR:
-		if (unlikely(list_empty(head)))
-			return NULL;
-
-		field = list_entry(head->prev, struct ftrace_event_field, link);
-		return field;
+		node = head;
+		break;
 
 	case FORMAT_PRINTFMT:
 		/* all done */
 		return NULL;
 	}
 
-	field = v;
-	if (field->link.prev == common_head)
+	node = node->prev;
+	if (node == common_head)
 		return (void *)FORMAT_FIELD_SEPERATOR;
-	else if (field->link.prev == head)
+	else if (node == head)
 		return (void *)FORMAT_PRINTFMT;
-
-	field = list_entry(field->link.prev, struct ftrace_event_field, link);
-
-	return field;
-}
-
-static void *f_start(struct seq_file *m, loff_t *pos)
-{
-	loff_t l = 0;
-	void *p;
-
-	/* Start by showing the header */
-	if (!*pos)
-		return (void *)FORMAT_HEADER;
-
-	p = (void *)FORMAT_HEADER;
-	do {
-		p = f_next(m, p, &l);
-	} while (p && l < *pos);
-
-	return p;
+	else
+		return node;
 }
 
 static int f_show(struct seq_file *m, void *v)
@@ -904,8 +878,7 @@ static int f_show(struct seq_file *m, void *v)
 		return 0;
 	}
 
-	field = v;
-
+	field = list_entry(v, struct ftrace_event_field, link);
 	/*
 	 * Smartly shows the array type(except dynamic array).
 	 * Normal:
@@ -932,6 +905,17 @@ static int f_show(struct seq_file *m, void *v)
 	return 0;
 }
 
+static void *f_start(struct seq_file *m, loff_t *pos)
+{
+	void *p = (void *)FORMAT_HEADER;
+	loff_t l = 0;
+
+	while (l < *pos && p)
+		p = f_next(m, p, &l);
+
+	return p;
+}
+
 static void f_stop(struct seq_file *m, void *p)
 {
 }

commit f77d09a384676bde6445413949d9d2c508ff3e62
Author: Alexander Z Lam <azl@google.com>
Date:   Thu Jul 18 11:18:44 2013 -0700

    tracing: Miscellaneous fixes for trace_array ref counting
    
    Some error paths did not handle ref counting properly, and some trace files need
    ref counting.
    
    Link: http://lkml.kernel.org/r/1374171524-11948-1-git-send-email-azl@google.com
    
    Cc: stable@vger.kernel.org # 3.10
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Cc: David Sharp <dhsharp@google.com>
    Cc: Alexander Z Lam <lambchop468@gmail.com>
    Signed-off-by: Alexander Z Lam <azl@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7d854290bf81..7a75cb22eab7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1218,6 +1218,7 @@ show_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 
 static int ftrace_event_avail_open(struct inode *inode, struct file *file);
 static int ftrace_event_set_open(struct inode *inode, struct file *file);
+static int ftrace_event_release(struct inode *inode, struct file *file);
 
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
@@ -1245,7 +1246,7 @@ static const struct file_operations ftrace_set_event_fops = {
 	.read = seq_read,
 	.write = ftrace_event_write,
 	.llseek = seq_lseek,
-	.release = seq_release,
+	.release = ftrace_event_release,
 };
 
 static const struct file_operations ftrace_enable_fops = {
@@ -1323,6 +1324,15 @@ ftrace_event_open(struct inode *inode, struct file *file,
 	return ret;
 }
 
+static int ftrace_event_release(struct inode *inode, struct file *file)
+{
+	struct trace_array *tr = inode->i_private;
+
+	trace_array_put(tr);
+
+	return seq_release(inode, file);
+}
+
 static int
 ftrace_event_avail_open(struct inode *inode, struct file *file)
 {
@@ -1336,12 +1346,19 @@ ftrace_event_set_open(struct inode *inode, struct file *file)
 {
 	const struct seq_operations *seq_ops = &show_set_event_seq_ops;
 	struct trace_array *tr = inode->i_private;
+	int ret;
+
+	if (trace_array_get(tr) < 0)
+		return -ENODEV;
 
 	if ((file->f_mode & FMODE_WRITE) &&
 	    (file->f_flags & O_TRUNC))
 		ftrace_clear_events(tr);
 
-	return ftrace_event_open(inode, file, seq_ops);
+	ret = ftrace_event_open(inode, file, seq_ops);
+	if (ret < 0)
+		trace_array_put(tr);
+	return ret;
 }
 
 static struct event_subsystem *

commit 2a6c24afab70dbcfee49f4c76e1511eec1a3298b
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jul 2 14:48:23 2013 -0400

    tracing: Fix race between deleting buffer and setting events
    
    While analyzing the code, I discovered that there's a potential race between
    deleting a trace instance and setting events. There are a few races that can
    occur if events are being traced as the buffer is being deleted. Mostly the
    problem comes with freeing the descriptor used by the trace event callback.
    To prevent problems like this, the events are disabled before the buffer is
    deleted. The problem with the current solution is that the event_mutex is let
    go between disabling the events and freeing the files, which means that the events
    could be enabled again while the freeing takes place.
    
    Cc: stable@vger.kernel.org # 3.10
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 920e08fb53b3..7d854290bf81 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -441,14 +441,14 @@ static int tracing_release_generic_file(struct inode *inode, struct file *filp)
 /*
  * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.
  */
-static int __ftrace_set_clr_event(struct trace_array *tr, const char *match,
-				  const char *sub, const char *event, int set)
+static int
+__ftrace_set_clr_event_nolock(struct trace_array *tr, const char *match,
+			      const char *sub, const char *event, int set)
 {
 	struct ftrace_event_file *file;
 	struct ftrace_event_call *call;
 	int ret = -EINVAL;
 
-	mutex_lock(&event_mutex);
 	list_for_each_entry(file, &tr->events, list) {
 
 		call = file->event_call;
@@ -474,6 +474,17 @@ static int __ftrace_set_clr_event(struct trace_array *tr, const char *match,
 
 		ret = 0;
 	}
+
+	return ret;
+}
+
+static int __ftrace_set_clr_event(struct trace_array *tr, const char *match,
+				  const char *sub, const char *event, int set)
+{
+	int ret;
+
+	mutex_lock(&event_mutex);
+	ret = __ftrace_set_clr_event_nolock(tr, match, sub, event, set);
 	mutex_unlock(&event_mutex);
 
 	return ret;
@@ -2408,11 +2419,11 @@ early_event_add_tracer(struct dentry *parent, struct trace_array *tr)
 
 int event_trace_del_tracer(struct trace_array *tr)
 {
-	/* Disable any running events */
-	__ftrace_set_clr_event(tr, NULL, NULL, NULL, 0);
-
 	mutex_lock(&event_mutex);
 
+	/* Disable any running events */
+	__ftrace_set_clr_event_nolock(tr, NULL, NULL, NULL, 0);
+
 	down_write(&trace_event_sem);
 	__trace_remove_event_dirs(tr);
 	debugfs_remove_recursive(tr->event_dir);

commit 8e2e2fa47129532a30cff6c25a47078dc97d9260
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Jul 2 15:30:53 2013 -0400

    tracing: Add trace_array_get/put() to event handling
    
    Commit a695cb58162 "tracing: Prevent deleting instances when they are being read"
    tried to fix a race between deleting a trace instance and reading contents
    of a trace file. But it wasn't good enough. The following could crash the kernel:
    
     # cd /sys/kernel/debug/tracing/instances
     # ( while :; do mkdir foo; rmdir foo; done ) &
     # ( while :; do echo 1 > foo/events/sched/sched_switch 2> /dev/null; done ) &
    
    Luckily this can only be done by root user, but it should be fixed regardless.
    
    The problem is that a delete of the file can happen after the write to the event
    is opened, but before the enabling happens.
    
    The solution is to make sure the trace_array is available before succeeding in
    opening for write, and incerment the ref counter while opened.
    
    Now the instance can be deleted when the events are writing to the buffer,
    but the deletion of the instance will disable all events before the instance
    is actually deleted.
    
    Cc: stable@vger.kernel.org # 3.10
    Reported-by: Alexander Lam <azl@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 35c6f23c71b2..920e08fb53b3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -409,6 +409,35 @@ static void put_system(struct ftrace_subsystem_dir *dir)
 	mutex_unlock(&event_mutex);
 }
 
+/*
+ * Open and update trace_array ref count.
+ * Must have the current trace_array passed to it.
+ */
+static int tracing_open_generic_file(struct inode *inode, struct file *filp)
+{
+	struct ftrace_event_file *file = inode->i_private;
+	struct trace_array *tr = file->tr;
+	int ret;
+
+	if (trace_array_get(tr) < 0)
+		return -ENODEV;
+
+	ret = tracing_open_generic(inode, filp);
+	if (ret < 0)
+		trace_array_put(tr);
+	return ret;
+}
+
+static int tracing_release_generic_file(struct inode *inode, struct file *filp)
+{
+	struct ftrace_event_file *file = inode->i_private;
+	struct trace_array *tr = file->tr;
+
+	trace_array_put(tr);
+
+	return 0;
+}
+
 /*
  * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.
  */
@@ -1032,9 +1061,17 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 	/* Some versions of gcc think dir can be uninitialized here */
 	WARN_ON(!dir);
 
+	/* Still need to increment the ref count of the system */
+	if (trace_array_get(tr) < 0) {
+		put_system(dir);
+		return -ENODEV;
+	}
+
 	ret = tracing_open_generic(inode, filp);
-	if (ret < 0)
+	if (ret < 0) {
+		trace_array_put(tr);
 		put_system(dir);
+	}
 
 	return ret;
 }
@@ -1045,16 +1082,23 @@ static int system_tr_open(struct inode *inode, struct file *filp)
 	struct trace_array *tr = inode->i_private;
 	int ret;
 
+	if (trace_array_get(tr) < 0)
+		return -ENODEV;
+
 	/* Make a temporary dir that has no system but points to tr */
 	dir = kzalloc(sizeof(*dir), GFP_KERNEL);
-	if (!dir)
+	if (!dir) {
+		trace_array_put(tr);
 		return -ENOMEM;
+	}
 
 	dir->tr = tr;
 
 	ret = tracing_open_generic(inode, filp);
-	if (ret < 0)
+	if (ret < 0) {
+		trace_array_put(tr);
 		kfree(dir);
+	}
 
 	filp->private_data = dir;
 
@@ -1065,6 +1109,8 @@ static int subsystem_release(struct inode *inode, struct file *file)
 {
 	struct ftrace_subsystem_dir *dir = file->private_data;
 
+	trace_array_put(dir->tr);
+
 	/*
 	 * If dir->subsystem is NULL, then this is a temporary
 	 * descriptor that was made for a trace_array to enable
@@ -1192,9 +1238,10 @@ static const struct file_operations ftrace_set_event_fops = {
 };
 
 static const struct file_operations ftrace_enable_fops = {
-	.open = tracing_open_generic,
+	.open = tracing_open_generic_file,
 	.read = event_enable_read,
 	.write = event_enable_write,
+	.release = tracing_release_generic_file,
 	.llseek = default_llseek,
 };
 

commit a82274151af2b075163e3c42c828529dee311487
Author: Alexander Z Lam <azl@google.com>
Date:   Mon Jul 1 19:37:54 2013 -0700

    tracing: Protect ftrace_trace_arrays list in trace_events.c
    
    There are multiple places where the ftrace_trace_arrays list is accessed in
    trace_events.c without the trace_types_lock held.
    
    Link: http://lkml.kernel.org/r/1372732674-22726-1-git-send-email-azl@google.com
    
    Cc: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Cc: David Sharp <dhsharp@google.com>
    Cc: Alexander Z Lam <lambchop468@gmail.com>
    Cc: stable@vger.kernel.org # 3.10
    Signed-off-by: Alexander Z Lam <azl@google.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5892470bc2ee..35c6f23c71b2 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1008,6 +1008,7 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 	int ret;
 
 	/* Make sure the system still exists */
+	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
 	list_for_each_entry(tr, &ftrace_trace_arrays, list) {
 		list_for_each_entry(dir, &tr->systems, list) {
@@ -1023,6 +1024,7 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 	}
  exit_loop:
 	mutex_unlock(&event_mutex);
+	mutex_unlock(&trace_types_lock);
 
 	if (!system)
 		return -ENODEV;
@@ -1617,6 +1619,7 @@ static void __add_event_to_tracers(struct ftrace_event_call *call,
 int trace_add_event_call(struct ftrace_event_call *call)
 {
 	int ret;
+	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
 
 	ret = __register_event(call, NULL);
@@ -1624,11 +1627,13 @@ int trace_add_event_call(struct ftrace_event_call *call)
 		__add_event_to_tracers(call, NULL);
 
 	mutex_unlock(&event_mutex);
+	mutex_unlock(&trace_types_lock);
 	return ret;
 }
 
 /*
- * Must be called under locking both of event_mutex and trace_event_sem.
+ * Must be called under locking of trace_types_lock, event_mutex and
+ * trace_event_sem.
  */
 static void __trace_remove_event_call(struct ftrace_event_call *call)
 {
@@ -1640,11 +1645,13 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 /* Remove an event_call */
 void trace_remove_event_call(struct ftrace_event_call *call)
 {
+	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
 	down_write(&trace_event_sem);
 	__trace_remove_event_call(call);
 	up_write(&trace_event_sem);
 	mutex_unlock(&event_mutex);
+	mutex_unlock(&trace_types_lock);
 }
 
 #define for_each_event(event, start, end)			\
@@ -1788,6 +1795,7 @@ static int trace_module_notify(struct notifier_block *self,
 {
 	struct module *mod = data;
 
+	mutex_lock(&trace_types_lock);
 	mutex_lock(&event_mutex);
 	switch (val) {
 	case MODULE_STATE_COMING:
@@ -1798,6 +1806,7 @@ static int trace_module_notify(struct notifier_block *self,
 		break;
 	}
 	mutex_unlock(&event_mutex);
+	mutex_unlock(&trace_types_lock);
 
 	return 0;
 }

commit 3baa5e4cf224b8a55220cc841bb475e164b84ceb
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Sat Jun 29 00:08:07 2013 -0500

    tracing: Fix disabling of soft disable
    
    The comment on the soft disable 'disable' case of
    __ftrace_event_enable_disable() states that the soft disable bit
    should be cleared in that case, but currently only the soft mode bit
    is actually cleared.
    
    This essentially leaves the standard non-soft-enable enable/disable
    paths as the only way to clear the soft disable flag, but the soft
    disable bit should also be cleared when removing a trigger with '!'.
    
    Also, the SOFT_DISABLED bit should never be set if SOFT_MODE is
    cleared.
    
    This fixes the above discrepancies.
    
    Link: http://lkml.kernel.org/r/b9c68dd50bc07019e6c67d3f9b29be4ef1b2badb.1372479499.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7ee08b95c384..5892470bc2ee 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -291,9 +291,11 @@ static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
 			}
 			call->class->reg(call, TRACE_REG_UNREGISTER, file);
 		}
-		/* If in SOFT_MODE, just set the SOFT_DISABLE_BIT */
+		/* If in SOFT_MODE, just set the SOFT_DISABLE_BIT, else clear it */
 		if (file->flags & FTRACE_EVENT_FL_SOFT_MODE)
 			set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
+		else
+			clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
 		break;
 	case 1:
 		/*

commit a439059610ecd257dba29a612729132e470d118f
Author: Tom Zanussi <tom.zanussi@linux.intel.com>
Date:   Sat Jun 29 00:08:04 2013 -0500

    tracing: Simplify code for showing of soft disabled flag
    
    Rather than enumerating each permutation, build the enable state
    string up from the combination of states.  This also allows for the
    simpler addition of more states.
    
    Link: http://lkml.kernel.org/r/9aff5af6dee2f5a40ca30df41c39d5f33e998d7a.1372479499.git.tom.zanussi@linux.intel.com
    
    Signed-off-by: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 903a0bf2685e..7ee08b95c384 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -638,17 +638,17 @@ event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
 	struct ftrace_event_file *file = filp->private_data;
-	char *buf;
+	char buf[4] = "0";
 
-	if (file->flags & FTRACE_EVENT_FL_ENABLED) {
-		if (file->flags & FTRACE_EVENT_FL_SOFT_DISABLED)
-			buf = "0*\n";
-		else if (file->flags & FTRACE_EVENT_FL_SOFT_MODE)
-			buf = "1*\n";
-		else
-			buf = "1\n";
-	} else
-		buf = "0\n";
+	if (file->flags & FTRACE_EVENT_FL_ENABLED &&
+	    !(file->flags & FTRACE_EVENT_FL_SOFT_DISABLED))
+		strcpy(buf, "1");
+
+	if (file->flags & FTRACE_EVENT_FL_SOFT_DISABLED ||
+	    file->flags & FTRACE_EVENT_FL_SOFT_MODE)
+		strcat(buf, "*");
+
+	strcat(buf, "\n");
 
 	return simple_read_from_buffer(ubuf, cnt, ppos, buf, strlen(buf));
 }

commit 6e94a780374ed31b280f939d4757e8d7858dff16
Author: Steven Rostedt <rostedt@goodmis.org>
Date:   Thu Jun 27 10:58:31 2013 -0400

    tracing: Failed to create system directory
    
    Running the following:
    
     # cd /sys/kernel/debug/tracing
     # echo p:i do_sys_open > kprobe_events
     # echo p:j schedule >> kprobe_events
     # cat kprobe_events
    p:kprobes/i do_sys_open
    p:kprobes/j schedule
     # echo p:i do_sys_open >> kprobe_events
     # cat kprobe_events
    p:kprobes/j schedule
    p:kprobes/i do_sys_open
     # ls /sys/kernel/debug/tracing/events/kprobes/
    enable  filter  j
    
    Notice that the 'i' is missing from the kprobes directory.
    
    The console produces:
    
    "Failed to create system directory kprobes"
    
    This is because kprobes passes in a allocated name for the system
    and the ftrace event subsystem saves off that name instead of creating
    a duplicate for it. But the kprobes may free the system name making
    the pointer to it invalid.
    
    This bug was introduced by 92edca073c37 "tracing: Use direct field, type
    and system names" which switched from using kstrdup() on the system name
    in favor of just keeping apointer to it, as the internal ftrace event
    system names are static and exist for the life of the computer being booted.
    
    Instead of reverting back to duplicating system names again, we can use
    core_kernel_data() to determine if the passed in name was allocated or
    static. Then use the MSB of the ref_count to be a flag to keep track if
    the name was allocated or not. Then we can still save from having to duplicate
    strings that will always exist, but still copy the ones that may be freed.
    
    Cc: stable@vger.kernel.org # 3.10
    Reported-by: "zhangwei(Jovi)" <jovi.zhangwei@huawei.com>
    Reported-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Tested-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f57b01574a30..903a0bf2685e 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -41,6 +41,23 @@ static LIST_HEAD(ftrace_common_fields);
 static struct kmem_cache *field_cachep;
 static struct kmem_cache *file_cachep;
 
+#define SYSTEM_FL_FREE_NAME		(1 << 31)
+
+static inline int system_refcount(struct event_subsystem *system)
+{
+	return system->ref_count & ~SYSTEM_FL_FREE_NAME;
+}
+
+static int system_refcount_inc(struct event_subsystem *system)
+{
+	return (system->ref_count++) & ~SYSTEM_FL_FREE_NAME;
+}
+
+static int system_refcount_dec(struct event_subsystem *system)
+{
+	return (--system->ref_count) & ~SYSTEM_FL_FREE_NAME;
+}
+
 /* Double loops, do not use break, only goto's work */
 #define do_for_each_event_file(tr, file)			\
 	list_for_each_entry(tr, &ftrace_trace_arrays, list) {	\
@@ -344,8 +361,8 @@ static void __put_system(struct event_subsystem *system)
 {
 	struct event_filter *filter = system->filter;
 
-	WARN_ON_ONCE(system->ref_count == 0);
-	if (--system->ref_count)
+	WARN_ON_ONCE(system_refcount(system) == 0);
+	if (system_refcount_dec(system))
 		return;
 
 	list_del(&system->list);
@@ -354,13 +371,15 @@ static void __put_system(struct event_subsystem *system)
 		kfree(filter->filter_string);
 		kfree(filter);
 	}
+	if (system->ref_count & SYSTEM_FL_FREE_NAME)
+		kfree(system->name);
 	kfree(system);
 }
 
 static void __get_system(struct event_subsystem *system)
 {
-	WARN_ON_ONCE(system->ref_count == 0);
-	system->ref_count++;
+	WARN_ON_ONCE(system_refcount(system) == 0);
+	system_refcount_inc(system);
 }
 
 static void __get_system_dir(struct ftrace_subsystem_dir *dir)
@@ -374,7 +393,7 @@ static void __put_system_dir(struct ftrace_subsystem_dir *dir)
 {
 	WARN_ON_ONCE(dir->ref_count == 0);
 	/* If the subsystem is about to be freed, the dir must be too */
-	WARN_ON_ONCE(dir->subsystem->ref_count == 1 && dir->ref_count != 1);
+	WARN_ON_ONCE(system_refcount(dir->subsystem) == 1 && dir->ref_count != 1);
 
 	__put_system(dir->subsystem);
 	if (!--dir->ref_count)
@@ -1274,7 +1293,15 @@ create_new_subsystem(const char *name)
 		return NULL;
 
 	system->ref_count = 1;
-	system->name = name;
+
+	/* Only allocate if dynamic (kprobes and modules) */
+	if (!core_kernel_data((unsigned long)name)) {
+		system->ref_count |= SYSTEM_FL_FREE_NAME;
+		system->name = kstrdup(name, GFP_KERNEL);
+		if (!system->name)
+			goto out_free;
+	} else
+		system->name = name;
 
 	system->filter = NULL;
 
@@ -1287,6 +1314,8 @@ create_new_subsystem(const char *name)
 	return system;
 
  out_free:
+	if (system->ref_count & SYSTEM_FL_FREE_NAME)
+		kfree(system->name);
 	kfree(system);
 	return NULL;
 }

commit aaf6ac0f0871cb7fc0f28f3a00edf329bc7adc29
Author: Namhyung Kim <namhyung.kim@lge.com>
Date:   Fri Jun 7 15:07:48 2013 +0900

    tracing: Do not call kmem_cache_free() on allocation failure
    
    There's no point calling it when _alloc() failed.
    
    Link: http://lkml.kernel.org/r/1370585268-29169-1-git-send-email-namhyung@kernel.org
    
    Signed-off-by: Namhyung Kim <namhyung@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index db086f172cf5..f57b01574a30 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -97,7 +97,7 @@ static int __trace_define_field(struct list_head *head, const char *type,
 
 	field = kmem_cache_alloc(field_cachep, GFP_TRACE);
 	if (!field)
-		goto err;
+		return -ENOMEM;
 
 	field->name = name;
 	field->type = type;
@@ -114,11 +114,6 @@ static int __trace_define_field(struct list_head *head, const char *type,
 	list_add(&field->link, head);
 
 	return 0;
-
-err:
-	kmem_cache_free(field_cachep, field);
-
-	return -ENOMEM;
 }
 
 int trace_define_field(struct ftrace_event_call *call, const char *type,

commit 8092e808a31839c502a52d391b15f31c1d8764f5
Author: Harsh Prateek Bora <harsh@linux.vnet.ibm.com>
Date:   Fri May 24 12:52:17 2013 +0530

    tracing/trivial: Consolidate error return condition
    
    Consolidate the checks for !enabled and !param to return -EINVAL
    in event_enable_func().
    
    Link: http://lkml.kernel.org/r/1369380137-12452-1-git-send-email-harsh@linux.vnet.ibm.com
    
    Signed-off-by: Harsh Prateek Bora <harsh@linux.vnet.ibm.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 27963e2bf4bf..db086f172cf5 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2011,10 +2011,7 @@ event_enable_func(struct ftrace_hash *hash,
 	int ret;
 
 	/* hash funcs only work with set_ftrace_filter */
-	if (!enabled)
-		return -EINVAL;
-
-	if (!param)
+	if (!enabled || !param)
 		return -EINVAL;
 
 	system = strsep(&param, ":");

commit 6ed0106667d76589cb648c27edb4f4ffbf9d59ca
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu May 16 20:48:49 2013 +0900

    tracing: Return -EBUSY when event_enable_func() fails to get module
    
    Since try_module_get() returns false( = 0) when it fails to
    pindown a module, event_enable_func() returns 0 which means
    "succeed". This can cause a kernel panic when the entry
    is removed, because the event is already released.
    
    This fixes the bug by returning -EBUSY, because the reason
    why it fails is that the module is being removed at that time.
    
    Link: http://lkml.kernel.org/r/20130516114848.13508.97899.stgit@mhiramat-M0-7522
    
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Tom Zanussi <tom.zanussi@intel.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7a0cf68027cc..27963e2bf4bf 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2072,8 +2072,10 @@ event_enable_func(struct ftrace_hash *hash,
  out_reg:
 	/* Don't let event modules unload while probe registered */
 	ret = try_module_get(file->event_call->mod);
-	if (!ret)
+	if (!ret) {
+		ret = -EBUSY;
 		goto out_free;
+	}
 
 	ret = __ftrace_event_enable_disable(file, 1, 1);
 	if (ret < 0)

commit da511bf33e47ea1f33f4b672f7da166d2a1b8a91
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu May 9 15:00:07 2013 -0400

    tracing: Add helper function trace_create_new_event() to remove duplicate code
    
    Both __trace_add_new_event() and __trace_early_add_new_event() do
    basically the same thing, except that __trace_add_new_event() does
    a little more.
    
    Instead of having duplicate code between the two functions, add
    a helper function trace_create_new_event() that both can use.
    This will help against having bugs fixed in one function but not
    the other.
    
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8be1224046f8..7a0cf68027cc 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1529,6 +1529,24 @@ __register_event(struct ftrace_event_call *call, struct module *mod)
 	return 0;
 }
 
+static struct ftrace_event_file *
+trace_create_new_event(struct ftrace_event_call *call,
+		       struct trace_array *tr)
+{
+	struct ftrace_event_file *file;
+
+	file = kmem_cache_alloc(file_cachep, GFP_TRACE);
+	if (!file)
+		return NULL;
+
+	file->event_call = call;
+	file->tr = tr;
+	atomic_set(&file->sm_ref, 0);
+	list_add(&file->list, &tr->events);
+
+	return file;
+}
+
 /* Add an event to a trace directory */
 static int
 __trace_add_new_event(struct ftrace_event_call *call,
@@ -1540,15 +1558,10 @@ __trace_add_new_event(struct ftrace_event_call *call,
 {
 	struct ftrace_event_file *file;
 
-	file = kmem_cache_alloc(file_cachep, GFP_TRACE);
+	file = trace_create_new_event(call, tr);
 	if (!file)
 		return -ENOMEM;
 
-	file->event_call = call;
-	file->tr = tr;
-	atomic_set(&file->sm_ref, 0);
-	list_add(&file->list, &tr->events);
-
 	return event_create_dir(tr->event_dir, file, id, enable, filter, format);
 }
 
@@ -1563,15 +1576,10 @@ __trace_early_add_new_event(struct ftrace_event_call *call,
 {
 	struct ftrace_event_file *file;
 
-	file = kmem_cache_alloc(file_cachep, GFP_TRACE);
+	file = trace_create_new_event(call, tr);
 	if (!file)
 		return -ENOMEM;
 
-	file->event_call = call;
-	file->tr = tr;
-	atomic_set(&file->sm_ref, 0);
-	list_add(&file->list, &tr->events);
-
 	return 0;
 }
 

commit 1cf4c0732db3cd3c49cadbc60ff6bda08604e6fa
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu May 9 14:44:29 2013 +0900

    tracing: Modify soft-mode only if there's no other referrer
    
    Modify soft-mode flag only if no other soft-mode referrer
    (currently only the ftrace triggers) by using a reference
    counter in each ftrace_event_file.
    
    Without this fix, adding and removing several different
    enable/disable_event triggers on the same event clear
    soft-mode bit from the ftrace_event_file. This also
    happens with a typo of glob on setting triggers.
    
    e.g.
    
     # echo vfs_symlink:enable_event:net:netif_rx > set_ftrace_filter
     # cat events/net/netif_rx/enable
     0*
     # echo typo_func:enable_event:net:netif_rx > set_ftrace_filter
     # cat events/net/netif_rx/enable
     0
     # cat set_ftrace_filter
     #### all functions enabled ####
     vfs_symlink:enable_event:net:netif_rx:unlimited
    
    As above, we still have a trigger, but soft-mode is gone.
    
    Link: http://lkml.kernel.org/r/20130509054429.30398.7464.stgit@mhiramat-M0-7522
    
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: David Sharp <dhsharp@google.com>
    Cc: Hiraku Toyooka <hiraku.toyooka.gu@hitachi.com>
    Cc: Tom Zanussi <tom.zanussi@intel.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 915c136d7bd1..8be1224046f8 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -251,7 +251,8 @@ static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
 	switch (enable) {
 	case 0:
 		/*
-		 * When soft_disable is set and enable is cleared, we want
+		 * When soft_disable is set and enable is cleared, the sm_ref
+		 * reference counter is decremented. If it reaches 0, we want
 		 * to clear the SOFT_DISABLED flag but leave the event in the
 		 * state that it was. That is, if the event was enabled and
 		 * SOFT_DISABLED isn't set, then do nothing. But if SOFT_DISABLED
@@ -263,6 +264,8 @@ static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
 		 * "soft enable"s (clearing the SOFT_DISABLED bit) wont work.
 		 */
 		if (soft_disable) {
+			if (atomic_dec_return(&file->sm_ref) > 0)
+				break;
 			disable = file->flags & FTRACE_EVENT_FL_SOFT_DISABLED;
 			clear_bit(FTRACE_EVENT_FL_SOFT_MODE_BIT, &file->flags);
 		} else
@@ -291,8 +294,11 @@ static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
 		 */
 		if (!soft_disable)
 			clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
-		else
+		else {
+			if (atomic_inc_return(&file->sm_ref) > 1)
+				break;
 			set_bit(FTRACE_EVENT_FL_SOFT_MODE_BIT, &file->flags);
+		}
 
 		if (!(file->flags & FTRACE_EVENT_FL_ENABLED)) {
 
@@ -1540,6 +1546,7 @@ __trace_add_new_event(struct ftrace_event_call *call,
 
 	file->event_call = call;
 	file->tr = tr;
+	atomic_set(&file->sm_ref, 0);
 	list_add(&file->list, &tr->events);
 
 	return event_create_dir(tr->event_dir, file, id, enable, filter, format);
@@ -1562,6 +1569,7 @@ __trace_early_add_new_event(struct ftrace_event_call *call,
 
 	file->event_call = call;
 	file->tr = tr;
+	atomic_set(&file->sm_ref, 0);
 	list_add(&file->list, &tr->events);
 
 	return 0;

commit 30052170dcc256c18a43fb3e76577a67394543f8
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu May 9 14:44:26 2013 +0900

    tracing: Indicate enabled soft-mode in enable file
    
    Indicate enabled soft-mode event as "1*" in "enable" file
    for each event, because it can be soft-disabled when disable_event
    trigger is hit.
    
    Link: http://lkml.kernel.org/r/20130509054426.30398.28202.stgit@mhiramat-M0-7522
    
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Tom Zanussi <tom.zanussi@intel.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 87e826f1c237..915c136d7bd1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -623,6 +623,8 @@ event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	if (file->flags & FTRACE_EVENT_FL_ENABLED) {
 		if (file->flags & FTRACE_EVENT_FL_SOFT_DISABLED)
 			buf = "0*\n";
+		else if (file->flags & FTRACE_EVENT_FL_SOFT_MODE)
+			buf = "1*\n";
 		else
 			buf = "1\n";
 	} else

commit ff305ded9ff83436039a16d31bc558dc6598d7ce
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Thu May 9 11:30:26 2013 -0400

    tracing: Return error if register_ftrace_function_probe() fails for event_enable_func()
    
    register_ftrace_function_probe() returns the number of functions
    it registered, which can be zero, it can also return a negative number
    if something went wrong. But event_enable_func() only checks for
    the case that it didn't register anything, it needs to also check
    for the case that something went wrong and return that error code
    as well.
    
    Added some comments about the code as well, to make it more
    understandable.
    
    Cc: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 44ac83614c3d..87e826f1c237 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2061,11 +2061,18 @@ event_enable_func(struct ftrace_hash *hash,
 	if (ret < 0)
 		goto out_put;
 	ret = register_ftrace_function_probe(glob, ops, data);
+	/*
+	 * The above returns on success the # of functions enabled,
+	 * but if it didn't find any functions it returns zero.
+	 * Consider no functions a failure too.
+	 */
 	if (!ret) {
 		ret = -ENOENT;
 		goto out_disable;
-	} else
-		ret = 0;
+	} else if (ret < 0)
+		goto out_disable;
+	/* Just return zero, not the number of enabled functions */
+	ret = 0;
  out:
 	mutex_unlock(&event_mutex);
 	return ret;

commit a5b85bd1557209b4ef18a8cf07e60a1ca3132468
Author: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
Date:   Thu May 9 14:44:14 2013 +0900

    tracing: Don't succeed if event_enable_func did not register anything
    
    Return 0 instead of the number of activated ftrace function probes if
    event_enable_func succeeded and return an error code if it failed or
    did not register any functions. But it currently returns the number
    of registered functions and if it didn't register anything, it returns 0,
    but that is considered success.
    
    This also fixes the return value. As if it succeeds, it returns the
    number of functions that were enabled, which is returned back to
    the user in ftrace_regex_write (the write() return code). If only
    one function is enabled, then the return code of the write is one,
    and this can confuse the user program in thinking it only wrote 1
    byte.
    
    Link: http://lkml.kernel.org/r/20130509054413.30398.55650.stgit@mhiramat-M0-7522
    
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Oleg Nesterov <oleg@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Tom Zanussi <tom.zanussi@intel.com>
    Signed-off-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    [ Rewrote change log to reflect that this fixes two bugs - SR ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 53582e982e51..44ac83614c3d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -2061,8 +2061,11 @@ event_enable_func(struct ftrace_hash *hash,
 	if (ret < 0)
 		goto out_put;
 	ret = register_ftrace_function_probe(glob, ops, data);
-	if (!ret)
+	if (!ret) {
+		ret = -ENOENT;
 		goto out_disable;
+	} else
+		ret = 0;
  out:
 	mutex_unlock(&event_mutex);
 	return ret;

commit 52f6ad6dc3f4c6de598fe7cc9b629888d624aa52
Author: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
Date:   Mon Mar 11 15:14:03 2013 +0800

    tracing: Rename trace_event_mutex to trace_event_sem
    
    trace_event_mutex is an rw semaphore now, not a mutex, change the name.
    
    Link: http://lkml.kernel.org/r/513D843B.40109@huawei.com
    
    Signed-off-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    [ Forward ported to my new code ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a71cdc3c5df9..53582e982e51 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1584,7 +1584,7 @@ int trace_add_event_call(struct ftrace_event_call *call)
 }
 
 /*
- * Must be called under locking both of event_mutex and trace_event_mutex.
+ * Must be called under locking both of event_mutex and trace_event_sem.
  */
 static void __trace_remove_event_call(struct ftrace_event_call *call)
 {
@@ -1597,9 +1597,9 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 void trace_remove_event_call(struct ftrace_event_call *call)
 {
 	mutex_lock(&event_mutex);
-	down_write(&trace_event_mutex);
+	down_write(&trace_event_sem);
 	__trace_remove_event_call(call);
-	up_write(&trace_event_mutex);
+	up_write(&trace_event_sem);
 	mutex_unlock(&event_mutex);
 }
 
@@ -1707,7 +1707,7 @@ static void trace_module_remove_events(struct module *mod)
 	struct ftrace_event_call *call, *p;
 	bool clear_trace = false;
 
-	down_write(&trace_event_mutex);
+	down_write(&trace_event_sem);
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
 		if (call->mod == mod) {
 			if (call->flags & TRACE_EVENT_FL_WAS_ENABLED)
@@ -1725,7 +1725,7 @@ static void trace_module_remove_events(struct module *mod)
 		list_del(&file_ops->list);
 		kfree(file_ops);
 	}
-	up_write(&trace_event_mutex);
+	up_write(&trace_event_sem);
 
 	/*
 	 * It is safest to reset the ring buffer if the module being unloaded
@@ -2262,9 +2262,9 @@ int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
 	if (ret)
 		goto out_unlock;
 
-	down_write(&trace_event_mutex);
+	down_write(&trace_event_sem);
 	__trace_add_event_dirs(tr);
-	up_write(&trace_event_mutex);
+	up_write(&trace_event_sem);
 
  out_unlock:
 	mutex_unlock(&event_mutex);
@@ -2287,9 +2287,9 @@ early_event_add_tracer(struct dentry *parent, struct trace_array *tr)
 	if (ret)
 		goto out_unlock;
 
-	down_write(&trace_event_mutex);
+	down_write(&trace_event_sem);
 	__trace_early_add_event_dirs(tr);
-	up_write(&trace_event_mutex);
+	up_write(&trace_event_sem);
 
  out_unlock:
 	mutex_unlock(&event_mutex);
@@ -2304,10 +2304,10 @@ int event_trace_del_tracer(struct trace_array *tr)
 
 	mutex_lock(&event_mutex);
 
-	down_write(&trace_event_mutex);
+	down_write(&trace_event_sem);
 	__trace_remove_event_dirs(tr);
 	debugfs_remove_recursive(tr->event_dir);
-	up_write(&trace_event_mutex);
+	up_write(&trace_event_sem);
 
 	tr->event_dir = NULL;
 

commit ad7067cebf3253412a7c0a169a9dd056b11e69ac
Author: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
Date:   Mon Mar 11 15:13:46 2013 +0800

    tracing: Convert trace_destroy_fields() to static
    
    trace_destroy_fields() is not used outside of the file. It can be
    a static function.
    
    Link: http://lkml.kernel.org/r/513D842A.2000907@huawei.com
    
    Signed-off-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ba523d7beea2..a71cdc3c5df9 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -158,7 +158,7 @@ static int trace_define_common_fields(void)
 	return ret;
 }
 
-void trace_destroy_fields(struct ftrace_event_call *call)
+static void trace_destroy_fields(struct ftrace_event_call *call)
 {
 	struct ftrace_event_field *field, *next;
 	struct list_head *head;

commit b3a8c6fd7bb61c910bd4f80ae1d75056e8f98c19
Author: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
Date:   Mon Mar 11 15:13:42 2013 +0800

    tracing: Move find_event_field() into trace_events.c
    
    By moving find_event_field() and trace_find_field() into trace_events.c,
    the ftrace_common_fields list and trace_get_fields() can become local to
    the trace_events.c file.
    
    find_event_field() is renamed to trace_find_event_field() to conform to
    the tracing global function names.
    
    Link: http://lkml.kernel.org/r/513D8426.9070109@huawei.com
    
    Signed-off-by: zhangwei(Jovi) <jovi.zhangwei@huawei.com>
    [ rostedt: Modified trace_find_field() to trace_find_event_field() ]
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c636523b1a59..ba523d7beea2 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -34,7 +34,7 @@ char event_storage[EVENT_STORAGE_SIZE];
 EXPORT_SYMBOL_GPL(event_storage);
 
 LIST_HEAD(ftrace_events);
-LIST_HEAD(ftrace_common_fields);
+static LIST_HEAD(ftrace_common_fields);
 
 #define GFP_TRACE (GFP_KERNEL | __GFP_ZERO)
 
@@ -54,7 +54,7 @@ static struct kmem_cache *file_cachep;
 #define while_for_each_event_file()		\
 	}
 
-struct list_head *
+static struct list_head *
 trace_get_fields(struct ftrace_event_call *event_call)
 {
 	if (!event_call->class->get_fields)
@@ -62,6 +62,33 @@ trace_get_fields(struct ftrace_event_call *event_call)
 	return event_call->class->get_fields(event_call);
 }
 
+static struct ftrace_event_field *
+__find_event_field(struct list_head *head, char *name)
+{
+	struct ftrace_event_field *field;
+
+	list_for_each_entry(field, head, link) {
+		if (!strcmp(field->name, name))
+			return field;
+	}
+
+	return NULL;
+}
+
+struct ftrace_event_field *
+trace_find_event_field(struct ftrace_event_call *call, char *name)
+{
+	struct ftrace_event_field *field;
+	struct list_head *head;
+
+	field = __find_event_field(&ftrace_common_fields, name);
+	if (field)
+		return field;
+
+	head = trace_get_fields(call);
+	return __find_event_field(head, name);
+}
+
 static int __trace_define_field(struct list_head *head, const char *type,
 				const char *name, int offset, int size,
 				int is_signed, int filter_type)

commit 3cd715de261182413b3487abfffe1b6af41b81b3
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Mar 12 19:35:13 2013 -0400

    tracing: Add function probe triggers to enable/disable events
    
    Add triggers to function tracer that lets an event get enabled or
    disabled when a function is called:
    
    format is:
    
     <function>:enable_event:<system>:<event>[:<count>]
     <function>:disable_event:<system>:<event>[:<count>]
    
     echo 'schedule:enable_event:sched:sched_switch' > /debug/tracing/set_ftrace_filter
    
    Every time schedule is called, it will enable the sched_switch event.
    
     echo 'schedule:disable_event:sched:sched_switch:2' > /debug/tracing/set_ftrace_filter
    
    The first two times schedule is called while the sched_switch
    event is enabled, it will disable it. It will not count for a time
    that the event is already disabled (or enabled for enable_event).
    
    [ fixed return without mutex_unlock() - thanks to Dan Carpenter and smatch ]
    
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 106640b0df4a..c636523b1a59 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1798,6 +1798,283 @@ __trace_add_event_dirs(struct trace_array *tr)
 	}
 }
 
+#ifdef CONFIG_DYNAMIC_FTRACE
+
+/* Avoid typos */
+#define ENABLE_EVENT_STR	"enable_event"
+#define DISABLE_EVENT_STR	"disable_event"
+
+struct event_probe_data {
+	struct ftrace_event_file	*file;
+	unsigned long			count;
+	int				ref;
+	bool				enable;
+};
+
+static struct ftrace_event_file *
+find_event_file(struct trace_array *tr, const char *system,  const char *event)
+{
+	struct ftrace_event_file *file;
+	struct ftrace_event_call *call;
+
+	list_for_each_entry(file, &tr->events, list) {
+
+		call = file->event_call;
+
+		if (!call->name || !call->class || !call->class->reg)
+			continue;
+
+		if (call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)
+			continue;
+
+		if (strcmp(event, call->name) == 0 &&
+		    strcmp(system, call->class->system) == 0)
+			return file;
+	}
+	return NULL;
+}
+
+static void
+event_enable_probe(unsigned long ip, unsigned long parent_ip, void **_data)
+{
+	struct event_probe_data **pdata = (struct event_probe_data **)_data;
+	struct event_probe_data *data = *pdata;
+
+	if (!data)
+		return;
+
+	if (data->enable)
+		clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &data->file->flags);
+	else
+		set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &data->file->flags);
+}
+
+static void
+event_enable_count_probe(unsigned long ip, unsigned long parent_ip, void **_data)
+{
+	struct event_probe_data **pdata = (struct event_probe_data **)_data;
+	struct event_probe_data *data = *pdata;
+
+	if (!data)
+		return;
+
+	if (!data->count)
+		return;
+
+	/* Skip if the event is in a state we want to switch to */
+	if (data->enable == !(data->file->flags & FTRACE_EVENT_FL_SOFT_DISABLED))
+		return;
+
+	if (data->count != -1)
+		(data->count)--;
+
+	event_enable_probe(ip, parent_ip, _data);
+}
+
+static int
+event_enable_print(struct seq_file *m, unsigned long ip,
+		      struct ftrace_probe_ops *ops, void *_data)
+{
+	struct event_probe_data *data = _data;
+
+	seq_printf(m, "%ps:", (void *)ip);
+
+	seq_printf(m, "%s:%s:%s",
+		   data->enable ? ENABLE_EVENT_STR : DISABLE_EVENT_STR,
+		   data->file->event_call->class->system,
+		   data->file->event_call->name);
+
+	if (data->count == -1)
+		seq_printf(m, ":unlimited\n");
+	else
+		seq_printf(m, ":count=%ld\n", data->count);
+
+	return 0;
+}
+
+static int
+event_enable_init(struct ftrace_probe_ops *ops, unsigned long ip,
+		  void **_data)
+{
+	struct event_probe_data **pdata = (struct event_probe_data **)_data;
+	struct event_probe_data *data = *pdata;
+
+	data->ref++;
+	return 0;
+}
+
+static void
+event_enable_free(struct ftrace_probe_ops *ops, unsigned long ip,
+		  void **_data)
+{
+	struct event_probe_data **pdata = (struct event_probe_data **)_data;
+	struct event_probe_data *data = *pdata;
+
+	if (WARN_ON_ONCE(data->ref <= 0))
+		return;
+
+	data->ref--;
+	if (!data->ref) {
+		/* Remove the SOFT_MODE flag */
+		__ftrace_event_enable_disable(data->file, 0, 1);
+		module_put(data->file->event_call->mod);
+		kfree(data);
+	}
+	*pdata = NULL;
+}
+
+static struct ftrace_probe_ops event_enable_probe_ops = {
+	.func			= event_enable_probe,
+	.print			= event_enable_print,
+	.init			= event_enable_init,
+	.free			= event_enable_free,
+};
+
+static struct ftrace_probe_ops event_enable_count_probe_ops = {
+	.func			= event_enable_count_probe,
+	.print			= event_enable_print,
+	.init			= event_enable_init,
+	.free			= event_enable_free,
+};
+
+static struct ftrace_probe_ops event_disable_probe_ops = {
+	.func			= event_enable_probe,
+	.print			= event_enable_print,
+	.init			= event_enable_init,
+	.free			= event_enable_free,
+};
+
+static struct ftrace_probe_ops event_disable_count_probe_ops = {
+	.func			= event_enable_count_probe,
+	.print			= event_enable_print,
+	.init			= event_enable_init,
+	.free			= event_enable_free,
+};
+
+static int
+event_enable_func(struct ftrace_hash *hash,
+		  char *glob, char *cmd, char *param, int enabled)
+{
+	struct trace_array *tr = top_trace_array();
+	struct ftrace_event_file *file;
+	struct ftrace_probe_ops *ops;
+	struct event_probe_data *data;
+	const char *system;
+	const char *event;
+	char *number;
+	bool enable;
+	int ret;
+
+	/* hash funcs only work with set_ftrace_filter */
+	if (!enabled)
+		return -EINVAL;
+
+	if (!param)
+		return -EINVAL;
+
+	system = strsep(&param, ":");
+	if (!param)
+		return -EINVAL;
+
+	event = strsep(&param, ":");
+
+	mutex_lock(&event_mutex);
+
+	ret = -EINVAL;
+	file = find_event_file(tr, system, event);
+	if (!file)
+		goto out;
+
+	enable = strcmp(cmd, ENABLE_EVENT_STR) == 0;
+
+	if (enable)
+		ops = param ? &event_enable_count_probe_ops : &event_enable_probe_ops;
+	else
+		ops = param ? &event_disable_count_probe_ops : &event_disable_probe_ops;
+
+	if (glob[0] == '!') {
+		unregister_ftrace_function_probe_func(glob+1, ops);
+		ret = 0;
+		goto out;
+	}
+
+	ret = -ENOMEM;
+	data = kzalloc(sizeof(*data), GFP_KERNEL);
+	if (!data)
+		goto out;
+
+	data->enable = enable;
+	data->count = -1;
+	data->file = file;
+
+	if (!param)
+		goto out_reg;
+
+	number = strsep(&param, ":");
+
+	ret = -EINVAL;
+	if (!strlen(number))
+		goto out_free;
+
+	/*
+	 * We use the callback data field (which is a pointer)
+	 * as our counter.
+	 */
+	ret = kstrtoul(number, 0, &data->count);
+	if (ret)
+		goto out_free;
+
+ out_reg:
+	/* Don't let event modules unload while probe registered */
+	ret = try_module_get(file->event_call->mod);
+	if (!ret)
+		goto out_free;
+
+	ret = __ftrace_event_enable_disable(file, 1, 1);
+	if (ret < 0)
+		goto out_put;
+	ret = register_ftrace_function_probe(glob, ops, data);
+	if (!ret)
+		goto out_disable;
+ out:
+	mutex_unlock(&event_mutex);
+	return ret;
+
+ out_disable:
+	__ftrace_event_enable_disable(file, 0, 1);
+ out_put:
+	module_put(file->event_call->mod);
+ out_free:
+	kfree(data);
+	goto out;
+}
+
+static struct ftrace_func_command event_enable_cmd = {
+	.name			= ENABLE_EVENT_STR,
+	.func			= event_enable_func,
+};
+
+static struct ftrace_func_command event_disable_cmd = {
+	.name			= DISABLE_EVENT_STR,
+	.func			= event_enable_func,
+};
+
+static __init int register_event_cmds(void)
+{
+	int ret;
+
+	ret = register_ftrace_command(&event_enable_cmd);
+	if (WARN_ON(ret < 0))
+		return ret;
+	ret = register_ftrace_command(&event_disable_cmd);
+	if (WARN_ON(ret < 0))
+		unregister_ftrace_command(&event_enable_cmd);
+	return ret;
+}
+#else
+static inline int register_event_cmds(void) { return 0; }
+#endif /* CONFIG_DYNAMIC_FTRACE */
+
 /*
  * The top level array has already had its ftrace_event_file
  * descriptors created in order to allow for early events to
@@ -2058,6 +2335,8 @@ static __init int event_trace_enable(void)
 
 	trace_printk_start_comm();
 
+	register_event_cmds();
+
 	return 0;
 }
 

commit 417944c4c7a0f657158d0515f3b8e8c043fd788f
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Mar 12 13:26:18 2013 -0400

    tracing: Add a way to soft disable trace events
    
    In order to let triggers enable or disable events, we need a 'soft'
    method for doing so. For example, if a function probe is added that
    lets a user enable or disable events when a function is called, that
    change must be done without taking locks or a mutex, and definitely
    it can't sleep. But the full enabling of a tracepoint is expensive.
    
    By adding a 'SOFT_DISABLE' flag, and converting the flags to be updated
    without the protection of a mutex (using set/clear_bit()), this soft
    disable flag can be used to allow critical sections to enable or disable
    events from being traced (after the event has been placed into "SOFT_MODE").
    
    Some caveats though: The comm recorder (to map pids with a comm) can not
    be soft disabled (yet). If you disable an event with with a "soft"
    disable and wait a while before reading the trace, the comm cache may be
    replaced and you'll get a bunch of <...> for comms in the trace.
    
    Reading the "enable" file for an event that is disabled will now give
    you "0*" where the '*' denotes that the tracepoint is still active but
    the event itself is "disabled".
    
    [ fixed _BIT used in & operation : thanks to Dan Carpenter and smatch ]
    
    Cc: Dan Carpenter <dan.carpenter@oracle.com>
    Cc: Tom Zanussi <tom.zanussi@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 38b54c5edeb9..106640b0df4a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -205,37 +205,77 @@ void trace_event_enable_cmd_record(bool enable)
 
 		if (enable) {
 			tracing_start_cmdline_record();
-			file->flags |= FTRACE_EVENT_FL_RECORDED_CMD;
+			set_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
 		} else {
 			tracing_stop_cmdline_record();
-			file->flags &= ~FTRACE_EVENT_FL_RECORDED_CMD;
+			clear_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
 		}
 	} while_for_each_event_file();
 	mutex_unlock(&event_mutex);
 }
 
-static int ftrace_event_enable_disable(struct ftrace_event_file *file,
-				       int enable)
+static int __ftrace_event_enable_disable(struct ftrace_event_file *file,
+					 int enable, int soft_disable)
 {
 	struct ftrace_event_call *call = file->event_call;
 	int ret = 0;
+	int disable;
 
 	switch (enable) {
 	case 0:
-		if (file->flags & FTRACE_EVENT_FL_ENABLED) {
-			file->flags &= ~FTRACE_EVENT_FL_ENABLED;
+		/*
+		 * When soft_disable is set and enable is cleared, we want
+		 * to clear the SOFT_DISABLED flag but leave the event in the
+		 * state that it was. That is, if the event was enabled and
+		 * SOFT_DISABLED isn't set, then do nothing. But if SOFT_DISABLED
+		 * is set we do not want the event to be enabled before we
+		 * clear the bit.
+		 *
+		 * When soft_disable is not set but the SOFT_MODE flag is,
+		 * we do nothing. Do not disable the tracepoint, otherwise
+		 * "soft enable"s (clearing the SOFT_DISABLED bit) wont work.
+		 */
+		if (soft_disable) {
+			disable = file->flags & FTRACE_EVENT_FL_SOFT_DISABLED;
+			clear_bit(FTRACE_EVENT_FL_SOFT_MODE_BIT, &file->flags);
+		} else
+			disable = !(file->flags & FTRACE_EVENT_FL_SOFT_MODE);
+
+		if (disable && (file->flags & FTRACE_EVENT_FL_ENABLED)) {
+			clear_bit(FTRACE_EVENT_FL_ENABLED_BIT, &file->flags);
 			if (file->flags & FTRACE_EVENT_FL_RECORDED_CMD) {
 				tracing_stop_cmdline_record();
-				file->flags &= ~FTRACE_EVENT_FL_RECORDED_CMD;
+				clear_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
 			}
 			call->class->reg(call, TRACE_REG_UNREGISTER, file);
 		}
+		/* If in SOFT_MODE, just set the SOFT_DISABLE_BIT */
+		if (file->flags & FTRACE_EVENT_FL_SOFT_MODE)
+			set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
 		break;
 	case 1:
+		/*
+		 * When soft_disable is set and enable is set, we want to
+		 * register the tracepoint for the event, but leave the event
+		 * as is. That means, if the event was already enabled, we do
+		 * nothing (but set SOFT_MODE). If the event is disabled, we
+		 * set SOFT_DISABLED before enabling the event tracepoint, so
+		 * it still seems to be disabled.
+		 */
+		if (!soft_disable)
+			clear_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
+		else
+			set_bit(FTRACE_EVENT_FL_SOFT_MODE_BIT, &file->flags);
+
 		if (!(file->flags & FTRACE_EVENT_FL_ENABLED)) {
+
+			/* Keep the event disabled, when going to SOFT_MODE. */
+			if (soft_disable)
+				set_bit(FTRACE_EVENT_FL_SOFT_DISABLED_BIT, &file->flags);
+
 			if (trace_flags & TRACE_ITER_RECORD_CMD) {
 				tracing_start_cmdline_record();
-				file->flags |= FTRACE_EVENT_FL_RECORDED_CMD;
+				set_bit(FTRACE_EVENT_FL_RECORDED_CMD_BIT, &file->flags);
 			}
 			ret = call->class->reg(call, TRACE_REG_REGISTER, file);
 			if (ret) {
@@ -244,7 +284,7 @@ static int ftrace_event_enable_disable(struct ftrace_event_file *file,
 					"%s\n", call->name);
 				break;
 			}
-			file->flags |= FTRACE_EVENT_FL_ENABLED;
+			set_bit(FTRACE_EVENT_FL_ENABLED_BIT, &file->flags);
 
 			/* WAS_ENABLED gets set but never cleared. */
 			call->flags |= TRACE_EVENT_FL_WAS_ENABLED;
@@ -255,6 +295,12 @@ static int ftrace_event_enable_disable(struct ftrace_event_file *file,
 	return ret;
 }
 
+static int ftrace_event_enable_disable(struct ftrace_event_file *file,
+				       int enable)
+{
+	return __ftrace_event_enable_disable(file, enable, 0);
+}
+
 static void ftrace_clear_events(struct trace_array *tr)
 {
 	struct ftrace_event_file *file;
@@ -547,12 +593,15 @@ event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	struct ftrace_event_file *file = filp->private_data;
 	char *buf;
 
-	if (file->flags & FTRACE_EVENT_FL_ENABLED)
-		buf = "1\n";
-	else
+	if (file->flags & FTRACE_EVENT_FL_ENABLED) {
+		if (file->flags & FTRACE_EVENT_FL_SOFT_DISABLED)
+			buf = "0*\n";
+		else
+			buf = "1\n";
+	} else
 		buf = "0\n";
 
-	return simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, strlen(buf));
 }
 
 static ssize_t

commit 55034cd6e648155393b0d665eef76b38d49ad6bf
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Thu Mar 7 22:48:09 2013 -0500

    tracing: Add alloc_snapshot kernel command line parameter
    
    If debugging the kernel, and the developer wants to use
    tracing_snapshot() in places where tracing_snapshot_alloc() may
    be difficult (or more likely, the developer is lazy and doesn't
    want to bother with tracing_snapshot_alloc() at all), then adding
    
      alloc_snapshot
    
    to the kernel command line parameter will tell ftrace to allocate
    the snapshot buffer (if configured) when it allocates the main
    tracing buffer.
    
    I also noticed that ring_buffer_expanded and tracing_selftest_disabled
    had inconsistent use of boolean "true" and "false" with "0" and "1".
    I cleaned that up too.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a376ab5eec5c..38b54c5edeb9 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1844,8 +1844,8 @@ static char bootup_event_buf[COMMAND_LINE_SIZE] __initdata;
 static __init int setup_trace_event(char *str)
 {
 	strlcpy(bootup_event_buf, str, COMMAND_LINE_SIZE);
-	ring_buffer_expanded = 1;
-	tracing_selftest_disabled = 1;
+	ring_buffer_expanded = true;
+	tracing_selftest_disabled = true;
 
 	return 1;
 }

commit 873c642f5964b260480850040dec21e42d0ae4e4
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Mon Mar 4 23:26:06 2013 -0500

    tracing: Clear all trace buffers when unloaded module event was used
    
    Currently we do not know what buffer a module event was enabled in.
    On unload, it is safest to clear all buffer instances, not just the
    top level buffer.
    
    Todo: Clear only the buffer that the event was used in. The
    infrastructure is there to do this, but it makes the code a bit
    more complex. Lets get the current code vetted before we add that.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9a7dc4bf1171..a376ab5eec5c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1649,14 +1649,18 @@ static void trace_module_remove_events(struct module *mod)
 		list_del(&file_ops->list);
 		kfree(file_ops);
 	}
+	up_write(&trace_event_mutex);
 
 	/*
 	 * It is safest to reset the ring buffer if the module being unloaded
-	 * registered any events that were used.
+	 * registered any events that were used. The only worry is if
+	 * a new module gets loaded, and takes on the same id as the events
+	 * of this module. When printing out the buffer, traced events left
+	 * over from this module may be passed to the new module events and
+	 * unexpected results may occur.
 	 */
 	if (clear_trace)
-		tracing_reset_current_online_cpus();
-	up_write(&trace_event_mutex);
+		tracing_reset_all_online_cpus();
 }
 
 static int trace_module_notify(struct notifier_block *self,

commit 575380da8b46969a2c6a7e14a51056a63b30fe2e
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Mon Mar 4 23:05:12 2013 -0500

    tracing: Only clear trace buffer on module unload if event was traced
    
    Currently, when a module with events is unloaded, the trace buffer is
    cleared. This is just a safety net in case the module might have some
    strange callback when its event is outputted. But there's no reason
    to reset the buffer if the module didn't have any of its events traced.
    
    Add a flag to the event "call" structure called WAS_ENABLED and gets set
    when the event is ever enabled, and this flag never gets cleared. When a
    module gets unloaded, if any of its events have this flag set, then the
    trace buffer will get cleared.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0f1307a29fcf..9a7dc4bf1171 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -245,6 +245,9 @@ static int ftrace_event_enable_disable(struct ftrace_event_file *file,
 				break;
 			}
 			file->flags |= FTRACE_EVENT_FL_ENABLED;
+
+			/* WAS_ENABLED gets set but never cleared. */
+			call->flags |= TRACE_EVENT_FL_WAS_ENABLED;
 		}
 		break;
 	}
@@ -1626,12 +1629,13 @@ static void trace_module_remove_events(struct module *mod)
 {
 	struct ftrace_module_file_ops *file_ops;
 	struct ftrace_event_call *call, *p;
-	bool found = false;
+	bool clear_trace = false;
 
 	down_write(&trace_event_mutex);
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
 		if (call->mod == mod) {
-			found = true;
+			if (call->flags & TRACE_EVENT_FL_WAS_ENABLED)
+				clear_trace = true;
 			__trace_remove_event_call(call);
 		}
 	}
@@ -1648,9 +1652,9 @@ static void trace_module_remove_events(struct module *mod)
 
 	/*
 	 * It is safest to reset the ring buffer if the module being unloaded
-	 * registered any events.
+	 * registered any events that were used.
 	 */
-	if (found)
+	if (clear_trace)
 		tracing_reset_current_online_cpus();
 	up_write(&trace_event_mutex);
 }

commit 315326c16ad08771fe0f075a08a18c99976f29f5
Author: Steven Rostedt (Red Hat) <srostedt@redhat.com>
Date:   Sat Mar 2 17:37:14 2013 -0500

    tracing: Fix trace events build without modules
    
    The new multi-buffers added a descriptor that kept track of module
    events, and the directories they use, with struct ftace_module_file_ops.
    This is used to add a ref count to keep modules from unloading while
    their files are being accessed.
    
    As the descriptor is only needed when CONFIG_MODULES is enabled, it
    is only declared when the config is enabled. But that struct is
    dereferenced in a few areas outside the #ifdef CONFIG_MODULES.
    
    By adding some helper routines and moving code around a little,
    events can be compiled again without modules.
    
    Reported-by: Fengguang Wu <fengguang.wu@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 63b4bdf84593..0f1307a29fcf 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1546,9 +1546,18 @@ struct ftrace_module_file_ops {
 	struct file_operations		filter;
 };
 
-static struct ftrace_module_file_ops *find_ftrace_file_ops(struct module *mod)
+static struct ftrace_module_file_ops *
+find_ftrace_file_ops(struct ftrace_module_file_ops *file_ops, struct module *mod)
 {
-	struct ftrace_module_file_ops *file_ops;
+	/*
+	 * As event_calls are added in groups by module,
+	 * when we find one file_ops, we don't need to search for
+	 * each call in that module, as the rest should be the
+	 * same. Only search for a new one if the last one did
+	 * not match.
+	 */
+	if (file_ops && mod == file_ops->mod)
+		return file_ops;
 
 	list_for_each_entry(file_ops, &ftrace_module_file_list, list) {
 		if (file_ops->mod == mod)
@@ -1664,16 +1673,35 @@ static int trace_module_notify(struct notifier_block *self,
 
 	return 0;
 }
+
+static int
+__trace_add_new_mod_event(struct ftrace_event_call *call,
+			  struct trace_array *tr,
+			  struct ftrace_module_file_ops *file_ops)
+{
+	return __trace_add_new_event(call, tr,
+				     &file_ops->id, &file_ops->enable,
+				     &file_ops->filter, &file_ops->format);
+}
+
 #else
-static struct ftrace_module_file_ops *find_ftrace_file_ops(struct module *mod)
+static inline struct ftrace_module_file_ops *
+find_ftrace_file_ops(struct ftrace_module_file_ops *file_ops, struct module *mod)
 {
 	return NULL;
 }
-static int trace_module_notify(struct notifier_block *self,
-			       unsigned long val, void *data)
+static inline int trace_module_notify(struct notifier_block *self,
+				      unsigned long val, void *data)
 {
 	return 0;
 }
+static inline int
+__trace_add_new_mod_event(struct ftrace_event_call *call,
+			  struct trace_array *tr,
+			  struct ftrace_module_file_ops *file_ops)
+{
+	return -ENODEV;
+}
 #endif /* CONFIG_MODULES */
 
 /* Create a new event directory structure for a trace directory. */
@@ -1692,20 +1720,11 @@ __trace_add_event_dirs(struct trace_array *tr)
 			 * want the module to disappear when reading one
 			 * of these files). The file_ops keep account of
 			 * the module ref count.
-			 *
-			 * As event_calls are added in groups by module,
-			 * when we find one file_ops, we don't need to search for
-			 * each call in that module, as the rest should be the
-			 * same. Only search for a new one if the last one did
-			 * not match.
 			 */
-			if (!file_ops || call->mod != file_ops->mod)
-				file_ops = find_ftrace_file_ops(call->mod);
+			file_ops = find_ftrace_file_ops(file_ops, call->mod);
 			if (!file_ops)
 				continue; /* Warn? */
-			ret = __trace_add_new_event(call, tr,
-					&file_ops->id, &file_ops->enable,
-					&file_ops->filter, &file_ops->format);
+			ret = __trace_add_new_mod_event(call, tr, file_ops);
 			if (ret < 0)
 				pr_warning("Could not create directory for event %s\n",
 					   call->name);
@@ -1794,9 +1813,7 @@ __add_event_to_tracers(struct ftrace_event_call *call,
 
 	list_for_each_entry(tr, &ftrace_trace_arrays, list) {
 		if (file_ops)
-			__trace_add_new_event(call, tr,
-					      &file_ops->id, &file_ops->enable,
-					      &file_ops->filter, &file_ops->format);
+			__trace_add_new_mod_event(call, tr, file_ops);
 		else
 			__trace_add_new_event(call, tr,
 					      &ftrace_event_id_fops,

commit 92edca073c374f66b8eee20ec6426fb0cdb6c4d5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Feb 27 20:41:37 2013 -0500

    tracing: Use direct field, type and system names
    
    The names used to display the field and type in the event format
    files are copied, as well as the system name that is displayed.
    
    All these names are created by constant values passed in.
    If one of theses values were to be removed by a module, the module
    would also be required to remove any event it created.
    
    By using the strings directly, we can save over 100K of memory.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5d8845d36fa8..63b4bdf84593 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -72,13 +72,8 @@ static int __trace_define_field(struct list_head *head, const char *type,
 	if (!field)
 		goto err;
 
-	field->name = kstrdup(name, GFP_KERNEL);
-	if (!field->name)
-		goto err;
-
-	field->type = kstrdup(type, GFP_KERNEL);
-	if (!field->type)
-		goto err;
+	field->name = name;
+	field->type = type;
 
 	if (filter_type == FILTER_OTHER)
 		field->filter_type = filter_assign_type(type);
@@ -94,8 +89,6 @@ static int __trace_define_field(struct list_head *head, const char *type,
 	return 0;
 
 err:
-	if (field)
-		kfree(field->name);
 	kmem_cache_free(field_cachep, field);
 
 	return -ENOMEM;
@@ -146,8 +139,6 @@ void trace_destroy_fields(struct ftrace_event_call *call)
 	head = trace_get_fields(call);
 	list_for_each_entry_safe(field, next, head, link) {
 		list_del(&field->link);
-		kfree(field->type);
-		kfree(field->name);
 		kmem_cache_free(field_cachep, field);
 	}
 }
@@ -286,7 +277,6 @@ static void __put_system(struct event_subsystem *system)
 		kfree(filter->filter_string);
 		kfree(filter);
 	}
-	kfree(system->name);
 	kfree(system);
 }
 
@@ -1202,10 +1192,7 @@ create_new_subsystem(const char *name)
 		return NULL;
 
 	system->ref_count = 1;
-	system->name = kstrdup(name, GFP_KERNEL);
-
-	if (!system->name)
-		goto out_free;
+	system->name = name;
 
 	system->filter = NULL;
 
@@ -1218,7 +1205,6 @@ create_new_subsystem(const char *name)
 	return system;
 
  out_free:
-	kfree(system->name);
 	kfree(system);
 	return NULL;
 }

commit d1a291437f75f6c841819b7855d95a21958cc822
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Feb 27 20:23:57 2013 -0500

    tracing: Use kmem_cache_alloc instead of kmalloc in trace_events.c
    
    The event structures used by the trace events are mostly persistent,
    but they are also allocated by kmalloc, which is not the best at
    allocating space for what is used. By converting these kmallocs
    into kmem_cache_allocs, we can save over 50K of space that is
    permanently allocated.
    
    After boot we have:
    
     slab name          active allocated size
     ---------          ------ --------- ----
    ftrace_event_file    979   1005     56   67    1
    ftrace_event_field   2301   2310     48   77    1
    
    The ftrace_event_file has at boot up 979 active objects out of
    1005 allocated in the slabs. Each object is 56 bytes. In a normal
    kmalloc, that would allocate 64 bytes for each object.
    
     1005 - 979  = 26 objects not used
     26 * 56 = 1456 bytes wasted
    
    But if we used kmalloc:
    
     64 - 56 = 8 bytes unused per allocation
     8 * 979 = 7832 bytes wasted
    
     7832 - 1456 = 6376 bytes in savings
    
    Doing the same for ftrace_event_field where there's 2301 objects
    allocated in a slab that can hold 2310 with 48 bytes each we have:
    
     2310 - 2301 = 9 objects not used
     9 * 48 = 432 bytes wasted
    
    A kmalloc would also use 64 bytes per object:
    
     64 - 48 = 16 bytes unused per allocation
     16 * 2301 = 36816 bytes wasted!
    
     36816 - 432 = 36384 bytes in savings
    
    This change gives us a total of 42760 bytes in savings. At least
    on my machine, but as there's a lot of these persistent objects
    for all configurations that use trace points, this is a net win.
    
    Thanks to Ezequiel Garcia for his trace_analyze presentation which
    pointed out the wasted space in my code.
    
    Cc: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 21fe83b4106a..5d8845d36fa8 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -36,6 +36,11 @@ EXPORT_SYMBOL_GPL(event_storage);
 LIST_HEAD(ftrace_events);
 LIST_HEAD(ftrace_common_fields);
 
+#define GFP_TRACE (GFP_KERNEL | __GFP_ZERO)
+
+static struct kmem_cache *field_cachep;
+static struct kmem_cache *file_cachep;
+
 /* Double loops, do not use break, only goto's work */
 #define do_for_each_event_file(tr, file)			\
 	list_for_each_entry(tr, &ftrace_trace_arrays, list) {	\
@@ -63,7 +68,7 @@ static int __trace_define_field(struct list_head *head, const char *type,
 {
 	struct ftrace_event_field *field;
 
-	field = kzalloc(sizeof(*field), GFP_KERNEL);
+	field = kmem_cache_alloc(field_cachep, GFP_TRACE);
 	if (!field)
 		goto err;
 
@@ -91,7 +96,7 @@ static int __trace_define_field(struct list_head *head, const char *type,
 err:
 	if (field)
 		kfree(field->name);
-	kfree(field);
+	kmem_cache_free(field_cachep, field);
 
 	return -ENOMEM;
 }
@@ -143,7 +148,7 @@ void trace_destroy_fields(struct ftrace_event_call *call)
 		list_del(&field->link);
 		kfree(field->type);
 		kfree(field->name);
-		kfree(field);
+		kmem_cache_free(field_cachep, field);
 	}
 }
 
@@ -1383,7 +1388,7 @@ static void remove_event_from_tracers(struct ftrace_event_call *call)
 		list_del(&file->list);
 		debugfs_remove_recursive(file->dir);
 		remove_subsystem(file->system);
-		kfree(file);
+		kmem_cache_free(file_cachep, file);
 
 		/*
 		 * The do_for_each_event_file_safe() is
@@ -1462,7 +1467,7 @@ __trace_add_new_event(struct ftrace_event_call *call,
 {
 	struct ftrace_event_file *file;
 
-	file = kzalloc(sizeof(*file), GFP_KERNEL);
+	file = kmem_cache_alloc(file_cachep, GFP_TRACE);
 	if (!file)
 		return -ENOMEM;
 
@@ -1484,7 +1489,7 @@ __trace_early_add_new_event(struct ftrace_event_call *call,
 {
 	struct ftrace_event_file *file;
 
-	file = kzalloc(sizeof(*file), GFP_KERNEL);
+	file = kmem_cache_alloc(file_cachep, GFP_TRACE);
 	if (!file)
 		return -ENOMEM;
 
@@ -1791,7 +1796,7 @@ __trace_remove_event_dirs(struct trace_array *tr)
 		list_del(&file->list);
 		debugfs_remove_recursive(file->dir);
 		remove_subsystem(file->system);
-		kfree(file);
+		kmem_cache_free(file_cachep, file);
 	}
 }
 
@@ -1947,6 +1952,13 @@ int event_trace_del_tracer(struct trace_array *tr)
 	return 0;
 }
 
+static __init int event_trace_memsetup(void)
+{
+	field_cachep = KMEM_CACHE(ftrace_event_field, SLAB_PANIC);
+	file_cachep = KMEM_CACHE(ftrace_event_file, SLAB_PANIC);
+	return 0;
+}
+
 static __init int event_trace_enable(void)
 {
 	struct trace_array *tr = top_trace_array();
@@ -2021,6 +2033,7 @@ static __init int event_trace_init(void)
 
 	return 0;
 }
+early_initcall(event_trace_memsetup);
 core_initcall(event_trace_enable);
 fs_initcall(event_trace_init);
 

commit 772482216f170ddc62fa92a3cc3271cdd1993525
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Feb 27 16:28:06 2013 -0500

    tracing: Get trace_events kernel command line working again
    
    With the new descriptors used to allow multiple buffers in the
    tracing directory added, the kernel command line parameter
    trace_events=... no longer works. This is because the top level
    (global) trace array now has a list of descriptors associated
    with the events and the files in the debugfs directory. But in
    early bootup, when the command line is processed and the events
    enabled, the trace array list of events has not been set up yet.
    
    Without the list of events in the trace array, the setting of
    events to record will fail because it would not match any events.
    
    The solution is to set up the top level array in two stages.
    The first is to just add the ftrace file descriptors that just point
    to the events. This will allow events to be enabled and start tracing.
    The second stage is called after the filesystem is set up, and this
    stage will create the debugfs event files and directories associated
    with the trace array events.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 06d6bc275221..21fe83b4106a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1473,6 +1473,28 @@ __trace_add_new_event(struct ftrace_event_call *call,
 	return event_create_dir(tr->event_dir, file, id, enable, filter, format);
 }
 
+/*
+ * Just create a decriptor for early init. A descriptor is required
+ * for enabling events at boot. We want to enable events before
+ * the filesystem is initialized.
+ */
+static __init int
+__trace_early_add_new_event(struct ftrace_event_call *call,
+			    struct trace_array *tr)
+{
+	struct ftrace_event_file *file;
+
+	file = kzalloc(sizeof(*file), GFP_KERNEL);
+	if (!file)
+		return -ENOMEM;
+
+	file->event_call = call;
+	file->tr = tr;
+	list_add(&file->list, &tr->events);
+
+	return 0;
+}
+
 struct ftrace_module_file_ops;
 static void __add_event_to_tracers(struct ftrace_event_call *call,
 				   struct ftrace_module_file_ops *file_ops);
@@ -1709,6 +1731,56 @@ __trace_add_event_dirs(struct trace_array *tr)
 	}
 }
 
+/*
+ * The top level array has already had its ftrace_event_file
+ * descriptors created in order to allow for early events to
+ * be recorded. This function is called after the debugfs has been
+ * initialized, and we now have to create the files associated
+ * to the events.
+ */
+static __init void
+__trace_early_add_event_dirs(struct trace_array *tr)
+{
+	struct ftrace_event_file *file;
+	int ret;
+
+
+	list_for_each_entry(file, &tr->events, list) {
+		ret = event_create_dir(tr->event_dir, file,
+				       &ftrace_event_id_fops,
+				       &ftrace_enable_fops,
+				       &ftrace_event_filter_fops,
+				       &ftrace_event_format_fops);
+		if (ret < 0)
+			pr_warning("Could not create directory for event %s\n",
+				   file->event_call->name);
+	}
+}
+
+/*
+ * For early boot up, the top trace array requires to have
+ * a list of events that can be enabled. This must be done before
+ * the filesystem is set up in order to allow events to be traced
+ * early.
+ */
+static __init void
+__trace_early_add_events(struct trace_array *tr)
+{
+	struct ftrace_event_call *call;
+	int ret;
+
+	list_for_each_entry(call, &ftrace_events, list) {
+		/* Early boot up should not have any modules loaded */
+		if (WARN_ON_ONCE(call->mod))
+			continue;
+
+		ret = __trace_early_add_new_event(call, tr);
+		if (ret < 0)
+			pr_warning("Could not create early event %s\n",
+				   call->name);
+	}
+}
+
 /* Remove the event directory structure for a trace directory. */
 static void
 __trace_remove_event_dirs(struct trace_array *tr)
@@ -1763,25 +1835,23 @@ static __init int setup_trace_event(char *str)
 }
 __setup("trace_event=", setup_trace_event);
 
-int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
+/* Expects to have event_mutex held when called */
+static int
+create_event_toplevel_files(struct dentry *parent, struct trace_array *tr)
 {
 	struct dentry *d_events;
 	struct dentry *entry;
 
-	mutex_lock(&event_mutex);
-
 	entry = debugfs_create_file("set_event", 0644, parent,
 				    tr, &ftrace_set_event_fops);
 	if (!entry) {
 		pr_warning("Could not create debugfs 'set_event' entry\n");
-		mutex_unlock(&event_mutex);
 		return -ENOMEM;
 	}
 
 	d_events = debugfs_create_dir("events", parent);
 	if (!d_events) {
 		pr_warning("Could not create debugfs 'events' directory\n");
-		mutex_unlock(&event_mutex);
 		return -ENOMEM;
 	}
 
@@ -1798,13 +1868,64 @@ int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
 			  tr, &ftrace_tr_enable_fops);
 
 	tr->event_dir = d_events;
+
+	return 0;
+}
+
+/**
+ * event_trace_add_tracer - add a instance of a trace_array to events
+ * @parent: The parent dentry to place the files/directories for events in
+ * @tr: The trace array associated with these events
+ *
+ * When a new instance is created, it needs to set up its events
+ * directory, as well as other files associated with events. It also
+ * creates the event hierachry in the @parent/events directory.
+ *
+ * Returns 0 on success.
+ */
+int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
+{
+	int ret;
+
+	mutex_lock(&event_mutex);
+
+	ret = create_event_toplevel_files(parent, tr);
+	if (ret)
+		goto out_unlock;
+
 	down_write(&trace_event_mutex);
 	__trace_add_event_dirs(tr);
 	up_write(&trace_event_mutex);
 
+ out_unlock:
 	mutex_unlock(&event_mutex);
 
-	return 0;
+	return ret;
+}
+
+/*
+ * The top trace array already had its file descriptors created.
+ * Now the files themselves need to be created.
+ */
+static __init int
+early_event_add_tracer(struct dentry *parent, struct trace_array *tr)
+{
+	int ret;
+
+	mutex_lock(&event_mutex);
+
+	ret = create_event_toplevel_files(parent, tr);
+	if (ret)
+		goto out_unlock;
+
+	down_write(&trace_event_mutex);
+	__trace_early_add_event_dirs(tr);
+	up_write(&trace_event_mutex);
+
+ out_unlock:
+	mutex_unlock(&event_mutex);
+
+	return ret;
 }
 
 int event_trace_del_tracer(struct trace_array *tr)
@@ -1842,6 +1963,14 @@ static __init int event_trace_enable(void)
 			list_add(&call->list, &ftrace_events);
 	}
 
+	/*
+	 * We need the top trace array to have a working set of trace
+	 * points at early init, before the debug files and directories
+	 * are created. Create the file entries now, and attach them
+	 * to the actual file dentries later.
+	 */
+	__trace_early_add_events(tr);
+
 	while (true) {
 		token = strsep(&buf, ",");
 
@@ -1882,7 +2011,7 @@ static __init int event_trace_init(void)
 	if (trace_define_common_fields())
 		pr_warning("tracing: Failed to allocate common fields");
 
-	ret = event_trace_add_tracer(d_tracer, tr);
+	ret = early_event_add_tracer(d_tracer, tr);
 	if (ret)
 		return ret;
 

commit 0c8916c34203734d3b05953ebace52d7c2969f16
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 7 16:14:16 2012 -0400

    tracing: Add rmdir to remove multibuffer instances
    
    Add a method to the hijacked dentry descriptor of the
    "instances" directory to allow for rmdir to remove an
    instance of a multibuffer.
    
    Example:
    
      cd /debug/tracing/instances
      mkdir hello
      ls
    hello/
      rmdir hello
      ls
    
    Like the mkdir method, the i_mutex is dropped for the instances
    directory. The instances directory is created at boot up and can
    not be renamed or removed. The trace_types_lock mutex is used to
    synchronize adding and removing of instances.
    
    I've run several stress tests with different threads trying to
    create and delete directories of the same name, and it has stood
    up fine.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 58a61302a733..06d6bc275221 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1709,6 +1709,20 @@ __trace_add_event_dirs(struct trace_array *tr)
 	}
 }
 
+/* Remove the event directory structure for a trace directory. */
+static void
+__trace_remove_event_dirs(struct trace_array *tr)
+{
+	struct ftrace_event_file *file, *next;
+
+	list_for_each_entry_safe(file, next, &tr->events, list) {
+		list_del(&file->list);
+		debugfs_remove_recursive(file->dir);
+		remove_subsystem(file->system);
+		kfree(file);
+	}
+}
+
 static void
 __add_event_to_tracers(struct ftrace_event_call *call,
 		       struct ftrace_module_file_ops *file_ops)
@@ -1793,6 +1807,25 @@ int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
 	return 0;
 }
 
+int event_trace_del_tracer(struct trace_array *tr)
+{
+	/* Disable any running events */
+	__ftrace_set_clr_event(tr, NULL, NULL, NULL, 0);
+
+	mutex_lock(&event_mutex);
+
+	down_write(&trace_event_mutex);
+	__trace_remove_event_dirs(tr);
+	debugfs_remove_recursive(tr->event_dir);
+	up_write(&trace_event_mutex);
+
+	tr->event_dir = NULL;
+
+	mutex_unlock(&event_mutex);
+
+	return 0;
+}
+
 static __init int event_trace_enable(void)
 {
 	struct trace_array *tr = top_trace_array();

commit 277ba04461c2746cf935353474c0961161951b68
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Aug 3 16:10:49 2012 -0400

    tracing: Add interface to allow multiple trace buffers
    
    Add the interface ("instances" directory) to add multiple buffers
    to ftrace. To create a new instance, simply do a mkdir in the
    instances directory:
    
    This will create a directory with the following:
    
     # cd instances
     # mkdir foo
     # ls foo
    buffer_size_kb        free_buffer  trace_clock    trace_pipe
    buffer_total_size_kb  set_event    trace_marker   tracing_enabled
    events/               trace        trace_options  tracing_on
    
    Currently only events are able to be set, and there isn't a way
    to delete a buffer when one is created (yet).
    
    Note, the i_mutex lock is dropped from the parent "instances"
    directory during the mkdir operation. As the "instances" directory
    can not be renamed or deleted (created on boot), I do not see
    any harm in dropping the lock. The creation of the sub directories
    is protected by trace_types_lock mutex, which only lets one
    instance get into the code path at a time. If two tasks try to
    create or delete directories of the same name, only one will occur
    and the other will fail with -EEXIST.
    
    Cc: Al Viro <viro@zeniv.linux.org.uk>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 439955239bae..58a61302a733 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1754,16 +1754,22 @@ int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
 	struct dentry *d_events;
 	struct dentry *entry;
 
+	mutex_lock(&event_mutex);
+
 	entry = debugfs_create_file("set_event", 0644, parent,
 				    tr, &ftrace_set_event_fops);
 	if (!entry) {
 		pr_warning("Could not create debugfs 'set_event' entry\n");
+		mutex_unlock(&event_mutex);
 		return -ENOMEM;
 	}
 
 	d_events = debugfs_create_dir("events", parent);
-	if (!d_events)
+	if (!d_events) {
 		pr_warning("Could not create debugfs 'events' directory\n");
+		mutex_unlock(&event_mutex);
+		return -ENOMEM;
+	}
 
 	/* ring buffer internal formats */
 	trace_create_file("header_page", 0444, d_events,
@@ -1778,7 +1784,11 @@ int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
 			  tr, &ftrace_tr_enable_fops);
 
 	tr->event_dir = d_events;
+	down_write(&trace_event_mutex);
 	__trace_add_event_dirs(tr);
+	up_write(&trace_event_mutex);
+
+	mutex_unlock(&event_mutex);
 
 	return 0;
 }

commit ae63b31e4d0e2ec09c569306ea46f664508ef717
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 3 23:09:03 2012 -0400

    tracing: Separate out trace events from global variables
    
    The trace events for ftrace are all defined via global variables.
    The arrays of events and event systems are linked to a global list.
    This prevents multiple users of the event system (what to enable and
    what not to).
    
    By adding descriptors to represent the event/file relation, as well
    as to which trace_array descriptor they are associated with, allows
    for more than one set of events to be defined. Once the trace events
    files have a link between the trace event and the trace_array they
    are associated with, we can create multiple trace_arrays that can
    record separate events in separate buffers.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 57e9b284250c..439955239bae 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -36,6 +36,19 @@ EXPORT_SYMBOL_GPL(event_storage);
 LIST_HEAD(ftrace_events);
 LIST_HEAD(ftrace_common_fields);
 
+/* Double loops, do not use break, only goto's work */
+#define do_for_each_event_file(tr, file)			\
+	list_for_each_entry(tr, &ftrace_trace_arrays, list) {	\
+		list_for_each_entry(file, &tr->events, list)
+
+#define do_for_each_event_file_safe(tr, file)			\
+	list_for_each_entry(tr, &ftrace_trace_arrays, list) {	\
+		struct ftrace_event_file *___n;				\
+		list_for_each_entry_safe(file, ___n, &tr->events, list)
+
+#define while_for_each_event_file()		\
+	}
+
 struct list_head *
 trace_get_fields(struct ftrace_event_call *event_call)
 {
@@ -149,15 +162,17 @@ EXPORT_SYMBOL_GPL(trace_event_raw_init);
 int ftrace_event_reg(struct ftrace_event_call *call,
 		     enum trace_reg type, void *data)
 {
+	struct ftrace_event_file *file = data;
+
 	switch (type) {
 	case TRACE_REG_REGISTER:
 		return tracepoint_probe_register(call->name,
 						 call->class->probe,
-						 call);
+						 file);
 	case TRACE_REG_UNREGISTER:
 		tracepoint_probe_unregister(call->name,
 					    call->class->probe,
-					    call);
+					    file);
 		return 0;
 
 #ifdef CONFIG_PERF_EVENTS
@@ -183,54 +198,57 @@ EXPORT_SYMBOL_GPL(ftrace_event_reg);
 
 void trace_event_enable_cmd_record(bool enable)
 {
-	struct ftrace_event_call *call;
+	struct ftrace_event_file *file;
+	struct trace_array *tr;
 
 	mutex_lock(&event_mutex);
-	list_for_each_entry(call, &ftrace_events, list) {
-		if (!(call->flags & TRACE_EVENT_FL_ENABLED))
+	do_for_each_event_file(tr, file) {
+
+		if (!(file->flags & FTRACE_EVENT_FL_ENABLED))
 			continue;
 
 		if (enable) {
 			tracing_start_cmdline_record();
-			call->flags |= TRACE_EVENT_FL_RECORDED_CMD;
+			file->flags |= FTRACE_EVENT_FL_RECORDED_CMD;
 		} else {
 			tracing_stop_cmdline_record();
-			call->flags &= ~TRACE_EVENT_FL_RECORDED_CMD;
+			file->flags &= ~FTRACE_EVENT_FL_RECORDED_CMD;
 		}
-	}
+	} while_for_each_event_file();
 	mutex_unlock(&event_mutex);
 }
 
-static int ftrace_event_enable_disable(struct ftrace_event_call *call,
-					int enable)
+static int ftrace_event_enable_disable(struct ftrace_event_file *file,
+				       int enable)
 {
+	struct ftrace_event_call *call = file->event_call;
 	int ret = 0;
 
 	switch (enable) {
 	case 0:
-		if (call->flags & TRACE_EVENT_FL_ENABLED) {
-			call->flags &= ~TRACE_EVENT_FL_ENABLED;
-			if (call->flags & TRACE_EVENT_FL_RECORDED_CMD) {
+		if (file->flags & FTRACE_EVENT_FL_ENABLED) {
+			file->flags &= ~FTRACE_EVENT_FL_ENABLED;
+			if (file->flags & FTRACE_EVENT_FL_RECORDED_CMD) {
 				tracing_stop_cmdline_record();
-				call->flags &= ~TRACE_EVENT_FL_RECORDED_CMD;
+				file->flags &= ~FTRACE_EVENT_FL_RECORDED_CMD;
 			}
-			call->class->reg(call, TRACE_REG_UNREGISTER, NULL);
+			call->class->reg(call, TRACE_REG_UNREGISTER, file);
 		}
 		break;
 	case 1:
-		if (!(call->flags & TRACE_EVENT_FL_ENABLED)) {
+		if (!(file->flags & FTRACE_EVENT_FL_ENABLED)) {
 			if (trace_flags & TRACE_ITER_RECORD_CMD) {
 				tracing_start_cmdline_record();
-				call->flags |= TRACE_EVENT_FL_RECORDED_CMD;
+				file->flags |= FTRACE_EVENT_FL_RECORDED_CMD;
 			}
-			ret = call->class->reg(call, TRACE_REG_REGISTER, NULL);
+			ret = call->class->reg(call, TRACE_REG_REGISTER, file);
 			if (ret) {
 				tracing_stop_cmdline_record();
 				pr_info("event trace: Could not enable event "
 					"%s\n", call->name);
 				break;
 			}
-			call->flags |= TRACE_EVENT_FL_ENABLED;
+			file->flags |= FTRACE_EVENT_FL_ENABLED;
 		}
 		break;
 	}
@@ -238,13 +256,13 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 	return ret;
 }
 
-static void ftrace_clear_events(void)
+static void ftrace_clear_events(struct trace_array *tr)
 {
-	struct ftrace_event_call *call;
+	struct ftrace_event_file *file;
 
 	mutex_lock(&event_mutex);
-	list_for_each_entry(call, &ftrace_events, list) {
-		ftrace_event_enable_disable(call, 0);
+	list_for_each_entry(file, &tr->events, list) {
+		ftrace_event_enable_disable(file, 0);
 	}
 	mutex_unlock(&event_mutex);
 }
@@ -257,6 +275,8 @@ static void __put_system(struct event_subsystem *system)
 	if (--system->ref_count)
 		return;
 
+	list_del(&system->list);
+
 	if (filter) {
 		kfree(filter->filter_string);
 		kfree(filter);
@@ -271,24 +291,45 @@ static void __get_system(struct event_subsystem *system)
 	system->ref_count++;
 }
 
-static void put_system(struct event_subsystem *system)
+static void __get_system_dir(struct ftrace_subsystem_dir *dir)
+{
+	WARN_ON_ONCE(dir->ref_count == 0);
+	dir->ref_count++;
+	__get_system(dir->subsystem);
+}
+
+static void __put_system_dir(struct ftrace_subsystem_dir *dir)
+{
+	WARN_ON_ONCE(dir->ref_count == 0);
+	/* If the subsystem is about to be freed, the dir must be too */
+	WARN_ON_ONCE(dir->subsystem->ref_count == 1 && dir->ref_count != 1);
+
+	__put_system(dir->subsystem);
+	if (!--dir->ref_count)
+		kfree(dir);
+}
+
+static void put_system(struct ftrace_subsystem_dir *dir)
 {
 	mutex_lock(&event_mutex);
-	__put_system(system);
+	__put_system_dir(dir);
 	mutex_unlock(&event_mutex);
 }
 
 /*
  * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.
  */
-static int __ftrace_set_clr_event(const char *match, const char *sub,
-				  const char *event, int set)
+static int __ftrace_set_clr_event(struct trace_array *tr, const char *match,
+				  const char *sub, const char *event, int set)
 {
+	struct ftrace_event_file *file;
 	struct ftrace_event_call *call;
 	int ret = -EINVAL;
 
 	mutex_lock(&event_mutex);
-	list_for_each_entry(call, &ftrace_events, list) {
+	list_for_each_entry(file, &tr->events, list) {
+
+		call = file->event_call;
 
 		if (!call->name || !call->class || !call->class->reg)
 			continue;
@@ -307,7 +348,7 @@ static int __ftrace_set_clr_event(const char *match, const char *sub,
 		if (event && strcmp(event, call->name) != 0)
 			continue;
 
-		ftrace_event_enable_disable(call, set);
+		ftrace_event_enable_disable(file, set);
 
 		ret = 0;
 	}
@@ -316,7 +357,7 @@ static int __ftrace_set_clr_event(const char *match, const char *sub,
 	return ret;
 }
 
-static int ftrace_set_clr_event(char *buf, int set)
+static int ftrace_set_clr_event(struct trace_array *tr, char *buf, int set)
 {
 	char *event = NULL, *sub = NULL, *match;
 
@@ -344,7 +385,7 @@ static int ftrace_set_clr_event(char *buf, int set)
 			event = NULL;
 	}
 
-	return __ftrace_set_clr_event(match, sub, event, set);
+	return __ftrace_set_clr_event(tr, match, sub, event, set);
 }
 
 /**
@@ -361,7 +402,9 @@ static int ftrace_set_clr_event(char *buf, int set)
  */
 int trace_set_clr_event(const char *system, const char *event, int set)
 {
-	return __ftrace_set_clr_event(NULL, system, event, set);
+	struct trace_array *tr = top_trace_array();
+
+	return __ftrace_set_clr_event(tr, NULL, system, event, set);
 }
 EXPORT_SYMBOL_GPL(trace_set_clr_event);
 
@@ -373,6 +416,8 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 		   size_t cnt, loff_t *ppos)
 {
 	struct trace_parser parser;
+	struct seq_file *m = file->private_data;
+	struct trace_array *tr = m->private;
 	ssize_t read, ret;
 
 	if (!cnt)
@@ -395,7 +440,7 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 
 		parser.buffer[parser.idx] = 0;
 
-		ret = ftrace_set_clr_event(parser.buffer + !set, set);
+		ret = ftrace_set_clr_event(tr, parser.buffer + !set, set);
 		if (ret)
 			goto out_put;
 	}
@@ -411,17 +456,20 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 static void *
 t_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_call *call = v;
+	struct ftrace_event_file *file = v;
+	struct ftrace_event_call *call;
+	struct trace_array *tr = m->private;
 
 	(*pos)++;
 
-	list_for_each_entry_continue(call, &ftrace_events, list) {
+	list_for_each_entry_continue(file, &tr->events, list) {
+		call = file->event_call;
 		/*
 		 * The ftrace subsystem is for showing formats only.
 		 * They can not be enabled or disabled via the event files.
 		 */
 		if (call->class && call->class->reg)
-			return call;
+			return file;
 	}
 
 	return NULL;
@@ -429,30 +477,32 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
-	struct ftrace_event_call *call;
+	struct ftrace_event_file *file;
+	struct trace_array *tr = m->private;
 	loff_t l;
 
 	mutex_lock(&event_mutex);
 
-	call = list_entry(&ftrace_events, struct ftrace_event_call, list);
+	file = list_entry(&tr->events, struct ftrace_event_file, list);
 	for (l = 0; l <= *pos; ) {
-		call = t_next(m, call, &l);
-		if (!call)
+		file = t_next(m, file, &l);
+		if (!file)
 			break;
 	}
-	return call;
+	return file;
 }
 
 static void *
 s_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_call *call = v;
+	struct ftrace_event_file *file = v;
+	struct trace_array *tr = m->private;
 
 	(*pos)++;
 
-	list_for_each_entry_continue(call, &ftrace_events, list) {
-		if (call->flags & TRACE_EVENT_FL_ENABLED)
-			return call;
+	list_for_each_entry_continue(file, &tr->events, list) {
+		if (file->flags & FTRACE_EVENT_FL_ENABLED)
+			return file;
 	}
 
 	return NULL;
@@ -460,23 +510,25 @@ s_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *s_start(struct seq_file *m, loff_t *pos)
 {
-	struct ftrace_event_call *call;
+	struct ftrace_event_file *file;
+	struct trace_array *tr = m->private;
 	loff_t l;
 
 	mutex_lock(&event_mutex);
 
-	call = list_entry(&ftrace_events, struct ftrace_event_call, list);
+	file = list_entry(&tr->events, struct ftrace_event_file, list);
 	for (l = 0; l <= *pos; ) {
-		call = s_next(m, call, &l);
-		if (!call)
+		file = s_next(m, file, &l);
+		if (!file)
 			break;
 	}
-	return call;
+	return file;
 }
 
 static int t_show(struct seq_file *m, void *v)
 {
-	struct ftrace_event_call *call = v;
+	struct ftrace_event_file *file = v;
+	struct ftrace_event_call *call = file->event_call;
 
 	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
 		seq_printf(m, "%s:", call->class->system);
@@ -494,10 +546,10 @@ static ssize_t
 event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
-	struct ftrace_event_call *call = filp->private_data;
+	struct ftrace_event_file *file = filp->private_data;
 	char *buf;
 
-	if (call->flags & TRACE_EVENT_FL_ENABLED)
+	if (file->flags & FTRACE_EVENT_FL_ENABLED)
 		buf = "1\n";
 	else
 		buf = "0\n";
@@ -509,10 +561,13 @@ static ssize_t
 event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
-	struct ftrace_event_call *call = filp->private_data;
+	struct ftrace_event_file *file = filp->private_data;
 	unsigned long val;
 	int ret;
 
+	if (!file)
+		return -EINVAL;
+
 	ret = kstrtoul_from_user(ubuf, cnt, 10, &val);
 	if (ret)
 		return ret;
@@ -525,7 +580,7 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	case 0:
 	case 1:
 		mutex_lock(&event_mutex);
-		ret = ftrace_event_enable_disable(call, val);
+		ret = ftrace_event_enable_disable(file, val);
 		mutex_unlock(&event_mutex);
 		break;
 
@@ -543,14 +598,18 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
 	const char set_to_char[4] = { '?', '0', '1', 'X' };
-	struct event_subsystem *system = filp->private_data;
+	struct ftrace_subsystem_dir *dir = filp->private_data;
+	struct event_subsystem *system = dir->subsystem;
 	struct ftrace_event_call *call;
+	struct ftrace_event_file *file;
+	struct trace_array *tr = dir->tr;
 	char buf[2];
 	int set = 0;
 	int ret;
 
 	mutex_lock(&event_mutex);
-	list_for_each_entry(call, &ftrace_events, list) {
+	list_for_each_entry(file, &tr->events, list) {
+		call = file->event_call;
 		if (!call->name || !call->class || !call->class->reg)
 			continue;
 
@@ -562,7 +621,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		 * or if all events or cleared, or if we have
 		 * a mixture.
 		 */
-		set |= (1 << !!(call->flags & TRACE_EVENT_FL_ENABLED));
+		set |= (1 << !!(file->flags & FTRACE_EVENT_FL_ENABLED));
 
 		/*
 		 * If we have a mixture, no need to look further.
@@ -584,7 +643,8 @@ static ssize_t
 system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		    loff_t *ppos)
 {
-	struct event_subsystem *system = filp->private_data;
+	struct ftrace_subsystem_dir *dir = filp->private_data;
+	struct event_subsystem *system = dir->subsystem;
 	const char *name = NULL;
 	unsigned long val;
 	ssize_t ret;
@@ -607,7 +667,7 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	if (system)
 		name = system->name;
 
-	ret = __ftrace_set_clr_event(NULL, name, NULL, val);
+	ret = __ftrace_set_clr_event(dir->tr, NULL, name, NULL, val);
 	if (ret)
 		goto out;
 
@@ -845,43 +905,75 @@ static LIST_HEAD(event_subsystems);
 static int subsystem_open(struct inode *inode, struct file *filp)
 {
 	struct event_subsystem *system = NULL;
+	struct ftrace_subsystem_dir *dir = NULL; /* Initialize for gcc */
+	struct trace_array *tr;
 	int ret;
 
-	if (!inode->i_private)
-		goto skip_search;
-
 	/* Make sure the system still exists */
 	mutex_lock(&event_mutex);
-	list_for_each_entry(system, &event_subsystems, list) {
-		if (system == inode->i_private) {
-			/* Don't open systems with no events */
-			if (!system->nr_events) {
-				system = NULL;
-				break;
+	list_for_each_entry(tr, &ftrace_trace_arrays, list) {
+		list_for_each_entry(dir, &tr->systems, list) {
+			if (dir == inode->i_private) {
+				/* Don't open systems with no events */
+				if (dir->nr_events) {
+					__get_system_dir(dir);
+					system = dir->subsystem;
+				}
+				goto exit_loop;
 			}
-			__get_system(system);
-			break;
 		}
 	}
+ exit_loop:
 	mutex_unlock(&event_mutex);
 
-	if (system != inode->i_private)
+	if (!system)
 		return -ENODEV;
 
- skip_search:
+	/* Some versions of gcc think dir can be uninitialized here */
+	WARN_ON(!dir);
+
 	ret = tracing_open_generic(inode, filp);
-	if (ret < 0 && system)
-		put_system(system);
+	if (ret < 0)
+		put_system(dir);
+
+	return ret;
+}
+
+static int system_tr_open(struct inode *inode, struct file *filp)
+{
+	struct ftrace_subsystem_dir *dir;
+	struct trace_array *tr = inode->i_private;
+	int ret;
+
+	/* Make a temporary dir that has no system but points to tr */
+	dir = kzalloc(sizeof(*dir), GFP_KERNEL);
+	if (!dir)
+		return -ENOMEM;
+
+	dir->tr = tr;
+
+	ret = tracing_open_generic(inode, filp);
+	if (ret < 0)
+		kfree(dir);
+
+	filp->private_data = dir;
 
 	return ret;
 }
 
 static int subsystem_release(struct inode *inode, struct file *file)
 {
-	struct event_subsystem *system = inode->i_private;
+	struct ftrace_subsystem_dir *dir = file->private_data;
 
-	if (system)
-		put_system(system);
+	/*
+	 * If dir->subsystem is NULL, then this is a temporary
+	 * descriptor that was made for a trace_array to enable
+	 * all subsystems.
+	 */
+	if (dir->subsystem)
+		put_system(dir);
+	else
+		kfree(dir);
 
 	return 0;
 }
@@ -890,7 +982,8 @@ static ssize_t
 subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 		      loff_t *ppos)
 {
-	struct event_subsystem *system = filp->private_data;
+	struct ftrace_subsystem_dir *dir = filp->private_data;
+	struct event_subsystem *system = dir->subsystem;
 	struct trace_seq *s;
 	int r;
 
@@ -915,7 +1008,7 @@ static ssize_t
 subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		       loff_t *ppos)
 {
-	struct event_subsystem *system = filp->private_data;
+	struct ftrace_subsystem_dir *dir = filp->private_data;
 	char *buf;
 	int err;
 
@@ -932,7 +1025,7 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	}
 	buf[cnt] = '\0';
 
-	err = apply_subsystem_event_filter(system, buf);
+	err = apply_subsystem_event_filter(dir, buf);
 	free_page((unsigned long) buf);
 	if (err < 0)
 		return err;
@@ -1041,30 +1134,35 @@ static const struct file_operations ftrace_system_enable_fops = {
 	.release = subsystem_release,
 };
 
+static const struct file_operations ftrace_tr_enable_fops = {
+	.open = system_tr_open,
+	.read = system_enable_read,
+	.write = system_enable_write,
+	.llseek = default_llseek,
+	.release = subsystem_release,
+};
+
 static const struct file_operations ftrace_show_header_fops = {
 	.open = tracing_open_generic,
 	.read = show_header,
 	.llseek = default_llseek,
 };
 
-static struct dentry *event_trace_events_dir(void)
+static int
+ftrace_event_open(struct inode *inode, struct file *file,
+		  const struct seq_operations *seq_ops)
 {
-	static struct dentry *d_tracer;
-	static struct dentry *d_events;
-
-	if (d_events)
-		return d_events;
-
-	d_tracer = tracing_init_dentry();
-	if (!d_tracer)
-		return NULL;
+	struct seq_file *m;
+	int ret;
 
-	d_events = debugfs_create_dir("events", d_tracer);
-	if (!d_events)
-		pr_warning("Could not create debugfs "
-			   "'events' directory\n");
+	ret = seq_open(file, seq_ops);
+	if (ret < 0)
+		return ret;
+	m = file->private_data;
+	/* copy tr over to seq ops */
+	m->private = inode->i_private;
 
-	return d_events;
+	return ret;
 }
 
 static int
@@ -1072,117 +1170,169 @@ ftrace_event_avail_open(struct inode *inode, struct file *file)
 {
 	const struct seq_operations *seq_ops = &show_event_seq_ops;
 
-	return seq_open(file, seq_ops);
+	return ftrace_event_open(inode, file, seq_ops);
 }
 
 static int
 ftrace_event_set_open(struct inode *inode, struct file *file)
 {
 	const struct seq_operations *seq_ops = &show_set_event_seq_ops;
+	struct trace_array *tr = inode->i_private;
 
 	if ((file->f_mode & FMODE_WRITE) &&
 	    (file->f_flags & O_TRUNC))
-		ftrace_clear_events();
+		ftrace_clear_events(tr);
 
-	return seq_open(file, seq_ops);
+	return ftrace_event_open(inode, file, seq_ops);
+}
+
+static struct event_subsystem *
+create_new_subsystem(const char *name)
+{
+	struct event_subsystem *system;
+
+	/* need to create new entry */
+	system = kmalloc(sizeof(*system), GFP_KERNEL);
+	if (!system)
+		return NULL;
+
+	system->ref_count = 1;
+	system->name = kstrdup(name, GFP_KERNEL);
+
+	if (!system->name)
+		goto out_free;
+
+	system->filter = NULL;
+
+	system->filter = kzalloc(sizeof(struct event_filter), GFP_KERNEL);
+	if (!system->filter)
+		goto out_free;
+
+	list_add(&system->list, &event_subsystems);
+
+	return system;
+
+ out_free:
+	kfree(system->name);
+	kfree(system);
+	return NULL;
 }
 
 static struct dentry *
-event_subsystem_dir(const char *name, struct dentry *d_events)
+event_subsystem_dir(struct trace_array *tr, const char *name,
+		    struct ftrace_event_file *file, struct dentry *parent)
 {
+	struct ftrace_subsystem_dir *dir;
 	struct event_subsystem *system;
 	struct dentry *entry;
 
 	/* First see if we did not already create this dir */
-	list_for_each_entry(system, &event_subsystems, list) {
+	list_for_each_entry(dir, &tr->systems, list) {
+		system = dir->subsystem;
 		if (strcmp(system->name, name) == 0) {
-			system->nr_events++;
-			return system->entry;
+			dir->nr_events++;
+			file->system = dir;
+			return dir->entry;
 		}
 	}
 
-	/* need to create new entry */
-	system = kmalloc(sizeof(*system), GFP_KERNEL);
-	if (!system) {
-		pr_warning("No memory to create event subsystem %s\n",
-			   name);
-		return d_events;
+	/* Now see if the system itself exists. */
+	list_for_each_entry(system, &event_subsystems, list) {
+		if (strcmp(system->name, name) == 0)
+			break;
 	}
+	/* Reset system variable when not found */
+	if (&system->list == &event_subsystems)
+		system = NULL;
 
-	system->entry = debugfs_create_dir(name, d_events);
-	if (!system->entry) {
-		pr_warning("Could not create event subsystem %s\n",
-			   name);
-		kfree(system);
-		return d_events;
-	}
+	dir = kmalloc(sizeof(*dir), GFP_KERNEL);
+	if (!dir)
+		goto out_fail;
 
-	system->nr_events = 1;
-	system->ref_count = 1;
-	system->name = kstrdup(name, GFP_KERNEL);
-	if (!system->name) {
-		debugfs_remove(system->entry);
-		kfree(system);
-		return d_events;
+	if (!system) {
+		system = create_new_subsystem(name);
+		if (!system)
+			goto out_free;
+	} else
+		__get_system(system);
+
+	dir->entry = debugfs_create_dir(name, parent);
+	if (!dir->entry) {
+		pr_warning("Failed to create system directory %s\n", name);
+		__put_system(system);
+		goto out_free;
 	}
 
-	list_add(&system->list, &event_subsystems);
-
-	system->filter = NULL;
-
-	system->filter = kzalloc(sizeof(struct event_filter), GFP_KERNEL);
-	if (!system->filter) {
-		pr_warning("Could not allocate filter for subsystem "
-			   "'%s'\n", name);
-		return system->entry;
-	}
+	dir->tr = tr;
+	dir->ref_count = 1;
+	dir->nr_events = 1;
+	dir->subsystem = system;
+	file->system = dir;
 
-	entry = debugfs_create_file("filter", 0644, system->entry, system,
+	entry = debugfs_create_file("filter", 0644, dir->entry, dir,
 				    &ftrace_subsystem_filter_fops);
 	if (!entry) {
 		kfree(system->filter);
 		system->filter = NULL;
-		pr_warning("Could not create debugfs "
-			   "'%s/filter' entry\n", name);
+		pr_warning("Could not create debugfs '%s/filter' entry\n", name);
 	}
 
-	trace_create_file("enable", 0644, system->entry, system,
+	trace_create_file("enable", 0644, dir->entry, dir,
 			  &ftrace_system_enable_fops);
 
-	return system->entry;
+	list_add(&dir->list, &tr->systems);
+
+	return dir->entry;
+
+ out_free:
+	kfree(dir);
+ out_fail:
+	/* Only print this message if failed on memory allocation */
+	if (!dir || !system)
+		pr_warning("No memory to create event subsystem %s\n",
+			   name);
+	return NULL;
 }
 
 static int
-event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
+event_create_dir(struct dentry *parent,
+		 struct ftrace_event_file *file,
 		 const struct file_operations *id,
 		 const struct file_operations *enable,
 		 const struct file_operations *filter,
 		 const struct file_operations *format)
 {
+	struct ftrace_event_call *call = file->event_call;
+	struct trace_array *tr = file->tr;
 	struct list_head *head;
+	struct dentry *d_events;
 	int ret;
 
 	/*
 	 * If the trace point header did not define TRACE_SYSTEM
 	 * then the system would be called "TRACE_SYSTEM".
 	 */
-	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
-		d_events = event_subsystem_dir(call->class->system, d_events);
-
-	call->dir = debugfs_create_dir(call->name, d_events);
-	if (!call->dir) {
-		pr_warning("Could not create debugfs "
-			   "'%s' directory\n", call->name);
+	if (strcmp(call->class->system, TRACE_SYSTEM) != 0) {
+		d_events = event_subsystem_dir(tr, call->class->system, file, parent);
+		if (!d_events)
+			return -ENOMEM;
+	} else
+		d_events = parent;
+
+	file->dir = debugfs_create_dir(call->name, d_events);
+	if (!file->dir) {
+		pr_warning("Could not create debugfs '%s' directory\n",
+			   call->name);
 		return -1;
 	}
 
 	if (call->class->reg && !(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))
-		trace_create_file("enable", 0644, call->dir, call,
+		trace_create_file("enable", 0644, file->dir, file,
 				  enable);
 
 #ifdef CONFIG_PERF_EVENTS
 	if (call->event.type && call->class->reg)
-		trace_create_file("id", 0444, call->dir, call,
+		trace_create_file("id", 0444, file->dir, call,
 		 		  id);
 #endif
 
@@ -1196,23 +1346,76 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		if (ret < 0) {
 			pr_warning("Could not initialize trace point"
 				   " events/%s\n", call->name);
-			return ret;
+			return -1;
 		}
 	}
-	trace_create_file("filter", 0644, call->dir, call,
+	trace_create_file("filter", 0644, file->dir, call,
 			  filter);
 
-	trace_create_file("format", 0444, call->dir, call,
+	trace_create_file("format", 0444, file->dir, call,
 			  format);
 
 	return 0;
 }
 
+static void remove_subsystem(struct ftrace_subsystem_dir *dir)
+{
+	if (!dir)
+		return;
+
+	if (!--dir->nr_events) {
+		debugfs_remove_recursive(dir->entry);
+		list_del(&dir->list);
+		__put_system_dir(dir);
+	}
+}
+
+static void remove_event_from_tracers(struct ftrace_event_call *call)
+{
+	struct ftrace_event_file *file;
+	struct trace_array *tr;
+
+	do_for_each_event_file_safe(tr, file) {
+
+		if (file->event_call != call)
+			continue;
+
+		list_del(&file->list);
+		debugfs_remove_recursive(file->dir);
+		remove_subsystem(file->system);
+		kfree(file);
+
+		/*
+		 * The do_for_each_event_file_safe() is
+		 * a double loop. After finding the call for this
+		 * trace_array, we use break to jump to the next
+		 * trace_array.
+		 */
+		break;
+	} while_for_each_event_file();
+}
+
 static void event_remove(struct ftrace_event_call *call)
 {
-	ftrace_event_enable_disable(call, 0);
+	struct trace_array *tr;
+	struct ftrace_event_file *file;
+
+	do_for_each_event_file(tr, file) {
+		if (file->event_call != call)
+			continue;
+		ftrace_event_enable_disable(file, 0);
+		/*
+		 * The do_for_each_event_file() is
+		 * a double loop. After finding the call for this
+		 * trace_array, we use break to jump to the next
+		 * trace_array.
+		 */
+		break;
+	} while_for_each_event_file();
+
 	if (call->event.funcs)
 		__unregister_ftrace_event(&call->event);
+	remove_event_from_tracers(call);
 	list_del(&call->list);
 }
 
@@ -1234,61 +1437,58 @@ static int event_init(struct ftrace_event_call *call)
 }
 
 static int
-__trace_add_event_call(struct ftrace_event_call *call, struct module *mod,
-		       const struct file_operations *id,
-		       const struct file_operations *enable,
-		       const struct file_operations *filter,
-		       const struct file_operations *format)
+__register_event(struct ftrace_event_call *call, struct module *mod)
 {
-	struct dentry *d_events;
 	int ret;
 
 	ret = event_init(call);
 	if (ret < 0)
 		return ret;
 
-	d_events = event_trace_events_dir();
-	if (!d_events)
-		return -ENOENT;
-
-	ret = event_create_dir(call, d_events, id, enable, filter, format);
-	if (!ret)
-		list_add(&call->list, &ftrace_events);
+	list_add(&call->list, &ftrace_events);
 	call->mod = mod;
 
-	return ret;
+	return 0;
 }
 
+/* Add an event to a trace directory */
+static int
+__trace_add_new_event(struct ftrace_event_call *call,
+		      struct trace_array *tr,
+		      const struct file_operations *id,
+		      const struct file_operations *enable,
+		      const struct file_operations *filter,
+		      const struct file_operations *format)
+{
+	struct ftrace_event_file *file;
+
+	file = kzalloc(sizeof(*file), GFP_KERNEL);
+	if (!file)
+		return -ENOMEM;
+
+	file->event_call = call;
+	file->tr = tr;
+	list_add(&file->list, &tr->events);
+
+	return event_create_dir(tr->event_dir, file, id, enable, filter, format);
+}
+
+struct ftrace_module_file_ops;
+static void __add_event_to_tracers(struct ftrace_event_call *call,
+				   struct ftrace_module_file_ops *file_ops);
+
 /* Add an additional event_call dynamically */
 int trace_add_event_call(struct ftrace_event_call *call)
 {
 	int ret;
 	mutex_lock(&event_mutex);
-	ret = __trace_add_event_call(call, NULL, &ftrace_event_id_fops,
-				     &ftrace_enable_fops,
-				     &ftrace_event_filter_fops,
-				     &ftrace_event_format_fops);
-	mutex_unlock(&event_mutex);
-	return ret;
-}
 
-static void remove_subsystem_dir(const char *name)
-{
-	struct event_subsystem *system;
+	ret = __register_event(call, NULL);
+	if (ret >= 0)
+		__add_event_to_tracers(call, NULL);
 
-	if (strcmp(name, TRACE_SYSTEM) == 0)
-		return;
-
-	list_for_each_entry(system, &event_subsystems, list) {
-		if (strcmp(system->name, name) == 0) {
-			if (!--system->nr_events) {
-				debugfs_remove_recursive(system->entry);
-				list_del(&system->list);
-				__put_system(system);
-			}
-			break;
-		}
-	}
+	mutex_unlock(&event_mutex);
+	return ret;
 }
 
 /*
@@ -1299,8 +1499,6 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 	event_remove(call);
 	trace_destroy_fields(call);
 	destroy_preds(call);
-	debugfs_remove_recursive(call->dir);
-	remove_subsystem_dir(call->class->system);
 }
 
 /* Remove an event_call */
@@ -1335,6 +1533,17 @@ struct ftrace_module_file_ops {
 	struct file_operations		filter;
 };
 
+static struct ftrace_module_file_ops *find_ftrace_file_ops(struct module *mod)
+{
+	struct ftrace_module_file_ops *file_ops;
+
+	list_for_each_entry(file_ops, &ftrace_module_file_list, list) {
+		if (file_ops->mod == mod)
+			return file_ops;
+	}
+	return NULL;
+}
+
 static struct ftrace_module_file_ops *
 trace_create_file_ops(struct module *mod)
 {
@@ -1386,9 +1595,8 @@ static void trace_module_add_events(struct module *mod)
 		return;
 
 	for_each_event(call, start, end) {
-		__trace_add_event_call(*call, mod,
-				       &file_ops->id, &file_ops->enable,
-				       &file_ops->filter, &file_ops->format);
+		__register_event(*call, mod);
+		__add_event_to_tracers(*call, file_ops);
 	}
 }
 
@@ -1444,6 +1652,10 @@ static int trace_module_notify(struct notifier_block *self,
 	return 0;
 }
 #else
+static struct ftrace_module_file_ops *find_ftrace_file_ops(struct module *mod)
+{
+	return NULL;
+}
 static int trace_module_notify(struct notifier_block *self,
 			       unsigned long val, void *data)
 {
@@ -1451,6 +1663,72 @@ static int trace_module_notify(struct notifier_block *self,
 }
 #endif /* CONFIG_MODULES */
 
+/* Create a new event directory structure for a trace directory. */
+static void
+__trace_add_event_dirs(struct trace_array *tr)
+{
+	struct ftrace_module_file_ops *file_ops = NULL;
+	struct ftrace_event_call *call;
+	int ret;
+
+	list_for_each_entry(call, &ftrace_events, list) {
+		if (call->mod) {
+			/*
+			 * Directories for events by modules need to
+			 * keep module ref counts when opened (as we don't
+			 * want the module to disappear when reading one
+			 * of these files). The file_ops keep account of
+			 * the module ref count.
+			 *
+			 * As event_calls are added in groups by module,
+			 * when we find one file_ops, we don't need to search for
+			 * each call in that module, as the rest should be the
+			 * same. Only search for a new one if the last one did
+			 * not match.
+			 */
+			if (!file_ops || call->mod != file_ops->mod)
+				file_ops = find_ftrace_file_ops(call->mod);
+			if (!file_ops)
+				continue; /* Warn? */
+			ret = __trace_add_new_event(call, tr,
+					&file_ops->id, &file_ops->enable,
+					&file_ops->filter, &file_ops->format);
+			if (ret < 0)
+				pr_warning("Could not create directory for event %s\n",
+					   call->name);
+			continue;
+		}
+		ret = __trace_add_new_event(call, tr,
+					    &ftrace_event_id_fops,
+					    &ftrace_enable_fops,
+					    &ftrace_event_filter_fops,
+					    &ftrace_event_format_fops);
+		if (ret < 0)
+			pr_warning("Could not create directory for event %s\n",
+				   call->name);
+	}
+}
+
+static void
+__add_event_to_tracers(struct ftrace_event_call *call,
+		       struct ftrace_module_file_ops *file_ops)
+{
+	struct trace_array *tr;
+
+	list_for_each_entry(tr, &ftrace_trace_arrays, list) {
+		if (file_ops)
+			__trace_add_new_event(call, tr,
+					      &file_ops->id, &file_ops->enable,
+					      &file_ops->filter, &file_ops->format);
+		else
+			__trace_add_new_event(call, tr,
+					      &ftrace_event_id_fops,
+					      &ftrace_enable_fops,
+					      &ftrace_event_filter_fops,
+					      &ftrace_event_format_fops);
+	}
+}
+
 static struct notifier_block trace_module_nb = {
 	.notifier_call = trace_module_notify,
 	.priority = 0,
@@ -1471,8 +1749,43 @@ static __init int setup_trace_event(char *str)
 }
 __setup("trace_event=", setup_trace_event);
 
+int event_trace_add_tracer(struct dentry *parent, struct trace_array *tr)
+{
+	struct dentry *d_events;
+	struct dentry *entry;
+
+	entry = debugfs_create_file("set_event", 0644, parent,
+				    tr, &ftrace_set_event_fops);
+	if (!entry) {
+		pr_warning("Could not create debugfs 'set_event' entry\n");
+		return -ENOMEM;
+	}
+
+	d_events = debugfs_create_dir("events", parent);
+	if (!d_events)
+		pr_warning("Could not create debugfs 'events' directory\n");
+
+	/* ring buffer internal formats */
+	trace_create_file("header_page", 0444, d_events,
+			  ring_buffer_print_page_header,
+			  &ftrace_show_header_fops);
+
+	trace_create_file("header_event", 0444, d_events,
+			  ring_buffer_print_entry_header,
+			  &ftrace_show_header_fops);
+
+	trace_create_file("enable", 0644, d_events,
+			  tr, &ftrace_tr_enable_fops);
+
+	tr->event_dir = d_events;
+	__trace_add_event_dirs(tr);
+
+	return 0;
+}
+
 static __init int event_trace_enable(void)
 {
+	struct trace_array *tr = top_trace_array();
 	struct ftrace_event_call **iter, *call;
 	char *buf = bootup_event_buf;
 	char *token;
@@ -1494,7 +1807,7 @@ static __init int event_trace_enable(void)
 		if (!*token)
 			continue;
 
-		ret = ftrace_set_clr_event(token, 1);
+		ret = ftrace_set_clr_event(tr, token, 1);
 		if (ret)
 			pr_warn("Failed to enable trace event: %s\n", token);
 	}
@@ -1506,61 +1819,29 @@ static __init int event_trace_enable(void)
 
 static __init int event_trace_init(void)
 {
-	struct ftrace_event_call *call;
+	struct trace_array *tr;
 	struct dentry *d_tracer;
 	struct dentry *entry;
-	struct dentry *d_events;
 	int ret;
 
+	tr = top_trace_array();
+
 	d_tracer = tracing_init_dentry();
 	if (!d_tracer)
 		return 0;
 
 	entry = debugfs_create_file("available_events", 0444, d_tracer,
-				    NULL, &ftrace_avail_fops);
+				    tr, &ftrace_avail_fops);
 	if (!entry)
 		pr_warning("Could not create debugfs "
 			   "'available_events' entry\n");
 
-	entry = debugfs_create_file("set_event", 0644, d_tracer,
-				    NULL, &ftrace_set_event_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'set_event' entry\n");
-
-	d_events = event_trace_events_dir();
-	if (!d_events)
-		return 0;
-
-	/* ring buffer internal formats */
-	trace_create_file("header_page", 0444, d_events,
-			  ring_buffer_print_page_header,
-			  &ftrace_show_header_fops);
-
-	trace_create_file("header_event", 0444, d_events,
-			  ring_buffer_print_entry_header,
-			  &ftrace_show_header_fops);
-
-	trace_create_file("enable", 0644, d_events,
-			  NULL, &ftrace_system_enable_fops);
-
 	if (trace_define_common_fields())
 		pr_warning("tracing: Failed to allocate common fields");
 
-	/*
-	 * Early initialization already enabled ftrace event.
-	 * Now it's only necessary to create the event directory.
-	 */
-	list_for_each_entry(call, &ftrace_events, list) {
-
-		ret = event_create_dir(call, d_events,
-				       &ftrace_event_id_fops,
-				       &ftrace_enable_fops,
-				       &ftrace_event_filter_fops,
-				       &ftrace_event_format_fops);
-		if (ret < 0)
-			event_remove(call);
-	}
+	ret = event_trace_add_tracer(d_tracer, tr);
+	if (ret)
+		return ret;
 
 	ret = register_module_notifier(&trace_module_nb);
 	if (ret)
@@ -1627,13 +1908,20 @@ static __init void event_test_stuff(void)
  */
 static __init void event_trace_self_tests(void)
 {
+	struct ftrace_subsystem_dir *dir;
+	struct ftrace_event_file *file;
 	struct ftrace_event_call *call;
 	struct event_subsystem *system;
+	struct trace_array *tr;
 	int ret;
 
+	tr = top_trace_array();
+
 	pr_info("Running tests on trace events:\n");
 
-	list_for_each_entry(call, &ftrace_events, list) {
+	list_for_each_entry(file, &tr->events, list) {
+
+		call = file->event_call;
 
 		/* Only test those that have a probe */
 		if (!call->class || !call->class->probe)
@@ -1657,15 +1945,15 @@ static __init void event_trace_self_tests(void)
 		 * If an event is already enabled, someone is using
 		 * it and the self test should not be on.
 		 */
-		if (call->flags & TRACE_EVENT_FL_ENABLED) {
+		if (file->flags & FTRACE_EVENT_FL_ENABLED) {
 			pr_warning("Enabled event during self test!\n");
 			WARN_ON_ONCE(1);
 			continue;
 		}
 
-		ftrace_event_enable_disable(call, 1);
+		ftrace_event_enable_disable(file, 1);
 		event_test_stuff();
-		ftrace_event_enable_disable(call, 0);
+		ftrace_event_enable_disable(file, 0);
 
 		pr_cont("OK\n");
 	}
@@ -1674,7 +1962,9 @@ static __init void event_trace_self_tests(void)
 
 	pr_info("Running tests on trace event systems:\n");
 
-	list_for_each_entry(system, &event_subsystems, list) {
+	list_for_each_entry(dir, &tr->systems, list) {
+
+		system = dir->subsystem;
 
 		/* the ftrace system is special, skip it */
 		if (strcmp(system->name, "ftrace") == 0)
@@ -1682,7 +1972,7 @@ static __init void event_trace_self_tests(void)
 
 		pr_info("Testing event system %s: ", system->name);
 
-		ret = __ftrace_set_clr_event(NULL, system->name, NULL, 1);
+		ret = __ftrace_set_clr_event(tr, NULL, system->name, NULL, 1);
 		if (WARN_ON_ONCE(ret)) {
 			pr_warning("error enabling system %s\n",
 				   system->name);
@@ -1691,7 +1981,7 @@ static __init void event_trace_self_tests(void)
 
 		event_test_stuff();
 
-		ret = __ftrace_set_clr_event(NULL, system->name, NULL, 0);
+		ret = __ftrace_set_clr_event(tr, NULL, system->name, NULL, 0);
 		if (WARN_ON_ONCE(ret)) {
 			pr_warning("error disabling system %s\n",
 				   system->name);
@@ -1706,7 +1996,7 @@ static __init void event_trace_self_tests(void)
 	pr_info("Running tests on all trace events:\n");
 	pr_info("Testing all events: ");
 
-	ret = __ftrace_set_clr_event(NULL, NULL, NULL, 1);
+	ret = __ftrace_set_clr_event(tr, NULL, NULL, NULL, 1);
 	if (WARN_ON_ONCE(ret)) {
 		pr_warning("error enabling all events\n");
 		return;
@@ -1715,7 +2005,7 @@ static __init void event_trace_self_tests(void)
 	event_test_stuff();
 
 	/* reset sysname */
-	ret = __ftrace_set_clr_event(NULL, NULL, NULL, 0);
+	ret = __ftrace_set_clr_event(tr, NULL, NULL, NULL, 0);
 	if (WARN_ON_ONCE(ret)) {
 		pr_warning("error disabling all events\n");
 		return;

commit b000c8065a92b0fe0e1694f41b2c8d8ba7b7b1ec
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jan 18 10:31:20 2013 -0500

    tracing: Remove the extra 4 bytes of padding in events
    
    Due to a userspace issue with PowerTop v2beta, which hardcoded
    the offset of event fields that it was using, it broke when
    we removed the Big Kernel Lock counter from the event header.
    
     (commit e6e1e2593 "tracing: Remove lock_depth from event entry")
    
    Because this broke userspace, it was determined that we must
    keep those 4 bytes around.
    
     (commit a3a4a5acd "Regression: partial revert "tracing: Remove lock_depth from event entry"")
    
    This unfortunately wastes space in the ring buffer. 4 bytes per
    event, where a lot of events are just 24 bytes. That's 16% of the
    buffer wasted. A million events will add 4 megs of white space
    into the buffer.
    
    It was later noticed that PowerTop v2beta could not work on systems
    where the kernel was 64 bit but the userspace was 32 bits.
    The reason was because the offsets are different between the
    two and the hard coded offset of one would not work with the other.
    
    With PowerTop v2 final, it implemented the same interface that both
    perf and trace-cmd use. That is, it reads the format file of
    the event to find the offsets of the fields it needs. This fixes
    the problem with running powertop on a 32 bit userspace running
    on a 64 bit kernel. It also no longer requires the 4 byte padding.
    
    As PowerTop v2 has been out for a while, and is included in all
    major distributions, it is time that we can safely remove the
    4 bytes of padding. Users of PowerTop v2beta should upgrade to
    PowerTop v2 final.
    
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Acked-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 880073d0b946..57e9b284250c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -116,7 +116,6 @@ static int trace_define_common_fields(void)
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
-	__common_field(int, padding);
 
 	return ret;
 }

commit 0d5c6e1c19bab82fad4837108c2902f557d62a04
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Nov 1 20:54:21 2012 -0400

    tracing: Use irq_work for wake ups and remove *_nowake_*() functions
    
    Have the ring buffer commit function use the irq_work infrastructure to
    wake up any waiters waiting on the ring buffer for new data. The irq_work
    was created for such a purpose, where doing the actual wake up at the
    time of adding data is too dangerous, as an event or function trace may
    be in the midst of the work queue locks and cause deadlocks. The irq_work
    will either delay the action to the next timer interrupt, or trigger an IPI
    to itself forcing an interrupt to do the work (in a safe location).
    
    With irq_work, all ring buffer commits can safely do wakeups, removing
    the need for the ring buffer commit "nowake" variants, which were used
    by events and function tracing. All commits can now safely use the
    normal commit, and the "nowake" variants can be removed.
    
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index cb2df3b70f7f..880073d0b946 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1760,7 +1760,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip,
 	entry->ip			= ip;
 	entry->parent_ip		= parent_ip;
 
-	trace_nowake_buffer_unlock_commit(buffer, event, flags, pc);
+	trace_buffer_unlock_commit(buffer, event, flags, pc);
 
  out:
 	atomic_dec(&per_cpu(ftrace_test_event_disable, cpu));

commit 15075cac423d634ddf39dac66f943b3bce847f87
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 3 14:57:28 2012 -0400

    tracing: Separate open function from set_event and available_events
    
    The open function used by available_events is the same as set_event even
    though it uses different seq functions. This causes a side effect of
    writing into available_events clearing all events, even though
    available_events is suppose to be read only.
    
    There's no reason to keep a single function for just the open and have
    both use different functions for everything else. It is a little
    confusing and causes strange behavior. Just have each have their own
    function.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index dec47e70e254..cb2df3b70f7f 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -491,19 +491,6 @@ static void t_stop(struct seq_file *m, void *p)
 	mutex_unlock(&event_mutex);
 }
 
-static int
-ftrace_event_seq_open(struct inode *inode, struct file *file)
-{
-	const struct seq_operations *seq_ops;
-
-	if ((file->f_mode & FMODE_WRITE) &&
-	    (file->f_flags & O_TRUNC))
-		ftrace_clear_events();
-
-	seq_ops = inode->i_private;
-	return seq_open(file, seq_ops);
-}
-
 static ssize_t
 event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
@@ -980,6 +967,9 @@ show_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 	return r;
 }
 
+static int ftrace_event_avail_open(struct inode *inode, struct file *file);
+static int ftrace_event_set_open(struct inode *inode, struct file *file);
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -995,14 +985,14 @@ static const struct seq_operations show_set_event_seq_ops = {
 };
 
 static const struct file_operations ftrace_avail_fops = {
-	.open = ftrace_event_seq_open,
+	.open = ftrace_event_avail_open,
 	.read = seq_read,
 	.llseek = seq_lseek,
 	.release = seq_release,
 };
 
 static const struct file_operations ftrace_set_event_fops = {
-	.open = ftrace_event_seq_open,
+	.open = ftrace_event_set_open,
 	.read = seq_read,
 	.write = ftrace_event_write,
 	.llseek = seq_lseek,
@@ -1078,6 +1068,26 @@ static struct dentry *event_trace_events_dir(void)
 	return d_events;
 }
 
+static int
+ftrace_event_avail_open(struct inode *inode, struct file *file)
+{
+	const struct seq_operations *seq_ops = &show_event_seq_ops;
+
+	return seq_open(file, seq_ops);
+}
+
+static int
+ftrace_event_set_open(struct inode *inode, struct file *file)
+{
+	const struct seq_operations *seq_ops = &show_set_event_seq_ops;
+
+	if ((file->f_mode & FMODE_WRITE) &&
+	    (file->f_flags & O_TRUNC))
+		ftrace_clear_events();
+
+	return seq_open(file, seq_ops);
+}
+
 static struct dentry *
 event_subsystem_dir(const char *name, struct dentry *d_events)
 {
@@ -1508,15 +1518,13 @@ static __init int event_trace_init(void)
 		return 0;
 
 	entry = debugfs_create_file("available_events", 0444, d_tracer,
-				    (void *)&show_event_seq_ops,
-				    &ftrace_avail_fops);
+				    NULL, &ftrace_avail_fops);
 	if (!entry)
 		pr_warning("Could not create debugfs "
 			   "'available_events' entry\n");
 
 	entry = debugfs_create_file("set_event", 0644, d_tracer,
-				    (void *)&show_set_event_seq_ops,
-				    &ftrace_set_event_fops);
+				    NULL, &ftrace_set_event_fops);
 	if (!entry)
 		pr_warning("Could not create debugfs "
 			   "'set_event' entry\n");

commit 81698831bc462ff16f76bc11249a1e492424da4c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Oct 11 10:15:05 2012 -0400

    tracing: Enable comm recording if trace_printk() is used
    
    If comm recording is not enabled when trace_printk() is used then
    you just get this type of output:
    
    [ adding trace_printk("hello! %d", irq); in do_IRQ ]
    
               <...>-2843  [001] d.h.    80.812300: do_IRQ: hello! 14
               <...>-2734  [002] d.h2    80.824664: do_IRQ: hello! 14
               <...>-2713  [003] d.h.    80.829971: do_IRQ: hello! 14
               <...>-2814  [000] d.h.    80.833026: do_IRQ: hello! 14
    
    By enabling the comm recorder when trace_printk is enabled:
    
           hackbench-6715  [001] d.h.   193.233776: do_IRQ: hello! 21
                sshd-2659  [001] d.h.   193.665862: do_IRQ: hello! 21
              <idle>-0     [001] d.h1   193.665996: do_IRQ: hello! 21
    
    Suggested-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d608d09d08c0..dec47e70e254 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1489,6 +1489,9 @@ static __init int event_trace_enable(void)
 		if (ret)
 			pr_warn("Failed to enable trace event: %s\n", token);
 	}
+
+	trace_printk_start_comm();
+
 	return 0;
 }
 

commit 8781915ad2716adcd8cd5cc52cee791fc8b00fdf
Author: Ezequiel Garcia <elezegarcia@gmail.com>
Date:   Wed Sep 12 11:47:57 2012 -0300

    trace: Move trace event enable from fs_initcall to core_initcall
    
    This patch splits trace event initialization in two stages:
     * ftrace enable
     * sysfs event entry creation
    
    This allows to capture trace events from an earlier point
    by using 'trace_event' kernel parameter and is important
    to trace boot-up allocations.
    
    Note that, in order to enable events at core_initcall,
    it's necessary to move init_ftrace_syscalls() from
    core_initcall to early_initcall.
    
    Link: http://lkml.kernel.org/r/1347461277-25302-1-git-send-email-elezegarcia@gmail.com
    
    Signed-off-by: Ezequiel Garcia <elezegarcia@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index bbb0e63d78e9..d608d09d08c0 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1199,6 +1199,31 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 	return 0;
 }
 
+static void event_remove(struct ftrace_event_call *call)
+{
+	ftrace_event_enable_disable(call, 0);
+	if (call->event.funcs)
+		__unregister_ftrace_event(&call->event);
+	list_del(&call->list);
+}
+
+static int event_init(struct ftrace_event_call *call)
+{
+	int ret = 0;
+
+	if (WARN_ON(!call->name))
+		return -EINVAL;
+
+	if (call->class->raw_init) {
+		ret = call->class->raw_init(call);
+		if (ret < 0 && ret != -ENOSYS)
+			pr_warn("Could not initialize trace events/%s\n",
+				call->name);
+	}
+
+	return ret;
+}
+
 static int
 __trace_add_event_call(struct ftrace_event_call *call, struct module *mod,
 		       const struct file_operations *id,
@@ -1209,19 +1234,9 @@ __trace_add_event_call(struct ftrace_event_call *call, struct module *mod,
 	struct dentry *d_events;
 	int ret;
 
-	/* The linker may leave blanks */
-	if (!call->name)
-		return -EINVAL;
-
-	if (call->class->raw_init) {
-		ret = call->class->raw_init(call);
-		if (ret < 0) {
-			if (ret != -ENOSYS)
-				pr_warning("Could not initialize trace events/%s\n",
-					   call->name);
-			return ret;
-		}
-	}
+	ret = event_init(call);
+	if (ret < 0)
+		return ret;
 
 	d_events = event_trace_events_dir();
 	if (!d_events)
@@ -1272,13 +1287,10 @@ static void remove_subsystem_dir(const char *name)
  */
 static void __trace_remove_event_call(struct ftrace_event_call *call)
 {
-	ftrace_event_enable_disable(call, 0);
-	if (call->event.funcs)
-		__unregister_ftrace_event(&call->event);
-	debugfs_remove_recursive(call->dir);
-	list_del(&call->list);
+	event_remove(call);
 	trace_destroy_fields(call);
 	destroy_preds(call);
+	debugfs_remove_recursive(call->dir);
 	remove_subsystem_dir(call->class->system);
 }
 
@@ -1450,15 +1462,43 @@ static __init int setup_trace_event(char *str)
 }
 __setup("trace_event=", setup_trace_event);
 
+static __init int event_trace_enable(void)
+{
+	struct ftrace_event_call **iter, *call;
+	char *buf = bootup_event_buf;
+	char *token;
+	int ret;
+
+	for_each_event(iter, __start_ftrace_events, __stop_ftrace_events) {
+
+		call = *iter;
+		ret = event_init(call);
+		if (!ret)
+			list_add(&call->list, &ftrace_events);
+	}
+
+	while (true) {
+		token = strsep(&buf, ",");
+
+		if (!token)
+			break;
+		if (!*token)
+			continue;
+
+		ret = ftrace_set_clr_event(token, 1);
+		if (ret)
+			pr_warn("Failed to enable trace event: %s\n", token);
+	}
+	return 0;
+}
+
 static __init int event_trace_init(void)
 {
-	struct ftrace_event_call **call;
+	struct ftrace_event_call *call;
 	struct dentry *d_tracer;
 	struct dentry *entry;
 	struct dentry *d_events;
 	int ret;
-	char *buf = bootup_event_buf;
-	char *token;
 
 	d_tracer = tracing_init_dentry();
 	if (!d_tracer)
@@ -1497,24 +1537,19 @@ static __init int event_trace_init(void)
 	if (trace_define_common_fields())
 		pr_warning("tracing: Failed to allocate common fields");
 
-	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
-		__trace_add_event_call(*call, NULL, &ftrace_event_id_fops,
+	/*
+	 * Early initialization already enabled ftrace event.
+	 * Now it's only necessary to create the event directory.
+	 */
+	list_for_each_entry(call, &ftrace_events, list) {
+
+		ret = event_create_dir(call, d_events,
+				       &ftrace_event_id_fops,
 				       &ftrace_enable_fops,
 				       &ftrace_event_filter_fops,
 				       &ftrace_event_format_fops);
-	}
-
-	while (true) {
-		token = strsep(&buf, ",");
-
-		if (!token)
-			break;
-		if (!*token)
-			continue;
-
-		ret = ftrace_set_clr_event(token, 1);
-		if (ret)
-			pr_warning("Failed to enable trace event: %s\n", token);
+		if (ret < 0)
+			event_remove(call);
 	}
 
 	ret = register_module_notifier(&trace_module_nb);
@@ -1523,6 +1558,7 @@ static __init int event_trace_init(void)
 
 	return 0;
 }
+core_initcall(event_trace_enable);
 fs_initcall(event_trace_init);
 
 #ifdef CONFIG_FTRACE_STARTUP_TEST

commit 76bab1b78ab6f25d5f74165f94526c25fc93d984
Author: Yuanhan Liu <yuanhan.liu@linux.intel.com>
Date:   Mon Aug 27 15:13:45 2012 +0800

    tracing: Skip printing "OK" if failed to disable event
    
    No acutal case found. But logically, we should skip "OK" in case any
    error met.
    
    Link: http://lkml.kernel.org/r/1346051625-25231-1-git-send-email-yuanhan.liu@linux.intel.com
    
    Signed-off-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 6825d833a257..bbb0e63d78e9 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1646,9 +1646,11 @@ static __init void event_trace_self_tests(void)
 		event_test_stuff();
 
 		ret = __ftrace_set_clr_event(NULL, system->name, NULL, 0);
-		if (WARN_ON_ONCE(ret))
+		if (WARN_ON_ONCE(ret)) {
 			pr_warning("error disabling system %s\n",
 				   system->name);
+			continue;
+		}
 
 		pr_cont("OK\n");
 	}

commit 4740974a6844156c14d741b0080b59d275679a23
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Jul 20 11:04:44 2012 -0400

    ftrace: Add default recursion protection for function tracing
    
    As more users of the function tracer utility are being added, they do
    not always add the necessary recursion protection. To protect from
    function recursion due to tracing, if the callback ftrace_ops does not
    specifically specify that it protects against recursion (by setting
    the FTRACE_OPS_FL_RECURSION_SAFE flag), the list operation will be
    called by the mcount trampoline which adds recursion protection.
    
    If the flag is set, then the function will be called directly with no
    extra protection.
    
    Note, the list operation is called if more than one function callback
    is registered, or if the arch does not support all of the function
    tracer features.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8c6696833686..6825d833a257 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1721,6 +1721,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip,
 static struct ftrace_ops trace_ops __initdata  =
 {
 	.func = function_test_events_call,
+	.flags = FTRACE_OPS_FL_RECURSION_SAFE,
 };
 
 static __init void event_trace_self_test_with_function(void)

commit a1e2e31d175a1349274eba3465d17616c6725f8c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Aug 9 12:50:46 2011 -0400

    ftrace: Return pt_regs to function trace callback
    
    Return as the 4th paramater to the function tracer callback the pt_regs.
    
    Later patches that implement regs passing for the architectures will require
    having the ftrace_ops set the SAVE_REGS flag, which will tell the arch
    to take the time to pass a full set of pt_regs to the ftrace_ops callback
    function. If the arch does not support it then it should pass NULL.
    
    If an arch can pass full regs, then it should define:
     ARCH_SUPPORTS_FTRACE_SAVE_REGS to 1
    
    Link: http://lkml.kernel.org/r/20120702201821.019966811@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 88daa5177bf4..8c6696833686 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1682,7 +1682,7 @@ static DEFINE_PER_CPU(atomic_t, ftrace_test_event_disable);
 
 static void
 function_test_events_call(unsigned long ip, unsigned long parent_ip,
-			  struct ftrace_ops *op)
+			  struct ftrace_ops *op, struct pt_regs *pt_regs)
 {
 	struct ring_buffer_event *event;
 	struct ring_buffer *buffer;

commit 2f5f6ad9390c1ebbf738d130dbfe80b60eaa167e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 8 16:57:47 2011 -0400

    ftrace: Pass ftrace_ops as third parameter to function trace callback
    
    Currently the function trace callback receives only the ip and parent_ip
    of the function that it traced. It would be more powerful to also return
    the ops that registered the function as well. This allows the same function
    to act differently depending on what ftrace_ops registered it.
    
    Link: http://lkml.kernel.org/r/20120612225424.267254552@goodmis.org
    
    Reviewed-by: Masami Hiramatsu <masami.hiramatsu.pt@hitachi.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 29111da1d100..88daa5177bf4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1681,7 +1681,8 @@ static __init void event_trace_self_tests(void)
 static DEFINE_PER_CPU(atomic_t, ftrace_test_event_disable);
 
 static void
-function_test_events_call(unsigned long ip, unsigned long parent_ip)
+function_test_events_call(unsigned long ip, unsigned long parent_ip,
+			  struct ftrace_ops *op)
 {
 	struct ring_buffer_event *event;
 	struct ring_buffer *buffer;

commit 9b63776fa3ca96c4ecda76f6fa947b7b0add66ac
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 10 15:55:43 2012 -0400

    tracing: Do not enable function event with enable
    
    With the adding of function tracing event to perf, it caused a
    side effect that produces the following warning when enabling all
    events in ftrace:
    
     # echo 1 > /sys/kernel/debug/tracing/events/enable
    
    [console]
    event trace: Could not enable event function
    
    This is because when enabling all events via the debugfs system
    it ignores events that do not have a ->reg() function assigned.
    This was to skip over the ftrace internal events (as they are
    not TRACE_EVENTs). But as the ftrace function event now has
    a ->reg() function attached to it for use with perf, it is no
    longer ignored.
    
    Worse yet, this ->reg() function is being called when it should
    not be. It returns an error and causes the above warning to
    be printed.
    
    By adding a new event_call flag (TRACE_EVENT_FL_IGNORE_ENABLE)
    and have all ftrace internel event structures have it set,
    setting the events/enable will no longe try to incorrectly enable
    the function event and does not warn.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 079a93ae8a9d..29111da1d100 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -294,6 +294,9 @@ static int __ftrace_set_clr_event(const char *match, const char *sub,
 		if (!call->name || !call->class || !call->class->reg)
 			continue;
 
+		if (call->flags & TRACE_EVENT_FL_IGNORE_ENABLE)
+			continue;
+
 		if (match &&
 		    strcmp(match, call->name) != 0 &&
 		    strcmp(match, call->class->system) != 0)
@@ -1164,7 +1167,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		return -1;
 	}
 
-	if (call->class->reg)
+	if (call->class->reg && !(call->flags & TRACE_EVENT_FL_IGNORE_ENABLE))
 		trace_create_file("enable", 0644, call->dir, call,
 				  enable);
 

commit 489c75c3b333dfda4c8d2b7ad1b00e5da024bfa7
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:50 2012 +0100

    ftrace, perf: Add add/del tracepoint perf registration actions
    
    Adding TRACE_REG_PERF_ADD and TRACE_REG_PERF_DEL to handle
    perf event schedule in/out actions.
    
    The add action is invoked for when the perf event is scheduled in,
    while the del action is invoked when the event is scheduled out.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-4-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5138fea37908..079a93ae8a9d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -173,6 +173,8 @@ int ftrace_event_reg(struct ftrace_event_call *call,
 		return 0;
 	case TRACE_REG_PERF_OPEN:
 	case TRACE_REG_PERF_CLOSE:
+	case TRACE_REG_PERF_ADD:
+	case TRACE_REG_PERF_DEL:
 		return 0;
 #endif
 	}

commit ceec0b6fc7cd43b38a40c2d40223f9cd0616f0cd
Author: Jiri Olsa <jolsa@redhat.com>
Date:   Wed Feb 15 15:51:49 2012 +0100

    ftrace, perf: Add open/close tracepoint perf registration actions
    
    Adding TRACE_REG_PERF_OPEN and TRACE_REG_PERF_CLOSE to differentiate
    register/unregister from open/close actions.
    
    The register/unregister actions are invoked for the first/last
    tracepoint user when opening/closing the event.
    
    The open/close actions are invoked for each tracepoint user when
    opening/closing the event.
    
    Link: http://lkml.kernel.org/r/1329317514-8131-3-git-send-email-jolsa@redhat.com
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c212a7f934ec..5138fea37908 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -147,7 +147,8 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 }
 EXPORT_SYMBOL_GPL(trace_event_raw_init);
 
-int ftrace_event_reg(struct ftrace_event_call *call, enum trace_reg type)
+int ftrace_event_reg(struct ftrace_event_call *call,
+		     enum trace_reg type, void *data)
 {
 	switch (type) {
 	case TRACE_REG_REGISTER:
@@ -170,6 +171,9 @@ int ftrace_event_reg(struct ftrace_event_call *call, enum trace_reg type)
 					    call->class->perf_probe,
 					    call);
 		return 0;
+	case TRACE_REG_PERF_OPEN:
+	case TRACE_REG_PERF_CLOSE:
+		return 0;
 #endif
 	}
 	return 0;
@@ -209,7 +213,7 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 				tracing_stop_cmdline_record();
 				call->flags &= ~TRACE_EVENT_FL_RECORDED_CMD;
 			}
-			call->class->reg(call, TRACE_REG_UNREGISTER);
+			call->class->reg(call, TRACE_REG_UNREGISTER, NULL);
 		}
 		break;
 	case 1:
@@ -218,7 +222,7 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 				tracing_start_cmdline_record();
 				call->flags |= TRACE_EVENT_FL_RECORDED_CMD;
 			}
-			ret = call->class->reg(call, TRACE_REG_REGISTER);
+			ret = call->class->reg(call, TRACE_REG_REGISTER, NULL);
 			if (ret) {
 				tracing_stop_cmdline_record();
 				pr_info("event trace: Could not enable event "

commit cb59974742aea24adf6637eb0c4b8e7b48bca6fb
Author: Ilya Dryomov <idryomov@gmail.com>
Date:   Mon Oct 31 11:07:42 2011 +0200

    tracing: fix event_subsystem ref counting
    
    Fix a bug introduced by e9dbfae5, which prevents event_subsystem from
    ever being released.
    
    Ref_count was added to keep track of subsystem users, not for counting
    events.  Subsystem is created with ref_count = 1, so there is no need to
    increment it for every event, we have nr_events for that.  Fix this by
    touching ref_count only when we actually have a new user -
    subsystem_open().
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Ilya Dryomov <idryomov@gmail.com>
    Link: http://lkml.kernel.org/r/1320052062-7846-1-git-send-email-idryomov@gmail.com
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 581876f9f387..c212a7f934ec 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1078,7 +1078,6 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 	/* First see if we did not already create this dir */
 	list_for_each_entry(system, &event_subsystems, list) {
 		if (strcmp(system->name, name) == 0) {
-			__get_system(system);
 			system->nr_events++;
 			return system->entry;
 		}

commit 492f73a303b488ffd67097b2351d54aa6e6c7c73
Merge: e08fbb78f03f f7bc8b61f657
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Jul 21 09:29:14 2011 +0200

    Merge branch 'perf/urgent' into perf/core
    
    Merge reason: pick up the latest fixes - they won't make v3.0.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 40ee4dffff061399eb9358e0c8fcfbaf8de4c8fe
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jul 5 14:32:51 2011 -0400

    tracing: Have "enable" file use refcounts like the "filter" file
    
    The "enable" file for the event system can be removed when a module
    is unloaded and the event system only has events from that module.
    As the event system nr_events count goes to zero, it may be freed
    if its ref_count is also set to zero.
    
    Like the "filter" file, the "enable" file may be opened by a task and
    referenced later, after a module has been unloaded and the events for
    that event system have been removed.
    
    Although the "filter" file referenced the event system structure,
    the "enable" file only references a pointer to the event system
    name. Since the name is freed when the event system is removed,
    it is possible that an access to the "enable" file may reference
    a freed pointer.
    
    Update the "enable" file to use the subsystem_open() routine that
    the "filter" file uses, to keep a reference to the event system
    structure while the "enable" file is opened.
    
    Cc: <stable@kernel.org>
    Reported-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ffc5b2884af1..3e2a7c91c548 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -557,7 +557,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
 	const char set_to_char[4] = { '?', '0', '1', 'X' };
-	const char *system = filp->private_data;
+	struct event_subsystem *system = filp->private_data;
 	struct ftrace_event_call *call;
 	char buf[2];
 	int set = 0;
@@ -568,7 +568,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		if (!call->name || !call->class || !call->class->reg)
 			continue;
 
-		if (system && strcmp(call->class->system, system) != 0)
+		if (system && strcmp(call->class->system, system->name) != 0)
 			continue;
 
 		/*
@@ -598,7 +598,8 @@ static ssize_t
 system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		    loff_t *ppos)
 {
-	const char *system = filp->private_data;
+	struct event_subsystem *system = filp->private_data;
+	const char *name = NULL;
 	unsigned long val;
 	char buf[64];
 	ssize_t ret;
@@ -622,7 +623,14 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	if (val != 0 && val != 1)
 		return -EINVAL;
 
-	ret = __ftrace_set_clr_event(NULL, system, NULL, val);
+	/*
+	 * Opening of "enable" adds a ref count to system,
+	 * so the name is safe to use.
+	 */
+	if (system)
+		name = system->name;
+
+	ret = __ftrace_set_clr_event(NULL, name, NULL, val);
 	if (ret)
 		goto out;
 
@@ -862,6 +870,9 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 	struct event_subsystem *system = NULL;
 	int ret;
 
+	if (!inode->i_private)
+		goto skip_search;
+
 	/* Make sure the system still exists */
 	mutex_lock(&event_mutex);
 	list_for_each_entry(system, &event_subsystems, list) {
@@ -880,8 +891,9 @@ static int subsystem_open(struct inode *inode, struct file *filp)
 	if (system != inode->i_private)
 		return -ENODEV;
 
+ skip_search:
 	ret = tracing_open_generic(inode, filp);
-	if (ret < 0)
+	if (ret < 0 && system)
 		put_system(system);
 
 	return ret;
@@ -891,7 +903,8 @@ static int subsystem_release(struct inode *inode, struct file *file)
 {
 	struct event_subsystem *system = inode->i_private;
 
-	put_system(system);
+	if (system)
+		put_system(system);
 
 	return 0;
 }
@@ -1041,10 +1054,11 @@ static const struct file_operations ftrace_subsystem_filter_fops = {
 };
 
 static const struct file_operations ftrace_system_enable_fops = {
-	.open = tracing_open_generic,
+	.open = subsystem_open,
 	.read = system_enable_read,
 	.write = system_enable_write,
 	.llseek = default_llseek,
+	.release = subsystem_release,
 };
 
 static const struct file_operations ftrace_show_header_fops = {
@@ -1133,8 +1147,7 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 			   "'%s/filter' entry\n", name);
 	}
 
-	trace_create_file("enable", 0644, system->entry,
-			  (void *)system->name,
+	trace_create_file("enable", 0644, system->entry, system,
 			  &ftrace_system_enable_fops);
 
 	return system->entry;

commit e9dbfae53eeb9fc3d4bb7da3df87fa9875f5da02
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jul 5 11:36:06 2011 -0400

    tracing: Fix bug when reading system filters on module removal
    
    The event system is freed when its nr_events is set to zero. This happens
    when a module created an event system and then later the module is
    removed. Modules may share systems, so the system is allocated when
    it is created and freed when the modules are unloaded and all the
    events under the system are removed (nr_events set to zero).
    
    The problem arises when a task opened the "filter" file for the
    system. If the module is unloaded and it removed the last event for
    that system, the system structure is freed. If the task that opened
    the filter file accesses the "filter" file after the system has
    been freed, the system will access an invalid pointer.
    
    By adding a ref_count, and using it to keep track of what
    is using the event system, we can free it after all users
    are finished with the event system.
    
    Cc: <stable@kernel.org>
    Reported-by: Johannes Berg <johannes.berg@intel.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 686ec399f2a8..ffc5b2884af1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -244,6 +244,35 @@ static void ftrace_clear_events(void)
 	mutex_unlock(&event_mutex);
 }
 
+static void __put_system(struct event_subsystem *system)
+{
+	struct event_filter *filter = system->filter;
+
+	WARN_ON_ONCE(system->ref_count == 0);
+	if (--system->ref_count)
+		return;
+
+	if (filter) {
+		kfree(filter->filter_string);
+		kfree(filter);
+	}
+	kfree(system->name);
+	kfree(system);
+}
+
+static void __get_system(struct event_subsystem *system)
+{
+	WARN_ON_ONCE(system->ref_count == 0);
+	system->ref_count++;
+}
+
+static void put_system(struct event_subsystem *system)
+{
+	mutex_lock(&event_mutex);
+	__put_system(system);
+	mutex_unlock(&event_mutex);
+}
+
 /*
  * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.
  */
@@ -826,6 +855,47 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return cnt;
 }
 
+static LIST_HEAD(event_subsystems);
+
+static int subsystem_open(struct inode *inode, struct file *filp)
+{
+	struct event_subsystem *system = NULL;
+	int ret;
+
+	/* Make sure the system still exists */
+	mutex_lock(&event_mutex);
+	list_for_each_entry(system, &event_subsystems, list) {
+		if (system == inode->i_private) {
+			/* Don't open systems with no events */
+			if (!system->nr_events) {
+				system = NULL;
+				break;
+			}
+			__get_system(system);
+			break;
+		}
+	}
+	mutex_unlock(&event_mutex);
+
+	if (system != inode->i_private)
+		return -ENODEV;
+
+	ret = tracing_open_generic(inode, filp);
+	if (ret < 0)
+		put_system(system);
+
+	return ret;
+}
+
+static int subsystem_release(struct inode *inode, struct file *file)
+{
+	struct event_subsystem *system = inode->i_private;
+
+	put_system(system);
+
+	return 0;
+}
+
 static ssize_t
 subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 		      loff_t *ppos)
@@ -963,10 +1033,11 @@ static const struct file_operations ftrace_event_filter_fops = {
 };
 
 static const struct file_operations ftrace_subsystem_filter_fops = {
-	.open = tracing_open_generic,
+	.open = subsystem_open,
 	.read = subsystem_filter_read,
 	.write = subsystem_filter_write,
 	.llseek = default_llseek,
+	.release = subsystem_release,
 };
 
 static const struct file_operations ftrace_system_enable_fops = {
@@ -1002,8 +1073,6 @@ static struct dentry *event_trace_events_dir(void)
 	return d_events;
 }
 
-static LIST_HEAD(event_subsystems);
-
 static struct dentry *
 event_subsystem_dir(const char *name, struct dentry *d_events)
 {
@@ -1013,6 +1082,7 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 	/* First see if we did not already create this dir */
 	list_for_each_entry(system, &event_subsystems, list) {
 		if (strcmp(system->name, name) == 0) {
+			__get_system(system);
 			system->nr_events++;
 			return system->entry;
 		}
@@ -1035,6 +1105,7 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 	}
 
 	system->nr_events = 1;
+	system->ref_count = 1;
 	system->name = kstrdup(name, GFP_KERNEL);
 	if (!system->name) {
 		debugfs_remove(system->entry);
@@ -1184,16 +1255,9 @@ static void remove_subsystem_dir(const char *name)
 	list_for_each_entry(system, &event_subsystems, list) {
 		if (strcmp(system->name, name) == 0) {
 			if (!--system->nr_events) {
-				struct event_filter *filter = system->filter;
-
 				debugfs_remove_recursive(system->entry);
 				list_del(&system->list);
-				if (filter) {
-					kfree(filter->filter_string);
-					kfree(filter);
-				}
-				kfree(system->name);
-				kfree(system);
+				__put_system(system);
 			}
 			break;
 		}

commit 22fe9b54d859e53bfbbbdc1a0a77a82bc453927c
Author: Peter Huewe <peterhuewe@gmx.de>
Date:   Tue Jun 7 21:58:27 2011 +0200

    tracing: Convert to kstrtoul_from_user
    
    This patch replaces the code for getting an unsigned long from a
    userspace buffer by a simple call to kstroul_from_user.
    This makes it easier to read and less error prone.
    
    Signed-off-by: Peter Huewe <peterhuewe@gmx.de>
    Link: http://lkml.kernel.org/r/1307476707-14762-1-git-send-email-peterhuewe@gmx.de
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 686ec399f2a8..4d7e1498ae91 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -486,20 +486,11 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
 	struct ftrace_event_call *call = filp->private_data;
-	char buf[64];
 	unsigned long val;
 	int ret;
 
-	if (cnt >= sizeof(buf))
-		return -EINVAL;
-
-	if (copy_from_user(&buf, ubuf, cnt))
-		return -EFAULT;
-
-	buf[cnt] = 0;
-
-	ret = strict_strtoul(buf, 10, &val);
-	if (ret < 0)
+	ret = kstrtoul_from_user(ubuf, cnt, 10, &val);
+	if (ret)
 		return ret;
 
 	ret = tracing_update_buffers();
@@ -571,19 +562,10 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 {
 	const char *system = filp->private_data;
 	unsigned long val;
-	char buf[64];
 	ssize_t ret;
 
-	if (cnt >= sizeof(buf))
-		return -EINVAL;
-
-	if (copy_from_user(&buf, ubuf, cnt))
-		return -EFAULT;
-
-	buf[cnt] = 0;
-
-	ret = strict_strtoul(buf, 10, &val);
-	if (ret < 0)
+	ret = kstrtoul_from_user(ubuf, cnt, 10, &val);
+	if (ret)
 		return ret;
 
 	ret = tracing_update_buffers();

commit 17bb615ad4f8d2d2c0f02794d27d7f83e0009ef4
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon May 23 15:27:46 2011 -0400

    tracing: Have event with function tracer check error return
    
    The self tests for event tracer does not check if the function
    tracing was successfully activated. It needs to before it continues
    the tests, otherwise the wrong errors may be reported.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2fe110341359..686ec399f2a8 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1657,7 +1657,12 @@ static struct ftrace_ops trace_ops __initdata  =
 
 static __init void event_trace_self_test_with_function(void)
 {
-	register_ftrace_function(&trace_ops);
+	int ret;
+	ret = register_ftrace_function(&trace_ops);
+	if (WARN_ON(ret < 0)) {
+		pr_info("Failed to enable function tracer for event tests\n");
+		return;
+	}
 	pr_info("Running tests again, along with the function tracer\n");
 	event_trace_self_tests();
 	unregister_ftrace_function(&trace_ops);

commit a3a4a5acd3bd2f6f1e102e1f1b9d2e2bb320a7fd
Author: Arjan van de Ven <arjan@linux.intel.com>
Date:   Thu May 5 23:55:18 2011 -0400

    Regression: partial revert "tracing: Remove lock_depth from event entry"
    
    This partially reverts commit e6e1e2593592a8f6f6380496655d8c6f67431266.
    
    That commit changed the structure layout of the trace structure, which
    in turn broke PowerTOP (1.9x generation) quite badly.
    
    I appreciate not wanting to expose the variable in question, and
    PowerTOP was not using it, so I've replaced the variable with just a
    padding field - that way if in the future a new field is needed it can
    just use this padding field.
    
    Signed-off-by: Arjan van de Ven <arjan@linux.intel.com>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e88f74fe1d4c..2fe110341359 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -116,6 +116,7 @@ static int trace_define_common_fields(void)
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
+	__common_field(int, padding);
 
 	return ret;
 }

commit 56355b83e2a24ce7e1870c8479205e2cdd332225
Author: Yuanhan Liu <yuanhan.liu@linux.intel.com>
Date:   Mon Nov 8 14:05:12 2010 +0800

    tracing: Export trace_set_clr_event()
    
    Trace events belonging to a module only exists when the module is
    loaded. Well, we can use trace_set_clr_event funtion to enable some
    trace event at the module init routine, so that we will not miss
    something while loading then module.
    
    So, Export the trace_set_clr_event function so that module can use it.
    
    Signed-off-by: Yuanhan Liu <yuanhan.liu@linux.intel.com>
    LKML-Reference: <1289196312-25323-1-git-send-email-yuanhan.liu@linux.intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e1d579b19834..e88f74fe1d4c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -325,6 +325,7 @@ int trace_set_clr_event(const char *system, const char *event, int set)
 {
 	return __ftrace_set_clr_event(NULL, system, event, set);
 }
+EXPORT_SYMBOL_GPL(trace_set_clr_event);
 
 /* 128 should be much more than enough */
 #define EVENT_BUF_SIZE		127

commit e6e1e2593592a8f6f6380496655d8c6f67431266
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Mar 9 10:41:56 2011 -0500

    tracing: Remove lock_depth from event entry
    
    The lock_depth field in the event headers was added as a temporary
    data point for help in removing the BKL. Now that the BKL is pretty
    much been removed, we can remove this field.
    
    This in turn changes the header from 12 bytes to 8 bytes,
    removing the 4 byte buffer that gcc would insert if the first field
    in the data load was 8 bytes in size.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5f499e0438a4..e1d579b19834 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -116,7 +116,6 @@ static int trace_define_common_fields(void)
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
-	__common_field(int, lock_depth);
 
 	return ret;
 }

commit e4a9ea5ee7c8812a7bf0c3fb725ceeaa3d4c2fcc
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jan 27 09:15:30 2011 -0500

    tracing: Replace trace_event struct array with pointer array
    
    Currently the trace_event structures are placed in the _ftrace_events
    section, and at link time, the linker makes one large array of all
    the trace_event structures. On boot up, this array is read (much like
    the initcall sections) and the events are processed.
    
    The problem is that there is no guarantee that gcc will place complex
    structures nicely together in an array format. Two structures in the
    same file may be placed awkwardly, because gcc has no clue that they
    are suppose to be in an array.
    
    A hack was used previous to force the alignment to 4, to pack the
    structures together. But this caused alignment issues with other
    architectures (sparc).
    
    Instead of packing the structures into an array, the structures' addresses
    are now put into the _ftrace_event section. As pointers are always the
    natural alignment, gcc should always pack them tightly together
    (otherwise initcall, extable, etc would also fail).
    
    By having the pointers to the structures in the section, we can still
    iterate the trace_events without causing unnecessary alignment problems
    with other architectures, or depending on the current behaviour of
    gcc that will likely change in the future just to tick us kernel developers
    off a little more.
    
    The _ftrace_event section is also moved into the .init.data section
    as it is now only needed at boot up.
    
    Suggested-by: David Miller <davem@davemloft.net>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 35fde09b81de..5f499e0438a4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1284,7 +1284,7 @@ trace_create_file_ops(struct module *mod)
 static void trace_module_add_events(struct module *mod)
 {
 	struct ftrace_module_file_ops *file_ops = NULL;
-	struct ftrace_event_call *call, *start, *end;
+	struct ftrace_event_call **call, **start, **end;
 
 	start = mod->trace_events;
 	end = mod->trace_events + mod->num_trace_events;
@@ -1297,7 +1297,7 @@ static void trace_module_add_events(struct module *mod)
 		return;
 
 	for_each_event(call, start, end) {
-		__trace_add_event_call(call, mod,
+		__trace_add_event_call(*call, mod,
 				       &file_ops->id, &file_ops->enable,
 				       &file_ops->filter, &file_ops->format);
 	}
@@ -1367,8 +1367,8 @@ static struct notifier_block trace_module_nb = {
 	.priority = 0,
 };
 
-extern struct ftrace_event_call __start_ftrace_events[];
-extern struct ftrace_event_call __stop_ftrace_events[];
+extern struct ftrace_event_call *__start_ftrace_events[];
+extern struct ftrace_event_call *__stop_ftrace_events[];
 
 static char bootup_event_buf[COMMAND_LINE_SIZE] __initdata;
 
@@ -1384,7 +1384,7 @@ __setup("trace_event=", setup_trace_event);
 
 static __init int event_trace_init(void)
 {
-	struct ftrace_event_call *call;
+	struct ftrace_event_call **call;
 	struct dentry *d_tracer;
 	struct dentry *entry;
 	struct dentry *d_events;
@@ -1430,7 +1430,7 @@ static __init int event_trace_init(void)
 		pr_warning("tracing: Failed to allocate common fields");
 
 	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
-		__trace_add_event_call(call, NULL, &ftrace_event_id_fops,
+		__trace_add_event_call(*call, NULL, &ftrace_event_id_fops,
 				       &ftrace_enable_fops,
 				       &ftrace_event_filter_fops,
 				       &ftrace_event_format_fops);

commit 042957801626465492b9428860de39a3cb2a8219
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Nov 12 22:32:11 2010 -0500

    tracing/events: Show real number in array fields
    
    Currently we have in something like the sched_switch event:
    
      field:char prev_comm[TASK_COMM_LEN];  offset:12;      size:16;        signed:1;
    
    When a userspace tool such as perf tries to parse this, the
    TASK_COMM_LEN is meaningless. This is done because the TRACE_EVENT() macro
    simply uses a #len to show the string of the length. When the length is
    an enum, we get a string that means nothing for tools.
    
    By adding a static buffer and a mutex to protect it, we can store the
    string into that buffer with snprintf and show the actual number.
    Now we get:
    
      field:char prev_comm[16];       offset:12;      size:16;        signed:1;
    
    Something much more useful.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0725eeab1937..35fde09b81de 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -27,6 +27,12 @@
 
 DEFINE_MUTEX(event_mutex);
 
+DEFINE_MUTEX(event_storage_mutex);
+EXPORT_SYMBOL_GPL(event_storage_mutex);
+
+char event_storage[EVENT_STORAGE_SIZE];
+EXPORT_SYMBOL_GPL(event_storage);
+
 LIST_HEAD(ftrace_events);
 LIST_HEAD(ftrace_common_fields);
 

commit 092e0e7e520a1fca03e13c9f2d157432a8657ff2
Merge: 79f14b7c56d3 776c163b1b93
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Oct 22 10:52:56 2010 -0700

    Merge branch 'llseek' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/bkl
    
    * 'llseek' of git://git.kernel.org/pub/scm/linux/kernel/git/arnd/bkl:
      vfs: make no_llseek the default
      vfs: don't use BKL in default_llseek
      llseek: automatically add .llseek fop
      libfs: use generic_file_llseek for simple_attr
      mac80211: disallow seeks in minstrel debug code
      lirc: make chardev nonseekable
      viotape: use noop_llseek
      raw: use explicit llseek file operations
      ibmasmfs: use generic_file_llseek
      spufs: use llseek in all file operations
      arm/omap: use generic_file_llseek in iommu_debug
      lkdtm: use generic_file_llseek in debugfs
      net/wireless: use generic_file_llseek in debugfs
      drm: use noop_llseek

commit 6038f373a3dc1f1c26496e60b6c40b164716f07e
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sun Aug 15 18:52:59 2010 +0200

    llseek: automatically add .llseek fop
    
    All file_operations should get a .llseek operation so we can make
    nonseekable_open the default for future file operations without a
    .llseek pointer.
    
    The three cases that we can automatically detect are no_llseek, seq_lseek
    and default_llseek. For cases where we can we can automatically prove that
    the file offset is always ignored, we use noop_llseek, which maintains
    the current behavior of not returning an error from a seek.
    
    New drivers should normally not use noop_llseek but instead use no_llseek
    and call nonseekable_open at open time.  Existing drivers can be converted
    to do the same when the maintainer knows for certain that no user code
    relies on calling seek on the device file.
    
    The generated code is often incorrectly indented and right now contains
    comments that clarify for each added line why a specific variant was
    chosen. In the version that gets submitted upstream, the comments will
    be gone and I will manually fix the indentation, because there does not
    seem to be a way to do that using coccinelle.
    
    Some amount of new code is currently sitting in linux-next that should get
    the same modifications, which I will do at the end of the merge window.
    
    Many thanks to Julia Lawall for helping me learn to write a semantic
    patch that does all this.
    
    ===== begin semantic patch =====
    // This adds an llseek= method to all file operations,
    // as a preparation for making no_llseek the default.
    //
    // The rules are
    // - use no_llseek explicitly if we do nonseekable_open
    // - use seq_lseek for sequential files
    // - use default_llseek if we know we access f_pos
    // - use noop_llseek if we know we don't access f_pos,
    //   but we still want to allow users to call lseek
    //
    @ open1 exists @
    identifier nested_open;
    @@
    nested_open(...)
    {
    <+...
    nonseekable_open(...)
    ...+>
    }
    
    @ open exists@
    identifier open_f;
    identifier i, f;
    identifier open1.nested_open;
    @@
    int open_f(struct inode *i, struct file *f)
    {
    <+...
    (
    nonseekable_open(...)
    |
    nested_open(...)
    )
    ...+>
    }
    
    @ read disable optional_qualifier exists @
    identifier read_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    expression E;
    identifier func;
    @@
    ssize_t read_f(struct file *f, char *p, size_t s, loff_t *off)
    {
    <+...
    (
       *off = E
    |
       *off += E
    |
       func(..., off, ...)
    |
       E = *off
    )
    ...+>
    }
    
    @ read_no_fpos disable optional_qualifier exists @
    identifier read_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    @@
    ssize_t read_f(struct file *f, char *p, size_t s, loff_t *off)
    {
    ... when != off
    }
    
    @ write @
    identifier write_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    expression E;
    identifier func;
    @@
    ssize_t write_f(struct file *f, const char *p, size_t s, loff_t *off)
    {
    <+...
    (
      *off = E
    |
      *off += E
    |
      func(..., off, ...)
    |
      E = *off
    )
    ...+>
    }
    
    @ write_no_fpos @
    identifier write_f;
    identifier f, p, s, off;
    type ssize_t, size_t, loff_t;
    @@
    ssize_t write_f(struct file *f, const char *p, size_t s, loff_t *off)
    {
    ... when != off
    }
    
    @ fops0 @
    identifier fops;
    @@
    struct file_operations fops = {
     ...
    };
    
    @ has_llseek depends on fops0 @
    identifier fops0.fops;
    identifier llseek_f;
    @@
    struct file_operations fops = {
    ...
     .llseek = llseek_f,
    ...
    };
    
    @ has_read depends on fops0 @
    identifier fops0.fops;
    identifier read_f;
    @@
    struct file_operations fops = {
    ...
     .read = read_f,
    ...
    };
    
    @ has_write depends on fops0 @
    identifier fops0.fops;
    identifier write_f;
    @@
    struct file_operations fops = {
    ...
     .write = write_f,
    ...
    };
    
    @ has_open depends on fops0 @
    identifier fops0.fops;
    identifier open_f;
    @@
    struct file_operations fops = {
    ...
     .open = open_f,
    ...
    };
    
    // use no_llseek if we call nonseekable_open
    ////////////////////////////////////////////
    @ nonseekable1 depends on !has_llseek && has_open @
    identifier fops0.fops;
    identifier nso ~= "nonseekable_open";
    @@
    struct file_operations fops = {
    ...  .open = nso, ...
    +.llseek = no_llseek, /* nonseekable */
    };
    
    @ nonseekable2 depends on !has_llseek @
    identifier fops0.fops;
    identifier open.open_f;
    @@
    struct file_operations fops = {
    ...  .open = open_f, ...
    +.llseek = no_llseek, /* open uses nonseekable */
    };
    
    // use seq_lseek for sequential files
    /////////////////////////////////////
    @ seq depends on !has_llseek @
    identifier fops0.fops;
    identifier sr ~= "seq_read";
    @@
    struct file_operations fops = {
    ...  .read = sr, ...
    +.llseek = seq_lseek, /* we have seq_read */
    };
    
    // use default_llseek if there is a readdir
    ///////////////////////////////////////////
    @ fops1 depends on !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier readdir_e;
    @@
    // any other fop is used that changes pos
    struct file_operations fops = {
    ... .readdir = readdir_e, ...
    +.llseek = default_llseek, /* readdir is present */
    };
    
    // use default_llseek if at least one of read/write touches f_pos
    /////////////////////////////////////////////////////////////////
    @ fops2 depends on !fops1 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read.read_f;
    @@
    // read fops use offset
    struct file_operations fops = {
    ... .read = read_f, ...
    +.llseek = default_llseek, /* read accesses f_pos */
    };
    
    @ fops3 depends on !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier write.write_f;
    @@
    // write fops use offset
    struct file_operations fops = {
    ... .write = write_f, ...
    +       .llseek = default_llseek, /* write accesses f_pos */
    };
    
    // Use noop_llseek if neither read nor write accesses f_pos
    ///////////////////////////////////////////////////////////
    
    @ fops4 depends on !fops1 && !fops2 && !fops3 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read_no_fpos.read_f;
    identifier write_no_fpos.write_f;
    @@
    // write fops use offset
    struct file_operations fops = {
    ...
     .write = write_f,
     .read = read_f,
    ...
    +.llseek = noop_llseek, /* read and write both use no f_pos */
    };
    
    @ depends on has_write && !has_read && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier write_no_fpos.write_f;
    @@
    struct file_operations fops = {
    ... .write = write_f, ...
    +.llseek = noop_llseek, /* write uses no f_pos */
    };
    
    @ depends on has_read && !has_write && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    identifier read_no_fpos.read_f;
    @@
    struct file_operations fops = {
    ... .read = read_f, ...
    +.llseek = noop_llseek, /* read uses no f_pos */
    };
    
    @ depends on !has_read && !has_write && !fops1 && !fops2 && !has_llseek && !nonseekable1 && !nonseekable2 && !seq @
    identifier fops0.fops;
    @@
    struct file_operations fops = {
    ...
    +.llseek = noop_llseek, /* no read or write fn */
    };
    ===== End semantic patch =====
    
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Cc: Julia Lawall <julia@diku.dk>
    Cc: Christoph Hellwig <hch@infradead.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 4c758f146328..0369c5e09984 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -951,6 +951,7 @@ static const struct file_operations ftrace_enable_fops = {
 	.open = tracing_open_generic,
 	.read = event_enable_read,
 	.write = event_enable_write,
+	.llseek = default_llseek,
 };
 
 static const struct file_operations ftrace_event_format_fops = {
@@ -963,29 +964,34 @@ static const struct file_operations ftrace_event_format_fops = {
 static const struct file_operations ftrace_event_id_fops = {
 	.open = tracing_open_generic,
 	.read = event_id_read,
+	.llseek = default_llseek,
 };
 
 static const struct file_operations ftrace_event_filter_fops = {
 	.open = tracing_open_generic,
 	.read = event_filter_read,
 	.write = event_filter_write,
+	.llseek = default_llseek,
 };
 
 static const struct file_operations ftrace_subsystem_filter_fops = {
 	.open = tracing_open_generic,
 	.read = subsystem_filter_read,
 	.write = subsystem_filter_write,
+	.llseek = default_llseek,
 };
 
 static const struct file_operations ftrace_system_enable_fops = {
 	.open = tracing_open_generic,
 	.read = system_enable_read,
 	.write = system_enable_write,
+	.llseek = default_llseek,
 };
 
 static const struct file_operations ftrace_show_header_fops = {
 	.open = tracing_open_generic,
 	.read = show_header,
+	.llseek = default_llseek,
 };
 
 static struct dentry *event_trace_events_dir(void)

commit 86397dc3ccfc0e17b7550d05eaf15fe91f6498dd
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Aug 17 13:53:06 2010 +0800

    tracing: Clean up seqfile code for format file
    
    Remove the nasty hack that marks a pointer's LSB to distinguish common
    fields from event fields. Replace it with a more sane approach.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4C6A23C2.9020606@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 4c758f146328..398c0e8b332c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -600,21 +600,29 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 enum {
 	FORMAT_HEADER		= 1,
-	FORMAT_PRINTFMT		= 2,
+	FORMAT_FIELD_SEPERATOR	= 2,
+	FORMAT_PRINTFMT		= 3,
 };
 
 static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 {
 	struct ftrace_event_call *call = m->private;
 	struct ftrace_event_field *field;
-	struct list_head *head;
+	struct list_head *common_head = &ftrace_common_fields;
+	struct list_head *head = trace_get_fields(call);
 
 	(*pos)++;
 
 	switch ((unsigned long)v) {
 	case FORMAT_HEADER:
-		head = &ftrace_common_fields;
+		if (unlikely(list_empty(common_head)))
+			return NULL;
+
+		field = list_entry(common_head->prev,
+				   struct ftrace_event_field, link);
+		return field;
 
+	case FORMAT_FIELD_SEPERATOR:
 		if (unlikely(list_empty(head)))
 			return NULL;
 
@@ -626,31 +634,10 @@ static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 		return NULL;
 	}
 
-	head = trace_get_fields(call);
-
-	/*
-	 * To separate common fields from event fields, the
-	 * LSB is set on the first event field. Clear it in case.
-	 */
-	v = (void *)((unsigned long)v & ~1L);
-
 	field = v;
-	/*
-	 * If this is a common field, and at the end of the list, then
-	 * continue with main list.
-	 */
-	if (field->link.prev == &ftrace_common_fields) {
-		if (unlikely(list_empty(head)))
-			return NULL;
-		field = list_entry(head->prev, struct ftrace_event_field, link);
-		/* Set the LSB to notify f_show to print an extra newline */
-		field = (struct ftrace_event_field *)
-			((unsigned long)field | 1);
-		return field;
-	}
-
-	/* If we are done tell f_show to print the format */
-	if (field->link.prev == head)
+	if (field->link.prev == common_head)
+		return (void *)FORMAT_FIELD_SEPERATOR;
+	else if (field->link.prev == head)
 		return (void *)FORMAT_PRINTFMT;
 
 	field = list_entry(field->link.prev, struct ftrace_event_field, link);
@@ -688,22 +675,16 @@ static int f_show(struct seq_file *m, void *v)
 		seq_printf(m, "format:\n");
 		return 0;
 
+	case FORMAT_FIELD_SEPERATOR:
+		seq_putc(m, '\n');
+		return 0;
+
 	case FORMAT_PRINTFMT:
 		seq_printf(m, "\nprint fmt: %s\n",
 			   call->print_fmt);
 		return 0;
 	}
 
-	/*
-	 * To separate common fields from event fields, the
-	 * LSB is set on the first event field. Clear it and
-	 * print a newline if it is set.
-	 */
-	if ((unsigned long)v & 1) {
-		seq_putc(m, '\n');
-		v = (void *)((unsigned long)v & ~1L);
-	}
-
 	field = v;
 
 	/*

commit d244b6bd41e09ecbb09c738cc2c108be227398c8
Merge: da5cabf80e24 1aa54bca6ee0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Aug 16 11:17:30 2010 -0400

    Merge branch 'tip/perf/urgent-3' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-2.6-trace into trace/tip/perf/urgent-4
    
    Conflicts:
            kernel/trace/trace_events.c
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

commit 2a37a3df57c44e947271758a1aa4bea7bff9feab
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 3 15:21:34 2010 -0400

    tracing/events: Convert format output to seq_file
    
    Two new events were added that broke the current format output.
    
    Both from the SCSI system: scsi_dispatch_cmd_done and scsi_dispatch_cmd_timeout
    
    The reason is that their print_fmt exceeded a page size. Since the output
    of the format used simple_read_from_buffer and trace_seq, it was limited
    to a page size in output.
    
    This patch converts the printing of the format of an event into seq_file,
    which allows greater than a page size to be shown.
    
    I diffed all event formats comparing the output with and without this
    patch. All matched except for the above two, which showed just:
    
      FORMAT TOO BIG
    
    without this patch, but now properly displays the output with this patch.
    
    v2: Remove updating *pos in seq start function.
       [ Thanks to Li Zefan for pointing that out ]
    
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Martin K. Petersen <martin.petersen@oracle.com>
    Cc: Kei Tokunaga <tokunaga.keiich@jp.fujitsu.com>
    Cc: James Bottomley <James.Bottomley@suse.de>
    Cc: Tomohiro Kusumi <kusumi.tomohiro@jp.fujitsu.com>
    Cc: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 53cffc0b0801..45a8968707aa 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -29,6 +29,8 @@ DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
 
+#define COMMON_FIELD_COUNT	5
+
 struct list_head *
 trace_get_fields(struct ftrace_event_call *event_call)
 {
@@ -544,85 +546,155 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return ret;
 }
 
-static ssize_t
-event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
-		  loff_t *ppos)
+enum {
+	FORMAT_HEADER		= 1,
+	FORMAT_PRINTFMT		= 2,
+};
+
+static void *f_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_call *call = filp->private_data;
+	struct ftrace_event_call *call = m->private;
 	struct ftrace_event_field *field;
 	struct list_head *head;
-	struct trace_seq *s;
-	int common_field_count = 5;
-	char *buf;
-	int r = 0;
-
-	if (*ppos)
-		return 0;
+	loff_t index = *pos;
 
-	s = kmalloc(sizeof(*s), GFP_KERNEL);
-	if (!s)
-		return -ENOMEM;
+	(*pos)++;
 
-	trace_seq_init(s);
+	head = trace_get_fields(call);
 
-	trace_seq_printf(s, "name: %s\n", call->name);
-	trace_seq_printf(s, "ID: %d\n", call->event.type);
-	trace_seq_printf(s, "format:\n");
+	switch ((unsigned long)v) {
+	case FORMAT_HEADER:
 
-	head = trace_get_fields(call);
-	list_for_each_entry_reverse(field, head, link) {
-		/*
-		 * Smartly shows the array type(except dynamic array).
-		 * Normal:
-		 *	field:TYPE VAR
-		 * If TYPE := TYPE[LEN], it is shown:
-		 *	field:TYPE VAR[LEN]
-		 */
-		const char *array_descriptor = strchr(field->type, '[');
-
-		if (!strncmp(field->type, "__data_loc", 10))
-			array_descriptor = NULL;
-
-		if (!array_descriptor) {
-			r = trace_seq_printf(s, "\tfield:%s %s;\toffset:%u;"
-					"\tsize:%u;\tsigned:%d;\n",
-					field->type, field->name, field->offset,
-					field->size, !!field->is_signed);
-		} else {
-			r = trace_seq_printf(s, "\tfield:%.*s %s%s;\toffset:%u;"
-					"\tsize:%u;\tsigned:%d;\n",
-					(int)(array_descriptor - field->type),
-					field->type, field->name,
-					array_descriptor, field->offset,
-					field->size, !!field->is_signed);
-		}
+		if (unlikely(list_empty(head)))
+			return NULL;
 
-		if (--common_field_count == 0)
-			r = trace_seq_printf(s, "\n");
+		field = list_entry(head->prev, struct ftrace_event_field, link);
+		return field;
 
-		if (!r)
-			break;
+	case FORMAT_PRINTFMT:
+		/* all done */
+		return NULL;
 	}
 
-	if (r)
-		r = trace_seq_printf(s, "\nprint fmt: %s\n",
-				call->print_fmt);
+	/*
+	 * To separate common fields from event fields, the
+	 * LSB is set on the first event field. Clear it in case.
+	 */
+	v = (void *)((unsigned long)v & ~1L);
 
-	if (!r) {
-		/*
-		 * ug!  The format output is bigger than a PAGE!!
-		 */
-		buf = "FORMAT TOO BIG\n";
-		r = simple_read_from_buffer(ubuf, cnt, ppos,
-					      buf, strlen(buf));
-		goto out;
+	field = v;
+	if (field->link.prev == head)
+		return (void *)FORMAT_PRINTFMT;
+
+	field = list_entry(field->link.prev, struct ftrace_event_field, link);
+
+	/* Set the LSB to notify f_show to print an extra newline */
+	if (index == COMMON_FIELD_COUNT)
+		field = (struct ftrace_event_field *)
+			((unsigned long)field | 1);
+
+	return field;
+}
+
+static void *f_start(struct seq_file *m, loff_t *pos)
+{
+	loff_t l = 0;
+	void *p;
+
+	/* Start by showing the header */
+	if (!*pos)
+		return (void *)FORMAT_HEADER;
+
+	p = (void *)FORMAT_HEADER;
+	do {
+		p = f_next(m, p, &l);
+	} while (p && l < *pos);
+
+	return p;
+}
+
+static int f_show(struct seq_file *m, void *v)
+{
+	struct ftrace_event_call *call = m->private;
+	struct ftrace_event_field *field;
+	const char *array_descriptor;
+
+	switch ((unsigned long)v) {
+	case FORMAT_HEADER:
+		seq_printf(m, "name: %s\n", call->name);
+		seq_printf(m, "ID: %d\n", call->event.type);
+		seq_printf(m, "format:\n");
+		return 0;
+
+	case FORMAT_PRINTFMT:
+		seq_printf(m, "\nprint fmt: %s\n",
+			   call->print_fmt);
+		return 0;
 	}
 
-	r = simple_read_from_buffer(ubuf, cnt, ppos,
-				    s->buffer, s->len);
- out:
-	kfree(s);
-	return r;
+	/*
+	 * To separate common fields from event fields, the
+	 * LSB is set on the first event field. Clear it and
+	 * print a newline if it is set.
+	 */
+	if ((unsigned long)v & 1) {
+		seq_putc(m, '\n');
+		v = (void *)((unsigned long)v & ~1L);
+	}
+
+	field = v;
+
+	/*
+	 * Smartly shows the array type(except dynamic array).
+	 * Normal:
+	 *	field:TYPE VAR
+	 * If TYPE := TYPE[LEN], it is shown:
+	 *	field:TYPE VAR[LEN]
+	 */
+	array_descriptor = strchr(field->type, '[');
+
+	if (!strncmp(field->type, "__data_loc", 10))
+		array_descriptor = NULL;
+
+	if (!array_descriptor)
+		seq_printf(m, "\tfield:%s %s;\toffset:%u;\tsize:%u;\tsigned:%d;\n",
+			   field->type, field->name, field->offset,
+			   field->size, !!field->is_signed);
+	else
+		seq_printf(m, "\tfield:%.*s %s%s;\toffset:%u;\tsize:%u;\tsigned:%d;\n",
+			   (int)(array_descriptor - field->type),
+			   field->type, field->name,
+			   array_descriptor, field->offset,
+			   field->size, !!field->is_signed);
+
+	return 0;
+}
+
+static void f_stop(struct seq_file *m, void *p)
+{
+}
+
+static const struct seq_operations trace_format_seq_ops = {
+	.start		= f_start,
+	.next		= f_next,
+	.stop		= f_stop,
+	.show		= f_show,
+};
+
+static int trace_format_open(struct inode *inode, struct file *file)
+{
+	struct ftrace_event_call *call = inode->i_private;
+	struct seq_file *m;
+	int ret;
+
+	ret = seq_open(file, &trace_format_seq_ops);
+	if (ret < 0)
+		return ret;
+
+	m = file->private_data;
+	m->private = call;
+
+	return 0;
 }
 
 static ssize_t
@@ -820,8 +892,10 @@ static const struct file_operations ftrace_enable_fops = {
 };
 
 static const struct file_operations ftrace_event_format_fops = {
-	.open = tracing_open_generic,
-	.read = event_format_read,
+	.open = trace_format_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
 };
 
 static const struct file_operations ftrace_event_id_fops = {

commit e870e9a1240bcef1157ffaaf71dac63362e71904
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Jul 2 11:07:32 2010 +0800

    tracing: Allow to disable cmdline recording
    
    We found that even enabling a single trace event that will rarely be
    triggered can add big overhead to context switch.
    
    (lmbench context switch test)
     -------------------------------------------------
     2p/0K 2p/16K 2p/64K 8p/16K 8p/64K 16p/16K 16p/64K
     ctxsw  ctxsw  ctxsw ctxsw  ctxsw   ctxsw   ctxsw
    ------ ------ ------ ------ ------ ------- -------
      2.19   2.3   2.21   2.56   2.13     2.54    2.07
      2.39   2.51  2.35   2.75   2.27     2.81    2.24
    
    The overhead is 6% ~ 11%.
    
    It's because when a trace event is enabled 3 tracepoints (sched_switch,
    sched_wakeup, sched_wakeup_new) will be activated to map pid to cmdname.
    
    We'd like to avoid this overhead, so add a trace option '(no)record-cmd'
    to allow to disable cmdline recording.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4C2D57F4.2050204@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e8e6043f4d29..09b4fa6e4d3b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -170,6 +170,26 @@ int ftrace_event_reg(struct ftrace_event_call *call, enum trace_reg type)
 }
 EXPORT_SYMBOL_GPL(ftrace_event_reg);
 
+void trace_event_enable_cmd_record(bool enable)
+{
+	struct ftrace_event_call *call;
+
+	mutex_lock(&event_mutex);
+	list_for_each_entry(call, &ftrace_events, list) {
+		if (!(call->flags & TRACE_EVENT_FL_ENABLED))
+			continue;
+
+		if (enable) {
+			tracing_start_cmdline_record();
+			call->flags |= TRACE_EVENT_FL_RECORDED_CMD;
+		} else {
+			tracing_stop_cmdline_record();
+			call->flags &= ~TRACE_EVENT_FL_RECORDED_CMD;
+		}
+	}
+	mutex_unlock(&event_mutex);
+}
+
 static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 					int enable)
 {
@@ -179,13 +199,19 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 	case 0:
 		if (call->flags & TRACE_EVENT_FL_ENABLED) {
 			call->flags &= ~TRACE_EVENT_FL_ENABLED;
-			tracing_stop_cmdline_record();
+			if (call->flags & TRACE_EVENT_FL_RECORDED_CMD) {
+				tracing_stop_cmdline_record();
+				call->flags &= ~TRACE_EVENT_FL_RECORDED_CMD;
+			}
 			call->class->reg(call, TRACE_REG_UNREGISTER);
 		}
 		break;
 	case 1:
 		if (!(call->flags & TRACE_EVENT_FL_ENABLED)) {
-			tracing_start_cmdline_record();
+			if (trace_flags & TRACE_ITER_RECORD_CMD) {
+				tracing_start_cmdline_record();
+				call->flags |= TRACE_EVENT_FL_RECORDED_CMD;
+			}
 			ret = call->class->reg(call, TRACE_REG_REGISTER);
 			if (ret) {
 				tracing_stop_cmdline_record();

commit a1d0ce8213e9ddf4046ef5ba95c55762d075f541
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 8 11:22:06 2010 -0400

    tracing: Use class->reg() for all registering of events
    
    Because kprobes and syscalls need special processing to register
    events, the class->reg() method was created to handle the differences.
    
    But instead of creating a default ->reg for perf and ftrace events,
    the code was scattered with:
    
            if (class->reg)
                    class->reg();
            else
                    default_reg();
    
    This is messy and can also lead to bugs.
    
    This patch cleans up this code and creates a default reg() entry for
    the events allowing for the code to directly call the class->reg()
    without the condition.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 69bee4cc0e10..e8e6043f4d29 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -141,6 +141,35 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 }
 EXPORT_SYMBOL_GPL(trace_event_raw_init);
 
+int ftrace_event_reg(struct ftrace_event_call *call, enum trace_reg type)
+{
+	switch (type) {
+	case TRACE_REG_REGISTER:
+		return tracepoint_probe_register(call->name,
+						 call->class->probe,
+						 call);
+	case TRACE_REG_UNREGISTER:
+		tracepoint_probe_unregister(call->name,
+					    call->class->probe,
+					    call);
+		return 0;
+
+#ifdef CONFIG_PERF_EVENTS
+	case TRACE_REG_PERF_REGISTER:
+		return tracepoint_probe_register(call->name,
+						 call->class->perf_probe,
+						 call);
+	case TRACE_REG_PERF_UNREGISTER:
+		tracepoint_probe_unregister(call->name,
+					    call->class->perf_probe,
+					    call);
+		return 0;
+#endif
+	}
+	return 0;
+}
+EXPORT_SYMBOL_GPL(ftrace_event_reg);
+
 static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 					int enable)
 {
@@ -151,23 +180,13 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 		if (call->flags & TRACE_EVENT_FL_ENABLED) {
 			call->flags &= ~TRACE_EVENT_FL_ENABLED;
 			tracing_stop_cmdline_record();
-			if (call->class->reg)
-				call->class->reg(call, TRACE_REG_UNREGISTER);
-			else
-				tracepoint_probe_unregister(call->name,
-							    call->class->probe,
-							    call);
+			call->class->reg(call, TRACE_REG_UNREGISTER);
 		}
 		break;
 	case 1:
 		if (!(call->flags & TRACE_EVENT_FL_ENABLED)) {
 			tracing_start_cmdline_record();
-			if (call->class->reg)
-				ret = call->class->reg(call, TRACE_REG_REGISTER);
-			else
-				ret = tracepoint_probe_register(call->name,
-								call->class->probe,
-								call);
+			ret = call->class->reg(call, TRACE_REG_REGISTER);
 			if (ret) {
 				tracing_stop_cmdline_record();
 				pr_info("event trace: Could not enable event "
@@ -205,8 +224,7 @@ static int __ftrace_set_clr_event(const char *match, const char *sub,
 	mutex_lock(&event_mutex);
 	list_for_each_entry(call, &ftrace_events, list) {
 
-		if (!call->name || !call->class ||
-		    (!call->class->probe && !call->class->reg))
+		if (!call->name || !call->class || !call->class->reg)
 			continue;
 
 		if (match &&
@@ -332,7 +350,7 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 		 * The ftrace subsystem is for showing formats only.
 		 * They can not be enabled or disabled via the event files.
 		 */
-		if (call->class && (call->class->probe || call->class->reg))
+		if (call->class && call->class->reg)
 			return call;
 	}
 
@@ -485,8 +503,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	mutex_lock(&event_mutex);
 	list_for_each_entry(call, &ftrace_events, list) {
-		if (!call->name || !call->class ||
-		    (!call->class->probe && !call->class->reg))
+		if (!call->name || !call->class || !call->class->reg)
 			continue;
 
 		if (system && strcmp(call->class->system, system) != 0)
@@ -977,12 +994,12 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		return -1;
 	}
 
-	if (call->class->probe || call->class->reg)
+	if (call->class->reg)
 		trace_create_file("enable", 0644, call->dir, call,
 				  enable);
 
 #ifdef CONFIG_PERF_EVENTS
-	if (call->event.type && (call->class->perf_probe || call->class->reg))
+	if (call->event.type && call->class->reg)
 		trace_create_file("id", 0444, call->dir, call,
 		 		  id);
 #endif

commit 67ead0a6ceb001b4cb891d782e440f0e79493ba2
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon May 24 16:25:13 2010 +0800

    tracing: Remove open-coded __trace_add_event_call()
    
    Let trace_module_add_events() and event_trace_init() call
    __trace_add_event_call().
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4BFA37E9.1020106@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5bad9cbbf974..69bee4cc0e10 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1009,11 +1009,17 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 	return 0;
 }
 
-static int __trace_add_event_call(struct ftrace_event_call *call)
+static int
+__trace_add_event_call(struct ftrace_event_call *call, struct module *mod,
+		       const struct file_operations *id,
+		       const struct file_operations *enable,
+		       const struct file_operations *filter,
+		       const struct file_operations *format)
 {
 	struct dentry *d_events;
 	int ret;
 
+	/* The linker may leave blanks */
 	if (!call->name)
 		return -EINVAL;
 
@@ -1021,8 +1027,8 @@ static int __trace_add_event_call(struct ftrace_event_call *call)
 		ret = call->class->raw_init(call);
 		if (ret < 0) {
 			if (ret != -ENOSYS)
-				pr_warning("Could not initialize trace "
-				"events/%s\n", call->name);
+				pr_warning("Could not initialize trace events/%s\n",
+					   call->name);
 			return ret;
 		}
 	}
@@ -1031,11 +1037,10 @@ static int __trace_add_event_call(struct ftrace_event_call *call)
 	if (!d_events)
 		return -ENOENT;
 
-	ret = event_create_dir(call, d_events, &ftrace_event_id_fops,
-				&ftrace_enable_fops, &ftrace_event_filter_fops,
-				&ftrace_event_format_fops);
+	ret = event_create_dir(call, d_events, id, enable, filter, format);
 	if (!ret)
 		list_add(&call->list, &ftrace_events);
+	call->mod = mod;
 
 	return ret;
 }
@@ -1045,7 +1050,10 @@ int trace_add_event_call(struct ftrace_event_call *call)
 {
 	int ret;
 	mutex_lock(&event_mutex);
-	ret = __trace_add_event_call(call);
+	ret = __trace_add_event_call(call, NULL, &ftrace_event_id_fops,
+				     &ftrace_enable_fops,
+				     &ftrace_event_filter_fops,
+				     &ftrace_event_format_fops);
 	mutex_unlock(&event_mutex);
 	return ret;
 }
@@ -1162,8 +1170,6 @@ static void trace_module_add_events(struct module *mod)
 {
 	struct ftrace_module_file_ops *file_ops = NULL;
 	struct ftrace_event_call *call, *start, *end;
-	struct dentry *d_events;
-	int ret;
 
 	start = mod->trace_events;
 	end = mod->trace_events + mod->num_trace_events;
@@ -1171,38 +1177,14 @@ static void trace_module_add_events(struct module *mod)
 	if (start == end)
 		return;
 
-	d_events = event_trace_events_dir();
-	if (!d_events)
+	file_ops = trace_create_file_ops(mod);
+	if (!file_ops)
 		return;
 
 	for_each_event(call, start, end) {
-		/* The linker may leave blanks */
-		if (!call->name)
-			continue;
-		if (call->class->raw_init) {
-			ret = call->class->raw_init(call);
-			if (ret < 0) {
-				if (ret != -ENOSYS)
-					pr_warning("Could not initialize trace "
-					"point events/%s\n", call->name);
-				continue;
-			}
-		}
-		/*
-		 * This module has events, create file ops for this module
-		 * if not already done.
-		 */
-		if (!file_ops) {
-			file_ops = trace_create_file_ops(mod);
-			if (!file_ops)
-				return;
-		}
-		call->mod = mod;
-		ret = event_create_dir(call, d_events,
+		__trace_add_event_call(call, mod,
 				       &file_ops->id, &file_ops->enable,
 				       &file_ops->filter, &file_ops->format);
-		if (!ret)
-			list_add(&call->list, &ftrace_events);
 	}
 }
 
@@ -1333,24 +1315,10 @@ static __init int event_trace_init(void)
 		pr_warning("tracing: Failed to allocate common fields");
 
 	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
-		/* The linker may leave blanks */
-		if (!call->name)
-			continue;
-		if (call->class->raw_init) {
-			ret = call->class->raw_init(call);
-			if (ret < 0) {
-				if (ret != -ENOSYS)
-					pr_warning("Could not initialize trace "
-					"point events/%s\n", call->name);
-				continue;
-			}
-		}
-		ret = event_create_dir(call, d_events, &ftrace_event_id_fops,
+		__trace_add_event_call(call, NULL, &ftrace_event_id_fops,
 				       &ftrace_enable_fops,
 				       &ftrace_event_filter_fops,
 				       &ftrace_event_format_fops);
-		if (!ret)
-			list_add(&call->list, &ftrace_events);
 	}
 
 	while (true) {

commit c9d932cf8a1c608b676021aef0189376ba6ef151
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon May 24 16:24:28 2010 +0800

    tracing: Remove test of NULL define_fields callback
    
    Every event (or event class) has it's define_fields callback,
    so the test is redundant.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4BFA37BC.8080707@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d3b4bdf00b39..5bad9cbbf974 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -987,23 +987,21 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 		  id);
 #endif
 
-	if (call->class->define_fields) {
-		/*
-		 * Other events may have the same class. Only update
-		 * the fields if they are not already defined.
-		 */
-		head = trace_get_fields(call);
-		if (list_empty(head)) {
-			ret = call->class->define_fields(call);
-			if (ret < 0) {
-				pr_warning("Could not initialize trace point"
-					   " events/%s\n", call->name);
-				return ret;
-			}
+	/*
+	 * Other events may have the same class. Only update
+	 * the fields if they are not already defined.
+	 */
+	head = trace_get_fields(call);
+	if (list_empty(head)) {
+		ret = call->class->define_fields(call);
+		if (ret < 0) {
+			pr_warning("Could not initialize trace point"
+				   " events/%s\n", call->name);
+			return ret;
 		}
-		trace_create_file("filter", 0644, call->dir, call,
-				  filter);
 	}
+	trace_create_file("filter", 0644, call->dir, call,
+			  filter);
 
 	trace_create_file("format", 0444, call->dir, call,
 			  format);

commit 8728fe501ed562c1b46dde3c195eadec77bca033
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Mon May 24 16:22:49 2010 +0800

    tracing: Don't allocate common fields for every trace events
    
    Every event has the same common fields, so it's a big waste of
    memory to have a copy of those fields for every event.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4BFA3759.30105@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a594f9a7ee3d..d3b4bdf00b39 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -28,6 +28,7 @@
 DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
+LIST_HEAD(ftrace_common_fields);
 
 struct list_head *
 trace_get_fields(struct ftrace_event_call *event_call)
@@ -37,15 +38,11 @@ trace_get_fields(struct ftrace_event_call *event_call)
 	return event_call->class->get_fields(event_call);
 }
 
-int trace_define_field(struct ftrace_event_call *call, const char *type,
-		       const char *name, int offset, int size, int is_signed,
-		       int filter_type)
+static int __trace_define_field(struct list_head *head, const char *type,
+				const char *name, int offset, int size,
+				int is_signed, int filter_type)
 {
 	struct ftrace_event_field *field;
-	struct list_head *head;
-
-	if (WARN_ON(!call->class))
-		return 0;
 
 	field = kzalloc(sizeof(*field), GFP_KERNEL);
 	if (!field)
@@ -68,7 +65,6 @@ int trace_define_field(struct ftrace_event_call *call, const char *type,
 	field->size = size;
 	field->is_signed = is_signed;
 
-	head = trace_get_fields(call);
 	list_add(&field->link, head);
 
 	return 0;
@@ -80,17 +76,32 @@ int trace_define_field(struct ftrace_event_call *call, const char *type,
 
 	return -ENOMEM;
 }
+
+int trace_define_field(struct ftrace_event_call *call, const char *type,
+		       const char *name, int offset, int size, int is_signed,
+		       int filter_type)
+{
+	struct list_head *head;
+
+	if (WARN_ON(!call->class))
+		return 0;
+
+	head = trace_get_fields(call);
+	return __trace_define_field(head, type, name, offset, size,
+				    is_signed, filter_type);
+}
 EXPORT_SYMBOL_GPL(trace_define_field);
 
 #define __common_field(type, item)					\
-	ret = trace_define_field(call, #type, "common_" #item,		\
-				 offsetof(typeof(ent), item),		\
-				 sizeof(ent.item),			\
-				 is_signed_type(type), FILTER_OTHER);	\
+	ret = __trace_define_field(&ftrace_common_fields, #type,	\
+				   "common_" #item,			\
+				   offsetof(typeof(ent), item),		\
+				   sizeof(ent.item),			\
+				   is_signed_type(type), FILTER_OTHER);	\
 	if (ret)							\
 		return ret;
 
-static int trace_define_common_fields(struct ftrace_event_call *call)
+static int trace_define_common_fields(void)
 {
 	int ret;
 	struct trace_entry ent;
@@ -544,32 +555,10 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return ret;
 }
 
-static ssize_t
-event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
-		  loff_t *ppos)
+static void print_event_fields(struct trace_seq *s, struct list_head *head)
 {
-	struct ftrace_event_call *call = filp->private_data;
 	struct ftrace_event_field *field;
-	struct list_head *head;
-	struct trace_seq *s;
-	int common_field_count = 5;
-	char *buf;
-	int r = 0;
-
-	if (*ppos)
-		return 0;
-
-	s = kmalloc(sizeof(*s), GFP_KERNEL);
-	if (!s)
-		return -ENOMEM;
-
-	trace_seq_init(s);
 
-	trace_seq_printf(s, "name: %s\n", call->name);
-	trace_seq_printf(s, "ID: %d\n", call->event.type);
-	trace_seq_printf(s, "format:\n");
-
-	head = trace_get_fields(call);
 	list_for_each_entry_reverse(field, head, link) {
 		/*
 		 * Smartly shows the array type(except dynamic array).
@@ -584,29 +573,54 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 			array_descriptor = NULL;
 
 		if (!array_descriptor) {
-			r = trace_seq_printf(s, "\tfield:%s %s;\toffset:%u;"
+			trace_seq_printf(s, "\tfield:%s %s;\toffset:%u;"
 					"\tsize:%u;\tsigned:%d;\n",
 					field->type, field->name, field->offset,
 					field->size, !!field->is_signed);
 		} else {
-			r = trace_seq_printf(s, "\tfield:%.*s %s%s;\toffset:%u;"
+			trace_seq_printf(s, "\tfield:%.*s %s%s;\toffset:%u;"
 					"\tsize:%u;\tsigned:%d;\n",
 					(int)(array_descriptor - field->type),
 					field->type, field->name,
 					array_descriptor, field->offset,
 					field->size, !!field->is_signed);
 		}
+	}
+}
 
-		if (--common_field_count == 0)
-			r = trace_seq_printf(s, "\n");
+static ssize_t
+event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
+		  loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	struct list_head *head;
+	struct trace_seq *s;
+	char *buf;
+	int r;
 
-		if (!r)
-			break;
-	}
+	if (*ppos)
+		return 0;
 
-	if (r)
-		r = trace_seq_printf(s, "\nprint fmt: %s\n",
-				call->print_fmt);
+	s = kmalloc(sizeof(*s), GFP_KERNEL);
+	if (!s)
+		return -ENOMEM;
+
+	trace_seq_init(s);
+
+	trace_seq_printf(s, "name: %s\n", call->name);
+	trace_seq_printf(s, "ID: %d\n", call->event.type);
+	trace_seq_printf(s, "format:\n");
+
+	/* print common fields */
+	print_event_fields(s, &ftrace_common_fields);
+
+	trace_seq_putc(s, '\n');
+
+	/* print event specific fields */
+	head = trace_get_fields(call);
+	print_event_fields(s, head);
+
+	r = trace_seq_printf(s, "\nprint fmt: %s\n", call->print_fmt);
 
 	if (!r) {
 		/*
@@ -980,9 +994,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 */
 		head = trace_get_fields(call);
 		if (list_empty(head)) {
-			ret = trace_define_common_fields(call);
-			if (!ret)
-				ret = call->class->define_fields(call);
+			ret = call->class->define_fields(call);
 			if (ret < 0) {
 				pr_warning("Could not initialize trace point"
 					   " events/%s\n", call->name);
@@ -1319,6 +1331,9 @@ static __init int event_trace_init(void)
 	trace_create_file("enable", 0644, d_events,
 			  NULL, &ftrace_system_enable_fops);
 
+	if (trace_define_common_fields())
+		pr_warning("tracing: Failed to allocate common fields");
+
 	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
 		/* The linker may leave blanks */
 		if (!call->name)

commit 5168ae50a66e3ff7184c2b16d661bd6d70367e50
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Jun 3 09:36:50 2010 -0400

    tracing: Remove ftrace_preempt_disable/enable
    
    The ftrace_preempt_disable/enable functions were to address a
    recursive race caused by the function tracer. The function tracer
    traces all functions which makes it easily susceptible to recursion.
    One area was preempt_enable(). This would call the scheduler and
    the schedulre would call the function tracer and loop.
    (So was it thought).
    
    The ftrace_preempt_disable/enable was made to protect against recursion
    inside the scheduler by storing the NEED_RESCHED flag. If it was
    set before the ftrace_preempt_disable() it would not call schedule
    on ftrace_preempt_enable(), thinking that if it was set before then
    it would have already scheduled unless it was already in the scheduler.
    
    This worked fine except in the case of SMP, where another task would set
    the NEED_RESCHED flag for a task on another CPU, and then kick off an
    IPI to trigger it. This could cause the NEED_RESCHED to be saved at
    ftrace_preempt_disable() but the IPI to arrive in the the preempt
    disabled section. The ftrace_preempt_enable() would not call the scheduler
    because the flag was already set before entring the section.
    
    This bug would cause a missed preemption check and cause lower latencies.
    
    Investigating further, I found that the recusion caused by the function
    tracer was not due to schedule(), but due to preempt_schedule(). Now
    that preempt_schedule is completely annotated with notrace, the recusion
    no longer is an issue.
    
    Reported-by: Thomas Gleixner <tglx@linutronix.de>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 53cffc0b0801..a594f9a7ee3d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1524,12 +1524,11 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip)
 	struct ftrace_entry *entry;
 	unsigned long flags;
 	long disabled;
-	int resched;
 	int cpu;
 	int pc;
 
 	pc = preempt_count();
-	resched = ftrace_preempt_disable();
+	preempt_disable_notrace();
 	cpu = raw_smp_processor_id();
 	disabled = atomic_inc_return(&per_cpu(ftrace_test_event_disable, cpu));
 
@@ -1551,7 +1550,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip)
 
  out:
 	atomic_dec(&per_cpu(ftrace_test_event_disable, cpu));
-	ftrace_preempt_enable(resched);
+	preempt_enable_notrace();
 }
 
 static struct ftrace_ops trace_ops __initdata  =

commit 553552ce1796c32cf4e3d4f45cd5b537de91dd1d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 23 11:12:36 2010 -0400

    tracing: Combine event filter_active and enable into single flags field
    
    The filter_active and enable both use an int (4 bytes each) to
    set a single flag. We can save 4 bytes per event by combining the
    two into a single integer.
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4894944 1018052  861512 6774508  675eec vmlinux.id
    4894871 1012292  861512 6768675  674823 vmlinux.flags
    
    This gives us another 5K in savings.
    
    The modification of both the enable and filter fields are done
    under the event_mutex, so it is still safe to combine the two.
    
    Note: Although Mathieu gave his Acked-by, he would like it documented
     that the reads of flags are not protected by the mutex. The way the
     code works, these reads will not break anything, but will have a
     residual effect. Since this behavior is the same even before this
     patch, describing this situation is left to another patch, as this
     patch does not change the behavior, but just brought it to Mathieu's
     attention.
    
    v2: Updated the event trace self test to for this change.
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8daaca5475b5..53cffc0b0801 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -137,8 +137,8 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 
 	switch (enable) {
 	case 0:
-		if (call->enabled) {
-			call->enabled = 0;
+		if (call->flags & TRACE_EVENT_FL_ENABLED) {
+			call->flags &= ~TRACE_EVENT_FL_ENABLED;
 			tracing_stop_cmdline_record();
 			if (call->class->reg)
 				call->class->reg(call, TRACE_REG_UNREGISTER);
@@ -149,7 +149,7 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 		}
 		break;
 	case 1:
-		if (!call->enabled) {
+		if (!(call->flags & TRACE_EVENT_FL_ENABLED)) {
 			tracing_start_cmdline_record();
 			if (call->class->reg)
 				ret = call->class->reg(call, TRACE_REG_REGISTER);
@@ -163,7 +163,7 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 					"%s\n", call->name);
 				break;
 			}
-			call->enabled = 1;
+			call->flags |= TRACE_EVENT_FL_ENABLED;
 		}
 		break;
 	}
@@ -352,7 +352,7 @@ s_next(struct seq_file *m, void *v, loff_t *pos)
 	(*pos)++;
 
 	list_for_each_entry_continue(call, &ftrace_events, list) {
-		if (call->enabled)
+		if (call->flags & TRACE_EVENT_FL_ENABLED)
 			return call;
 	}
 
@@ -411,7 +411,7 @@ event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	struct ftrace_event_call *call = filp->private_data;
 	char *buf;
 
-	if (call->enabled)
+	if (call->flags & TRACE_EVENT_FL_ENABLED)
 		buf = "1\n";
 	else
 		buf = "0\n";
@@ -486,7 +486,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		 * or if all events or cleared, or if we have
 		 * a mixture.
 		 */
-		set |= (1 << !!call->enabled);
+		set |= (1 << !!(call->flags & TRACE_EVENT_FL_ENABLED));
 
 		/*
 		 * If we have a mixture, no need to look further.
@@ -1447,7 +1447,7 @@ static __init void event_trace_self_tests(void)
 		 * If an event is already enabled, someone is using
 		 * it and the self test should not be on.
 		 */
-		if (call->enabled) {
+		if (call->flags & TRACE_EVENT_FL_ENABLED) {
 			pr_warning("Enabled event during self test!\n");
 			WARN_ON_ONCE(1);
 			continue;

commit 32c0edaeaad74a7883e736ae0f3798784cfc2a80
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 23 10:38:03 2010 -0400

    tracing: Remove duplicate id information in event structure
    
    Now that the trace_event structure is embedded in the ftrace_event_call
    structure, there is no need for the ftrace_event_call id field.
    The id field is the same as the trace_event type field.
    
    Removing the id and re-arranging the structure brings down the tracepoint
    footprint by another 5K.
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4895024 1023812  861512 6780348  6775bc vmlinux.print
    4894944 1018052  861512 6774508  675eec vmlinux.id
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index aafe5bff8f59..8daaca5475b5 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -125,7 +125,6 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 	id = register_ftrace_event(&call->event);
 	if (!id)
 		return -ENODEV;
-	call->id = id;
 
 	return 0;
 }
@@ -567,7 +566,7 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 	trace_seq_init(s);
 
 	trace_seq_printf(s, "name: %s\n", call->name);
-	trace_seq_printf(s, "ID: %d\n", call->id);
+	trace_seq_printf(s, "ID: %d\n", call->event.type);
 	trace_seq_printf(s, "format:\n");
 
 	head = trace_get_fields(call);
@@ -641,7 +640,7 @@ event_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 		return -ENOMEM;
 
 	trace_seq_init(s);
-	trace_seq_printf(s, "%d\n", call->id);
+	trace_seq_printf(s, "%d\n", call->event.type);
 
 	r = simple_read_from_buffer(ubuf, cnt, ppos,
 				    s->buffer, s->len);
@@ -969,7 +968,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 				  enable);
 
 #ifdef CONFIG_PERF_EVENTS
-	if (call->id && (call->class->perf_probe || call->class->reg))
+	if (call->event.type && (call->class->perf_probe || call->class->reg))
 		trace_create_file("id", 0444, call->dir, call,
 		 		  id);
 #endif

commit 80decc70afc57c87eee9d6b836aec2ecacba3457
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 23 10:00:22 2010 -0400

    tracing: Move print functions into event class
    
    Currently, every event has its own trace_event structure. This is
    fine since the structure is needed anyway. But the print function
    structure (trace_event_functions) is now separate. Since the output
    of the trace event is done by the class (with the exception of events
    defined by DEFINE_EVENT_PRINT), it makes sense to have the class
    define the print functions that all events in the class can use.
    
    This makes a bigger deal with the syscall events since all syscall events
    use the same class. The savings here is another 30K.
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4900382 1048964  861512 6810858  67ecea vmlinux.init
    4900446 1049028  861512 6810986  67ed6a vmlinux.preprint
    4895024 1023812  861512 6780348  6775bc vmlinux.print
    
    To accomplish this, and to let the class know what event is being
    printed, the event structure is embedded in the ftrace_event_call
    structure. This should not be an issues since the event structure
    was created for each event anyway.
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8e94255a45e7..aafe5bff8f59 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -122,7 +122,7 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 {
 	int id;
 
-	id = register_ftrace_event(call->event);
+	id = register_ftrace_event(&call->event);
 	if (!id)
 		return -ENODEV;
 	call->id = id;
@@ -1073,8 +1073,8 @@ static void remove_subsystem_dir(const char *name)
 static void __trace_remove_event_call(struct ftrace_event_call *call)
 {
 	ftrace_event_enable_disable(call, 0);
-	if (call->event)
-		__unregister_ftrace_event(call->event);
+	if (call->event.funcs)
+		__unregister_ftrace_event(&call->event);
 	debugfs_remove_recursive(call->dir);
 	list_del(&call->list);
 	trace_destroy_fields(call);

commit 0405ab80aa94afb13bf9ac4a6fc9f2923d4b9114
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 22 11:46:44 2010 -0400

    tracing: Move raw_init from events to class
    
    The raw_init function pointer in the event is used to initialize
    various kinds of events. The type of initialization needed is usually
    classed to the kind of event it is.
    
    Two events with the same class will always have the same initialization
    function, so it makes sense to move this to the class structure.
    
    Perhaps even making a special system structure would work since
    the initialization is the same for all events within a system.
    But since there's no system structure (yet), this will just move it
    to the class.
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4900375 1053380  861512 6815267  67fe23 vmlinux.fields
    4900382 1048964  861512 6810858  67ecea vmlinux.init
    
    The text grew very slightly, but this is a constant growth that happened
    with the changing of the C files that call the init code.
    The bigger savings is the data which will be saved the more events share
    a class.
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index acc0f55742c3..8e94255a45e7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1008,8 +1008,8 @@ static int __trace_add_event_call(struct ftrace_event_call *call)
 	if (!call->name)
 		return -EINVAL;
 
-	if (call->raw_init) {
-		ret = call->raw_init(call);
+	if (call->class->raw_init) {
+		ret = call->class->raw_init(call);
 		if (ret < 0) {
 			if (ret != -ENOSYS)
 				pr_warning("Could not initialize trace "
@@ -1170,8 +1170,8 @@ static void trace_module_add_events(struct module *mod)
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;
-		if (call->raw_init) {
-			ret = call->raw_init(call);
+		if (call->class->raw_init) {
+			ret = call->class->raw_init(call);
 			if (ret < 0) {
 				if (ret != -ENOSYS)
 					pr_warning("Could not initialize trace "
@@ -1324,8 +1324,8 @@ static __init int event_trace_init(void)
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;
-		if (call->raw_init) {
-			ret = call->raw_init(call);
+		if (call->class->raw_init) {
+			ret = call->class->raw_init(call);
 			if (ret < 0) {
 				if (ret != -ENOSYS)
 					pr_warning("Could not initialize trace "

commit 2e33af029556cb8bd22bf4f86f42d540249177ea
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 22 10:35:55 2010 -0400

    tracing: Move fields from event to class structure
    
    Move the defined fields from the event to the class structure.
    Since the fields of the event are defined by the class they belong
    to, it makes sense to have the class hold the information instead
    of the individual events. The events of the same class would just
    hold duplicate information.
    
    After this change the size of the kernel dropped another 3K:
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4900252 1057412  861512 6819176  680d68 vmlinux.regs
    4900375 1053380  861512 6815267  67fe23 vmlinux.fields
    
    Although the text increased, this was mainly due to the C files
    having to adapt to the change. This is a constant increase, where
    new tracepoints will not increase the Text. But the big drop is
    in the data size (as well as needed allocations to hold the fields).
    This will give even more savings as more tracepoints are created.
    
    Note, if just TRACE_EVENT()s are used and not DECLARE_EVENT_CLASS()
    with several DEFINE_EVENT()s, then the savings will be lost. But
    we are pushing developers to consolidate events with DEFINE_EVENT()
    so this should not be an issue.
    
    The kprobes define a unique class to every new event, but are dynamic
    so it should not be a issue.
    
    The syscalls however have a single class but the fields for the individual
    events are different. The syscalls use a metadata to define the
    fields. I moved the fields list from the event to the metadata and
    added a "get_fields()" function to the class. This function is used
    to find the fields. For normal events and kprobes, get_fields() just
    returns a pointer to the fields list_head in the class. For syscall
    events, it returns the fields list_head in the metadata for the event.
    
    v2:  Fixed the syscall fields. The syscall metadata needs a list
         of fields for both enter and exit.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 19d1eb0a7188..acc0f55742c3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -29,11 +29,23 @@ DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
 
+struct list_head *
+trace_get_fields(struct ftrace_event_call *event_call)
+{
+	if (!event_call->class->get_fields)
+		return &event_call->class->fields;
+	return event_call->class->get_fields(event_call);
+}
+
 int trace_define_field(struct ftrace_event_call *call, const char *type,
 		       const char *name, int offset, int size, int is_signed,
 		       int filter_type)
 {
 	struct ftrace_event_field *field;
+	struct list_head *head;
+
+	if (WARN_ON(!call->class))
+		return 0;
 
 	field = kzalloc(sizeof(*field), GFP_KERNEL);
 	if (!field)
@@ -56,7 +68,8 @@ int trace_define_field(struct ftrace_event_call *call, const char *type,
 	field->size = size;
 	field->is_signed = is_signed;
 
-	list_add(&field->link, &call->fields);
+	head = trace_get_fields(call);
+	list_add(&field->link, head);
 
 	return 0;
 
@@ -94,8 +107,10 @@ static int trace_define_common_fields(struct ftrace_event_call *call)
 void trace_destroy_fields(struct ftrace_event_call *call)
 {
 	struct ftrace_event_field *field, *next;
+	struct list_head *head;
 
-	list_for_each_entry_safe(field, next, &call->fields, link) {
+	head = trace_get_fields(call);
+	list_for_each_entry_safe(field, next, head, link) {
 		list_del(&field->link);
 		kfree(field->type);
 		kfree(field->name);
@@ -111,7 +126,6 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 	if (!id)
 		return -ENODEV;
 	call->id = id;
-	INIT_LIST_HEAD(&call->fields);
 
 	return 0;
 }
@@ -537,6 +551,7 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 {
 	struct ftrace_event_call *call = filp->private_data;
 	struct ftrace_event_field *field;
+	struct list_head *head;
 	struct trace_seq *s;
 	int common_field_count = 5;
 	char *buf;
@@ -555,7 +570,8 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 	trace_seq_printf(s, "ID: %d\n", call->id);
 	trace_seq_printf(s, "format:\n");
 
-	list_for_each_entry_reverse(field, &call->fields, link) {
+	head = trace_get_fields(call);
+	list_for_each_entry_reverse(field, head, link) {
 		/*
 		 * Smartly shows the array type(except dynamic array).
 		 * Normal:
@@ -931,6 +947,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 const struct file_operations *filter,
 		 const struct file_operations *format)
 {
+	struct list_head *head;
 	int ret;
 
 	/*
@@ -957,14 +974,21 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 		  id);
 #endif
 
-	if (call->define_fields) {
-		ret = trace_define_common_fields(call);
-		if (!ret)
-			ret = call->define_fields(call);
-		if (ret < 0) {
-			pr_warning("Could not initialize trace point"
-				   " events/%s\n", call->name);
-			return ret;
+	if (call->class->define_fields) {
+		/*
+		 * Other events may have the same class. Only update
+		 * the fields if they are not already defined.
+		 */
+		head = trace_get_fields(call);
+		if (list_empty(head)) {
+			ret = trace_define_common_fields(call);
+			if (!ret)
+				ret = call->class->define_fields(call);
+			if (ret < 0) {
+				pr_warning("Could not initialize trace point"
+					   " events/%s\n", call->name);
+				return ret;
+			}
 		}
 		trace_create_file("filter", 0644, call->dir, call,
 				  filter);

commit 2239291aeb0379fe47980b0e560e0eb9fd7e82ec
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Apr 21 12:27:06 2010 -0400

    tracing: Remove per event trace registering
    
    This patch removes the register functions of TRACE_EVENT() to enable
    and disable tracepoints. The registering of a event is now down
    directly in the trace_events.c file. The tracepoint_probe_register()
    is now called directly.
    
    The prototypes are no longer type checked, but this should not be
    an issue since the tracepoints are created automatically by the
    macros. If a prototype is incorrect in the TRACE_EVENT() macro, then
    other macros will catch it.
    
    The trace_event_class structure now holds the probes to be called
    by the callbacks. This removes needing to have each event have
    a separate pointer for the probe.
    
    To handle kprobes and syscalls, since they register probes in a
    different manner, a "reg" field is added to the ftrace_event_class
    structure. If the "reg" field is assigned, then it will be called for
    enabling and disabling of the probe for either ftrace or perf. To let
    the reg function know what is happening, a new enum (trace_reg) is
    created that has the type of control that is needed.
    
    With this new rework, the 82 kernel events and 618 syscall events
    has their footprint dramatically lowered:
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4914025 1088868  861512 6864405  68be15 vmlinux.class
    4918492 1084612  861512 6864616  68bee8 vmlinux.tracepoint
    4900252 1057412  861512 6819176  680d68 vmlinux.regs
    
    The size went from 6863829 to 6819176, that's a total of 44K
    in savings. With tracepoints being continuously added, this is
    critical that the footprint becomes minimal.
    
    v5: Added #ifdef CONFIG_PERF_EVENTS around a reference to perf
        specific structure in trace_events.c.
    
    v4: Fixed trace self tests to check probe because regfunc no longer
        exists.
    
    v3: Updated to handle void *data in beginning of probe parameters.
        Also added the tracepoint: check_trace_callback_type_##call().
    
    v2: Changed the callback probes to pass void * and typecast the
        value within the function.
    
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2f54b48d3632..19d1eb0a7188 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -127,13 +127,23 @@ static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 		if (call->enabled) {
 			call->enabled = 0;
 			tracing_stop_cmdline_record();
-			call->unregfunc(call);
+			if (call->class->reg)
+				call->class->reg(call, TRACE_REG_UNREGISTER);
+			else
+				tracepoint_probe_unregister(call->name,
+							    call->class->probe,
+							    call);
 		}
 		break;
 	case 1:
 		if (!call->enabled) {
 			tracing_start_cmdline_record();
-			ret = call->regfunc(call);
+			if (call->class->reg)
+				ret = call->class->reg(call, TRACE_REG_REGISTER);
+			else
+				ret = tracepoint_probe_register(call->name,
+								call->class->probe,
+								call);
 			if (ret) {
 				tracing_stop_cmdline_record();
 				pr_info("event trace: Could not enable event "
@@ -171,7 +181,8 @@ static int __ftrace_set_clr_event(const char *match, const char *sub,
 	mutex_lock(&event_mutex);
 	list_for_each_entry(call, &ftrace_events, list) {
 
-		if (!call->name || !call->regfunc)
+		if (!call->name || !call->class ||
+		    (!call->class->probe && !call->class->reg))
 			continue;
 
 		if (match &&
@@ -297,7 +308,7 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 		 * The ftrace subsystem is for showing formats only.
 		 * They can not be enabled or disabled via the event files.
 		 */
-		if (call->regfunc)
+		if (call->class && (call->class->probe || call->class->reg))
 			return call;
 	}
 
@@ -450,7 +461,8 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	mutex_lock(&event_mutex);
 	list_for_each_entry(call, &ftrace_events, list) {
-		if (!call->name || !call->regfunc)
+		if (!call->name || !call->class ||
+		    (!call->class->probe && !call->class->reg))
 			continue;
 
 		if (system && strcmp(call->class->system, system) != 0)
@@ -935,13 +947,15 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		return -1;
 	}
 
-	if (call->regfunc)
+	if (call->class->probe || call->class->reg)
 		trace_create_file("enable", 0644, call->dir, call,
 				  enable);
 
-	if (call->id && call->perf_event_enable)
+#ifdef CONFIG_PERF_EVENTS
+	if (call->id && (call->class->perf_probe || call->class->reg))
 		trace_create_file("id", 0444, call->dir, call,
 		 		  id);
+#endif
 
 	if (call->define_fields) {
 		ret = trace_define_common_fields(call);
@@ -1388,8 +1402,8 @@ static __init void event_trace_self_tests(void)
 
 	list_for_each_entry(call, &ftrace_events, list) {
 
-		/* Only test those that have a regfunc */
-		if (!call->regfunc)
+		/* Only test those that have a probe */
+		if (!call->class || !call->class->probe)
 			continue;
 
 /*

commit 8f0820183056ad26dabc0202115848a92f1143fc
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 20 10:47:33 2010 -0400

    tracing: Create class struct for events
    
    This patch creates a ftrace_event_class struct that event structs point to.
    This class struct will be made to hold information to modify the
    events. Currently the class struct only holds the events system name.
    
    This patch slightly increases the size, but this change lays the ground work
    of other changes to make the footprint of tracepoints smaller.
    
    With 82 standard tracepoints, and 618 system call tracepoints
    (two tracepoints per syscall: enter and exit):
    
       text    data     bss     dec     hex filename
    4913961 1088356  861512 6863829  68bbd5 vmlinux.orig
    4914025 1088868  861512 6864405  68be15 vmlinux.class
    
    This patch also cleans up some stale comments in ftrace.h.
    
    v2: Fixed missing semi-colon in macro.
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Mathieu Desnoyers <mathieu.desnoyers@efficios.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c697c7043349..2f54b48d3632 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -176,10 +176,10 @@ static int __ftrace_set_clr_event(const char *match, const char *sub,
 
 		if (match &&
 		    strcmp(match, call->name) != 0 &&
-		    strcmp(match, call->system) != 0)
+		    strcmp(match, call->class->system) != 0)
 			continue;
 
-		if (sub && strcmp(sub, call->system) != 0)
+		if (sub && strcmp(sub, call->class->system) != 0)
 			continue;
 
 		if (event && strcmp(event, call->name) != 0)
@@ -355,8 +355,8 @@ static int t_show(struct seq_file *m, void *v)
 {
 	struct ftrace_event_call *call = v;
 
-	if (strcmp(call->system, TRACE_SYSTEM) != 0)
-		seq_printf(m, "%s:", call->system);
+	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
+		seq_printf(m, "%s:", call->class->system);
 	seq_printf(m, "%s\n", call->name);
 
 	return 0;
@@ -453,7 +453,7 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		if (!call->name || !call->regfunc)
 			continue;
 
-		if (system && strcmp(call->system, system) != 0)
+		if (system && strcmp(call->class->system, system) != 0)
 			continue;
 
 		/*
@@ -925,8 +925,8 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 	 * If the trace point header did not define TRACE_SYSTEM
 	 * then the system would be called "TRACE_SYSTEM".
 	 */
-	if (strcmp(call->system, TRACE_SYSTEM) != 0)
-		d_events = event_subsystem_dir(call->system, d_events);
+	if (strcmp(call->class->system, TRACE_SYSTEM) != 0)
+		d_events = event_subsystem_dir(call->class->system, d_events);
 
 	call->dir = debugfs_create_dir(call->name, d_events);
 	if (!call->dir) {
@@ -1041,7 +1041,7 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 	list_del(&call->list);
 	trace_destroy_fields(call);
 	destroy_preds(call);
-	remove_subsystem_dir(call->system);
+	remove_subsystem_dir(call->class->system);
 }
 
 /* Remove an event_call */
@@ -1399,8 +1399,8 @@ static __init void event_trace_self_tests(void)
  * syscalls as we test.
  */
 #ifndef CONFIG_EVENT_TRACE_TEST_SYSCALLS
-		if (call->system &&
-		    strcmp(call->system, "syscalls") == 0)
+		if (call->class->system &&
+		    strcmp(call->class->system, "syscalls") == 0)
 			continue;
 #endif
 

commit 5a0e3ad6af8660be21ca98a971cd00f331318c05
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Mar 24 17:04:11 2010 +0900

    include cleanup: Update gfp.h and slab.h includes to prepare for breaking implicit slab.h inclusion from percpu.h
    
    percpu.h is included by sched.h and module.h and thus ends up being
    included when building most .c files.  percpu.h includes slab.h which
    in turn includes gfp.h making everything defined by the two files
    universally available and complicating inclusion dependencies.
    
    percpu.h -> slab.h dependency is about to be removed.  Prepare for
    this change by updating users of gfp and slab facilities include those
    headers directly instead of assuming availability.  As this conversion
    needs to touch large number of source files, the following script is
    used as the basis of conversion.
    
      http://userweb.kernel.org/~tj/misc/slabh-sweep.py
    
    The script does the followings.
    
    * Scan files for gfp and slab usages and update includes such that
      only the necessary includes are there.  ie. if only gfp is used,
      gfp.h, if slab is used, slab.h.
    
    * When the script inserts a new include, it looks at the include
      blocks and try to put the new include such that its order conforms
      to its surrounding.  It's put in the include block which contains
      core kernel includes, in the same order that the rest are ordered -
      alphabetical, Christmas tree, rev-Xmas-tree or at the end if there
      doesn't seem to be any matching order.
    
    * If the script can't find a place to put a new include (mostly
      because the file doesn't have fitting include block), it prints out
      an error message indicating which .h file needs to be added to the
      file.
    
    The conversion was done in the following steps.
    
    1. The initial automatic conversion of all .c files updated slightly
       over 4000 files, deleting around 700 includes and adding ~480 gfp.h
       and ~3000 slab.h inclusions.  The script emitted errors for ~400
       files.
    
    2. Each error was manually checked.  Some didn't need the inclusion,
       some needed manual addition while adding it to implementation .h or
       embedding .c file was more appropriate for others.  This step added
       inclusions to around 150 files.
    
    3. The script was run again and the output was compared to the edits
       from #2 to make sure no file was left behind.
    
    4. Several build tests were done and a couple of problems were fixed.
       e.g. lib/decompress_*.c used malloc/free() wrappers around slab
       APIs requiring slab.h to be added manually.
    
    5. The script was run on all .h files but without automatically
       editing them as sprinkling gfp.h and slab.h inclusions around .h
       files could easily lead to inclusion dependency hell.  Most gfp.h
       inclusion directives were ignored as stuff from gfp.h was usually
       wildly available and often used in preprocessor macros.  Each
       slab.h inclusion directive was examined and added manually as
       necessary.
    
    6. percpu.h was updated not to include slab.h.
    
    7. Build test were done on the following configurations and failures
       were fixed.  CONFIG_GCOV_KERNEL was turned off for all tests (as my
       distributed build env didn't work with gcov compiles) and a few
       more options had to be turned off depending on archs to make things
       build (like ipr on powerpc/64 which failed due to missing writeq).
    
       * x86 and x86_64 UP and SMP allmodconfig and a custom test config.
       * powerpc and powerpc64 SMP allmodconfig
       * sparc and sparc64 SMP allmodconfig
       * ia64 SMP allmodconfig
       * s390 SMP allmodconfig
       * alpha SMP allmodconfig
       * um on x86_64 SMP allmodconfig
    
    8. percpu.h modifications were reverted so that it could be applied as
       a separate patch and serve as bisection point.
    
    Given the fact that I had only a couple of failures from tests on step
    6, I'm fairly confident about the coverage of this conversion patch.
    If there is a breakage, it's likely to be something in one of the arch
    headers which should be easily discoverable easily on most builds of
    the specific arch.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Guess-its-ok-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Lee Schermerhorn <Lee.Schermerhorn@hp.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index beab8bf2f310..c697c7043349 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -15,6 +15,7 @@
 #include <linux/uaccess.h>
 #include <linux/module.h>
 #include <linux/ctype.h>
+#include <linux/slab.h>
 #include <linux/delay.h>
 
 #include <asm/setup.h>

commit 97d5a22005f38057b4bc0d95f81cd26510268794
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Mar 5 05:35:37 2010 +0100

    perf: Drop the obsolete profile naming for trace events
    
    Drop the obsolete "profile" naming used by perf for trace events.
    Perf can now do more than simple events counting, so generalize
    the API naming.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Jason Baron <jbaron@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 3f972ad98d04..beab8bf2f310 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -938,7 +938,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		trace_create_file("enable", 0644, call->dir, call,
 				  enable);
 
-	if (call->id && call->profile_enable)
+	if (call->id && call->perf_event_enable)
 		trace_create_file("id", 0444, call->dir, call,
 		 		  id);
 

commit 7b60997f73865b019e595720185c85285ca3df9a
Author: Wenji Huang <wenji.huang@oracle.com>
Date:   Wed Feb 24 15:40:26 2010 +0800

    tracing: Simplify memory recycle of trace_define_field
    
    Discard freeing field->type since it is not necessary.
    
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Wenji Huang <wenji.huang@oracle.com>
    LKML-Reference: <1266997226-6833-5-git-send-email-wenji.huang@oracle.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c2a3077b7353..3f972ad98d04 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -60,10 +60,8 @@ int trace_define_field(struct ftrace_event_call *call, const char *type,
 	return 0;
 
 err:
-	if (field) {
+	if (field)
 		kfree(field->name);
-		kfree(field->type);
-	}
 	kfree(field);
 
 	return -ENOMEM;

commit 0fa0edaf32b9a78b9854f1da98d4511a501089b0
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Dec 15 15:39:57 2009 +0800

    tracing: Remove show_format and related macros from TRACE_EVENT
    
    The previous patches added the use of print_fmt string and changes
    the trace_define_field() function to also create the fields and
    format output for the event format files.
    
       text    data     bss     dec     hex filename
    5857201 1355780 9336808 16549789         fc879d vmlinux
    5884589 1351684 9337896 16574169         fce6d9 vmlinux-orig
    
    The above shows the size of the vmlinux after this patch set
    compared to the vmlinux-orig which is before the patch set.
    
    This saves us 27k on text, 1k on bss and adds just 4k of data.
    
    The total savings of 24k in size.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <4B273D4D.40604@cn.fujitsu.com>
    Acked-by: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 250ec865d5f5..c2a3077b7353 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -520,14 +520,6 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return ret;
 }
 
-extern char *__bad_type_size(void);
-
-#undef FIELD
-#define FIELD(type, name)						\
-	sizeof(type) != sizeof(field.name) ? __bad_type_size() :	\
-	#type, "common_" #name, offsetof(typeof(field), name),		\
-		sizeof(field.name), is_signed_type(type)
-
 static ssize_t
 event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
@@ -965,10 +957,6 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 				  filter);
 	}
 
-	/* A trace may not want to export its format */
-	if (!call->show_format)
-		return 0;
-
 	trace_create_file("format", 0444, call->dir, call,
 			  format);
 

commit 5a65e956220efc2421e21ee56d6153fd5c533a95
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Dec 15 15:39:53 2009 +0800

    tracing: Use defined fields and print_fmt to print formats
    
    The calls ftrace_format_##call() and ftrace_define_fields_##call()
    are almost duplicate in functionality. With the addition of the
    print_fmt in previous patches, these two functions can be merged
    into one.
    
    The trace_define_field() defines the fields and links them into
    the struct ftrace_event_call. The previous patches introduced
    the print_fmt field and this can now be used with the trace_define_field()
    to create the event format file fields and print_fmt field.
    
    The struct ftrace_event_call->fields are used to print the fields
    The struct ftrace_event_call->print_fmt is used to print
    the "print fmt: XXXXXXXXXXX" line.
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    LKML-Reference: <4B273D49.5000006@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 189b09baf4fb..250ec865d5f5 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -528,33 +528,16 @@ extern char *__bad_type_size(void);
 	#type, "common_" #name, offsetof(typeof(field), name),		\
 		sizeof(field.name), is_signed_type(type)
 
-static int trace_write_header(struct trace_seq *s)
-{
-	struct trace_entry field;
-
-	/* struct trace_entry */
-	return trace_seq_printf(s,
-			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
-			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
-			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
-			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
-			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
-			"\n",
-			FIELD(unsigned short, type),
-			FIELD(unsigned char, flags),
-			FIELD(unsigned char, preempt_count),
-			FIELD(int, pid),
-			FIELD(int, lock_depth));
-}
-
 static ssize_t
 event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
 {
 	struct ftrace_event_call *call = filp->private_data;
+	struct ftrace_event_field *field;
 	struct trace_seq *s;
+	int common_field_count = 5;
 	char *buf;
-	int r;
+	int r = 0;
 
 	if (*ppos)
 		return 0;
@@ -565,14 +548,48 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	/* If any of the first writes fail, so will the show_format. */
-
 	trace_seq_printf(s, "name: %s\n", call->name);
 	trace_seq_printf(s, "ID: %d\n", call->id);
 	trace_seq_printf(s, "format:\n");
-	trace_write_header(s);
 
-	r = call->show_format(call, s);
+	list_for_each_entry_reverse(field, &call->fields, link) {
+		/*
+		 * Smartly shows the array type(except dynamic array).
+		 * Normal:
+		 *	field:TYPE VAR
+		 * If TYPE := TYPE[LEN], it is shown:
+		 *	field:TYPE VAR[LEN]
+		 */
+		const char *array_descriptor = strchr(field->type, '[');
+
+		if (!strncmp(field->type, "__data_loc", 10))
+			array_descriptor = NULL;
+
+		if (!array_descriptor) {
+			r = trace_seq_printf(s, "\tfield:%s %s;\toffset:%u;"
+					"\tsize:%u;\tsigned:%d;\n",
+					field->type, field->name, field->offset,
+					field->size, !!field->is_signed);
+		} else {
+			r = trace_seq_printf(s, "\tfield:%.*s %s%s;\toffset:%u;"
+					"\tsize:%u;\tsigned:%d;\n",
+					(int)(array_descriptor - field->type),
+					field->type, field->name,
+					array_descriptor, field->offset,
+					field->size, !!field->is_signed);
+		}
+
+		if (--common_field_count == 0)
+			r = trace_seq_printf(s, "\n");
+
+		if (!r)
+			break;
+	}
+
+	if (r)
+		r = trace_seq_printf(s, "\nprint fmt: %s\n",
+				call->print_fmt);
+
 	if (!r) {
 		/*
 		 * ug!  The format output is bigger than a PAGE!!

commit 3b8e4273814a7f9e9a74ece517d9206fea919aaa
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Dec 8 11:14:52 2009 +0800

    tracing: Move a printk out of ftrace_raw_reg_event_foo()
    
    Move the printk from each ftrace_raw_reg_event_foo() to
    its caller ftrace_event_enable_disable(). This avoids each
    regfunc trace event callbacks to handle a same error report
    that can be carried from the caller.
    
    See how much space this saves:
    
       text    data     bss     dec     hex filename
    5345151 1961864 7103260 14410275         dbe223 vmlinux.o.old
    5331487 1961864 7103260 14396611         dbacc3 vmlinux.o
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jason Baron <jbaron@redhat.com>
    LKML-Reference: <4B1DC4AC.802@cn.fujitsu.com>
    [start cmdline record before calling regfunc to avoid lost
    window of pid to comm resolution]
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 97b0b3aa166d..189b09baf4fb 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -118,9 +118,11 @@ int trace_event_raw_init(struct ftrace_event_call *call)
 }
 EXPORT_SYMBOL_GPL(trace_event_raw_init);
 
-static void ftrace_event_enable_disable(struct ftrace_event_call *call,
+static int ftrace_event_enable_disable(struct ftrace_event_call *call,
 					int enable)
 {
+	int ret = 0;
+
 	switch (enable) {
 	case 0:
 		if (call->enabled) {
@@ -131,12 +133,20 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 		break;
 	case 1:
 		if (!call->enabled) {
-			call->enabled = 1;
 			tracing_start_cmdline_record();
-			call->regfunc(call);
+			ret = call->regfunc(call);
+			if (ret) {
+				tracing_stop_cmdline_record();
+				pr_info("event trace: Could not enable event "
+					"%s\n", call->name);
+				break;
+			}
+			call->enabled = 1;
 		}
 		break;
 	}
+
+	return ret;
 }
 
 static void ftrace_clear_events(void)
@@ -415,7 +425,7 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	case 0:
 	case 1:
 		mutex_lock(&event_mutex);
-		ftrace_event_enable_disable(call, val);
+		ret = ftrace_event_enable_disable(call, val);
 		mutex_unlock(&event_mutex);
 		break;
 
@@ -425,7 +435,7 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 	*ppos += cnt;
 
-	return cnt;
+	return ret ? ret : cnt;
 }
 
 static ssize_t

commit 614a71a26ba3d97e9fa85649db69a682b78e407d
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Dec 8 11:14:36 2009 +0800

    tracing: Pull up calls to trace_define_common_fields()
    
    Call trace_define_common_fields() in event_create_dir() only.
    This avoids trace events to handle it from their define_fields
    callbacks and shrinks the kernel code size:
    
       text    data     bss     dec     hex filename
    5346802 1961864 7103260 14411926         dbe896 vmlinux.o.old
    5345151 1961864 7103260 14410275         dbe223 vmlinux.o
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    LKML-Reference: <4B1DC49C.8000107@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8ed66e0d476b..97b0b3aa166d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -78,7 +78,7 @@ EXPORT_SYMBOL_GPL(trace_define_field);
 	if (ret)							\
 		return ret;
 
-int trace_define_common_fields(struct ftrace_event_call *call)
+static int trace_define_common_fields(struct ftrace_event_call *call)
 {
 	int ret;
 	struct trace_entry ent;
@@ -91,7 +91,6 @@ int trace_define_common_fields(struct ftrace_event_call *call)
 
 	return ret;
 }
-EXPORT_SYMBOL_GPL(trace_define_common_fields);
 
 void trace_destroy_fields(struct ftrace_event_call *call)
 {
@@ -927,7 +926,9 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 		  id);
 
 	if (call->define_fields) {
-		ret = call->define_fields(call);
+		ret = trace_define_common_fields(call);
+		if (!ret)
+			ret = call->define_fields(call);
 		if (ret < 0) {
 			pr_warning("Could not initialize trace point"
 				   " events/%s\n", call->name);

commit 87d9b4e1c52867a45331a9a5495f6448e0c68b23
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Dec 8 11:14:20 2009 +0800

    tracing: Extract duplicate ftrace_raw_init_event_foo()
    
    Use a generic trace_event_raw_init() function for all event's raw_init
    callbacks (but kprobes) instead of defining the same version for each
    of these.
    This shrinks the kernel code:
    
       text    data     bss     dec     hex filename
    5355293 1961928 7103260 14420481         dc0a01 vmlinux.o.old
    5346802 1961864 7103260 14411926         dbe896 vmlinux.o
    
    raw_init can't be removed, because ftrace events and kprobe events
    use different raw_init callbacks. Though it's possible to totally
    remove raw_init, I choose to leave it as it is for now.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    LKML-Reference: <4B1DC48C.7080603@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1d18315dc836..8ed66e0d476b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -105,6 +105,20 @@ void trace_destroy_fields(struct ftrace_event_call *call)
 	}
 }
 
+int trace_event_raw_init(struct ftrace_event_call *call)
+{
+	int id;
+
+	id = register_ftrace_event(call->event);
+	if (!id)
+		return -ENODEV;
+	call->id = id;
+	INIT_LIST_HEAD(&call->fields);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(trace_event_raw_init);
+
 static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 					int enable)
 {

commit 43315956509ca6913764861ac7dec128b91eb1ec
Merge: 9bf4e7fba800 6beba7adbe09
Author: Ingo Molnar <mingo@elte.hu>
Date:   Fri Oct 23 08:23:20 2009 +0200

    Merge branch 'perf/core' into perf/probes
    
    Conflicts:
            tools/perf/Makefile
    
    Merge reason:
    
     - fix the conflict
     - pick up the pr_*() infrastructure to queue up dependent patch
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit bb3c3e807140816b5f5fd4840473ee52a916ad4f
Merge: 595c36490deb 012abeea669e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sat Oct 17 09:58:25 2009 +0200

    Merge commit 'v2.6.32-rc5' into perf/probes
    
    Conflicts:
            kernel/trace/trace_event_profile.c
    
    Merge reason: update to -rc5 and resolve conflict.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 713490e02eed242b4c1c672b3c0c8b708f8b6f1d
Merge: c4dc775f5313 1beee96bae0d
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Oct 15 11:33:56 2009 +0200

    Merge branch 'tracing/core' into perf/core
    
    Merge reason: to add event filter support we need the following
    commits from the tracing tree:
    
     3f6fe06: tracing/filters: Unify the regex parsing helpers
     1889d20: tracing/filters: Provide basic regex support
     737f453: tracing/filters: Cleanup useless headers
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 26a50744b21fff65bd754874072857bee8967f4d
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Oct 6 01:09:50 2009 -0500

    tracing/events: Add 'signed' field to format files
    
    The sign info used for filters in the kernel is also useful to
    applications that process the trace stream.  Add it to the format
    files and make it available to userspace.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: rostedt@goodmis.org
    Cc: lizf@cn.fujitsu.com
    Cc: hch@infradead.org
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    LKML-Reference: <1254809398-8078-2-git-send-email-tzanussi@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d128f65778e6..cf3cabf6ce14 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -507,7 +507,7 @@ extern char *__bad_type_size(void);
 #define FIELD(type, name)						\
 	sizeof(type) != sizeof(field.name) ? __bad_type_size() :	\
 	#type, "common_" #name, offsetof(typeof(field), name),		\
-		sizeof(field.name)
+		sizeof(field.name), is_signed_type(type)
 
 static int trace_write_header(struct trace_seq *s)
 {
@@ -515,17 +515,17 @@ static int trace_write_header(struct trace_seq *s)
 
 	/* struct trace_entry */
 	return trace_seq_printf(s,
-				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
-				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
-				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
-				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
-				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
-				"\n",
-				FIELD(unsigned short, type),
-				FIELD(unsigned char, flags),
-				FIELD(unsigned char, preempt_count),
-				FIELD(int, pid),
-				FIELD(int, lock_depth));
+			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
+			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
+			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
+			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
+			"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\tsigned:%u;\n"
+			"\n",
+			FIELD(unsigned short, type),
+			FIELD(unsigned char, flags),
+			FIELD(unsigned char, preempt_count),
+			FIELD(int, pid),
+			FIELD(int, lock_depth));
 }
 
 static ssize_t

commit 88f70d7590538e427c8405a2e02ac2624847386c
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Fri Sep 25 11:20:54 2009 -0700

    tracing/ftrace: Fix to check create_event_dir() when adding new events
    
    Check result of event_create_dir() and add ftrace_event_call to
    ftrace_events list only if it is succeeded. Thanks to Li for pointing
    it out.
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Ingo Molnar <mingo@elte.hu>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <20090925182054.10157.55219.stgit@omoto>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a4b7c9a9130c..155b5d5a4e45 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -957,12 +957,12 @@ static int __trace_add_event_call(struct ftrace_event_call *call)
 	if (!d_events)
 		return -ENOENT;
 
-	list_add(&call->list, &ftrace_events);
 	ret = event_create_dir(call, d_events, &ftrace_event_id_fops,
 				&ftrace_enable_fops, &ftrace_event_filter_fops,
 				&ftrace_event_format_fops);
-	if (ret < 0)
-		list_del(&call->list);
+	if (!ret)
+		list_add(&call->list, &ftrace_events);
+
 	return ret;
 }
 
@@ -1124,10 +1124,11 @@ static void trace_module_add_events(struct module *mod)
 				return;
 		}
 		call->mod = mod;
-		list_add(&call->list, &ftrace_events);
-		event_create_dir(call, d_events,
-				 &file_ops->id, &file_ops->enable,
-				 &file_ops->filter, &file_ops->format);
+		ret = event_create_dir(call, d_events,
+				       &file_ops->id, &file_ops->enable,
+				       &file_ops->filter, &file_ops->format);
+		if (!ret)
+			list_add(&call->list, &ftrace_events);
 	}
 }
 
@@ -1267,10 +1268,12 @@ static __init int event_trace_init(void)
 				continue;
 			}
 		}
-		list_add(&call->list, &ftrace_events);
-		event_create_dir(call, d_events, &ftrace_event_id_fops,
-				 &ftrace_enable_fops, &ftrace_event_filter_fops,
-				 &ftrace_event_format_fops);
+		ret = event_create_dir(call, d_events, &ftrace_event_id_fops,
+				       &ftrace_enable_fops,
+				       &ftrace_event_filter_fops,
+				       &ftrace_event_format_fops);
+		if (!ret)
+			list_add(&call->list, &ftrace_events);
 	}
 
 	while (true) {

commit 0aa73ba1c4e1ad1d51a29e0df95ccd9f746918b6
Merge: 925936ebf35a 33974093c024
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Oct 1 11:20:33 2009 +0200

    Merge branch 'tracing/urgent' into tracing/core
    
    Merge reason: Pick up latest fixes and update to latest upstream.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit f3f3f0092477d0165f3f1bf0fd518550b2abd097
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Sep 24 15:27:41 2009 +0200

    tracing/event: Cleanup the useless dentry variable
    
    Cleanup the useless dentry variable while creating a kernel
    event set of files. trace_create_file() warns if it fails to
    create the file anyway, and we don't store the dentry anywhere.
    
    v2: Fix a small conflict in kernel/trace/trace_events.c
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 56c260b83a9c..8c91b7c8f047 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -898,9 +898,9 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 			   "'%s/filter' entry\n", name);
 	}
 
-	entry = trace_create_file("enable", 0644, system->entry,
-				  (void *)system->name,
-				  &ftrace_system_enable_fops);
+	trace_create_file("enable", 0644, system->entry,
+			  (void *)system->name,
+			  &ftrace_system_enable_fops);
 
 	return system->entry;
 }
@@ -912,7 +912,6 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 const struct file_operations *filter,
 		 const struct file_operations *format)
 {
-	struct dentry *entry;
 	int ret;
 
 	/*
@@ -930,12 +929,12 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 	}
 
 	if (call->regfunc)
-		entry = trace_create_file("enable", 0644, call->dir, call,
-					  enable);
+		trace_create_file("enable", 0644, call->dir, call,
+				  enable);
 
 	if (call->id && call->profile_enable)
-		entry = trace_create_file("id", 0444, call->dir, call,
-					  id);
+		trace_create_file("id", 0444, call->dir, call,
+		 		  id);
 
 	if (call->define_fields) {
 		ret = call->define_fields(call);
@@ -944,16 +943,16 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 				   " events/%s\n", call->name);
 			return ret;
 		}
-		entry = trace_create_file("filter", 0644, call->dir, call,
-					  filter);
+		trace_create_file("filter", 0644, call->dir, call,
+				  filter);
 	}
 
 	/* A trace may not want to export its format */
 	if (!call->show_format)
 		return 0;
 
-	entry = trace_create_file("format", 0444, call->dir, call,
-				  format);
+	trace_create_file("format", 0444, call->dir, call,
+			  format);
 
 	return 0;
 }

commit d7a4b414eed51f1653bb05ebe84122bf9a7ae18b
Merge: 1f0ab4097646 a724eada8c2a
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Wed Sep 23 23:08:43 2009 +0200

    Merge commit 'linus/master' into tracing/kprobes
    
    Conflicts:
            kernel/trace/Makefile
            kernel/trace/trace.h
            kernel/trace/trace_event_types.h
            kernel/trace/trace_export.c
    
    Merge reason:
            Sync with latest significant tracing core changes.

commit 4ba7978e98a3ff92cebfb0f31a02c309e3ffa1fe
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Sep 22 13:52:20 2009 +0800

    tracing: Check the return value of trace_get_user()
    
    Return immediately if trace_get_user() returned failure.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Jiri Olsa <jolsa@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <4AB86614.7020803@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 6f03c8a1105e..d128f65778e6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -232,10 +232,9 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 		   size_t cnt, loff_t *ppos)
 {
 	struct trace_parser parser;
-	size_t read = 0;
-	ssize_t ret;
+	ssize_t read, ret;
 
-	if (!cnt || cnt < 0)
+	if (!cnt)
 		return 0;
 
 	ret = tracing_update_buffers();
@@ -247,7 +246,7 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 
 	read = trace_get_user(&parser, ubuf, cnt, ppos);
 
-	if (trace_parser_loaded((&parser))) {
+	if (read >= 0 && trace_parser_loaded((&parser))) {
 		int set = 1;
 
 		if (*parser.buffer == '!')

commit 30bd39cd6244ffe3258c9203405286ef77b1c4eb
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Sep 18 14:07:05 2009 +0800

    tracing/events: use list_for_entry_continue
    
    Simplify s_next() and t_next().
    
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4AB32389.1030005@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 56c260b83a9c..6f03c8a1105e 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -271,42 +271,32 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 static void *
 t_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct list_head *list = m->private;
-	struct ftrace_event_call *call;
+	struct ftrace_event_call *call = v;
 
 	(*pos)++;
 
-	for (;;) {
-		if (list == &ftrace_events)
-			return NULL;
-
-		call = list_entry(list, struct ftrace_event_call, list);
-
+	list_for_each_entry_continue(call, &ftrace_events, list) {
 		/*
 		 * The ftrace subsystem is for showing formats only.
 		 * They can not be enabled or disabled via the event files.
 		 */
 		if (call->regfunc)
-			break;
-
-		list = list->next;
+			return call;
 	}
 
-	m->private = list->next;
-
-	return call;
+	return NULL;
 }
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
-	struct ftrace_event_call *call = NULL;
+	struct ftrace_event_call *call;
 	loff_t l;
 
 	mutex_lock(&event_mutex);
 
-	m->private = ftrace_events.next;
+	call = list_entry(&ftrace_events, struct ftrace_event_call, list);
 	for (l = 0; l <= *pos; ) {
-		call = t_next(m, NULL, &l);
+		call = t_next(m, call, &l);
 		if (!call)
 			break;
 	}
@@ -316,37 +306,28 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 static void *
 s_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct list_head *list = m->private;
-	struct ftrace_event_call *call;
+	struct ftrace_event_call *call = v;
 
 	(*pos)++;
 
- retry:
-	if (list == &ftrace_events)
-		return NULL;
-
-	call = list_entry(list, struct ftrace_event_call, list);
-
-	if (!call->enabled) {
-		list = list->next;
-		goto retry;
+	list_for_each_entry_continue(call, &ftrace_events, list) {
+		if (call->enabled)
+			return call;
 	}
 
-	m->private = list->next;
-
-	return call;
+	return NULL;
 }
 
 static void *s_start(struct seq_file *m, loff_t *pos)
 {
-	struct ftrace_event_call *call = NULL;
+	struct ftrace_event_call *call;
 	loff_t l;
 
 	mutex_lock(&event_mutex);
 
-	m->private = ftrace_events.next;
+	call = list_entry(&ftrace_events, struct ftrace_event_call, list);
 	for (l = 0; l <= *pos; ) {
-		call = s_next(m, NULL, &l);
+		call = s_next(m, call, &l);
 		if (!call)
 			break;
 	}

commit 45bd00d31de886f8425b4dd33204b911b0a466a9
Merge: 40d9d82c8ab8 ab86e5765d41
Author: Ingo Molnar <mingo@elte.hu>
Date:   Thu Sep 17 20:52:23 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: Pick up kernel/softirq.c update for dependent fix.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit 4fead8e46fded93cc0d432ced774d9a3a8d21bad
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Mon Sep 14 16:49:12 2009 -0400

    ftrace: Fix trace_remove_event_call() to lock trace_event_mutex
    
    Lock not only event_mutex but also trace_event_mutex in
    trace_remove_event_call() to protect __unregister_ftrace_event().
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <20090914204912.18779.68734.stgit@dhcp-100-2-132.bos.redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 83cc2c01195d..f85b0f1cb942 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1054,6 +1054,9 @@ static void remove_subsystem_dir(const char *name)
 	}
 }
 
+/*
+ * Must be called under locking both of event_mutex and trace_event_mutex.
+ */
 static void __trace_remove_event_call(struct ftrace_event_call *call)
 {
 	ftrace_event_enable_disable(call, 0);
@@ -1070,7 +1073,9 @@ static void __trace_remove_event_call(struct ftrace_event_call *call)
 void trace_remove_event_call(struct ftrace_event_call *call)
 {
 	mutex_lock(&event_mutex);
+	down_write(&trace_event_mutex);
 	__trace_remove_event_call(call);
+	up_write(&trace_event_mutex);
 	mutex_unlock(&event_mutex);
 }
 

commit 588bebb74fe87270f94c2810652bd683d63c4b54
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Wed Sep 16 11:42:55 2009 -0400

    ftrace: Fix trace_add_event_call() to initialize list
    
    Handle failure path in trace_add_event_call() to fix the below bug
    which occurred when I tried to add invalid event twice.
    
    Could not create debugfs 'kmalloc' directory
    Failed to register kprobe event: kmalloc
    Faild to register probe event(-1)
    ------------[ cut here ]------------
    WARNING: at /home/mhiramat/ksrc/random-tracing/lib/list_debug.c:26
    __list_add+0x27/0x5c()
    Hardware name:
    list_add corruption. next->prev should be prev (c07d78cc), but was
    00001000. (next=d854236c).
    Modules linked in: sunrpc uinput virtio_net virtio_balloon i2c_piix4 pcspkr
    i2c_core virtio_blk virtio_pci virtio_ring virtio [last unloaded:
    scsi_wait_scan]
    Pid: 1394, comm: tee Not tainted 2.6.31-rc9 #51
    Call Trace:
     [<c0438424>] warn_slowpath_common+0x65/0x7c
     [<c05371b3>] ? __list_add+0x27/0x5c
     [<c043846f>] warn_slowpath_fmt+0x24/0x27
     [<c05371b3>] __list_add+0x27/0x5c
     [<c047f050>] list_add+0xa/0xc
     [<c047f8f5>] trace_add_event_call+0x60/0x97
     [<c0483133>] command_trace_probe+0x42c/0x51b
     [<c044a1b3>] ? remove_wait_queue+0x22/0x27
     [<c042a9c0>] ? __wake_up+0x32/0x3b
     [<c04832f6>] probes_write+0xd4/0x10a
     [<c0483222>] ? probes_write+0x0/0x10a
     [<c04b27a9>] vfs_write+0x80/0xdf
     [<c04b289c>] sys_write+0x3b/0x5d
     [<c0670d41>] syscall_call+0x7/0xb
    ---[ end trace 2b962b5dc1fdc07d ]---
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <4AB1077F.6020107@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ba3492076ab2..83cc2c01195d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1010,9 +1010,12 @@ static int __trace_add_event_call(struct ftrace_event_call *call)
 		return -ENOENT;
 
 	list_add(&call->list, &ftrace_events);
-	return event_create_dir(call, d_events, &ftrace_event_id_fops,
+	ret = event_create_dir(call, d_events, &ftrace_event_id_fops,
 				&ftrace_enable_fops, &ftrace_event_filter_fops,
 				&ftrace_event_format_fops);
+	if (ret < 0)
+		list_del(&call->list);
+	return ret;
 }
 
 /* Add an additional event_call dynamically */

commit ada3fa15057205b7d3f727bba5cd26b5912e350f
Merge: 2f82af08fcc7 5579fd7e6aed
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Sep 15 09:39:44 2009 -0700

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/tj/percpu: (46 commits)
      powerpc64: convert to dynamic percpu allocator
      sparc64: use embedding percpu first chunk allocator
      percpu: kill lpage first chunk allocator
      x86,percpu: use embedding for 64bit NUMA and page for 32bit NUMA
      percpu: update embedding first chunk allocator to handle sparse units
      percpu: use group information to allocate vmap areas sparsely
      vmalloc: implement pcpu_get_vm_areas()
      vmalloc: separate out insert_vmalloc_vm()
      percpu: add chunk->base_addr
      percpu: add pcpu_unit_offsets[]
      percpu: introduce pcpu_alloc_info and pcpu_group_info
      percpu: move pcpu_lpage_build_unit_map() and pcpul_lpage_dump_cfg() upward
      percpu: add @align to pcpu_fc_alloc_fn_t
      percpu: make @dyn_size mandatory for pcpu_setup_first_chunk()
      percpu: drop @static_size from first chunk allocators
      percpu: generalize first chunk allocator selection
      percpu: build first chunk allocators selectively
      percpu: rename 4k first chunk allocator to page
      percpu: improve boot messages
      percpu: fix pcpu_reclaim() locking
      ...
    
    Fix trivial conflict as by Tejun Heo in kernel/sched.c

commit 1f5a6b45416694ff8c0d04625f1a438a0e380add
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Sep 14 11:58:24 2009 -0400

    tracing: make testing syscall events a separate configuration
    
    Parag noticed that the number of event tests has increased tremendously:
    
    grep "Testing event" dmesg.31rc9 |wc -l
    100
    
    grep "Testing event" dmesg.31git |wc -l
    1172
    
    This is due to the testing of every syscall event when ftrace self
    test is enabled. This adds a bit more time to kernel boot up and can
    affect development by slowing down the time it takes between reboots.
    
    This option makes the testing of the syscall events into a separate
    config, to still be able to test most of ftrace internals at boot up
    but not have to wait for all the syscall events to be tested.
    
    The syscall event testing only tests the enabling and disabling of
    the trace point, since the syscalls are not executed. What really needs
    to be done is to somehow have a userspace tool test the syscall tracepoints
    as well.
    
    Reported-by: Parag Warudkar <parag.lkml@gmail.com>
    LKML-Reference: <f7848160909130815l3e768a30n3b28808bbe5c254b@mail.gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0fa8f9faa61c..787f0fb0994e 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1326,6 +1326,18 @@ static __init void event_trace_self_tests(void)
 		if (!call->regfunc)
 			continue;
 
+/*
+ * Testing syscall events here is pretty useless, but
+ * we still do it if configured. But this is time consuming.
+ * What we really need is a user thread to perform the
+ * syscalls as we test.
+ */
+#ifndef CONFIG_EVENT_TRACE_TEST_SYSCALLS
+		if (call->system &&
+		    strcmp(call->system, "syscalls") == 0)
+			continue;
+#endif
+
 		pr_info("Testing event %s: ", call->name);
 
 		/*

commit ec827c7ece8901044e6b3f92aeea489be9e1bcf7
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Sep 14 10:50:23 2009 -0400

    tracing: add static to generated TRACE_EVENT functions
    
    Some of the generated functions used in the TRACE_EVENT macros are
    not declared static, but they are not global.
    
    Discovered by sparse.
    
    Reported-by: Jaswinder Singh Rajput <jaswinder@kernel.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index adbed124c3e7..0fa8f9faa61c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1154,7 +1154,7 @@ static int trace_module_notify(struct notifier_block *self,
 }
 #endif /* CONFIG_MODULES */
 
-struct notifier_block trace_module_nb = {
+static struct notifier_block trace_module_nb = {
 	.notifier_call = trace_module_notify,
 	.priority = 0,
 };

commit 4e5292ea1ac0c2939e815e6c44fad3d8696ea281
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Sep 12 19:26:21 2009 -0400

    tracing: use the new trace_entries.h to create format files
    
    This patch changes the way the format files in
    
      debugfs/tracing/events/ftrace/*/format
    
    are created. It uses the new trace_entries.h file to automate the
    creation of the format files to ensure that they are always in sync
    with the actual structures. This is the same methodology used to
    create the format files for the TRACE_EVENT macro.
    
    This also updates the filter creation that was built on the creation
    of the format files.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f46d14cefdec..adbed124c3e7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -21,6 +21,7 @@
 
 #include "trace_output.h"
 
+#undef TRACE_SYSTEM
 #define TRACE_SYSTEM "TRACE_SYSTEM"
 
 DEFINE_MUTEX(event_mutex);

commit 489663644c35d50a20f58d468a7cbc705e6a29ce
Author: jolsa@redhat.com <jolsa@redhat.com>
Date:   Fri Sep 11 17:29:28 2009 +0200

    tracing: trace parser support for set_event
    
    Convert the parsing of the file 'set_event' to use the generic
    trace_praser 'trace_get_user' function.
    
    Signed-off-by: Jiri Olsa <jolsa@redhat.com>
    LKML-Reference: <1252682969-3366-3-git-send-email-jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 975f324a07e7..f46d14cefdec 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -230,11 +230,9 @@ static ssize_t
 ftrace_event_write(struct file *file, const char __user *ubuf,
 		   size_t cnt, loff_t *ppos)
 {
+	struct trace_parser parser;
 	size_t read = 0;
-	int i, set = 1;
 	ssize_t ret;
-	char *buf;
-	char ch;
 
 	if (!cnt || cnt < 0)
 		return 0;
@@ -243,60 +241,28 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 	if (ret < 0)
 		return ret;
 
-	ret = get_user(ch, ubuf++);
-	if (ret)
-		return ret;
-	read++;
-	cnt--;
-
-	/* skip white space */
-	while (cnt && isspace(ch)) {
-		ret = get_user(ch, ubuf++);
-		if (ret)
-			return ret;
-		read++;
-		cnt--;
-	}
-
-	/* Only white space found? */
-	if (isspace(ch)) {
-		file->f_pos += read;
-		ret = read;
-		return ret;
-	}
-
-	buf = kmalloc(EVENT_BUF_SIZE+1, GFP_KERNEL);
-	if (!buf)
+	if (trace_parser_get_init(&parser, EVENT_BUF_SIZE + 1))
 		return -ENOMEM;
 
-	if (cnt > EVENT_BUF_SIZE)
-		cnt = EVENT_BUF_SIZE;
+	read = trace_get_user(&parser, ubuf, cnt, ppos);
+
+	if (trace_parser_loaded((&parser))) {
+		int set = 1;
 
-	i = 0;
-	while (cnt && !isspace(ch)) {
-		if (!i && ch == '!')
+		if (*parser.buffer == '!')
 			set = 0;
-		else
-			buf[i++] = ch;
 
-		ret = get_user(ch, ubuf++);
+		parser.buffer[parser.idx] = 0;
+
+		ret = ftrace_set_clr_event(parser.buffer + !set, set);
 		if (ret)
-			goto out_free;
-		read++;
-		cnt--;
+			goto out_put;
 	}
-	buf[i] = 0;
-
-	file->f_pos += read;
-
-	ret = ftrace_set_clr_event(buf, set);
-	if (ret)
-		goto out_free;
 
 	ret = read;
 
- out_free:
-	kfree(buf);
+ out_put:
+	trace_parser_put(&parser);
 
 	return ret;
 }

commit 637e7e864103a7a68c1ce43ada27dfc25c0d113f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Sep 11 13:55:35 2009 -0400

    tracing: add lock depth to entries
    
    This patch adds the lock depth of the big kernel lock to the generic
    entry header. This way we can see the depth of the lock and help
    in removing the BKL.
    
    Example:
    
     #                  _------=> CPU#
     #                 / _-----=> irqs-off
     #                | / _----=> need-resched
     #                || / _---=> hardirq/softirq
     #                ||| / _--=> preempt-depth
     #                |||| /_--=> lock-depth
     #                |||||/     delay
     #  cmd     pid   |||||| time  |   caller
     #     \   /      ||||||   \   |   /
       <idle>-0       2.N..3 5902255250us+: lock_acquire: read rcu_read_lock
       <idle>-0       2.N..3 5902255253us+: lock_release: rcu_read_lock
       <idle>-0       2dN..3 5902255257us+: lock_acquire: xtime_lock
       <idle>-0       2dN..4 5902255259us : lock_acquire: clocksource_lock
       <idle>-0       2dN..4 5902255261us+: lock_release: clocksource_lock
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 28d92027a93c..975f324a07e7 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -86,6 +86,7 @@ int trace_define_common_fields(struct ftrace_event_call *call)
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
+	__common_field(int, lock_depth);
 
 	return ret;
 }
@@ -567,6 +568,7 @@ static int trace_write_header(struct trace_seq *s)
 
 	/* struct trace_entry */
 	return trace_seq_printf(s,
+				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
@@ -575,7 +577,8 @@ static int trace_write_header(struct trace_seq *s)
 				FIELD(unsigned short, type),
 				FIELD(unsigned char, flags),
 				FIELD(unsigned char, preempt_count),
-				FIELD(int, pid));
+				FIELD(int, pid),
+				FIELD(int, lock_depth));
 }
 
 static ssize_t

commit 48659d31195bb76d688e99dabd816c5472fb1656
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Sep 11 11:36:23 2009 -0400

    tracing: move tgid out of generic entry and into userstack
    
    The userstack trace required the recording of the tgid entry.
    Unfortunately, it was added to the generic entry where it wasted
    4 bytes of every entry and was only used by one entry.
    
    This patch moves it out of the generic field and moves it into the
    only user (userstack_entry).
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 78b1ed230177..28d92027a93c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -86,7 +86,6 @@ int trace_define_common_fields(struct ftrace_event_call *call)
 	__common_field(unsigned char, flags);
 	__common_field(unsigned char, preempt_count);
 	__common_field(int, pid);
-	__common_field(int, tgid);
 
 	return ret;
 }
@@ -568,7 +567,6 @@ static int trace_write_header(struct trace_seq *s)
 
 	/* struct trace_entry */
 	return trace_seq_printf(s,
-				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
@@ -577,8 +575,7 @@ static int trace_write_header(struct trace_seq *s)
 				FIELD(unsigned short, type),
 				FIELD(unsigned char, flags),
 				FIELD(unsigned char, preempt_count),
-				FIELD(int, pid),
-				FIELD(int, tgid));
+				FIELD(int, pid));
 }
 
 static ssize_t

commit 8f8ffe2485bcaa890800681451d380779cea06af
Merge: 70069577323e d28daf923ac5
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Fri Sep 11 01:09:23 2009 +0200

    Merge commit 'tracing/core' into tracing/kprobes
    
    Conflicts:
            kernel/trace/trace_export.c
            kernel/trace/trace_kprobe.c
    
    Merge reason: This topic branch lacks an important
    build fix in tracing/core:
    
            0dd7b74787eaf7858c6c573353a83c3e2766e674:
            tracing: Fix double CPP substitution in TRACE_EVENT_FN
    
    that prevents from multiple tracepoint headers inclusion crashes.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

commit e77405ad80f53966524b5c31244e13fbbbecbd84
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Sep 2 14:17:06 2009 -0400

    tracing: pass around ring buffer instead of tracer
    
    The latency tracers (irqsoff and wakeup) can swap trace buffers
    on the fly. If an event is happening and has reserved data on one of
    the buffers, and the latency tracer swaps the global buffer with the
    max buffer, the result is that the event may commit the data to the
    wrong buffer.
    
    This patch changes the API to the trace recording to be recieve the
    buffer that was used to reserve a commit. Then this buffer can be passed
    in to the commit.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d33bcdeffe69..78b1ed230177 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1438,6 +1438,7 @@ static void
 function_test_events_call(unsigned long ip, unsigned long parent_ip)
 {
 	struct ring_buffer_event *event;
+	struct ring_buffer *buffer;
 	struct ftrace_entry *entry;
 	unsigned long flags;
 	long disabled;
@@ -1455,7 +1456,8 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip)
 
 	local_save_flags(flags);
 
-	event = trace_current_buffer_lock_reserve(TRACE_FN, sizeof(*entry),
+	event = trace_current_buffer_lock_reserve(&buffer,
+						  TRACE_FN, sizeof(*entry),
 						  flags, pc);
 	if (!event)
 		goto out;
@@ -1463,7 +1465,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip)
 	entry->ip			= ip;
 	entry->parent_ip		= parent_ip;
 
-	trace_nowake_buffer_unlock_commit(event, flags, pc);
+	trace_nowake_buffer_unlock_commit(buffer, event, flags, pc);
 
  out:
 	atomic_dec(&per_cpu(test_event_disable, cpu));

commit aeaeae1187d7520f1c5559623f0a149da6a1c96e
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Aug 27 05:09:51 2009 +0200

    tracing: Restore the const qualifier for field names and types definition
    
    Restore the const qualifier in field's name and type parameters of
    trace_define_field that was lost while solving a conflict.
    
    Fields names and types are defined as builtin constant strings in
    static TRACE_EVENTs. But kprobes allocates these dynamically.
    
    That said, we still want to always pass these strings as const char *
    in trace_define_fields() to avoid any further accidental writes on
    the pointed strings.
    
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8079bb511c43..197cdaa96c43 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -27,8 +27,8 @@ DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
 
-int trace_define_field(struct ftrace_event_call *call, char *type,
-		       char *name, int offset, int size, int is_signed,
+int trace_define_field(struct ftrace_event_call *call, const char *type,
+		       const char *name, int offset, int size, int is_signed,
 		       int filter_type)
 {
 	struct ftrace_event_field *field;

commit bd1a5c849bdcc5c89e4a6a18216cd2b9a7a8a78f
Author: Masami Hiramatsu <mhiramat@redhat.com>
Date:   Thu Aug 13 16:34:53 2009 -0400

    tracing: Ftrace dynamic ftrace_event_call support
    
    Add dynamic ftrace_event_call support to ftrace. Trace engines can add
    new ftrace_event_call to ftrace on the fly. Each operator function of
    the call takes an ftrace_event_call data structure as an argument,
    because these functions may be shared among several ftrace_event_calls.
    
    Changes from v13:
     - Define remove_subsystem_dir() always (revirt a2ca5e03), because
       trace_remove_event_call() uses it.
     - Modify syscall tracer because of ftrace_event_call change.
    
    [fweisbec@gmail.com: Fixed conflict against latest tracing/core]
    
    Signed-off-by: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Ananth N Mavinakayanahalli <ananth@in.ibm.com>
    Cc: Avi Kivity <avi@redhat.com>
    Cc: Andi Kleen <ak@linux.intel.com>
    Cc: Christoph Hellwig <hch@infradead.org>
    Cc: Frank Ch. Eigler <fche@redhat.com>
    Cc: H. Peter Anvin <hpa@zytor.com>
    Cc: Ingo Molnar <mingo@elte.hu>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Jim Keniston <jkenisto@us.ibm.com>
    Cc: K.Prasad <prasad@linux.vnet.ibm.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Przemysaw Paweczyk <przemyslaw@pawelczyk.it>
    Cc: Roland McGrath <roland@redhat.com>
    Cc: Sam Ravnborg <sam@ravnborg.org>
    Cc: Srikar Dronamraju <srikar@linux.vnet.ibm.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Vegard Nossum <vegard.nossum@gmail.com>
    LKML-Reference: <20090813203453.31965.71901.stgit@localhost.localdomain>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d33bcdeffe69..8079bb511c43 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -27,8 +27,8 @@ DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
 
-int trace_define_field(struct ftrace_event_call *call, const char *type,
-		       const char *name, int offset, int size, int is_signed,
+int trace_define_field(struct ftrace_event_call *call, char *type,
+		       char *name, int offset, int size, int is_signed,
 		       int filter_type)
 {
 	struct ftrace_event_field *field;
@@ -92,9 +92,7 @@ int trace_define_common_fields(struct ftrace_event_call *call)
 }
 EXPORT_SYMBOL_GPL(trace_define_common_fields);
 
-#ifdef CONFIG_MODULES
-
-static void trace_destroy_fields(struct ftrace_event_call *call)
+void trace_destroy_fields(struct ftrace_event_call *call)
 {
 	struct ftrace_event_field *field, *next;
 
@@ -106,8 +104,6 @@ static void trace_destroy_fields(struct ftrace_event_call *call)
 	}
 }
 
-#endif /* CONFIG_MODULES */
-
 static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 					int enable)
 {
@@ -116,14 +112,14 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 		if (call->enabled) {
 			call->enabled = 0;
 			tracing_stop_cmdline_record();
-			call->unregfunc(call->data);
+			call->unregfunc(call);
 		}
 		break;
 	case 1:
 		if (!call->enabled) {
 			call->enabled = 1;
 			tracing_start_cmdline_record();
-			call->regfunc(call->data);
+			call->regfunc(call);
 		}
 		break;
 	}
@@ -991,27 +987,43 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 	return 0;
 }
 
-#define for_each_event(event, start, end)			\
-	for (event = start;					\
-	     (unsigned long)event < (unsigned long)end;		\
-	     event++)
+static int __trace_add_event_call(struct ftrace_event_call *call)
+{
+	struct dentry *d_events;
+	int ret;
 
-#ifdef CONFIG_MODULES
+	if (!call->name)
+		return -EINVAL;
 
-static LIST_HEAD(ftrace_module_file_list);
+	if (call->raw_init) {
+		ret = call->raw_init(call);
+		if (ret < 0) {
+			if (ret != -ENOSYS)
+				pr_warning("Could not initialize trace "
+				"events/%s\n", call->name);
+			return ret;
+		}
+	}
 
-/*
- * Modules must own their file_operations to keep up with
- * reference counting.
- */
-struct ftrace_module_file_ops {
-	struct list_head		list;
-	struct module			*mod;
-	struct file_operations		id;
-	struct file_operations		enable;
-	struct file_operations		format;
-	struct file_operations		filter;
-};
+	d_events = event_trace_events_dir();
+	if (!d_events)
+		return -ENOENT;
+
+	list_add(&call->list, &ftrace_events);
+	return event_create_dir(call, d_events, &ftrace_event_id_fops,
+				&ftrace_enable_fops, &ftrace_event_filter_fops,
+				&ftrace_event_format_fops);
+}
+
+/* Add an additional event_call dynamically */
+int trace_add_event_call(struct ftrace_event_call *call)
+{
+	int ret;
+	mutex_lock(&event_mutex);
+	ret = __trace_add_event_call(call);
+	mutex_unlock(&event_mutex);
+	return ret;
+}
 
 static void remove_subsystem_dir(const char *name)
 {
@@ -1039,6 +1051,48 @@ static void remove_subsystem_dir(const char *name)
 	}
 }
 
+static void __trace_remove_event_call(struct ftrace_event_call *call)
+{
+	ftrace_event_enable_disable(call, 0);
+	if (call->event)
+		__unregister_ftrace_event(call->event);
+	debugfs_remove_recursive(call->dir);
+	list_del(&call->list);
+	trace_destroy_fields(call);
+	destroy_preds(call);
+	remove_subsystem_dir(call->system);
+}
+
+/* Remove an event_call */
+void trace_remove_event_call(struct ftrace_event_call *call)
+{
+	mutex_lock(&event_mutex);
+	__trace_remove_event_call(call);
+	mutex_unlock(&event_mutex);
+}
+
+#define for_each_event(event, start, end)			\
+	for (event = start;					\
+	     (unsigned long)event < (unsigned long)end;		\
+	     event++)
+
+#ifdef CONFIG_MODULES
+
+static LIST_HEAD(ftrace_module_file_list);
+
+/*
+ * Modules must own their file_operations to keep up with
+ * reference counting.
+ */
+struct ftrace_module_file_ops {
+	struct list_head		list;
+	struct module			*mod;
+	struct file_operations		id;
+	struct file_operations		enable;
+	struct file_operations		format;
+	struct file_operations		filter;
+};
+
 static struct ftrace_module_file_ops *
 trace_create_file_ops(struct module *mod)
 {
@@ -1096,7 +1150,7 @@ static void trace_module_add_events(struct module *mod)
 		if (!call->name)
 			continue;
 		if (call->raw_init) {
-			ret = call->raw_init();
+			ret = call->raw_init(call);
 			if (ret < 0) {
 				if (ret != -ENOSYS)
 					pr_warning("Could not initialize trace "
@@ -1131,14 +1185,7 @@ static void trace_module_remove_events(struct module *mod)
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
 		if (call->mod == mod) {
 			found = true;
-			ftrace_event_enable_disable(call, 0);
-			if (call->event)
-				__unregister_ftrace_event(call->event);
-			debugfs_remove_recursive(call->dir);
-			list_del(&call->list);
-			trace_destroy_fields(call);
-			destroy_preds(call);
-			remove_subsystem_dir(call->system);
+			__trace_remove_event_call(call);
 		}
 	}
 
@@ -1256,7 +1303,7 @@ static __init int event_trace_init(void)
 		if (!call->name)
 			continue;
 		if (call->raw_init) {
-			ret = call->raw_init();
+			ret = call->raw_init(call);
 			if (ret < 0) {
 				if (ret != -ENOSYS)
 					pr_warning("Could not initialize trace "

commit 43b51ead3f752a3935116e5b1a94254b8573734f
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Aug 7 10:33:22 2009 +0800

    tracing/filters: Add __field_ext() to TRACE_EVENT
    
    Add __field_ext(), so a field can be assigned to a specific
    filter_type, which matches a corresponding filter function.
    
    For example, a later patch will allow this:
            __field_ext(const char *, str, FILTER_PTR_STR);
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4A7B9272.6050709@cn.fujitsu.com>
    
    [
      Fixed a -1 to FILTER_OTHER
      Forward ported to latest kernel.
    ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5740e90f4ca1..d33bcdeffe69 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -28,7 +28,8 @@ DEFINE_MUTEX(event_mutex);
 LIST_HEAD(ftrace_events);
 
 int trace_define_field(struct ftrace_event_call *call, const char *type,
-		       const char *name, int offset, int size, int is_signed)
+		       const char *name, int offset, int size, int is_signed,
+		       int filter_type)
 {
 	struct ftrace_event_field *field;
 
@@ -44,7 +45,11 @@ int trace_define_field(struct ftrace_event_call *call, const char *type,
 	if (!field->type)
 		goto err;
 
-	field->filter_type = filter_assign_type(type);
+	if (filter_type == FILTER_OTHER)
+		field->filter_type = filter_assign_type(type);
+	else
+		field->filter_type = filter_type;
+
 	field->offset = offset;
 	field->size = size;
 	field->is_signed = is_signed;
@@ -68,7 +73,7 @@ EXPORT_SYMBOL_GPL(trace_define_field);
 	ret = trace_define_field(call, #type, "common_" #item,		\
 				 offsetof(typeof(ent), item),		\
 				 sizeof(ent.item),			\
-				 is_signed_type(type));			\
+				 is_signed_type(type), FILTER_OTHER);	\
 	if (ret)							\
 		return ret;
 

commit aa38e9fc3ea804290efd3a39316d7f7e6c945800
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri Aug 7 10:33:02 2009 +0800

    tracing/filters: Add filter_type to struct ftrace_event_field
    
    The type of a field is stored as a string in @type, and here
    we add @filter_type which is an enum value.
    
    This prepares for later patches, so we can specifically assign
    different @filter_type for the same @type.
    
    For example normally a "char *" field is treated as a ptr,
    but we may want it to be treated as a string when doing filting.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <4A7B925E.9030605@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 79d352027a61..5740e90f4ca1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -44,9 +44,11 @@ int trace_define_field(struct ftrace_event_call *call, const char *type,
 	if (!field->type)
 		goto err;
 
+	field->filter_type = filter_assign_type(type);
 	field->offset = offset;
 	field->size = size;
 	field->is_signed = is_signed;
+
 	list_add(&field->link, &call->fields);
 
 	return 0;

commit 540b7b8d65575c80162f2a0f38e1d313c92a6042
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Aug 19 15:54:51 2009 +0800

    tracing/syscalls: Add filtering support
    
    Add filtering support for syscall events:
    
     # echo 'mode == 0666' > events/syscalls/sys_enter_open
     # echo 'ret == 0' > events/syscalls/sys_exit_open
     # echo 1 > events/syscalls/sys_enter_open
     # echo 1 > events/syscalls/sys_exit_open
     # cat trace
     ...
       modprobe-3084 [001] 117.463140: sys_open(filename: 917d3e8, flags: 0, mode: 1b6)
       modprobe-3084 [001] 117.463176: sys_open -> 0x0
           less-3086 [001] 117.510455: sys_open(filename: 9c6bdb8, flags: 8000, mode: 1b6)
       sendmail-2574 [001] 122.145840: sys_open(filename: b807a365, flags: 0, mode: 1b6)
     ...
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A8BAFCB.1040006@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9c7ecfb3416f..79d352027a61 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -27,8 +27,8 @@ DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
 
-int trace_define_field(struct ftrace_event_call *call, char *type,
-		       char *name, int offset, int size, int is_signed)
+int trace_define_field(struct ftrace_event_call *call, const char *type,
+		       const char *name, int offset, int size, int is_signed)
 {
 	struct ftrace_event_field *field;
 
@@ -83,6 +83,7 @@ int trace_define_common_fields(struct ftrace_event_call *call)
 
 	return ret;
 }
+EXPORT_SYMBOL_GPL(trace_define_common_fields);
 
 #ifdef CONFIG_MODULES
 

commit e647d6b314266adb904d4b84973eda0afa856946
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Aug 19 15:54:32 2009 +0800

    tracing/events: Add trace_define_common_fields()
    
    Extract duplicate code. Also prepare for the later patch.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A8BAFB8.1010304@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index af8fb8ebef0b..9c7ecfb3416f 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -62,6 +62,28 @@ int trace_define_field(struct ftrace_event_call *call, char *type,
 }
 EXPORT_SYMBOL_GPL(trace_define_field);
 
+#define __common_field(type, item)					\
+	ret = trace_define_field(call, #type, "common_" #item,		\
+				 offsetof(typeof(ent), item),		\
+				 sizeof(ent.item),			\
+				 is_signed_type(type));			\
+	if (ret)							\
+		return ret;
+
+int trace_define_common_fields(struct ftrace_event_call *call)
+{
+	int ret;
+	struct trace_entry ent;
+
+	__common_field(unsigned short, type);
+	__common_field(unsigned char, flags);
+	__common_field(unsigned char, preempt_count);
+	__common_field(int, pid);
+	__common_field(int, tgid);
+
+	return ret;
+}
+
 #ifdef CONFIG_MODULES
 
 static void trace_destroy_fields(struct ftrace_event_call *call)

commit 14be96c9716cb8c46dca94bd890defd7856e0734
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Aug 19 15:53:52 2009 +0800

    tracing/events: Add ftrace_event_call param to define_fields()
    
    This parameter is needed by syscall events to add define_fields()
    handler.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Jason Baron <jbaron@redhat.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A8BAF90.6060801@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b568ade8f453..af8fb8ebef0b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -941,7 +941,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 					  id);
 
 	if (call->define_fields) {
-		ret = call->define_fields();
+		ret = call->define_fields(call);
 		if (ret < 0) {
 			pr_warning("Could not initialize trace point"
 				   " events/%s\n", call->name);

commit 384be2b18a5f9475eab9ca2bdfa95cc1a04ef59c
Merge: a76761b621bc 142d44b0dd67
Author: Tejun Heo <tj@kernel.org>
Date:   Fri Aug 14 14:41:02 2009 +0900

    Merge branch 'percpu-for-linus' into percpu-for-next
    
    Conflicts:
            arch/sparc/kernel/smp_64.c
            arch/x86/kernel/cpu/perf_counter.c
            arch/x86/kernel/setup_percpu.c
            drivers/cpufreq/cpufreq_ondemand.c
            mm/percpu.c
    
    Conflicts in core and arch percpu codes are mostly from commit
    ed78e1e078dd44249f88b1dd8c76dafb39567161 which substituted many
    num_possible_cpus() with nr_cpu_ids.  As for-next branch has moved all
    the first chunk allocators into mm/percpu.c, the changes are moved
    from arch code to mm/percpu.c.
    
    Signed-off-by: Tejun Heo <tj@kernel.org>

commit e8f9f4d79a677f55c8ec3acbe87b33a87e2df0de
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Tue Aug 11 17:42:52 2009 +0200

    tracing: Add ftrace event call parameter to its field descriptor handler
    
    Add the struct ftrace_event_call as a parameter of its show_format()
    callback. This way we can use it from the syscall trace events to
    retrieve the syscall name from the ftrace event call parameter and
    describe its fields using the syscalls metadata.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Cc: Jason Baron <jbaron@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1d289e2d6693..b568ade8f453 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -576,7 +576,7 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 	trace_seq_printf(s, "format:\n");
 	trace_write_header(s);
 
-	r = call->show_format(s);
+	r = call->show_format(call, s);
 	if (!r) {
 		/*
 		 * ug!  The format output is bigger than a PAGE!!

commit 69fd4f0eb2ececbf8ade55e31a933e174965745e
Author: Jason Baron <jbaron@redhat.com>
Date:   Mon Aug 10 16:52:44 2009 -0400

    tracing: Add ftrace_event_call void * 'data' field
    
    add an optional void * pointer to 'ftrace_event_call' that is
    passed in for regfunc and unregfunc.
    
    This prepares for syscall tracepoints creation by passing the name of
    the syscall we want to trace and then retrieve its number through our
    arch syscall table.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f95f8470dd38..1d289e2d6693 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -86,14 +86,14 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 		if (call->enabled) {
 			call->enabled = 0;
 			tracing_stop_cmdline_record();
-			call->unregfunc();
+			call->unregfunc(call->data);
 		}
 		break;
 	case 1:
 		if (!call->enabled) {
 			call->enabled = 1;
 			tracing_start_cmdline_record();
-			call->regfunc();
+			call->regfunc(call->data);
 		}
 		break;
 	}

commit f744bd576a827c5b02e756b81fc2578edf8179b8
Author: Jason Baron <jbaron@redhat.com>
Date:   Mon Aug 10 16:52:39 2009 -0400

    tracing: Raw_init() bailout in trace event register fail case
    
    Allow the return value of raw_init() trace event callback to bail us out
    of creating a trace event file, in case we fail to register our
    event.
    
    Also, we plan to return -ENOSYS for syscall events that don't match any
    syscalls listed in our arch tracing syscall table, we don't want to warn
    in that case, we just want this event to be invisible in debugfs and
    ignored.
    
    Signed-off-by: Jason Baron <jbaron@redhat.com>
    Cc: Lai Jiangshan <laijs@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Mathieu Desnoyers <mathieu.desnoyers@polymtl.ca>
    Cc: Jiaying Zhang <jiayingz@google.com>
    Cc: Martin Bligh <mbligh@google.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Masami Hiramatsu <mhiramat@redhat.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index e0cbede96783..f95f8470dd38 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -925,15 +925,6 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 	if (strcmp(call->system, TRACE_SYSTEM) != 0)
 		d_events = event_subsystem_dir(call->system, d_events);
 
-	if (call->raw_init) {
-		ret = call->raw_init();
-		if (ret < 0) {
-			pr_warning("Could not initialize trace point"
-				   " events/%s\n", call->name);
-			return ret;
-		}
-	}
-
 	call->dir = debugfs_create_dir(call->name, d_events);
 	if (!call->dir) {
 		pr_warning("Could not create debugfs "
@@ -1058,6 +1049,7 @@ static void trace_module_add_events(struct module *mod)
 	struct ftrace_module_file_ops *file_ops = NULL;
 	struct ftrace_event_call *call, *start, *end;
 	struct dentry *d_events;
+	int ret;
 
 	start = mod->trace_events;
 	end = mod->trace_events + mod->num_trace_events;
@@ -1073,7 +1065,15 @@ static void trace_module_add_events(struct module *mod)
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;
-
+		if (call->raw_init) {
+			ret = call->raw_init();
+			if (ret < 0) {
+				if (ret != -ENOSYS)
+					pr_warning("Could not initialize trace "
+					"point events/%s\n", call->name);
+				continue;
+			}
+		}
 		/*
 		 * This module has events, create file ops for this module
 		 * if not already done.
@@ -1225,6 +1225,15 @@ static __init int event_trace_init(void)
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;
+		if (call->raw_init) {
+			ret = call->raw_init();
+			if (ret < 0) {
+				if (ret != -ENOSYS)
+					pr_warning("Could not initialize trace "
+					"point events/%s\n", call->name);
+				continue;
+			}
+		}
 		list_add(&call->list, &ftrace_events);
 		event_create_dir(call, d_events, &ftrace_event_id_fops,
 				 &ftrace_enable_fops, &ftrace_event_filter_fops,

commit 89034bc2c7b839702c00a704e79d112737f98be0
Merge: fb82ad719831 85dfd81dc57e
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Aug 11 14:19:09 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Conflicts:
            kernel/trace/trace_events_filter.c
    
    We use the tracing/core version.
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit a2ca5e03b6a5a1d401062f0a7f78888cf9e5e3b0
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Thu Aug 6 07:32:21 2009 +0200

    tracing/events: Only define remove_subsystem_dir() if CONFIG_MODULES
    
    If we disable modules, we get the following warning in ftrace events
    file:
    
    kernel/trace/trace_events.c:912: attention : remove_subsystem_dir defined but not used
    
    remove_subystem_dir() is useless if !CONFIG_MODULES, then move it to
    the appropriate #ifdef section of trace_events.c
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 90cf9360e140..70ecb7653b46 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -908,32 +908,6 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 	return system->entry;
 }
 
-static void remove_subsystem_dir(const char *name)
-{
-	struct event_subsystem *system;
-
-	if (strcmp(name, TRACE_SYSTEM) == 0)
-		return;
-
-	list_for_each_entry(system, &event_subsystems, list) {
-		if (strcmp(system->name, name) == 0) {
-			if (!--system->nr_events) {
-				struct event_filter *filter = system->filter;
-
-				debugfs_remove_recursive(system->entry);
-				list_del(&system->list);
-				if (filter) {
-					kfree(filter->filter_string);
-					kfree(filter);
-				}
-				kfree(system->name);
-				kfree(system);
-			}
-			break;
-		}
-	}
-}
-
 static int
 event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 const struct file_operations *id,
@@ -1018,6 +992,32 @@ struct ftrace_module_file_ops {
 	struct file_operations		filter;
 };
 
+static void remove_subsystem_dir(const char *name)
+{
+	struct event_subsystem *system;
+
+	if (strcmp(name, TRACE_SYSTEM) == 0)
+		return;
+
+	list_for_each_entry(system, &event_subsystems, list) {
+		if (strcmp(system->name, name) == 0) {
+			if (!--system->nr_events) {
+				struct event_filter *filter = system->filter;
+
+				debugfs_remove_recursive(system->entry);
+				list_del(&system->list);
+				if (filter) {
+					kfree(filter->filter_string);
+					kfree(filter);
+				}
+				kfree(system->name);
+				kfree(system);
+			}
+			break;
+		}
+	}
+}
+
 static struct ftrace_module_file_ops *
 trace_create_file_ops(struct module *mod)
 {

commit af6af30c0fcd77e621638e53ef8b176bca8bd3b4
Author: Peter Zijlstra <peterz@infradead.org>
Date:   Wed Aug 5 20:41:04 2009 +0200

    ftrace: Fix perf-tracepoint OOPS
    
    Not all tracepoints are created equal, in specific the ftrace
    tracepoints are created with TRACE_EVENT_FORMAT() which does
    not generate the needed bits to tie them into perf counters.
    
    For those events, don't create the 'id' file and fail
    ->profile_enable when their ID is specified through other
    means.
    
    Reported-by: Chris Mason <chris.mason@oracle.com>
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <1249497664.5890.4.camel@laptop>
    [ v2: fix build error in the !CONFIG_EVENT_PROFILE case ]
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 23d2972b22d6..e75276a49cf5 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -940,7 +940,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		entry = trace_create_file("enable", 0644, call->dir, call,
 					  enable);
 
-	if (call->id)
+	if (call->id && call->profile_enable)
 		entry = trace_create_file("id", 0444, call->dir, call,
 					  id);
 

commit 8650ae32ef7045e763825dee6256dde7f331bb85
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Jul 22 23:29:30 2009 -0400

    tracing: only truncate ftrace files when O_TRUNC is set
    
    The current code will truncate the ftrace files contents if O_APPEND
    is not set and the file is opened in write mode. This is incorrect.
    It should only truncate the file if O_TRUNC is set. Otherwise
    if one of these files is opened by a C program with fopen "r+",
    it will incorrectly truncate the file.
    
    Reported-by: Jiri Olsa <jolsa@redhat.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 53c8fd376a88..23d2972b22d6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -376,7 +376,7 @@ ftrace_event_seq_open(struct inode *inode, struct file *file)
 	const struct seq_operations *seq_ops;
 
 	if ((file->f_mode & FMODE_WRITE) &&
-	    !(file->f_flags & O_APPEND))
+	    (file->f_flags & O_TRUNC))
 		ftrace_clear_events();
 
 	seq_ops = inode->i_private;

commit dc82ec98a4727fd51b77e92d05fe7d2db3dcc11c
Author: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
Date:   Thu Jul 9 16:22:22 2009 +0800

    tracing/filter: Remove empty subsystem and its directory
    
    Remove empty subsystem and its directory when module unload.
    
    Before patch:
     # rmmod trace-events-sample.ko
     # ls sample
     enable  filter
    
    After patch:
     # rmmod trace-events-sample.ko
     # ls sample
     ls: cannot access sample: No such file or directory
    
    Signed-off-by: Xiao Guangrong <xiaoguangrong@cn.fujitsu.com>
    Acked-by: Tom Zanussi <tzanussi@gmail.com>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <4A55A8BE.9010707@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fecac1314cbe..90cf9360e140 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -851,8 +851,10 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 
 	/* First see if we did not already create this dir */
 	list_for_each_entry(system, &event_subsystems, list) {
-		if (strcmp(system->name, name) == 0)
+		if (strcmp(system->name, name) == 0) {
+			system->nr_events++;
 			return system->entry;
+		}
 	}
 
 	/* need to create new entry */
@@ -871,6 +873,7 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 		return d_events;
 	}
 
+	system->nr_events = 1;
 	system->name = kstrdup(name, GFP_KERNEL);
 	if (!system->name) {
 		debugfs_remove(system->entry);
@@ -905,6 +908,32 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 	return system->entry;
 }
 
+static void remove_subsystem_dir(const char *name)
+{
+	struct event_subsystem *system;
+
+	if (strcmp(name, TRACE_SYSTEM) == 0)
+		return;
+
+	list_for_each_entry(system, &event_subsystems, list) {
+		if (strcmp(system->name, name) == 0) {
+			if (!--system->nr_events) {
+				struct event_filter *filter = system->filter;
+
+				debugfs_remove_recursive(system->entry);
+				list_del(&system->list);
+				if (filter) {
+					kfree(filter->filter_string);
+					kfree(filter);
+				}
+				kfree(system->name);
+				kfree(system);
+			}
+			break;
+		}
+	}
+}
+
 static int
 event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
 		 const struct file_operations *id,
@@ -1079,6 +1108,7 @@ static void trace_module_remove_events(struct module *mod)
 			list_del(&call->list);
 			trace_destroy_fields(call);
 			destroy_preds(call);
+			remove_subsystem_dir(call->system);
 		}
 	}
 

commit c43768cbb7655ea5ff782ae250f6e2ef4297cf98
Merge: 1a8dd307cc0a 746a99a5af60
Author: Tejun Heo <tj@kernel.org>
Date:   Sat Jul 4 07:13:18 2009 +0900

    Merge branch 'master' into for-next
    
    Pull linus#master to merge PER_CPU_DEF_ATTRIBUTES and alpha build fix
    changes.  As alpha in percpu tree uses 'weak' attribute instead of
    inline assembly, there's no need for __used attribute.
    
    Conflicts:
            arch/alpha/include/asm/percpu.h
            arch/mn10300/kernel/vmlinux.lds.S
            include/linux/percpu-defs.h

commit 020e5f85cb087a40572c8b8b2dd06292a14fa212
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Jul 1 10:47:05 2009 +0800

    tracing/events: Add trace_event boot option
    
    We already have ftrace= boot option, and this adds a similar
    boot option for trace events, so allow trace events to be
    enabled at boot, for boot debugging purpose.
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A4ACE29.3010407@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 53c8fd376a88..fecac1314cbe 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -17,6 +17,8 @@
 #include <linux/ctype.h>
 #include <linux/delay.h>
 
+#include <asm/setup.h>
+
 #include "trace_output.h"
 
 #define TRACE_SYSTEM "TRACE_SYSTEM"
@@ -1133,6 +1135,18 @@ struct notifier_block trace_module_nb = {
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];
 
+static char bootup_event_buf[COMMAND_LINE_SIZE] __initdata;
+
+static __init int setup_trace_event(char *str)
+{
+	strlcpy(bootup_event_buf, str, COMMAND_LINE_SIZE);
+	ring_buffer_expanded = 1;
+	tracing_selftest_disabled = 1;
+
+	return 1;
+}
+__setup("trace_event=", setup_trace_event);
+
 static __init int event_trace_init(void)
 {
 	struct ftrace_event_call *call;
@@ -1140,6 +1154,8 @@ static __init int event_trace_init(void)
 	struct dentry *entry;
 	struct dentry *d_events;
 	int ret;
+	char *buf = bootup_event_buf;
+	char *token;
 
 	d_tracer = tracing_init_dentry();
 	if (!d_tracer)
@@ -1185,6 +1201,19 @@ static __init int event_trace_init(void)
 				 &ftrace_event_format_fops);
 	}
 
+	while (true) {
+		token = strsep(&buf, ",");
+
+		if (!token)
+			break;
+		if (!*token)
+			continue;
+
+		ret = ftrace_set_clr_event(token, 1);
+		if (ret)
+			pr_warning("Failed to enable trace event: %s\n", token);
+	}
+
 	ret = register_module_notifier(&trace_module_nb);
 	if (ret)
 		pr_warning("Failed to register trace events module notifier\n");
@@ -1392,10 +1421,10 @@ static __init void event_trace_self_test_with_function(void)
 
 static __init int event_trace_self_tests_init(void)
 {
-
-	event_trace_self_tests();
-
-	event_trace_self_test_with_function();
+	if (!tracing_selftest_disabled) {
+		event_trace_self_tests();
+		event_trace_self_test_with_function();
+	}
 
 	return 0;
 }

commit e1c7e2a6e67fe9db19dd15e71614526a31b5fdb1
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Jun 24 09:52:29 2009 +0800

    tracing/events: Don't increment @pos in s_start()
    
    While testing syscall tracepoints posted by Jason, I found 3 entries
    were missing when reading available_events. The output size of
    available_events is < 4 pages, which means we lost 1 entry per page.
    
    The cause is, it's wrong to increment @pos in s_start().
    
    Actually there's another bug here -- reading avaiable_events/set_events
    can race with module unload:
    
      # cat available_events               |
          s_start()                        |
          s_stop()                         |
                                           | # rmmod foo.ko
          s_start()                        |
            call = list_entry(m->private)  |
    
    @call might be freed and accessing it will lead to crash.
    
    Reviewed-by: Liming Wang <liming.wang@windriver.com>
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A4186DD.6090405@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index aa08be69a1b6..53c8fd376a88 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -300,10 +300,18 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
+	struct ftrace_event_call *call = NULL;
+	loff_t l;
+
 	mutex_lock(&event_mutex);
-	if (*pos == 0)
-		m->private = ftrace_events.next;
-	return t_next(m, NULL, pos);
+
+	m->private = ftrace_events.next;
+	for (l = 0; l <= *pos; ) {
+		call = t_next(m, NULL, &l);
+		if (!call)
+			break;
+	}
+	return call;
 }
 
 static void *
@@ -332,10 +340,18 @@ s_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *s_start(struct seq_file *m, loff_t *pos)
 {
+	struct ftrace_event_call *call = NULL;
+	loff_t l;
+
 	mutex_lock(&event_mutex);
-	if (*pos == 0)
-		m->private = ftrace_events.next;
-	return s_next(m, NULL, pos);
+
+	m->private = ftrace_events.next;
+	for (l = 0; l <= *pos; ) {
+		call = s_next(m, NULL, &l);
+		if (!call)
+			break;
+	}
+	return call;
 }
 
 static int t_show(struct seq_file *m, void *v)

commit 245b2e70eabd797932adb263a65da0bab3711753
Author: Tejun Heo <tj@kernel.org>
Date:   Wed Jun 24 15:13:48 2009 +0900

    percpu: clean up percpu variable definitions
    
    Percpu variable definition is about to be updated such that all percpu
    symbols including the static ones must be unique.  Update percpu
    variable definitions accordingly.
    
    * as,cfq: rename ioc_count uniquely
    
    * cpufreq: rename cpu_dbs_info uniquely
    
    * xen: move nesting_count out of xen_evtchn_do_upcall() and rename it
    
    * mm: move ratelimits out of balance_dirty_pages_ratelimited_nr() and
      rename it
    
    * ipv4,6: rename cookie_scratch uniquely
    
    * x86 perf_counter: rename prev_left to pmc_prev_left, irq_entry to
      pmc_irq_entry and nmi_entry to pmc_nmi_entry
    
    * perf_counter: rename disable_count to perf_disable_count
    
    * ftrace: rename test_event_disable to ftrace_test_event_disable
    
    * kmemleak: rename test_pointer to kmemleak_test_pointer
    
    * mce: rename next_interval to mce_next_interval
    
    [ Impact: percpu usage cleanups, no duplicate static percpu var names ]
    
    Signed-off-by: Tejun Heo <tj@kernel.org>
    Reviewed-by: Christoph Lameter <cl@linux-foundation.org>
    Cc: Ivan Kokshaysky <ink@jurassic.park.msu.ru>
    Cc: Jens Axboe <jens.axboe@oracle.com>
    Cc: Dave Jones <davej@redhat.com>
    Cc: Jeremy Fitzhardinge <jeremy@xensource.com>
    Cc: linux-mm <linux-mm@kvack.org>
    Cc: David S. Miller <davem@davemloft.net>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Steven Rostedt <srostedt@redhat.com>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Catalin Marinas <catalin.marinas@arm.com>
    Cc: Andi Kleen <andi@firstfloor.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index aa08be69a1b6..54b1de5074b6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1318,7 +1318,7 @@ static __init void event_trace_self_tests(void)
 
 #ifdef CONFIG_FUNCTION_TRACER
 
-static DEFINE_PER_CPU(atomic_t, test_event_disable);
+static DEFINE_PER_CPU(atomic_t, ftrace_test_event_disable);
 
 static void
 function_test_events_call(unsigned long ip, unsigned long parent_ip)
@@ -1334,7 +1334,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip)
 	pc = preempt_count();
 	resched = ftrace_preempt_disable();
 	cpu = raw_smp_processor_id();
-	disabled = atomic_inc_return(&per_cpu(test_event_disable, cpu));
+	disabled = atomic_inc_return(&per_cpu(ftrace_test_event_disable, cpu));
 
 	if (disabled != 1)
 		goto out;
@@ -1352,7 +1352,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip)
 	trace_nowake_buffer_unlock_commit(event, flags, pc);
 
  out:
-	atomic_dec(&per_cpu(test_event_disable, cpu));
+	atomic_dec(&per_cpu(ftrace_test_event_disable, cpu));
 	ftrace_preempt_enable(resched);
 }
 

commit 110bf2b764eb6026b868d84499263cb24b1bcc8d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 9 17:29:07 2009 -0400

    tracing: add protection around module events unload
    
    When reading the trace buffer, there is a race that when a module
    is unloaded it removes events that is stilled referenced in the buffers.
    This patch adds the protection around the unloading of the events
    from modules and the reading of the trace buffers.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 6c81f9c21426..aa08be69a1b6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1050,12 +1050,13 @@ static void trace_module_remove_events(struct module *mod)
 	struct ftrace_event_call *call, *p;
 	bool found = false;
 
+	down_write(&trace_event_mutex);
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
 		if (call->mod == mod) {
 			found = true;
 			ftrace_event_enable_disable(call, 0);
 			if (call->event)
-				unregister_ftrace_event(call->event);
+				__unregister_ftrace_event(call->event);
 			debugfs_remove_recursive(call->dir);
 			list_del(&call->list);
 			trace_destroy_fields(call);
@@ -1079,6 +1080,7 @@ static void trace_module_remove_events(struct module *mod)
 	 */
 	if (found)
 		tracing_reset_current_online_cpus();
+	up_write(&trace_event_mutex);
 }
 
 static int trace_module_notify(struct notifier_block *self,

commit 0e907c99391362385c8e3af2c43b904dd1fd5d73
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Mon May 25 18:13:59 2009 +0800

    ftrace: clean up of using ftrace_event_enable_disable()
    
    Always use ftrace_event_enable_disable() to enable/disable an event
    so that we can factorize out the event toggling code.
    
    [ Impact: factorize and cleanup event tracing code ]
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <4A14FDFE.2080402@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9b246eb01d5f..6c81f9c21426 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -76,26 +76,9 @@ static void trace_destroy_fields(struct ftrace_event_call *call)
 
 #endif /* CONFIG_MODULES */
 
-static void ftrace_clear_events(void)
-{
-	struct ftrace_event_call *call;
-
-	mutex_lock(&event_mutex);
-	list_for_each_entry(call, &ftrace_events, list) {
-
-		if (call->enabled) {
-			call->enabled = 0;
-			tracing_stop_cmdline_record();
-			call->unregfunc();
-		}
-	}
-	mutex_unlock(&event_mutex);
-}
-
 static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 					int enable)
 {
-
 	switch (enable) {
 	case 0:
 		if (call->enabled) {
@@ -114,6 +97,17 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 	}
 }
 
+static void ftrace_clear_events(void)
+{
+	struct ftrace_event_call *call;
+
+	mutex_lock(&event_mutex);
+	list_for_each_entry(call, &ftrace_events, list) {
+		ftrace_event_enable_disable(call, 0);
+	}
+	mutex_unlock(&event_mutex);
+}
+
 /*
  * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.
  */
@@ -1059,11 +1053,7 @@ static void trace_module_remove_events(struct module *mod)
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
 		if (call->mod == mod) {
 			found = true;
-			if (call->enabled) {
-				call->enabled = 0;
-				tracing_stop_cmdline_record();
-				call->unregfunc();
-			}
+			ftrace_event_enable_disable(call, 0);
 			if (call->event)
 				unregister_ftrace_event(call->event);
 			debugfs_remove_recursive(call->dir);
@@ -1265,15 +1255,9 @@ static __init void event_trace_self_tests(void)
 			continue;
 		}
 
-		call->enabled = 1;
-		tracing_start_cmdline_record();
-		call->regfunc();
-
+		ftrace_event_enable_disable(call, 1);
 		event_test_stuff();
-
-		call->unregfunc();
-		tracing_stop_cmdline_record();
-		call->enabled = 0;
+		ftrace_event_enable_disable(call, 0);
 
 		pr_cont("OK\n");
 	}

commit b11c53e12f94a46b50bccc7a1a953d7ca1d54a31
Author: Zhaolei <zhaolei@cn.fujitsu.com>
Date:   Mon May 25 18:11:59 2009 +0800

    ftrace: Add task_comm support for trace_event
    
    If we enable a trace event alone without any tracer running (such as
    function tracer, sched switch tracer, etc...) it can't output enough
    task command information.
    
    We need to use the tracing_{start/stop}_cmdline_record() helpers
    which are designed to keep track of cmdlines for any tasks that
    were scheduled during the tracing.
    
    Before this patch:
     # echo 1 > debugfs/tracing/events/sched/sched_switch/enable
     # cat debugfs/tracing/trace
     # tracer: nop
     #
     #           TASK-PID    CPU#    TIMESTAMP  FUNCTION
     #              | |       |          |         |
                <...>-2289  [000] 526276.724790: sched_switch: task bash:2289 [120] ==> sshd:2287 [120]
                <...>-2287  [000] 526276.725231: sched_switch: task sshd:2287 [120] ==> bash:2289 [120]
                <...>-2289  [000] 526276.725452: sched_switch: task bash:2289 [120] ==> sshd:2287 [120]
                <...>-2287  [000] 526276.727181: sched_switch: task sshd:2287 [120] ==> swapper:0 [140]
               <idle>-0     [000] 526277.032734: sched_switch: task swapper:0 [140] ==> events/0:5 [115]
                <...>-5     [000] 526277.032782: sched_switch: task events/0:5 [115] ==> swapper:0 [140]
     ...
    
    After this patch:
     # tracer: nop
     #
     #           TASK-PID    CPU#    TIMESTAMP  FUNCTION
     #              | |       |          |         |
                 bash-2269  [000] 527347.989229: sched_switch: task bash:2269 [120] ==> sshd:2267 [120]
                 sshd-2267  [000] 527347.990960: sched_switch: task sshd:2267 [120] ==> bash:2269 [120]
                 bash-2269  [000] 527347.991143: sched_switch: task bash:2269 [120] ==> sshd:2267 [120]
                 sshd-2267  [000] 527347.992959: sched_switch: task sshd:2267 [120] ==> swapper:0 [140]
               <idle>-0     [000] 527348.531989: sched_switch: task swapper:0 [140] ==> events/0:5 [115]
             events/0-5     [000] 527348.532115: sched_switch: task events/0:5 [115] ==> swapper:0 [140]
     ...
    
    Changelog:
    v1->v2: Update Kconfig to select CONTEXT_SWITCH_TRACER in
            ENABLE_EVENT_TRACING
    v2->v3: v2 can solve problem that was caused by config EVENT_TRACING
            alone, but when CONFIG_FTRACE is off and CONFIG_TRACING is
            selected by other config, compile fail happened again.
            This version solves it.
    
    [ Impact: fix incomplete output of event tracing ]
    
    Signed-off-by: Zhao Lei <zhaolei@cn.fujitsu.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <4A14FDFE.2080402@cn.fujitsu.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9e91c4ad7c8b..9b246eb01d5f 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -85,6 +85,7 @@ static void ftrace_clear_events(void)
 
 		if (call->enabled) {
 			call->enabled = 0;
+			tracing_stop_cmdline_record();
 			call->unregfunc();
 		}
 	}
@@ -99,12 +100,14 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 	case 0:
 		if (call->enabled) {
 			call->enabled = 0;
+			tracing_stop_cmdline_record();
 			call->unregfunc();
 		}
 		break;
 	case 1:
 		if (!call->enabled) {
 			call->enabled = 1;
+			tracing_start_cmdline_record();
 			call->regfunc();
 		}
 		break;
@@ -1058,6 +1061,7 @@ static void trace_module_remove_events(struct module *mod)
 			found = true;
 			if (call->enabled) {
 				call->enabled = 0;
+				tracing_stop_cmdline_record();
 				call->unregfunc();
 			}
 			if (call->event)
@@ -1262,11 +1266,13 @@ static __init void event_trace_self_tests(void)
 		}
 
 		call->enabled = 1;
+		tracing_start_cmdline_record();
 		call->regfunc();
 
 		event_test_stuff();
 
 		call->unregfunc();
+		tracing_stop_cmdline_record();
 		call->enabled = 0;
 
 		pr_cont("OK\n");

commit 5537937696c55530447c20aa27daccb8d0d29b33
Author: Ming Lei <tom.leiming@gmail.com>
Date:   Mon May 18 23:04:46 2009 +0800

    ftrace: fix check for return value of register_module_notifier in event_trace_init
    
    register_module_notifier() returns zero in the success case.
    So fix the inverted fail case check in trace events modules
    handler.
    
    [ Impact: fix spurious warning on ftrace initialization]
    
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Ming Lei <tom.leiming@gmail.com>
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 0eec0c55dd87..9e91c4ad7c8b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1174,7 +1174,7 @@ static __init int event_trace_init(void)
 	}
 
 	ret = register_module_notifier(&trace_module_nb);
-	if (!ret)
+	if (ret)
 		pr_warning("Failed to register trace events module notifier\n");
 
 	return 0;

commit 4671c79408a3f8a5a6a45e39c4c164dada3a5678
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 8 16:27:41 2009 -0400

    tracing: add trace_set_clr_event to export event enabling function
    
    Other parts of the kernel may need to be able to enable or disable
    specific events. Especially parts that create trace events.
    
    [ Impact: allow enabling of trace events by those that create the event ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 2eecb87e42d3..0eec0c55dd87 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -177,6 +177,23 @@ static int ftrace_set_clr_event(char *buf, int set)
 	return __ftrace_set_clr_event(match, sub, event, set);
 }
 
+/**
+ * trace_set_clr_event - enable or disable an event
+ * @system: system name to match (NULL for any system)
+ * @event: event name to match (NULL for all events, within system)
+ * @set: 1 to enable, 0 to disable
+ *
+ * This is a way for other parts of the kernel to enable or disable
+ * event recording.
+ *
+ * Returns 0 on success, -EINVAL if the parameters do not match any
+ * registered events.
+ */
+int trace_set_clr_event(const char *system, const char *event, int set)
+{
+	return __ftrace_set_clr_event(NULL, system, event, set);
+}
+
 /* 128 should be much more than enough */
 #define EVENT_BUF_SIZE		127
 

commit 29f93943d1916d1a3faa3f10f4a06994347ac990
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 8 16:06:47 2009 -0400

    tracing: initialize return value for __ftrace_set_clr_event
    
    Commit 8f31bfe538ebafac187d2d4465a92e1d9ee6d8c2
    tracing/events: clean up for ftrace_set_clr_event()
    
    Moved out the code for ftrace_set_clr_event into a helper funciton but
    did not initialize the return value. As a result, we do not warn about
    a typo in the echoing of events in set_event.
    
    This patch restores the old warning:
    
     # echo foobar > set_event
    -bash: echo: write error: Invalid argument
    
    [ Impact: restore warning of invalid entries to set_event ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index df394bc6d54b..2eecb87e42d3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -118,7 +118,7 @@ static int __ftrace_set_clr_event(const char *match, const char *sub,
 				  const char *event, int set)
 {
 	struct ftrace_event_call *call;
-	int ret;
+	int ret = -EINVAL;
 
 	mutex_lock(&event_mutex);
 	list_for_each_entry(call, &ftrace_events, list) {

commit c142b15dc56ee6d55cb97a062e3c8e9c61e384c0
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri May 8 10:32:05 2009 +0800

    tracing/events: simplify system_enable_read()
    
    A smarter way to figure out the output of an enable file.
    
    [ Impact: clean up ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A0399A5.2080603@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 45f1099386b6..df394bc6d54b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -414,10 +414,11 @@ static ssize_t
 system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
+	const char set_to_char[4] = { '?', '0', '1', 'X' };
 	const char *system = filp->private_data;
 	struct ftrace_event_call *call;
 	char buf[2];
-	int set = -1;
+	int set = 0;
 	int ret;
 
 	mutex_lock(&event_mutex);
@@ -433,47 +434,18 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 		 * or if all events or cleared, or if we have
 		 * a mixture.
 		 */
-		if (call->enabled) {
-			switch (set) {
-			case -1:
-				set = 1;
-				break;
-			case 0:
-				set = 2;
-				break;
-			}
-		} else {
-			switch (set) {
-			case -1:
-				set = 0;
-				break;
-			case 1:
-				set = 2;
-				break;
-			}
-		}
+		set |= (1 << !!call->enabled);
+
 		/*
 		 * If we have a mixture, no need to look further.
 		 */
-		if (set == 2)
+		if (set == 3)
 			break;
 	}
 	mutex_unlock(&event_mutex);
 
+	buf[0] = set_to_char[set];
 	buf[1] = '\n';
-	switch (set) {
-	case 0:
-		buf[0] = '0';
-		break;
-	case 1:
-		buf[0] = '1';
-		break;
-	case 2:
-		buf[0] = 'X';
-		break;
-	default:
-		buf[0] = '?';
-	}
 
 	ret = simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);
 

commit 8f31bfe538ebafac187d2d4465a92e1d9ee6d8c2
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Fri May 8 10:31:42 2009 +0800

    tracing/events: clean up for ftrace_set_clr_event()
    
    Add a helper function __ftrace_set_clr_event(), and replace some
    ftrace_set_clr_event() calls with this helper, thus we don't need any
    kstrdup() or kmalloc().
    
    As a side effect, this patch fixes an issue in self tests code, which is
    similar to the one fixed in commit d6bf81ef0f7474434c2a049e8bf3c9146a14dd96
    ("tracing: append ":*" to internal setting of system events")
    
    It's a small issue and won't cause any bug in fact, but we should do things
    right anyway.
    
    [ Impact: prevent spurious event-enabling in tracing self-tests ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <4A03998E.3020503@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8d0fae3af595..45f1099386b6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -111,11 +111,44 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 	}
 }
 
-static int ftrace_set_clr_event(char *buf, int set)
+/*
+ * __ftrace_set_clr_event(NULL, NULL, NULL, set) will set/unset all events.
+ */
+static int __ftrace_set_clr_event(const char *match, const char *sub,
+				  const char *event, int set)
 {
 	struct ftrace_event_call *call;
+	int ret;
+
+	mutex_lock(&event_mutex);
+	list_for_each_entry(call, &ftrace_events, list) {
+
+		if (!call->name || !call->regfunc)
+			continue;
+
+		if (match &&
+		    strcmp(match, call->name) != 0 &&
+		    strcmp(match, call->system) != 0)
+			continue;
+
+		if (sub && strcmp(sub, call->system) != 0)
+			continue;
+
+		if (event && strcmp(event, call->name) != 0)
+			continue;
+
+		ftrace_event_enable_disable(call, set);
+
+		ret = 0;
+	}
+	mutex_unlock(&event_mutex);
+
+	return ret;
+}
+
+static int ftrace_set_clr_event(char *buf, int set)
+{
 	char *event = NULL, *sub = NULL, *match;
-	int ret = -EINVAL;
 
 	/*
 	 * The buf format can be <subsystem>:<event-name>
@@ -141,30 +174,7 @@ static int ftrace_set_clr_event(char *buf, int set)
 			event = NULL;
 	}
 
-	mutex_lock(&event_mutex);
-	list_for_each_entry(call, &ftrace_events, list) {
-
-		if (!call->name || !call->regfunc)
-			continue;
-
-		if (match &&
-		    strcmp(match, call->name) != 0 &&
-		    strcmp(match, call->system) != 0)
-			continue;
-
-		if (sub && strcmp(sub, call->system) != 0)
-			continue;
-
-		if (event && strcmp(event, call->name) != 0)
-			continue;
-
-		ftrace_event_enable_disable(call, set);
-
-		ret = 0;
-	}
-	mutex_unlock(&event_mutex);
-
-	return ret;
+	return __ftrace_set_clr_event(match, sub, event, set);
 }
 
 /* 128 should be much more than enough */
@@ -408,18 +418,14 @@ system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	struct ftrace_event_call *call;
 	char buf[2];
 	int set = -1;
-	int all = 0;
 	int ret;
 
-	if (system[0] == '*')
-		all = 1;
-
 	mutex_lock(&event_mutex);
 	list_for_each_entry(call, &ftrace_events, list) {
 		if (!call->name || !call->regfunc)
 			continue;
 
-		if (!all && strcmp(call->system, system) != 0)
+		if (system && strcmp(call->system, system) != 0)
 			continue;
 
 		/*
@@ -480,7 +486,6 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 {
 	const char *system = filp->private_data;
 	unsigned long val;
-	char *command;
 	char buf[64];
 	ssize_t ret;
 
@@ -500,30 +505,16 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	if (ret < 0)
 		return ret;
 
-	switch (val) {
-	case 0:
-	case 1:
-		break;
-
-	default:
+	if (val != 0 && val != 1)
 		return -EINVAL;
-	}
 
-	/* +3 for the ":*\0" */
-	command = kmalloc(strlen(system)+3, GFP_KERNEL);
-	if (!command)
-		return -ENOMEM;
-	sprintf(command, "%s:*", system);
-
-	ret = ftrace_set_clr_event(command, val);
+	ret = __ftrace_set_clr_event(NULL, system, NULL, val);
 	if (ret)
-		goto out_free;
+		goto out;
 
 	ret = cnt;
 
- out_free:
-	kfree(command);
-
+out:
 	*ppos += cnt;
 
 	return ret;
@@ -1181,7 +1172,7 @@ static __init int event_trace_init(void)
 			  &ftrace_show_header_fops);
 
 	trace_create_file("enable", 0644, d_events,
-			  "*", &ftrace_system_enable_fops);
+			  NULL, &ftrace_system_enable_fops);
 
 	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
 		/* The linker may leave blanks */
@@ -1259,7 +1250,6 @@ static __init void event_trace_self_tests(void)
 {
 	struct ftrace_event_call *call;
 	struct event_subsystem *system;
-	char *sysname;
 	int ret;
 
 	pr_info("Running tests on trace events:\n");
@@ -1305,14 +1295,7 @@ static __init void event_trace_self_tests(void)
 
 		pr_info("Testing event system %s: ", system->name);
 
-		/* ftrace_set_clr_event can modify the name passed in. */
-		sysname = kstrdup(system->name, GFP_KERNEL);
-		if (WARN_ON(!sysname)) {
-			pr_warning("Can't allocate memory, giving up!\n");
-			return;
-		}
-		ret = ftrace_set_clr_event(sysname, 1);
-		kfree(sysname);
+		ret = __ftrace_set_clr_event(NULL, system->name, NULL, 1);
 		if (WARN_ON_ONCE(ret)) {
 			pr_warning("error enabling system %s\n",
 				   system->name);
@@ -1321,14 +1304,7 @@ static __init void event_trace_self_tests(void)
 
 		event_test_stuff();
 
-		sysname = kstrdup(system->name, GFP_KERNEL);
-		if (WARN_ON(!sysname)) {
-			pr_warning("Can't allocate memory, giving up!\n");
-			return;
-		}
-		ret = ftrace_set_clr_event(sysname, 0);
-		kfree(sysname);
-
+		ret = __ftrace_set_clr_event(NULL, system->name, NULL, 0);
 		if (WARN_ON_ONCE(ret))
 			pr_warning("error disabling system %s\n",
 				   system->name);
@@ -1341,15 +1317,8 @@ static __init void event_trace_self_tests(void)
 	pr_info("Running tests on all trace events:\n");
 	pr_info("Testing all events: ");
 
-	sysname = kmalloc(4, GFP_KERNEL);
-	if (WARN_ON(!sysname)) {
-		pr_warning("Can't allocate memory, giving up!\n");
-		return;
-	}
-	memcpy(sysname, "*:*", 4);
-	ret = ftrace_set_clr_event(sysname, 1);
+	ret = __ftrace_set_clr_event(NULL, NULL, NULL, 1);
 	if (WARN_ON_ONCE(ret)) {
-		kfree(sysname);
 		pr_warning("error enabling all events\n");
 		return;
 	}
@@ -1357,10 +1326,7 @@ static __init void event_trace_self_tests(void)
 	event_test_stuff();
 
 	/* reset sysname */
-	memcpy(sysname, "*:*", 4);
-	ret = ftrace_set_clr_event(sysname, 0);
-	kfree(sysname);
-
+	ret = __ftrace_set_clr_event(NULL, NULL, NULL, 0);
 	if (WARN_ON_ONCE(ret)) {
 		pr_warning("error disabling all events\n");
 		return;

commit d6bf81ef0f7474434c2a049e8bf3c9146a14dd96
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 7 11:49:35 2009 -0400

    tracing: append ":*" to internal setting of system events
    
    The system enabling of events uses the same code as the set_event file.
    It passes in the name of the system to the parser and that will enable
    all the events that has that system as a name.
    
    The problem is that it will also enable events with the same name as the
    system.
    
    If you have system name foo, and system name bar, but within the system
    bar, there exists an event called foo. By setting the system name foo,
    you will also be enabling the event foo in the system bar. This is not
    an expected result.
    
    The solution is to pass in "foo:*", which will only enable the system
    foo and not events called foo.
    
    [ Impact: prevent accidental enabling of events with same name as a system ]
    
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 87feb0117ce2..8d0fae3af595 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -509,9 +509,11 @@ system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		return -EINVAL;
 	}
 
-	command = kstrdup(system, GFP_KERNEL);
+	/* +3 for the ":*\0" */
+	command = kmalloc(strlen(system)+3, GFP_KERNEL);
 	if (!command)
 		return -ENOMEM;
+	sprintf(command, "%s:*", system);
 
 	ret = ftrace_set_clr_event(command, val);
 	if (ret)
@@ -1179,7 +1181,7 @@ static __init int event_trace_init(void)
 			  &ftrace_show_header_fops);
 
 	trace_create_file("enable", 0644, d_events,
-			  "*:*", &ftrace_system_enable_fops);
+			  "*", &ftrace_system_enable_fops);
 
 	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
 		/* The linker may leave blanks */

commit 8ae79a138e88aceeeb07077bff2883245fb7c218
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 6 22:52:15 2009 -0400

    tracing: add hierarchical enabling of events
    
    With the current event directory, you can only enable individual events.
    The file debugfs/tracing/set_event is used to be able to enable or
    disable several events at once. But that can still be awkward.
    
    This patch adds hierarchical enabling of events. That is, each directory
    in debugfs/tracing/events has an "enable" file. This file can enable
    or disable all events within the directory and below.
    
     # echo 1 > /debugfs/tracing/events/enable
    
    will enable all events.
    
     # echo 1 > /debugfs/tracing/events/sched/enable
    
    will enable all events in the sched subsystem.
    
     # echo 1 > /debugfs/tracing/events/enable
     # echo 0 > /debugfs/tracing/events/irq/enable
    
    will enable all events, but then disable just the irq subsystem events.
    
    When reading one of these enable files, there are four results:
    
     0 - all events this file affects are disabled
     1 - all events this file affects are enabled
     X - there is a mixture of events enabled and disabled
     ? - this file does not affect any event
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 6d2c842a0248..87feb0117ce2 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -400,6 +400,133 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return cnt;
 }
 
+static ssize_t
+system_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
+		   loff_t *ppos)
+{
+	const char *system = filp->private_data;
+	struct ftrace_event_call *call;
+	char buf[2];
+	int set = -1;
+	int all = 0;
+	int ret;
+
+	if (system[0] == '*')
+		all = 1;
+
+	mutex_lock(&event_mutex);
+	list_for_each_entry(call, &ftrace_events, list) {
+		if (!call->name || !call->regfunc)
+			continue;
+
+		if (!all && strcmp(call->system, system) != 0)
+			continue;
+
+		/*
+		 * We need to find out if all the events are set
+		 * or if all events or cleared, or if we have
+		 * a mixture.
+		 */
+		if (call->enabled) {
+			switch (set) {
+			case -1:
+				set = 1;
+				break;
+			case 0:
+				set = 2;
+				break;
+			}
+		} else {
+			switch (set) {
+			case -1:
+				set = 0;
+				break;
+			case 1:
+				set = 2;
+				break;
+			}
+		}
+		/*
+		 * If we have a mixture, no need to look further.
+		 */
+		if (set == 2)
+			break;
+	}
+	mutex_unlock(&event_mutex);
+
+	buf[1] = '\n';
+	switch (set) {
+	case 0:
+		buf[0] = '0';
+		break;
+	case 1:
+		buf[0] = '1';
+		break;
+	case 2:
+		buf[0] = 'X';
+		break;
+	default:
+		buf[0] = '?';
+	}
+
+	ret = simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);
+
+	return ret;
+}
+
+static ssize_t
+system_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
+		    loff_t *ppos)
+{
+	const char *system = filp->private_data;
+	unsigned long val;
+	char *command;
+	char buf[64];
+	ssize_t ret;
+
+	if (cnt >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	buf[cnt] = 0;
+
+	ret = strict_strtoul(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	ret = tracing_update_buffers();
+	if (ret < 0)
+		return ret;
+
+	switch (val) {
+	case 0:
+	case 1:
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	command = kstrdup(system, GFP_KERNEL);
+	if (!command)
+		return -ENOMEM;
+
+	ret = ftrace_set_clr_event(command, val);
+	if (ret)
+		goto out_free;
+
+	ret = cnt;
+
+ out_free:
+	kfree(command);
+
+	*ppos += cnt;
+
+	return ret;
+}
+
 extern char *__bad_type_size(void);
 
 #undef FIELD
@@ -686,6 +813,12 @@ static const struct file_operations ftrace_subsystem_filter_fops = {
 	.write = subsystem_filter_write,
 };
 
+static const struct file_operations ftrace_system_enable_fops = {
+	.open = tracing_open_generic,
+	.read = system_enable_read,
+	.write = system_enable_write,
+};
+
 static const struct file_operations ftrace_show_header_fops = {
 	.open = tracing_open_generic,
 	.read = show_header,
@@ -768,6 +901,10 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 			   "'%s/filter' entry\n", name);
 	}
 
+	entry = trace_create_file("enable", 0644, system->entry,
+				  (void *)system->name,
+				  &ftrace_system_enable_fops);
+
 	return system->entry;
 }
 
@@ -1041,6 +1178,9 @@ static __init int event_trace_init(void)
 			  ring_buffer_print_entry_header,
 			  &ftrace_show_header_fops);
 
+	trace_create_file("enable", 0644, d_events,
+			  "*:*", &ftrace_system_enable_fops);
+
 	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
 		/* The linker may leave blanks */
 		if (!call->name)

commit 9456f0fa6d3cb944d3b9fc31c9a244e0362c26ea
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 6 21:54:09 2009 -0400

    tracing: reset ring buffer when removing modules with events
    
    Li Zefan found that there's a race using the event ids of events and
    modules. When a module is loaded, an event id is incremented. We only
    have 16 bits for event ids (65536) and there is a possible (but highly
    unlikely) race that we could load and unload a module that registers
    events so many times that the event id counter overflows.
    
    When it overflows, it then restarts and goes looking for available
    ids. An id is available if it was added by a module and released.
    
    The race is if you have one module add an id, and then is removed.
    Another module loaded can use that same event id. But if the old module
    still had events in the ring buffer, the new module's call back would
    get bogus data.  At best (and most likely) the output would just be
    garbage. But if the module for some reason used pointers (not recommended)
    then this could potentially crash.
    
    The safest thing to do is just reset the ring buffer if a module that
    registered events is removed.
    
    [ Impact: prevent unpredictable results of event id overflows ]
    
    Reported-by: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <49FEAFD0.30106@cn.fujitsu.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8d579ff23610..6d2c842a0248 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -932,9 +932,11 @@ static void trace_module_remove_events(struct module *mod)
 {
 	struct ftrace_module_file_ops *file_ops;
 	struct ftrace_event_call *call, *p;
+	bool found = false;
 
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
 		if (call->mod == mod) {
+			found = true;
 			if (call->enabled) {
 				call->enabled = 0;
 				call->unregfunc();
@@ -957,6 +959,13 @@ static void trace_module_remove_events(struct module *mod)
 		list_del(&file_ops->list);
 		kfree(file_ops);
 	}
+
+	/*
+	 * It is safest to reset the ring buffer if the module being unloaded
+	 * registered any events.
+	 */
+	if (found)
+		tracing_reset_current_online_cpus();
 }
 
 static int trace_module_notify(struct notifier_block *self,

commit 20c8928abe70e204bd077ab6cfe23002d7788983
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed May 6 10:33:45 2009 +0800

    tracing/events: fix concurrent access to ftrace_events list
    
    A module will add/remove its trace events when it gets loaded/unloaded, so
    the ftrace_events list is not "const", and concurrent access needs to be
    protected.
    
    This patch thus fixes races between loading/unloding modules and read
    'available_events' or read/write 'set_event', etc.
    
    Below shows how to reproduce the race:
    
     # for ((; ;)) { cat /mnt/tracing/available_events; } > /dev/null &
     # for ((; ;)) { insmod trace-events-sample.ko; rmmod sample; } &
    
    After a while:
    
    BUG: unable to handle kernel paging request at 0010011c
    IP: [<c1080f27>] t_next+0x1b/0x2d
    ...
    Call Trace:
     [<c10c90e6>] ? seq_read+0x217/0x30d
     [<c10c8ecf>] ? seq_read+0x0/0x30d
     [<c10b4c19>] ? vfs_read+0x8f/0x136
     [<c10b4fc3>] ? sys_read+0x40/0x65
     [<c1002a68>] ? sysenter_do_call+0x12/0x36
    
    [ Impact: fix races when concurrent accessing ftrace_events list ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    LKML-Reference: <4A00F709.3080800@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f251a150e75e..8d579ff23610 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -21,7 +21,7 @@
 
 #define TRACE_SYSTEM "TRACE_SYSTEM"
 
-static DEFINE_MUTEX(event_mutex);
+DEFINE_MUTEX(event_mutex);
 
 LIST_HEAD(ftrace_events);
 
@@ -80,6 +80,7 @@ static void ftrace_clear_events(void)
 {
 	struct ftrace_event_call *call;
 
+	mutex_lock(&event_mutex);
 	list_for_each_entry(call, &ftrace_events, list) {
 
 		if (call->enabled) {
@@ -87,6 +88,7 @@ static void ftrace_clear_events(void)
 			call->unregfunc();
 		}
 	}
+	mutex_unlock(&event_mutex);
 }
 
 static void ftrace_event_enable_disable(struct ftrace_event_call *call,
@@ -274,6 +276,9 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *t_start(struct seq_file *m, loff_t *pos)
 {
+	mutex_lock(&event_mutex);
+	if (*pos == 0)
+		m->private = ftrace_events.next;
 	return t_next(m, NULL, pos);
 }
 
@@ -303,6 +308,9 @@ s_next(struct seq_file *m, void *v, loff_t *pos)
 
 static void *s_start(struct seq_file *m, loff_t *pos)
 {
+	mutex_lock(&event_mutex);
+	if (*pos == 0)
+		m->private = ftrace_events.next;
 	return s_next(m, NULL, pos);
 }
 
@@ -319,12 +327,12 @@ static int t_show(struct seq_file *m, void *v)
 
 static void t_stop(struct seq_file *m, void *p)
 {
+	mutex_unlock(&event_mutex);
 }
 
 static int
 ftrace_event_seq_open(struct inode *inode, struct file *file)
 {
-	int ret;
 	const struct seq_operations *seq_ops;
 
 	if ((file->f_mode & FMODE_WRITE) &&
@@ -332,13 +340,7 @@ ftrace_event_seq_open(struct inode *inode, struct file *file)
 		ftrace_clear_events();
 
 	seq_ops = inode->i_private;
-	ret = seq_open(file, seq_ops);
-	if (!ret) {
-		struct seq_file *m = file->private_data;
-
-		m->private = ftrace_events.next;
-	}
-	return ret;
+	return seq_open(file, seq_ops);
 }
 
 static ssize_t

commit 2df75e415709ad12862028916c772c1f377f6a7c
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed May 6 10:33:04 2009 +0800

    tracing/events: fix memory leak when unloading module
    
    When unloading a module, memory allocated by init_preds() and
    trace_define_field() is not freed.
    
    [ Impact: fix memory leak ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <4A00F6E0.3040503@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f789ca540fe1..f251a150e75e 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -60,6 +60,22 @@ int trace_define_field(struct ftrace_event_call *call, char *type,
 }
 EXPORT_SYMBOL_GPL(trace_define_field);
 
+#ifdef CONFIG_MODULES
+
+static void trace_destroy_fields(struct ftrace_event_call *call)
+{
+	struct ftrace_event_field *field, *next;
+
+	list_for_each_entry_safe(field, next, &call->fields, link) {
+		list_del(&field->link);
+		kfree(field->type);
+		kfree(field->name);
+		kfree(field);
+	}
+}
+
+#endif /* CONFIG_MODULES */
+
 static void ftrace_clear_events(void)
 {
 	struct ftrace_event_call *call;
@@ -925,6 +941,8 @@ static void trace_module_remove_events(struct module *mod)
 				unregister_ftrace_event(call->event);
 			debugfs_remove_recursive(call->dir);
 			list_del(&call->list);
+			trace_destroy_fields(call);
+			destroy_preds(call);
 		}
 	}
 

commit 8b3725621074040d380664964ffbc40610aef8c6
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Apr 28 03:04:59 2009 -0500

    tracing/filters: a better event parser
    
    Replace the current event parser hack with a better one.  Filters are
    no longer specified predicate by predicate, but all at once and can
    use parens and any of the following operators:
    
    numeric fields:
    
    ==, !=, <, <=, >, >=
    
    string fields:
    
    ==, !=
    
    predicates can be combined with the logical operators:
    
    &&, ||
    
    examples:
    
    "common_preempt_count > 4" > filter
    
    "((sig >= 10 && sig < 15) || sig == 17) && comm != bash" > filter
    
    If there was an error, the erroneous string along with an error
    message can be seen by looking at the filter e.g.:
    
    ((sig >= 10 && sig < 15) || dsig == 17) && comm != bash
    ^
    parse_error: Field not found
    
    Currently the caret for an error always appears at the beginning of
    the filter; a real position should be used, but the error message
    should be useful even without it.
    
    To clear a filter, '0' can be written to the filter file.
    
    Filters can also be set or cleared for a complete subsystem by writing
    the same filter as would be written to an individual event to the
    filter file at the root of the subsytem.  Note however, that if any
    event in the subsystem lacks a field specified in the filter being
    set, the set will fail and all filters in the subsytem are
    automatically cleared.  This change from the previous version was made
    because using only the fields that happen to exist for a given event
    would most likely result in a meaningless filter.
    
    Because the logical operators are now implemented as predicates, the
    maximum number of predicates in a filter was increased from 8 to 16.
    
    [ Impact: add new, extended trace-filter implementation ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <1240905899.6416.121.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index bbbea7479371..f789ca540fe1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -492,7 +492,7 @@ event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	filter_print_preds(call, s);
+	print_event_filter(call, s);
 	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
@@ -505,40 +505,26 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		   loff_t *ppos)
 {
 	struct ftrace_event_call *call = filp->private_data;
-	char buf[64], *pbuf = buf;
-	struct filter_pred *pred;
+	char *buf;
 	int err;
 
-	if (cnt >= sizeof(buf))
+	if (cnt >= PAGE_SIZE)
 		return -EINVAL;
 
-	if (copy_from_user(&buf, ubuf, cnt))
-		return -EFAULT;
-	buf[cnt] = '\0';
-
-	pred = kzalloc(sizeof(*pred), GFP_KERNEL);
-	if (!pred)
+	buf = (char *)__get_free_page(GFP_TEMPORARY);
+	if (!buf)
 		return -ENOMEM;
 
-	err = filter_parse(&pbuf, pred);
-	if (err < 0) {
-		filter_free_pred(pred);
-		return err;
-	}
-
-	if (pred->clear) {
-		filter_disable_preds(call);
-		filter_free_pred(pred);
-		return cnt;
+	if (copy_from_user(buf, ubuf, cnt)) {
+		free_page((unsigned long) buf);
+		return -EFAULT;
 	}
+	buf[cnt] = '\0';
 
-	err = filter_add_pred(call, pred);
-	if (err < 0) {
-		filter_free_pred(pred);
+	err = apply_event_filter(call, buf);
+	free_page((unsigned long) buf);
+	if (err < 0)
 		return err;
-	}
-
-	filter_free_pred(pred);
 
 	*ppos += cnt;
 
@@ -562,7 +548,7 @@ subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	filter_print_subsystem_preds(system, s);
+	print_subsystem_event_filter(system, s);
 	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
@@ -575,38 +561,26 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		       loff_t *ppos)
 {
 	struct event_subsystem *system = filp->private_data;
-	char buf[64], *pbuf = buf;
-	struct filter_pred *pred;
+	char *buf;
 	int err;
 
-	if (cnt >= sizeof(buf))
+	if (cnt >= PAGE_SIZE)
 		return -EINVAL;
 
-	if (copy_from_user(&buf, ubuf, cnt))
-		return -EFAULT;
-	buf[cnt] = '\0';
-
-	pred = kzalloc(sizeof(*pred), GFP_KERNEL);
-	if (!pred)
+	buf = (char *)__get_free_page(GFP_TEMPORARY);
+	if (!buf)
 		return -ENOMEM;
 
-	err = filter_parse(&pbuf, pred);
-	if (err < 0) {
-		filter_free_pred(pred);
-		return err;
-	}
-
-	if (pred->clear) {
-		filter_free_subsystem_preds(system);
-		filter_free_pred(pred);
-		return cnt;
+	if (copy_from_user(buf, ubuf, cnt)) {
+		free_page((unsigned long) buf);
+		return -EFAULT;
 	}
+	buf[cnt] = '\0';
 
-	err = filter_add_subsystem_pred(system, pred);
-	if (err < 0) {
-		filter_free_pred(pred);
+	err = apply_subsystem_event_filter(system, buf);
+	free_page((unsigned long) buf);
+	if (err < 0)
 		return err;
-	}
 
 	*ppos += cnt;
 
@@ -760,11 +734,21 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 
 	system->filter = NULL;
 
+	system->filter = kzalloc(sizeof(struct event_filter), GFP_KERNEL);
+	if (!system->filter) {
+		pr_warning("Could not allocate filter for subsystem "
+			   "'%s'\n", name);
+		return system->entry;
+	}
+
 	entry = debugfs_create_file("filter", 0644, system->entry, system,
 				    &ftrace_subsystem_filter_fops);
-	if (!entry)
+	if (!entry) {
+		kfree(system->filter);
+		system->filter = NULL;
 		pr_warning("Could not create debugfs "
 			   "'%s/filter' entry\n", name);
+	}
 
 	return system->entry;
 }

commit a118e4d1402f1349fe3d953493e4168a300a752d
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Apr 28 03:04:53 2009 -0500

    tracing/filters: distinguish between signed and unsigned fields
    
    The new filter comparison ops need to be able to distinguish between
    signed and unsigned field types, so add an is_signed flag/param to the
    event field struct/trace_define_fields().  Also define a simple macro,
    is_signed_type() to determine the signedness at compile time, used in the
    trace macros.  If the is_signed_type() macro won't work with a specific
    type, a new slightly modified version of TRACE_FIELD() called
    TRACE_FIELD_SIGN(), allows the signedness to be set explicitly.
    
    [ Impact: extend trace-filter code for new feature ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <1240905893.6416.120.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1cd1f37373dd..bbbea7479371 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -26,7 +26,7 @@ static DEFINE_MUTEX(event_mutex);
 LIST_HEAD(ftrace_events);
 
 int trace_define_field(struct ftrace_event_call *call, char *type,
-		       char *name, int offset, int size)
+		       char *name, int offset, int size, int is_signed)
 {
 	struct ftrace_event_field *field;
 
@@ -44,6 +44,7 @@ int trace_define_field(struct ftrace_event_call *call, char *type,
 
 	field->offset = offset;
 	field->size = size;
+	field->is_signed = is_signed;
 	list_add(&field->link, &call->fields);
 
 	return 0;

commit 30e673b230f9d556eb81ef68a7b1a08c8b3b142c
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Apr 28 03:04:47 2009 -0500

    tracing/filters: move preds into event_filter object
    
    Create a new event_filter object, and move the pred-related members
    out of the call and subsystem objects and into the filter object - the
    details of the filter implementation don't need to be exposed in the
    call and subsystem in any case, and it will also help make the new
    parser implementation a little cleaner.
    
    [ Impact: refactor trace-filter code to prepare for new features ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: fweisbec@gmail.com
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    LKML-Reference: <1240905887.6416.119.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index be4d3a437c17..1cd1f37373dd 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -757,8 +757,7 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 
 	list_add(&system->list, &event_subsystems);
 
-	system->preds = NULL;
-	system->n_preds = 0;
+	system->filter = NULL;
 
 	entry = debugfs_create_file("filter", 0644, system->entry, system,
 				    &ftrace_subsystem_filter_fops);

commit 701970b3a83cc639c1ec8fc6f40a7871cb99426f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 24 23:11:22 2009 -0400

    tracing/events: make modules have their own file_operations structure
    
    For proper module reference counting, the file_operations that modules use
    must have the "owner" field set to the module. Unfortunately, the trace events
    use share file_operations. The same file_operations are used by all both
    kernel core and all modules.
    
    This patch makes the modules allocate their own file_operations and
    copies the functions from the core kernel. This allows those file
    operations to be owned by the module.
    
    Care is taken to free this code on module unload.
    
    Thanks to Greg KH for reminding me that file_operations must be owned
    by the module to have reference counting take place.
    
    [ Impact: fix modular tracepoints / potential crash ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Acked-by: Greg Kroah-Hartman <gregkh@suse.de>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b92081588088..be4d3a437c17 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -770,7 +770,11 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 }
 
 static int
-event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
+event_create_dir(struct ftrace_event_call *call, struct dentry *d_events,
+		 const struct file_operations *id,
+		 const struct file_operations *enable,
+		 const struct file_operations *filter,
+		 const struct file_operations *format)
 {
 	struct dentry *entry;
 	int ret;
@@ -800,11 +804,11 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 
 	if (call->regfunc)
 		entry = trace_create_file("enable", 0644, call->dir, call,
-					  &ftrace_enable_fops);
+					  enable);
 
 	if (call->id)
 		entry = trace_create_file("id", 0444, call->dir, call,
-					  &ftrace_event_id_fops);
+					  id);
 
 	if (call->define_fields) {
 		ret = call->define_fields();
@@ -814,7 +818,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 			return ret;
 		}
 		entry = trace_create_file("filter", 0644, call->dir, call,
-					  &ftrace_event_filter_fops);
+					  filter);
 	}
 
 	/* A trace may not want to export its format */
@@ -822,7 +826,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 		return 0;
 
 	entry = trace_create_file("format", 0444, call->dir, call,
-				  &ftrace_event_format_fops);
+				  format);
 
 	return 0;
 }
@@ -833,8 +837,60 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 	     event++)
 
 #ifdef CONFIG_MODULES
+
+static LIST_HEAD(ftrace_module_file_list);
+
+/*
+ * Modules must own their file_operations to keep up with
+ * reference counting.
+ */
+struct ftrace_module_file_ops {
+	struct list_head		list;
+	struct module			*mod;
+	struct file_operations		id;
+	struct file_operations		enable;
+	struct file_operations		format;
+	struct file_operations		filter;
+};
+
+static struct ftrace_module_file_ops *
+trace_create_file_ops(struct module *mod)
+{
+	struct ftrace_module_file_ops *file_ops;
+
+	/*
+	 * This is a bit of a PITA. To allow for correct reference
+	 * counting, modules must "own" their file_operations.
+	 * To do this, we allocate the file operations that will be
+	 * used in the event directory.
+	 */
+
+	file_ops = kmalloc(sizeof(*file_ops), GFP_KERNEL);
+	if (!file_ops)
+		return NULL;
+
+	file_ops->mod = mod;
+
+	file_ops->id = ftrace_event_id_fops;
+	file_ops->id.owner = mod;
+
+	file_ops->enable = ftrace_enable_fops;
+	file_ops->enable.owner = mod;
+
+	file_ops->filter = ftrace_event_filter_fops;
+	file_ops->filter.owner = mod;
+
+	file_ops->format = ftrace_event_format_fops;
+	file_ops->format.owner = mod;
+
+	list_add(&file_ops->list, &ftrace_module_file_list);
+
+	return file_ops;
+}
+
 static void trace_module_add_events(struct module *mod)
 {
+	struct ftrace_module_file_ops *file_ops = NULL;
 	struct ftrace_event_call *call, *start, *end;
 	struct dentry *d_events;
 
@@ -852,14 +908,27 @@ static void trace_module_add_events(struct module *mod)
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;
+
+		/*
+		 * This module has events, create file ops for this module
+		 * if not already done.
+		 */
+		if (!file_ops) {
+			file_ops = trace_create_file_ops(mod);
+			if (!file_ops)
+				return;
+		}
 		call->mod = mod;
 		list_add(&call->list, &ftrace_events);
-		event_create_dir(call, d_events);
+		event_create_dir(call, d_events,
+				 &file_ops->id, &file_ops->enable,
+				 &file_ops->filter, &file_ops->format);
 	}
 }
 
 static void trace_module_remove_events(struct module *mod)
 {
+	struct ftrace_module_file_ops *file_ops;
 	struct ftrace_event_call *call, *p;
 
 	list_for_each_entry_safe(call, p, &ftrace_events, list) {
@@ -874,6 +943,16 @@ static void trace_module_remove_events(struct module *mod)
 			list_del(&call->list);
 		}
 	}
+
+	/* Now free the file_operations */
+	list_for_each_entry(file_ops, &ftrace_module_file_list, list) {
+		if (file_ops->mod == mod)
+			break;
+	}
+	if (&file_ops->list != &ftrace_module_file_list) {
+		list_del(&file_ops->list);
+		kfree(file_ops);
+	}
 }
 
 static int trace_module_notify(struct notifier_block *self,
@@ -954,7 +1033,9 @@ static __init int event_trace_init(void)
 		if (!call->name)
 			continue;
 		list_add(&call->list, &ftrace_events);
-		event_create_dir(call, d_events);
+		event_create_dir(call, d_events, &ftrace_event_id_fops,
+				 &ftrace_enable_fops, &ftrace_event_filter_fops,
+				 &ftrace_event_format_fops);
 	}
 
 	ret = register_module_notifier(&trace_module_nb);

commit 75db37d2f4c0ad9466ead57d467277d097b4105c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 26 11:43:36 2009 -0400

    tracing: add size checks for exported ftrace internal structures
    
    The events exported by TRACE_EVENT are automated and are guaranteed
    to be correct when used.
    
    The internal ftrace structures on the other hand are more manually
    exported. These require the ftrace maintainer to make sure they
    are up to date.
    
    This patch adds a size check to help flag when a type changes in
    an internal ftrace data structure, and the update needs to be reflected
    in the export.
    
    If a export is incorrect, then the only harm is that the user space
    tools will not know how to correctly read the internal structures of
    ftrace.
    
    [ Impact: help prevent inconsistent ftrace format print outs ]
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9887131afa03..b92081588088 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -381,8 +381,11 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return cnt;
 }
 
+extern char *__bad_type_size(void);
+
 #undef FIELD
 #define FIELD(type, name)						\
+	sizeof(type) != sizeof(field.name) ? __bad_type_size() :	\
 	#type, "common_" #name, offsetof(typeof(field), name),		\
 		sizeof(field.name)
 

commit 89ec0dee9eba6275d47be0b878cf5f6d5c2fb6eb
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 26 11:03:29 2009 -0400

    tracing: increase size of number of possible events
    
    With the new event tracing registration, we must increase the number
    of events that can be registered. Currently the type field is only
    one byte, which leaves us only 256 possible events.
    
    Since we do not save the CPU number in the tracer anymore (it is determined
    by the per cpu ring buffer that is used) we have an extra byte to use.
    
    This patch increases the size of type from 1 byte (256 events) to
    2 bytes (65,536 events).
    
    It also adds a WARN_ON_ONCE if we exceed that limit.
    
    [ Impact: allow more than 255 events ]
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5d6e879cf875..9887131afa03 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -398,7 +398,7 @@ static int trace_write_header(struct trace_seq *s)
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\n",
-				FIELD(int, type),
+				FIELD(unsigned short, type),
 				FIELD(unsigned char, flags),
 				FIELD(unsigned char, preempt_count),
 				FIELD(int, pid),

commit 7a4f453b6d7379a7c380825949977c5a838aa012
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Wed Apr 22 16:53:34 2009 +0800

    tracing/events: make struct trace_entry->type to be int type
    
    struct trace_entry->type is unsigned char, while trace event's id is
    int type, thus for a event with id >= 256, it's entry->type is cast
    to (id % 256), and then we can't see the trace output of this event.
    
     # insmod trace-events-sample.ko
     # echo foo_bar > /mnt/tracing/set_event
     # cat /debug/tracing/events/trace-events-sample/foo_bar/id
     256
     # cat /mnt/tracing/trace_pipe
               <...>-3548  [001]   215.091142: Unknown type 0
               <...>-3548  [001]   216.089207: Unknown type 0
               <...>-3548  [001]   217.087271: Unknown type 0
               <...>-3548  [001]   218.085332: Unknown type 0
    
    [ Impact: fix output for trace events with id >= 256 ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <49EEDB0E.5070207@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 9ea55a7dfdec..5d6e879cf875 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -398,7 +398,7 @@ static int trace_write_header(struct trace_seq *s)
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\n",
-				FIELD(unsigned char, type),
+				FIELD(int, type),
 				FIELD(unsigned char, flags),
 				FIELD(unsigned char, preempt_count),
 				FIELD(int, pid),

commit e8082f3f5a17d7a7bfc7dd1050a3f958dc034e9a
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Tue Apr 21 17:11:46 2009 +0800

    tracing/filters: don't remove old filters when failed to write subsys->filter
    
    If writing subsys->filter returns EINVAL or ENOSPC, the original
    filters in subsys/ and subsys/events/ will be removed. This is
    definitely wrong.
    
    [ Impact: fix filter setting semantics on error condition ]
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <49ED8DD2.2070700@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 672b195f86c9..9ea55a7dfdec 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -600,7 +600,6 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 	err = filter_add_subsystem_pred(system, pred);
 	if (err < 0) {
-		filter_free_subsystem_preds(system);
 		filter_free_pred(pred);
 		return err;
 	}

commit cb4764a6dbffd9bb3cf759421ae82384071a933d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Apr 20 18:16:44 2009 -0400

    tracing: use nowakeup version of commit for function event trace tests
    
    The startup tests for the event tracer also runs with the function
    tracer enabled. The "wakeup" version of the trace commit was used
    which can grab spinlocks. If a task was preempted by an NMI
    that called a function being traced, it could deadlock due to the
    function tracer trying to grab the same lock.
    
    Thanks to Frederic Weisbecker for pointing out where the bug was.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Reported-by: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 98daf5dc74a6..672b195f86c9 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1164,7 +1164,7 @@ function_test_events_call(unsigned long ip, unsigned long parent_ip)
 	entry->ip			= ip;
 	entry->parent_ip		= parent_ip;
 
-	trace_current_buffer_unlock_commit(event, flags, pc);
+	trace_nowake_buffer_unlock_commit(event, flags, pc);
 
  out:
 	atomic_dec(&per_cpu(test_event_disable, cpu));

commit 28d20e2d6e94434827e11c310788b87204b84559
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Apr 20 12:12:44 2009 -0400

    tracing/events: call the correct event trace selftest init function
    
    The late_initcall calls a helper function instead of the proper
    init event selftest function.
    
    This update may have been lost due to conflicting merges.
    
    [ Impact: fix compiler warning and call extended event trace self tests ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 64f9d6d2735b..98daf5dc74a6 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1199,6 +1199,6 @@ static __init int event_trace_self_tests_init(void)
 	return 0;
 }
 
-late_initcall(event_trace_self_tests);
+late_initcall(event_trace_self_tests_init);
 
 #endif

commit ac1adc55fc71c7515caa2eb0e63e49b3d1c6a47c
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Fri Apr 17 00:27:08 2009 -0500

    tracing/filters: add filter_mutex to protect filter predicates
    
    This patch adds a filter_mutex to prevent the filter predicates from
    being accessed concurrently by various external functions.
    
    It's based on a previous patch by Li Zefan:
            "[PATCH 7/7] tracing/filters: make filter preds RCU safe"
    
    v2 changes:
    
    - fixed wrong value returned in a add_subsystem_pred() failure case
      noticed by Li Zefan.
    
    [ Impact: fix trace filter corruption/crashes on parallel access ]
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Reviewed-by: Li Zefan <lizf@cn.fujitsu.com>
    Tested-by: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: paulmck@linux.vnet.ibm.com
    LKML-Reference: <1239946028.6639.13.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1137f951be42..64f9d6d2735b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -488,7 +488,7 @@ event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	filter_print_preds(call->preds, call->n_preds, s);
+	filter_print_preds(call, s);
 	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
@@ -558,7 +558,7 @@ subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	filter_print_preds(system->preds, system->n_preds, s);
+	filter_print_subsystem_preds(system, s);
 	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);

commit 9ea21c1ecdb35ecdcac5fd9d95f62a1f6a7ffec0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Apr 16 12:15:44 2009 -0400

    tracing/events: perform function tracing in event selftests
    
    We can find some bugs in the trace events if we stress the writes as well.
    The function tracer is a good way to stress the events.
    
    [ Impact: extend scope of event tracer self-tests ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Andrew Morton <akpm@linux-foundation.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <20090416161746.604786131@goodmis.org>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7163a2bb021a..1137f951be42 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -1017,7 +1017,7 @@ static __init void event_test_stuff(void)
  * For every trace event defined, we will test each trace point separately,
  * and then by groups, and finally all trace points.
  */
-static __init int event_trace_self_tests(void)
+static __init void event_trace_self_tests(void)
 {
 	struct ftrace_event_call *call;
 	struct event_subsystem *system;
@@ -1071,7 +1071,7 @@ static __init int event_trace_self_tests(void)
 		sysname = kstrdup(system->name, GFP_KERNEL);
 		if (WARN_ON(!sysname)) {
 			pr_warning("Can't allocate memory, giving up!\n");
-			return 0;
+			return;
 		}
 		ret = ftrace_set_clr_event(sysname, 1);
 		kfree(sysname);
@@ -1086,7 +1086,7 @@ static __init int event_trace_self_tests(void)
 		sysname = kstrdup(system->name, GFP_KERNEL);
 		if (WARN_ON(!sysname)) {
 			pr_warning("Can't allocate memory, giving up!\n");
-			return 0;
+			return;
 		}
 		ret = ftrace_set_clr_event(sysname, 0);
 		kfree(sysname);
@@ -1106,14 +1106,14 @@ static __init int event_trace_self_tests(void)
 	sysname = kmalloc(4, GFP_KERNEL);
 	if (WARN_ON(!sysname)) {
 		pr_warning("Can't allocate memory, giving up!\n");
-		return 0;
+		return;
 	}
 	memcpy(sysname, "*:*", 4);
 	ret = ftrace_set_clr_event(sysname, 1);
 	if (WARN_ON_ONCE(ret)) {
 		kfree(sysname);
 		pr_warning("error enabling all events\n");
-		return 0;
+		return;
 	}
 
 	event_test_stuff();
@@ -1125,10 +1125,76 @@ static __init int event_trace_self_tests(void)
 
 	if (WARN_ON_ONCE(ret)) {
 		pr_warning("error disabling all events\n");
-		return 0;
+		return;
 	}
 
 	pr_cont("OK\n");
+}
+
+#ifdef CONFIG_FUNCTION_TRACER
+
+static DEFINE_PER_CPU(atomic_t, test_event_disable);
+
+static void
+function_test_events_call(unsigned long ip, unsigned long parent_ip)
+{
+	struct ring_buffer_event *event;
+	struct ftrace_entry *entry;
+	unsigned long flags;
+	long disabled;
+	int resched;
+	int cpu;
+	int pc;
+
+	pc = preempt_count();
+	resched = ftrace_preempt_disable();
+	cpu = raw_smp_processor_id();
+	disabled = atomic_inc_return(&per_cpu(test_event_disable, cpu));
+
+	if (disabled != 1)
+		goto out;
+
+	local_save_flags(flags);
+
+	event = trace_current_buffer_lock_reserve(TRACE_FN, sizeof(*entry),
+						  flags, pc);
+	if (!event)
+		goto out;
+	entry	= ring_buffer_event_data(event);
+	entry->ip			= ip;
+	entry->parent_ip		= parent_ip;
+
+	trace_current_buffer_unlock_commit(event, flags, pc);
+
+ out:
+	atomic_dec(&per_cpu(test_event_disable, cpu));
+	ftrace_preempt_enable(resched);
+}
+
+static struct ftrace_ops trace_ops __initdata  =
+{
+	.func = function_test_events_call,
+};
+
+static __init void event_trace_self_test_with_function(void)
+{
+	register_ftrace_function(&trace_ops);
+	pr_info("Running tests again, along with the function tracer\n");
+	event_trace_self_tests();
+	unregister_ftrace_function(&trace_ops);
+}
+#else
+static __init void event_trace_self_test_with_function(void)
+{
+}
+#endif
+
+static __init int event_trace_self_tests_init(void)
+{
+
+	event_trace_self_tests();
+
+	event_trace_self_test_with_function();
 
 	return 0;
 }

commit d1b182a8d49ed6416325b4e0a1cb0f17cd4e702a
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Apr 15 16:53:47 2009 -0400

    tracing/events/ring-buffer: expose format of ring buffer headers to users
    
    Currently, every thing needed to read the binary output from the
    ring buffers is available, with the exception of the way the ring
    buffers handles itself internally.
    
    This patch creates two special files in the debugfs/tracing/events
    directory:
    
     # cat /debug/tracing/events/header_page
            field: u64 timestamp;   offset:0;       size:8;
            field: local_t commit;  offset:8;       size:8;
            field: char data;       offset:16;      size:4080;
    
     # cat /debug/tracing/events/header_event
            type        :    2 bits
            len         :    3 bits
            time_delta  :   27 bits
            array       :   32 bits
    
            padding     : type == 0
            time_extend : type == 1
            data        : type == 3
    
    This is to allow a userspace app to see if the ring buffer format changes
    or not.
    
    [ Impact: allow userspace apps to know of ringbuffer format changes ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index f81d6eec4e43..7163a2bb021a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -610,6 +610,30 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return cnt;
 }
 
+static ssize_t
+show_header(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	int (*func)(struct trace_seq *s) = filp->private_data;
+	struct trace_seq *s;
+	int r;
+
+	if (*ppos)
+		return 0;
+
+	s = kmalloc(sizeof(*s), GFP_KERNEL);
+	if (!s)
+		return -ENOMEM;
+
+	trace_seq_init(s);
+
+	func(s);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
+
+	kfree(s);
+
+	return r;
+}
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -667,6 +691,11 @@ static const struct file_operations ftrace_subsystem_filter_fops = {
 	.write = subsystem_filter_write,
 };
 
+static const struct file_operations ftrace_show_header_fops = {
+	.open = tracing_open_generic,
+	.read = show_header,
+};
+
 static struct dentry *event_trace_events_dir(void)
 {
 	static struct dentry *d_tracer;
@@ -909,6 +938,15 @@ static __init int event_trace_init(void)
 	if (!d_events)
 		return 0;
 
+	/* ring buffer internal formats */
+	trace_create_file("header_page", 0444, d_events,
+			  ring_buffer_print_page_header,
+			  &ftrace_show_header_fops);
+
+	trace_create_file("header_event", 0444, d_events,
+			  ring_buffer_print_entry_header,
+			  &ftrace_show_header_fops);
+
 	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
 		/* The linker may leave blanks */
 		if (!call->name)

commit e6187007d6c365b551c69ea3df46f06fd1c8bd19
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Apr 15 13:36:40 2009 -0400

    tracing/events: add startup tests for events
    
    As events start to become popular, and the new way to add tracing
    infrastructure into ftrace, it is important to catch any problems
    that might happen with a mistake in the TRACE_EVENT macro.
    
    This patch introduces a startup self test on the registered trace
    events. Note, it can only do a generic test, any type of testing that
    needs more involement is needed to be implemented by the tracepoint
    creators.
    
    The test goes down one by one enabling a trace point and running
    some random tasks (random in the sense that I just made them up).
    Those tasks are creating threads, grabbing mutexes and spinlocks
    and using workqueues.
    
    After testing each event individually, it does the same test after
    enabling each system of trace points. Like sched, irq, lockdep.
    
    Then finally it enables all tracepoints and performs the tasks again.
    The output to the console on bootup will look like this when everything
    works:
    
    Running tests on trace events:
    Testing event kfree_skb: OK
    Testing event kmalloc: OK
    Testing event kmem_cache_alloc: OK
    Testing event kmalloc_node: OK
    Testing event kmem_cache_alloc_node: OK
    Testing event kfree: OK
    Testing event kmem_cache_free: OK
    Testing event irq_handler_exit: OK
    Testing event irq_handler_entry: OK
    Testing event softirq_entry: OK
    Testing event softirq_exit: OK
    Testing event lock_acquire: OK
    Testing event lock_release: OK
    Testing event sched_kthread_stop: OK
    Testing event sched_kthread_stop_ret: OK
    Testing event sched_wait_task: OK
    Testing event sched_wakeup: OK
    Testing event sched_wakeup_new: OK
    Testing event sched_switch: OK
    Testing event sched_migrate_task: OK
    Testing event sched_process_free: OK
    Testing event sched_process_exit: OK
    Testing event sched_process_wait: OK
    Testing event sched_process_fork: OK
    Testing event sched_signal_send: OK
    Running tests on trace event systems:
    Testing event system skb: OK
    Testing event system kmem: OK
    Testing event system irq: OK
    Testing event system lockdep: OK
    Testing event system sched: OK
    Running tests on all trace events:
    Testing all events: OK
    
    [ folded in:
    
      tracing: add #include <linux/delay.h> to fix build failure in test_work()
    
      This build failure occured on a few rare configs:
    
       kernel/trace/trace_events.c: In function test_work:
       kernel/trace/trace_events.c:975: error: implicit declaration of function udelay
       kernel/trace/trace_events.c:980: error: implicit declaration of function msleep
    
      delay.h is included in way too many other headers, hiding cases
      where new usage is added without header inclusion.
    
      [ Impact: build fix ]
    
      Signed-off-by: Ingo Molnar <mingo@elte.hu>
    ]
    
    [ Impact: add event tracer self-tests ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 6591d83e1e7a..f81d6eec4e43 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -8,10 +8,14 @@
  *
  */
 
+#include <linux/workqueue.h>
+#include <linux/spinlock.h>
+#include <linux/kthread.h>
 #include <linux/debugfs.h>
 #include <linux/uaccess.h>
 #include <linux/module.h>
 #include <linux/ctype.h>
+#include <linux/delay.h>
 
 #include "trace_output.h"
 
@@ -920,3 +924,177 @@ static __init int event_trace_init(void)
 	return 0;
 }
 fs_initcall(event_trace_init);
+
+#ifdef CONFIG_FTRACE_STARTUP_TEST
+
+static DEFINE_SPINLOCK(test_spinlock);
+static DEFINE_SPINLOCK(test_spinlock_irq);
+static DEFINE_MUTEX(test_mutex);
+
+static __init void test_work(struct work_struct *dummy)
+{
+	spin_lock(&test_spinlock);
+	spin_lock_irq(&test_spinlock_irq);
+	udelay(1);
+	spin_unlock_irq(&test_spinlock_irq);
+	spin_unlock(&test_spinlock);
+
+	mutex_lock(&test_mutex);
+	msleep(1);
+	mutex_unlock(&test_mutex);
+}
+
+static __init int event_test_thread(void *unused)
+{
+	void *test_malloc;
+
+	test_malloc = kmalloc(1234, GFP_KERNEL);
+	if (!test_malloc)
+		pr_info("failed to kmalloc\n");
+
+	schedule_on_each_cpu(test_work);
+
+	kfree(test_malloc);
+
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop())
+		schedule();
+
+	return 0;
+}
+
+/*
+ * Do various things that may trigger events.
+ */
+static __init void event_test_stuff(void)
+{
+	struct task_struct *test_thread;
+
+	test_thread = kthread_run(event_test_thread, NULL, "test-events");
+	msleep(1);
+	kthread_stop(test_thread);
+}
+
+/*
+ * For every trace event defined, we will test each trace point separately,
+ * and then by groups, and finally all trace points.
+ */
+static __init int event_trace_self_tests(void)
+{
+	struct ftrace_event_call *call;
+	struct event_subsystem *system;
+	char *sysname;
+	int ret;
+
+	pr_info("Running tests on trace events:\n");
+
+	list_for_each_entry(call, &ftrace_events, list) {
+
+		/* Only test those that have a regfunc */
+		if (!call->regfunc)
+			continue;
+
+		pr_info("Testing event %s: ", call->name);
+
+		/*
+		 * If an event is already enabled, someone is using
+		 * it and the self test should not be on.
+		 */
+		if (call->enabled) {
+			pr_warning("Enabled event during self test!\n");
+			WARN_ON_ONCE(1);
+			continue;
+		}
+
+		call->enabled = 1;
+		call->regfunc();
+
+		event_test_stuff();
+
+		call->unregfunc();
+		call->enabled = 0;
+
+		pr_cont("OK\n");
+	}
+
+	/* Now test at the sub system level */
+
+	pr_info("Running tests on trace event systems:\n");
+
+	list_for_each_entry(system, &event_subsystems, list) {
+
+		/* the ftrace system is special, skip it */
+		if (strcmp(system->name, "ftrace") == 0)
+			continue;
+
+		pr_info("Testing event system %s: ", system->name);
+
+		/* ftrace_set_clr_event can modify the name passed in. */
+		sysname = kstrdup(system->name, GFP_KERNEL);
+		if (WARN_ON(!sysname)) {
+			pr_warning("Can't allocate memory, giving up!\n");
+			return 0;
+		}
+		ret = ftrace_set_clr_event(sysname, 1);
+		kfree(sysname);
+		if (WARN_ON_ONCE(ret)) {
+			pr_warning("error enabling system %s\n",
+				   system->name);
+			continue;
+		}
+
+		event_test_stuff();
+
+		sysname = kstrdup(system->name, GFP_KERNEL);
+		if (WARN_ON(!sysname)) {
+			pr_warning("Can't allocate memory, giving up!\n");
+			return 0;
+		}
+		ret = ftrace_set_clr_event(sysname, 0);
+		kfree(sysname);
+
+		if (WARN_ON_ONCE(ret))
+			pr_warning("error disabling system %s\n",
+				   system->name);
+
+		pr_cont("OK\n");
+	}
+
+	/* Test with all events enabled */
+
+	pr_info("Running tests on all trace events:\n");
+	pr_info("Testing all events: ");
+
+	sysname = kmalloc(4, GFP_KERNEL);
+	if (WARN_ON(!sysname)) {
+		pr_warning("Can't allocate memory, giving up!\n");
+		return 0;
+	}
+	memcpy(sysname, "*:*", 4);
+	ret = ftrace_set_clr_event(sysname, 1);
+	if (WARN_ON_ONCE(ret)) {
+		kfree(sysname);
+		pr_warning("error enabling all events\n");
+		return 0;
+	}
+
+	event_test_stuff();
+
+	/* reset sysname */
+	memcpy(sysname, "*:*", 4);
+	ret = ftrace_set_clr_event(sysname, 0);
+	kfree(sysname);
+
+	if (WARN_ON_ONCE(ret)) {
+		pr_warning("error disabling all events\n");
+		return 0;
+	}
+
+	pr_cont("OK\n");
+
+	return 0;
+}
+
+late_initcall(event_trace_self_tests);
+
+#endif

commit 61f919a12fbdc3fd20f980a34a118d597198a392
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 14 18:22:32 2009 -0400

    tracing/events: fix compile for modules disabled
    
    Impact: compile fix
    
    The addition of TRACE_EVENT for modules breaks the build for when
    modules are disabled. This code fixes that.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a4b177720a6c..6591d83e1e7a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -797,6 +797,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 	     (unsigned long)event < (unsigned long)end;		\
 	     event++)
 
+#ifdef CONFIG_MODULES
 static void trace_module_add_events(struct module *mod)
 {
 	struct ftrace_event_call *call, *start, *end;
@@ -840,8 +841,8 @@ static void trace_module_remove_events(struct module *mod)
 	}
 }
 
-int trace_module_notify(struct notifier_block *self,
-			unsigned long val, void *data)
+static int trace_module_notify(struct notifier_block *self,
+			       unsigned long val, void *data)
 {
 	struct module *mod = data;
 
@@ -858,6 +859,13 @@ int trace_module_notify(struct notifier_block *self,
 
 	return 0;
 }
+#else
+static int trace_module_notify(struct notifier_block *self,
+			       unsigned long val, void *data)
+{
+	return 0;
+}
+#endif /* CONFIG_MODULES */
 
 struct notifier_block trace_module_nb = {
 	.notifier_call = trace_module_notify,

commit 6d723736e472f7a0cd5b62c84152fceead241328
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 14:53:50 2009 -0400

    tracing/events: add support for modules to TRACE_EVENT
    
    Impact: allow modules to add TRACE_EVENTS on load
    
    This patch adds the final hooks to allow modules to use the TRACE_EVENT
    macro. A notifier and a data structure are used to link the TRACE_EVENTs
    defined in the module to connect them with the ftrace event tracing system.
    
    It also adds the necessary automated clean ups to the trace events when a
    module is removed.
    
    Cc: Rusty Russell <rusty@rustcorp.com.au>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 8b9e621b80b4..a4b177720a6c 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -713,7 +713,13 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 		return d_events;
 	}
 
-	system->name = name;
+	system->name = kstrdup(name, GFP_KERNEL);
+	if (!system->name) {
+		debugfs_remove(system->entry);
+		kfree(system);
+		return d_events;
+	}
+
 	list_add(&system->list, &event_subsystems);
 
 	system->preds = NULL;
@@ -738,7 +744,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 	 * If the trace point header did not define TRACE_SYSTEM
 	 * then the system would be called "TRACE_SYSTEM".
 	 */
-	if (strcmp(call->system, "TRACE_SYSTEM") != 0)
+	if (strcmp(call->system, TRACE_SYSTEM) != 0)
 		d_events = event_subsystem_dir(call->system, d_events);
 
 	if (call->raw_init) {
@@ -757,21 +763,13 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 		return -1;
 	}
 
-	if (call->regfunc) {
-		entry = debugfs_create_file("enable", 0644, call->dir, call,
-					    &ftrace_enable_fops);
-		if (!entry)
-			pr_warning("Could not create debugfs "
-				   "'%s/enable' entry\n", call->name);
-	}
+	if (call->regfunc)
+		entry = trace_create_file("enable", 0644, call->dir, call,
+					  &ftrace_enable_fops);
 
-	if (call->id) {
-		entry = debugfs_create_file("id", 0444, call->dir, call,
-				&ftrace_event_id_fops);
-		if (!entry)
-			pr_warning("Could not create debugfs '%s/id' entry\n",
-					call->name);
-	}
+	if (call->id)
+		entry = trace_create_file("id", 0444, call->dir, call,
+					  &ftrace_event_id_fops);
 
 	if (call->define_fields) {
 		ret = call->define_fields();
@@ -780,40 +778,102 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 				   " events/%s\n", call->name);
 			return ret;
 		}
-		entry = debugfs_create_file("filter", 0644, call->dir, call,
-					    &ftrace_event_filter_fops);
-		if (!entry)
-			pr_warning("Could not create debugfs "
-				   "'%s/filter' entry\n", call->name);
+		entry = trace_create_file("filter", 0644, call->dir, call,
+					  &ftrace_event_filter_fops);
 	}
 
 	/* A trace may not want to export its format */
 	if (!call->show_format)
 		return 0;
 
-	entry = debugfs_create_file("format", 0444, call->dir, call,
-				    &ftrace_event_format_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'%s/format' entry\n", call->name);
+	entry = trace_create_file("format", 0444, call->dir, call,
+				  &ftrace_event_format_fops);
+
+	return 0;
+}
+
+#define for_each_event(event, start, end)			\
+	for (event = start;					\
+	     (unsigned long)event < (unsigned long)end;		\
+	     event++)
+
+static void trace_module_add_events(struct module *mod)
+{
+	struct ftrace_event_call *call, *start, *end;
+	struct dentry *d_events;
+
+	start = mod->trace_events;
+	end = mod->trace_events + mod->num_trace_events;
+
+	if (start == end)
+		return;
+
+	d_events = event_trace_events_dir();
+	if (!d_events)
+		return;
+
+	for_each_event(call, start, end) {
+		/* The linker may leave blanks */
+		if (!call->name)
+			continue;
+		call->mod = mod;
+		list_add(&call->list, &ftrace_events);
+		event_create_dir(call, d_events);
+	}
+}
+
+static void trace_module_remove_events(struct module *mod)
+{
+	struct ftrace_event_call *call, *p;
+
+	list_for_each_entry_safe(call, p, &ftrace_events, list) {
+		if (call->mod == mod) {
+			if (call->enabled) {
+				call->enabled = 0;
+				call->unregfunc();
+			}
+			if (call->event)
+				unregister_ftrace_event(call->event);
+			debugfs_remove_recursive(call->dir);
+			list_del(&call->list);
+		}
+	}
+}
+
+int trace_module_notify(struct notifier_block *self,
+			unsigned long val, void *data)
+{
+	struct module *mod = data;
+
+	mutex_lock(&event_mutex);
+	switch (val) {
+	case MODULE_STATE_COMING:
+		trace_module_add_events(mod);
+		break;
+	case MODULE_STATE_GOING:
+		trace_module_remove_events(mod);
+		break;
+	}
+	mutex_unlock(&event_mutex);
 
 	return 0;
 }
 
+struct notifier_block trace_module_nb = {
+	.notifier_call = trace_module_notify,
+	.priority = 0,
+};
+
 extern struct ftrace_event_call __start_ftrace_events[];
 extern struct ftrace_event_call __stop_ftrace_events[];
 
-#define for_each_event(event)						\
-	for (event = __start_ftrace_events;				\
-	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
-	     event++)
-
 static __init int event_trace_init(void)
 {
 	struct ftrace_event_call *call;
 	struct dentry *d_tracer;
 	struct dentry *entry;
 	struct dentry *d_events;
+	int ret;
 
 	d_tracer = tracing_init_dentry();
 	if (!d_tracer)
@@ -837,7 +897,7 @@ static __init int event_trace_init(void)
 	if (!d_events)
 		return 0;
 
-	for_each_event(call) {
+	for_each_event(call, __start_ftrace_events, __stop_ftrace_events) {
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;
@@ -845,6 +905,10 @@ static __init int event_trace_init(void)
 		event_create_dir(call, d_events);
 	}
 
+	ret = register_module_notifier(&trace_module_nb);
+	if (!ret)
+		pr_warning("Failed to register trace events module notifier\n");
+
 	return 0;
 }
 fs_initcall(event_trace_init);

commit 17c873ec280a03894bc718af817f7f24fa787ae1
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 18:12:50 2009 -0400

    tracing/events: add export symbols for trace events in modules
    
    Impact: let modules add trace events
    
    The trace event code requires some functions to be exported to allow
    modules to use TRACE_EVENT. This patch adds EXPORT_SYMBOL_GPL to the
    necessary functions.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 5c66aaff07c1..8b9e621b80b4 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -53,6 +53,7 @@ int trace_define_field(struct ftrace_event_call *call, char *type,
 
 	return -ENOMEM;
 }
+EXPORT_SYMBOL_GPL(trace_define_field);
 
 static void ftrace_clear_events(void)
 {

commit a59fd6027218bd7c994e39d14afe0242f895144f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Apr 10 13:52:20 2009 -0400

    tracing/events: convert event call sites to use a link list
    
    Impact: makes it possible to define events in modules
    
    The events are created by reading down the section that they are linked
    in by the macros. But this is not scalable to modules. This patch converts
    the manipulations to use a global link list, and on boot up it adds
    the items in the section to the list.
    
    This change will allow modules to add their tracing events to the list as
    well.
    
    Note, this change alone does not permit modules to use the TRACE_EVENT macros,
    but the change is needed for them to eventually do so.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ead68ac99191..5c66aaff07c1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -19,6 +19,8 @@
 
 static DEFINE_MUTEX(event_mutex);
 
+LIST_HEAD(ftrace_events);
+
 int trace_define_field(struct ftrace_event_call *call, char *type,
 		       char *name, int offset, int size)
 {
@@ -54,16 +56,14 @@ int trace_define_field(struct ftrace_event_call *call, char *type,
 
 static void ftrace_clear_events(void)
 {
-	struct ftrace_event_call *call = (void *)__start_ftrace_events;
-
+	struct ftrace_event_call *call;
 
-	while ((unsigned long)call < (unsigned long)__stop_ftrace_events) {
+	list_for_each_entry(call, &ftrace_events, list) {
 
 		if (call->enabled) {
 			call->enabled = 0;
 			call->unregfunc();
 		}
-		call++;
 	}
 }
 
@@ -89,7 +89,7 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 
 static int ftrace_set_clr_event(char *buf, int set)
 {
-	struct ftrace_event_call *call = __start_ftrace_events;
+	struct ftrace_event_call *call;
 	char *event = NULL, *sub = NULL, *match;
 	int ret = -EINVAL;
 
@@ -118,7 +118,7 @@ static int ftrace_set_clr_event(char *buf, int set)
 	}
 
 	mutex_lock(&event_mutex);
-	for_each_event(call) {
+	list_for_each_entry(call, &ftrace_events, list) {
 
 		if (!call->name || !call->regfunc)
 			continue;
@@ -224,15 +224,17 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 static void *
 t_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_call *call = m->private;
-	struct ftrace_event_call *next = call;
+	struct list_head *list = m->private;
+	struct ftrace_event_call *call;
 
 	(*pos)++;
 
 	for (;;) {
-		if ((unsigned long)call >= (unsigned long)__stop_ftrace_events)
+		if (list == &ftrace_events)
 			return NULL;
 
+		call = list_entry(list, struct ftrace_event_call, list);
+
 		/*
 		 * The ftrace subsystem is for showing formats only.
 		 * They can not be enabled or disabled via the event files.
@@ -240,11 +242,10 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 		if (call->regfunc)
 			break;
 
-		call++;
-		next = call;
+		list = list->next;
 	}
 
-	m->private = ++next;
+	m->private = list->next;
 
 	return call;
 }
@@ -257,22 +258,23 @@ static void *t_start(struct seq_file *m, loff_t *pos)
 static void *
 s_next(struct seq_file *m, void *v, loff_t *pos)
 {
-	struct ftrace_event_call *call = m->private;
-	struct ftrace_event_call *next;
+	struct list_head *list = m->private;
+	struct ftrace_event_call *call;
 
 	(*pos)++;
 
  retry:
-	if ((unsigned long)call >= (unsigned long)__stop_ftrace_events)
+	if (list == &ftrace_events)
 		return NULL;
 
+	call = list_entry(list, struct ftrace_event_call, list);
+
 	if (!call->enabled) {
-		call++;
+		list = list->next;
 		goto retry;
 	}
 
-	next = call;
-	m->private = ++next;
+	m->private = list->next;
 
 	return call;
 }
@@ -312,7 +314,7 @@ ftrace_event_seq_open(struct inode *inode, struct file *file)
 	if (!ret) {
 		struct seq_file *m = file->private_data;
 
-		m->private = __start_ftrace_events;
+		m->private = ftrace_events.next;
 	}
 	return ret;
 }
@@ -797,9 +799,17 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 	return 0;
 }
 
+extern struct ftrace_event_call __start_ftrace_events[];
+extern struct ftrace_event_call __stop_ftrace_events[];
+
+#define for_each_event(event)						\
+	for (event = __start_ftrace_events;				\
+	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
+	     event++)
+
 static __init int event_trace_init(void)
 {
-	struct ftrace_event_call *call = __start_ftrace_events;
+	struct ftrace_event_call *call;
 	struct dentry *d_tracer;
 	struct dentry *entry;
 	struct dentry *d_events;
@@ -830,6 +840,7 @@ static __init int event_trace_init(void)
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;
+		list_add(&call->list, &ftrace_events);
 		event_create_dir(call, d_events);
 	}
 

commit 0a19e53c1514ad8e9c3cbab40c6c3f52c86f403d
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Mon Apr 13 03:17:50 2009 -0500

    tracing/filters: allow on-the-fly filter switching
    
    This patch allows event filters to be safely removed or switched
    on-the-fly while avoiding the use of rcu or the suspension of tracing of
    previous versions.
    
    It does it by adding a new filter_pred_none() predicate function which
    does nothing and by never deallocating either the predicates or any of
    the filter_pred members used in matching; the predicate lists are
    allocated and initialized during ftrace_event_calls initialization.
    
    Whenever a filter is removed or replaced, the filter_pred_* functions
    currently in use by the affected ftrace_event_call are immediately
    switched over to to the filter_pred_none() function, while the rest of
    the filter_pred members are left intact, allowing any currently
    executing filter_pred_* functions to finish up, using the values they're
    currently using.
    
    In the case of filter replacement, the new predicate values are copied
    into the old predicates after the above step, and the filter_pred_none()
    functions are replaced by the filter_pred_* functions for the new
    filter.  In this case, it is possible though very unlikely that a
    previous filter_pred_* is still running even after the
    filter_pred_none() switch and the switch to the new filter_pred_*.  In
    that case, however, because nothing has been deallocated in the
    filter_pred, the worst that can happen is that the old filter_pred_*
    function sees the new values and as a result produces either a false
    positive or a false negative, depending on the values it finds.
    
    So one downside to this method is that rarely, it can produce a bad
    match during the filter switch, but it should be possible to live with
    that, IMHO.
    
    The other downside is that at least in this patch the predicate lists
    are always pre-allocated, taking up memory from the start.  They could
    probably be allocated on first-use, and de-allocated when tracing is
    completely stopped - if this patch makes sense, I could create another
    one to do that later on.
    
    Oh, and it also places a restriction on the size of __arrays in events,
    currently set to 128, since they can't be larger than the now embedded
    str_val arrays in the filter_pred struct.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: paulmck@linux.vnet.ibm.com
    LKML-Reference: <1239610670.6660.49.camel@tropicana>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 789e14eb09a5..ead68ac99191 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -481,7 +481,7 @@ event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	filter_print_preds(call->preds, s);
+	filter_print_preds(call->preds, call->n_preds, s);
 	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
@@ -516,7 +516,7 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	}
 
 	if (pred->clear) {
-		filter_free_preds(call);
+		filter_disable_preds(call);
 		filter_free_pred(pred);
 		return cnt;
 	}
@@ -527,6 +527,8 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		return err;
 	}
 
+	filter_free_pred(pred);
+
 	*ppos += cnt;
 
 	return cnt;
@@ -549,7 +551,7 @@ subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	filter_print_preds(system->preds, s);
+	filter_print_preds(system->preds, system->n_preds, s);
 	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
@@ -712,6 +714,7 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 	list_add(&system->list, &event_subsystems);
 
 	system->preds = NULL;
+	system->n_preds = 0;
 
 	entry = debugfs_create_file("filter", 0644, system->entry, system,
 				    &ftrace_subsystem_filter_fops);

commit b5c851a88a369854c04e511cefb84ea2d0cfa209
Merge: eb02ce017dd8 80a04d3f2f94
Author: Ingo Molnar <mingo@elte.hu>
Date:   Tue Apr 14 00:02:16 2009 +0200

    Merge branch 'linus' into tracing/core
    
    Merge reason: merge latest tracing fixes to avoid conflicts in
                  kernel/trace/trace_events_filter.c with upcoming change
    
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

commit e1112b4d96859367a93468027c9635e2ac04eb3f
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Mar 31 00:48:49 2009 -0500

    tracing/filters: add run-time field descriptions to TRACE_EVENT_FORMAT events
    
    This patch adds run-time field descriptions to all the event formats
    exported using TRACE_EVENT_FORMAT.  It also hooks up all the tracers
    that use them (i.e. the tracers in the 'ftrace subsystem') so they can
    also have their output filtered by the event-filtering mechanism.
    
    When I was testing this, there were a couple of things that fooled me
    into thinking the filters weren't working, when actually they were -
    I'll mention them here so others don't make the same mistakes (and file
    bug reports. ;-)
    
    One is that some of the tracers trace multiple events e.g. the
    sched_switch tracer uses the context_switch and wakeup events, and if
    you don't set filters on all of the traced events, the unfiltered output
    from the events without filters on them can make it look like the
    filtering as a whole isn't working properly, when actually it is doing
    what it was asked to do - it just wasn't asked to do the right thing.
    
    The other is that for the really high-volume tracers e.g. the function
    tracer, the volume of filtered events can be so high that it pushes the
    unfiltered events out of the ring buffer before they can be read so e.g.
    cat'ing the trace file repeatedly shows either no output, or once in
    awhile some output but that isn't there the next time you read the
    trace, which isn't what you normally expect when reading the trace file.
    If you read from the trace_pipe file though, you can catch them before
    they disappear.
    
    Changes from v1:
    
    As suggested by Frederic Weisbecker:
    
    - get rid of externs in functions
    - added unlikely() to filter_check_discard()
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 64ec4d278ffb..be9299a53e2a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -680,6 +680,7 @@ static struct dentry *
 event_subsystem_dir(const char *name, struct dentry *d_events)
 {
 	struct event_subsystem *system;
+	struct dentry *entry;
 
 	/* First see if we did not already create this dir */
 	list_for_each_entry(system, &event_subsystems, list) {
@@ -708,6 +709,12 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 
 	system->preds = NULL;
 
+	entry = debugfs_create_file("filter", 0644, system->entry, system,
+				    &ftrace_subsystem_filter_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'%s/filter' entry\n", name);
+
 	return system->entry;
 }
 

commit 44e9c8b7adc52079f0535f9de0c2c2477831389b
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Sat Apr 11 15:55:28 2009 +0800

    tracing/filters: return proper error code when writing filter file
    
    - propagate return value of filter_add_pred() to the user
    
    - return -ENOSPC but not -ENOMEM or -EINVAL when the filter array
      is full
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <49E04CF0.3010105@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 054bc1802bcd..576f4fa2af0d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -521,9 +521,10 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		return cnt;
 	}
 
-	if (filter_add_pred(call, pred)) {
+	err = filter_add_pred(call, pred);
+	if (err < 0) {
 		filter_free_pred(pred);
-		return -EINVAL;
+		return err;
 	}
 
 	*ppos += cnt;
@@ -588,10 +589,11 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 		return cnt;
 	}
 
-	if (filter_add_subsystem_pred(system, pred)) {
+	err = filter_add_subsystem_pred(system, pred);
+	if (err < 0) {
 		filter_free_subsystem_preds(system);
 		filter_free_pred(pred);
-		return -EINVAL;
+		return err;
 	}
 
 	*ppos += cnt;

commit 8433a40eb7f2c4883ad57f9900f63e4d59240eb7
Author: Li Zefan <lizf@cn.fujitsu.com>
Date:   Sat Apr 11 15:52:18 2009 +0800

    tracing/filters: NIL-terminate user input filter
    
    Make sure messages from user space are NIL-terminated strings,
    otherwise we could dump random memory while reading filter file.
    
    Try this:
     # echo 'parent_comm ==' > events/sched/sched_process_fork/filter
     # cat events/sched/sched_process_fork/filter
     parent_comm == 
    
    Signed-off-by: Li Zefan <lizf@cn.fujitsu.com>
    Acked-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <49E04C32.6060508@cn.fujitsu.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 64ec4d278ffb..054bc1802bcd 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -503,6 +503,7 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 	if (copy_from_user(&buf, ubuf, cnt))
 		return -EFAULT;
+	buf[cnt] = '\0';
 
 	pred = kzalloc(sizeof(*pred), GFP_KERNEL);
 	if (!pred)
@@ -569,6 +570,7 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 	if (copy_from_user(&buf, ubuf, cnt))
 		return -EFAULT;
+	buf[cnt] = '\0';
 
 	pred = kzalloc(sizeof(*pred), GFP_KERNEL);
 	if (!pred)

commit 9a8118baaeb0eaa148913bed77bf9c6335f6ca63
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Thu Mar 26 01:24:34 2009 -0500

    tracing: filter fix for TRACE_EVENT_FORMAT events
    
    Impact: fix crash (hang) when using TRACE_EVENT_FORMAT filter files
    
    filters are only hooked up to the tracepoint events defined using
    TRACE_EVENT but not the tracers that use TRACE_EVENT_FORMAT, such
    as ftrace.
    
    Do not display the filter files at all for TRACE_EVENT_FORMAT events
    for the time being.
    
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: =?ISO-8859-1?Q?Fr=E9d=E9ric?= Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237878882.8339.61.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d132997ab756..64ec4d278ffb 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -680,7 +680,6 @@ static struct dentry *
 event_subsystem_dir(const char *name, struct dentry *d_events)
 {
 	struct event_subsystem *system;
-	struct dentry *entry;
 
 	/* First see if we did not already create this dir */
 	list_for_each_entry(system, &event_subsystems, list) {
@@ -709,12 +708,6 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 
 	system->preds = NULL;
 
-	entry = debugfs_create_file("filter", 0644, system->entry, system,
-				    &ftrace_subsystem_filter_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'%s/filter' entry\n", name);
-
 	return system->entry;
 }
 
@@ -770,14 +763,13 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 				   " events/%s\n", call->name);
 			return ret;
 		}
+		entry = debugfs_create_file("filter", 0644, call->dir, call,
+					    &ftrace_event_filter_fops);
+		if (!entry)
+			pr_warning("Could not create debugfs "
+				   "'%s/filter' entry\n", call->name);
 	}
 
-	entry = debugfs_create_file("filter", 0644, call->dir, call,
-				    &ftrace_event_filter_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'%s/filter' entry\n", call->name);
-
 	/* A trace may not want to export its format */
 	if (!call->show_format)
 		return 0;

commit 4bda2d517bfa3ce3d7044e06988cdddae7adffe2
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Mar 24 02:14:31 2009 -0500

    tracing/filters: use trace_seq_printf() to print filters
    
    Impact: cleanup
    
    Instead of just using the trace_seq buffer to print the filters, use
    trace_seq_printf() as it was intended to be used.
    
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: =?ISO-8859-1?Q?Fr=E9d=E9ric?= Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237878871.8339.59.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a9381384aa9e..d132997ab756 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -481,8 +481,8 @@ event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	r = filter_print_preds(call->preds, s->buffer);
-	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, r);
+	filter_print_preds(call->preds, s);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
 
@@ -547,8 +547,8 @@ subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
 
 	trace_seq_init(s);
 
-	r = filter_print_preds(system->preds, s->buffer);
-	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, r);
+	filter_print_preds(system->preds, s);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, s->len);
 
 	kfree(s);
 

commit 09f1f245c79585383de63e3ca54d0f91824bff3a
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Mar 24 02:14:11 2009 -0500

    tracing/filters: free pred when clearing filters
    
    Impact: fix (small) per trace filter modification memory leak
    
    Free the current pred when clearing the filters via the filter files.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: =?ISO-8859-1?Q?Fr=E9d=E9ric?= Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237878851.8339.58.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fdab30d6c835..a9381384aa9e 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -516,6 +516,7 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 	if (pred->clear) {
 		filter_free_preds(call);
+		filter_free_pred(pred);
 		return cnt;
 	}
 
@@ -581,6 +582,7 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 	if (pred->clear) {
 		filter_free_subsystem_preds(system);
+		filter_free_pred(pred);
 		return cnt;
 	}
 

commit c4cff064be678f1e8344d907499f2a81282edc19
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Mon Mar 23 03:26:48 2009 -0500

    tracing/filters: clean up filter_add_subsystem_pred()
    
    Impact: cleanup, memory leak fix
    
    This patch cleans up filter_add_subsystem_pred():
    
    - searches for the field before creating a copy of the pred
    
    - fixes memory leak in the case a predicate isn't applied
    
    - if -ENOMEM, makes sure there's no longer a reference to the
      pred so the caller can free the half-finished filter
    
    - changes the confusing i == MAX_FILTER_PRED - 1 comparison
      previously remarked upon
    
    This affects only per-subsystem event filtering.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: =?ISO-8859-1?Q?Fr=E9d=E9ric?= Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237796808.7527.40.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 19f61dd23219..fdab30d6c835 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -585,6 +585,7 @@ subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	}
 
 	if (filter_add_subsystem_pred(system, pred)) {
+		filter_free_subsystem_preds(system);
 		filter_free_pred(pred);
 		return -EINVAL;
 	}

commit 9bd7d099ab3f10dd666da399c064999bae427cd9
Author: Frederic Weisbecker <fweisbec@gmail.com>
Date:   Sun Mar 22 23:10:43 2009 +0100

    tracing/events: make the filter files writable
    
    We need the filter files to be writable, the current
    filter file permissions are only set readable.
    
    Signed-off-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <1237759847-21025-1-git-send-email-fweisbec@gmail.com>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 594d78aaa185..19f61dd23219 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -706,7 +706,7 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 
 	system->preds = NULL;
 
-	entry = debugfs_create_file("filter", 0444, system->entry, system,
+	entry = debugfs_create_file("filter", 0644, system->entry, system,
 				    &ftrace_subsystem_filter_fops);
 	if (!entry)
 		pr_warning("Could not create debugfs "
@@ -769,7 +769,7 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 		}
 	}
 
-	entry = debugfs_create_file("filter", 0444, call->dir, call,
+	entry = debugfs_create_file("filter", 0644, call->dir, call,
 				    &ftrace_event_filter_fops);
 	if (!entry)
 		pr_warning("Could not create debugfs "

commit fe9f57f250ab4d781b99504caeb218ca2db14c1a
Author: Ingo Molnar <mingo@elte.hu>
Date:   Sun Mar 22 18:41:59 2009 +0100

    tracing: add run-time field descriptions for event filtering, kfree fix
    
    Impact: fix potential kfree of random data in (rare) failure path
    
    Zero-initialize the field structure.
    
    Reported-by: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Tom Zanussi <tzanussi@gmail.com>
    LKML-Reference: <1237710639.7703.46.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 97d4daaddd9a..594d78aaa185 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -24,26 +24,31 @@ int trace_define_field(struct ftrace_event_call *call, char *type,
 {
 	struct ftrace_event_field *field;
 
-	field = kmalloc(sizeof(*field), GFP_KERNEL);
+	field = kzalloc(sizeof(*field), GFP_KERNEL);
 	if (!field)
 		goto err;
+
 	field->name = kstrdup(name, GFP_KERNEL);
 	if (!field->name)
 		goto err;
+
 	field->type = kstrdup(type, GFP_KERNEL);
 	if (!field->type)
 		goto err;
+
 	field->offset = offset;
 	field->size = size;
 	list_add(&field->link, &call->fields);
 
 	return 0;
+
 err:
 	if (field) {
 		kfree(field->name);
 		kfree(field->type);
 	}
 	kfree(field);
+
 	return -ENOMEM;
 }
 

commit cfb180f3e71b2a280a254c8646a9ab1beab63f84
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Sun Mar 22 03:31:17 2009 -0500

    tracing: add per-subsystem filtering
    
    This patch adds per-subsystem filtering to the event tracing subsystem.
    
    It adds a 'filter' debugfs file to each subsystem directory.  This file
    can be written to to set filters; reading from it will display the
    current set of filters set for that subsystem.
    
    Basically what it does is propagate the filter down to each event
    contained in the subsystem.  If a particular event doesn't have a field
    with the name specified in the filter, it simply doesn't get set for
    that event.  You can verify whether or not the filter was set for a
    particular event by looking at the filter file for that event.
    
    As with per-event filters, compound expressions are supported, echoing
    '0' to the subsystem's filter file clears all filters in the subsystem,
    etc.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237710677.7703.49.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 97470c48956e..97d4daaddd9a 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -524,6 +524,71 @@ event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return cnt;
 }
 
+static ssize_t
+subsystem_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
+		      loff_t *ppos)
+{
+	struct event_subsystem *system = filp->private_data;
+	struct trace_seq *s;
+	int r;
+
+	if (*ppos)
+		return 0;
+
+	s = kmalloc(sizeof(*s), GFP_KERNEL);
+	if (!s)
+		return -ENOMEM;
+
+	trace_seq_init(s);
+
+	r = filter_print_preds(system->preds, s->buffer);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, r);
+
+	kfree(s);
+
+	return r;
+}
+
+static ssize_t
+subsystem_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
+		       loff_t *ppos)
+{
+	struct event_subsystem *system = filp->private_data;
+	char buf[64], *pbuf = buf;
+	struct filter_pred *pred;
+	int err;
+
+	if (cnt >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	pred = kzalloc(sizeof(*pred), GFP_KERNEL);
+	if (!pred)
+		return -ENOMEM;
+
+	err = filter_parse(&pbuf, pred);
+	if (err < 0) {
+		filter_free_pred(pred);
+		return err;
+	}
+
+	if (pred->clear) {
+		filter_free_subsystem_preds(system);
+		return cnt;
+	}
+
+	if (filter_add_subsystem_pred(system, pred)) {
+		filter_free_pred(pred);
+		return -EINVAL;
+	}
+
+	*ppos += cnt;
+
+	return cnt;
+}
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -575,6 +640,12 @@ static const struct file_operations ftrace_event_filter_fops = {
 	.write = event_filter_write,
 };
 
+static const struct file_operations ftrace_subsystem_filter_fops = {
+	.open = tracing_open_generic,
+	.read = subsystem_filter_read,
+	.write = subsystem_filter_write,
+};
+
 static struct dentry *event_trace_events_dir(void)
 {
 	static struct dentry *d_tracer;
@@ -595,18 +666,13 @@ static struct dentry *event_trace_events_dir(void)
 	return d_events;
 }
 
-struct event_subsystem {
-	struct list_head	list;
-	const char		*name;
-	struct dentry		*entry;
-};
-
 static LIST_HEAD(event_subsystems);
 
 static struct dentry *
 event_subsystem_dir(const char *name, struct dentry *d_events)
 {
 	struct event_subsystem *system;
+	struct dentry *entry;
 
 	/* First see if we did not already create this dir */
 	list_for_each_entry(system, &event_subsystems, list) {
@@ -633,6 +699,14 @@ event_subsystem_dir(const char *name, struct dentry *d_events)
 	system->name = name;
 	list_add(&system->list, &event_subsystems);
 
+	system->preds = NULL;
+
+	entry = debugfs_create_file("filter", 0444, system->entry, system,
+				    &ftrace_subsystem_filter_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'%s/filter' entry\n", name);
+
 	return system->entry;
 }
 

commit 7ce7e4249921d5073e764f7ff7ad83cfa9894bd7
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Sun Mar 22 03:31:04 2009 -0500

    tracing: add per-event filtering
    
    This patch adds per-event filtering to the event tracing subsystem.
    
    It adds a 'filter' debugfs file to each event directory.  This file can
    be written to to set filters; reading from it will display the current
    set of filters set for that event.
    
    Basically, any field listed in the 'format' file for an event can be
    filtered on (including strings, but not yet other array types) using
    either matching ('==') or non-matching ('!=') 'predicates'.  A
    'predicate' can be either a single expression:
    
     # echo pid != 0 > filter
    
     # cat filter
     pid != 0
    
    or a compound expression of up to 8 sub-expressions combined using '&&'
    or '||':
    
     # echo comm == Xorg > filter
     # echo "&& sig != 29" > filter
    
     # cat filter
     comm == Xorg
     && sig != 29
    
    Only events having field values matching an expression will be available
    in the trace output; non-matching events are discarded.
    
    Note that a compound expression is built up by echoing each
    sub-expression separately - it's not the most efficient way to do
    things, but it keeps the parser simple and assumes that compound
    expressions will be relatively uncommon.  In any case, a subsequent
    patch introducing a way to set filters for entire subsystems should
    mitigate any need to do this for lots of events.
    
    Setting a filter without an '&&' or '||' clears the previous filter
    completely and sets the filter to the new expression:
    
     # cat filter
     comm == Xorg
     && sig != 29
    
     # echo comm != Xorg
    
     # cat filter
     comm != Xorg
    
    To clear a filter, echo 0 to the filter file:
    
     # echo 0 > filter
     # cat filter
     none
    
    The limit of 8 predicates for a compound expression is arbitrary - for
    efficiency, it's implemented as an array of pointers to predicates, and
    8 seemed more than enough for any filter...
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237710665.7703.48.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 961b057da28b..97470c48956e 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -459,6 +459,71 @@ event_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
 	return r;
 }
 
+static ssize_t
+event_filter_read(struct file *filp, char __user *ubuf, size_t cnt,
+		  loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	struct trace_seq *s;
+	int r;
+
+	if (*ppos)
+		return 0;
+
+	s = kmalloc(sizeof(*s), GFP_KERNEL);
+	if (!s)
+		return -ENOMEM;
+
+	trace_seq_init(s);
+
+	r = filter_print_preds(call->preds, s->buffer);
+	r = simple_read_from_buffer(ubuf, cnt, ppos, s->buffer, r);
+
+	kfree(s);
+
+	return r;
+}
+
+static ssize_t
+event_filter_write(struct file *filp, const char __user *ubuf, size_t cnt,
+		   loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	char buf[64], *pbuf = buf;
+	struct filter_pred *pred;
+	int err;
+
+	if (cnt >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	pred = kzalloc(sizeof(*pred), GFP_KERNEL);
+	if (!pred)
+		return -ENOMEM;
+
+	err = filter_parse(&pbuf, pred);
+	if (err < 0) {
+		filter_free_pred(pred);
+		return err;
+	}
+
+	if (pred->clear) {
+		filter_free_preds(call);
+		return cnt;
+	}
+
+	if (filter_add_pred(call, pred)) {
+		filter_free_pred(pred);
+		return -EINVAL;
+	}
+
+	*ppos += cnt;
+
+	return cnt;
+}
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -504,6 +569,12 @@ static const struct file_operations ftrace_event_id_fops = {
 	.read = event_id_read,
 };
 
+static const struct file_operations ftrace_event_filter_fops = {
+	.open = tracing_open_generic,
+	.read = event_filter_read,
+	.write = event_filter_write,
+};
+
 static struct dentry *event_trace_events_dir(void)
 {
 	static struct dentry *d_tracer;
@@ -619,6 +690,12 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 		}
 	}
 
+	entry = debugfs_create_file("filter", 0444, call->dir, call,
+				    &ftrace_event_filter_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'%s/filter' entry\n", call->name);
+
 	/* A trace may not want to export its format */
 	if (!call->show_format)
 		return 0;

commit cf027f645e6aee4f0ca6197a6b6a57f327fdb13f
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Sun Mar 22 03:30:39 2009 -0500

    tracing: add run-time field descriptions for event filtering
    
    This patch makes the field descriptions defined for event tracing
    available at run-time, for the event-filtering mechanism introduced
    in a subsequent patch.
    
    The common event fields are prepended with 'common_' in the format
    display, allowing them to be distinguished from the other fields
    that might internally have same name and can therefore be
    unambiguously used in filters.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Acked-by: Frederic Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237710639.7703.46.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 3047b56f6637..961b057da28b 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -19,6 +19,34 @@
 
 static DEFINE_MUTEX(event_mutex);
 
+int trace_define_field(struct ftrace_event_call *call, char *type,
+		       char *name, int offset, int size)
+{
+	struct ftrace_event_field *field;
+
+	field = kmalloc(sizeof(*field), GFP_KERNEL);
+	if (!field)
+		goto err;
+	field->name = kstrdup(name, GFP_KERNEL);
+	if (!field->name)
+		goto err;
+	field->type = kstrdup(type, GFP_KERNEL);
+	if (!field->type)
+		goto err;
+	field->offset = offset;
+	field->size = size;
+	list_add(&field->link, &call->fields);
+
+	return 0;
+err:
+	if (field) {
+		kfree(field->name);
+		kfree(field->type);
+	}
+	kfree(field);
+	return -ENOMEM;
+}
+
 static void ftrace_clear_events(void)
 {
 	struct ftrace_event_call *call = (void *)__start_ftrace_events;
@@ -343,7 +371,8 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 #undef FIELD
 #define FIELD(type, name)						\
-	#type, #name, offsetof(typeof(field), name), sizeof(field.name)
+	#type, "common_" #name, offsetof(typeof(field), name),		\
+		sizeof(field.name)
 
 static int trace_write_header(struct trace_seq *s)
 {
@@ -581,6 +610,15 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 					call->name);
 	}
 
+	if (call->define_fields) {
+		ret = call->define_fields();
+		if (ret < 0) {
+			pr_warning("Could not initialize trace point"
+				   " events/%s\n", call->name);
+			return ret;
+		}
+	}
+
 	/* A trace may not want to export its format */
 	if (!call->show_format)
 		return 0;

commit ac199db0189c091f2863312061c0575937f68810
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 19 20:26:15 2009 +0100

    ftrace: event profile hooks
    
    Impact: new tracing infrastructure feature
    
    Provide infrastructure to generate software perf counter events
    from tracepoints.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20090319194233.557364871@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 7763db8fd0b3..3047b56f6637 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -19,11 +19,6 @@
 
 static DEFINE_MUTEX(event_mutex);
 
-#define events_for_each(event)						\
-	for (event = __start_ftrace_events;				\
-	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
-	     event++)
-
 static void ftrace_clear_events(void)
 {
 	struct ftrace_event_call *call = (void *)__start_ftrace_events;
@@ -90,7 +85,7 @@ static int ftrace_set_clr_event(char *buf, int set)
 	}
 
 	mutex_lock(&event_mutex);
-	events_for_each(call) {
+	for_each_event(call) {
 
 		if (!call->name || !call->regfunc)
 			continue;
@@ -628,7 +623,7 @@ static __init int event_trace_init(void)
 	if (!d_events)
 		return 0;
 
-	events_for_each(call) {
+	for_each_event(call) {
 		/* The linker may leave blanks */
 		if (!call->name)
 			continue;

commit 23725aeeab10ba02bcf10ec49ad73146b54cb52f
Author: Peter Zijlstra <a.p.zijlstra@chello.nl>
Date:   Thu Mar 19 20:26:13 2009 +0100

    ftrace: provide an id file for each event
    
    Since not every event has a format file to read the id from,
    expose it explicitly in a separate file.
    
    Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Paul Mackerras <paulus@samba.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    LKML-Reference: <20090319194233.372534033@chello.nl>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index c88227b3b9db..7763db8fd0b3 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -412,6 +412,29 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 	return r;
 }
 
+static ssize_t
+event_id_read(struct file *filp, char __user *ubuf, size_t cnt, loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	struct trace_seq *s;
+	int r;
+
+	if (*ppos)
+		return 0;
+
+	s = kmalloc(sizeof(*s), GFP_KERNEL);
+	if (!s)
+		return -ENOMEM;
+
+	trace_seq_init(s);
+	trace_seq_printf(s, "%d\n", call->id);
+
+	r = simple_read_from_buffer(ubuf, cnt, ppos,
+				    s->buffer, s->len);
+	kfree(s);
+	return r;
+}
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -452,6 +475,11 @@ static const struct file_operations ftrace_event_format_fops = {
 	.read = event_format_read,
 };
 
+static const struct file_operations ftrace_event_id_fops = {
+	.open = tracing_open_generic,
+	.read = event_id_read,
+};
+
 static struct dentry *event_trace_events_dir(void)
 {
 	static struct dentry *d_tracer;
@@ -550,6 +578,14 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 				   "'%s/enable' entry\n", call->name);
 	}
 
+	if (call->id) {
+		entry = debugfs_create_file("id", 0444, call->dir, call,
+				&ftrace_event_id_fops);
+		if (!entry)
+			pr_warning("Could not create debugfs '%s/id' entry\n",
+					call->name);
+	}
+
 	/* A trace may not want to export its format */
 	if (!call->show_format)
 		return 0;

commit c269fc8c537d761f36cb98e637ae934d9331a9d5
Author: Tom Zanussi <tzanussi@gmail.com>
Date:   Tue Mar 17 01:20:59 2009 -0500

    tracing: fix leak in event_format_read()
    
    Impact: fix memory leak
    
    If event_format_read() exits early due to nonzero ppos, the
    previous kmalloc doesn't get freed - might as well do the
    check before the kmalloc and avoid the problem.
    
    Signed-off-by: Tom Zanussi <tzanussi@gmail.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: =?ISO-8859-1?Q?Fr=E9d=E9ric?= Weisbecker <fweisbec@gmail.com>
    LKML-Reference: <1237270859.8033.141.camel@charm-linux>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 238ea95a4115..c88227b3b9db 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -378,15 +378,15 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 	char *buf;
 	int r;
 
+	if (*ppos)
+		return 0;
+
 	s = kmalloc(sizeof(*s), GFP_KERNEL);
 	if (!s)
 		return -ENOMEM;
 
 	trace_seq_init(s);
 
-	if (*ppos)
-		return 0;
-
 	/* If any of the first writes fail, so will the show_format. */
 
 	trace_seq_printf(s, "name: %s\n", call->name);

commit e9fb2b6d5845e24f104713591286b6f39761c027
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 12 14:19:25 2009 -0400

    tracing: have event_trace_printk use static tracer
    
    Impact: speed up on event tracing
    
    The event_trace_printk is currently a wrapper function that calls
    trace_vprintk. Because it uses a variable for the fmt it misses out
    on the optimization of using the binary printk.
    
    This patch makes event_trace_printk into a macro wrapper to use the
    fmt as the same as the trace_printks.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index ca624df73591..238ea95a4115 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -24,16 +24,6 @@ static DEFINE_MUTEX(event_mutex);
 	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
 	     event++)
 
-void event_trace_printk(unsigned long ip, const char *fmt, ...)
-{
-	va_list ap;
-
-	va_start(ap, fmt);
-	tracing_record_cmdline(current);
-	trace_vprintk(ip, task_curr_ret_stack(current), fmt, ap);
-	va_end(ap);
-}
-
 static void ftrace_clear_events(void)
 {
 	struct ftrace_event_call *call = (void *)__start_ftrace_events;

commit 1852fcce181faa237c010a3dbedb473cf9d4555f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Mar 11 14:33:00 2009 -0400

    tracing: expand the ring buffers when an event is activated
    
    To save memory, the tracer ring buffers are set to a minimum.
    The activating of a trace expands the ring buffer size. This patch
    adds this expanding, when an event is activated.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 769dfd00fc85..ca624df73591 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -141,6 +141,10 @@ ftrace_event_write(struct file *file, const char __user *ubuf,
 	if (!cnt || cnt < 0)
 		return 0;
 
+	ret = tracing_update_buffers();
+	if (ret < 0)
+		return ret;
+
 	ret = get_user(ch, ubuf++);
 	if (ret)
 		return ret;
@@ -331,6 +335,10 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	if (ret < 0)
 		return ret;
 
+	ret = tracing_update_buffers();
+	if (ret < 0)
+		return ret;
+
 	switch (val) {
 	case 0:
 	case 1:

commit 2314c4ae1461c9e8b26cf8b9a851f280bc5769e1
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 10 12:04:02 2009 -0400

    tracing: add back the available_events file
    
    The event directory files type and available_types were no longer
    needed with the new TRACE_EVENT_FORMAT macros, they were deleted.
    But by accident the available_events file was also removed.
    This patch brings it back.
    
    Reported-by: KOSAKI Motohiro <kosaki.motohiro@jp.fujitsu.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 85ec10fbb38d..769dfd00fc85 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -428,6 +428,13 @@ static const struct seq_operations show_set_event_seq_ops = {
 	.stop = t_stop,
 };
 
+static const struct file_operations ftrace_avail_fops = {
+	.open = ftrace_event_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
 static const struct file_operations ftrace_set_event_fops = {
 	.open = ftrace_event_seq_open,
 	.read = seq_read,
@@ -569,6 +576,13 @@ static __init int event_trace_init(void)
 	if (!d_tracer)
 		return 0;
 
+	entry = debugfs_create_file("available_events", 0444, d_tracer,
+				    (void *)&show_event_seq_ops,
+				    &ftrace_avail_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'available_events' entry\n");
+
 	entry = debugfs_create_file("set_event", 0644, d_tracer,
 				    (void *)&show_set_event_seq_ops,
 				    &ftrace_set_event_fops);

commit 40e26815fafd3b8c4aced17b1f22e68ef33eb8db
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 10 11:32:40 2009 -0400

    tracing: do not allow modifying the ftrace events via the event files
    
    Impact: fix to prevent crash on calling NULL function pointer
    
    The ftrace internal records have their format exported via the event
    system under the ftrace subsystem. These are only for exporting the
    format to allow binary readers to be able to parse them in a binary
    output.
    
    The ftrace subsystem events can only be enabled via the ftrace tracers
    and do not have a registering function. The event files expect the
    event record to have registering function and will call it directly.
    Passing in a ftrace subsystem event will cause the kernel to crash
    because it will execute a NULL pointer.
    
    This patch prevents the ftrace subsystem from being viewable to the
    event enabling files.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index a0b41cc26f26..85ec10fbb38d 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -102,7 +102,7 @@ static int ftrace_set_clr_event(char *buf, int set)
 	mutex_lock(&event_mutex);
 	events_for_each(call) {
 
-		if (!call->name)
+		if (!call->name || !call->regfunc)
 			continue;
 
 		if (match &&
@@ -207,8 +207,20 @@ t_next(struct seq_file *m, void *v, loff_t *pos)
 
 	(*pos)++;
 
-	if ((unsigned long)call >= (unsigned long)__stop_ftrace_events)
-		return NULL;
+	for (;;) {
+		if ((unsigned long)call >= (unsigned long)__stop_ftrace_events)
+			return NULL;
+
+		/*
+		 * The ftrace subsystem is for showing formats only.
+		 * They can not be enabled or disabled via the event files.
+		 */
+		if (call->regfunc)
+			break;
+
+		call++;
+		next = call;
+	}
 
 	m->private = ++next;
 

commit ce8eb2bf05042452107e489782105d2e235cbdd0
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Mar 10 10:14:35 2009 -0400

    tracing: fix printk format specifier
    
    Impact: clean up
    
    The offsetof and sizeof are of type size_t, and instead of typecasting
    them to unsigned int for printk formatting, one could just use %zu.
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1880a6438097..a0b41cc26f26 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -338,8 +338,7 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 #undef FIELD
 #define FIELD(type, name)						\
-	#type, #name, (unsigned int)offsetof(typeof(field), name),	\
-		(unsigned int)sizeof(field.name)
+	#type, #name, offsetof(typeof(field), name), sizeof(field.name)
 
 static int trace_write_header(struct trace_seq *s)
 {
@@ -347,11 +346,11 @@ static int trace_write_header(struct trace_seq *s)
 
 	/* struct trace_entry */
 	return trace_seq_printf(s,
-				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
-				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
-				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
-				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
-				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
+				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
+				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
+				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
+				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
+				"\tfield:%s %s;\toffset:%zu;\tsize:%zu;\n"
 				"\n",
 				FIELD(unsigned char, type),
 				FIELD(unsigned char, flags),

commit da4d03020c2af32f73e8bfbab0a66620d85bb9bb
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 9 17:14:30 2009 -0400

    tracing: new format for specialized trace points
    
    Impact: clean up and enhancement
    
    The TRACE_EVENT_FORMAT macro looks quite ugly and is limited in its
    ability to save data as well as to print the record out. Working with
    Ingo Molnar, we came up with a new format that is much more pleasing to
    the eye of C developers. This new macro is more C style than the old
    macro, and is more obvious to what it does.
    
    Here's the example. The only updated macro in this patch is the
    sched_switch trace point.
    
    The old method looked like this:
    
     TRACE_EVENT_FORMAT(sched_switch,
            TP_PROTO(struct rq *rq, struct task_struct *prev,
                    struct task_struct *next),
            TP_ARGS(rq, prev, next),
            TP_FMT("task %s:%d ==> %s:%d",
                  prev->comm, prev->pid, next->comm, next->pid),
            TRACE_STRUCT(
                    TRACE_FIELD(pid_t, prev_pid, prev->pid)
                    TRACE_FIELD(int, prev_prio, prev->prio)
                    TRACE_FIELD_SPECIAL(char next_comm[TASK_COMM_LEN],
                                        next_comm,
                                        TP_CMD(memcpy(TRACE_ENTRY->next_comm,
                                                     next->comm,
                                                     TASK_COMM_LEN)))
                    TRACE_FIELD(pid_t, next_pid, next->pid)
                    TRACE_FIELD(int, next_prio, next->prio)
            ),
            TP_RAW_FMT("prev %d:%d ==> next %s:%d:%d")
            );
    
    The above method is hard to read and requires two format fields.
    
    The new method:
    
     /*
      * Tracepoint for task switches, performed by the scheduler:
      *
      * (NOTE: the 'rq' argument is not used by generic trace events,
      *        but used by the latency tracer plugin. )
      */
     TRACE_EVENT(sched_switch,
    
            TP_PROTO(struct rq *rq, struct task_struct *prev,
                     struct task_struct *next),
    
            TP_ARGS(rq, prev, next),
    
            TP_STRUCT__entry(
                    __array(        char,   prev_comm,      TASK_COMM_LEN   )
                    __field(        pid_t,  prev_pid                        )
                    __field(        int,    prev_prio                       )
                    __array(        char,   next_comm,      TASK_COMM_LEN   )
                    __field(        pid_t,  next_pid                        )
                    __field(        int,    next_prio                       )
            ),
    
            TP_printk("task %s:%d [%d] ==> %s:%d [%d]",
                    __entry->prev_comm, __entry->prev_pid, __entry->prev_prio,
                    __entry->next_comm, __entry->next_pid, __entry->next_prio),
    
            TP_fast_assign(
                    memcpy(__entry->next_comm, next->comm, TASK_COMM_LEN);
                    __entry->prev_pid       = prev->pid;
                    __entry->prev_prio      = prev->prio;
                    memcpy(__entry->prev_comm, prev->comm, TASK_COMM_LEN);
                    __entry->next_pid       = next->pid;
                    __entry->next_prio      = next->prio;
            )
     );
    
    This macro is called TRACE_EVENT, it is broken up into 5 parts:
    
     TP_PROTO:        the proto type of the trace point
     TP_ARGS:         the arguments of the trace point
     TP_STRUCT_entry: the structure layout of the entry in the ring buffer
     TP_printk:       the printk format
     TP_fast_assign:  the method used to write the entry into the ring buffer
    
    The structure is the definition of how the event will be saved in the
    ring buffer. The printk is used by the internal tracing in case of
    an oops, and the kernel needs to print out the format of the record
    to the console. This the TP_printk gives a means to show the records
    in a human readable format. It is also used to print out the data
    from the trace file.
    
    The TP_fast_assign is executed directly. It is basically like a C function,
    where the __entry is the handle to the record.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index fa32ca320767..1880a6438097 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -59,22 +59,12 @@ static void ftrace_event_enable_disable(struct ftrace_event_call *call,
 			call->enabled = 0;
 			call->unregfunc();
 		}
-		if (call->raw_enabled) {
-			call->raw_enabled = 0;
-			call->raw_unreg();
-		}
 		break;
 	case 1:
-		if (!call->enabled &&
-		    (call->type & TRACE_EVENT_TYPE_PRINTF)) {
+		if (!call->enabled) {
 			call->enabled = 1;
 			call->regfunc();
 		}
-		if (!call->raw_enabled &&
-		    (call->type & TRACE_EVENT_TYPE_RAW)) {
-			call->raw_enabled = 1;
-			call->raw_reg();
-		}
 		break;
 	}
 }
@@ -300,7 +290,7 @@ event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	struct ftrace_event_call *call = filp->private_data;
 	char *buf;
 
-	if (call->enabled || call->raw_enabled)
+	if (call->enabled)
 		buf = "1\n";
 	else
 		buf = "0\n";
@@ -346,107 +336,6 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return cnt;
 }
 
-static ssize_t
-event_type_read(struct file *filp, char __user *ubuf, size_t cnt,
-		loff_t *ppos)
-{
-	struct ftrace_event_call *call = filp->private_data;
-	char buf[16];
-	int r = 0;
-
-	if (call->type & TRACE_EVENT_TYPE_PRINTF)
-		r += sprintf(buf, "printf\n");
-
-	if (call->type & TRACE_EVENT_TYPE_RAW)
-		r += sprintf(buf+r, "raw\n");
-
-	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
-}
-
-static ssize_t
-event_type_write(struct file *filp, const char __user *ubuf, size_t cnt,
-		 loff_t *ppos)
-{
-	struct ftrace_event_call *call = filp->private_data;
-	char buf[64];
-
-	/*
-	 * If there's only one type, we can't change it.
-	 * And currently we always have printf type, and we
-	 * may or may not have raw type.
-	 *
-	 * This is a redundant check, the file should be read
-	 * only if this is the case anyway.
-	 */
-
-	if (!call->raw_init)
-		return -EPERM;
-
-	if (cnt >= sizeof(buf))
-		return -EINVAL;
-
-	if (copy_from_user(&buf, ubuf, cnt))
-		return -EFAULT;
-
-	buf[cnt] = 0;
-
-	if (!strncmp(buf, "printf", 6) &&
-	    (!buf[6] || isspace(buf[6]))) {
-
-		call->type = TRACE_EVENT_TYPE_PRINTF;
-
-		/*
-		 * If raw enabled, the disable it and enable
-		 * printf type.
-		 */
-		if (call->raw_enabled) {
-			call->raw_enabled = 0;
-			call->raw_unreg();
-
-			call->enabled = 1;
-			call->regfunc();
-		}
-
-	} else if (!strncmp(buf, "raw", 3) &&
-	    (!buf[3] || isspace(buf[3]))) {
-
-		call->type = TRACE_EVENT_TYPE_RAW;
-
-		/*
-		 * If printf enabled, the disable it and enable
-		 * raw type.
-		 */
-		if (call->enabled) {
-			call->enabled = 0;
-			call->unregfunc();
-
-			call->raw_enabled = 1;
-			call->raw_reg();
-		}
-	} else
-		return -EINVAL;
-
-	*ppos += cnt;
-
-	return cnt;
-}
-
-static ssize_t
-event_available_types_read(struct file *filp, char __user *ubuf, size_t cnt,
-			   loff_t *ppos)
-{
-	struct ftrace_event_call *call = filp->private_data;
-	char buf[16];
-	int r = 0;
-
-	r += sprintf(buf, "printf\n");
-
-	if (call->raw_init)
-		r += sprintf(buf+r, "raw\n");
-
-	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
-}
-
 #undef FIELD
 #define FIELD(type, name)						\
 	#type, #name, (unsigned int)offsetof(typeof(field), name),	\
@@ -470,6 +359,7 @@ static int trace_write_header(struct trace_seq *s)
 				FIELD(int, pid),
 				FIELD(int, tgid));
 }
+
 static ssize_t
 event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
@@ -527,13 +417,6 @@ static const struct seq_operations show_set_event_seq_ops = {
 	.stop = t_stop,
 };
 
-static const struct file_operations ftrace_avail_fops = {
-	.open = ftrace_event_seq_open,
-	.read = seq_read,
-	.llseek = seq_lseek,
-	.release = seq_release,
-};
-
 static const struct file_operations ftrace_set_event_fops = {
 	.open = ftrace_event_seq_open,
 	.read = seq_read,
@@ -548,17 +431,6 @@ static const struct file_operations ftrace_enable_fops = {
 	.write = event_enable_write,
 };
 
-static const struct file_operations ftrace_type_fops = {
-	.open = tracing_open_generic,
-	.read = event_type_read,
-	.write = event_type_write,
-};
-
-static const struct file_operations ftrace_available_types_fops = {
-	.open = tracing_open_generic,
-	.read = event_available_types_read,
-};
-
 static const struct file_operations ftrace_event_format_fops = {
 	.open = tracing_open_generic,
 	.read = event_format_read,
@@ -647,9 +519,6 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 		}
 	}
 
-	/* default the output to printf */
-	call->type = TRACE_EVENT_TYPE_PRINTF;
-
 	call->dir = debugfs_create_dir(call->name, d_events);
 	if (!call->dir) {
 		pr_warning("Could not create debugfs "
@@ -665,21 +534,6 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 				   "'%s/enable' entry\n", call->name);
 	}
 
-	/* Only let type be writable, if we can change it */
-	entry = debugfs_create_file("type",
-				    call->raw_init ? 0644 : 0444,
-				    call->dir, call,
-				    &ftrace_type_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'%s/type' entry\n", call->name);
-
-	entry = debugfs_create_file("available_types", 0444, call->dir, call,
-				    &ftrace_available_types_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'%s/available_types' entry\n", call->name);
-
 	/* A trace may not want to export its format */
 	if (!call->show_format)
 		return 0;
@@ -704,13 +558,6 @@ static __init int event_trace_init(void)
 	if (!d_tracer)
 		return 0;
 
-	entry = debugfs_create_file("available_events", 0444, d_tracer,
-				    (void *)&show_event_seq_ops,
-				    &ftrace_avail_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'available_events' entry\n");
-
 	entry = debugfs_create_file("set_event", 0644, d_tracer,
 				    (void *)&show_set_event_seq_ops,
 				    &ftrace_set_event_fops);

commit 156b5f172a64103bcb13b6d26288388b9019caa3
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Mar 6 10:50:53 2009 -0500

    tracing: typecast sizeof and offsetof to unsigned int
    
    Impact: fix compiler warnings
    
    On x86_64 sizeof and offsetof are treated as long, where as on x86_32
    they are int. This patch typecasts them to unsigned int to avoid
    one arch giving warnings while the other does not.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 4488d90e75ef..fa32ca320767 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -448,8 +448,9 @@ event_available_types_read(struct file *filp, char __user *ubuf, size_t cnt,
 }
 
 #undef FIELD
-#define FIELD(type, name) \
-	#type, #name, offsetof(typeof(field), name), sizeof(field.name)
+#define FIELD(type, name)						\
+	#type, #name, (unsigned int)offsetof(typeof(field), name),	\
+		(unsigned int)sizeof(field.name)
 
 static int trace_write_header(struct trace_seq *s)
 {
@@ -457,11 +458,11 @@ static int trace_write_header(struct trace_seq *s)
 
 	/* struct trace_entry */
 	return trace_seq_printf(s,
-				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
-				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
-				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
-				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
-				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
+				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
+				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
+				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
+				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
+				"\tfield:%s %s;\toffset:%u;\tsize:%u;\n"
 				"\n",
 				FIELD(unsigned char, type),
 				FIELD(unsigned char, flags),

commit 770cb24345c0f6e0d47bd2b94aa6d67bea6f8b54
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu Mar 5 21:35:29 2009 -0500

    tracing: add format files for ftrace default entries
    
    Impact: allow user apps to read binary format of basic ftrace entries
    
    Currently, only defined raw events export their formats so a binary
    reader can parse them. There's no reason that the default ftrace entries
    can't export their formats.
    
    This patch adds a subsystem called "ftrace" in the events directory
    that includes the ftrace entries for basic ftrace recorded items.
    
    These only have three files in the events directory:
    
     type             : printf
     available_types  : printf
     format           : format for the event entry
    
    For example:
    
     # cat /debug/tracing/events/ftrace/wakeup/format
    name: wakeup
    ID: 3
    format:
            field:unsigned char type;       offset:0;       size:1;
            field:unsigned char flags;      offset:1;       size:1;
            field:unsigned char preempt_count;      offset:2;       size:1;
            field:int pid;  offset:4;       size:4;
            field:int tgid; offset:8;       size:4;
    
            field:unsigned int prev_pid;    offset:12;      size:4;
            field:unsigned char prev_prio;  offset:16;      size:1;
            field:unsigned char prev_state; offset:17;      size:1;
            field:unsigned int next_pid;    offset:20;      size:4;
            field:unsigned char next_prio;  offset:24;      size:1;
            field:unsigned char next_state; offset:25;      size:1;
            field:unsigned int next_cpu;    offset:28;      size:4;
    
    print fmt: "%u:%u:%u  ==+ %u:%u:%u [%03u]"
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 210e71ff82db..4488d90e75ef 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -656,11 +656,13 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 		return -1;
 	}
 
-	entry = debugfs_create_file("enable", 0644, call->dir, call,
-				    &ftrace_enable_fops);
-	if (!entry)
-		pr_warning("Could not create debugfs "
-			   "'%s/enable' entry\n", call->name);
+	if (call->regfunc) {
+		entry = debugfs_create_file("enable", 0644, call->dir, call,
+					    &ftrace_enable_fops);
+		if (!entry)
+			pr_warning("Could not create debugfs "
+				   "'%s/enable' entry\n", call->name);
+	}
 
 	/* Only let type be writable, if we can change it */
 	entry = debugfs_create_file("type",

commit c5e4e19271edfdf1abd4184933d40d646da6a091
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 2 15:10:02 2009 -0500

    tracing: add trace name and id to event formats
    
    To be able to identify the trace in the binary format output, the
    id of the trace event (which is dynamically assigned) must also be listed.
    
    This patch adds the name of the trace point as well as the id assigned.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index cdcc3aed76fd..210e71ff82db 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -487,7 +487,11 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 	if (*ppos)
 		return 0;
 
-	/* If this fails, so will the show_format. */
+	/* If any of the first writes fail, so will the show_format. */
+
+	trace_seq_printf(s, "name: %s\n", call->name);
+	trace_seq_printf(s, "ID: %d\n", call->id);
+	trace_seq_printf(s, "format:\n");
 	trace_write_header(s);
 
 	r = call->show_format(s);

commit 91729ef96661bfa7dc53923746cd90b62d5495cc
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 2 15:03:01 2009 -0500

    tracing: add ftrace headers to event format files
    
    This patch includes the ftrace header to the event formats files:
    
     # cat /debug/tracing/events/sched/sched_switch/format
            field:unsigned char type;       offset:0;       size:1;
            field:unsigned char flags;      offset:1;       size:1;
            field:unsigned char preempt_count;      offset:2;       size:1;
            field:int pid;  offset:4;       size:4;
            field:int tgid; offset:8;       size:4;
    
            field:pid_t prev_pid;   offset:12;      size:4;
            field:int prev_prio;    offset:16;      size:4;
            field special:char next_comm[TASK_COMM_LEN];    offset:20;      size:16;
            field:pid_t next_pid;   offset:36;      size:4;
            field:int next_prio;    offset:40;      size:4;
    
    A blank line is used as a deliminator between the ftrace header and the
    trace point fields.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index d57a772981c1..cdcc3aed76fd 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -13,7 +13,7 @@
 #include <linux/module.h>
 #include <linux/ctype.h>
 
-#include "trace.h"
+#include "trace_output.h"
 
 #define TRACE_SYSTEM "TRACE_SYSTEM"
 
@@ -447,6 +447,28 @@ event_available_types_read(struct file *filp, char __user *ubuf, size_t cnt,
 	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
 }
 
+#undef FIELD
+#define FIELD(type, name) \
+	#type, #name, offsetof(typeof(field), name), sizeof(field.name)
+
+static int trace_write_header(struct trace_seq *s)
+{
+	struct trace_entry field;
+
+	/* struct trace_entry */
+	return trace_seq_printf(s,
+				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
+				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
+				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
+				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
+				"\tfield:%s %s;\toffset:%lu;\tsize:%lu;\n"
+				"\n",
+				FIELD(unsigned char, type),
+				FIELD(unsigned char, flags),
+				FIELD(unsigned char, preempt_count),
+				FIELD(int, pid),
+				FIELD(int, tgid));
+}
 static ssize_t
 event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 		  loff_t *ppos)
@@ -465,6 +487,9 @@ event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
 	if (*ppos)
 		return 0;
 
+	/* If this fails, so will the show_format. */
+	trace_write_header(s);
+
 	r = call->show_format(s);
 	if (!r) {
 		/*

commit 981d081ec8b958b7d962ee40d433581a55d40fc5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 2 13:53:59 2009 -0500

    tracing: add format file to describe event struct fields
    
    This patch adds the "format" file to the trace point event directory.
    This is based off of work by Tom Zanussi, in which a file is exported
    to be tread from user land such that a user space app may read the
    binary record stored in the ring buffer.
    
     # cat /debug/tracing/events/sched/sched_switch/format
            field:pid_t prev_pid;   offset:12;      size:4;
            field:int prev_prio;    offset:16;      size:4;
            field special:char next_comm[TASK_COMM_LEN];    offset:20;      size:16;
            field:pid_t next_pid;   offset:36;      size:4;
            field:int next_prio;    offset:40;      size:4;
    
    Idea-from: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 26069fa6b3b0..d57a772981c1 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -3,6 +3,9 @@
  *
  * Copyright (C) 2008 Red Hat Inc, Steven Rostedt <srostedt@redhat.com>
  *
+ *  - Added format output of fields of the trace point.
+ *    This was based off of work by Tom Zanussi <tzanussi@gmail.com>.
+ *
  */
 
 #include <linux/debugfs.h>
@@ -444,6 +447,42 @@ event_available_types_read(struct file *filp, char __user *ubuf, size_t cnt,
 	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
 }
 
+static ssize_t
+event_format_read(struct file *filp, char __user *ubuf, size_t cnt,
+		  loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	struct trace_seq *s;
+	char *buf;
+	int r;
+
+	s = kmalloc(sizeof(*s), GFP_KERNEL);
+	if (!s)
+		return -ENOMEM;
+
+	trace_seq_init(s);
+
+	if (*ppos)
+		return 0;
+
+	r = call->show_format(s);
+	if (!r) {
+		/*
+		 * ug!  The format output is bigger than a PAGE!!
+		 */
+		buf = "FORMAT TOO BIG\n";
+		r = simple_read_from_buffer(ubuf, cnt, ppos,
+					      buf, strlen(buf));
+		goto out;
+	}
+
+	r = simple_read_from_buffer(ubuf, cnt, ppos,
+				    s->buffer, s->len);
+ out:
+	kfree(s);
+	return r;
+}
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -490,6 +529,11 @@ static const struct file_operations ftrace_available_types_fops = {
 	.read = event_available_types_read,
 };
 
+static const struct file_operations ftrace_event_format_fops = {
+	.open = tracing_open_generic,
+	.read = event_format_read,
+};
+
 static struct dentry *event_trace_events_dir(void)
 {
 	static struct dentry *d_tracer;
@@ -602,7 +646,17 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 				    &ftrace_available_types_fops);
 	if (!entry)
 		pr_warning("Could not create debugfs "
-			   "'%s/type' available_types\n", call->name);
+			   "'%s/available_types' entry\n", call->name);
+
+	/* A trace may not want to export its format */
+	if (!call->show_format)
+		return 0;
+
+	entry = debugfs_create_file("format", 0444, call->dir, call,
+				    &ftrace_event_format_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'%s/format' entry\n", call->name);
 
 	return 0;
 }

commit 11a241a3302277db05561e01477528629d806c4e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Mon Mar 2 11:49:04 2009 -0500

    tracing: add protection around modify trace event fields
    
    The trace event objects are currently not proctected against
    reentrancy. This patch adds a mutex around the modifications of
    the trace event fields.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 1d07f800a9ce..26069fa6b3b0 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -14,6 +14,8 @@
 
 #define TRACE_SYSTEM "TRACE_SYSTEM"
 
+static DEFINE_MUTEX(event_mutex);
+
 #define events_for_each(event)						\
 	for (event = __start_ftrace_events;				\
 	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
@@ -104,6 +106,7 @@ static int ftrace_set_clr_event(char *buf, int set)
 			event = NULL;
 	}
 
+	mutex_lock(&event_mutex);
 	events_for_each(call) {
 
 		if (!call->name)
@@ -124,6 +127,8 @@ static int ftrace_set_clr_event(char *buf, int set)
 
 		ret = 0;
 	}
+	mutex_unlock(&event_mutex);
+
 	return ret;
 }
 
@@ -324,7 +329,9 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	switch (val) {
 	case 0:
 	case 1:
+		mutex_lock(&event_mutex);
 		ftrace_event_enable_disable(call, val);
+		mutex_unlock(&event_mutex);
 		break;
 
 	default:

commit fd99498989f3b3feeab89dcadf537138ba136d24
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Sat Feb 28 02:41:25 2009 -0500

    tracing: add raw fast tracing interface for trace events
    
    This patch adds the interface to enable the C style trace points.
    In the directory /debugfs/tracing/events/subsystem/event
    We now have three files:
    
     enable : values 0 or 1 to enable or disable the trace event.
    
     available_types: values 'raw' and 'printf' which indicate the tracing
           types available for the trace point. If a developer does not
           use the TRACE_EVENT_FORMAT macro and just uses the TRACE_FORMAT
           macro, then only 'printf' will be available. This file is
           read only.
    
     type: values 'raw' or 'printf'. This indicates which type of tracing
           is active for that trace point. 'printf' is the default and
           if 'raw' is not available, this file is read only.
    
     # echo raw > /debug/tracing/events/sched/sched_wakeup/type
     # echo 1 > /debug/tracing/events/sched/sched_wakeup/enable
    
     Will enable the C style tracing for the sched_wakeup trace point.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 77a5c02bd634..1d07f800a9ce 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -44,6 +44,36 @@ static void ftrace_clear_events(void)
 	}
 }
 
+static void ftrace_event_enable_disable(struct ftrace_event_call *call,
+					int enable)
+{
+
+	switch (enable) {
+	case 0:
+		if (call->enabled) {
+			call->enabled = 0;
+			call->unregfunc();
+		}
+		if (call->raw_enabled) {
+			call->raw_enabled = 0;
+			call->raw_unreg();
+		}
+		break;
+	case 1:
+		if (!call->enabled &&
+		    (call->type & TRACE_EVENT_TYPE_PRINTF)) {
+			call->enabled = 1;
+			call->regfunc();
+		}
+		if (!call->raw_enabled &&
+		    (call->type & TRACE_EVENT_TYPE_RAW)) {
+			call->raw_enabled = 1;
+			call->raw_reg();
+		}
+		break;
+	}
+}
+
 static int ftrace_set_clr_event(char *buf, int set)
 {
 	struct ftrace_event_call *call = __start_ftrace_events;
@@ -90,19 +120,8 @@ static int ftrace_set_clr_event(char *buf, int set)
 		if (event && strcmp(event, call->name) != 0)
 			continue;
 
-		if (set) {
-			/* Already set? */
-			if (call->enabled)
-				return 0;
-			call->enabled = 1;
-			call->regfunc();
-		} else {
-			/* Already cleared? */
-			if (!call->enabled)
-				return 0;
-			call->enabled = 0;
-			call->unregfunc();
-		}
+		ftrace_event_enable_disable(call, set);
+
 		ret = 0;
 	}
 	return ret;
@@ -273,7 +292,7 @@ event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
 	struct ftrace_event_call *call = filp->private_data;
 	char *buf;
 
-	if (call->enabled)
+	if (call->enabled || call->raw_enabled)
 		buf = "1\n";
 	else
 		buf = "0\n";
@@ -304,18 +323,8 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 
 	switch (val) {
 	case 0:
-		if (!call->enabled)
-			break;
-
-		call->enabled = 0;
-		call->unregfunc();
-		break;
 	case 1:
-		if (call->enabled)
-			break;
-
-		call->enabled = 1;
-		call->regfunc();
+		ftrace_event_enable_disable(call, val);
 		break;
 
 	default:
@@ -327,6 +336,107 @@ event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
 	return cnt;
 }
 
+static ssize_t
+event_type_read(struct file *filp, char __user *ubuf, size_t cnt,
+		loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	char buf[16];
+	int r = 0;
+
+	if (call->type & TRACE_EVENT_TYPE_PRINTF)
+		r += sprintf(buf, "printf\n");
+
+	if (call->type & TRACE_EVENT_TYPE_RAW)
+		r += sprintf(buf+r, "raw\n");
+
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
+}
+
+static ssize_t
+event_type_write(struct file *filp, const char __user *ubuf, size_t cnt,
+		 loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	char buf[64];
+
+	/*
+	 * If there's only one type, we can't change it.
+	 * And currently we always have printf type, and we
+	 * may or may not have raw type.
+	 *
+	 * This is a redundant check, the file should be read
+	 * only if this is the case anyway.
+	 */
+
+	if (!call->raw_init)
+		return -EPERM;
+
+	if (cnt >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	buf[cnt] = 0;
+
+	if (!strncmp(buf, "printf", 6) &&
+	    (!buf[6] || isspace(buf[6]))) {
+
+		call->type = TRACE_EVENT_TYPE_PRINTF;
+
+		/*
+		 * If raw enabled, the disable it and enable
+		 * printf type.
+		 */
+		if (call->raw_enabled) {
+			call->raw_enabled = 0;
+			call->raw_unreg();
+
+			call->enabled = 1;
+			call->regfunc();
+		}
+
+	} else if (!strncmp(buf, "raw", 3) &&
+	    (!buf[3] || isspace(buf[3]))) {
+
+		call->type = TRACE_EVENT_TYPE_RAW;
+
+		/*
+		 * If printf enabled, the disable it and enable
+		 * raw type.
+		 */
+		if (call->enabled) {
+			call->enabled = 0;
+			call->unregfunc();
+
+			call->raw_enabled = 1;
+			call->raw_reg();
+		}
+	} else
+		return -EINVAL;
+
+	*ppos += cnt;
+
+	return cnt;
+}
+
+static ssize_t
+event_available_types_read(struct file *filp, char __user *ubuf, size_t cnt,
+			   loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	char buf[16];
+	int r = 0;
+
+	r += sprintf(buf, "printf\n");
+
+	if (call->raw_init)
+		r += sprintf(buf+r, "raw\n");
+
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, r);
+}
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -362,6 +472,17 @@ static const struct file_operations ftrace_enable_fops = {
 	.write = event_enable_write,
 };
 
+static const struct file_operations ftrace_type_fops = {
+	.open = tracing_open_generic,
+	.read = event_type_read,
+	.write = event_type_write,
+};
+
+static const struct file_operations ftrace_available_types_fops = {
+	.open = tracing_open_generic,
+	.read = event_available_types_read,
+};
+
 static struct dentry *event_trace_events_dir(void)
 {
 	static struct dentry *d_tracer;
@@ -427,6 +548,7 @@ static int
 event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 {
 	struct dentry *entry;
+	int ret;
 
 	/*
 	 * If the trace point header did not define TRACE_SYSTEM
@@ -435,6 +557,18 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 	if (strcmp(call->system, "TRACE_SYSTEM") != 0)
 		d_events = event_subsystem_dir(call->system, d_events);
 
+	if (call->raw_init) {
+		ret = call->raw_init();
+		if (ret < 0) {
+			pr_warning("Could not initialize trace point"
+				   " events/%s\n", call->name);
+			return ret;
+		}
+	}
+
+	/* default the output to printf */
+	call->type = TRACE_EVENT_TYPE_PRINTF;
+
 	call->dir = debugfs_create_dir(call->name, d_events);
 	if (!call->dir) {
 		pr_warning("Could not create debugfs "
@@ -448,6 +582,21 @@ event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 		pr_warning("Could not create debugfs "
 			   "'%s/enable' entry\n", call->name);
 
+	/* Only let type be writable, if we can change it */
+	entry = debugfs_create_file("type",
+				    call->raw_init ? 0644 : 0444,
+				    call->dir, call,
+				    &ftrace_type_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'%s/type' entry\n", call->name);
+
+	entry = debugfs_create_file("available_types", 0444, call->dir, call,
+				    &ftrace_available_types_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'%s/type' available_types\n", call->name);
+
 	return 0;
 }
 

commit c32e827b25054cb17b79cf97fb5e63ae4ce2223c
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Feb 27 19:12:30 2009 -0500

    tracing: add raw trace point recording infrastructure
    
    Impact: lower overhead tracing
    
    The current event tracer can automatically pick up trace points
    that are registered with the TRACE_FORMAT macro. But it required
    a printf format string and parsing. Although, this adds the ability
    to get guaranteed information like task names and such, it took
    a hit in overhead processing. This processing can add about 500-1000
    nanoseconds overhead, but in some cases that too is considered
    too much and we want to shave off as much from this overhead as
    possible.
    
    Tom Zanussi recently posted tracing patches to lkml that are based
    on a nice idea about capturing the data via C structs using
    STRUCT_ENTER, STRUCT_EXIT type of macros.
    
    I liked that method very much, but did not like the implementation
    that required a developer to add data/code in several disjoint
    locations.
    
    This patch extends the event_tracer macros to do a similar "raw C"
    approach that Tom Zanussi did. But instead of having the developers
    needing to tweak a bunch of code all over the place, they can do it
    all in one macro - preferably placed near the code that it is
    tracing. That makes it much more likely that tracepoints will be
    maintained on an ongoing basis by the code they modify.
    
    The new macro TRACE_EVENT_FORMAT is created for this approach. (Note,
    a developer may still utilize the more low level DECLARE_TRACE macros
    if they don't care about getting their traces automatically in the event
    tracer.)
    
    They can also use the existing TRACE_FORMAT if they don't need to code
    the tracepoint in C, but just want to use the convenience of printf.
    
    So if the developer wants to "hardwire" a tracepoint in the fastest
    possible way, and wants to acquire their data via a user space utility
    in a raw binary format, or wants to see it in the trace output but not
    sacrifice any performance, then they can implement the faster but
    more complex TRACE_EVENT_FORMAT macro.
    
    Here's what usage looks like:
    
      TRACE_EVENT_FORMAT(name,
            TPPROTO(proto),
            TPARGS(args),
            TPFMT(fmt, fmt_args),
            TRACE_STUCT(
                    TRACE_FIELD(type1, item1, assign1)
                    TRACE_FIELD(type2, item2, assign2)
                            [...]
            ),
            TPRAWFMT(raw_fmt)
            );
    
    Note name, proto, args, and fmt, are all identical to what TRACE_FORMAT
    uses.
    
     name: is the unique identifier of the trace point
     proto: The proto type that the trace point uses
     args: the args in the proto type
     fmt: printf format to use with the event printf tracer
     fmt_args: the printf argments to match fmt
    
     TRACE_STRUCT starts the ability to create a structure.
     Each item in the structure is defined with a TRACE_FIELD
    
      TRACE_FIELD(type, item, assign)
    
     type: the C type of item.
     item: the name of the item in the stucture
     assign: what to assign the item in the trace point callback
    
     raw_fmt is a way to pretty print the struct. It must match
      the order of the items are added in TRACE_STUCT
    
     An example of this would be:
    
     TRACE_EVENT_FORMAT(sched_wakeup,
            TPPROTO(struct rq *rq, struct task_struct *p, int success),
            TPARGS(rq, p, success),
            TPFMT("task %s:%d %s",
                  p->comm, p->pid, success?"succeeded":"failed"),
            TRACE_STRUCT(
                    TRACE_FIELD(pid_t, pid, p->pid)
                    TRACE_FIELD(int, success, success)
            ),
            TPRAWFMT("task %d success=%d")
            );
    
     This creates us a unique struct of:
    
     struct {
            pid_t           pid;
            int             success;
     };
    
     And the way the call back would assign these values would be:
    
            entry->pid = p->pid;
            entry->success = success;
    
    The nice part about this is that the creation of the assignent is done
    via macro magic in the event tracer.  Once the TRACE_EVENT_FORMAT is
    created, the developer will then have a faster method to record
    into the ring buffer. They do not need to worry about the tracer itself.
    
    The developer would only need to touch the files in include/trace/*.h
    
    Again, I would like to give special thanks to Tom Zanussi for this
    nice idea.
    
    Idea-from: Tom Zanussi <tzanussi@gmail.com>
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index b811eb343522..77a5c02bd634 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -10,7 +10,7 @@
 #include <linux/module.h>
 #include <linux/ctype.h>
 
-#include "trace_events.h"
+#include "trace.h"
 
 #define TRACE_SYSTEM "TRACE_SYSTEM"
 

commit b628b3e629b1436710e59a21cc020fbb04a52ce1
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Feb 27 23:32:58 2009 -0500

    tracing: make the set_event and available_events subsystem aware
    
    This patch makes the event files, set_event and available_events
    aware of the subsystem.
    
    Now you can enable an entire subsystem with:
    
      echo 'irq:*' > set_event
    
    Note: the '*' is not needed.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 19332200c457..b811eb343522 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -12,6 +12,8 @@
 
 #include "trace_events.h"
 
+#define TRACE_SYSTEM "TRACE_SYSTEM"
+
 #define events_for_each(event)						\
 	for (event = __start_ftrace_events;				\
 	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
@@ -45,14 +47,47 @@ static void ftrace_clear_events(void)
 static int ftrace_set_clr_event(char *buf, int set)
 {
 	struct ftrace_event_call *call = __start_ftrace_events;
+	char *event = NULL, *sub = NULL, *match;
+	int ret = -EINVAL;
+
+	/*
+	 * The buf format can be <subsystem>:<event-name>
+	 *  *:<event-name> means any event by that name.
+	 *  :<event-name> is the same.
+	 *
+	 *  <subsystem>:* means all events in that subsystem
+	 *  <subsystem>: means the same.
+	 *
+	 *  <name> (no ':') means all events in a subsystem with
+	 *  the name <name> or any event that matches <name>
+	 */
+
+	match = strsep(&buf, ":");
+	if (buf) {
+		sub = match;
+		event = buf;
+		match = NULL;
 
+		if (!strlen(sub) || strcmp(sub, "*") == 0)
+			sub = NULL;
+		if (!strlen(event) || strcmp(event, "*") == 0)
+			event = NULL;
+	}
 
 	events_for_each(call) {
 
 		if (!call->name)
 			continue;
 
-		if (strcmp(buf, call->name) != 0)
+		if (match &&
+		    strcmp(match, call->name) != 0 &&
+		    strcmp(match, call->system) != 0)
+			continue;
+
+		if (sub && strcmp(sub, call->system) != 0)
+			continue;
+
+		if (event && strcmp(event, call->name) != 0)
 			continue;
 
 		if (set) {
@@ -68,9 +103,9 @@ static int ftrace_set_clr_event(char *buf, int set)
 			call->enabled = 0;
 			call->unregfunc();
 		}
-		return 0;
+		ret = 0;
 	}
-	return -EINVAL;
+	return ret;
 }
 
 /* 128 should be much more than enough */
@@ -200,6 +235,8 @@ static int t_show(struct seq_file *m, void *v)
 {
 	struct ftrace_event_call *call = v;
 
+	if (strcmp(call->system, TRACE_SYSTEM) != 0)
+		seq_printf(m, "%s:", call->system);
 	seq_printf(m, "%s\n", call->name);
 
 	return 0;

commit 6ecc2d1ca39177edb6fbdb7412948b0e9f409d02
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri Feb 27 21:33:02 2009 -0500

    tracing: add subsystem level to trace events
    
    If a trace point header defines TRACE_SYSTEM, then it will add the
    following trace points into that event system.
    
    If include/trace/irq_event_types.h has:
    
     #define TRACE_SYSTEM irq
    
    at the top and
    
     #undef TRACE_SYSTEM
    
    at the bottom, then a directory "irq" will be created in the
    /debug/tracing/events directory. Inside that directory will contain the
    two trace points that are defined in include/trace/irq_event_types.h.
    
    Only adding the above to irq and not to sched, we get:
    
     # ls /debug/tracing/events/
    irq                     sched_process_exit  sched_signal_send  sched_wakeup_new
    sched_kthread_stop      sched_process_fork  sched_switch
    sched_kthread_stop_ret  sched_process_free  sched_wait_task
    sched_migrate_task      sched_process_wait  sched_wakeup
    
     # ls /debug/tracing/events/irq
    irq_handler_entry  irq_handler_exit
    
    If we add #define TRACE_SYSTEM sched to the trace/sched_event_types.h
    then the rest of the trace events will be put in a sched directory
    within the events directory.
    
    I've been playing with this idea of the subsystem for a while, but
    recently Tom Zanussi posted some patches to lkml that included this
    method. Tom's approach was clean and got me to finally put some effort
    to clean up the event trace points.
    
    Thanks to Tom Zanussi for demonstrating how nice the subsystem
    method is.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 3bcb9df93342..19332200c457 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -345,11 +345,59 @@ static struct dentry *event_trace_events_dir(void)
 	return d_events;
 }
 
+struct event_subsystem {
+	struct list_head	list;
+	const char		*name;
+	struct dentry		*entry;
+};
+
+static LIST_HEAD(event_subsystems);
+
+static struct dentry *
+event_subsystem_dir(const char *name, struct dentry *d_events)
+{
+	struct event_subsystem *system;
+
+	/* First see if we did not already create this dir */
+	list_for_each_entry(system, &event_subsystems, list) {
+		if (strcmp(system->name, name) == 0)
+			return system->entry;
+	}
+
+	/* need to create new entry */
+	system = kmalloc(sizeof(*system), GFP_KERNEL);
+	if (!system) {
+		pr_warning("No memory to create event subsystem %s\n",
+			   name);
+		return d_events;
+	}
+
+	system->entry = debugfs_create_dir(name, d_events);
+	if (!system->entry) {
+		pr_warning("Could not create event subsystem %s\n",
+			   name);
+		kfree(system);
+		return d_events;
+	}
+
+	system->name = name;
+	list_add(&system->list, &event_subsystems);
+
+	return system->entry;
+}
+
 static int
 event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
 {
 	struct dentry *entry;
 
+	/*
+	 * If the trace point header did not define TRACE_SYSTEM
+	 * then the system would be called "TRACE_SYSTEM".
+	 */
+	if (strcmp(call->system, "TRACE_SYSTEM") != 0)
+		d_events = event_subsystem_dir(call->system, d_events);
+
 	call->dir = debugfs_create_dir(call->name, d_events);
 	if (!call->dir) {
 		pr_warning("Could not create debugfs "

commit 1473e4417c79f12d91ef91a469699bfa911f510f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 24 14:15:08 2009 -0500

    tracing: make event directory structure
    
    This patch adds the directory /debug/tracing/events/ that will contain
    all the registered trace points.
    
     # ls /debug/tracing/events/
    sched_kthread_stop      sched_process_fork  sched_switch
    sched_kthread_stop_ret  sched_process_free  sched_wait_task
    sched_migrate_task      sched_process_wait  sched_wakeup
    sched_process_exit      sched_signal_send   sched_wakeup_new
    
     # ls /debug/tracing/events/sched_switch/
    enable
    
     # cat /debug/tracing/events/sched_switch/enable
    1
    
     # cat /debug/tracing/set_event
    sched_switch
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
index 05bc80ec8d2c..3bcb9df93342 100644
--- a/kernel/trace/trace_events.c
+++ b/kernel/trace/trace_events.c
@@ -12,6 +12,11 @@
 
 #include "trace_events.h"
 
+#define events_for_each(event)						\
+	for (event = __start_ftrace_events;				\
+	     (unsigned long)event < (unsigned long)__stop_ftrace_events; \
+	     event++)
+
 void event_trace_printk(unsigned long ip, const char *fmt, ...)
 {
 	va_list ap;
@@ -39,15 +44,16 @@ static void ftrace_clear_events(void)
 
 static int ftrace_set_clr_event(char *buf, int set)
 {
-	struct ftrace_event_call *call = (void *)__start_ftrace_events;
+	struct ftrace_event_call *call = __start_ftrace_events;
 
 
-	while ((unsigned long)call < (unsigned long)__stop_ftrace_events) {
+	events_for_each(call) {
 
-		if (strcmp(buf, call->name) != 0) {
-			call++;
+		if (!call->name)
+			continue;
+
+		if (strcmp(buf, call->name) != 0)
 			continue;
-		}
 
 		if (set) {
 			/* Already set? */
@@ -223,6 +229,67 @@ ftrace_event_seq_open(struct inode *inode, struct file *file)
 	return ret;
 }
 
+static ssize_t
+event_enable_read(struct file *filp, char __user *ubuf, size_t cnt,
+		  loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	char *buf;
+
+	if (call->enabled)
+		buf = "1\n";
+	else
+		buf = "0\n";
+
+	return simple_read_from_buffer(ubuf, cnt, ppos, buf, 2);
+}
+
+static ssize_t
+event_enable_write(struct file *filp, const char __user *ubuf, size_t cnt,
+		   loff_t *ppos)
+{
+	struct ftrace_event_call *call = filp->private_data;
+	char buf[64];
+	unsigned long val;
+	int ret;
+
+	if (cnt >= sizeof(buf))
+		return -EINVAL;
+
+	if (copy_from_user(&buf, ubuf, cnt))
+		return -EFAULT;
+
+	buf[cnt] = 0;
+
+	ret = strict_strtoul(buf, 10, &val);
+	if (ret < 0)
+		return ret;
+
+	switch (val) {
+	case 0:
+		if (!call->enabled)
+			break;
+
+		call->enabled = 0;
+		call->unregfunc();
+		break;
+	case 1:
+		if (call->enabled)
+			break;
+
+		call->enabled = 1;
+		call->regfunc();
+		break;
+
+	default:
+		return -EINVAL;
+	}
+
+	*ppos += cnt;
+
+	return cnt;
+}
+
 static const struct seq_operations show_event_seq_ops = {
 	.start = t_start,
 	.next = t_next,
@@ -252,10 +319,59 @@ static const struct file_operations ftrace_set_event_fops = {
 	.release = seq_release,
 };
 
+static const struct file_operations ftrace_enable_fops = {
+	.open = tracing_open_generic,
+	.read = event_enable_read,
+	.write = event_enable_write,
+};
+
+static struct dentry *event_trace_events_dir(void)
+{
+	static struct dentry *d_tracer;
+	static struct dentry *d_events;
+
+	if (d_events)
+		return d_events;
+
+	d_tracer = tracing_init_dentry();
+	if (!d_tracer)
+		return NULL;
+
+	d_events = debugfs_create_dir("events", d_tracer);
+	if (!d_events)
+		pr_warning("Could not create debugfs "
+			   "'events' directory\n");
+
+	return d_events;
+}
+
+static int
+event_create_dir(struct ftrace_event_call *call, struct dentry *d_events)
+{
+	struct dentry *entry;
+
+	call->dir = debugfs_create_dir(call->name, d_events);
+	if (!call->dir) {
+		pr_warning("Could not create debugfs "
+			   "'%s' directory\n", call->name);
+		return -1;
+	}
+
+	entry = debugfs_create_file("enable", 0644, call->dir, call,
+				    &ftrace_enable_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'%s/enable' entry\n", call->name);
+
+	return 0;
+}
+
 static __init int event_trace_init(void)
 {
+	struct ftrace_event_call *call = __start_ftrace_events;
 	struct dentry *d_tracer;
 	struct dentry *entry;
+	struct dentry *d_events;
 
 	d_tracer = tracing_init_dentry();
 	if (!d_tracer)
@@ -275,6 +391,17 @@ static __init int event_trace_init(void)
 		pr_warning("Could not create debugfs "
 			   "'set_event' entry\n");
 
+	d_events = event_trace_events_dir();
+	if (!d_events)
+		return 0;
+
+	events_for_each(call) {
+		/* The linker may leave blanks */
+		if (!call->name)
+			continue;
+		event_create_dir(call, d_events);
+	}
+
 	return 0;
 }
 fs_initcall(event_trace_init);

commit b77e38aa240c3bd9c55c98b9f7c81541e042eae5
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Feb 24 10:21:36 2009 -0500

    tracing: add event trace infrastructure
    
    This patch creates the event tracing infrastructure of ftrace.
    It will create the files:
    
     /debug/tracing/available_events
     /debug/tracing/set_event
    
    The available_events will list the trace points that have been
    registered with the event tracer.
    
    set_events will allow the user to enable or disable an event hook.
    
    example:
    
     # echo sched_wakeup > /debug/tracing/set_event
    
    Will enable the sched_wakeup event (if it is registered).
    
     # echo "!sched_wakeup" >> /debug/tracing/set_event
    
    Will disable the sched_wakeup event (and only that event).
    
     # echo > /debug/tracing/set_event
    
    Will disable all events (notice the '>')
    
     # cat /debug/tracing/available_events > /debug/tracing/set_event
    
    Will enable all registered event hooks.
    
    Signed-off-by: Steven Rostedt <srostedt@redhat.com>

diff --git a/kernel/trace/trace_events.c b/kernel/trace/trace_events.c
new file mode 100644
index 000000000000..05bc80ec8d2c
--- /dev/null
+++ b/kernel/trace/trace_events.c
@@ -0,0 +1,280 @@
+/*
+ * event tracer
+ *
+ * Copyright (C) 2008 Red Hat Inc, Steven Rostedt <srostedt@redhat.com>
+ *
+ */
+
+#include <linux/debugfs.h>
+#include <linux/uaccess.h>
+#include <linux/module.h>
+#include <linux/ctype.h>
+
+#include "trace_events.h"
+
+void event_trace_printk(unsigned long ip, const char *fmt, ...)
+{
+	va_list ap;
+
+	va_start(ap, fmt);
+	tracing_record_cmdline(current);
+	trace_vprintk(ip, task_curr_ret_stack(current), fmt, ap);
+	va_end(ap);
+}
+
+static void ftrace_clear_events(void)
+{
+	struct ftrace_event_call *call = (void *)__start_ftrace_events;
+
+
+	while ((unsigned long)call < (unsigned long)__stop_ftrace_events) {
+
+		if (call->enabled) {
+			call->enabled = 0;
+			call->unregfunc();
+		}
+		call++;
+	}
+}
+
+static int ftrace_set_clr_event(char *buf, int set)
+{
+	struct ftrace_event_call *call = (void *)__start_ftrace_events;
+
+
+	while ((unsigned long)call < (unsigned long)__stop_ftrace_events) {
+
+		if (strcmp(buf, call->name) != 0) {
+			call++;
+			continue;
+		}
+
+		if (set) {
+			/* Already set? */
+			if (call->enabled)
+				return 0;
+			call->enabled = 1;
+			call->regfunc();
+		} else {
+			/* Already cleared? */
+			if (!call->enabled)
+				return 0;
+			call->enabled = 0;
+			call->unregfunc();
+		}
+		return 0;
+	}
+	return -EINVAL;
+}
+
+/* 128 should be much more than enough */
+#define EVENT_BUF_SIZE		127
+
+static ssize_t
+ftrace_event_write(struct file *file, const char __user *ubuf,
+		   size_t cnt, loff_t *ppos)
+{
+	size_t read = 0;
+	int i, set = 1;
+	ssize_t ret;
+	char *buf;
+	char ch;
+
+	if (!cnt || cnt < 0)
+		return 0;
+
+	ret = get_user(ch, ubuf++);
+	if (ret)
+		return ret;
+	read++;
+	cnt--;
+
+	/* skip white space */
+	while (cnt && isspace(ch)) {
+		ret = get_user(ch, ubuf++);
+		if (ret)
+			return ret;
+		read++;
+		cnt--;
+	}
+
+	/* Only white space found? */
+	if (isspace(ch)) {
+		file->f_pos += read;
+		ret = read;
+		return ret;
+	}
+
+	buf = kmalloc(EVENT_BUF_SIZE+1, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	if (cnt > EVENT_BUF_SIZE)
+		cnt = EVENT_BUF_SIZE;
+
+	i = 0;
+	while (cnt && !isspace(ch)) {
+		if (!i && ch == '!')
+			set = 0;
+		else
+			buf[i++] = ch;
+
+		ret = get_user(ch, ubuf++);
+		if (ret)
+			goto out_free;
+		read++;
+		cnt--;
+	}
+	buf[i] = 0;
+
+	file->f_pos += read;
+
+	ret = ftrace_set_clr_event(buf, set);
+	if (ret)
+		goto out_free;
+
+	ret = read;
+
+ out_free:
+	kfree(buf);
+
+	return ret;
+}
+
+static void *
+t_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct ftrace_event_call *call = m->private;
+	struct ftrace_event_call *next = call;
+
+	(*pos)++;
+
+	if ((unsigned long)call >= (unsigned long)__stop_ftrace_events)
+		return NULL;
+
+	m->private = ++next;
+
+	return call;
+}
+
+static void *t_start(struct seq_file *m, loff_t *pos)
+{
+	return t_next(m, NULL, pos);
+}
+
+static void *
+s_next(struct seq_file *m, void *v, loff_t *pos)
+{
+	struct ftrace_event_call *call = m->private;
+	struct ftrace_event_call *next;
+
+	(*pos)++;
+
+ retry:
+	if ((unsigned long)call >= (unsigned long)__stop_ftrace_events)
+		return NULL;
+
+	if (!call->enabled) {
+		call++;
+		goto retry;
+	}
+
+	next = call;
+	m->private = ++next;
+
+	return call;
+}
+
+static void *s_start(struct seq_file *m, loff_t *pos)
+{
+	return s_next(m, NULL, pos);
+}
+
+static int t_show(struct seq_file *m, void *v)
+{
+	struct ftrace_event_call *call = v;
+
+	seq_printf(m, "%s\n", call->name);
+
+	return 0;
+}
+
+static void t_stop(struct seq_file *m, void *p)
+{
+}
+
+static int
+ftrace_event_seq_open(struct inode *inode, struct file *file)
+{
+	int ret;
+	const struct seq_operations *seq_ops;
+
+	if ((file->f_mode & FMODE_WRITE) &&
+	    !(file->f_flags & O_APPEND))
+		ftrace_clear_events();
+
+	seq_ops = inode->i_private;
+	ret = seq_open(file, seq_ops);
+	if (!ret) {
+		struct seq_file *m = file->private_data;
+
+		m->private = __start_ftrace_events;
+	}
+	return ret;
+}
+
+static const struct seq_operations show_event_seq_ops = {
+	.start = t_start,
+	.next = t_next,
+	.show = t_show,
+	.stop = t_stop,
+};
+
+static const struct seq_operations show_set_event_seq_ops = {
+	.start = s_start,
+	.next = s_next,
+	.show = t_show,
+	.stop = t_stop,
+};
+
+static const struct file_operations ftrace_avail_fops = {
+	.open = ftrace_event_seq_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+static const struct file_operations ftrace_set_event_fops = {
+	.open = ftrace_event_seq_open,
+	.read = seq_read,
+	.write = ftrace_event_write,
+	.llseek = seq_lseek,
+	.release = seq_release,
+};
+
+static __init int event_trace_init(void)
+{
+	struct dentry *d_tracer;
+	struct dentry *entry;
+
+	d_tracer = tracing_init_dentry();
+	if (!d_tracer)
+		return 0;
+
+	entry = debugfs_create_file("available_events", 0444, d_tracer,
+				    (void *)&show_event_seq_ops,
+				    &ftrace_avail_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'available_events' entry\n");
+
+	entry = debugfs_create_file("set_event", 0644, d_tracer,
+				    (void *)&show_set_event_seq_ops,
+				    &ftrace_set_event_fops);
+	if (!entry)
+		pr_warning("Could not create debugfs "
+			   "'set_event' entry\n");
+
+	return 0;
+}
+fs_initcall(event_trace_init);
