commit 13292494379f92f532de71b31a54018336adc589
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Fri Dec 13 13:58:57 2019 -0500

    tracing: Make struct ring_buffer less ambiguous
    
    As there's two struct ring_buffers in the kernel, it causes some confusion.
    The other one being the perf ring buffer. It was agreed upon that as neither
    of the ring buffers are generic enough to be used globally, they should be
    renamed as:
    
       perf's ring_buffer -> perf_buffer
       ftrace's ring_buffer -> trace_buffer
    
    This implements the changes to the ring buffer that ftrace uses.
    
    Link: https://lore.kernel.org/r/20191213140531.116b3200@gandalf.local.home
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 32149e46551c..8df0aa810950 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -29,7 +29,7 @@ static int reader_finish;
 static DECLARE_COMPLETION(read_start);
 static DECLARE_COMPLETION(read_done);
 
-static struct ring_buffer *buffer;
+static struct trace_buffer *buffer;
 static struct task_struct *producer;
 static struct task_struct *consumer;
 static unsigned long read;

commit a82a4804b4ee3636c8988fea14d44f70f4de45f1
Author: Xianting Tian <xianting_tian@126.com>
Date:   Sat Nov 16 10:05:55 2019 -0500

    ring-buffer: Fix typos in function ring_buffer_producer
    
    Fix spelling and other typos
    
    Link: http://lkml.kernel.org/r/1573916755-32478-1-git-send-email-xianting_tian@126.com
    
    Signed-off-by: Xianting Tian <xianting_tian@126.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 09b0b49f346e..32149e46551c 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -269,10 +269,10 @@ static void ring_buffer_producer(void)
 
 #ifndef CONFIG_PREEMPTION
 		/*
-		 * If we are a non preempt kernel, the 10 second run will
+		 * If we are a non preempt kernel, the 10 seconds run will
 		 * stop everything while it runs. Instead, we will call
 		 * cond_resched and also add any time that was lost by a
-		 * rescedule.
+		 * reschedule.
 		 *
 		 * Do a cond resched at the same frequency we would wake up
 		 * the reader.

commit 30c937043b2db09ae3408f5534824f9ececdb581
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Fri Jul 26 23:19:40 2019 +0200

    tracing: Use CONFIG_PREEMPTION
    
    CONFIG_PREEMPTION is selected by CONFIG_PREEMPT and by
    CONFIG_PREEMPT_RT. Both PREEMPT and PREEMPT_RT require the same
    functionality which today depends on CONFIG_PREEMPT.
    
    Switch the conditionals in the tracer over to CONFIG_PREEMPTION.
    
    This is the first step to make the tracer work on RT. The other small
    tweaks are submitted separately.
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Acked-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Masami Hiramatsu <mhiramat@kernel.org>
    Cc: Paolo Bonzini <pbonzini@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Link: http://lkml.kernel.org/r/20190726212124.409766323@linutronix.de
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 0564f6db0561..09b0b49f346e 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -267,7 +267,7 @@ static void ring_buffer_producer(void)
 		if (consumer && !(cnt % wakeup_interval))
 			wake_up_process(consumer);
 
-#ifndef CONFIG_PREEMPT
+#ifndef CONFIG_PREEMPTION
 		/*
 		 * If we are a non preempt kernel, the 10 second run will
 		 * stop everything while it runs. Instead, we will call

commit 5c173bedb24dfcc8e412d3c3f111c504e4408dd5
Author: Yangtao Li <tiny.windzz@gmail.com>
Date:   Thu Nov 1 11:46:40 2018 -0400

    ring-buffer: Fix mispelling of Calculate
    
    It's not "Caculate".
    
    Link: http://lkml.kernel.org/r/20181101154640.23162-1-tiny.windzz@gmail.com
    
    Signed-off-by: Yangtao Li <tiny.windzz@gmail.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index ffba6789c0e2..0564f6db0561 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -362,7 +362,7 @@ static void ring_buffer_producer(void)
 			hit--; /* make it non zero */
 		}
 
-		/* Caculate the average time in nanosecs */
+		/* Calculate the average time in nanosecs */
 		avg = NSEC_PER_MSEC / (hit + missed);
 		trace_printk("%ld ns per entry\n", avg);
 	}

commit bcea3f96e11cf2f0232d851e0fdb854f5ada425a
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Thu Aug 16 11:23:53 2018 -0400

    tracing: Add SPDX License format tags to tracing files
    
    Add the SPDX License header to ease license compliance management.
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 68ee79afe31c..ffba6789c0e2 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -1,3 +1,4 @@
+// SPDX-License-Identifier: GPL-2.0
 /*
  * ring buffer tester and benchmark
  *

commit a7e52ad7ed82e21273eccff93d1477a7b313aabb
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Wed Aug 2 14:20:54 2017 -0400

    ring-buffer: Have ring_buffer_alloc_read_page() return error on offline CPU
    
    Chunyu Hu reported:
      "per_cpu trace directories and files are created for all possible cpus,
       but only the cpus which have ever been on-lined have their own per cpu
       ring buffer (allocated by cpuhp threads). While trace_buffers_open, the
       open handler for trace file 'trace_pipe_raw' is always trying to access
       field of ring_buffer_per_cpu, and would panic with the NULL pointer.
    
       Align the behavior of trace_pipe_raw with trace_pipe, that returns -NODEV
       when openning it if that cpu does not have trace ring buffer.
    
       Reproduce:
       cat /sys/kernel/debug/tracing/per_cpu/cpu31/trace_pipe_raw
       (cpu31 is never on-lined, this is a 16 cores x86_64 box)
    
       Tested with:
       1) boot with maxcpus=14, read trace_pipe_raw of cpu15.
          Got -NODEV.
       2) oneline cpu15, read trace_pipe_raw of cpu15.
          Get the raw trace data.
    
       Call trace:
       [ 5760.950995] RIP: 0010:ring_buffer_alloc_read_page+0x32/0xe0
       [ 5760.961678]  tracing_buffers_read+0x1f6/0x230
       [ 5760.962695]  __vfs_read+0x37/0x160
       [ 5760.963498]  ? __vfs_read+0x5/0x160
       [ 5760.964339]  ? security_file_permission+0x9d/0xc0
       [ 5760.965451]  ? __vfs_read+0x5/0x160
       [ 5760.966280]  vfs_read+0x8c/0x130
       [ 5760.967070]  SyS_read+0x55/0xc0
       [ 5760.967779]  do_syscall_64+0x67/0x150
       [ 5760.968687]  entry_SYSCALL64_slow_path+0x25/0x25"
    
    This was introduced by the addition of the feature to reuse reader pages
    instead of re-allocating them. The problem is that the allocation of a
    reader page (which is per cpu) does not check if the cpu is online and set
    up for the ring buffer.
    
    Link: http://lkml.kernel.org/r/1500880866-1177-1-git-send-email-chuhu@redhat.com
    
    Cc: stable@vger.kernel.org
    Fixes: 73a757e63114 ("ring-buffer: Return reader page back into existing ring buffer")
    Reported-by: Chunyu Hu <chuhu@redhat.com>
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 9fbcaf567886..68ee79afe31c 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -113,7 +113,7 @@ static enum event_status read_page(int cpu)
 	int i;
 
 	bpage = ring_buffer_alloc_read_page(buffer, cpu);
-	if (!bpage)
+	if (IS_ERR(bpage))
 		return EVENT_DROPPED;
 
 	ret = ring_buffer_read_page(buffer, &bpage, PAGE_SIZE, cpu, 1);

commit 73a757e63114dfd765f1c5d1ff7e994f123d0234
Author: Steven Rostedt (VMware) <rostedt@goodmis.org>
Date:   Mon May 1 09:35:09 2017 -0400

    ring-buffer: Return reader page back into existing ring buffer
    
    When reading the ring buffer for consuming, it is optimized for splice,
    where a page is taken out of the ring buffer (zero copy) and sent to the
    reading consumer. When the read is finished with the page, it calls
    ring_buffer_free_read_page(), which simply frees the page. The next time the
    reader needs to get a page from the ring buffer, it must call
    ring_buffer_alloc_read_page() which allocates and initializes a reader page
    for the ring buffer to be swapped into the ring buffer for a new filled page
    for the reader.
    
    The problem is that there's no reason to actually free the page when it is
    passed back to the ring buffer. It can hold it off and reuse it for the next
    iteration. This completely removes the interaction with the page_alloc
    mechanism.
    
    Using the trace-cmd utility to record all events (causing trace-cmd to
    require reading lots of pages from the ring buffer, and calling
    ring_buffer_alloc/free_read_page() several times), and also assigning a
    stack trace trigger to the mm_page_alloc event, we can see how many times
    the ring_buffer_alloc_read_page() needed to allocate a page for the ring
    buffer.
    
    Before this change:
    
      # trace-cmd record -e all -e mem_page_alloc -R stacktrace sleep 1
      # trace-cmd report |grep ring_buffer_alloc_read_page | wc -l
      9968
    
    After this change:
    
      # trace-cmd record -e all -e mem_page_alloc -R stacktrace sleep 1
      # trace-cmd report |grep ring_buffer_alloc_read_page | wc -l
      4
    
    Signed-off-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index c190a4d5013c..9fbcaf567886 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -171,7 +171,7 @@ static enum event_status read_page(int cpu)
 			}
 		}
 	}
-	ring_buffer_free_read_page(buffer, bpage);
+	ring_buffer_free_read_page(buffer, cpu, bpage);
 
 	if (ret < 0)
 		return EVENT_DROPPED;

commit ae7e81c077d60507dcec139e40a6d10cf932cf4b
Author: Ingo Molnar <mingo@kernel.org>
Date:   Wed Feb 1 18:07:51 2017 +0100

    sched/headers: Prepare for new header dependencies before moving code to <uapi/linux/sched/types.h>
    
    We are going to move scheduler ABI details to <uapi/linux/sched/types.h>,
    which will be used from a number of .c files.
    
    Create empty placeholder header that maps to <linux/types.h>.
    
    Include the new header in the files that are going to need it.
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 6df9a83e20d7..c190a4d5013c 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -6,6 +6,7 @@
 #include <linux/ring_buffer.h>
 #include <linux/completion.h>
 #include <linux/kthread.h>
+#include <uapi/linux/sched/types.h>
 #include <linux/module.h>
 #include <linux/ktime.h>
 #include <asm/local.h>

commit 54ed1444052467044e9e01334ac8123dd6345211
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Tue Nov 3 16:19:02 2015 -0500

    ring_buffer: Remove unneeded smp_wmb() before wakeup of reader benchmark
    
    wake_up_process() has a memory barrier before doing anything, thus adding a
    memory barrier before calling it is redundant.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 9e00fd178226..6df9a83e20d7 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -288,8 +288,6 @@ static void ring_buffer_producer(void)
 		/* the completions must be visible before the finish var */
 		smp_wmb();
 		reader_finish = 1;
-		/* finish var visible before waking up the consumer */
-		smp_wmb();
 		wake_up_process(consumer);
 		wait_for_completion(&read_done);
 	}

commit f47cb66df2f89dd1f796742c64f9ead77e548a6a
Author: Petr Mladek <pmladek@suse.com>
Date:   Mon Sep 7 14:38:38 2015 +0200

    ring_buffer: Fix more races when terminating the producer in the benchmark
    
    The commit b44754d8262d3aab8 ("ring_buffer: Allow to exit the ring
    buffer benchmark immediately") added a hack into ring_buffer_producer()
    that set @kill_test when kthread_should_stop() returned true. It improved
    the situation a lot. It stopped the kthread in most cases because
    the producer spent most of the time in the patched while cycle.
    
    But there are still few possible races when kthread_should_stop()
    is set outside of the cycle. Then we do not set @kill_test and
    some other checks pass.
    
    This patch adds a better fix. It renames @test_kill/TEST_KILL() into
    a better descriptive @test_error/TEST_ERROR(). Also it introduces
    break_test() function that checks for both @test_error and
    kthread_should_stop().
    
    The new function is used in the producer when the check for @test_error
    is not enough. It is not used in the consumer because its state
    is manipulated by the producer via the "reader_finish" variable.
    
    Also we add a missing check into ring_buffer_producer_thread()
    between setting TASK_INTERRUPTIBLE and calling schedule_timeout().
    Otherwise, we might miss a wakeup from kthread_stop().
    
    Link: http://lkml.kernel.org/r/1441629518-32712-3-git-send-email-pmladek@suse.com
    
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 9ea7949366b3..9e00fd178226 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -60,12 +60,12 @@ MODULE_PARM_DESC(consumer_fifo, "fifo prio for consumer");
 
 static int read_events;
 
-static int kill_test;
+static int test_error;
 
-#define KILL_TEST()				\
+#define TEST_ERROR()				\
 	do {					\
-		if (!kill_test) {		\
-			kill_test = 1;		\
+		if (!test_error) {		\
+			test_error = 1;		\
 			WARN_ON(1);		\
 		}				\
 	} while (0)
@@ -75,6 +75,11 @@ enum event_status {
 	EVENT_DROPPED,
 };
 
+static bool break_test(void)
+{
+	return test_error || kthread_should_stop();
+}
+
 static enum event_status read_event(int cpu)
 {
 	struct ring_buffer_event *event;
@@ -87,7 +92,7 @@ static enum event_status read_event(int cpu)
 
 	entry = ring_buffer_event_data(event);
 	if (*entry != cpu) {
-		KILL_TEST();
+		TEST_ERROR();
 		return EVENT_DROPPED;
 	}
 
@@ -115,10 +120,10 @@ static enum event_status read_page(int cpu)
 		rpage = bpage;
 		/* The commit may have missed event flags set, clear them */
 		commit = local_read(&rpage->commit) & 0xfffff;
-		for (i = 0; i < commit && !kill_test; i += inc) {
+		for (i = 0; i < commit && !test_error ; i += inc) {
 
 			if (i >= (PAGE_SIZE - offsetof(struct rb_page, data))) {
-				KILL_TEST();
+				TEST_ERROR();
 				break;
 			}
 
@@ -128,7 +133,7 @@ static enum event_status read_page(int cpu)
 			case RINGBUF_TYPE_PADDING:
 				/* failed writes may be discarded events */
 				if (!event->time_delta)
-					KILL_TEST();
+					TEST_ERROR();
 				inc = event->array[0] + 4;
 				break;
 			case RINGBUF_TYPE_TIME_EXTEND:
@@ -137,12 +142,12 @@ static enum event_status read_page(int cpu)
 			case 0:
 				entry = ring_buffer_event_data(event);
 				if (*entry != cpu) {
-					KILL_TEST();
+					TEST_ERROR();
 					break;
 				}
 				read++;
 				if (!event->array[0]) {
-					KILL_TEST();
+					TEST_ERROR();
 					break;
 				}
 				inc = event->array[0] + 4;
@@ -150,17 +155,17 @@ static enum event_status read_page(int cpu)
 			default:
 				entry = ring_buffer_event_data(event);
 				if (*entry != cpu) {
-					KILL_TEST();
+					TEST_ERROR();
 					break;
 				}
 				read++;
 				inc = ((event->type_len + 1) * 4);
 			}
-			if (kill_test)
+			if (test_error)
 				break;
 
 			if (inc <= 0) {
-				KILL_TEST();
+				TEST_ERROR();
 				break;
 			}
 		}
@@ -185,7 +190,7 @@ static void ring_buffer_consumer(void)
 	while (!READ_ONCE(reader_finish)) {
 		int found = 1;
 
-		while (found && !kill_test) {
+		while (found && !test_error) {
 			int cpu;
 
 			found = 0;
@@ -197,7 +202,7 @@ static void ring_buffer_consumer(void)
 				else
 					stat = read_page(cpu);
 
-				if (kill_test)
+				if (test_error)
 					break;
 
 				if (stat == EVENT_FOUND)
@@ -273,10 +278,7 @@ static void ring_buffer_producer(void)
 		if (cnt % wakeup_interval)
 			cond_resched();
 #endif
-		if (kthread_should_stop())
-			kill_test = 1;
-
-	} while (ktime_before(end_time, timeout) && !kill_test);
+	} while (ktime_before(end_time, timeout) && !break_test());
 	trace_printk("End ring buffer hammer\n");
 
 	if (consumer) {
@@ -297,7 +299,7 @@ static void ring_buffer_producer(void)
 	entries = ring_buffer_entries(buffer);
 	overruns = ring_buffer_overruns(buffer);
 
-	if (kill_test && !kthread_should_stop())
+	if (test_error)
 		trace_printk("ERROR!\n");
 
 	if (!disable_reader) {
@@ -378,15 +380,14 @@ static void wait_to_die(void)
 
 static int ring_buffer_consumer_thread(void *arg)
 {
-	while (!kthread_should_stop() && !kill_test) {
+	while (!break_test()) {
 		complete(&read_start);
 
 		ring_buffer_consumer();
 
 		set_current_state(TASK_INTERRUPTIBLE);
-		if (kthread_should_stop() || kill_test)
+		if (break_test())
 			break;
-
 		schedule();
 	}
 	__set_current_state(TASK_RUNNING);
@@ -399,7 +400,7 @@ static int ring_buffer_consumer_thread(void *arg)
 
 static int ring_buffer_producer_thread(void *arg)
 {
-	while (!kthread_should_stop() && !kill_test) {
+	while (!break_test()) {
 		ring_buffer_reset(buffer);
 
 		if (consumer) {
@@ -408,15 +409,18 @@ static int ring_buffer_producer_thread(void *arg)
 		}
 
 		ring_buffer_producer();
-		if (kill_test)
+		if (break_test())
 			goto out_kill;
 
 		trace_printk("Sleeping for 10 secs\n");
 		set_current_state(TASK_INTERRUPTIBLE);
+		if (break_test())
+			goto out_kill;
 		schedule_timeout(HZ * SLEEP_TIME);
 	}
 
 out_kill:
+	__set_current_state(TASK_RUNNING);
 	if (!kthread_should_stop())
 		wait_to_die();
 

commit 8b46ff6938d2c78a16520a37086944ecbaa8ab8b
Author: Petr Mladek <pmladek@suse.com>
Date:   Mon Sep 7 14:38:37 2015 +0200

    ring_buffer: Do no not complete benchmark reader too early
    
    It seems that complete(&read_done) might be called too early
    in some situations.
    
    1st scenario:
    -------------
    
    CPU0                                    CPU1
    
    ring_buffer_producer_thread()
      wake_up_process(consumer);
      wait_for_completion(&read_start);
    
                                            ring_buffer_consumer_thread()
                                              complete(&read_start);
    
      ring_buffer_producer()
        # producing data in
        # the do-while cycle
    
                                              ring_buffer_consumer();
                                                # reading data
                                                # got error
                                                # set kill_test = 1;
                                                set_current_state(
                                                    TASK_INTERRUPTIBLE);
                                                if (reader_finish)  # false
                                                schedule();
    
        # producer still in the middle of
        # do-while cycle
        if (consumer && !(cnt % wakeup_interval))
          wake_up_process(consumer);
    
                                                # spurious wakeup
                                                while (!reader_finish &&
                                                       !kill_test)
                                                # leaving because
                                                # kill_test == 1
                                                reader_finish = 0;
                                                complete(&read_done);
    
    1st BANG: We might access uninitialized "read_done" if this is the
              the first round.
    
        # producer finally leaving
        # the do-while cycle because kill_test == 1;
    
        if (consumer) {
          reader_finish = 1;
          wake_up_process(consumer);
          wait_for_completion(&read_done);
    
    2nd BANG: This will never complete because consumer already did
              the completion.
    
    2nd scenario:
    -------------
    
    CPU0                                    CPU1
    
    ring_buffer_producer_thread()
      wake_up_process(consumer);
      wait_for_completion(&read_start);
    
                                            ring_buffer_consumer_thread()
                                              complete(&read_start);
    
      ring_buffer_producer()
        # CPU3 removes the module     <--- difference from
        # and stops producer          <--- the 1st scenario
        if (kthread_should_stop())
          kill_test = 1;
    
                                              ring_buffer_consumer();
                                                while (!reader_finish &&
                                                       !kill_test)
                                                # kill_test == 1 => we never go
                                                # into the top level while()
                                                reader_finish = 0;
                                                complete(&read_done);
    
        # producer still in the middle of
        # do-while cycle
        if (consumer && !(cnt % wakeup_interval))
          wake_up_process(consumer);
    
                                                # spurious wakeup
                                                while (!reader_finish &&
                                                       !kill_test)
                                                # leaving because kill_test == 1
                                                reader_finish = 0;
                                                complete(&read_done);
    
    BANG: We are in the same "bang" situations as in the 1st scenario.
    
    Root of the problem:
    --------------------
    
    ring_buffer_consumer() must complete "read_done" only when "reader_finish"
    variable is set. It must not be skipped due to other conditions.
    
    Note that we still must keep the check for "reader_finish" in a loop
    because there might be spurious wakeups as described in the
    above scenarios.
    
    Solution:
    ----------
    
    The top level cycle in ring_buffer_consumer() will finish only when
    "reader_finish" is set. The data will be read in "while-do" cycle
    so that they are not read after an error (kill_test == 1)
    or a spurious wake up.
    
    In addition, "reader_finish" is manipulated by the producer thread.
    Therefore we add READ_ONCE() to make sure that the fresh value is
    read in each cycle. Also we add the corresponding barrier
    to synchronize the sleep check.
    
    Next we set the state back to TASK_RUNNING for the situation where we
    did not sleep.
    
    Just from paranoid reasons, we initialize both completions statically.
    This is safer, in case there are other races that we are unaware of.
    
    As a side effect we could remove the memory barrier from
    ring_buffer_producer_thread(). IMHO, this was the reason for
    the barrier. ring_buffer_reset() uses spin locks that should
    provide the needed memory barrier for using the buffer.
    
    Link: http://lkml.kernel.org/r/1441629518-32712-2-git-send-email-pmladek@suse.com
    
    Signed-off-by: Petr Mladek <pmladek@suse.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index a1503a027ee2..9ea7949366b3 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -24,8 +24,8 @@ struct rb_page {
 static int wakeup_interval = 100;
 
 static int reader_finish;
-static struct completion read_start;
-static struct completion read_done;
+static DECLARE_COMPLETION(read_start);
+static DECLARE_COMPLETION(read_done);
 
 static struct ring_buffer *buffer;
 static struct task_struct *producer;
@@ -178,10 +178,14 @@ static void ring_buffer_consumer(void)
 	read_events ^= 1;
 
 	read = 0;
-	while (!reader_finish && !kill_test) {
-		int found;
+	/*
+	 * Continue running until the producer specifically asks to stop
+	 * and is ready for the completion.
+	 */
+	while (!READ_ONCE(reader_finish)) {
+		int found = 1;
 
-		do {
+		while (found && !kill_test) {
 			int cpu;
 
 			found = 0;
@@ -195,17 +199,23 @@ static void ring_buffer_consumer(void)
 
 				if (kill_test)
 					break;
+
 				if (stat == EVENT_FOUND)
 					found = 1;
+
 			}
-		} while (found && !kill_test);
+		}
 
+		/* Wait till the producer wakes us up when there is more data
+		 * available or when the producer wants us to finish reading.
+		 */
 		set_current_state(TASK_INTERRUPTIBLE);
 		if (reader_finish)
 			break;
 
 		schedule();
 	}
+	__set_current_state(TASK_RUNNING);
 	reader_finish = 0;
 	complete(&read_done);
 }
@@ -389,13 +399,10 @@ static int ring_buffer_consumer_thread(void *arg)
 
 static int ring_buffer_producer_thread(void *arg)
 {
-	init_completion(&read_start);
-
 	while (!kthread_should_stop() && !kill_test) {
 		ring_buffer_reset(buffer);
 
 		if (consumer) {
-			smp_wmb();
 			wake_up_process(consumer);
 			wait_for_completion(&read_start);
 		}

commit e382608254e06c8109f40044f5e693f2e04f3899
Merge: fcbc1777ce8b b44754d8262d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 14:02:43 2015 -0700

    Merge tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This patch series contains several clean ups and even a new trace
      clock "monitonic raw".  Also some enhancements to make the ring buffer
      even faster.  But the biggest and most noticeable change is the
      renaming of the ftrace* files, structures and variables that have to
      deal with trace events.
    
      Over the years I've had several developers tell me about their
      confusion with what ftrace is compared to events.  Technically,
      "ftrace" is the infrastructure to do the function hooks, which include
      tracing and also helps with live kernel patching.  But the trace
      events are a separate entity altogether, and the files that affect the
      trace events should not be named "ftrace".  These include:
    
        include/trace/ftrace.h         ->    include/trace/trace_events.h
        include/linux/ftrace_event.h   ->    include/linux/trace_events.h
    
      Also, functions that are specific for trace events have also been renamed:
    
        ftrace_print_*()               ->    trace_print_*()
        (un)register_ftrace_event()    ->    (un)register_trace_event()
        ftrace_event_name()            ->    trace_event_name()
        ftrace_trigger_soft_disabled() ->    trace_trigger_soft_disabled()
        ftrace_define_fields_##call()  ->    trace_define_fields_##call()
        ftrace_get_offsets_##call()    ->    trace_get_offsets_##call()
    
      Structures have been renamed:
    
        ftrace_event_file              ->    trace_event_file
        ftrace_event_{call,class}      ->    trace_event_{call,class}
        ftrace_event_buffer            ->    trace_event_buffer
        ftrace_subsystem_dir           ->    trace_subsystem_dir
        ftrace_event_raw_##call        ->    trace_event_raw_##call
        ftrace_event_data_offset_##call->    trace_event_data_offset_##call
        ftrace_event_type_funcs_##call ->    trace_event_type_funcs_##call
    
      And a few various variables and flags have also been updated.
    
      This has been sitting in linux-next for some time, and I have not
      heard a single complaint about this rename breaking anything.  Mostly
      because these functions, variables and structures are mostly internal
      to the tracing system and are seldom (if ever) used by anything
      external to that"
    
    * tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (33 commits)
      ring_buffer: Allow to exit the ring buffer benchmark immediately
      ring-buffer-benchmark: Fix the wrong type
      ring-buffer-benchmark: Fix the wrong param in module_param
      ring-buffer: Add enum names for the context levels
      ring-buffer: Remove useless unused tracing_off_permanent()
      ring-buffer: Give NMIs a chance to lock the reader_lock
      ring-buffer: Add trace_recursive checks to ring_buffer_write()
      ring-buffer: Allways do the trace_recursive checks
      ring-buffer: Move recursive check to per_cpu descriptor
      ring-buffer: Add unlikelys to make fast path the default
      tracing: Rename ftrace_get_offsets_##call() to trace_event_get_offsets_##call()
      tracing: Rename ftrace_define_fields_##call() to trace_event_define_fields_##call()
      tracing: Rename ftrace_event_type_funcs_##call to trace_event_type_funcs_##call
      tracing: Rename ftrace_data_offset_##call to trace_event_data_offset_##call
      tracing: Rename ftrace_raw_##call event structures to trace_event_raw_##call
      tracing: Rename ftrace_trigger_soft_disabled() to trace_trigger_soft_disabled()
      tracing: Rename FTRACE_EVENT_FL_* flags to EVENT_FILE_FL_*
      tracing: Rename struct ftrace_subsystem_dir to trace_subsystem_dir
      tracing: Rename ftrace_event_name() to trace_event_name()
      tracing: Rename FTRACE_MAX_EVENT to TRACE_EVENT_TYPE_MAX
      ...

commit b44754d8262d3aab842998cf747f44fe6090be9f
Author: Petr Mladek <pmladek@suse.cz>
Date:   Mon Jun 15 15:53:10 2015 +0200

    ring_buffer: Allow to exit the ring buffer benchmark immediately
    
    It takes a while until the ring_buffer_benchmark module is removed
    when the ring buffer hammer is running. It is because it takes
    few seconds and kthread_should_stop() is not being checked.
    
    This patch adds the check for kthread termination into the producer.
    It uses the existing @kill_test flag to finish the kthreads as
    cleanly as possible.
    
    It disables printing the "ERROR" message when the kthread is going.
    
    It makes sure that producer does not go into the 10sec sleep
    when it is being killed.
    
    Finally, it does not call wait_to_die() when kthread_should_stop()
    already returns true.
    
    Link: http://lkml.kernel.org/r/20150615155428.GD3135@pathway.suse.cz
    
    Signed-off-by: Petr Mladek <pmladek@suse.cz>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 2430563cf2bc..5ffbae3d3f86 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -263,6 +263,8 @@ static void ring_buffer_producer(void)
 		if (cnt % wakeup_interval)
 			cond_resched();
 #endif
+		if (kthread_should_stop())
+			kill_test = 1;
 
 	} while (ktime_before(end_time, timeout) && !kill_test);
 	trace_printk("End ring buffer hammer\n");
@@ -285,7 +287,7 @@ static void ring_buffer_producer(void)
 	entries = ring_buffer_entries(buffer);
 	overruns = ring_buffer_overruns(buffer);
 
-	if (kill_test)
+	if (kill_test && !kthread_should_stop())
 		trace_printk("ERROR!\n");
 
 	if (!disable_reader) {
@@ -379,7 +381,7 @@ static int ring_buffer_consumer_thread(void *arg)
 	}
 	__set_current_state(TASK_RUNNING);
 
-	if (kill_test)
+	if (!kthread_should_stop())
 		wait_to_die();
 
 	return 0;
@@ -399,13 +401,16 @@ static int ring_buffer_producer_thread(void *arg)
 		}
 
 		ring_buffer_producer();
+		if (kill_test)
+			goto out_kill;
 
 		trace_printk("Sleeping for 10 secs\n");
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule_timeout(HZ * SLEEP_TIME);
 	}
 
-	if (kill_test)
+out_kill:
+	if (!kthread_should_stop())
 		wait_to_die();
 
 	return 0;

commit 108029323910c5dd1ef8fa2d10da1ce5fbce6e12
Author: Wang Long <long.wanglong@huawei.com>
Date:   Wed Jun 10 08:12:37 2015 +0000

    ring-buffer-benchmark: Fix the wrong sched_priority of producer
    
    The producer should be used producer_fifo as its sched_priority,
    so correct it.
    
    Link: http://lkml.kernel.org/r/1433923957-67842-1-git-send-email-long.wanglong@huawei.com
    
    Cc: stable@vger.kernel.org # 2.6.33+
    Signed-off-by: Wang Long <long.wanglong@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 13d945c0d03f..1b28df2d9104 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -450,7 +450,7 @@ static int __init ring_buffer_benchmark_init(void)
 
 	if (producer_fifo >= 0) {
 		struct sched_param param = {
-			.sched_priority = consumer_fifo
+			.sched_priority = producer_fifo
 		};
 		sched_setscheduler(producer, SCHED_FIFO, &param);
 	} else

commit 33d657d1381025542b84b79d5c7b548598e40a4d
Author: Wang Long <long.wanglong@huawei.com>
Date:   Wed Jun 10 08:12:07 2015 +0000

    ring-buffer-benchmark: Fix the wrong type
    
    The macro 'module_param' shows that the type of the
    variable disable_reader and write_iteration is unsigned
    integer. so, we change their type form int to unsigned int.
    
    Link: http://lkml.kernel.org/r/1433923927-67782-1-git-send-email-long.wanglong@huawei.com
    
    Signed-off-by: Wang Long <long.wanglong@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index cdff7d3df902..2430563cf2bc 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -32,11 +32,11 @@ static struct task_struct *producer;
 static struct task_struct *consumer;
 static unsigned long read;
 
-static int disable_reader;
+static unsigned int disable_reader;
 module_param(disable_reader, uint, 0644);
 MODULE_PARM_DESC(disable_reader, "only run producer");
 
-static int write_iteration = 50;
+static unsigned int write_iteration = 50;
 module_param(write_iteration, uint, 0644);
 MODULE_PARM_DESC(write_iteration, "# of writes between timestamp readings");
 

commit 7364e86547bc52a555c9317b32cc4e81c4d4fd7c
Author: Wang Long <long.wanglong@huawei.com>
Date:   Wed Jun 10 08:11:13 2015 +0000

    ring-buffer-benchmark: Fix the wrong param in module_param
    
    The {producer|consumer}_{nice|fifo} parameters are integer
    type, we should use 'int' as the second param in module_param.
    
    For example(consumer_fifo):
            the default value of consumer_fifo is -1.
       Without this patch:
            # cat /sys/module/ring_buffer_benchmark/parameters/consumer_fifo
            4294967295
       With this patch:
            # cat /sys/module/ring_buffer_benchmark/parameters/consumer_fifo
            -1
    
    Link: http://lkml.kernel.org/r/1433923873-67712-1-git-send-email-long.wanglong@huawei.com
    
    Signed-off-by: Wang Long <long.wanglong@huawei.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 13d945c0d03f..cdff7d3df902 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -46,16 +46,16 @@ static int consumer_nice = MAX_NICE;
 static int producer_fifo = -1;
 static int consumer_fifo = -1;
 
-module_param(producer_nice, uint, 0644);
+module_param(producer_nice, int, 0644);
 MODULE_PARM_DESC(producer_nice, "nice prio for producer");
 
-module_param(consumer_nice, uint, 0644);
+module_param(consumer_nice, int, 0644);
 MODULE_PARM_DESC(consumer_nice, "nice prio for consumer");
 
-module_param(producer_fifo, uint, 0644);
+module_param(producer_fifo, int, 0644);
 MODULE_PARM_DESC(producer_fifo, "fifo prio for producer");
 
-module_param(consumer_fifo, uint, 0644);
+module_param(consumer_fifo, int, 0644);
 MODULE_PARM_DESC(consumer_fifo, "fifo prio for consumer");
 
 static int read_events;

commit da194930ede6d87945786f72b1c36c7a4ed0f3a6
Author: Tina Ruchandani <ruchandani.tina@gmail.com>
Date:   Wed Jan 28 19:46:11 2015 +0530

    trace: Use 64-bit timekeeping
    
    The ring_buffer_producer uses 'struct timeval' to measure
    its start and end times. 'struct timeval' on 32-bit systems
    will have its tv_sec value overflow in year 2038 and beyond.
    This patch replaces struct timeval with 'ktime_t' which uses
    64-bit representation for nanoseconds.
    
    Link: http://lkml.kernel.org/r/20150128141611.GA2701@tinar
    
    Suggested-by: Arnd Bergmann <arnd@arndb.de>
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Tina Ruchandani <ruchandani.tina@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 3f9e328c30b5..13d945c0d03f 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -7,7 +7,7 @@
 #include <linux/completion.h>
 #include <linux/kthread.h>
 #include <linux/module.h>
-#include <linux/time.h>
+#include <linux/ktime.h>
 #include <asm/local.h>
 
 struct rb_page {
@@ -17,7 +17,7 @@ struct rb_page {
 };
 
 /* run time and sleep time in seconds */
-#define RUN_TIME	10
+#define RUN_TIME	10ULL
 #define SLEEP_TIME	10
 
 /* number of events for writer to wake up the reader */
@@ -212,8 +212,7 @@ static void ring_buffer_consumer(void)
 
 static void ring_buffer_producer(void)
 {
-	struct timeval start_tv;
-	struct timeval end_tv;
+	ktime_t start_time, end_time, timeout;
 	unsigned long long time;
 	unsigned long long entries;
 	unsigned long long overruns;
@@ -227,7 +226,8 @@ static void ring_buffer_producer(void)
 	 * make the system stall)
 	 */
 	trace_printk("Starting ring buffer hammer\n");
-	do_gettimeofday(&start_tv);
+	start_time = ktime_get();
+	timeout = ktime_add_ns(start_time, RUN_TIME * NSEC_PER_SEC);
 	do {
 		struct ring_buffer_event *event;
 		int *entry;
@@ -244,7 +244,7 @@ static void ring_buffer_producer(void)
 				ring_buffer_unlock_commit(buffer, event);
 			}
 		}
-		do_gettimeofday(&end_tv);
+		end_time = ktime_get();
 
 		cnt++;
 		if (consumer && !(cnt % wakeup_interval))
@@ -264,7 +264,7 @@ static void ring_buffer_producer(void)
 			cond_resched();
 #endif
 
-	} while (end_tv.tv_sec < (start_tv.tv_sec + RUN_TIME) && !kill_test);
+	} while (ktime_before(end_time, timeout) && !kill_test);
 	trace_printk("End ring buffer hammer\n");
 
 	if (consumer) {
@@ -280,9 +280,7 @@ static void ring_buffer_producer(void)
 		wait_for_completion(&read_done);
 	}
 
-	time = end_tv.tv_sec - start_tv.tv_sec;
-	time *= USEC_PER_SEC;
-	time += (long long)((long)end_tv.tv_usec - (long)start_tv.tv_usec);
+	time = ktime_us_delta(end_time, start_time);
 
 	entries = ring_buffer_entries(buffer);
 	overruns = ring_buffer_overruns(buffer);

commit f139caf2e89713687514d9db847a4fa2e29c87a2
Author: Kirill Tkhai <ktkhai@parallels.com>
Date:   Fri Sep 12 17:40:54 2014 +0400

    sched, cleanup, treewide: Remove set_current_state(TASK_RUNNING) after schedule()
    
    schedule(), io_schedule() and schedule_timeout() always return
    with TASK_RUNNING state set, so one more setting is unnecessary.
    
    (All places in patch are visible good, only exception is
     kiblnd_scheduler() from:
    
          drivers/staging/lustre/lnet/klnds/o2iblnd/o2iblnd_cb.c
    
     Its schedule() is one line above standard 3 lines of unified diff)
    
    No places where set_current_state() is used for mb().
    
    Signed-off-by: Kirill Tkhai <ktkhai@parallels.com>
    Signed-off-by: Peter Zijlstra (Intel) <peterz@infradead.org>
    Link: http://lkml.kernel.org/r/1410529254.3569.23.camel@tkhai
    Cc: Alasdair Kergon <agk@redhat.com>
    Cc: Anil Belur <askb23@gmail.com>
    Cc: Arnd Bergmann <arnd@arndb.de>
    Cc: Dave Kleikamp <shaggy@kernel.org>
    Cc: David Airlie <airlied@linux.ie>
    Cc: David Howells <dhowells@redhat.com>
    Cc: Dmitry Eremin <dmitry.eremin@intel.com>
    Cc: Frank Blaschka <blaschka@linux.vnet.ibm.com>
    Cc: Greg Kroah-Hartman <gregkh@linuxfoundation.org>
    Cc: Heiko Carstens <heiko.carstens@de.ibm.com>
    Cc: Helge Deller <deller@gmx.de>
    Cc: Isaac Huang <he.huang@intel.com>
    Cc: James E.J. Bottomley <JBottomley@parallels.com>
    Cc: James E.J. Bottomley <jejb@parisc-linux.org>
    Cc: J. Bruce Fields <bfields@fieldses.org>
    Cc: Jeff Dike <jdike@addtoit.com>
    Cc: Jesper Nilsson <jesper.nilsson@axis.com>
    Cc: Jiri Slaby <jslaby@suse.cz>
    Cc: Laura Abbott <lauraa@codeaurora.org>
    Cc: Liang Zhen <liang.zhen@intel.com>
    Cc: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Martin Schwidefsky <schwidefsky@de.ibm.com>
    Cc: Masaru Nomura <massa.nomura@gmail.com>
    Cc: Michael Opdenacker <michael.opdenacker@free-electrons.com>
    Cc: Mikael Starvik <starvik@axis.com>
    Cc: Mike Snitzer <snitzer@redhat.com>
    Cc: Neil Brown <neilb@suse.de>
    Cc: Oleg Drokin <green@linuxhacker.ru>
    Cc: Peng Tao <bergwolf@gmail.com>
    Cc: Richard Weinberger <richard@nod.at>
    Cc: Robert Love <robert.w.love@intel.com>
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Trond Myklebust <trond.myklebust@primarydata.com>
    Cc: Ursula Braun <ursula.braun@de.ibm.com>
    Cc: Zi Shen Lim <zlim.lnx@gmail.com>
    Cc: devel@driverdev.osuosl.org
    Cc: dm-devel@redhat.com
    Cc: dri-devel@lists.freedesktop.org
    Cc: fcoe-devel@open-fcoe.org
    Cc: jfs-discussion@lists.sourceforge.net
    Cc: linux390@de.ibm.com
    Cc: linux-afs@lists.infradead.org
    Cc: linux-cris-kernel@axis.com
    Cc: linux-kernel@vger.kernel.org
    Cc: linux-nfs@vger.kernel.org
    Cc: linux-parisc@vger.kernel.org
    Cc: linux-raid@vger.kernel.org
    Cc: linux-s390@vger.kernel.org
    Cc: linux-scsi@vger.kernel.org
    Cc: qla2xxx-upstream@qlogic.com
    Cc: user-mode-linux-devel@lists.sourceforge.net
    Cc: user-mode-linux-user@lists.sourceforge.net
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 0434ff1b808e..3f9e328c30b5 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -205,7 +205,6 @@ static void ring_buffer_consumer(void)
 			break;
 
 		schedule();
-		__set_current_state(TASK_RUNNING);
 	}
 	reader_finish = 0;
 	complete(&read_done);
@@ -379,7 +378,6 @@ static int ring_buffer_consumer_thread(void *arg)
 			break;
 
 		schedule();
-		__set_current_state(TASK_RUNNING);
 	}
 	__set_current_state(TASK_RUNNING);
 
@@ -407,7 +405,6 @@ static int ring_buffer_producer_thread(void *arg)
 		trace_printk("Sleeping for 10 secs\n");
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule_timeout(HZ * SLEEP_TIME);
-		__set_current_state(TASK_RUNNING);
 	}
 
 	if (kill_test)

commit 2b3942e4bb20ef8ef26515bd958c2df83d9a6210
Author: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
Date:   Mon Feb 24 22:12:01 2014 +0800

    trace: Replace hardcoding of 19 with MAX_NICE
    
    Use MAX_NICE instead of the value 19 for ring_buffer_benchmark.
    
    Signed-off-by: Dongsheng Yang <yangds.fnst@cn.fujitsu.com>
    Signed-off-by: Peter Zijlstra <peterz@infradead.org>
    Acked-by: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Link: http://lkml.kernel.org/r/1393251121-25534-1-git-send-email-yangds.fnst@cn.fujitsu.com
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index a5457d577b98..0434ff1b808e 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -40,8 +40,8 @@ static int write_iteration = 50;
 module_param(write_iteration, uint, 0644);
 MODULE_PARM_DESC(write_iteration, "# of writes between timestamp readings");
 
-static int producer_nice = 19;
-static int consumer_nice = 19;
+static int producer_nice = MAX_NICE;
+static int consumer_nice = MAX_NICE;
 
 static int producer_fifo = -1;
 static int consumer_fifo = -1;
@@ -308,7 +308,7 @@ static void ring_buffer_producer(void)
 
 	/* Let the user know that the test is running at low priority */
 	if (producer_fifo < 0 && consumer_fifo < 0 &&
-	    producer_nice == 19 && consumer_nice == 19)
+	    producer_nice == MAX_NICE && consumer_nice == MAX_NICE)
 		trace_printk("WARNING!!! This test is running at lowest priority.\n");
 
 	trace_printk("Time:     %lld (usecs)\n", time);

commit 7ea5906405a1f3fc1c0033dfd7e02f2cfd1de5e5
Author: Vaibhav Nagarnaik <vnagarnaik@google.com>
Date:   Tue May 3 17:56:42 2011 -0700

    tracing: Use NUMA allocation for per-cpu ring buffer pages
    
    The tracing ring buffer is a group of per-cpu ring buffers where
    allocation and logging is done on a per-cpu basis. The events that are
    generated on a particular CPU are logged in the corresponding buffer.
    This is to provide wait-free writes between CPUs and good NUMA node
    locality while accessing the ring buffer.
    
    However, the allocation routines consider NUMA locality only for buffer
    page metadata and not for the actual buffer page. This causes the pages
    to be allocated on the NUMA node local to the CPU where the allocation
    routine is running at the time.
    
    This patch fixes the problem by using a NUMA node specific allocation
    routine so that the pages are allocated from a NUMA node local to the
    logging CPU.
    
    I tested with the getuid_microbench from autotest. It is a simple binary
    that calls getuid() in a loop and measures the average time for the
    syscall to complete. The following command was used to test:
    $ getuid_microbench 1000000
    
    Compared the numbers found on kernel with and without this patch and
    found that logging latency decreases by 30-50 ns/call.
    tracing with non-NUMA allocation - 569 ns/call
    tracing with NUMA allocation     - 512 ns/call
    
    Signed-off-by: Vaibhav Nagarnaik <vnagarnaik@google.com>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Ingo Molnar <mingo@redhat.com>
    Cc: Michael Rubin <mrubin@google.com>
    Cc: David Sharp <dhsharp@google.com>
    Link: http://lkml.kernel.org/r/1304470602-20366-1-git-send-email-vnagarnaik@google.com
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 302f8a614635..a5457d577b98 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -106,7 +106,7 @@ static enum event_status read_page(int cpu)
 	int inc;
 	int i;
 
-	bpage = ring_buffer_alloc_read_page(buffer);
+	bpage = ring_buffer_alloc_read_page(buffer, cpu);
 	if (!bpage)
 		return EVENT_DROPPED;
 

commit a838b2e634405fb89ddbf4fa9412acb33911911f
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Apr 27 13:26:58 2010 -0400

    ring-buffer: Make benchmark handle missed events
    
    With the addition of the "missed events" flags that is stored in the
    commit field of the ring buffer page, the ring_buffer_benchmark
    was not updated to handle this. If events are missed, then the
    missed events flag is set in the ring buffer page, the benchmark
    will count that flag as part of the size of the page and will hit the BUG()
    when it tries to read beyond the page.
    
    The solution is simply to have the ring buffer benchmark mask off
    the extra bits.
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index dc56556b55a2..302f8a614635 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -113,7 +113,8 @@ static enum event_status read_page(int cpu)
 	ret = ring_buffer_read_page(buffer, &bpage, PAGE_SIZE, cpu, 1);
 	if (ret >= 0) {
 		rpage = bpage;
-		commit = local_read(&rpage->commit);
+		/* The commit may have missed event flags set, clear them */
+		commit = local_read(&rpage->commit) & 0xfffff;
 		for (i = 0; i < commit && !kill_test; i += inc) {
 
 			if (i >= (PAGE_SIZE - offsetof(struct rb_page, data))) {

commit 66a8cb95ed04025664d1db4e952155ee1dccd048
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Mar 31 13:21:56 2010 -0400

    ring-buffer: Add place holder recording of dropped events
    
    Currently, when the ring buffer drops events, it does not record
    the fact that it did so. It does inform the writer that the event
    was dropped by returning a NULL event, but it does not put in any
    place holder where the event was dropped.
    
    This is not a trivial thing to add because the ring buffer mostly
    runs in overwrite (flight recorder) mode. That is, when the ring
    buffer is full, new data will overwrite old data.
    
    In a produce/consumer mode, where new data is simply dropped when
    the ring buffer is full, it is trivial to add the placeholder
    for dropped events. When there's more room to write new data, then
    a special event can be added to notify the reader about the dropped
    events.
    
    But in overwrite mode, any new write can overwrite events. A place
    holder can not be inserted into the ring buffer since there never
    may be room. A reader could also come in at anytime and miss the
    placeholder.
    
    Luckily, the way the ring buffer works, the read side can find out
    if events were lost or not, and how many events. Everytime a write
    takes place, if it overwrites the header page (the next read) it
    updates a "overrun" variable that keeps track of the number of
    lost events. When a reader swaps out a page from the ring buffer,
    it can record this number, perfom the swap, and then check to
    see if the number changed, and take the diff if it has, which would be
    the number of events dropped. This can be stored by the reader
    and returned to callers of the reader.
    
    Since the reader page swap will fail if the writer moved the head
    page since the time the reader page set up the swap, this gives room
    to record the overruns without worrying about races. If the reader
    sets up the pages, records the overrun, than performs the swap,
    if the swap succeeds, then the overrun variable has not been
    updated since the setup before the swap.
    
    For binary readers of the ring buffer, a flag is set in the header
    of each sub page (sub buffer) of the ring buffer. This flag is embedded
    in the size field of the data on the sub buffer, in the 31st bit (the size
    can be 32 or 64 bits depending on the architecture), but only 27
    bits needs to be used for the actual size (less actually).
    
    We could add a new field in the sub buffer header to also record the
    number of events dropped since the last read, but this will change the
    format of the binary ring buffer a bit too much. Perhaps this change can
    be made if the information on the number of events dropped is considered
    important enough.
    
    Note, the notification of dropped events is only used by consuming reads
    or peeking at the ring buffer. Iterating over the ring buffer does not
    keep this information because the necessary data is only available when
    a page swap is made, and the iterator does not swap out pages.
    
    Cc: Robert Richter <robert.richter@amd.com>
    Cc: Andi Kleen <andi@firstfloor.org>
    Cc: Li Zefan <lizf@cn.fujitsu.com>
    Cc: Arnaldo Carvalho de Melo <acme@redhat.com>
    Cc: "Luis Claudio R. Goncalves" <lclaudio@uudg.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index df74c7982255..dc56556b55a2 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -81,7 +81,7 @@ static enum event_status read_event(int cpu)
 	int *entry;
 	u64 ts;
 
-	event = ring_buffer_consume(buffer, cpu, &ts);
+	event = ring_buffer_consume(buffer, cpu, &ts, NULL);
 	if (!event)
 		return EVENT_DROPPED;
 

commit 79615760f380ec86cd58204744e774c33fab9211
Author: Christoph Lameter <cl@linux-foundation.org>
Date:   Tue Jan 5 15:34:50 2010 +0900

    local_t: Move local.h include to ringbuffer.c and ring_buffer_benchmark.c
    
    ringbuffer*.c are the last users of local.h.
    
    Remove the include from modules.h and add it to ringbuffer files.
    
    Signed-off-by: Christoph Lameter <cl@linux-foundation.org>
    Signed-off-by: Tejun Heo <tj@kernel.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index b2477caf09c2..df74c7982255 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -8,6 +8,7 @@
 #include <linux/kthread.h>
 #include <linux/module.h>
 #include <linux/time.h>
+#include <asm/local.h>
 
 struct rb_page {
 	u64		ts;

commit 7ac074340480018681a0d72b324d4487543bdc0e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 25 13:22:21 2009 -0500

    ring-buffer-benchmark: Add parameters to set produce/consumer priorities
    
    Running the ring-buffer-benchmark's threads at the lowest priority may
    work well for keeping it in the background, but it is not appropriate
    for the benchmarks.
    
    This patch adds 4 parameters to the module:
    
      consumer_fifo
      consumer_nice
      producer_fifo
      producer_nice
    
    By default the consumer and producer still run at nice +19.
    
    If the *_fifo options are set, they will override the *_nice values.
    
     modprobe ring_buffer_benchmark consumer_nice=0 producer_fifo=10
    
    The above will set the consumer thread to a nice value of 0, and
    the producer thread to a RT SCHED_FIFO priority of 10.
    
    Note, this patch also fixes a bug where calling set_user_nice on the
    consumer thread would oops the kernel when the parameter "disable_reader"
    is set.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 3875d49da990..b2477caf09c2 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -39,6 +39,24 @@ static int write_iteration = 50;
 module_param(write_iteration, uint, 0644);
 MODULE_PARM_DESC(write_iteration, "# of writes between timestamp readings");
 
+static int producer_nice = 19;
+static int consumer_nice = 19;
+
+static int producer_fifo = -1;
+static int consumer_fifo = -1;
+
+module_param(producer_nice, uint, 0644);
+MODULE_PARM_DESC(producer_nice, "nice prio for producer");
+
+module_param(consumer_nice, uint, 0644);
+MODULE_PARM_DESC(consumer_nice, "nice prio for consumer");
+
+module_param(producer_fifo, uint, 0644);
+MODULE_PARM_DESC(producer_fifo, "fifo prio for producer");
+
+module_param(consumer_fifo, uint, 0644);
+MODULE_PARM_DESC(consumer_fifo, "fifo prio for consumer");
+
 static int read_events;
 
 static int kill_test;
@@ -270,6 +288,27 @@ static void ring_buffer_producer(void)
 
 	if (kill_test)
 		trace_printk("ERROR!\n");
+
+	if (!disable_reader) {
+		if (consumer_fifo < 0)
+			trace_printk("Running Consumer at nice: %d\n",
+				     consumer_nice);
+		else
+			trace_printk("Running Consumer at SCHED_FIFO %d\n",
+				     consumer_fifo);
+	}
+	if (producer_fifo < 0)
+		trace_printk("Running Producer at nice: %d\n",
+			     producer_nice);
+	else
+		trace_printk("Running Producer at SCHED_FIFO %d\n",
+			     producer_fifo);
+
+	/* Let the user know that the test is running at low priority */
+	if (producer_fifo < 0 && consumer_fifo < 0 &&
+	    producer_nice == 19 && consumer_nice == 19)
+		trace_printk("WARNING!!! This test is running at lowest priority.\n");
+
 	trace_printk("Time:     %lld (usecs)\n", time);
 	trace_printk("Overruns: %lld\n", overruns);
 	if (disable_reader)
@@ -402,8 +441,23 @@ static int __init ring_buffer_benchmark_init(void)
 	/*
 	 * Run them as low-prio background tasks by default:
 	 */
-	set_user_nice(consumer, 19);
-	set_user_nice(producer, 19);
+	if (!disable_reader) {
+		if (consumer_fifo >= 0) {
+			struct sched_param param = {
+				.sched_priority = consumer_fifo
+			};
+			sched_setscheduler(consumer, SCHED_FIFO, &param);
+		} else
+			set_user_nice(consumer, consumer_nice);
+	}
+
+	if (producer_fifo >= 0) {
+		struct sched_param param = {
+			.sched_priority = consumer_fifo
+		};
+		sched_setscheduler(producer, SCHED_FIFO, &param);
+	} else
+		set_user_nice(producer, producer_nice);
 
 	return 0;
 

commit 98e4833ba3c314c99dc364012fba6ac894230ad0
Author: Ingo Molnar <mingo@elte.hu>
Date:   Mon Nov 23 08:03:09 2009 +0100

    ring-buffer benchmark: Run producer/consumer threads at nice +19
    
    The ring-buffer benchmark threads run on nice 0 by default, using
    up a lot of CPU time and slowing down the system:
    
       PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
      1024 root      20   0     0    0    0 D 95.3  0.0   4:01.67 rb_producer
      1023 root      20   0     0    0    0 R 93.5  0.0   2:54.33 rb_consumer
     21569 mingo     40   0 14852 1048  772 R  3.6  0.1   0:00.05 top
         1 root      40   0  4080  928  668 S  0.0  0.0   0:23.98 init
    
    Renice them to +19 to make them less intrusive.
    
    Cc: Steven Rostedt <rostedt@goodmis.org>
    Cc: Frederic Weisbecker <fweisbec@gmail.com>
    Cc: Peter Zijlstra <a.p.zijlstra@chello.nl>
    Cc: Mike Galbraith <efault@gmx.de>
    LKML-Reference: <new-submission>
    Signed-off-by: Ingo Molnar <mingo@elte.hu>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 70df73e4ff21..3875d49da990 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -399,6 +399,12 @@ static int __init ring_buffer_benchmark_init(void)
 	if (IS_ERR(producer))
 		goto out_kill;
 
+	/*
+	 * Run them as low-prio background tasks by default:
+	 */
+	set_user_nice(consumer, 19);
+	set_user_nice(producer, 19);
+
 	return 0;
 
  out_kill:

commit a6f0eb6adc42e5eed3f35af99c61c0e411b16f8e
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Nov 11 17:14:07 2009 -0500

    ring-buffer: Add multiple iterations between benchmark timestamps
    
    The ring_buffer_benchmark does a gettimeofday after every write to the
    ring buffer in its measurements. This adds the overhead of the call
    to gettimeofday to the measurements and does not give an accurate picture
    of the length of time it takes to record a trace.
    
    This was first noticed with perf top:
    
    ------------------------------------------------------------------------------
       PerfTop:     679 irqs/sec  kernel:99.9% [1000Hz cpu-clock-msecs],  (all, 4 CPUs)
    ------------------------------------------------------------------------------
    
                 samples    pcnt   kernel function
                 _______   _____   _______________
    
                 1673.00 - 27.8% : trace_clock_local
                  806.00 - 13.4% : do_gettimeofday
                  590.00 -  9.8% : rb_reserve_next_event
                  554.00 -  9.2% : native_read_tsc
                  431.00 -  7.2% : ring_buffer_lock_reserve
                  365.00 -  6.1% : __rb_reserve_next
                  355.00 -  5.9% : rb_end_commit
                  322.00 -  5.4% : getnstimeofday
                  268.00 -  4.5% : ring_buffer_unlock_commit
                  262.00 -  4.4% : ring_buffer_producer_thread      [ring_buffer_benchmark]
                  113.00 -  1.9% : read_tsc
                   91.00 -  1.5% : debug_smp_processor_id
                   69.00 -  1.1% : trace_recursive_unlock
                   66.00 -  1.1% : ring_buffer_event_data
                   25.00 -  0.4% : _spin_unlock_irq
    
    And the length of each write to the ring buffer measured at 310ns.
    
    This patch adds a new module parameter called "write_interval" which is
    defaulted to 50. This is the number of writes performed between
    timestamps. After this patch perf top shows:
    
    ------------------------------------------------------------------------------
       PerfTop:     244 irqs/sec  kernel:100.0% [1000Hz cpu-clock-msecs],  (all, 4 CPUs)
    ------------------------------------------------------------------------------
    
                 samples    pcnt   kernel function
                 _______   _____   _______________
    
                 2842.00 - 40.4% : trace_clock_local
                 1043.00 - 14.8% : rb_reserve_next_event
                  784.00 - 11.1% : ring_buffer_lock_reserve
                  600.00 -  8.5% : __rb_reserve_next
                  579.00 -  8.2% : rb_end_commit
                  440.00 -  6.3% : ring_buffer_unlock_commit
                  290.00 -  4.1% : ring_buffer_producer_thread      [ring_buffer_benchmark]
                  155.00 -  2.2% : debug_smp_processor_id
                  117.00 -  1.7% : trace_recursive_unlock
                  103.00 -  1.5% : ring_buffer_event_data
                   28.00 -  0.4% : do_gettimeofday
                   22.00 -  0.3% : _spin_unlock_irq
                   14.00 -  0.2% : native_read_tsc
                   11.00 -  0.2% : getnstimeofday
    
    do_gettimeofday dropped from 13% usage to a mere 0.4%! (using the default
    50 interval)  The measurement for each timestamp went from 310ns to 210ns.
    That's 100ns (1/3rd) overhead that the gettimeofday call was introducing.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 573d3cc762c3..70df73e4ff21 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -35,6 +35,10 @@ static int disable_reader;
 module_param(disable_reader, uint, 0644);
 MODULE_PARM_DESC(disable_reader, "only run producer");
 
+static int write_iteration = 50;
+module_param(write_iteration, uint, 0644);
+MODULE_PARM_DESC(write_iteration, "# of writes between timestamp readings");
+
 static int read_events;
 
 static int kill_test;
@@ -208,15 +212,18 @@ static void ring_buffer_producer(void)
 	do {
 		struct ring_buffer_event *event;
 		int *entry;
-
-		event = ring_buffer_lock_reserve(buffer, 10);
-		if (!event) {
-			missed++;
-		} else {
-			hit++;
-			entry = ring_buffer_event_data(event);
-			*entry = smp_processor_id();
-			ring_buffer_unlock_commit(buffer, event);
+		int i;
+
+		for (i = 0; i < write_iteration; i++) {
+			event = ring_buffer_lock_reserve(buffer, 10);
+			if (!event) {
+				missed++;
+			} else {
+				hit++;
+				entry = ring_buffer_event_data(event);
+				*entry = smp_processor_id();
+				ring_buffer_unlock_commit(buffer, event);
+			}
 		}
 		do_gettimeofday(&end_tv);
 

commit 4b221f0313f0f7f1f7aa0a1fd16ad400840def26
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed Jun 17 17:01:09 2009 -0400

    ring-buffer: have benchmark test print to trace buffer
    
    Currently the output of the ring buffer benchmark/test prints to
    the console. This test runs for ten seconds every ten seconds and
    ouputs the result after every iteration. This needlessly fills up
    the logs.
    
    This patch makes the ring buffer benchmark/test print to the ftrace
    buffer using trace_printk. To view the test results, you must examine
    the debug/tracing/trace file.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index cf6b0f50134e..573d3cc762c3 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -203,7 +203,7 @@ static void ring_buffer_producer(void)
 	 * Hammer the buffer for 10 secs (this may
 	 * make the system stall)
 	 */
-	pr_info("Starting ring buffer hammer\n");
+	trace_printk("Starting ring buffer hammer\n");
 	do_gettimeofday(&start_tv);
 	do {
 		struct ring_buffer_event *event;
@@ -239,7 +239,7 @@ static void ring_buffer_producer(void)
 #endif
 
 	} while (end_tv.tv_sec < (start_tv.tv_sec + RUN_TIME) && !kill_test);
-	pr_info("End ring buffer hammer\n");
+	trace_printk("End ring buffer hammer\n");
 
 	if (consumer) {
 		/* Init both completions here to avoid races */
@@ -262,49 +262,50 @@ static void ring_buffer_producer(void)
 	overruns = ring_buffer_overruns(buffer);
 
 	if (kill_test)
-		pr_info("ERROR!\n");
-	pr_info("Time:     %lld (usecs)\n", time);
-	pr_info("Overruns: %lld\n", overruns);
+		trace_printk("ERROR!\n");
+	trace_printk("Time:     %lld (usecs)\n", time);
+	trace_printk("Overruns: %lld\n", overruns);
 	if (disable_reader)
-		pr_info("Read:     (reader disabled)\n");
+		trace_printk("Read:     (reader disabled)\n");
 	else
-		pr_info("Read:     %ld  (by %s)\n", read,
+		trace_printk("Read:     %ld  (by %s)\n", read,
 			read_events ? "events" : "pages");
-	pr_info("Entries:  %lld\n", entries);
-	pr_info("Total:    %lld\n", entries + overruns + read);
-	pr_info("Missed:   %ld\n", missed);
-	pr_info("Hit:      %ld\n", hit);
+	trace_printk("Entries:  %lld\n", entries);
+	trace_printk("Total:    %lld\n", entries + overruns + read);
+	trace_printk("Missed:   %ld\n", missed);
+	trace_printk("Hit:      %ld\n", hit);
 
 	/* Convert time from usecs to millisecs */
 	do_div(time, USEC_PER_MSEC);
 	if (time)
 		hit /= (long)time;
 	else
-		pr_info("TIME IS ZERO??\n");
+		trace_printk("TIME IS ZERO??\n");
 
-	pr_info("Entries per millisec: %ld\n", hit);
+	trace_printk("Entries per millisec: %ld\n", hit);
 
 	if (hit) {
 		/* Calculate the average time in nanosecs */
 		avg = NSEC_PER_MSEC / hit;
-		pr_info("%ld ns per entry\n", avg);
+		trace_printk("%ld ns per entry\n", avg);
 	}
 
 	if (missed) {
 		if (time)
 			missed /= (long)time;
 
-		pr_info("Total iterations per millisec: %ld\n", hit + missed);
+		trace_printk("Total iterations per millisec: %ld\n",
+			     hit + missed);
 
 		/* it is possible that hit + missed will overflow and be zero */
 		if (!(hit + missed)) {
-			pr_info("hit + missed overflowed and totalled zero!\n");
+			trace_printk("hit + missed overflowed and totalled zero!\n");
 			hit--; /* make it non zero */
 		}
 
 		/* Caculate the average time in nanosecs */
 		avg = NSEC_PER_MSEC / (hit + missed);
-		pr_info("%ld ns per entry\n", avg);
+		trace_printk("%ld ns per entry\n", avg);
 	}
 }
 
@@ -355,7 +356,7 @@ static int ring_buffer_producer_thread(void *arg)
 
 		ring_buffer_producer();
 
-		pr_info("Sleeping for 10 secs\n");
+		trace_printk("Sleeping for 10 secs\n");
 		set_current_state(TASK_INTERRUPTIBLE);
 		schedule_timeout(HZ * SLEEP_TIME);
 		__set_current_state(TASK_RUNNING);

commit 9086c7b90abbf4ec29543e8f2424e3ecd14e955d
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue Jun 16 11:46:09 2009 -0400

    ring-buffer: have benchmark test handle discarded events
    
    With the addition of commit:
    
      c7b0930857e2278f2e7714db6294e94c57f623b0
      ring-buffer: prevent adding write in discarded area
    
    The ring buffer may now add discarded events when a write passes
    the end of a buffer page. Before, a discarded event was only added
    when the tracer deliberately created one. The ring buffer benchmark
    test does not handle discarded events when it reads the buffer and
    fails when it encounters one.
    
    Also fix the increment for large data entries (luckily, the test did
    not add any yet).
    
    [ Impact: fix false failure of ring buffer self test ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 8d68e149a8b3..cf6b0f50134e 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -102,8 +102,10 @@ static enum event_status read_page(int cpu)
 			event = (void *)&rpage->data[i];
 			switch (event->type_len) {
 			case RINGBUF_TYPE_PADDING:
-				/* We don't expect any padding */
-				KILL_TEST();
+				/* failed writes may be discarded events */
+				if (!event->time_delta)
+					KILL_TEST();
+				inc = event->array[0] + 4;
 				break;
 			case RINGBUF_TYPE_TIME_EXTEND:
 				inc = 8;
@@ -119,7 +121,7 @@ static enum event_status read_page(int cpu)
 					KILL_TEST();
 					break;
 				}
-				inc = event->array[0];
+				inc = event->array[0] + 4;
 				break;
 			default:
 				entry = ring_buffer_event_data(event);

commit d988ff94c1074c4c914235c8591bcceafb585ecf
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 8 11:03:57 2009 -0400

    ring-buffer: check for divide by zero in ring-buffer-benchmark
    
    Although we check if "missed" is not zero, we divide by hit + missed,
    and the addition can possible overflow and become a divide by zero.
    
    This patch checks for this case, and will report it when it happens
    then modify "hit" to make the calculation be non zero.
    
    [ Impact: prevent possible divide by zero in ring-buffer-benchmark ]
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 7d3aef93c49f..8d68e149a8b3 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -294,6 +294,12 @@ static void ring_buffer_producer(void)
 
 		pr_info("Total iterations per millisec: %ld\n", hit + missed);
 
+		/* it is possible that hit + missed will overflow and be zero */
+		if (!(hit + missed)) {
+			pr_info("hit + missed overflowed and totalled zero!\n");
+			hit--; /* make it non zero */
+		}
+
 		/* Caculate the average time in nanosecs */
 		avg = NSEC_PER_MSEC / (hit + missed);
 		pr_info("%ld ns per entry\n", avg);

commit 5a772b2b3c68e7e0b503c5a48469113bb0634314
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Fri May 8 10:56:33 2009 -0400

    ring-buffer: replace constants with time macros in ring-buffer-benchmark
    
    The use of numeric constants is discouraged. It is cleaner and more
    descriptive to use macros for constant time conversions.
    
    This patch also removes an extra new line.
    
    [ Impact: more descriptive time conversions ]
    
    Reported-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index a21aa7b3d05e..7d3aef93c49f 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -253,7 +253,7 @@ static void ring_buffer_producer(void)
 	}
 
 	time = end_tv.tv_sec - start_tv.tv_sec;
-	time *= 1000000;
+	time *= USEC_PER_SEC;
 	time += (long long)((long)end_tv.tv_usec - (long)start_tv.tv_usec);
 
 	entries = ring_buffer_entries(buffer);
@@ -273,7 +273,8 @@ static void ring_buffer_producer(void)
 	pr_info("Missed:   %ld\n", missed);
 	pr_info("Hit:      %ld\n", hit);
 
-	do_div(time, 1000);
+	/* Convert time from usecs to millisecs */
+	do_div(time, USEC_PER_MSEC);
 	if (time)
 		hit /= (long)time;
 	else
@@ -282,18 +283,19 @@ static void ring_buffer_producer(void)
 	pr_info("Entries per millisec: %ld\n", hit);
 
 	if (hit) {
-		avg = 1000000 / hit;
+		/* Calculate the average time in nanosecs */
+		avg = NSEC_PER_MSEC / hit;
 		pr_info("%ld ns per entry\n", avg);
 	}
 
-
 	if (missed) {
 		if (time)
 			missed /= (long)time;
 
 		pr_info("Total iterations per millisec: %ld\n", hit + missed);
 
-		avg = 1000000 / (hit + missed);
+		/* Caculate the average time in nanosecs */
+		avg = NSEC_PER_MSEC / (hit + missed);
 		pr_info("%ld ns per entry\n", avg);
 	}
 }

commit 7da3046d6ce6ea97494020081c509b642b7016af
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 7 19:52:20 2009 -0400

    ring-buffer: add total count in ring-buffer-benchmark
    
    It is nice to see the overhead of the benchmark test when tracing is
    disabled. That is, we turn off the ring buffer just to see what the
    cost of running the loop that calls into the ring buffer is.
    
    Currently, if no entries wer made, we get 0. This is not informative.
    This patch changes it to check if we had any "missed" (non recorded)
    events. If so, a total count is also reported.
    
    [ Impact: evaluate the over head of the ring buffer benchmark test ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index a7c048bb4466..a21aa7b3d05e 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -285,6 +285,17 @@ static void ring_buffer_producer(void)
 		avg = 1000000 / hit;
 		pr_info("%ld ns per entry\n", avg);
 	}
+
+
+	if (missed) {
+		if (time)
+			missed /= (long)time;
+
+		pr_info("Total iterations per millisec: %ld\n", hit + missed);
+
+		avg = 1000000 / (hit + missed);
+		pr_info("%ld ns per entry\n", avg);
+	}
 }
 
 static void wait_to_die(void)

commit 0574ea421b90e0e45a72c447dd3c2c79ffd8c153
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 7 14:20:28 2009 -0400

    ring-buffer: only periodically call cond_resched to ring-buffer-benchmark
    
    Calling cond_resched at every iteration of the loop adds a bit of
    overhead to the benchmark.
    
    This patch does two things.
    
    1) only calls cond-resched when CONFIG_PREEMPT is not enabled
    2) only calls cond-resched after so many traces has been performed.
    
    [ Impact: less overhead to the ring-buffer-benchmark ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index f4ceb453c7dd..a7c048bb4466 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -218,16 +218,23 @@ static void ring_buffer_producer(void)
 		}
 		do_gettimeofday(&end_tv);
 
-		if (consumer && !(++cnt % wakeup_interval))
+		cnt++;
+		if (consumer && !(cnt % wakeup_interval))
 			wake_up_process(consumer);
 
+#ifndef CONFIG_PREEMPT
 		/*
 		 * If we are a non preempt kernel, the 10 second run will
 		 * stop everything while it runs. Instead, we will call
 		 * cond_resched and also add any time that was lost by a
 		 * rescedule.
+		 *
+		 * Do a cond resched at the same frequency we would wake up
+		 * the reader.
 		 */
-		cond_resched();
+		if (cnt % wakeup_interval)
+			cond_resched();
+#endif
 
 	} while (end_tv.tv_sec < (start_tv.tv_sec + RUN_TIME) && !kill_test);
 	pr_info("End ring buffer hammer\n");

commit 29c8000ee7da3a6756d26143991e573eaaf2a9f6
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Thu May 7 11:13:42 2009 -0400

    ring-buffer: remove complex calculations in ring-buffer-test
    
    Ingo Molnar thought that the code to calculate the time in cond_resched
    is a bit too ugly and is not needed. This patch removes it and replaces
    it with a simple call to cond_resched. I kept the comment that explains
    the reason for the cond_resched.
    
    [ Impact: remove ugly code ]
    
    Reported-by: Ingo Molnar <mingo@elte.hu>
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index a26fc67b63bb..f4ceb453c7dd 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -185,35 +185,6 @@ static void ring_buffer_consumer(void)
 	complete(&read_done);
 }
 
-/*
- * If we are a non preempt kernel, the 10 second run will
- * stop everything while it runs. Instead, we will call cond_resched
- * and also add any time that was lost by a rescedule.
- */
-#ifdef CONFIG_PREEMPT
-static void sched_if_needed(struct timeval *start_tv, struct timeval *end_tv)
-{
-}
-#else
-static void sched_if_needed(struct timeval *start_tv, struct timeval *end_tv)
-{
-	struct timeval tv;
-
-	cond_resched();
-	do_gettimeofday(&tv);
-	if (tv.tv_usec < end_tv->tv_usec) {
-		tv.tv_usec += 1000000;
-		tv.tv_sec--;
-	}
-	start_tv->tv_sec += tv.tv_sec - end_tv->tv_sec;
-	start_tv->tv_usec += tv.tv_usec - end_tv->tv_usec;
-	if (start_tv->tv_usec > 1000000) {
-		start_tv->tv_usec -= 1000000;
-		start_tv->tv_sec++;
-	}
-}
-#endif
-
 static void ring_buffer_producer(void)
 {
 	struct timeval start_tv;
@@ -250,7 +221,13 @@ static void ring_buffer_producer(void)
 		if (consumer && !(++cnt % wakeup_interval))
 			wake_up_process(consumer);
 
-		sched_if_needed(&start_tv, &end_tv);
+		/*
+		 * If we are a non preempt kernel, the 10 second run will
+		 * stop everything while it runs. Instead, we will call
+		 * cond_resched and also add any time that was lost by a
+		 * rescedule.
+		 */
+		cond_resched();
 
 	} while (end_tv.tv_sec < (start_tv.tv_sec + RUN_TIME) && !kill_test);
 	pr_info("End ring buffer hammer\n");

commit 3e07a4f680adc66dfa175aa5021aedf340251b12
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 6 18:36:59 2009 -0400

    ring-buffer: change test to be more latency friendly
    
    The ring buffer benchmark/test runs a producer for 10 seconds.
    This is done with preemption and interrupts enabled. But if the kernel
    is not compiled with CONFIG_PREEMPT, it basically stops everything
    but interrupts for 10 seconds.
    
    Although this is just a test and is not for production, this attribute
    can be quite annoying. It can also spawn badness elsewhere.
    
    This patch solves the issues by calling "cond_resched" when the system
    is not compiled with CONFIG_PREEMPT. It also keeps track of the time
    spent to call cond_resched such that it does not go against the
    time calculations. That is, if the task schedules away, the time scheduled
    out is removed from the test data. Note, this only works for non PREEMPT
    because we do not know when the task is scheduled out if we have PREEMPT
    enabled.
    
    [ Impact: prevent test from stopping the world for 10 seconds ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index dcd75e9e49f4..a26fc67b63bb 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -185,6 +185,35 @@ static void ring_buffer_consumer(void)
 	complete(&read_done);
 }
 
+/*
+ * If we are a non preempt kernel, the 10 second run will
+ * stop everything while it runs. Instead, we will call cond_resched
+ * and also add any time that was lost by a rescedule.
+ */
+#ifdef CONFIG_PREEMPT
+static void sched_if_needed(struct timeval *start_tv, struct timeval *end_tv)
+{
+}
+#else
+static void sched_if_needed(struct timeval *start_tv, struct timeval *end_tv)
+{
+	struct timeval tv;
+
+	cond_resched();
+	do_gettimeofday(&tv);
+	if (tv.tv_usec < end_tv->tv_usec) {
+		tv.tv_usec += 1000000;
+		tv.tv_sec--;
+	}
+	start_tv->tv_sec += tv.tv_sec - end_tv->tv_sec;
+	start_tv->tv_usec += tv.tv_usec - end_tv->tv_usec;
+	if (start_tv->tv_usec > 1000000) {
+		start_tv->tv_usec -= 1000000;
+		start_tv->tv_sec++;
+	}
+}
+#endif
+
 static void ring_buffer_producer(void)
 {
 	struct timeval start_tv;
@@ -221,6 +250,8 @@ static void ring_buffer_producer(void)
 		if (consumer && !(++cnt % wakeup_interval))
 			wake_up_process(consumer);
 
+		sched_if_needed(&start_tv, &end_tv);
+
 	} while (end_tv.tv_sec < (start_tv.tv_sec + RUN_TIME) && !kill_test);
 	pr_info("End ring buffer hammer\n");
 

commit 00c81a58c5b4e0de14ee33bfbc3d71c90f69f9ea
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Wed May 6 12:40:51 2009 -0400

    ring-buffer: check for failed allocation in ring buffer benchmark
    
    The result of the allocation of the ring buffer read page in the
    ring buffer bench mark does not check the return to see if a page
    was actually allocated. This patch fixes that.
    
    [ Impact: avoid NULL dereference ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
index 747244acb8fd..dcd75e9e49f4 100644
--- a/kernel/trace/ring_buffer_benchmark.c
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -84,6 +84,9 @@ static enum event_status read_page(int cpu)
 	int i;
 
 	bpage = ring_buffer_alloc_read_page(buffer);
+	if (!bpage)
+		return EVENT_DROPPED;
+
 	ret = ring_buffer_read_page(buffer, &bpage, PAGE_SIZE, cpu, 1);
 	if (ret >= 0) {
 		rpage = bpage;

commit 5092dbc96f3acdac5433b27c06860352dc6d23b9
Author: Steven Rostedt <srostedt@redhat.com>
Date:   Tue May 5 22:47:18 2009 -0400

    ring-buffer: add benchmark and tester
    
    This patch adds code that can benchmark the ring buffer as well as
    test it. This code can be compiled into the kernel (not recommended)
    or as a module.
    
    A separate ring buffer is used to not interfer with other users, like
    ftrace. It creates a producer and a consumer (option to disable creation
    of the consumer) and will run for 10 seconds, then sleep for 10 seconds
    and then repeat.
    
    While running, the producer will write 10 byte loads into the ring
    buffer with just putting in the current CPU number. The reader will
    continually try to read the buffer. The reader will alternate from reading
    the buffer via event by event, or by full pages.
    
    The output is a pr_info, thus it will fill up the syslogs.
    
      Starting ring buffer hammer
      End ring buffer hammer
      Time:     9000349 (usecs)
      Overruns: 12578640
      Read:     5358440  (by events)
      Entries:  0
      Total:    17937080
      Missed:   0
      Hit:      17937080
      Entries per millisec: 1993
      501 ns per entry
      Sleeping for 10 secs
      Starting ring buffer hammer
      End ring buffer hammer
      Time:     9936350 (usecs)
      Overruns: 0
      Read:     28146644  (by pages)
      Entries:  74
      Total:    28146718
      Missed:   0
      Hit:      28146718
      Entries per millisec: 2832
      353 ns per entry
      Sleeping for 10 secs
    
    Time:      is the time the test ran
    Overruns:  the number of events that were overwritten and not read
    Read:      the number of events read (either by pages or events)
    Entries:   the number of entries left in the buffer
                     (the by pages will only read full pages)
    Total:     Entries + Read + Overruns
    Missed:    the number of entries that failed to write
    Hit:       the number of entries that were written
    
    The above example shows that it takes ~353 nanosecs per entry when
    there is a reader, reading by pages (and no overruns)
    
    The event by event reader slowed the producer down to 501 nanosecs.
    
    [ Impact: see how changes to the ring buffer affect stability and performance ]
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/trace/ring_buffer_benchmark.c b/kernel/trace/ring_buffer_benchmark.c
new file mode 100644
index 000000000000..747244acb8fd
--- /dev/null
+++ b/kernel/trace/ring_buffer_benchmark.c
@@ -0,0 +1,379 @@
+/*
+ * ring buffer tester and benchmark
+ *
+ * Copyright (C) 2009 Steven Rostedt <srostedt@redhat.com>
+ */
+#include <linux/ring_buffer.h>
+#include <linux/completion.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/time.h>
+
+struct rb_page {
+	u64		ts;
+	local_t		commit;
+	char		data[4080];
+};
+
+/* run time and sleep time in seconds */
+#define RUN_TIME	10
+#define SLEEP_TIME	10
+
+/* number of events for writer to wake up the reader */
+static int wakeup_interval = 100;
+
+static int reader_finish;
+static struct completion read_start;
+static struct completion read_done;
+
+static struct ring_buffer *buffer;
+static struct task_struct *producer;
+static struct task_struct *consumer;
+static unsigned long read;
+
+static int disable_reader;
+module_param(disable_reader, uint, 0644);
+MODULE_PARM_DESC(disable_reader, "only run producer");
+
+static int read_events;
+
+static int kill_test;
+
+#define KILL_TEST()				\
+	do {					\
+		if (!kill_test) {		\
+			kill_test = 1;		\
+			WARN_ON(1);		\
+		}				\
+	} while (0)
+
+enum event_status {
+	EVENT_FOUND,
+	EVENT_DROPPED,
+};
+
+static enum event_status read_event(int cpu)
+{
+	struct ring_buffer_event *event;
+	int *entry;
+	u64 ts;
+
+	event = ring_buffer_consume(buffer, cpu, &ts);
+	if (!event)
+		return EVENT_DROPPED;
+
+	entry = ring_buffer_event_data(event);
+	if (*entry != cpu) {
+		KILL_TEST();
+		return EVENT_DROPPED;
+	}
+
+	read++;
+	return EVENT_FOUND;
+}
+
+static enum event_status read_page(int cpu)
+{
+	struct ring_buffer_event *event;
+	struct rb_page *rpage;
+	unsigned long commit;
+	void *bpage;
+	int *entry;
+	int ret;
+	int inc;
+	int i;
+
+	bpage = ring_buffer_alloc_read_page(buffer);
+	ret = ring_buffer_read_page(buffer, &bpage, PAGE_SIZE, cpu, 1);
+	if (ret >= 0) {
+		rpage = bpage;
+		commit = local_read(&rpage->commit);
+		for (i = 0; i < commit && !kill_test; i += inc) {
+
+			if (i >= (PAGE_SIZE - offsetof(struct rb_page, data))) {
+				KILL_TEST();
+				break;
+			}
+
+			inc = -1;
+			event = (void *)&rpage->data[i];
+			switch (event->type_len) {
+			case RINGBUF_TYPE_PADDING:
+				/* We don't expect any padding */
+				KILL_TEST();
+				break;
+			case RINGBUF_TYPE_TIME_EXTEND:
+				inc = 8;
+				break;
+			case 0:
+				entry = ring_buffer_event_data(event);
+				if (*entry != cpu) {
+					KILL_TEST();
+					break;
+				}
+				read++;
+				if (!event->array[0]) {
+					KILL_TEST();
+					break;
+				}
+				inc = event->array[0];
+				break;
+			default:
+				entry = ring_buffer_event_data(event);
+				if (*entry != cpu) {
+					KILL_TEST();
+					break;
+				}
+				read++;
+				inc = ((event->type_len + 1) * 4);
+			}
+			if (kill_test)
+				break;
+
+			if (inc <= 0) {
+				KILL_TEST();
+				break;
+			}
+		}
+	}
+	ring_buffer_free_read_page(buffer, bpage);
+
+	if (ret < 0)
+		return EVENT_DROPPED;
+	return EVENT_FOUND;
+}
+
+static void ring_buffer_consumer(void)
+{
+	/* toggle between reading pages and events */
+	read_events ^= 1;
+
+	read = 0;
+	while (!reader_finish && !kill_test) {
+		int found;
+
+		do {
+			int cpu;
+
+			found = 0;
+			for_each_online_cpu(cpu) {
+				enum event_status stat;
+
+				if (read_events)
+					stat = read_event(cpu);
+				else
+					stat = read_page(cpu);
+
+				if (kill_test)
+					break;
+				if (stat == EVENT_FOUND)
+					found = 1;
+			}
+		} while (found && !kill_test);
+
+		set_current_state(TASK_INTERRUPTIBLE);
+		if (reader_finish)
+			break;
+
+		schedule();
+		__set_current_state(TASK_RUNNING);
+	}
+	reader_finish = 0;
+	complete(&read_done);
+}
+
+static void ring_buffer_producer(void)
+{
+	struct timeval start_tv;
+	struct timeval end_tv;
+	unsigned long long time;
+	unsigned long long entries;
+	unsigned long long overruns;
+	unsigned long missed = 0;
+	unsigned long hit = 0;
+	unsigned long avg;
+	int cnt = 0;
+
+	/*
+	 * Hammer the buffer for 10 secs (this may
+	 * make the system stall)
+	 */
+	pr_info("Starting ring buffer hammer\n");
+	do_gettimeofday(&start_tv);
+	do {
+		struct ring_buffer_event *event;
+		int *entry;
+
+		event = ring_buffer_lock_reserve(buffer, 10);
+		if (!event) {
+			missed++;
+		} else {
+			hit++;
+			entry = ring_buffer_event_data(event);
+			*entry = smp_processor_id();
+			ring_buffer_unlock_commit(buffer, event);
+		}
+		do_gettimeofday(&end_tv);
+
+		if (consumer && !(++cnt % wakeup_interval))
+			wake_up_process(consumer);
+
+	} while (end_tv.tv_sec < (start_tv.tv_sec + RUN_TIME) && !kill_test);
+	pr_info("End ring buffer hammer\n");
+
+	if (consumer) {
+		/* Init both completions here to avoid races */
+		init_completion(&read_start);
+		init_completion(&read_done);
+		/* the completions must be visible before the finish var */
+		smp_wmb();
+		reader_finish = 1;
+		/* finish var visible before waking up the consumer */
+		smp_wmb();
+		wake_up_process(consumer);
+		wait_for_completion(&read_done);
+	}
+
+	time = end_tv.tv_sec - start_tv.tv_sec;
+	time *= 1000000;
+	time += (long long)((long)end_tv.tv_usec - (long)start_tv.tv_usec);
+
+	entries = ring_buffer_entries(buffer);
+	overruns = ring_buffer_overruns(buffer);
+
+	if (kill_test)
+		pr_info("ERROR!\n");
+	pr_info("Time:     %lld (usecs)\n", time);
+	pr_info("Overruns: %lld\n", overruns);
+	if (disable_reader)
+		pr_info("Read:     (reader disabled)\n");
+	else
+		pr_info("Read:     %ld  (by %s)\n", read,
+			read_events ? "events" : "pages");
+	pr_info("Entries:  %lld\n", entries);
+	pr_info("Total:    %lld\n", entries + overruns + read);
+	pr_info("Missed:   %ld\n", missed);
+	pr_info("Hit:      %ld\n", hit);
+
+	do_div(time, 1000);
+	if (time)
+		hit /= (long)time;
+	else
+		pr_info("TIME IS ZERO??\n");
+
+	pr_info("Entries per millisec: %ld\n", hit);
+
+	if (hit) {
+		avg = 1000000 / hit;
+		pr_info("%ld ns per entry\n", avg);
+	}
+}
+
+static void wait_to_die(void)
+{
+	set_current_state(TASK_INTERRUPTIBLE);
+	while (!kthread_should_stop()) {
+		schedule();
+		set_current_state(TASK_INTERRUPTIBLE);
+	}
+	__set_current_state(TASK_RUNNING);
+}
+
+static int ring_buffer_consumer_thread(void *arg)
+{
+	while (!kthread_should_stop() && !kill_test) {
+		complete(&read_start);
+
+		ring_buffer_consumer();
+
+		set_current_state(TASK_INTERRUPTIBLE);
+		if (kthread_should_stop() || kill_test)
+			break;
+
+		schedule();
+		__set_current_state(TASK_RUNNING);
+	}
+	__set_current_state(TASK_RUNNING);
+
+	if (kill_test)
+		wait_to_die();
+
+	return 0;
+}
+
+static int ring_buffer_producer_thread(void *arg)
+{
+	init_completion(&read_start);
+
+	while (!kthread_should_stop() && !kill_test) {
+		ring_buffer_reset(buffer);
+
+		if (consumer) {
+			smp_wmb();
+			wake_up_process(consumer);
+			wait_for_completion(&read_start);
+		}
+
+		ring_buffer_producer();
+
+		pr_info("Sleeping for 10 secs\n");
+		set_current_state(TASK_INTERRUPTIBLE);
+		schedule_timeout(HZ * SLEEP_TIME);
+		__set_current_state(TASK_RUNNING);
+	}
+
+	if (kill_test)
+		wait_to_die();
+
+	return 0;
+}
+
+static int __init ring_buffer_benchmark_init(void)
+{
+	int ret;
+
+	/* make a one meg buffer in overwite mode */
+	buffer = ring_buffer_alloc(1000000, RB_FL_OVERWRITE);
+	if (!buffer)
+		return -ENOMEM;
+
+	if (!disable_reader) {
+		consumer = kthread_create(ring_buffer_consumer_thread,
+					  NULL, "rb_consumer");
+		ret = PTR_ERR(consumer);
+		if (IS_ERR(consumer))
+			goto out_fail;
+	}
+
+	producer = kthread_run(ring_buffer_producer_thread,
+			       NULL, "rb_producer");
+	ret = PTR_ERR(producer);
+
+	if (IS_ERR(producer))
+		goto out_kill;
+
+	return 0;
+
+ out_kill:
+	if (consumer)
+		kthread_stop(consumer);
+
+ out_fail:
+	ring_buffer_free(buffer);
+	return ret;
+}
+
+static void __exit ring_buffer_benchmark_exit(void)
+{
+	kthread_stop(producer);
+	if (consumer)
+		kthread_stop(consumer);
+	ring_buffer_free(buffer);
+}
+
+module_init(ring_buffer_benchmark_init);
+module_exit(ring_buffer_benchmark_exit);
+
+MODULE_AUTHOR("Steven Rostedt");
+MODULE_DESCRIPTION("ring_buffer_benchmark");
+MODULE_LICENSE("GPL");
