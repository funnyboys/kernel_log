commit 77a40f97030b27b3fc1640a3ed203870f0817f57
Author: Joel Fernandes (Google) <joel@joelfernandes.org>
Date:   Fri Aug 30 12:36:32 2019 -0400

    rcu: Remove kfree_rcu() special casing and lazy-callback handling
    
    This commit removes kfree_rcu() special-casing and the lazy-callback
    handling from Tree RCU.  It moves some of this special casing to Tiny RCU,
    the removal of which will be the subject of later commits.
    
    This results in a nice negative delta.
    
    Suggested-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Signed-off-by: Joel Fernandes (Google) <joel@joelfernandes.org>
    [ paulmck: Add slab.h #include, thanks to kbuild test robot <lkp@intel.com>. ]
    Signed-off-by: Paul E. McKenney <paulmck@kernel.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 477b4eb44af5..dd572ce7c747 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -22,6 +22,7 @@
 #include <linux/time.h>
 #include <linux/cpu.h>
 #include <linux/prefetch.h>
+#include <linux/slab.h>
 
 #include "rcu.h"
 
@@ -73,6 +74,31 @@ void rcu_sched_clock_irq(int user)
 	}
 }
 
+/*
+ * Reclaim the specified callback, either by invoking it for non-kfree cases or
+ * freeing it directly (for kfree). Return true if kfreeing, false otherwise.
+ */
+static inline bool rcu_reclaim_tiny(struct rcu_head *head)
+{
+	rcu_callback_t f;
+	unsigned long offset = (unsigned long)head->func;
+
+	rcu_lock_acquire(&rcu_callback_map);
+	if (__is_kfree_rcu_offset(offset)) {
+		trace_rcu_invoke_kfree_callback("", head, offset);
+		kfree((void *)head - offset);
+		rcu_lock_release(&rcu_callback_map);
+		return true;
+	}
+
+	trace_rcu_invoke_callback("", head);
+	f = head->func;
+	WRITE_ONCE(head->func, (rcu_callback_t)0L);
+	f(head);
+	rcu_lock_release(&rcu_callback_map);
+	return false;
+}
+
 /* Invoke the RCU callbacks whose grace period has elapsed.  */
 static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)
 {
@@ -100,7 +126,7 @@ static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused
 		prefetch(next);
 		debug_rcu_head_unqueue(list);
 		local_bh_disable();
-		__rcu_reclaim("", list);
+		rcu_reclaim_tiny(list);
 		local_bh_enable();
 		list = next;
 	}

commit 18d7e40679ef574e428f893101be1c0035e95ee3
Author: Cyrill Gorcunov <gorcunov@gmail.com>
Date:   Thu Jan 24 21:14:37 2019 +0300

    rcu: rcu_qs -- Use raise_softirq_irqoff to not save irqs twice
    
    The rcu_qs is disabling IRQs by self so no need to do the same in raise_softirq
    but instead we can save some cycles using raise_softirq_irqoff directly.
    
    CC: Paul E. McKenney <paulmck@linux.ibm.com>
    Signed-off-by: Cyrill Gorcunov <gorcunov@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 911bd9076d43..477b4eb44af5 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -52,7 +52,7 @@ void rcu_qs(void)
 	local_irq_save(flags);
 	if (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {
 		rcu_ctrlblk.donetail = rcu_ctrlblk.curtail;
-		raise_softirq(RCU_SOFTIRQ);
+		raise_softirq_irqoff(RCU_SOFTIRQ);
 	}
 	local_irq_restore(flags);
 }

commit e7ffb4eb9a6d89678e7f62461737899f88dab64e
Merge: 87d1779dc48f 423a86a610ca 6706dae90d85 5a4eb3cb2012 e81baf4cb19a 6684880a8b16
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Sat Feb 9 08:47:52 2019 -0800

    Merge branches 'doc.2019.01.26a', 'fixes.2019.01.26a', 'sil.2019.01.26a', 'spdx.2019.02.09a', 'srcu.2019.01.26a' and 'torture.2019.01.26a' into HEAD
    
    doc.2019.01.26a:  Documentation updates.
    fixes.2019.01.26a:  Miscellaneous fixes.
    sil.2019.01.26a:  Removal of a few more spin_is_locked() instances.
    spdx.2019.02.09a:  Add SPDX identifiers to RCU files
    srcu.2019.01.26a:  SRCU updates.
    torture.2019.01.26a: Torture-test updates.

commit 00de9d7415352ef546e0bf0b8ff5d092e265c080
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Thu Jan 17 10:21:12 2019 -0800

    rcu/tiny: Convert to SPDX license identifier
    
    Replace the license boiler plate with a SPDX license identifier.
    While in the area, update an email address.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>
    Reviewed-by: Thomas Gleixner <tglx@linutronix.de>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 5f5963ba313e..f142c579fda8 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -1,23 +1,10 @@
+// SPDX-License-Identifier: GPL-2.0+
 /*
  * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.
  *
- * This program is free software; you can redistribute it and/or modify
- * it under the terms of the GNU General Public License as published by
- * the Free Software Foundation; either version 2 of the License, or
- * (at your option) any later version.
- *
- * This program is distributed in the hope that it will be useful,
- * but WITHOUT ANY WARRANTY; without even the implied warranty of
- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
- * GNU General Public License for more details.
- *
- * You should have received a copy of the GNU General Public License
- * along with this program; if not, you can access it online at
- * http://www.gnu.org/licenses/gpl-2.0.html.
- *
  * Copyright IBM Corporation, 2008
  *
- * Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
+ * Author: Paul E. McKenney <paulmck@linux.ibm.com>
  *
  * For detailed explanation of Read-Copy Update mechanism see -
  *		Documentation/RCU

commit c98cac603f1ce7d00e2a802b5640bced3bc3c1f2
Author: Paul E. McKenney <paulmck@linux.ibm.com>
Date:   Wed Nov 21 11:35:03 2018 -0800

    rcu: Rename rcu_check_callbacks() to rcu_sched_clock_irq()
    
    The name rcu_check_callbacks() arguably made sense back in the early
    2000s when RCU was quite a bit simpler than it is today, but it has
    become quite misleading, especially with the advent of dyntick-idle
    and NO_HZ_FULL.  The rcu_check_callbacks() function is RCU's hook into
    the scheduling-clock interrupt, and is now but one of many ways that
    callbacks get promoted to invocable state.
    
    This commit therefore changes the name to rcu_sched_clock_irq(),
    which is the same number of characters and clearly indicates this
    function's relation to the rest of the Linux kernel.  In addition, for
    the sake of consistency, rcu_flavor_check_callbacks() is also renamed
    to rcu_flavor_sched_clock_irq().
    
    While in the area, the header comments for both functions are reworked.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 5f5963ba313e..d7a9135b9471 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -76,7 +76,7 @@ void rcu_qs(void)
  * be called from hardirq context.  It is normally called from the
  * scheduling-clock interrupt.
  */
-void rcu_check_callbacks(int user)
+void rcu_sched_clock_irq(int user)
 {
 	if (user) {
 		rcu_qs();

commit b56ada120921fbb0a4fb2a5bee163717182e7e9e
Merge: 5c3f78ec285b 894d45bbf7e7 4e6ea4ef56f9 7c590fcca66b
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 30 16:12:53 2018 -0700

    Merge branches 'doc.2018.08.30a', 'dynticks.2018.08.30b', 'srcu.2018.08.30b' and 'torture.2018.08.29a' into HEAD
    
    doc.2018.08.30a: Documentation updates
    dynticks.2018.08.30b: RCU flavor consolidation updates and cleanups
    srcu.2018.08.30b: SRCU updates
    torture.2018.08.29a: Torture-test updates

commit e0fcba9ac02af5aeb1e1c3e842eab987f817c309
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Aug 14 08:45:54 2018 -0700

    srcu: Make call_srcu() available during very early boot
    
    Event tracing is moving to SRCU in order to take advantage of the fact
    that SRCU may be safely used from idle and even offline CPUs.  However,
    event tracing can invoke call_srcu() very early in the boot process,
    even before workqueue_init_early() is invoked (let alone rcu_init()).
    Therefore, call_srcu()'s attempts to queue work fail miserably.
    
    This commit therefore detects this situation, and refrains from attempting
    to queue work before rcu_init() time, but does everything else that it
    would have done, and in addition, adds the srcu_struct to a global list.
    The rcu_init() function now invokes a new srcu_init() function, which
    is empty if CONFIG_SRCU=n.  Otherwise, srcu_init() queues work for
    each srcu_struct on the list.  This all happens early enough in boot
    that there is but a single CPU with interrupts disabled, which allows
    synchronization to be dispensed with.
    
    Of course, the queued work won't actually be invoked until after
    workqueue_init() is invoked, which happens shortly after the scheduler
    is up and running.  This means that although call_srcu() may be invoked
    any time after per-CPU variables have been set up, there is still a very
    narrow window when synchronize_srcu() won't work, and this window
    extends from the time that the scheduler starts until the time that
    workqueue_init() returns.  This can be fixed in a manner similar to
    the fix for synchronize_rcu_expedited() and friends, but until someone
    actually needs to use synchronize_srcu() during this window, this fix
    is added churn for no benefit.
    
    Finally, note that Tree SRCU's new srcu_init() function invokes
    queue_work() rather than the queue_delayed_work() function that is
    invoked post-boot.  The reason is that queue_delayed_work() will (as you
    would expect) post a timer, and timers have not yet been initialized.
    So use of queue_work() avoids the complaints about use of uninitialized
    spinlocks that would otherwise result.  Besides, some delay is already
    provide by the aforementioned fact that the queued work won't actually
    be invoked until after the scheduler is up and running.
    
    Requested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Tested-by: Steven Rostedt (VMware) <rostedt@goodmis.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index befc9321a89c..101ed5bb836c 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -236,4 +236,5 @@ void __init rcu_init(void)
 {
 	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
 	rcu_early_boot_tests();
+	srcu_init();
 }

commit c5bacd94173ec49d7dce7ac7c64bbdde3a6e69ae
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jul 20 14:18:23 2018 -0700

    rcu: Motivate Tiny RCU forward progress
    
    If a long-running CPU-bound in-kernel task invokes call_rcu(), the
    callback won't be invoked until the next context switch.  If there are
    no other runnable tasks (which is not an uncommon situation on deep
    embedded systems), the callback might never be invoked.
    
    This commit therefore causes rcu_check_callbacks() to ask the scheduler
    for a context switch if there are callbacks posted that are still waiting
    for a grace period.
    
    Suggested-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index a77853b73bfe..1745d30e170e 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -78,8 +78,12 @@ void rcu_qs(void)
  */
 void rcu_check_callbacks(int user)
 {
-	if (user)
+	if (user) {
 		rcu_qs();
+	} else if (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {
+		set_tsk_need_resched(current);
+		set_preempt_need_resched();
+	}
 }
 
 /* Invoke the RCU callbacks whose grace period has elapsed.  */

commit 679d3f30923eb687ce3bcd3dfaf108a2809d5a57
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sat Jul 7 18:12:26 2018 -0700

    rcu: Clean up flavor-related definitions and comments in tiny.c
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 30826fb6e438..a77853b73bfe 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -117,9 +117,9 @@ static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused
 
 /*
  * Wait for a grace period to elapse.  But it is illegal to invoke
- * synchronize_sched() from within an RCU read-side critical section.
- * Therefore, any legal call to synchronize_sched() is a quiescent
- * state, and so on a UP system, synchronize_sched() need do nothing.
+ * synchronize_rcu() from within an RCU read-side critical section.
+ * Therefore, any legal call to synchronize_rcu() is a quiescent
+ * state, and so on a UP system, synchronize_rcu() need do nothing.
  * (But Lai Jiangshan points out the benefits of doing might_sleep()
  * to reduce latency.)
  *
@@ -130,12 +130,12 @@ void synchronize_rcu(void)
 	RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||
 			 lock_is_held(&rcu_lock_map) ||
 			 lock_is_held(&rcu_sched_lock_map),
-			 "Illegal synchronize_sched() in RCU read-side critical section");
+			 "Illegal synchronize_rcu() in RCU read-side critical section");
 }
 EXPORT_SYMBOL_GPL(synchronize_rcu);
 
 /*
- * Post an RCU callback to be invoked after the end of an RCU-sched grace
+ * Post an RCU callback to be invoked after the end of an RCU grace
  * period.  But since we have but one CPU, that would be after any
  * quiescent state.
  */

commit 709fdce7545c978e69f52eb19082ea3af44332f5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Jul 3 10:44:44 2018 -0700

    rcu: Express Tiny RCU updates in terms of RCU rather than RCU-sched
    
    This commit renames Tiny RCU functions so that the lowest level of
    functionality is RCU (e.g., synchronize_rcu()) rather than RCU-sched
    (e.g., synchronize_sched()).  This provides greater naming compatibility
    with Tree RCU, which will in turn permit more LoC removal once
    the RCU-sched and RCU-bh update-side API is removed.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Fix Tiny call_rcu()'s EXPORT_SYMBOL() in response to a bug
      report from kbuild test robot. ]

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index cadcf63c4889..30826fb6e438 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -46,25 +46,25 @@ struct rcu_ctrlblk {
 };
 
 /* Definition for rcupdate control block. */
-static struct rcu_ctrlblk rcu_sched_ctrlblk = {
-	.donetail	= &rcu_sched_ctrlblk.rcucblist,
-	.curtail	= &rcu_sched_ctrlblk.rcucblist,
+static struct rcu_ctrlblk rcu_ctrlblk = {
+	.donetail	= &rcu_ctrlblk.rcucblist,
+	.curtail	= &rcu_ctrlblk.rcucblist,
 };
 
-void rcu_barrier_sched(void)
+void rcu_barrier(void)
 {
-	wait_rcu_gp(call_rcu_sched);
+	wait_rcu_gp(call_rcu);
 }
-EXPORT_SYMBOL(rcu_barrier_sched);
+EXPORT_SYMBOL(rcu_barrier);
 
 /* Record an rcu quiescent state.  */
-void rcu_sched_qs(void)
+void rcu_qs(void)
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
-	if (rcu_sched_ctrlblk.donetail != rcu_sched_ctrlblk.curtail) {
-		rcu_sched_ctrlblk.donetail = rcu_sched_ctrlblk.curtail;
+	if (rcu_ctrlblk.donetail != rcu_ctrlblk.curtail) {
+		rcu_ctrlblk.donetail = rcu_ctrlblk.curtail;
 		raise_softirq(RCU_SOFTIRQ);
 	}
 	local_irq_restore(flags);
@@ -79,7 +79,7 @@ void rcu_sched_qs(void)
 void rcu_check_callbacks(int user)
 {
 	if (user)
-		rcu_sched_qs();
+		rcu_qs();
 }
 
 /* Invoke the RCU callbacks whose grace period has elapsed.  */
@@ -90,17 +90,17 @@ static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused
 
 	/* Move the ready-to-invoke callbacks to a local list. */
 	local_irq_save(flags);
-	if (rcu_sched_ctrlblk.donetail == &rcu_sched_ctrlblk.rcucblist) {
+	if (rcu_ctrlblk.donetail == &rcu_ctrlblk.rcucblist) {
 		/* No callbacks ready, so just leave. */
 		local_irq_restore(flags);
 		return;
 	}
-	list = rcu_sched_ctrlblk.rcucblist;
-	rcu_sched_ctrlblk.rcucblist = *rcu_sched_ctrlblk.donetail;
-	*rcu_sched_ctrlblk.donetail = NULL;
-	if (rcu_sched_ctrlblk.curtail == rcu_sched_ctrlblk.donetail)
-		rcu_sched_ctrlblk.curtail = &rcu_sched_ctrlblk.rcucblist;
-	rcu_sched_ctrlblk.donetail = &rcu_sched_ctrlblk.rcucblist;
+	list = rcu_ctrlblk.rcucblist;
+	rcu_ctrlblk.rcucblist = *rcu_ctrlblk.donetail;
+	*rcu_ctrlblk.donetail = NULL;
+	if (rcu_ctrlblk.curtail == rcu_ctrlblk.donetail)
+		rcu_ctrlblk.curtail = &rcu_ctrlblk.rcucblist;
+	rcu_ctrlblk.donetail = &rcu_ctrlblk.rcucblist;
 	local_irq_restore(flags);
 
 	/* Invoke the callbacks on the local list. */
@@ -125,21 +125,21 @@ static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused
  *
  * Cool, huh?  (Due to Josh Triplett.)
  */
-void synchronize_sched(void)
+void synchronize_rcu(void)
 {
 	RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||
 			 lock_is_held(&rcu_lock_map) ||
 			 lock_is_held(&rcu_sched_lock_map),
 			 "Illegal synchronize_sched() in RCU read-side critical section");
 }
-EXPORT_SYMBOL_GPL(synchronize_sched);
+EXPORT_SYMBOL_GPL(synchronize_rcu);
 
 /*
  * Post an RCU callback to be invoked after the end of an RCU-sched grace
  * period.  But since we have but one CPU, that would be after any
  * quiescent state.
  */
-void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
+void call_rcu(struct rcu_head *head, rcu_callback_t func)
 {
 	unsigned long flags;
 
@@ -148,16 +148,16 @@ void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
 	head->next = NULL;
 
 	local_irq_save(flags);
-	*rcu_sched_ctrlblk.curtail = head;
-	rcu_sched_ctrlblk.curtail = &head->next;
+	*rcu_ctrlblk.curtail = head;
+	rcu_ctrlblk.curtail = &head->next;
 	local_irq_restore(flags);
 
 	if (unlikely(is_idle_task(current))) {
-		/* force scheduling for rcu_sched_qs() */
+		/* force scheduling for rcu_qs() */
 		resched_cpu(0);
 	}
 }
-EXPORT_SYMBOL_GPL(call_rcu_sched);
+EXPORT_SYMBOL_GPL(call_rcu);
 
 void __init rcu_init(void)
 {

commit 65cfe3583b612a22e12fba9a7bbd2d37ca5ad941
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Sun Jul 1 07:40:52 2018 -0700

    rcu: Define RCU-bh update API in terms of RCU
    
    Now that the main RCU API knows about softirq disabling and softirq's
    quiescent states, the RCU-bh update code can be dispensed with.
    This commit therefore removes the RCU-bh update-side implementation and
    defines RCU-bh's update-side API in terms of that of either RCU-preempt or
    RCU-sched, depending on the setting of the CONFIG_PREEMPT Kconfig option.
    
    In kernels built with CONFIG_RCU_NOCB_CPU=y this has the knock-on effect
    of reducing by one the number of rcuo kthreads per CPU.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index befc9321a89c..cadcf63c4889 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -51,64 +51,22 @@ static struct rcu_ctrlblk rcu_sched_ctrlblk = {
 	.curtail	= &rcu_sched_ctrlblk.rcucblist,
 };
 
-static struct rcu_ctrlblk rcu_bh_ctrlblk = {
-	.donetail	= &rcu_bh_ctrlblk.rcucblist,
-	.curtail	= &rcu_bh_ctrlblk.rcucblist,
-};
-
-void rcu_barrier_bh(void)
-{
-	wait_rcu_gp(call_rcu_bh);
-}
-EXPORT_SYMBOL(rcu_barrier_bh);
-
 void rcu_barrier_sched(void)
 {
 	wait_rcu_gp(call_rcu_sched);
 }
 EXPORT_SYMBOL(rcu_barrier_sched);
 
-/*
- * Helper function for rcu_sched_qs() and rcu_bh_qs().
- * Also irqs are disabled to avoid confusion due to interrupt handlers
- * invoking call_rcu().
- */
-static int rcu_qsctr_help(struct rcu_ctrlblk *rcp)
-{
-	if (rcp->donetail != rcp->curtail) {
-		rcp->donetail = rcp->curtail;
-		return 1;
-	}
-
-	return 0;
-}
-
-/*
- * Record an rcu quiescent state.  And an rcu_bh quiescent state while we
- * are at it, given that any rcu quiescent state is also an rcu_bh
- * quiescent state.  Use "+" instead of "||" to defeat short circuiting.
- */
+/* Record an rcu quiescent state.  */
 void rcu_sched_qs(void)
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
-	if (rcu_qsctr_help(&rcu_sched_ctrlblk) +
-	    rcu_qsctr_help(&rcu_bh_ctrlblk))
-		raise_softirq(RCU_SOFTIRQ);
-	local_irq_restore(flags);
-}
-
-/*
- * Record an rcu_bh quiescent state.
- */
-void rcu_bh_qs(void)
-{
-	unsigned long flags;
-
-	local_irq_save(flags);
-	if (rcu_qsctr_help(&rcu_bh_ctrlblk))
+	if (rcu_sched_ctrlblk.donetail != rcu_sched_ctrlblk.curtail) {
+		rcu_sched_ctrlblk.donetail = rcu_sched_ctrlblk.curtail;
 		raise_softirq(RCU_SOFTIRQ);
+	}
 	local_irq_restore(flags);
 }
 
@@ -122,32 +80,27 @@ void rcu_check_callbacks(int user)
 {
 	if (user)
 		rcu_sched_qs();
-	if (user || !in_softirq())
-		rcu_bh_qs();
 }
 
-/*
- * Invoke the RCU callbacks on the specified rcu_ctrlkblk structure
- * whose grace period has elapsed.
- */
-static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
+/* Invoke the RCU callbacks whose grace period has elapsed.  */
+static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)
 {
 	struct rcu_head *next, *list;
 	unsigned long flags;
 
 	/* Move the ready-to-invoke callbacks to a local list. */
 	local_irq_save(flags);
-	if (rcp->donetail == &rcp->rcucblist) {
+	if (rcu_sched_ctrlblk.donetail == &rcu_sched_ctrlblk.rcucblist) {
 		/* No callbacks ready, so just leave. */
 		local_irq_restore(flags);
 		return;
 	}
-	list = rcp->rcucblist;
-	rcp->rcucblist = *rcp->donetail;
-	*rcp->donetail = NULL;
-	if (rcp->curtail == rcp->donetail)
-		rcp->curtail = &rcp->rcucblist;
-	rcp->donetail = &rcp->rcucblist;
+	list = rcu_sched_ctrlblk.rcucblist;
+	rcu_sched_ctrlblk.rcucblist = *rcu_sched_ctrlblk.donetail;
+	*rcu_sched_ctrlblk.donetail = NULL;
+	if (rcu_sched_ctrlblk.curtail == rcu_sched_ctrlblk.donetail)
+		rcu_sched_ctrlblk.curtail = &rcu_sched_ctrlblk.rcucblist;
+	rcu_sched_ctrlblk.donetail = &rcu_sched_ctrlblk.rcucblist;
 	local_irq_restore(flags);
 
 	/* Invoke the callbacks on the local list. */
@@ -162,19 +115,13 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 	}
 }
 
-static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)
-{
-	__rcu_process_callbacks(&rcu_sched_ctrlblk);
-	__rcu_process_callbacks(&rcu_bh_ctrlblk);
-}
-
 /*
  * Wait for a grace period to elapse.  But it is illegal to invoke
  * synchronize_sched() from within an RCU read-side critical section.
  * Therefore, any legal call to synchronize_sched() is a quiescent
  * state, and so on a UP system, synchronize_sched() need do nothing.
- * Ditto for synchronize_rcu_bh().  (But Lai Jiangshan points out the
- * benefits of doing might_sleep() to reduce latency.)
+ * (But Lai Jiangshan points out the benefits of doing might_sleep()
+ * to reduce latency.)
  *
  * Cool, huh?  (Due to Josh Triplett.)
  */
@@ -188,11 +135,11 @@ void synchronize_sched(void)
 EXPORT_SYMBOL_GPL(synchronize_sched);
 
 /*
- * Helper function for call_rcu() and call_rcu_bh().
+ * Post an RCU callback to be invoked after the end of an RCU-sched grace
+ * period.  But since we have but one CPU, that would be after any
+ * quiescent state.
  */
-static void __call_rcu(struct rcu_head *head,
-		       rcu_callback_t func,
-		       struct rcu_ctrlblk *rcp)
+void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
 {
 	unsigned long flags;
 
@@ -201,8 +148,8 @@ static void __call_rcu(struct rcu_head *head,
 	head->next = NULL;
 
 	local_irq_save(flags);
-	*rcp->curtail = head;
-	rcp->curtail = &head->next;
+	*rcu_sched_ctrlblk.curtail = head;
+	rcu_sched_ctrlblk.curtail = &head->next;
 	local_irq_restore(flags);
 
 	if (unlikely(is_idle_task(current))) {
@@ -210,28 +157,8 @@ static void __call_rcu(struct rcu_head *head,
 		resched_cpu(0);
 	}
 }
-
-/*
- * Post an RCU callback to be invoked after the end of an RCU-sched grace
- * period.  But since we have but one CPU, that would be after any
- * quiescent state.
- */
-void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
-{
-	__call_rcu(head, func, &rcu_sched_ctrlblk);
-}
 EXPORT_SYMBOL_GPL(call_rcu_sched);
 
-/*
- * Post an RCU bottom-half callback to be invoked after any subsequent
- * quiescent state.
- */
-void call_rcu_bh(struct rcu_head *head, rcu_callback_t func)
-{
-	__call_rcu(head, func, &rcu_bh_ctrlblk);
-}
-EXPORT_SYMBOL_GPL(call_rcu_bh);
-
 void __init rcu_init(void)
 {
 	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);

commit 07f27570dcd148a5f4de7dc3513c1d1cd069b362
Author: Byungchul Park <byungchul.park@lge.com>
Date:   Fri May 11 17:30:34 2018 +0900

    rcu: Improve rcu_note_voluntary_context_switch() reporting
    
    We expect a quiescent state of TASKS_RCU when cond_resched_tasks_rcu_qs()
    is called, no matter whether it actually be scheduled or not. However,
    it currently doesn't report the quiescent state when the task enters
    into __schedule() as it's called with preempt = true. So make it report
    the quiescent state unconditionally when cond_resched_tasks_rcu_qs() is
    called.
    
    And in TINY_RCU, even though the quiescent state of rcu_bh also should
    be reported when the tick interrupt comes from user, it doesn't. So make
    it reported.
    
    Lastly in TREE_RCU, rcu_note_voluntary_context_switch() should be
    reported when the tick interrupt comes from not only user but also idle,
    as an extended quiescent state.
    
    Signed-off-by: Byungchul Park <byungchul.park@lge.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    [ paulmck: Simplify rcutiny portion given no RCU-tasks for !PREEMPT. ]

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index a64eee0db39e..befc9321a89c 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -122,10 +122,8 @@ void rcu_check_callbacks(int user)
 {
 	if (user)
 		rcu_sched_qs();
-	else if (!in_softirq())
+	if (user || !in_softirq())
 		rcu_bh_qs();
-	if (user)
-		rcu_note_voluntary_context_switch(current);
 }
 
 /*

commit 825c5bd2fd47d30148db15fc121216c483682b01
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri May 26 16:16:40 2017 -0700

    srcu: Move rcu_scheduler_starting() from Tiny RCU to Tiny SRCU
    
    Other than lockdep support, Tiny RCU has no need for the
    scheduler status.  However, Tiny SRCU will need this to control
    boot-time behavior independent of lockdep.  Therefore, this commit
    moves rcu_scheduler_starting() from kernel/rcu/tiny_plugin.h to
    kernel/rcu/srcutiny.c.  This in turn allows the complete removal of
    kernel/rcu/tiny_plugin.h.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index f8488965250f..a64eee0db39e 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -56,8 +56,6 @@ static struct rcu_ctrlblk rcu_bh_ctrlblk = {
 	.curtail	= &rcu_bh_ctrlblk.rcucblist,
 };
 
-#include "tiny_plugin.h"
-
 void rcu_barrier_bh(void)
 {
 	wait_rcu_gp(call_rcu_bh);

commit 6d48152eafde1f0d0a4a9e0584fa7d9ff4fbfdac
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 17 10:54:29 2017 -0700

    rcu: Remove RCU CPU stall warnings from Tiny RCU
    
    Tiny RCU's job is to be tiny, so this commit removes its RCU CPU
    stall warning code.  After this, there is no longer any need for
    rcu_sched_ctrlblk and rcu_bh_ctrlblk to be in tiny_plugin.h, so this
    commit also moves them to tiny.c.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 595cb1bf944f..f8488965250f 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -38,11 +38,23 @@
 
 #include "rcu.h"
 
-/* Forward declarations for tiny_plugin.h. */
-struct rcu_ctrlblk;
-static void __call_rcu(struct rcu_head *head,
-		       rcu_callback_t func,
-		       struct rcu_ctrlblk *rcp);
+/* Global control variables for rcupdate callback mechanism. */
+struct rcu_ctrlblk {
+	struct rcu_head *rcucblist;	/* List of pending callbacks (CBs). */
+	struct rcu_head **donetail;	/* ->next pointer of last "done" CB. */
+	struct rcu_head **curtail;	/* ->next pointer of last CB. */
+};
+
+/* Definition for rcupdate control block. */
+static struct rcu_ctrlblk rcu_sched_ctrlblk = {
+	.donetail	= &rcu_sched_ctrlblk.rcucblist,
+	.curtail	= &rcu_sched_ctrlblk.rcucblist,
+};
+
+static struct rcu_ctrlblk rcu_bh_ctrlblk = {
+	.donetail	= &rcu_bh_ctrlblk.rcucblist,
+	.curtail	= &rcu_bh_ctrlblk.rcucblist,
+};
 
 #include "tiny_plugin.h"
 
@@ -65,7 +77,6 @@ EXPORT_SYMBOL(rcu_barrier_sched);
  */
 static int rcu_qsctr_help(struct rcu_ctrlblk *rcp)
 {
-	RCU_TRACE(reset_cpu_stall_ticks(rcp);)
 	if (rcp->donetail != rcp->curtail) {
 		rcp->donetail = rcp->curtail;
 		return 1;
@@ -111,7 +122,6 @@ void rcu_bh_qs(void)
  */
 void rcu_check_callbacks(int user)
 {
-	RCU_TRACE(check_cpu_stalls();)
 	if (user)
 		rcu_sched_qs();
 	else if (!in_softirq())
@@ -126,10 +136,8 @@ void rcu_check_callbacks(int user)
  */
 static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 {
-	const char *rn = NULL;
 	struct rcu_head *next, *list;
 	unsigned long flags;
-	RCU_TRACE(int cb_count = 0;)
 
 	/* Move the ready-to-invoke callbacks to a local list. */
 	local_irq_save(flags);
@@ -147,18 +155,15 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 	local_irq_restore(flags);
 
 	/* Invoke the callbacks on the local list. */
-	RCU_TRACE(rn = rcp->name;)
 	while (list) {
 		next = list->next;
 		prefetch(next);
 		debug_rcu_head_unqueue(list);
 		local_bh_disable();
-		__rcu_reclaim(rn, list);
+		__rcu_reclaim("", list);
 		local_bh_enable();
 		list = next;
-		RCU_TRACE(cb_count++;)
 	}
-	RCU_TRACE(rcu_trace_sub_qlen(rcp, cb_count);)
 }
 
 static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)
@@ -202,7 +207,6 @@ static void __call_rcu(struct rcu_head *head,
 	local_irq_save(flags);
 	*rcp->curtail = head;
 	rcp->curtail = &head->next;
-	RCU_TRACE(rcp->qlen++;)
 	local_irq_restore(flags);
 
 	if (unlikely(is_idle_task(current))) {
@@ -235,8 +239,5 @@ EXPORT_SYMBOL_GPL(call_rcu_bh);
 void __init rcu_init(void)
 {
 	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
-	RCU_TRACE(reset_cpu_stall_ticks(&rcu_sched_ctrlblk);)
-	RCU_TRACE(reset_cpu_stall_ticks(&rcu_bh_ctrlblk);)
-
 	rcu_early_boot_tests();
 }

commit c23484f0e7bc89e1facb04103ce24efeebee76b9
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 17 10:17:03 2017 -0700

    rcu: Remove event tracing from Tiny RCU
    
    This commit saves a few lines by getting rid of Tiny RCU's event tracing.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 2306cab2195d..595cb1bf944f 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -35,7 +35,6 @@
 #include <linux/time.h>
 #include <linux/cpu.h>
 #include <linux/prefetch.h>
-#include <linux/trace_events.h>
 
 #include "rcu.h"
 
@@ -139,7 +138,6 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 		local_irq_restore(flags);
 		return;
 	}
-	RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, rcp->qlen, -1);)
 	list = rcp->rcucblist;
 	rcp->rcucblist = *rcp->donetail;
 	*rcp->donetail = NULL;
@@ -161,10 +159,6 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 		RCU_TRACE(cb_count++;)
 	}
 	RCU_TRACE(rcu_trace_sub_qlen(rcp, cb_count);)
-	RCU_TRACE(trace_rcu_batch_end(rcp->name,
-				      cb_count, 0, need_resched(),
-				      is_idle_task(current),
-				      false));
 }
 
 static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)

commit 791875d16e2f6e2e5b90328ccac643f512ac76c4
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Wed May 3 11:06:05 2017 -0700

    rcu: Eliminate the unused __rcu_is_watching() function
    
    The __rcu_is_watching() function is currently not used, aside from
    to implement the rcu_is_watching() function.  This commit therefore
    eliminates __rcu_is_watching(), which has the beneficial side-effect
    of shrinking include/linux/rcupdate.h a bit.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index e5385731e391..2306cab2195d 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -59,19 +59,6 @@ void rcu_barrier_sched(void)
 }
 EXPORT_SYMBOL(rcu_barrier_sched);
 
-#if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE)
-
-/*
- * Test whether RCU thinks that the current CPU is idle.
- */
-bool notrace __rcu_is_watching(void)
-{
-	return true;
-}
-EXPORT_SYMBOL(__rcu_is_watching);
-
-#endif /* defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
-
 /*
  * Helper function for rcu_sched_qs() and rcu_bh_qs().
  * Also irqs are disabled to avoid confusion due to interrupt handlers

commit 6c8c148542cf947cd95afb2c385310ce47111427
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jan 23 12:02:09 2017 -0800

    rcu: Semicolon inside RCU_TRACE() for Tiny RCU
    
    The current use of "RCU_TRACE(statement);" can cause odd bugs, especially
    where "statement" is a local-variable declaration, as it can leave a
    misplaced ";" in the source code.  This commit therefore converts these
    to "RCU_TRACE(statement;)", which avoids the misplaced ";".
    
    Reported-by: Josh Triplett <josh@joshtriplett.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 6ad330dbbae2..e5385731e391 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -79,7 +79,7 @@ EXPORT_SYMBOL(__rcu_is_watching);
  */
 static int rcu_qsctr_help(struct rcu_ctrlblk *rcp)
 {
-	RCU_TRACE(reset_cpu_stall_ticks(rcp));
+	RCU_TRACE(reset_cpu_stall_ticks(rcp);)
 	if (rcp->donetail != rcp->curtail) {
 		rcp->donetail = rcp->curtail;
 		return 1;
@@ -125,7 +125,7 @@ void rcu_bh_qs(void)
  */
 void rcu_check_callbacks(int user)
 {
-	RCU_TRACE(check_cpu_stalls());
+	RCU_TRACE(check_cpu_stalls();)
 	if (user)
 		rcu_sched_qs();
 	else if (!in_softirq())
@@ -143,7 +143,7 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 	const char *rn = NULL;
 	struct rcu_head *next, *list;
 	unsigned long flags;
-	RCU_TRACE(int cb_count = 0);
+	RCU_TRACE(int cb_count = 0;)
 
 	/* Move the ready-to-invoke callbacks to a local list. */
 	local_irq_save(flags);
@@ -152,7 +152,7 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 		local_irq_restore(flags);
 		return;
 	}
-	RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, rcp->qlen, -1));
+	RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, rcp->qlen, -1);)
 	list = rcp->rcucblist;
 	rcp->rcucblist = *rcp->donetail;
 	*rcp->donetail = NULL;
@@ -162,7 +162,7 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 	local_irq_restore(flags);
 
 	/* Invoke the callbacks on the local list. */
-	RCU_TRACE(rn = rcp->name);
+	RCU_TRACE(rn = rcp->name;)
 	while (list) {
 		next = list->next;
 		prefetch(next);
@@ -171,9 +171,9 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 		__rcu_reclaim(rn, list);
 		local_bh_enable();
 		list = next;
-		RCU_TRACE(cb_count++);
+		RCU_TRACE(cb_count++;)
 	}
-	RCU_TRACE(rcu_trace_sub_qlen(rcp, cb_count));
+	RCU_TRACE(rcu_trace_sub_qlen(rcp, cb_count);)
 	RCU_TRACE(trace_rcu_batch_end(rcp->name,
 				      cb_count, 0, need_resched(),
 				      is_idle_task(current),
@@ -221,7 +221,7 @@ static void __call_rcu(struct rcu_head *head,
 	local_irq_save(flags);
 	*rcp->curtail = head;
 	rcp->curtail = &head->next;
-	RCU_TRACE(rcp->qlen++);
+	RCU_TRACE(rcp->qlen++;)
 	local_irq_restore(flags);
 
 	if (unlikely(is_idle_task(current))) {
@@ -254,8 +254,8 @@ EXPORT_SYMBOL_GPL(call_rcu_bh);
 void __init rcu_init(void)
 {
 	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
-	RCU_TRACE(reset_cpu_stall_ticks(&rcu_sched_ctrlblk));
-	RCU_TRACE(reset_cpu_stall_ticks(&rcu_bh_ctrlblk));
+	RCU_TRACE(reset_cpu_stall_ticks(&rcu_sched_ctrlblk);)
+	RCU_TRACE(reset_cpu_stall_ticks(&rcu_bh_ctrlblk);)
 
 	rcu_early_boot_tests();
 }

commit f9411ebe3d85cbbea06298241e6053d031d281fc
Author: Ingo Molnar <mingo@kernel.org>
Date:   Mon Feb 6 09:50:49 2017 +0100

    rcu: Separate the RCU synchronization types and APIs into <linux/rcupdate_wait.h>
    
    So rcupdate.h is a pretty complex header, in particular it includes
    <linux/completion.h> which includes <linux/wait.h> - creating a
    dependency that includes <linux/wait.h> in <linux/sched.h>,
    which prevents the isolation of <linux/sched.h> from the derived
    <linux/wait.h> header.
    
    Solve part of the problem by decoupling rcupdate.h from completions:
    this can be done by separating out the rcu_synchronize types and APIs,
    and updating their usage sites.
    
    Since this is a mostly RCU-internal types this will not just simplify
    <linux/sched.h>'s dependencies, but will make all the hundreds of
    .c files that include rcupdate.h but not completions or wait.h build
    faster.
    
    ( For rcutiny this means that two dependent APIs have to be uninlined,
      but that shouldn't be much of a problem as they are rare variants. )
    
    Acked-by: Linus Torvalds <torvalds@linux-foundation.org>
    Cc: Mike Galbraith <efault@gmx.de>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: linux-kernel@vger.kernel.org
    Signed-off-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index fa6a48d3917b..6ad330dbbae2 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -25,7 +25,7 @@
 #include <linux/completion.h>
 #include <linux/interrupt.h>
 #include <linux/notifier.h>
-#include <linux/rcupdate.h>
+#include <linux/rcupdate_wait.h>
 #include <linux/kernel.h>
 #include <linux/export.h>
 #include <linux/mutex.h>
@@ -47,6 +47,18 @@ static void __call_rcu(struct rcu_head *head,
 
 #include "tiny_plugin.h"
 
+void rcu_barrier_bh(void)
+{
+	wait_rcu_gp(call_rcu_bh);
+}
+EXPORT_SYMBOL(rcu_barrier_bh);
+
+void rcu_barrier_sched(void)
+{
+	wait_rcu_gp(call_rcu_sched);
+}
+EXPORT_SYMBOL(rcu_barrier_sched);
+
 #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE)
 
 /*

commit 2535db485cad8f302196af05888721ce82528978
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Nov 17 11:00:44 2016 -0800

    rcu: Remove unneeded rcu_process_callbacks() declarations
    
    The declarations of __rcu_process_callbacks() and rcu_process_callbacks()
    are not needed, as the definition of both of these functions appear before
    any uses.  This commit therefore removes both declarations.
    
    Reported-by: "Ahmed, Iftekhar" <ahmedi@oregonstate.edu>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index b23a4d076f3d..fa6a48d3917b 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -41,8 +41,6 @@
 
 /* Forward declarations for tiny_plugin.h. */
 struct rcu_ctrlblk;
-static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp);
-static void rcu_process_callbacks(struct softirq_action *unused);
 static void __call_rcu(struct rcu_head *head,
 		       rcu_callback_t func,
 		       struct rcu_ctrlblk *rcp);

commit f466ae66fa6a599f9a53b5f9bafea4b8cfffa7fb
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon Jan 9 23:23:15 2017 -0800

    rcu: Remove cond_resched() from Tiny synchronize_sched()
    
    It is now legal to invoke synchronize_sched() at early boot, which causes
    Tiny RCU's synchronize_sched() to emit spurious splats.  This commit
    therefore removes the cond_resched() from Tiny RCU's synchronize_sched().
    
    Fixes: 8b355e3bc140 ("rcu: Drive expedited grace periods from workqueue")
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org> # 4.9.0-

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 1898559e6b60..b23a4d076f3d 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -185,9 +185,6 @@ static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused
  * benefits of doing might_sleep() to reduce latency.)
  *
  * Cool, huh?  (Due to Josh Triplett.)
- *
- * But we want to make this a static inline later.  The cond_resched()
- * currently makes this problematic.
  */
 void synchronize_sched(void)
 {
@@ -195,7 +192,6 @@ void synchronize_sched(void)
 			 lock_is_held(&rcu_lock_map) ||
 			 lock_is_held(&rcu_sched_lock_map),
 			 "Illegal synchronize_sched() in RCU read-side critical section");
-	cond_resched();
 }
 EXPORT_SYMBOL_GPL(synchronize_sched);
 

commit 0766f788eb727e2e330d55d30545db65bcf2623f
Author: Emese Revfy <re.emese@gmail.com>
Date:   Mon Jun 20 20:42:34 2016 +0200

    latent_entropy: Mark functions with __latent_entropy
    
    The __latent_entropy gcc attribute can be used only on functions and
    variables.  If it is on a function then the plugin will instrument it for
    gathering control-flow entropy. If the attribute is on a variable then
    the plugin will initialize it with random contents.  The variable must
    be an integer, an integer array type or a structure with integer fields.
    
    These specific functions have been selected because they are init
    functions (to help gather boot-time entropy), are called at unpredictable
    times, or they have variable loops, each of which provide some level of
    latent entropy.
    
    Signed-off-by: Emese Revfy <re.emese@gmail.com>
    [kees: expanded commit message]
    Signed-off-by: Kees Cook <keescook@chromium.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 944b1b491ed8..1898559e6b60 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -170,7 +170,7 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 				      false));
 }
 
-static void rcu_process_callbacks(struct softirq_action *unused)
+static __latent_entropy void rcu_process_callbacks(struct softirq_action *unused)
 {
 	__rcu_process_callbacks(&rcu_sched_ctrlblk);
 	__rcu_process_callbacks(&rcu_bh_ctrlblk);

commit b6a4ae766e3133a4db73fabc81e522d1601cb623
Author: Boqun Feng <boqun.feng@gmail.com>
Date:   Wed Jul 29 13:29:38 2015 +0800

    rcu: Use rcu_callback_t in call_rcu*() and friends
    
    As we now have rcu_callback_t typedefs as the type of rcu callbacks, we
    should use it in call_rcu*() and friends as the type of parameters. This
    could save us a few lines of code and make it clear which function
    requires an rcu callbacks rather than other callbacks as its argument.
    
    Besides, this can also help cscope to generate a better database for
    code reading.
    
    Signed-off-by: Boqun Feng <boqun.feng@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index d0471056d0af..944b1b491ed8 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -44,7 +44,7 @@ struct rcu_ctrlblk;
 static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp);
 static void rcu_process_callbacks(struct softirq_action *unused);
 static void __call_rcu(struct rcu_head *head,
-		       void (*func)(struct rcu_head *rcu),
+		       rcu_callback_t func,
 		       struct rcu_ctrlblk *rcp);
 
 #include "tiny_plugin.h"
@@ -203,7 +203,7 @@ EXPORT_SYMBOL_GPL(synchronize_sched);
  * Helper function for call_rcu() and call_rcu_bh().
  */
 static void __call_rcu(struct rcu_head *head,
-		       void (*func)(struct rcu_head *rcu),
+		       rcu_callback_t func,
 		       struct rcu_ctrlblk *rcp)
 {
 	unsigned long flags;
@@ -229,7 +229,7 @@ static void __call_rcu(struct rcu_head *head,
  * period.  But since we have but one CPU, that would be after any
  * quiescent state.
  */
-void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+void call_rcu_sched(struct rcu_head *head, rcu_callback_t func)
 {
 	__call_rcu(head, func, &rcu_sched_ctrlblk);
 }
@@ -239,7 +239,7 @@ EXPORT_SYMBOL_GPL(call_rcu_sched);
  * Post an RCU bottom-half callback to be invoked after any subsequent
  * quiescent state.
  */
-void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+void call_rcu_bh(struct rcu_head *head, rcu_callback_t func)
 {
 	__call_rcu(head, func, &rcu_bh_ctrlblk);
 }

commit f78f5b90c4ffa559e400c3919a02236101f29f3f
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jun 18 15:50:02 2015 -0700

    rcu: Rename rcu_lockdep_assert() to RCU_LOCKDEP_WARN()
    
    This commit renames rcu_lockdep_assert() to RCU_LOCKDEP_WARN() for
    consistency with the WARN() series of macros.  This also requires
    inverting the sense of the conditional, which this commit also does.
    
    Reported-by: Ingo Molnar <mingo@kernel.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index c291bd65d2cb..d0471056d0af 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -191,10 +191,10 @@ static void rcu_process_callbacks(struct softirq_action *unused)
  */
 void synchronize_sched(void)
 {
-	rcu_lockdep_assert(!lock_is_held(&rcu_bh_lock_map) &&
-			   !lock_is_held(&rcu_lock_map) &&
-			   !lock_is_held(&rcu_sched_lock_map),
-			   "Illegal synchronize_sched() in RCU read-side critical section");
+	RCU_LOCKDEP_WARN(lock_is_held(&rcu_bh_lock_map) ||
+			 lock_is_held(&rcu_lock_map) ||
+			 lock_is_held(&rcu_sched_lock_map),
+			 "Illegal synchronize_sched() in RCU read-side critical section");
 	cond_resched();
 }
 EXPORT_SYMBOL_GPL(synchronize_sched);

commit e382608254e06c8109f40044f5e693f2e04f3899
Merge: fcbc1777ce8b b44754d8262d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Jun 26 14:02:43 2015 -0700

    Merge tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing updates from Steven Rostedt:
     "This patch series contains several clean ups and even a new trace
      clock "monitonic raw".  Also some enhancements to make the ring buffer
      even faster.  But the biggest and most noticeable change is the
      renaming of the ftrace* files, structures and variables that have to
      deal with trace events.
    
      Over the years I've had several developers tell me about their
      confusion with what ftrace is compared to events.  Technically,
      "ftrace" is the infrastructure to do the function hooks, which include
      tracing and also helps with live kernel patching.  But the trace
      events are a separate entity altogether, and the files that affect the
      trace events should not be named "ftrace".  These include:
    
        include/trace/ftrace.h         ->    include/trace/trace_events.h
        include/linux/ftrace_event.h   ->    include/linux/trace_events.h
    
      Also, functions that are specific for trace events have also been renamed:
    
        ftrace_print_*()               ->    trace_print_*()
        (un)register_ftrace_event()    ->    (un)register_trace_event()
        ftrace_event_name()            ->    trace_event_name()
        ftrace_trigger_soft_disabled() ->    trace_trigger_soft_disabled()
        ftrace_define_fields_##call()  ->    trace_define_fields_##call()
        ftrace_get_offsets_##call()    ->    trace_get_offsets_##call()
    
      Structures have been renamed:
    
        ftrace_event_file              ->    trace_event_file
        ftrace_event_{call,class}      ->    trace_event_{call,class}
        ftrace_event_buffer            ->    trace_event_buffer
        ftrace_subsystem_dir           ->    trace_subsystem_dir
        ftrace_event_raw_##call        ->    trace_event_raw_##call
        ftrace_event_data_offset_##call->    trace_event_data_offset_##call
        ftrace_event_type_funcs_##call ->    trace_event_type_funcs_##call
    
      And a few various variables and flags have also been updated.
    
      This has been sitting in linux-next for some time, and I have not
      heard a single complaint about this rename breaking anything.  Mostly
      because these functions, variables and structures are mostly internal
      to the tracing system and are seldom (if ever) used by anything
      external to that"
    
    * tag 'trace-v4.2' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace: (33 commits)
      ring_buffer: Allow to exit the ring buffer benchmark immediately
      ring-buffer-benchmark: Fix the wrong type
      ring-buffer-benchmark: Fix the wrong param in module_param
      ring-buffer: Add enum names for the context levels
      ring-buffer: Remove useless unused tracing_off_permanent()
      ring-buffer: Give NMIs a chance to lock the reader_lock
      ring-buffer: Add trace_recursive checks to ring_buffer_write()
      ring-buffer: Allways do the trace_recursive checks
      ring-buffer: Move recursive check to per_cpu descriptor
      ring-buffer: Add unlikelys to make fast path the default
      tracing: Rename ftrace_get_offsets_##call() to trace_event_get_offsets_##call()
      tracing: Rename ftrace_define_fields_##call() to trace_event_define_fields_##call()
      tracing: Rename ftrace_event_type_funcs_##call to trace_event_type_funcs_##call
      tracing: Rename ftrace_data_offset_##call to trace_event_data_offset_##call
      tracing: Rename ftrace_raw_##call event structures to trace_event_raw_##call
      tracing: Rename ftrace_trigger_soft_disabled() to trace_trigger_soft_disabled()
      tracing: Rename FTRACE_EVENT_FL_* flags to EVENT_FILE_FL_*
      tracing: Rename struct ftrace_subsystem_dir to trace_subsystem_dir
      tracing: Rename ftrace_event_name() to trace_event_name()
      tracing: Rename FTRACE_MAX_EVENT to TRACE_EVENT_TYPE_MAX
      ...

commit 6e91f8cb138625be96070b778d9ba71ce520ea7e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Mon May 11 11:13:05 2015 -0700

    rcu: Correctly handle non-empty Tiny RCU callback list with none ready
    
    If, at the time __rcu_process_callbacks() is invoked,  there are callbacks
    in Tiny RCU's callback list, but none of them are ready to be invoked,
    the current list-management code will knit the non-ready callbacks out
    of the list.  This can result in hangs and possibly worse.  This commit
    therefore inserts a check for there being no callbacks that can be
    invoked immediately.
    
    This bug is unlikely to occur -- you have to get a new callback between
    the time rcu_sched_qs() or rcu_bh_qs() was called, but before we get to
    __rcu_process_callbacks().  It was detected by the addition of RCU-bh
    testing to rcutorture, which in turn was instigated by Iftekhar Ahmed's
    mutation testing.  Although this bug was made much more likely by
    915e8a4fe45e (rcu: Remove fastpath from __rcu_process_callbacks()), this
    did not cause the bug, but rather made it much more probable.   That
    said, it takes more than 40 hours of rcutorture testing, on average,
    for this bug to appear, so this fix cannot be considered an emergency.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Cc: <stable@vger.kernel.org>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index a501b4ab9b1c..591af0cb7b9f 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -137,6 +137,11 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 
 	/* Move the ready-to-invoke callbacks to a local list. */
 	local_irq_save(flags);
+	if (rcp->donetail == &rcp->rcucblist) {
+		/* No callbacks ready, so just leave. */
+		local_irq_restore(flags);
+		return;
+	}
 	RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, rcp->qlen, -1));
 	list = rcp->rcucblist;
 	rcp->rcucblist = *rcp->donetail;

commit 51952bc633064311410b041fad38da1614f4539e
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Apr 21 11:15:30 2015 -0700

    rcu: Further shrink Tiny RCU by making empty functions static inlines
    
    The Tiny RCU counterparts to rcu_idle_enter(), rcu_idle_exit(),
    rcu_irq_enter(), and rcu_irq_exit() are empty functions, but each has
    EXPORT_SYMBOL_GPL(), which needlessly consumes extra memory, especially
    in kernels built with module support.  This commit therefore moves these
    functions to static inlines in rcutiny.h, removing the need for exports.
    
    This won't affect the size of the tiniest kernels, which are likely
    built without module support, but might help semi-tiny kernels that
    might include module support.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 069742d61c68..a501b4ab9b1c 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -49,39 +49,6 @@ static void __call_rcu(struct rcu_head *head,
 
 #include "tiny_plugin.h"
 
-/*
- * Enter idle, which is an extended quiescent state if we have fully
- * entered that mode.
- */
-void rcu_idle_enter(void)
-{
-}
-EXPORT_SYMBOL_GPL(rcu_idle_enter);
-
-/*
- * Exit an interrupt handler towards idle.
- */
-void rcu_irq_exit(void)
-{
-}
-EXPORT_SYMBOL_GPL(rcu_irq_exit);
-
-/*
- * Exit idle, so that we are no longer in an extended quiescent state.
- */
-void rcu_idle_exit(void)
-{
-}
-EXPORT_SYMBOL_GPL(rcu_idle_exit);
-
-/*
- * Enter an interrupt handler, moving away from idle.
- */
-void rcu_irq_enter(void)
-{
-}
-EXPORT_SYMBOL_GPL(rcu_irq_enter);
-
 #if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE)
 
 /*

commit af658dca221207174fc0a7bcdcd4cff7c589fdd8
Author: Steven Rostedt (Red Hat) <rostedt@goodmis.org>
Date:   Wed Apr 29 14:36:05 2015 -0400

    tracing: Rename ftrace_event.h to trace_events.h
    
    The term "ftrace" is really the infrastructure of the function hooks,
    and not the trace events. Rename ftrace_event.h to trace_events.h to
    represent the trace_event infrastructure and decouple the term ftrace
    from it.
    
    Signed-off-by: Steven Rostedt <rostedt@goodmis.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 069742d61c68..665a4f4d265f 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -35,7 +35,7 @@
 #include <linux/time.h>
 #include <linux/cpu.h>
 #include <linux/prefetch.h>
-#include <linux/ftrace_event.h>
+#include <linux/trace_events.h>
 
 #include "rcu.h"
 

commit 915e8a4fe45eab871a862f6467ec7e59864735b2
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Wed Feb 11 15:42:38 2015 +0100

    rcu: Remove fastpath from __rcu_process_callbacks()
    
    The standard code path accommodates a condition when no
    RCU callbacks are ready to invoke. Since size of the code
    is a priority for tiny RCU, remove the fast path.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index d4e7fe5f3baf..069742d61c68 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -168,17 +168,6 @@ static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
 	unsigned long flags;
 	RCU_TRACE(int cb_count = 0);
 
-	/* If no RCU callbacks ready to invoke, just return. */
-	if (&rcp->rcucblist == rcp->donetail) {
-		RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, 0, -1));
-		RCU_TRACE(trace_rcu_batch_end(rcp->name, 0,
-					      !!ACCESS_ONCE(rcp->rcucblist),
-					      need_resched(),
-					      is_idle_task(current),
-					      false));
-		return;
-	}
-
 	/* Move the ready-to-invoke callbacks to a local list. */
 	local_irq_save(flags);
 	RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, rcp->qlen, -1));

commit 27153acbe1141ceecf098ca5d24c2ae2714c1a5f
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Wed Feb 11 15:42:37 2015 +0100

    rcu: Remove unnecessary condition check in rcu_qsctr_help()
    
    When the ->curtail and ->donetail pointers differ, ->rcucblist
    always points to the beginning of the current list and thus
    cannot be NULL. Therefore, the check ->rcucblist != NULL is
    redundant and this commit removes it.
    
    Cc: "Paul E. McKenney" <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index cc9ceca7bde1..d4e7fe5f3baf 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -103,8 +103,7 @@ EXPORT_SYMBOL(__rcu_is_watching);
 static int rcu_qsctr_help(struct rcu_ctrlblk *rcp)
 {
 	RCU_TRACE(reset_cpu_stall_ticks(rcp));
-	if (rcp->rcucblist != NULL &&
-	    rcp->donetail != rcp->curtail) {
+	if (rcp->donetail != rcp->curtail) {
 		rcp->donetail = rcp->curtail;
 		return 1;
 	}

commit 78e691f4ae2d5edea0199ca802bb505b9cdced88
Merge: d87510c5a6e3 60479676eb6e ab954c167ed9 83fe27ea5311 630181c4a915 7602de4af192
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jan 15 23:34:34 2015 -0800

    Merge branches 'doc.2015.01.07a', 'fixes.2015.01.15a', 'preempt.2015.01.06a', 'srcu.2015.01.06a', 'stall.2015.01.16a' and 'torture.2015.01.11a' into HEAD
    
    doc.2015.01.07a: Documentation updates.
    fixes.2015.01.15a: Miscellaneous fixes.
    preempt.2015.01.06a: Changes to handling of lists of preempted tasks.
    srcu.2015.01.06a: SRCU updates.
    stall.2015.01.16a: RCU CPU stall-warning updates and fixes.
    torture.2015.01.11a: RCU torture-test updates and fixes.

commit 630181c4a915edb0761bb282c567d3abc8ca2f35
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Dec 23 21:33:14 2014 -0800

    rcu: Initialize tiny RCU stall-warning timeouts at boot
    
    The current tiny RCU stall-warning code assumes that the jiffies counter
    starts at zero, however, it is sometimes initialized to other values,
    for example, -30,000.  This commit therefore changes rcu_init() to
    invoke reset_cpu_stall_ticks() for both flavors of RCU to initialize
    the stall-warning times properly at boot.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 0db5649f8817..fc0f2c07af3a 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -383,6 +383,8 @@ EXPORT_SYMBOL_GPL(call_rcu_bh);
 void __init rcu_init(void)
 {
 	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
+	RCU_TRACE(reset_cpu_stall_ticks(&rcu_sched_ctrlblk));
+	RCU_TRACE(reset_cpu_stall_ticks(&rcu_bh_ctrlblk));
 
 	rcu_early_boot_tests();
 }

commit 5f6130fa52ee0df0b1da4518c5bbef42bcfe7d83
Author: Lai Jiangshan <laijs@cn.fujitsu.com>
Date:   Tue Dec 9 17:53:34 2014 +0800

    tiny_rcu: Directly force QS when call_rcu_[bh|sched]() on idle_task
    
    For RCU in UP, context-switch = QS = GP, thus we can force a
    context-switch when any call_rcu_[bh|sched]() is happened on idle_task.
    After doing so, rcu_idle/irq_enter/exit() are useless, so we can simply
    make these functions empty.
    
    More important, this change does not change the functionality logically.
    Note: raise_softirq(RCU_SOFTIRQ)/rcu_sched_qs() in rcu_idle_enter() and
    outmost rcu_irq_exit() will have to wake up the ksoftirqd
    (due to in_interrupt() == 0).
    
    Before this patch               After this patch:
    call_rcu_sched() in idle;       call_rcu_sched() in idle
                                      set resched
    do other stuffs;                do other stuffs
    outmost rcu_irq_exit()          outmost rcu_irq_exit() (empty function)
      (or rcu_idle_enter())           (or rcu_idle_enter(), also empty function)
                                    start to resched. (see above)
      rcu_sched_qs()                rcu_sched_qs()
        QS,and GP and advance cb      QS,and GP and advance cb
        wake up the ksoftirqd           wake up the ksoftirqd
          set resched
    resched to ksoftirqd (or other) resched to ksoftirqd (or other)
    
    These two code patches are almost the same.
    
    Size changed after patched:
    
    size kernel/rcu/tiny-old.o kernel/rcu/tiny-patched.o
       text    data     bss     dec     hex filename
       3449     206       8    3663     e4f kernel/rcu/tiny-old.o
       2406     144       8    2558     9fe kernel/rcu/tiny-patched.o
    
    Signed-off-by: Lai Jiangshan <laijs@cn.fujitsu.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 805b6d59c854..85287ec054fd 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -47,54 +47,14 @@ static void __call_rcu(struct rcu_head *head,
 		       void (*func)(struct rcu_head *rcu),
 		       struct rcu_ctrlblk *rcp);
 
-static long long rcu_dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
-
 #include "tiny_plugin.h"
 
-/* Common code for rcu_idle_enter() and rcu_irq_exit(), see kernel/rcu/tree.c. */
-static void rcu_idle_enter_common(long long newval)
-{
-	if (newval) {
-		RCU_TRACE(trace_rcu_dyntick(TPS("--="),
-					    rcu_dynticks_nesting, newval));
-		rcu_dynticks_nesting = newval;
-		return;
-	}
-	RCU_TRACE(trace_rcu_dyntick(TPS("Start"),
-				    rcu_dynticks_nesting, newval));
-	if (IS_ENABLED(CONFIG_RCU_TRACE) && !is_idle_task(current)) {
-		struct task_struct *idle __maybe_unused = idle_task(smp_processor_id());
-
-		RCU_TRACE(trace_rcu_dyntick(TPS("Entry error: not idle task"),
-					    rcu_dynticks_nesting, newval));
-		ftrace_dump(DUMP_ALL);
-		WARN_ONCE(1, "Current pid: %d comm: %s / Idle pid: %d comm: %s",
-			  current->pid, current->comm,
-			  idle->pid, idle->comm); /* must be idle task! */
-	}
-	rcu_sched_qs(); /* implies rcu_bh_inc() */
-	barrier();
-	rcu_dynticks_nesting = newval;
-}
-
 /*
  * Enter idle, which is an extended quiescent state if we have fully
- * entered that mode (i.e., if the new value of dynticks_nesting is zero).
+ * entered that mode.
  */
 void rcu_idle_enter(void)
 {
-	unsigned long flags;
-	long long newval;
-
-	local_irq_save(flags);
-	WARN_ON_ONCE((rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK) == 0);
-	if ((rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK) ==
-	    DYNTICK_TASK_NEST_VALUE)
-		newval = 0;
-	else
-		newval = rcu_dynticks_nesting - DYNTICK_TASK_NEST_VALUE;
-	rcu_idle_enter_common(newval);
-	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(rcu_idle_enter);
 
@@ -103,55 +63,14 @@ EXPORT_SYMBOL_GPL(rcu_idle_enter);
  */
 void rcu_irq_exit(void)
 {
-	unsigned long flags;
-	long long newval;
-
-	local_irq_save(flags);
-	newval = rcu_dynticks_nesting - 1;
-	WARN_ON_ONCE(newval < 0);
-	rcu_idle_enter_common(newval);
-	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(rcu_irq_exit);
 
-/* Common code for rcu_idle_exit() and rcu_irq_enter(), see kernel/rcu/tree.c. */
-static void rcu_idle_exit_common(long long oldval)
-{
-	if (oldval) {
-		RCU_TRACE(trace_rcu_dyntick(TPS("++="),
-					    oldval, rcu_dynticks_nesting));
-		return;
-	}
-	RCU_TRACE(trace_rcu_dyntick(TPS("End"), oldval, rcu_dynticks_nesting));
-	if (IS_ENABLED(CONFIG_RCU_TRACE) && !is_idle_task(current)) {
-		struct task_struct *idle __maybe_unused = idle_task(smp_processor_id());
-
-		RCU_TRACE(trace_rcu_dyntick(TPS("Exit error: not idle task"),
-			  oldval, rcu_dynticks_nesting));
-		ftrace_dump(DUMP_ALL);
-		WARN_ONCE(1, "Current pid: %d comm: %s / Idle pid: %d comm: %s",
-			  current->pid, current->comm,
-			  idle->pid, idle->comm); /* must be idle task! */
-	}
-}
-
 /*
  * Exit idle, so that we are no longer in an extended quiescent state.
  */
 void rcu_idle_exit(void)
 {
-	unsigned long flags;
-	long long oldval;
-
-	local_irq_save(flags);
-	oldval = rcu_dynticks_nesting;
-	WARN_ON_ONCE(rcu_dynticks_nesting < 0);
-	if (rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK)
-		rcu_dynticks_nesting += DYNTICK_TASK_NEST_VALUE;
-	else
-		rcu_dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
-	rcu_idle_exit_common(oldval);
-	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(rcu_idle_exit);
 
@@ -160,15 +79,6 @@ EXPORT_SYMBOL_GPL(rcu_idle_exit);
  */
 void rcu_irq_enter(void)
 {
-	unsigned long flags;
-	long long oldval;
-
-	local_irq_save(flags);
-	oldval = rcu_dynticks_nesting;
-	rcu_dynticks_nesting++;
-	WARN_ON_ONCE(rcu_dynticks_nesting == 0);
-	rcu_idle_exit_common(oldval);
-	local_irq_restore(flags);
 }
 EXPORT_SYMBOL_GPL(rcu_irq_enter);
 
@@ -179,7 +89,7 @@ EXPORT_SYMBOL_GPL(rcu_irq_enter);
  */
 bool notrace __rcu_is_watching(void)
 {
-	return rcu_dynticks_nesting;
+	return true;
 }
 EXPORT_SYMBOL(__rcu_is_watching);
 
@@ -347,6 +257,11 @@ static void __call_rcu(struct rcu_head *head,
 	rcp->curtail = &head->next;
 	RCU_TRACE(rcp->qlen++);
 	local_irq_restore(flags);
+
+	if (unlikely(is_idle_task(current))) {
+		/* force scheduling for rcu_sched_qs() */
+		resched_cpu(0);
+	}
 }
 
 /*

commit ca9558a33f658155c3b69f92897c2e6a848684f5
Author: Alexander Gordeev <agordeev@redhat.com>
Date:   Fri Oct 31 14:55:05 2014 +0000

    rcu: Remove redundant rcu_is_cpu_rrupt_from_idle() from tiny RCU
    
    Let's start assuming that something in the idle loop posts a callback,
    and scheduling-clock interrupt occurs:
    
    1. The system is idle and stays that way, no runnable tasks.
    
    2. Scheduling-clock interrupt occurs, rcu_check_callbacks() is called
       as result, which in turn calls rcu_is_cpu_rrupt_from_idle().
    
    3. rcu_is_cpu_rrupt_from_idle() reports the CPU was interrupted from
       idle, which results in rcu_sched_qs() call, which does a
       raise_softirq(RCU_SOFTIRQ).
    
    4. Upon return from interrupt, rcu_irq_exit() is invoked, which calls
       rcu_idle_enter_common(), which in turn calls rcu_sched_qs() again,
       which does another raise_softirq(RCU_SOFTIRQ).
    
    5. The softirq happens shortly and invokes rcu_process_callbacks(),
       which invokes __rcu_process_callbacks().
    
    6. So now callbacks can be invoked. At least they can be if
       ->donetail has been updated. Which it will have been because
       rcu_sched_qs() invokes rcu_qsctr_help().
    
    In the described scenario rcu_sched_qs() and raise_softirq(RCU_SOFTIRQ)
    get called twice in steps 3 and 4. This redundancy could be eliminated
    by removing rcu_is_cpu_rrupt_from_idle() function.
    
    Signed-off-by: Alexander Gordeev <agordeev@redhat.com>
    Cc: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 0db5649f8817..805b6d59c854 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -185,16 +185,6 @@ EXPORT_SYMBOL(__rcu_is_watching);
 
 #endif /* defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
 
-/*
- * Test whether the current CPU was interrupted from idle.  Nested
- * interrupts don't count, we must be running at the first interrupt
- * level.
- */
-static int rcu_is_cpu_rrupt_from_idle(void)
-{
-	return rcu_dynticks_nesting <= 1;
-}
-
 /*
  * Helper function for rcu_sched_qs() and rcu_bh_qs().
  * Also irqs are disabled to avoid confusion due to interrupt handlers
@@ -250,7 +240,7 @@ void rcu_bh_qs(void)
 void rcu_check_callbacks(int user)
 {
 	RCU_TRACE(check_cpu_stalls());
-	if (user || rcu_is_cpu_rrupt_from_idle())
+	if (user)
 		rcu_sched_qs();
 	else if (!in_softirq())
 		rcu_bh_qs();

commit 9ea6c5885681e3d9ce9844ba9dc57371a5cfc6d2
Merge: b8969d1a506c 62db99f4783e 8ab8b3e1837f 60ced4950c50 392809b25833 bbe5d7a93a39
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Nov 13 10:39:04 2014 -0800

    Merge branches 'torture.2014.11.03a', 'cpu.2014.11.03a', 'doc.2014.11.13a', 'fixes.2014.11.13a', 'signal.2014.10.29a' and 'rt.2014.10.29a' into HEAD
    
    cpu.2014.11.03a: Changes for per-CPU variables.
    doc.2014.11.13a: Documentation updates.
    fixes.2014.11.13a: Miscellaneous fixes.
    signal.2014.10.29a: Signal changes.
    rt.2014.10.29a: Real-time changes.
    torture.2014.11.03a: torture-test changes.

commit aa23c6fbc50c4f9d8b43682f37fc4580a7851413
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Fri Sep 19 11:32:29 2014 -0400

    rcutorture: Add early boot self tests
    
    Add early boot self tests for RCU under CONFIG_PROVE_RCU.
    
    Currently the only test is adding a dummy callback which increments a counter
    which we then later verify after calling rcu_barrier*().
    
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index c0623fc47125..d3d44c589122 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -380,7 +380,9 @@ void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
 }
 EXPORT_SYMBOL_GPL(call_rcu_bh);
 
-void rcu_init(void)
+void __init rcu_init(void)
 {
 	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
+
+	rcu_early_boot_tests();
 }

commit c3377c2da6e594504c900d5ef72374c109e4ca99
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 21 07:53:02 2014 -0700

    rcu: Remove "cpu" argument to rcu_check_callbacks()
    
    The "cpu" argument was kept around on the off-chance that RCU might
    offload scheduler-clock interrupts.  However, this offload approach
    has been replaced by NO_HZ_FULL, which offloads -all- RCU processing
    from qualifying CPUs.  It is therefore time to remove the "cpu" argument
    to rcu_check_callbacks(), which this commit does.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Pranith Kumar <bobby.prani@gmail.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index c0623fc47125..01570c68d237 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -247,7 +247,7 @@ void rcu_bh_qs(void)
  * be called from hardirq context.  It is normally called from the
  * scheduling-clock interrupt.
  */
-void rcu_check_callbacks(int cpu, int user)
+void rcu_check_callbacks(int user)
 {
 	RCU_TRACE(check_cpu_stalls());
 	if (user || rcu_is_cpu_rrupt_from_idle())

commit 96b4672703ed4538c7fc25de36df4415a0ee237c
Merge: e98d06dd6cd7 a53dd6a65668
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Sep 16 10:10:44 2014 -0700

    Merge branch 'rcu-tasks.2014.09.10a' into HEAD
    
    rcu-tasks.2014.09.10a: Add RCU-tasks flavor of RCU.

commit 284a8c93af47306beed967a303d84730b32bab39
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Aug 14 16:38:46 2014 -0700

    rcu: Per-CPU operation cleanups to rcu_*_qs() functions
    
    The rcu_bh_qs(), rcu_preempt_qs(), and rcu_sched_qs() functions use
    old-style per-CPU variable access and write to ->passed_quiesce even
    if it is already set.  This commit therefore updates to use the new-style
    per-CPU variable access functions and avoids the spurious writes.
    This commit also eliminates the "cpu" argument to these functions because
    they are always invoked on the indicated CPU.
    
    Reported-by: Peter Zijlstra <peterz@infradead.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 717f00854fc0..61b8d2ccc2cb 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -72,7 +72,7 @@ static void rcu_idle_enter_common(long long newval)
 			  current->pid, current->comm,
 			  idle->pid, idle->comm); /* must be idle task! */
 	}
-	rcu_sched_qs(0); /* implies rcu_bh_qsctr_inc(0) */
+	rcu_sched_qs(); /* implies rcu_bh_inc() */
 	barrier();
 	rcu_dynticks_nesting = newval;
 }
@@ -217,7 +217,7 @@ static int rcu_qsctr_help(struct rcu_ctrlblk *rcp)
  * are at it, given that any rcu quiescent state is also an rcu_bh
  * quiescent state.  Use "+" instead of "||" to defeat short circuiting.
  */
-void rcu_sched_qs(int cpu)
+void rcu_sched_qs(void)
 {
 	unsigned long flags;
 
@@ -231,7 +231,7 @@ void rcu_sched_qs(int cpu)
 /*
  * Record an rcu_bh quiescent state.
  */
-void rcu_bh_qs(int cpu)
+void rcu_bh_qs(void)
 {
 	unsigned long flags;
 
@@ -251,9 +251,9 @@ void rcu_check_callbacks(int cpu, int user)
 {
 	RCU_TRACE(check_cpu_stalls());
 	if (user || rcu_is_cpu_rrupt_from_idle())
-		rcu_sched_qs(cpu);
+		rcu_sched_qs();
 	else if (!in_softirq())
-		rcu_bh_qs(cpu);
+		rcu_bh_qs();
 	if (user)
 		rcu_note_voluntary_context_switch(current);
 }

commit 8315f42295d2667a7f942f154b73a86fd7cb2227
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Fri Jun 27 13:42:20 2014 -0700

    rcu: Add call_rcu_tasks()
    
    This commit adds a new RCU-tasks flavor of RCU, which provides
    call_rcu_tasks().  This RCU flavor's quiescent states are voluntary
    context switch (not preemption!) and userspace execution (not the idle
    loop -- use some sort of schedule_on_each_cpu() if you need to handle the
    idle tasks.  Note that unlike other RCU flavors, these quiescent states
    occur in tasks, not necessarily CPUs.  Includes fixes from Steven Rostedt.
    
    This RCU flavor is assumed to have very infrequent latency-tolerant
    updaters.  This assumption permits significant simplifications, including
    a single global callback list protected by a single global lock, along
    with a single task-private linked list containing all tasks that have not
    yet passed through a quiescent state.  If experience shows this assumption
    to be incorrect, the required additional complexity will be added.
    
    Suggested-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index d9efcc13008c..717f00854fc0 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -254,6 +254,8 @@ void rcu_check_callbacks(int cpu, int user)
 		rcu_sched_qs(cpu);
 	else if (!in_softirq())
 		rcu_bh_qs(cpu);
+	if (user)
+		rcu_note_voluntary_context_switch(current);
 }
 
 /*

commit ade9862470dd0595d8e292ecea8445ed90b98df5
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Thu Jul 31 16:02:33 2014 -0700

    rcu: Make TINY_RCU tinier by putting error checks under #ifdef
    
    The rcu_idle_enter_common() and rcu_idle_exit_common() functions contain
    error checks that have to the best of my knowledge have never triggered
    over the past several years.  These are nevertheless valuable when
    creating new architectures or doing other low-level changes, so the
    checks should not be deleted.  This commit instead places these checks
    under #ifdef CONFIG_RCU_TRACE so that they are executed only when
    specifically requested.
    
    The savings are significant:
    
            Before:
    
               text    data     bss     dec     hex filename
               1749      39       0    1788     6fc /tmp/b/kernel/rcu/tiny.o
                632     152       0     784     310 /tmp/b/kernel/rcu/update.o
                                       ----
                                       2572
    
            After:
    
               text    data     bss     dec     hex filename
               1281      37       0    1318     526 /tmp/b/kernel/rcu/tiny.o
                632     152       0     784     310 /tmp/b/kernel/rcu/update.o
                                       ----
                                       2102
    
    This amounts to 470 bytes, or 18% of the original.
    
    Switched from #ifdef to IS_ENABLED() on Josh Triplett's advice.
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 6bd785c34add..4a55a2416e3c 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -62,7 +62,7 @@ static void rcu_idle_enter_common(long long newval)
 	}
 	RCU_TRACE(trace_rcu_dyntick(TPS("Start"),
 				    rcu_dynticks_nesting, newval));
-	if (!is_idle_task(current)) {
+	if (IS_ENABLED(CONFIG_RCU_TRACE) && !is_idle_task(current)) {
 		struct task_struct *idle __maybe_unused = idle_task(smp_processor_id());
 
 		RCU_TRACE(trace_rcu_dyntick(TPS("Entry error: not idle task"),
@@ -123,7 +123,7 @@ static void rcu_idle_exit_common(long long oldval)
 		return;
 	}
 	RCU_TRACE(trace_rcu_dyntick(TPS("End"), oldval, rcu_dynticks_nesting));
-	if (!is_idle_task(current)) {
+	if (IS_ENABLED(CONFIG_RCU_TRACE) && !is_idle_task(current)) {
 		struct task_struct *idle __maybe_unused = idle_task(smp_processor_id());
 
 		RCU_TRACE(trace_rcu_dyntick(TPS("Exit error: not idle task"),

commit fafb6e843f229a6e842a22773f16d93194ca06e4
Author: Pranith Kumar <bobby.prani@gmail.com>
Date:   Tue Jul 15 18:31:47 2014 -0400

    rcu: Update tiny.c references to tree.c
    
    This commit updates the references to rcutree.c which is now rcu/tree.c
    
    Signed-off-by: Pranith Kumar <bobby.prani@gmail.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index d9efcc13008c..6bd785c34add 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -51,7 +51,7 @@ static long long rcu_dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
 
 #include "tiny_plugin.h"
 
-/* Common code for rcu_idle_enter() and rcu_irq_exit(), see kernel/rcutree.c. */
+/* Common code for rcu_idle_enter() and rcu_irq_exit(), see kernel/rcu/tree.c. */
 static void rcu_idle_enter_common(long long newval)
 {
 	if (newval) {
@@ -114,7 +114,7 @@ void rcu_irq_exit(void)
 }
 EXPORT_SYMBOL_GPL(rcu_irq_exit);
 
-/* Common code for rcu_idle_exit() and rcu_irq_enter(), see kernel/rcutree.c. */
+/* Common code for rcu_idle_exit() and rcu_irq_enter(), see kernel/rcu/tree.c. */
 static void rcu_idle_exit_common(long long oldval)
 {
 	if (oldval) {

commit 5cb5c6e18f822b19bd41a2c0f9930c82b3ec0bc9
Author: Paul Gortmaker <paul.gortmaker@windriver.com>
Date:   Wed Feb 19 14:33:27 2014 -0500

    rcu: Ensure kernel/rcu/rcu.h can be sourced/used stand-alone
    
    The kbuild test bot uncovered an implicit dependence on the
    trace header being present before rcu.h in ia64 allmodconfig
    that looks like this:
    
    In file included from kernel/ksysfs.c:22:0:
    kernel/rcu/rcu.h: In function '__rcu_reclaim':
    kernel/rcu/rcu.h:107:3: error: implicit declaration of function 'trace_rcu_invoke_kfree_callback' [-Werror=implicit-function-declaration]
    kernel/rcu/rcu.h:112:3: error: implicit declaration of function 'trace_rcu_invoke_callback' [-Werror=implicit-function-declaration]
    cc1: some warnings being treated as errors
    
    Looking at other rcu.h users, we can find that they all
    were sourcing the trace header in advance of rcu.h itself,
    as seen in the context of this diff.  There were also some
    inconsistencies as to whether it was or wasn't sourced based
    on the parent tracing Kconfig.
    
    Rather than "fix" it at each use site, and have inconsistent
    use based on whether "#ifdef CONFIG_RCU_TRACE" was used or not,
    lets just source the trace header just once, in the actual consumer
    of it, which is rcu.h itself.  We include it unconditionally, as
    build testing shows us that is a hard requirement for some files.
    
    Reported-by: kbuild test robot <fengguang.wu@intel.com>
    Signed-off-by: Paul Gortmaker <paul.gortmaker@windriver.com>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 53b95bbf4abb..d9efcc13008c 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -37,10 +37,6 @@
 #include <linux/prefetch.h>
 #include <linux/ftrace_event.h>
 
-#ifdef CONFIG_RCU_TRACE
-#include <trace/events/rcu.h>
-#endif /* #else #ifdef CONFIG_RCU_TRACE */
-
 #include "rcu.h"
 
 /* Forward declarations for tiny_plugin.h. */

commit 87de1cfdc55b16b794e245b07322340725149d62
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Dec 3 10:02:52 2013 -0800

    rcu: Stop tracking FSF's postal address
    
    All of the RCU source files have the usual GPL header, which contains a
    long-obsolete postal address for FSF.  To avoid the need to track the
    FSF office's movements, this commit substitutes the URL where GPL may
    be found.
    
    Reported-by: Greg KH <gregkh@linuxfoundation.org>
    Reported-by: Steven Rostedt <rostedt@goodmis.org>
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Josh Triplett <josh@joshtriplett.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
index 1254f312d024..53b95bbf4abb 100644
--- a/kernel/rcu/tiny.c
+++ b/kernel/rcu/tiny.c
@@ -12,8 +12,8 @@
  * GNU General Public License for more details.
  *
  * You should have received a copy of the GNU General Public License
- * along with this program; if not, write to the Free Software
- * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ * along with this program; if not, you can access it online at
+ * http://www.gnu.org/licenses/gpl-2.0.html.
  *
  * Copyright IBM Corporation, 2008
  *

commit b29c8306a368cf65782669eba079f81dc861c54d
Merge: 0bde7294e2ad 3a81a5210b7d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Nov 16 12:23:18 2013 -0800

    Merge tag 'trace-3.13' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace
    
    Pull tracing update from Steven Rostedt:
     "This batch of changes is mostly clean ups and small bug fixes.  The
      only real feature that was added this release is from Namhyung Kim,
      who introduced "set_graph_notrace" filter that lets you run the
      function graph tracer and not trace particular functions and their
      call chain.
    
      Tom Zanussi added some updates to the ftrace multibuffer tracing that
      made it more consistent with the top level tracing.
    
      One of the fixes for perf function tracing required an API change in
      RCU; the addition of "rcu_is_watching()".  As Paul McKenney is pushing
      that change in this release too, he gave me a branch that included all
      the changes to get that working, and I pulled that into my tree in
      order to complete the perf function tracing fix"
    
    * tag 'trace-3.13' of git://git.kernel.org/pub/scm/linux/kernel/git/rostedt/linux-trace:
      tracing: Add rcu annotation for syscall trace descriptors
      tracing: Do not use signed enums with unsigned long long in fgragh output
      tracing: Remove unused function ftrace_off_permanent()
      tracing: Do not assign filp->private_data to freed memory
      tracing: Add helper function tracing_is_disabled()
      tracing: Open tracer when ftrace_dump_on_oops is used
      tracing: Add support for SOFT_DISABLE to syscall events
      tracing: Make register/unregister_ftrace_command __init
      tracing: Update event filters for multibuffer
      recordmcount.pl: Add support for __fentry__
      ftrace: Have control op function callback only trace when RCU is watching
      rcu: Do not trace rcu_is_watching() functions
      ftrace/x86: skip over the breakpoint for ftrace caller
      trace/trace_stat: use rbtree postorder iteration helper instead of opencoding
      ftrace: Add set_graph_notrace filter
      ftrace: Narrow down the protected area of graph_lock
      ftrace: Introduce struct ftrace_graph_data
      ftrace: Get rid of ftrace_graph_filter_enabled
      tracing: Fix potential out-of-bounds in trace_get_user()
      tracing: Show more exact help information about snapshot

commit 4102adab9189c8ea2f0cdd2f88345fd25d2790f1
Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
Date:   Tue Oct 8 20:23:47 2013 -0700

    rcu: Move RCU-related source code to kernel/rcu directory
    
    Signed-off-by: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
    Reviewed-by: Ingo Molnar <mingo@kernel.org>

diff --git a/kernel/rcu/tiny.c b/kernel/rcu/tiny.c
new file mode 100644
index 000000000000..0c9a934cfec1
--- /dev/null
+++ b/kernel/rcu/tiny.c
@@ -0,0 +1,388 @@
+/*
+ * Read-Copy Update mechanism for mutual exclusion, the Bloatwatch edition.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.
+ *
+ * Copyright IBM Corporation, 2008
+ *
+ * Author: Paul E. McKenney <paulmck@linux.vnet.ibm.com>
+ *
+ * For detailed explanation of Read-Copy Update mechanism see -
+ *		Documentation/RCU
+ */
+#include <linux/completion.h>
+#include <linux/interrupt.h>
+#include <linux/notifier.h>
+#include <linux/rcupdate.h>
+#include <linux/kernel.h>
+#include <linux/export.h>
+#include <linux/mutex.h>
+#include <linux/sched.h>
+#include <linux/types.h>
+#include <linux/init.h>
+#include <linux/time.h>
+#include <linux/cpu.h>
+#include <linux/prefetch.h>
+#include <linux/ftrace_event.h>
+
+#ifdef CONFIG_RCU_TRACE
+#include <trace/events/rcu.h>
+#endif /* #else #ifdef CONFIG_RCU_TRACE */
+
+#include "rcu.h"
+
+/* Forward declarations for tiny_plugin.h. */
+struct rcu_ctrlblk;
+static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp);
+static void rcu_process_callbacks(struct softirq_action *unused);
+static void __call_rcu(struct rcu_head *head,
+		       void (*func)(struct rcu_head *rcu),
+		       struct rcu_ctrlblk *rcp);
+
+static long long rcu_dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
+
+#include "tiny_plugin.h"
+
+/* Common code for rcu_idle_enter() and rcu_irq_exit(), see kernel/rcutree.c. */
+static void rcu_idle_enter_common(long long newval)
+{
+	if (newval) {
+		RCU_TRACE(trace_rcu_dyntick(TPS("--="),
+					    rcu_dynticks_nesting, newval));
+		rcu_dynticks_nesting = newval;
+		return;
+	}
+	RCU_TRACE(trace_rcu_dyntick(TPS("Start"),
+				    rcu_dynticks_nesting, newval));
+	if (!is_idle_task(current)) {
+		struct task_struct *idle __maybe_unused = idle_task(smp_processor_id());
+
+		RCU_TRACE(trace_rcu_dyntick(TPS("Entry error: not idle task"),
+					    rcu_dynticks_nesting, newval));
+		ftrace_dump(DUMP_ALL);
+		WARN_ONCE(1, "Current pid: %d comm: %s / Idle pid: %d comm: %s",
+			  current->pid, current->comm,
+			  idle->pid, idle->comm); /* must be idle task! */
+	}
+	rcu_sched_qs(0); /* implies rcu_bh_qsctr_inc(0) */
+	barrier();
+	rcu_dynticks_nesting = newval;
+}
+
+/*
+ * Enter idle, which is an extended quiescent state if we have fully
+ * entered that mode (i.e., if the new value of dynticks_nesting is zero).
+ */
+void rcu_idle_enter(void)
+{
+	unsigned long flags;
+	long long newval;
+
+	local_irq_save(flags);
+	WARN_ON_ONCE((rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK) == 0);
+	if ((rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK) ==
+	    DYNTICK_TASK_NEST_VALUE)
+		newval = 0;
+	else
+		newval = rcu_dynticks_nesting - DYNTICK_TASK_NEST_VALUE;
+	rcu_idle_enter_common(newval);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(rcu_idle_enter);
+
+/*
+ * Exit an interrupt handler towards idle.
+ */
+void rcu_irq_exit(void)
+{
+	unsigned long flags;
+	long long newval;
+
+	local_irq_save(flags);
+	newval = rcu_dynticks_nesting - 1;
+	WARN_ON_ONCE(newval < 0);
+	rcu_idle_enter_common(newval);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(rcu_irq_exit);
+
+/* Common code for rcu_idle_exit() and rcu_irq_enter(), see kernel/rcutree.c. */
+static void rcu_idle_exit_common(long long oldval)
+{
+	if (oldval) {
+		RCU_TRACE(trace_rcu_dyntick(TPS("++="),
+					    oldval, rcu_dynticks_nesting));
+		return;
+	}
+	RCU_TRACE(trace_rcu_dyntick(TPS("End"), oldval, rcu_dynticks_nesting));
+	if (!is_idle_task(current)) {
+		struct task_struct *idle __maybe_unused = idle_task(smp_processor_id());
+
+		RCU_TRACE(trace_rcu_dyntick(TPS("Exit error: not idle task"),
+			  oldval, rcu_dynticks_nesting));
+		ftrace_dump(DUMP_ALL);
+		WARN_ONCE(1, "Current pid: %d comm: %s / Idle pid: %d comm: %s",
+			  current->pid, current->comm,
+			  idle->pid, idle->comm); /* must be idle task! */
+	}
+}
+
+/*
+ * Exit idle, so that we are no longer in an extended quiescent state.
+ */
+void rcu_idle_exit(void)
+{
+	unsigned long flags;
+	long long oldval;
+
+	local_irq_save(flags);
+	oldval = rcu_dynticks_nesting;
+	WARN_ON_ONCE(rcu_dynticks_nesting < 0);
+	if (rcu_dynticks_nesting & DYNTICK_TASK_NEST_MASK)
+		rcu_dynticks_nesting += DYNTICK_TASK_NEST_VALUE;
+	else
+		rcu_dynticks_nesting = DYNTICK_TASK_EXIT_IDLE;
+	rcu_idle_exit_common(oldval);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(rcu_idle_exit);
+
+/*
+ * Enter an interrupt handler, moving away from idle.
+ */
+void rcu_irq_enter(void)
+{
+	unsigned long flags;
+	long long oldval;
+
+	local_irq_save(flags);
+	oldval = rcu_dynticks_nesting;
+	rcu_dynticks_nesting++;
+	WARN_ON_ONCE(rcu_dynticks_nesting == 0);
+	rcu_idle_exit_common(oldval);
+	local_irq_restore(flags);
+}
+EXPORT_SYMBOL_GPL(rcu_irq_enter);
+
+#if defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE)
+
+/*
+ * Test whether RCU thinks that the current CPU is idle.
+ */
+bool __rcu_is_watching(void)
+{
+	return rcu_dynticks_nesting;
+}
+EXPORT_SYMBOL(__rcu_is_watching);
+
+#endif /* defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_RCU_TRACE) */
+
+/*
+ * Test whether the current CPU was interrupted from idle.  Nested
+ * interrupts don't count, we must be running at the first interrupt
+ * level.
+ */
+static int rcu_is_cpu_rrupt_from_idle(void)
+{
+	return rcu_dynticks_nesting <= 1;
+}
+
+/*
+ * Helper function for rcu_sched_qs() and rcu_bh_qs().
+ * Also irqs are disabled to avoid confusion due to interrupt handlers
+ * invoking call_rcu().
+ */
+static int rcu_qsctr_help(struct rcu_ctrlblk *rcp)
+{
+	RCU_TRACE(reset_cpu_stall_ticks(rcp));
+	if (rcp->rcucblist != NULL &&
+	    rcp->donetail != rcp->curtail) {
+		rcp->donetail = rcp->curtail;
+		return 1;
+	}
+
+	return 0;
+}
+
+/*
+ * Record an rcu quiescent state.  And an rcu_bh quiescent state while we
+ * are at it, given that any rcu quiescent state is also an rcu_bh
+ * quiescent state.  Use "+" instead of "||" to defeat short circuiting.
+ */
+void rcu_sched_qs(int cpu)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	if (rcu_qsctr_help(&rcu_sched_ctrlblk) +
+	    rcu_qsctr_help(&rcu_bh_ctrlblk))
+		raise_softirq(RCU_SOFTIRQ);
+	local_irq_restore(flags);
+}
+
+/*
+ * Record an rcu_bh quiescent state.
+ */
+void rcu_bh_qs(int cpu)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	if (rcu_qsctr_help(&rcu_bh_ctrlblk))
+		raise_softirq(RCU_SOFTIRQ);
+	local_irq_restore(flags);
+}
+
+/*
+ * Check to see if the scheduling-clock interrupt came from an extended
+ * quiescent state, and, if so, tell RCU about it.  This function must
+ * be called from hardirq context.  It is normally called from the
+ * scheduling-clock interrupt.
+ */
+void rcu_check_callbacks(int cpu, int user)
+{
+	RCU_TRACE(check_cpu_stalls());
+	if (user || rcu_is_cpu_rrupt_from_idle())
+		rcu_sched_qs(cpu);
+	else if (!in_softirq())
+		rcu_bh_qs(cpu);
+}
+
+/*
+ * Invoke the RCU callbacks on the specified rcu_ctrlkblk structure
+ * whose grace period has elapsed.
+ */
+static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp)
+{
+	const char *rn = NULL;
+	struct rcu_head *next, *list;
+	unsigned long flags;
+	RCU_TRACE(int cb_count = 0);
+
+	/* If no RCU callbacks ready to invoke, just return. */
+	if (&rcp->rcucblist == rcp->donetail) {
+		RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, 0, -1));
+		RCU_TRACE(trace_rcu_batch_end(rcp->name, 0,
+					      !!ACCESS_ONCE(rcp->rcucblist),
+					      need_resched(),
+					      is_idle_task(current),
+					      false));
+		return;
+	}
+
+	/* Move the ready-to-invoke callbacks to a local list. */
+	local_irq_save(flags);
+	RCU_TRACE(trace_rcu_batch_start(rcp->name, 0, rcp->qlen, -1));
+	list = rcp->rcucblist;
+	rcp->rcucblist = *rcp->donetail;
+	*rcp->donetail = NULL;
+	if (rcp->curtail == rcp->donetail)
+		rcp->curtail = &rcp->rcucblist;
+	rcp->donetail = &rcp->rcucblist;
+	local_irq_restore(flags);
+
+	/* Invoke the callbacks on the local list. */
+	RCU_TRACE(rn = rcp->name);
+	while (list) {
+		next = list->next;
+		prefetch(next);
+		debug_rcu_head_unqueue(list);
+		local_bh_disable();
+		__rcu_reclaim(rn, list);
+		local_bh_enable();
+		list = next;
+		RCU_TRACE(cb_count++);
+	}
+	RCU_TRACE(rcu_trace_sub_qlen(rcp, cb_count));
+	RCU_TRACE(trace_rcu_batch_end(rcp->name,
+				      cb_count, 0, need_resched(),
+				      is_idle_task(current),
+				      false));
+}
+
+static void rcu_process_callbacks(struct softirq_action *unused)
+{
+	__rcu_process_callbacks(&rcu_sched_ctrlblk);
+	__rcu_process_callbacks(&rcu_bh_ctrlblk);
+}
+
+/*
+ * Wait for a grace period to elapse.  But it is illegal to invoke
+ * synchronize_sched() from within an RCU read-side critical section.
+ * Therefore, any legal call to synchronize_sched() is a quiescent
+ * state, and so on a UP system, synchronize_sched() need do nothing.
+ * Ditto for synchronize_rcu_bh().  (But Lai Jiangshan points out the
+ * benefits of doing might_sleep() to reduce latency.)
+ *
+ * Cool, huh?  (Due to Josh Triplett.)
+ *
+ * But we want to make this a static inline later.  The cond_resched()
+ * currently makes this problematic.
+ */
+void synchronize_sched(void)
+{
+	rcu_lockdep_assert(!lock_is_held(&rcu_bh_lock_map) &&
+			   !lock_is_held(&rcu_lock_map) &&
+			   !lock_is_held(&rcu_sched_lock_map),
+			   "Illegal synchronize_sched() in RCU read-side critical section");
+	cond_resched();
+}
+EXPORT_SYMBOL_GPL(synchronize_sched);
+
+/*
+ * Helper function for call_rcu() and call_rcu_bh().
+ */
+static void __call_rcu(struct rcu_head *head,
+		       void (*func)(struct rcu_head *rcu),
+		       struct rcu_ctrlblk *rcp)
+{
+	unsigned long flags;
+
+	debug_rcu_head_queue(head);
+	head->func = func;
+	head->next = NULL;
+
+	local_irq_save(flags);
+	*rcp->curtail = head;
+	rcp->curtail = &head->next;
+	RCU_TRACE(rcp->qlen++);
+	local_irq_restore(flags);
+}
+
+/*
+ * Post an RCU callback to be invoked after the end of an RCU-sched grace
+ * period.  But since we have but one CPU, that would be after any
+ * quiescent state.
+ */
+void call_rcu_sched(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+{
+	__call_rcu(head, func, &rcu_sched_ctrlblk);
+}
+EXPORT_SYMBOL_GPL(call_rcu_sched);
+
+/*
+ * Post an RCU bottom-half callback to be invoked after any subsequent
+ * quiescent state.
+ */
+void call_rcu_bh(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
+{
+	__call_rcu(head, func, &rcu_bh_ctrlblk);
+}
+EXPORT_SYMBOL_GPL(call_rcu_bh);
+
+void rcu_init(void)
+{
+	open_softirq(RCU_SOFTIRQ, rcu_process_callbacks);
+}
